2005.mtsummit-papers.36,N01-1018,0,0.0681459,"ation of multiple feature functions is implemented (Och and Ney, 2002). Additionally, original word-based translation models (Brown et al., 1993) have been replaced by phrase-based translation models (Zens et al., 2002) and (Koehn et al., 2003), which are estimated from aligned bilingual corpora by using relative frequencies. 275 On the other hand, the translation problem has also been approached from the ﬁnite-state perspective as the most natural way for integrating speech recognition and machine translation into a speech-to-speech translation system (Riccardi et al., 1996), (Vidal, 1997), (Bangalore and Riccardi, 2001) and (Casacuberta, 2001). In this kind of systems the translation model constitutes a ﬁnite-state network which is learned from training data. The translation system described in this paper implements a translation model based on the ﬁnite-state perspective, (de Gispert and Mari˜ no, 2002) and (de Gispert et al., 2004), which is used along with a log-linear combination of four additional feature functions (Crego et al., 2005). The implemented translation model, which is referred to as tuple n-gram model, diﬀers from the well known phrase-model approach (Koehn et al., 2003) in two basic issues."
2005.mtsummit-papers.36,J96-1002,0,0.115652,"entioned in the introduction, the translation system presented here implements a log-linear combination of feature functions 1 In the present version of the system, target words aligned to NULL are always attached to the following word. Further work in this area is proposed in the last section. along with the tuple n-gram model. This section describes the log-linear model and each of the four speciﬁc feature functions that are used. Finally, a brief description of the customized decoding tool that is used is presented. 3.1 Log-Linear Model Framework According to the maximum entropy framework (Berger et al., 1996), the corresponding translation hypothesis T , for a given source sentence S, is deﬁned by the target sentence that maximizes a log-linear combination of feature functions hi (S, T ), as described in the following equation:  argmax λi hi (S, T ) (2) T i where the λi ’s constitute the weighting coeﬃcients of the log-linear combination and the feature function hi (S, T ) corresponds to a logarithmic scaling of the ith -model probabilities. These weights are computed via an optimization procedure which maximizes the translation BLEU (Papineni et al., 2002) over a given development set. This opti"
2005.mtsummit-papers.36,J90-2002,0,0.523079,"Missing"
2005.mtsummit-papers.36,J93-2003,0,0.0465928,"ementation of translation algorithms based on statistical methods (Brown et al., 1990) and (1993). The ﬁrst SMT systems were based on the noisy channel approach, which models the probability of a target language sentence T given a source language sentence S as a translation model probability p(S|T ) times a target language model probability p(T ). In recent systems such an approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney, 2002). Additionally, original word-based translation models (Brown et al., 1993) have been replaced by phrase-based translation models (Zens et al., 2002) and (Koehn et al., 2003), which are estimated from aligned bilingual corpora by using relative frequencies. 275 On the other hand, the translation problem has also been approached from the ﬁnite-state perspective as the most natural way for integrating speech recognition and machine translation into a speech-to-speech translation system (Riccardi et al., 1996), (Vidal, 1997), (Bangalore and Riccardi, 2001) and (Casacuberta, 2001). In this kind of systems the translation model constitutes a ﬁnite-state network which is l"
2005.mtsummit-papers.36,N04-1033,0,0.157603,"Missing"
2005.mtsummit-papers.36,2005.iwslt-1.23,1,0.857305,"Missing"
2005.mtsummit-papers.36,2004.iwslt-evaluation.14,1,0.687498,"Missing"
2005.mtsummit-papers.36,P05-2012,1,0.78658,"Missing"
2005.mtsummit-papers.36,N03-1017,0,0.067546,"The ﬁrst SMT systems were based on the noisy channel approach, which models the probability of a target language sentence T given a source language sentence S as a translation model probability p(S|T ) times a target language model probability p(T ). In recent systems such an approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney, 2002). Additionally, original word-based translation models (Brown et al., 1993) have been replaced by phrase-based translation models (Zens et al., 2002) and (Koehn et al., 2003), which are estimated from aligned bilingual corpora by using relative frequencies. 275 On the other hand, the translation problem has also been approached from the ﬁnite-state perspective as the most natural way for integrating speech recognition and machine translation into a speech-to-speech translation system (Riccardi et al., 1996), (Vidal, 1997), (Bangalore and Riccardi, 2001) and (Casacuberta, 2001). In this kind of systems the translation model constitutes a ﬁnite-state network which is learned from training data. The translation system described in this paper implements a translation"
2005.mtsummit-papers.36,P02-1038,0,0.221853,"otivated by the development of computer resources needed to allow the implementation of translation algorithms based on statistical methods (Brown et al., 1990) and (1993). The ﬁrst SMT systems were based on the noisy channel approach, which models the probability of a target language sentence T given a source language sentence S as a translation model probability p(S|T ) times a target language model probability p(T ). In recent systems such an approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney, 2002). Additionally, original word-based translation models (Brown et al., 1993) have been replaced by phrase-based translation models (Zens et al., 2002) and (Koehn et al., 2003), which are estimated from aligned bilingual corpora by using relative frequencies. 275 On the other hand, the translation problem has also been approached from the ﬁnite-state perspective as the most natural way for integrating speech recognition and machine translation into a speech-to-speech translation system (Riccardi et al., 1996), (Vidal, 1997), (Bangalore and Riccardi, 2001) and (Casacuberta, 2001). In this kind of"
2005.mtsummit-papers.36,N04-1021,0,0.0290838,"length penalization in order to compensate the system preference for short target sentences caused by the presence of the previous target language model. This penalization depends on the total number of words contained in the partial translation hypothesis, and it is computed as follows: wp(Tk ) = exp(number of words in Tk ) (4) where, again, Tk refers to the partial translation hypothesis. The fourth and ﬁfth feature functions correspond to a forward and backward lexicon models. These models provide IBM 1 translation probabilities for each tuple based on the IBM 1 lexical parameters p(t|s) (Och et al., 2004). These lexicon models are computed according to the following equation: pIBM 1 ((t, s)n ) = I J   1 p(tin |sjn ) (5) (I + 1)J j=1 i=0 (3) where sjn and tin are the j th and ith words in the source and target sides of tuple (t, s)n , being J and I the corresponding total number words in each side of it. For computing the forward lexicon model, IBM model 1 lexical parameters from GIZA++ source-to-target alignments are used. In the case of the backward lexicon model, GIZA++ target-to-source alignments are used instead. where Tk refers to the partial translation hypothesis and wn to the nth wor"
2005.mtsummit-papers.36,P02-1040,0,0.115781,"cording to the maximum entropy framework (Berger et al., 1996), the corresponding translation hypothesis T , for a given source sentence S, is deﬁned by the target sentence that maximizes a log-linear combination of feature functions hi (S, T ), as described in the following equation:  argmax λi hi (S, T ) (2) T i where the λi ’s constitute the weighting coeﬃcients of the log-linear combination and the feature function hi (S, T ) corresponds to a logarithmic scaling of the ith -model probabilities. These weights are computed via an optimization procedure which maximizes the translation BLEU (Papineni et al., 2002) over a given development set. This optimization is performed by using an in-house developed optimization algorithm, which is based on a simplex method (Press et al., 2002). 3.2 Implemented Feature Functions The proposed translation system implements a total of ﬁve feature functions: • tuple 3-gram model, • target language model, • word penalty model, • source-to-target lexicon model, and • target-to-source lexicon model. The ﬁrst of these functions is the tuple 3-gram model, which was already described in the previous section. The second feature function implemented is a target language model"
2005.mtsummit-papers.36,2002.tmi-tutorials.2,0,0.0433398,"al., 1990) and (1993). The ﬁrst SMT systems were based on the noisy channel approach, which models the probability of a target language sentence T given a source language sentence S as a translation model probability p(S|T ) times a target language model probability p(T ). In recent systems such an approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney, 2002). Additionally, original word-based translation models (Brown et al., 1993) have been replaced by phrase-based translation models (Zens et al., 2002) and (Koehn et al., 2003), which are estimated from aligned bilingual corpora by using relative frequencies. 275 On the other hand, the translation problem has also been approached from the ﬁnite-state perspective as the most natural way for integrating speech recognition and machine translation into a speech-to-speech translation system (Riccardi et al., 1996), (Vidal, 1997), (Bangalore and Riccardi, 2001) and (Casacuberta, 2001). In this kind of systems the translation model constitutes a ﬁnite-state network which is learned from training data. The translation system described in this paper"
2005.mtsummit-papers.36,P00-1056,0,\N,Missing
2006.iwslt-evaluation.18,2005.iwslt-1.24,1,0.787344,"TALP phrase-based statistical machine translation system, enriched with the statistical machine reordering technique. We also report the combination of this system and the TALP-tuple, the n-gram-based statistical machine translation system. We report the results for all the tasks (Chinese, Arabic, Italian and Japanese to English) in the framework of the third evaluation campaign of the International Workshop on Spoken Language Translation. 1. Introduction This paper describes the TALP-phrase system for the IWSLT 2006, which is an enhanced version of the system reported in the 2005 evaluation [1]. The main difference is the integration of a new reordering technique called statistical machine reordering, which was presented in [2] in a different framework. Additionally, we report the results of combining the outputs of the two statistical machine translation TALP systems: phrase-based and n-gram-based. The latter of the two also participated in the 2005 evaluation and is described in [3]. Statistical machine translation systems are now usually modelled through a log-linear maximum entropy framework. e˜ = argmax e ( M X m=1 λm hm (e, f ) ) TALP-tuple. Finally, in Section 4, we report th"
2006.iwslt-evaluation.18,W06-1609,1,0.921033,"e combination of this system and the TALP-tuple, the n-gram-based statistical machine translation system. We report the results for all the tasks (Chinese, Arabic, Italian and Japanese to English) in the framework of the third evaluation campaign of the International Workshop on Spoken Language Translation. 1. Introduction This paper describes the TALP-phrase system for the IWSLT 2006, which is an enhanced version of the system reported in the 2005 evaluation [1]. The main difference is the integration of a new reordering technique called statistical machine reordering, which was presented in [2] in a different framework. Additionally, we report the results of combining the outputs of the two statistical machine translation TALP systems: phrase-based and n-gram-based. The latter of the two also participated in the 2005 evaluation and is described in [3]. Statistical machine translation systems are now usually modelled through a log-linear maximum entropy framework. e˜ = argmax e ( M X m=1 λm hm (e, f ) ) TALP-tuple. Finally, in Section 4, we report the results obtained for all the tasks of the evaluation, which include the translations from Chinese, Arabic, Italian and Japanese to Eng"
2006.iwslt-evaluation.18,W06-3125,1,0.836838,"ional Workshop on Spoken Language Translation. 1. Introduction This paper describes the TALP-phrase system for the IWSLT 2006, which is an enhanced version of the system reported in the 2005 evaluation [1]. The main difference is the integration of a new reordering technique called statistical machine reordering, which was presented in [2] in a different framework. Additionally, we report the results of combining the outputs of the two statistical machine translation TALP systems: phrase-based and n-gram-based. The latter of the two also participated in the 2005 evaluation and is described in [3]. Statistical machine translation systems are now usually modelled through a log-linear maximum entropy framework. e˜ = argmax e ( M X m=1 λm hm (e, f ) ) TALP-tuple. Finally, in Section 4, we report the results obtained for all the tasks of the evaluation, which include the translations from Chinese, Arabic, Italian and Japanese to English. 2. Description of the TALP-phrase System 2.1. Phrase-based Model The basic idea of phrase-based translation is to segment the given source sentence into units (here called phrases), then translate each phrase and finally compose the target sentence from th"
2006.iwslt-evaluation.18,P02-1038,0,0.0577834,"re consecutive along both sides of the bilingual phrase and (2) no word on either side of the phrase is aligned to a word outside the phrase. 2.2. Feature functions The baseline phrase-based system implements a log-linear combination of four feature functions, which are described as follows. • The translation model is estimated with relative frequencies. Given the collected phrase pairs, we estimate the phrase translation probability distribution by relative frequency in both directions. (1) The feature functions, hm , and weights, λi , are typically optimized to maximize the scoring function [4]. Two basic issues differentiate the n-gram-based system from the phrase-based system: the bilingual units are extracted from a monotonic segmentation of the training data; the unit probabilities are based on a standard back-off language model rather than directly on relative frequencies. In both systems, the introduction of reordering capabilities is crucial for certain language pairs. This paper is organized as follows. Section 2 describes the TALP-phrase system, with particular emphasis on a new reordering technique: the statistical machine reordering approach. In Section 3, we combine the"
2006.iwslt-evaluation.18,J04-4002,0,0.0302302,"X m=1 λm hm (e, f ) ) TALP-tuple. Finally, in Section 4, we report the results obtained for all the tasks of the evaluation, which include the translations from Chinese, Arabic, Italian and Japanese to English. 2. Description of the TALP-phrase System 2.1. Phrase-based Model The basic idea of phrase-based translation is to segment the given source sentence into units (here called phrases), then translate each phrase and finally compose the target sentence from these phrase translations. Given a sentence pair and a corresponding word alignment, phrases are extracted following the criterion in [5]. A phrase (or bilingual phrase) is any pair of m source words and n target words that satisfies two basic constraints (1) words are consecutive along both sides of the bilingual phrase and (2) no word on either side of the phrase is aligned to a word outside the phrase. 2.2. Feature functions The baseline phrase-based system implements a log-linear combination of four feature functions, which are described as follows. • The translation model is estimated with relative frequencies. Given the collected phrase pairs, we estimate the phrase translation probability distribution by relative frequen"
2006.iwslt-evaluation.18,2005.iwslt-1.6,0,0.0322379,"using the GIZA++ tool [7]. During word alignment, we used 50 classes per language. We aligned both translation directions and combined the two alignments with the union operation. train dev4 dev123 test ASRtest • Word classes (which were used to help the aligner and to perform the SMR process) were determined using “mkcls”, a tool freely-available with GIZA++. • The language model was estimated using the SRILM toolkit [8]. • The decoder was MARIE [9]. • The optimization tool used for computing log-linear weights was based on the simplex method [6]. Following the consensus strategy proposed in [10], the objective function was set to 100 · BLEU + 4 · N IST . 489 500 500 500 voc. 9.7k 9.6k 1,096 909 1,292 1,311 slen. 6.7 7.0 11.2 6.0 11.7 11.6 refs. 1 7 16 7 7 Corpus statistics for all language pairs can be found in Tables 1, 2, 3 and 4, respectively, where number of sentences, running words, vocabulary, sentence length and human references are shown. sent. train dev4 dev123 test ASRtest Experiments were carried out for all tasks of the IWSLT06 evaluation (Zh2En, Jp2En, Ar2En and It2En) using the BTEC Corpus provided for the open data track1 . it en it it it it 24.6k 489 500 500 500 wrds"
2006.iwslt-evaluation.18,A00-1031,0,0.0229282,"while randomly selecting 500 sentences from developments 1, 2 and 3 (around 160 sentences from each) to build an internal test set (dev123). zh en zh zh zh zh 4.4. Language-dependent preprocessing For all language pairs, training sentences were split by using full stops on both sides of the bilingual text (when the number of stops was equal), increasing the number of sentences and reducing their length. Specific preprocessing for each language is detailed in the respective section below. 4.4.1. English English preprocessing includes part-of-speech tagging using the freely-available TnT tagger [11] and lemmatization using wnmorph, included in the WordNet package [12]. 126 sent. train dev4 dev123 test ASRtest jp en jp jp jp jp 45.2k 489 500 500 500 wrds 390k 325k 6,758 3,818 7,367 7,494 voc. 10.6k 9.6k 1,169 936 1,301 1,331 slen. 8.6 7.2 13.8 7.6 14.7 15.0 refs. 1 7 16 7 7 Table 4: Japanese→English corpus statistics. 4.4.2. Arabic Following a similar approach to that taken in [13], we use the Buckwalter Arabic Morphological Analyzer2 to obtain possible word analyses for Arabic, and disambiguate them using the Morphological Analysis and Disambiguation for Arabic (MADA) tool [14], kindly p"
2006.iwslt-evaluation.18,N06-2013,0,0.0411805,"ing their length. Specific preprocessing for each language is detailed in the respective section below. 4.4.1. English English preprocessing includes part-of-speech tagging using the freely-available TnT tagger [11] and lemmatization using wnmorph, included in the WordNet package [12]. 126 sent. train dev4 dev123 test ASRtest jp en jp jp jp jp 45.2k 489 500 500 500 wrds 390k 325k 6,758 3,818 7,367 7,494 voc. 10.6k 9.6k 1,169 936 1,301 1,331 slen. 8.6 7.2 13.8 7.6 14.7 15.0 refs. 1 7 16 7 7 Table 4: Japanese→English corpus statistics. 4.4.2. Arabic Following a similar approach to that taken in [13], we use the Buckwalter Arabic Morphological Analyzer2 to obtain possible word analyses for Arabic, and disambiguate them using the Morphological Analysis and Disambiguation for Arabic (MADA) tool [14], kindly provided by the University of Columbia. Once analyzed, Arabic words are segmented by separating all prefixes (prepositions, conjunctions, the article and the future marker) and suffixes (pronominal clitics). The tool also provides POS tags for the resultant tokens. search (with m = 5 and j = 3) for all tasks and for all systems (with or without SMR technique); except for the Italian to E"
2006.iwslt-evaluation.18,P05-1071,0,0.0382995,"nT tagger [11] and lemmatization using wnmorph, included in the WordNet package [12]. 126 sent. train dev4 dev123 test ASRtest jp en jp jp jp jp 45.2k 489 500 500 500 wrds 390k 325k 6,758 3,818 7,367 7,494 voc. 10.6k 9.6k 1,169 936 1,301 1,331 slen. 8.6 7.2 13.8 7.6 14.7 15.0 refs. 1 7 16 7 7 Table 4: Japanese→English corpus statistics. 4.4.2. Arabic Following a similar approach to that taken in [13], we use the Buckwalter Arabic Morphological Analyzer2 to obtain possible word analyses for Arabic, and disambiguate them using the Morphological Analysis and Disambiguation for Arabic (MADA) tool [14], kindly provided by the University of Columbia. Once analyzed, Arabic words are segmented by separating all prefixes (prepositions, conjunctions, the article and the future marker) and suffixes (pronominal clitics). The tool also provides POS tags for the resultant tokens. search (with m = 5 and j = 3) for all tasks and for all systems (with or without SMR technique); except for the Italian to English task where a monotonic search was used. The primary system of each task is that which had the best performance in the internal test. In all tasks, the SMR improved the results in the internal te"
2006.iwslt-evaluation.18,W03-1730,0,0.0516051,"Missing"
2006.iwslt-evaluation.18,atserias-etal-2006-freeling,0,0.0138517,"for the internal test set (specially, for the Arabic and Japanese tasks). The higher the number of unknown words, the worse the SMR output and, consequently, the quality of translation. Here, a possible solution would be to predict word classes for unknown words in order to avoid their bad influence in the SMR output. 4.4.3. Chinese Set development test evaluation Chinese preprocessing included re-segmentation and POStagging. These tasks were performed using ICTCLAS [15]. 4.4.4. Italian Italian was POS-tagged and lemmatized using the freelyavailable FreeLing morpho-syntactic analysis package [16]. Additionally, Italian contracted prepositions were separated into preposition + article, for example ’alla’→’a la’, ’degli’→’di gli’ or ’dallo’→’da lo’. 4.4.5. Japanese When dealing with Japanese, one has to come up with new methods for overcoming the absence of delimiters between words. We addressed this issue by word segmentation using the freely available JUMAN tool [17] version 5.1. This tool was also used for POS-tagging of the Japanese text. 4.5. Results In Table 6 we show the results for all the TALP systems that participated in the IWSLT 2006: the TALP-phrase, the TALP-tuple and the"
2006.iwslt-evaluation.18,2004.iwslt-evaluation.8,0,\N,Missing
2006.iwslt-papers.5,P02-1038,0,0.052325,"eriments with the Simultaneous Perturbation Stochastic Approximation (SPSA) algorithm, and compare them to tuning with the widely used downhill simplex method. With IWSLT 2006 Chinese-English data, both methods showed similar performance, but SPSA was more robust to the choice of initial settings. 1. Introduction Statistical machine translation (SMT) was originally based on the noisy channel approach [1]. In present SMT systems, the noisy channel approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented [2]. Translation quality can be improved by adjusting the weight of each feature function in the log-linear combination. This can be effectively performed by minimising translation error over a development corpus for which manually translated references are available [3]. This minimisation problem in multiple dimensions is difficult because of three main characteristics of the objective function. Firstly, it has no analytic representation, so the gradient cannot be calculated. Secondly, it has many local minima. Finally, its evaluation has a significant computational cost (depending on the scheme"
2006.iwslt-papers.5,P03-1021,0,0.34622,"he choice of initial settings. 1. Introduction Statistical machine translation (SMT) was originally based on the noisy channel approach [1]. In present SMT systems, the noisy channel approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented [2]. Translation quality can be improved by adjusting the weight of each feature function in the log-linear combination. This can be effectively performed by minimising translation error over a development corpus for which manually translated references are available [3]. This minimisation problem in multiple dimensions is difficult because of three main characteristics of the objective function. Firstly, it has no analytic representation, so the gradient cannot be calculated. Secondly, it has many local minima. Finally, its evaluation has a significant computational cost (depending on the scheme, it implies translating the development corpus or re-ranking an n-best list for this corpus, and calculating some translation error measure). Gradient may be approximated, but this is costly since it requires typically as many function evaluations as the number of sc"
2006.iwslt-papers.5,2005.mtsummit-posters.19,0,0.0137668,"computational cost (depending on the scheme, it implies translating the development corpus or re-ranking an n-best list for this corpus, and calculating some translation error measure). Gradient may be approximated, but this is costly since it requires typically as many function evaluations as the number of scaling factors. Thus, algorithms based on derivatives are discarded. Algorithms which require many objective functions evaluations, such as simulated annealing or genetic algorithms, are also discarded. Two popular alternatives are Powell’s method [4, 5, 3] and the downhill simplex method [6, 5, 7]. In recent experiments at the 2006 John Hopkins University Summer Workshop on SMT, both methods achieved similar performance [8]. The simplex method is self-contained and straightforward and thus widely used for SMT tuning [7, 9, 10, 11], although it is not very efficient in terms of number of objective function evaluations for a high number of dimensions [12]. However, from the authors experience, a slight modification of initial parameters in the simplex optimisation can result in an appreciable difference in both the value of the local minimum found and the value of the optimal parameters."
2006.iwslt-papers.5,W05-0821,0,0.0206377,"y since it requires typically as many function evaluations as the number of scaling factors. Thus, algorithms based on derivatives are discarded. Algorithms which require many objective functions evaluations, such as simulated annealing or genetic algorithms, are also discarded. Two popular alternatives are Powell’s method [4, 5, 3] and the downhill simplex method [6, 5, 7]. In recent experiments at the 2006 John Hopkins University Summer Workshop on SMT, both methods achieved similar performance [8]. The simplex method is self-contained and straightforward and thus widely used for SMT tuning [7, 9, 10, 11], although it is not very efficient in terms of number of objective function evaluations for a high number of dimensions [12]. However, from the authors experience, a slight modification of initial parameters in the simplex optimisation can result in an appreciable difference in both the value of the local minimum found and the value of the optimal parameters. This difference is transmitted when these parameters are used to translate a test corpus. When a translation system is compared to a baseline, the difference arising only from the tuning process can be even greater than the difference ar"
2006.iwslt-papers.5,2005.mtsummit-papers.36,1,0.871032,"y since it requires typically as many function evaluations as the number of scaling factors. Thus, algorithms based on derivatives are discarded. Algorithms which require many objective functions evaluations, such as simulated annealing or genetic algorithms, are also discarded. Two popular alternatives are Powell’s method [4, 5, 3] and the downhill simplex method [6, 5, 7]. In recent experiments at the 2006 John Hopkins University Summer Workshop on SMT, both methods achieved similar performance [8]. The simplex method is self-contained and straightforward and thus widely used for SMT tuning [7, 9, 10, 11], although it is not very efficient in terms of number of objective function evaluations for a high number of dimensions [12]. However, from the authors experience, a slight modification of initial parameters in the simplex optimisation can result in an appreciable difference in both the value of the local minimum found and the value of the optimal parameters. This difference is transmitted when these parameters are used to translate a test corpus. When a translation system is compared to a baseline, the difference arising only from the tuning process can be even greater than the difference ar"
2006.iwslt-papers.5,W06-3125,1,0.814894,"imate the gradient as in equation 2 Step 5 Update λ estimate as in equation 1 Step 6 Iteration or termination. Return to Step 2 with k + 1 replacing k. Terminate if the maximum number of iterations have been reached or if there is little change in several successive iterates. 3. Experimental Settings 3.1. Translation system used Although the following discussion would be valid in many contexts, and in particular for any Empirical MT system, it is convenient here to present briefly the models implemented by our system, and whose respective weights are tuned. For a more complete description see [16]. The SMT approach used here considers a translation model which is based on a 4-grams language model of bilingual units which are referred to as tuples. Tuples are extracted from Viterbi alignments and can be formally defined as the set of shortest phrases that provides a monotonic segmentation of the bilingual corpus. In addition to the bilingual 4-gram translation model, the translation system implements a log linear combination of five additional feature functions: a 4-gram language model of the target language (denoted TM); a 4-gram language model of target POS-tags (TTM) which helps, alo"
2006.iwslt-papers.5,2001.mtsummit-papers.68,0,0.0164973,"down in which range the scaling factor of each model behaved. We selected the initial value of each parameter randomly within its corresponding range. Table 1 displays the initial points 1 Actually, 191 SPSA could also be used instead used in the experiments. ID 1 2 3 4 5 6 7 TM 0.29 0.5 0.58 1 1.1 1.2 1.3 TTM 0.52 0.5 0.42 1 0.22 0.53 0.34 WB 0.32 0.5 1.4 1 1.5 1.5 1.2 L1 1.7 0.5 0.2 1 1.6 1.3 0.85 L2 0.84 0.5 0.075 1 0.29 0.89 0.44 Table 1: Sets of initial parameters used in the experiments. In table 3 points are referred to by their ID number. The error function we choose is the BLEU score [17]. Actually it does not measure an error but a translation accuracy, so its opposite is to be minimised. were T was empirically set to 0.005. According to this probability distribution, the worse the new set of lambdas, the less probability to accept it. ˆ k ), we reSince we introduced an evaluation of E(λ placed the two-sided gradient approximation by a one-sided ˆ k ) and E(λ ˆk + one, which involves evaluations of E(λ perturbation). If the noise caused by the one-sided approximation (as opposed to the two-sided approximation) is small compared to the noise arising from the simultaneous appro"
2006.iwslt-papers.5,J06-4004,1,0.901943,".1±0.47 18.9±0.89 19.0±0.72 18.9±0.54 19.1±0.71 18.9±0.65 8 6 4 2 0 0 0.5 1 1.5 2 2.5 L1 WEIGHT 14 SPSA 12 10 NUMBER OF CASES ID 8 6 4 Table 4: Average BLEU score and standard deviation obtained with the simplex method (above) and SPSA method (below) in the development set, after 20, 40, 60, and 90 function evaluations, for each seed used to generate different algorithm conditions. Only seed ID numbers are displayed. 2 0 0 0.5 1 1.5 2 2.5 L1 WEIGHT Figure 1: Histogram of L1 model weights for simplex (above) and SPSA (below) nant, although this model has got a big impact in translation quality [19]. This is an indication of the interdependence of the various models. Figure 1 also suggests that there would be no point in averaging parameter values in order to gain generalisation power. As ultimate goal, we need to see if the stability of SPSA optimisations is conserved when translating new text (i.e. the test corpus). For each initial point and seed, and after a given number of function evaluations, we collected the optimum parameter set over the development corpus, and translated the test corpus with these parameters. Results are brought together in Table 5. Table 5 instructs, as expect"
2006.iwslt-papers.5,2006.iwslt-evaluation.17,1,\N,Missing
2006.iwslt-papers.5,J93-2003,0,\N,Missing
2006.iwslt-papers.5,P02-1040,0,\N,Missing
2007.iwslt-1.26,2006.iwslt-papers.2,1,0.882609,"4]. Efforts have been focused on improving translation according to human evaluation by further developing different stages of the SMT system: alignment and rescoring. As in previous years, we aligned the training corpus using Giza++ software. However, instead of keeping the default parameters, we performed a minimum translation error training procedure to adjust Giza++ smoothing parameters to the task. This procedure had been successful with an alignment system based on discriminative training [5]. For the rescoring we incorporate a neural network language model as previously experienced in [6]. The neural network language model mainly is able to produce a better generalization in the translation system. This paper is organized as follows. Section 2 briefly reviews last year’s system, including tuple definition and extraction, translation model and feature functions, decoding tool and reordering and optimization criterion. Section 3 describes the alignment translation-minimum-error training procedure. Section 4 focuses on rescoring using a neural language model (NNLM). Next, Section 5 reports on all experiments carried out from Arabic and Chinese into English for IWSLT 2007. Finally"
2007.iwslt-1.26,N04-1033,0,0.0623584,"ls, our translation model is estimated as a standard n-gram model of a bilingual language expressed in tuples. In this way, it approximates the joint probability between source and target languages capturing bilingual context, as described by the following equation: p(S, T ) = K Y p((˜ s, t˜)k |(˜ s, t˜)k−N +1 , ..., (˜ s, t˜)k−1 ) (1) k=1 where s refers to source, t to target, and (˜ s, t˜)k to the k th tuple of a given bilingual sentence pair segmented in K tuples. 2.2. Tuple extraction Given a certain word-aligned parallel corpus, tuples are extracted according to the following constraints [9]: • a monotonic segmentation of each bilingual sentence pair is produced • no word in a tuple is aligned to words outside of it • no smaller tuples can be extracted without violating the previous constraints However, when dealing with pairs of languages with nonmonotonic word order, a certain reordering strategy is required to extract more reusable units (less sparse). Hence, we allow the source words to be reordered before extracting translation units from training sentence pairs by following the word-to-word alignments. The unfolding technique is fully described in [10]. Figure 1 shows an ex"
2007.iwslt-1.26,2005.mtsummit-papers.37,1,0.856996,"following constraints [9]: • a monotonic segmentation of each bilingual sentence pair is produced • no word in a tuple is aligned to words outside of it • no smaller tuples can be extracted without violating the previous constraints However, when dealing with pairs of languages with nonmonotonic word order, a certain reordering strategy is required to extract more reusable units (less sparse). Hence, we allow the source words to be reordered before extracting translation units from training sentence pairs by following the word-to-word alignments. The unfolding technique is fully described in [10]. Figure 1 shows an example of tuple unfolding compared to the monotonic extraction. The unfolding technique produces a different bilingual n-gram language model with reordered source words. where tn refers to the nth word in the partial translation hypothesis T . Usually, this feature is accompanied by a word bonus model based on sentence length, compensating the target language model preference for short sentences (in number of target words). This bonus depends on the number of target words in the partial hypothesis, denoted as: pW P (T ) = exp(number of words in T ). The third and fourth fe"
2007.iwslt-1.26,2006.iwslt-papers.5,1,0.867108,"ChineseEnglish task, a secondary run was performed with a rescoring module, as described in Sections 4 and 5.3.2. 2.5. Feature Weights Optimization To tune the weight of each feature function in the SMT system, we used the Simultaneous Perturbation Stochastic Approximation (SPSA) algorithm [12]. SPSA is a stochastic implementation of the conjugate gradient method which requires only two evaluations of the objective function in each iteration, regardless of the dimension of the optimization problem. It was observed to be more robust than the Downhill Simplex method when tuning SMT coefficients [13]. The SPSA procedure is in the general recursive stochastic approximation form: ˆ k+1 = λ ˆ k − ak g ˆk ) ˆk (λ λ (5) lation tuples (as no word within a tuple can be linked to a word out of it [9]). Starting from the monotonic graph, each sequence of input POS tags fulfilling a source-side rewrite rule implies the addition of a reordering arc (which encodes the reordering detailed in the target-side of the rule). Figure 2 shows how three rewrite rules applied over an input sentence extend the search graph given the reordering patterns that match the source POS tag sequence 1 . ˆ k ) is the esˆ"
2007.iwslt-1.26,N07-2022,1,0.823419,"o phrase-based and other state-of-the-art systems in previous evaluation campaigns, as shown in [3, 4]. Efforts have been focused on improving translation according to human evaluation by further developing different stages of the SMT system: alignment and rescoring. As in previous years, we aligned the training corpus using Giza++ software. However, instead of keeping the default parameters, we performed a minimum translation error training procedure to adjust Giza++ smoothing parameters to the task. This procedure had been successful with an alignment system based on discriminative training [5]. For the rescoring we incorporate a neural network language model as previously experienced in [6]. The neural network language model mainly is able to produce a better generalization in the translation system. This paper is organized as follows. Section 2 briefly reviews last year’s system, including tuple definition and extraction, translation model and feature functions, decoding tool and reordering and optimization criterion. Section 3 describes the alignment translation-minimum-error training procedure. Section 4 focuses on rescoring using a neural language model (NNLM). Next, Section 5"
2007.iwslt-1.26,N06-2013,0,0.076004,"fluency and METEOR is well correlated to adequacy [4], we supposed that adding all references was beneficial to monolingual language models but not to the bilingual language model. Table 2: Chinese→English corpus statistics. 5.2. Data Preprocessing For all language pairs, training sentences were split by using final dots on both sides of the bilingual text (when the number of dots was equal), increasing the number of sentences and reducing its length. Specific preprocessing for each language is detailed in the following respective section. 5.2.1. Arabic Following a similar approach to that in [16], we used the MADA+TOKAN system for disambiguation and tokenization. For disambiguation only diacritic uni-gram statistics were employed. For tokenization we used the D3 scheme with -TAGBIES option. The D3 scheme splits the following set of clitics: w+, f+, b+, k+, l+, Al+ and pronominal clitics. The -TAGBIES option produces Bies POS tags on all taggable tokens. 5.2.2. Chinese Chinese preprocessing included re-segmentation using ICTCLAS [17] and POS tagging using the freely available Stanford Parser4 . 5.2.3. English English preprocessing includes Part-Of-Speech tagging using freely-available"
2007.iwslt-1.26,W03-1730,0,0.018621,"ts length. Specific preprocessing for each language is detailed in the following respective section. 5.2.1. Arabic Following a similar approach to that in [16], we used the MADA+TOKAN system for disambiguation and tokenization. For disambiguation only diacritic uni-gram statistics were employed. For tokenization we used the D3 scheme with -TAGBIES option. The D3 scheme splits the following set of clitics: w+, f+, b+, k+, l+, Al+ and pronominal clitics. The -TAGBIES option produces Bies POS tags on all taggable tokens. 5.2.2. Chinese Chinese preprocessing included re-segmentation using ICTCLAS [17] and POS tagging using the freely available Stanford Parser4 . 5.2.3. English English preprocessing includes Part-Of-Speech tagging using freely-available TnT tagger [18]. For alignment purpose only (of the ZhEn system), the English corpus was stemmed using the Snowball stemmer 5 , based on Porter’s algorithm. 5.3. Results 5.3.1. Alignment In the ZhEn system development work, we tried to improve word alignment by stemming the English corpus and make use of classes [19]. We also performed several combinations of source-target and target-source GIZA++ alignments (union, growing forward diagonal"
2007.iwslt-1.26,A00-1031,0,0.0277074,"he MADA+TOKAN system for disambiguation and tokenization. For disambiguation only diacritic uni-gram statistics were employed. For tokenization we used the D3 scheme with -TAGBIES option. The D3 scheme splits the following set of clitics: w+, f+, b+, k+, l+, Al+ and pronominal clitics. The -TAGBIES option produces Bies POS tags on all taggable tokens. 5.2.2. Chinese Chinese preprocessing included re-segmentation using ICTCLAS [17] and POS tagging using the freely available Stanford Parser4 . 5.2.3. English English preprocessing includes Part-Of-Speech tagging using freely-available TnT tagger [18]. For alignment purpose only (of the ZhEn system), the English corpus was stemmed using the Snowball stemmer 5 , based on Porter’s algorithm. 5.3. Results 5.3.1. Alignment In the ZhEn system development work, we tried to improve word alignment by stemming the English corpus and make use of classes [19]. We also performed several combinations of source-target and target-source GIZA++ alignments (union, growing forward diagonal method and Och’s refined method [20]), as well as concatenations of various of these combinations. Using stems and classes in the alignment improved translation results i"
2007.iwslt-1.26,E99-1010,0,0.0816899,"ion produces Bies POS tags on all taggable tokens. 5.2.2. Chinese Chinese preprocessing included re-segmentation using ICTCLAS [17] and POS tagging using the freely available Stanford Parser4 . 5.2.3. English English preprocessing includes Part-Of-Speech tagging using freely-available TnT tagger [18]. For alignment purpose only (of the ZhEn system), the English corpus was stemmed using the Snowball stemmer 5 , based on Porter’s algorithm. 5.3. Results 5.3.1. Alignment In the ZhEn system development work, we tried to improve word alignment by stemming the English corpus and make use of classes [19]. We also performed several combinations of source-target and target-source GIZA++ alignments (union, growing forward diagonal method and Och’s refined method [20]), as well as concatenations of various of these combinations. Using stems and classes in the alignment improved translation results in all cases, and the best combination for the system with pattern-based reordering was the union6 . At the end, the best alignment configuration for our baseline system was obtained with Giza++ software, running respectively 5, 5, 3 and 3 iterations of models 1, HMM, 3 and 4, using English stems and 50"
2007.iwslt-1.26,J03-1002,0,0.00791937,"y available Stanford Parser4 . 5.2.3. English English preprocessing includes Part-Of-Speech tagging using freely-available TnT tagger [18]. For alignment purpose only (of the ZhEn system), the English corpus was stemmed using the Snowball stemmer 5 , based on Porter’s algorithm. 5.3. Results 5.3.1. Alignment In the ZhEn system development work, we tried to improve word alignment by stemming the English corpus and make use of classes [19]. We also performed several combinations of source-target and target-source GIZA++ alignments (union, growing forward diagonal method and Och’s refined method [20]), as well as concatenations of various of these combinations. Using stems and classes in the alignment improved translation results in all cases, and the best combination for the system with pattern-based reordering was the union6 . At the end, the best alignment configuration for our baseline system was obtained with Giza++ software, running respectively 5, 5, 3 and 3 iterations of models 1, HMM, 3 and 4, using English stems and 50 classes and taking the union of source-target and target-source alignments. Table 3 show results for the new features of this year’s system. We optimized the foll"
2008.eamt-1.15,H05-1011,0,0.619332,"is paper deals with core aspects of discriminative word alignment systems, namely basic word association models as well as search strategies. We compare various low-computational-cost word association models: χ2 score, log-likelihood ratio and IBM model 1. We also compare three beam-search strategies. We show that it is more flexible and accurate to let links to the same word compete together, than introducing them sequentially in the alignment hypotheses, which is the strategy followed in several systems. 1 Introduction In this paper, we study core aspects of discriminative alignment systems [1, 2]. In these systems, the best alignment hypothesis is the one that maximises a linear combination of features. In Sect. 2 we propose some improvements of the beam-search algorithm implemented by Moore [1]. Then we present experimental results for different low-computational-cost word association score features (Sect. 3.1) and for the proposed search strategies (Sect. 3.2). Finally, we give some conclusions. 2 Search Strategies Search aims at finding the alignment (i.e. the set of links between source and target words) which maximises the sum of each feature cost, weighted by its respective weig"
2008.eamt-1.15,P05-1057,0,0.0787703,"is paper deals with core aspects of discriminative word alignment systems, namely basic word association models as well as search strategies. We compare various low-computational-cost word association models: χ2 score, log-likelihood ratio and IBM model 1. We also compare three beam-search strategies. We show that it is more flexible and accurate to let links to the same word compete together, than introducing them sequentially in the alignment hypotheses, which is the strategy followed in several systems. 1 Introduction In this paper, we study core aspects of discriminative alignment systems [1, 2]. In these systems, the best alignment hypothesis is the one that maximises a linear combination of features. In Sect. 2 we propose some improvements of the beam-search algorithm implemented by Moore [1]. Then we present experimental results for different low-computational-cost word association score features (Sect. 3.1) and for the proposed search strategies (Sect. 3.2). Finally, we give some conclusions. 2 Search Strategies Search aims at finding the alignment (i.e. the set of links between source and target words) which maximises the sum of each feature cost, weighted by its respective weig"
2008.eamt-1.15,J00-2004,0,0.695757,"lty, the hypothesis with the new link is better than the previous one. In the example of Fig. 2 (left figure), suppose that this was the case successively for links 1-2, 3-3 and 0-0, so that the best alignment hypothesis is {1-2, 3-3, 0-0}. Now if this hypothesis is expanded with link 2-2, the association cost is compensated by the decrease of the unlinked feature cost for “state”, and the new best hypothesis will include link 2-2. Expanding now this last hypothesis with link 2-1, the unlinked feature gain for “pais” cannot compensate for the distortion feature cost (due to crossing 4 Melamed [3] also starts with the empty alignment and links are added from most to least probable. 98 12th EAMT conference, 22-23 September 2008, Hamburg, Germany Fig. 2. Left: Baseline search: link-by-link search following word association score order [1]. Right: “source-word-score” search strategy. with “member-miembr”) plus the association cost. Thus link 2-1 is not included in the final hypothesis. On the contrary, if we would expand the hypotheses with link 2-1 first, the double unlinked feature gain (for “pais” and “state”) would compensate for the other costs, and link 2-1 would appear in the final"
2008.eamt-1.15,J03-1002,0,0.0219937,"This is much more efficient than Liu et al.’s search [2], which considers all possible links before selecting each link. http://gps-tsc.upc.es/veu/LR 100 12th EAMT conference, 22-23 September 2008, Hamburg, Germany contains 1.28 million sentence pairs of respectively 27.2 and 28.5 words length in average for English and Spanish. English and Spanish vocabulary size are respectively 106 and 153 thousand words. We divided randomly the alignment reference corpus in a 246-sentence development set and a 245-sentence test set. Evaluation was done with precision, recall and alignment error rate (AER) [5]. 3.1 Basic Word Association Models Our aim in this section is to compare very simple word association measures reported in the literature and which can be very useful for some applications. Cherry and Lin [6] and Lambert et al. [7] use χ2 scores [8]. However, Dunning [9] showed that the log-likelihood ratio (LLR) was a better method of accounting for rare events occurring in large samples. χ2 score indeed overestimates their significance. For example, the association between two singletons cooccurring in the same sentence pair gets the best possible χ2 score, and this association is 4 orders"
2008.eamt-1.15,P03-1012,0,0.0195281,"rmany contains 1.28 million sentence pairs of respectively 27.2 and 28.5 words length in average for English and Spanish. English and Spanish vocabulary size are respectively 106 and 153 thousand words. We divided randomly the alignment reference corpus in a 246-sentence development set and a 245-sentence test set. Evaluation was done with precision, recall and alignment error rate (AER) [5]. 3.1 Basic Word Association Models Our aim in this section is to compare very simple word association measures reported in the literature and which can be very useful for some applications. Cherry and Lin [6] and Lambert et al. [7] use χ2 scores [8]. However, Dunning [9] showed that the log-likelihood ratio (LLR) was a better method of accounting for rare events occurring in large samples. χ2 score indeed overestimates their significance. For example, the association between two singletons cooccurring in the same sentence pair gets the best possible χ2 score, and this association is 4 orders of magnitude less than the best score according to the LLR statistics. The LLR score was used by Melamed [3] for automatically constructing translation lexicons and by Moore [1] as a word association feature."
2008.eamt-1.15,N07-2022,1,0.810924,"lion sentence pairs of respectively 27.2 and 28.5 words length in average for English and Spanish. English and Spanish vocabulary size are respectively 106 and 153 thousand words. We divided randomly the alignment reference corpus in a 246-sentence development set and a 245-sentence test set. Evaluation was done with precision, recall and alignment error rate (AER) [5]. 3.1 Basic Word Association Models Our aim in this section is to compare very simple word association measures reported in the literature and which can be very useful for some applications. Cherry and Lin [6] and Lambert et al. [7] use χ2 scores [8]. However, Dunning [9] showed that the log-likelihood ratio (LLR) was a better method of accounting for rare events occurring in large samples. χ2 score indeed overestimates their significance. For example, the association between two singletons cooccurring in the same sentence pair gets the best possible χ2 score, and this association is 4 orders of magnitude less than the best score according to the LLR statistics. The LLR score was used by Melamed [3] for automatically constructing translation lexicons and by Moore [1] as a word association feature. We compared these assoc"
2008.eamt-1.15,J93-1003,0,0.0715936,"and 28.5 words length in average for English and Spanish. English and Spanish vocabulary size are respectively 106 and 153 thousand words. We divided randomly the alignment reference corpus in a 246-sentence development set and a 245-sentence test set. Evaluation was done with precision, recall and alignment error rate (AER) [5]. 3.1 Basic Word Association Models Our aim in this section is to compare very simple word association measures reported in the literature and which can be very useful for some applications. Cherry and Lin [6] and Lambert et al. [7] use χ2 scores [8]. However, Dunning [9] showed that the log-likelihood ratio (LLR) was a better method of accounting for rare events occurring in large samples. χ2 score indeed overestimates their significance. For example, the association between two singletons cooccurring in the same sentence pair gets the best possible χ2 score, and this association is 4 orders of magnitude less than the best score according to the LLR statistics. The LLR score was used by Melamed [3] for automatically constructing translation lexicons and by Moore [1] as a word association feature. We compared these association measures to IBM model 1 probabili"
2008.eamt-1.15,J93-2003,0,0.0317814,"ed that the log-likelihood ratio (LLR) was a better method of accounting for rare events occurring in large samples. χ2 score indeed overestimates their significance. For example, the association between two singletons cooccurring in the same sentence pair gets the best possible χ2 score, and this association is 4 orders of magnitude less than the best score according to the LLR statistics. The LLR score was used by Melamed [3] for automatically constructing translation lexicons and by Moore [1] as a word association feature. We compared these association measures to IBM model 1 probabilities [10]. Table 1 shows the alignment results for a basic system composed of the following features: word association, link bonus, unlinked word penalty and two distortion features (counting the number and amplitude of crossing links). The value of the word association feature was calculated as the sum of the word association costs of the links present in the alignment. This cost was simply obtained by taking (minus) the logarithm of respectively the χ2 score, IBM model 1 probabilities or the LLR score normalised to 1. For IBM model 1 probabilities, we had two features, one for each direction (source-"
2008.eamt-1.15,W04-3243,0,0.0157457,"a 11 points drop in precision.7 IBM model 1 probabilities are better than association scores and yield a 3.5 points improvement over χ2 word association scores. Of course, state-of-the-art models like IBM model 4 are expected to perform better. In lines 1 to 3 of Table 1, the unlinked penalty feature is uniform. In the “IBM1+UM” system, this feature was substituted by a penalty proportional to model 1 NULL link probability, yielding a gain of 2 points in precision and 1 point in recall. 7 This result may be surprising at first sight. In fact, it makes sense. To take the same example as Moore [11], in our corpus, singletons appearing in each side of the same sentence pair constitute a very significant event. The IBM model 1 probability in this case is actually equal to 1, and the χ2 score is also the best possible. Although no word can have a higher LLR score with a singleton than another singleton, the LLR score between more frequent words can be much higher. This makes a difference because the alignment hypotheses are expanded with the most probable links first. Thus compared to χ2 , the LLR score gives a relatively higher importance to links involving frequent words, which may be st"
2008.eamt-1.15,H91-1026,0,\N,Missing
2009.eamt-1.34,J93-2003,0,0.0123787,"orithm. The interpolation factor λ can be optimised on development set. When a zeroorder transition model (a uniform transition distribution) is used, we constrain the emission probability by a threshold t, which is set as the minimal reliability score for each link. Again, t can be optimised according to the development set. The decoding is performed separately in two directions (Chinese-to-English and English-toChinese), and we then obtain the refined alignments as the final word alignment. 4.3 4.3.1 Baselines Word Alignment We used the G IZA ++ implementation of IBM word alignment model 4 (Brown et al., 1993; Och and Ney, 2003) for word alignment, and the heuristics described in (Koehn et al., 2003) to derive the intersection and refined alignment. 4.3.2 Machine Translation 4.4 Evaluation We evaluate the intrinsic quality of the predicted alignment A with Precision, Recall and the balanced F-score with α = 0.5 (cf. (Fraser and Marcu, 2007)). Recall = F-score(A, S, α) = More specifically, we performed 5 iterations of Model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4. 1 + α P recision(A,S) 1−α Recall(A,S) Research has shown that an increase in AER does not necessari"
2009.eamt-1.34,N06-4004,0,0.0300402,"Missing"
2009.eamt-1.34,J93-1003,0,0.0238346,"Y p(aj |cJ1 , eI1 , a1j−1 , A∆ )1−λ · ¯ j∈∆ J Y p(aj |aj−1 , A∆ )λ j=1 We can use factor λ to weight the emission model and transition model probabilities so that the system can be optimised according to different objectives. 3 Feature Functions for Syntactically Enhanced Word Alignment The various features used in our syntactically enhanced model can be classified into two groups: statistics-based features and syntactic features which are similar to those in (Ma et al., 2008) 3.1 Statistics-based Features The statistics-based features we used include IBM model 1 score, Log-likelihood ratio (Dunning, 1993) and POS translation probability. We choose these features because they are empirically proven to be effective in word alignment tasks (Melamed, 2000; Liu et al., 2005; Moore, 2005). 3.2 Syntactic Features The dependency relation Re (resp. Rc ) between two English (resp. Chinese) words ei and ei′ (resp. cj and cj ′ ) in the dependency tree of the English sentence eI1 (resp. Chinese sentence cJ1 ) can be represented as a triple &lt;ei , Re , ei′ > (resp. &lt;cj , Rc , ej ′ >). Given cJ1 , eI1 and their syntactic dependency trees TcJ , TeI , if ei is aligned to cj and 1 1 ei′ aligned to cj ′ , accordi"
2009.eamt-1.34,W08-0409,1,0.62805,"es into such models. On the other hand, discriminative models are more flexible to incorporate arbitrary features. In this paper, we introduce a simple yet flexible framework for word alignment. To take the advantage of the strength of generative models, we use these models to obtain a set of anchor alignments. We then incorporate syntactic features induced by the anchor alignments into a discriminative word alignment model. The syntactic features we used are syntactic dependencies. This decision is motivated by the fact that if words tend to be dependent on each other, so does the alignment (Ma et al., 2008). If we can first obtain a set of reliable anchor links, we could take advantage of the syntactic dependencies relating unaligned words to aligned anchor words to expand the alignment. Figure 1 gives an illustrative example. Note that the link (c2 , e4 ) can be easily identified, but the link involving the fourth Chinese word (a function word denoting ‘time’) (c4 , e4 ) is hard. In such cases, we can make use of the dependency relationship (‘tclause’) between c2 and c4 to help the alignment process. Figure 1: Dependencies for word alignment c 2009 European Association for Machine Translation."
2009.eamt-1.34,J00-2004,0,0.0406643,"ities so that the system can be optimised according to different objectives. 3 Feature Functions for Syntactically Enhanced Word Alignment The various features used in our syntactically enhanced model can be classified into two groups: statistics-based features and syntactic features which are similar to those in (Ma et al., 2008) 3.1 Statistics-based Features The statistics-based features we used include IBM model 1 score, Log-likelihood ratio (Dunning, 1993) and POS translation probability. We choose these features because they are empirically proven to be effective in word alignment tasks (Melamed, 2000; Liu et al., 2005; Moore, 2005). 3.2 Syntactic Features The dependency relation Re (resp. Rc ) between two English (resp. Chinese) words ei and ei′ (resp. cj and cj ′ ) in the dependency tree of the English sentence eI1 (resp. Chinese sentence cJ1 ) can be represented as a triple &lt;ei , Re , ei′ > (resp. &lt;cj , Rc , ej ′ >). Given cJ1 , eI1 and their syntactic dependency trees TcJ , TeI , if ei is aligned to cj and 1 1 ei′ aligned to cj ′ , according to the dependency correspondence assumption (Hwa et al., 2002), there exists a triple &lt;cj , Rc , cj ′ >. While we are not aiming to justify the fe"
2009.eamt-1.34,J07-3002,0,0.142988,"† Patrik Lambert‡ Andy Way†‡ † National Centre for Language Technology ‡ Centre for Next Generation for Localisation School of Computing, Dublin City University Dublin 9, Ireland {yma,plambert,away@computing.dcu.ie} Abstract However, these models need a certain amount of annotated word alignment data, which is often subject to criticism since the annotation of word alignment is a highly subjective task. Moreover, parameters optimised on manually annotated data are not necessarily optimal for MT tasks. Recent research attempts to combine the merits of both generative and discriminative models (Fraser and Marcu, 2007), or to tune a discriminative model according to MT metrics (Lambert et al., 2007). We introduce a syntactically enhanced word alignment model that is more flexible than state-of-the-art generative word alignment models and can be tuned according to different end tasks. First of all, this model takes the advantages of both unsupervised and supervised word alignment approaches by obtaining anchor alignments from unsupervised generative models and seeding the anchor alignments into a supervised discriminative model. Second, this model offers the flexibility of tuning the alignment according to d"
2009.eamt-1.34,H05-1011,0,0.01725,"ptimised according to different objectives. 3 Feature Functions for Syntactically Enhanced Word Alignment The various features used in our syntactically enhanced model can be classified into two groups: statistics-based features and syntactic features which are similar to those in (Ma et al., 2008) 3.1 Statistics-based Features The statistics-based features we used include IBM model 1 score, Log-likelihood ratio (Dunning, 1993) and POS translation probability. We choose these features because they are empirically proven to be effective in word alignment tasks (Melamed, 2000; Liu et al., 2005; Moore, 2005). 3.2 Syntactic Features The dependency relation Re (resp. Rc ) between two English (resp. Chinese) words ei and ei′ (resp. cj and cj ′ ) in the dependency tree of the English sentence eI1 (resp. Chinese sentence cJ1 ) can be represented as a triple &lt;ei , Re , ei′ > (resp. &lt;cj , Rc , ej ′ >). Given cJ1 , eI1 and their syntactic dependency trees TcJ , TeI , if ei is aligned to cj and 1 1 ei′ aligned to cj ′ , according to the dependency correspondence assumption (Hwa et al., 2002), there exists a triple &lt;cj , Rc , cj ′ >. While we are not aiming to justify the feasibility of the dependency corr"
2009.eamt-1.34,P02-1050,0,0.0154312,"e features because they are empirically proven to be effective in word alignment tasks (Melamed, 2000; Liu et al., 2005; Moore, 2005). 3.2 Syntactic Features The dependency relation Re (resp. Rc ) between two English (resp. Chinese) words ei and ei′ (resp. cj and cj ′ ) in the dependency tree of the English sentence eI1 (resp. Chinese sentence cJ1 ) can be represented as a triple &lt;ei , Re , ei′ > (resp. &lt;cj , Rc , ej ′ >). Given cJ1 , eI1 and their syntactic dependency trees TcJ , TeI , if ei is aligned to cj and 1 1 ei′ aligned to cj ′ , according to the dependency correspondence assumption (Hwa et al., 2002), there exists a triple &lt;cj , Rc , cj ′ >. While we are not aiming to justify the feasibility of the dependency correspondence assumption by proving to what extent Re = Rc under the condition described above, we do believe that cj and cj ′ are likely to be dependent on each other. Given the anchor alignment A∆ , a candidate link (j, i) and the dependency trees, we can design four classes of feature functions. 3.2.1 Agreement features The agreement features can be further classified into dependency agreement features and dependency label agreement features. Given a candidate link (j, i) and the"
2009.eamt-1.34,H05-1012,0,0.0155429,"ment set. In our experiments we set α = 0.9. 2.2.2 Syntactically Enhanced Word Alignment The syntactically enhanced model is used to model the alignment of the words left unaligned after anchoring. We directly model the linkage between source and target words using a discriminative word alignment framework where various features can be incorporated. Given a source word cj and the target sentence eI1 , we search for the alignment aj such that: aˆj = argmax{pλM (aj |cJ1 , eI1 , a1j−1 , A∆ )} 1 aj A We use a model (1) that directly models the linkage between source and target words similarly to (Ittycheriah and Roukos, 2005). The Chinese-toEnglish word alignment AC→E = {i|aj = i} is modelled as shown in (1). We decompose this model into an emission model and a transition model ( 4). The emission model can be further decomposed into an anchor alignment model (2) and a syntactically enhanced model (3) by distinguishing the anchor alignment from the nonanchor alignment. p(A|cJ1 , eI1 ) = J Y p(aj |cJ1 , eI1 , a1j−1 ) 1 · pǫ (A∆ |cJ1 , eI1 ) · (2) Z Y p(aj |cJ1 , eI1 , a1j−1 , A∆ ) ·(3) ¯ j∈∆ J Y p(aj |aj−1 , A∆ ) (4) j=1 2.2 aj λm hm (cJ1 , eI1 , aj1 , A∆ , Tc , Te )} m=1 In this decision rule, we assume that a set"
2009.eamt-1.34,N03-1017,0,0.0058174,"nsition model (a uniform transition distribution) is used, we constrain the emission probability by a threshold t, which is set as the minimal reliability score for each link. Again, t can be optimised according to the development set. The decoding is performed separately in two directions (Chinese-to-English and English-toChinese), and we then obtain the refined alignments as the final word alignment. 4.3 4.3.1 Baselines Word Alignment We used the G IZA ++ implementation of IBM word alignment model 4 (Brown et al., 1993; Och and Ney, 2003) for word alignment, and the heuristics described in (Koehn et al., 2003) to derive the intersection and refined alignment. 4.3.2 Machine Translation 4.4 Evaluation We evaluate the intrinsic quality of the predicted alignment A with Precision, Recall and the balanced F-score with α = 0.5 (cf. (Fraser and Marcu, 2007)). Recall = F-score(A, S, α) = More specifically, we performed 5 iterations of Model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4. 1 + α P recision(A,S) 1−α Recall(A,S) Research has shown that an increase in AER does not necessarily imply an improvement in translation quality (Liang et al., 2006) and vice-versa (Vilar et"
2009.eamt-1.34,P07-2045,0,0.0140549,"Missing"
2009.eamt-1.34,N07-2022,1,0.799024,"Next Generation for Localisation School of Computing, Dublin City University Dublin 9, Ireland {yma,plambert,away@computing.dcu.ie} Abstract However, these models need a certain amount of annotated word alignment data, which is often subject to criticism since the annotation of word alignment is a highly subjective task. Moreover, parameters optimised on manually annotated data are not necessarily optimal for MT tasks. Recent research attempts to combine the merits of both generative and discriminative models (Fraser and Marcu, 2007), or to tune a discriminative model according to MT metrics (Lambert et al., 2007). We introduce a syntactically enhanced word alignment model that is more flexible than state-of-the-art generative word alignment models and can be tuned according to different end tasks. First of all, this model takes the advantages of both unsupervised and supervised word alignment approaches by obtaining anchor alignments from unsupervised generative models and seeding the anchor alignments into a supervised discriminative model. Second, this model offers the flexibility of tuning the alignment according to different optimisation criteria. Our experiments show that using our word alignment"
2009.eamt-1.34,N06-1014,0,0.0193518,"heuristics described in (Koehn et al., 2003) to derive the intersection and refined alignment. 4.3.2 Machine Translation 4.4 Evaluation We evaluate the intrinsic quality of the predicted alignment A with Precision, Recall and the balanced F-score with α = 0.5 (cf. (Fraser and Marcu, 2007)). Recall = F-score(A, S, α) = More specifically, we performed 5 iterations of Model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4. 1 + α P recision(A,S) 1−α Recall(A,S) Research has shown that an increase in AER does not necessarily imply an improvement in translation quality (Liang et al., 2006) and vice-versa (Vilar et al., 2006). Hereafter, we use a Chinese– English MT task to extrinsically evaluate the quality of our word alignment. The translation output is measured using B LEU (Papineni et al., 2002). 5 Experiments 5.1 Word Alignment Results We performed word alignment bidirectionally using our approach to obtain the refined alignments (Koehn et al., 2003) and compared our results with two strong baselines based on generative word alignment models. The results are shown in Table 3. We can see that both the syntactically enhanced model based on HMM intersection anchors and on IBM"
2009.eamt-1.34,P05-1057,0,0.0163677,"he system can be optimised according to different objectives. 3 Feature Functions for Syntactically Enhanced Word Alignment The various features used in our syntactically enhanced model can be classified into two groups: statistics-based features and syntactic features which are similar to those in (Ma et al., 2008) 3.1 Statistics-based Features The statistics-based features we used include IBM model 1 score, Log-likelihood ratio (Dunning, 1993) and POS translation probability. We choose these features because they are empirically proven to be effective in word alignment tasks (Melamed, 2000; Liu et al., 2005; Moore, 2005). 3.2 Syntactic Features The dependency relation Re (resp. Rc ) between two English (resp. Chinese) words ei and ei′ (resp. cj and cj ′ ) in the dependency tree of the English sentence eI1 (resp. Chinese sentence cJ1 ) can be represented as a triple &lt;ei , Re , ei′ > (resp. &lt;cj , Rc , ej ′ >). Given cJ1 , eI1 and their syntactic dependency trees TcJ , TeI , if ei is aligned to cj and 1 1 ei′ aligned to cj ′ , according to the dependency correspondence assumption (Hwa et al., 2002), there exists a triple &lt;cj , Rc , cj ′ >. While we are not aiming to justify the feasibility of the d"
2009.eamt-1.34,J03-1002,0,0.0332275,"mation of pˆ(aj |aj−1 ) is calculated following the homogeneous HMM model (Vogel et al., 1996). Under this model, we assume that the 251 (5) probability depends only on the jump width (i−i′ ), in order to make the alignment parameters independent of absolute word positions. Using a set of non-negative parameters {c(i − i′ )}, the transition probability can be written in the form: p(aj |aj−1 , A∆ ) = PI c(i − i′ ) ′′ i′′ =1 c(i − i′ ) We use the refined model which extends the HMM network with I empty words e2I I+1 and adds parameter p0 to account for the transition probability to empty words (Och and Ney, 2003). If a zero-order dependence is assumed in a transition model, the emission models is the only information to guide the word alignment. 2.4 Model Interpolation We interpolate the general alignment model (1) as follows: p(A|cJ1 , eI1 ) = 1 · pǫ (A∆ |cJ1 , eI1 )1−λ · Z Y p(aj |cJ1 , eI1 , a1j−1 , A∆ )1−λ · ¯ j∈∆ J Y p(aj |aj−1 , A∆ )λ j=1 We can use factor λ to weight the emission model and transition model probabilities so that the system can be optimised according to different objectives. 3 Feature Functions for Syntactically Enhanced Word Alignment The various features used in our syntactical"
2009.eamt-1.34,P03-1021,0,0.0215296,"Missing"
2009.eamt-1.34,P02-1040,0,0.0789029,"ion, Recall and the balanced F-score with α = 0.5 (cf. (Fraser and Marcu, 2007)). Recall = F-score(A, S, α) = More specifically, we performed 5 iterations of Model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4. 1 + α P recision(A,S) 1−α Recall(A,S) Research has shown that an increase in AER does not necessarily imply an improvement in translation quality (Liang et al., 2006) and vice-versa (Vilar et al., 2006). Hereafter, we use a Chinese– English MT task to extrinsically evaluate the quality of our word alignment. The translation output is measured using B LEU (Papineni et al., 2002). 5 Experiments 5.1 Word Alignment Results We performed word alignment bidirectionally using our approach to obtain the refined alignments (Koehn et al., 2003) and compared our results with two strong baselines based on generative word alignment models. The results are shown in Table 3. We can see that both the syntactically enhanced model based on HMM intersection anchors and on IBM model 4 anchors achieved higher Fscores than the pure generative word alignment models. It is also can be seen that zero-order syntactic models are better in precision and firstorder models are superior in recall."
2009.eamt-1.34,W96-0213,0,0.0601125,"Missing"
2009.eamt-1.34,2006.iwslt-papers.7,0,0.0135253,"l., 2003) to derive the intersection and refined alignment. 4.3.2 Machine Translation 4.4 Evaluation We evaluate the intrinsic quality of the predicted alignment A with Precision, Recall and the balanced F-score with α = 0.5 (cf. (Fraser and Marcu, 2007)). Recall = F-score(A, S, α) = More specifically, we performed 5 iterations of Model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4. 1 + α P recision(A,S) 1−α Recall(A,S) Research has shown that an increase in AER does not necessarily imply an improvement in translation quality (Liang et al., 2006) and vice-versa (Vilar et al., 2006). Hereafter, we use a Chinese– English MT task to extrinsically evaluate the quality of our word alignment. The translation output is measured using B LEU (Papineni et al., 2002). 5 Experiments 5.1 Word Alignment Results We performed word alignment bidirectionally using our approach to obtain the refined alignments (Koehn et al., 2003) and compared our results with two strong baselines based on generative word alignment models. The results are shown in Table 3. We can see that both the syntactically enhanced model based on HMM intersection anchors and on IBM model 4 anchors achieved higher Fsc"
2009.eamt-1.34,C96-2141,0,0.066659,"l pǫ (A∆ ) aims to find a set of high-precision links. Various approaches can be used for this purpose. Given the anchor alignment, the first-order transition probability model ( 4) can be defined as follows: ( 1.0 if ∈ ∆, p(aj |aj−1 , A∆ ) = pˆ(aj |aj−1 ) otherwise. Such a definition implies that an anchor alignment is always believed to be a correct alignment, maximum likelihood estimates obtained on a goldstandard word alignment corpus are used when the current word fj is not involved in an anchor alignment. The estimation of pˆ(aj |aj−1 ) is calculated following the homogeneous HMM model (Vogel et al., 1996). Under this model, we assume that the 251 (5) probability depends only on the jump width (i−i′ ), in order to make the alignment parameters independent of absolute word positions. Using a set of non-negative parameters {c(i − i′ )}, the transition probability can be written in the form: p(aj |aj−1 , A∆ ) = PI c(i − i′ ) ′′ i′′ =1 c(i − i′ ) We use the refined model which extends the HMM network with I empty words e2I I+1 and adds parameter p0 to account for the transition probability to empty words (Och and Ney, 2003). If a zero-order dependence is assumed in a transition model, the emission"
2009.eamt-1.34,2007.iwslt-1.1,0,\N,Missing
2009.iwslt-evaluation.10,2006.iwslt-evaluation.15,0,0.0321155,"and Dev3 corpora. The target language model was trained on the English side of the those corpora. No additional texts were used (constrained condition). We report results on Dev6 (development data) and Dev7 (internal test set). All BLEU scores are case-sensitive and include punctuations. For some systems, the Dev6 corpus was added to the training material after optimizing the system and the full system was retrained, keeping all settings unmodified. By these means we hope to lower the OOV rate on the official test set. This idea was already successfully proposed in previous IWSLT evaluations [1]. The statistical phrase-based system is based on the Moses SMT toolkit [2] and constructed as follows. First, Giza++ is used to perform word alignments in both directions. Second, phrases and lexical reorderings are extracted. Both steps use the default settings of the Moses SMT toolkit. A 4-gram back-off target language model (LM) is constructed on all available English data. The translation itself is performed in two passes: first, Moses is run and a 1000-best list is generated for each sentence. In our system fourteen features functions were used, namely phrase and lexical translation prob"
2009.iwslt-evaluation.10,P07-2045,0,0.00776614,"e of the those corpora. No additional texts were used (constrained condition). We report results on Dev6 (development data) and Dev7 (internal test set). All BLEU scores are case-sensitive and include punctuations. For some systems, the Dev6 corpus was added to the training material after optimizing the system and the full system was retrained, keeping all settings unmodified. By these means we hope to lower the OOV rate on the official test set. This idea was already successfully proposed in previous IWSLT evaluations [1]. The statistical phrase-based system is based on the Moses SMT toolkit [2] and constructed as follows. First, Giza++ is used to perform word alignments in both directions. Second, phrases and lexical reorderings are extracted. Both steps use the default settings of the Moses SMT toolkit. A 4-gram back-off target language model (LM) is constructed on all available English data. The translation itself is performed in two passes: first, Moses is run and a 1000-best list is generated for each sentence. In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion"
2009.iwslt-evaluation.10,W07-0732,0,0.0227542,"us 376k words En Btec human translations Dev1−3 Giza++ Phrase extraction SRILM CSLM Phrase table 4g LM 4g CSLM phrase table SMT system 4−gram LM Moses Ar/En Src Moses 1000 bests Trg Src LM rescoring phrase table Trg SYSTRAN decode optimized with MERT λi Moses En’/En Condor BLEU SPE system 2nd pass optimisation Figure 2: Comparison of SMT and SPE systems. Figure 1: Architecture of the SMT system. 3. SPE System In the last years, there is increasing interest in the interaction between rule-based and statistical machine translation. A popular and successful idea is statistical post editing (SPE) [6, 7]. The principle idea is to train an SMT system to correct the outputs of a rule-based translation system. This is shown in figure 2. The operation performed by the rulebased translation system could also be seen as a very good tokenization or preprocessing, that actually performs many of the translation steps. Therefore, the task of the SMT system itself is very simplified. Accordingly, we argue that an SMT and SPE system are only two extreme cases of the interaction between tokenization/preprocessing and translation itself. An interesting question is whether both systems can be combined since"
2009.iwslt-evaluation.10,J07-2003,0,0.131844,"Missing"
2009.iwslt-evaluation.10,W09-0424,0,0.0287796,"Missing"
2009.iwslt-evaluation.10,2008.iwslt-evaluation.6,0,0.0423786,"Missing"
2009.iwslt-evaluation.10,P07-1040,0,0.0322066,"he development set (Dev6) using the provided Z-MERT procedure. The grammar rules extraction tools and Z-MERT are provided in the Joshua toolkit. Figure 3 summarizes the architecture of the Joshua translation system. The decoder is based on the token pass decoding algorithm. The scores used to evaluate the hypotheses are the following: 5. System combination • the system score : this replaces the score of the translation model. Until now, the words given by all systems 1 have the same probability which is . M The system combination approach is based on confusion network decoding as described in [11, 12] and shown in Figure 4. The protocol can be decomposed into three steps : • the language model (LM) probability. The 4-gram LM used for the combination is the same than the one used by each single system. 1. 1-best hypotheses from all M systems are aligned and confusion networks are built. It is obvious that this combination framework is not optimal, but as we can see in the results section, this simple architecture can already achieve improvements when combining only two systems. 2. All confusion networks are connected into a single lattice. 6. Experimental Evaluation 3. A 4-gram language mod"
2009.iwslt-evaluation.10,2006.amta-papers.25,0,0.0173726,"ter optimize our hierarchical systems built with Joshua. Rescoring the n-best lists with the continuous space LM achieved an improvement of 1.2 BLEU on the internal test set for the Arabic/English SMT system, and 0.6 BLEU for the SPE system. Due to time constraints, the continuous space LM was not applied on the hierarchical system. The improvements obtained by the CSLM are generally smaller 5.1. Hypotheses alignment and confusion network generation For each segment, the best hypotheses of M − 1 systems are aligned against the last one used as backbone. The alignment is done with the TER tool [13], without any tuning performed at this step (default edit costs are used). M confusion networks are generated in this way. Then all the confusion networks are connected into a single lattice by adding a first and last node. The probability of the first arcs must reflect how well such system provides a well structured hypothesis (good order). In our experiments, no tuning was done at this step, and we chose equal prior probabilities for all systems. - 67 - Proceedings of IWSLT 2009, Tokyo - Japan Approach: Train bitexts Arabic/English: Btec+Dev123 Btec+Dev1236 LM SMT Moses Dev Test Hierarchical"
2009.iwslt-evaluation.10,W07-0728,0,\N,Missing
2009.iwslt-evaluation.10,2008.iwslt-evaluation.10,0,\N,Missing
2009.mtsummit-posters.12,P06-1002,0,0.222937,"of the data used. In Section 5, the results are discussed. Finally, some conclusions are provided together with avenues for further research. 2 Related Work In this section, we review some alignment characteristics that have been observed to have some impact in phrase extraction and MT output, for the SMT approaches we consider in this paper. We will thus consider these characteristics (and more) to investigate what kind of alignment helps depending on the system and type or amount of training data. In several papers the impact of higher precision or higher recall alignments has been studied. Ayan and Dorr (2006) and Chen and Federico (2006) observed that higher precision alignments favoured a phrase-based SMT system. In the former case, it was observed with an English Chinese training corpus of 1.1 million running words (for the English side), and with an English Arabic corpus of 3.3 million words of News and treebank data. In the latter case, the BTEC corpus was used (Chinese, Japanese, Arabic and Italian to English, with 180k English words). Fraser and Marcu (2007) compared the performance of translation systems trained on several alignments of varying quality. Their results on large corpora do not"
2009.mtsummit-posters.12,J93-2003,0,0.0156789,"a log-linear combination of feature functions calculated at the sentence pair level. In a first pass, the training corpus was aligned selecting for each sentence pair (s, t) the alignment ˆ which maximises a combination of varhypothesis a ious models, as expressed in (1): (1) (1) (1) ˆ(1) = arg max λa1 ha1 + λa2 ha2 + λlb hlb a a (1) (1) (1) + λ(1) um hum + λcn hcn + λcl hcl + λhp hhp (1) where h stands for the feature functions h(s, t) used, and the λs are their corresponding weights. ha1 and ha2 are word association models based on source-target and target-source IBM model 1 probabilities (Brown et al., 1993). hlb is proportional to the number of links in a. hum is an unlinked word model proportional to the IBM model 1 NULL link probability. hcn and hcl are distortion models, counting respectively the number and amplitude (the difference between target word positions) of crossing links. Finally, hhp is a “hole penalty” model, proportional to the number of embedded positions between two target words linked to the same source words (or vice-versa). We performed a second alignment pass in which the association score model with IBM1 probabilities and the unlinked model were substituted by two improved"
2009.mtsummit-posters.12,J07-3002,0,0.259443,"system and type or amount of training data. In several papers the impact of higher precision or higher recall alignments has been studied. Ayan and Dorr (2006) and Chen and Federico (2006) observed that higher precision alignments favoured a phrase-based SMT system. In the former case, it was observed with an English Chinese training corpus of 1.1 million running words (for the English side), and with an English Arabic corpus of 3.3 million words of News and treebank data. In the latter case, the BTEC corpus was used (Chinese, Japanese, Arabic and Italian to English, with 180k English words). Fraser and Marcu (2007) compared the performance of translation systems trained on several alignments of varying quality. Their results on large corpora do not confirm the hypothesis that higher precision alignments help phrase-based SMT systems more than higher recall alignments. For example, among their 3 systems trained on a 67 million word French English corpus, the highest precision alignment has the best BLEU score when alignment quality is low, and the highest recall alignment is the best when alignment quality is high. Their results suggest that when there is not enough data to produce good quality alignment"
2009.mtsummit-posters.12,N03-1017,0,0.0123354,"evaluation was performed with the BLEU score (Papineni et al., 2002). Translations were computed either by Moses (Koehn et al., 2007) with all default parameters, or by a baseline n-gram-based system with constrained reordered search (Crego and Mari˜no, 2007). 5 Results We present intrinsic and extrinsic evaluation results as well as some statistics for 9 alignment sets. 3 sets are baseline sets, and correspond to combinations of the Giza++ (Och and Ney, 2003) source-target and target-source alignments computed by Moses scripts: intersection (I), union (U) and grow-diagfinal heuristic (GDF) (Koehn et al., 2003). The other sets were aligned with the optimum weights of the discriminative aligner (Section 3.1) resulting from optimisations according to the F-score, to the phrase-based system BLEU score and to the ngram-based system BLEU score (referred to as F, PB and NB, respectively). Because the optimisation algorithm can get stuck in a poor local maximum, the optimisation with each criterion was performed with three different random seeds. To have an idea of the error introduced by the optimisation process, we kept the weights of the two optimisations which reached the highest values in the developm"
2009.mtsummit-posters.12,P07-2045,0,0.0220958,"o an alignment quality metric (F-score, see Section 4.3). In this way, we can investigate for any alignment characteristic how it is affected by the change of tuning criterion. If there exist alignment characteristics which are helpful in translation, they should not depend on the aligner used. However, they could depend on the MT system, the language pair, or the corpus size or type. The second contribution of this paper is to study more systematically how the considered characteristics depend on these parameters. We report results for two different SMT systems: a phrase-based system (Moses (Koehn et al., 2007)) and an n-gram-based system (Crego and Mari˜no, 2007). We performed this comparison on two different tasks: translation from Chinese to English, trained with IWSLT data (BTEC corpus, a small corpus in the travelling domain), and translation from Spanish to English, trained on a 2.7 million word corpus of the European Parliament proceedings. First we discuss related work. In Section 3, we describe the alignment optimisation procedures according to F-score and BLEU, and give more details on the alignment system used. Then in Section 4, we provide a summary of the experiments performed on each t"
2009.mtsummit-posters.12,N07-2022,1,0.865518,"ms (a phrase-based and an n-gram-based system) on Chinese to English IWSLT data, and Spanish to English European Parliament data. We give alignment hints to improve BLEU score, depending on the SMT system used and the type of corpus. 1 Introduction Most statistical machine translation (SMT) systems (e.g. phrase-based, n-gram-based) extract their translation models from word alignment trained in a previous stage. Many papers have shown that alignment quality is poorly correlated with MT quality (for example Vilar et al. (2006)). Then, we can tune the alignment directly according to MT metrics (Lambert et al., 2007). In this paper we rather try to find out which alignment characteristics help or worsen translation. In the related papers (see next section) some alignment characteristics are usually considered, and the impact on MT of alignments with different values for these characteristics is evaluated. The contributions of this paper are twofold. Firstly, the problem is considered from the inverse point of view: we start from an initial alignment and tune it directly according to a translation quality metric (BLEU score (Papineni et al., 2002)) and according to an alignment quality metric (F-score, see"
2009.mtsummit-posters.12,J06-4004,1,0.922385,"Missing"
2009.mtsummit-posters.12,J00-2004,0,0.0193689,"1 NULL link probability. hcn and hcl are distortion models, counting respectively the number and amplitude (the difference between target word positions) of crossing links. Finally, hhp is a “hole penalty” model, proportional to the number of embedded positions between two target words linked to the same source words (or vice-versa). We performed a second alignment pass in which the association score model with IBM1 probabilities and the unlinked model were substituted by two improved models benefiting from the first-pass links: an Association score model har with Relative link probabilities (Melamed, 2000), and source and target fertility models (hf s and hf t ) giving the probability for a given word to have one, two, three or four or more links. Second pass models are listed in (2). (2) (2) (2) (2) ˆ(2) = arg max λ(2) a ar har + λlb hlb + λf s hf s + a (2) λf t hf t + λ(2) cn hcn + λcl hcl + λhp hhp (2) To find the best hypothesis, we implemented a beam-search algorithm based on dynamic programming. In a given sentence pair, the best 3 links for each source and for each target word are considered in search. The parameters of the first and second alignment passes were optimised together, to gi"
2009.mtsummit-posters.12,H05-1011,0,0.110043,"ingual phrases involved with embedded words are still extracted. However, those bilingual phrases involving the long-distance oneto-many link itself may be large and thus not easy to reuse. The same problem may happen with long crossing links. Thus we expect that alignments optimised according to BLEU will have shorter links, or shorter crossing links, or fewer embedded words than manual alignments. 3 Alignment Optimisation Procedure Our aim was to obtain alignments optimised according to both an intrinsic and an extrinsic criterion. To achieve this, we used a discriminative alignment system (Moore, 2005) because of its flexibility. For both criteria, the optimisation consisted of maximising a function of the alignment system parameters: F-score (intrinsic criterion) and BLEU score (extrinsic criterion). First we describe the alignment system used, then the optimisation procedure. 3.1 Discriminative Alignment System This aligner implements a log-linear combination of feature functions calculated at the sentence pair level. In a first pass, the training corpus was aligned selecting for each sentence pair (s, t) the alignment ˆ which maximises a combination of varhypothesis a ious models, as exp"
2009.mtsummit-posters.12,J03-1002,0,0.0178041,", F = , |A| |GS | P +R where A, GS and G are respectively the computed link set, the reference sure link set, and the total reference link set. Extrinsic evaluation was performed with the BLEU score (Papineni et al., 2002). Translations were computed either by Moses (Koehn et al., 2007) with all default parameters, or by a baseline n-gram-based system with constrained reordered search (Crego and Mari˜no, 2007). 5 Results We present intrinsic and extrinsic evaluation results as well as some statistics for 9 alignment sets. 3 sets are baseline sets, and correspond to combinations of the Giza++ (Och and Ney, 2003) source-target and target-source alignments computed by Moses scripts: intersection (I), union (U) and grow-diagfinal heuristic (GDF) (Koehn et al., 2003). The other sets were aligned with the optimum weights of the discriminative aligner (Section 3.1) resulting from optimisations according to the F-score, to the phrase-based system BLEU score and to the ngram-based system BLEU score (referred to as F, PB and NB, respectively). Because the optimisation algorithm can get stuck in a poor local maximum, the optimisation with each criterion was performed with three different random seeds. To have"
2009.mtsummit-posters.12,P03-1021,0,0.00416473,"he F-score (see Section 4). This constitutes the first iteration of the optimisation algorithm, realised with initial parameters. 1 The weights in each pass can be normalised such that one (1) (2) weight is set to 1. This is why λa1 and λar were not free parameters. Then, alignment system parameters were simply adjusted by the optimisation algorithm so as to maximise the F-score. In the case of Function (4), the training corpus was aligned with initial parameters and these alignments were used to build either an n-gram-based or a phrase-based SMT system. The model weights were tuned via MERT (Och, 2003), with the Moses MERT utility (which was adapted to the n-grambased system). Then a translation of a development corpus was obtained and evaluated using BLEU (see Section 4). Thus, at each iteration, the considered parallel corpus was aligned (with the two successive passes), an SMT system was built from the resulting alignments (including bilingual phrase extraction, model(s) estimation and MERT) and the development set was translated to obtain the BLEU score. At the end of this process we obtained the alignment parameters which maximise the BLEU score. Note that we used two developments sets"
2009.mtsummit-posters.12,P02-1040,0,0.0763899,"we can tune the alignment directly according to MT metrics (Lambert et al., 2007). In this paper we rather try to find out which alignment characteristics help or worsen translation. In the related papers (see next section) some alignment characteristics are usually considered, and the impact on MT of alignments with different values for these characteristics is evaluated. The contributions of this paper are twofold. Firstly, the problem is considered from the inverse point of view: we start from an initial alignment and tune it directly according to a translation quality metric (BLEU score (Papineni et al., 2002)) and according to an alignment quality metric (F-score, see Section 4.3). In this way, we can investigate for any alignment characteristic how it is affected by the change of tuning criterion. If there exist alignment characteristics which are helpful in translation, they should not depend on the aligner used. However, they could depend on the MT system, the language pair, or the corpus size or type. The second contribution of this paper is to study more systematically how the considered characteristics depend on these parameters. We report results for two different SMT systems: a phrase-base"
2009.mtsummit-posters.12,takezawa-etal-2002-toward,0,0.0614575,"form: ˆ k+1 = λ ˆ k − ak g ˆk ) ˆ k (λ λ (5) ˆ k ) is the estimate of the gradient g(λ) ≡ ˆk (λ where g ˆ k based on the previous eval∂E/∂λ at the iterate λ uations of the objective function. ak denotes a positive number that usually decreases as k increases. We performed about 80 evaluations of the objective function. Note that in general, SPSA converges to a local maximum. 4 4.1 Experiments Chinese English BTEC Task The experiments were carried out using the Chinese–English datasets provided within the IWSLT 2007 evaluation campaign, extracted from the Basic Travel Expression Corpus (BTEC) (Takezawa et al., 2002). Training data consisted of the default training set, to which we added the sets devset1, devset2 and devset3. The resulting corpus contains 41.5k sentence pairs having respectively 9.4 and 8.7 words on average for English and Chinese. English and Chinese vocabulary sizes are respectively 9.8k and 11.4k. Manual annotation of word alignment was carried out on devset3, of which 251 sentence pairs were used as the development set and 251 for testing. For MT evaluation, we used IWSLT 2006 test set (500 sentences, 6.1k words, 7 references) as development set for the internal SMT MERT procedure, an"
2009.mtsummit-posters.12,2006.iwslt-papers.7,0,0.727119,"teristics that are helpful in translation. We report results for two different SMT systems (a phrase-based and an n-gram-based system) on Chinese to English IWSLT data, and Spanish to English European Parliament data. We give alignment hints to improve BLEU score, depending on the SMT system used and the type of corpus. 1 Introduction Most statistical machine translation (SMT) systems (e.g. phrase-based, n-gram-based) extract their translation models from word alignment trained in a previous stage. Many papers have shown that alignment quality is poorly correlated with MT quality (for example Vilar et al. (2006)). Then, we can tune the alignment directly according to MT metrics (Lambert et al., 2007). In this paper we rather try to find out which alignment characteristics help or worsen translation. In the related papers (see next section) some alignment characteristics are usually considered, and the impact on MT of alignments with different values for these characteristics is evaluated. The contributions of this paper are twofold. Firstly, the problem is considered from the inverse point of view: we start from an initial alignment and tune it directly according to a translation quality metric (BLEU"
2010.eamt-1.7,P06-1002,0,0.0598936,"(Lambert et al., 2007). In this paper we instead try to discover which alignment characteristics improve or worsen translation quality by analysing the word alignment produced by c 2010 European Association for Machine Translation. the alignment model with different tuning criteria. The findings can potentially benefit our understanding of existing SMT systems as well as designing novel word alignment models. A considerable amount of research effort has been devoted to the investigation of alignment characteristics that benefit MT. These characteristics include alignment precision and recall (Ayan and Dorr, 2006; Chen and Federico, 2006; Mari˜no et al., 2006; Fraser and Marcu, 2007), longdistance links (Vilar et al., 2006), unlinked words (Guzman et al., 2009; Lambert et al., 2009), etc. In most of the related papers some alignment characteristics are usually considered, and the impact on MT of alignments with different values for these characteristics is evaluated. In this work, we start from an initial alignment and tune it directly according to an intrinsic alignment quality metric (F-score, see Section 3.4) and according to an extrinsic translation quality metric (BLEU score (Papineni et al., 200"
2010.eamt-1.7,P07-2045,0,0.0128199,"Missing"
2010.eamt-1.7,2008.eamt-1.15,1,0.814014,"untranslated words (words present in the training corpus but not translated) PB BLEU score 3 Experimental Setup Our aim is to obtain alignments optimised according to both an intrinsic and an extrinsic criterion. For each criterion, the optimisation consists of maximising a function of the alignment system parameters: F-score (intrinsic criterion), and BLEU score (extrinsic criterion). We use a discriminative alignment system (Moore, 2005) because of its flexibility. First we describe this aligner, and then the optimisation procedure. 3.1 Discriminative Alignment System This alignment system (Lambert and Banchs, 2008) implements a log-linear combination of N feature functions which are calculated at the sentence pair level. The alignment is performed in two passes. First pass features include word association models based on IBM model 1 probabilities (Brown et al., 1993), an unlinked word model proportional to the IBM model 1 NULL link probability, a feature counting the number of links in the hypothesis, distortion models, etc. In the second alignment pass, the association score model with IBM1 probabilities and the unlinked model are substituted by two improved models benefiting from the first-pass links"
2010.eamt-1.7,N07-2022,1,0.832994,"alignment characteristics that are correlated with BLEU score, we give alignment hints to improve BLEU score using a phrase-based SMT system and different types of corpus. 1 Introduction Most statistical machine translation (SMT) systems (e.g. phrase-based, n-gram-based) build their translation models from word alignments trained in a previous stage. Many papers have shown that intrinsic alignment quality is poorly correlated with MT quality (for example (Vilar et al., 2006)). Accordingly, some research has attempted to tune the alignment directly according to specific MT evaluation metrics (Lambert et al., 2007). In this paper we instead try to discover which alignment characteristics improve or worsen translation quality by analysing the word alignment produced by c 2010 European Association for Machine Translation. the alignment model with different tuning criteria. The findings can potentially benefit our understanding of existing SMT systems as well as designing novel word alignment models. A considerable amount of research effort has been devoted to the investigation of alignment characteristics that benefit MT. These characteristics include alignment precision and recall (Ayan and Dorr, 2006; C"
2010.eamt-1.7,2009.mtsummit-posters.12,1,0.171962,"uced by c 2010 European Association for Machine Translation. the alignment model with different tuning criteria. The findings can potentially benefit our understanding of existing SMT systems as well as designing novel word alignment models. A considerable amount of research effort has been devoted to the investigation of alignment characteristics that benefit MT. These characteristics include alignment precision and recall (Ayan and Dorr, 2006; Chen and Federico, 2006; Mari˜no et al., 2006; Fraser and Marcu, 2007), longdistance links (Vilar et al., 2006), unlinked words (Guzman et al., 2009; Lambert et al., 2009), etc. In most of the related papers some alignment characteristics are usually considered, and the impact on MT of alignments with different values for these characteristics is evaluated. In this work, we start from an initial alignment and tune it directly according to an intrinsic alignment quality metric (F-score, see Section 3.4) and according to an extrinsic translation quality metric (BLEU score (Papineni et al., 2002)). In this way, we can investigate for any alignment characteristic how it is affected by the change of tuning criterion. If there exist alignment characteristics which ar"
2010.eamt-1.7,J06-4004,1,0.898484,"Missing"
2010.eamt-1.7,H05-1011,0,0.0612816,"of a cluster, in the source or target side For each system we calculated the value for the following alignment and translation quantities: Translation pb notr Number of untranslated words (words present in the training corpus but not translated) PB BLEU score 3 Experimental Setup Our aim is to obtain alignments optimised according to both an intrinsic and an extrinsic criterion. For each criterion, the optimisation consists of maximising a function of the alignment system parameters: F-score (intrinsic criterion), and BLEU score (extrinsic criterion). We use a discriminative alignment system (Moore, 2005) because of its flexibility. First we describe this aligner, and then the optimisation procedure. 3.1 Discriminative Alignment System This alignment system (Lambert and Banchs, 2008) implements a log-linear combination of N feature functions which are calculated at the sentence pair level. The alignment is performed in two passes. First pass features include word association models based on IBM model 1 probabilities (Brown et al., 1993), an unlinked word model proportional to the IBM model 1 NULL link probability, a feature counting the number of links in the hypothesis, distortion models, etc"
2010.eamt-1.7,J93-2003,0,0.0146222,"ximising a function of the alignment system parameters: F-score (intrinsic criterion), and BLEU score (extrinsic criterion). We use a discriminative alignment system (Moore, 2005) because of its flexibility. First we describe this aligner, and then the optimisation procedure. 3.1 Discriminative Alignment System This alignment system (Lambert and Banchs, 2008) implements a log-linear combination of N feature functions which are calculated at the sentence pair level. The alignment is performed in two passes. First pass features include word association models based on IBM model 1 probabilities (Brown et al., 1993), an unlinked word model proportional to the IBM model 1 NULL link probability, a feature counting the number of links in the hypothesis, distortion models, etc. In the second alignment pass, the association score model with IBM1 probabilities and the unlinked model are substituted by two improved models benefiting from the first-pass links: an association score model with relative link probabilities, and source and target fertility models giving the probability for a given word to have one, two, three or four or more links. The best hypothesis is the one with best score for the weighted sum o"
2010.eamt-1.7,J03-1002,0,0.0081337,"ch (Crego and Mari˜no, 2007). In order to limit the error introduced by MERT, we ran 4 MERT instances, each with a different random seed. We then either consider the average of the 4 values, or take the 4 values into account in the statistical analysis of the results. Table 2: BLEU score using different alignment sets on the Spanish–English test data and Chinese– English test data 4 4.2 P = 4.1 Results and Statistical Analysis Translation Results We produced 10 alignment sets in total obtained using different methods. This includes 3 baseline sets, corresponding to combinations of the Giza++ (Och and Ney, 2003) source–target and target–source alignments computed by Moses scripts: intersection (I), union (U) and grow-diagfinal heuristic (GDF) (Koehn et al., 2003). 6 sets were produced with the optimum weights of the discriminative aligner (Section 3.2) resulting from optimisations according to F-score, to the phrasebased system BLEU score and to the n-gramTable 2 shows the performance of the phrasebased SMT system using the 10 different alignments described above. The optimisation procedure was effective for this system. The best systems built from discriminative alignments were indeed those optimise"
2010.eamt-1.7,P02-1040,0,0.0784161,"(Ayan and Dorr, 2006; Chen and Federico, 2006; Mari˜no et al., 2006; Fraser and Marcu, 2007), longdistance links (Vilar et al., 2006), unlinked words (Guzman et al., 2009; Lambert et al., 2009), etc. In most of the related papers some alignment characteristics are usually considered, and the impact on MT of alignments with different values for these characteristics is evaluated. In this work, we start from an initial alignment and tune it directly according to an intrinsic alignment quality metric (F-score, see Section 3.4) and according to an extrinsic translation quality metric (BLEU score (Papineni et al., 2002)). In this way, we can investigate for any alignment characteristic how it is affected by the change of tuning criterion. If there exist alignment characteristics which are helpful in translation, they should not depend on the specific aligner used. However, they could depend on parameters such as the type of MT system, the language pair, or the corpus size or type. In this way we can study more systematically how the considered characteristics depend on these parameters. We report results for the Moses phrasebased SMT system (Koehn et al., 2007). We undertook this comparison on two different"
2010.eamt-1.7,J07-3002,0,0.322306,"ch alignment characteristics improve or worsen translation quality by analysing the word alignment produced by c 2010 European Association for Machine Translation. the alignment model with different tuning criteria. The findings can potentially benefit our understanding of existing SMT systems as well as designing novel word alignment models. A considerable amount of research effort has been devoted to the investigation of alignment characteristics that benefit MT. These characteristics include alignment precision and recall (Ayan and Dorr, 2006; Chen and Federico, 2006; Mari˜no et al., 2006; Fraser and Marcu, 2007), longdistance links (Vilar et al., 2006), unlinked words (Guzman et al., 2009; Lambert et al., 2009), etc. In most of the related papers some alignment characteristics are usually considered, and the impact on MT of alignments with different values for these characteristics is evaluated. In this work, we start from an initial alignment and tune it directly according to an intrinsic alignment quality metric (F-score, see Section 3.4) and according to an extrinsic translation quality metric (BLEU score (Papineni et al., 2002)). In this way, we can investigate for any alignment characteristic ho"
2010.eamt-1.7,2009.mtsummit-papers.5,0,0.234221,"e word alignment produced by c 2010 European Association for Machine Translation. the alignment model with different tuning criteria. The findings can potentially benefit our understanding of existing SMT systems as well as designing novel word alignment models. A considerable amount of research effort has been devoted to the investigation of alignment characteristics that benefit MT. These characteristics include alignment precision and recall (Ayan and Dorr, 2006; Chen and Federico, 2006; Mari˜no et al., 2006; Fraser and Marcu, 2007), longdistance links (Vilar et al., 2006), unlinked words (Guzman et al., 2009; Lambert et al., 2009), etc. In most of the related papers some alignment characteristics are usually considered, and the impact on MT of alignments with different values for these characteristics is evaluated. In this work, we start from an initial alignment and tune it directly according to an intrinsic alignment quality metric (F-score, see Section 3.4) and according to an extrinsic translation quality metric (BLEU score (Papineni et al., 2002)). In this way, we can investigate for any alignment characteristic how it is affected by the change of tuning criterion. If there exist alignment c"
2010.eamt-1.7,N03-1017,0,0.0201224,"consider the average of the 4 values, or take the 4 values into account in the statistical analysis of the results. Table 2: BLEU score using different alignment sets on the Spanish–English test data and Chinese– English test data 4 4.2 P = 4.1 Results and Statistical Analysis Translation Results We produced 10 alignment sets in total obtained using different methods. This includes 3 baseline sets, corresponding to combinations of the Giza++ (Och and Ney, 2003) source–target and target–source alignments computed by Moses scripts: intersection (I), union (U) and grow-diagfinal heuristic (GDF) (Koehn et al., 2003). 6 sets were produced with the optimum weights of the discriminative aligner (Section 3.2) resulting from optimisations according to F-score, to the phrasebased system BLEU score and to the n-gramTable 2 shows the performance of the phrasebased SMT system using the 10 different alignments described above. The optimisation procedure was effective for this system. The best systems built from discriminative alignments were indeed those optimised with the phrase-based BLEU score as the objective function. When the alignment weights were tuned on the corresponding training corpus (all tasks except"
2010.eamt-1.7,takezawa-etal-2002-toward,0,0.0817791,"Missing"
2010.eamt-1.7,2006.iwslt-papers.7,0,0.017326,"phrasebased SMT system on Chinese-to-English IWSLT data, and Spanish-to-English European Parliament data. With a statistical analysis into alignment characteristics that are correlated with BLEU score, we give alignment hints to improve BLEU score using a phrase-based SMT system and different types of corpus. 1 Introduction Most statistical machine translation (SMT) systems (e.g. phrase-based, n-gram-based) build their translation models from word alignments trained in a previous stage. Many papers have shown that intrinsic alignment quality is poorly correlated with MT quality (for example (Vilar et al., 2006)). Accordingly, some research has attempted to tune the alignment directly according to specific MT evaluation metrics (Lambert et al., 2007). In this paper we instead try to discover which alignment characteristics improve or worsen translation quality by analysing the word alignment produced by c 2010 European Association for Machine Translation. the alignment model with different tuning criteria. The findings can potentially benefit our understanding of existing SMT systems as well as designing novel word alignment models. A considerable amount of research effort has been devoted to the inv"
2010.eamt-1.7,J00-2004,0,\N,Missing
arranz-etal-2004-bilingual,ide-etal-2000-xces,0,\N,Missing
arranz-etal-2004-bilingual,W02-1012,0,\N,Missing
arranz-etal-2004-bilingual,N01-1026,0,\N,Missing
arranz-etal-2004-bilingual,W02-0808,0,\N,Missing
C16-1152,D08-1014,0,0.0315629,"is not available in most language combinations. For these other languages, it would be useful to find alternative methods which do not require machine translation. 2 The system achieves a BLEU score 45.3 in Spanish-English translation with true-case. 1614 CLSA via Machine Translation Advances in machine translation have made it possible to translate data from English to a target language or vice versa and use this data to train a classifying algorithm. There are reasons to believe that for well-resourced languages machine translation has reached a level that is useful for sentiment analysis (Banea et al., 2008; Duh et al., 2011; Balahur and Turchi, 2014; Mohammad et al., 2015). Much of the work has concentrated on the best combination of translation direction, classifiers and features (Banea et al., 2008; Banea et al., 2013; Balahur and Turchi, 2014). The advantage of this approach is that it is straight-forward to use a quality SMT system to create new resources by translating annotated corpora or sentiment lexicons (Mihalcea et al., 2007). Nonetheless, there are disadvantages to using directly translated resources. Poor translation introduces a large amount of noise which hurts the performance of"
C16-1152,P14-1009,0,0.0151456,"s seem to suggest that using smaller, task-specific in-domain datasets which are automatically discovered from larger datasets may be key in improving performance in CLSC. Representation Besides using the average of the vectors in the opinion unit as a representation, we also experimented with summation and using Long Short-term Memory networks (LSTMs) as a way to deal with the different lengths of the opinion units for our vector-based methods. Although it lacks a strong theoretical motivation, summation is often used in distributional semantics as a baseline for combining vectors (Giorgiana Dinu and Baroni, 2014). Summation led to results that were slightly worse than averaging. This is likely due to the fact that longer opinion units result in vectors which are a magnitude larger than shorter opinion units. We discuss LSTMs below. Classifiers Apart from the SVM classifiers used in all experiments, we conducted further experiments using deep feed-forward networks during the zero-shot and bilingual word embeddings experiments. We used the 1620 DAN model (Iyyer et al., 2015), with three hidden layers of 300 dimensions. This model performed better on the zero-shot learning experiments, but similar to SVM"
C16-1152,N15-1071,0,0.0241473,"other languages would be a great advantage. This would increase the performance of ABSA systems which are built using limited amounts of data in lowresourced languages and enable the creation of sentiment analysis systems in languages which have none at the moment. Similarly, within CLSA, most researchers have worked at document- and sentence-level. In fact, there are only a handful of articles that deal with aspect-based CLSA. Zhou et al. (2012), Lin et al. (2014) and Klinger and Cimiano (2015) concentrate on extracting bilingual aspects but offer few ways to improve classification accuracy. Hass and Versley (2015) used machine translation and word alignment to map annotated syntactic nodes from English to German. One of the difficulties at aspect-level is that the opinions attach to specific groupings of words, rather than a sentence or document. If we use SMT to create a new target language dataset, the opinionated units (e.g. opinion holder, opinion target, and opinion phrase) may be scattered or reordered. This would effectively reduce the usefulness of our new data because it would be difficult to project the opinion labels onto their corresponding word or phrase in the new dataset. Lambert (2015)"
C16-1152,P15-1162,0,0.146784,"e same train and test split shown in Table 1. For each experiment, we trained a classifier on the English training data, performed the cross-lingual transfer on the Spanish test data and used this new data to test our classifier, as in Figure 1. One difficulty encountered when using vector representations is that the opinion units are variable length. This means that to train a classifier either we find a fixed-length representation for all opinion units or we use a classifier that accepts variable-length input. We decided to take an averaging approach, which has shown promise in other works (Iyyer et al., 2015). For each opinion unit we took the arithmetic mean of the words that compose the opinion unit, 3 http://www.statmt.org/europarl 1616 as shown in Figure 2, in order to create a fixed-length vector representation for each sentence. We then use these vectors to train a classifier. For the SMT transfer methods, we trained the classifier on unigram features. In all experiments, we used the sequential minimal optimization (SMO) classifier from the WEKA toolkit (Hall et al., 2009). Figure 1: The process of cross-lingual sentiment classification. We assume that the opinion units have already been det"
C16-1152,S14-2076,0,0.0260659,"competitive with machine translation for CLSC tasks. 2 Related Work 2.1 Monolingual Aspect-based Sentiment Analysis Aspect-based sentiment analysis (ABSA) is a fine-grained approach to sentiment analysis. Many of the state-of-the-art ABSA systems in English require sophisticated NLP tools or hand-crafted sentiment lexicons. Hu and Liu (2004) propose WordNet-based methods for classifying aspect sentiment. Zhu et al. (2009) use sentiment lexicons. Moghaddam (2010) extracts an aspect and its nearest adjective and use a k nearest neighbor algorithm in order to estimate the rating of each aspect. Kiritchenko et al. (2014) use extracted features (part-of-speech tags, parsing features, sentiment lexicons, characterbased information, n-grams) to train a support vector machine (SVM) for sentiment classification. These language-specific approaches do not lend themselves easily to CLSA because the target language often lacks the necessary resources. 2.2 Cross-lingual Sentiment Analysis Aspect-based CLSA In under-resourced languages, we lack resources and NLP tools which would allow us to create stateof-the-art systems similar to those mentioned. Therefore, the ability to leverage resources that already exist in Engl"
C16-1152,C12-1089,0,0.176398,"at, but the service needs to improve” contains two aspects (rooms and service) which pertain to the entity hotel. It is more useful to know that rooms is positive and service is negative than to know the overall sentiment towards hotel 1613 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1613–1623, Osaka, Japan, December 11-17 2016. et al., 2015). Finally, there are tasks in which systems which use distributed semantic representations to map between languages outperform SMT systems, e.g. cross-lingual document classification (Klementiev et al., 2012). For this reason, a different representation of words and phrases, e.g. distributional vector representations, could prove to be a more effective approach and enable us to leverage information from resourcerich languages (English) to perform CLSA in a target language that lacks these resources (e.g. Spanish, Catalan, Basque). This paper makes the following contributions: • According to our knowledge, this is the most complete comparison of several types of distributed representations and machine translation for cross-lingual sentiment analysis. • We give an analysis of the errors and possible"
C16-1152,K15-1016,0,0.145838,"those mentioned. Therefore, the ability to leverage resources that already exist in English to perform sentiment analysis in other languages would be a great advantage. This would increase the performance of ABSA systems which are built using limited amounts of data in lowresourced languages and enable the creation of sentiment analysis systems in languages which have none at the moment. Similarly, within CLSA, most researchers have worked at document- and sentence-level. In fact, there are only a handful of articles that deal with aspect-based CLSA. Zhou et al. (2012), Lin et al. (2014) and Klinger and Cimiano (2015) concentrate on extracting bilingual aspects but offer few ways to improve classification accuracy. Hass and Versley (2015) used machine translation and word alignment to map annotated syntactic nodes from English to German. One of the difficulties at aspect-level is that the opinions attach to specific groupings of words, rather than a sentence or document. If we use SMT to create a new target language dataset, the opinionated units (e.g. opinion holder, opinion target, and opinion phrase) may be scattered or reordered. This would effectively reduce the usefulness of our new data because it w"
C16-1152,2005.mtsummit-papers.11,0,0.278176,"to create the word embeddings are an English and Spanish Wikipedia corpus. These were taken from Wikipedia dumps in January 2016 and preprocessed to remove html markup and lowercase all words. We then performed sentence and word tokenization. We did not remove punctuation because this is often useful information for sentiment analysis. Table 2 gives the statistics for these corpora. Wikipedia Corpora Number of sentences Number of tokens English 118,900,197 2,055,786,401 Spanish 26,777,415 506,612,108 Table 2: Statistics of Wikipedia Corpora The English-Spanish part of the Europarl v7 corpus3 (Koehn, 2005) is used as parallel data. It contains around 2 million aligned sentences from the European Parliament. Table 3 shows the statistics for this corpus. Europarl v7 Corpus Number of sentences Number of tokens English 1,965,734 49,093,806 Spanish 1,965,734 51,575,784 Table 3: Statistics of Europarl v7 Corpus 3.2 Experiments We performed a set of experiments in order to test different approaches for aspect-based CLSA. Each experiment requires a different amount of parallel data. Representation of Training and Test Data for Sentiment Classification For all experiments we use the same train and test"
C16-1152,P15-2128,1,0.942744,"Versley (2015) used machine translation and word alignment to map annotated syntactic nodes from English to German. One of the difficulties at aspect-level is that the opinions attach to specific groupings of words, rather than a sentence or document. If we use SMT to create a new target language dataset, the opinionated units (e.g. opinion holder, opinion target, and opinion phrase) may be scattered or reordered. This would effectively reduce the usefulness of our new data because it would be difficult to project the opinion labels onto their corresponding word or phrase in the new dataset. Lambert (2015) deals with this by using constrained SMT to translate the opinionated units within the context of the sentence. The classifiers trained on this SMT data achieve comparable results to their monolingual version. However, this is a state-of-the-art SMT system2 which is not available in most language combinations. For these other languages, it would be useful to find alternative methods which do not require machine translation. 2 The system achieves a BLEU score 45.3 in Spanish-English translation with true-case. 1614 CLSA via Machine Translation Advances in machine translation have made it possi"
C16-1152,P15-1027,0,0.0171865,"th other approaches. In our approach, however, all of the weight of correctly classifying a phrase fell on the accuracy of the mapping scheme. Therefore, it seems that any error in the mapping resulted in the propagation of error during classification. Another problem that arose is that there were some words whose vector representation always appeared as the nearest neighbor of many other words, although they were not semantically similar with any of them. This problem is known as hubness and is an intrinsic problem with high-dimensional vector space. Our work seems to confirm the research of Lazaridou et al. (2015) and Georgiana Dinu et al. (2015), who showed that hubness is compounded when trying to create a linear mapping between two sets of word embeddings. Bilingual Word Embeddings The next set of experiments required the use of parallel sentences to create bilingual word embeddings (BWEs). Following the work of Luong et al. (2015), we created bilingual word embeddings using the Bilingual Skip-gram algorithm, which uses the Skip-gram model (Mikolov et al., 2013a) with an added bilingual objective. This algorithm creates vector representations in which words that appear in parallel sentences have sim"
C16-1152,P11-1033,0,0.0204488,"lation introduces by presenting classifiers with complementary views. Wan (2009) creates a bilingual representation of the data through SMT and then uses co-training to take advantage of classifiers that commit complementary errors. This research seems promising, but there are some reasons to believe that the benefits of these techniques may have more to do with semi-supervised learning than cross-lingual transfer (Demirtas and Pechenizkiy, 2013). Pan et al. (2011) use a bi-view non-negative matrix tri-factorization approach which allows for the incorporation of sentiment lexicon information. Lu et al. (2011) incorporate a joint bilingual model which makes use of unlabeled parallel or pseudo-parallel data in order to improve sentiment classification for both languages simultaneously. CLSA via Latent View Zhou et al. (2016) employ stacked denoising autoencoders to create a language independent representation of their data. This representation was then used as input for a linear SVM classifier. For cross-lingual document classification, Prettenhofer and Stein (2011) use structural correspondence learning in order to find ’pivot’ features. They use these features to create a representation of each do"
C16-1152,P07-1123,0,0.0369833,"classifying algorithm. There are reasons to believe that for well-resourced languages machine translation has reached a level that is useful for sentiment analysis (Banea et al., 2008; Duh et al., 2011; Balahur and Turchi, 2014; Mohammad et al., 2015). Much of the work has concentrated on the best combination of translation direction, classifiers and features (Banea et al., 2008; Banea et al., 2013; Balahur and Turchi, 2014). The advantage of this approach is that it is straight-forward to use a quality SMT system to create new resources by translating annotated corpora or sentiment lexicons (Mihalcea et al., 2007). Nonetheless, there are disadvantages to using directly translated resources. Poor translation introduces a large amount of noise which hurts the performance of the classifier (Balahur and Turchi, 2014; Mohammad et al., 2015). It is clear that under-resourced languages are particularly susceptible to poor translation. Even with high quality machine translation, there is still a cross-lingual adaptation problem; the distribution of words and their polarity do not necessarily hold in cross-lingual contexts (Guo and Xiao, 2012; Mohammad et al., 2015). Therefore, we must find ways to minimize the"
C16-1152,P16-2083,0,0.061411,"Missing"
C16-1152,P14-1146,0,0.0428748,"positional models of meaning and it may improve the performance for sentiment analysis as well. Secondly, due to the fact that they have similar distributions, antonyms are often given similar vector representations. This is not a problem for POS-taggers or parsers, but it is detrimental to sentiment analysis systems based on word embeddings because these words have opposing polarities and should therefore have different vector representations. To remedy this, one could add a classification task in the problem formulation that would better separate these antonyms into differing vector spaces (Tang et al., 2014). Another option is to decompose the word vectors into interpretable subspaces, train them to differentiate for a certain property, and use only these spaces as features (Rothe and Sch¨utze, 2016). Stacked Denoising Autoencoders Following the work of Zhou et al. (2016) we trained a stacked bilingual denoising autoencoder (SDBA) on parallel sentences from the Europarl corpus. This approach aims to encode the parallel sentences into a common latent space. Given a vocabulary of length n, the autoencoder maps the sentences, which are represented as n-dimensional one-hot vectors, to a lower dimensi"
C16-1152,P09-1027,0,0.0392896,"usceptible to poor translation. Even with high quality machine translation, there is still a cross-lingual adaptation problem; the distribution of words and their polarity do not necessarily hold in cross-lingual contexts (Guo and Xiao, 2012; Mohammad et al., 2015). Therefore, we must find ways to minimize the undesirable effects of translation in cross-lingual sentiment analysis. CLSA via Bilingual View Another approach is to create a bilingual view of the data. The essence of this approach is to reduce the noise that translation introduces by presenting classifiers with complementary views. Wan (2009) creates a bilingual representation of the data through SMT and then uses co-training to take advantage of classifiers that commit complementary errors. This research seems promising, but there are some reasons to believe that the benefits of these techniques may have more to do with semi-supervised learning than cross-lingual transfer (Demirtas and Pechenizkiy, 2013). Pan et al. (2011) use a bi-view non-negative matrix tri-factorization approach which allows for the incorporation of sentiment lexicon information. Lu et al. (2011) incorporate a joint bilingual model which makes use of unlabele"
E12-2003,P07-2045,0,0.00303578,"Table 1 presents statistics of these in-domain data. The data extracted from HAL were used to adapt a generic system to the scientific literature domain. The generic system was mostly trained on data provided for the shared task of Sixth Workshop on Statistical Machine Translation6 (WMT 2011), described in Table 2. Table 3 presents results showing, in the English–French direction, the impact on the statistical engine of introducing the resources extracted from HAL, as well as the impact of domain adaptation techniques. The baseline statistical engine is a standard PBSMT system based on Moses (Koehn et al., 2007) and the SRILM tookit (Stolcke, 2002). Is was trained and tuned only on WMT11 data (out-of-domain). Incorporating the HAL data into the language model and tuning the system on the HAL development set, Domain Lg Monolingual data Train cs En 2.5 M Fr 761 k phys En 2.1 M Fr 662 k 54 M 19 M 50 M 17 M 457 k 274 k 646 k 292 k Table 1: Statistics for the parallel training, development, and test data sets extracted from thesis abstracts contained in HAL, as well as monolingual data extracted from all documents in HAL, in computer science (cs) and physics (phys). The following statistics are given for"
E12-2003,2008.iwslt-papers.6,1,0.861276,"f the corpus: the number of sentences, the number of running words (after tokenisation) and the number of words in the vocabulary (M and k stand for millions and thousands, respectively). yielded a gain of more than 7 BLEU points, in both domains (computer science and physics). Including the theses abstracts in the parallel training corpus, a further gain of 2.3 BLEU points is observed for computer science, and 3.1 points for physics. The last experiment performed aims at increasing the amount of in-domain parallel texts by translating automatically in-domain monolingual data, as suggested by Schwenk (2008). The synthesised bitext does not bring new words into the system, but increases the probability of indomain bilingual phrases. By adding a synthetic bitext of 12 million words to the parallel training data, we observed a gain of 0.5 BLEU point for computer science, and 0.7 points for physics. Although not shown here, similar results were obtained in the French–English direction. The French–English system is actually slightly better than the English–French one as it is an easier translation direction. 13 Translation Model Language Model Tuning Domain wmt11 wmt11+hal wmt11+hal wmt11+hal wmt11 h"
J06-4004,W05-0823,1,0.838951,"Missing"
J06-4004,W00-0508,0,0.0142428,"y approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney 2002). As an extension of the machine translation problem, technological advances in the fields of automatic speech recognition (ASR) and text to speech synthesis (TTS) made it possible to envision the challenge of spoken language translation (SLT) (Kay, Gawron, and Norvig 1992). According to this, SMT has also been approached from a finite-state point of view as the most natural way of integrating ASR and SMT (Riccardi, Pieraccini, and Bocchieri 1996; Vidal 1997; Knight and Al-Onaizan 1998; Bangalore and Riccardi 2000). In this SMT approach, translation models are implemented by means of finitestate transducers for which transition probabilities are learned from bilingual data. As opposed to phrase-based translation models, which consider probabilities between target and source units referred to as phrases, finite-state translation models rely on probabilities among sequences of bilingual units, which are defined by the transitions of the transducer. The translation system described in this article implements a translation model that has been derived from the finite-state perspective—more specifically, from"
J06-4004,J96-1002,0,0.02988,"Missing"
J06-4004,J90-2002,0,0.81424,"nslation was conceived as the problem of finding a sentence by decoding a given “encrypted” version of it (Weaver 1955). Although the idea seemed very feasible, enthusiasm faded shortly afterward because of the computational limitations of the time (Hutchins 1986). Finally, during the nineties, two factors made it possible for SMT to become an actual and practical technology: first, significant increment in both the computational power and storage capacity of computers, and second, the availability of large volumes of bilingual data. The first SMT systems were developed in the early nineties (Brown et al. 1990, 1993). These systems were based on the so-called noisy channel approach, which models the probability of a target language sentence T given a source language sentence S as the product of a translation-model probability p(S|T), which accounts for adequacy of translation contents, times a target language probability p(T), which accounts for fluency of target constructions. For these first SMT systems, translation-model probabilities at the sentence level were approximated from word-based translation models that were trained by using bilingual corpora (Brown et al. 1993). In the case of target"
J06-4004,J93-2003,0,0.0556776,"d in the early nineties (Brown et al. 1990, 1993). These systems were based on the so-called noisy channel approach, which models the probability of a target language sentence T given a source language sentence S as the product of a translation-model probability p(S|T), which accounts for adequacy of translation contents, times a target language probability p(T), which accounts for fluency of target constructions. For these first SMT systems, translation-model probabilities at the sentence level were approximated from word-based translation models that were trained by using bilingual corpora (Brown et al. 1993). In the case of target language probabilities, these were generally trained from monolingual data by using n-grams. Present SMT systems have evolved from the original ones in such a way that mainly differ from them in two respects: first, word-based translation models have been ∗ Department of Signal Theory and Communications, Campus Nord, Barcelona 08034, Spain. Submission received: 9 August 2005; revised submission received: 26 April 2006; accepted for publication: 5 July 2006 © 2006 Association for Computational Linguistics Computational Linguistics Volume 32, Number 4 replaced by phrase-b"
J06-4004,J04-2004,0,0.856987,"models are implemented by means of finitestate transducers for which transition probabilities are learned from bilingual data. As opposed to phrase-based translation models, which consider probabilities between target and source units referred to as phrases, finite-state translation models rely on probabilities among sequences of bilingual units, which are defined by the transitions of the transducer. The translation system described in this article implements a translation model that has been derived from the finite-state perspective—more specifically, from the work of Casacuberta (2001) and Casacuberta and Vidal (2004). However, whereas in this earlier work the translation model is implemented by using a finite-state transducer, in the system presented here the translation model is implemented by using n-grams. In this way, the proposed translation system can take full advantage of the smoothing and consistency provided by standard back-off n-gram models. The translation model presented here actually constitutes a language model of a sort of “bilanguage” composed of bilin˜ 2002). An alternagual units, which will be referred to as tuples (de Gispert and Marino tive approach, which relies on bilingual-unit un"
J06-4004,N04-1033,0,0.0140655,"Missing"
J06-4004,2005.iwslt-1.23,1,0.883592,"Missing"
J06-4004,2005.mtsummit-papers.37,1,0.855856,"Missing"
J06-4004,2006.amta-papers.4,1,0.763242,"Missing"
J06-4004,P05-2012,1,0.804737,"Missing"
J06-4004,C86-1155,0,0.079146,"ament Plenary Sessions (EPPS). 1. Introduction The beginnings of statistical machine translation (SMT) can be traced back to the early fifties, closely related to the ideas from which information theory arose (Shannon and Weaver 1949) and inspired by works on cryptography (Shannon 1949, 1951) during World War II. According to this view, machine translation was conceived as the problem of finding a sentence by decoding a given “encrypted” version of it (Weaver 1955). Although the idea seemed very feasible, enthusiasm faded shortly afterward because of the computational limitations of the time (Hutchins 1986). Finally, during the nineties, two factors made it possible for SMT to become an actual and practical technology: first, significant increment in both the computational power and storage capacity of computers, and second, the availability of large volumes of bilingual data. The first SMT systems were developed in the early nineties (Brown et al. 1990, 1993). These systems were based on the so-called noisy channel approach, which models the probability of a target language sentence T given a source language sentence S as the product of a translation-model probability p(S|T), which accounts for"
J06-4004,knight-al-onaizan-1998-translation,0,0.230855,"more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney 2002). As an extension of the machine translation problem, technological advances in the fields of automatic speech recognition (ASR) and text to speech synthesis (TTS) made it possible to envision the challenge of spoken language translation (SLT) (Kay, Gawron, and Norvig 1992). According to this, SMT has also been approached from a finite-state point of view as the most natural way of integrating ASR and SMT (Riccardi, Pieraccini, and Bocchieri 1996; Vidal 1997; Knight and Al-Onaizan 1998; Bangalore and Riccardi 2000). In this SMT approach, translation models are implemented by means of finitestate transducers for which transition probabilities are learned from bilingual data. As opposed to phrase-based translation models, which consider probabilities between target and source units referred to as phrases, finite-state translation models rely on probabilities among sequences of bilingual units, which are defined by the transitions of the transducer. The translation system described in this article implements a translation model that has been derived from the finite-state persp"
J06-4004,N03-1017,0,0.0258625,"Missing"
J06-4004,2005.mtsummit-papers.36,1,0.177804,"Missing"
J06-4004,P00-1056,0,0.0440511,"tence pairs are removed from the training data to allow for a better performance of the alignment tool. Sentence pairs are removed according to the following two criteria: r r Fertility filtering: removes sentence pairs with a word ratio larger than a predefined threshold value. Length filtering: removes sentence pairs with at least one sentence of more than 100 words in length. This helps to maintain bounded alignment computational times. After preprocessing, word-to-word alignments are performed in both directions, source-to-target and target-to-source. In our system implementation, GIZA++ (Och and Ney 2000) is used for computing the alignments. A total of five iterations for models IBM-1 and HMM, and three iterations for models IBM-3 and IBM-4, are performed. Then, the obtained alignment sets are used for computing the intersection and the union of alignments from which tuples and embedded-word tuples are extracted, respectively. 4.2.2 Tuple Extraction and Pruning. A tuple set for each translation direction is extracted from the union set of alignments while avoiding source-nulled tuples by using the procedure described in Section 2.2.2. Then, the resulting tuple vocabularies are pruned accordin"
J06-4004,P02-1038,0,0.884384,"034, Spain. Submission received: 9 August 2005; revised submission received: 26 April 2006; accepted for publication: 5 July 2006 © 2006 Association for Computational Linguistics Computational Linguistics Volume 32, Number 4 replaced by phrase-based translation models (Zens, Och, and Ney 2002; Koehn, Och, and Marcu 2003) which are directly estimated from aligned bilingual corpora by considering relative frequencies, and second, the noisy channel approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney 2002). As an extension of the machine translation problem, technological advances in the fields of automatic speech recognition (ASR) and text to speech synthesis (TTS) made it possible to envision the challenge of spoken language translation (SLT) (Kay, Gawron, and Norvig 1992). According to this, SMT has also been approached from a finite-state point of view as the most natural way of integrating ASR and SMT (Riccardi, Pieraccini, and Bocchieri 1996; Vidal 1997; Knight and Al-Onaizan 1998; Bangalore and Riccardi 2000). In this SMT approach, translation models are implemented by means of finitesta"
J06-4004,J03-1002,0,0.00706932,"gram model alone, in the case of Table 2, and by using the tuple n-gram model along with the additional four feature functions described in Section 3.2, in the case of Table 3. Both translation directions, Spanish to English (ES → EN) and English to Spanish (EN → ES), are considered in each table. In the case of Table 2, model size and translation accuracy are evaluated against the type of alignment set used for extracting tuples. Three different alignment sets are considered: source-to-target, the union of source-to-target and target-to-source, and the “refined” alignment method described by Och and Ney (2003). For the results presented in Table 2, a pruning parameter value of N = 20 was used for the Spanish-to-English direction, while a value of N = 30 was used for the English-to-Spanish direction. As can be clearly seen in Table 2, the union alignment set happens to be the most favorable one for extracting tuples in both translation directions since it provides a significantly better translation accuracy, in terms of BLEU score, than the other two alignment sets considered. Notice also in Table 2 that the union set is the one providing the smallest model sizes according to the number of bigrams a"
J06-4004,P02-1040,0,0.105596,"alignment sets. Notice that BLEU measurements in this table correspond to translations computed by using the tuple n-gram model alone. Direction Alignment set Tuple voc. Bigrams Trigrams BLEU ES → EN Source-to-target union refined Source-to-target union refined 1.920 2.040 2.111 1.813 2.023 2.081 6.426 6.009 6.851 6.263 6.092 6.920 2.353 1.798 2.398 2.268 1.747 2.323 0.4424 0.4745 0.4594 0.4152 0.4276 0.4193 EN → ES when tuples are extracted from different alignment sets and when different pruning parameters are used, respectively. Translation accuracy is measured in terms of the BLEU score (Papineni et al. 2002), which is computed here for translations generated by using the tuple n-gram model alone, in the case of Table 2, and by using the tuple n-gram model along with the additional four feature functions described in Section 3.2, in the case of Table 3. Both translation directions, Spanish to English (ES → EN) and English to Spanish (EN → ES), are considered in each table. In the case of Table 2, model size and translation accuracy are evaluated against the type of alignment set used for extracting tuples. Three different alignment sets are considered: source-to-target, the union of source-to-targ"
J06-4004,N03-2036,0,0.00459439,"k the translation model is implemented by using a finite-state transducer, in the system presented here the translation model is implemented by using n-grams. In this way, the proposed translation system can take full advantage of the smoothing and consistency provided by standard back-off n-gram models. The translation model presented here actually constitutes a language model of a sort of “bilanguage” composed of bilin˜ 2002). An alternagual units, which will be referred to as tuples (de Gispert and Marino tive approach, which relies on bilingual-unit unigram probabilities, was developed by Tillmann and Xia (2003); in contrast, the approach presented here considers bilingualunit n-gram probabilities. In addition to the tuple n-gram translation model, the translation system presented here implements four specific feature functions that are log-linearly combined along with the translation model for performing the decoding ˜ et al. 2005). (Marino This article is intended to provide a detailed description of the n-gram-based translation system, as well as to demonstrate the system performance in a widedomain, large-vocabulary translation task. The article is structured as follows. First, Section 2 presents"
J06-4004,2002.tmi-tutorials.2,0,0.201063,"Missing"
J06-4004,2004.iwslt-evaluation.14,1,\N,Missing
J06-4004,N04-1021,0,\N,Missing
L18-1104,P15-1040,0,0.253922,"ned opinion research. More recently, a number of SemEval tasks have concentrated on aspect-level sentiment analysis (Pontiki et al., 2014; Pontiki et al., 2015; Pontiki et al., 2016). The Iberian peninsula contains two official languages (Portuguese and Spanish), as well as three co-official languages (Basque, Catalan, and Galician) and several smaller languages (Aragonese, Gascon). The two official languages do have available resources for sentiment at tweet-level (Villena-Rom´an et al., 2013; Arruda et al., 2015), as well as at aspect-level (Agerri et al., 2013; Villena-Rom´an et al., 2015; Almeida et al., 2015). The co-official languages, however, have almost none. The authors are aware of a small discourse-related sentiment corpus available in Basque (Alkorta et al., 2015), as well as a stance corpus in Catalan (Bosco et al., 2016). These resources, however, are limited in size and scope. 3. Data Collection In order to improve the lack of data in low-resource languages, we introduce two aspect-level sentiment datasets to the community, available for Catalan and Basque. To collect suitable corpora, we crawl hotel reviews from www. booking.com. Booking.com allows you to search for reviews in Catalan,"
L18-1104,C12-2008,0,0.0650962,"Missing"
L18-1104,W15-5614,0,0.0443154,"a single entity. In English, the MPQA corpus (Wiebe et al., 2005) has been widely used in fine-grained opinion research. More recently, a number of SemEval tasks have concentrated on aspect-level sentiment analysis (Pontiki et al., 2014; Pontiki et al., 2015; Pontiki et al., 2016). The Iberian peninsula contains two official languages (Portuguese and Spanish), as well as three co-official languages (Basque, Catalan, and Galician) and several smaller languages (Aragonese, Gascon). The two official languages do have available resources for sentiment at tweet-level (Villena-Rom´an et al., 2013; Arruda et al., 2015), as well as at aspect-level (Agerri et al., 2013; Villena-Rom´an et al., 2015; Almeida et al., 2015). The co-official languages, however, have almost none. The authors are aware of a small discourse-related sentiment corpus available in Basque (Alkorta et al., 2015), as well as a stance corpus in Catalan (Bosco et al., 2016). These resources, however, are limited in size and scope. 3. Data Collection In order to improve the lack of data in low-resource languages, we introduce two aspect-level sentiment datasets to the community, available for Catalan and Basque. To collect suitable corpora, w"
L18-1104,P07-1056,0,0.100748,"hey are compatible with similarly compiled corpora available in a number of languages (Agerri et al., 2013). This allows for further research into cross-lingual sentiment analysis, as well as introducing the first resource for aspect-level sentiment analysis in Catalan and Basque. The corpus is available at http: //hdl.handle.net/10230/33928 or https:// jbarnesspain.github.io/resources/. Related Work In English there are many datasets available for documentand sentence-level sentiment analysis across different domains and at different levels of annotation (Pang et al., 2002; Hu and Liu, 2004; Blitzer et al., 2007; Socher et al., 2013; Nakov et al., 2013). These resources have been built up over a period of more than a decade and are currently necessary to achieve state-of-the-art performance. Corpora annotated at fine-grained levels (opinion- or aspect-level) require more effort from annotators, but are able to capture information which is not present at document- or sentence-level, such as nested opinions or differing polarities of different aspects of a single entity. In English, the MPQA corpus (Wiebe et al., 2005) has been widely used in fine-grained opinion research. More recently, a number of Se"
L18-1104,J15-1002,0,0.0230103,"aspect-level, under-resourced, opinion mining, cross-lingual 1. 2. Introduction Sentiment analysis has become an established field with a number of subfields (aspect-level sentiment analysis, social media sentiment analysis, cross-lingual sentiment analysis), all of which require some kind of annotated resource, either to train a machine-learning based classifier or to test the performance of proposed approaches. Although much research into multi-lingual and crosslingual sentiment analysis has focused on unsupervised or semi-supervised approaches (A.R. et al., 2012; Perez-Rosas et al., 2012; Gao et al., 2015), these techniques still require certain resources (linked wordnets, seed lexicon) and do not generally reach the performance of supervised approaches. In English the state-of-the-art for binary sentiment analysis often reaches nearly 90 percent accuracy (Tai et al., 2015; Kim, 2014; Irsoy and Cardie, 2014), but for other languages there is a marked drop in accuracy. This is mainly due to the lack of annotations and resources in these languages. This is especially true of corpora annotated at aspect-level. Unlike document- or tweet-level annotation, aspect-level annotation requires a large amo"
L18-1104,D14-1181,0,0.00513756,"of annotated resource, either to train a machine-learning based classifier or to test the performance of proposed approaches. Although much research into multi-lingual and crosslingual sentiment analysis has focused on unsupervised or semi-supervised approaches (A.R. et al., 2012; Perez-Rosas et al., 2012; Gao et al., 2015), these techniques still require certain resources (linked wordnets, seed lexicon) and do not generally reach the performance of supervised approaches. In English the state-of-the-art for binary sentiment analysis often reaches nearly 90 percent accuracy (Tai et al., 2015; Kim, 2014; Irsoy and Cardie, 2014), but for other languages there is a marked drop in accuracy. This is mainly due to the lack of annotations and resources in these languages. This is especially true of corpora annotated at aspect-level. Unlike document- or tweet-level annotation, aspect-level annotation requires a large amount of effort from the annotators, which further reduces the likelihood of finding an aspect-level sentiment corpus in under-resourced languages. We are, however, aware of one corpus annotated for aspects in German (Klinger and Cimiano, 2014), although German is not a particularly l"
L18-1104,klinger-cimiano-2014-usage,0,0.0533261,"ten reaches nearly 90 percent accuracy (Tai et al., 2015; Kim, 2014; Irsoy and Cardie, 2014), but for other languages there is a marked drop in accuracy. This is mainly due to the lack of annotations and resources in these languages. This is especially true of corpora annotated at aspect-level. Unlike document- or tweet-level annotation, aspect-level annotation requires a large amount of effort from the annotators, which further reduces the likelihood of finding an aspect-level sentiment corpus in under-resourced languages. We are, however, aware of one corpus annotated for aspects in German (Klinger and Cimiano, 2014), although German is not a particularly low-resource language. The movement towards multi-lingual datasets for sentiment analysis is important because many languages offer different challenges, such as complex morphology or highly productive word formation, which can not be overcome by focusing only on English data. The novelty of this work lies in creating corpora which cover both Basque and Catalan languages and are annotated in such a way that they are compatible with similarly compiled corpora available in a number of languages (Agerri et al., 2013). This allows for further research into c"
L18-1104,S13-2052,0,0.0528281,"corpora available in a number of languages (Agerri et al., 2013). This allows for further research into cross-lingual sentiment analysis, as well as introducing the first resource for aspect-level sentiment analysis in Catalan and Basque. The corpus is available at http: //hdl.handle.net/10230/33928 or https:// jbarnesspain.github.io/resources/. Related Work In English there are many datasets available for documentand sentence-level sentiment analysis across different domains and at different levels of annotation (Pang et al., 2002; Hu and Liu, 2004; Blitzer et al., 2007; Socher et al., 2013; Nakov et al., 2013). These resources have been built up over a period of more than a decade and are currently necessary to achieve state-of-the-art performance. Corpora annotated at fine-grained levels (opinion- or aspect-level) require more effort from annotators, but are able to capture information which is not present at document- or sentence-level, such as nested opinions or differing polarities of different aspects of a single entity. In English, the MPQA corpus (Wiebe et al., 2005) has been widely used in fine-grained opinion research. More recently, a number of SemEval tasks have concentrated on aspect-le"
L18-1104,W02-1011,0,0.0236232,"nd are annotated in such a way that they are compatible with similarly compiled corpora available in a number of languages (Agerri et al., 2013). This allows for further research into cross-lingual sentiment analysis, as well as introducing the first resource for aspect-level sentiment analysis in Catalan and Basque. The corpus is available at http: //hdl.handle.net/10230/33928 or https:// jbarnesspain.github.io/resources/. Related Work In English there are many datasets available for documentand sentence-level sentiment analysis across different domains and at different levels of annotation (Pang et al., 2002; Hu and Liu, 2004; Blitzer et al., 2007; Socher et al., 2013; Nakov et al., 2013). These resources have been built up over a period of more than a decade and are currently necessary to achieve state-of-the-art performance. Corpora annotated at fine-grained levels (opinion- or aspect-level) require more effort from annotators, but are able to capture information which is not present at document- or sentence-level, such as nested opinions or differing polarities of different aspects of a single entity. In English, the MPQA corpus (Wiebe et al., 2005) has been widely used in fine-grained opinion"
L18-1104,perez-rosas-etal-2012-learning,0,0.0831273,"Missing"
L18-1104,S14-2004,0,0.177625,"Missing"
L18-1104,S15-2082,0,0.04348,"riod of more than a decade and are currently necessary to achieve state-of-the-art performance. Corpora annotated at fine-grained levels (opinion- or aspect-level) require more effort from annotators, but are able to capture information which is not present at document- or sentence-level, such as nested opinions or differing polarities of different aspects of a single entity. In English, the MPQA corpus (Wiebe et al., 2005) has been widely used in fine-grained opinion research. More recently, a number of SemEval tasks have concentrated on aspect-level sentiment analysis (Pontiki et al., 2014; Pontiki et al., 2015; Pontiki et al., 2016). The Iberian peninsula contains two official languages (Portuguese and Spanish), as well as three co-official languages (Basque, Catalan, and Galician) and several smaller languages (Aragonese, Gascon). The two official languages do have available resources for sentiment at tweet-level (Villena-Rom´an et al., 2013; Arruda et al., 2015), as well as at aspect-level (Agerri et al., 2013; Villena-Rom´an et al., 2015; Almeida et al., 2015). The co-official languages, however, have almost none. The authors are aware of a small discourse-related sentiment corpus available in B"
L18-1104,S16-1002,0,0.0817144,"Missing"
L18-1104,D13-1170,0,0.0112903,"h similarly compiled corpora available in a number of languages (Agerri et al., 2013). This allows for further research into cross-lingual sentiment analysis, as well as introducing the first resource for aspect-level sentiment analysis in Catalan and Basque. The corpus is available at http: //hdl.handle.net/10230/33928 or https:// jbarnesspain.github.io/resources/. Related Work In English there are many datasets available for documentand sentence-level sentiment analysis across different domains and at different levels of annotation (Pang et al., 2002; Hu and Liu, 2004; Blitzer et al., 2007; Socher et al., 2013; Nakov et al., 2013). These resources have been built up over a period of more than a decade and are currently necessary to achieve state-of-the-art performance. Corpora annotated at fine-grained levels (opinion- or aspect-level) require more effort from annotators, but are able to capture information which is not present at document- or sentence-level, such as nested opinions or differing polarities of different aspects of a single entity. In English, the MPQA corpus (Wiebe et al., 2005) has been widely used in fine-grained opinion research. More recently, a number of SemEval tasks have conc"
L18-1104,P15-1150,0,0.0633325,"require some kind of annotated resource, either to train a machine-learning based classifier or to test the performance of proposed approaches. Although much research into multi-lingual and crosslingual sentiment analysis has focused on unsupervised or semi-supervised approaches (A.R. et al., 2012; Perez-Rosas et al., 2012; Gao et al., 2015), these techniques still require certain resources (linked wordnets, seed lexicon) and do not generally reach the performance of supervised approaches. In English the state-of-the-art for binary sentiment analysis often reaches nearly 90 percent accuracy (Tai et al., 2015; Kim, 2014; Irsoy and Cardie, 2014), but for other languages there is a marked drop in accuracy. This is mainly due to the lack of annotations and resources in these languages. This is especially true of corpora annotated at aspect-level. Unlike document- or tweet-level annotation, aspect-level annotation requires a large amount of effort from the annotators, which further reduces the likelihood of finding an aspect-level sentiment corpus in under-resourced languages. We are, however, aware of one corpus annotated for aspects in German (Klinger and Cimiano, 2014), although German is not a par"
L18-1104,agerri-etal-2014-ixa,0,0.156089,"Missing"
lambert-etal-2012-automatic,W10-1716,1,\N,Missing
lambert-etal-2012-automatic,J93-2003,0,\N,Missing
lambert-etal-2012-automatic,2008.iwslt-papers.6,1,\N,Missing
lambert-etal-2012-automatic,P07-2045,0,\N,Missing
lambert-etal-2012-automatic,W07-0733,0,\N,Missing
lambert-etal-2012-automatic,P03-1021,0,\N,Missing
lambert-etal-2012-automatic,W09-0439,0,\N,Missing
lambert-rodriguez-penagos-2014-adapting,W06-2920,0,\N,Missing
lambert-rodriguez-penagos-2014-adapting,W06-2922,0,\N,Missing
lambert-rodriguez-penagos-2014-adapting,branco-silva-2004-evaluating,0,\N,Missing
lambert-rodriguez-penagos-2014-adapting,barreto-etal-2006-open,0,\N,Missing
lambert-rodriguez-penagos-2014-adapting,padro-stanilovsky-2012-freeling,0,\N,Missing
lambert-rodriguez-penagos-2014-adapting,afonso-etal-2002-floresta,0,\N,Missing
N07-2022,P06-1002,0,0.0591237,"systems use a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney, 2002). Within this 1 Hereinafter, alignment will refer to word alignment, unless otherwise stated. A more general difficulty, however, is that of finding an alignment evaluation metric favoring alignments which benefit Machine Translation. The fact that the required alignment characteristics depend on each particular system makes it even more difficult. It seems that high precision alignments are better for phrase-based SMT (Chen and Federico, 2006; Ayan and Dorr, 2006), whereas high recall alignments are more suited to N-gram SMT (Mari˜no et al., 2006). In this context, alignment quality improvements does not necessarily imply translation quality improvements. This is in agreement with the observation of a poor correlation between word alignment error rate (AER (Och and Ney, 2000)) and automatic translation evaluation metrics (Ittycheriah and Roukos, 2005; Vilar et al., 2006). 85 Proceedings of NAACL HLT 2007, Companion Volume, pages 85–88, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics Recently some alignment evaluation metrics"
N07-2022,J93-2003,0,0.0320352,"Missing"
N07-2022,N06-1014,0,0.0312338,"ot tuned in function of the translation task, but only indirectly. In this paper, we propose a novel framework for discriminative training of alignment models with automated translation metrics as maximization criterion. In this approach, alignments are optimized for the translation task. In addition, no link labels at the word level are needed. This framework is evaluated in terms of automatic translation evaluation metrics, and an improvement of translation quality is observed. 1 Tuning alignment for an MT system is subject to practical difficulties. Unsupervised systems (Och and Ney, 2003; Liang et al., 2006) are based on generative models trained with the EM algorithm. They require large computational resources, and incorporating new features is difficult. In contrast, adding new features to some supervised systems (Liu et al., 2005; Moore, 2005; Ittycheriah and Roukos, 2005) is easy, but the need of annotated data is a problem. Introduction In the first SMT systems (Brown et al., 1993), word alignment was introduced as a hidden variable of the translation model. When word-based translation models have been replaced by phrase-based models (Zens et al., 2002), alignment1 and translation model trai"
N07-2022,P05-1057,0,0.0715665,"roach, alignments are optimized for the translation task. In addition, no link labels at the word level are needed. This framework is evaluated in terms of automatic translation evaluation metrics, and an improvement of translation quality is observed. 1 Tuning alignment for an MT system is subject to practical difficulties. Unsupervised systems (Och and Ney, 2003; Liang et al., 2006) are based on generative models trained with the EM algorithm. They require large computational resources, and incorporating new features is difficult. In contrast, adding new features to some supervised systems (Liu et al., 2005; Moore, 2005; Ittycheriah and Roukos, 2005) is easy, but the need of annotated data is a problem. Introduction In the first SMT systems (Brown et al., 1993), word alignment was introduced as a hidden variable of the translation model. When word-based translation models have been replaced by phrase-based models (Zens et al., 2002), alignment1 and translation model training have become two separated tasks. The system of Brown et al. was based on the noisy channel approach. Present SMT systems use a more general maximum entropy approach in which a log-linear combination of multiple feature funct"
N07-2022,J06-4004,1,0.894597,"Missing"
N07-2022,H05-1011,0,0.215348,"are optimized for the translation task. In addition, no link labels at the word level are needed. This framework is evaluated in terms of automatic translation evaluation metrics, and an improvement of translation quality is observed. 1 Tuning alignment for an MT system is subject to practical difficulties. Unsupervised systems (Och and Ney, 2003; Liang et al., 2006) are based on generative models trained with the EM algorithm. They require large computational resources, and incorporating new features is difficult. In contrast, adding new features to some supervised systems (Liu et al., 2005; Moore, 2005; Ittycheriah and Roukos, 2005) is easy, but the need of annotated data is a problem. Introduction In the first SMT systems (Brown et al., 1993), word alignment was introduced as a hidden variable of the translation model. When word-based translation models have been replaced by phrase-based models (Zens et al., 2002), alignment1 and translation model training have become two separated tasks. The system of Brown et al. was based on the noisy channel approach. Present SMT systems use a more general maximum entropy approach in which a log-linear combination of multiple feature functions is imple"
N07-2022,C00-2163,0,0.0304844,"ion metric favoring alignments which benefit Machine Translation. The fact that the required alignment characteristics depend on each particular system makes it even more difficult. It seems that high precision alignments are better for phrase-based SMT (Chen and Federico, 2006; Ayan and Dorr, 2006), whereas high recall alignments are more suited to N-gram SMT (Mari˜no et al., 2006). In this context, alignment quality improvements does not necessarily imply translation quality improvements. This is in agreement with the observation of a poor correlation between word alignment error rate (AER (Och and Ney, 2000)) and automatic translation evaluation metrics (Ittycheriah and Roukos, 2005; Vilar et al., 2006). 85 Proceedings of NAACL HLT 2007, Companion Volume, pages 85–88, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics Recently some alignment evaluation metrics have been proposed which are more informative when the alignments are used to extract translation units (Fraser and Marcu, 2006; Ayan and Dorr, 2006). However, these metrics assess translation quality very indirectly. In this paper, we propose a novel framework for discriminative training of alignment models with au"
N07-2022,P02-1038,0,0.0396225,"eriah and Roukos, 2005) is easy, but the need of annotated data is a problem. Introduction In the first SMT systems (Brown et al., 1993), word alignment was introduced as a hidden variable of the translation model. When word-based translation models have been replaced by phrase-based models (Zens et al., 2002), alignment1 and translation model training have become two separated tasks. The system of Brown et al. was based on the noisy channel approach. Present SMT systems use a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney, 2002). Within this 1 Hereinafter, alignment will refer to word alignment, unless otherwise stated. A more general difficulty, however, is that of finding an alignment evaluation metric favoring alignments which benefit Machine Translation. The fact that the required alignment characteristics depend on each particular system makes it even more difficult. It seems that high precision alignments are better for phrase-based SMT (Chen and Federico, 2006; Ayan and Dorr, 2006), whereas high recall alignments are more suited to N-gram SMT (Mari˜no et al., 2006). In this context, alignment quality improveme"
N07-2022,J03-1002,0,0.0134264,"el parameters are not tuned in function of the translation task, but only indirectly. In this paper, we propose a novel framework for discriminative training of alignment models with automated translation metrics as maximization criterion. In this approach, alignments are optimized for the translation task. In addition, no link labels at the word level are needed. This framework is evaluated in terms of automatic translation evaluation metrics, and an improvement of translation quality is observed. 1 Tuning alignment for an MT system is subject to practical difficulties. Unsupervised systems (Och and Ney, 2003; Liang et al., 2006) are based on generative models trained with the EM algorithm. They require large computational resources, and incorporating new features is difficult. In contrast, adding new features to some supervised systems (Liu et al., 2005; Moore, 2005; Ittycheriah and Roukos, 2005) is easy, but the need of annotated data is a problem. Introduction In the first SMT systems (Brown et al., 1993), word alignment was introduced as a hidden variable of the translation model. When word-based translation models have been replaced by phrase-based models (Zens et al., 2002), alignment1 and t"
N07-2022,P03-1021,0,0.0892164,"Missing"
N07-2022,P06-1097,0,0.0225194,"ignment quality improvements does not necessarily imply translation quality improvements. This is in agreement with the observation of a poor correlation between word alignment error rate (AER (Och and Ney, 2000)) and automatic translation evaluation metrics (Ittycheriah and Roukos, 2005; Vilar et al., 2006). 85 Proceedings of NAACL HLT 2007, Companion Volume, pages 85–88, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics Recently some alignment evaluation metrics have been proposed which are more informative when the alignments are used to extract translation units (Fraser and Marcu, 2006; Ayan and Dorr, 2006). However, these metrics assess translation quality very indirectly. In this paper, we propose a novel framework for discriminative training of alignment models with automated translation metrics as maximization criterion. Thus we just need a reference aligned at the sentence level instead of link labels at the word level. The paper is structured as follows. Section 2 explains the models used in our word aligner, focusing on the features designed to account for the specificities of the SMT system. In section 3, our minimum error training procedure is described and experim"
N07-2022,H91-1026,0,0.185519,"d because word positions s2 and s3 are embedded between links s1-t1 and s4-t1. Thus the link s4-t1 may introduces data sparseness in the translation model, although it may be a correct link. So we want to have a feature which counts the number of embedded word positions in an alignment. Figure 1: Word positions embedded in a tuple. In addition to the embedded word position feature, we used the same two distortion features as Moore to penalize reorderings in the alignment (one sums the number of crossing links, and the other one sums the amplitude of crossing links). We also used the φ2 score (Gale and Church, 1991) as a word association model, and as a POS-tags association model. 3 Experimental Work For these experiments we used the ChineseEnglish data provided for IWSLT’06 evaluation campaign (Paul, 2006). The training set contains 46000 sentences (of 6.7 and 7.0 average length). Parameters were tuned over the development set (dev4) provided, consisting of 489 sentences of 11.2 words in average, with 7 references. Our test set was a selection of 500 sentences (of 6 words in average, with 16 references) among dev1, dev2 and dev3 sets. 3.1 Optimization Procedure Once the alignment models were computed, a"
N07-2022,H05-1012,0,0.141103,"for the translation task. In addition, no link labels at the word level are needed. This framework is evaluated in terms of automatic translation evaluation metrics, and an improvement of translation quality is observed. 1 Tuning alignment for an MT system is subject to practical difficulties. Unsupervised systems (Och and Ney, 2003; Liang et al., 2006) are based on generative models trained with the EM algorithm. They require large computational resources, and incorporating new features is difficult. In contrast, adding new features to some supervised systems (Liu et al., 2005; Moore, 2005; Ittycheriah and Roukos, 2005) is easy, but the need of annotated data is a problem. Introduction In the first SMT systems (Brown et al., 1993), word alignment was introduced as a hidden variable of the translation model. When word-based translation models have been replaced by phrase-based models (Zens et al., 2002), alignment1 and translation model training have become two separated tasks. The system of Brown et al. was based on the noisy channel approach. Present SMT systems use a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney, 2002). Wit"
N07-2022,2006.iwslt-papers.7,0,0.200467,"gnment characteristics depend on each particular system makes it even more difficult. It seems that high precision alignments are better for phrase-based SMT (Chen and Federico, 2006; Ayan and Dorr, 2006), whereas high recall alignments are more suited to N-gram SMT (Mari˜no et al., 2006). In this context, alignment quality improvements does not necessarily imply translation quality improvements. This is in agreement with the observation of a poor correlation between word alignment error rate (AER (Och and Ney, 2000)) and automatic translation evaluation metrics (Ittycheriah and Roukos, 2005; Vilar et al., 2006). 85 Proceedings of NAACL HLT 2007, Companion Volume, pages 85–88, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics Recently some alignment evaluation metrics have been proposed which are more informative when the alignments are used to extract translation units (Fraser and Marcu, 2006; Ayan and Dorr, 2006). However, these metrics assess translation quality very indirectly. In this paper, we propose a novel framework for discriminative training of alignment models with automated translation metrics as maximization criterion. Thus we just need a reference aligned at t"
N07-2022,2006.iwslt-papers.5,1,0.800234,"TM). A baseline SMT system, consisting of MARIE decoder and this translation model as unique feature2 , was used to produce a translation (OUT) of the development source set. Then, translation quality over the development set is maximized by iteratively varying the set of coefficients. The optimization procedure was performed by using the SPSA algorithm (Spall, 1992). SPSA is a stochastic implementation of the conjugate gradient method which requires only two evaluations of the objective function. It was observed to be more robust than the Downhill Simplex method when tuning SMT coefficients (Lambert and Banchs, 2006). Each function evaluation required to align the training corpus and build a new translation model. The algorithm converged after about 80 evaluations, lasting each 17 minutes with a 3 GHz processor. Alignment decoding was performed with a beam of 10 (it took 50 seconds and required 8 MB memory). Finally, the corpus was aligned with the optimum set of coefficients, and a full SMT system was build, with a target language model (trained on the provided training data), a word bonus model and two lexical models. SMT models weights were optimized with a standard Minimum Error Training (MET) strateg"
N07-2022,2002.tmi-tutorials.2,0,0.015191,"upervised systems (Och and Ney, 2003; Liang et al., 2006) are based on generative models trained with the EM algorithm. They require large computational resources, and incorporating new features is difficult. In contrast, adding new features to some supervised systems (Liu et al., 2005; Moore, 2005; Ittycheriah and Roukos, 2005) is easy, but the need of annotated data is a problem. Introduction In the first SMT systems (Brown et al., 1993), word alignment was introduced as a hidden variable of the translation model. When word-based translation models have been replaced by phrase-based models (Zens et al., 2002), alignment1 and translation model training have become two separated tasks. The system of Brown et al. was based on the noisy channel approach. Present SMT systems use a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney, 2002). Within this 1 Hereinafter, alignment will refer to word alignment, unless otherwise stated. A more general difficulty, however, is that of finding an alignment evaluation metric favoring alignments which benefit Machine Translation. The fact that the required alignment characteristics depen"
N07-2022,2006.iwslt-evaluation.1,0,\N,Missing
P15-2128,banea-etal-2008-bootstrapping,0,0.0307359,"fer approach, a source sentiment lexicon is transferred into the target language and a lexicon-based classifier is build in the target language. Approaches to transfer lexica include machine translation (MT) (Mihalcea et al., 2007), Wordnet (Banea et al., 2011; Hassan et al., 2011; Perez-Rosas et al., 2012), relations between dictionaries represented in graphs (Scheible et al., 2010), or triangulation (Steinberger et al., 2012). The corpus transfer approach consists of transferring a source training corpus into the target language and building a corpus-based classifier in the target language. Banea et al. (2008) follow this approach, translating an annotated corpus via MT. Balamurali et al. (2012) use linked Wordnets to T EST Proj T OUTLT EST Use case II only makes sense for aspect-level analysis,2 and to our knowledge, it was not addressed in the literature so far. Use case III. We want to benefit from data available in several languages, either to have more examples and improve the classifier accuracy, or to have a broader view of the opinions under study. In this paper we focus on use cases I and II. TRAINL0 TEST SALT EST Figure 2: Use case II settings. T EST SA (2) T OUTLT RAIN Figure 1: Use case"
P15-2128,C12-2008,0,0.0810862,"nd a lexicon-based classifier is build in the target language. Approaches to transfer lexica include machine translation (MT) (Mihalcea et al., 2007), Wordnet (Banea et al., 2011; Hassan et al., 2011; Perez-Rosas et al., 2012), relations between dictionaries represented in graphs (Scheible et al., 2010), or triangulation (Steinberger et al., 2012). The corpus transfer approach consists of transferring a source training corpus into the target language and building a corpus-based classifier in the target language. Banea et al. (2008) follow this approach, translating an annotated corpus via MT. Balamurali et al. (2012) use linked Wordnets to T EST Proj T OUTLT EST Use case II only makes sense for aspect-level analysis,2 and to our knowledge, it was not addressed in the literature so far. Use case III. We want to benefit from data available in several languages, either to have more examples and improve the classifier accuracy, or to have a broader view of the opinions under study. In this paper we focus on use cases I and II. TRAINL0 TEST SALT EST Figure 2: Use case II settings. T EST SA (2) T OUTLT RAIN Figure 1: Use case I settings. SA refers to Sentiment Analisys, T to Translation, Proj to Projection and"
P15-2128,C10-1004,0,0.0658224,"P. replace words in training and test corpora by their (language-independent) synset identifiers. Gui et al. (2014) reduce negative transfer in the process of transfer learning. Popat et al. (2013) perform CLSA with clusters as features, bridging target and source language clusters with word alignment. In the test translation approach, test sentences from the target language are translated into the source language and they are classified using a source language classifier (Bautin et al., 2008). Work on joint classification includes training a classifier with features from multilingual views (Banea et al., 2010; Xiao and Guo, 2012), co-training (Wan, 2009; Demirtas and Pechenizkiy, 2013), joint learning (Lu et al., 2011), structural correspondence learning (Wei and Pal, 2010; Prettenhofer and Stein, 2010) or mixture models (Meng et al., 2012). Gui et al. (2013) compare several of these approaches. Brooke et al. (2009) and Balamurali et al. (2013) conclude that at document level, it is cheaper to annotate resources in the target language than building CLSA systems. This may not be true at aspect level, in which the annotation cost is much higher. In any case, when the skills to build such annotated r"
P15-2128,R09-1010,0,0.0886302,"test translation approach, test sentences from the target language are translated into the source language and they are classified using a source language classifier (Bautin et al., 2008). Work on joint classification includes training a classifier with features from multilingual views (Banea et al., 2010; Xiao and Guo, 2012), co-training (Wan, 2009; Demirtas and Pechenizkiy, 2013), joint learning (Lu et al., 2011), structural correspondence learning (Wei and Pal, 2010; Prettenhofer and Stein, 2010) or mixture models (Meng et al., 2012). Gui et al. (2013) compare several of these approaches. Brooke et al. (2009) and Balamurali et al. (2013) conclude that at document level, it is cheaper to annotate resources in the target language than building CLSA systems. This may not be true at aspect level, in which the annotation cost is much higher. In any case, when the skills to build such annotated resources are lacking, CLSA may be the only option. In language pairs in which no high-quality MT systems are available, MT may not be an appropriate transfer method (Popat et al., 2013; Balamurali et al., 2012). However, Balahur and Turchi (2014) conclude that MT systems can be used to build sentiment analysis s"
P15-2128,P11-1033,0,0.0431248,"4) reduce negative transfer in the process of transfer learning. Popat et al. (2013) perform CLSA with clusters as features, bridging target and source language clusters with word alignment. In the test translation approach, test sentences from the target language are translated into the source language and they are classified using a source language classifier (Bautin et al., 2008). Work on joint classification includes training a classifier with features from multilingual views (Banea et al., 2010; Xiao and Guo, 2012), co-training (Wan, 2009; Demirtas and Pechenizkiy, 2013), joint learning (Lu et al., 2011), structural correspondence learning (Wei and Pal, 2010; Prettenhofer and Stein, 2010) or mixture models (Meng et al., 2012). Gui et al. (2013) compare several of these approaches. Brooke et al. (2009) and Balamurali et al. (2013) conclude that at document level, it is cheaper to annotate resources in the target language than building CLSA systems. This may not be true at aspect level, in which the annotation cost is much higher. In any case, when the skills to build such annotated resources are lacking, CLSA may be the only option. In language pairs in which no high-quality MT systems are ava"
P15-2128,P12-1060,0,0.020655,"s, bridging target and source language clusters with word alignment. In the test translation approach, test sentences from the target language are translated into the source language and they are classified using a source language classifier (Bautin et al., 2008). Work on joint classification includes training a classifier with features from multilingual views (Banea et al., 2010; Xiao and Guo, 2012), co-training (Wan, 2009; Demirtas and Pechenizkiy, 2013), joint learning (Lu et al., 2011), structural correspondence learning (Wei and Pal, 2010; Prettenhofer and Stein, 2010) or mixture models (Meng et al., 2012). Gui et al. (2013) compare several of these approaches. Brooke et al. (2009) and Balamurali et al. (2013) conclude that at document level, it is cheaper to annotate resources in the target language than building CLSA systems. This may not be true at aspect level, in which the annotation cost is much higher. In any case, when the skills to build such annotated resources are lacking, CLSA may be the only option. In language pairs in which no high-quality MT systems are available, MT may not be an appropriate transfer method (Popat et al., 2013; Balamurali et al., 2012). However, Balahur and Tur"
P15-2128,P14-2139,0,0.0147829,"reordering constraints are implemented with the zone and wall tags, as indicated in Figure 3. Moses also allows mark-up to be directly passed to the translation, via the x tag. We use this functionality to keep track, via the tags &lt;ou[id][-label]> and &lt;/ou[id]>, of the segment boundaries (ou stands for Opinionated Unit), of the opinionated segment identifier ([id]) and, for training and evaluation purposes, of the polarity label ([-label]). In the example of Figure 3, the id is 1 and the label is P. replace words in training and test corpora by their (language-independent) synset identifiers. Gui et al. (2014) reduce negative transfer in the process of transfer learning. Popat et al. (2013) perform CLSA with clusters as features, bridging target and source language clusters with word alignment. In the test translation approach, test sentences from the target language are translated into the source language and they are classified using a source language classifier (Bautin et al., 2008). Work on joint classification includes training a classifier with features from multilingual views (Banea et al., 2010; Xiao and Guo, 2012), co-training (Wan, 2009; Demirtas and Pechenizkiy, 2013), joint learning (Lu"
P15-2128,P07-1123,0,0.0899354,"the source language by projecting the labels or/and opinionated units onto the test set. T TRAIN (a) TEST 0 L T EST Learn OUTLT EST (b) T TESTL0 T RAIN 3 OUTL0 SALT RAIN TESTL0 T OUTL0 OU T SALOU T OU T OUT Related Work The main CLSC approaches described in the literature are via lexicon transfer, via corpus transfer, via test translation and via joint classification. In the lexicon transfer approach, a source sentiment lexicon is transferred into the target language and a lexicon-based classifier is build in the target language. Approaches to transfer lexica include machine translation (MT) (Mihalcea et al., 2007), Wordnet (Banea et al., 2011; Hassan et al., 2011; Perez-Rosas et al., 2012), relations between dictionaries represented in graphs (Scheible et al., 2010), or triangulation (Steinberger et al., 2012). The corpus transfer approach consists of transferring a source training corpus into the target language and building a corpus-based classifier in the target language. Banea et al. (2008) follow this approach, translating an annotated corpus via MT. Balamurali et al. (2012) use linked Wordnets to T EST Proj T OUTLT EST Use case II only makes sense for aspect-level analysis,2 and to our knowledge,"
P15-2128,N15-1071,0,0.0527572,"ills to build such annotated resources are lacking, CLSA may be the only option. In language pairs in which no high-quality MT systems are available, MT may not be an appropriate transfer method (Popat et al., 2013; Balamurali et al., 2012). However, Balahur and Turchi (2014) conclude that MT systems can be used to build sentiment analysis systems that can obtain comparable performances to the one obtained for English. All this work was performed at sentence or document level. Zhou et al. (2012) and Lin et al. (2014) work at the aspect level, but they focus on cross-lingual aspect extraction. Haas and Versley (2015) use CLSA for individual syntactic nodes, however they need to map target-language and source-language nodes with word alignment. 4 5 CLSA experiments In order to compare CLSA settings a and b (of use case I), we needed data with opinion annotations at the aspect level, in two different languages and in the same domain. We used the OpeNER4 opinion corpus,5 and more specifically the opinion expression and polarity label annotations of the hotel review component, in Spanish and English. We split the data in training (train) and evaluation (test) sets as indicated in Table 1. The SMT system was t"
P15-2128,P10-2041,0,0.0116927,"3.4 3.5 1.7 2.6 23.4 4.4 Table 2: Size of the available and selected corpora (in million words) in English (EN) and Spanish (ES) used to train the SMT system. able data from the 2013 workshop on Statistical Machine Translation6 (WMT 2013). We also crawled monolingual data in the hotel booking domain, from booking.com and TripAdvisor.com. From these in-domain data we extracted 100k and 50k word corpora, respectively for data selection and language model (LM) interpolation tuning. We selected the data closest to the domain in the English-Spanish parallel corpora via a crossentropy-based method (Moore and Lewis, 2010), using the open source XenC tool (Rousseau, 2013). The size of available and selected corpora are indicated in the first 4 rows of Table 2. The LM was an interpolation of LMs trained with the target part of the parallel corpora and with the rest of the Booking and Trip Advisor data (last 2 rows of Table 2). We used Moses Experiment Management System (Koehn, 2010) with all default options to build the SMT system.7 Because the common crawl corpus contained English sentences in the Spanish side, we applied an LM-based filter to select only sentence pairs in which the Spanish side was better scor"
P15-2128,perez-rosas-etal-2012-learning,0,0.0244275,"the test set. T TRAIN (a) TEST 0 L T EST Learn OUTLT EST (b) T TESTL0 T RAIN 3 OUTL0 SALT RAIN TESTL0 T OUTL0 OU T SALOU T OU T OUT Related Work The main CLSC approaches described in the literature are via lexicon transfer, via corpus transfer, via test translation and via joint classification. In the lexicon transfer approach, a source sentiment lexicon is transferred into the target language and a lexicon-based classifier is build in the target language. Approaches to transfer lexica include machine translation (MT) (Mihalcea et al., 2007), Wordnet (Banea et al., 2011; Hassan et al., 2011; Perez-Rosas et al., 2012), relations between dictionaries represented in graphs (Scheible et al., 2010), or triangulation (Steinberger et al., 2012). The corpus transfer approach consists of transferring a source training corpus into the target language and building a corpus-based classifier in the target language. Banea et al. (2008) follow this approach, translating an annotated corpus via MT. Balamurali et al. (2012) use linked Wordnets to T EST Proj T OUTLT EST Use case II only makes sense for aspect-level analysis,2 and to our knowledge, it was not addressed in the literature so far. Use case III. We want to bene"
P15-2128,P11-2104,0,0.0202016,"pinionated units onto the test set. T TRAIN (a) TEST 0 L T EST Learn OUTLT EST (b) T TESTL0 T RAIN 3 OUTL0 SALT RAIN TESTL0 T OUTL0 OU T SALOU T OU T OUT Related Work The main CLSC approaches described in the literature are via lexicon transfer, via corpus transfer, via test translation and via joint classification. In the lexicon transfer approach, a source sentiment lexicon is transferred into the target language and a lexicon-based classifier is build in the target language. Approaches to transfer lexica include machine translation (MT) (Mihalcea et al., 2007), Wordnet (Banea et al., 2011; Hassan et al., 2011; Perez-Rosas et al., 2012), relations between dictionaries represented in graphs (Scheible et al., 2010), or triangulation (Steinberger et al., 2012). The corpus transfer approach consists of transferring a source training corpus into the target language and building a corpus-based classifier in the target language. Banea et al. (2008) follow this approach, translating an annotated corpus via MT. Balamurali et al. (2012) use linked Wordnets to T EST Proj T OUTLT EST Use case II only makes sense for aspect-level analysis,2 and to our knowledge, it was not addressed in the literature so far. Us"
P15-2128,P13-1041,0,0.0147065,"in Figure 3. Moses also allows mark-up to be directly passed to the translation, via the x tag. We use this functionality to keep track, via the tags &lt;ou[id][-label]> and &lt;/ou[id]>, of the segment boundaries (ou stands for Opinionated Unit), of the opinionated segment identifier ([id]) and, for training and evaluation purposes, of the polarity label ([-label]). In the example of Figure 3, the id is 1 and the label is P. replace words in training and test corpora by their (language-independent) synset identifiers. Gui et al. (2014) reduce negative transfer in the process of transfer learning. Popat et al. (2013) perform CLSA with clusters as features, bridging target and source language clusters with word alignment. In the test translation approach, test sentences from the target language are translated into the source language and they are classified using a source language classifier (Bautin et al., 2008). Work on joint classification includes training a classifier with features from multilingual views (Banea et al., 2010; Xiao and Guo, 2012), co-training (Wan, 2009; Demirtas and Pechenizkiy, 2013), joint learning (Lu et al., 2011), structural correspondence learning (Wei and Pal, 2010; Prettenhofe"
P15-2128,P07-2045,0,0.00317172,"applications, would then be lost. Furthermore, the translation quality would be worse than when the segments are translated within the whole sentence context. To solve these problems, we translate the whole sentences but with reordering constraints ensuring that the opinionated segments are preserved during translation. That is, the text between the relevant segment boundaries is not reordered nor mixed with the text outside these boundaries.3 Thus the text in the target language segment comes only from the corresponding source language segment. We use the Moses statistical MT (SMT) toolkit (Koehn et al., 2007) to perform the translation. In Moses, these reordering constraints are implemented with the zone and wall tags, as indicated in Figure 3. Moses also allows mark-up to be directly passed to the translation, via the x tag. We use this functionality to keep track, via the tags &lt;ou[id][-label]> and &lt;/ou[id]>, of the segment boundaries (ou stands for Opinionated Unit), of the opinionated segment identifier ([id]) and, for training and evaluation purposes, of the polarity label ([-label]). In the example of Figure 3, the id is 1 and the label is P. replace words in training and test corpora by thei"
P15-2128,C10-2127,0,0.0541744,"Missing"
P15-2128,P09-1027,0,0.130128,"r (language-independent) synset identifiers. Gui et al. (2014) reduce negative transfer in the process of transfer learning. Popat et al. (2013) perform CLSA with clusters as features, bridging target and source language clusters with word alignment. In the test translation approach, test sentences from the target language are translated into the source language and they are classified using a source language classifier (Bautin et al., 2008). Work on joint classification includes training a classifier with features from multilingual views (Banea et al., 2010; Xiao and Guo, 2012), co-training (Wan, 2009; Demirtas and Pechenizkiy, 2013), joint learning (Lu et al., 2011), structural correspondence learning (Wei and Pal, 2010; Prettenhofer and Stein, 2010) or mixture models (Meng et al., 2012). Gui et al. (2013) compare several of these approaches. Brooke et al. (2009) and Balamurali et al. (2013) conclude that at document level, it is cheaper to annotate resources in the target language than building CLSA systems. This may not be true at aspect level, in which the annotation cost is much higher. In any case, when the skills to build such annotated resources are lacking, CLSA may be the only op"
P15-2128,P10-2048,0,0.0947348,"learning. Popat et al. (2013) perform CLSA with clusters as features, bridging target and source language clusters with word alignment. In the test translation approach, test sentences from the target language are translated into the source language and they are classified using a source language classifier (Bautin et al., 2008). Work on joint classification includes training a classifier with features from multilingual views (Banea et al., 2010; Xiao and Guo, 2012), co-training (Wan, 2009; Demirtas and Pechenizkiy, 2013), joint learning (Lu et al., 2011), structural correspondence learning (Wei and Pal, 2010; Prettenhofer and Stein, 2010) or mixture models (Meng et al., 2012). Gui et al. (2013) compare several of these approaches. Brooke et al. (2009) and Balamurali et al. (2013) conclude that at document level, it is cheaper to annotate resources in the target language than building CLSA systems. This may not be true at aspect level, in which the annotation cost is much higher. In any case, when the skills to build such annotated resources are lacking, CLSA may be the only option. In language pairs in which no high-quality MT systems are available, MT may not be an appropriate transfer method (P"
P15-2128,C12-1174,0,0.0143869,"training and test corpora by their (language-independent) synset identifiers. Gui et al. (2014) reduce negative transfer in the process of transfer learning. Popat et al. (2013) perform CLSA with clusters as features, bridging target and source language clusters with word alignment. In the test translation approach, test sentences from the target language are translated into the source language and they are classified using a source language classifier (Bautin et al., 2008). Work on joint classification includes training a classifier with features from multilingual views (Banea et al., 2010; Xiao and Guo, 2012), co-training (Wan, 2009; Demirtas and Pechenizkiy, 2013), joint learning (Lu et al., 2011), structural correspondence learning (Wei and Pal, 2010; Prettenhofer and Stein, 2010) or mixture models (Meng et al., 2012). Gui et al. (2013) compare several of these approaches. Brooke et al. (2009) and Balamurali et al. (2013) conclude that at document level, it is cheaper to annotate resources in the target language than building CLSA systems. This may not be true at aspect level, in which the annotation cost is much higher. In any case, when the skills to build such annotated resources are lacking,"
P15-2128,W11-1704,0,\N,Missing
P15-2128,P10-1114,0,\N,Missing
P19-2035,C18-1096,0,0.0200748,"ormation for judging the polarity; and that by adding lexical features, it is possible to reduce this effect. Following this idea, we also experimented reducing the over-fitting effect by introducing an attention regularizer. Unlike previously mentioned ideas, we want the attention weights to be less sparse. Details of our approach are in following sections. sign higher weights to more relevant words given different aspects. Following this idea, a number of researches have been carried out to keep improving the attention network for ABSA (Ma et al., 2017; Tay et al., 2017; Cheng et al., 2017; He et al., 2018; Zhu and Qian, 2018). On the other hand, a lot of works have been done focusing on leveraging existing linguistic resources such as sentiment to enhance the performance; however, most works are performed at document and sentence level. For instance, at document level, Teng et al. (2016) proposed a weighted-sum model which consists of representing the final prediction as a weighted sum of the network prediction and the polarities provided by the lexicon. Zou et al. (2018) described a framework to assign higher weights to opinion words found in the lexicon by transforming lexicon polarity to se"
P19-2035,P82-1020,0,0.819166,"Missing"
P19-2035,W02-1011,0,0.0254019,"effect of regularizing attention vectors to allow the network to have a broader “focus” on different parts of the sentence. The experimental results demonstrate the effectiveness of our approach. 1 Toni Badia Universitat Pompeu Fabra toni.badia@upf.edu Introduction Sentiment analysis (also called opinion mining) has been one of the most active fields in NLP due to its important value to business and society. It is the field of study that tries to extract opinion (positive, neutral, negative) expressed in natural languages. Most sentiment analysis works have been carried out at document level (Pang et al., 2002; Turney, 2002) and sentence level (Wilson et al., 2004), but as opinion expressed by words is highly context dependent, one word may express opposite sentiment under different circumstances. Thus aspect-level sentiment analysis (ABSA) was proposed to address this problem. It finds the polarity of an opinion associated with a certain aspect, such as food, ambiance, service, or price in a restaurant domain. Although deep neural networks yield significant improvement across a variety of tasks compared to 2 Related works ABSA is a fine-grained task which requires the model to be able to produce a"
P19-2035,P17-1154,0,0.0387564,"Missing"
P19-2035,W17-5220,0,0.120049,"account the aspect embeddings. Thus the network is able to assign higher weights to more relevant parts of a given sentence with respect to a specific aspect. Formally, given a sentence S, let {w1 , w2 , ..., wN } be the word vectors of each word where N is the length of the sentence; va ∈ Rda represents the aspect embeddings where da is its dimension; let H ∈ Rd×N be a matrix of the hidden states {h1 , h2 , ..., hN ∈ Rd } produced by LSTM where d is the number of neurons of the LSTM cell. Thus the attention vector α is computed as follows:   Wh H M = tanh( ) Wv va ⊗ e N At sentence level, Shin et al. (2017) used two convolutional neural networks to separately process sentence and lexicon inputs. Lei et al. (2018) described a multi-head attention network where the attention weights are jointly learned with lexicon inputs. Wu et al. (2018) proposed a new labeling strategy which breaks a sentence into clauses by punctuation to produce more lower-level examples, inputs are then processed at different levels with linguistic information such as lexicon and POS, and finally merged back to perform sentence level prediction. Meanwhile, some other similar works that incorporate linguistic resources for se"
P19-2035,D16-1169,0,0.231887,"Missing"
P19-2035,P02-1053,0,0.0309681,"ing attention vectors to allow the network to have a broader “focus” on different parts of the sentence. The experimental results demonstrate the effectiveness of our approach. 1 Toni Badia Universitat Pompeu Fabra toni.badia@upf.edu Introduction Sentiment analysis (also called opinion mining) has been one of the most active fields in NLP due to its important value to business and society. It is the field of study that tries to extract opinion (positive, neutral, negative) expressed in natural languages. Most sentiment analysis works have been carried out at document level (Pang et al., 2002; Turney, 2002) and sentence level (Wilson et al., 2004), but as opinion expressed by words is highly context dependent, one word may express opposite sentiment under different circumstances. Thus aspect-level sentiment analysis (ABSA) was proposed to address this problem. It finds the polarity of an opinion associated with a certain aspect, such as food, ambiance, service, or price in a restaurant domain. Although deep neural networks yield significant improvement across a variety of tasks compared to 2 Related works ABSA is a fine-grained task which requires the model to be able to produce accurate predict"
P19-2035,D16-1058,0,0.416886,"mstances. Thus aspect-level sentiment analysis (ABSA) was proposed to address this problem. It finds the polarity of an opinion associated with a certain aspect, such as food, ambiance, service, or price in a restaurant domain. Although deep neural networks yield significant improvement across a variety of tasks compared to 2 Related works ABSA is a fine-grained task which requires the model to be able to produce accurate prediction given different aspects. As it is common that one sentence may contain opposite polarities associated to different aspects at the same time, attention-based LSTM (Wang et al., 2016) was first proposed to allow the network to be able to as253 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 253–259 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics tention vector is likely to over-fit which forces the network to “focus” on particular words while ignoring positions that provide key information for judging the polarity; and that by adding lexical features, it is possible to reduce this effect. Following this idea, we also experimented reducing the over-fitting e"
P19-2035,C18-1074,0,0.0492484,"ave been carried out to keep improving the attention network for ABSA (Ma et al., 2017; Tay et al., 2017; Cheng et al., 2017; He et al., 2018; Zhu and Qian, 2018). On the other hand, a lot of works have been done focusing on leveraging existing linguistic resources such as sentiment to enhance the performance; however, most works are performed at document and sentence level. For instance, at document level, Teng et al. (2016) proposed a weighted-sum model which consists of representing the final prediction as a weighted sum of the network prediction and the polarities provided by the lexicon. Zou et al. (2018) described a framework to assign higher weights to opinion words found in the lexicon by transforming lexicon polarity to sentiment degree. 3 3.1 Methodology Baseline AT-LSTM In our experiments, we replicate AT-LSTM proposed by Wang et al. (2016) as our baseline system. Comparing with a traditional LSTM network (Hochreiter and Schmidhuber, 1997), AT-LSTM is able to learn the attention vector and at the same time to take into account the aspect embeddings. Thus the network is able to assign higher weights to more relevant parts of a given sentence with respect to a specific aspect. Formally, gi"
S13-2080,baccianella-etal-2010-sentiwordnet,0,0.0375953,"itish English. No efforts were made to adapt our linguistic processing modules and dictionaries to this data. Tweets and SMSs were processed with a UIMA1 based pipeline consisting of a set of linguistic and opinion-oriented modules, which includes: Basic linguistic processing: Sentence segmentation, tokenization, POS-tagging, lemmatization. Syntax: Dependency parsing. Lexicon-based annotations: • Basic polarity, distinguishing among: positive, negative, and neutral, as encoded in Wilson et al. (2010). • Polarity strength, using the score for positive and negative polarity in SentiWordnet 3.0 (Baccianella et al., 2010). Each SentiWordNet synset has an associated triplet of numerical scores (positive, negative, and objective) expressing the intensity of positive, negative and objective polarity of the terms it contains. They range from 0.0 to 1.0, and their sum is 1.0 for each synset (Esuli and Sebastiani, 2007). We selected only the synset 1 The different classifiers employed by FBM constructed their vectors from this output to learn global and contextual polarities. 3 Task A: Ensemble System Our system combined Machine Learning and rulebased approaches. The aim was to combine the strengths of each individu"
S13-2080,S13-2052,0,0.148022,"Missing"
S13-2080,J09-3003,0,\N,Missing
W05-0823,N04-1033,0,0.265878,"Missing"
W05-0823,2004.iwslt-evaluation.14,1,0.848754,"Missing"
W05-0823,N03-1017,0,0.0598703,"Missing"
W05-0823,P00-1056,0,0.0836903,"e pairs with a word ratio larger than 2.4. As a result of this preprocessing, the number of sentences in each training set was slightly reduced. However, no significant reduction was produced. In the case of French, a re-tokenizing procedure was performed in which all apostrophes appearing alone were attached to their corresponding words. For example, pairs of tokens such as l ’ and qu ’ were reduced to single tokens such as l’ and qu’. 134 Once the training data was preprocessed, a wordto-word alignment was performed in both directions, source-to-target and target-to-source, by using GIZA++ (Och and Ney, 2000). As an approximation to the most probable alignment, the Viterbi alignment was considered. Then, the intersection and union of alignment sets in both directions were computed for each training set. 3.2 Feature Function Computation The considered translation system implements a total of five feature functions. The first of these models is the tuple 3-gram model, which was already described in section 2. Tuples for the translation model were extracted from the union set of alignments as shown in Figure 1. Once tuples had been extracted, the tuple vocabulary was pruned by using histogram pruning"
W05-0823,P02-1038,0,0.0900962,"were generated by using a statistical machine translation system which implements a log-linear combination of feature functions along with a bilingual n-gram translation model. 2 Bilingual N-gram Translation Model 1 Introduction During the last decade, statistical machine translation (SMT) systems have evolved from the original word-based approach (Brown et al., 1993) into phrase-based translation systems (Koehn et al., 2003). Similarly, the noisy channel approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple models is implemented (Och and Ney, 2002). The SMT approach used in this work implements a log-linear combination of feature functions along with a translation model which is based on bilingual n-grams. This translation model was developed by de Gispert and Mari˜no (2002), and it differs from the well known phrase-based translation model in two basic issues: first, training data is monotonously segmented into bilingual units; and second, the model considers n-gram probabilities instead of relative frequencies. This model is described in section 2. Translation results from the four source languages made available for the shared task ("
W05-0823,P02-1040,0,0.0765659,"ts a beam-search strategy based on dynamic programming and takes into account all the five feature functions described above simultaneously. It also allows for three different pruning methods: threshold pruning, histogram pruning, and hypothesis recombination. For all the results presented in this work the decoder’s monotonic search modality was used. An optimization tool, which is based on a simplex method (Press et al., 2002), was developed and used for computing log-linear weights for each of the feature functions described above. This algorithm adjusts the log-linear weights so that BLEU (Papineni et al., 2002) is maximized over a given development set. One optimization for each language pair was performed by using the 2000-sentence development sets made available for the shared task. 4 Shared Task Results Table 2 presents the BLEU scores obtained for the shared task test data. Each test set consisted of 2000 sentences. The computed BLEU scores were case insensitive and used one translation reference. 135 sufficient and that , in the future , it is necessary to develop the Union better and a different structure... It is evident from these translation outputs that translation quality decreases when m"
W05-0823,J93-2003,0,\N,Missing
W06-2402,N03-1017,0,0.0300367,"valuations are performed by using real-life data, namely the European Parliament corpus. Results from translation tasks from English-to-Spanish and from Spanish-to-English are presented and discussed. 1 Introduction Statistical machine translation (SMT) was originally focused on word to word translation and was based on the noisy channel approach (Brown et al., 1993). Present SMT systems have evolved from the original ones in such a way that mainly differ from them in two issues: first, word-based translation models have been replaced by phrasebased translation models (Zens et al., 2002) and (Koehn et al., 2003); and second, the noisy channel approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney, 2002). Nevertheless, it is interesting to call the attention about one important fact. Despite the change from a word-based to a phrase-based translation approach, word to word approaches for inferring alignment models from bilingual data (Vogel et al., 1996; Och and Ney, 2003) continue to be widely used. On the other hand, from observing bilingual data sets, it becomes evident that in some cases it 1"
W06-2402,2005.mtsummit-posters.11,1,0.647276,"feature functions involved which can actually help the SMT system to get the right translation. However these ideas motivate for exploring alternatives for using multi-word expression information in order to improve alignment quality and consequently translation accuracy. In this sense, our idea of a multi-word expression (hereafter MWE) refers in principle to word sequences which cannot be translated literally word-to-word. However, the automatic technique studied in this work for extracting and identifying MWEs does not necessarily follow this definition rigorously. In a preliminary study (Lambert and Banchs, 2005), we presented a technique for extracting bilingual multi-word expressions (BMWE) from parallel corpora. In that study, BMWEs identified in a small corpus2 were grouped as a unique toThis paper studies a strategy for identifying and using multi-word expressions in Statistical Machine Translation. The performance of the proposed strategy for various types of multi-word expressions (like nouns or verbs) is evaluated in terms of alignment quality as well as translation accuracy. Evaluations are performed by using real-life data, namely the European Parliament corpus. Results from translation task"
W06-2402,2005.mtsummit-papers.36,1,0.827507,"Missing"
W06-2402,W02-2001,0,0.0279132,"e, alignment results strongly depends on corpus statistics. 2 VERBMOBIL (Arranz et al., 2003) 9 • no word inside the tuple is aligned to words outside the tuple, and • no smaller tuples can be extracted without violating the previous constraints. ken before training alignment models. As a result, both alignment quality and translation accuracy were slightly improved. In this paper we applied the same BMWE extraction technique, with various improvements, to a large corpus (EPPS, described in section 4.1). Since this is a statistical technique, and frequencies of multi-word expressions are low (Baldwin and Villavicencio, 2002), the size of the corpus is an important factor. A few very basic rules based on part-of-speech have also been added to filter out noisy entries in the dictionary. Finally, BMWEs have been classified into three categories (nouns, verbs and others). In addition to the impact of the whole set, the impact of each category has been evaluated separately. The technique will be explained in section 3, after presenting the baseline translation system used (section 2). Experimental results are presented in section 4. Finally some conclusions are presented and further work in this area is depicted. 2 As"
W06-2402,P02-1038,0,0.0133702,"1 Introduction Statistical machine translation (SMT) was originally focused on word to word translation and was based on the noisy channel approach (Brown et al., 1993). Present SMT systems have evolved from the original ones in such a way that mainly differ from them in two issues: first, word-based translation models have been replaced by phrasebased translation models (Zens et al., 2002) and (Koehn et al., 2003); and second, the noisy channel approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney, 2002). Nevertheless, it is interesting to call the attention about one important fact. Despite the change from a word-based to a phrase-based translation approach, word to word approaches for inferring alignment models from bilingual data (Vogel et al., 1996; Och and Ney, 2003) continue to be widely used. On the other hand, from observing bilingual data sets, it becomes evident that in some cases it 1 Of course, alignment results strongly depends on corpus statistics. 2 VERBMOBIL (Arranz et al., 2003) 9 • no word inside the tuple is aligned to words outside the tuple, and • no smaller tuples can be"
W06-2402,A00-1031,0,0.0297219,"ure 2 would be the following: “lo siento ; esto es verdad – I ’m sorry , this is true”. In order to increase the recall, BMWE detection was insensitive to the case of the first letter of each multi-word. The detection engine also allows a search based on lemmas. Two strategies are possible. In the first one, search is first carried out with full forms, so that lemmas are resorted to only if no match is found with full forms. In the second strategy, only lemmas are considered. • Its source or target side ends with an indefinite determiner English data have been POS-tagged using the TnT tagger (Brants, 2000), after the lemmas have been extracted with wnmorph, included in the Wordnet package (Miller et al., 1991). POS-tagging for Spanish has been performed using the FreeLing analysis tool (Carreras et al., 2004). Finally, the BMWE set has been divided in three subsets, according to the following criteria, applied in this order: • If source AND target sides of a BMWE contain at least a verb, it is assigned to the “verb” class. 3.4 • If source AND target sides of a BMWE contain at least a noun, it is assigned to the “noun” class. The modified training corpus, with identified BMWEs grouped in a uniqu"
W06-2402,J03-1002,0,0.027423,"wo issues: first, word-based translation models have been replaced by phrasebased translation models (Zens et al., 2002) and (Koehn et al., 2003); and second, the noisy channel approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney, 2002). Nevertheless, it is interesting to call the attention about one important fact. Despite the change from a word-based to a phrase-based translation approach, word to word approaches for inferring alignment models from bilingual data (Vogel et al., 1996; Och and Ney, 2003) continue to be widely used. On the other hand, from observing bilingual data sets, it becomes evident that in some cases it 1 Of course, alignment results strongly depends on corpus statistics. 2 VERBMOBIL (Arranz et al., 2003) 9 • no word inside the tuple is aligned to words outside the tuple, and • no smaller tuples can be extracted without violating the previous constraints. ken before training alignment models. As a result, both alignment quality and translation accuracy were slightly improved. In this paper we applied the same BMWE extraction technique, with various improvements, to a la"
W06-2402,J93-2003,0,0.0101771,"using multi-word expressions in Statistical Machine Translation. The performance of the proposed strategy for various types of multi-word expressions (like nouns or verbs) is evaluated in terms of alignment quality as well as translation accuracy. Evaluations are performed by using real-life data, namely the European Parliament corpus. Results from translation tasks from English-to-Spanish and from Spanish-to-English are presented and discussed. 1 Introduction Statistical machine translation (SMT) was originally focused on word to word translation and was based on the noisy channel approach (Brown et al., 1993). Present SMT systems have evolved from the original ones in such a way that mainly differ from them in two issues: first, word-based translation models have been replaced by phrasebased translation models (Zens et al., 2002) and (Koehn et al., 2003); and second, the noisy channel approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney, 2002). Nevertheless, it is interesting to call the attention about one important fact. Despite the change from a word-based to a phrase-based translation a"
W06-2402,J04-4002,0,0.0143237,"the alignment (and not in asymmetries only), so most BP are not BMWEs but word sequences that can be decomposed and translated word to word. 3.2 Lexical and Morpho-syntactic Filters In English and Spanish, a list of stop words3 (respectively 19 and 26) was established. The BMWE dictionary was also processed by a PartOf-Speech (POS) tagger and eight rules were written to filter out noisy entries. These rules depend on the tag set used. Examples of criteria to reject a BMWE include: 3.1.2 Scoring Based on Bilingual Phrases Here we refer to Bilingual Phrase (BP) as the bilingual phrases used by Och and Ney (2004). The BP are pairs of word groups which are supposed to be the translation of each other. The set of BP is consistent with the alignment and consists of all phrase pairs in which all words within the target language are only aligned to the words of the source language and vice versa. At least one word of the target language phrase has to be aligned with at least one word of the source language phrase. Finally, the algorithm takes into ac• Its source or target side only contains stop words • Its source or target side ends with a coordination conjunction 3 frequently occurring, semantically insi"
W06-2402,carreras-etal-2004-freeling,0,0.028016,"Missing"
W06-2402,2001.mtsummit-papers.68,0,0.0685027,"Missing"
W06-2402,2005.iwslt-1.23,0,0.0323502,"Missing"
W06-2402,C96-2141,0,0.0459094,"iffer from them in two issues: first, word-based translation models have been replaced by phrasebased translation models (Zens et al., 2002) and (Koehn et al., 2003); and second, the noisy channel approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney, 2002). Nevertheless, it is interesting to call the attention about one important fact. Despite the change from a word-based to a phrase-based translation approach, word to word approaches for inferring alignment models from bilingual data (Vogel et al., 1996; Och and Ney, 2003) continue to be widely used. On the other hand, from observing bilingual data sets, it becomes evident that in some cases it 1 Of course, alignment results strongly depends on corpus statistics. 2 VERBMOBIL (Arranz et al., 2003) 9 • no word inside the tuple is aligned to words outside the tuple, and • no smaller tuples can be extracted without violating the previous constraints. ken before training alignment models. As a result, both alignment quality and translation accuracy were slightly improved. In this paper we applied the same BMWE extraction technique, with various i"
W06-2402,2002.tmi-tutorials.2,0,0.0310025,"translation accuracy. Evaluations are performed by using real-life data, namely the European Parliament corpus. Results from translation tasks from English-to-Spanish and from Spanish-to-English are presented and discussed. 1 Introduction Statistical machine translation (SMT) was originally focused on word to word translation and was based on the noisy channel approach (Brown et al., 1993). Present SMT systems have evolved from the original ones in such a way that mainly differ from them in two issues: first, word-based translation models have been replaced by phrasebased translation models (Zens et al., 2002) and (Koehn et al., 2003); and second, the noisy channel approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney, 2002). Nevertheless, it is interesting to call the attention about one important fact. Despite the change from a word-based to a phrase-based translation approach, word to word approaches for inferring alignment models from bilingual data (Vogel et al., 1996; Och and Ney, 2003) continue to be widely used. On the other hand, from observing bilingual data sets, it becomes evident"
W06-2402,2004.iwslt-evaluation.14,0,0.0234831,"Missing"
W06-3101,P04-1079,0,0.0149782,"sibilities for improvements. 1 Patrik Lambert† Rafael Banchs† 2 Introduction The evaluation of the generated output is an important issue for all natural language processing (NLP) tasks, especially for machine translation (MT). Automatic evaluation is preferred because human evaluation is a time consuming and expensive task. Related Work There is a number of publications dealing with various automatic evaluation measures for machine translation output, some of them proposing new measures, some proposing improvements and extensions of the existing ones (Doddington, 2002; Papineni et al., 2002; Babych and Hartley, 2004; Matusov et al., 2005). Semi-automatic evaluation measures have been also investigated, for example in (Nießen et al., 2000). An automatic metric which uses base forms and synonyms of the words in order to correlate better to human judgements has been 1 Proceedings of the Workshop on Statistical Machine Translation, pages 1–6, c New York City, June 2006. 2006 Association for Computational Linguistics proposed in (Banerjee and Lavie, 2005). However, error analysis is still a rather unexplored area. A framework for human error analysis and error classification has been proposed in (Vilar et al."
W06-3101,W05-0909,0,0.0366343,"lation output, some of them proposing new measures, some proposing improvements and extensions of the existing ones (Doddington, 2002; Papineni et al., 2002; Babych and Hartley, 2004; Matusov et al., 2005). Semi-automatic evaluation measures have been also investigated, for example in (Nießen et al., 2000). An automatic metric which uses base forms and synonyms of the words in order to correlate better to human judgements has been 1 Proceedings of the Workshop on Statistical Machine Translation, pages 1–6, c New York City, June 2006. 2006 Association for Computational Linguistics proposed in (Banerjee and Lavie, 2005). However, error analysis is still a rather unexplored area. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out. Automatic methods for error analysis to our knowledge have not been studied yet. Many publications propose the use of morphosyntactic information for improving the performance of a statistical machine translation system. Various methods for treating morphological and syntactical differences between German and English are investigated in (Nießen and Ney, 2000; Nie"
W06-3101,N04-4015,0,0.00900173,"has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out. Automatic methods for error analysis to our knowledge have not been studied yet. Many publications propose the use of morphosyntactic information for improving the performance of a statistical machine translation system. Various methods for treating morphological and syntactical differences between German and English are investigated in (Nießen and Ney, 2000; Nießen and Ney, 2001a; Nießen and Ney, 2001b). Morphological analysis has been used for improving Arabic-English translation (Lee, 2004), for SerbianEnglish translation (Popovi´c et al., 2005) as well as for Czech-English translation (Goldwater and McClosky, 2005). Inflectional morphology of Spanish verbs is dealt with in (Popovi´c and Ney, 2004; de Gispert et al., 2005). To the best of our knowledge, the use of morpho-syntactic information for error analysis of translation output has not been investigated so far. 3 Morpho-syntactic Information and Automatic Evaluation We propose the use of morpho-syntactic information in combination with the automatic evaluation measures WER and PER in order to get more details about the tran"
W06-3101,2005.iwslt-1.19,1,0.729357,"s. 1 Patrik Lambert† Rafael Banchs† 2 Introduction The evaluation of the generated output is an important issue for all natural language processing (NLP) tasks, especially for machine translation (MT). Automatic evaluation is preferred because human evaluation is a time consuming and expensive task. Related Work There is a number of publications dealing with various automatic evaluation measures for machine translation output, some of them proposing new measures, some proposing improvements and extensions of the existing ones (Doddington, 2002; Papineni et al., 2002; Babych and Hartley, 2004; Matusov et al., 2005). Semi-automatic evaluation measures have been also investigated, for example in (Nießen et al., 2000). An automatic metric which uses base forms and synonyms of the words in order to correlate better to human judgements has been 1 Proceedings of the Workshop on Statistical Machine Translation, pages 1–6, c New York City, June 2006. 2006 Association for Computational Linguistics proposed in (Banerjee and Lavie, 2005). However, error analysis is still a rather unexplored area. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006) and a detailed"
W06-3101,C00-2162,1,0.81769,"Banerjee and Lavie, 2005). However, error analysis is still a rather unexplored area. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out. Automatic methods for error analysis to our knowledge have not been studied yet. Many publications propose the use of morphosyntactic information for improving the performance of a statistical machine translation system. Various methods for treating morphological and syntactical differences between German and English are investigated in (Nießen and Ney, 2000; Nießen and Ney, 2001a; Nießen and Ney, 2001b). Morphological analysis has been used for improving Arabic-English translation (Lee, 2004), for SerbianEnglish translation (Popovi´c et al., 2005) as well as for Czech-English translation (Goldwater and McClosky, 2005). Inflectional morphology of Spanish verbs is dealt with in (Popovi´c and Ney, 2004; de Gispert et al., 2005). To the best of our knowledge, the use of morpho-syntactic information for error analysis of translation output has not been investigated so far. 3 Morpho-syntactic Information and Automatic Evaluation We propose the use of"
W06-3101,2001.mtsummit-papers.45,1,0.834325,"05). However, error analysis is still a rather unexplored area. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out. Automatic methods for error analysis to our knowledge have not been studied yet. Many publications propose the use of morphosyntactic information for improving the performance of a statistical machine translation system. Various methods for treating morphological and syntactical differences between German and English are investigated in (Nießen and Ney, 2000; Nießen and Ney, 2001a; Nießen and Ney, 2001b). Morphological analysis has been used for improving Arabic-English translation (Lee, 2004), for SerbianEnglish translation (Popovi´c et al., 2005) as well as for Czech-English translation (Goldwater and McClosky, 2005). Inflectional morphology of Spanish verbs is dealt with in (Popovi´c and Ney, 2004; de Gispert et al., 2005). To the best of our knowledge, the use of morpho-syntactic information for error analysis of translation output has not been investigated so far. 3 Morpho-syntactic Information and Automatic Evaluation We propose the use of morpho-syntactic infor"
W06-3101,W01-1407,1,0.840883,"05). However, error analysis is still a rather unexplored area. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out. Automatic methods for error analysis to our knowledge have not been studied yet. Many publications propose the use of morphosyntactic information for improving the performance of a statistical machine translation system. Various methods for treating morphological and syntactical differences between German and English are investigated in (Nießen and Ney, 2000; Nießen and Ney, 2001a; Nießen and Ney, 2001b). Morphological analysis has been used for improving Arabic-English translation (Lee, 2004), for SerbianEnglish translation (Popovi´c et al., 2005) as well as for Czech-English translation (Goldwater and McClosky, 2005). Inflectional morphology of Spanish verbs is dealt with in (Popovi´c and Ney, 2004; de Gispert et al., 2005). To the best of our knowledge, the use of morpho-syntactic information for error analysis of translation output has not been investigated so far. 3 Morpho-syntactic Information and Automatic Evaluation We propose the use of morpho-syntactic infor"
W06-3101,niessen-etal-2000-evaluation,1,0.397729,"nt issue for all natural language processing (NLP) tasks, especially for machine translation (MT). Automatic evaluation is preferred because human evaluation is a time consuming and expensive task. Related Work There is a number of publications dealing with various automatic evaluation measures for machine translation output, some of them proposing new measures, some proposing improvements and extensions of the existing ones (Doddington, 2002; Papineni et al., 2002; Babych and Hartley, 2004; Matusov et al., 2005). Semi-automatic evaluation measures have been also investigated, for example in (Nießen et al., 2000). An automatic metric which uses base forms and synonyms of the words in order to correlate better to human judgements has been 1 Proceedings of the Workshop on Statistical Machine Translation, pages 1–6, c New York City, June 2006. 2006 Association for Computational Linguistics proposed in (Banerjee and Lavie, 2005). However, error analysis is still a rather unexplored area. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out. Automatic methods for error analysis to our kno"
W06-3101,P02-1040,0,0.115841,"t`ecnica de Catalunya (UPC), Barcelona, Spain ⊥ ITC-irst, Centro per la Ricerca Scientifica e Tecnologica, Trento, Italy {popovic,ney}@informatik.rwth-aachen.de {gupta,federico}@itc.it {agispert,canton}@gps.tsc.upc.es {lambert,banchs}@gps.tsc.upc.es Abstract A variety of automatic evaluation measures have been proposed and studied over the last years, some of them are shown to be a very useful tool for comparing different systems as well as for evaluating improvements within one system. The most widely used are Word Error Rate (WER), Position Independent Word Error Rate (PER), the BLEU score (Papineni et al., 2002) and the NIST score (Doddington, 2002). However, none of these measures give any details about the nature of translation errors. A relationship between these error measures and the actual errors in the translation outputs is not easy to find. Therefore some analysis of the translation errors is necessary in order to define the main problems and to focus the research efforts. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006), but like human evaluation, this is also a time consuming task. The goal of this work is to present a framework for au"
W06-3101,popovic-ney-2004-towards,1,0.340303,"Missing"
W06-3101,popovic-ney-2006-pos,1,0.738548,"Missing"
W06-3101,2005.mtsummit-papers.34,1,0.721067,"173 0.15 0.09 2.7 1.7 840 1094 22774 26917 4081 3958 0.14 0.25 2.8 2.6 Table 1: Corpus statistics for the Spanish-English EPPS task (running words include punctuation marks) corpus). The statistics of the corpora can be seen in Table 1. The statistical machine translation system used in this work is based on a log-linear combination of seven different models. The most important ones are phrase based models in both directions, additionally IBM1 models at the phrase level in both directions as well as phrase and length penalty are used. A more detailed description of the system can be found in (Vilar et al., 2005; Zens et al., 2005). 4.3 Experiments The translation experiments have been done in both translation directions on both sizes of the corpus. In order to examine improvements of the baseline system, a new system with POS-based word reorderings of nouns and adjectives as proposed in (Popovi´c and Ney, 2006) is also analysed. Adjectives in the Spanish language are usually placed after the corresponding noun, whereas for English it is the other way round. Therefore, local reorderings of nouns and ad3 WER 34.5 33.5 41.8 38.9 PER 25.5 25.2 30.7 29.5 BLEU 54.7 56.4 43.2 48.5 English→Spanish full base"
W06-3101,vilar-etal-2006-error,1,0.542481,"Missing"
W06-3101,2005.iwslt-1.20,1,0.479674,"7 840 1094 22774 26917 4081 3958 0.14 0.25 2.8 2.6 Table 1: Corpus statistics for the Spanish-English EPPS task (running words include punctuation marks) corpus). The statistics of the corpora can be seen in Table 1. The statistical machine translation system used in this work is based on a log-linear combination of seven different models. The most important ones are phrase based models in both directions, additionally IBM1 models at the phrase level in both directions as well as phrase and length penalty are used. A more detailed description of the system can be found in (Vilar et al., 2005; Zens et al., 2005). 4.3 Experiments The translation experiments have been done in both translation directions on both sizes of the corpus. In order to examine improvements of the baseline system, a new system with POS-based word reorderings of nouns and adjectives as proposed in (Popovi´c and Ney, 2006) is also analysed. Adjectives in the Spanish language are usually placed after the corresponding noun, whereas for English it is the other way round. Therefore, local reorderings of nouns and ad3 WER 34.5 33.5 41.8 38.9 PER 25.5 25.2 30.7 29.5 BLEU 54.7 56.4 43.2 48.5 English→Spanish full baseline reorder 13k bas"
W06-3101,H05-1085,0,\N,Missing
W06-3120,A00-1031,0,0.0229195,"ese figures in the optimization function. 3 Shared Task Results 3.1 Data The data provided for this shared task corresponds to a subset of the official transcriptions of the European Parliament Plenary Sessions, and it is available through the shared task website at: http://www.statmt.org/wmt06/shared-task/. The development set used to tune the system consists of a subset (500 first sentences) of the official development set made available for the Shared Task. We carried out a morphological analysis of the data. The English POS-tagging has been carried out using freely available T N T tagger (Brants, 2000). In the Spanish case, we have used the F reeling (Carreras et al., 2004) analysis tool which generates the POS-tagging for each input word. 3.2 Systems configurations The baseline system is the same for all tasks and includes the following features functions: cp, pp, lm, ibm1, ibm1−1 , wb, pb. The POStag target language model has been used in those tasks for which the tagger was available. Table 1 shows the reordering configuration used for each task. The Block Reordering (application 2) has been used when the source language belongs to the Romanic family. The length of the block is limited t"
W06-3120,W05-0827,1,0.879283,"Missing"
W06-3120,2005.iwslt-1.23,1,0.907982,"Missing"
W06-3120,W06-3125,1,0.884485,"Missing"
W06-3120,P05-2012,1,0.889176,"Missing"
W06-3120,W05-0831,0,0.0297232,"Full verb forms The morphology of the verbs usually differs in each language. Therefore, it is interesting to classify the verbs in order to address the rich variety of verbal forms. Each verb is reduced into its base form and reduced POS tag as explained in (de Gispert, 2005). This transformation is only done for the alignment, and its goal is to simplify the work of the word alignment improving its quality. Block reordering (br) The difference in word order between two languages is one of the most significant sources of error in SMT. Related works either deal with reordering in general as (Kanthak et al., 2005) or deal with local reordering as (Tillmann and Ney, 2003). We report a local reordering technique, which is implemented as a preprocessing stage, with two applications: (1) to improve only alignment quality, and (2) to improve alignment quality and to infer reordering in translation. Here, we present a short explanation of the algorithm, for further details see Costa-juss`a and Fonollosa (2006). 142 Proceedings of the Workshop on Statistical Machine Translation, pages 142–145, c New York City, June 2006. 2006 Association for Computational Linguistics of the bilingual phrase, and no word on ei"
W06-3120,W06-3114,0,0.0221223,"ts to observe its efficiency in all the pairs used in this evaluation. The rgraph has been applied in those cases where: we do not use br2 (there is no sense in applying them simultaneously); and we have the tagger for the source language model available. In the case of the pair GeEn, we have not experimented any reordering, we left the application of both reordering approaches as future work. 3.3 Discussion Table 2 presents the BLEU scores evaluated on the test set (using TRUECASE) for each configuration. The official results were slightly better because a lowercase evaluation was used, see (Koehn and Monz, 2006). For both, Es2En and Fr2En tasks, br helps slightly. The improvement of the approach depends on the quality of the alignment. The better alignments allow to extract higher quality Alignment Blocks (Costa-juss`a and Fonollosa, 2006). The En2Es task is improved when adding both br1 and rgraph. Similarly, the En2Fr task seems to perform fairly well when using the rgraph. In this case, the improvement of the approach depends on the quality of the alignment patterns (Crego et al., 2006). However, it has the advantage of delaying the final decision of reordering to the overall search, where all mod"
W06-3120,N03-1017,0,0.00728769,"o infer reordering in translation. Here, we present a short explanation of the algorithm, for further details see Costa-juss`a and Fonollosa (2006). 142 Proceedings of the Workshop on Statistical Machine Translation, pages 142–145, c New York City, June 2006. 2006 Association for Computational Linguistics of the bilingual phrase, and no word on either side of the phrase is aligned to a word out of the phrase. We limit the maximum size of any given phrase to 7. The huge increase in computational and storage cost of including longer phrases does not provide a significant improvement in quality (Koehn et al., 2003) as the probability of reappearance of larger phrases decreases. 2.3 Figure 1: Example of an Alignment Block, i.e. a pair of consecutive blocks whose target translation is swapped This reordering strategy is intended to infer the most probable reordering for sequences of words, which are referred to as blocks, in order to monotonize current data alignments and generalize reordering for unseen pairs of blocks. Given a word alignment, we identify those pairs of consecutive source blocks whose translation is swapped, i.e. those blocks which, if swapped, generate a correct monotone translation. Fi"
W06-3120,J04-4002,0,0.0268059,"created). Based on this information, the source side of the bilingual corpora are reordered. In case of applying the reordering technique for purpose (1), we modify only the source training corpora to realign and then we recover the original order of the training corpora. In case of using Block Reordering for purpose (2), we modify all the source corpora (both training and test), and we use the new training corpora to realign and build the final translation system. 2.2 Phrase Extraction Given a sentence pair and a corresponding word alignment, phrases are extracted following the criterion in Och and Ney (2004). A phrase (or bilingual phrase) is any pair of m source words and n target words that satisfies two basic constraints: words are consecutive along both sides 143 Feature functions Conditional and posterior probability (cp, pp) Given the collected phrase pairs, we estimate the phrase translation probability distribution by relative frequency in both directions. The target language model (lm) consists of an n-gram model, in which the probability of a translation hypothesis is approximated by the product of word n-gram probabilities. As default language model feature, we use a standard word-base"
W06-3120,carreras-etal-2004-freeling,0,\N,Missing
W06-3120,J03-1005,0,\N,Missing
W06-3120,N04-1033,0,\N,Missing
W06-3125,W05-0823,1,0.872672,"Missing"
W06-3125,A00-1031,0,0.117706,"d over the development set for each of the six translation directions considered. 163 This baseline system is actually very similar to the system used for last year’s shared task “Exploiting Parallel Texts for Statistical Machine Translation” of ACL’05 Workshop on Building and Using Parallel Texts: Data-Driven Machine Translation and Beyond (Banchs et al., 2005), whose results are available at: http://www.statmt.org/wpt05/ mt-shared-task/. A more detailed description of the system can be found in (2005). The tools used for POS-tagging were Freeling (Carreras et al., 2004) for Spanish and TnT (Brants, 2000) for English. All language models were estimated using the SRI language modeling toolkit. Word-to-word alignments were extracted with GIZA++. Improvements in word-toword alignments were achieved through verb group classification as described in (de Gispert, 2005). 3 Reordering Framework In this section we outline the reordering framework used for the experiments (Crego and Mari˜no, 2006). A highly constrained reordered search is performed by means of a set of reordering patterns (linguistically motivated rewrite patterns) which are used to extend the monotone search graph with additional arcs."
W06-3125,carreras-etal-2004-freeling,0,0.0548444,"Missing"
W06-3125,W06-3120,1,0.883993,"Missing"
W06-3125,N04-1033,0,0.0842803,"Missing"
W06-3125,P05-2012,1,0.901179,"Missing"
W06-3125,N03-1017,0,0.00542142,"luation with a tagged target language model (using Part-Of-Speech tags). For both Spanish-English translation directions and the English-to-French translation task, the baseline system allows for linguistically motivated sourceside reorderings. 2 Baseline N-gram-based SMT System 1 Introduction The statistical machine translation approach used in this work implements a log-linear combination of feature functions along with a translation model which is based on bilingual n-grams (de Gispert and Mari˜no, 2002). This translation model differs from the well known phrase-based translation approach (Koehn et al., 2003) in two basic issues: first, training data is monotonously segmented into bilingual units; and second, the model considers n-gram probabilities instead of relative frequencies. This translation approach is described in detail in (Mari˜no et al., 2005). For those translation tasks with Spanish or English as target language, an additional tagged (usAs already mentioned, the translation model used here is based on bilingual n-grams. It actually constitutes a language model of bilingual units, referred to as tuples, which approximates the joint probability between source and target languages by us"
W06-3125,2005.mtsummit-papers.36,1,0.909254,"Missing"
W06-3125,J93-2003,0,\N,Missing
W07-0720,W06-3125,1,0.849606,"Missing"
W07-0720,W06-1609,1,0.795127,"Missing"
W07-0720,W06-3114,0,0.0213063,"this system participation in the ACL 2007 SECOND WORK SHOP ON STATISTICAL MACHINE TRANSLA TION . Results on three pairs of languages are reported, namely from Spanish, French and German into English (and the other way round) for both the in-domain and out-of-domain tasks. 2 Baseline N-gram-based SMT System 1 Introduction Based on estimating a joint-probability model between the source and the target languages, Ngram-based SMT has proved to be a very competitive alternatively to phrase-based and other state-of-the-art systems in previous evaluation campaigns, as shown in (Koehn and Monz, 2005; Koehn and Monz, 2006). Given the challenge of domain adaptation, efforts have been focused on improving strategies for Ngram-based SMT which could generalize better. Specifically, a novel reordering strategy is explored. It is based on extending the search by using precomputed statistical information. Results are promising while keeping computational expenses at a similar level as monotonic search. Additionally, a bonus for tuples from the out-of-domain corpus is The translation model is based on bilingual n-grams. It actually constitutes a language model of bilingual units, referred to as tuples, which approximat"
W07-0720,J06-4004,1,0.847357,"Missing"
W07-0720,E99-1010,0,0.731366,"smaller tuples which reduces the translation vocabulary sparseness. These new tuples are used to build the SMT system. 3 Baseline System Enhanced with a Weighted Reordering Input Graph This section briefly describes the statistical machine reordering (SMR) technique. Further details on the architecture of SMR system can be found on (Costa-juss`a and Fonollosa, 2006). 3.1 Concept The SMR system can be seen as a SMT system which translates from an original source language (S) to a reordered source language (S’), given a target language (T). The SMR technique works with statistical word classes (Och, 1999) instead of words themselves (particularly, we have used 200 classes in all experiments). Figure 1: SMR approach in the (A) training step (B) in the test step (the weight of each arch is in brackets). 3.2 Using SMR technique to improve SMT training The original source corpus S is translated into the reordered source corpus S’ with the SMR system. Figure 1 (A) shows the corresponding block diagram. The reordered training source corpus and the original training target corpus are used to build the SMT system. The main difference here is that the training is computed with the S’2T task instead of"
W07-0720,W05-0820,0,\N,Missing
W08-0315,W08-0315,1,0.0512755,"Missing"
W08-0315,J90-2002,0,0.809551,"Missing"
W08-0315,W07-0718,0,0.152941,"Missing"
W08-0315,carreras-etal-2004-freeling,0,0.138533,"Missing"
W08-0315,W06-3114,0,0.151633,"Missing"
W08-0315,P00-1056,0,0.073606,"Missing"
W08-0315,J04-4002,0,0.0735646,"Missing"
W08-0315,W05-0820,0,\N,Missing
W08-0315,A00-1031,0,\N,Missing
W08-0315,J06-4004,1,\N,Missing
W10-1716,E09-1003,1,0.842246,"sulting bitext has no new words in the English side, since all words of the translation output come from the translation model, but it contains new combinations (phrases) of known words, and reinforces the probability of some phrase pairs (Schwenk, 2008). Second, as in last year’s evaluation, we automatically extracted and aligned parallel sentences from comparable in-domain corpora. This year we used the AFP and APW news texts since there are available in the French and English LDC Gigaword corpora. The general architecture of our parallel sentence extraction system is described in detail by Abdul-Rauf and Schwenk (2009). We first translated 91M words from French into English using our first stage SMT system. These English sentences were then used to search for translations in the English AFP and APW texts of the Gigaword corpus using information retrieval techniques. The Lemur toolkit (Ogilvie and Callan, 2.4 Monolingual data Development data All development was done on news-test2008, and newstest2009 was used as internal test set. For all corpora except the French side of the bitexts used to train the French–English system (see above), the default Moses tokenization was used. However, we added abbreviations"
W10-1716,J93-2003,0,0.0276462,"Missing"
W10-1716,W09-0401,0,0.103917,"Missing"
W10-1716,2003.mtsummit-tttt.3,0,0.211376,"reviations for the French tokenizer. All our models are case sensitive and include punctuation. The BLEU scores reported in this paper were calculated with the multi-bleu.perl tool and are case sensitive. The BLEU score was one of metrics with the best correlation with human ratings in last year evaluation (CallisonBurch et al., 2009) for the French–English and English–French directions. 3 Architecture of the SMT system The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . It is today common practice to use phrases as translation units (Koehn et al., 2003; Och and Ney, 2003) and a log linear framework in order to introduce several models explaining the 122 translation process: numbers in parentheses are the standard deviation of these three values. The standard deviation gives a lower bound of the significance of the difference between two systems. If the difference between two average scores is less than the sum of the standard deviations, we can say that this difference is not significant. The reverse is not true. Note that most of the improvements shown in the tables are small and not significant. However many of the gains are cumulative an"
W10-1716,2008.iwslt-papers.6,1,0.866721,"the UN corpus seem to be out-of domain for this task. We used two types of automatically extracted resources to adapt our system to the task domain. First, we generated automatic translations of the French News corpus provided (231M words), and selected the sentences with a normalised translation cost (returned by the decoder) inferior to a threshold. The resulting bitext has no new words in the English side, since all words of the translation output come from the translation model, but it contains new combinations (phrases) of known words, and reinforces the probability of some phrase pairs (Schwenk, 2008). Second, as in last year’s evaluation, we automatically extracted and aligned parallel sentences from comparable in-domain corpora. This year we used the AFP and APW news texts since there are available in the French and English LDC Gigaword corpora. The general architecture of our parallel sentence extraction system is described in detail by Abdul-Rauf and Schwenk (2009). We first translated 91M words from French into English using our first stage SMT system. These English sentences were then used to search for translations in the English AFP and APW texts of the Gigaword corpus using inform"
W10-1716,P07-2045,0,0.0135341,"Missing"
W10-1716,P02-1038,0,0.114464,"erence between two systems. If the difference between two average scores is less than the sum of the standard deviations, we can say that this difference is not significant. The reverse is not true. Note that most of the improvements shown in the tables are small and not significant. However many of the gains are cumulative and the sum of several small gains makes a significant difference. e∗ = arg max p(e|f ) e X = arg max{exp( e λi hi (e, f ))} (1) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). The system is based on the Moses SMT toolkit (Koehn et al., 2007) and constructed as follows. First, word alignments in both directions are calculated. We used a multi-threaded version of the GIZA++ tool (Gao and Vogel, 2008).1 This speeds up the process and corrects an error of GIZA++ that can appear with rare words. Phrases and lexical reorderings are ex"
W10-1716,J03-1002,0,0.00523272,"rench tokenizer. All our models are case sensitive and include punctuation. The BLEU scores reported in this paper were calculated with the multi-bleu.perl tool and are case sensitive. The BLEU score was one of metrics with the best correlation with human ratings in last year evaluation (CallisonBurch et al., 2009) for the French–English and English–French directions. 3 Architecture of the SMT system The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . It is today common practice to use phrases as translation units (Koehn et al., 2003; Och and Ney, 2003) and a log linear framework in order to introduce several models explaining the 122 translation process: numbers in parentheses are the standard deviation of these three values. The standard deviation gives a lower bound of the significance of the difference between two systems. If the difference between two average scores is less than the sum of the standard deviations, we can say that this difference is not significant. The reverse is not true. Note that most of the improvements shown in the tables are small and not significant. However many of the gains are cumulative and the sum of several"
W10-1716,W09-0423,1,0.605151,"have been discarded because they contain many unknown or unfrequent n-grams. The lexical filter was based on the IBM model 1 cost (Brown et al., 1993) of each side of a sentence pair given the other side, normalised with respect to both sentence lengths. This filter Introduction This paper describes the machine translation systems developed by the Computer Science laboratory at the University of Le Mans (LIUM) for the 2010 WMT shared task evaluation. We only considered the translation between French and English (in both directions). The main differences with respect to previous year’s system (Schwenk et al., 2009) are as follows: restriction to the data recommended for the workshop, usage of the (filtered) French–English gigaword bitext, pruning of the phrase table, and usage of automatic translations of the monolingual news corpus to improve the translation model. We also used a larger amount of bilingual data extracted from comparable corpora than was done in 2009. These different points are described in the rest of the paper, together with a summary of the experimental results showing the impact of each component. 121 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and Metri"
W10-1716,D07-1103,0,\N,Missing
W10-1716,N03-1017,0,\N,Missing
W10-1716,W08-0509,0,\N,Missing
W11-2132,2008.iwslt-papers.1,0,0.0203009,"nstance, Wuebker et al. (2010) proposed to translate the training data, using forced alignment and a leave-one-out technique, and to use the induced alignments to extract phrases. They have observed improvements with respect to word alignment obtained by GIZA++. On the other hand, Bertoldi and Federico (2009) adapted an SMT system with automatic translations and trained the translation and reordering models on the word alignment used by moses. They reported a very small drop in performance with respect to training word alignments with GIZA++. Similar ideas were also used in pivot translation. Bertoldi et al. (2008) translated from the pivot language to the source language to create parallel training data for the direct translation. 2.3 Treatment of unknown words Statistical machine translation systems have some trouble dealing with morphologically rich languages. It can happen, in function of the available training data, that translations of words are only Source language French finies effac´es hawaienne ... Source language stemmed form fini effac´e hawaien ... Target language English finished erased Hawaiian ... Table 1: Example of translations from French to English which are automatically extracted f"
W11-2132,P08-2040,0,0.346638,"te to the task and to decrease the probabilities of the other ones. Ideally, we should also add new translations or source phrase, but this seems to be more challenging without any additional parallel data. A common way to modify a statistical model is to use a mixture model and to optimize the coefficients to the adaptation domain. This was investigated in the framework of SMT by several authors, for instance for word alignment (Civera and Juan, 2007), for language modeling (Zhao et al., 2004; Koehn and Schroeder, 2007) and to a lesser extent for the translation model (Foster and Kuhn, 2007; Chen et al., 2008). This mixture approach has the advantage that only few parameters need to be modified, the mixture coefficients. On the other hand, many translation probabilities are modified at once and it is not possible to selectively modify the probabilities of particular phrases. Another direction of research is self-enhancing of the translation model. This was first proposed by Ueffing (2006). The idea is to translate the test data, to filter the translations with help of a confidence score and to use the most reliable ones to train an additional small phrase table that is jointly used with the generic"
W11-2132,W07-0722,0,0.0325184,"distribution of the existing phrases without necessarily modifying the entries. The idea is of course to increase the probabilities of translations that are appropriate to the task and to decrease the probabilities of the other ones. Ideally, we should also add new translations or source phrase, but this seems to be more challenging without any additional parallel data. A common way to modify a statistical model is to use a mixture model and to optimize the coefficients to the adaptation domain. This was investigated in the framework of SMT by several authors, for instance for word alignment (Civera and Juan, 2007), for language modeling (Zhao et al., 2004; Koehn and Schroeder, 2007) and to a lesser extent for the translation model (Foster and Kuhn, 2007; Chen et al., 2008). This mixture approach has the advantage that only few parameters need to be modified, the mixture coefficients. On the other hand, many translation probabilities are modified at once and it is not possible to selectively modify the probabilities of particular phrases. Another direction of research is self-enhancing of the translation model. This was first proposed by Ueffing (2006). The idea is to translate the test data, to filter"
W11-2132,W07-0717,0,0.0237288,"ions that are appropriate to the task and to decrease the probabilities of the other ones. Ideally, we should also add new translations or source phrase, but this seems to be more challenging without any additional parallel data. A common way to modify a statistical model is to use a mixture model and to optimize the coefficients to the adaptation domain. This was investigated in the framework of SMT by several authors, for instance for word alignment (Civera and Juan, 2007), for language modeling (Zhao et al., 2004; Koehn and Schroeder, 2007) and to a lesser extent for the translation model (Foster and Kuhn, 2007; Chen et al., 2008). This mixture approach has the advantage that only few parameters need to be modified, the mixture coefficients. On the other hand, many translation probabilities are modified at once and it is not possible to selectively modify the probabilities of particular phrases. Another direction of research is self-enhancing of the translation model. This was first proposed by Ueffing (2006). The idea is to translate the test data, to filter the translations with help of a confidence score and to use the most reliable ones to train an additional small phrase table that is jointly u"
W11-2132,W08-0509,0,0.0164172,"Table 4: Case sensitive BLEU scores as a function of the amount of parallel training data. (Eparl=Europarl, nc=News Commentary, crawled1/2=sub-sampled crawled bitexts, un=sub-sampled United Nations bitexts). Corpus Bitexts: Europarl News Commentary United Nations Crawled (109 bitexts) Development data: newstest2009 newstest2010 Monolingual data: LDC Gigaword Crawled news English French 50.5M 2.9M 344M 667M 54.4M 3.3M 393M 794M 65k 62k 73k 71k 4.1G 2.6G 920M 612M structed as follows. First, word alignments in both directions are calculated. We used a multi-threaded version of the GIZA++ tool (Gao and Vogel, 2008). Phrases and lexical reorderings are extracted using the default settings of the Moses toolkit. All the bitexts were concatenated. The parameters of Moses are tuned on the development data using the MERT tool. For most of the runs, we performed three optimizations using different starting points and report average results. English and French texts were tokenised using a modified version of the tools of the Moses suite. Punctuation and case were preserved. Table 3: Available training data for the translation between French and English for the translation evaluation at WMT’11 (number of words a"
W11-2132,P08-2015,0,0.0150668,"ons of words are only Source language French finies effac´es hawaienne ... Source language stemmed form fini effac´e hawaien ... Target language English finished erased Hawaiian ... Table 1: Example of translations from French to English which are automatically extracted from the phrase-table with the stemmed form. known in some forms and not in others. For instance, for a user of MT technology it is quite difficult to understand why the system can translate the French word “je pense”1 , but not “tu penses”2 . There have been attempts in the literature to address this problem, for instance by Habash (2008) to deal with the Arabic language. It is actually possible to automatically infer possible translations when translating from a morphologically rich language, to a simpler language. In our case we use this approach to translate from French to English. Several of the unknown words are actually adjectives, nouns or verbs in a particular form that itself is not known, but the phrase table would contain the translation of a different form. As an example we can mention the French adjective finies which is in the female plural form. After stemming we may be able to find the translation in a dictiona"
W11-2132,D07-1103,0,0.0263124,"es of the phrase-table are automatically extracted from sentence aligned parallel data and they are usually quite noisy. It is not uncommon to encounter several hundreds, or even thousands of possible translations of frequent source phrases. Many of these automatically extracted translations are probably wrong and are never used since their probabilities are (fortunately) small in comparison to better translations. Therefore, several approaches were proposed to filter these phrase-tables, reducing considerably their size without any loss of the quality, or even achieving improved performance (Johnson et al., 2007). Given these observations, adaptation of the translation model of PBSMT systems could be performed by modifying the probability distribution of the existing phrases without necessarily modifying the entries. The idea is of course to increase the probabilities of translations that are appropriate to the task and to decrease the probabilities of the other ones. Ideally, we should also add new translations or source phrase, but this seems to be more challenging without any additional parallel data. A common way to modify a statistical model is to use a mixture model and to optimize the coefficie"
W11-2132,W07-0733,0,0.0618445,"the entries. The idea is of course to increase the probabilities of translations that are appropriate to the task and to decrease the probabilities of the other ones. Ideally, we should also add new translations or source phrase, but this seems to be more challenging without any additional parallel data. A common way to modify a statistical model is to use a mixture model and to optimize the coefficients to the adaptation domain. This was investigated in the framework of SMT by several authors, for instance for word alignment (Civera and Juan, 2007), for language modeling (Zhao et al., 2004; Koehn and Schroeder, 2007) and to a lesser extent for the translation model (Foster and Kuhn, 2007; Chen et al., 2008). This mixture approach has the advantage that only few parameters need to be modified, the mixture coefficients. On the other hand, many translation probabilities are modified at once and it is not possible to selectively modify the probabilities of particular phrases. Another direction of research is self-enhancing of the translation model. This was first proposed by Ueffing (2006). The idea is to translate the test data, to filter the translations with help of a confidence score and to use the most r"
W11-2132,P07-2045,0,0.00738019,"rench and English for the translation evaluation at WMT’11 (number of words after tokenisation). in July 2011. Preliminary results of this evaluation are available on the Internet.4 Table 3 summarizes the available training and development data. We optimized our systems on newstest2009 and used newstest2010 as internal test set. For both corpora, only one reference translations is available. Scoring was performed with NIST’s implementation of the BLEU score (‘mt-eval’ version 13). 3.1 Baseline system The baseline system is a standard phrase-based SMT system based on the the Moses SMT toolkit (Koehn et al., 2007). It uses fourteen features functions for translation, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty, and a target language model. It is con4 http://matrix.statmt.org 288 The language models were trained on all the available data, i.e. the target side of the bitexts, the whole Gigaword corpus and the crawled monolingual data. We build 4-gram back-off LMs with the SRI LM toolkit using Modified Kneser-Ney and no cut-off on all the n-grams. Past experience has shown that keeping all n-grams"
W11-2132,W10-1716,1,0.860867,"perience has shown that keeping all n-grams slightly improves the performance although this produces quite huge models (10G and 30G of disk space for French and English respectively). Table 4 gives the baseline results using various amounts of bitexts. Starting with the Europarl and the News Commentary corpora, various amounts of human translated data were added. The organizers of the evaluation provide the so called 109 FrenchEnglish parallel corpus which contains almost 800 million words of data crawled from Canadian and European Internet pages. Following works from the 2010 WMT evaluation (Lambert et al., 2010), we filtered this data using IBM-1 probabilities and language model scores to keep only the most reliable translations. Two subsets were built with 115M and 232M English words respectively (using two differalignment giza reused giza reused moses Dev Test BLEU BLEU TER 27.34 (0.01) 27.40 (0.05) 27.42 (0.02) 29.80 (0.06) 29.82 (0.10) 29.77 (0.06) 55.34 (0.06) 55.30 (0.02) 55.27 (0.03) Table 5: Results for systems trained via different word alignment configurations. The values are the average over 3 MERT runs performed with different seeds. The numbers in parentheses are the standard deviation o"
W11-2132,J03-1002,0,0.0141338,"g as proposed by Ueffing (2006), lightly-supervised training does not adapt itself to the test data, but large amounts of monolingual training data are translated and a completely new model is built. This model can be applied to any test data, including a WEB service. In this paper we propose to extend this approach in several ways. First, we argue that the automatic translations should not be performed from the source to the target language, but in the opposite direction. Second, we propose to use the segmentation obtained during translation instead of performing word alignments with GIZA++ (Och and Ney, 2003) of the automatic translations. Finally, we propose to enrich the vocabulary of the adapted system by detecting untranslated words and automatically inferring possible translations from the stemmed form and the existing translations in the phrase table. This paper is organized as follows. In the next section we first describe our approach in detail. Section 3 describes the considered task, the available resources and the baseline PBSMT system. Results are summarized in section 4 and the paper concludes with a discussion and perspectives of this work. 2 Architecture of the approach In this pape"
W11-2132,2009.mtsummit-posters.17,1,0.691222,"a WEB service, since often the translation of some sentences only is requested. In follow up work, this approach was refined (Ueffing et al., 2007). Domain adaptation was also performed simultaneously for the translation, language and reordering model (Chen et al., 2008). A somehow related approach was named lightlysupervised training (Schwenk, 2008). In that work an SMT system is used to translate large amounts of monolingual texts, to filter them and to add them to the translation model training data. This approach was reported to obtain interesting improvements in the translations quality (Schwenk and Senellart, 2009; Bertoldi and Federico, 2009). In comparison to self enhancing as proposed by Ueffing (2006), lightly-supervised training does not adapt itself to the test data, but large amounts of monolingual training data are translated and a completely new model is built. This model can be applied to any test data, including a WEB service. In this paper we propose to extend this approach in several ways. First, we argue that the automatic translations should not be performed from the source to the target language, but in the opposite direction. Second, we propose to use the segmentation obtained during t"
W11-2132,2008.iwslt-papers.6,1,0.819379,"only feasible when large amounts of test data are collected and processed at once, e.g. a typical evaluation set up with a test set of about 50k words. This method of self-enhancing the translation model seems to be more difficult to apply for on-line SMT, e.g. a WEB service, since often the translation of some sentences only is requested. In follow up work, this approach was refined (Ueffing et al., 2007). Domain adaptation was also performed simultaneously for the translation, language and reordering model (Chen et al., 2008). A somehow related approach was named lightlysupervised training (Schwenk, 2008). In that work an SMT system is used to translate large amounts of monolingual texts, to filter them and to add them to the translation model training data. This approach was reported to obtain interesting improvements in the translations quality (Schwenk and Senellart, 2009; Bertoldi and Federico, 2009). In comparison to self enhancing as proposed by Ueffing (2006), lightly-supervised training does not adapt itself to the test data, but large amounts of monolingual training data are translated and a completely new model is built. This model can be applied to any test data, including a WEB ser"
W11-2132,P07-1004,0,0.0831173,"that is jointly used with the generic phrase table. This could be also seen as a mixture model with the in-domain component being build on-the-fly for each test set. In practice, such 285 an approach is probably only feasible when large amounts of test data are collected and processed at once, e.g. a typical evaluation set up with a test set of about 50k words. This method of self-enhancing the translation model seems to be more difficult to apply for on-line SMT, e.g. a WEB service, since often the translation of some sentences only is requested. In follow up work, this approach was refined (Ueffing et al., 2007). Domain adaptation was also performed simultaneously for the translation, language and reordering model (Chen et al., 2008). A somehow related approach was named lightlysupervised training (Schwenk, 2008). In that work an SMT system is used to translate large amounts of monolingual texts, to filter them and to add them to the translation model training data. This approach was reported to obtain interesting improvements in the translations quality (Schwenk and Senellart, 2009; Bertoldi and Federico, 2009). In comparison to self enhancing as proposed by Ueffing (2006), lightly-supervised traini"
W11-2132,2006.iwslt-papers.3,0,0.283523,"l authors, for instance for word alignment (Civera and Juan, 2007), for language modeling (Zhao et al., 2004; Koehn and Schroeder, 2007) and to a lesser extent for the translation model (Foster and Kuhn, 2007; Chen et al., 2008). This mixture approach has the advantage that only few parameters need to be modified, the mixture coefficients. On the other hand, many translation probabilities are modified at once and it is not possible to selectively modify the probabilities of particular phrases. Another direction of research is self-enhancing of the translation model. This was first proposed by Ueffing (2006). The idea is to translate the test data, to filter the translations with help of a confidence score and to use the most reliable ones to train an additional small phrase table that is jointly used with the generic phrase table. This could be also seen as a mixture model with the in-domain component being build on-the-fly for each test set. In practice, such 285 an approach is probably only feasible when large amounts of test data are collected and processed at once, e.g. a typical evaluation set up with a test set of about 50k words. This method of self-enhancing the translation model seems t"
W11-2132,P10-1049,0,0.0221497,"f millions of words is a very time consuming step. Therefore we propose to use the segmentation into phrases and words obtained implicitly during the translation of the monolingual data with the moses toolkit. These alignments are simply added to the previously calculated alignments of the human translated bitexts and a new phrase table is built. This new procedure does not only speed-up the overall processing, but there are also investigations that these alignments obtained by decoding are more suitable to extract phrases than the symmetrized word alignments produced by GIZA++. For instance, Wuebker et al. (2010) proposed to translate the training data, using forced alignment and a leave-one-out technique, and to use the induced alignments to extract phrases. They have observed improvements with respect to word alignment obtained by GIZA++. On the other hand, Bertoldi and Federico (2009) adapted an SMT system with automatic translations and trained the translation and reordering models on the word alignment used by moses. They reported a very small drop in performance with respect to training word alignments with GIZA++. Similar ideas were also used in pivot translation. Bertoldi et al. (2008) transla"
W11-2132,C04-1059,0,0.024749,"cessarily modifying the entries. The idea is of course to increase the probabilities of translations that are appropriate to the task and to decrease the probabilities of the other ones. Ideally, we should also add new translations or source phrase, but this seems to be more challenging without any additional parallel data. A common way to modify a statistical model is to use a mixture model and to optimize the coefficients to the adaptation domain. This was investigated in the framework of SMT by several authors, for instance for word alignment (Civera and Juan, 2007), for language modeling (Zhao et al., 2004; Koehn and Schroeder, 2007) and to a lesser extent for the translation model (Foster and Kuhn, 2007; Chen et al., 2008). This mixture approach has the advantage that only few parameters need to be modified, the mixture coefficients. On the other hand, many translation probabilities are modified at once and it is not possible to selectively modify the probabilities of particular phrases. Another direction of research is self-enhancing of the translation model. This was first proposed by Ueffing (2006). The idea is to translate the test data, to filter the translations with help of a confidence"
W11-2132,W09-0432,0,\N,Missing
W11-2158,E09-1003,1,0.897701,"Missing"
W11-2158,J93-2003,0,0.0301933,"Missing"
W11-2158,J07-2003,0,0.0504628,"010 was used as internal test set. The default Moses tokenization was used. However, we added abbreviations for the French tokenizer. All our models are case sensitive and include punctuation. The BLEU scores reported in this paper were calculated with the tool multi-bleu.perl and are case sensitive. 3 Architecture of the SMT system The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . Our main system is a phrase-based system (Koehn et al., 2003; Och and Ney, 2003), but we have also performed some experiments with a hierarchical system (Chiang, 2007). Both use a log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) = arg max{exp( e X λi hi (e, f ))} (1) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). The phrase-based system uses fourteen features functions, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). The hierarchical system"
W11-2158,W08-0509,0,0.0820888,"Missing"
W11-2158,P08-2015,0,0.0489218,"Missing"
W11-2158,D07-1103,0,0.0170723,"nal bitext added to Eparl+NC+1092 , we observe no improvement in French to English, and a very small improvement in English to French. However, added to the base468 line system (Eparl+NC+1092 ) adapted with the News data, the IR additional bitexts yield a small (0.2 BLEU) improvement in both translation directions. Final System In both translation directions our best system was the one trained on Eparl+NC+1092 +News+IR. We further achieved small improvements by pruning the phrase-table and by increasing the beam size. To prune the phrase-table, we used the ‘sigtest-filter’ available in Moses (Johnson et al., 2007), more precisely the α −  filter3 . We also build hierarchical systems on the various human translated corpora, using up to 323M words (corpora Eparl+NC+1092 ). The systems yielded similar results than the phrase-based approach, but required much more computational resources, in particular large amounts of main memory to perform the translations. Running the decoder was actually only possible with binarized rule-tables. Therefore, the hierarchical system was not used in the evaluation system. 3 The p-value of two-by-two contingency tables (describing the degree of association between a source"
W11-2158,2003.mtsummit-tttt.3,0,0.0945186,"oved from the Gigaword collections. 2.4 Development data All development was done on newstest2009, and newstest2010 was used as internal test set. The default Moses tokenization was used. However, we added abbreviations for the French tokenizer. All our models are case sensitive and include punctuation. The BLEU scores reported in this paper were calculated with the tool multi-bleu.perl and are case sensitive. 3 Architecture of the SMT system The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . Our main system is a phrase-based system (Koehn et al., 2003; Och and Ney, 2003), but we have also performed some experiments with a hierarchical system (Chiang, 2007). Both use a log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) = arg max{exp( e X λi hi (e, f ))} (1) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). The phrase-based system uses fourteen features functions, namely phrase and lexical translation probabilities in both directions, seven features for the lexicali"
W11-2158,P07-2045,0,0.0249647,"Missing"
W11-2158,W10-1716,1,0.404823,"o performed initial experiments with hierarchical systems. Additional, new features this year include improved translation model adaptation using monolingual data, a continuous space language model and the treatment of unknown words. 1 Resources Used 2.1 Introduction This paper describes the statistical machine translation systems developed by the Computer Science laboratory at the University of Le Mans (LIUM) for the 2011 WMT shared task evaluation. We only considered the translation between French and English (in both directions). The main differences with respect to previous year’s system (Lambert et al., 2010) are as follows: use of more training data as provided by the organizers, improved translation model adaptation by unsupervised training, a continuous space language model for the translation into French, some attempts to automatically induce translations of unknown words and first experiments with hierarchical systems. These different points are described in the rest of the paper, together with a summary of the experimental results showing the impact of each component. Bilingual data Our system was developed in two stages. First, a baseline system was built to generate automatic translations"
W11-2158,W11-2132,1,0.92181,"contains new combinations (phrases) of known words, and reinforces the probability of some phrase pairs (Schwenk, 2008). This year, we improved this method in the following way. In the original approach, the automatic translations are added to the human translated bitexts and a complete new system is build, including time consuming word alignment with GIZA++. For WMT’11, we directly used the word-to-word alignments produced by the decoder at the output instead of GIZA’s alignments. This speeds-up the procedure and yields the same results in our experiments. A detailed comparison is given in (Lambert et al., 2011). Second, as in last year’s evaluation, we automatically extracted and aligned parallel sentences from comparable in-domain corpora. We used the AFP and APW news texts since there are available in the French and English LDC Gigaword corpora. The general architecture of our parallel sentence extraction system is described in detail by Abdul-Rauf and Schwenk (2009). We first translated 91M words from French into English using our first stage SMT system. These English sentences were then used to search for translations in the English AFP and APW texts of the Gigaword corpus using information retr"
W11-2158,P02-1038,0,0.26782,"Missing"
W11-2158,J03-1002,0,0.0108264,"rd collections. 2.4 Development data All development was done on newstest2009, and newstest2010 was used as internal test set. The default Moses tokenization was used. However, we added abbreviations for the French tokenizer. All our models are case sensitive and include punctuation. The BLEU scores reported in this paper were calculated with the tool multi-bleu.perl and are case sensitive. 3 Architecture of the SMT system The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . Our main system is a phrase-based system (Koehn et al., 2003; Och and Ney, 2003), but we have also performed some experiments with a hierarchical system (Chiang, 2007). Both use a log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) = arg max{exp( e X λi hi (e, f ))} (1) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). The phrase-based system uses fourteen features functions, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model"
W11-2158,2008.iwslt-papers.6,1,0.92273,"Missing"
W11-2158,N03-1017,0,\N,Missing
W12-3147,E09-1003,1,0.887484,"Missing"
W12-3147,2011.mtsummit-papers.1,0,0.0352814,"were no differences when more training data is available. Due to time constraints, this procedure was not used in the submitted system. 4 Results and Discussion The results of our SMT systems are summarized in Table 2. The MT metric scores for the development set are the average of three optimisations performed with different seeds (see Section 3). For the test set, they are the average of four values: the three values corresponding to these different optimisations, plus a fourth value obtained by taking as weight for each model, the average of the weights obtained in the three optimisations (Cettolo et al., 2011). The numbers in parentheses are the standard deviation of these three or four values. The standard deviation gives a lower bound of the significance of the difference between two systems. If the difference between two average scores is less than the sum of the standard deviations, we can say that this difference is not significant. The reverse is not true. The results of Table 2 show that adding several adapted corpora (the filtered 109 corpus, the synBitext Translation : En→Fr Eparl+NC Eparl+NC+ntsXX Eparl+NC+ntsXX+109f Eparl+NC+ntsXX+109f +IR Eparl+NC+ntsXX+109f +news+IR Translation : Fr→En"
W12-3147,W08-0509,0,0.0287729,"Missing"
W12-3147,2003.mtsummit-tttt.3,0,0.0303475,". We had time to apply the domain-based data selection only for French. Thus all data were used for English. We used this method to filter the French–English 109 parallel corpus as well, based on the difference between in-domain cross-entropy and out-ofdomain cross-entropy calculated for each sentence of the English side of the corpus. We kept 49 million words (in the English side) to train our models, called 109f . 3 Architecture of the SMT system The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . We have build phrase-based systems (Koehn et al., 2003; Och and Ney, 2003), using the standard log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) = arg max{exp( e X i λi hi (e, f ))} (1) The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set (Och, 2003). The phrase-based system uses fourteen features functions, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). The"
W12-3147,P07-2045,0,0.0187376,"Missing"
W12-3147,W11-2132,1,0.887796,"Missing"
W12-3147,P10-2041,0,0.0502836,"cribes the statistical machine translation systems developed by the Computer Science laboratory at the University of Le Mans (LIUM) for the 2012 WMT shared task evaluation. We only considered the translation between French and English (in both directions). The main differences with respect to previous year’s system (Schwenk et al., 2011) are as follows: (i) use of more training data as provided by the organizers and (ii) better selection of the monolingual and parallel data according to the domain, using the cross-entropy difference with respect to in-domain and out-of-domain language models (Moore and Lewis, 2010). We kept some previous features: the improvement of the translation model adaptation by unsupervised training, a parallel corpus retrieved by Information Retrieval (IR) techniques and finally, the rescoring with a continuous space target language model for the translation into French. These different points are described in the rest of the paper, together with a summary of the experimental results showing the impact of each component. Bilingual data The latest version of the News-Commentary (NC) corpus and of the Europarl (Eparl) corpus (version 7) were used. We also took as training data a s"
W12-3147,J03-1002,0,0.0037201,"ly the domain-based data selection only for French. Thus all data were used for English. We used this method to filter the French–English 109 parallel corpus as well, based on the difference between in-domain cross-entropy and out-ofdomain cross-entropy calculated for each sentence of the English side of the corpus. We kept 49 million words (in the English side) to train our models, called 109f . 3 Architecture of the SMT system The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . We have build phrase-based systems (Koehn et al., 2003; Och and Ney, 2003), using the standard log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) = arg max{exp( e X i λi hi (e, f ))} (1) The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set (Och, 2003). The phrase-based system uses fourteen features functions, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). The system is based on"
W12-3147,P03-1021,0,0.0190644,"Missing"
W12-3147,W11-2158,1,0.664778,"ase-based systems based on the Moses decoder, trained on the provided data only. Additionally, new features this year included improved language and translation model adaptation using the cross-entropy score for the corpus selection. 1 Resources Used 2.1 Introduction This paper describes the statistical machine translation systems developed by the Computer Science laboratory at the University of Le Mans (LIUM) for the 2012 WMT shared task evaluation. We only considered the translation between French and English (in both directions). The main differences with respect to previous year’s system (Schwenk et al., 2011) are as follows: (i) use of more training data as provided by the organizers and (ii) better selection of the monolingual and parallel data according to the domain, using the cross-entropy difference with respect to in-domain and out-of-domain language models (Moore and Lewis, 2010). We kept some previous features: the improvement of the translation model adaptation by unsupervised training, a parallel corpus retrieved by Information Retrieval (IR) techniques and finally, the rescoring with a continuous space target language model for the translation into French. These different points are des"
W12-3147,2008.iwslt-papers.6,1,0.898559,"Missing"
W12-3147,N03-1017,0,\N,Missing
W13-2801,W13-2816,0,0.0501134,"Missing"
W13-2801,W13-2813,0,0.0217301,"Missing"
W13-2801,2008.eamt-1.6,0,0.0725213,"Missing"
W13-2801,W13-2804,0,0.0365849,"Missing"
W13-2801,W13-2805,0,0.0466293,"Missing"
W13-2801,W13-2814,0,0.0251731,"d in corresponding representations (a RBMT example is LFG (Lexical Functional Grammars) analysis and the corresponding XLE translation architecture). In HyTra 2013 there are three approaches dealing with multilevel information: • Turki Khemakhem et al. (2013) present work about an English-Arabic SMT system that uses morphological decomposition and morpho-syntactic annotation of the target language and incorporates the corresponding information in a statistical feature model. Essentially, the statistical feature language model replaces words by feature arrays. 3.4 Other multilevel approaches • Pal et al. (2013) propose a combination of aligners: GIZA++, Berkeley and rule-based for English-Bengali. • Hsieh et al. (2013) use comparable corpora extracted from Wikipedia to extract parallel fragments for the purpose of extending an English-Bengali training corpus. Semantic approaches The introduction of semantics in statistical MT has been approached to solve word sense disambiguation challenges covering the area of lexical semantics and, more recently, there have been different techniques using semantic roles covering shallow semantics, as well as the use of distributional semantics for improving transl"
W13-2801,W13-2806,0,0.0246185,"Missing"
W13-2801,W13-2807,0,0.0435616,"Missing"
W13-2801,W13-2817,0,0.0294611,"responding XLE translation architecture). In HyTra 2013 there are three approaches dealing with multilevel information: • Turki Khemakhem et al. (2013) present work about an English-Arabic SMT system that uses morphological decomposition and morpho-syntactic annotation of the target language and incorporates the corresponding information in a statistical feature model. Essentially, the statistical feature language model replaces words by feature arrays. 3.4 Other multilevel approaches • Pal et al. (2013) propose a combination of aligners: GIZA++, Berkeley and rule-based for English-Bengali. • Hsieh et al. (2013) use comparable corpora extracted from Wikipedia to extract parallel fragments for the purpose of extending an English-Bengali training corpus. Semantic approaches The introduction of semantics in statistical MT has been approached to solve word sense disambiguation challenges covering the area of lexical semantics and, more recently, there have been different techniques using semantic roles covering shallow semantics, as well as the use of distributional semantics for improving translation unit selection. Approaches treating the incorporation of semantics into MT in HyTra 2013 include the fol"
W13-2801,W13-2815,0,0.0580296,"Missing"
W13-2801,W13-2811,0,0.014586,"nd corresponding POS-based restructuring of the input. Basically, they focus on taking advantage of the fact that Korean has compound words, which - for the purpose of alignment - are split and reordered similarly to Chinese. 3.5 In a number of linguistic theories information from the morphological, syntactic and semantic level is considered conjointly and merged in corresponding representations (a RBMT example is LFG (Lexical Functional Grammars) analysis and the corresponding XLE translation architecture). In HyTra 2013 there are three approaches dealing with multilevel information: • Turki Khemakhem et al. (2013) present work about an English-Arabic SMT system that uses morphological decomposition and morpho-syntactic annotation of the target language and incorporates the corresponding information in a statistical feature model. Essentially, the statistical feature language model replaces words by feature arrays. 3.4 Other multilevel approaches • Pal et al. (2013) propose a combination of aligners: GIZA++, Berkeley and rule-based for English-Bengali. • Hsieh et al. (2013) use comparable corpora extracted from Wikipedia to extract parallel fragments for the purpose of extending an English-Bengali train"
W13-2801,W13-2810,0,0.0239078,"Missing"
W13-2801,W13-2818,0,0.0263833,"extracted from Wikipedia to extract parallel fragments for the purpose of extending an English-Bengali training corpus. Semantic approaches The introduction of semantics in statistical MT has been approached to solve word sense disambiguation challenges covering the area of lexical semantics and, more recently, there have been different techniques using semantic roles covering shallow semantics, as well as the use of distributional semantics for improving translation unit selection. Approaches treating the incorporation of semantics into MT in HyTra 2013 include the following research work: • Tambouratzis et al. (2013) describe a hybrid MT architecture that uses very few bilingual corpus and a large monolingual one. The linguistic information is extracted using pattern recognition techniques. Table 1 summarizes the papers that have been presented in the Second HyTra Workshop. The papers are arranged into the table according to the linguistic level they address. • Rudnick et al. (2013) present a combination of Maximum Entropy Markov Models and HMM to perform lexical selection in the sense of cross-lingual word sense disambiguation (i.e. by choice from the set of translation alternatives). The system is meant"
W13-2801,W13-2808,0,0.058822,"Missing"
W13-2801,W10-1737,0,0.0651783,"Missing"
W13-2801,W13-2803,0,0.0495517,"Missing"
W13-2801,W13-2809,0,0.0597288,"Missing"
W13-2801,W13-2812,0,0.0196328,"often considered and represented simultaneously (not only in unification-based approaches) and the same is true for MT systems. Syntax had been addressed originally in SMT in the form of so called phrase-based SMT without any reference to linguistic structures; during 3 • Bouillon et al. (2013) presents two methodologies to correct homophone confusions. The first one is based on hand-coded rules and the second one is based on weighted graphs derived from a pronunciation resource. • Laki et al. (2013) combine pre-reordering rules with morphological and factored models for English-to-Turkish. • Li et al. (2013) propose pre-reordering rules to be used for alignment-based reordering, and corresponding POS-based restructuring of the input. Basically, they focus on taking advantage of the fact that Korean has compound words, which - for the purpose of alignment - are split and reordered similarly to Chinese. 3.5 In a number of linguistic theories information from the morphological, syntactic and semantic level is considered conjointly and merged in corresponding representations (a RBMT example is LFG (Lexical Functional Grammars) analysis and the corresponding XLE translation architecture). In HyTra 201"
W19-6727,D16-1162,0,0.0200931,"lar segmentation techniques. It is now defacto to use subwords in NMT as with the better vocab coverage it enables the NMT models with excellent copying capability. The copying behaviour is required when the named entities need to be copied from the source text to the target translation. Although subword NMT works quite well at copying, it sometimes fails to copy the complete sequence of subwords in the translated text and results in spelling errors. 3.2.4 Spelling Errors in Subword NMT In general, NMT models perform quite poorly on rare words, (Luong and Manning, 2015; Sennrich et al., 2016; Arthur et al., 2016) due to the fixed vocabulary of NMT models. The most common categories of rare words are named entities and nouns. These entities often pass through the NMT system unchanged. For example, the word ”Gonzalez” is broken into ”G@@ on@@ z@@ al@@ e@@ z” by BPE and passes through the NMT system unchanged. However, when it fails, the model can drop or wrongly translate subwords which results in perceived misspellings. Subword Dropped In this case when a subword (which is part of a named entity) is not copied in Dublin, Aug. 19-23, 2019 |p. 145 the translated text. For example, the word Stephen is spl"
W19-6727,W17-4713,0,0.0743068,"thek ist .(de) the idea behind littlebits is that its a growing library . (en) Table 5: Comparison of translation on a sentence from test corpus the training data. A standard technique to adapt a generic model to a specific domain is to continue the training with a small amount of in-domain parallel data. This technique, referred to as finetuning, is very effective. Our translation models are dynamically adapted to the source text context at each sentence, using fine-tuning but without knowing the source domain in advance. This adaptation is performed with a method similar to that proposed by Farajian et al. (2017). If a segment similar to the source sentence is found in the training corpus, the model is fine-tuned with the corresponding sentence pair for a few epochs. To this end, the training corpus is indexed into a translation memory. At test time, the translation memory is queried with the source sentence by information retrieval tools3 . The number of epochs and the learning rate of the fine tuning with the retrieved sentence pair depends on the similarity between its source side and the source sentence. If they are not similar, fine tuning the model with the retrieved sentence may worsen the tran"
W19-6727,W18-2709,0,0.0366814,"Missing"
W19-6727,P02-1040,0,0.10691,"The statistics of the data are shown in Table 1. In the case of the Moses pipeline, the development set was selected at random. The test set was the same, but processed according to each pipeline. We trained small transformer models with the Fairseq tool (Ott et al., 2018), with the same parameters as those indicated in the fairseq github site for IWSLT’14 German to English. We averaged the 5 checkpoints around the best model. We repeated the training 3 times and report the average and standard deviation of the 3 runs. Results are reported in Table 2. Training with our pipeline improves BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores respectively by 2.3 and 3.0 points. The difference is larger than standard deviation error bars, thus it is statistically significant according to this criterion. This suggests that efforts to better clean the data and to choose the validation set carefully are beneficial in terms of automated quality metrics. NMT models are not perfect at controlling the output length and sometimes drop or duplicate content. To evaluate this category of errors, the rest of metrics measure over-generation (repetitions) and under-generation (source text not covered). OVER s"
W19-6727,P16-1162,0,0.309754,"ing (BPE) or other similar segmentation techniques. It is now defacto to use subwords in NMT as with the better vocab coverage it enables the NMT models with excellent copying capability. The copying behaviour is required when the named entities need to be copied from the source text to the target translation. Although subword NMT works quite well at copying, it sometimes fails to copy the complete sequence of subwords in the translated text and results in spelling errors. 3.2.4 Spelling Errors in Subword NMT In general, NMT models perform quite poorly on rare words, (Luong and Manning, 2015; Sennrich et al., 2016; Arthur et al., 2016) due to the fixed vocabulary of NMT models. The most common categories of rare words are named entities and nouns. These entities often pass through the NMT system unchanged. For example, the word ”Gonzalez” is broken into ”G@@ on@@ z@@ al@@ e@@ z” by BPE and passes through the NMT system unchanged. However, when it fails, the model can drop or wrongly translate subwords which results in perceived misspellings. Subword Dropped In this case when a subword (which is part of a named entity) is not copied in Dublin, Aug. 19-23, 2019 |p. 145 the translated text. For example, t"
W19-6727,2006.amta-papers.25,0,0.0709087,"hown in Table 1. In the case of the Moses pipeline, the development set was selected at random. The test set was the same, but processed according to each pipeline. We trained small transformer models with the Fairseq tool (Ott et al., 2018), with the same parameters as those indicated in the fairseq github site for IWSLT’14 German to English. We averaged the 5 checkpoints around the best model. We repeated the training 3 times and report the average and standard deviation of the 3 runs. Results are reported in Table 2. Training with our pipeline improves BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores respectively by 2.3 and 3.0 points. The difference is larger than standard deviation error bars, thus it is statistically significant according to this criterion. This suggests that efforts to better clean the data and to choose the validation set carefully are beneficial in terms of automated quality metrics. NMT models are not perfect at controlling the output length and sometimes drop or duplicate content. To evaluate this category of errors, the rest of metrics measure over-generation (repetitions) and under-generation (source text not covered). OVER simply counts repetitions in th"
W19-6727,D18-1339,0,0.0373017,"in tackling out-ofvocabulary (OOV) problem in NMT. It helps in improving the coverage by splitting words. Therefore, the system can translate different forms of a word even if it was not seen during training. 3.2 Issues with Tokenization Too much tokenization can also cause issues. We often come across words and phrases which should be left untouched during translation. They are in general entities and they can represent file numbers, file paths, formatting tags, commands, product names, email address, URLs, terms etc. In Neural MT, this process of copying is also learned during translation (Knowles and Koehn, 2018). However, if we do not pay attention to such entities it gets difficult to recover then successfully as some parts of the entities may get modified during translation. Therefore, we focus on learning the translation part and normalize the other data where we require untouched copy as a part of pre-processing and post-processing. 3.2.1 Do Not Translate Terms We define do-not-translate terms (DNTs) as terms which are exact copy from the source. They are neither translated nor transliterated. The languages where the source and target have different scripts and do not share characters, it is easi"
W19-6727,W17-3204,0,0.0219989,"9.55 50.11 Table 4: Evaluation scores. K: threshold for the unk-count The evaluation scores are detailed in Table 4. The quality scores have not improved using the proposed methods, but in manual evaluation, it was found that the model trained with ”no more split” setting preserves better the named entities. This is depicted with an example in Table 5. The model with ”protect unseen” with threshold value of 4 is slightly better than baseline, but in manual evaluation, we have seen that it is not better at translating the named entities compared to the baseline. 4 Domain Adaptation As shown by Koehn and Knowles (2017), NMT is even more sensitive to the domain than phrasebased SMT. Translation quality drops abruptly when the source text is in a different domain to 2 https://wit3.fbk.eu/archive/2014-01/ texts/de/en/de-en.tgz Dublin, Aug. 19-23, 2019 |p. 146 input reference baseline no more split die idee hinter littlebits ist , dass es eine wachsende bibliothek ist . (de) the idea behind littlebits is that its a growing library . (en) die idee hinter li@@ tt@@ leb@@ it@@ s ist , dass es eine wachsen@@ de bibliothe@@ k ist .(de) the idea behind littlement is that its a growing library . (en) die idee hinter l"
W19-6727,2015.iwslt-evaluation.11,0,0.0262138,"lem using byte-pair encoding (BPE) or other similar segmentation techniques. It is now defacto to use subwords in NMT as with the better vocab coverage it enables the NMT models with excellent copying capability. The copying behaviour is required when the named entities need to be copied from the source text to the target translation. Although subword NMT works quite well at copying, it sometimes fails to copy the complete sequence of subwords in the translated text and results in spelling errors. 3.2.4 Spelling Errors in Subword NMT In general, NMT models perform quite poorly on rare words, (Luong and Manning, 2015; Sennrich et al., 2016; Arthur et al., 2016) due to the fixed vocabulary of NMT models. The most common categories of rare words are named entities and nouns. These entities often pass through the NMT system unchanged. For example, the word ”Gonzalez” is broken into ”G@@ on@@ z@@ al@@ e@@ z” by BPE and passes through the NMT system unchanged. However, when it fails, the model can drop or wrongly translate subwords which results in perceived misspellings. Subword Dropped In this case when a subword (which is part of a named entity) is not copied in Dublin, Aug. 19-23, 2019 |p. 145 the translat"
W19-6727,P18-2059,0,0.0189137,"the validation set carefully are beneficial in terms of automated quality metrics. NMT models are not perfect at controlling the output length and sometimes drop or duplicate content. To evaluate this category of errors, the rest of metrics measure over-generation (repetitions) and under-generation (source text not covered). OVER simply counts repetitions in the output, while UNDER counts under-generation based on the ratio of number of source and output words. REP and DROP count respectively the number of repetitions and under-generation in the output based on the alignment with the source (Malaviya et al., 2018). Interestingly, the REP score is significantly lower with our pipeline. The DROP score average is also lower although the difference lies within the standard deviation. This suggests that the engine is more robust to under- and overgeneration with our pipeline. Replacing do-not-translate phrases (DNTs) by placeholders (see section 3.2.1) yields slightly worse BLEU and TER scores. However, the OVER, REP and UNDER scores are improved. Proceedings of MT Summit XVII, volume 2 Thus using DNTs may improve robustness. The worsening of BLEU and TER may be due to the fact that we used only one type of"
W19-6727,W18-6301,0,0.0205947,"±0.2 2.7 ±0.1 UNDER 8.0 ±1.0 8.3 ±0.6 6.7 ±1.2 8.3 ±0.6 6.7 ±1.2 DROP 9.5 ±0.8 8.7 ±0.3 8.8 ±0.5 8.7 ±0.2 8.8 ±0.5 Table 2: Evaluation scores for training on data processed by Moses tools, our pipeline without (Iconic) and with (Iconic+DNT) replacement of do-not-translate phrases by placeholders. (175 words) and the same true-casing models. The statistics of the data are shown in Table 1. In the case of the Moses pipeline, the development set was selected at random. The test set was the same, but processed according to each pipeline. We trained small transformer models with the Fairseq tool (Ott et al., 2018), with the same parameters as those indicated in the fairseq github site for IWSLT’14 German to English. We averaged the 5 checkpoints around the best model. We repeated the training 3 times and report the average and standard deviation of the 3 runs. Results are reported in Table 2. Training with our pipeline improves BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores respectively by 2.3 and 3.0 points. The difference is larger than standard deviation error bars, thus it is statistically significant according to this criterion. This suggests that efforts to better clean the dat"
