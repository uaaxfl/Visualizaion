2005.iwslt-1.14,J93-2003,0,0.0102317,"Missing"
2005.iwslt-1.14,J82-2005,0,0.0798235,"udied. We translate numeral phrases first by using a standard templates depository. We develop a phrase-based decoder that employs a beam search algorithm. To make the result more reasonable, we apply those words with fertility probability of zero. We improve the previously proposed tracing back algorithm to get the best path. Some experiments concerned are presented. 1 Introduction Statistical machine translation is a promising approach for large vocabulary text translation. In the early 90s, IBM developed Candide system. Since then, many statistical machine translation systems were proposed [2][3]. These systems apply a translation model to capture the relationship between the source and target languages, and use a language model to drive the search process. The primary IBM model was purely word-based. To get more complex structure, better lexical choice and more reliable local reordering, the phrase-based statistical machine translation systems were proposed. Yamada and Knight [4] used phrase translation in a syntax-based translation system; March and Wong [5] introduced a joint-probability model for phrase translation; CMU and IBM also improved their systems with phrase translatio"
2005.iwslt-1.14,P01-1067,0,0.0994031,"al machine translation is a promising approach for large vocabulary text translation. In the early 90s, IBM developed Candide system. Since then, many statistical machine translation systems were proposed [2][3]. These systems apply a translation model to capture the relationship between the source and target languages, and use a language model to drive the search process. The primary IBM model was purely word-based. To get more complex structure, better lexical choice and more reliable local reordering, the phrase-based statistical machine translation systems were proposed. Yamada and Knight [4] used phrase translation in a syntax-based translation system; March and Wong [5] introduced a joint-probability model for phrase translation; CMU and IBM also improved their systems with phrase translation capability. Our system applies a phrase-based translation model to capture the corresponding relationship between two languages. We propose a formula to compute the phrase translation probability through word alignment. The phrase-based decoder we developed employs a beam search algorithm, similar to the one in [6]. We applied zero fertility words in the target language. Because the transla"
2005.iwslt-1.14,1983.tc-1.13,0,0.514714,"ion. In the early 90s, IBM developed Candide system. Since then, many statistical machine translation systems were proposed [2][3]. These systems apply a translation model to capture the relationship between the source and target languages, and use a language model to drive the search process. The primary IBM model was purely word-based. To get more complex structure, better lexical choice and more reliable local reordering, the phrase-based statistical machine translation systems were proposed. Yamada and Knight [4] used phrase translation in a syntax-based translation system; March and Wong [5] introduced a joint-probability model for phrase translation; CMU and IBM also improved their systems with phrase translation capability. Our system applies a phrase-based translation model to capture the corresponding relationship between two languages. We propose a formula to compute the phrase translation probability through word alignment. The phrase-based decoder we developed employs a beam search algorithm, similar to the one in [6]. We applied zero fertility words in the target language. Because the translation quality largely depends on the accuracy of phrase-to-phrase translation pair"
2005.iwslt-1.14,N03-1017,0,0.0722604,"sed statistical machine translation systems were proposed. Yamada and Knight [4] used phrase translation in a syntax-based translation system; March and Wong [5] introduced a joint-probability model for phrase translation; CMU and IBM also improved their systems with phrase translation capability. Our system applies a phrase-based translation model to capture the corresponding relationship between two languages. We propose a formula to compute the phrase translation probability through word alignment. The phrase-based decoder we developed employs a beam search algorithm, similar to the one in [6]. We applied zero fertility words in the target language. Because the translation quality largely depends on the accuracy of phrase-to-phrase translation pairs extracted from bilingual corpora, we propose a different tracing back algorithm to find the best path. Four methods are studied to extract bilingual phrase pairs. We describe these methods and phrase-based translation model in Section 2. Section 3 explains the method of numeral phrase translation. Section 4 outlines the architecture of the decoder that combines the translation model, distortion model, language model to generate target s"
2005.iwslt-1.14,C96-2141,0,0.149702,"der two constraints: (1). all the cells in the expanded region accord with the evaluation function ; (2). all the cells should not be marked. d. The words in this area make up of a phrase pair. Mark all nodes between x-coordinate and y-coordinate in this matrix, then search other max points and corresponding rectangles among the rest unmarked nodes until all nodes in the MI matrix of this sentence pair are marked.[7] 2-3 Extracting Phrase Pairs From HMM Word Alignment Model A simple way to extract phrase pairs is using a word alignment model. We use the HMM-based alignment model introduced in [8]. For a source phrase that ranges from position j1 to j2 in sentence, we can get the corresponding target phrase’s beginning position and ending position to extract the phrase translation. Just like the method described in 2-1, a given factor that prevents the length of the phrase pairs differ greatly is needed. 2-4 Extracting phrase pair by Giza++ toolkit The Giza++ toolkit can be used to establish word-based alignments. There are some heuristic functions can improve the quality of alignment and extract phrase pair. In [6], the parallel corpus is aligned bidirectionally, some additional align"
2005.iwslt-1.14,W99-0604,0,0.0566577,"events the length of the phrase pairs differ greatly is needed. 2-4 Extracting phrase pair by Giza++ toolkit The Giza++ toolkit can be used to establish word-based alignments. There are some heuristic functions can improve the quality of alignment and extract phrase pair. In [6], the parallel corpus is aligned bidirectionally, some additional alignment points are added to the intersection of the two alignments. All aligned phrase pairs are connected to be consistent with the word alignment: each word corresponds strictly to another word in a legal phrase pair, not to any word outside the pair [9]. 2-5 Phrase Translation Probability CMU used the phrase translation probability formula based on the IBM1 alignment model: p( c |e ) = ∏∑ p( ci |e j ) (2-2) i j There is a drawback for this method: If only one word of source phrase has no appropriate corresponding word in target phrase, the phrase translation probability will be small. Since there are many auxiliary words and mood words in Chinese, this issue becomes more serious. To prevent this, we use the word alignment generated by the IBM model 4 to divide the whole phrase pair into several small phrase pair blocks. If one source word"
2005.iwslt-1.14,W01-1408,0,0.0408713,"the source phrase of the existing hypothesis. The hypotheses are stored in different stacks. Each of them has a sequence number. The odd stack s2 p −1 contains all hypotheses whose target phrases are not F-zerowords and in which p source words have been translated so far. (If the target phrase of the hypothesis is not F-zeroword, it stored in the stack 2p-1, p is the number of source words translated), the even stack s2 p contains all hypotheses whose target phrases are F-zerowords and in which p source words have been translated accumulatively. We recombine search hypotheses as described in [10], and prune out weak hypotheses based on the probability they incurred so far and a future score estimated as in [6]. All these reduce the number of hypotheses stored in stacks to speed up the decoder. The current probability of the new hypothesis is the probability of the original hypothesis multiplied with the translation, distortion and language probability of the added phrasal translation, the probability formula is: p( e |c ) = pT ( c |e)λt × pL ( e) λl × pD ( e, c) λd (3-1) In which pT ( c |e) is the translation model computed according to (2-3), pL ( e) is the target language model in w"
2005.mtsummit-invited.8,J97-3002,0,0.0525454,"Missing"
2005.mtsummit-invited.8,W99-0604,0,0.043609,"and alignment template, however, we use directly a phrase translation table. This allows us to build a more compact, more transparent and faster decoder, 3.2.1 The methods of phrase-extraction Because the quality of translations is largely dependent on the quality of phrase translation pairs extracted from bilingual corpora. Our system integrated with four methods to extract bilingual phrase pairs: 1. Integrated segmentation and phrase alignment (ISA) (Zhang, 2003) 2. Extracting phrase pairs from HMM word alignment model (Vogel et al., 1996) 3. phrases from bi-direction Word-Based Alignment (Och et al., 1999). 4. Phrase-extraction using Inversion Transduction Grammar (Wu, 1997) Table 1 gives the different scale of extracted phrases i - 24 (Maxlength: 3 words), Training corpus: 130 thousand sentence pairs. Size ISA HMM WBA ITG 100,000 130,000 250,000 300,000 E: E: Wang C:****** P:.323 E:want to C ****** P .358 E: the(FW) C: ****** P:.067 C:****** P Table 1 the scale of extracted phrase 3.2.2 E: Mr. C:****** P:.598 Decoder The decoding process works in two stages: First, the phrase translations should be generated for the input text, this is done before the searching begin. Second, the search proces"
2005.mtsummit-invited.8,C96-2141,0,0.0268776,"ar to (Och, 1999) alignment template model. Instead of using word class and alignment template, however, we use directly a phrase translation table. This allows us to build a more compact, more transparent and faster decoder, 3.2.1 The methods of phrase-extraction Because the quality of translations is largely dependent on the quality of phrase translation pairs extracted from bilingual corpora. Our system integrated with four methods to extract bilingual phrase pairs: 1. Integrated segmentation and phrase alignment (ISA) (Zhang, 2003) 2. Extracting phrase pairs from HMM word alignment model (Vogel et al., 1996) 3. phrases from bi-direction Word-Based Alignment (Och et al., 1999). 4. Phrase-extraction using Inversion Transduction Grammar (Wu, 1997) Table 1 gives the different scale of extracted phrases i - 24 (Maxlength: 3 words), Training corpus: 130 thousand sentence pairs. Size ISA HMM WBA ITG 100,000 130,000 250,000 300,000 E: E: Wang C:****** P:.323 E:want to C ****** P .358 E: the(FW) C: ****** P:.067 C:****** P Table 1 the scale of extracted phrase 3.2.2 E: Mr. C:****** P:.598 Decoder The decoding process works in two stages: First, the phrase translations should be generated for the input tex"
2011.mtsummit-papers.45,N03-1017,0,0.0242097,"X is a non-terminal, J and D are strings of terminals and non-terminals. ~ is a one-to-one correspondence for the non-terminals appearing in J and D . The production rules are induced from a bilingual corpus with the help of word alignments. The procedure is as follows: First, given a word-aligned sentence pair ( f , e, A) , let f and e stand for any continuous sequences of f and e . Here, A represents the many-to-many word alignments which are induced by running a one-to-many word alignment model, such as GIZA++2, in both directions and by combining the results based on a heuristic approach (Koehn et al., 2003).Then a rule ( f , e) is an initial phrase pair of ( f , e, A) iff f i  f : (i, j )  A o e j  e e j  e : (i, j )  A o f i  f Motivation The expressive power of 2-SCFG is gained through looking for continuous phrases that 2 398 (1) http://code.google.com/p/giza-pp/ Second, based on the extracted continuous phrases, production rules are accumulated by computing the “holes” for contiguous phrases (Chiang, 2005): 1. A continuous initial phrase pair ( f , e) constitutes a rule 2. limits, the model also lost some useful information because it only extracted reordering patterns in the bilingu"
2011.mtsummit-papers.45,W04-3250,0,0.181265,"Missing"
2011.mtsummit-papers.45,D07-1104,0,0.040294,"Missing"
2011.mtsummit-papers.45,P03-1021,0,0.00887483,"Missing"
2011.mtsummit-papers.45,J04-4002,0,0.0251086,"has no requirement for strict alignment between subphrases. Thus, the discontinuous phrases with gaps impose more flexible hierarchical constraints and can better account 399 for various linguistic phenomena such as set phrases and long-distance reordering. 3. rules like (8) are composed of discontinuous phrases which have gaps on both sides. Based on the many-to-many word alignments between each sentence pair, we extract discontinuous phrases (J , D 1¡ e D 2 ) which are continuous on the source side, following the method adopted in traditional phrase-based MT approaches (Zens and Ney, 2003; Och and Ney, 2004). The difference lies in that the target phrase D 1 ¡ e D 2 has one gap symbol ¡ e  {¡ eR , ¡ eL } , which stands for the sequence of words aligned outside of the source phrase J . When extracting System implementation 3.1. Discontinuous phrase extraction If arbitrary number of words and gaps may be rules, the amount of source discontinuous phrases that cover each sentence is exponential in the sentence length. This is especially problematic for training and decoding. In order to make the balance between efficiency and accuracy, we should impose some limitations on discontinuous phrase extrac"
2011.mtsummit-papers.45,P02-1040,0,0.0894106,"Missing"
2011.mtsummit-papers.45,W09-2303,0,0.0208612,"Missing"
2011.mtsummit-papers.45,W09-3805,0,0.0209456,"Missing"
2011.mtsummit-papers.45,P06-1123,0,0.0444667,"Missing"
2011.mtsummit-papers.45,W05-0909,0,0.0485666,"Missing"
2011.mtsummit-papers.45,2007.mtsummit-tutorials.1,0,0.0260954,"inuous phrases can account for these missed patterns and proposed a generalization of conventional phrase-based decoding to handle discontinuities in both source and target phrases, which yields significant improvements over Joshua(Li et al., 2009)1, a state-of-the-art HPB system. Some other attempts to exploit phrasal discontinuities are also based on phrase-based SMT. Simard et al. (2005) presented an extension to Moses that allows one-word gaps in source and target phrases. This makes decoding simpler, but fixed-size discontinuous phrases are less general and will increase data sparseness. Cancedda et al. (2007) extended Simard’s work by using flexible phrases that may contain gaps of variable lengths. It should be noted that these attempts with discontinuous phrases were mainly carried out using the left-right SMT decoder which is ineffective in allowing phrasal discontinuities. Although they tried to extend the linear decoding to support phrases with gaps, the linguistic patterns within the hierarchical structures of discontinuous phrases are difficult to be utilized fully. Additionally, He and Zong (2008) proposed a generalized reordering model for phrase-based SMT which developed a CKY style deco"
2011.mtsummit-papers.45,P05-1033,0,0.701442,"les, the translation rules with discontinuities can be seamlessly integrated into the CKY decoder. Experimental results show that the proposed approach for incorporating the linguistic discontinuities achieves statistically significant improvements over the traditional HPB system. 1. Introduction Hierarchical phrase-based (HPB) translation has emerged as one of the dominant current approaches to statistical machine translation (SMT), which combines the ideas of syntax-based translation and phrase-based translation. Based on the binary synchronous context-free grammar (2-SCFG), the HPB system (Chiang, 2005) has better generalization capability and can capture long distance reordering. However, due to the limitation of phrasal continuity, HPB model cannot extract the frequent patterns in Chinese-to-English translation, such as “а…ቡėas soon as”(Figure 1(a)), where words in the source(Chinese) phrases may be separated by gaps. Inaddition, 1 It also significantly outperformed the conventional phrase-based MT(Moses) 397 Figure 1: an example of Chinese-to-English sentence pair and some useful translation rules within it. (a) and (b) are the flexible patterns missed in HPB model: set phrases and long-"
2011.mtsummit-papers.45,P07-1037,0,0.0309797,"us phrases. The procedure of the additional rule generation is as follows: ~ First, let f and e~ stand for any subsequences of f and e , including both continuous and D D 1 eD 2 constitute a rule X o (J 1 X k J 2 , D1 X kD 2 ) 3. (3) Two glue rules are added so that it prefers combining hierarchical phrases in a serial manner: S o ( SX , SX ) S o (X , X ) phrases ( f , e) and discontinuous phrases ( f ¡ , e¡ ) . ~ A phrase ( f , e~) is an initial phrase pair of ( f , e, A) iff (4) To reduce the rule set size and spurious ambiguity, most HPB systems use some constraints to filter the rule set (Chiang, 2007). ~ f i  f : (i, j )  A o e j  e~ ~ e j  ~ e : (i, j )  A o f i  f 2.2. Problems with current HPB model Then, each discontinuous phrase ( f ¡ , e¡ ) should include a sequence of words and gaps (indicated by the symbol ¡ ): the gap here acts as a placeholder for the sequence of unspecified words. To avoid redundancy, phrases may not begin or end with a gap. Thus a discontinuous initial phrase pair ( f ¡ , e¡ ) can be rewritten Even though 2-SCFG allows some phrasal discontinuities, it is heavily dependent on continuous phrases and imposes hard hierarchical constraints (Galley, 2010). Fo"
2013.iwslt-evaluation.25,P05-1033,0,0.208163,"Missing"
2013.iwslt-evaluation.25,P03-1021,0,0.0232596,"Missing"
2013.iwslt-evaluation.25,N03-1017,0,0.0357243,"Missing"
2020.ccl-1.36,W06-0901,0,0.162555,"Missing"
2020.ccl-1.36,W17-4211,0,0.0298028,"Missing"
2020.ccl-1.36,P15-1017,0,0.0513033,"Missing"
2020.ccl-1.36,P13-1008,0,0.0596844,"Missing"
2020.ccl-1.36,N16-1034,0,0.0304466,"Missing"
2020.ccl-1.36,D19-1032,0,0.0219827,"Missing"
2020.ccl-1.36,N19-1423,0,0.0629689,"Missing"
2020.ccl-1.36,N19-1080,0,0.0211131,"Missing"
2020.ccl-1.36,P10-1081,0,0.0895254,"Missing"
2020.coling-main.392,P19-1258,1,0.839294,"information flow in the self-attention (Cheng et al., 2016) and context-attention (Zhang et al., 2018) modules in the Incremental Transformer allows our model to exploit contextual information more efficiently. Recently, a considerable literature has grown up around the theme of incorporating external knowledge in generative conversation systems, including question answering systems (Hao et al., 2017; Mihaylov and Frank, 2018), open-domain dialogue systems (Young et al., 2018; Zhou et al., 2018b; Zhong et al., 2019a), and task-oriented dialogue systems (He et al., 2019; Madotto et al., 2018; Chen et al., 2019). Zhong et al. (2019b) proposes a Knowledge-Enriched Transformer (KET) achieving the state-ofthe-art performance on multiple textual conversation datasets, where contextual utterances are encoded using hierarchical self-attention and commonsense knowledge is incorporated using a context-aware affective graph attention mechanism. However, they ignore various relations in external knowledge base, which may cause the loss of semantic information. By contrast, our dual-level graph attention mechanism, can take advantage of the various relations in external knowledge base to better augment the sema"
2020.coling-main.392,D16-1053,0,0.0810081,"e retrieved relevant knowledge graphs from a large-scale commonsense knowledge base. Specifically, we propose a dual level graph attention mechanism to encode these relevant knowledge graphs, which consists of a node-level attention to learn the importance of different neighboring nodes and a relation-level attention to learn the importance of different relations to the current node. Then we apply the Incremental Transformer (Li et al., 2019) to incrementally encode multi-turn contextual utterances, which could capture the intra-utterance and interutterance correlations by the self-attention (Cheng et al., 2016) and context-attention (Zhang et al., 2018) modules, respectively. Moreover, we introduce multi-task learning to alleviate the confusion between a few non-neutral utterances and much more neutral ones, as shown in Table 1. Specifically, we first focus on the binary classification, “non-neutral” versus “neutral”, and then classifies the “non-neutral” ones into fine-grained emotion categories. These two auxiliary tasks are jointly trained with the original emotion recognition task. In summary, this paper makes the following contributions: • We devise a dual-level graph attention mechanism to sup"
2020.coling-main.392,D14-1179,0,0.02933,"Missing"
2020.coling-main.392,N19-1423,0,0.0252872,"luation metric of each dataset is the same as the one used in (Zhong et al., 2019b). 4435 4.2 Baselines We compare our proposed model with the following baselines: cLSTM: It first adopt a bidirectional LSTM to extract utterance-level features and then use a contextlevel unibidirectional LSTM to model the contextual utterances. CNN (Kim, 2014): A single-layer CNN is trained on utterance-level without context. CNN+cLSTM (Poria et al., 2017): It first adopt an CNN to extract utterance-level features and then apply a context-level unibidirectional LSTM to learn context representations. BERT BASE (Devlin et al., 2019): Base version of Bert. It takes as input each utterance with its context as a single text. DialogueRNN (Majumder et al., 2019): It exploits three gated recurrent units (GRU) to capture speaker information, context and emotional information of the preceding utterances, respectively. KET (Zhong et al., 2019b): It’s the state-of-the-art model for ERTC, where contextual utterances are encoded using hierarchical self-attention and commonsense knowledge is incorporated using a contextaware affective graph attention mechanism. 4.3 Hyper-parameter Settings We use Adam optimizer (Kingma and Ba, 2015)"
2020.coling-main.392,P17-1021,0,0.0241325,"onversation datasets. Compared to these gated RNNs and CNNs based models, we apply the Incremental Transformer (Li et al., 2019) to incrementally encode multi-turn contextual utterances, where the shorter path of information flow in the self-attention (Cheng et al., 2016) and context-attention (Zhang et al., 2018) modules in the Incremental Transformer allows our model to exploit contextual information more efficiently. Recently, a considerable literature has grown up around the theme of incorporating external knowledge in generative conversation systems, including question answering systems (Hao et al., 2017; Mihaylov and Frank, 2018), open-domain dialogue systems (Young et al., 2018; Zhou et al., 2018b; Zhong et al., 2019a), and task-oriented dialogue systems (He et al., 2019; Madotto et al., 2018; Chen et al., 2019). Zhong et al. (2019b) proposes a Knowledge-Enriched Transformer (KET) achieving the state-ofthe-art performance on multiple textual conversation datasets, where contextual utterances are encoded using hierarchical self-attention and commonsense knowledge is incorporated using a context-aware affective graph attention mechanism. However, they ignore various relations in external know"
2020.coling-main.392,D18-1280,0,0.0984614,"Zhou et al., 2018a) and its widespread applications in opinion mining, recommender systems, emotion-aware dialogues generation, and so on (Poria et al., 2019b). Some of the deep learning-based models have been proposed for emotion recognition in conversations, in only textual and multimodal settings (containing textual, acoustic, and visual information). Poria et al. (2017) proposes a long short-term memory network (LSTM) (Hochreiter and Schmidhuber, 1997) based model to capture contextual correlations from the utterances of a user-generated video 4430 for multimodal sentiment classification. Hazarika et al. (2018b) proposes conversational memory network (CMN) that exploits distinct memory units for each speaker to model emotional dynamics and detect emotion in a dyadic conversation. Later, Hazarika et al. (2018a) improves upon this approach with interactive conversational memory network (ICON), which utilizes the interactive memory unit to hierarchically model the self- and inter-speaker emotional influences for emotion recognition in conversational videos. Majumder et al. (2019) proposes the DialogueRNN model that exploits three gated recurrent units (GRU) (Cho et al., 2014) to capture speaker inform"
2020.coling-main.392,N18-1193,0,0.0682013,"Zhou et al., 2018a) and its widespread applications in opinion mining, recommender systems, emotion-aware dialogues generation, and so on (Poria et al., 2019b). Some of the deep learning-based models have been proposed for emotion recognition in conversations, in only textual and multimodal settings (containing textual, acoustic, and visual information). Poria et al. (2017) proposes a long short-term memory network (LSTM) (Hochreiter and Schmidhuber, 1997) based model to capture contextual correlations from the utterances of a user-generated video 4430 for multimodal sentiment classification. Hazarika et al. (2018b) proposes conversational memory network (CMN) that exploits distinct memory units for each speaker to model emotional dynamics and detect emotion in a dyadic conversation. Later, Hazarika et al. (2018a) improves upon this approach with interactive conversational memory network (ICON), which utilizes the interactive memory unit to hierarchically model the self- and inter-speaker emotional influences for emotion recognition in conversational videos. Majumder et al. (2019) proposes the DialogueRNN model that exploits three gated recurrent units (GRU) (Cho et al., 2014) to capture speaker inform"
2020.coling-main.392,N18-2008,0,0.0647722,"Missing"
2020.coling-main.392,D14-1181,0,0.00769251,"otion labels include sad, mad, scared, powerful, peaceful, joyful and neutral, which are different from MELD. IEMOCAP (Busso et al., 2008): Two-way emotional conversation. Its emotion labels include happiness, sadness, anger, frustrated, excited and neutral. The evaluation metric of each dataset is the same as the one used in (Zhong et al., 2019b). 4435 4.2 Baselines We compare our proposed model with the following baselines: cLSTM: It first adopt a bidirectional LSTM to extract utterance-level features and then use a contextlevel unibidirectional LSTM to model the contextual utterances. CNN (Kim, 2014): A single-layer CNN is trained on utterance-level without context. CNN+cLSTM (Poria et al., 2017): It first adopt an CNN to extract utterance-level features and then apply a context-level unibidirectional LSTM to learn context representations. BERT BASE (Devlin et al., 2019): Base version of Bert. It takes as input each utterance with its context as a single text. DialogueRNN (Majumder et al., 2019): It exploits three gated recurrent units (GRU) to capture speaker information, context and emotional information of the preceding utterances, respectively. KET (Zhong et al., 2019b): It’s the stat"
2020.coling-main.392,I17-1099,0,0.219139,"leviate the aforementioned confusion and thus further improve emotion recognition performance. • Experimental results show that our proposed KAITML model outperforms the state-of-the-art models across five benchmark datasets in F1 score. In addition, context, commonsense knowledge and multi-task learning are all beneficial to the emotion recognition performance. 2 Related Work Emotion recognition in conversations has grabbed much attention from researchers in the past few years due to the proliferation of publicly available conversational dataset (Poria et al., 2019a; Chatterjee et al., 2019; Li et al., 2017; Zhou et al., 2018a) and its widespread applications in opinion mining, recommender systems, emotion-aware dialogues generation, and so on (Poria et al., 2019b). Some of the deep learning-based models have been proposed for emotion recognition in conversations, in only textual and multimodal settings (containing textual, acoustic, and visual information). Poria et al. (2017) proposes a long short-term memory network (LSTM) (Hochreiter and Schmidhuber, 1997) based model to capture contextual correlations from the utterances of a user-generated video 4430 for multimodal sentiment classification"
2020.coling-main.392,P19-1002,0,0.181796,"Learning (KAITML) to address the aforementioned challenges. Firstly, we enhance the background and semantic information of the given utterance to facilitate ERTC with the retrieved relevant knowledge graphs from a large-scale commonsense knowledge base. Specifically, we propose a dual level graph attention mechanism to encode these relevant knowledge graphs, which consists of a node-level attention to learn the importance of different neighboring nodes and a relation-level attention to learn the importance of different relations to the current node. Then we apply the Incremental Transformer (Li et al., 2019) to incrementally encode multi-turn contextual utterances, which could capture the intra-utterance and interutterance correlations by the self-attention (Cheng et al., 2016) and context-attention (Zhang et al., 2018) modules, respectively. Moreover, we introduce multi-task learning to alleviate the confusion between a few non-neutral utterances and much more neutral ones, as shown in Table 1. Specifically, we first focus on the binary classification, “non-neutral” versus “neutral”, and then classifies the “non-neutral” ones into fine-grained emotion categories. These two auxiliary tasks are jo"
2020.coling-main.392,P18-1136,0,0.0295687,"re the shorter path of information flow in the self-attention (Cheng et al., 2016) and context-attention (Zhang et al., 2018) modules in the Incremental Transformer allows our model to exploit contextual information more efficiently. Recently, a considerable literature has grown up around the theme of incorporating external knowledge in generative conversation systems, including question answering systems (Hao et al., 2017; Mihaylov and Frank, 2018), open-domain dialogue systems (Young et al., 2018; Zhou et al., 2018b; Zhong et al., 2019a), and task-oriented dialogue systems (He et al., 2019; Madotto et al., 2018; Chen et al., 2019). Zhong et al. (2019b) proposes a Knowledge-Enriched Transformer (KET) achieving the state-ofthe-art performance on multiple textual conversation datasets, where contextual utterances are encoded using hierarchical self-attention and commonsense knowledge is incorporated using a context-aware affective graph attention mechanism. However, they ignore various relations in external knowledge base, which may cause the loss of semantic information. By contrast, our dual-level graph attention mechanism, can take advantage of the various relations in external knowledge base to bet"
2020.coling-main.392,P18-1076,0,0.0206393,"ts. Compared to these gated RNNs and CNNs based models, we apply the Incremental Transformer (Li et al., 2019) to incrementally encode multi-turn contextual utterances, where the shorter path of information flow in the self-attention (Cheng et al., 2016) and context-attention (Zhang et al., 2018) modules in the Incremental Transformer allows our model to exploit contextual information more efficiently. Recently, a considerable literature has grown up around the theme of incorporating external knowledge in generative conversation systems, including question answering systems (Hao et al., 2017; Mihaylov and Frank, 2018), open-domain dialogue systems (Young et al., 2018; Zhou et al., 2018b; Zhong et al., 2019a), and task-oriented dialogue systems (He et al., 2019; Madotto et al., 2018; Chen et al., 2019). Zhong et al. (2019b) proposes a Knowledge-Enriched Transformer (KET) achieving the state-ofthe-art performance on multiple textual conversation datasets, where contextual utterances are encoded using hierarchical self-attention and commonsense knowledge is incorporated using a context-aware affective graph attention mechanism. However, they ignore various relations in external knowledge base, which may cause"
2020.coling-main.392,D14-1162,0,0.0901632,"nowledge is incorporated using a contextaware affective graph attention mechanism. 4.3 Hyper-parameter Settings We use Adam optimizer (Kingma and Ba, 2015) to train our model with learning rate of 0.0001 and a batch size of 64 in all datasets. We set the class weights in cross-entropy loss as the ratio of the class distribution in the validation set to the class distribution in the training set for each dataset (Zhong et al., 2019b). Thus, we can tackle the mismatch in class distribution between validation set and training set. The initial token and node embeddings are pre-trained with GloVe (Pennington et al., 2014). The detailed hyper-parameter settings for KAITML are presented in Table 3. Dataset d p f M L h λ1 λ2 EC DailyDialogue MELD EmoryNLP IEMOCAP 300 300 300 100 300 400 400 400 200 400 1 3 1 1 1 2 6 6 6 6 2 3 1 2 1 4 4 4 4 4 1.0 1.0 1.0 0.9 0.9 0.7 1.0 0.7 0.5 0.6 Table 3: Hyper-parameter settings for KAITML. d: token/node embedding size. p: hidden size in FFN. f : minimum token frequency in vocabulary. M : context length. L: number of encoder layers. h: number of heads in MultiHead. λ1 , λ2 : weight coefficients of loss2 , loss3 , repectively. 5 Result Analysis Model EC DailyDialogue MELD EmoryN"
2020.coling-main.392,P17-1081,0,0.476433,"in conversations, and the heavily imbalanced class distribution can easily lead to the confusion between a few non-neutral utterances (e.g., happy, sad, and angry, etc.) and much more neutral ones (e.g., neutral or others), which restrains the emotion recognition performance. Table 1 shows a confusion matrix of emotion recognition results from the current state-of-the-art model and there appears a serious confusion between a few nonneutral categories and much more neutral category. Some prior studies have been conducted to model contextual information for emotion recognition in conversations (Poria et al., 2017; Majumder et al., 2019). These methods first adopt convolutional neural networks (CNN) to extract utterance-level features and then use context-level recurrent neural networks (RNN) to model the contextual utterances in conversation. However, RNN and CNN have difficulty modeling long-distance dependencies (Vaswani et al., 2017), which may be useful in ERTC. Zhong et al. (2019b) uses a context-aware affective graph attention mechanism to incorporate external knowledge for ERTC. However, they don’t consider various relations in external knowledge base, which may cause the loss of semantic infor"
2020.coling-main.392,P19-1050,0,0.440299,"f-the-art models across five benchmark datasets. 1 Introduction Emotion recognition in textual conversations (ERTC), which aims to identify the emotion of each utterance from the transcript of a conversation, has become a popular research topic in recent years. ERTC can be widely used in various scenarios, such as opinion mining of comments in social media (Chatterjee et al., 2019), emotion analysis of customers in artificial customer service, and others. In addition, it can also be applied to chat robots to analyze the user’s emotional state in real time and generate emotion-aware responses (Poria et al., 2019b; Zhou et al., 2018a; Huang et al., 2018). Truth Others Others Angry Sad Happy 4424 54 44 88 Prediction Angry Sad 101 237 11 0 60 6 192 2 Happy 92 1 3 194 Table 1: A confusion matrix of the emotion recognition results on the EmoContext test dataset (Chatterjee et al., 2019) from Knowledge-Enriched Transformer (Zhong et al., 2019b), which is the current stateof-the-art model. We notice that there are barely miss-classifications among the non-neutral categories (Angry, Sad, and Happy). Most of the errors, shown in the bold font, correspond to the confusion between a few non-neutral categories a"
2020.coling-main.392,N18-1194,0,0.0223296,"collection of hutterance, labeli pairs in a given conversation dataset, where N denotes the number of conversations and Ni denotes the number of utterances in the ith conversation. The objective of the task is to maximize the following function: arg max θ (i) where Xj Ni N Y Y (i) P(Yj (i) (i) (i) |Xj , Xj−1 , ..., Xj−M ; θ). (1) i=1 j=1 (i) denotes target utterance, Yj denotes the emotion label of target utterance, θ denotes the (i) (i) model parameters we need to optimize and Xj−1 , ..., Xj−M denote contextual utterances. Here, we limit the number of contextual utterances to M . We follow (Su et al., 2018; Zhong et al., 2019b) to directly discard early contextual utterances. Similar to (Zhong et al., 2019b; Poria et al., 2017), we clip (i) and pad each utterance Xj to a fixed K number of tokens. The overview of our KAITML model and detailed architecture of model components are presented in Figure 1. 3.2 Knowledge Interpreter Commonsense knowledge is fundamental to understanding conversations (Zhou et al., 2018b). We use ConceptNet (Speer et al., 2017) as a external commonsense knowledge base in our model. ConceptNet is a large-scale multilingual semantic graph where concepts are nodes in the g"
2020.coling-main.392,P12-2018,0,0.0101183,"f the emotion recognition results on the EmoContext test dataset (Chatterjee et al., 2019) from Knowledge-Enriched Transformer (Zhong et al., 2019b), which is the current stateof-the-art model. We notice that there are barely miss-classifications among the non-neutral categories (Angry, Sad, and Happy). Most of the errors, shown in the bold font, correspond to the confusion between a few non-neutral categories and much more neutral category (Others). However, there are several challenges when analyzing emotion in natural conversations. Firstly, unlike vanilla emotion recognition of sentences (Wang and Manning, 2012; Seyeditabari et al., 2018), ERTC requires comprehensively considering the context in the conversation. Secondly, knowledge plays an *The first two authors contributed equally. This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ Licence details: http:// 4429 Proceedings of the 28th International Conference on Computational Linguistics, pages 4429–4440 Barcelona, Spain (Online), December 8-13, 2020 important role in ERTC as speakers often express emotions relying on the context and commonsense knowledge (Zhong et al., 2019b"
2020.coling-main.392,D18-1049,0,0.345612,"a large-scale commonsense knowledge base. Specifically, we propose a dual level graph attention mechanism to encode these relevant knowledge graphs, which consists of a node-level attention to learn the importance of different neighboring nodes and a relation-level attention to learn the importance of different relations to the current node. Then we apply the Incremental Transformer (Li et al., 2019) to incrementally encode multi-turn contextual utterances, which could capture the intra-utterance and interutterance correlations by the self-attention (Cheng et al., 2016) and context-attention (Zhang et al., 2018) modules, respectively. Moreover, we introduce multi-task learning to alleviate the confusion between a few non-neutral utterances and much more neutral ones, as shown in Table 1. Specifically, we first focus on the binary classification, “non-neutral” versus “neutral”, and then classifies the “non-neutral” ones into fine-grained emotion categories. These two auxiliary tasks are jointly trained with the original emotion recognition task. In summary, this paper makes the following contributions: • We devise a dual-level graph attention mechanism to support better understanding of utterances for"
2020.coling-main.392,D19-1016,0,0.241884,"Missing"
2020.emnlp-main.275,D18-2029,0,0.0135609,"t training and the prior distribution k ∼ πθ (Kt ) at inference. Figure 1 clearly shows this discrepancy which will cause the decoder to have to generate with knowledge selected from the unfamiliar prior distribution. These issues lead to the gap between prior and posterior knowledge selection, which we try to deal with in this paper. Sentence Encoding. For any sentence sentt with Nw words at t-th turn, SKT uses a shared BERT (Devlin et al., 2019) to obtain the context aware word representations Hsent with d dims and t then converts them into the sentence representation hsent by mean pooling (Cer et al., 2018): t Hsent = BERT (sentt ) ∈ RNw ×d t .  hsent = Mean Hsent ∈ Rd t t (3) As a result, we obtain Hxt and hxt for the message kl xt , Hyt and hyt for the response2 yt , and Ht t and kl ht t for any knowledge sentence ktl ∈ Kt . Knowledge Selection. To utilize the dialogue history and selection history, two GRUs (Cho et al., 2014) are used to summarize them as corresponding states shist and skh t t with zero initialization:   d shist = GRUdial [hxt ; hyt ] , shist t t−1 ∈ R  sel  , (4) kt kh d skh t = GRUsel ht , st−1 ∈ R ksel where hxt , hyt and ht t are sentence vectors of message xt , resp"
2020.emnlp-main.275,K18-1048,0,0.0282681,"erage the given knowledge to enhance dialogue understanding and/or improve dialogue generation (Zhao et al., 2019b; Sun et al., 2020; Madotto et al., 2018; Chen et al., 2019; Yavuz et al., 2019; Tang and Hu, 2019; Li et al., 2019; Zheng and Zhou, 2019; Niu et al., 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompose this task into two subproblems: knowledge selection and response generation. This pipeline framework has been widely used for the open domain setting (Chen et al., 2017; Min et al., 2018; Nie et al., 2019) and shows promising performance with explicit use of knowledge in knowledge-grounded dialogue (Dinan et al., 2019). Knowledge selection plays an important role in open-domain knowledge-grounded dialogue (Di3426 Proceedings of the 2020 Conference on Empirical M"
2020.emnlp-main.275,P17-1171,0,0.0217147,"Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompose this task into two subproblems: knowledge selection and response generation. This pipeline framework has been widely used for the open domain setting (Chen et al., 2017; Min et al., 2018; Nie et al., 2019) and shows promising performance with explicit use of knowledge in knowledge-grounded dialogue (Dinan et al., 2019). Knowledge selection plays an important role in open-domain knowledge-grounded dialogue (Di3426 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3426–3437, c November 16–20, 2020. 2020 Association for Computational Linguistics nan et al., 2019) since the inappropriate knowledge selection may prevent the model from leveraging the knowledge accurately (Lian et al., 2019), or even lead to an inappropri"
2020.emnlp-main.275,P19-1258,1,0.851583,"p their selection decisions in turn. The prior knowledge selection only depends on context while the posterior knowledge selection means selection with context and response (Lian et al., 2019). Introduction Knowledge-grounded dialogue (Ghazvininejad et al., 2018) which leverages external knowledge to generate more informative responses, has become a popular research topic in recent years. Many researchers have studied how to effectively leverage the given knowledge to enhance dialogue understanding and/or improve dialogue generation (Zhao et al., 2019b; Sun et al., 2020; Madotto et al., 2018; Chen et al., 2019; Yavuz et al., 2019; Tang and Hu, 2019; Li et al., 2019; Zheng and Zhou, 2019; Niu et al., 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompose this task into tw"
2020.emnlp-main.275,N19-1423,0,0.100024,"see that the selected knowledge for response generation at training and inference is drawn from different distributions, i.e., the posterior distribution k ∼ qφ (Kt ) at training and the prior distribution k ∼ πθ (Kt ) at inference. Figure 1 clearly shows this discrepancy which will cause the decoder to have to generate with knowledge selected from the unfamiliar prior distribution. These issues lead to the gap between prior and posterior knowledge selection, which we try to deal with in this paper. Sentence Encoding. For any sentence sentt with Nw words at t-th turn, SKT uses a shared BERT (Devlin et al., 2019) to obtain the context aware word representations Hsent with d dims and t then converts them into the sentence representation hsent by mean pooling (Cer et al., 2018): t Hsent = BERT (sentt ) ∈ RNw ×d t .  hsent = Mean Hsent ∈ Rd t t (3) As a result, we obtain Hxt and hxt for the message kl xt , Hyt and hyt for the response2 yt , and Ht t and kl ht t for any knowledge sentence ktl ∈ Kt . Knowledge Selection. To utilize the dialogue history and selection history, two GRUs (Cho et al., 2014) are used to summarize them as corresponding states shist and skh t t with zero initialization:   d shi"
2020.emnlp-main.275,P16-1154,0,0.0375727,"on is based on the knowledge selected by the prior module, which is guided by the well-trained teacher, and we only update the green blocks at this stage. Finally, the knowledge ktsel is selected by sampling from the posterior distribution qφ (Kt ) = post at (Kt ) at training while selected with the highest probability over the prior distribution πθ (Kt ) = prior at (Kt ) at inference. Generation with Knowledge. SKT takes the concatenation of message xt and selected knowledge sentence ktsel as input and generates responses by the Transformer decoder (Vaswani et al., 2017) with copy mechanism (Gu et al., 2016). Though there are various models studying how to improve the generation quality based on the given knowledge, here, we simply follow the decoder of SKT and mainly focus on the knowledge selection issue. 3 Approach In this section, we show how to bridge the gap between prior and posterior knowledge selection in knowledge-grounded dialogue. Firstly, we design the Posterior Information Prediction Module (PIPM) to enhance the prior selection module with the necessary posterior information. Secondly, we design a Knowledge Distillation Based Training Strategy (KDBTS) to train the decoder with the k"
2020.emnlp-main.275,P84-1044,0,0.723364,"Missing"
2020.emnlp-main.275,P19-1002,1,0.84127,"election only depends on context while the posterior knowledge selection means selection with context and response (Lian et al., 2019). Introduction Knowledge-grounded dialogue (Ghazvininejad et al., 2018) which leverages external knowledge to generate more informative responses, has become a popular research topic in recent years. Many researchers have studied how to effectively leverage the given knowledge to enhance dialogue understanding and/or improve dialogue generation (Zhao et al., 2019b; Sun et al., 2020; Madotto et al., 2018; Chen et al., 2019; Yavuz et al., 2019; Tang and Hu, 2019; Li et al., 2019; Zheng and Zhou, 2019; Niu et al., 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompose this task into two subproblems: knowledge selection and response generati"
2020.emnlp-main.275,P18-1136,0,0.0236704,"nt responses which help their selection decisions in turn. The prior knowledge selection only depends on context while the posterior knowledge selection means selection with context and response (Lian et al., 2019). Introduction Knowledge-grounded dialogue (Ghazvininejad et al., 2018) which leverages external knowledge to generate more informative responses, has become a popular research topic in recent years. Many researchers have studied how to effectively leverage the given knowledge to enhance dialogue understanding and/or improve dialogue generation (Zhao et al., 2019b; Sun et al., 2020; Madotto et al., 2018; Chen et al., 2019; Yavuz et al., 2019; Tang and Hu, 2019; Li et al., 2019; Zheng and Zhou, 2019; Niu et al., 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompos"
2020.emnlp-main.275,P18-1160,0,0.0174108,"However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompose this task into two subproblems: knowledge selection and response generation. This pipeline framework has been widely used for the open domain setting (Chen et al., 2017; Min et al., 2018; Nie et al., 2019) and shows promising performance with explicit use of knowledge in knowledge-grounded dialogue (Dinan et al., 2019). Knowledge selection plays an important role in open-domain knowledge-grounded dialogue (Di3426 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3426–3437, c November 16–20, 2020. 2020 Association for Computational Linguistics nan et al., 2019) since the inappropriate knowledge selection may prevent the model from leveraging the knowledge accurately (Lian et al., 2019), or even lead to an inappropriate response. The"
2020.emnlp-main.275,D18-1255,0,0.46899,"jad et al., 2018) which leverages external knowledge to generate more informative responses, has become a popular research topic in recent years. Many researchers have studied how to effectively leverage the given knowledge to enhance dialogue understanding and/or improve dialogue generation (Zhao et al., 2019b; Sun et al., 2020; Madotto et al., 2018; Chen et al., 2019; Yavuz et al., 2019; Tang and Hu, 2019; Li et al., 2019; Zheng and Zhou, 2019; Niu et al., 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompose this task into two subproblems: knowledge selection and response generation. This pipeline framework has been widely used for the open domain setting (Chen et al., 2017; Min et al., 2018; Nie et al., 2019) and shows promising performance with explicit use of knowl"
2020.emnlp-main.275,D19-1258,0,0.0160588,"lly use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompose this task into two subproblems: knowledge selection and response generation. This pipeline framework has been widely used for the open domain setting (Chen et al., 2017; Min et al., 2018; Nie et al., 2019) and shows promising performance with explicit use of knowledge in knowledge-grounded dialogue (Dinan et al., 2019). Knowledge selection plays an important role in open-domain knowledge-grounded dialogue (Di3426 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3426–3437, c November 16–20, 2020. 2020 Association for Computational Linguistics nan et al., 2019) since the inappropriate knowledge selection may prevent the model from leveraging the knowledge accurately (Lian et al., 2019), or even lead to an inappropriate response. The example in Table 1"
2020.emnlp-main.275,D19-1187,0,0.0502611,"Missing"
2020.emnlp-main.275,W19-5917,0,0.0915067,"Missing"
2020.emnlp-main.275,D18-1073,0,0.0249398,"e to enhance dialogue understanding and/or improve dialogue generation (Zhao et al., 2019b; Sun et al., 2020; Madotto et al., 2018; Chen et al., 2019; Yavuz et al., 2019; Tang and Hu, 2019; Li et al., 2019; Zheng and Zhou, 2019; Niu et al., 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompose this task into two subproblems: knowledge selection and response generation. This pipeline framework has been widely used for the open domain setting (Chen et al., 2017; Min et al., 2018; Nie et al., 2019) and shows promising performance with explicit use of knowledge in knowledge-grounded dialogue (Dinan et al., 2019). Knowledge selection plays an important role in open-domain knowledge-grounded dialogue (Di3426 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Proce"
2020.emnlp-main.275,P18-1205,0,0.0287203,"dialogue (Ghazvininejad et al., 2018) which leverages external knowledge to generate more informative responses, has become a popular research topic in recent years. Many researchers have studied how to effectively leverage the given knowledge to enhance dialogue understanding and/or improve dialogue generation (Zhao et al., 2019b; Sun et al., 2020; Madotto et al., 2018; Chen et al., 2019; Yavuz et al., 2019; Tang and Hu, 2019; Li et al., 2019; Zheng and Zhou, 2019; Niu et al., 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompose this task into two subproblems: knowledge selection and response generation. This pipeline framework has been widely used for the open domain setting (Chen et al., 2017; Min et al., 2018; Nie et al., 2019) and shows promising performance with e"
2020.emnlp-main.275,P19-1539,0,0.0176836,"ich leverages external knowledge to generate more informative responses, has become a popular research topic in recent years. Many researchers have studied how to effectively leverage the given knowledge to enhance dialogue understanding and/or improve dialogue generation (Zhao et al., 2019b; Sun et al., 2020; Madotto et al., 2018; Chen et al., 2019; Yavuz et al., 2019; Tang and Hu, 2019; Li et al., 2019; Zheng and Zhou, 2019; Niu et al., 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompose this task into two subproblems: knowledge selection and response generation. This pipeline framework has been widely used for the open domain setting (Chen et al., 2017; Min et al., 2018; Nie et al., 2019) and shows promising performance with explicit use of knowledge in knowledge-g"
2020.emnlp-main.275,P19-1426,1,0.820016,"Missing"
2020.emnlp-main.275,N19-1123,0,0.304558,"nowledge sentences to generate different responses which help their selection decisions in turn. The prior knowledge selection only depends on context while the posterior knowledge selection means selection with context and response (Lian et al., 2019). Introduction Knowledge-grounded dialogue (Ghazvininejad et al., 2018) which leverages external knowledge to generate more informative responses, has become a popular research topic in recent years. Many researchers have studied how to effectively leverage the given knowledge to enhance dialogue understanding and/or improve dialogue generation (Zhao et al., 2019b; Sun et al., 2020; Madotto et al., 2018; Chen et al., 2019; Yavuz et al., 2019; Tang and Hu, 2019; Li et al., 2019; Zheng and Zhou, 2019; Niu et al., 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently"
2020.emnlp-main.275,P17-1061,0,0.121102,"ge selection in knowledge-grounded dialogue. Firstly, we design the Posterior Information Prediction Module (PIPM) to enhance the prior selection module with the necessary posterior information. Secondly, we design a Knowledge Distillation Based Training Strategy (KDBTS) to train the decoder with the knowledge selected from the prior distribution, removing the exposure bias of knowledge selection. 3.1 information for knowledge selection at inference. Following the typical setting in latent variable models (Lian et al., 2019; Kim et al., 2020), we use the response in bag-of-words (BOW) format (Zhao et al., 2017) as the posterior information. Here we take the dialogue context and the knowledge pool as input to generate the posterior information. We firstly summarize as the query of  histthe xcontext  this module qPI = s ; h and use it to get the t t t−1 PI attention distribution at (Kt ) over the knowledge pool Kt by Equation 6. Then, we summarize the knowledge representation in the knowledge pool with the weights in aPI t (Kt ) considered: h 1 i kt ktL d (7) hPI = h , · · · , h · aPI t t (Kt ) ∈ R . t t Secondly, we take the summarization of the dialogue context and the knowledge pool as input and"
2020.emnlp-main.275,D19-1193,0,0.0263512,"nowledge sentences to generate different responses which help their selection decisions in turn. The prior knowledge selection only depends on context while the posterior knowledge selection means selection with context and response (Lian et al., 2019). Introduction Knowledge-grounded dialogue (Ghazvininejad et al., 2018) which leverages external knowledge to generate more informative responses, has become a popular research topic in recent years. Many researchers have studied how to effectively leverage the given knowledge to enhance dialogue understanding and/or improve dialogue generation (Zhao et al., 2019b; Sun et al., 2020; Madotto et al., 2018; Chen et al., 2019; Yavuz et al., 2019; Tang and Hu, 2019; Li et al., 2019; Zheng and Zhou, 2019; Niu et al., 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently"
2021.ccl-1.23,N16-1016,0,0.048617,"Missing"
2021.ccl-1.23,W16-4308,0,0.0378073,"Missing"
2021.ccl-1.23,N18-2018,0,0.0309795,"Missing"
2021.ccl-1.23,H05-1067,0,0.122955,"Missing"
2021.ccl-1.23,D17-1051,0,0.0364723,"Missing"
2021.ccl-1.23,D15-1284,0,0.0576731,"Missing"
2021.naacl-main.156,N18-1100,0,0.226852,"u et al., 2019; Dong et al., 2020; Ma et al., 2020b). The application of deep learning in medicine requires adequate medical explanations for the result. Specific to the diagnosis of EMR, the model needs to provide the text description supporting the diagnosis results. 1 As shown in Figure 1, an irregular EMR is a document of disease-related information, including symptoms, history of the disease, preliminary examination results, and so on, which is disordered and sparse with meaningless noisy text. Existing methods provide explanation through medical entities (Yuan et al., 2020), text spans (Mullenbach et al., 2018) and the weights of external knowledge (Ma et al., 2018). The entity is critical to the diagnosis (Sha and Wang, 2017; Girardi et al., 2018), but for the medical explanation, it cannot provide specific information of symptoms (such as positive or negative). And the form of the span is too fragmented and lacks readability. Therefore, the clause as a more informative and readable representation is needed to be combined above the level of entities. Most of the previous methods provide reliable explanations for diagnosis by calculating the similarity with an external medical knowledge base (ICD2 a"
2021.naacl-main.156,2020.acl-demos.14,0,0.0134146,"e remaining data. This shows that our method can easily annotate largescale data. With these labels, we can gather the same types of features together in the feature space, thereby enhancing the model’s overall attention to important types of features. Please refer to Appendix B.2 for more details. 2.3 Input Encoder After building the multi-granularity graph for a medical record, each node in the graph contains a 5 https://github.com/daiyizheng123/Bert-BiLSTM-CRFsequence Xnode = [x1 , x2 , · · · , xn ] with n words, pytorch 6 which is tokenized by the tokenizer of BERT (DeWe recommend Stanza (Qi et al., 2020; Zhang et al., 2020) for English EMR. https://github.com/stanfordnlp/stanza vlin et al., 2019). In order to maintain the con1944 sistency of the results of different granularity encoding, we use one bi-directional RNN (Schuster and Paliwal, 1997) with GRU (Cho et al., 2014) to cover the sequence of sentences, clauses, entities and general information into hidden state sequence respectively H m = (h1 , h2 , · · · , hn ): Taking entity node classification as an example, for each entity node, we use a two-layer MLP with the ReLU activation function to calculate the probability. For an entity nod"
C02-2028,J87-1004,0,\N,Missing
C16-1216,P16-1014,0,0.181802,"y hurt the performance. Along the direction of that work, we believe that a joint learning model can achieve a better performance by designing a hierarchical architecture, with sentence-level and word-level components, which has shown promising results on document modeling (Lin et al., 2015) and document classification (Yang et al., 2016). Besides, rare and unknown word problem as an important issue should be considered in NLP tasks, especially for QA task, where the words that we are mainly interested in are usually named entities which are mostly unknown or rare words (Marrero et al., 2013; Gulcehre et al., 2016). In order to control the computational complexity, many methods limit the trained vocabulary size, which further leads to lots of low-frequency words outside the trained vocabulary (Li et al., 2016). Traditional methods directly mask the rare or unknown words with meaningless unk which may lose the important information for answer selection task. For example, given a set of sentences as follows: 1. Miss, what is your name? 2. Uh, my name is Wainwright. 3. Please tell me your passport number. 4. Ok, it is 899917359. Assume that the words Wainwright and 8999173591 are rare words or outside the"
C16-1216,D15-1106,0,0.0271092,"omponent uses MemNN to search the supporting sentences and Response component uses NMT or NTM to generate answer on the selected sentences. However, that work needs the supervision of the supporting facts to guide the training of Search component and the combination of these two components through a separate training way may hurt the performance. Along the direction of that work, we believe that a joint learning model can achieve a better performance by designing a hierarchical architecture, with sentence-level and word-level components, which has shown promising results on document modeling (Lin et al., 2015) and document classification (Yang et al., 2016). Besides, rare and unknown word problem as an important issue should be considered in NLP tasks, especially for QA task, where the words that we are mainly interested in are usually named entities which are mostly unknown or rare words (Marrero et al., 2013; Gulcehre et al., 2016). In order to control the computational complexity, many methods limit the trained vocabulary size, which further leads to lots of low-frequency words outside the trained vocabulary (Li et al., 2016). Traditional methods directly mask the rare or unknown words with mean"
C16-1216,P15-1152,0,0.0298163,"bert et al., 2011; Sutskever et al., 2014; Zeng et al., 2014; Feng et al., 2015). The main merits of these representation learning based methods are that they do not rely on any linguistic tools and can be applied to different languages or domains. However, the memory of these methods, such as Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Cho et al., 2014) compressing all the external sentences into a fixed-length vector, is typically too small to accurately remember facts from the past, and may lose important details for response generation (Shang et al., 2015). Due to the drawback, these traditional DNN models encounter great limitation on Question Answering (QA), as a complex NLP task, which requires deep understanding of semantic abstraction and reasoning over facts that are relevant to a question (Hermann et al., 2015; Yu et al., 2015). Recently, lots of deep learning methods with explicit memory and attention mechanism are explored for Question Answering (QA) task, such as Memory Networks (MemNN) (Sukhbaatar et al., 2015), Neural Machine Translation (NMT) and Neural Turing Machine (NTM) (Yu et al., 2015). These methods exploit a external memory"
C16-1216,N16-1174,0,0.0205715,"ntences and Response component uses NMT or NTM to generate answer on the selected sentences. However, that work needs the supervision of the supporting facts to guide the training of Search component and the combination of these two components through a separate training way may hurt the performance. Along the direction of that work, we believe that a joint learning model can achieve a better performance by designing a hierarchical architecture, with sentence-level and word-level components, which has shown promising results on document modeling (Lin et al., 2015) and document classification (Yang et al., 2016). Besides, rare and unknown word problem as an important issue should be considered in NLP tasks, especially for QA task, where the words that we are mainly interested in are usually named entities which are mostly unknown or rare words (Marrero et al., 2013; Gulcehre et al., 2016). In order to control the computational complexity, many methods limit the trained vocabulary size, which further leads to lots of low-frequency words outside the trained vocabulary (Li et al., 2016). Traditional methods directly mask the rare or unknown words with meaningless unk which may lose the important informa"
C16-1216,C14-1220,0,0.0407613,"memory to focus the words in the selected sentences. Finally, the prediction is jointly learned over the outputs of the sentence-level reasoning module and the word-level attention mechanism. The experimental results demonstrate that our approach successfully conducts answer selection on unknown words and achieves a better performance than memory networks. 1 Introduction With the recent resurgence of interest in Deep Neural Networks (DNN), many researchers have concentrated on using deep learning to solve natural language processing (NLP) tasks (Collobert et al., 2011; Sutskever et al., 2014; Zeng et al., 2014; Feng et al., 2015). The main merits of these representation learning based methods are that they do not rely on any linguistic tools and can be applied to different languages or domains. However, the memory of these methods, such as Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Cho et al., 2014) compressing all the external sentences into a fixed-length vector, is typically too small to accurately remember facts from the past, and may lose important details for response generation (Shang et al., 2015). Due to the drawback, these traditional"
C16-1288,P15-1001,0,0.034195,"most glaring limitations is that the NMT model is weak in handling the rare and out-of-vocabulary (OOV) words, since the NMT system usually uses the top-N (30000-50000) frequent words in the training corpus and regards other words as unseen words (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Cohn et al., 2016). Two different kinds of approaches have been proposed to handle the OOV problem in NMT: vocabulary-specific approaches and unit-specific approaches. The vocabulary-specific approaches seek to cover more words by using a larger vocabulary (Mnih and Kavukcuoglu, 2013; Cho et al., 2015) or using an identity translation dictionary in a post-processing step (Luong et al., 2014). Intuitively, these approaches can alleviate the OOV problems to a certain extent only if the vocabulary can be expanded large enough. However, these approaches are incapable of solving the OOV problems completely since the vocabulary is always limited. As opposed to vocabulary-specific approaches, the unit-specific approaches try to use more finegrained processing units than words, like sub-word units (Sennrich et al., 2015) or even character-level units (Ling et al., 2015b; Chung et al., 2016). Regard"
C16-1288,P16-1160,0,0.0913497,"lu, 2013; Cho et al., 2015) or using an identity translation dictionary in a post-processing step (Luong et al., 2014). Intuitively, these approaches can alleviate the OOV problems to a certain extent only if the vocabulary can be expanded large enough. However, these approaches are incapable of solving the OOV problems completely since the vocabulary is always limited. As opposed to vocabulary-specific approaches, the unit-specific approaches try to use more finegrained processing units than words, like sub-word units (Sennrich et al., 2015) or even character-level units (Ling et al., 2015b; Chung et al., 2016). Regarding the character as the basic processing unit is a new trend in the field of NLP and the character-level models have been widely used in NLP tasks (Ling et al., 2015a; Zhang et al., 2015; Golub and He, 2016). Developing character-level NMT models is attractive for multiple reasons. Firstly, it opens the possibility for models to generate unseen source words, since each word can be composed from different characters. Secondly, the vocabulary size of the * Wei Chen is the corresponding author of this paper This work is licensed under a Creative Commons Attribution 4.0 International Lice"
C16-1288,D16-1166,0,0.0235837,"abulary can be expanded large enough. However, these approaches are incapable of solving the OOV problems completely since the vocabulary is always limited. As opposed to vocabulary-specific approaches, the unit-specific approaches try to use more finegrained processing units than words, like sub-word units (Sennrich et al., 2015) or even character-level units (Ling et al., 2015b; Chung et al., 2016). Regarding the character as the basic processing unit is a new trend in the field of NLP and the character-level models have been widely used in NLP tasks (Ling et al., 2015a; Zhang et al., 2015; Golub and He, 2016). Developing character-level NMT models is attractive for multiple reasons. Firstly, it opens the possibility for models to generate unseen source words, since each word can be composed from different characters. Secondly, the vocabulary size of the * Wei Chen is the corresponding author of this paper This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ Licence details: http:// 3063 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 3063–3070, Osaka, Japan, Dec"
C16-1288,D13-1176,0,0.0758547,"between each word (as the whitespace boundaries in English), it is also applied to languages without explicit word segmentations (like Chinese). Experimental results on Chinese-English translation tasks show that the proposed character-aware NMT model can achieve comparable translation performance with the traditional word based NMT models. Despite the target side is still word based, the proposed model is able to generate much less unknown words. 1 Introduction Neural machine translation conducts end-to-end translation with a source encoder and a target decoder, producing promising results (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014). With the emerging of the attention-based encoder-decoder model, NMT has achieved comparable even better translation performance with the traditional statistical machine translation (SMT) (Bahdanau et al., 2014; Ranzato et al., 2015; Shen et al., 2015; Tu et al., 2016). The success of NMT lies in its strong ability of composing the global context information. However, as a newly approach, the NMT model has some flaws and limitations that may jeopardize its translation performance (Luong et al., 2014; Sennrich et al., 2015; He et al., 2016). One of th"
C16-1288,P16-1100,0,0.0343863,"ings are able to get full trained. Hence they represent their corresponding characters very well. However, in the word-based NMT, the word embeddings for rare words, which hardly occur in the training corpus, are absent of enough training. Based on the state-of-the-art attention based encoderdecoder framework, some character-level NMT models have been proposed recently. (Chung et al., 2016) focus on representing the target side as a character sequence with a bi-scale recurrent neural network. In (Ling et al., 2015b), a character-based word representation model is proposed in the source side. (Luong and Manning, 2016) proposes a hybrid architecture for NMT that translates mostly at the word level and consults the character components for rare words when necessary. Most of the works mentioned above apply the bidirectional RNN to compose the word representation from its characters. Hence, its necessary to know the boundary between two words beforehand. These models are applicable for languages in which the words are segmented with explicit boundaries, such as English, French and etc. In this work, we propose a novel character-aware NMT model that learns to encode at the character level. On the use of row con"
C16-1288,P16-5005,0,0.0195369,"ased NMT models. Despite the target side is still word based, the proposed model is able to generate much less unknown words. 1 Introduction Neural machine translation conducts end-to-end translation with a source encoder and a target decoder, producing promising results (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014). With the emerging of the attention-based encoder-decoder model, NMT has achieved comparable even better translation performance with the traditional statistical machine translation (SMT) (Bahdanau et al., 2014; Ranzato et al., 2015; Shen et al., 2015; Tu et al., 2016). The success of NMT lies in its strong ability of composing the global context information. However, as a newly approach, the NMT model has some flaws and limitations that may jeopardize its translation performance (Luong et al., 2014; Sennrich et al., 2015; He et al., 2016). One of the most glaring limitations is that the NMT model is weak in handling the rare and out-of-vocabulary (OOV) words, since the NMT system usually uses the top-N (30000-50000) frequent words in the training corpus and regards other words as unseen words (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014"
C16-1329,D16-1053,0,0.018708,"the four models on different length of sentences. In the figure, the x-axis represents sentence lengths and y-axis is accuracy. The sentences collected in test set are no 3491 NN ReNN CNN RNN Other ours Model RNTN (Socher et al., 2013) DRNN (Irsoy and Cardie, 2014) DCNN (Kalchbrenner et al., 2014) CNN-non-static (Kim, 2014) CNN-MC (Kim, 2014) TBCNN(Mou et al., 2015) Molding-CNN (Lei et al., 2015) CNN-Ana (Zhang and Wallace, 2015) MVCNN (Yin and Sch¨utze, 2016) RCNN (Lai et al., 2015) S-LSTM (Zhu et al., 2015) LSTM (Tai et al., 2015) BLSTM (Tai et al., 2015) Tree-LSTM (Tai et al., 2015) LSTMN (Cheng et al., 2016) Multi-Task (Liu et al., 2016) PV (Le and Mikolov, 2014) DAN (Iyyer et al., 2015) combine-skip (Kiros et al., 2015) AdaSent (Zhao et al., 2015) LSTM-RNN (Le and Zuidema, 2015) C-LSTM (Zhou et al., 2015) DSCNN (Zhang et al., 2016) BLSTM BLSTM-Att BLSTM-2DPooling BLSTM-2DCNN SST-1 45.7 49.8 48.5 48.0 47.4 51.4 51.2 45.98 49.6 47.21 46.4 49.1 51.0 49.3 49.6 48.7 48.2 49.9 49.2 49.7 49.1 49.8 50.5 52.4 SST-2 85.4 86.6 86.8 87.2 88.1 87.9 88.6 85.45 89.4 81.9 84.9 87.5 88.0 87.3 87.9 87.8 86.8 88.0 87.8 89.1 87.6 88.2 88.3 89.5 Subj 93.4 93.2 93.66 93.9 94.1 93.6 95.5 93.2 92.1 93.5 93.7 94.0 TREC"
C16-1329,P15-1162,0,0.0247746,"Missing"
C16-1329,P14-1062,0,0.734286,"Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ Licence details: http:// 3485 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 3485–3495, Osaka, Japan, December 11-17 2016. the time-step dimension of the matrix, to obtain a fixed-length vector. Both of the two operators ignore features on the feature vector dimension, which maybe important for sentence representation, therefore the use of 1D max pooling and attention-based operators may pose a serious limitation. Convolutional Neural Networks (CNN) (Kalchbrenner et al., 2014; Kim, 2014) utilizes 1D convolution to perform the feature mapping, and then applies 1D max pooling operation over the time-step dimension to obtain a fixed-length output. However the elements in the matrix learned by RNN are not independent, as RNN reads a sentence word by word, one can effectively treat the matrix as an ’image’. Unlike in NLP, CNN in image processing tasks (LeCun et al., 1998; Krizhevsky et al., 2012) applies 2D convolution and 2D pooling operation to get a representation of the input. It is a good choice to utilize 2D convolution and 2D pooling to sample more meaningful fe"
C16-1329,D14-1181,0,0.221224,"al Licence. creativecommons.org/licenses/by/4.0/ Licence details: http:// 3485 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 3485–3495, Osaka, Japan, December 11-17 2016. the time-step dimension of the matrix, to obtain a fixed-length vector. Both of the two operators ignore features on the feature vector dimension, which maybe important for sentence representation, therefore the use of 1D max pooling and attention-based operators may pose a serious limitation. Convolutional Neural Networks (CNN) (Kalchbrenner et al., 2014; Kim, 2014) utilizes 1D convolution to perform the feature mapping, and then applies 1D max pooling operation over the time-step dimension to obtain a fixed-length output. However the elements in the matrix learned by RNN are not independent, as RNN reads a sentence word by word, one can effectively treat the matrix as an ’image’. Unlike in NLP, CNN in image processing tasks (LeCun et al., 1998; Krizhevsky et al., 2012) applies 2D convolution and 2D pooling operation to get a representation of the input. It is a good choice to utilize 2D convolution and 2D pooling to sample more meaningful features on bo"
C16-1329,S15-1002,0,0.107059,"eNN CNN RNN Other ours Model RNTN (Socher et al., 2013) DRNN (Irsoy and Cardie, 2014) DCNN (Kalchbrenner et al., 2014) CNN-non-static (Kim, 2014) CNN-MC (Kim, 2014) TBCNN(Mou et al., 2015) Molding-CNN (Lei et al., 2015) CNN-Ana (Zhang and Wallace, 2015) MVCNN (Yin and Sch¨utze, 2016) RCNN (Lai et al., 2015) S-LSTM (Zhu et al., 2015) LSTM (Tai et al., 2015) BLSTM (Tai et al., 2015) Tree-LSTM (Tai et al., 2015) LSTMN (Cheng et al., 2016) Multi-Task (Liu et al., 2016) PV (Le and Mikolov, 2014) DAN (Iyyer et al., 2015) combine-skip (Kiros et al., 2015) AdaSent (Zhao et al., 2015) LSTM-RNN (Le and Zuidema, 2015) C-LSTM (Zhou et al., 2015) DSCNN (Zhang et al., 2016) BLSTM BLSTM-Att BLSTM-2DPooling BLSTM-2DCNN SST-1 45.7 49.8 48.5 48.0 47.4 51.4 51.2 45.98 49.6 47.21 46.4 49.1 51.0 49.3 49.6 48.7 48.2 49.9 49.2 49.7 49.1 49.8 50.5 52.4 SST-2 85.4 86.6 86.8 87.2 88.1 87.9 88.6 85.45 89.4 81.9 84.9 87.5 88.0 87.3 87.9 87.8 86.8 88.0 87.8 89.1 87.6 88.2 88.3 89.5 Subj 93.4 93.2 93.66 93.9 94.1 93.6 95.5 93.2 92.1 93.5 93.7 94.0 TREC 93.0 93.6 92 96.0 91.37 92.2 92.4 94.6 95.4 93.0 93.8 94.8 96.1 MR 81.02 76.5 83.1 81.5 80.0 81.0 81.5 82.3 20Ng 96.49 94.0 94.6 95.5 96.5 Table 2: Classification results on s"
C16-1329,D15-1180,0,0.05452,"ive for modeling sentence, even document. To better understand the effect of 2D operations, this work conducts a sensitivity analysis on SST-1 dataset. 5.2 Effect of Sentence Length Figure 2 depicts the performance of the four models on different length of sentences. In the figure, the x-axis represents sentence lengths and y-axis is accuracy. The sentences collected in test set are no 3491 NN ReNN CNN RNN Other ours Model RNTN (Socher et al., 2013) DRNN (Irsoy and Cardie, 2014) DCNN (Kalchbrenner et al., 2014) CNN-non-static (Kim, 2014) CNN-MC (Kim, 2014) TBCNN(Mou et al., 2015) Molding-CNN (Lei et al., 2015) CNN-Ana (Zhang and Wallace, 2015) MVCNN (Yin and Sch¨utze, 2016) RCNN (Lai et al., 2015) S-LSTM (Zhu et al., 2015) LSTM (Tai et al., 2015) BLSTM (Tai et al., 2015) Tree-LSTM (Tai et al., 2015) LSTMN (Cheng et al., 2016) Multi-Task (Liu et al., 2016) PV (Le and Mikolov, 2014) DAN (Iyyer et al., 2015) combine-skip (Kiros et al., 2015) AdaSent (Zhao et al., 2015) LSTM-RNN (Le and Zuidema, 2015) C-LSTM (Zhou et al., 2015) DSCNN (Zhang et al., 2016) BLSTM BLSTM-Att BLSTM-2DPooling BLSTM-2DCNN SST-1 45.7 49.8 48.5 48.0 47.4 51.4 51.2 45.98 49.6 47.21 46.4 49.1 51.0 49.3 49.6 48.7 48.2 49.9 49.2 49."
C16-1329,C02-1150,0,0.102784,"assify a review as fine-grained labels (very negative, negative, neutral, positive, very positive). • SST-2: Same as SST-1 but with neutral reviews removed and binary labels (negative, positive). For both experiments, phrases and sentences are used to train the model, but only sentences are scored at test time (Socher et al., 2013; Le and Mikolov, 2014). Thus the training set is an order of magnitude larger than listed in table 1. • Subj4 : Subjectivity dataset (Pang and Lee, 2004). The task is to classify a sentence as being subjective or objective. • TREC5 : Question classification dataset (Li and Roth, 2002). The task involves classifying a question into 6 question types (abbreviation, description, entity, human, location, numeric value). • 20Newsgroups6 : The 20Ng dataset contains messages from twenty newsgroups. We use the bydate version preprocessed by Cachopo (2007). We select four major categories (comp, politics, rec and religion) followed by Hingmire et al. (2013). 4.2 Word Embeddings The word embeddings are pre-trained on much larger unannotated corpora to achieve better generalization given limited amount of training data (Turian et al., 2010). In particular, our experiments utilize the"
C16-1329,D15-1279,0,0.117965,"oling operation are more effective for modeling sentence, even document. To better understand the effect of 2D operations, this work conducts a sensitivity analysis on SST-1 dataset. 5.2 Effect of Sentence Length Figure 2 depicts the performance of the four models on different length of sentences. In the figure, the x-axis represents sentence lengths and y-axis is accuracy. The sentences collected in test set are no 3491 NN ReNN CNN RNN Other ours Model RNTN (Socher et al., 2013) DRNN (Irsoy and Cardie, 2014) DCNN (Kalchbrenner et al., 2014) CNN-non-static (Kim, 2014) CNN-MC (Kim, 2014) TBCNN(Mou et al., 2015) Molding-CNN (Lei et al., 2015) CNN-Ana (Zhang and Wallace, 2015) MVCNN (Yin and Sch¨utze, 2016) RCNN (Lai et al., 2015) S-LSTM (Zhu et al., 2015) LSTM (Tai et al., 2015) BLSTM (Tai et al., 2015) Tree-LSTM (Tai et al., 2015) LSTMN (Cheng et al., 2016) Multi-Task (Liu et al., 2016) PV (Le and Mikolov, 2014) DAN (Iyyer et al., 2015) combine-skip (Kiros et al., 2015) AdaSent (Zhao et al., 2015) LSTM-RNN (Le and Zuidema, 2015) C-LSTM (Zhou et al., 2015) DSCNN (Zhang et al., 2016) BLSTM BLSTM-Att BLSTM-2DPooling BLSTM-2DCNN SST-1 45.7 49.8 48.5 48.0 47.4 51.4 51.2 45.98 49.6 47.21 46.4 49.1 51.0 49"
C16-1329,P04-1035,0,0.0268541,"t positive/negative reviews. • SST-13 : Stanford Sentiment Treebank is an extension of MR from Socher et al. (2013). The aim is to classify a review as fine-grained labels (very negative, negative, neutral, positive, very positive). • SST-2: Same as SST-1 but with neutral reviews removed and binary labels (negative, positive). For both experiments, phrases and sentences are used to train the model, but only sentences are scored at test time (Socher et al., 2013; Le and Mikolov, 2014). Thus the training set is an order of magnitude larger than listed in table 1. • Subj4 : Subjectivity dataset (Pang and Lee, 2004). The task is to classify a sentence as being subjective or objective. • TREC5 : Question classification dataset (Li and Roth, 2002). The task involves classifying a question into 6 question types (abbreviation, description, entity, human, location, numeric value). • 20Newsgroups6 : The 20Ng dataset contains messages from twenty newsgroups. We use the bydate version preprocessed by Cachopo (2007). We select four major categories (comp, politics, rec and religion) followed by Hingmire et al. (2013). 4.2 Word Embeddings The word embeddings are pre-trained on much larger unannotated corpora to ac"
C16-1329,P05-1015,0,0.109875,"2 836 test 2210 1821 CV 500 CV 5563 |V | 17836 16185 21057 9137 20191 51379 |Vpre | 12745 11490 17671 5990 16746 30575 Table 1: Summary statistics for the datasets. c: number of target classes, l: average sentence length, m: maximum sentence length, train/dev/test: train/development/test set size, |V |: vocabulary size, |Vpre |: number of words present in the set of pre-trained word embeddings, CV: 10-fold cross validation. 4 Experimental Setup 4.1 Datasets The proposed models are tested on six datasets. Summary statistics of the datasets are in Table 1. • MR2 : Sentence polarity dataset from Pang and Lee (2005). The task is to detect positive/negative reviews. • SST-13 : Stanford Sentiment Treebank is an extension of MR from Socher et al. (2013). The aim is to classify a review as fine-grained labels (very negative, negative, neutral, positive, very positive). • SST-2: Same as SST-1 but with neutral reviews removed and binary labels (negative, positive). For both experiments, phrases and sentences are used to train the model, but only sentences are scored at test time (Socher et al., 2013; Le and Mikolov, 2014). Thus the training set is an order of magnitude larger than listed in table 1. • Subj4 :"
C16-1329,D14-1162,0,0.118483,"Missing"
C16-1329,D13-1170,0,0.399526,"meaningful information of the matrix. Experiments are conducted on six text classification tasks, including sentiment analysis, question classification, subjectivity classification and newsgroup classification. Compared with the state-of-the-art models, the proposed models achieve excellent performance on 4 out of 6 tasks. Specifically, one of the proposed models achieves highest accuracy on Stanford Sentiment Treebank binary classification and fine-grained classification tasks. 1 Introduction Text classification is an essential component in many NLP applications, such as sentiment analysis (Socher et al., 2013), relation extraction (Zeng et al., 2014) and spam detection (Wang, 2010). Therefore, it has attracted considerable attention from many researchers, and various types of models have been proposed. As a traditional method, the bag-of-words (BoW) model treats texts as unordered sets of words (Wang and Manning, 2012). In this way, however, it fails to encode word order and syntactic feature. Recently, order-sensitive models based on neural networks have achieved tremendous success for text classification, and shown more significant progress compared with BoW models. The challenge for textual mode"
C16-1329,P15-1150,0,0.59841,"uents based on the parsing tree. Irsoy and Cardie 1 To avoid confusion with RNN, we named Recursive Neural Networks as RecNN. 3486 (2014) proposed deep recursive neural network, which is constructed by stacking multiple recursive layers on top of each other, to modeling sentence. Recurrent Neural Networks: RNN has obtained much attention because of their superior ability to preserve sequence information over time. Tang et al. (2015) developed target dependent Long ShortTerm Memory Networks (LSTM (Hochreiter and Schmidhuber, 1997)), where target information is automatically taken into account. Tai et al. (2015) generalized LSTM to Tree-LSTM where each LSTM unit gains information from its children units. Zhou et al. (2016) introduced BLSTM with attention mechanism to automatically select features that have a decisive effect on classification. Yang et al. (2016) introduced a hierarchical network with two levels of attention mechanisms, which are word attention and sentence attention, for document classification. This paper also implements an attention-based model BLSTM-Att like the model in Zhou et al. (2016). Convolution Neural Networks: CNN (LeCun et al., 1998) is a feedforward neural network with 2"
C16-1329,P10-1040,0,0.0218286,"Missing"
C16-1329,P12-2018,0,0.029435,"t of 6 tasks. Specifically, one of the proposed models achieves highest accuracy on Stanford Sentiment Treebank binary classification and fine-grained classification tasks. 1 Introduction Text classification is an essential component in many NLP applications, such as sentiment analysis (Socher et al., 2013), relation extraction (Zeng et al., 2014) and spam detection (Wang, 2010). Therefore, it has attracted considerable attention from many researchers, and various types of models have been proposed. As a traditional method, the bag-of-words (BoW) model treats texts as unordered sets of words (Wang and Manning, 2012). In this way, however, it fails to encode word order and syntactic feature. Recently, order-sensitive models based on neural networks have achieved tremendous success for text classification, and shown more significant progress compared with BoW models. The challenge for textual modeling is how to capture features for different text units, such as phrases, sentences and documents. Benefiting from its recurrent structure, RNN, as an alternative type of neural networks, is very suitable to process the variable-length text. RNN can capitalize on distributed representations of words by first conv"
C16-1329,N16-1174,0,0.0407709,"ther, to modeling sentence. Recurrent Neural Networks: RNN has obtained much attention because of their superior ability to preserve sequence information over time. Tang et al. (2015) developed target dependent Long ShortTerm Memory Networks (LSTM (Hochreiter and Schmidhuber, 1997)), where target information is automatically taken into account. Tai et al. (2015) generalized LSTM to Tree-LSTM where each LSTM unit gains information from its children units. Zhou et al. (2016) introduced BLSTM with attention mechanism to automatically select features that have a decisive effect on classification. Yang et al. (2016) introduced a hierarchical network with two levels of attention mechanisms, which are word attention and sentence attention, for document classification. This paper also implements an attention-based model BLSTM-Att like the model in Zhou et al. (2016). Convolution Neural Networks: CNN (LeCun et al., 1998) is a feedforward neural network with 2D convolution layers and 2D pooling layers, originally developed for image processing. Then CNN is applied to NLP tasks, such as sentence classification (Kalchbrenner et al., 2014; Kim, 2014), and relation classification (Zeng et al., 2014). The differen"
C16-1329,C14-1220,0,0.0487993,"riments are conducted on six text classification tasks, including sentiment analysis, question classification, subjectivity classification and newsgroup classification. Compared with the state-of-the-art models, the proposed models achieve excellent performance on 4 out of 6 tasks. Specifically, one of the proposed models achieves highest accuracy on Stanford Sentiment Treebank binary classification and fine-grained classification tasks. 1 Introduction Text classification is an essential component in many NLP applications, such as sentiment analysis (Socher et al., 2013), relation extraction (Zeng et al., 2014) and spam detection (Wang, 2010). Therefore, it has attracted considerable attention from many researchers, and various types of models have been proposed. As a traditional method, the bag-of-words (BoW) model treats texts as unordered sets of words (Wang and Manning, 2012). In this way, however, it fails to encode word order and syntactic feature. Recently, order-sensitive models based on neural networks have achieved tremendous success for text classification, and shown more significant progress compared with BoW models. The challenge for textual modeling is how to capture features for diffe"
C16-1329,N16-1177,0,0.14004,"as 2D convolution. The details will be described in Section 3.2. Other Neural Networks: In addition to the models described above, lots of other neural networks have been proposed for text classification. Iyyer et al. (2015) introduced a deep averaging network, which fed an unweighted average of word embeddings through multiple hidden layers before classification. Zhou et al. (2015) used CNN to extract a sequence of higher-level phrase representations, then the representations were fed into a LSTM to obtain the sentence representation. The proposed model BLSTM-2DCNN is most relevant to DSCNN (Zhang et al., 2016) and RCNN (Wen et al., 2016). The difference is that the former two utilize LSTM, bidirectional RNN respectively, while this work applies BLSTM, to capture long-term sentence dependencies. After that the former two both apply 1D convolution and 1D max pooling operation, while this paper uses 2D convolution and 2D max pooling operation, to obtain the whole sentence representation. 3 Model As shown in Figure 1, the overall model consists of four parts: BLSTM Layer, Two-dimensional Convolution Layer, Two dimensional max pooling Layer, and Output Layer. The details of different components are desc"
C16-1329,P16-2034,1,0.0508007,"units, such as phrases, sentences and documents. Benefiting from its recurrent structure, RNN, as an alternative type of neural networks, is very suitable to process the variable-length text. RNN can capitalize on distributed representations of words by first converting the tokens comprising each text into vectors, which form a matrix. This matrix includes two dimensions: the time-step dimension and the feature vector dimension, and it will be updated in the process of learning feature representation. Then RNN utilizes 1D max pooling operation (Lai et al., 2015) or attention-based operation (Zhou et al., 2016), which extracts maximum values or generates a weighted representation over ∗ Correspondence author: zhenyu.qi@ia.ac.cn This work is licenced under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ Licence details: http:// 3485 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 3485–3495, Osaka, Japan, December 11-17 2016. the time-step dimension of the matrix, to obtain a fixed-length vector. Both of the two operators ignore features on the feature vector dimension, which maybe import"
C18-1299,1983.tc-1.13,0,0.518702,"Missing"
C18-1299,2013.iwslt-papers.12,0,0.618873,"Missing"
C18-1299,N15-1029,0,0.641901,"Missing"
C18-1299,N16-1162,0,0.0784726,"Missing"
C18-1299,Q14-1011,0,0.593825,"Missing"
C18-1299,P04-1005,0,0.515818,"Missing"
C18-1299,N13-1102,0,0.296775,"Missing"
C18-1299,D13-1013,0,0.638382,"Missing"
C18-1299,C16-1011,0,0.0503307,"Missing"
C18-1299,C16-1027,0,0.552779,"Missing"
C18-1299,D17-1296,0,0.443129,"Missing"
C18-1299,P15-1048,0,0.62879,"Missing"
D07-1098,W06-2920,0,0.100051,"e probability of the whole dependency tree. The tree with the maximal probability is outputted. The experiments are carried on 10 languages, and the results show that our probabilistic parsing action models outperform the original deterministic dependency parser. 1 Bo Xu Institute of Automation, Chinese Academy of Sciences xubo@hitic.ia.ac.cn 2 Introduction The target of CoNLL 2007 shared task (Nivre et al., 2007) is to parse texts in multiple languages by using a single dependency parser that has the capacity to learn from treebank data. Among parsers participating in CoNLL 2006 shared task (Buchholz et al., 2006), deterministic dependency parser shows great efficiency in time and comparable performances for multi-lingual dependency parsing (Nivre et al., 2006). Deterministic parser regards parsing as a sequence of parsing actions that are taken step by step on the input sentence. Parsing actions construct dependency relations between words. Deterministic dependency parser does not score the entire dependency tree as most of state-of-theart parsers. They only stepwisely choose the most probable parsing action. In this paper, to globally Introduction of Deterministic Dependency Parsing There are mainly"
D07-1098,W03-3017,0,0.138064,"nces for multi-lingual dependency parsing (Nivre et al., 2006). Deterministic parser regards parsing as a sequence of parsing actions that are taken step by step on the input sentence. Parsing actions construct dependency relations between words. Deterministic dependency parser does not score the entire dependency tree as most of state-of-theart parsers. They only stepwisely choose the most probable parsing action. In this paper, to globally Introduction of Deterministic Dependency Parsing There are mainly two representative deterministic dependency parsing algorithms proposed respectively by Nivre (2003), Yamada and Matsumoto (2003). Here we briefly introduce Yamada and Matsumoto’s algorithm, which is adopted by our models, to illustrate deterministic dependency parsing. The other representative method of Nivre also parses sentences in a similar deterministic manner except different data structure and parsing actions. Yamada’s method originally focuses on unlabeled dependency parsing. Three kinds of parsing actions are applied to construct the dependency between two focus words. The two focus words are the current sub tree’s root and the succeeding (right) sub tree’s root given the current pa"
D07-1098,P05-1013,0,0.0868862,"Missing"
D07-1098,W06-2933,0,0.0293614,"lts show that our probabilistic parsing action models outperform the original deterministic dependency parser. 1 Bo Xu Institute of Automation, Chinese Academy of Sciences xubo@hitic.ia.ac.cn 2 Introduction The target of CoNLL 2007 shared task (Nivre et al., 2007) is to parse texts in multiple languages by using a single dependency parser that has the capacity to learn from treebank data. Among parsers participating in CoNLL 2006 shared task (Buchholz et al., 2006), deterministic dependency parser shows great efficiency in time and comparable performances for multi-lingual dependency parsing (Nivre et al., 2006). Deterministic parser regards parsing as a sequence of parsing actions that are taken step by step on the input sentence. Parsing actions construct dependency relations between words. Deterministic dependency parser does not score the entire dependency tree as most of state-of-theart parsers. They only stepwisely choose the most probable parsing action. In this paper, to globally Introduction of Deterministic Dependency Parsing There are mainly two representative deterministic dependency parsing algorithms proposed respectively by Nivre (2003), Yamada and Matsumoto (2003). Here we briefly int"
D07-1098,W03-3023,0,0.720096,"-lingual dependency parsing (Nivre et al., 2006). Deterministic parser regards parsing as a sequence of parsing actions that are taken step by step on the input sentence. Parsing actions construct dependency relations between words. Deterministic dependency parser does not score the entire dependency tree as most of state-of-theart parsers. They only stepwisely choose the most probable parsing action. In this paper, to globally Introduction of Deterministic Dependency Parsing There are mainly two representative deterministic dependency parsing algorithms proposed respectively by Nivre (2003), Yamada and Matsumoto (2003). Here we briefly introduce Yamada and Matsumoto’s algorithm, which is adopted by our models, to illustrate deterministic dependency parsing. The other representative method of Nivre also parses sentences in a similar deterministic manner except different data structure and parsing actions. Yamada’s method originally focuses on unlabeled dependency parsing. Three kinds of parsing actions are applied to construct the dependency between two focus words. The two focus words are the current sub tree’s root and the succeeding (right) sub tree’s root given the current parsing state. Every parsing st"
D07-1098,W07-2416,0,0.0284177,"ent score (UAS) and labeled accuracy (LA). Table 1 shows that Parsing Action Chain Model (PACM) outperform original Yamada’s parsing method for all languages. The LAS improvements range from 0.60 percentage points to 1.71 percentage points. Note that the original Yamada’s method still gives testing results above the official reported average performance of all languages. Experiments and Results Experiments are carried on 10 languages provided by CoNLL 2007 shared-task organizers (Nivre et al., 2007). Among these languages, Chinese (Chen et al., 2003), Catalan (Martí et al., 2007) and English (Johansson and Nugues, 2007) have low perAra Bas Cat Chi Cze Eng Gre Hun Ita Tur LASYam 69.31 69.67 83.26 81.88 74.63 84.81 72.75 76.24 80.08 73.94 UASYam 78.93 75.86 88.53 86.17 80.11 85.83 79.45 79.97 83.69 79.79 LAYam 81.13 75.71 88.36 84.56 82.10 89.71 82.58 88.37 86.93 80.81 LAS PACM 69.91 71.26 84.95 82.58 75.34 85.83 74.29 77.06 80.75 75.03 UAS PACM 79.04 77.57 89.71 86.88 80.82 86.97 80.77 80.66 84.20 81.03 LAPACM 81.40 77.35 89.55 85.35 83.17 90.57 83.87 88.92 87.32 81.17 Table 1. The performances of Yamada’s method (Yam) and Parsing Action Chain Model (PACM). 943 4.2 Results of PAPM Not all languages have only"
D07-1098,J93-2004,0,0.0282222,"Missing"
D07-1098,D07-1096,0,\N,Missing
D12-1047,P11-2078,0,0.108536,"lead data selection process (Xu et al., 2001; Tam et al., 2006 and 2007; Wei and Pal, 2010), and thus take noisy data into the selected training data, which causes noisy proliferation and degrades the performance of adapted LM. Furthermore, traditional approaches for LM adaptation are based on bag-of-words models and considered to be context independent, despite of their state-of-the-art performance, such as TF-IDF (Eck et al., 2004; Zhao et al., 2004; Hildebrand et al., 2005; Kim, 2005; Foster and Kuhn, 2007), centroid similarity (Masskey and Sethy, 2010), and cross-lingual similarity (CLS) (Ananthakrishnan et al., 2011a). They all perform at the word level, exact only ter512 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 512–522, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics m matching schemes, and do not take into account any contextual information when modeling the selection by single words in isolation, which degrade the quality of selected sentences. In this paper, we argue that it is beneficial to model the data selection based on the source translation task directly"
D12-1047,D11-1033,0,0.314826,"Xiaoyin Fu, and Bo Xu Interactive Digital Media Technology Research Center Institute of Automation, Chinese Academy of Sciences 95 Zhongguancun East Road, Haidian District, Beijing 100190, China {shixiang.lu,wei.wei.media,xiaoyin.fu,xubo}@ia.ac.cn Abstract this reason, most researchers preferred to select similar training data from the large training corpus in the past few years (Eck et al., 2004; Zhao et al., 2004; Kim, 2005; Masskey and Sethy, 2010; Axelrod et al., 2011). This would empirically provide more accurate lexical probabilities, and thus better match the translation task at hand (Axelrod et al., 2011). In this paper, we propose a novel translation model (TM) based cross-lingual data selection model for language model (LM) adaptation in statistical machine translation (SMT), from word models to phrase models. Given a source sentence in the translation task, this model directly estimates the probability that a sentence in the target LM training corpus is similar. Compared with the traditional approaches which utilize the first pass translation hypotheses, cross-lingual data selection model avoids the problem of noisy proliferation. Furthermore, phrase TM based cross-lingual data selection mo"
D12-1047,D07-1090,0,0.0239171,"-lingual data selection model is more effective than the traditional approaches based on bag-ofwords models and word-based TM, because it captures contextual information in modeling the selection of phrase as a whole. Experiments conducted on large-scale data sets demonstrate that our approach significantly outperforms the state-of-the-art approaches on both LM perplexity and SMT performance. 1 Introduction Language model (LM) plays a critical role in statistical machine translation (SMT). It seems to be a universal truth that LM performance can always be improved by using more training data (Brants et al., 2007), but only if the training data is reasonably well-matched with the desired output (Moore and Lewis, 2010). It is also obvious that among the large training data the topics or domains of discussion will change (Eck et al., 2004), which causes the mismatch problems with the translation task. For Many previous data selection approaches for LM adaptation in SMT depend on the first pass translation hypotheses (Eck et al., 2004; Zhao et al., 2004; Kim, 2005; Masskey and Sethy, 2010), they select the sentences which are similar to the translation hypotheses. These schemes are overall limited by the"
D12-1047,J93-2003,0,0.0409951,"P (S|Q) = (2) P (Q) where the prior probability P (S) can be viewed as uniform, and the P (Q) is constant across all sentences. Therefore, selecting a sentence to maximize P (S|Q) is equivalent to selecting a sentence that maximizes P (Q|S). 4.1 4.1.1 Cross-Lingual Sentence Selection Model Following the work of (Xu et al., 2001; Snover et al., 2008), CLWTM can be described as Y P (Q|S) = P (q|S) (3) q∈Q P (q|S) = αP (q|Cq ) + (1 − α) 514 X P (q|w)P (w|S) w∈S (4) where α is the interpolation weight empirically set as a constant1 , P (q|w) is the word-based TM which is estimated by IBM Model 1 (Brown et al., 1993) from the parallel corpus, P (q|Cq ) and P (w|S) are the un-smoothed background and sentence model, respectively, estimated using maximum likelihood estimation (MLE) as (1) where the interpolation factor µ can be simply estimated using the Powell Search algorithm (Press et al., 1992) via cross-validation. Our work focuses on TM based cross-lingual data selection model, from word model to phrase models, and the quality of this model is crucial to the performance of adapted LM. Word-Based Translation Model for Cross-Lingual Data Selection (CLWTM) P (q|Cq ) = f req(q, Cq ) |Cq | (5) P (w|S) = f r"
D12-1047,P05-1033,0,0.217294,"Missing"
D12-1047,J07-2003,0,0.148875,"Missing"
D12-1047,W02-1001,0,0.24704,"rm phrase in a source sentence given a phrase in the target sentence of LM training corpus. Compared with bag-of-words models and word-based TM that account for selecting single words in isolation, this model performs at the phrase level and captures some contextual information in modeling the selection of phrase as a whole, thus it is potentially more effective. More precise data selection can be determined for phrases than for words. In this model, we propose a linear ranking model framework to further improve the performance, referred to the linear discriminant function (Duda et al., 2001; Collins, 2002; Gao et al., 2005) in pattern classification and information retrieval (IR), where different models are incorporated as features, as we will show in our experiments. Unlike the general TM in SMT, we explore the use of TextRank algorithm (Mihalcea et al., 2004) to identify and eliminate unimportant words (e.g., non-topical words, common words) for corpus preprocessing, and construct TM by important words. This reduces the average number of words in crosslingual data selection model, thus improving the efficiency. Moreover, TextRank utilizes the contex513 t information of words to assign term w"
D12-1047,eck-etal-2004-language,0,0.20596,"onducted on large-scale data sets demonstrate that our approach significantly outperforms the state-of-the-art approaches on both LM perplexity and SMT performance. 1 Introduction Language model (LM) plays a critical role in statistical machine translation (SMT). It seems to be a universal truth that LM performance can always be improved by using more training data (Brants et al., 2007), but only if the training data is reasonably well-matched with the desired output (Moore and Lewis, 2010). It is also obvious that among the large training data the topics or domains of discussion will change (Eck et al., 2004), which causes the mismatch problems with the translation task. For Many previous data selection approaches for LM adaptation in SMT depend on the first pass translation hypotheses (Eck et al., 2004; Zhao et al., 2004; Kim, 2005; Masskey and Sethy, 2010), they select the sentences which are similar to the translation hypotheses. These schemes are overall limited by the quality of the translation hypotheses (Tam et al., 2007 and 2008), and better initial translation hypotheses lead to better selected sentences (Zhao et al., 2004). However, while SMT has achieved a great deal of development in r"
D12-1047,W07-0717,0,0.115665,"till far from perfect (Wei and Pal, 2010), which have many noisy data. The noisy translation hypotheses mislead data selection process (Xu et al., 2001; Tam et al., 2006 and 2007; Wei and Pal, 2010), and thus take noisy data into the selected training data, which causes noisy proliferation and degrades the performance of adapted LM. Furthermore, traditional approaches for LM adaptation are based on bag-of-words models and considered to be context independent, despite of their state-of-the-art performance, such as TF-IDF (Eck et al., 2004; Zhao et al., 2004; Hildebrand et al., 2005; Kim, 2005; Foster and Kuhn, 2007), centroid similarity (Masskey and Sethy, 2010), and cross-lingual similarity (CLS) (Ananthakrishnan et al., 2011a). They all perform at the word level, exact only ter512 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 512–522, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics m matching schemes, and do not take into account any contextual information when modeling the selection by single words in isolation, which degrade the quality of selected sentences. In this"
D12-1047,2005.eamt-1.19,0,0.129715,"rs, the translation hypotheses are still far from perfect (Wei and Pal, 2010), which have many noisy data. The noisy translation hypotheses mislead data selection process (Xu et al., 2001; Tam et al., 2006 and 2007; Wei and Pal, 2010), and thus take noisy data into the selected training data, which causes noisy proliferation and degrades the performance of adapted LM. Furthermore, traditional approaches for LM adaptation are based on bag-of-words models and considered to be context independent, despite of their state-of-the-art performance, such as TF-IDF (Eck et al., 2004; Zhao et al., 2004; Hildebrand et al., 2005; Kim, 2005; Foster and Kuhn, 2007), centroid similarity (Masskey and Sethy, 2010), and cross-lingual similarity (CLS) (Ananthakrishnan et al., 2011a). They all perform at the word level, exact only ter512 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 512–522, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics m matching schemes, and do not take into account any contextual information when modeling the selection by single words in isolation, which degrade the qua"
D12-1047,N03-1017,0,0.114952,",V,M )∈ B(S,Q) w∈S Ps (w|S) = βP (w|Cs ) + (1 − β)P (w|S) P (w|Cs ) = f req(w, Cs ) |Cs | (7) (8) (9) P (Q|S) ≈ where P (w|Cs ) is the un-smoothed background model, estimated using MLE as Equation (5), Cs refers to the LM training corpus and |Cs |refers to its size. Here, β is interpolation weight; notice that letting β = 0 in Equation (8) reduces the model to the un-smoothed model in Equation (4). 4.2 (10) Then, we use the maximum approximation to the sum: Phrase-Based Translation Model for Cross-Lingual Data Selection (CLPTM) 4.2.1 Cross-Lingual Sentence Selection Model The phrase-based TM (Koehn et al., 2003; Och and Ney, 2004) has shown superior performance compared to the word-based TM. In this paper, the goal of phrase-based TM is to transfer S into Q. Rather than transferring single words in isolation, the phrase model transfers one sequence of words into another sequence of words, thus incorporating contextual information. Inspired by the work of web search (Gao et al., 2010) and question retrieval in community question answer (Q&A) (Zhou et al., 2011), we assume the following generative process: first the sentence S is broken into K nonempty word sequences w1 , . . . , wk , then each is tra"
D12-1047,D08-1043,0,0.128158,"al., 2005) in pattern classification and information retrieval (IR), where different models are incorporated as features, as we will show in our experiments. Unlike the general TM in SMT, we explore the use of TextRank algorithm (Mihalcea et al., 2004) to identify and eliminate unimportant words (e.g., non-topical words, common words) for corpus preprocessing, and construct TM by important words. This reduces the average number of words in crosslingual data selection model, thus improving the efficiency. Moreover, TextRank utilizes the contex513 t information of words to assign term weights (Lee et al., 2008), which makes phrase TM based crosslingual data selection model play its advantage of capturing the contextual information, thus further improving the performance. The remainder of this paper is organized as follows. Section 2 introduces the related work of LM adaptation. Section 3 presents the framework of cross-lingual data selection for LM adaptation. Section 4 describes our proposed TM based crosslingual data selection model: from word models to phrase models. In section 5 we present large-scale experiments and analyses, and followed by conclusions and future work in section 6. 2 Related W"
D12-1047,W04-3252,0,0.00390624,"corpus and the parallel corpus in our task are constructed by the key words or important words, and thus construct TM by the key words or important words, which is another key difference between our task and SMT. We identify and eliminate unimportant words, somewhat similar to Q&A retrieval (Lee et al., 2008; Zhou et al., 2011). Thus, the average number of words (the total word number in Q and S) in cross-lingual sentence selection model would be minimized naturally, and the efficiency of cross-lingual data selection would be improved. In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graphbased ranking model for key word extraction which achieves state-of-the-art accuracy. It identifies and eliminates unimportant words from the corpus, and assumes that a word is unimportant if it holds a relatively low significance in the corpus. Compared with the traditional approaches, such as TF-IDF, TextRank utilizes the context information of words to assign term weights (Lee et al., 2008), so it further improves the performance of CLPTM, as we will show in the experiments. Following the work of (Lee et al., 2008), the ranking algorithm proceeds as follows. First, all the words in"
D12-1047,P10-2041,0,0.178333,"els and word-based TM, because it captures contextual information in modeling the selection of phrase as a whole. Experiments conducted on large-scale data sets demonstrate that our approach significantly outperforms the state-of-the-art approaches on both LM perplexity and SMT performance. 1 Introduction Language model (LM) plays a critical role in statistical machine translation (SMT). It seems to be a universal truth that LM performance can always be improved by using more training data (Brants et al., 2007), but only if the training data is reasonably well-matched with the desired output (Moore and Lewis, 2010). It is also obvious that among the large training data the topics or domains of discussion will change (Eck et al., 2004), which causes the mismatch problems with the translation task. For Many previous data selection approaches for LM adaptation in SMT depend on the first pass translation hypotheses (Eck et al., 2004; Zhao et al., 2004; Kim, 2005; Masskey and Sethy, 2010), they select the sentences which are similar to the translation hypotheses. These schemes are overall limited by the quality of the translation hypotheses (Tam et al., 2007 and 2008), and better initial translation hypothes"
D12-1047,P03-1021,0,0.0290668,"Missing"
D12-1047,J04-4002,0,0.476379,"s (w|S) = βP (w|Cs ) + (1 − β)P (w|S) P (w|Cs ) = f req(w, Cs ) |Cs | (7) (8) (9) P (Q|S) ≈ where P (w|Cs ) is the un-smoothed background model, estimated using MLE as Equation (5), Cs refers to the LM training corpus and |Cs |refers to its size. Here, β is interpolation weight; notice that letting β = 0 in Equation (8) reduces the model to the un-smoothed model in Equation (4). 4.2 (10) Then, we use the maximum approximation to the sum: Phrase-Based Translation Model for Cross-Lingual Data Selection (CLPTM) 4.2.1 Cross-Lingual Sentence Selection Model The phrase-based TM (Koehn et al., 2003; Och and Ney, 2004) has shown superior performance compared to the word-based TM. In this paper, the goal of phrase-based TM is to transfer S into Q. Rather than transferring single words in isolation, the phrase model transfers one sequence of words into another sequence of words, thus incorporating contextual information. Inspired by the work of web search (Gao et al., 2010) and question retrieval in community question answer (Q&A) (Zhou et al., 2011), we assume the following generative process: first the sentence S is broken into K nonempty word sequences w1 , . . . , wk , then each is transferred into a new"
D12-1047,P02-1040,0,0.0835701,"Missing"
D12-1047,D08-1090,0,0.256561,"y of corresponding n-gram through weighting the LM training corpus. However, these two cross-lingual approaches focus on modify LM itself, which are different from data selection method for LM adaptation. In our comparable experiments, we apply CLS for the first time to the task of cross-lingual data selection for LM adaptation. Due to lack of smoothing measure for sparse vector representation in CLS, the similarity computation is not accurate which degrades the performance of adapted LM. To avoid this, we add smoothing measure like TF-IDF, called CLSs , as we will discuss in the experiments. Snover et al. (2008) used a word TM based CLIR system (Xu et al., 2001) to select a subset of target documents comparable to the source document for adapting LM. Because of the data sparseness in the document state and it operated at the document level, this model selected large quantities of irrelevant text, which may degrade the adapted LM (Eck et al., 2004; Ananthakrishnan et al., 2011b). In our word TM based cross-lingual data selection model, we operate at the sentence level and add the smoothing mechanism by integrating with the background word frequency model, and these can significantly improve the perfor"
D12-1047,P07-1066,0,0.138373,"onably well-matched with the desired output (Moore and Lewis, 2010). It is also obvious that among the large training data the topics or domains of discussion will change (Eck et al., 2004), which causes the mismatch problems with the translation task. For Many previous data selection approaches for LM adaptation in SMT depend on the first pass translation hypotheses (Eck et al., 2004; Zhao et al., 2004; Kim, 2005; Masskey and Sethy, 2010), they select the sentences which are similar to the translation hypotheses. These schemes are overall limited by the quality of the translation hypotheses (Tam et al., 2007 and 2008), and better initial translation hypotheses lead to better selected sentences (Zhao et al., 2004). However, while SMT has achieved a great deal of development in recent years, the translation hypotheses are still far from perfect (Wei and Pal, 2010), which have many noisy data. The noisy translation hypotheses mislead data selection process (Xu et al., 2001; Tam et al., 2006 and 2007; Wei and Pal, 2010), and thus take noisy data into the selected training data, which causes noisy proliferation and degrades the performance of adapted LM. Furthermore, traditional approaches for LM adap"
D12-1047,P10-2048,0,0.0197957,"Many previous data selection approaches for LM adaptation in SMT depend on the first pass translation hypotheses (Eck et al., 2004; Zhao et al., 2004; Kim, 2005; Masskey and Sethy, 2010), they select the sentences which are similar to the translation hypotheses. These schemes are overall limited by the quality of the translation hypotheses (Tam et al., 2007 and 2008), and better initial translation hypotheses lead to better selected sentences (Zhao et al., 2004). However, while SMT has achieved a great deal of development in recent years, the translation hypotheses are still far from perfect (Wei and Pal, 2010), which have many noisy data. The noisy translation hypotheses mislead data selection process (Xu et al., 2001; Tam et al., 2006 and 2007; Wei and Pal, 2010), and thus take noisy data into the selected training data, which causes noisy proliferation and degrades the performance of adapted LM. Furthermore, traditional approaches for LM adaptation are based on bag-of-words models and considered to be context independent, despite of their state-of-the-art performance, such as TF-IDF (Eck et al., 2004; Zhao et al., 2004; Hildebrand et al., 2005; Kim, 2005; Foster and Kuhn, 2007), centroid similari"
D12-1047,C04-1059,0,0.282483,"role in statistical machine translation (SMT). It seems to be a universal truth that LM performance can always be improved by using more training data (Brants et al., 2007), but only if the training data is reasonably well-matched with the desired output (Moore and Lewis, 2010). It is also obvious that among the large training data the topics or domains of discussion will change (Eck et al., 2004), which causes the mismatch problems with the translation task. For Many previous data selection approaches for LM adaptation in SMT depend on the first pass translation hypotheses (Eck et al., 2004; Zhao et al., 2004; Kim, 2005; Masskey and Sethy, 2010), they select the sentences which are similar to the translation hypotheses. These schemes are overall limited by the quality of the translation hypotheses (Tam et al., 2007 and 2008), and better initial translation hypotheses lead to better selected sentences (Zhao et al., 2004). However, while SMT has achieved a great deal of development in recent years, the translation hypotheses are still far from perfect (Wei and Pal, 2010), which have many noisy data. The noisy translation hypotheses mislead data selection process (Xu et al., 2001; Tam et al., 2006 an"
D12-1047,P11-1066,0,\N,Missing
D15-1142,I05-3018,0,0.0209533,"riments In our evaluation, the F-score was used as the accuracy measure. The precision p is defined as the percentage of words in the decoder output that are segmented correctly, and the recall r is the percentage of gold-standard output words that are correctly segmented by the decoder. The balanced F- score is calculated as 2pr/(p + r). We also report the recall of OOV words in our experiments. In the following, we refer to our methods as ”SLBD” (segmenter leveraging bilingual data). Initially, we evaluated state-of-the-art supervised CWS methods, i.e., those of (Peng et al., 2004) (Peng); (Asahara et al., 2005) (Asahara); (Zhang and Clark, 2007) (Z&C); (Zhao et al., 2010) (Zhao), whose models are trained only on manually segmented data. Moreover, we also evaluated the performance of our sub-models by 1213 methods Peng Asahara Z&C Zhao character-level Inner log-linear Outer log-linear F 91.6 92.2 92.9 93.1 92.3 95.9 96.7 AS OOV 52.5 63.1 69.9 72 58.6 78.8 80.8 PKU F OOV 91.1 59 91.4 61.6 91.6 67.9 92.3 60.6 92.9 60.8 96.1 81 97.1 85 methods Sun S&X Zeng SLBD Bilingual data F OOV 93.9 63.1 94.1 66 94.0 64.5 96.7 80.8 Monolingual data F OOV 94.6 67.9 94.4 71 94.8 63.2 - Table 3: Word segmentation perfo"
D15-1142,W08-0336,0,0.640636,"tences. After obtaining these multilevel features, we normalize them and combine them into two log-linear models in a cascaded structure, which is illustrated in Fig 2. Finally, we segment the bilingual unlabeled data using the proposed model and use the segmentation of those data to justify the original superFigure 2: The structure of cascaded log-linear model with multilevel features vised CWS model, which was trained on a standard manually segmented corpus. In fact, several semi-supervised CWS methods have previously been proposed that leverage bilingual unlabeled data ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)). However, most were developed for statistical machine translation (SMT), causing them to focus on decreasing the perplexity of the bilingual data and the word alignment process rather than on achieving more accurate segmentation. These methods achieve significant improvement in SMT performance but are not very suitable for common NLP tasks because in many situations, they ignore the standard grammars to satisfy the needs of SMT. By contrast, we employ various types of features to capture both monolingual standard grammars and biling"
D15-1142,D09-1075,0,0.0439829,"Missing"
D15-1142,E09-1063,0,0.273706,"g these multilevel features, we normalize them and combine them into two log-linear models in a cascaded structure, which is illustrated in Fig 2. Finally, we segment the bilingual unlabeled data using the proposed model and use the segmentation of those data to justify the original superFigure 2: The structure of cascaded log-linear model with multilevel features vised CWS model, which was trained on a standard manually segmented corpus. In fact, several semi-supervised CWS methods have previously been proposed that leverage bilingual unlabeled data ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)). However, most were developed for statistical machine translation (SMT), causing them to focus on decreasing the perplexity of the bilingual data and the word alignment process rather than on achieving more accurate segmentation. These methods achieve significant improvement in SMT performance but are not very suitable for common NLP tasks because in many situations, they ignore the standard grammars to satisfy the needs of SMT. By contrast, we employ various types of features to capture both monolingual standard grammars and bilingual segmenting infor"
D15-1142,P11-1064,0,0.0573009,"Missing"
D15-1142,P12-1018,0,0.049536,"Missing"
D15-1142,P06-1027,0,0.256493,"-supervised Chinese Word Segmentation based on Bilingual Information Wei Chen Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China wei.chen.media@ia.ac.cn Bo Xu Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China xubo@ia.ac.cn Abstract CWS using machine learning techniques in recent years. However, the reliability of CWS that can be achieved using machine learning techniques relies heavily on the availability of a large amount of high-quality, manually segmented data. Because hand-labeling individual words and word boundaries is very difficult (Jiao et al., 2006), producing segmented Chinese texts is very time-consuming and expensive. Although a number of manually segmented datasets have been constructed by various organizations, it is not feasible to combine them into a single complete dataset because of their incompatibility due to the use of various segmenting standards. Thus, it is difficult to build a large-scale manually segmented corpus, and the resulting lack of such a corpus is detrimental to further enhancement of the accuracy of CWS. This paper presents a bilingual semisupervised Chinese word segmentation (CWS) method that leverages the nat"
D15-1142,N03-1017,0,0.0261709,"ce” into ”ai l i s i/a l i c e” —by transforming the Chinese word into its pronunciation (represented by the function Fpy (·)) and splitting the English word into its constituent letters (represented by the function Flet (·)). Then, we train two phrase-based translation models (Chinese-English and English-Chinese) on the data obtained from the converted NE dictionary. Specifically, we apply two standard log-linear phrase-based SMT models. The GIZA++ aligner is adopted to obtain word alignments (Och and Ney, 2000) from the converted NE dictionary. The heuristic strategy of grow-diag-final-and (Koehn et al., 2003) is used to combine the bidirectional alignments to extract phrase translations and to reorder tables. A 5-gram language model with Kneser-Ney smoothing is trained using SRILM (Stolcke et al., 2002) on the target language. Moses (Koehn et al., 2007) is used as a decoder. Minimum error rate training (MERT) (Och et al., 2003) is applied to tune the feature parameters on the development dataset. Given these two phrase-based translation models, we calculate each span < i, jm , jn &gt; in AOne for the Chinese word wn using the following formula: Str (< i, jm , jn &gt;) = Sch−en (< i, jm , jn &gt;) +Sen−ch ("
D15-1142,P07-2045,0,0.00674338,"Missing"
D15-1142,P00-1056,0,0.247024,"ntroduce an initial NE dictionary and convert each dictionary item—for example, we convert ”Ow d/Alice” into ”ai l i s i/a l i c e” —by transforming the Chinese word into its pronunciation (represented by the function Fpy (·)) and splitting the English word into its constituent letters (represented by the function Flet (·)). Then, we train two phrase-based translation models (Chinese-English and English-Chinese) on the data obtained from the converted NE dictionary. Specifically, we apply two standard log-linear phrase-based SMT models. The GIZA++ aligner is adopted to obtain word alignments (Och and Ney, 2000) from the converted NE dictionary. The heuristic strategy of grow-diag-final-and (Koehn et al., 2003) is used to combine the bidirectional alignments to extract phrase translations and to reorder tables. A 5-gram language model with Kneser-Ney smoothing is trained using SRILM (Stolcke et al., 2002) on the target language. Moses (Koehn et al., 2007) is used as a decoder. Minimum error rate training (MERT) (Och et al., 2003) is applied to tune the feature parameters on the development dataset. Given these two phrase-based translation models, we calculate each span < i, jm , jn &gt; in AOne for the"
D15-1142,P03-1021,0,0.0141056,"Missing"
D15-1142,C04-1081,0,0.490714,"nd bilingual semi-supervised approaches. 2 Related Work First, we review related work on monolingual supervised and semi-supervised CWS methods. Then, we review bilingual semi-supervised CWS. 2.1 Monolingual Supervised and Semi-supervised CWS Methods Considerable efforts have been made in the NLP community in the study of Chinese word segmentation. The most popular supervised approach treats word segmentation as a sequence labeling problem, as first proposed by (Xue et al., 2003). Most previous systems have addressed this task using linear statistical models with carefully designed features ((Peng et al., 2004); (Asahara et 1208 al., 2005); (Zhang and Clark, 2007); (Zhao et al., 2010)). However, the primary shortcoming of these approaches is that they rely heavily on a large amount of labeled data, which is very timeconsuming and expensive to produce. Thus, the scale of available manually labeled data has placed considerable limitations on the further enhancement of supervised CWS methods. To address this problem, a number of semisupervised CWS approaches have been intensively investigated in recent years. For example, (Sun and Xu, 2011) enhanced their segmentation results by interpolating statistic"
D15-1142,P12-1027,0,0.0130025,"nificant improvement in performance when we combined the character-level and phrase-level features in the inner log-linear model, demonstrating that the proposed phrase-level features can be used to efficiently obtain bilingual segmenting information. Moreover, the outer log-linear model achieves a further enhancement, thereby demonstrating that the sentence-level features can be used to effectively re-rank the candidate segmentations produced by the inner log-linear model. Next, we compared the SLBD method with several state-of-the-art monolingual semi-supervised methods, including those of (Sun et al., 2012) (Sun); (Sun and Xu, 2011) (S&X); (Zeng et al., 2013b) (Zeng). To ensure a fair comparison, we performed the evaluation in two steps. First, we input the entire bilingual unlabeled dataset into the SLBD method and input only the Chinese sentences from the bilingual unlabeled dataset into the other semi-supervised methods. Then, because the available monolingual unlabeled dataset was much larger than the bilingual unlabeled dataset in natural, we used the XIN CMN portion of Chinese Gigaword 2.0 as an additional unlabeled dataset for the monolingual semi-supervised methods. which contains 204 mi"
D15-1142,P12-2056,0,0.235354,"them and combine them into two log-linear models in a cascaded structure, which is illustrated in Fig 2. Finally, we segment the bilingual unlabeled data using the proposed model and use the segmentation of those data to justify the original superFigure 2: The structure of cascaded log-linear model with multilevel features vised CWS model, which was trained on a standard manually segmented corpus. In fact, several semi-supervised CWS methods have previously been proposed that leverage bilingual unlabeled data ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)). However, most were developed for statistical machine translation (SMT), causing them to focus on decreasing the perplexity of the bilingual data and the word alignment process rather than on achieving more accurate segmentation. These methods achieve significant improvement in SMT performance but are not very suitable for common NLP tasks because in many situations, they ignore the standard grammars to satisfy the needs of SMT. By contrast, we employ various types of features to capture both monolingual standard grammars and bilingual segmenting information, which allows our semi-supervised"
D15-1142,W04-1118,0,0.0474533,"e extract linguistic knowledge without any manual assumptions or bias. 2.2 Bilingual Semi-supervised CWS Methods Some previous work ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)) has been performed on leveraging bilingual unlabeled data to achieve better segmentation, although most such studies have focused on statistical machine translation (SMT). These approaches leverage the mappings of individual English words to one or more consecutive Chinese characters either to construct a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004) or to form a labeled dataset for training a sequence labeling model (Peng et al., 2004). (Zeng et al., 2014) also used such mappings to bias a supervised segmentation model toward a better solution for SMT. However, because most of these approaches focus on SMT performance, they emphasize decreasing the perplexity of the bilingual data and word alignment rather than improving the CWS accuracy. Thus, they sometimes ignore the standard grammars during segmentation in favor of satisfying the needs of SMT, thereby causing these methods to be rather unsuitable for other NLP tasks. By contrast, we"
D15-1142,C08-1128,0,0.427943,"ponding English sentences. After obtaining these multilevel features, we normalize them and combine them into two log-linear models in a cascaded structure, which is illustrated in Fig 2. Finally, we segment the bilingual unlabeled data using the proposed model and use the segmentation of those data to justify the original superFigure 2: The structure of cascaded log-linear model with multilevel features vised CWS model, which was trained on a standard manually segmented corpus. In fact, several semi-supervised CWS methods have previously been proposed that leverage bilingual unlabeled data ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)). However, most were developed for statistical machine translation (SMT), causing them to focus on decreasing the perplexity of the bilingual data and the word alignment process rather than on achieving more accurate segmentation. These methods achieve significant improvement in SMT performance but are not very suitable for common NLP tasks because in many situations, they ignore the standard grammars to satisfy the needs of SMT. By contrast, we employ various types of features to capture both monolingual standa"
D15-1142,O03-4002,0,0.122296,"Missing"
D15-1142,P07-1106,0,0.192712,"Work First, we review related work on monolingual supervised and semi-supervised CWS methods. Then, we review bilingual semi-supervised CWS. 2.1 Monolingual Supervised and Semi-supervised CWS Methods Considerable efforts have been made in the NLP community in the study of Chinese word segmentation. The most popular supervised approach treats word segmentation as a sequence labeling problem, as first proposed by (Xue et al., 2003). Most previous systems have addressed this task using linear statistical models with carefully designed features ((Peng et al., 2004); (Asahara et 1208 al., 2005); (Zhang and Clark, 2007); (Zhao et al., 2010)). However, the primary shortcoming of these approaches is that they rely heavily on a large amount of labeled data, which is very timeconsuming and expensive to produce. Thus, the scale of available manually labeled data has placed considerable limitations on the further enhancement of supervised CWS methods. To address this problem, a number of semisupervised CWS approaches have been intensively investigated in recent years. For example, (Sun and Xu, 2011) enhanced their segmentation results by interpolating statistics-based features derived from unlabeled data into a CR"
D15-1142,P13-1076,0,0.208688,"cessary first step in most Chinese NLP tasks because Chinese sentences are written in continuous sequences of characters with no explicit delimiters (e.g., the spaces in English). Many studies have been conducted in this area, resulting in extensive investigation of the problem of To address the scarcity of manually segmented corpora, a number of semi-supervised CWS approaches have been intensively investigated in recent years. These approaches attempt to either learn the predicted label distribution (Jiao et al., 2006) or extract mutual information ((Liang et al., 2005); (Sun and Xu, 2011); (Zeng et al., 2013a)) from large-scale monolingual unlabeled data to update the baseline model (from manually segmented corpora). In addition to these techniques, several co-training approaches (Zeng et al., 2013b) using character-based and word-based models have also been employed. However, because monolingual unlabeled data contain limited natural segmenting information, in most semisupervised methods, the objective function tends to be optimized based on the personal experience and knowledge of the researchers. This practice means that these methods can typically yield high performance in certain specialized"
D15-1142,P13-2031,0,0.0280216,"Missing"
D15-1142,P14-1128,0,0.0277307,"Missing"
D15-1142,D11-1090,0,\N,Missing
D17-1154,D13-1176,0,0.0900149,"lutional encoder and weight shared embeddings. Then weight pruning is applied to obtain a sparse model. Next, we propose a fast sequence interpolation approach which enables the greedy decoding to achieve performance on par with the beam search. Hence, the time-consuming beam search can be replaced by simple greedy decoding. Finally, vocabulary selection is used to reduce the computation of softmax layer. Our final model achieves 10× speedup, 17× parameters reduction, &lt;35MB storage size and comparable performance compared to the baseline model. 1 Introduction Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has recently gained popularity in solving the machine translation problem. Although NMT has achieved state-of-the-art performance for several language pairs (Jean et al., 2015; Wu et al., 2016), like many other deep learning domains, it is both computationally intensive and memory intensive. This leads to a challenge of deploying NMT models on the devices with limited computation and memory budgets. ∗ *Corresponding author Numerous approaches have been proposed for compression and inference speedup of neural networks, including but not limited t"
D17-1154,W14-3346,0,0.0201285,"cording to (2) we have:  ˜ S) &gt; P (Y, S) P (S, ˜ Y ) &gt; E(S, Y ) E(S, We note that using S˜ as a label is more attractive than Y for improving the performance of greedy decoding. The reason is that S and Y are often quite different (Kim and Rush, 2016), resulting in a relatively low P (Y, S). We bridge the gap between S and Y by interpolating inner sequence between them. Specifically, we edit S toward Y , which can be seen as interpolation. Editing is a heuristic operation as illustrated in Figure 2. Concretely, let Ss be a subsequence of S and let Ys be 3 We use smoothed sentence-level BLEU (Chen and Cherry, 2014). Algorithm 1 Editing algorithm of fast sequence interpolation. Input: (X, Y, S, k): (X, Y ) is a sequence pair in training data. S is the result of the greedy decoding using source sequence X. k is the maximum number of tokens in replaced subsequence of S or Y . ˜ the edited sample. Output: S: 1: for si in S, yj in Y do 2: if (si == yj ) then 3: for 1 ≤ p ≤ k + 1, 1 ≤ q ≤ k + 1 do 4: if (si+p == yj+q ) then 5: Ss = (si , ..., sp ) 6: Y s = (yj , ..., yq ) 7: Break 8: end if 9: end for 10: end if 11: Replace subsequence of S: Ss ← Ys 12: Break 13: end for ˜←S 14: S a subsequence of Y . Given t"
D17-1154,D16-1139,0,0.218674,"oying NMT models on the devices with limited computation and memory budgets. ∗ *Corresponding author Numerous approaches have been proposed for compression and inference speedup of neural networks, including but not limited to low-rank approximation (Denton et al., 2014), hash function (Chen et al., 2015), knowledge distillation (Hinton et al., 2015), quantization (Courbariaux et al., 2015; Han et al., 2016; Zhou et al., 2017) and sparsification (Han et al., 2015; Wen et al., 2016). Weight pruning and knowledge distillation have been proved to be able to compress NMT models (See et al., 2016; Kim and Rush, 2016; Freitag et al., 2017). The above methods reduce the parameters from a global perspective. However, embeddings dominate the parameters in a relatively compact NMT model even if subword (Sennrich et al., 2016) (typical about 30K) is used. Character-aware methods (Ling et al., 2015; Lee et al., 2016) have fewer embeddings while suffer from slower decoding speed (Wu et al., 2016). Recent work by Li et al. (2016) has shown that weight sharing can be adopted to compress embeddings in language model. We are interested in applying embeddings weight sharing to NMT. As for decoding speedup, Gehring et"
D17-1154,N13-1073,0,0.0473578,"BaselineS , the embedding size is 256 and the hidden size is 512. Our baseline models are similar to the architecture in DL4MT6 . For the convolutional encoder model, 512 hidden units are used for the 6-layer CNN-a, and 256 hidden units are used for the 8-layer CNN-c. The embedding size is 256. The hidden size of the de4 https://github.com/clab/fast align http://statmt.org/wmt14 6 https://github.com/nyu-dl/dl4mt-tutorial 5 0.5 substitution rate(k=2) substitution rate(k=3) 0.4 BLEU(k=2) BLEU(k=3) 0.3 18.0 17.5 17.0 16.5 0.2 16.0 Experiments 3.1 18.5 Vocabulary Selection We use word alignment4 (Dyer et al., 2013) to build a candidate dictionary. For each source word, we build a list of candidate target words. When decoding, top n candidates of each word are merged to form a short-list for softmax layer. We do not apply vocabulary selection in training. 3 0.6 Substitution rate 2.4 19.0 BLEU over the training set. In summary, the following procedure is done iteratively: (1) get a new batch of (X, Y ), (2) run batched greedy decoding on X, ˜ (4) get Y˜ according to the (3) edit S to obtain S, substitution rule, (5) train on the batched (X, Y˜ ). 0.1 15.5 15.0 0.05 0.10 0.15 0.20 threshold 0.25 0.0 0.30 F"
D17-1154,W17-3207,0,0.0145877,"(2016) tried to improve the parallelism in NMT by substituting CNNs for RNNs . Kim and Rush (2016) proposed sequencelevel knowledge distillation which allows us to replace beam search with greedy decoding. Gu et al. (2017) exploited trainable greedy decoding by the actor-critic algorithm (Konda and Tsitsiklis, 2002). Wu et al. (2016) evaluated the quantized inference of NMT. Vocabulary selection (Jean et al., 2015; Mi et al., 2016; L’Hostis et al., 2016) was commonly used to speed up the softmax layer. Search pruning was also applied to speed up beam search (Hu et al., 2015; Wu et al., 2016; Freitag and Al-Onaizan, 2017). Compared to search pruning, the speedup of greedy decoding is more attractive. Knowledge distillation improves the per1475 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1475–1481 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics target language attention CNN-a context vectors mean Initial state CNN-c source word representation GRU decoder target word representation concatenation concatenation class embeddings location embeddings position embeddings class embeddings location embeddings Figure 1: Our netw"
D17-1154,D17-1210,0,0.0901126,"Missing"
D17-1154,P16-1160,0,0.0221573,"2015), knowledge distillation (Hinton et al., 2015), quantization (Courbariaux et al., 2015; Han et al., 2016; Zhou et al., 2017) and sparsification (Han et al., 2015; Wen et al., 2016). Weight pruning and knowledge distillation have been proved to be able to compress NMT models (See et al., 2016; Kim and Rush, 2016; Freitag et al., 2017). The above methods reduce the parameters from a global perspective. However, embeddings dominate the parameters in a relatively compact NMT model even if subword (Sennrich et al., 2016) (typical about 30K) is used. Character-aware methods (Ling et al., 2015; Lee et al., 2016) have fewer embeddings while suffer from slower decoding speed (Wu et al., 2016). Recent work by Li et al. (2016) has shown that weight sharing can be adopted to compress embeddings in language model. We are interested in applying embeddings weight sharing to NMT. As for decoding speedup, Gehring et al. (2016); Kalchbrenner et al. (2016) tried to improve the parallelism in NMT by substituting CNNs for RNNs . Kim and Rush (2016) proposed sequencelevel knowledge distillation which allows us to replace beam search with greedy decoding. Gu et al. (2017) exploited trainable greedy decoding by the a"
D17-1154,P16-2021,0,0.0195393,"dopted to compress embeddings in language model. We are interested in applying embeddings weight sharing to NMT. As for decoding speedup, Gehring et al. (2016); Kalchbrenner et al. (2016) tried to improve the parallelism in NMT by substituting CNNs for RNNs . Kim and Rush (2016) proposed sequencelevel knowledge distillation which allows us to replace beam search with greedy decoding. Gu et al. (2017) exploited trainable greedy decoding by the actor-critic algorithm (Konda and Tsitsiklis, 2002). Wu et al. (2016) evaluated the quantized inference of NMT. Vocabulary selection (Jean et al., 2015; Mi et al., 2016; L’Hostis et al., 2016) was commonly used to speed up the softmax layer. Search pruning was also applied to speed up beam search (Hu et al., 2015; Wu et al., 2016; Freitag and Al-Onaizan, 2017). Compared to search pruning, the speedup of greedy decoding is more attractive. Knowledge distillation improves the per1475 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1475–1481 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics target language attention CNN-a context vectors mean Initial state CNN-c source word"
D17-1154,P02-1040,0,0.104824,"ence pairs with 116M English words and 110M German words. We choose newstest2013 as the development set and newstest2014 as the test set. The Chinese-English training data consists of 1.6M pairs with 34M Chinese words and 38M English words. We choose NIST 2002 as the development set and NIST 2005 as the test set. For the two translation task, top 50K and 30k most frequent words are kept respectively. The rest words are replaced with UNK. We only use sentences of length up to 50 symbols. We do not use any UNK handling methods for fair comparison. The evaluation metric is case-insensitive BLEU (Papineni et al., 2002) as calculated by the multi-bleu.perl script. Hyper-parameters: For the baseline model, we use a 2-layer bidirectional GRU encoder (1 layer in each direction) and a 1-layer GRU decoder. In BaselineL , the embedding size is 512 and the hidden size is 1024. In BaselineS , the embedding size is 256 and the hidden size is 512. Our baseline models are similar to the architecture in DL4MT6 . For the convolutional encoder model, 512 hidden units are used for the 6-layer CNN-a, and 256 hidden units are used for the 8-layer CNN-c. The embedding size is 256. The hidden size of the de4 https://github.com"
D17-1154,K16-1029,0,0.0510732,"Missing"
D17-1154,P16-1162,0,0.0921114,"ng but not limited to low-rank approximation (Denton et al., 2014), hash function (Chen et al., 2015), knowledge distillation (Hinton et al., 2015), quantization (Courbariaux et al., 2015; Han et al., 2016; Zhou et al., 2017) and sparsification (Han et al., 2015; Wen et al., 2016). Weight pruning and knowledge distillation have been proved to be able to compress NMT models (See et al., 2016; Kim and Rush, 2016; Freitag et al., 2017). The above methods reduce the parameters from a global perspective. However, embeddings dominate the parameters in a relatively compact NMT model even if subword (Sennrich et al., 2016) (typical about 30K) is used. Character-aware methods (Ling et al., 2015; Lee et al., 2016) have fewer embeddings while suffer from slower decoding speed (Wu et al., 2016). Recent work by Li et al. (2016) has shown that weight sharing can be adopted to compress embeddings in language model. We are interested in applying embeddings weight sharing to NMT. As for decoding speedup, Gehring et al. (2016); Kalchbrenner et al. (2016) tried to improve the parallelism in NMT by substituting CNNs for RNNs . Kim and Rush (2016) proposed sequencelevel knowledge distillation which allows us to replace beam"
D17-1154,Q17-1026,0,\N,Missing
D18-1118,N16-1181,0,0.468615,"Connections and differences between previous “program-generating” works and our model: other models generate/control multi-step image-comprehension processes with single question representation, while we put more attention on language logics and let multi-modal information modulate each other in each step. The question and image are taken as a visual-reasoning example from CLEVR dataset. Most previous visual reasoning models focus on using the question to guide the multi-step computing on visual features (which can be defined as a image-comprehension “program”). Neural Module Networks (NMN) (Andreas et al., 2016a,b) and Program Generator + Execution Engine (PG+EE) (Johnson et al., 2017b) learn to combine specific image-processing modules, guided by question semantics. Feature-modulating methods like FiLM (De Vries et al., 2017; Perez et al., 2018) control image-comprehension process using modulation-parameters generated from the question, allowing models to be trained end-to-end. However, the image-comprehension program in visual reasoning tasks can be extremely long and sophisticated. Using a single question representation to generate or control the whole imagecomprehension process raises difficulti"
D18-1118,P17-2034,0,0.294075,"l intelligence to perform reasoning with both textual and visual inputs. Visual reasoning task is designed for researches in this field. It is a special visual question answering (VQA) (Antol et al., 2015) problem, requiring a model to infer the relations between entities in both image and text, and generate a textual answer to the question correctly. Unlike other VQA tasks, questions in visual reasoning often contain extensive logical phenomena, and refer to multiple entities, specific attributes and complex relations. Visual reasoning datasets such as CLEVR (Johnson et al., 2017a) and NLVR (Suhr et al., 2017) are built on unbiased, synthetic images, with either complex synthetic questions or natural-language descriptions, facilitating indepth analyses on reasoning ability itself. * Corresponding Author 975 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 975–980 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics 3.1 that one (language) is the “program generator”, and the other (image) is the “executor”. One way to avoid making this assumption is to perform multiple steps of reasoning with each modality bei"
D18-1118,N18-2071,0,0.453361,"Missing"
D18-1272,S17-2079,0,0.383868,"Missing"
D18-1272,N16-1079,0,0.211473,"mographic puns have increasingly become a respectable research topic, which widely appears in rhetoric and literary criticism. However, there were little related works in the fields of computational linguistics and natural language processing by (Miller, Tristan and Turkovi´c, Mladen, 2016). In this subsection, we mainly introduce some puns detecting methods. There are many useful methods to classify the puns in NLP. For example, (Kao et al., 2016; Huang et al., 2017) used a probability statistical model to capture the latent semantic information between words for detecting homographic puns. (Jaech et al., 2016) proposed a new prob2508 1 WordNet: http://wordnet.princeton.edu/ ability model to learn phoneme edit probabilities for classifying the homophonic puns. The system ECNU(Xiu et al., 2017) applied a supervised training classifier, which helpful features derived from WordNet and Word2Vec embeddings to distinguish between homographic puns. The system Fermi (Indurthi and Oota, 2017) employed a supervised approach for the detection of homographic puns. It used a bi-directional RNN for a classification model and adopted the distributed semantic word embeddings as input features. These methods do not"
D18-1272,J81-4005,0,0.674737,"Missing"
D18-1272,D14-1082,0,0.0104683,"sparsity problem and obtain more semantic relations between words compared with one-hot representation. (Rumelhart et al., 1986) proposed the idea of word distributed representation, which converts all the words into a low-dimensional continuous semantic space. This space took each word as a vector. These distributed low-dimensional word representation have been widely applied in many NLP tasks, including machine translation(Sutskever et al., 2014; Bahdanau et al., 2014), text classification (Niu et al., 2017; Du et al., 2017), neural language models (Mikolov et al., 2010, 2013) and parsing (Chen and Manning, 2014; Chen et al., 2015). Word embedding is taken as the essential and available inputs for NLP tasks, which enables encoding semantic representation in meaningful vector space. The studies show that word representations are useful to achieve a good balance between effectiveness and efficiency, such as Word2Vec (Mikolov et al., 2013) and GloVe(Pennington et al., 2014). Therefore, the semantic meanings of words can reflect in the contexts according to these distributed representation models. However, homographic puns always have multiple meanings. The word representation, considering as only one ve"
D18-1272,P15-1070,0,0.0723416,"Missing"
D18-1272,S17-2005,0,0.132421,"Missing"
D18-1272,P17-1187,0,0.017168,"Representation In recent years, word representation has the great improvement because it solves data sparsity problem and obtain more semantic relations between words compared with one-hot representation. (Rumelhart et al., 1986) proposed the idea of word distributed representation, which converts all the words into a low-dimensional continuous semantic space. This space took each word as a vector. These distributed low-dimensional word representation have been widely applied in many NLP tasks, including machine translation(Sutskever et al., 2014; Bahdanau et al., 2014), text classification (Niu et al., 2017; Du et al., 2017), neural language models (Mikolov et al., 2010, 2013) and parsing (Chen and Manning, 2014; Chen et al., 2015). Word embedding is taken as the essential and available inputs for NLP tasks, which enables encoding semantic representation in meaningful vector space. The studies show that word representations are useful to achieve a good balance between effectiveness and efficiency, such as Word2Vec (Mikolov et al., 2013) and GloVe(Pennington et al., 2014). Therefore, the semantic meanings of words can reflect in the contexts according to these distributed representation models. H"
D18-1272,D14-1162,0,0.0883467,"n widely applied in many NLP tasks, including machine translation(Sutskever et al., 2014; Bahdanau et al., 2014), text classification (Niu et al., 2017; Du et al., 2017), neural language models (Mikolov et al., 2010, 2013) and parsing (Chen and Manning, 2014; Chen et al., 2015). Word embedding is taken as the essential and available inputs for NLP tasks, which enables encoding semantic representation in meaningful vector space. The studies show that word representations are useful to achieve a good balance between effectiveness and efficiency, such as Word2Vec (Mikolov et al., 2013) and GloVe(Pennington et al., 2014). Therefore, the semantic meanings of words can reflect in the contexts according to these distributed representation models. However, homographic puns always have multiple meanings. The word representation, considering as only one vector for each word, which puzzled by the understanding for polysemy of puns. This paper combines the representations of lemmas, synsets and words from WordNet1 (Miller, 2002) to understand multiple meanings of homographic puns. The lemma and synset annotation in WordNet provide helpful semantic information for detecting homographic puns. 2.2 Homographic Pun Recogn"
D18-1272,S17-2078,0,0.281264,"putational linguistics and natural language processing by (Miller, Tristan and Turkovi´c, Mladen, 2016). In this subsection, we mainly introduce some puns detecting methods. There are many useful methods to classify the puns in NLP. For example, (Kao et al., 2016; Huang et al., 2017) used a probability statistical model to capture the latent semantic information between words for detecting homographic puns. (Jaech et al., 2016) proposed a new prob2508 1 WordNet: http://wordnet.princeton.edu/ ability model to learn phoneme edit probabilities for classifying the homophonic puns. The system ECNU(Xiu et al., 2017) applied a supervised training classifier, which helpful features derived from WordNet and Word2Vec embeddings to distinguish between homographic puns. The system Fermi (Indurthi and Oota, 2017) employed a supervised approach for the detection of homographic puns. It used a bi-directional RNN for a classification model and adopted the distributed semantic word embeddings as input features. These methods do not consider the collocation between words in homographic puns. The attention mechanism proposed by (Bahdanau et al., 2014) to settle machine translation problem, which was used to select th"
D18-1272,N16-1174,0,0.0555075,"ndurthi and Oota, 2017) employed a supervised approach for the detection of homographic puns. It used a bi-directional RNN for a classification model and adopted the distributed semantic word embeddings as input features. These methods do not consider the collocation between words in homographic puns. The attention mechanism proposed by (Bahdanau et al., 2014) to settle machine translation problem, which was used to select the reference words for words before translation. (Xu et al., 2015) used attention model for image generation to select the similar image regions. For text classification, (Yang et al., 2016) applied attention mechanism into solving document-level classification. Many other tasks in NLP used this mechanism, including natural language question answering (Kumar et al., 2015), parsing (Vinyals et al., 2014), image question answering(Yang et al., 2015), and classification(Shen et al., 2018; Tan et al., 2018). Therefore, this model is capable of discovering the important and semantic information. Meanwhile, attention mechanism can also improve the performance of classification tasks. Hence, we explore an attention mechanism for collocation to mine the latent semantic information betwee"
H05-1054,W98-1120,0,0.0685884,"Missing"
H05-1054,M95-1012,0,0.0352926,"Missing"
H05-1054,M98-1001,0,0.029034,"Missing"
H05-1054,W03-1509,1,0.889564,"Missing"
H05-1054,P00-1015,0,0.0625794,"Missing"
H05-1054,C02-1012,0,0.224382,"ised learning like Error-driven [Aberdeen, et al. 1995], Decision Tree [Sekine, et al. 1998], HMM[Bikel, et al. 1997] and Maximum Entropy[Borthwick, et al. 1999][Mikheev, et al.1998]. Similarly, the models for Chinese NER can also be divided into two categories: Individual Model and Integrated Model. Individual Model[Chen, et al. 1998][Sun, et al. 1994][Zheng, et al. 2000] consists of several submodels, each of them deals with a kind of entities. For example, the recognition of PN may be statistical-based model, while LN and ON may be rulebased model like [Chen, et al. 1998]. Integrated Model[Sun, et al. 2002] [Zhang, et al. 2003][Yu, et al. 1998][Chua, et al. 2002] deals with all kinds of 428 entities in a unified statistical framework. Most of these integrated models can be viewed as a HMM model. The differences among them are the definition of state and the features used in entity model and context model. In fact, a NER model recognizes named entities through mining the intrinsic features in the entities and the contextual features around the entities. Most of existing approaches employ either coarse particle features, like POS and ROLE[Zhang, et al. 2003], or fine particle features like word."
H05-1054,O03-5002,0,0.0465195,"Missing"
H05-1054,A97-1029,0,0.0360009,"Missing"
H05-1054,M98-1021,0,0.0955011,"Missing"
H05-1054,M95-1001,0,0.0633419,"Missing"
H05-1054,M98-1014,0,0.0383546,"efore keywords are accepted as the candidate ONs. 8. An organization name template list: We mainly use organization name templates to recognize the missed nested ONs in the statistical model. Some of these templates are as follows: ON--&gt;LN D* OrgKeyWord ON--&gt;PN D* OrgKeyWord ON--&gt;ON OrgKeyWord D and OrgKeyWord denote words in the middle of ONs and ONs keywords. D* means repeating zero or more times. 5 Back-off Model to Smooth Data sparseness problem still exists. As some parameters were never observed in training corpus, the model will back off to a less powerful model. The escape probability[Black, et al. 1998] was adopted to smooth the statistical model shown as (15). ^ p( W N W1 LW N 1 ) = λ N p( W N W 1LW N 1 ) + (15) λ N 1 p( W N W2 LWN 1 ) + L + λ1 p( W N ) + λ0 p0 431 N where λ N = 1 e N , λ i = ( 1 e i ) ∑e k ,0 &lt; i &lt; N , and ei k = i +1 is the escape probability which can be estimated by equation (16). eN = q( W1W 2 LW N 1 ) f ( W1W 2 LW N 1 ) (16) q(w1w2…wN-1) in (16) denotes the number of different symbol wN that have directly followed the word sequence w1w2…wN-1. 6 Experiments In this chapter, we will conduct experiments to answer the following questions. Will the Hybrid Model be more ef"
H05-1054,W99-0613,0,0.104038,"Missing"
H05-1054,M98-1016,0,0.776795,"Missing"
I05-3006,W99-0613,0,0.132517,"Missing"
I05-3006,bick-2004-named,0,0.0412842,"Missing"
I05-3006,O04-2004,0,0.0493047,"Missing"
I05-3006,P03-1043,0,0.0134333,"eristics for this task. [Pierre 2002] developed an English NER system capable of identifying product names in product views. It employed a simple Boolean classifier for identifying product name, which was constructed from the list of product names. The method is similar to token matching and has a limitation for product NER applications. [Bick et al. 2004] recognized named entities including product names based on constraint grammar based parser for Danish. This rule-based approach is highly dependent on the performance of Danish parser and suffers from its weakness in system portability. [C. Niu et al. 2003] presented a bootstrapping approach for English named entity recognition using successive learners of parsing-based decision 1 Introduction Named entity recognition(NER) plays a significantly important role in information extraction(IE) and many other applications. Previous study on NER is mainly focused either on the proper name identification of person(PER), location(LOC), organization(ORG), time(TIM) and numeral(NUM) expressions almost in news domain, which can be viewed as general NER, or other named entity (NE) recognition in specific domain such as biology. As far as we know, however, t"
I05-3006,M95-1012,0,\N,Missing
I05-3006,C02-1012,0,\N,Missing
I05-3006,O03-5002,0,\N,Missing
I05-3006,A97-1029,0,\N,Missing
I13-1129,J03-1002,0,0.00926456,"Missing"
I13-1129,P02-1040,0,0.0912291,"Missing"
I13-1129,2007.mtsummit-papers.50,0,0.0185859,"rase-based (HPB) translation model to detect the alignments in comparable sentence pairs. This method enables us to extract useful training data for statistical machine translation (SMT) system. We evaluate our method by fragment detection and large-scale translation tasks, which show that our method can effectively extract parallel fragments and improve the performance of the state-of-the-art SMT system. 1 Figure 1: Example of comparable sentence pairs. The parallel fragments are marked by underlines. Introduction In order to deal with this problem, further efforts (Munteanu and Marcu, 2006; Quirk et al., 2007; Kumano et al., 2007; Lardilleux et al., 2012) were made to obtain parallel data at the fragment level. The work of (Riesa and Marcu, 2012) detected parallel fragments using the hierarchical alignment model. However, this approach obtains fragments from parallel sentence pairs, which limits its application in comparable corpora. (Hewavitharana and Vogel, 2011) have explored several alignment approaches to detect parallel fragments embedded in comparable sentences. However, these approaches extract fragments mainly using the lexical features and considering the words in parallel fragments are"
I13-1129,E09-1003,0,0.0433688,"Missing"
I13-1129,N12-1061,0,0.0199773,"ing data for statistical machine translation (SMT) system. We evaluate our method by fragment detection and large-scale translation tasks, which show that our method can effectively extract parallel fragments and improve the performance of the state-of-the-art SMT system. 1 Figure 1: Example of comparable sentence pairs. The parallel fragments are marked by underlines. Introduction In order to deal with this problem, further efforts (Munteanu and Marcu, 2006; Quirk et al., 2007; Kumano et al., 2007; Lardilleux et al., 2012) were made to obtain parallel data at the fragment level. The work of (Riesa and Marcu, 2012) detected parallel fragments using the hierarchical alignment model. However, this approach obtains fragments from parallel sentence pairs, which limits its application in comparable corpora. (Hewavitharana and Vogel, 2011) have explored several alignment approaches to detect parallel fragments embedded in comparable sentences. However, these approaches extract fragments mainly using the lexical features and considering the words in parallel fragments are independent, which make it difficult to measure the alignments exactly. In this paper, we present a phrase-based method, which considers bot"
I13-1129,P05-1033,0,0.22315,"Missing"
I13-1129,J07-2003,0,0.188554,"Missing"
I13-1129,N10-1063,0,0.0475286,"Missing"
I13-1129,2009.iwslt-papers.2,0,0.0414044,"Missing"
I13-1129,J05-4003,0,0.0749088,"Missing"
I13-1129,W11-1209,0,0.0348081,"prove the performance of the state-of-the-art SMT system. 1 Figure 1: Example of comparable sentence pairs. The parallel fragments are marked by underlines. Introduction In order to deal with this problem, further efforts (Munteanu and Marcu, 2006; Quirk et al., 2007; Kumano et al., 2007; Lardilleux et al., 2012) were made to obtain parallel data at the fragment level. The work of (Riesa and Marcu, 2012) detected parallel fragments using the hierarchical alignment model. However, this approach obtains fragments from parallel sentence pairs, which limits its application in comparable corpora. (Hewavitharana and Vogel, 2011) have explored several alignment approaches to detect parallel fragments embedded in comparable sentences. However, these approaches extract fragments mainly using the lexical features and considering the words in parallel fragments are independent, which make it difficult to measure the alignments exactly. In this paper, we present a phrase-based method, which considers both the lexical and phrasal features, to extract parallel fragments from comparable corpora. We introduce a force decoder based on the HPB translation model to detect parallel fragments for each sentence pair. The results sho"
I13-1129,C10-1054,0,0.0361433,"Missing"
I13-1129,2007.tmi-papers.12,0,0.045067,"nslation model to detect the alignments in comparable sentence pairs. This method enables us to extract useful training data for statistical machine translation (SMT) system. We evaluate our method by fragment detection and large-scale translation tasks, which show that our method can effectively extract parallel fragments and improve the performance of the state-of-the-art SMT system. 1 Figure 1: Example of comparable sentence pairs. The parallel fragments are marked by underlines. Introduction In order to deal with this problem, further efforts (Munteanu and Marcu, 2006; Quirk et al., 2007; Kumano et al., 2007; Lardilleux et al., 2012) were made to obtain parallel data at the fragment level. The work of (Riesa and Marcu, 2012) detected parallel fragments using the hierarchical alignment model. However, this approach obtains fragments from parallel sentence pairs, which limits its application in comparable corpora. (Hewavitharana and Vogel, 2011) have explored several alignment approaches to detect parallel fragments embedded in comparable sentences. However, these approaches extract fragments mainly using the lexical features and considering the words in parallel fragments are independent, which ma"
I13-1129,2012.eamt-1.62,0,0.0210267,"ect the alignments in comparable sentence pairs. This method enables us to extract useful training data for statistical machine translation (SMT) system. We evaluate our method by fragment detection and large-scale translation tasks, which show that our method can effectively extract parallel fragments and improve the performance of the state-of-the-art SMT system. 1 Figure 1: Example of comparable sentence pairs. The parallel fragments are marked by underlines. Introduction In order to deal with this problem, further efforts (Munteanu and Marcu, 2006; Quirk et al., 2007; Kumano et al., 2007; Lardilleux et al., 2012) were made to obtain parallel data at the fragment level. The work of (Riesa and Marcu, 2012) detected parallel fragments using the hierarchical alignment model. However, this approach obtains fragments from parallel sentence pairs, which limits its application in comparable corpora. (Hewavitharana and Vogel, 2011) have explored several alignment approaches to detect parallel fragments embedded in comparable sentences. However, these approaches extract fragments mainly using the lexical features and considering the words in parallel fragments are independent, which make it difficult to measure"
I13-1129,P06-1011,0,0.0169447,"sed on the hierarchical phrase-based (HPB) translation model to detect the alignments in comparable sentence pairs. This method enables us to extract useful training data for statistical machine translation (SMT) system. We evaluate our method by fragment detection and large-scale translation tasks, which show that our method can effectively extract parallel fragments and improve the performance of the state-of-the-art SMT system. 1 Figure 1: Example of comparable sentence pairs. The parallel fragments are marked by underlines. Introduction In order to deal with this problem, further efforts (Munteanu and Marcu, 2006; Quirk et al., 2007; Kumano et al., 2007; Lardilleux et al., 2012) were made to obtain parallel data at the fragment level. The work of (Riesa and Marcu, 2012) detected parallel fragments using the hierarchical alignment model. However, this approach obtains fragments from parallel sentence pairs, which limits its application in comparable corpora. (Hewavitharana and Vogel, 2011) have explored several alignment approaches to detect parallel fragments embedded in comparable sentences. However, these approaches extract fragments mainly using the lexical features and considering the words in par"
I13-1129,P03-1021,0,\N,Missing
I17-1017,P14-1062,0,0.0123784,"actions between local context and previous tag. Chen et al. (2015a) proposed a gated recursive neural network (GRNN) to model the combinations of context characters. Chen et al. (2015b) utilized Long short-term memory (LSTM) to capture long distant dependencies. Xu and Sun (2016) combined LSTM and GRNN to efficiently integrate local and long-distance features. Our proposed model is also a neural sequence labeling model. The difference from above models lies in that CNN is used to encode contextual information. CNNs have been successfully applied in many NLP tasks, such as text classification (Kalchbrenner et al., 2014; Kim, 2014; Zhang et al., 2015; Conneau et al., 2016), language modeling (Kim et al., 2016; Pham et al., 2016; Dauphin et al., 2016), machine translation (Meng et al., 2015; Kalchbrenner et al., 2016; Gehring et al., 2016). Experimental results show that the convolutional layers are capable to capture more n-gram features than previous introduced networks. Collobert et al. (2011) also proposed a CNN based seuqence labeling model. However, our model Simillar to this work, Wang et al. (2011) and Zhang et al. (2013) also enhanced character-based CWS systems by utilizing auto-segmented data. Howe"
I17-1017,W06-1655,0,0.44592,"st natural language processing (NLP) applications are word-based. Therefore, word segmentation is an essential step for processing those languages. CWS is often treated as a characterbased sequence labeling task (Xue et al., 2003; Peng et al., 2004). Figure 1 gives an intuitive 163 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 163–172, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP their works. The second is that they make no use of full word information. Full word information has shown its effectiveness in word-based CWS systems (Andrew, 2006; Zhang and Clark, 2007; Sun et al., 2009). Recently, Liu et al. (2016); Zhang et al. (2016) utilized word embeddings to boost performance of word-based CWS models. However, for character-based CWS models, word information is not easy to be integrated. For the first weakness, we propose a convolutional neural model, which is also character-based. Previous works have shown that convolutional layers have the ablity to capture rich n-gram features (Kim et al., 2016). We use stacked convolutional layers to derive contextual representations from input sequence, which are then fed into a CRF layer f"
I17-1017,D14-1181,0,0.00475005,"xt and previous tag. Chen et al. (2015a) proposed a gated recursive neural network (GRNN) to model the combinations of context characters. Chen et al. (2015b) utilized Long short-term memory (LSTM) to capture long distant dependencies. Xu and Sun (2016) combined LSTM and GRNN to efficiently integrate local and long-distance features. Our proposed model is also a neural sequence labeling model. The difference from above models lies in that CNN is used to encode contextual information. CNNs have been successfully applied in many NLP tasks, such as text classification (Kalchbrenner et al., 2014; Kim, 2014; Zhang et al., 2015; Conneau et al., 2016), language modeling (Kim et al., 2016; Pham et al., 2016; Dauphin et al., 2016), machine translation (Meng et al., 2015; Kalchbrenner et al., 2016; Gehring et al., 2016). Experimental results show that the convolutional layers are capable to capture more n-gram features than previous introduced networks. Collobert et al. (2011) also proposed a CNN based seuqence labeling model. However, our model Simillar to this work, Wang et al. (2011) and Zhang et al. (2013) also enhanced character-based CWS systems by utilizing auto-segmented data. However, they d"
I17-1017,J96-1002,0,0.377619,"Missing"
I17-1017,P16-1039,0,0.609998,"5. Train the student model with word feature using the pretrained word embeddings. Note that no external labeled data is used in this procedure. 4 http://www.sogou.com/labs/resource/ ca.php 5 We also try to use fixed word embeddings as Zhang et al. (2016) do but no significant difference is observed. 2 The threshold of frequency is set to 5, which is the default setting of word2vec. 3 https://code.google.com/p/word2vec 166 Models (Tseng, 2005) (Zhang and Clark, 2007) (Zhao and Kit, 2011) (Sun et al., 2012) (Zhang et al., 2013) (Pei et al., 2014) (Chen et al., 2015a)∗ (Chen et al., 2015b)∗ (Cai and Zhao, 2016) (Liu et al., 2016) (Zhang et al., 2016) (Xu and Sun, 2016)∗ CONV-SEG WE-CONV-SEG (+ word embeddings) PKU R 95.4 94.9 95.8 96.3 96.4 95.2 95.2 96.1 P 94.6 95.8 96.5 96.5 96.6 95.8 96.1 96.8 F 95.0 94.5 95.40 95.4 96.1 95.2 96.4 96.5 95.5 95.67 95.7 96.1 95.7 96.5 MSR R 96.6 97.2 97.8 97.3 96.8 97.3 98.1 P 96.2 97.6 97.4 97.5 96.3 97.4 97.9 F 96.4 97.2 97.58 97.4 97.45 97.2 97.6 97.4 96.5 97.58 97.7 96.3 97.3 98.0 Table 3: Performance of our models and previous state-of-the-art models. Note that (Chen et al., 2015a,b; Xu and Sun, 2016) used a external Chinese idiom dictionary. To make the com"
I17-1017,P15-1168,0,0.15131,"Missing"
I17-1017,D15-1141,0,0.362506,"Missing"
I17-1017,I13-1181,0,0.143541,"performance is significantly improved by the method. We also observe that the training cost of AVEBE-CONV-SEG is much lower than CONV-SEG. Hence we can conclude that the inconsistency is casued by overfitting. A reasonable conjecture is that the model CONV-SEG already capture abundant bigram feature automatically, therefore the model is tend to overfit when bigram feature is explicitly added. A practicable way to overcome overfitting is to introduce priori knowledge. We introduce priori knowledge by using bigram embeddings directly pretrained on large unlabeled data, which is simmillar with (Mansur et al., 2013). We convert the unlabeled text to bigram sequence and then apply word2vec to pretrain the bigram embeddings directly. The result model is named W2VBE-CONV-SEG, and the performance is also shown in Table 5. This method leads to substantial improvements (+0.5 on PKU and +0.4 MSR) over AVEBE-CONV-SEG. However, compared to CONV-SEG, there are only slight gains (+0.2 on PKU and MSR). All above observations verify that our proposed network has considerable superiority in capturing n-gram, at least bigram features automatically. 4.6 PKU MSR 98.0 0.4 test score 97.5 97.0 96.5 0.4 96.0 95.5 1/1 1/2 1/"
I17-1017,P15-1003,0,0.025873,"15b) utilized Long short-term memory (LSTM) to capture long distant dependencies. Xu and Sun (2016) combined LSTM and GRNN to efficiently integrate local and long-distance features. Our proposed model is also a neural sequence labeling model. The difference from above models lies in that CNN is used to encode contextual information. CNNs have been successfully applied in many NLP tasks, such as text classification (Kalchbrenner et al., 2014; Kim, 2014; Zhang et al., 2015; Conneau et al., 2016), language modeling (Kim et al., 2016; Pham et al., 2016; Dauphin et al., 2016), machine translation (Meng et al., 2015; Kalchbrenner et al., 2016; Gehring et al., 2016). Experimental results show that the convolutional layers are capable to capture more n-gram features than previous introduced networks. Collobert et al. (2011) also proposed a CNN based seuqence labeling model. However, our model Simillar to this work, Wang et al. (2011) and Zhang et al. (2013) also enhanced character-based CWS systems by utilizing auto-segmented data. However, they didn’t use word embeddings, but only used statistics features. Sun (2010) and Wang et al. (2014) combined character-based and wordbased CWS model via bagging and d"
I17-1017,P14-1028,0,0.257561,"ver, they often depend heavily on well-designed hand-crafted features. Recently, neural networks have been widely used for NLP tasks. Collobert et al. (2011) proposed a unified neural architecture for various sequence labeling tasks. Instead of exploiting handcrafted input features carefully optimized for each task, their system learns internal representations automatically. As for CWS, there are a series of works, which share the main idea with Collobert et al. (2011) but vary in the network architecture. In particular, feed-forward neural network (Zheng et al., 2013), tensor neural network (Pei et al., 2014), recursive neural network (Chen et al., 2015a), long-short term memory (LSTM) (Chen et al., 2015b), as well as the combination of LSTM and recursive neural network (Xu and Sun, 2016) have been used to derive contextual representations from input character sequences, which are then fed to a prediction layer. Despite of the great success of above models, they have two weaknesses. The first is that they are not good at capturing n-gram features automatically. Experimental results show that their models perform badly when no bigram feature is explicitly used. One of the strengths of neural networ"
I17-1017,P14-2032,0,0.0184035,"016; Pham et al., 2016; Dauphin et al., 2016), machine translation (Meng et al., 2015; Kalchbrenner et al., 2016; Gehring et al., 2016). Experimental results show that the convolutional layers are capable to capture more n-gram features than previous introduced networks. Collobert et al. (2011) also proposed a CNN based seuqence labeling model. However, our model Simillar to this work, Wang et al. (2011) and Zhang et al. (2013) also enhanced character-based CWS systems by utilizing auto-segmented data. However, they didn’t use word embeddings, but only used statistics features. Sun (2010) and Wang et al. (2014) combined character-based and wordbased CWS model via bagging and dual decomposition respectively and achieved better performance than single model. 6 Conclusion In this paper, we address the weaknesses of character-based CWS models. We propose a novel neural model for CWS. The model utilizes stacked convolutional layers to derive contextual representations from input sequence, which are then fed to a CRF layer for prediction. The model is capable to capture rich n-gram features automatically. Furthermore, we propose an effective approach to integrate the proposed model with word embeddings, w"
I17-1017,C04-1081,0,0.833055,"ir models perform badly when no bigram feature is explicitly used. One of the strengths of neural networks is the ability to learn features automatically. However, this strength has not been well exploited in Introduction Unlike English and other western languages, most east Asian languages, including Chinese, are written without explicit word delimiters. However, most natural language processing (NLP) applications are word-based. Therefore, word segmentation is an essential step for processing those languages. CWS is often treated as a characterbased sequence labeling task (Xue et al., 2003; Peng et al., 2004). Figure 1 gives an intuitive 163 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 163–172, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP their works. The second is that they make no use of full word information. Full word information has shown its effectiveness in word-based CWS systems (Andrew, 2006; Zhang and Clark, 2007; Sun et al., 2009). Recently, Liu et al. (2016); Zhang et al. (2016) utilized word embeddings to boost performance of word-based CWS models. However, for character-based CWS models, word information is not easy to"
I17-1017,I11-1035,0,0.133772,"Missing"
I17-1017,D16-1123,0,0.0187979,"o model the combinations of context characters. Chen et al. (2015b) utilized Long short-term memory (LSTM) to capture long distant dependencies. Xu and Sun (2016) combined LSTM and GRNN to efficiently integrate local and long-distance features. Our proposed model is also a neural sequence labeling model. The difference from above models lies in that CNN is used to encode contextual information. CNNs have been successfully applied in many NLP tasks, such as text classification (Kalchbrenner et al., 2014; Kim, 2014; Zhang et al., 2015; Conneau et al., 2016), language modeling (Kim et al., 2016; Pham et al., 2016; Dauphin et al., 2016), machine translation (Meng et al., 2015; Kalchbrenner et al., 2016; Gehring et al., 2016). Experimental results show that the convolutional layers are capable to capture more n-gram features than previous introduced networks. Collobert et al. (2011) also proposed a CNN based seuqence labeling model. However, our model Simillar to this work, Wang et al. (2011) and Zhang et al. (2013) also enhanced character-based CWS systems by utilizing auto-segmented data. However, they didn’t use word embeddings, but only used statistics features. Sun (2010) and Wang et al. (2014) com"
I17-1017,P16-2092,0,0.810384,"architecture for various sequence labeling tasks. Instead of exploiting handcrafted input features carefully optimized for each task, their system learns internal representations automatically. As for CWS, there are a series of works, which share the main idea with Collobert et al. (2011) but vary in the network architecture. In particular, feed-forward neural network (Zheng et al., 2013), tensor neural network (Pei et al., 2014), recursive neural network (Chen et al., 2015a), long-short term memory (LSTM) (Chen et al., 2015b), as well as the combination of LSTM and recursive neural network (Xu and Sun, 2016) have been used to derive contextual representations from input character sequences, which are then fed to a prediction layer. Despite of the great success of above models, they have two weaknesses. The first is that they are not good at capturing n-gram features automatically. Experimental results show that their models perform badly when no bigram feature is explicitly used. One of the strengths of neural networks is the ability to learn features automatically. However, this strength has not been well exploited in Introduction Unlike English and other western languages, most east Asian langu"
I17-1017,O03-4002,0,0.66948,"Missing"
I17-1017,D13-1031,0,0.102642,"been successfully applied in many NLP tasks, such as text classification (Kalchbrenner et al., 2014; Kim, 2014; Zhang et al., 2015; Conneau et al., 2016), language modeling (Kim et al., 2016; Pham et al., 2016; Dauphin et al., 2016), machine translation (Meng et al., 2015; Kalchbrenner et al., 2016; Gehring et al., 2016). Experimental results show that the convolutional layers are capable to capture more n-gram features than previous introduced networks. Collobert et al. (2011) also proposed a CNN based seuqence labeling model. However, our model Simillar to this work, Wang et al. (2011) and Zhang et al. (2013) also enhanced character-based CWS systems by utilizing auto-segmented data. However, they didn’t use word embeddings, but only used statistics features. Sun (2010) and Wang et al. (2014) combined character-based and wordbased CWS model via bagging and dual decomposition respectively and achieved better performance than single model. 6 Conclusion In this paper, we address the weaknesses of character-based CWS models. We propose a novel neural model for CWS. The model utilizes stacked convolutional layers to derive contextual representations from input sequence, which are then fed to a CRF laye"
I17-1017,P16-1040,0,0.533952,"mentation is an essential step for processing those languages. CWS is often treated as a characterbased sequence labeling task (Xue et al., 2003; Peng et al., 2004). Figure 1 gives an intuitive 163 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 163–172, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP their works. The second is that they make no use of full word information. Full word information has shown its effectiveness in word-based CWS systems (Andrew, 2006; Zhang and Clark, 2007; Sun et al., 2009). Recently, Liu et al. (2016); Zhang et al. (2016) utilized word embeddings to boost performance of word-based CWS models. However, for character-based CWS models, word information is not easy to be integrated. For the first weakness, we propose a convolutional neural model, which is also character-based. Previous works have shown that convolutional layers have the ablity to capture rich n-gram features (Kim et al., 2016). We use stacked convolutional layers to derive contextual representations from input sequence, which are then fed into a CRF layer for sequence-level prediction. For the second weakness, we propose an effective approach to i"
I17-1017,C10-2139,0,0.0580636,"(Kim et al., 2016; Pham et al., 2016; Dauphin et al., 2016), machine translation (Meng et al., 2015; Kalchbrenner et al., 2016; Gehring et al., 2016). Experimental results show that the convolutional layers are capable to capture more n-gram features than previous introduced networks. Collobert et al. (2011) also proposed a CNN based seuqence labeling model. However, our model Simillar to this work, Wang et al. (2011) and Zhang et al. (2013) also enhanced character-based CWS systems by utilizing auto-segmented data. However, they didn’t use word embeddings, but only used statistics features. Sun (2010) and Wang et al. (2014) combined character-based and wordbased CWS model via bagging and dual decomposition respectively and achieved better performance than single model. 6 Conclusion In this paper, we address the weaknesses of character-based CWS models. We propose a novel neural model for CWS. The model utilizes stacked convolutional layers to derive contextual representations from input sequence, which are then fed to a CRF layer for prediction. The model is capable to capture rich n-gram features automatically. Furthermore, we propose an effective approach to integrate the proposed model"
I17-1017,P07-1106,0,0.420068,"guage processing (NLP) applications are word-based. Therefore, word segmentation is an essential step for processing those languages. CWS is often treated as a characterbased sequence labeling task (Xue et al., 2003; Peng et al., 2004). Figure 1 gives an intuitive 163 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 163–172, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP their works. The second is that they make no use of full word information. Full word information has shown its effectiveness in word-based CWS systems (Andrew, 2006; Zhang and Clark, 2007; Sun et al., 2009). Recently, Liu et al. (2016); Zhang et al. (2016) utilized word embeddings to boost performance of word-based CWS models. However, for character-based CWS models, word information is not easy to be integrated. For the first weakness, we propose a convolutional neural model, which is also character-based. Previous works have shown that convolutional layers have the ablity to capture rich n-gram features (Kim et al., 2016). We use stacked convolutional layers to derive contextual representations from input sequence, which are then fed into a CRF layer for sequence-level predi"
I17-1017,P12-1027,0,0.0377426,"Missing"
I17-1017,D13-1061,0,0.761184,"idely used for sequence labeling tasks. However, they often depend heavily on well-designed hand-crafted features. Recently, neural networks have been widely used for NLP tasks. Collobert et al. (2011) proposed a unified neural architecture for various sequence labeling tasks. Instead of exploiting handcrafted input features carefully optimized for each task, their system learns internal representations automatically. As for CWS, there are a series of works, which share the main idea with Collobert et al. (2011) but vary in the network architecture. In particular, feed-forward neural network (Zheng et al., 2013), tensor neural network (Pei et al., 2014), recursive neural network (Chen et al., 2015a), long-short term memory (LSTM) (Chen et al., 2015b), as well as the combination of LSTM and recursive neural network (Xu and Sun, 2016) have been used to derive contextual representations from input character sequences, which are then fed to a prediction layer. Despite of the great success of above models, they have two weaknesses. The first is that they are not good at capturing n-gram features automatically. Experimental results show that their models perform badly when no bigram feature is explicitly u"
I17-1017,N09-1007,0,0.454712,"applications are word-based. Therefore, word segmentation is an essential step for processing those languages. CWS is often treated as a characterbased sequence labeling task (Xue et al., 2003; Peng et al., 2004). Figure 1 gives an intuitive 163 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 163–172, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP their works. The second is that they make no use of full word information. Full word information has shown its effectiveness in word-based CWS systems (Andrew, 2006; Zhang and Clark, 2007; Sun et al., 2009). Recently, Liu et al. (2016); Zhang et al. (2016) utilized word embeddings to boost performance of word-based CWS models. However, for character-based CWS models, word information is not easy to be integrated. For the first weakness, we propose a convolutional neural model, which is also character-based. Previous works have shown that convolutional layers have the ablity to capture rich n-gram features (Kim et al., 2016). We use stacked convolutional layers to derive contextual representations from input sequence, which are then fed into a CRF layer for sequence-level prediction. For the seco"
I17-1017,I05-3017,0,\N,Missing
N18-1122,D13-1176,0,0.0894095,"ionally, the static sentence-level BLEU is utilized as the reinforced objective for the generator, which biases the generation towards high BLEU points. During training, both the dynamic discriminator and the static BLEU objective are employed to evaluate the generated sentences and feedback the evaluations to guide the learning of the generator. Experimental results show that the proposed model consistently outperforms the traditional RNNSearch and the newly emerged state-ofthe-art Transformer on English-German and Chinese-English translation tasks. 1 Introduction Neural machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014) which directly leverages a single neural network to transform the source sentence into the target sentence, has drawn more and more attention in both academia and industry (Shen et al., 2015; Wu et al., 2016; Johnson et al., 2016; Gehring et al., 2017; Vaswani et al., 2017). This end-to-end NMT typically consists of two sub neural networks. The encoder network reads and encodes the source sentence into the context vector representation; and the decoder network generates the target sentence word by word based on the context vect"
N18-1122,E06-1032,0,0.0930963,"Missing"
N18-1122,D17-1230,0,0.178479,"use the idea to domain adaptation tasks. For sequence generation problem, (Yu et al., 2016) leverage policy gradient reinforcement learning to back-propagate the reward from the discriminator, showing presentable results for poem generation, speech language generation and music generation. Similarly, (Zhang et al., 2016) generate the text from random noise via adversarial training. A striking difference from the works mentioned above is that, our work is in the conditional setting where the target-language sentence is generated conditioned on the source-language one. In parallel to our work, (Li et al., 2017) propose a similar conditional sequence generative adversarial training for dialogue generation. They use a hierarchical long-short term memory (LSTM) architecture for the discriminator. In contrast to their approach, we apply the CNN-based discriminator for the machine translation task. Furthermore, we propose to utilize the sentence-level BLEU as the specific objective for the generator. Detailed training strategies for the proposed model and extensive quantitative results are reported. We noticed that (Wu et al., 2017) is exploring the potential of GAN in NMT too. There are some differences"
N18-1122,C12-1121,0,0.0298604,"a human-generated sentence, and the discriminator makes efforts not to be fooled by improving its ability to distinguish the machine-generated sentence from the human-generated one. This kind of adversarial training achieves a win-win situation when the generator and discriminator reach a Nash Equilibrium (Zhao et al., 2016; Arora et al., 2017; Guimaraes et al., 2017). Besides generating the desired distribution, we also want to directly guide the generator with a static and specific objective, such as generating sentences with high BLEU points. To this end, the smoothed sentence-level BLEU (Nakov et al., 2012) is utilized as the reinforced objective for the generator. During training, we employ both the dynamic discriminator and the static BLEU objective to evaluate the generated sentences and feedback the evaluations to guide the learning of the generator. In summary, we mainly make the following contributions: • To the best of our knowledge, this work is among the first endeavors to introduce the generative adversarial training into NMT. We directly train the NMT model to generate sentences which are hard to be discriminated from human translations. The proposed model can be applied to any end-to"
N18-1122,Q17-1036,0,0.0333077,"2 Generative adversarial nets Generative adversarial network, has enjoyed great success in computer vision and has been widely applied to image generation (Zhu et al., 2017; Radford et al., 2015). The conditional generative adversarial nets (Gauthier, 2014) apply an extension of generative adversarial network to a conditional setting, which enables the networks to condition on some arbitrary external data. Some recent works have begun to apply the generative adversarial training into the NLP area: (Chen et al., 2016) apply the idea of generative adversarial training to sentiment analysis and (Zhang et al., 2017) use the idea to domain adaptation tasks. For sequence generation problem, (Yu et al., 2016) leverage policy gradient reinforcement learning to back-propagate the reward from the discriminator, showing presentable results for poem generation, speech language generation and music generation. Similarly, (Zhang et al., 2016) generate the text from random noise via adversarial training. A striking difference from the works mentioned above is that, our work is in the conditional setting where the target-language sentence is generated conditioned on the source-language one. In parallel to our work,"
N18-1122,C16-1288,1,0.853438,"hinese-English translation, our training data consists of 1.6M sentence pairs randomly extracted from LDC corpora 5 . Both the source and target sentences are encoded with byte-pair encoding and the tokens in the source and target vocabulary is about 38000 and 34000 respectively 6 . We choose the NIST02 as the development set. For testing, we use NIST03, NIST04 and NIST05 data sets. To speed up the training procedure, sentences of length over 50 words are removed when we conduct experiments on the RNNSearch model. This is widely used by previous works (Ranzato et al., 2015; Shen et al., 2015; Yang et al., 2016). 4.2 Model parameters and evaluation For the Transformer, following the base model in (Vaswani et al., 2017), we set the dimension of word embedding as 512, dropout rate as 0.1 and the head number as 8. The encoder and decoder both have a stack of 6 layers. We use beam search with a beam size of 4 and length penalty α = 0.6. For the RNNSearch, following (Bahdanau et al., 2014), We set the hidden units for both encoders and decoders as 512. The dimension of the word embedding is also set as 512. We do not apply dropout for training the RNNSearch. During testing, we use beam search with a beam"
N19-1266,P17-1045,0,0.0276104,"Missing"
N19-1266,D16-1044,0,0.0460143,"ery single response of one agent in the two, based on the image and dialog history. There are also some different task settings such as directly training two agents to complete a goaldriven cooperative task such as Guessing Game (Das et al., 2017b). Tasks involving both the physical world (visual images) and abstract world (languages) share a core issue: how to establish connections between these two worlds, and is there a framework to leverage these connections for learning? Temporarily, the majority of answers are learning end-to-end models with multi-modal feature fusion (Kim et al., 2016; Fukui et al., 2016; Yu et al., 2018). These methods usually merge the visual and language features into rich representations containing information from both sides. Some cross-modal attention methods (Lu et al., 2016; Nam et al., 2017) formulate the visual-language connections explicitly by parameterizing the attention weights to learn whether there is high correlation within certain pairs of language and visual feature vectors. However, in all these works, the merged representations or attention weights are only learned from pairwise (one image, one sentence) co-occurrence, and serve for the optimization of a"
N19-1266,D17-1321,0,0.0187174,"iscrete, state-dependent and style-free, thus some reinforcement learning (RL) methods have been proposed (Li et al., 2016). Das et al. (2017b) built an cooperative image guessing task on VisDial: they train both the questioner and the answerer, making them complete a same goal to help the questioner produce a guessing or “imagination” of the unseen image described by 2589 the answerer. The distance between the guessing and the target image is used as reward for reinforcement learning. In some extreme settings, such a task deﬁnition can even lead to emergence of a new language between robots (Kottur et al., 2017). After per-training, using their reinforcement learning method as an auxiliary loss can also bring performance gain in standard VisDial metrics such as mean rank. However, generating a reward based on just one target image for a training sample may lead to a kind of overﬁtting. Language is highly abstract: one dialog can correctly describe a lot of different scenes in real world, so why should we force a dialog to ﬁt one single example among them? Therefore, generating a reward from adversarial training is a more efﬁcient way because it goes beyond individual samples into distributions. There"
N19-1266,D16-1127,0,0.0227141,"alog is a cluster of tasks sharing two properties: multi-turn and cross-modality. VisDial (Das et al., 2017a) is a widely-used benchmark with question-answering style dialogs grounded on real-world images. As a special case of dialog generation tasks, VisDial share some of the research concerns with single-modal natural language dialog generation (Dhingra et al., 2016; Serban et al., 2017; Sordoni et al., 2015; Serban et al., 2016; Liu et al., 2016). Natural language dialogs are usually discrete, state-dependent and style-free, thus some reinforcement learning (RL) methods have been proposed (Li et al., 2016). Das et al. (2017b) built an cooperative image guessing task on VisDial: they train both the questioner and the answerer, making them complete a same goal to help the questioner produce a guessing or “imagination” of the unseen image described by 2589 the answerer. The distance between the guessing and the target image is used as reward for reinforcement learning. In some extreme settings, such a task deﬁnition can even lead to emergence of a new language between robots (Kottur et al., 2017). After per-training, using their reinforcement learning method as an auxiliary loss can also bring per"
N19-1266,D17-1230,0,0.0382117,"sky et al., 2017) is a successful method using critic learning loss and weight clipping operations. We borrow some ideas from WGAN in the adversarial training of our model. GAN well suits the image generation tasks because image signals are continuous and thus differentiable, enabling the gradient directly ﬂowing back from the discriminator to generator. In language generation tasks, however, how to deal with the discrete sequence of symbols generated by the generator has long been a problem. A widely-used solution is applying RL with rewards generated by the discriminator (Wang et al., 2018; Li et al., 2017). As mentioned above, this is time-costing because RL needs to explore a large action space by sampling multiple action sequence. Besides, how the immediate reward is computed after generating each word is also a difﬁcult problem. Another solution is to avoid the discrete problem by applying adversarial training on the hidden states of the generator. This requires that there is a known distribution p for the hidden states we want the model to generate. A successful case is reported by (Lample et al., 2018): using adversarial training to restrict the hidden states of source language and target"
N19-1266,D16-1230,0,0.0338476,"ns to different visual dialog models. We achieve state-of-the-arts on most metrics of VisDial v0.5/v0.9 generative tasks. 2 2.1 Related Work Visual Dialog Visual Dialog is a cluster of tasks sharing two properties: multi-turn and cross-modality. VisDial (Das et al., 2017a) is a widely-used benchmark with question-answering style dialogs grounded on real-world images. As a special case of dialog generation tasks, VisDial share some of the research concerns with single-modal natural language dialog generation (Dhingra et al., 2016; Serban et al., 2017; Sordoni et al., 2015; Serban et al., 2016; Liu et al., 2016). Natural language dialogs are usually discrete, state-dependent and style-free, thus some reinforcement learning (RL) methods have been proposed (Li et al., 2016). Das et al. (2017b) built an cooperative image guessing task on VisDial: they train both the questioner and the answerer, making them complete a same goal to help the questioner produce a guessing or “imagination” of the unseen image described by 2589 the answerer. The distance between the guessing and the target image is used as reward for reinforcement learning. In some extreme settings, such a task deﬁnition can even lead to emer"
N19-1266,I17-1047,0,0.0204687,"tasks. 1 Introduction In recent years, there has been a rising attention in Artiﬁcial Intelligence on how to train a model to understand visual inputs from the physical world, and communicate them with human language. Typical problems include Visual Question Answering (VQA) (Antol et al., 2015) and Image Captioning (Xu et al., 2015). These tasks require a model to read an image and generate a proper response, such as answering a question grounded on the image, or generating a sentence to describe the image. As a more difﬁcult extension, Visual Dialog (De Vries et al., 2017; Das et al., 2017a; Mostafazadeh et al., 2017) is a cluster of tasks featuring two agents conducting a multi-turn dialog grounded on an image. A model is usually trained ∗ Corresponding Author to predict every single response of one agent in the two, based on the image and dialog history. There are also some different task settings such as directly training two agents to complete a goaldriven cooperative task such as Guessing Game (Das et al., 2017b). Tasks involving both the physical world (visual images) and abstract world (languages) share a core issue: how to establish connections between these two worlds, and is there a framework to"
N19-1266,N15-1020,0,0.0604441,"Missing"
N19-1266,P18-1083,0,0.0230398,"n GAN (WGAN) (Arjovsky et al., 2017) is a successful method using critic learning loss and weight clipping operations. We borrow some ideas from WGAN in the adversarial training of our model. GAN well suits the image generation tasks because image signals are continuous and thus differentiable, enabling the gradient directly ﬂowing back from the discriminator to generator. In language generation tasks, however, how to deal with the discrete sequence of symbols generated by the generator has long been a problem. A widely-used solution is applying RL with rewards generated by the discriminator (Wang et al., 2018; Li et al., 2017). As mentioned above, this is time-costing because RL needs to explore a large action space by sampling multiple action sequence. Besides, how the immediate reward is computed after generating each word is also a difﬁcult problem. Another solution is to avoid the discrete problem by applying adversarial training on the hidden states of the generator. This requires that there is a known distribution p for the hidden states we want the model to generate. A successful case is reported by (Lample et al., 2018): using adversarial training to restrict the hidden states of source la"
P12-1006,N10-1099,0,0.165634,"Road, Haidian district, Beijing 100190, China {xingyuan.peng,dengfeng.ke,xubo}@ia.ac.cn ∗ Abstract cus on the integrality, fluency, pronunciation, and prosody (Cucchiarini et al., 2000; Neumeyer et al., 2000; Maier et al., 2009; Huang et al., 2010) of the speech, which are highly predictable like the exam form of the read-aloud text passage. Another form of CALL is textual assessment. This work is also named AES. Efforts in this area usually focus on the content, arrangement and language usage (Landauer et al., 2003; Ishioka and Kameda, 2004; Kakkonen et al., 2005; Attali and Burstein, 2006; Burstein et al., 2010; Persing et al., 2010; Peng et al., 2010; Attali, 2011; Yannakoudakis et al., 2011) of the text written by the learner under a certain form of examination. Conventional Automated Essay Scoring (AES) measures may cause severe problems when directly applied in scoring Automatic Speech Recognition (ASR) transcription as they are error sensitive and unsuitable for the characteristic of ASR transcription. Therefore, we introduce a framework of Finite State Transducer (FST) to avoid the shortcomings. Compared with the Latent Semantic Analysis with Support Vector Regression (LSA-SVR) method (stands"
P12-1006,W05-0206,0,0.204628,"hinese Academy of Sciences No.95 Zhongguancun East Road, Haidian district, Beijing 100190, China {xingyuan.peng,dengfeng.ke,xubo}@ia.ac.cn ∗ Abstract cus on the integrality, fluency, pronunciation, and prosody (Cucchiarini et al., 2000; Neumeyer et al., 2000; Maier et al., 2009; Huang et al., 2010) of the speech, which are highly predictable like the exam form of the read-aloud text passage. Another form of CALL is textual assessment. This work is also named AES. Efforts in this area usually focus on the content, arrangement and language usage (Landauer et al., 2003; Ishioka and Kameda, 2004; Kakkonen et al., 2005; Attali and Burstein, 2006; Burstein et al., 2010; Persing et al., 2010; Peng et al., 2010; Attali, 2011; Yannakoudakis et al., 2011) of the text written by the learner under a certain form of examination. Conventional Automated Essay Scoring (AES) measures may cause severe problems when directly applied in scoring Automatic Speech Recognition (ASR) transcription as they are error sensitive and unsuitable for the characteristic of ASR transcription. Therefore, we introduce a framework of Finite State Transducer (FST) to avoid the shortcomings. Compared with the Latent Semantic Analysis with S"
P12-1006,W03-0209,0,0.0853238,"Missing"
P12-1006,N04-3012,0,0.0514243,"he two words. Then with the help of different cost of substitutions, each word edge is extended to some of its synonym word edges under the cost of similarity. The new edit distance is calculated by equation (7) as follows: FST build Set2 Set3 Set1 Set3 Set1 Set2 SVR train Set3 Set2 Set3 Set1 Set2 Set1 SVR test Set1 Set2 Set3 CRR transcription 0.7999 0.8185 0.8557 0.8111 0.9085 0.8860 ASR transcription 0.7505 0.7401 0.7372 0.7257 0.8086 0.8086 Table 4: Correlation Between the SSF and the Expert Scores where, sim is the similarity of two words. We used the Wordnet::Similarity software package (Pedersen et al., 2004) to calculate the similarity between every two words at first. However, the performance’s reduction of the AES system indicates that the similarity is not good enough to extend the FST model. Therefore, we seek for human help to accurate the similarity calculation. We manually checked the similarity, and deleted some improper similarity. Thus the final similarity applied in our experiment is the Wordnet::Similarity software computing result after the manual check. which includes the capitalization processing and the stemming processing. We divide them into 3 sets by the same distribution of th"
P12-1006,D10-1023,0,0.147061,", Beijing 100190, China {xingyuan.peng,dengfeng.ke,xubo}@ia.ac.cn ∗ Abstract cus on the integrality, fluency, pronunciation, and prosody (Cucchiarini et al., 2000; Neumeyer et al., 2000; Maier et al., 2009; Huang et al., 2010) of the speech, which are highly predictable like the exam form of the read-aloud text passage. Another form of CALL is textual assessment. This work is also named AES. Efforts in this area usually focus on the content, arrangement and language usage (Landauer et al., 2003; Ishioka and Kameda, 2004; Kakkonen et al., 2005; Attali and Burstein, 2006; Burstein et al., 2010; Persing et al., 2010; Peng et al., 2010; Attali, 2011; Yannakoudakis et al., 2011) of the text written by the learner under a certain form of examination. Conventional Automated Essay Scoring (AES) measures may cause severe problems when directly applied in scoring Automatic Speech Recognition (ASR) transcription as they are error sensitive and unsuitable for the characteristic of ASR transcription. Therefore, we introduce a framework of Finite State Transducer (FST) to avoid the shortcomings. Compared with the Latent Semantic Analysis with Support Vector Regression (LSA-SVR) method (stands for the conventional m"
P12-1006,P11-1019,0,0.053219,"}@ia.ac.cn ∗ Abstract cus on the integrality, fluency, pronunciation, and prosody (Cucchiarini et al., 2000; Neumeyer et al., 2000; Maier et al., 2009; Huang et al., 2010) of the speech, which are highly predictable like the exam form of the read-aloud text passage. Another form of CALL is textual assessment. This work is also named AES. Efforts in this area usually focus on the content, arrangement and language usage (Landauer et al., 2003; Ishioka and Kameda, 2004; Kakkonen et al., 2005; Attali and Burstein, 2006; Burstein et al., 2010; Persing et al., 2010; Peng et al., 2010; Attali, 2011; Yannakoudakis et al., 2011) of the text written by the learner under a certain form of examination. Conventional Automated Essay Scoring (AES) measures may cause severe problems when directly applied in scoring Automatic Speech Recognition (ASR) transcription as they are error sensitive and unsuitable for the characteristic of ASR transcription. Therefore, we introduce a framework of Finite State Transducer (FST) to avoid the shortcomings. Compared with the Latent Semantic Analysis with Support Vector Regression (LSA-SVR) method (stands for the conventional measures), our FST method shows better performance especially t"
P14-1012,J10-4005,0,0.0231756,"or DAE feature learning. 3.1 le X X4 → P (f ), P (e) 1 Backward LM has been introduced by Xiong et al. (2011), which successfully capture both the preceding and succeeding contexts of the current word, and we estimate the backward LM by inverting the order in each sentence in the training data from the original order to the reverse order. 2 This corpus is used to train the translation model in our experiments, and we will describe it in detail in section 5.1. (k1 + 1)wfi (k3 + 1)wafi (K + wfi )(k3 + wafi ) 124 3.5 Phrase length Phrase length plays an important role in the translation process (Koehn, 2010; Hopkins and May, 2011). We normalize bidirectional phrase length by the maximum phrase length, and introduce them as the last type of input features X5 → len , lfn In summary, except for the first type of phrase feature X1 which is used by (Maskey and Zhou, 2012), we introduce another four types of effective phrase features X2 , X3 , X4 and X5 . Now, the input original phrase features X includes 16 features in our experiments, as follows, Figure 1: Pre-training consists of learning a stack of RBMs, and these RBMs create an unsupervised DBN. X → X1 , X2 , X3 , X4 , X5 We build the DAE network"
P14-1012,D13-1106,0,0.0677894,"ith the unsupervised DBN features, our semi-supervised DAE features are more effective and stable. 2 Related Work Recently, there has been growing interest in use of DNN for SMT tasks. Le et al. (2012) improved translation quality of n-gram translation model by using a bilingual neural LM, where translation probabilities are estimated using a continuous representation of translation units in lieu of standard discrete representations. Kalchbrenner and Blunsom (2013) introduced recurrent continuous translation models that comprise a class for purely continuous sentence-level translation models. Auli et al. (2013) presented a joint language and translation model based on a recurrent neural network which predicts target words based on an unbounded history of both source and target words. Liu et al. (2013) went beyond the log-linear model for SMT and proposed a novel additive neural networks based translation model, which overcome some of the shortcomings suffered by the log-linear model: linearity and the lack of deep interpretation and representation in features. Li et al. (2013) presented an ITG reordering classifier based on recursive autoencoders, and generated vector space representations for varia"
P14-1012,P07-2045,0,0.00340389,"h words). The LM corpus is the English side of the parallel data (BTEC, CJK and CWMT083 ) (1.34M sentences). Our development set is IWSLT 2005 test set (506 sentences), and our test set is IWSLT 2007 test set (489 sentences). NIST. The bilingual corpus is LDC4 (3.4M sentence pairs with 64/70M Chinese/English words). The LM corpus is the English side of the parallel data as well as the English Gigaword corpus (LDC2007T07) (11.3M sentences). Our development set is NIST 2005 MT evaluation set (1084 sentences), and our test set is NIST 2006 MT evaluation set (1664 sentences). We choose the Moses (Koehn et al., 2007) framework to implement our phrase-based machine system. The 4-gram LMs are estimated by the SRILM toolkit with modified Kneser-Ney To learn high-dimensional feature representation and to further improve the performance, we introduce a natural horizontal composition for DAEs that can be used to create large hidden layer representations simply by horizontally combining two (or more) DAEs (Baldi, 2012), as shown in Figure 3. Two single DAEs with architectures 16/m1 /16 and 16/m2 /16 can be trained and the hidden layers can be combined to yield an expanded hidden feature representation of size m1"
P14-1012,N03-1017,0,0.0772259,"odel. Using the 4 original phrase features in the phrase table as the input features, they pre-trained the DBN by contrastive divergence (Hinton, 2002), and generated new unsupervised DBN features using forward computation. These new features are appended as extra features to the phrase table for the translation decoder. However, the above approach has two major shortcomings. First, the input original features for the DBN feature learning are too simple, the limited 4 phrase features of each phrase pair, such as bidirectional phrase translation probability and bidirectional lexical weighting (Koehn et al., 2003), which are a bottleneck for learning effective feature representation. Second, it only uses the unsupervised layer-wise pre-training of DBN built with stacked sets of Restricted Boltzmann Machines (RBM) (Hinton, 2002), does not have a training objective, so its performance relies on the empirical parameters. Thus, this approach is unstable and the improvement is limited. In this paper, we strive to effectively address the above two shortcomings, and systematically explore the possibility of learning new features using deep (multilayer) neural networks (DNN, which is usually referred under the"
P14-1012,N12-1005,0,0.0162184,"AE addresses the problem of “back-propagation without a teacher” (Rumelhart et al., 1986), which makes the DAE learn more powerful and abstract features (Hinton and Salakhutdinov, 2006). For our semisupervised DAE feature learning task, we use the unsupervised pre-trained DBN to initialize DAE’s parameters and use the input original phrase features as the “teacher” for semi-supervised backpropagation. Compared with the unsupervised DBN features, our semi-supervised DAE features are more effective and stable. 2 Related Work Recently, there has been growing interest in use of DNN for SMT tasks. Le et al. (2012) improved translation quality of n-gram translation model by using a bilingual neural LM, where translation probabilities are estimated using a continuous representation of translation units in lieu of standard discrete representations. Kalchbrenner and Blunsom (2013) introduced recurrent continuous translation models that comprise a class for purely continuous sentence-level translation models. Auli et al. (2013) presented a joint language and translation model based on a recurrent neural network which predicts target words based on an unbounded history of both source and target words. Liu et"
P14-1012,D13-1054,0,0.0141773,"ced recurrent continuous translation models that comprise a class for purely continuous sentence-level translation models. Auli et al. (2013) presented a joint language and translation model based on a recurrent neural network which predicts target words based on an unbounded history of both source and target words. Liu et al. (2013) went beyond the log-linear model for SMT and proposed a novel additive neural networks based translation model, which overcome some of the shortcomings suffered by the log-linear model: linearity and the lack of deep interpretation and representation in features. Li et al. (2013) presented an ITG reordering classifier based on recursive autoencoders, and generated vector space representations for variable-sized phrases, which enable predicting orders to exploit syntactic and semantic information. Lu et al. (2014) adapted and extended the max-margin based RNN (Socher et al., 2011) into HPB translation with force decoding and converting tree, and proposed a RNN based word topology model for HPB translation, which successfully capture the topological structure of the words on the source side in a syntactically and semantically meaningful order. However, none of these abo"
P14-1012,P13-1078,0,0.200441,"(2012) improved translation quality of n-gram translation model by using a bilingual neural LM, where translation probabilities are estimated using a continuous representation of translation units in lieu of standard discrete representations. Kalchbrenner and Blunsom (2013) introduced recurrent continuous translation models that comprise a class for purely continuous sentence-level translation models. Auli et al. (2013) presented a joint language and translation model based on a recurrent neural network which predicts target words based on an unbounded history of both source and target words. Liu et al. (2013) went beyond the log-linear model for SMT and proposed a novel additive neural networks based translation model, which overcome some of the shortcomings suffered by the log-linear model: linearity and the lack of deep interpretation and representation in features. Li et al. (2013) presented an ITG reordering classifier based on recursive autoencoders, and generated vector space representations for variable-sized phrases, which enable predicting orders to exploit syntactic and semantic information. Lu et al. (2014) adapted and extended the max-margin based RNN (Socher et al., 2011) into HPB tra"
P14-1012,D10-1044,0,0.172011,"e strive to effectively address the above two shortcomings, and systematically explore the possibility of learning new features using deep (multilayer) neural networks (DNN, which is usually referred under the name Deep Learning) for SMT. To address the first shortcoming, we adapt and extend some simple but effective phrase features as the input features for new DNN feature learning, and these features have been shown significant improvement for SMT, such as, phrase pair similarity (Zhao et al., 2004), phrase frequency, phrase length (Hopkins and May, 2011), and phrase generative probability (Foster et al., 2010), which also show further improvement for new phrase feature learning in our experiments. To address the second shortcoming, inspired by the successful use of DAEs for handwritten digits recognition (Hinton and Salakhutdinov, 2006; Hinton et al., 2006), information retrieval (Salakhutdinov and Hinton, 2009; Mirowski et In this paper, instead of designing new features based on intuition, linguistic knowledge and domain, we learn some new and effective features using the deep autoencoder (DAE) paradigm for phrase-based translation model. Using the unsupervised pre-trained deep belief net (DBN) t"
P14-1012,P08-1114,0,0.0441917,"Missing"
P14-1012,W07-0710,0,0.0172756,"le learning suitable features (representations) is the superiority of DNN since it has been proposed. In this paper, we systematically explore the possibility of learning new feaMoreover, to learn high dimensional feature representation, we introduce a natural horizontal composition for DAEs (HCDAE) that can be used to create large hidden layer representations simply by horizontally combining two (or more) DAEs (Baldi, 2012), which shows further improvement compared with single DAE in our experiments. It is encouraging that, non-parametric feature expansion using gaussian mixture model (GMM) (Nguyen et al., 2007), which guarantees invariance to the specific embodiment of the original features, has been proved as a feasible feature generation approach for SMT. Deep models such as DNN have the potential to be much more representationally efficient for feature learning than shallow models like GMM. Thus, instead of GMM, we use DNN (DBN, DAE and HCDAE) to learn new non-parametric features, which has the similar evolution in speech recognition (Dahl et al., 2012; Hinton et al., 2012). DNN features are learned from the non-linear combination of the input original features, they strong capture highorder corr"
P14-1012,P00-1056,0,0.109037,"Missing"
P14-1012,D11-1125,0,0.118034,"Missing"
P14-1012,D13-1176,0,0.0209519,"ervised pre-trained DBN to initialize DAE’s parameters and use the input original phrase features as the “teacher” for semi-supervised backpropagation. Compared with the unsupervised DBN features, our semi-supervised DAE features are more effective and stable. 2 Related Work Recently, there has been growing interest in use of DNN for SMT tasks. Le et al. (2012) improved translation quality of n-gram translation model by using a bilingual neural LM, where translation probabilities are estimated using a continuous representation of translation units in lieu of standard discrete representations. Kalchbrenner and Blunsom (2013) introduced recurrent continuous translation models that comprise a class for purely continuous sentence-level translation models. Auli et al. (2013) presented a joint language and translation model based on a recurrent neural network which predicts target words based on an unbounded history of both source and target words. Liu et al. (2013) went beyond the log-linear model for SMT and proposed a novel additive neural networks based translation model, which overcome some of the shortcomings suffered by the log-linear model: linearity and the lack of deep interpretation and representation in fe"
P14-1012,P02-1038,0,0.107601,"rning a stack of RBMs, and these RBMs create an unsupervised DBN. X → X1 , X2 , X3 , X4 , X5 We build the DAE network where the first layer with visible nodes equaling to 16, and each visible node vi corresponds to the above original features X in each phrase pair. The two conditional distributions can be shown to correspond to the generative model, 4 where, P (v, h) = Semi-Supervised Deep Auto-encoder Features Learning for SMT Z= 1 exp(−E(v, h)) Z X e−E(v,h) v,h Each translation rule in the phrase-based translation model has a set number of features that are combined in the log-linear model (Och and Ney, 2002), and our semi-supervised DAE features can also be combined in this model. In this section, we design our DAE network with various network structures for new feature learning. E(v, h) = −bT v − cT h − v T W h Inspired by (Maskey and Zhou, 2012), we first learn a deep generative model for feature learning using DBN. DBN is composed of multiple layers of latent variables with the first layer representing the visible feature vectors, which is built with stacked sets of RBMs (Hinton, 2002). For a RBM, there is full connectivity between layers, but no connections within either layer. The connection"
P14-1012,W04-3250,0,0.15861,"∗∗∗ 34.47+∗∗∗ 34.65+∗∗∗∗ 34.46+∗∗∗ 34.57+∗∗∗∗ Table 2: The translation results by adding new DNN features (DBN feature (Maskey and Zhou, 2012), our proposed DAE and HCDAE feature) as extra features to the phrase table on two tasks. “DBN X1 xf”, “DBN X xf”, “DAE X1 xf” and “DAE X xf” represent that we use DBN and DAE, input features X1 and X, to learn x-dimensional features, respectively. “HCDAE X x+xf” represents horizontally combining two DAEs and each DAE has the same x-dimensional learned features. All improvements on two test sets are statistically significant by the bootstrap resampling (Koehn, 2004). *: significantly better than the baseline (p &lt; 0.05), **: significantly better than “DBN X1 xf” or “DBN X xf” (p &lt; 0.01), ***: significantly better than “DAE X1 xf” or “DAE X xf” (p &lt; 0.01), ****: significantly better than “HCDAE X x+xf” (p &lt; 0.01), +: significantly better than “X2 +X3 +X4 +X5 ” (p &lt; 0.01). discounting. We perform pairwise ranking optimization (Hopkins and May, 2011) to tune feature weights. The translation quality is evaluated by case-insensitive IBM BLEU-4 metric. The baseline translation models are generated by Moses with default parameter settings. In the contrast experi"
P14-1012,J04-4002,0,0.0118487,", respectively. Following (Maskey and Zhou, 2012), we use the following 4 phrase features of each phrase pair (Koehn et al., 2003) in the phrase table as the first type of input features, bidirectional phrase translation probability (P (e|f ) and P (f |e)), bidirectional lexical weighting (Lex(e|f ) and Lex(f |e)), 3.2 e idfej where, k1 , b, k3 are set to be 1, 1 and 1000, respectively. K = k1 ((1 − b) + J/avg(l)), and J is the phrase length (le or lf ), avg(l) is the average phrase length. Thus, we have the second type of input features The phrase-based translation model (Koehn et al., 2003; Och and Ney, 2004) has demonstrated superior performance and been widely used in current SMT systems, and we employ our implementation on this translation model. Next, we adapt and extend some original phrase features as the input features for DAE feature learning. 3.1 le X X4 → P (f ), P (e) 1 Backward LM has been introduced by Xiong et al. (2011), which successfully capture both the preceding and succeeding contexts of the current word, and we estimate the backward LM by inverting the order in each sentence in the training data from the original order to the reverse order. 2 This corpus is used to train the t"
P14-1012,P11-1129,0,0.0317395,"Missing"
P14-1012,W04-3227,0,0.194343,"es on the empirical parameters. Thus, this approach is unstable and the improvement is limited. In this paper, we strive to effectively address the above two shortcomings, and systematically explore the possibility of learning new features using deep (multilayer) neural networks (DNN, which is usually referred under the name Deep Learning) for SMT. To address the first shortcoming, we adapt and extend some simple but effective phrase features as the input features for new DNN feature learning, and these features have been shown significant improvement for SMT, such as, phrase pair similarity (Zhao et al., 2004), phrase frequency, phrase length (Hopkins and May, 2011), and phrase generative probability (Foster et al., 2010), which also show further improvement for new phrase feature learning in our experiments. To address the second shortcoming, inspired by the successful use of DAEs for handwritten digits recognition (Hinton and Salakhutdinov, 2006; Hinton et al., 2006), information retrieval (Salakhutdinov and Hinton, 2009; Mirowski et In this paper, instead of designing new features based on intuition, linguistic knowledge and domain, we learn some new and effective features using the deep autoenc"
P15-2058,D13-1170,0,0.00174962,"d the Dynamic Convolutional Neural Network (DCNN) for modeling sentences. Their work is closely related to our study in that k-max pooling is utilized to capture global feature vector and do not rely on parse tree. Kim (2014) proposed a simple improvement to the convolutional architecture that two input channels are used to allow the employment of task-specific and static word embeddings simultaneously. Related Works Zeng et al. (2014) developed a deep convolutional neural network (DNN) to extract lexical and sentence level features, which are concatenated and fed into the softmax classifier. Socher et al. (2013) proposed the Recursive Neural Network (RNN) that has been proven to be efficient in terms of constructing sentences representations. In order to reduce the overfitting of neural network especially trained on small data set, Hinton et al. (2012) used random dropout to prevent Traditional statistics-based methods usually fail to achieve satisfactory performance for short texts classification due to their sparsity of representations (Sriram et al., 2010). Based on external Wikipedia corpus, Phan et al. (2008) proposed a method to discover hidden topics using LDA and 1 Semantic units are defined"
P15-2058,D11-1016,0,0.0145829,"Missing"
P15-2058,C14-1220,0,0.0118763,"sentation of texts. Le and Mikolov (2014) presented the Paragraph Vector algorithm to learn a fixed-size feature representation for documents. Kalchbrenner et al. (2014) introduced the Dynamic Convolutional Neural Network (DCNN) for modeling sentences. Their work is closely related to our study in that k-max pooling is utilized to capture global feature vector and do not rely on parse tree. Kim (2014) proposed a simple improvement to the convolutional architecture that two input channels are used to allow the employment of task-specific and static word embeddings simultaneously. Related Works Zeng et al. (2014) developed a deep convolutional neural network (DNN) to extract lexical and sentence level features, which are concatenated and fed into the softmax classifier. Socher et al. (2013) proposed the Recursive Neural Network (RNN) that has been proven to be efficient in terms of constructing sentences representations. In order to reduce the overfitting of neural network especially trained on small data set, Hinton et al. (2012) used random dropout to prevent Traditional statistics-based methods usually fail to achieve satisfactory performance for short texts classification due to their sparsity of"
P15-2058,N13-1090,0,0.0180782,"Missing"
P15-2058,P14-1062,0,0.0350566,"cisely. Neural networks have been used to model languages, and the word embeddings can be learned simultaneously (Mnih and Teh, 2012). Mikolov et al. (2013b) introduced the continuous Skip-gram model that is an efficient method for learning high quality word embeddings from large-scale unstructured text data. Recently, various pre-trained word embeddings are publicly available, and many composition-based methods are proposed to induce the semantic representation of texts. Le and Mikolov (2014) presented the Paragraph Vector algorithm to learn a fixed-size feature representation for documents. Kalchbrenner et al. (2014) introduced the Dynamic Convolutional Neural Network (DCNN) for modeling sentences. Their work is closely related to our study in that k-max pooling is utilized to capture global feature vector and do not rely on parse tree. Kim (2014) proposed a simple improvement to the convolutional architecture that two input channels are used to allow the employment of task-specific and static word embeddings simultaneously. Related Works Zeng et al. (2014) developed a deep convolutional neural network (DNN) to extract lexical and sentence level features, which are concatenated and fed into the softmax cl"
P15-2058,D14-1181,0,\N,Missing
P15-2058,C02-1150,0,\N,Missing
P15-2058,N15-1011,0,\N,Missing
P15-2131,P10-1040,0,0.034973,"Missing"
P16-2034,H05-1091,0,0.0200522,"Missing"
P16-2034,W09-2415,0,0.354341,"Missing"
P16-2034,D12-1110,0,0.0127696,"Missing"
P16-2034,P10-1040,0,0.0301752,"al Setup Experiments are conducted on SemEval-2010 Task 8 dataset (Hendrickx et al., 2009). This dataset contains 9 relationships (with two directions) and an undirected Other class. There are 10,717 annotated examples, including 8,000 sentences for training, and 2,717 for testing. We adopt the official evaluation metric to evaluate our systems, which is based on macro-averaged F1score for the nine actual relations (excluding the Other relation) and takes the directionality into consideration. In order to compare with the work by Zhang and Wang (2015), we use the same word vectors proposed by Turian et al. (2010) (50-dimensional) to initialize the embedding layer. Additionally, to (14) The cost function is the negative log-likelihood of the true class labels yˆ: m 1 ∑ J (θ) = − ti log(yi ) + λ∥θ∥2F m (15) i=1 where t ∈ ℜm is the one-hot represented ground truth and y ∈ ℜm is the estimated probability for each class by softmax (m is the number of target classes), and λ is an L2 regularization hyperparameter. In this paper, we combine dropout with L2 regularization to alleviate overfitting. 210 sources with bidirectional LSTM networks to learn the sentence level features, and they achieved state-of-the-"
P16-2034,P10-1013,0,0.0186889,"llenge is that important information can appear at any position in the sentence. To tackle these problems, we propose Attention-Based Bidirectional Long Short-Term Memory Networks(AttBLSTM) to capture the most important semantic information in a sentence. The experimental results on the SemEval-2010 relation classification task show that our method outperforms most of the existing methods, with only word vectors. 1 Introduction Relation classification is the task of finding semantic relations between pairs of nominals, which is useful for many NLP applications, such as information extraction (Wu and Weld, 2010), question answering (Yao and Van Durme, 2014). For instance, the following sentence contains an example of the Entity-Destination relation between the nominals Flowers and chapel. ⟨e1 ⟩ Flowers ⟨/e1 ⟩ are carried into the ⟨e2 ⟩ chapel ⟨/e2 ⟩. ⟨e1 ⟩, ⟨/e1 ⟩, ⟨e2 ⟩, ⟨/e2 ⟩ are four position indicators which specify the starting and ending of the nominals (Hendrickx et al., 2009). Traditional relation classification methods that employ handcrafted features from lexical resources, are usually based on pattern matching, and have achieved high performance (Bunescu ∗ Correspondence author: zhenyu.qi"
P16-2034,D15-1206,0,0.485199,"ectional RNN to learn patterns of relations from raw text data. Although bidirectional RNN has access to both past and future context information, the range of context is limited due to the vanishing gradient problem. To overcome this problem, Long short-Term memory(LSTM) units are introduced by Hochreiter and Schmidhuber (1997). ei = W wrd v i (1) where v i is a vector of size |V |which has value 1 at index ei and 0 in all other positions. Then the sentence is feed into the next layer as a real-valued vectors embs = {e1 , e2 , . . . , eT } . Another related work is SDP-LSTM model proposed by Yan et al. (2015). This model leverages the shortest dependency path(SDP) between two nominals, then it picks up heterogeneous information along the SDP with LSTM units. While our method regards the raw text as a sequence. 3.2 Bidirectional Network LSTM units are firstly proposed by Hochreiter and Schmidhuber (1997) to overcome gradient vanishing problem. The main idea is to introduce an adaptive gating mechanism, which decides the degree to which LSTM units keep the previous state and memorize the extracted features of the current data input. Then lots of LSTM variants have been proposed. We adopt a variant i"
P16-2034,P14-1090,0,0.00758004,"Missing"
P16-2034,C14-1220,0,0.799616,"or relation classification. These components will be presented in detail in this section. Related Work Over the years, various methods have been proposed for relation classification. Most of them are based on pattern matching and apply extra NLP systems to derive lexical features. One related work is proposed by Rink and Harabagiu (2010), which utilizes many features derived from external corpora for a Support Vector Machine(SVM) classifier. Recently, deep neural networks can learn underlying features automatically and have been used in the literature. Most representative progress was made by Zeng et al. (2014), who utilized convolutional neural networks(CNN) for relation classification. While CNN is not suitable for learning long-distance semantic information, so our approach builds on Recurrent Neural Network(RNN) (Mikolov et al., 2010). 3.1 Word Embeddings Given a sentence consisting of T words S = {x1 , x2 , . . . , xT }, every word xi is converted into a real-valued vector ei . For each word in S, we first look up the embedding matrix W wrd ∈ w Rd |V |, where V is a fixed-sized vocabulary, and dw is the size of word embedding. The matrix W wrd is a parameter to be learned, and dw is a hyper-par"
P16-2034,P09-1113,0,0.900583,"Missing"
P16-2034,Y15-1009,0,0.270956,"to introduce an adaptive gating mechanism, which decides the degree to which LSTM units keep the previous state and memorize the extracted features of the current data input. Then lots of LSTM variants have been proposed. We adopt a variant introduced by Graves et al. (2013), which adds weighted peephole connections from the Constant Error Carousel (CEC) to the gates of the same memory block. By directly employing the current cell state to generate the gate degrees, the peephole connections allow all gates to inspect into the cell (i.e. Finally, our work is related to BLSTM model proposed by Zhang et al. (2015). This model utilizing NLP tools and lexical resources to get word, position, POS, NER, dependency parse and hypernym features, together with LSTM units, achieved a comparable result to the state-ofthe-art. However, comparing to the complicated features that employed by Zhang et al. (2015), our method regards the four position indicators ⟨e1⟩, ⟨/e1⟩, ⟨e2⟩, ⟨/e2⟩ as single words, and transforms all words to word vectors, forming a simple but competing model. 208  Output Layer + h1 hT ... h3 h2 Attention Layer ¬ ¬ ¬ ¬ h1 h2 h3 hT ® ® ® ® h1 h2 h3 hT LSTM Layer e1 e2 e3 ... eT Embedding Layer x1"
P16-2034,D14-1162,0,0.12596,"Missing"
P16-2034,S10-1057,0,0.426275,"BLSTM to get high level features from step (2); (4) Attention layer: produce a weight vector, and merge word-level features from each time step into a sentence-level feature vector, by multiplying the weight vector; (5) Output layer: the sentence-level feature vector is finally used for relation classification. These components will be presented in detail in this section. Related Work Over the years, various methods have been proposed for relation classification. Most of them are based on pattern matching and apply extra NLP systems to derive lexical features. One related work is proposed by Rink and Harabagiu (2010), which utilizes many features derived from external corpora for a Support Vector Machine(SVM) classifier. Recently, deep neural networks can learn underlying features automatically and have been used in the literature. Most representative progress was made by Zeng et al. (2014), who utilized convolutional neural networks(CNN) for relation classification. While CNN is not suitable for learning long-distance semantic information, so our approach builds on Recurrent Neural Network(RNN) (Mikolov et al., 2010). 3.1 Word Embeddings Given a sentence consisting of T words S = {x1 , x2 , . . . , xT },"
P17-1113,P15-1061,0,0.0223236,"Missing"
P17-1113,D15-1205,0,0.026916,"cting methods and the end-to-end methods based our tagging scheme. For the pipelined methods, we follow (Ren et al., 2017)’s settings: The NER results are obtained by CoType (Ren et al., 2017) then several classical relation classification methods are applied to detect the relations. These methods are: (1) DS-logistic (Mintz et al., 2009) is a distant supervised and feature based method, which combines the advantages of supervised IE and unsupervised IE features; (2) LINE (Tang et al., 2015) is a network embedding method, which is suitable for arbitrary types of information networks; (3) FCM (Gormley et al., 2015) is a compositional model that combines lexicalized linguistic context and word embeddings for relation extraction. The jointly extracting methods used in this paper are listed as follows: (4) DS-Joint (Li and Ji, 2014) is a supervised method, which jointly extracts entities and relations using structured perceptron on human-annotated dataset; (5) MultiR (Hoffmann et al., 2011) is a typical distant supervised method based on multi-instance learning algorithms to combat the noisy training data; (6) CoType (Ren et al., 2017) is a domain independent framework by jointly embedding entity mentions,"
P17-1113,P11-1055,0,0.0945882,"sed method, which combines the advantages of supervised IE and unsupervised IE features; (2) LINE (Tang et al., 2015) is a network embedding method, which is suitable for arbitrary types of information networks; (3) FCM (Gormley et al., 2015) is a compositional model that combines lexicalized linguistic context and word embeddings for relation extraction. The jointly extracting methods used in this paper are listed as follows: (4) DS-Joint (Li and Ji, 2014) is a supervised method, which jointly extracts entities and relations using structured perceptron on human-annotated dataset; (5) MultiR (Hoffmann et al., 2011) is a typical distant supervised method based on multi-instance learning algorithms to combat the noisy training data; (6) CoType (Ren et al., 2017) is a domain independent framework by jointly embedding entity mentions, relation mentions, text features and type labels into meaningful representations. In addition, we also compare our method with two classical end-to-end tagging models: LSTMCRF (Lample et al., 2016) and LSTM-LSTM (Vaswani et al., 2016). LSTM-CRF is proposed for entity recognition by using a bidirectional LSTM to encode input sentence and a conditional random fields to predict t"
P17-1113,D13-1176,0,0.0108274,"feature-based structured systems (Ren et al., 2017; Yang and Cardie, 2013; Singh et al., 2013; Miwa and Sasaki, 2014; Li and Ji, 2014). Recently, (Miwa and Bansal, 2016) uses a LSTM-based model to extract entities and relations, which can reduce the manual work. Different from the above methods, the method proposed in this paper is based on a special tagging manner, so that we can easily use end-toend model to extract results without NER and RC. end-to-end method is to map the input sentence into meaningful vectors and then back to produce a sequence. It is widely used in machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014) and sequence tagging tasks (Lample et al., 2016; Vaswani et al., 2016). Most methods apply bidirectional LSTM to encode the input sentences, but the decoding methods are always different. For examples, (Lample et al., 2016) use a CRF layers to decode the tag sequence, while 1228 Input Sentence: The United States President Trump will visit the Apple Inc founded by Steven Paul Jobs Tags: O B-CP-1 E-CP-1 Final Results: O S-CP-2 O O {United States, Country-President, Trump} O B-CF-1 E-CF-1 O O B-CF-2 I-CF-2 E-CF-2 {Apple Inc, Company-Founder, Steven Paul Jobs} Figure 2: G"
P17-1113,P16-1087,0,0.0540289,"ethods are always different. For examples, (Lample et al., 2016) use a CRF layers to decode the tag sequence, while 1228 Input Sentence: The United States President Trump will visit the Apple Inc founded by Steven Paul Jobs Tags: O B-CP-1 E-CP-1 Final Results: O S-CP-2 O O {United States, Country-President, Trump} O B-CF-1 E-CF-1 O O B-CF-2 I-CF-2 E-CF-2 {Apple Inc, Company-Founder, Steven Paul Jobs} Figure 2: Gold standard annotation for an example sentence based on our tagging scheme, where “CP” is short for “Country-President” and “CF” is short for “Company-Founder”. (Vaswani et al., 2016; Katiyar and Cardie, 2016) apply LSTM layer to produce the tag sequence. 3 Method We propose a novel tagging scheme and an end-toend model with biased objective function to jointly extract entities and their relations. In this section, we firstly introduce how to change the extraction problem to a tagging problem based on our tagging method. Then we detail the model we used to extract results. 3.1 The Tagging Scheme Figure 2 is an example of how the results are tagged. Each word is assigned a label that contributes to extract the results. Tag “O” represents the “Other” tag, which means that the corresponding word is in"
P17-1113,N16-1030,0,0.231647,"motivations, we propose a tagging scheme accompanied with the end-to-end model to settle this problem. We design a kind of novel tags which contain the information of entities and the relationships they hold. Based on this tagging scheme, the joint extraction of entities and relations can be transformed into a tagging problem. In this way, we can also easily use neural networks to model the task without complicated feature engineering. Recently, end-to-end models based on LSTM (Hochreiter and Schmidhuber, 1997) have been successfully applied to various tagging tasks: Named Entity Recognition (Lample et al., 2016), CCG Supertagging (Vaswani et al., 2016), Chunking (Zhai et al., 2017) et al. LSTM is capable of learning long-term dependencies, which is beneficial to sequence modeling tasks. Therefore, based on our tagging scheme, we investigate different kinds of LSTM-based end-to-end models to jointly extract the entities and relations. We also modify the decoding method by adding a biased loss to make it more suitable for our special tags. The method we proposed is a supervised learning algorithm. In reality, however, the process of manually labeling a training set with a large number of entity and rel"
P17-1113,P14-1038,0,0.735175,"and the experimental results show that the tagging based methods are better than most of the existing pipelined and joint learning methods. What’s more, the end-to-end model proposed in this paper, achieves the best results on the public dataset. 1 None None Extracted Results {United States, Country-President, Trump} Figure 1: A standard example sentence for the task. “Country-President” is a relation in the predefined relation set. and each subtask is an independent model. The results of entity recognition may affect the performance of relation classification and lead to erroneous delivery (Li and Ji, 2014). Introduction Joint extraction of entities and relations is to detect entity mentions and recognize their semantic relations simultaneously from unstructured text, as Figure 1 shows. Different from open information extraction (Open IE) (Banko et al., 2007) whose relation words are extracted from the given sentence, in this task, relation words are extracted from a predefined relation set which may not appear in the given sentence. It is an important issue in knowledge extraction and automatic construction of knowledge base. Traditional methods handle this task in a pipelined manner, i.e., ext"
P17-1113,D15-1104,0,0.0282897,"traction is an important step to construct a knowledge base, which can be benefit for many NLP tasks. Two main frameworks have been widely used to solve the problem of extracting entity and their relationships. One is the pipelined method and the other is the joint learning method. The pipelined method treats this task as two separated tasks, i.e., named entity recognition (NER) (Nadeau and Sekine, 2007) and relation classification (RC) (Rink, 2010). Classical NER models are linear statistical models, such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Passos et al., 2014; Luo et al., 2015). Recently, several neural network architectures (Chiu and Nichols, 2015; Huang et al., 2015; Lample et al., 2016) have been successfully applied to NER, which is regarded as a sequential token tagging task. Existing methods for relation classification can also be divided into handcrafted feature based methods (Rink, 2010; Kambhatla, 2004) and neural network based methods (Xu, 2015a; Zheng et al., 2016; Zeng, 2014; Xu, 2015b; dos Santos, 2015). While joint models extract entities and relations using a single model. Most of the joint methods are feature-based structured systems (Ren et al., 201"
P17-1113,P09-1113,0,0.0402388,", we not only report the results of precision, recall and F1, we also compute their standard deviation. Baselines We compare our method with several classical triplet extraction methods, which can be divided into the following categories: the pipelined methods, the jointly extracting methods and the end-to-end methods based our tagging scheme. For the pipelined methods, we follow (Ren et al., 2017)’s settings: The NER results are obtained by CoType (Ren et al., 2017) then several classical relation classification methods are applied to detect the relations. These methods are: (1) DS-logistic (Mintz et al., 2009) is a distant supervised and feature based method, which combines the advantages of supervised IE and unsupervised IE features; (2) LINE (Tang et al., 2015) is a network embedding method, which is suitable for arbitrary types of information networks; (3) FCM (Gormley et al., 2015) is a compositional model that combines lexicalized linguistic context and word embeddings for relation extraction. The jointly extracting methods used in this paper are listed as follows: (4) DS-Joint (Li and Ji, 2014) is a supervised method, which jointly extracts entities and relations using structured perceptron o"
P17-1113,P16-1105,0,0.312594,"e pipelined methods, joint learning framework is to extract entities together with relations using a single model. It can effectively integrate the information of entities and relations, and it has been shown to achieve better results in this task. However, most existing joint methods are feature-based structured systems (Li and Ji, 2014; Miwa and Sasaki, 2014; Yu and Lam, 2010; Ren et al., 2017). They need complicated feature engineering and heavily rely on the other NLP toolkits, which might also lead to error propagation. In order to reduce the manual work in feature extraction, recently, (Miwa and Bansal, 2016) presents a neural networkbased method for the end-to-end entities and relations extraction. Although the joint models can represent both entities and relations with shared parameters in a single model, they also extract the entities and relations separately and produce redundant information. For instance, the sentence in Figure 1 contains three entities: “United States”, “Trump” and “Apple Inc”. But only “United States” and “Trump” hold a fix relation “CountryPresident”. Entity “Apple Inc” has no obvious relationship with the other entities in this sen1227 Proceedings of the 55th Annual Meeti"
P17-1113,D14-1200,0,0.636803,"(Nadeau and Sekine, 2007) first and then recognizing their relations (Rink, 2010). This separated framework makes the task easy to deal with, and each component can be more flexible. But it neglects the relevance between these two sub-tasks Different from the pipelined methods, joint learning framework is to extract entities together with relations using a single model. It can effectively integrate the information of entities and relations, and it has been shown to achieve better results in this task. However, most existing joint methods are feature-based structured systems (Li and Ji, 2014; Miwa and Sasaki, 2014; Yu and Lam, 2010; Ren et al., 2017). They need complicated feature engineering and heavily rely on the other NLP toolkits, which might also lead to error propagation. In order to reduce the manual work in feature extraction, recently, (Miwa and Bansal, 2016) presents a neural networkbased method for the end-to-end entities and relations extraction. Although the joint models can represent both entities and relations with shared parameters in a single model, they also extract the entities and relations separately and produce redundant information. For instance, the sentence in Figure 1 contain"
P17-1113,C10-2160,0,0.602645,"07) first and then recognizing their relations (Rink, 2010). This separated framework makes the task easy to deal with, and each component can be more flexible. But it neglects the relevance between these two sub-tasks Different from the pipelined methods, joint learning framework is to extract entities together with relations using a single model. It can effectively integrate the information of entities and relations, and it has been shown to achieve better results in this task. However, most existing joint methods are feature-based structured systems (Li and Ji, 2014; Miwa and Sasaki, 2014; Yu and Lam, 2010; Ren et al., 2017). They need complicated feature engineering and heavily rely on the other NLP toolkits, which might also lead to error propagation. In order to reduce the manual work in feature extraction, recently, (Miwa and Bansal, 2016) presents a neural networkbased method for the end-to-end entities and relations extraction. Although the joint models can represent both entities and relations with shared parameters in a single model, they also extract the entities and relations separately and produce redundant information. For instance, the sentence in Figure 1 contains three entities:"
P17-1113,C14-1220,0,0.0356991,"Missing"
P17-1113,W14-1609,0,0.0141522,"ties and relations extraction is an important step to construct a knowledge base, which can be benefit for many NLP tasks. Two main frameworks have been widely used to solve the problem of extracting entity and their relationships. One is the pipelined method and the other is the joint learning method. The pipelined method treats this task as two separated tasks, i.e., named entity recognition (NER) (Nadeau and Sekine, 2007) and relation classification (RC) (Rink, 2010). Classical NER models are linear statistical models, such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Passos et al., 2014; Luo et al., 2015). Recently, several neural network architectures (Chiu and Nichols, 2015; Huang et al., 2015; Lample et al., 2016) have been successfully applied to NER, which is regarded as a sequential token tagging task. Existing methods for relation classification can also be divided into handcrafted feature based methods (Rink, 2010; Kambhatla, 2004) and neural network based methods (Xu, 2015a; Zheng et al., 2016; Zeng, 2014; Xu, 2015b; dos Santos, 2015). While joint models extract entities and relations using a single model. Most of the joint methods are feature-based structured syste"
P17-1113,S10-1057,0,0.109843,"Missing"
P17-1113,N16-1027,0,0.159497,"accompanied with the end-to-end model to settle this problem. We design a kind of novel tags which contain the information of entities and the relationships they hold. Based on this tagging scheme, the joint extraction of entities and relations can be transformed into a tagging problem. In this way, we can also easily use neural networks to model the task without complicated feature engineering. Recently, end-to-end models based on LSTM (Hochreiter and Schmidhuber, 1997) have been successfully applied to various tagging tasks: Named Entity Recognition (Lample et al., 2016), CCG Supertagging (Vaswani et al., 2016), Chunking (Zhai et al., 2017) et al. LSTM is capable of learning long-term dependencies, which is beneficial to sequence modeling tasks. Therefore, based on our tagging scheme, we investigate different kinds of LSTM-based end-to-end models to jointly extract the entities and relations. We also modify the decoding method by adding a biased loss to make it more suitable for our special tags. The method we proposed is a supervised learning algorithm. In reality, however, the process of manually labeling a training set with a large number of entity and relation is too expensive and error-prone. T"
P17-1113,D15-1062,0,0.0213997,"Missing"
P17-1113,D15-1206,0,0.0631243,"Missing"
P17-1113,P13-1161,0,0.0193107,"ecently, several neural network architectures (Chiu and Nichols, 2015; Huang et al., 2015; Lample et al., 2016) have been successfully applied to NER, which is regarded as a sequential token tagging task. Existing methods for relation classification can also be divided into handcrafted feature based methods (Rink, 2010; Kambhatla, 2004) and neural network based methods (Xu, 2015a; Zheng et al., 2016; Zeng, 2014; Xu, 2015b; dos Santos, 2015). While joint models extract entities and relations using a single model. Most of the joint methods are feature-based structured systems (Ren et al., 2017; Yang and Cardie, 2013; Singh et al., 2013; Miwa and Sasaki, 2014; Li and Ji, 2014). Recently, (Miwa and Bansal, 2016) uses a LSTM-based model to extract entities and relations, which can reduce the manual work. Different from the above methods, the method proposed in this paper is based on a special tagging manner, so that we can easily use end-toend model to extract results without NER and RC. end-to-end method is to map the input sentence into meaningful vectors and then back to produce a sequence. It is widely used in machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014) and sequence tagg"
P18-1005,N16-1162,0,0.0527587,"s: Denoising auto-encoding Firstly, we train the two AEs to reconstruct their inputs respectively. In this form, each encoder should learn to compose the embeddings of its corresponding language and each decoder is expected to learn to decompose this representation into its corresponding language. Nevertheless, without any constraint, the AE quickly learns to merely copy every word one by one, without capturing any internal structure of the language involved. To address this problem, we utilize the same strategy of denoising AE (Vincent et al., 2008) and add some noise to the input sentences (Hill et al., 2016; Artetxe et al., 2017b). To this end, we shufﬂe the input sentences randomly. Speciﬁcally, we apply a random permutation ε to the input sentence, verifying the condition: steps ] + 1), n), ∀i ∈ {1, n} s (5) where n is the length of the input sentence, steps is the global steps the model has been updated, k and s are the tunable parameters which can be set by users beforehand. This way, the system needs to learn some useful structure of the involved languages to be able to recover the correct word order. In practice, we set k = 2 and s = 100000. Back-translation In spite of denoising autoencod"
P18-1005,D16-1250,0,0.0466503,"supervised and unsupervised learning settings. In the supervised setting, bilingual corpora is available for training the NMT model. In the unsupervised setting, we only have two independent monolingual corpora with one for each language and there is no bilingual training example to provide alignment information for the two languages. Due to lack of alignment information, the unsupervised NMT is considered more challenging. However, this task is very promising, since the monolingual corpora is usually easy to be collected. Motivated by recent success in unsupervised cross-lingual embeddings (Artetxe et al., 2016; Zhang et al., 2017b; Conneau et al., 2017), the models proposed for unsupervised NMT often assume that a pair of sentences from two different languages can be mapped to a same latent representation in a shared-latent space (Lample et al., 2017; Artetxe et al., 2017b). Following this assumption, Lample et al. (2017) use a single encoder and a single decoder for both the source and target languages. The encoder and decoder, acting as a standard auto-encoder (AE), are trained to reconstruct the inputs. And Artetxe et al. (2017b) utilize a shared encoder but two independent decoders. With some g"
P18-1005,P17-1042,0,0.0975159,"aining example to provide alignment information for the two languages. Due to lack of alignment information, the unsupervised NMT is considered more challenging. However, this task is very promising, since the monolingual corpora is usually easy to be collected. Motivated by recent success in unsupervised cross-lingual embeddings (Artetxe et al., 2016; Zhang et al., 2017b; Conneau et al., 2017), the models proposed for unsupervised NMT often assume that a pair of sentences from two different languages can be mapped to a same latent representation in a shared-latent space (Lample et al., 2017; Artetxe et al., 2017b). Following this assumption, Lample et al. (2017) use a single encoder and a single decoder for both the source and target languages. The encoder and decoder, acting as a standard auto-encoder (AE), are trained to reconstruct the inputs. And Artetxe et al. (2017b) utilize a shared encoder but two independent decoders. With some good performance, they share a glaring defect, i.e., only one encoder is shared by the source and target languages. Although the shared encoder is vital for mapping sentences from different languages into the shared-latent space, it is weak in keeping the uniqueness a"
P18-1005,D13-1176,0,0.0711016,"e, such as the style, terminology, and sentence structure. To address this issue, we introduce an extension by utilizing two independent encoders but sharing some partial weights which are responsible for extracting high-level representations of the input sentences. Besides, two different generative adversarial networks (GANs), namely the local GAN and global GAN, are proposed to enhance the cross-language translation. With this new approach, we achieve signiﬁcant improvements on English-German, EnglishFrench and Chinese-to-English translation tasks. 1 Introduction Neural machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014), directly applying a single neural network to transform the source sentence into the target sentence, has now reached impressive performance (Shen et al., 2015; Wu et al., 2016; Johnson et al., 2016; Gehring et al., 2017; Vaswani et al., 2017). The NMT typically consists of two sub neural networks. The encoder network reads and encodes the source sentence into a 1 Feng Wang is the corresponding author of this paper 46 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 46"
P18-1005,C16-1011,0,0.16295,"odel. Experimental results reveal that it deserves more efforts for researchers to investigate the temporal order information within self-attention layers of NMT. 2 Related Work Several approaches have been proposed to train NMT models without direct parallel corpora. The scenario that has been widely investigated is one where two languages have little parallel data between them but are well connected by one pivot language. The most typical approach in this scenario is to independently translate from the source language to the pivot language and from the pivot language to the target language (Saha et al., 2016; Cheng et al., 2017). To improve the translation performance, Johnson et al. (2016) propose a multilingual extension of a standard NMT model and they achieve substantial improvement for language pairs without direct parallel training data. Recently, motivated by the success of crosslingual embeddings, researchers begin to show interests in exploring the more ambitious scenario where an NMT model is trained from monolingual corpora only. Lample et al. (2017) and Artetxe et al. (2017b) simultaneously propose an approach for this scenario, which is based on pre-trained cross lingual embeddings."
P18-1005,J82-2005,0,0.73518,"Missing"
P18-1005,Q16-1027,0,0.0226505,"he motivation behind is twofold. Firstly, taking the ﬁxed cross-lingual embedding as the other encoding component is helpful to reinforce the sharedlatent space. Additionally, from the point of multichannel encoders (Xiong et al., 2017), providing encoding components with different levels of composition enables the decoder to take pieces of source sentence at varying composition levels suiting its own linguistic structure. (1) (2) With the forward mask M f , the later token only makes attention connections to the early tokens in the sequence, and vice versa with the backward mask. Similar to (Zhou et al., 2016; Wang et al., 2017), we utilize a self-attention network to process the input sequence in forward direction. The output of this layer is taken by an upper self-attention network as input, processed in the reverse direction. Weight sharing Based on the shared-latent space assumption, we apply the weight sharing constraint to relate the two AEs. Speciﬁcally, we share the weights of the last few layers of the Encs and Enct , which are responsible for extracting high-level representations of the input sentences. Similarly, we also share the ﬁrst few layers of the Decs and Dect , which are expecte"
P18-1005,P17-1013,0,0.017713,"d is twofold. Firstly, taking the ﬁxed cross-lingual embedding as the other encoding component is helpful to reinforce the sharedlatent space. Additionally, from the point of multichannel encoders (Xiong et al., 2017), providing encoding components with different levels of composition enables the decoder to take pieces of source sentence at varying composition levels suiting its own linguistic structure. (1) (2) With the forward mask M f , the later token only makes attention connections to the early tokens in the sequence, and vice versa with the backward mask. Similar to (Zhou et al., 2016; Wang et al., 2017), we utilize a self-attention network to process the input sequence in forward direction. The output of this layer is taken by an upper self-attention network as input, processed in the reverse direction. Weight sharing Based on the shared-latent space assumption, we apply the weight sharing constraint to relate the two AEs. Speciﬁcally, we share the weights of the last few layers of the Encs and Enct , which are responsible for extracting high-level representations of the input sentences. Similarly, we also share the ﬁrst few layers of the Decs and Dect , which are expected to decode high-lev"
P18-1005,P17-1139,0,0.123955,"rvised learning settings. In the supervised setting, bilingual corpora is available for training the NMT model. In the unsupervised setting, we only have two independent monolingual corpora with one for each language and there is no bilingual training example to provide alignment information for the two languages. Due to lack of alignment information, the unsupervised NMT is considered more challenging. However, this task is very promising, since the monolingual corpora is usually easy to be collected. Motivated by recent success in unsupervised cross-lingual embeddings (Artetxe et al., 2016; Zhang et al., 2017b; Conneau et al., 2017), the models proposed for unsupervised NMT often assume that a pair of sentences from two different languages can be mapped to a same latent representation in a shared-latent space (Lample et al., 2017; Artetxe et al., 2017b). Following this assumption, Lample et al. (2017) use a single encoder and a single decoder for both the source and target languages. The encoder and decoder, acting as a standard auto-encoder (AE), are trained to reconstruct the inputs. And Artetxe et al. (2017b) utilize a shared encoder but two independent decoders. With some good performance, the"
P18-1005,D16-1160,0,0.041863,"put sentence from the source/target language to the target/source language. For the cross language training, we utilize the back-translation approach for our unsupervised training procedure. Back-translation has shown its great effectiveness on improving NMT |ε(i) − i |≤ min(k([ where Hr is the ﬁnal output sequence of the encoder which will be attended by the decoder (In Transformer, H is the ﬁnal output of the encoder), g is a gate unit and computed as: g = σ(W1 E + W2 H + b) Unsupervised Training (4) 49 model with monolingual data and has been widely investigated by (Sennrich et al., 2015a; Zhang and Zong, 2016). In our approach, given an input sentence in a given language, we apply the corresponding encoder and the decoder of the other language to translate it to the other language 3 . By combining the translation with its original sentence, we get a pseudo-parallel corpus which is utilized to train the model to reconstruct the original sentence from its translation. Local GAN Although the weight sharing constraint is vital for the shared-latent space assumption, it alone does not guarantee that the corresponding sentences in two languages will have the same or similar latent code. To further enforc"
P18-1005,P17-1179,0,0.268462,"rvised learning settings. In the supervised setting, bilingual corpora is available for training the NMT model. In the unsupervised setting, we only have two independent monolingual corpora with one for each language and there is no bilingual training example to provide alignment information for the two languages. Due to lack of alignment information, the unsupervised NMT is considered more challenging. However, this task is very promising, since the monolingual corpora is usually easy to be collected. Motivated by recent success in unsupervised cross-lingual embeddings (Artetxe et al., 2016; Zhang et al., 2017b; Conneau et al., 2017), the models proposed for unsupervised NMT often assume that a pair of sentences from two different languages can be mapped to a same latent representation in a shared-latent space (Lample et al., 2017; Artetxe et al., 2017b). Following this assumption, Lample et al. (2017) use a single encoder and a single decoder for both the source and target languages. The encoder and decoder, acting as a standard auto-encoder (AE), are trained to reconstruct the inputs. And Artetxe et al. (2017b) utilize a shared encoder but two independent decoders. With some good performance, the"
P18-1005,P02-1040,0,\N,Missing
P18-2024,P14-2009,0,0.0231907,"Missing"
P18-2024,P13-1067,0,0.0399233,"Missing"
P18-2024,S13-2053,0,0.0565976,"Missing"
P18-2024,S16-2003,0,0.109617,"Missing"
P18-2024,T87-1040,0,0.762814,"Missing"
P18-2024,shutova-teufel-2010-metaphor,0,0.324049,"Missing"
P18-2024,J13-2003,0,0.0555366,"Missing"
P18-2024,S07-1013,0,0.141817,"Missing"
P19-1258,E17-2075,0,0.49692,"en representations of the dialog history, and then feed them into the episodic memory. Extensive experiments on two task-oriented dialog datasets demonstrate that our WMM2Seq significantly outperforms the state-of-the-art results in several evaluation metrics. 1 Introduction Task-oriented dialog systems, such as hotel booking or technical support service, help users to achieve specific goals with natural language. Compared with traditional pipeline solutions (Williams and Young, 2007; Young et al., 2013; Wen et al., 2017), end-to-end approaches recently gain much attention (Zhao et al., 2017; Eric and Manning, 2017a; Lei et al., 2018), because they directly map dialog history to the output responses and consequently reduce human effort for modular designs and hand-crafted state labels. To effectively incorporate KB information and perform knowledge∗ Corresponding Author based reasoning, memory augmented models have been proposed (Bordes et al., 2017; Seo et al., 2017; Eric and Manning, 2017b; Madotto et al., 2018; Raghu et al., 2018; Reddy et al., 2019; Wu et al., 2019). Bordes et al. (2017) and Seo et al. (2017) attended to retrieval models, lacking the ability of generation, while others incorporated"
P19-1258,W17-5506,0,0.530822,"en representations of the dialog history, and then feed them into the episodic memory. Extensive experiments on two task-oriented dialog datasets demonstrate that our WMM2Seq significantly outperforms the state-of-the-art results in several evaluation metrics. 1 Introduction Task-oriented dialog systems, such as hotel booking or technical support service, help users to achieve specific goals with natural language. Compared with traditional pipeline solutions (Williams and Young, 2007; Young et al., 2013; Wen et al., 2017), end-to-end approaches recently gain much attention (Zhao et al., 2017; Eric and Manning, 2017a; Lei et al., 2018), because they directly map dialog history to the output responses and consequently reduce human effort for modular designs and hand-crafted state labels. To effectively incorporate KB information and perform knowledge∗ Corresponding Author based reasoning, memory augmented models have been proposed (Bordes et al., 2017; Seo et al., 2017; Eric and Manning, 2017b; Madotto et al., 2018; Raghu et al., 2018; Reddy et al., 2019; Wu et al., 2019). Bordes et al. (2017) and Seo et al. (2017) attended to retrieval models, lacking the ability of generation, while others incorporated"
P19-1258,P16-1154,0,0.0386495,"an effort for modular designs and hand-crafted state labels. To effectively incorporate KB information and perform knowledge∗ Corresponding Author based reasoning, memory augmented models have been proposed (Bordes et al., 2017; Seo et al., 2017; Eric and Manning, 2017b; Madotto et al., 2018; Raghu et al., 2018; Reddy et al., 2019; Wu et al., 2019). Bordes et al. (2017) and Seo et al. (2017) attended to retrieval models, lacking the ability of generation, while others incorporated the memory (i.e. end-to-end memory networks, abbreviated as MemNNs, Sukhbaatar et al. (2015)) and copy mechanism (Gu et al., 2016) into a sequential generative architecture. However, most models tended to confound the dialog history with KB tuples and simply stored them into one memory. A shared memory forces the memory reader to reason over the two different types of data, which makes the task harder, especially when the memory is large. To explore this problem, Reddy et al. (2019) very recently proposed to separate memories for modeling dialog context and KB results. In this paper, we adopt working memory to interact with two longterm memories. Furthermore, compared to Reddy et al. (2019), we leverage the reasoning abi"
P19-1258,P16-1014,0,0.0652737,"ning rate is simply fixed to 0.001 and the dropout ratio is sampled from [0.1, 0.4]. Furthermore, we randomly mask some memory cells with the same dropout ratio to simulate the OOV situation for both episodic and semantic memories. The hyper-parameters for best models are given in the supplementary material. 3.1 Results and Analysis We use Per-response/dialog Accuracy (Bordes et al., 2017), BLEU (Papineni et al., 2002) and Entity F1 (Madotto et al., 2018) to compare the performance of different models. And the baseline models are Seq2Seq+Attn (Luong et al., 2015), Pointer to Unknown (Ptr-Unk, Gulcehre et al. (2016)), Mem2Seq (Madotto et al., 2018), Hierarchical Pointer Generator Memory Network (HyPMN, Raghu et al. (2018)) and Global-to-Local Memory Pointer (GLMP, Wu et al. (2019)). Automatic Evaluation: The results on the bAbI dialog dataset are given in Table 2. We can see that our model does much better on the OOV situation and is on par with the best results on T5. Moreover, our model can perfectly issue API calls (task 1), update API calls (task 2) and provide extra information (task 4). As task 5 is a combination of tasks 1-4, our best performance on T5-OOV exhibits the powerful reasoning ability t"
P19-1258,W14-4337,0,0.0789501,"U 100 (100) 100 (100) 95.32 (68.2) 100 (100) 99.34 (90.3) 100 (100) 100 (100) 94.64 (61.6) 100 (100) 92.56 (24.3) WMM2Seq 100 (100) 100 (100) 94.94 (63.9) 100 (100) 97.95 (71.2) 91.28 (57.2) 83.28 (0) 94.54 (60.5) 100 (100) 84.45 (3.6) WMM2Seq+biGRU (H1) 100 (100) 100 (100) 95.01 (64.6) 100 (100) 99.26 (88.8) 100 (100) 100 (100) 94.80 (62.2) 100 (100) 91.86 (22.0) Table 2: Per-response and per-dialog (in the parentheses) accuracy on bAbI dialogs. 3 Experiments We conduct experiments on the simulated bAbI Dialogue dataset (Bordes et al., 2017) and the Dialog State Tracking Challenge 2 (DSTC2) (Henderson et al., 2014). We actually adopt the refined version of DSTC2 from Bordes et al. (2017) and their statistics are given in the supplementary material. Our model is trained end-to-end using Adam optimizer (Kingma and Ba, 2014), and the responses are generated using greedy search without any rescoring techniques. The shared size of embedding and hidden units is selected from [64, 512] and the default hop K = 3 is used for all MemNNs. The learning rate is simply fixed to 0.001 and the dropout ratio is sampled from [0.1, 0.4]. Furthermore, we randomly mask some memory cells with the same dropout ratio to simula"
P19-1258,P18-1133,0,0.0245954,"dialog history, and then feed them into the episodic memory. Extensive experiments on two task-oriented dialog datasets demonstrate that our WMM2Seq significantly outperforms the state-of-the-art results in several evaluation metrics. 1 Introduction Task-oriented dialog systems, such as hotel booking or technical support service, help users to achieve specific goals with natural language. Compared with traditional pipeline solutions (Williams and Young, 2007; Young et al., 2013; Wen et al., 2017), end-to-end approaches recently gain much attention (Zhao et al., 2017; Eric and Manning, 2017a; Lei et al., 2018), because they directly map dialog history to the output responses and consequently reduce human effort for modular designs and hand-crafted state labels. To effectively incorporate KB information and perform knowledge∗ Corresponding Author based reasoning, memory augmented models have been proposed (Bordes et al., 2017; Seo et al., 2017; Eric and Manning, 2017b; Madotto et al., 2018; Raghu et al., 2018; Reddy et al., 2019; Wu et al., 2019). Bordes et al. (2017) and Seo et al. (2017) attended to retrieval models, lacking the ability of generation, while others incorporated the memory (i.e. end"
P19-1258,D15-1166,0,0.102072,"default hop K = 3 is used for all MemNNs. The learning rate is simply fixed to 0.001 and the dropout ratio is sampled from [0.1, 0.4]. Furthermore, we randomly mask some memory cells with the same dropout ratio to simulate the OOV situation for both episodic and semantic memories. The hyper-parameters for best models are given in the supplementary material. 3.1 Results and Analysis We use Per-response/dialog Accuracy (Bordes et al., 2017), BLEU (Papineni et al., 2002) and Entity F1 (Madotto et al., 2018) to compare the performance of different models. And the baseline models are Seq2Seq+Attn (Luong et al., 2015), Pointer to Unknown (Ptr-Unk, Gulcehre et al. (2016)), Mem2Seq (Madotto et al., 2018), Hierarchical Pointer Generator Memory Network (HyPMN, Raghu et al. (2018)) and Global-to-Local Memory Pointer (GLMP, Wu et al. (2019)). Automatic Evaluation: The results on the bAbI dialog dataset are given in Table 2. We can see that our model does much better on the OOV situation and is on par with the best results on T5. Moreover, our model can perfectly issue API calls (task 1), update API calls (task 2) and provide extra information (task 4). As task 5 is a combination of tasks 1-4, our best performanc"
P19-1258,P18-1136,0,0.626784,"language. Compared with traditional pipeline solutions (Williams and Young, 2007; Young et al., 2013; Wen et al., 2017), end-to-end approaches recently gain much attention (Zhao et al., 2017; Eric and Manning, 2017a; Lei et al., 2018), because they directly map dialog history to the output responses and consequently reduce human effort for modular designs and hand-crafted state labels. To effectively incorporate KB information and perform knowledge∗ Corresponding Author based reasoning, memory augmented models have been proposed (Bordes et al., 2017; Seo et al., 2017; Eric and Manning, 2017b; Madotto et al., 2018; Raghu et al., 2018; Reddy et al., 2019; Wu et al., 2019). Bordes et al. (2017) and Seo et al. (2017) attended to retrieval models, lacking the ability of generation, while others incorporated the memory (i.e. end-to-end memory networks, abbreviated as MemNNs, Sukhbaatar et al. (2015)) and copy mechanism (Gu et al., 2016) into a sequential generative architecture. However, most models tended to confound the dialog history with KB tuples and simply stored them into one memory. A shared memory forces the memory reader to reason over the two different types of data, which makes the task harder,"
P19-1258,P02-1040,0,0.104575,"are generated using greedy search without any rescoring techniques. The shared size of embedding and hidden units is selected from [64, 512] and the default hop K = 3 is used for all MemNNs. The learning rate is simply fixed to 0.001 and the dropout ratio is sampled from [0.1, 0.4]. Furthermore, we randomly mask some memory cells with the same dropout ratio to simulate the OOV situation for both episodic and semantic memories. The hyper-parameters for best models are given in the supplementary material. 3.1 Results and Analysis We use Per-response/dialog Accuracy (Bordes et al., 2017), BLEU (Papineni et al., 2002) and Entity F1 (Madotto et al., 2018) to compare the performance of different models. And the baseline models are Seq2Seq+Attn (Luong et al., 2015), Pointer to Unknown (Ptr-Unk, Gulcehre et al. (2016)), Mem2Seq (Madotto et al., 2018), Hierarchical Pointer Generator Memory Network (HyPMN, Raghu et al. (2018)) and Global-to-Local Memory Pointer (GLMP, Wu et al. (2019)). Automatic Evaluation: The results on the bAbI dialog dataset are given in Table 2. We can see that our model does much better on the OOV situation and is on par with the best results on T5. Moreover, our model can perfectly issue"
P19-1258,N19-1375,0,0.367182,"ne solutions (Williams and Young, 2007; Young et al., 2013; Wen et al., 2017), end-to-end approaches recently gain much attention (Zhao et al., 2017; Eric and Manning, 2017a; Lei et al., 2018), because they directly map dialog history to the output responses and consequently reduce human effort for modular designs and hand-crafted state labels. To effectively incorporate KB information and perform knowledge∗ Corresponding Author based reasoning, memory augmented models have been proposed (Bordes et al., 2017; Seo et al., 2017; Eric and Manning, 2017b; Madotto et al., 2018; Raghu et al., 2018; Reddy et al., 2019; Wu et al., 2019). Bordes et al. (2017) and Seo et al. (2017) attended to retrieval models, lacking the ability of generation, while others incorporated the memory (i.e. end-to-end memory networks, abbreviated as MemNNs, Sukhbaatar et al. (2015)) and copy mechanism (Gu et al., 2016) into a sequential generative architecture. However, most models tended to confound the dialog history with KB tuples and simply stored them into one memory. A shared memory forces the memory reader to reason over the two different types of data, which makes the task harder, especially when the memory is large. To"
P19-1258,E17-1042,0,0.0651544,"Missing"
P19-1258,N16-1174,0,0.0523255,"2.1 MemNN Encoder Here, on the context of our task, we give a brief description of K-hop MemNN with adjacent weight tying and more details can be found in (Sukhbaatar et al., 2015). The memory of MemNN is represented by a set of trainable embedding matrices C = {C 1 , . . . , C K+1 }. Given input tokens in the dialog history X, MemNN first writes them into memories by Eq. 2 and then uses a query to iteratively read from them with multi hops to reason about the required response by Eq. 3 and Eq. 4. For each hop k, we update the query by Eq. 5 and the initial query is a learnable vector as like Yang et al. (2016). The MemNN encoder finally outputs a user intent vector oK . Aki = C k (xi ) (2) pki = Softmax((q k )T Aki ) (3) ok = X pki Ak+1 i (4) i q k+1 = q k + ok (5) To incorporate the context information, we explore two context-aware transformation TRANS(·) by replacing Eq. 2 with Aki = TRANS(C k (xi )), which is defined as follows: 2 Note, all variables belonging to the episodic memory are with subscript E, and semantic memory are with subscript S. 2688 hi = TRANS(φe (xi )) , = CNN([φe (xi−2 ), . . . , φe (xi+2 )]) (6) what Encoder do yˆ 1 … … oK you think yˆt 1 yˆ t …… Decoder Predicted Response"
P19-1258,W17-5505,0,0.0351368,"process for the token representations of the dialog history, and then feed them into the episodic memory. Extensive experiments on two task-oriented dialog datasets demonstrate that our WMM2Seq significantly outperforms the state-of-the-art results in several evaluation metrics. 1 Introduction Task-oriented dialog systems, such as hotel booking or technical support service, help users to achieve specific goals with natural language. Compared with traditional pipeline solutions (Williams and Young, 2007; Young et al., 2013; Wen et al., 2017), end-to-end approaches recently gain much attention (Zhao et al., 2017; Eric and Manning, 2017a; Lei et al., 2018), because they directly map dialog history to the output responses and consequently reduce human effort for modular designs and hand-crafted state labels. To effectively incorporate KB information and perform knowledge∗ Corresponding Author based reasoning, memory augmented models have been proposed (Bordes et al., 2017; Seo et al., 2017; Eric and Manning, 2017b; Madotto et al., 2018; Raghu et al., 2018; Reddy et al., 2019; Wu et al., 2019). Bordes et al. (2017) and Seo et al. (2017) attended to retrieval models, lacking the ability of generation, wh"
W02-0226,J00-4006,0,\N,Missing
W02-0226,J97-1005,0,\N,Missing
W02-0226,J97-1002,0,\N,Missing
W02-0226,J00-3003,0,\N,Missing
W02-0226,J96-2004,0,\N,Missing
W02-0226,P91-1007,0,\N,Missing
W02-0226,J86-3001,0,\N,Missing
W02-0709,W02-0700,0,0.202904,"Missing"
W02-0709,P98-1070,0,0.037312,"Missing"
W02-0709,W97-0404,0,0.0454114,"Missing"
W02-0709,C00-2174,1,0.893786,"Missing"
W02-0709,C98-1067,0,\N,Missing
W03-1509,C02-1012,0,0.262269,"Missing"
W03-1509,E99-1001,0,\N,Missing
W03-1509,J92-4003,0,\N,Missing
W03-1509,J00-3004,0,\N,Missing
W03-1509,W99-0613,0,\N,Missing
W03-1509,W99-0612,0,\N,Missing
W03-1509,M98-1017,0,\N,Missing
W03-1509,A97-1029,0,\N,Missing
W06-0108,P03-1003,0,0.058534,"Missing"
W15-1509,P14-1062,0,0.0210148,"rate their great performance in terms of constructing text representation, such as Recursive Neural Network (RecNN) (Socher et al., 2011; Socher et al., 2013) and Recurrent Neural Network (RNN) (Mikolov et al., 2011). However, RecNN exhibits high time complexity to construct the textual tree, and RNN, using the layer computed at the last word to represent the text, is a biased model (Lai et al., 2015). More recently, Convolution Neural Network (CNN), applying convolutional filters to capture local features, has achieved a better performance in many NLP applications, such as sentence modeling (Blunsom et al., 2014), relation classification (Zeng et al., 2014), and other traditional NLP tasks (Collobert et al., 2011). Most of the previous works focus CNN on solving supervised NLP tasks, while in this paper we aim to explore the power of CNN on one unsupervised NLP task, short text clustering. To address the above challenges, we systematically introduce a short text clustering method via con62 Proceedings of NAACL-HLT 2015, pages 62–69, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics             !   $"
W15-1509,P12-1092,0,0.0186165,"ith term frequency (TF) and term frequency-inverse document frequency (TFIDF). • Spectral Clustering This baseline (Belkin and Niyogi, 2001) uses Laplacian Eigenmaps (LE) and subsequently employ K-means algorithm. The dimension of subspace is default set to the number of clusters (Ng et al., 2002; Cai et al., 2005), we also iterate the dimensions ranging from 10:10:200 to get the best performance, that is 20 on SearchSnippets and 70 on StackOverflow in our expriments. • Average Embedding K-means on the weighted average of the word embeddings which are respectively weighted with TF and TF-IDF. Huang et al. (2012) also used this strategy as the global context in their task and Lai et al. (2015) used this in text classification. 3.4 Evaluation Metrics The clustering performance is evaluated by comparing the clustering results of texts with the tags/labels provided by the text corpus. Two metrics, the accuracy (ACC) and the normalized mutual information metric (NMI), are used to measure the clustering performance (Cai et al., 2005; Huang et al., 2014). Given a text xi , let ci and yi be the obtained cluster label and the label provided by the corpus, respectively. Accuracy is defined as: ∑n δ(yi , map(ci"
W15-1509,D14-1181,0,0.00711383,"Missing"
W15-1509,D14-1162,0,0.0996863,"arning to learn features. Hinton and Salakhutdinov (2006) use DAE to learn text representation. During the fine-tuning procedure, they use backpropagation to find codes that are good at reconstructing the wordcount vector. Recently, researchers propose to use external corpus to learn a distributed representation for each word, called word embedding (Turian et al., 2010), to improve DNN performance on NLP tasks. The 67 skip-gram and continuous bag-of-words models of (Mikolov et al., 2013) propose a simple singlelayer architecture based on the inner product between two word vectors, and Jeffrey Pennington et al. (2014) introduce a new model for word representation, called GloVe, which captures the global corpus statistics. Based on word embedding, neural networks can capture true meaningful syntactic and semantic regularities, such as RecNN (Socher et al., 2011; Socher et al., 2013) and RNN (Mikolov et al., 2011). However, RecNN exhibits high time complexity to construct the textual tree, and RNN, using the layer computed at the last word to represent the text, is a biased model. Recently, CNN, applying convolving filters to local features, has been successfully exploited for many supervised NLP learning ta"
W15-1509,D11-1014,0,0.162887,"of data from Wikipedia (Banerjee et al., 2007) or an ontology (Fodeh et al., 2011). However, these methods involve solid natural language processing (NLP) With the recent revival of interest in Deep Neural Network (DNN), many researchers have concentrated on using Deep Learning to learn features. Hinton and Salakhutdinov (2006) use deep auto encoder (DAE) to learn text representation from raw text representation. Recently, with the help of word embedding, neural networks demonstrate their great performance in terms of constructing text representation, such as Recursive Neural Network (RecNN) (Socher et al., 2011; Socher et al., 2013) and Recurrent Neural Network (RNN) (Mikolov et al., 2011). However, RecNN exhibits high time complexity to construct the textual tree, and RNN, using the layer computed at the last word to represent the text, is a biased model (Lai et al., 2015). More recently, Convolution Neural Network (CNN), applying convolutional filters to capture local features, has achieved a better performance in many NLP applications, such as sentence modeling (Blunsom et al., 2014), relation classification (Zeng et al., 2014), and other traditional NLP tasks (Collobert et al., 2011). Most of th"
W15-1509,P10-1040,0,0.0161003,"ed solid NLP knowledge while the later works are shallow structures which can not fully capture accurate semantic similarities. 4.2 Deep Neural Networks With the recent revival of interest in DNN, many researchers have concentrated on using Deep Learning to learn features. Hinton and Salakhutdinov (2006) use DAE to learn text representation. During the fine-tuning procedure, they use backpropagation to find codes that are good at reconstructing the wordcount vector. Recently, researchers propose to use external corpus to learn a distributed representation for each word, called word embedding (Turian et al., 2010), to improve DNN performance on NLP tasks. The 67 skip-gram and continuous bag-of-words models of (Mikolov et al., 2013) propose a simple singlelayer architecture based on the inner product between two word vectors, and Jeffrey Pennington et al. (2014) introduce a new model for word representation, called GloVe, which captures the global corpus statistics. Based on word embedding, neural networks can capture true meaningful syntactic and semantic regularities, such as RecNN (Socher et al., 2011; Socher et al., 2013) and RNN (Mikolov et al., 2011). However, RecNN exhibits high time complexity t"
W15-1509,C14-1220,1,0.111164,"cting text representation, such as Recursive Neural Network (RecNN) (Socher et al., 2011; Socher et al., 2013) and Recurrent Neural Network (RNN) (Mikolov et al., 2011). However, RecNN exhibits high time complexity to construct the textual tree, and RNN, using the layer computed at the last word to represent the text, is a biased model (Lai et al., 2015). More recently, Convolution Neural Network (CNN), applying convolutional filters to capture local features, has achieved a better performance in many NLP applications, such as sentence modeling (Blunsom et al., 2014), relation classification (Zeng et al., 2014), and other traditional NLP tasks (Collobert et al., 2011). Most of the previous works focus CNN on solving supervised NLP tasks, while in this paper we aim to explore the power of CNN on one unsupervised NLP task, short text clustering. To address the above challenges, we systematically introduce a short text clustering method via con62 Proceedings of NAACL-HLT 2015, pages 62–69, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics             !   $    ; 6  2"
W15-4648,P95-1016,0,0.0442426,"Missing"
W15-4648,P10-1040,0,0.0242782,"Missing"
W15-4648,P07-1063,0,\N,Missing
W15-4648,J00-3003,0,\N,Missing
W15-4648,P14-1047,0,\N,Missing
W16-4404,N16-1181,0,0.0140438,"swer; and optionally c) extracting and verifying the answer (Prager, 2006; Ferrucci, 2012). In this paper, we pay close attention to the answer sentence selection. Being considered as a key subtask of QA, the selection is to identify the answer-bearing sentences from all candidate sentences. The selected sentences should be relevant to and answer the input questions (Wang and Nyberg, 2015). Several corpora have been created for these tasks like TRECQA , WikiQA (Wang et al., 2007; Yang et al., 2015), allowing researchers to build effective question answering systems (Voorhees and others, 1999; Andreas et al., 2016; Dai et al., 2016; Yih et al., 2014; Yu et al., 2014; Zhang et al., 2016). The nature of this task is to match not only the words but also the meaning between question and answer sentences. For example, the answer to “Where was James born ?”is more likely to be “He came from New York .”than “James was born in summer.”, even though the latter is more similar in the superficial level. Further, the crisis of the task is to find the sentence most closely related to the intention of the question. There have been many works towards the sentence selection task (Heilman and Smith, 2010; Wang and Nybe"
W16-4404,P16-1076,0,0.0146589,") extracting and verifying the answer (Prager, 2006; Ferrucci, 2012). In this paper, we pay close attention to the answer sentence selection. Being considered as a key subtask of QA, the selection is to identify the answer-bearing sentences from all candidate sentences. The selected sentences should be relevant to and answer the input questions (Wang and Nyberg, 2015). Several corpora have been created for these tasks like TRECQA , WikiQA (Wang et al., 2007; Yang et al., 2015), allowing researchers to build effective question answering systems (Voorhees and others, 1999; Andreas et al., 2016; Dai et al., 2016; Yih et al., 2014; Yu et al., 2014; Zhang et al., 2016). The nature of this task is to match not only the words but also the meaning between question and answer sentences. For example, the answer to “Where was James born ?”is more likely to be “He came from New York .”than “James was born in summer.”, even though the latter is more similar in the superficial level. Further, the crisis of the task is to find the sentence most closely related to the intention of the question. There have been many works towards the sentence selection task (Heilman and Smith, 2010; Wang and Nyberg, 2015; Wang and"
W16-4404,N10-1145,0,0.0261795,"and others, 1999; Andreas et al., 2016; Dai et al., 2016; Yih et al., 2014; Yu et al., 2014; Zhang et al., 2016). The nature of this task is to match not only the words but also the meaning between question and answer sentences. For example, the answer to “Where was James born ?”is more likely to be “He came from New York .”than “James was born in summer.”, even though the latter is more similar in the superficial level. Further, the crisis of the task is to find the sentence most closely related to the intention of the question. There have been many works towards the sentence selection task (Heilman and Smith, 2010; Wang and Nyberg, 2015; Wang and Manning, 2010; Severyn and Moschitti, 2013). Basicly, those models could be divided into two categories: the lexical models and semantic-based models. The relatedness between the question-answer sentence pair measured by lexical models is mostly based on some metrics such as Longest common substring (LCS), Bag-of-Words (BOW) and Word Overlap Ratio as well as This work is licensed under a Creative Commons Attribution 4.0 International License.License details:http:// creativecommons.org/licenses/by/4.0/ ∗ Corresponding author. 30 Proceedings of the Open Knowledg"
W16-4404,D13-1044,0,0.013585,"14; Yu et al., 2014; Zhang et al., 2016). The nature of this task is to match not only the words but also the meaning between question and answer sentences. For example, the answer to “Where was James born ?”is more likely to be “He came from New York .”than “James was born in summer.”, even though the latter is more similar in the superficial level. Further, the crisis of the task is to find the sentence most closely related to the intention of the question. There have been many works towards the sentence selection task (Heilman and Smith, 2010; Wang and Nyberg, 2015; Wang and Manning, 2010; Severyn and Moschitti, 2013). Basicly, those models could be divided into two categories: the lexical models and semantic-based models. The relatedness between the question-answer sentence pair measured by lexical models is mostly based on some metrics such as Longest common substring (LCS), Bag-of-Words (BOW) and Word Overlap Ratio as well as This work is licensed under a Creative Commons Attribution 4.0 International License.License details:http:// creativecommons.org/licenses/by/4.0/ ∗ Corresponding author. 30 Proceedings of the Open Knowledge Base and Question Answering (OKBQA) Workshop, pages 30–38, Osaka, Japan, De"
W16-4404,C10-1131,0,0.0192303,"l., 2016; Yih et al., 2014; Yu et al., 2014; Zhang et al., 2016). The nature of this task is to match not only the words but also the meaning between question and answer sentences. For example, the answer to “Where was James born ?”is more likely to be “He came from New York .”than “James was born in summer.”, even though the latter is more similar in the superficial level. Further, the crisis of the task is to find the sentence most closely related to the intention of the question. There have been many works towards the sentence selection task (Heilman and Smith, 2010; Wang and Nyberg, 2015; Wang and Manning, 2010; Severyn and Moschitti, 2013). Basicly, those models could be divided into two categories: the lexical models and semantic-based models. The relatedness between the question-answer sentence pair measured by lexical models is mostly based on some metrics such as Longest common substring (LCS), Bag-of-Words (BOW) and Word Overlap Ratio as well as This work is licensed under a Creative Commons Attribution 4.0 International License.License details:http:// creativecommons.org/licenses/by/4.0/ ∗ Corresponding author. 30 Proceedings of the Open Knowledge Base and Question Answering (OKBQA) Workshop,"
W16-4404,P15-2116,0,0.036345,"Missing"
W16-4404,D07-1003,0,0.0132633,"level major steps: a) question analysis and retrieval of candidate passages; b) ranking and selecting of passages which contain the answer; and optionally c) extracting and verifying the answer (Prager, 2006; Ferrucci, 2012). In this paper, we pay close attention to the answer sentence selection. Being considered as a key subtask of QA, the selection is to identify the answer-bearing sentences from all candidate sentences. The selected sentences should be relevant to and answer the input questions (Wang and Nyberg, 2015). Several corpora have been created for these tasks like TRECQA , WikiQA (Wang et al., 2007; Yang et al., 2015), allowing researchers to build effective question answering systems (Voorhees and others, 1999; Andreas et al., 2016; Dai et al., 2016; Yih et al., 2014; Yu et al., 2014; Zhang et al., 2016). The nature of this task is to match not only the words but also the meaning between question and answer sentences. For example, the answer to “Where was James born ?”is more likely to be “He came from New York .”than “James was born in summer.”, even though the latter is more similar in the superficial level. Further, the crisis of the task is to find the sentence most closely related"
W16-4404,D15-1237,0,0.0141633,"a) question analysis and retrieval of candidate passages; b) ranking and selecting of passages which contain the answer; and optionally c) extracting and verifying the answer (Prager, 2006; Ferrucci, 2012). In this paper, we pay close attention to the answer sentence selection. Being considered as a key subtask of QA, the selection is to identify the answer-bearing sentences from all candidate sentences. The selected sentences should be relevant to and answer the input questions (Wang and Nyberg, 2015). Several corpora have been created for these tasks like TRECQA , WikiQA (Wang et al., 2007; Yang et al., 2015), allowing researchers to build effective question answering systems (Voorhees and others, 1999; Andreas et al., 2016; Dai et al., 2016; Yih et al., 2014; Yu et al., 2014; Zhang et al., 2016). The nature of this task is to match not only the words but also the meaning between question and answer sentences. For example, the answer to “Where was James born ?”is more likely to be “He came from New York .”than “James was born in summer.”, even though the latter is more similar in the superficial level. Further, the crisis of the task is to find the sentence most closely related to the intention of"
W16-4404,P14-2105,0,0.0153159,"erifying the answer (Prager, 2006; Ferrucci, 2012). In this paper, we pay close attention to the answer sentence selection. Being considered as a key subtask of QA, the selection is to identify the answer-bearing sentences from all candidate sentences. The selected sentences should be relevant to and answer the input questions (Wang and Nyberg, 2015). Several corpora have been created for these tasks like TRECQA , WikiQA (Wang et al., 2007; Yang et al., 2015), allowing researchers to build effective question answering systems (Voorhees and others, 1999; Andreas et al., 2016; Dai et al., 2016; Yih et al., 2014; Yu et al., 2014; Zhang et al., 2016). The nature of this task is to match not only the words but also the meaning between question and answer sentences. For example, the answer to “Where was James born ?”is more likely to be “He came from New York .”than “James was born in summer.”, even though the latter is more similar in the superficial level. Further, the crisis of the task is to find the sentence most closely related to the intention of the question. There have been many works towards the sentence selection task (Heilman and Smith, 2010; Wang and Nyberg, 2015; Wang and Manning, 2010; Se"
W19-5511,D14-1148,0,0.0437568,"Missing"
W19-5511,C16-1201,0,0.0278204,"Missing"
W19-5511,D11-1142,0,0.186695,"Missing"
W19-5511,P18-1183,0,0.0304456,"Missing"
W19-5511,N07-4013,0,0.153404,"Missing"
W19-5511,D18-1350,0,0.0640093,"Missing"
W19-5511,P15-1131,0,0.063595,"Missing"
W19-5511,D18-1458,0,0.0501207,"Missing"
