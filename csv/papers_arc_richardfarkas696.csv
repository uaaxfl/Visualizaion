L18-1061,{S}zeged{K}oref: A {H}ungarian Coreference Corpus,2018,0,0,4,0.291142,15691,veronika vincze,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1208,{E}-magyar {--} A Digital Language Processing System,2018,0,0,7,0,17457,tamas varadi,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
E17-1034,{U}niversal {D}ependencies and Morphology for {H}ungarian - and on the Price of Universality,2017,14,0,4,0.296664,15691,veronika vincze,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",0,"In this paper, we present how the principles of universal dependencies and morphology have been adapted to Hungarian. We report the most challenging grammatical phenomena and our solutions to those. On the basis of the adapted guidelines, we have converted and manually corrected 1,800 sentences from the Szeged Treebank to universal dependency format. We also introduce experiments on this manually annotated corpus for evaluating automatic conversion and the added value of language-specific, i.e. non-universal, annotations. Our results reveal that converting to universal dependencies is not necessarily trivial, moreover, using language-specific morphological features may have an impact on overall performance."
W14-6110,Introducing the {IMS}-Wroc{\\l}aw-{S}zeged-{CIS} entry at the {SPMRL} 2014 Shared Task: Reranking and Morpho-syntax meet Unlabeled Data,2014,33,7,4,0.740741,18856,anders bjorkelund,Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages,0,"We summarize our approach taken in the SPMRL 2014 Shared Task on parsing morphologically rich languages. Our approach builds upon our contribution from last year, with a number of modifications and extensions. Though this paper summarizes our contribution, a more detailed description and evaluation will be presented in the accompanying volume containing notes from the SPMRL 2014 Shared Task."
S14-2107,{SZTE}-{NLP}: Aspect level opinion mining exploiting syntactic cues,2014,7,2,4,0.625,5205,viktor hangya,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"In this paper, we introduce our contributions to the SemEval-2014 Task 4 xe2x80x90 Aspect Based Sentiment Analysis (Pontiki et al., 2014) challenge. We participated in the aspect term polarity subtask where the goal was to classify opinions related to a given aspect into positive, negative, neutral or conflict classes. To solve this problem, we employed supervised machine learning techniques exploiting a rich feature set. Our feature templates exploited both phrase structure and dependency parses."
S14-2108,{SZTE}-{NLP}: Clinical Text Analysis with Named Entity Recognition,2014,10,0,2,0,39033,melinda katona,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,This paper introduces our contribution to the SemEval-2014 Task 7 on Analysis of Clinical Text. We implemented a sys- tem which combines MetaMap taggings and Illinois NER Tagger. MetaMap is de- veloped to link the text of medical doc- uments to the knowledge embedded in UMLS Metathesaurus. The UMLS con- tains a very rich lexicon while the promise of a NER system is to carry out context- sensitive tagging. Our system's perfor- mance was 0.345 F-measure in terms of strict evaluation and 0.551 F-measure in terms of relaxed evaluation.
vincze-etal-2014-szeged,{S}zeged Corpus 2.5: Morphological Modifications in a Manually {POS}-tagged {H}ungarian Corpus,2014,4,0,6,0.435275,15691,veronika vincze,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"The Szeged Corpus is the largest manually annotated database containing the possible morphological analyses and lemmas for each word form. In this work, we present its latest version, Szeged Corpus 2.5, in which the new harmonized morphological coding system of Hungarian has been employed and, on the other hand, the majority of misspelled words have been corrected and tagged with the proper morphological code. New morphological codes are introduced for participles, causative / modal / frequentative verbs, adverbial pronouns and punctuation marks, moreover, the distinction between common and proper nouns is eliminated. We also report some statistical data on the frequency of the new morphological codes. The new version of the corpus made it possible to train magyarlanc, a data-driven POS-tagger of Hungarian on a dataset with the new harmonized codes. According to the results, magyarlanc is able to achieve a state-of-the-art accuracy score on the 2.5 version as well."
E14-1015,Special Techniques for Constituent Parsing of Morphologically Rich Languages,2014,27,6,2,0.925926,19327,zsolt szanto,Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,We introduce three techniques for improving constituent parsing for morphologically rich languages. We propose a novel approach to automatically find an optimal preterminal set by clustering morphological feature values and we conduct experiments with enhanced lexical models and feature engineering for rerankers. These techniques are specially designed for morphologically rich languages (but they are language-agnostic). We report empirical results on the treebanks of five morphologically rich languages and show a considerable improvement in accuracy and in parsing speed as well.
D14-1103,Dependency parsing with latent refinements of part-of-speech tags,2014,9,2,2,0,27147,thomas mueller,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,In this paper we propose a method to increase dependency parser performance without using additional labeled or unlabeled data by refining the layer of predicted part-of-speech (POS) tags. We perform experiments on English and German and show significant improvements for both languages. The refinement is based on generative split-merge training for Hidden Markov models (HMMs).
C14-1132,An Empirical Evaluation of Automatic Conversion from Constituency to Dependency in {H}ungarian,2014,12,0,4,0.654762,31421,katalin simko,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"In this paper, we investigate the differences between Hungarian sentence parses based on automatically converted and manually annotated dependency trees. We also train constituency parsers on the manually annotated constituency treebank and then convert their output to dependency trees. We argue for the importance of training on gold standard corpora, and we also demonstrate that although the results obtained by training on the constituency treebank and converting the output to dependency format and those obtained by training on the automatically converted dependency treebank are similar in terms of accuracy scores, the typical errors made by different systems differ from each other."
W13-4916,(Re)ranking Meets Morphosyntax: State-of-the-art Results from the {SPMRL} 2013 Shared Task,2013,38,37,3,0.740741,18856,anders bjorkelund,Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages,0,"This paper describes the IMS-SZEGED-CIS contribution to the SPMRL 2013 Shared Task. We participate in both the constituency and dependency tracks, and achieve state-of-theart for all languages. For both tracks we make significant improvements through high quality preprocessing and (re)ranking on top of strong baselines. Our system came out first for both tracks."
W13-4917,Overview of the {SPMRL} 2013 Shared Task: A Cross-Framework Evaluation of Parsing Morphologically Rich Languages,2013,110,38,6,0,167,djame seddah,Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages,0,"This paper reports on the first shared task on statistical parsing of morphologically rich languages (MRLs). The task features data sets from nine languages, each available both in constituency and dependency annotation. We report on the preparation of the data sets, on the proposed parsing scenarios, and on the evaluation metrics for parsing MRLs given different representation types. We present and analyze parsing results obtained by the task participants, and then provide an analysis and comparison of the parsers across languages and frameworks, reported for gold input as well as more realistic parsing scenarios."
W13-3608,{LFG}-based Features for Noun Number and Article Grammatical Errors,2013,10,6,4,1,1666,gabor berend,Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,0,We introduce here a participating system of the CoNLL-2013 Shared Task xe2x80x9cGrammatical Error Correctionxe2x80x9d. We focused on the noun number and article error categories and constructed a supervised learning system for solving these tasks. We carried out feature engineering and we found that (among others) the f-structure of an LFG parser can provide very informative features for the machine learning system.
W13-2213,{M}unich-{E}dinburgh-{S}tuttgart Submissions of {OSM} Systems at {WMT}13,2013,26,12,5,0,3159,nadir durrani,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,"This paper describes Munich-EdinburghStuttgartxe2x80x99s submissions to the Eighth Workshop on Statistical Machine Translation. We report results of the translation tasks from German, Spanish, Czech and Russian into English and from English to German, Spanish, Czech, French and Russian. The systems described in this paper use OSM (Operation Sequence Model). We explain different pre-/post-processing steps that we carried out for different language pairs. For German-English we used constituent parsing for reordering and compound splitting as preprocessing steps. For Russian-English we transliterated the unknown words. The transliteration system is learned with the help of an unsupervised transliteration mining algorithm."
W13-2230,{M}unich-{E}dinburgh-{S}tuttgart Submissions at {WMT}13: Morphological and Syntactic Processing for {SMT},2013,24,7,8,0,36553,marion weller,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,"We present 5 systems of the MunichEdinburgh-Stuttgart 1 joint submissions to the 2013 SMT Shared Task: FR-EN, ENFR, RU-EN, DE-EN and EN-DE. The first three systems employ inflectional generalization, while the latter two employ parser-based reordering, and DE-EN performs compound splitting. For our experiments, we use standard phrase-based Moses systems and operation sequence models (OSM)."
S13-2092,{SZTE}-{NLP}: Sentiment Detection on {T}witter Messages,2013,10,7,3,0.625,5205,viktor hangya,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation ({S}em{E}val 2013)",0,"In this paper we introduce our contribution to the SemEval-2013 Task 2 on xe2x80x9cSentiment Analysis in Twitterxe2x80x9d. We participated in xe2x80x9ctask Bxe2x80x9d, where the objective was to build models which classify tweets into three classes (positive, negative or neutral) by their contents. To solve this problem we basically followed the supervised learning approach and proposed several domain (i.e. microblog) specific improvements including text preprocessing and feature engineering. Beyond the supervised setting we also introduce some early results employing a huge, automatically annotated tweet dataset."
R13-1099,magyarlanc: A Tool for Morphological and Dependency Parsing of {H}ungarian,2013,16,33,3,0,39508,janos zsibrita,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,0,"Hungarian is the stereotype of morphologically rich and free word order languages. Here, we introduce magyarlanc, a natural language toolkit developed for the linguistic preprocessing xe2x80x90 segmentation, morphological analysis, POS-tagging and dependency parsing xe2x80x90 of Hungarian texts. We hope that the free availability of the toolkit fosters the research not just on the Hungarian language but on all the morphologically rich languages in general. The main novelties of the tool are the application of a new harmonized morphological coding system of Hungarian, the datadriven approach and the integration of a dependency parser. The system is implemented in JAVA, hence it can be used in a platform-independent way."
Q13-1034,Joint Morphological and Syntactic Analysis for Richly Inflected Languages,2013,50,49,4,0,16528,bernd bohnet,Transactions of the Association for Computational Linguistics,0,"Joint morphological and syntactic analysis has been proposed as a way of improving parsing accuracy for richly inflected languages. Starting from a transition-based model for joint part-of-speech tagging and dependency parsing, we explore different ways of integrating morphological features into the model. We also investigate the use of rule-based morphological analyzers to provide hard or soft lexical constraints and the use of word clusters to tackle the sparsity of lexical features. Evaluation on five morphologically rich languages (Czech, Finnish, German, Hungarian, and Russian) shows consistent improvements in both morphological and syntactic accuracy for joint prediction over a pipeline model, with further improvements thanks to lexical constraints and word clusters. The final results improve the state of the art in dependency parsing for all languages."
P13-2046,Identifying {E}nglish and {H}ungarian Light Verb Constructions: A Contrastive Approach,2013,27,6,3,0.59475,15691,veronika vincze,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Here, we introduce a machine learningbased approach that allows us to identify light verb constructions (LVCs) in Hungarian and English free texts. We also present the results of our experiments on the SzegedParalellFX Englishxe2x80x90Hungarian parallel corpus where LVCs were manually annotated in both languages. With our approach, we were able to contrast the performance of our method and define language-specific features for these typologically different languages. Our presented method proved to be sufficiently robust as it achieved approximately the same scores on the two typologically different languages."
J13-1005,"Knowledge Sources for Constituent Parsing of {G}erman, a Morphologically Rich and Less-Configurational Language",2013,58,14,3,0,3265,alexander fraser,Computational Linguistics,0,"We study constituent parsing of German, a morphologically rich and less-configurational language. We use a probabilistic context-free grammar treebank grammar that has been adapted to the morphologically rich properties of German by markovization and special features added to its productions. We evaluate the impact of adding lexical knowledge. Then we examine both monolingual and bilingual approaches to parse reranking. Our reranking parser is the new state of the art in constituency parsing of the TIGER Treebank. We perform an analysis, concluding with lessons learned, which apply to parsing other morphologically rich and less-configurational languages."
I13-2005,Keyphrase-Driven Document Visualization Tool,2013,11,2,2,1,1666,gabor berend,The Companion Volume of the Proceedings of {IJCNLP} 2013: System Demonstrations,0,"The need to navigate through massive document sets is getting common due to the abundant data available around us. To alleviate navigation, tools that are able to grasp the most relevant aspects of document subsets and their relations to other parts of the corpus can be highly beneficial. In this paper, we shall introduce an application1 that processes and visualizes corpora to reveal the main topics and their relative roles to each other. Our suggested solution combines natural language processing and graph theoretic techniques for the visualization of documents based on their automatically detected keyphrases. Furthermore keyphrases that describe thematically related subcorpora are also extracted based on information-theoretic grounds. As for demonstration purposes our application currently deals with papers published at ACL workshops."
I13-1038,Full-coverage Identification of {E}nglish Light Verb Constructions,2013,22,4,3,0.543478,38803,istvan nagy,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"The identification of light verb constructions (LVC) is an important task for several applications. Previous studies focused on some limited set of light verb constructions. Here, we address the full coverage of LVCs. We investigate the performance of different candidate extraction methods on two English full-coverage LVC annotated corpora, where we found that less severe candidate extraction methods should be applied. Then we follow a machine learning approach that makes use of an extended and rich feature set to select LVCs among extracted candidates."
W12-4503,Data-driven Multilingual Coreference Resolution using Resolver Stacking,2012,11,31,2,0.740741,18856,anders bjorkelund,Joint Conference on {EMNLP} and {C}o{NLL} - Shared Task,0,This paper describes our contribution to the CoNLL 2012 Shared Task. We present a novel decoding algorithm for coreference resolution which is combined with a standard pair-wise coreference resolver in a stacking approach. The stacked decoders are evaluated on the three languages of the Shared Task. We obtain an official overall score of 58.25 which is the second highest in the Shared Task.
J12-2004,Cross-Genre and Cross-Domain Detection of Semantic Uncertainty,2012,59,34,3,0.945946,20401,gyorgy szarvas,Computational Linguistics,0,"Uncertainty is an important linguistic phenomenon that is relevant in various Natural Language Processing applications, in diverse genres from medical to community generated, newswire or scientific discourse, and domains from science to humanities. The semantic uncertainty of a proposition can be identified in most cases by using a finite dictionary (i.e., lexical cues) and the key steps of uncertainty detection in an application include the steps of locating the (genre-and domain-specific) lexical cues, disambiguating them, and linking them with the units of interest for the particular application (e.g., identified events in information extraction). In this study, we focus on the genre and domain differences of the context-dependent semantic uncertainty cue recognition task.n n We introduce a unified subcategorization of semantic uncertainty as different domain applications can apply different uncertainty categories. Based on this categorization, we normalized the annotation of three corpora and present results with a state-of-the-art uncertainty cue recognition model for four fine-grained categories of semantic uncertainty.n n Our results reveal the domain and genre dependence of the problem; nevertheless, we also show that even a distant source domain data set can contribute to the recognition and disambiguation of uncertainty cues, efficiently reducing the annotation costs needed to cover a new domain. Thus, the unified subcategorization and domain adaptation for training the models offer an efficient solution for cross-domain and cross-genre semantic uncertainty recognition."
E12-1007,Dependency Parsing of {H}ungarian: Baseline Results and Challenges,2012,24,15,1,1,29565,richard farkas,Proceedings of the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Hungarian is a stereotype of morphologically rich and non-configurational languages. Here, we introduce results on dependency parsing of Hungarian that employ a 80K, multi-domain, fully manually annotated corpus, the Szeged Dependency Treebank. We show that the results achieved by state-of-the-art data-driven parsers on Hungarian and English (which is at the other end of the configurational-non-configurational spectrum) are quite similar to each other in terms of attachment scores. We reveal the reasons for this and present a systematic and comparative linguistically motivated error analysis on both languages. This analysis highlights that addressing the language-specific phenomena is required for a further remarkable error reduction."
D12-1095,Forest Reranking through Subtree Ranking,2012,19,4,1,1,29565,richard farkas,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"We propose the subtree ranking approach to parse forest reranking which is a generalization of current perceptron-based reranking methods. For the training of the reranker, we extract competing local subtrees, hence the training instances (candidate subtree sets) are very similar to those used during beam-search parsing. This leads to better parameter optimization. Another chief advantage of the framework is that arbitrary learning to rank methods can be applied. We evaluated our reranking approach on German and English phrase structure parsing tasks and compared it to various state-of-the-art reranking approaches such as the perceptron-based forest reranker. The subtree ranking approach with a Maximum Entropy model significantly outperformed the other approaches."
C12-2105,Data-driven Dependency Parsing With Empty Heads,2012,24,8,2,0,17921,wolfgang seeker,Proceedings of {COLING} 2012: Posters,0,"Syntactic dependency structures are based on the assumption that there is exactly one node in the structure for each word in the sentence. However representing elliptical constructions (e.g. missing verbs) is problematic as the question where the dependents of the elided material should be attached to has to be solved. In this paper, we present an in-depth study into the challenges of introducing empty heads into dependency structures during automatic parsing. Structurally, empty heads provide an attachment site for the dependents of the non-overt material and thus preserve the linguistically plausible structure of the sentence. We compare three different (computational) approaches to the introduction of empty heads and evaluate them against German and Hungarian data. We then conduct a fine-grained error analysis on the output of one of the approaches to highlight some of the difficulties of the task. We find that while a clearly defined part of the phenomena can be learned by the parser, more involved elliptical structures are still mostly out of reach of the automatic tools."
C12-1052,Stacking of Dependency and Phrase Structure Parsers,2012,36,9,1,1,29565,richard farkas,Proceedings of {COLING} 2012,0,"We investigate the stacking of dependency and phrase structure parsers, i.e. we define features from the output of a phrase structure parser for a dependency parser and vice versa. Our features are based on the original form of the external parses and we also compare this approach to converting phrase structures to dependencies then applying standard stacking on the converted output. The proposed method provides high accuracy gains for both phrase structure and dependency parsing. With the features derived from the phrase structures, we achieved a gain of 0.89 percentage points over a state-of-the-art parser and reach 93.95 UAS, which is the highest reported accuracy score on dependency parsing of the Penn Treebank. The phrase structure parser obtains 91.72 F-score with the features derived from the dependency trees, and this is also competitive with the best reported PARSEVAL scores for the Penn Treebank."
W11-2924,Features for Phrase-Structure Reranking from Dependency Parses,2011,22,6,1,1,29565,richard farkas,Proceedings of the 12th International Conference on Parsing Technologies,0,"Radically different approaches have been proved to be effective for phrase-structure and dependency parsers in the last decade. Here, we aim to exploit the divergence in these approaches and show the utility of features extracted from the automatic dependency parses of sentences for a discriminative phrase-structure parser. Our experiments show a significant improvement over the state-of-the-art German discriminative constituent parser."
D11-1070,Learning Local Content Shift Detectors from Document-level Information,2011,26,1,1,1,29565,richard farkas,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,"Information-oriented document labeling is a special document multi-labeling task where the target labels refer to a specific information instead of the topic of the whole document. These kind of tasks are usually solved by looking up indicator phrases and analyzing their local context to filter false positive matches. Here, we introduce an approach for machine learning local content shifters which detects irrelevant local contexts using just the original document-level training labels. We handle content shifters in general, instead of learning a particular language phenomenon detector (e.g. negation or hedging) and form a single system for document labeling and content shift detection. Our empirical results achieved 24% error reduction -- compared to supervised baseline methods -- on three document labeling tasks."
W10-3001,The {C}o{NLL}-2010 Shared Task: Learning to Detect Hedges and their Scope in Natural Language Text,2010,26,195,1,1,29565,richard farkas,Proceedings of the Fourteenth Conference on Computational Natural Language Learning {--} Shared Task,0,"The CoNLL-2010 Shared Task was dedicated to the detection of uncertainty cues and their linguistic scope in natural language texts. The motivation behind this task was that distinguishing factual and uncertain information in texts is of essential importance in information extraction. This paper provides a general overview of the shared task, including the annotation protocols of the training and evaluation datasets, the exact task definitions, the evaluation metrics employed and the overall results. The paper concludes with an analysis of the prominent approaches and an overview of the systems submitted to the shared task."
S10-1040,{SZTERGAK} : Feature Engineering for Keyphrase Extraction,2010,6,20,2,0.740741,1666,gabor berend,Proceedings of the 5th International Workshop on Semantic Evaluation,0,"Automatically assigning keyphrases to documents has a great variety of applications. Here we focus on the keyphrase extraction of scientific publications and present a novel set of features for the supervised learning of keyphraseness. Although these features are intended for extracting keyphrases from scientific papers, because of their generality and robustness, they should have uses in other domains as well. With the help of these features SZTERGAK achieved top results on the SemEval-2 shared task on Automatic Keyphrase Extraction from Scientific Articles and exceeded its baseline by 10%."
W09-3601,Researcher affiliation extraction from homepages,2009,-1,-1,2,0.543478,38803,istvan nagy,Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries ({NLPIR}4{DL}),0,None
W09-1420,Exploring ways beyond the simple supervised learning approach for biological event extraction,2009,6,11,2,0,43386,gyorgy mora,Proceedings of the {B}io{NLP} 2009 Workshop Companion Volume for Shared Task,0,"Our paper presents the comparison of a machine-learnt and a manually constructed expert-rule-based biological event extraction system and some preliminary experiments to apply a negation and speculation detection system to further classify the extracted events. We report results on the BioNLP'09 Shared Task on Event Extraction evaluation datasets, and also on an external dataset for negation and speculation detection."
W08-0606,"The {B}io{S}cope corpus: annotation for negation, uncertainty and their scope in biomedical texts",2008,12,248,3,1,20401,gyorgy szarvas,Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing,0,"Backgroundn Detecting uncertain and negative assertions is essential in most BioMedical Text Mining tasks where, in general, the aim is to derive factual knowledge from textual data. This article reports on a corpus annotation project that has produced a freely available resource for research on handling negation and uncertainty in biomedical texts (we call this corpus the BioScope corpus)."
vincze-etal-2008-hungarian,{H}ungarian Word-Sense Disambiguated Corpus,2008,5,4,6,0.453954,15691,veronika vincze,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"To create the first Hungarian WSD corpus, 39 suitable word form samples were selected for the purpose of word sense disambiguation. Among others, selection criteria required the given word form to be frequent in Hungarian language usage, and to have more than one sense considered frequent in usage. HNC and its Heti Vil{\'a}ggazdas{\'a}g subcorpus provided the basis for corpus text selection. This way, each sample has a relevant context (whole article), and information on the lemma, POS-tagging and automatic tokenization is also available. When planning the corpus, 300-500 samples of each word form were to be annotated. This size makes it possible that the subcorpora prepared for the individual word forms can be compared to data available for other languages. However, the finalized database also contains unannotated samples and samples with single annotation, which were annotated only by one of the linguists. The corpus follows the ACLÂs SensEval/SemEval WSD tasks format. The first version of the corpus was developed within the scope of the project titled The construction Hungarian WordNet Ontology and its application in Information Extraction Systems (Hatvani et al., 2007). The corpus Â for research and educational purposesÂ is available and can be downloaded free of charge."
S07-1033,{GYDER}: Maxent Metonymy Resolution,2007,8,12,1,1,29565,richard farkas,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"Though the GYDER system has achieved the highest accuracy scores for the metonymy resolution shared task at SemEval-2007 in all six subtasks, we don't consider the results (72.80% accuracy for org, 84.36% for loc) particularly impressive, and argue that metonymy resolution needs more features."
szarvas-etal-2006-highly,A highly accurate Named Entity corpus for {H}ungarian,2006,10,16,2,0,20401,gyorgy szarvas,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"A highly accurate Named Entity (NE) corpus for Hungarian that is publicly available for research purposes is introduced in the paper, along with its main properties. The results of experiments that apply various Machine Learning models and classifier combination schemes are also presented to serve as a benchmark for further research based on the corpus. The data is a segment of the Szeged Corpus (Csendes et al., 2004), consisting of short business news articles collected from MTI (Hungarian News Agency, www.mti.hu). The annotation procedure was carried out paying special attention to annotation accuracy. The corpus went through a parallel annotation phase done by two annotators, resulting in a tagging with inter-annotator agreement rate of 99.89{\%}. Controversial taggings were collected and discussed by the two annotators and a linguist with several years of experience in corpus annotation. These examples were tagged following the decision they made together, and finally all entities that had suspicious or dubious annotations were collected and checked for consistency. We consider the result of this correcting process virtually be free of errors. Our best performing Named Entity Recognizer (NER) model attained an accuracy of 92.86{\%} F measure on the corpus."
