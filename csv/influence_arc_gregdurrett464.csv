2020.acl-main.22,N19-1254,0,0.300351,"s. From these, we construct a guide reordering of the input sentence which then informs the generation of output paraphrases. Introduction Paraphrase generation (McKeown, 1983; Barzilay and Lee, 2003) has seen a recent surge of interest, both with large-scale dataset collection and curation (Lan et al., 2017; Wieting and Gimpel, 2018) and with modeling advances such as deep generative models (Gupta et al., 2018; Li et al., 2019). Paraphrasing models have proven to be especially useful if they expose control mechanisms that can be manipulated to produce diverse paraphrases (Iyyer et al., 2018; Chen et al., 2019b; Park et al., 2019), which allows these models to be employed for data augmentation (Yu et al., 2018) and 1 Data and code are available at https://github. com/tagoyal/sow-reap-paraphrasing adversarial example generation (Iyyer et al., 2018). However, prior methods involving syntactic control mechanisms do not effectively cover the space of paraphrase possibilities. Using syntactic templates covering the top of the parse tree (Iyyer et al., 2018) is inflexible, and using fully-specified exemplar sentences (Chen et al., 2019b) poses the problem of how to effectively retrieve such sentences. Fo"
2020.acl-main.22,P19-1599,0,0.295306,"s. From these, we construct a guide reordering of the input sentence which then informs the generation of output paraphrases. Introduction Paraphrase generation (McKeown, 1983; Barzilay and Lee, 2003) has seen a recent surge of interest, both with large-scale dataset collection and curation (Lan et al., 2017; Wieting and Gimpel, 2018) and with modeling advances such as deep generative models (Gupta et al., 2018; Li et al., 2019). Paraphrasing models have proven to be especially useful if they expose control mechanisms that can be manipulated to produce diverse paraphrases (Iyyer et al., 2018; Chen et al., 2019b; Park et al., 2019), which allows these models to be employed for data augmentation (Yu et al., 2018) and 1 Data and code are available at https://github. com/tagoyal/sow-reap-paraphrasing adversarial example generation (Iyyer et al., 2018). However, prior methods involving syntactic control mechanisms do not effectively cover the space of paraphrase possibilities. Using syntactic templates covering the top of the parse tree (Iyyer et al., 2018) is inflexible, and using fully-specified exemplar sentences (Chen et al., 2019b) poses the problem of how to effectively retrieve such sentences. Fo"
2020.acl-main.22,D19-1308,0,0.0203019,"rsedecoding model that uses the above transformer seq2seq model with diverse decoding (Kumar et al., 2019) during generation. Here, the induced diversity is uncontrolled and aimed at maximizing metrics such as distinct n-grams and edit distance between the generated sentences. iv) A LSTM version of our model where the R EAP model uses LSTMs with attention (Bahdanau et al., 2014) and copy (See et al., 2017) instead of transformers. We still use the transformer-based phrase transducer to obtain the source sentence reorderings, and still use positional encodings in the LSTM attention. Similar to Cho et al. (2019), we report two types of metrics: 1. Quality: Given k generated paraphrases Y = {y1 , y2 . . . yk } for each input sentence in the ˆ best that achieves the best test set, we select y (oracle) sentence-level score with the ground truth paraphrase y. The corpus level evaluation is performed using pairs (ˆ ybest , y). 2. Diversity: We calculate BLEU or WER be6 The score is computed using a weighted mean of the contextual similarity between individual words in the phrases, where the weights are determined by the corpus-level inversedocument frequency of the words. Details in the Appendix. 7 Prior"
2020.acl-main.22,P05-1066,0,0.366999,"Missing"
2020.acl-main.22,D13-1049,0,0.0989724,"by a reordering score based on the position of each word in the source and its aligned word in the target sentence). This leaves us with over 350k paired paraphrase pairs. 3.1 Training Data for R EAP To train our R EAP model (outlined in Section 2.2), we take existing paraphrase pairs (x, y∗ ) and derive pseudo-ground truth rearrangements r∗ of the source sentence tokens based on their alignment with the target sentence. To obtain these rearrangements, we first get contextual embeddings (Devlin et al., 2019) for all tokens in the source and target sentences. We follow the strategy outlined in Lerner and Petrov (2013) and perform reorderings as we traverse down the dependency tree. Starting at the root node of the source sentence, we determine the order between the head and its children (independent of other decisions) based on the order 241 If it continues to rain I will carry an umbrella I will carry an umbrella if rain continues Figure 4: Paraphrase sentence pair and its aligned tuples A → B, C and A0 → B 0 , C 0 . These produce the training data for the S OW M ODEL. of the corresponding aligned words in the target sentence. We continue this traversal recursively to get the sentence level-rearrangement."
2020.acl-main.22,D18-1421,0,0.1142,"between the input sentence x and the ground truth sentence y∗ . The dotted line denotes the ideal performance of the model in terms of agreement with the perfect reordering r∗ . The plot shows that the R EAP model performs as desired; the monotone generation results in high Kendall’s Tau between input and output. Conditioning on the pseudo-ground truth reorderings (r∗ ) produces rearrangements that exhibit the same amount of reordering as the ideal rearrangement. 5 Related Work Paraphrase Generation Compared to prior seq2seq approaches for paraphrasing (Hasan et al., 2016; Gupta et al., 2018; Li et al., 2018), our model is able to achieve much stronger controllability with an interpretable control mechanism. Like these approaches, we can leverage a wide variety of resources to train on, including backtranslation (Pavlick et al., 2015; Wieting and Gimpel, 2018; Hu et al., 2019) or other curated data sources (Fader et al., 2013; Lan et al., 2017). Controlled Generation Recent work on controlled generation aims at controlling attributes such as sentiment (Shen et al., 2017), gender or political slant (Prabhumoye et al., 2018), topic (Wang et al., 2017), etc. However, these methods cannot achieve fine"
2020.acl-main.22,P19-1332,0,0.28691,"model. First, we choose various pairs of constituents to abstract away in the source sentence, then use a neural transducer to generate possible reorderings of the abstracted sentences. From these, we construct a guide reordering of the input sentence which then informs the generation of output paraphrases. Introduction Paraphrase generation (McKeown, 1983; Barzilay and Lee, 2003) has seen a recent surge of interest, both with large-scale dataset collection and curation (Lan et al., 2017; Wieting and Gimpel, 2018) and with modeling advances such as deep generative models (Gupta et al., 2018; Li et al., 2019). Paraphrasing models have proven to be especially useful if they expose control mechanisms that can be manipulated to produce diverse paraphrases (Iyyer et al., 2018; Chen et al., 2019b; Park et al., 2019), which allows these models to be employed for data augmentation (Yu et al., 2018) and 1 Data and code are available at https://github. com/tagoyal/sow-reap-paraphrasing adversarial example generation (Iyyer et al., 2018). However, prior methods involving syntactic control mechanisms do not effectively cover the space of paraphrase possibilities. Using syntactic templates covering the top of"
2020.acl-main.22,D17-1228,0,0.0960305,"Missing"
2020.acl-main.22,P18-1042,0,0.445488,"q YNP won by XNP Source order encoding 4 3 Clippers won 1 2 the game Figure 1: Overview of our paraphrase model. First, we choose various pairs of constituents to abstract away in the source sentence, then use a neural transducer to generate possible reorderings of the abstracted sentences. From these, we construct a guide reordering of the input sentence which then informs the generation of output paraphrases. Introduction Paraphrase generation (McKeown, 1983; Barzilay and Lee, 2003) has seen a recent surge of interest, both with large-scale dataset collection and curation (Lan et al., 2017; Wieting and Gimpel, 2018) and with modeling advances such as deep generative models (Gupta et al., 2018; Li et al., 2019). Paraphrasing models have proven to be especially useful if they expose control mechanisms that can be manipulated to produce diverse paraphrases (Iyyer et al., 2018; Chen et al., 2019b; Park et al., 2019), which allows these models to be employed for data augmentation (Yu et al., 2018) and 1 Data and code are available at https://github. com/tagoyal/sow-reap-paraphrasing adversarial example generation (Iyyer et al., 2018). However, prior methods involving syntactic control mechanisms do not effect"
2020.acl-main.22,D17-1026,0,0.0792326,"Missing"
2020.acl-main.22,D19-1253,0,0.0753238,"Missing"
2020.acl-main.22,N04-1035,0,\N,Missing
2020.acl-main.22,D11-1018,0,\N,Missing
2020.acl-main.22,J83-1001,0,\N,Missing
2020.acl-main.22,N10-1128,0,\N,Missing
2020.acl-main.22,C04-1073,0,\N,Missing
2020.acl-main.22,D09-1105,0,\N,Missing
2020.acl-main.22,N03-1003,0,\N,Missing
2020.acl-main.22,P13-1158,0,\N,Missing
2020.acl-main.22,C10-1043,0,\N,Missing
2020.acl-main.22,J97-3002,0,\N,Missing
2020.acl-main.22,D07-1077,0,\N,Missing
2020.acl-main.22,P15-2070,0,\N,Missing
2020.acl-main.22,C16-1275,0,\N,Missing
2020.acl-main.22,P17-1099,0,\N,Missing
2020.acl-main.22,N10-1017,0,\N,Missing
2020.acl-main.22,P16-1162,0,\N,Missing
2020.acl-main.22,N19-1423,0,\N,Missing
2020.acl-main.22,D19-1313,0,\N,Missing
2020.acl-main.22,N19-1363,0,\N,Missing
2020.acl-main.541,N18-1197,0,0.0221069,"ard graphical schema for depicting the patterns they recognize. This changes the properties of the generated descriptions, leading to higher levels of compositionality and ambiguity because what’s being described is not naturally an image. Program and regex synthesis Recent research has tackled the problem of program synthesis from examples (Gulwani, 2011; Gulwani and Jain, 2017; Alur et al., 2013; Wang et al., 2016; Feng et al., 2018; Devlin et al., 2017; Nye et al., 2019). A closer line of work to ours uses both examples and natural language input (Yaghmazadeh et al., 2017; Ye et al., 2019; Andreas et al., 2018), which involves fundamentally different techniques. However, our work does not rely on the same sort of program synthesizer to build final outputs (Yaghmazadeh et al., 2017; Ye et al., 2019). Moreover, Andreas et al. (2018) only use language at train time, whereas we use NL at both train and test time. Finally, while several datasets on regex synthesis specifically have been released (Kushman and Barzilay, 2013; Locascio et al., 2016), we are the first to incorporate examples in the dataset. Other methods have been proposed to parse natural language into regex via rule-based (Ranta, 1998), gr"
2020.acl-main.541,D13-1160,0,0.162941,"Missing"
2020.acl-main.541,D19-1394,0,0.0179439,"e emergence of NL-to-regex datasets including 1 Code and data available at https://www.cs. utexas.edu/˜xiye/streg/. KB13 (Kushman and Barzilay, 2013) and NLT URK (Locascio et al., 2016). However, KB13 is small in size, with only 814 NL-regex pairs with even fewer distinct regexes. Locascio et al. (2016) subsequently employed a generate-and-paraphrase procedure (Wang et al., 2015) to create the larger NL-T URK dataset. However, the regexes in this dataset are very simple, and the descriptions are short, formulaic, and not linguistically diverse because of the paraphrasing annotation procedure (Herzig and Berant, 2019). As a result, even when models achieve credible performance on these datasets, they completely fail when evaluated on the S TACKOVERFLOW dataset (Ye et al., 2019), a real-world dataset collected from users seeking help on StackOverflow. The limited size of this dataset (only 62 NL-regex pairs) makes it 6081 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6081–6094 c July 5 - 10, 2020. 2020 Association for Computational Linguistics (a) I need to validate the next pattern: starts with “C0” and finish with 4 digits exactly. and(startwith(&lt;C0>)),endw"
2020.acl-main.541,N13-1103,0,0.611184,"ender an abstract figure and generate distinguishing positive/negative examples. We present the figure and examples to crowdworkers to collect natural language descriptions. Introduction Regular expressions (regexes) are known for their usefulness and wide applicability, and yet they are hard to understand and write, even for many programmers (Friedl, 2006). Recent research has therefore studied how to construct regexes from natural language (NL) descriptions, leading to the emergence of NL-to-regex datasets including 1 Code and data available at https://www.cs. utexas.edu/˜xiye/streg/. KB13 (Kushman and Barzilay, 2013) and NLT URK (Locascio et al., 2016). However, KB13 is small in size, with only 814 NL-regex pairs with even fewer distinct regexes. Locascio et al. (2016) subsequently employed a generate-and-paraphrase procedure (Wang et al., 2015) to create the larger NL-T URK dataset. However, the regexes in this dataset are very simple, and the descriptions are short, formulaic, and not linguistically diverse because of the paraphrasing annotation procedure (Herzig and Berant, 2019). As a result, even when models achieve credible performance on these datasets, they completely fail when evaluated on the S"
2020.acl-main.541,D16-1197,0,0.0627803,"tinguishing positive/negative examples. We present the figure and examples to crowdworkers to collect natural language descriptions. Introduction Regular expressions (regexes) are known for their usefulness and wide applicability, and yet they are hard to understand and write, even for many programmers (Friedl, 2006). Recent research has therefore studied how to construct regexes from natural language (NL) descriptions, leading to the emergence of NL-to-regex datasets including 1 Code and data available at https://www.cs. utexas.edu/˜xiye/streg/. KB13 (Kushman and Barzilay, 2013) and NLT URK (Locascio et al., 2016). However, KB13 is small in size, with only 814 NL-regex pairs with even fewer distinct regexes. Locascio et al. (2016) subsequently employed a generate-and-paraphrase procedure (Wang et al., 2015) to create the larger NL-T URK dataset. However, the regexes in this dataset are very simple, and the descriptions are short, formulaic, and not linguistically diverse because of the paraphrasing annotation procedure (Herzig and Berant, 2019). As a result, even when models achieve credible performance on these datasets, they completely fail when evaluated on the S TACKOVERFLOW dataset (Ye et al., 201"
2020.acl-main.541,P16-1138,0,0.0209861,"ing Collecting large-scale data for semantic parsing and related tasks is a long-standing challenge (Berant et al., 2013; Wang et al., 2015). Wang et al. (2015) proposed the generate-and-paraphrase framework, which has been adopted to collect datasets in various domains (Locascio et al., 2016; Ravichander et al., 2017; Johnson et al., 2017). However, this process often biases annotators towards using formulaic language (Ravichander et al., 2017; Herzig and Berant, 2019). Similar to our work, past work has sought to elicit linguistically diverse data using visual elements for semantic parsing (Long et al., 2016), natural language generation (Novikova et al., 2016), and visual reasoning (Suhr et al., 2017, 2019). However, for these other tasks, the images used are depictions of an inherently graphical underlying world state; e.g., the NLVR dataset (Suhr et al., 2017) and NLVR2 (Suhr et al., 2019) are based on reasoning over the presented images, and the Tangrams dataset (Long et al., 2016) involves describing shape transformations. By contrast, regexes are typically represented as source code; there is no standard graphical schema for depicting the patterns they recognize. This changes the properties"
2020.acl-main.541,D15-1166,0,0.0828071,"Missing"
2020.acl-main.541,W16-6644,0,0.0248686,"ing and related tasks is a long-standing challenge (Berant et al., 2013; Wang et al., 2015). Wang et al. (2015) proposed the generate-and-paraphrase framework, which has been adopted to collect datasets in various domains (Locascio et al., 2016; Ravichander et al., 2017; Johnson et al., 2017). However, this process often biases annotators towards using formulaic language (Ravichander et al., 2017; Herzig and Berant, 2019). Similar to our work, past work has sought to elicit linguistically diverse data using visual elements for semantic parsing (Long et al., 2016), natural language generation (Novikova et al., 2016), and visual reasoning (Suhr et al., 2017, 2019). However, for these other tasks, the images used are depictions of an inherently graphical underlying world state; e.g., the NLVR dataset (Suhr et al., 2017) and NLVR2 (Suhr et al., 2019) are based on reasoning over the presented images, and the Tangrams dataset (Long et al., 2016) involves describing shape transformations. By contrast, regexes are typically represented as source code; there is no standard graphical schema for depicting the patterns they recognize. This changes the properties of the generated descriptions, leading to higher leve"
2020.acl-main.541,W98-1308,0,0.176509,"as et al., 2018), which involves fundamentally different techniques. However, our work does not rely on the same sort of program synthesizer to build final outputs (Yaghmazadeh et al., 2017; Ye et al., 2019). Moreover, Andreas et al. (2018) only use language at train time, whereas we use NL at both train and test time. Finally, while several datasets on regex synthesis specifically have been released (Kushman and Barzilay, 2013; Locascio et al., 2016), we are the first to incorporate examples in the dataset. Other methods have been proposed to parse natural language into regex via rule-based (Ranta, 1998), grammar-based (Kushman and Barzilay, 2013), or neural models (Locascio et al., 2016; Zhong et al., 2018; Ye et al., 2019). Notably, Zhong et al. (2018) also generate distinguishing examples to facilitate translation, but they require a trained model to generate examples, and we organically derive examples from the structure of regexes without additional input. 8 Conclusion We introduce S TRUCTURED R EGEX, a new dataset for regex synthesis from natural language and examples. Our dataset contains compositionally structured regexes paired with linguistically diverse language, and organically in"
2020.acl-main.541,W17-5545,0,0.0280463,"Missing"
2020.acl-main.541,P17-2034,0,0.0142147,"enge (Berant et al., 2013; Wang et al., 2015). Wang et al. (2015) proposed the generate-and-paraphrase framework, which has been adopted to collect datasets in various domains (Locascio et al., 2016; Ravichander et al., 2017; Johnson et al., 2017). However, this process often biases annotators towards using formulaic language (Ravichander et al., 2017; Herzig and Berant, 2019). Similar to our work, past work has sought to elicit linguistically diverse data using visual elements for semantic parsing (Long et al., 2016), natural language generation (Novikova et al., 2016), and visual reasoning (Suhr et al., 2017, 2019). However, for these other tasks, the images used are depictions of an inherently graphical underlying world state; e.g., the NLVR dataset (Suhr et al., 2017) and NLVR2 (Suhr et al., 2019) are based on reasoning over the presented images, and the Tangrams dataset (Long et al., 2016) involves describing shape transformations. By contrast, regexes are typically represented as source code; there is no standard graphical schema for depicting the patterns they recognize. This changes the properties of the generated descriptions, leading to higher levels of compositionality and ambiguity beca"
2020.acl-main.541,P19-1644,0,0.020773,"2016; Ravichander et al., 2017; Johnson et al., 2017). However, this process often biases annotators towards using formulaic language (Ravichander et al., 2017; Herzig and Berant, 2019). Similar to our work, past work has sought to elicit linguistically diverse data using visual elements for semantic parsing (Long et al., 2016), natural language generation (Novikova et al., 2016), and visual reasoning (Suhr et al., 2017, 2019). However, for these other tasks, the images used are depictions of an inherently graphical underlying world state; e.g., the NLVR dataset (Suhr et al., 2017) and NLVR2 (Suhr et al., 2019) are based on reasoning over the presented images, and the Tangrams dataset (Long et al., 2016) involves describing shape transformations. By contrast, regexes are typically represented as source code; there is no standard graphical schema for depicting the patterns they recognize. This changes the properties of the generated descriptions, leading to higher levels of compositionality and ambiguity because what’s being described is not naturally an image. Program and regex synthesis Recent research has tackled the problem of program synthesis from examples (Gulwani, 2011; Gulwani and Jain, 2017"
2020.acl-main.541,P15-1129,0,0.0536637,"lness and wide applicability, and yet they are hard to understand and write, even for many programmers (Friedl, 2006). Recent research has therefore studied how to construct regexes from natural language (NL) descriptions, leading to the emergence of NL-to-regex datasets including 1 Code and data available at https://www.cs. utexas.edu/˜xiye/streg/. KB13 (Kushman and Barzilay, 2013) and NLT URK (Locascio et al., 2016). However, KB13 is small in size, with only 814 NL-regex pairs with even fewer distinct regexes. Locascio et al. (2016) subsequently employed a generate-and-paraphrase procedure (Wang et al., 2015) to create the larger NL-T URK dataset. However, the regexes in this dataset are very simple, and the descriptions are short, formulaic, and not linguistically diverse because of the paraphrasing annotation procedure (Herzig and Berant, 2019). As a result, even when models achieve credible performance on these datasets, they completely fail when evaluated on the S TACKOVERFLOW dataset (Ye et al., 2019), a real-world dataset collected from users seeking help on StackOverflow. The limited size of this dataset (only 62 NL-regex pairs) makes it 6081 Proceedings of the 58th Annual Meeting of the As"
2020.emnlp-main.21,D15-1075,0,0.0488728,"2018; Peng et al., 2018; Miller, 2019; Desai et al., 2019). However, we explicitly elect to use tasks where out-of-domain performance is substantially lower and challenging domain shifts are exhibited. Below, we describe our in-domain and out-of-domain datasets.2 For all datasets, we split the development set in half to obtain a held-out, non-blind test set. Natural Language Inference. The Stanford Natural Language Inference (SNLI) corpus is a large-scale entailment dataset where the task is to determine whether a hypothesis is entailed, contradicted by, or neutral with respect to a premise (Bowman et al., 2015). Multi-Genre Natural Language Inference (MNLI) (Williams et al., 2018) contains similar entailment data across several domains, which we can use as unseen test domains. Paraphrase Detection. Quora Question Pairs (QQP) contains sentence pairs from Quora that are semantically equivalent (Iyer et al., 2017). Our outof-domain setting is TwitterPPDB (TPPDB), which contains sentence pairs from Twitter where tweets are considered paraphrases if they have shared URLs (Lan et al., 2017). Experiments Tasks and Datasets We perform evaluations on three language understanding tasks: natural language infer"
2020.emnlp-main.21,P17-1152,0,0.0900042,"ese tasks represent standard evaluation settings for pretrained models, and critically, challenging out-ofdomain test datasets are available for each. Such test data allows us to measure calibration in more realistic settings where samples stem from a dissimilar input distribution, which is exactly the scenario where we hope a well-calibrated model would avoid making confident yet incorrect predictions. Our experiments yield several key results. First, even when used out-of-the-box, pre-trained models are calibrated in-domain. In out-of-domain settings, where non-pre-trained models like ESIM (Chen et al., 2017) are overconfident, we find that pretrained models are significantly better calibrated. Second, we show that temperature scaling (Guo et al., 2017), multiplying non-normalized logits by a single scalar hyperparameter, is widely effective at improving in-domain calibration. Finally, we show that regularizing the model to be less certain during training can beneficially smooth probabilities, improving out-of-domain calibration. 295 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 295–302, c November 16–20, 2020. 2020 Association for Computational Ling"
2020.emnlp-main.21,W19-4828,0,0.0595202,"Missing"
2020.emnlp-main.21,D19-6117,1,0.84613,"382K 4M 110M 110M LSTM Bi-LSTM Transformer Transformer 7 7 3 3 Table 1: Models in this work. Decomposable Attention (DA) (Parikh et al., 2016) and Enhanced Sequential Inference Model (ESIM) (Chen et al., 2017) use LSTMs and attention on top of GloVe embeddings (Pennington et al., 2014) to model pairwise semantic similarities. In contrast, BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) are large-scale, pre-trained language models with stacked, general purpose Transformer (Vaswani et al., 2017) layers. ness using sentiment analysis (Chen et al., 2018; Peng et al., 2018; Miller, 2019; Desai et al., 2019). However, we explicitly elect to use tasks where out-of-domain performance is substantially lower and challenging domain shifts are exhibited. Below, we describe our in-domain and out-of-domain datasets.2 For all datasets, we split the development set in half to obtain a held-out, non-blind test set. Natural Language Inference. The Stanford Natural Language Inference (SNLI) corpus is a large-scale entailment dataset where the task is to determine whether a hypothesis is entailed, contradicted by, or neutral with respect to a premise (Bowman et al., 2015). Multi-Genre Natural Language Inferenc"
2020.emnlp-main.21,D19-1445,0,0.0589306,"Missing"
2020.emnlp-main.21,D17-1126,0,0.0287622,"the task is to determine whether a hypothesis is entailed, contradicted by, or neutral with respect to a premise (Bowman et al., 2015). Multi-Genre Natural Language Inference (MNLI) (Williams et al., 2018) contains similar entailment data across several domains, which we can use as unseen test domains. Paraphrase Detection. Quora Question Pairs (QQP) contains sentence pairs from Quora that are semantically equivalent (Iyer et al., 2017). Our outof-domain setting is TwitterPPDB (TPPDB), which contains sentence pairs from Twitter where tweets are considered paraphrases if they have shared URLs (Lan et al., 2017). Experiments Tasks and Datasets We perform evaluations on three language understanding tasks: natural language inference, paraphrase detection, and commonsense reasoning. Significant past work has studied cross-domain robust296 Commonsense Reasoning. Situations With Adversarial Generations (SWAG) is a grounded commonsense reasoning task where models must select the most plausible continuation of a sentence among four candidates (Zellers et al., 2018). HellaSWAG (HSWAG), an adversarial out-of-domain dataset, serves as a more challenging benchmark for pre-trained models (Zellers et al., 2019);"
2020.emnlp-main.21,2021.ccl-1.108,0,0.184307,"Missing"
2020.emnlp-main.21,N19-1039,0,0.0248759,"e Pre-trained 382K 4M 110M 110M LSTM Bi-LSTM Transformer Transformer 7 7 3 3 Table 1: Models in this work. Decomposable Attention (DA) (Parikh et al., 2016) and Enhanced Sequential Inference Model (ESIM) (Chen et al., 2017) use LSTMs and attention on top of GloVe embeddings (Pennington et al., 2014) to model pairwise semantic similarities. In contrast, BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) are large-scale, pre-trained language models with stacked, general purpose Transformer (Vaswani et al., 2017) layers. ness using sentiment analysis (Chen et al., 2018; Peng et al., 2018; Miller, 2019; Desai et al., 2019). However, we explicitly elect to use tasks where out-of-domain performance is substantially lower and challenging domain shifts are exhibited. Below, we describe our in-domain and out-of-domain datasets.2 For all datasets, we split the development set in half to obtain a held-out, non-blind test set. Natural Language Inference. The Stanford Natural Language Inference (SNLI) corpus is a large-scale entailment dataset where the task is to determine whether a hypothesis is entailed, contradicted by, or neutral with respect to a premise (Bowman et al., 2015). Multi-Genre Natu"
2020.emnlp-main.21,D15-1182,0,0.175917,"Missing"
2020.emnlp-main.21,N18-1101,0,0.018845,"we explicitly elect to use tasks where out-of-domain performance is substantially lower and challenging domain shifts are exhibited. Below, we describe our in-domain and out-of-domain datasets.2 For all datasets, we split the development set in half to obtain a held-out, non-blind test set. Natural Language Inference. The Stanford Natural Language Inference (SNLI) corpus is a large-scale entailment dataset where the task is to determine whether a hypothesis is entailed, contradicted by, or neutral with respect to a premise (Bowman et al., 2015). Multi-Genre Natural Language Inference (MNLI) (Williams et al., 2018) contains similar entailment data across several domains, which we can use as unseen test domains. Paraphrase Detection. Quora Question Pairs (QQP) contains sentence pairs from Quora that are semantically equivalent (Iyer et al., 2017). Our outof-domain setting is TwitterPPDB (TPPDB), which contains sentence pairs from Twitter where tweets are considered paraphrases if they have shared URLs (Lan et al., 2017). Experiments Tasks and Datasets We perform evaluations on three language understanding tasks: natural language inference, paraphrase detection, and commonsense reasoning. Significant past"
2020.emnlp-main.21,D16-1244,0,0.0901472,"Missing"
2020.emnlp-main.21,P18-1233,0,0.031241,"ameters Architecture Pre-trained 382K 4M 110M 110M LSTM Bi-LSTM Transformer Transformer 7 7 3 3 Table 1: Models in this work. Decomposable Attention (DA) (Parikh et al., 2016) and Enhanced Sequential Inference Model (ESIM) (Chen et al., 2017) use LSTMs and attention on top of GloVe embeddings (Pennington et al., 2014) to model pairwise semantic similarities. In contrast, BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) are large-scale, pre-trained language models with stacked, general purpose Transformer (Vaswani et al., 2017) layers. ness using sentiment analysis (Chen et al., 2018; Peng et al., 2018; Miller, 2019; Desai et al., 2019). However, we explicitly elect to use tasks where out-of-domain performance is substantially lower and challenging domain shifts are exhibited. Below, we describe our in-domain and out-of-domain datasets.2 For all datasets, we split the development set in half to obtain a held-out, non-blind test set. Natural Language Inference. The Stanford Natural Language Inference (SNLI) corpus is a large-scale entailment dataset where the task is to determine whether a hypothesis is entailed, contradicted by, or neutral with respect to a premise (Bowman et al., 2015). Mu"
2020.emnlp-main.21,D14-1162,0,0.082316,"Missing"
2020.emnlp-main.21,D18-1009,0,0.0145785,"-domain setting is TwitterPPDB (TPPDB), which contains sentence pairs from Twitter where tweets are considered paraphrases if they have shared URLs (Lan et al., 2017). Experiments Tasks and Datasets We perform evaluations on three language understanding tasks: natural language inference, paraphrase detection, and commonsense reasoning. Significant past work has studied cross-domain robust296 Commonsense Reasoning. Situations With Adversarial Generations (SWAG) is a grounded commonsense reasoning task where models must select the most plausible continuation of a sentence among four candidates (Zellers et al., 2018). HellaSWAG (HSWAG), an adversarial out-of-domain dataset, serves as a more challenging benchmark for pre-trained models (Zellers et al., 2019); it is 2 Dataset splits are detailed in Appendix A. Furthermore, out-of-domain datasets are strictly used for evaluating the generalization of in-domain models, so the training split is unused. Model Accuracy ID ECE OD ID OD 57.12 60.91 73.52 78.79 1.02 1.33 2.54 1.93 8.79 12.78 7.03 3.62 3.37 3.65 2.71 2.33 9.79 8.38 8.51 9.55 5.98 7.01 2.49 1.76 40.37 19.57 12.62 11.93 Task: SNLI/MNLI DA ESIM BERT RoBERTa 84.63 88.32 90.04 91.23 Task: QQP/TwitterPPDB"
2020.emnlp-main.21,P19-1472,0,0.0124968,"URLs (Lan et al., 2017). Experiments Tasks and Datasets We perform evaluations on three language understanding tasks: natural language inference, paraphrase detection, and commonsense reasoning. Significant past work has studied cross-domain robust296 Commonsense Reasoning. Situations With Adversarial Generations (SWAG) is a grounded commonsense reasoning task where models must select the most plausible continuation of a sentence among four candidates (Zellers et al., 2018). HellaSWAG (HSWAG), an adversarial out-of-domain dataset, serves as a more challenging benchmark for pre-trained models (Zellers et al., 2019); it is 2 Dataset splits are detailed in Appendix A. Furthermore, out-of-domain datasets are strictly used for evaluating the generalization of in-domain models, so the training split is unused. Model Accuracy ID ECE OD ID OD 57.12 60.91 73.52 78.79 1.02 1.33 2.54 1.93 8.79 12.78 7.03 3.62 3.37 3.65 2.71 2.33 9.79 8.38 8.51 9.55 5.98 7.01 2.49 1.76 40.37 19.57 12.62 11.93 Task: SNLI/MNLI DA ESIM BERT RoBERTa 84.63 88.32 90.04 91.23 Task: QQP/TwitterPPDB DA ESIM BERT RoBERTa 85.85 87.75 90.27 91.11 83.36 84.00 87.63 86.72 Task: SWAG/HellaSWAG DA ESIM BERT RoBERTa 46.80 52.09 79.40 82.45 32.48 3"
2020.emnlp-main.21,N19-1423,0,\N,Missing
2020.emnlp-main.21,Q18-1039,0,\N,Missing
2020.emnlp-main.507,P13-1020,0,0.021295,"examples), and on XSum → WikiHow, it comes within 0.3 in-domain average ROUGE. These results suggest that our system could be applied widely by crowdsourcing a relatively small number of summaries in a new domain. 8 Related Work Compressive Summarization. Our work follows in a line of systems that use auxiliary training data or objectives to learn sentence compression (Martins and Smith, 2009; Woodsend and Lapata, 2012; Qian and Liu, 2013). Unlike these past approaches, our compression system uses both a plausibility model optimized for grammaticality and a salience model optimized for ROUGE. Almeida and Martins (2013) leverage such modules and learn them jointly in a multi-task learning setup, but face an intractable inference problem in their model which needs sophisticated approximations. Our approach, by contrast, does not need such approximations or expensive inference machinery like ILP solvers (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Durrett et al., 2016). The highly decoupled nature of our pipelined compressive system is an advantage in terms of training simplicity: we use only simple MLE-based objectives for extraction and compression, as opposed to recent compressive methods that u"
2020.emnlp-main.507,P11-1049,0,0.194953,"between the robustness of extractive models and the flexibility of abstractive models. Compression has historically been useful in heuristic-driven systems (Knight and Marcu, 2000, 2002; Wang et al., 2013) or in systems with only certain components being learned (Martins and Smith, 2009; Woodsend and Lapata, 2012; Qian and Liu, 2013). End-to-end learning-based compressive methods are not straightforward to train: 1 Code and datasets available at https://github. com/shreydesai/cups exact derivations of which compressions should be applied are not available, and deriving oracles based on ROUGE (Berg-Kirkpatrick et al., 2011; Durrett et al., 2016; Xu and Durrett, 2019; Mendes et al., 2019) optimizes only for content selection, not grammaticality or factuality of the summary. As a result, past approaches require significant engineering, such as creating a highly specific list of syntactic compression rules to identify permissible deletions (Berg-Kirkpatrick et al., 2011; Li et al., 2014; Wang et al., 2013; Xu and Durrett, 2019). Such manually specified, hand-curated rules are fundamentally inflexible and hard to generalize to new domains. In this work, we build a summarization system that compresses text in a more"
2020.emnlp-main.507,W04-1013,0,0.117559,"ction and compression models in our summarization system are trained separately, but both used in a pipeline during inference. Because the summarization datasets we use do not come with labels for extraction and compression, we chiefly rely on structured oracles that provide supervision for our models. In this section, we describe our oracle design decisions, learning objectives, and inference procedures.4 4.1 Extraction Supervision Following Liu and Lapata (2019), we derive an oracle extractive summary using a greedy algorithm that selects up to k sentences in a document that maximize ROUGE (Lin, 2004) with respect to the reference summary.5 4.2 Compression Supervision Because plausibility and salience are two different views of compression, as introduced in Section 2.3, we have different methods for deriving their supervision. However, their oracles share the same high-level structure, which procedurally operate as follows: an oracle takes in as input an uncompressed sentence x, compressed sentence or paragraph y, and a similarity function f . Using the list of available compression rules R(Tx ) for x, if x without a constituent ck ∈ R(Tx ) results in f (xck , y) &gt; f (x, y), we assign ck"
2020.emnlp-main.507,D19-1387,0,0.175678,"entences s1 , · · · , sn , where each sentence si has words wi1 , · · · , wim . We constrain n to be the maximum number of sentences that collectively have less than 512 wordpieces when tokenized. Each sentence has an associated constituency parse Ti (Kitaev and Klein, 2018) comprised of constituents c = (t, i0 , j 0 ) where t is the constituent’s part-ofspeech tag and (i0 , j 0 ) are the indices of the text span. Let R(Ti ) denote the set of spans proposed for deletion by our compression rules (see Section 2.3). 3.2 Extraction Our extraction model is a re-implementation of the BERTSum model (Liu and Lapata, 2019), which predicts a set of sentences to select as an extractive summary. The model encodes the document sentences s1 , · · · , sn using BERT (Devlin et al., 2019), also preprending [CLS] and adding [SEP] as a delimiter between sentences.2 We denote the token-level representations thus obtained doc as: [hdoc 11 , · · · , hnm ] = Encoder([s1 , · · · , sn ]) During fine-tuning, the [CLS] tokens are treated as sentence-level representations. We collect the [CLS] vectors over all sentences hdoc i1 , d dot each with a weight vector w ∈ R , and use a sigmoid to obtain selection probabilities: P (Si ="
2020.emnlp-main.507,N19-1173,0,0.0122025,"7.62 20.24 20.86 18.81 21.47 36.67 39.63 40.55 38.93 41.11 ext ext cmp cmp CUPSEXT (BERT) CUPSEXT CUPS MatchSum + CUPSCMP 43.16 43.65 44.02 44.69 20.10 20.57 20.57 20.71 39.52 40.02 40.38 40.86 Table 2: Results on CNN/DM. Notably, a pipeline with MatchSum (Zhong et al., 2020) extraction and our compression module achieves state-of-the-art ROUGE1. ♦ Extractive SOTA; ♥ Abstractive SOTA. Systems for Comparison. We refer to our full compressive system as CUPS9 , which includes CUPSEXT and CUPSCMP , the extraction and compression components, respectively. CUPSEXT is a re-implementation of BERTSum (Liu et al., 2019) and CUPSCMP is a module consisting of both the plausibility and salience models. The pre-trained encoders in the extraction and compression modules are set to ELECTRABASE (Clark et al., 2020), unless specified otherwise. Because our approach is fundamentally extractive (albeit with compression), we chiefly compare against state-of-the-art extractive models: BERTSum (Liu et al., 2019), the canonical architecture for sentence-level extraction with pre-trained encoders, and MatchSum (Zhong et al., 2020), a summary-level semantic matching model that uses BERTSum to prune irrelevant sentences. The"
2020.emnlp-main.507,W09-1801,0,0.277365,"or grammatical and factual deletions. Furthermore, the flexibility of our approach allows it to generalize cross-domain: our system fine-tuned on only 500 samples from a new domain can match or exceed an in-domain extractive model trained on much more data.1 1 Introduction Compressive summarization systems offer an appealing tradeoff between the robustness of extractive models and the flexibility of abstractive models. Compression has historically been useful in heuristic-driven systems (Knight and Marcu, 2000, 2002; Wang et al., 2013) or in systems with only certain components being learned (Martins and Smith, 2009; Woodsend and Lapata, 2012; Qian and Liu, 2013). End-to-end learning-based compressive methods are not straightforward to train: 1 Code and datasets available at https://github. com/shreydesai/cups exact derivations of which compressions should be applied are not available, and deriving oracles based on ROUGE (Berg-Kirkpatrick et al., 2011; Durrett et al., 2016; Xu and Durrett, 2019; Mendes et al., 2019) optimizes only for content selection, not grammaticality or factuality of the summary. As a result, past approaches require significant engineering, such as creating a highly specific list of"
2020.emnlp-main.507,N19-1397,0,0.0283504,"Missing"
2020.emnlp-main.507,D18-1206,0,0.489354,"important sentences; (2) for each sentence, high-recall compression rules yield span candidates; (3) two pre-trained Transformer models (Clark et al., 2020) judge the plausibility and salience of spans, respectively, and only spans 6259 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6259–6274, c November 16–20, 2020. 2020 Association for Computational Linguistics which are both plausible and not salient are deleted. We evaluate our approach on several summarization benchmarks. On CNN (Hermann et al., 2015), WikiHow (Koupaee and Wang, 2018), XSum (Narayan et al., 2018), and Reddit (Kim et al., 2019), our compressive system consistently outperforms strong extractive methods by roughly 2 ROUGE-1, and on CNN/Daily Mail (Hermann et al., 2015), we achieve state-of-the-art ROUGE1 by using our compression on top of MatchSum (Zhong et al., 2020) extraction. We also perform additional analysis of each compression component: human evaluation shows plausibility generally yields grammatical and factual deletions, while salience is required to weigh the content relevance of plausible spans according to patterns learned during training. Furthermore, we conduct out-of-dom"
2020.emnlp-main.507,D13-1156,0,0.128353,"he flexibility of our approach allows it to generalize cross-domain: our system fine-tuned on only 500 samples from a new domain can match or exceed an in-domain extractive model trained on much more data.1 1 Introduction Compressive summarization systems offer an appealing tradeoff between the robustness of extractive models and the flexibility of abstractive models. Compression has historically been useful in heuristic-driven systems (Knight and Marcu, 2000, 2002; Wang et al., 2013) or in systems with only certain components being learned (Martins and Smith, 2009; Woodsend and Lapata, 2012; Qian and Liu, 2013). End-to-end learning-based compressive methods are not straightforward to train: 1 Code and datasets available at https://github. com/shreydesai/cups exact derivations of which compressions should be applied are not available, and deriving oracles based on ROUGE (Berg-Kirkpatrick et al., 2011; Durrett et al., 2016; Xu and Durrett, 2019; Mendes et al., 2019) optimizes only for content selection, not grammaticality or factuality of the summary. As a result, past approaches require significant engineering, such as creating a highly specific list of syntactic compression rules to identify permiss"
2020.emnlp-main.507,D18-1412,0,0.0247889,"–[,–NP2 –,]); (6) relative clauses (SBAR); and (7) conjoined noun phrases (e.g., NP1 – [CC–NP2 ]), verb phrases (e.g., VP1 –[CC–VP2 ]), and sentences (e.g., S1 –[CC–S2 ]). Brackets specify the constituent span(s) to be deleted, e.g., CC–NP2 in NP1 –[CC–NP2 ]. Much more refined rules would be needed to ensure grammaticality: for example, in She was [at the tennis courts]PP , deletion of the PP leads to an unacceptable sentence. However, this base set of spans is nevertheless a good set of building blocks, and reliance on syntax gives a useful inductive bias for generalization to other domains (Swayamdipta et al., 2018). 3 Summarization System We now describe our compressive summarization system that leverages our notions of plausibility and salience. For an input document, an off-theshelf extractive model first chooses relevant sentences, then for each extracted sentence, our two compression models decide which sub-sentential spans to delete. Although the plausibility and salience models have different objectives, they both output a posterior over constituent spans, and thus use the same base model architecture. Preprocessing Our system takes as input a document D with sentences s1 , · · · , sn , where each"
2020.emnlp-main.507,D18-1088,0,0.122833,"hes are typed as ext, abs, and cmp, respectively, throughout the experiments. 6 6.1 In-Domain Experiments Benchmark Results Table 1 (CNN, WikiHow, XSum, Reddit) and 2 (CNN/DM) show ROUGE results. From these tables, we make the following observations: Compression consistently improves ROUGE, even when coupled with a strong extractive model. Across the board, we see improvements in ROUGE when using CUPS. Our results particularly contrast with recent trends in compressive summarization where span-based compression (in joint and pipelined forms) decreases ROUGE over sentence extractive baselines (Zhang et al., 2018; Mendes et al., 2019). Gains are especially pronounced on datasets with more abstractive summaries, where applying compression roughly adds +2 ROUGE-1; however, we note there is a large gap between extractive and abstractive approaches on tasks like XSum due to the amount of paraphrasing in reference summaries (Narayan et al., 2018). Nonetheless, our system outperforms strong extractive models on these datasets, and also yields 6264 opening statements in the murder trial of movie theater massacre suspect james holmes are scheduled for april 27, more than a month ahead of schedule, a colorado"
2020.emnlp-main.507,2020.acl-main.552,0,0.189166,"mpirical Methods in Natural Language Processing, pages 6259–6274, c November 16–20, 2020. 2020 Association for Computational Linguistics which are both plausible and not salient are deleted. We evaluate our approach on several summarization benchmarks. On CNN (Hermann et al., 2015), WikiHow (Koupaee and Wang, 2018), XSum (Narayan et al., 2018), and Reddit (Kim et al., 2019), our compressive system consistently outperforms strong extractive methods by roughly 2 ROUGE-1, and on CNN/Daily Mail (Hermann et al., 2015), we achieve state-of-the-art ROUGE1 by using our compression on top of MatchSum (Zhong et al., 2020) extraction. We also perform additional analysis of each compression component: human evaluation shows plausibility generally yields grammatical and factual deletions, while salience is required to weigh the content relevance of plausible spans according to patterns learned during training. Furthermore, we conduct out-of-domain experiments to examine the cross-domain generalizability of our approach. Because plausibility is a more domain-independent notion, we can hold our plausibility model constant and adapt the extraction and salience models to a new setting with a small number of examples."
2020.emnlp-main.508,W19-4828,0,0.026224,"nal values, and more likely to be copied. Within these constituents, the model is very certain about what to generate next, supporting the connection with low syntactic distance. 5 Understanding Decoder Self-Attention While we have analyzed the model’s predictions, we have not yet determined how the different behaviors we see emerge from the context. Our goal is to explore what the encoder attention places its emphasis during generation and how it correlates with the prediction entropy.5 Blocking Low-information Tokens. Analyzing the inner workings of attention in Transformers is challenging (Clark et al., 2019; Kovaleva et al., 2019), particularly because many heads are useless, redundant, or noisy, and they frequently attend to 5 In PEGASUS and BART models, the encoder and decoder attention during decoding are two separate distributions where the encoder attention looks at the encoding context and the decoder attention attends to the previously decoded tokens. In this paper we chiefly examine the encoder attention to understand how the model references the input document. 6278 3.0 2.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Prediction Entropy 3.5 4.0 BART 0.50 0.25 0.00 0.4 Vocab Projected Attention CNN/DM XS"
2020.emnlp-main.508,D19-1223,0,0.0234286,"d (Ribeiro et al., 2016; Ghaeini et al., 2018; Jain and Wallace, 2019; Desai and Durrett, 2020), summarization models specifically have not received similar attention, with analysis efforts often focused on datasets and evaluation (Kryscinski et al., 2019). In this work, we focus on interpreting and understanding abstractive summarization models through the lens of decoder uncertainty, or the entropy of decisions during generation. While uncertainty in generation has been studied from the perspective of data (Ott et al., 2018), sampling (Fan et al., 2018; Holtzman et al., 2019), and training (Correia et al., 2019; Kang and Hashimoto, 2020), it is underutilized as a technique for analysis and inspection of generation systems. We study two prominent summarization models, PEGASUS (Zhang et al., 2020) and BART (Lewis et al., 2020), fine-tuned on two English summarization datasets, CNN/Daily Mail (Hermann et al., 2015) and XSum (Narayan et al., 2018), to understand model behavior in each setting. First, by comparing n-grams between the input document and generated summaries, we establish two coarse types for decoded tokens, copy and generate (See et al., 2017). We find that the entropy of the generation de"
2020.emnlp-main.508,2020.emnlp-main.21,1,0.831277,"matic metrics like ROUGE (Lin, 2004), abstractive models are not as straightforward and interpretable as their extractive counterparts. Free-form generation in these models also leads to serious downstream errors, such as factual inconsistencies with the input document (Cao et al., 2018; Kry´sci´nski et al., 2020; 1 Code is available at https://github.com/ jiacheng-xu/text-sum-uncertainty Wang et al., 2020; Durmus et al., 2020; Goyal and Durrett, 2020). Although the interpretability of NLU models has been extensively studied (Ribeiro et al., 2016; Ghaeini et al., 2018; Jain and Wallace, 2019; Desai and Durrett, 2020), summarization models specifically have not received similar attention, with analysis efforts often focused on datasets and evaluation (Kryscinski et al., 2019). In this work, we focus on interpreting and understanding abstractive summarization models through the lens of decoder uncertainty, or the entropy of decisions during generation. While uncertainty in generation has been studied from the perspective of data (Ott et al., 2018), sampling (Fan et al., 2018; Holtzman et al., 2019), and training (Correia et al., 2019; Kang and Hashimoto, 2020), it is underutilized as a technique for analysi"
2020.emnlp-main.508,2020.acl-main.454,0,0.114923,"rs pre-trained on autoregressive language modeling objectives (Hoang et al., 2019; Khandelwal et al., 2019; Lewis et al., 2020; Zhang et al., 2020). Despite their strong performance on automatic metrics like ROUGE (Lin, 2004), abstractive models are not as straightforward and interpretable as their extractive counterparts. Free-form generation in these models also leads to serious downstream errors, such as factual inconsistencies with the input document (Cao et al., 2018; Kry´sci´nski et al., 2020; 1 Code is available at https://github.com/ jiacheng-xu/text-sum-uncertainty Wang et al., 2020; Durmus et al., 2020; Goyal and Durrett, 2020). Although the interpretability of NLU models has been extensively studied (Ribeiro et al., 2016; Ghaeini et al., 2018; Jain and Wallace, 2019; Desai and Durrett, 2020), summarization models specifically have not received similar attention, with analysis efforts often focused on datasets and evaluation (Kryscinski et al., 2019). In this work, we focus on interpreting and understanding abstractive summarization models through the lens of decoder uncertainty, or the entropy of decisions during generation. While uncertainty in generation has been studied from the perspec"
2020.emnlp-main.508,P18-1082,0,0.0372489,"terpretability of NLU models has been extensively studied (Ribeiro et al., 2016; Ghaeini et al., 2018; Jain and Wallace, 2019; Desai and Durrett, 2020), summarization models specifically have not received similar attention, with analysis efforts often focused on datasets and evaluation (Kryscinski et al., 2019). In this work, we focus on interpreting and understanding abstractive summarization models through the lens of decoder uncertainty, or the entropy of decisions during generation. While uncertainty in generation has been studied from the perspective of data (Ott et al., 2018), sampling (Fan et al., 2018; Holtzman et al., 2019), and training (Correia et al., 2019; Kang and Hashimoto, 2020), it is underutilized as a technique for analysis and inspection of generation systems. We study two prominent summarization models, PEGASUS (Zhang et al., 2020) and BART (Lewis et al., 2020), fine-tuned on two English summarization datasets, CNN/Daily Mail (Hermann et al., 2015) and XSum (Narayan et al., 2018), to understand model behavior in each setting. First, by comparing n-grams between the input document and generated summaries, we establish two coarse types for decoded tokens, copy and generate (See"
2020.emnlp-main.508,N01-1021,0,0.154151,"y change in those tokens’ generation decisions for PEGASUS summaries. The median entropy change is depicted as a dashed black line. At points of high syntactic distance, the model’s behavior is less restricted by the context, correlating with higher entropy. we obtain constituency parses for each summary sentence using the Berkeley Neural Parser (Kitaev and Klein, 2018) and explore connections between syntax and uncertainty in more depth. Low and high entropy decisions can be localized to constituent span boundaries. Parsing has long been used to explain psycholinguistic notions of surprisal (Hale, 2001; Roark et al., 2009, inter alia), which are in turn related to uncertainty under a language model. In our case, uncertainty about generating a text is a different notion than uncertainty when a reader is processing it. Hence, rather than looking at an incremental parser’s behavior, we instead look at a simpler notion of syntactic distance (Shen et al., 2018), or the number of left and right parentheses between wt and wt+1 in a linearized constituency tree. Our hypothesis is that when these words exhibit high syntactic distance, this word boundary is a “choice point” where the model may be les"
2020.emnlp-main.508,N19-1357,0,0.0307047,"rong performance on automatic metrics like ROUGE (Lin, 2004), abstractive models are not as straightforward and interpretable as their extractive counterparts. Free-form generation in these models also leads to serious downstream errors, such as factual inconsistencies with the input document (Cao et al., 2018; Kry´sci´nski et al., 2020; 1 Code is available at https://github.com/ jiacheng-xu/text-sum-uncertainty Wang et al., 2020; Durmus et al., 2020; Goyal and Durrett, 2020). Although the interpretability of NLU models has been extensively studied (Ribeiro et al., 2016; Ghaeini et al., 2018; Jain and Wallace, 2019; Desai and Durrett, 2020), summarization models specifically have not received similar attention, with analysis efforts often focused on datasets and evaluation (Kryscinski et al., 2019). In this work, we focus on interpreting and understanding abstractive summarization models through the lens of decoder uncertainty, or the entropy of decisions during generation. While uncertainty in generation has been studied from the perspective of data (Ott et al., 2018), sampling (Fan et al., 2018; Holtzman et al., 2019), and training (Correia et al., 2019; Kang and Hashimoto, 2020), it is underutilized"
2020.emnlp-main.508,2020.acl-main.66,0,0.0146859,"6; Ghaeini et al., 2018; Jain and Wallace, 2019; Desai and Durrett, 2020), summarization models specifically have not received similar attention, with analysis efforts often focused on datasets and evaluation (Kryscinski et al., 2019). In this work, we focus on interpreting and understanding abstractive summarization models through the lens of decoder uncertainty, or the entropy of decisions during generation. While uncertainty in generation has been studied from the perspective of data (Ott et al., 2018), sampling (Fan et al., 2018; Holtzman et al., 2019), and training (Correia et al., 2019; Kang and Hashimoto, 2020), it is underutilized as a technique for analysis and inspection of generation systems. We study two prominent summarization models, PEGASUS (Zhang et al., 2020) and BART (Lewis et al., 2020), fine-tuned on two English summarization datasets, CNN/Daily Mail (Hermann et al., 2015) and XSum (Narayan et al., 2018), to understand model behavior in each setting. First, by comparing n-grams between the input document and generated summaries, we establish two coarse types for decoded tokens, copy and generate (See et al., 2017). We find that the entropy of the generation decision correlates with whet"
2020.emnlp-main.508,P18-1249,0,0.0256086,"rsenal vs Reading]1.2 [:]0.6 [the game that changed the game]3.1 NP → NP , SBAR , [driver]0.5 [,]0.4 [who has not been identified]2.2 [,]0.1 NP → CD NN NNS NP → NNP CD Figure 3: Correlating syntactic distance between neighboring tokens with the entropy change in those tokens’ generation decisions for PEGASUS summaries. The median entropy change is depicted as a dashed black line. At points of high syntactic distance, the model’s behavior is less restricted by the context, correlating with higher entropy. we obtain constituency parses for each summary sentence using the Berkeley Neural Parser (Kitaev and Klein, 2018) and explore connections between syntax and uncertainty in more depth. Low and high entropy decisions can be localized to constituent span boundaries. Parsing has long been used to explain psycholinguistic notions of surprisal (Hale, 2001; Roark et al., 2009, inter alia), which are in turn related to uncertainty under a language model. In our case, uncertainty about generating a text is a different notion than uncertainty when a reader is processing it. Hence, rather than looking at an incremental parser’s behavior, we instead look at a simpler notion of syntactic distance (Shen et al., 2018),"
2020.emnlp-main.508,D19-1445,0,0.0277961,"likely to be copied. Within these constituents, the model is very certain about what to generate next, supporting the connection with low syntactic distance. 5 Understanding Decoder Self-Attention While we have analyzed the model’s predictions, we have not yet determined how the different behaviors we see emerge from the context. Our goal is to explore what the encoder attention places its emphasis during generation and how it correlates with the prediction entropy.5 Blocking Low-information Tokens. Analyzing the inner workings of attention in Transformers is challenging (Clark et al., 2019; Kovaleva et al., 2019), particularly because many heads are useless, redundant, or noisy, and they frequently attend to 5 In PEGASUS and BART models, the encoder and decoder attention during decoding are two separate distributions where the encoder attention looks at the encoding context and the decoder attention attends to the previously decoded tokens. In this paper we chiefly examine the encoder attention to understand how the model references the input document. 6278 3.0 2.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Prediction Entropy 3.5 4.0 BART 0.50 0.25 0.00 0.4 Vocab Projected Attention CNN/DM XSum Attention Entropy 3.5"
2020.emnlp-main.508,D18-1537,0,0.0205064,"020). Despite their strong performance on automatic metrics like ROUGE (Lin, 2004), abstractive models are not as straightforward and interpretable as their extractive counterparts. Free-form generation in these models also leads to serious downstream errors, such as factual inconsistencies with the input document (Cao et al., 2018; Kry´sci´nski et al., 2020; 1 Code is available at https://github.com/ jiacheng-xu/text-sum-uncertainty Wang et al., 2020; Durmus et al., 2020; Goyal and Durrett, 2020). Although the interpretability of NLU models has been extensively studied (Ribeiro et al., 2016; Ghaeini et al., 2018; Jain and Wallace, 2019; Desai and Durrett, 2020), summarization models specifically have not received similar attention, with analysis efforts often focused on datasets and evaluation (Kryscinski et al., 2019). In this work, we focus on interpreting and understanding abstractive summarization models through the lens of decoder uncertainty, or the entropy of decisions during generation. While uncertainty in generation has been studied from the perspective of data (Ott et al., 2018), sampling (Fan et al., 2018; Holtzman et al., 2019), and training (Correia et al., 2019; Kang and Hashimoto, 202"
2020.emnlp-main.508,D19-1051,0,0.0206382,"hese models also leads to serious downstream errors, such as factual inconsistencies with the input document (Cao et al., 2018; Kry´sci´nski et al., 2020; 1 Code is available at https://github.com/ jiacheng-xu/text-sum-uncertainty Wang et al., 2020; Durmus et al., 2020; Goyal and Durrett, 2020). Although the interpretability of NLU models has been extensively studied (Ribeiro et al., 2016; Ghaeini et al., 2018; Jain and Wallace, 2019; Desai and Durrett, 2020), summarization models specifically have not received similar attention, with analysis efforts often focused on datasets and evaluation (Kryscinski et al., 2019). In this work, we focus on interpreting and understanding abstractive summarization models through the lens of decoder uncertainty, or the entropy of decisions during generation. While uncertainty in generation has been studied from the perspective of data (Ott et al., 2018), sampling (Fan et al., 2018; Holtzman et al., 2019), and training (Correia et al., 2019; Kang and Hashimoto, 2020), it is underutilized as a technique for analysis and inspection of generation systems. We study two prominent summarization models, PEGASUS (Zhang et al., 2020) and BART (Lewis et al., 2020), fine-tuned on tw"
2020.emnlp-main.508,2020.findings-emnlp.322,1,0.815325,"oregressive language modeling objectives (Hoang et al., 2019; Khandelwal et al., 2019; Lewis et al., 2020; Zhang et al., 2020). Despite their strong performance on automatic metrics like ROUGE (Lin, 2004), abstractive models are not as straightforward and interpretable as their extractive counterparts. Free-form generation in these models also leads to serious downstream errors, such as factual inconsistencies with the input document (Cao et al., 2018; Kry´sci´nski et al., 2020; 1 Code is available at https://github.com/ jiacheng-xu/text-sum-uncertainty Wang et al., 2020; Durmus et al., 2020; Goyal and Durrett, 2020). Although the interpretability of NLU models has been extensively studied (Ribeiro et al., 2016; Ghaeini et al., 2018; Jain and Wallace, 2019; Desai and Durrett, 2020), summarization models specifically have not received similar attention, with analysis efforts often focused on datasets and evaluation (Kryscinski et al., 2019). In this work, we focus on interpreting and understanding abstractive summarization models through the lens of decoder uncertainty, or the entropy of decisions during generation. While uncertainty in generation has been studied from the perspective of data (Ott et al.,"
2020.emnlp-main.508,2020.emnlp-main.750,0,0.344092,"Missing"
2020.emnlp-main.508,2020.acl-main.703,0,0.48295,"dels via Uncertainty Jiacheng Xu Shrey Desai Greg Durrett Department of Computer Science The University of Texas at Austin {jcxu,gdurrett}@cs.utexas.edu shreydesai@utexas.edu Abstract An advantage of seq2seq abstractive summarization models is that they generate text in a free-form manner, but this flexibility makes it difficult to interpret model behavior. In this work, we analyze summarization decoders in both blackbox and whitebox ways by studying on the entropy, or uncertainty, of the model’s token-level predictions. For two strong pretrained models, PEGASUS (Zhang et al., 2020) and BART (Lewis et al., 2020) on two summarization datasets, we find a strong correlation between low prediction entropy and where the model copies tokens rather than generating novel text. The decoder’s uncertainty also connects to factors like sentence position and syntactic distance between adjacent pairs of tokens, giving a sense of what factors make a context particularly selective for the model’s next output token. Finally, we study the relationship of decoder uncertainty and attention behavior to understand how attention gives rise to these observed effects in the model. We show that uncertainty is a useful perspec"
2020.emnlp-main.508,W04-1013,0,0.0587263,"we study the relationship of decoder uncertainty and attention behavior to understand how attention gives rise to these observed effects in the model. We show that uncertainty is a useful perspective for analyzing summarization and text generation models more broadly.1 1 Introduction Recent progress in abstractive summarization has been fueled by the advent of large-scale Transformers pre-trained on autoregressive language modeling objectives (Hoang et al., 2019; Khandelwal et al., 2019; Lewis et al., 2020; Zhang et al., 2020). Despite their strong performance on automatic metrics like ROUGE (Lin, 2004), abstractive models are not as straightforward and interpretable as their extractive counterparts. Free-form generation in these models also leads to serious downstream errors, such as factual inconsistencies with the input document (Cao et al., 2018; Kry´sci´nski et al., 2020; 1 Code is available at https://github.com/ jiacheng-xu/text-sum-uncertainty Wang et al., 2020; Durmus et al., 2020; Goyal and Durrett, 2020). Although the interpretability of NLU models has been extensively studied (Ribeiro et al., 2016; Ghaeini et al., 2018; Jain and Wallace, 2019; Desai and Durrett, 2020), summarizat"
2020.emnlp-main.508,D18-1206,0,0.232802,"on models through the lens of decoder uncertainty, or the entropy of decisions during generation. While uncertainty in generation has been studied from the perspective of data (Ott et al., 2018), sampling (Fan et al., 2018; Holtzman et al., 2019), and training (Correia et al., 2019; Kang and Hashimoto, 2020), it is underutilized as a technique for analysis and inspection of generation systems. We study two prominent summarization models, PEGASUS (Zhang et al., 2020) and BART (Lewis et al., 2020), fine-tuned on two English summarization datasets, CNN/Daily Mail (Hermann et al., 2015) and XSum (Narayan et al., 2018), to understand model behavior in each setting. First, by comparing n-grams between the input document and generated summaries, we establish two coarse types for decoded tokens, copy and generate (See et al., 2017). We find that the entropy of the generation decision correlates with whether the model is copying or generating, as well as where in the sentence the token is. This paints a picture of certain contexts being more restrictive from the standpoint of generation, particularly early in sentences where a model has not “decided” what to copy yet, and illustrates the interaction of content"
2020.emnlp-main.508,N16-3020,0,0.100009,"2020; Zhang et al., 2020). Despite their strong performance on automatic metrics like ROUGE (Lin, 2004), abstractive models are not as straightforward and interpretable as their extractive counterparts. Free-form generation in these models also leads to serious downstream errors, such as factual inconsistencies with the input document (Cao et al., 2018; Kry´sci´nski et al., 2020; 1 Code is available at https://github.com/ jiacheng-xu/text-sum-uncertainty Wang et al., 2020; Durmus et al., 2020; Goyal and Durrett, 2020). Although the interpretability of NLU models has been extensively studied (Ribeiro et al., 2016; Ghaeini et al., 2018; Jain and Wallace, 2019; Desai and Durrett, 2020), summarization models specifically have not received similar attention, with analysis efforts often focused on datasets and evaluation (Kryscinski et al., 2019). In this work, we focus on interpreting and understanding abstractive summarization models through the lens of decoder uncertainty, or the entropy of decisions during generation. While uncertainty in generation has been studied from the perspective of data (Ott et al., 2018), sampling (Fan et al., 2018; Holtzman et al., 2019), and training (Correia et al., 2019; K"
2020.emnlp-main.508,D09-1034,0,0.504159,"l., 2017). We find that the entropy of the generation decision correlates with whether the model is copying or generating, as well as where in the sentence the token is. This paints a picture of certain contexts being more restrictive from the standpoint of generation, particularly early in sentences where a model has not “decided” what to copy yet, and illustrates the interaction of content selection and lexical choice. Second, we extend this analysis by looking at how uncertainty relates to the syntax of the generated sentence: whether uncertainty connects to syntactic notions of surprisal (Roark et al., 2009) and how the entropy 6275 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6275–6281, c November 16–20, 2020. 2020 Association for Computational Linguistics varies across certain syntactic productions. Finally, we derive a way to quantify decoder attention by aggregating distinct self-attention heads, revealing the correlation between the attention entropy and prediction entropy, and investigating the correspondence between the prediction entropy and the fraction of the past and future decoded tokens. Taking this analysis together, we find that the"
2020.emnlp-main.508,P17-1099,0,0.369725,"2018; Holtzman et al., 2019), and training (Correia et al., 2019; Kang and Hashimoto, 2020), it is underutilized as a technique for analysis and inspection of generation systems. We study two prominent summarization models, PEGASUS (Zhang et al., 2020) and BART (Lewis et al., 2020), fine-tuned on two English summarization datasets, CNN/Daily Mail (Hermann et al., 2015) and XSum (Narayan et al., 2018), to understand model behavior in each setting. First, by comparing n-grams between the input document and generated summaries, we establish two coarse types for decoded tokens, copy and generate (See et al., 2017). We find that the entropy of the generation decision correlates with whether the model is copying or generating, as well as where in the sentence the token is. This paints a picture of certain contexts being more restrictive from the standpoint of generation, particularly early in sentences where a model has not “decided” what to copy yet, and illustrates the interaction of content selection and lexical choice. Second, we extend this analysis by looking at how uncertainty relates to the syntax of the generated sentence: whether uncertainty connects to syntactic notions of surprisal (Roark et"
2020.emnlp-main.508,2020.acl-main.450,0,0.026471,"ge-scale Transformers pre-trained on autoregressive language modeling objectives (Hoang et al., 2019; Khandelwal et al., 2019; Lewis et al., 2020; Zhang et al., 2020). Despite their strong performance on automatic metrics like ROUGE (Lin, 2004), abstractive models are not as straightforward and interpretable as their extractive counterparts. Free-form generation in these models also leads to serious downstream errors, such as factual inconsistencies with the input document (Cao et al., 2018; Kry´sci´nski et al., 2020; 1 Code is available at https://github.com/ jiacheng-xu/text-sum-uncertainty Wang et al., 2020; Durmus et al., 2020; Goyal and Durrett, 2020). Although the interpretability of NLU models has been extensively studied (Ribeiro et al., 2016; Ghaeini et al., 2018; Jain and Wallace, 2019; Desai and Durrett, 2020), summarization models specifically have not received similar attention, with analysis efforts often focused on datasets and evaluation (Kryscinski et al., 2019). In this work, we focus on interpreting and understanding abstractive summarization models through the lens of decoder uncertainty, or the entropy of decisions during generation. While uncertainty in generation has been stu"
2020.emnlp-main.530,2020.tacl-1.30,0,0.0295435,"are factual (Rajpurkar et al., 2016) or conversational (Choi et al., 2018; Reddy et al., 2019). In contrast, we present a new dataset targeting questions that reflect the semantic and discourse processes during text comprehension. Several other question answering datasets contain questions that are more information-seeking in nature. Some of them are collected from questions that users type in search engines (Yang et al., 2015; Bajaj et al., 2016; Dunn et al., 2017; Kwiatkowski et al., 2019). Others are collected given a small amount of information on a topic sentence (Trischler et al., 2017; Clark et al., 2020) or in the context of a conversation (Choi et al., 2018; Reddy et al., 2019; Qi et al., 2020). Our data is collected from news articles and our questions are precisely anchored to spans in the article, making our questions less open-ended than those in past datasets. While Li et al. (2016b) also collected a 6545 small number of reader questions from news articles, their goal was to study underspecified phrases in sentences when considered out-of-context. Contemporaneously, Westera et al. (2020) presented a dataset of 2.4K naturally elicited questions on TED talks, with the goal to study lingui"
2020.emnlp-main.530,N19-1423,0,0.0319009,"tions are not simply copied from the training data or the article, though the n-gram scores are comparatively much higher than questions asked by human. 6 Question generation from scratch In this section, we assume that spans are not given, and discuss two approaches to generating questions from scratch: a pipeline approach with span prediction, and a question generation model without Models Pipeline. The pipeline approach consists of 2 stages: span prediction and question generation using Inquirer. To predict the span, we use a model similar to the BERT model for the question answering task (Devlin et al., 2019) on SQuAD 1.1 (Rajpurkar et al., 2016). We replace the concatenation passage and question with a concatenation of the target sentence and the previous sentences in the document. Now, the span to ask a question about is treated analogously to the answer span in SQuAD: we find its position in the “passage” which is now the target sentence.5 Sentence+Context. We also experiment with a Sentence+Context model, where the model has no knowledge of any span information in both training and testing; it is trained based purely on (context, sentence) pairs from our dataset. This baseline evaluates the us"
2020.emnlp-main.530,Q19-1026,0,0.117249,"ligent systems, to reflect the ability to understand language, to gather new information, and to engage with users (Vanderwende, 2007, 2008; Piwek and Boyer, 2012; Rus et al., 2010; Huang et al., 2017). A recent line of work on data-driven question generation techniques (Zhou et al., 2017; Yuan et al., 2017; Song et al., 2018; Zhao et al., 2018) focuses on generating questions for datasets like SQuAD (Rajpurkar et al., 2016, 2018). However, factoid questions generated with an answer in mind after the user has read the full text look very different from more natural questions users might have (Kwiatkowski et al., 2019). This has led to work on “answer-agnostic” question generation (Du et al., 2017; Subramanian et al., 2018; Scialom and Staiano, 2019), but the sources of data still emphasize simple factoid questions. Other prior work used question generation to acquire domainspecific knowledge (Yang et al., 2018) and seek clarification in conversation (Rao and Daum´e III, 2018, 2019; Braslavski et al., 2017). However, data-driven generation of questions that reflect text understanding in a more general setting is challenging because of the lack of appropriate training data. We introduce I NQUISITIVE, a new,"
2020.emnlp-main.530,D17-1219,0,0.0197226,"these questions could be used to prioritize what information to keep. The spans and the questions could also help probing the specific and vague parts of the text, which can be useful in conversational AI. Because of the high level nature of our questions, this resource can also be useful for building education applications targeting reading comprehension. 2 Related Work Among question generation settings, ours is most related to answer-agnostic, or answer-unaware question generation: generating a question from text without specifying the location of the answer (Du et al., 2017). Recent work (Du and Cardie, 2017; Subramanian et al., 2018; Wang et al., 2019; Nakanishi et al., 2019) trains models that can extract phrases or sentences that are question-worthy, and uses this information to generate better questions. Scialom and Staiano (2019) paired the question with other sentences in the article that do not contain the answers to construct curiosity-driven questions. However, these approaches are trained by re-purposing questionanswering datasets that are factual (Rajpurkar et al., 2016) or conversational (Choi et al., 2018; Reddy et al., 2019). In contrast, we present a new dataset targeting questions"
2020.emnlp-main.530,N16-1014,0,0.0374568,"at are more information-seeking in nature. Some of them are collected from questions that users type in search engines (Yang et al., 2015; Bajaj et al., 2016; Dunn et al., 2017; Kwiatkowski et al., 2019). Others are collected given a small amount of information on a topic sentence (Trischler et al., 2017; Clark et al., 2020) or in the context of a conversation (Choi et al., 2018; Reddy et al., 2019; Qi et al., 2020). Our data is collected from news articles and our questions are precisely anchored to spans in the article, making our questions less open-ended than those in past datasets. While Li et al. (2016b) also collected a 6545 small number of reader questions from news articles, their goal was to study underspecified phrases in sentences when considered out-of-context. Contemporaneously, Westera et al. (2020) presented a dataset of 2.4K naturally elicited questions on TED talks, with the goal to study linguistic theories of discourse. Previous work generating clarification questions (Rao and Daum´e III, 2018, 2019; Braslavski et al., 2017) uses questions crawled on forums and product reviews. The answers to the questions were used in the models to improve the utility of the generated questio"
2020.emnlp-main.530,P17-1123,0,0.218638,"and to engage with users (Vanderwende, 2007, 2008; Piwek and Boyer, 2012; Rus et al., 2010; Huang et al., 2017). A recent line of work on data-driven question generation techniques (Zhou et al., 2017; Yuan et al., 2017; Song et al., 2018; Zhao et al., 2018) focuses on generating questions for datasets like SQuAD (Rajpurkar et al., 2016, 2018). However, factoid questions generated with an answer in mind after the user has read the full text look very different from more natural questions users might have (Kwiatkowski et al., 2019). This has led to work on “answer-agnostic” question generation (Du et al., 2017; Subramanian et al., 2018; Scialom and Staiano, 2019), but the sources of data still emphasize simple factoid questions. Other prior work used question generation to acquire domainspecific knowledge (Yang et al., 2018) and seek clarification in conversation (Rao and Daum´e III, 2018, 2019; Braslavski et al., 2017). However, data-driven generation of questions that reflect text understanding in a more general setting is challenging because of the lack of appropriate training data. We introduce I NQUISITIVE, a new, large-scale dataset of questions that target high level processing of document c"
2020.emnlp-main.530,L16-1620,1,0.898424,"Missing"
2020.emnlp-main.530,P14-5010,0,0.00558799,"Missing"
2020.emnlp-main.530,P18-1191,0,0.0209807,"hat lead sentences are known to be the most critical for news articles (Errico et al., 1997). For each sentence, we asked 5 distinct annotators from Amazon Mechanical Turk to ask questions. For quality control, we restrict to workers who are located in English speaking countries, and who have completed at least 100 tasks with an approval rating above 0.98. 3.3 Question validation To ensure that our final corpus contain high quality questions, we design a second crowdsourcing task for the validation of these questions, inspired by prior work that also validated crowdsourced questions manually (FitzGerald et al., 2018). At a high level, we want to ensure that the questions are grammatically correct and semantically plausible, related to the highlighted span, and not already answered in the sentence or any previous sentence in the article. Specifically, for each question gathered in Sec6546 Average length std.dev Question 7.1 Highlighted span 3.2 4.1 3.4 2.3 Table 1: Average length and standard deviation of the questions asked by workers and the chosen span tion 3.2, we show the validation annotators the first few sentences of the article up to the sentence where the question is asked, so that the validation"
2020.emnlp-main.530,J93-2004,0,0.0758434,"t questions as one reads (Section 3.2). We then discuss a second validation step for each question we collected as quality control for the data (Section 3.3). 3.1 Text sources In this work we focus on news articles as our source of documents. News articles consist of rich (yet consistent) linguistic structure around a targeted series of events, and are written to engage the readers, hence they are natural test beds for eliciting inquisitive questions that reflect high level processes. We use 1500 news articles, 500 each from three sources: the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993), Associated Press articles from the TIPSTER corpus (Harman and Liberman, 1993), and Newsela (Xu et al., 2015), a commonly used source in text simplification (we use the most advanced reading level only). We select articles that are not opinion pieces and contain more than 8 sentences to make sure that they are indeed news stories and that would involve sufficiently complex scenarios. 3.2 Question collection To capture questions that occur as one reads, we design a crowdsourcing task in which the annotators ask questions about what they are reading currently and without access to any upcoming"
2020.emnlp-main.530,R11-1037,0,0.0182659,"Missing"
2020.emnlp-main.530,C00-1044,0,0.366283,"tions across 1,500 documents1 , each question accompanied by the specific span of the text the question is about, illustrated in Figure 1. The questions are verified to ensure that they are grammatically correct, semantically plausible and meaningful, and not already answered in previous context. We show that the questions capture a variety of phenomena related to high-level semantic and discourse processes, e.g., making causal inferences upon seeing an event or a description, being curious about more detailed information, seeking clarification, interpreting the scale of a gradable adjective (Hatzivassiloglou and Wiebe, 2000), seeking information of an underspecified event (where key participants are missing), and seeking background knowledge. Our analyses reveal that the questions have a very different distribution from those in existing factoid and conversational question answering datasets (Rajpurkar et al., 2016; Trischler et al., 2017; Choi et al., 2018), thus enabling research into generating natural, inquisitive questions. We further present question generation models on this data using GPT-2 (Radford et al., 2019), a state-of-the-art pre-trained language model often used in natural language generation task"
2020.emnlp-main.530,D19-5809,0,0.0257896,"ep. The spans and the questions could also help probing the specific and vague parts of the text, which can be useful in conversational AI. Because of the high level nature of our questions, this resource can also be useful for building education applications targeting reading comprehension. 2 Related Work Among question generation settings, ours is most related to answer-agnostic, or answer-unaware question generation: generating a question from text without specifying the location of the answer (Du et al., 2017). Recent work (Du and Cardie, 2017; Subramanian et al., 2018; Wang et al., 2019; Nakanishi et al., 2019) trains models that can extract phrases or sentences that are question-worthy, and uses this information to generate better questions. Scialom and Staiano (2019) paired the question with other sentences in the article that do not contain the answers to construct curiosity-driven questions. However, these approaches are trained by re-purposing questionanswering datasets that are factual (Rajpurkar et al., 2016) or conversational (Choi et al., 2018; Reddy et al., 2019). In contrast, we present a new dataset targeting questions that reflect the semantic and discourse processes during text compreh"
2020.emnlp-main.530,2020.findings-emnlp.3,0,0.0200645,"In contrast, we present a new dataset targeting questions that reflect the semantic and discourse processes during text comprehension. Several other question answering datasets contain questions that are more information-seeking in nature. Some of them are collected from questions that users type in search engines (Yang et al., 2015; Bajaj et al., 2016; Dunn et al., 2017; Kwiatkowski et al., 2019). Others are collected given a small amount of information on a topic sentence (Trischler et al., 2017; Clark et al., 2020) or in the context of a conversation (Choi et al., 2018; Reddy et al., 2019; Qi et al., 2020). Our data is collected from news articles and our questions are precisely anchored to spans in the article, making our questions less open-ended than those in past datasets. While Li et al. (2016b) also collected a 6545 small number of reader questions from news articles, their goal was to study underspecified phrases in sentences when considered out-of-context. Contemporaneously, Westera et al. (2020) presented a dataset of 2.4K naturally elicited questions on TED talks, with the goal to study linguistic theories of discourse. Previous work generating clarification questions (Rao and Daum´e"
2020.emnlp-main.530,W17-2623,0,0.030809,"Missing"
2020.emnlp-main.530,P18-2124,0,0.0778419,"T-2 (Radford et al., 2019), a state-of-the-art pre-trained language model often used in natural language generation tasks. Human evaluation reveals that our best model is able to generate high-quality questions, though still falls short of human-generated questions in terms of semantic validity, and the questions it generates are more often already answered. Additionally, our experiments explore the importance of model access to already-established common ground (article context), as well as annotations of which part of the text to ask about. Finally, transfer learning results from SQuAD 2.0 (Rajpurkar et al., 2018) show that generating inquisitive questions is a distinct task from question generation using factoid question answering datasets. 1 Data available at https://github.com/wjko2/ INQUISITIVE The capability for question generation models to simulate human-like curiosity and cognitive processing opens up a new realm of applications. One example for this sort of question generation is guided text writing for either machines or humans: we could use these questions to identify important points of information that are not mentioned yet and should be included. In text simplification and summarization,"
2020.emnlp-main.530,D16-1264,0,0.441465,"of a gap in knowledge or understanding” (Loewenstein, 1994). Because of its prominence in human cognition and behavior, being able to formulate the right question is highly sought after in intelligent systems, to reflect the ability to understand language, to gather new information, and to engage with users (Vanderwende, 2007, 2008; Piwek and Boyer, 2012; Rus et al., 2010; Huang et al., 2017). A recent line of work on data-driven question generation techniques (Zhou et al., 2017; Yuan et al., 2017; Song et al., 2018; Zhao et al., 2018) focuses on generating questions for datasets like SQuAD (Rajpurkar et al., 2016, 2018). However, factoid questions generated with an answer in mind after the user has read the full text look very different from more natural questions users might have (Kwiatkowski et al., 2019). This has led to work on “answer-agnostic” question generation (Du et al., 2017; Subramanian et al., 2018; Scialom and Staiano, 2019), but the sources of data still emphasize simple factoid questions. Other prior work used question generation to acquire domainspecific knowledge (Yang et al., 2018) and seek clarification in conversation (Rao and Daum´e III, 2018, 2019; Braslavski et al., 2017). Howe"
2020.emnlp-main.530,P18-1255,0,0.0634711,"Missing"
2020.emnlp-main.530,N19-1013,0,0.132128,"Missing"
2020.emnlp-main.530,Q19-1016,0,0.0217788,"the location of the answer (Du et al., 2017). Recent work (Du and Cardie, 2017; Subramanian et al., 2018; Wang et al., 2019; Nakanishi et al., 2019) trains models that can extract phrases or sentences that are question-worthy, and uses this information to generate better questions. Scialom and Staiano (2019) paired the question with other sentences in the article that do not contain the answers to construct curiosity-driven questions. However, these approaches are trained by re-purposing questionanswering datasets that are factual (Rajpurkar et al., 2016) or conversational (Choi et al., 2018; Reddy et al., 2019). In contrast, we present a new dataset targeting questions that reflect the semantic and discourse processes during text comprehension. Several other question answering datasets contain questions that are more information-seeking in nature. Some of them are collected from questions that users type in search engines (Yang et al., 2015; Bajaj et al., 2016; Dunn et al., 2017; Kwiatkowski et al., 2019). Others are collected given a small amount of information on a topic sentence (Trischler et al., 2017; Clark et al., 2020) or in the context of a conversation (Choi et al., 2018; Reddy et al., 2019"
2020.emnlp-main.530,W10-4234,0,0.0433648,"e meaningful, inquisitive questions is natural to humans. Studies among children (Jirout, 2011) showed that questions serving to better understand natural language text are an organic reflection of curiosity, which “arise from the perception of a gap in knowledge or understanding” (Loewenstein, 1994). Because of its prominence in human cognition and behavior, being able to formulate the right question is highly sought after in intelligent systems, to reflect the ability to understand language, to gather new information, and to engage with users (Vanderwende, 2007, 2008; Piwek and Boyer, 2012; Rus et al., 2010; Huang et al., 2017). A recent line of work on data-driven question generation techniques (Zhou et al., 2017; Yuan et al., 2017; Song et al., 2018; Zhao et al., 2018) focuses on generating questions for datasets like SQuAD (Rajpurkar et al., 2016, 2018). However, factoid questions generated with an answer in mind after the user has read the full text look very different from more natural questions users might have (Kwiatkowski et al., 2019). This has led to work on “answer-agnostic” question generation (Du et al., 2017; Subramanian et al., 2018; Scialom and Staiano, 2019), but the sources of"
2020.emnlp-main.530,N18-2090,0,0.0465461,"natural language text are an organic reflection of curiosity, which “arise from the perception of a gap in knowledge or understanding” (Loewenstein, 1994). Because of its prominence in human cognition and behavior, being able to formulate the right question is highly sought after in intelligent systems, to reflect the ability to understand language, to gather new information, and to engage with users (Vanderwende, 2007, 2008; Piwek and Boyer, 2012; Rus et al., 2010; Huang et al., 2017). A recent line of work on data-driven question generation techniques (Zhou et al., 2017; Yuan et al., 2017; Song et al., 2018; Zhao et al., 2018) focuses on generating questions for datasets like SQuAD (Rajpurkar et al., 2016, 2018). However, factoid questions generated with an answer in mind after the user has read the full text look very different from more natural questions users might have (Kwiatkowski et al., 2019). This has led to work on “answer-agnostic” question generation (Du et al., 2017; Subramanian et al., 2018; Scialom and Staiano, 2019), but the sources of data still emphasize simple factoid questions. Other prior work used question generation to acquire domainspecific knowledge (Yang et al., 2018) an"
2020.emnlp-main.530,2020.lrec-1.141,0,0.0403886,"Others are collected given a small amount of information on a topic sentence (Trischler et al., 2017; Clark et al., 2020) or in the context of a conversation (Choi et al., 2018; Reddy et al., 2019; Qi et al., 2020). Our data is collected from news articles and our questions are precisely anchored to spans in the article, making our questions less open-ended than those in past datasets. While Li et al. (2016b) also collected a 6545 small number of reader questions from news articles, their goal was to study underspecified phrases in sentences when considered out-of-context. Contemporaneously, Westera et al. (2020) presented a dataset of 2.4K naturally elicited questions on TED talks, with the goal to study linguistic theories of discourse. Previous work generating clarification questions (Rao and Daum´e III, 2018, 2019; Braslavski et al., 2017) uses questions crawled on forums and product reviews. The answers to the questions were used in the models to improve the utility of the generated question. In our data, clarification is only one of the pragmatic goals. In addition, we focus on news articles which contains more narrative discourse and temporal progression. 3 I NQUISITIVE: A corpus of questions T"
2020.emnlp-main.530,Q15-1021,0,0.026248,"quality control for the data (Section 3.3). 3.1 Text sources In this work we focus on news articles as our source of documents. News articles consist of rich (yet consistent) linguistic structure around a targeted series of events, and are written to engage the readers, hence they are natural test beds for eliciting inquisitive questions that reflect high level processes. We use 1500 news articles, 500 each from three sources: the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993), Associated Press articles from the TIPSTER corpus (Harman and Liberman, 1993), and Newsela (Xu et al., 2015), a commonly used source in text simplification (we use the most advanced reading level only). We select articles that are not opinion pieces and contain more than 8 sentences to make sure that they are indeed news stories and that would involve sufficiently complex scenarios. 3.2 Question collection To capture questions that occur as one reads, we design a crowdsourcing task in which the annotators ask questions about what they are reading currently and without access to any upcoming context. The annotators start from the beginning of an It's not enough for people to get regular moderate exer"
2020.emnlp-main.530,D15-1237,0,0.084767,"Missing"
2020.emnlp-main.530,W17-2603,0,0.0188337,"o better understand natural language text are an organic reflection of curiosity, which “arise from the perception of a gap in knowledge or understanding” (Loewenstein, 1994). Because of its prominence in human cognition and behavior, being able to formulate the right question is highly sought after in intelligent systems, to reflect the ability to understand language, to gather new information, and to engage with users (Vanderwende, 2007, 2008; Piwek and Boyer, 2012; Rus et al., 2010; Huang et al., 2017). A recent line of work on data-driven question generation techniques (Zhou et al., 2017; Yuan et al., 2017; Song et al., 2018; Zhao et al., 2018) focuses on generating questions for datasets like SQuAD (Rajpurkar et al., 2016, 2018). However, factoid questions generated with an answer in mind after the user has read the full text look very different from more natural questions users might have (Kwiatkowski et al., 2019). This has led to work on “answer-agnostic” question generation (Du et al., 2017; Subramanian et al., 2018; Scialom and Staiano, 2019), but the sources of data still emphasize simple factoid questions. Other prior work used question generation to acquire domainspecific knowledge (Ya"
2020.emnlp-main.530,D18-1424,0,0.0622216,"ext are an organic reflection of curiosity, which “arise from the perception of a gap in knowledge or understanding” (Loewenstein, 1994). Because of its prominence in human cognition and behavior, being able to formulate the right question is highly sought after in intelligent systems, to reflect the ability to understand language, to gather new information, and to engage with users (Vanderwende, 2007, 2008; Piwek and Boyer, 2012; Rus et al., 2010; Huang et al., 2017). A recent line of work on data-driven question generation techniques (Zhou et al., 2017; Yuan et al., 2017; Song et al., 2018; Zhao et al., 2018) focuses on generating questions for datasets like SQuAD (Rajpurkar et al., 2016, 2018). However, factoid questions generated with an answer in mind after the user has read the full text look very different from more natural questions users might have (Kwiatkowski et al., 2019). This has led to work on “answer-agnostic” question generation (Du et al., 2017; Subramanian et al., 2018; Scialom and Staiano, 2019), but the sources of data still emphasize simple factoid questions. Other prior work used question generation to acquire domainspecific knowledge (Yang et al., 2018) and seek clarification"
2020.emnlp-main.530,P17-2060,0,0.0155734,"questions serving to better understand natural language text are an organic reflection of curiosity, which “arise from the perception of a gap in knowledge or understanding” (Loewenstein, 1994). Because of its prominence in human cognition and behavior, being able to formulate the right question is highly sought after in intelligent systems, to reflect the ability to understand language, to gather new information, and to engage with users (Vanderwende, 2007, 2008; Piwek and Boyer, 2012; Rus et al., 2010; Huang et al., 2017). A recent line of work on data-driven question generation techniques (Zhou et al., 2017; Yuan et al., 2017; Song et al., 2018; Zhao et al., 2018) focuses on generating questions for datasets like SQuAD (Rajpurkar et al., 2016, 2018). However, factoid questions generated with an answer in mind after the user has read the full text look very different from more natural questions users might have (Kwiatkowski et al., 2019). This has led to work on “answer-agnostic” question generation (Du et al., 2017; Subramanian et al., 2018; Scialom and Staiano, 2019), but the sources of data still emphasize simple factoid questions. Other prior work used question generation to acquire domainspe"
2020.findings-emnlp.322,W13-2322,0,0.038157,"endency arcs as semantic units that can be interpreted in isolation. Each arc is therefore judged indepen3592 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3592–3603 c November 16 - 20, 2020. 2020 Association for Computational Linguistics dently based on whether the relation it implies is entailed by the source sentence. This is helpful in localizing generation errors and consequently providing more interpretable model decisions. Decomposing the factuality evaluation over components of structured representations can also be extended to other formalisms like AMR (Banarescu et al., 2013), UDS (White et al., 2016), and more. The chief advantage of dependency parsing over these is that pre-existing tools for dependency parsing report very high performance. Another line of work focuses on question answering-based semantic representations (FitzGerald et al., 2018; Michael et al., 2018) or generating freeform questions to capture factuality (Wang et al., 2020; Durmus et al., 2020). However, these systems require a separate question generation step at inference time, so generation is baked into their formalisms in a heavyweight way whereas we only require dependencies. A final adva"
2020.findings-emnlp.322,D15-1075,0,0.274178,"ual generation data provide much better task-specific supervision compared to general entailment datasets. Finally, we demonstrate that predicted entailment scores for individual dependency arcs are meaningful and can be leveraged to understand and localize errors in system generations. 2 Dependency Arc Entailment (DAE) Defining arc entailment Our notion of entailment starts by assuming a rough correspondence between predicates and arguments in two sentences. In natural language inference (NLI) annotation efforts, this has taken the form of anchoring judgments in an underlying imagined scene (Bowman et al., 2015). We make a similar assumption, that events and actors in the source and target sentences are in correspondence unless there is direct evidence to the contrary. For instance, in Figure 1, the military coup in the target sentence and its corresponding amod(coup→military) arc should be evaluated with respect to the military takeover in the source, giving coreference of the two the benefit of the doubt here. With this assumption, we say that a dependency arc in the target sentence is entailed by the source if the semantic relationship it implies between its head and child is entailed by the sourc"
2020.findings-emnlp.322,P18-1063,0,0.0167156,"rations Moving beyond the dependency-level inference task, we now want to evaluate the sentence-level performance of our model formulation. Namely, can it usefully reject erroneous generations produced by models for summarization (Section 6.1) and paraphrasing (Section 6.2)? 6.1 Summary Ranking We perform our evaluation on an abstractive summarization test dataset introduced in Falke et al. (2019) and used in other previous work. It contains 373 test samples, each containing an input source sentence from CNN/DM and two summary sentences covering the same content generated using the model from Chen and Bansal (2018). One of these summary sentences is factually correct and the other is factually incorrect. The evaluation protocol measures how often the correct summary is scored higher than the incorrect summary for each candidate scoring technique. We compare against the following baselines: 1. NLI models: Following Falke et al. (2019), we use entailment predictions of NLI models to rerank candidates. We compare the performance of pretrained encoders (B ERT, RO B ERTA and E LECTRA) fine-tuned on the MNLI dataset (Williams et al., 2018).6 3597 6 We fine-tune the B ERT and E LECTRA models ourselves 72.3 78."
2020.findings-emnlp.322,N19-1423,0,0.00823172,"zing the erroneous parts of the generation.1 expl nsubj Genera&on Hypotheses Abstract there was talk about the possibility of a military coup. military coup was the feverish talk. nmod:of nmod:about amod There was talk about the possibility of a military coup. amod nsubj amod military coup was the feverish talk. Figure 1: Overview of our dependency arc entailment formulation using a filtered set of Stanford Enhanced Dependencies. The DAE model makes independent factuality decisions for each dependency arc from the two generated hypotheses. Introduction The rise of pre-trained language models (Devlin et al., 2019; Radford et al., 2019) has led to strong text generation models for applications including summarization (Dong et al., 2019; Lewis et al., 2020), paraphrasing (Goyal and Durrett, 2020; Shen et al., 2020), story generation (Mao et al., 2019), and data augmentation (Yu et al., 2018; Zhang and Bansal, 2019). However, while these models generate fluent and grammatical text, they are prone to making factual errors that contradict 1 Data and code available at https://github.com/ tagoyal/dae-factuality the input text (Cao et al., 2018). Automatic metrics used to evaluate text generation, such as ROU"
2020.findings-emnlp.322,2020.acl-main.454,0,0.155461,"rs and consequently providing more interpretable model decisions. Decomposing the factuality evaluation over components of structured representations can also be extended to other formalisms like AMR (Banarescu et al., 2013), UDS (White et al., 2016), and more. The chief advantage of dependency parsing over these is that pre-existing tools for dependency parsing report very high performance. Another line of work focuses on question answering-based semantic representations (FitzGerald et al., 2018; Michael et al., 2018) or generating freeform questions to capture factuality (Wang et al., 2020; Durmus et al., 2020). However, these systems require a separate question generation step at inference time, so generation is baked into their formalisms in a heavyweight way whereas we only require dependencies. A final advantage of our approach is that dependency arcs can be produced in an online fashion during inference, and hence, factuality can be enforced incrementally. We evaluate our proposed dependency arc entailment approach in both summarization and paraphrase settings. In both settings, we show that we can automatically derive labels from actual generation data rather than rely on human annotation of d"
2020.findings-emnlp.322,P19-1213,0,0.0340247,"Missing"
2020.findings-emnlp.322,P18-1191,0,0.0116342,"based on whether the relation it implies is entailed by the source sentence. This is helpful in localizing generation errors and consequently providing more interpretable model decisions. Decomposing the factuality evaluation over components of structured representations can also be extended to other formalisms like AMR (Banarescu et al., 2013), UDS (White et al., 2016), and more. The chief advantage of dependency parsing over these is that pre-existing tools for dependency parsing report very high performance. Another line of work focuses on question answering-based semantic representations (FitzGerald et al., 2018; Michael et al., 2018) or generating freeform questions to capture factuality (Wang et al., 2020; Durmus et al., 2020). However, these systems require a separate question generation step at inference time, so generation is baked into their formalisms in a heavyweight way whereas we only require dependencies. A final advantage of our approach is that dependency arcs can be produced in an online fashion during inference, and hence, factuality can be enforced incrementally. We evaluate our proposed dependency arc entailment approach in both summarization and paraphrase settings. In both settings"
2020.findings-emnlp.322,2020.acl-main.22,1,0.738724,"nmod:of nmod:about amod There was talk about the possibility of a military coup. amod nsubj amod military coup was the feverish talk. Figure 1: Overview of our dependency arc entailment formulation using a filtered set of Stanford Enhanced Dependencies. The DAE model makes independent factuality decisions for each dependency arc from the two generated hypotheses. Introduction The rise of pre-trained language models (Devlin et al., 2019; Radford et al., 2019) has led to strong text generation models for applications including summarization (Dong et al., 2019; Lewis et al., 2020), paraphrasing (Goyal and Durrett, 2020; Shen et al., 2020), story generation (Mao et al., 2019), and data augmentation (Yu et al., 2018; Zhang and Bansal, 2019). However, while these models generate fluent and grammatical text, they are prone to making factual errors that contradict 1 Data and code available at https://github.com/ tagoyal/dae-factuality the input text (Cao et al., 2018). Automatic metrics used to evaluate text generation, such as ROUGE and BERTScore (Zhang et al., 2020), are not correlated with the factual consistency or faithfulness of the generated text (Falke et al., 2019; Kry´sci´nski et al., 2019). To address"
2020.findings-emnlp.322,P18-1064,0,0.0198312,"multi-task formulations, and post-processing methods. The first group leverages structured knowledge, like Open IE triples (Cao et al., 2018; Goodrich et al., 2019), dependency trees (Song et al., 2018), or generated semantic roles (Fan et al., 2018) as additional input for generation. However, incorporation of these as additional embeddings in model architectures does not explain how these influence model generations. The second group leverages multi-task formulations and trains the generation model jointly with other factuality-related tasks, such as NLI entailment and question generation (Guo et al., 2018). Other work additionally incorporates a reward for generating summaries entailed by the the input (Li et al., 2018; Pasunuru and Bansal, 2018). Our approach can be used to rank/filter outputs from any generation model in a black-box way, without additional augmentation or retraining. In post-processing approaches, recent work has explored NLI-based (Falke et al., 2019; Maynez et al., 2020) post-generation filtering or ranking of output summaries. Our dependency-level models perform on par with these approaches, while additionally localizing the error in the generations. Other work (Durmus et"
2020.findings-emnlp.322,N18-2017,0,0.0193373,"ual errors that contradict 1 Data and code available at https://github.com/ tagoyal/dae-factuality the input text (Cao et al., 2018). Automatic metrics used to evaluate text generation, such as ROUGE and BERTScore (Zhang et al., 2020), are not correlated with the factual consistency or faithfulness of the generated text (Falke et al., 2019; Kry´sci´nski et al., 2019). To address this, recent work has studied the use of textual entailment models to rank and filter non-factual generations (Falke et al., 2019; Maynez et al., 2020). However, these models suffer from issues such as dataset biases (Gururangan et al., 2018; Zhou and Bansal, 2020) and a mismatch between the training data (entailment) and the test data (model generations). In this paper, we propose to decompose entailment decisions in a sentence to evaluate the faithfulness of generated text in a more fine-grained way. Rather than making a sentence-level entailment decision, we instead evaluate the entailment of dependency arcs of the generated sentence, as illustrated in Figure 1. This approach views dependency arcs as semantic units that can be interpreted in isolation. Each arc is therefore judged indepen3592 Findings of the Association for Co"
2020.findings-emnlp.322,D19-1051,0,0.121184,"Missing"
2020.findings-emnlp.322,2020.acl-main.703,0,0.0352105,"litary coup was the feverish talk. nmod:of nmod:about amod There was talk about the possibility of a military coup. amod nsubj amod military coup was the feverish talk. Figure 1: Overview of our dependency arc entailment formulation using a filtered set of Stanford Enhanced Dependencies. The DAE model makes independent factuality decisions for each dependency arc from the two generated hypotheses. Introduction The rise of pre-trained language models (Devlin et al., 2019; Radford et al., 2019) has led to strong text generation models for applications including summarization (Dong et al., 2019; Lewis et al., 2020), paraphrasing (Goyal and Durrett, 2020; Shen et al., 2020), story generation (Mao et al., 2019), and data augmentation (Yu et al., 2018; Zhang and Bansal, 2019). However, while these models generate fluent and grammatical text, they are prone to making factual errors that contradict 1 Data and code available at https://github.com/ tagoyal/dae-factuality the input text (Cao et al., 2018). Automatic metrics used to evaluate text generation, such as ROUGE and BERTScore (Zhang et al., 2020), are not correlated with the factual consistency or faithfulness of the generated text (Falke et al., 2019;"
2020.findings-emnlp.322,C18-1121,0,0.0174362,"riples (Cao et al., 2018; Goodrich et al., 2019), dependency trees (Song et al., 2018), or generated semantic roles (Fan et al., 2018) as additional input for generation. However, incorporation of these as additional embeddings in model architectures does not explain how these influence model generations. The second group leverages multi-task formulations and trains the generation model jointly with other factuality-related tasks, such as NLI entailment and question generation (Guo et al., 2018). Other work additionally incorporates a reward for generating summaries entailed by the the input (Li et al., 2018; Pasunuru and Bansal, 2018). Our approach can be used to rank/filter outputs from any generation model in a black-box way, without additional augmentation or retraining. In post-processing approaches, recent work has explored NLI-based (Falke et al., 2019; Maynez et al., 2020) post-generation filtering or ranking of output summaries. Our dependency-level models perform on par with these approaches, while additionally localizing the error in the generations. Other work (Durmus et al., 2020; Wang et al., 2020) has looked at using question generation and answering to reveal factual inconsistenci"
2020.findings-emnlp.322,P14-5010,0,0.00473447,"Missing"
2020.findings-emnlp.322,D19-1615,0,0.0283481,"a military coup. amod nsubj amod military coup was the feverish talk. Figure 1: Overview of our dependency arc entailment formulation using a filtered set of Stanford Enhanced Dependencies. The DAE model makes independent factuality decisions for each dependency arc from the two generated hypotheses. Introduction The rise of pre-trained language models (Devlin et al., 2019; Radford et al., 2019) has led to strong text generation models for applications including summarization (Dong et al., 2019; Lewis et al., 2020), paraphrasing (Goyal and Durrett, 2020; Shen et al., 2020), story generation (Mao et al., 2019), and data augmentation (Yu et al., 2018; Zhang and Bansal, 2019). However, while these models generate fluent and grammatical text, they are prone to making factual errors that contradict 1 Data and code available at https://github.com/ tagoyal/dae-factuality the input text (Cao et al., 2018). Automatic metrics used to evaluate text generation, such as ROUGE and BERTScore (Zhang et al., 2020), are not correlated with the factual consistency or faithfulness of the generated text (Falke et al., 2019; Kry´sci´nski et al., 2019). To address this, recent work has studied the use of textual entailm"
2020.findings-emnlp.322,2020.acl-main.173,0,0.0914386,"while these models generate fluent and grammatical text, they are prone to making factual errors that contradict 1 Data and code available at https://github.com/ tagoyal/dae-factuality the input text (Cao et al., 2018). Automatic metrics used to evaluate text generation, such as ROUGE and BERTScore (Zhang et al., 2020), are not correlated with the factual consistency or faithfulness of the generated text (Falke et al., 2019; Kry´sci´nski et al., 2019). To address this, recent work has studied the use of textual entailment models to rank and filter non-factual generations (Falke et al., 2019; Maynez et al., 2020). However, these models suffer from issues such as dataset biases (Gururangan et al., 2018; Zhou and Bansal, 2020) and a mismatch between the training data (entailment) and the test data (model generations). In this paper, we propose to decompose entailment decisions in a sentence to evaluate the faithfulness of generated text in a more fine-grained way. Rather than making a sentence-level entailment decision, we instead evaluate the entailment of dependency arcs of the generated sentence, as illustrated in Figure 1. This approach views dependency arcs as semantic units that can be interpreted"
2020.findings-emnlp.322,N18-2089,0,0.0210507,"tion it implies is entailed by the source sentence. This is helpful in localizing generation errors and consequently providing more interpretable model decisions. Decomposing the factuality evaluation over components of structured representations can also be extended to other formalisms like AMR (Banarescu et al., 2013), UDS (White et al., 2016), and more. The chief advantage of dependency parsing over these is that pre-existing tools for dependency parsing report very high performance. Another line of work focuses on question answering-based semantic representations (FitzGerald et al., 2018; Michael et al., 2018) or generating freeform questions to capture factuality (Wang et al., 2020; Durmus et al., 2020). However, these systems require a separate question generation step at inference time, so generation is baked into their formalisms in a heavyweight way whereas we only require dependencies. A final advantage of our approach is that dependency arcs can be produced in an online fashion during inference, and hence, factuality can be enforced incrementally. We evaluate our proposed dependency arc entailment approach in both summarization and paraphrase settings. In both settings, we show that we can a"
2020.findings-emnlp.322,N18-2102,0,0.0281394,"., 2018; Goodrich et al., 2019), dependency trees (Song et al., 2018), or generated semantic roles (Fan et al., 2018) as additional input for generation. However, incorporation of these as additional embeddings in model architectures does not explain how these influence model generations. The second group leverages multi-task formulations and trains the generation model jointly with other factuality-related tasks, such as NLI entailment and question generation (Guo et al., 2018). Other work additionally incorporates a reward for generating summaries entailed by the the input (Li et al., 2018; Pasunuru and Bansal, 2018). Our approach can be used to rank/filter outputs from any generation model in a black-box way, without additional augmentation or retraining. In post-processing approaches, recent work has explored NLI-based (Falke et al., 2019; Maynez et al., 2020) post-generation filtering or ranking of output summaries. Our dependency-level models perform on par with these approaches, while additionally localizing the error in the generations. Other work (Durmus et al., 2020; Wang et al., 2020) has looked at using question generation and answering to reveal factual inconsistencies in generated text. Howeve"
2020.findings-emnlp.322,2020.acl-main.641,0,0.0227427,"here was talk about the possibility of a military coup. amod nsubj amod military coup was the feverish talk. Figure 1: Overview of our dependency arc entailment formulation using a filtered set of Stanford Enhanced Dependencies. The DAE model makes independent factuality decisions for each dependency arc from the two generated hypotheses. Introduction The rise of pre-trained language models (Devlin et al., 2019; Radford et al., 2019) has led to strong text generation models for applications including summarization (Dong et al., 2019; Lewis et al., 2020), paraphrasing (Goyal and Durrett, 2020; Shen et al., 2020), story generation (Mao et al., 2019), and data augmentation (Yu et al., 2018; Zhang and Bansal, 2019). However, while these models generate fluent and grammatical text, they are prone to making factual errors that contradict 1 Data and code available at https://github.com/ tagoyal/dae-factuality the input text (Cao et al., 2018). Automatic metrics used to evaluate text generation, such as ROUGE and BERTScore (Zhang et al., 2020), are not correlated with the factual consistency or faithfulness of the generated text (Falke et al., 2019; Kry´sci´nski et al., 2019). To address this, recent work h"
2020.findings-emnlp.322,C18-1146,0,0.0241353,"r model is trained on the PARA NMT-50 M dataset, which itself is constructed through a noisy backtranslation process. Therefore, we rely on noisy gold data for constructing our model. We believe that better quality paraphrase pairs would lead to a better quality model. 8 Related Work Recent work in addressing faithfulness of text generations can be broadly divided into three groups: structured information based, multi-task formulations, and post-processing methods. The first group leverages structured knowledge, like Open IE triples (Cao et al., 2018; Goodrich et al., 2019), dependency trees (Song et al., 2018), or generated semantic roles (Fan et al., 2018) as additional input for generation. However, incorporation of these as additional embeddings in model architectures does not explain how these influence model generations. The second group leverages multi-task formulations and trains the generation model jointly with other factuality-related tasks, such as NLI entailment and question generation (Guo et al., 2018). Other work additionally incorporates a reward for generating summaries entailed by the the input (Li et al., 2018; Pasunuru and Bansal, 2018). Our approach can be used to rank/filter o"
2020.findings-emnlp.322,2020.acl-main.450,0,0.206585,"ing generation errors and consequently providing more interpretable model decisions. Decomposing the factuality evaluation over components of structured representations can also be extended to other formalisms like AMR (Banarescu et al., 2013), UDS (White et al., 2016), and more. The chief advantage of dependency parsing over these is that pre-existing tools for dependency parsing report very high performance. Another line of work focuses on question answering-based semantic representations (FitzGerald et al., 2018; Michael et al., 2018) or generating freeform questions to capture factuality (Wang et al., 2020; Durmus et al., 2020). However, these systems require a separate question generation step at inference time, so generation is baked into their formalisms in a heavyweight way whereas we only require dependencies. A final advantage of our approach is that dependency arcs can be produced in an online fashion during inference, and hence, factuality can be enforced incrementally. We evaluate our proposed dependency arc entailment approach in both summarization and paraphrase settings. In both settings, we show that we can automatically derive labels from actual generation data rather than rely on"
2020.findings-emnlp.322,D16-1177,0,0.0700128,"Missing"
2020.findings-emnlp.322,P18-1042,0,0.0907985,"Missing"
2020.findings-emnlp.322,N18-1101,0,0.0490279,"first hypothesis in Figure 1. Many of the arcs here either contain information analogous to that in semantic roles, or they specify nominal modifiers capturing important entity properties.2 In our implementation, we exclude certain arc types which are not strongly tied to semantics, such as arcs involving punctuation; see the Appendix for details. Note that our method does not support commenting on arcs of the input that do not exist in the output; we discuss this later in Section 7.2. In some ways, our view of entailment is equivalent with the entailment of NLI settings (Bowman et al., 2015; Williams et al., 2018): if a hypothesis is entailed under the NLI definition, then all dependency arcs of the hypothesis must be entailed by our DAE definition. However, in our formulation, arc entailment is a 2-class classification task with labels ∈ {entailed, non-entailed}. This means that arcs that would be neutral or contradiction in the generic entailment formulation are considered non-entailed in our scenario. Annotating arc entailment To model this formulation, we require entailment annotations at the dependency arc level. However, there are several challenges associated with human annotation of arc entailm"
2020.findings-emnlp.322,D14-1162,0,0.0848384,"Missing"
2020.findings-emnlp.322,D19-1253,0,0.0243051,"erish talk. Figure 1: Overview of our dependency arc entailment formulation using a filtered set of Stanford Enhanced Dependencies. The DAE model makes independent factuality decisions for each dependency arc from the two generated hypotheses. Introduction The rise of pre-trained language models (Devlin et al., 2019; Radford et al., 2019) has led to strong text generation models for applications including summarization (Dong et al., 2019; Lewis et al., 2020), paraphrasing (Goyal and Durrett, 2020; Shen et al., 2020), story generation (Mao et al., 2019), and data augmentation (Yu et al., 2018; Zhang and Bansal, 2019). However, while these models generate fluent and grammatical text, they are prone to making factual errors that contradict 1 Data and code available at https://github.com/ tagoyal/dae-factuality the input text (Cao et al., 2018). Automatic metrics used to evaluate text generation, such as ROUGE and BERTScore (Zhang et al., 2020), are not correlated with the factual consistency or faithfulness of the generated text (Falke et al., 2019; Kry´sci´nski et al., 2019). To address this, recent work has studied the use of textual entailment models to rank and filter non-factual generations (Falke et a"
2020.findings-emnlp.322,N19-1131,0,0.0574986,"Missing"
2020.findings-emnlp.322,2020.acl-main.773,0,0.0693175,"t 1 Data and code available at https://github.com/ tagoyal/dae-factuality the input text (Cao et al., 2018). Automatic metrics used to evaluate text generation, such as ROUGE and BERTScore (Zhang et al., 2020), are not correlated with the factual consistency or faithfulness of the generated text (Falke et al., 2019; Kry´sci´nski et al., 2019). To address this, recent work has studied the use of textual entailment models to rank and filter non-factual generations (Falke et al., 2019; Maynez et al., 2020). However, these models suffer from issues such as dataset biases (Gururangan et al., 2018; Zhou and Bansal, 2020) and a mismatch between the training data (entailment) and the test data (model generations). In this paper, we propose to decompose entailment decisions in a sentence to evaluate the faithfulness of generated text in a more fine-grained way. Rather than making a sentence-level entailment decision, we instead evaluate the entailment of dependency arcs of the generated sentence, as illustrated in Figure 1. This approach views dependency arcs as semantic units that can be interpreted in isolation. Each arc is therefore judged indepen3592 Findings of the Association for Computational Linguistics:"
2020.findings-emnlp.414,D19-1141,0,0.108706,"Missing"
2020.findings-emnlp.414,D16-1264,0,0.0299418,"of RO BERTA - BASE (Liu et al., 2019) using the reference fairseq implementation (Ott et al., 2019). Two are pretrained on the text of English Wikipedia, comprising ∼3B tokens under either tokenization. The other two are pretrained on the text of Japanese Wikipedia, comprising ∼0.6B tokens. In each pair, one model is pretrained on the BPE tokenization of the corpus, and the other on the unigram LM tokenization, each with a vocabulary of 20,000 tokens. Hyperparameters are listed in Appendix A. We subsequently fine-tune each of the pretrained English models on the SQuAD question-answering task (Rajpurkar et al., 2016), the MNLI textual entailment task (Williams et al., 2018), and the English portion of the CoNLL 2003 named-entity recognition shared task (Tjong Kim Sang and De Meulder, 2003). We fine-tune the Japanese models on the Japanese minimal-answer subset of the TyDi question-answering task (Clark et al., 2020). We base our fine-tuning implementations on those of the transformers toolkit (Wolf et al., 2019). The results of our fine-tuning experiments are presented in Table 4. We show that fine-tuning models pretrained with unigram LM tokenization produces better performance than fine-tuning models pr"
2020.findings-emnlp.414,P16-1162,0,0.641653,"BPE. We hope that developers of future pretrained LMs will consider adopting the unigram LM method over the more prevalent BPE. 1 Introduction Large transformers (Vaswani et al., 2017) pretrained with variants of a language modeling objective, such as BERT (Devlin et al., 2019), have proven their effectiveness at flexibly transferring to a variety of domains and tasks. One design decision that makes them particularly adaptable is their graceful handling of the open vocabulary problem through subword tokenization. Subword tokenization, popularized in the neural machine translation literature (Sennrich et al., 2016; Vaswani et al., 2017; Wu et al., 2016), produces tokens at multiple levels of granularity, from individual characters to full words. As a result, rare words are broken down into a collection of subword units, bottoming out in characters in the worst case. Critically, a pretrained language model’s subword vocabulary cannot be altered: any downstream application of these models must tokenize input or generate output using the original subword vocabulary, making the choice of tokenization a particularly significant decision. A variety of subword tokenization methods have seen use in pretrained"
2020.findings-emnlp.414,N18-1101,0,0.0196296,"airseq implementation (Ott et al., 2019). Two are pretrained on the text of English Wikipedia, comprising ∼3B tokens under either tokenization. The other two are pretrained on the text of Japanese Wikipedia, comprising ∼0.6B tokens. In each pair, one model is pretrained on the BPE tokenization of the corpus, and the other on the unigram LM tokenization, each with a vocabulary of 20,000 tokens. Hyperparameters are listed in Appendix A. We subsequently fine-tune each of the pretrained English models on the SQuAD question-answering task (Rajpurkar et al., 2016), the MNLI textual entailment task (Williams et al., 2018), and the English portion of the CoNLL 2003 named-entity recognition shared task (Tjong Kim Sang and De Meulder, 2003). We fine-tune the Japanese models on the Japanese minimal-answer subset of the TyDi question-answering task (Clark et al., 2020). We base our fine-tuning implementations on those of the transformers toolkit (Wolf et al., 2019). The results of our fine-tuning experiments are presented in Table 4. We show that fine-tuning models pretrained with unigram LM tokenization produces better performance than fine-tuning models pretrained with BPE tokenization for all tasks. These result"
2020.findings-emnlp.414,W03-0419,0,\N,Missing
2020.findings-emnlp.54,P19-1279,0,0.0232647,"rpretable entity representations. (1) A mention and its context are fed into (2) an embedding model. (3) An entity embedding vector consists of probabilities for corresponding types. (4) We can reduce the size of the type set for a particular downstream task in a learning-based way (Section 5.2). (5) We can also incorporate domain knowledge via rules to modify bad type probabilities and improve model performance (Section 5.3). ture. Dense representations of entities have similarly been applied to entity linking (Yamada et al., 2016; Eshel et al., 2017), as well as relation extraction (Baldini Soares et al., 2019), entity typing (Ling et al., 2020), and question answering (F´evry et al., 2020). Those approaches use millions of predefined entities, while our approach uses a much smaller number of types (10k or 60k). This makes it simultaneously more compact and also more flexible when generalizing to unknown entities. We evaluate our embedding approach on benchmark tasks for entity representations. We use coreference arc prediction (CAP) and named entity disambiguation on CoNLL-YAGO, two tasks in the EntEval suite (Chen et al., 2019), as well as entity linking on WikilinksNED (Eshel et al., 2017), which"
2020.findings-emnlp.54,N19-1423,0,0.691018,"nstream models achieve competitive performance with ELMoand BERT-based embeddings in trained models. We also show that it is possible to reduce the size of our type set in a learning-based way for particular domains. Finally, we show that these embeddings can be post-hoc modified through a small number of rules to incorporate domain knowledge and improve performance. 1 Introduction In typical neural NLP systems, entities are embedded in the same space as other words either in context-independent (Mikolov et al., 2013; Pennington et al., 2014) or in context-dependent ways (Peters et al., 2018; Devlin et al., 2019). Such approaches are powerful: pre-trained language models implicitly learn factual knowledge about those entities (Petroni et al., 2019; Roberts et al., 2020; Jiang et al., 2020) and these representations can be grounded in structured and human-curated knowledge bases (Logan et al., 2019; Levine et al., 2019; Peters et al., 2019; Zhang et al., 2019; Poerner et al., 2019; Xiong et al., 2020; Wang et al., 2020). However, these embeddings do not explicitly maintain representations of this knowledge, and dense entity representations are not directly interpretable. Knowledge probing tasks can be"
2020.findings-emnlp.54,Q14-1037,1,0.867677,"tors for millions of predefined entities. Yamada et al. (2016) and Eshel et al. (2017) embed words and entities in the same continuous space particularly for NED. Ling et al. (2020) learn general purpose entity embeddings from context and entity relationships in a knowledge base while F´evry et al. (2020) does not rely on that structured information about entities. Our approach only stores type embeddings which can be substantially smaller than the entity embedding matrix. Entity typing information has been used across a range of NLP tasks, including models for entity linking and coreference (Durrett and Klein, 2014). 11 In entity linking specifically, typing has been explored for cross-domain entity linking (Gupta et al., 2017; Onoe and Durrett, 2020). Past work by Raiman and Raiman (2018) has also explored learning a type system for this task. Our approach to learning types starts from a large set and filters it down, which is a simpler problem. A range of approaches have also considered augmenting pretrained models with type information (Peters et al., 2019); however, in these models, the types inform dense embeddings which are still uninterpretable. A related thrust of the literature has looked at und"
2020.findings-emnlp.54,K17-1008,0,0.499911,"imple rules to modify probabilities 0.0 cities Figure 1: Interpretable entity representations. (1) A mention and its context are fed into (2) an embedding model. (3) An entity embedding vector consists of probabilities for corresponding types. (4) We can reduce the size of the type set for a particular downstream task in a learning-based way (Section 5.2). (5) We can also incorporate domain knowledge via rules to modify bad type probabilities and improve model performance (Section 5.3). ture. Dense representations of entities have similarly been applied to entity linking (Yamada et al., 2016; Eshel et al., 2017), as well as relation extraction (Baldini Soares et al., 2019), entity typing (Ling et al., 2020), and question answering (F´evry et al., 2020). Those approaches use millions of predefined entities, while our approach uses a much smaller number of types (10k or 60k). This makes it simultaneously more compact and also more flexible when generalizing to unknown entities. We evaluate our embedding approach on benchmark tasks for entity representations. We use coreference arc prediction (CAP) and named entity disambiguation on CoNLL-YAGO, two tasks in the EntEval suite (Chen et al., 2019), as well"
2020.findings-emnlp.54,2020.emnlp-main.400,0,0.0929991,"Missing"
2020.findings-emnlp.54,D18-1016,0,0.0266053,"ext, 6 7 we assign 1 to a collection of relevant entity types such as women’s and 0 to irrelevant types such as davis cup teams (the international team competition in men’s tennis). Critically, because our representations have interpretable axes, we can more easily transform our entity representations and incorporate this kind of domain knowledge. 6 We evaluate the “out-of-the-box” quality of our entity representations and baselines on two entity probing tasks as discussed in the previous section. 6.1 Datasets Coreference Arc Prediction (CAP) We use the English CAP dataset derived from PreCo (Chen et al., 2018) by Chen et al. (2019). The creators of the dataset partition the data by cosine similarity of GloVe (Pennington et al., 2014) embeddings of mention spans and balance the number of positive and negative examples in each bucket, so that models do not solve the task by capturing surface features of entity mention spans. The original data split provides 8k examples for each of the training, development, and test sets. Named Entity Disambiguation (NED) We use the standard English CoNLL-YAGO benchmark (Hoffart et al., 2011) preprocessed by Chen et al. (2019). For each entity mention, at most 30 can"
2020.findings-emnlp.54,K19-1049,0,0.0146645,"derperforms our EntEmbeddings by 10 points. 7.2 Named Entity Disambiguation (NED) We use the entity typing model trained on the WikiContext data (see Section 4) to get the mention and context representation t. In the CoNLL-YAGO setting, similar to past work (Onoe and Durrett, 2019; F´evry et al., 2020), we prepend the document title and the first sentence to the input to enrich the context information. To obtain the candidate representations {c1 , c2 , ..., cj , ...}, we use the model trained on the Wiki-Description data, which is specialized for entity descriptions (see Section 4) similar to Gillick et al. (2019). We choose Wikipedia datasets here because UFET does not support entity descriptions. We rank the candidate entities based on cosine similarity between t and cj , and the entity with the highest score is our model’s prediction. The M OST F REQUENT baseline chooses the most frequently observed entity for a given mention as a prediction, based on a prior probability pprior computed from link counts on Wikipedia. All baselines except M OST F REQUENT combine the classifier output and the  prior probability to make  a prediction: arg max pprior (c) + pclassifier (c) .10 9 Code and datasets used"
2020.findings-emnlp.54,D19-1040,0,0.165961,"et al., 2019; Roberts et al., 2020; Jiang et al., 2020) and these representations can be grounded in structured and human-curated knowledge bases (Logan et al., 2019; Levine et al., 2019; Peters et al., 2019; Zhang et al., 2019; Poerner et al., 2019; Xiong et al., 2020; Wang et al., 2020). However, these embeddings do not explicitly maintain representations of this knowledge, and dense entity representations are not directly interpretable. Knowledge probing tasks can be used to measure LMs’ factual knowledge (Petroni et al., 2019), but designing the right probing task is another hard problem (Chen et al., 2019; Poerner et al., 2019), particularly if the probes are parameter-rich (Hewitt and Manning, 2019). In this work, we explore a set of interpretable entity representations that are simultaneously human and machine readable. The key idea of this approach is to use fine-grained entity typing models with large type inventories (Ling and Weld, 2012; Gillick et al., 2014; Choi et al., 2018; Onoe and Durrett, 2020). Given an entity mention and context words, our typing model outputs a highdimensional vector whose values are associated with predefined fine-grained entity types. Each value ranges betwee"
2020.findings-emnlp.54,D17-1284,0,0.122797,"same continuous space particularly for NED. Ling et al. (2020) learn general purpose entity embeddings from context and entity relationships in a knowledge base while F´evry et al. (2020) does not rely on that structured information about entities. Our approach only stores type embeddings which can be substantially smaller than the entity embedding matrix. Entity typing information has been used across a range of NLP tasks, including models for entity linking and coreference (Durrett and Klein, 2014). 11 In entity linking specifically, typing has been explored for cross-domain entity linking (Gupta et al., 2017; Onoe and Durrett, 2020). Past work by Raiman and Raiman (2018) has also explored learning a type system for this task. Our approach to learning types starts from a large set and filters it down, which is a simpler problem. A range of approaches have also considered augmenting pretrained models with type information (Peters et al., 2019); however, in these models, the types inform dense embeddings which are still uninterpretable. A related thrust of the literature has looked at understanding entities using interpretable embeddings based around feature norms (McRae et al., 2005); this has adva"
2020.findings-emnlp.54,P18-1009,0,0.228937,"they require end-task fine-tuning and are fundamentally difficult to interpret. In this paper, we present an approach to creating entity representations that are human readable and achieve high performance on entity-related tasks out of the box. Our representations are vectors whose values correspond to posterior probabilities over finegrained entity types, indicating the confidence of a typing model’s decision that the entity belongs to the corresponding type. We obtain these representations using a fine-grained entity typing model, trained either on supervised ultra-fine entity typing data (Choi et al., 2018) or distantly-supervised examples from Wikipedia. On entity probing tasks involving recognizing entity identity, our embeddings used in parameter-free downstream models achieve competitive performance with ELMoand BERT-based embeddings in trained models. We also show that it is possible to reduce the size of our type set in a learning-based way for particular domains. Finally, we show that these embeddings can be post-hoc modified through a small number of rules to incorporate domain knowledge and improve performance. 1 Introduction In typical neural NLP systems, entities are embedded in the s"
2020.findings-emnlp.54,D19-1275,0,0.0349081,"s advantages for learning in few-shot setups (Wang et al., 2017). However, most of this past work has used embeddings that are much lowerdimensional than ours, and don’t necessarily to scale to broad-domain text or all of Wikipedia. Another line of past work tests if type information or other knowledge is captured by pre-trained LMs. Peters et al. (2018) report that ELMo performs well on word sense disambiguation and POS tagging. Some other work also investigates models’ ability to induce syntactic information by measuring accuracy of a probe (Zhang and Bowman, 2018; Hewitt and Manning, 2019; Hewitt and Liang, 2019). However, there is significant uncertainty about how to calibrate such probing results (Voita and Titov, 2020); our model’s representations are more directly interpretable and don’t require posthoc probing. Lastly, our work is distinct from SPINE (Subramanian et al., 2017), a past technique for learning sparse interpretable embeddings. However, this technique requires an additional step to reveal their interpretability. Each dimension of our entity representations has a name (i.e., a fine-grained type) with a probability, and thus it is immediately interpretable. The SOTA performance on the o"
2020.findings-emnlp.54,N19-1419,0,0.0819724,"grounded in structured and human-curated knowledge bases (Logan et al., 2019; Levine et al., 2019; Peters et al., 2019; Zhang et al., 2019; Poerner et al., 2019; Xiong et al., 2020; Wang et al., 2020). However, these embeddings do not explicitly maintain representations of this knowledge, and dense entity representations are not directly interpretable. Knowledge probing tasks can be used to measure LMs’ factual knowledge (Petroni et al., 2019), but designing the right probing task is another hard problem (Chen et al., 2019; Poerner et al., 2019), particularly if the probes are parameter-rich (Hewitt and Manning, 2019). In this work, we explore a set of interpretable entity representations that are simultaneously human and machine readable. The key idea of this approach is to use fine-grained entity typing models with large type inventories (Ling and Weld, 2012; Gillick et al., 2014; Choi et al., 2018; Onoe and Durrett, 2020). Given an entity mention and context words, our typing model outputs a highdimensional vector whose values are associated with predefined fine-grained entity types. Each value ranges between 0 and 1, corresponding to the confidence of the model’s decision that the entity has the proper"
2020.findings-emnlp.54,2020.emnlp-main.21,1,0.833885,"Embeddings This output layer is a single linear layer whose parameter matrix can be viewed as a matrix of type embeddings E ∈ R|T |×d , where d is the dimension of the mention and context representation h[CLS] . We obtain the output probabilities t by multiplying E by h[CLS] , followed by an element-wise sigmoid function: t = σ (E · h[CLS] ).3 Similar to previous work (Choi et al., 2018; Onoe and Durrett, 2019), we assume independence between all entity type in T . One assumption in our approach is that the model’s output probabilities are a meaningful measure of class membership. Past work (Desai and Durrett, 2020) has observed that this is true for other models involving BERT variants. Training Following Choi et al. (2018), the loss is a sum of binary cross-entropy losses over all entity types T over all training examples D. That is, we treat each type prediction for each example as an independent binary decision, with shared parameters in the BERT encoder. 4 Training Data To train our entity typing model, we need labeled examples consisting of (m, s, t∗ ) triples. Although there are labeled typing datasets such as UFET 2 We use BERT-large uncased (whole word masking) in our experiments. We experimente"
2020.findings-emnlp.54,2020.tacl-1.28,0,0.139558,"g-based way for particular domains. Finally, we show that these embeddings can be post-hoc modified through a small number of rules to incorporate domain knowledge and improve performance. 1 Introduction In typical neural NLP systems, entities are embedded in the same space as other words either in context-independent (Mikolov et al., 2013; Pennington et al., 2014) or in context-dependent ways (Peters et al., 2018; Devlin et al., 2019). Such approaches are powerful: pre-trained language models implicitly learn factual knowledge about those entities (Petroni et al., 2019; Roberts et al., 2020; Jiang et al., 2020) and these representations can be grounded in structured and human-curated knowledge bases (Logan et al., 2019; Levine et al., 2019; Peters et al., 2019; Zhang et al., 2019; Poerner et al., 2019; Xiong et al., 2020; Wang et al., 2020). However, these embeddings do not explicitly maintain representations of this knowledge, and dense entity representations are not directly interpretable. Knowledge probing tasks can be used to measure LMs’ factual knowledge (Petroni et al., 2019), but designing the right probing task is another hard problem (Chen et al., 2019; Poerner et al., 2019), particularly"
2020.findings-emnlp.54,2021.ccl-1.108,0,0.135227,"Missing"
2020.findings-emnlp.54,P19-1598,0,0.0203662,"mall number of rules to incorporate domain knowledge and improve performance. 1 Introduction In typical neural NLP systems, entities are embedded in the same space as other words either in context-independent (Mikolov et al., 2013; Pennington et al., 2014) or in context-dependent ways (Peters et al., 2018; Devlin et al., 2019). Such approaches are powerful: pre-trained language models implicitly learn factual knowledge about those entities (Petroni et al., 2019; Roberts et al., 2020; Jiang et al., 2020) and these representations can be grounded in structured and human-curated knowledge bases (Logan et al., 2019; Levine et al., 2019; Peters et al., 2019; Zhang et al., 2019; Poerner et al., 2019; Xiong et al., 2020; Wang et al., 2020). However, these embeddings do not explicitly maintain representations of this knowledge, and dense entity representations are not directly interpretable. Knowledge probing tasks can be used to measure LMs’ factual knowledge (Petroni et al., 2019), but designing the right probing task is another hard problem (Chen et al., 2019; Poerner et al., 2019), particularly if the probes are parameter-rich (Hewitt and Manning, 2019). In this work, we explore a set of interpretable e"
2020.findings-emnlp.54,N19-1250,1,0.667471,"that in the financial domain Wall Street does not refer to a location. We show that simple rules based on prior knowledge can improve performance further (discussed in Section 5.3); critically, this is done without having to annotate data in the target domain, giving system designers another technique for adapting these models. 3 Embedding Model Our model fθ to produce these embeddings is shown in Figure 2: it takes as input the mention m and its context s and predicts probabilities for predefined entity types T . This is a Transformer-based typing model following the BERT model presented in Onoe and Durrett (2019). First, a Transformerbased encoder (Vaswani et al., 2017) maps the input variables, m and s, to an intermediate vector repreType Embeddings This output layer is a single linear layer whose parameter matrix can be viewed as a matrix of type embeddings E ∈ R|T |×d , where d is the dimension of the mention and context representation h[CLS] . We obtain the output probabilities t by multiplying E by h[CLS] , followed by an element-wise sigmoid function: t = σ (E · h[CLS] ).3 Similar to previous work (Choi et al., 2018; Onoe and Durrett, 2019), we assume independence between all entity type in T ."
2020.findings-emnlp.54,P09-1113,0,0.131261,"ties corresponding to fine-grained entity types T . These entity types are predefined and static, so their meanings are identical for all entities. Our goal here is to learn parameters θ of a function fθ that maps the mention m and its context s to a vector t, which capture salient features of the entity mention with the context. We learn the parameters θ in a supervised manner. We use a labeled dataset D = ∗ (1) {(m, s, t ) , ..., (m, s, t∗ )(k) } to train an entity typing model. The gold labels t∗ are obtained by manual annotation or distant-supervision techniques (Craven and Kumlien, 1999; Mintz et al., 2009). We select a predefined types T from modified Wikipedia categories, or we use an existing type set such as UFET (Choi et al., 2018) (discussed in Section 4). We use the output vectors t as general purpose 613 Embedding Model Entity Embedding Sig. sentation. A type embedding layer then projects the intermediate representation to a vector whose dimensions correspond to the entity types T . Finally, we apply a sigmoid function on each real-valued score in the vector to obtain the posterior probabilities that form our entity representation t (top of the figure). Element-wise Sigmoid Dot Dot Produ"
2020.findings-emnlp.54,D14-1162,0,0.0908918,"Missing"
2020.findings-emnlp.54,N18-1202,0,0.729575,"in parameter-free downstream models achieve competitive performance with ELMoand BERT-based embeddings in trained models. We also show that it is possible to reduce the size of our type set in a learning-based way for particular domains. Finally, we show that these embeddings can be post-hoc modified through a small number of rules to incorporate domain knowledge and improve performance. 1 Introduction In typical neural NLP systems, entities are embedded in the same space as other words either in context-independent (Mikolov et al., 2013; Pennington et al., 2014) or in context-dependent ways (Peters et al., 2018; Devlin et al., 2019). Such approaches are powerful: pre-trained language models implicitly learn factual knowledge about those entities (Petroni et al., 2019; Roberts et al., 2020; Jiang et al., 2020) and these representations can be grounded in structured and human-curated knowledge bases (Logan et al., 2019; Levine et al., 2019; Peters et al., 2019; Zhang et al., 2019; Poerner et al., 2019; Xiong et al., 2020; Wang et al., 2020). However, these embeddings do not explicitly maintain representations of this knowledge, and dense entity representations are not directly interpretable. Knowledge"
2020.findings-emnlp.54,D19-1005,0,0.114622,"n knowledge and improve performance. 1 Introduction In typical neural NLP systems, entities are embedded in the same space as other words either in context-independent (Mikolov et al., 2013; Pennington et al., 2014) or in context-dependent ways (Peters et al., 2018; Devlin et al., 2019). Such approaches are powerful: pre-trained language models implicitly learn factual knowledge about those entities (Petroni et al., 2019; Roberts et al., 2020; Jiang et al., 2020) and these representations can be grounded in structured and human-curated knowledge bases (Logan et al., 2019; Levine et al., 2019; Peters et al., 2019; Zhang et al., 2019; Poerner et al., 2019; Xiong et al., 2020; Wang et al., 2020). However, these embeddings do not explicitly maintain representations of this knowledge, and dense entity representations are not directly interpretable. Knowledge probing tasks can be used to measure LMs’ factual knowledge (Petroni et al., 2019), but designing the right probing task is another hard problem (Chen et al., 2019; Poerner et al., 2019), particularly if the probes are parameter-rich (Hewitt and Manning, 2019). In this work, we explore a set of interpretable entity representations that are simultaneou"
2020.findings-emnlp.54,D19-1250,0,0.105003,"Missing"
2020.findings-emnlp.54,2020.emnlp-main.437,0,0.0123774,"type set in a learning-based way for particular domains. Finally, we show that these embeddings can be post-hoc modified through a small number of rules to incorporate domain knowledge and improve performance. 1 Introduction In typical neural NLP systems, entities are embedded in the same space as other words either in context-independent (Mikolov et al., 2013; Pennington et al., 2014) or in context-dependent ways (Peters et al., 2018; Devlin et al., 2019). Such approaches are powerful: pre-trained language models implicitly learn factual knowledge about those entities (Petroni et al., 2019; Roberts et al., 2020; Jiang et al., 2020) and these representations can be grounded in structured and human-curated knowledge bases (Logan et al., 2019; Levine et al., 2019; Peters et al., 2019; Zhang et al., 2019; Poerner et al., 2019; Xiong et al., 2020; Wang et al., 2020). However, these embeddings do not explicitly maintain representations of this knowledge, and dense entity representations are not directly interpretable. Knowledge probing tasks can be used to measure LMs’ factual knowledge (Petroni et al., 2019), but designing the right probing task is another hard problem (Chen et al., 2019; Poerner et al.,"
2020.findings-emnlp.54,spitkovsky-chang-2012-cross,0,0.0276144,"the data by cosine similarity of GloVe (Pennington et al., 2014) embeddings of mention spans and balance the number of positive and negative examples in each bucket, so that models do not solve the task by capturing surface features of entity mention spans. The original data split provides 8k examples for each of the training, development, and test sets. Named Entity Disambiguation (NED) We use the standard English CoNLL-YAGO benchmark (Hoffart et al., 2011) preprocessed by Chen et al. (2019). For each entity mention, at most 30 candidate entities are selected using the CrossWikis dictionary (Spitkovsky and Chang, 2012). This dataset contains 18.5k training, 4.8k dev, and 4.5k test examples from newswire text, so the variety of entities and the writing styles are limited. For this reason, we create another NED dataset from WikilinksNED (Eshel et al., 2017), which includes a wide range of entities and diverse writing styles from scraped English web text linking to Wikipedia. We limit the number of candidate entities to 3 for each instance, which still makes a challenging benchmark. We create 5k training, 1k dev, and 1k test examples and call this dataset WLNED. In both CoNLL-YAGO and WLNED, we form descriptio"
2020.findings-emnlp.54,W18-5448,0,0.0197147,"around feature norms (McRae et al., 2005); this has advantages for learning in few-shot setups (Wang et al., 2017). However, most of this past work has used embeddings that are much lowerdimensional than ours, and don’t necessarily to scale to broad-domain text or all of Wikipedia. Another line of past work tests if type information or other knowledge is captured by pre-trained LMs. Peters et al. (2018) report that ELMo performs well on word sense disambiguation and POS tagging. Some other work also investigates models’ ability to induce syntactic information by measuring accuracy of a probe (Zhang and Bowman, 2018; Hewitt and Manning, 2019; Hewitt and Liang, 2019). However, there is significant uncertainty about how to calibrate such probing results (Voita and Titov, 2020); our model’s representations are more directly interpretable and don’t require posthoc probing. Lastly, our work is distinct from SPINE (Subramanian et al., 2017), a past technique for learning sparse interpretable embeddings. However, this technique requires an additional step to reveal their interpretability. Each dimension of our entity representations has a name (i.e., a fine-grained type) with a probability, and thus it is immed"
2020.findings-emnlp.54,P19-1139,0,0.029397,"ve performance. 1 Introduction In typical neural NLP systems, entities are embedded in the same space as other words either in context-independent (Mikolov et al., 2013; Pennington et al., 2014) or in context-dependent ways (Peters et al., 2018; Devlin et al., 2019). Such approaches are powerful: pre-trained language models implicitly learn factual knowledge about those entities (Petroni et al., 2019; Roberts et al., 2020; Jiang et al., 2020) and these representations can be grounded in structured and human-curated knowledge bases (Logan et al., 2019; Levine et al., 2019; Peters et al., 2019; Zhang et al., 2019; Poerner et al., 2019; Xiong et al., 2020; Wang et al., 2020). However, these embeddings do not explicitly maintain representations of this knowledge, and dense entity representations are not directly interpretable. Knowledge probing tasks can be used to measure LMs’ factual knowledge (Petroni et al., 2019), but designing the right probing task is another hard problem (Chen et al., 2019; Poerner et al., 2019), particularly if the probes are parameter-rich (Hewitt and Manning, 2019). In this work, we explore a set of interpretable entity representations that are simultaneously human and machin"
2020.findings-emnlp.54,2020.emnlp-main.14,0,0.0120015,"dings that are much lowerdimensional than ours, and don’t necessarily to scale to broad-domain text or all of Wikipedia. Another line of past work tests if type information or other knowledge is captured by pre-trained LMs. Peters et al. (2018) report that ELMo performs well on word sense disambiguation and POS tagging. Some other work also investigates models’ ability to induce syntactic information by measuring accuracy of a probe (Zhang and Bowman, 2018; Hewitt and Manning, 2019; Hewitt and Liang, 2019). However, there is significant uncertainty about how to calibrate such probing results (Voita and Titov, 2020); our model’s representations are more directly interpretable and don’t require posthoc probing. Lastly, our work is distinct from SPINE (Subramanian et al., 2017), a past technique for learning sparse interpretable embeddings. However, this technique requires an additional step to reveal their interpretability. Each dimension of our entity representations has a name (i.e., a fine-grained type) with a probability, and thus it is immediately interpretable. The SOTA performance on the original split is around 40 F1. 620 9 Conclusion In this work, we presented an approach to creating interpretabl"
2020.findings-emnlp.54,I17-1021,0,0.027134,"Raiman and Raiman (2018) has also explored learning a type system for this task. Our approach to learning types starts from a large set and filters it down, which is a simpler problem. A range of approaches have also considered augmenting pretrained models with type information (Peters et al., 2019); however, in these models, the types inform dense embeddings which are still uninterpretable. A related thrust of the literature has looked at understanding entities using interpretable embeddings based around feature norms (McRae et al., 2005); this has advantages for learning in few-shot setups (Wang et al., 2017). However, most of this past work has used embeddings that are much lowerdimensional than ours, and don’t necessarily to scale to broad-domain text or all of Wikipedia. Another line of past work tests if type information or other knowledge is captured by pre-trained LMs. Peters et al. (2018) report that ELMo performs well on word sense disambiguation and POS tagging. Some other work also investigates models’ ability to induce syntactic information by measuring accuracy of a probe (Zhang and Bowman, 2018; Hewitt and Manning, 2019; Hewitt and Liang, 2019). However, there is significant uncertain"
2020.findings-emnlp.54,K16-1025,0,0.15602,"cities Mention Use simple rules to modify probabilities 0.0 cities Figure 1: Interpretable entity representations. (1) A mention and its context are fed into (2) an embedding model. (3) An entity embedding vector consists of probabilities for corresponding types. (4) We can reduce the size of the type set for a particular downstream task in a learning-based way (Section 5.2). (5) We can also incorporate domain knowledge via rules to modify bad type probabilities and improve model performance (Section 5.3). ture. Dense representations of entities have similarly been applied to entity linking (Yamada et al., 2016; Eshel et al., 2017), as well as relation extraction (Baldini Soares et al., 2019), entity typing (Ling et al., 2020), and question answering (F´evry et al., 2020). Those approaches use millions of predefined entities, while our approach uses a much smaller number of types (10k or 60k). This makes it simultaneously more compact and also more flexible when generalizing to unknown entities. We evaluate our embedding approach on benchmark tasks for entity representations. We use coreference arc prediction (CAP) and named entity disambiguation on CoNLL-YAGO, two tasks in the EntEval suite (Chen e"
2020.findings-emnlp.54,D11-1072,0,\N,Missing
2020.tacl-1.44,P17-1097,0,0.0236501,"language description and regex pair, multiple syntactically different sketches can yield semantically equivalent regexes. We can therefore maximize the marginal likelihood of generating a sketch that leads us to the semantically correct regex, instead of a generating a particular gold sketch. Namely, we learn the parameters by maximizing: X X log arg max 1[synth(S ) = r ∗ ]pθ (S |L) θ (r ∗ ,L) S where r ∗ is the ground truth regex and synth denotes running the synthesizer. Computing the sum over all sketches is intractable, so we sample sketches from beam search to approximate the gradients (Guu et al., 2017). Enumeration Order Our synthesizer maintains a worklist of partial programs to complete, and enumerates complete programs in increasing order of depth. Specifically, at each step, we pop the 4 Program Synthesizer In this section, we describe the program synthesizer, which takes as input a sketch and a set of 683 Dataset size #. unique words Avg. NL length Avg. regex size Avg. regex depth KB13 824 207 8 5 3 TURK 10, 000 557 12 5 2 SO 62 301 25 13 4 Table 1: Statistics of our datasets. Compared with KB13 and TURK, STACKOVERFLOW contains more sophisticated descriptions and regexes. Figure 4: Exa"
2020.tacl-1.44,D15-1198,0,0.0175134,"coder models to generated logical forms or programs represented by sequences (Dong and Lapata, 2016), and ASTs (Rabinovich et al., 2017; Yin and Neubig, 2017; Iyer et al., 2019; Shin et al., 2019). However, some of the most challenging code settings such as the Hearthstone dataset (Ling et al., 2016) only evaluates the produced strings by exact match accuracy or BLEU score, rather than executing the programs on real data as we do. There is also recent work using neural models to generate logical forms utilizing a coarse-tofine approach (Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2013; Artzi et al., 2015; Dong and Lapata, 2018; Wang et al., 2019), which first generates an abstract logical form and then concretizes it using neural modules, whereas we complete the sketch via a synthesizer. STACKOVERFLOW We show some solved and unsolved examples using GRAMMARSKETCH from the STACKOVERFLOW dataset. Our approach can successfully deal with multiple-sentence inputs like pairs (e) and (f). They both contain multiple sentences with each one describing certain a component or constraint, which seems to be a common pattern of describing real world regexes. Our approach is effective for this structure beca"
2020.tacl-1.44,D19-1545,0,0.0116801,"a regex. Polosukhin and Skidanov, 2018; Zhong et al., 2020), they only use the examples to verify the generated programs, whereas our approach heavily engages examples when searching for the instantiation of sketches to make the synthesizer more efficient. Another line of work has focused on exploring which deep learning techniques are most effective for directly predicting programs from natural language. Recent work has built encoder-decoder models to generated logical forms or programs represented by sequences (Dong and Lapata, 2016), and ASTs (Rabinovich et al., 2017; Yin and Neubig, 2017; Iyer et al., 2019; Shin et al., 2019). However, some of the most challenging code settings such as the Hearthstone dataset (Ling et al., 2016) only evaluates the produced strings by exact match accuracy or BLEU score, rather than executing the programs on real data as we do. There is also recent work using neural models to generate logical forms utilizing a coarse-tofine approach (Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2013; Artzi et al., 2015; Dong and Lapata, 2018; Wang et al., 2019), which first generates an abstract logical form and then concretizes it using neural modules, whereas we complete"
2020.tacl-1.44,D13-1160,0,0.717246,"etch and examples. Each leaf node in the search tree is a concrete regex; we return the first one consistent with all the examples. produce a regex consistent with both the sketch and the provided examples. Critically, this twostage approach modularizes the language interpretation and program synthesis, allowing us to freely swap out these components. We evaluate our framework on several English datasets. Because these datasets vary in scale, we consider two sketch generation approaches: neural network-based with a seq-to-seq model (Luong et al., 2015) or grammar-based with a semantic parser (Berant et al., 2013). We use two large-scale datasets from past work, the KB13 dataset of Kushman and Barzilay (2013) and the TURK dataset of Locascio et al. (2016), augmented with automatically produced positive/negative examples for each regex. Our neural sketch model can exploit these large labeled datasets, allowing our sketch-driven approach to outperform existing seq-to-seq methods, even when those methods are modified to take advantage of examples. To test our model in a more realistic setting, we also evaluate on a dataset of real-world regex synthesis problems from Stack Overflow. These problems organica"
2020.tacl-1.44,P17-1089,0,0.0140471,"y from the natural language description whereas traditional sketch-based synthesis (Solar-Lezama, 2008) relies on a user-provided sketch, and our sketches are hierarchical and constrained compared to other neural sketch-based approaches (Nye et al., 2019). 7 Related Work Other NL and Program Synthesis There has been recent interest in synthesizing programs from natural language. One line of work uses either grammar-based or neural semantic parsing to synthesize programs. Particularly, several techniques have been proposed to translate natural language to SQL queries (Yaghmazadeh et al., 2017; Iyer et al., 2017; Suhr et al., 2018), ‘‘ifthis-then-that’’ recipes (Quirk et al., 2015), bash commands (Lin et al., 2018), Java expressions (Gvero and Kuncak, 2015) and more. Our work is different from prior work in that it utilizes input-output examples in addition to natural language. Although several past approaches use both natural language and examples (Kulal et al., 2019; 8 Conclusion We have proposed a sketch-driven regular expression synthesis framework that utilizes both natural 690 language and examples, and we have instantiated this framework with both a neural and a grammarbased parser. Experiment"
2020.tacl-1.44,P16-1004,0,0.0609985,"only because the formulaic description is easy enough to translate directly into a regex. Polosukhin and Skidanov, 2018; Zhong et al., 2020), they only use the examples to verify the generated programs, whereas our approach heavily engages examples when searching for the instantiation of sketches to make the synthesizer more efficient. Another line of work has focused on exploring which deep learning techniques are most effective for directly predicting programs from natural language. Recent work has built encoder-decoder models to generated logical forms or programs represented by sequences (Dong and Lapata, 2016), and ASTs (Rabinovich et al., 2017; Yin and Neubig, 2017; Iyer et al., 2019; Shin et al., 2019). However, some of the most challenging code settings such as the Hearthstone dataset (Ling et al., 2016) only evaluates the produced strings by exact match accuracy or BLEU score, rather than executing the programs on real data as we do. There is also recent work using neural models to generate logical forms utilizing a coarse-tofine approach (Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2013; Artzi et al., 2015; Dong and Lapata, 2018; Wang et al., 2019), which first generates an abstract log"
2020.tacl-1.44,D16-1197,0,0.195753,"atural Language and Examples Xi Ye♦ Qiaochu Chen♦ Xinyu Wang♠ Isil Dillig♦ Greg Durrett♦ ♦ ♠ Department of Computer Science, The University of Texas at Austin Computer Science and Engineering Department, University of Michigan, Ann Arbor {xiye,qchen,isil,gdurrett}@cs.utexas.edu xwangsd@umich.edu Abstract posts on Stack Overflow, with over 200,000 posts. Recent research has attempted to build semantic parsers that can translate natural language descriptions into regexes, via rule-based techniques (Ranta, 1998), semantic parsing (Kushman and Barzilay, 2013), or seq-to-seq neural network models (Locascio et al., 2016; Zhong et al., 2018a; Park et al., 2019). Although this prior work has achieved relatively high accuracy on benchmark datasets, trained models still do not generalize to real-world applications: These benchmarks describe simple regexes with short natural language descriptions and limited vocabulary. Real-world regexes are more complex in terms of length and tree-depth, requiring natural language descriptions that are longer and more complicated (Zhong et al., 2018b). Moreover, these descriptions may be under-specified or ambiguous. One way to supplement such descriptions is by including posit"
2020.tacl-1.44,D15-1166,0,0.0929863,"Missing"
2020.tacl-1.44,N13-1103,0,0.536226,"Missing"
2020.tacl-1.44,D13-1161,0,0.0271954,"work has built encoder-decoder models to generated logical forms or programs represented by sequences (Dong and Lapata, 2016), and ASTs (Rabinovich et al., 2017; Yin and Neubig, 2017; Iyer et al., 2019; Shin et al., 2019). However, some of the most challenging code settings such as the Hearthstone dataset (Ling et al., 2016) only evaluates the produced strings by exact match accuracy or BLEU score, rather than executing the programs on real data as we do. There is also recent work using neural models to generate logical forms utilizing a coarse-tofine approach (Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2013; Artzi et al., 2015; Dong and Lapata, 2018; Wang et al., 2019), which first generates an abstract logical form and then concretizes it using neural modules, whereas we complete the sketch via a synthesizer. STACKOVERFLOW We show some solved and unsolved examples using GRAMMARSKETCH from the STACKOVERFLOW dataset. Our approach can successfully deal with multiple-sentence inputs like pairs (e) and (f). They both contain multiple sentences with each one describing certain a component or constraint, which seems to be a common pattern of describing real world regexes. Our approach is effective for"
2020.tacl-1.44,D19-1677,0,0.022854,"Missing"
2020.tacl-1.44,D19-1391,0,0.0139698,"programs represented by sequences (Dong and Lapata, 2016), and ASTs (Rabinovich et al., 2017; Yin and Neubig, 2017; Iyer et al., 2019; Shin et al., 2019). However, some of the most challenging code settings such as the Hearthstone dataset (Ling et al., 2016) only evaluates the produced strings by exact match accuracy or BLEU score, rather than executing the programs on real data as we do. There is also recent work using neural models to generate logical forms utilizing a coarse-tofine approach (Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2013; Artzi et al., 2015; Dong and Lapata, 2018; Wang et al., 2019), which first generates an abstract logical form and then concretizes it using neural modules, whereas we complete the sketch via a synthesizer. STACKOVERFLOW We show some solved and unsolved examples using GRAMMARSKETCH from the STACKOVERFLOW dataset. Our approach can successfully deal with multiple-sentence inputs like pairs (e) and (f). They both contain multiple sentences with each one describing certain a component or constraint, which seems to be a common pattern of describing real world regexes. Our approach is effective for this structure because the parser can extract fragments from e"
2020.tacl-1.44,P15-1085,0,0.0264717,"sed synthesis (Solar-Lezama, 2008) relies on a user-provided sketch, and our sketches are hierarchical and constrained compared to other neural sketch-based approaches (Nye et al., 2019). 7 Related Work Other NL and Program Synthesis There has been recent interest in synthesizing programs from natural language. One line of work uses either grammar-based or neural semantic parsing to synthesize programs. Particularly, several techniques have been proposed to translate natural language to SQL queries (Yaghmazadeh et al., 2017; Iyer et al., 2017; Suhr et al., 2018), ‘‘ifthis-then-that’’ recipes (Quirk et al., 2015), bash commands (Lin et al., 2018), Java expressions (Gvero and Kuncak, 2015) and more. Our work is different from prior work in that it utilizes input-output examples in addition to natural language. Although several past approaches use both natural language and examples (Kulal et al., 2019; 8 Conclusion We have proposed a sketch-driven regular expression synthesis framework that utilizes both natural 690 language and examples, and we have instantiated this framework with both a neural and a grammarbased parser. Experimental results reveal the artificialness of existing public datasets and de"
2020.tacl-1.44,W98-1308,0,0.411456,"Missing"
2020.tacl-1.44,2020.acl-main.208,0,0.0218633,"ruck) matches any string but truck). The semantics are hard to pin down with examples, but the correct regex is also artificial and unlikely to appear in real word applications. Our DEEPSKETCH fails on this pair because the synthesizer fails to catch the at least 7 times constraint when strings that have fewer than 7 characters can also be accepted (since without truck can match the empty string). DEEPREGEX is able to produce the ground-truth regex in this case, but this is only because the formulaic description is easy enough to translate directly into a regex. Polosukhin and Skidanov, 2018; Zhong et al., 2020), they only use the examples to verify the generated programs, whereas our approach heavily engages examples when searching for the instantiation of sketches to make the synthesizer more efficient. Another line of work has focused on exploring which deep learning techniques are most effective for directly predicting programs from natural language. Recent work has built encoder-decoder models to generated logical forms or programs represented by sequences (Dong and Lapata, 2016), and ASTs (Rabinovich et al., 2017; Yin and Neubig, 2017; Iyer et al., 2019; Shin et al., 2019). However, some of the"
2021.acl-long.160,D19-1040,0,0.0904865,"their logits depending on how long they are trained, we post-hoc calibrate each of our models using temperature scaling (Guo et al., 2017) and a shift parameter. We report the total error (e.g., the sum of the errors between the mean confidence and the empirical accuracy) on the UFET dev set and the OntoNotes dev set. Entity Representations We are interested in the usefulness of the trained entity typing models in a downstream task. Following Onoe and Durrett (2020b), we evaluate entity representation given by the box-based and vector-based models on the Coreference Arc Prediction (CAP) task (Chen et al., 2019) derived from PreCo (Chen et al., 2018). This task is a binary classification problem, requiring to judge if two mention spans (either in one sentence or two sentences) are the same entity or not. As in Onoe and Durrett (2020b), we obtain type predictions (a vector of probabilities associated with types) for each span and use it as an entity representation. The final prediction of coreference for a pair of mentions is given by the cosine similarity between the entity type probability vectors with a threshold 0.5. The original data split provides 8k examples for each of the training, dev, and t"
2021.acl-long.160,2020.acl-main.749,0,0.680801,"Missing"
2021.acl-long.160,P18-1009,0,0.107586,"ve typing performance, our box-based model shows better performance in prediction consistency (predicting a supertype and a subtype together) and confidence (i.e., calibration), demonstrating that the box-based model captures the latent type hierarchies better than the vector-based model does.1 1 Introduction The development of named entity recognition and entity typing has been characterized by a growth in the size and complexity of type sets: from 4 (Tjong Kim Sang and De Meulder, 2003) to 17 (Hovy et al., 2006) to hundreds (Weischedel and Brunstein, 2005; Ling and Weld, 2012) or thousands (Choi et al., 2018). These types follow some kind 1 The code is available at https://github.com/ yasumasaonoe/Box4Types. of hierarchical structure (Weischedel and Brunstein, 2005; Ling and Weld, 2012; Gillick et al., 2014; Murty et al., 2018), so effective models for these tasks frequently engage with this hierarchy explicitly. Prior systems incorporate this structure via hierarchical losses (Murty et al., 2018; Xu and Barbosa, 2018; Chen et al., 2020) or by embedding types into a high-dimensional Euclidean or hyperbolic space (Yogatama et al., 2015; L´opez and Strube, 2020). However, the former approach require"
2021.acl-long.160,Q14-1037,1,0.790002,"types as boxes. Entity typing Entity typing and named entity recognition (Tjong Kim Sang and De Meulder, 2003) are old problems in NLP. Recent work has focused chiefly on predicted fine-grained entity types (Ling and Weld, 2012; Gillick et al., 2014; Choi et al., 2018), as these convey significantly more information for downstream tasks. As a result, there is a challenge of scaling to large type inventories, which has inspired work on type embeddings (Ren et al., 2016a,b). Entity typing information has been used across a range of NLP tasks, including models for entity linking and coreference (Durrett and Klein, 2014). Typing has been shown to be useful for crossdomain entity linking specifically (Gupta et al., 2017; Onoe and Durrett, 2020a). It has also recently been applied to coreference resolution (Onoe and Durrett, 2020b; Khosla and Rose, 2020) and text generation (Dong et al., 2020), suggesting that it can be a useful intermediate layer even in pretrained neural models. 7 Conclusion In this paper, we investigated a box-based model for fine-grained entity typing. By representing entity types in a box embedding space and projecting entity mentions into the same space, we can naturally capture the hiera"
2021.acl-long.160,W09-1109,0,0.0535765,"mates the effective region within the actor box. Now the edges of actor are contained in the edges of person in the most of dimensions, indicating that the person box almost contains this “effective” actor box. 6 Related Work Embeddings Embedding concepts/words into a high-dimensional vector space (Hinton, 1986) has a long history and has been an essential part of neural networks for language (Bengio et al., 2003; Collobert et al., 2011). There is similarly a long history of rethinking the semantics of these embedding spaces, such as treating words as regions using sparse count-based vectors (Erk, 2009a,b) or dense distributed vectors (Vilnis and McCallum, 2015). Order embeddings (Vendrov et al., 2016) or their probabilistic version (POE) (Lai and Hockenmaier, 2017) are one technique suited for hierarchical modeling. However, OE can only handle binary entailment decisions, and POE cannot model negative correlations between types, a critical limitation in its use as a probabilistic model; these shortcomings directly led to the development of box embeddings. Hyperbolic embeddings (Nickel and Kiela, 2058 (a) (b) Figure 3: Edges of (a) the person box vs the actor box and (b) the person box vs t"
2021.acl-long.160,W09-3711,0,0.0522225,"mates the effective region within the actor box. Now the edges of actor are contained in the edges of person in the most of dimensions, indicating that the person box almost contains this “effective” actor box. 6 Related Work Embeddings Embedding concepts/words into a high-dimensional vector space (Hinton, 1986) has a long history and has been an essential part of neural networks for language (Bengio et al., 2003; Collobert et al., 2011). There is similarly a long history of rethinking the semantics of these embedding spaces, such as treating words as regions using sparse count-based vectors (Erk, 2009a,b) or dense distributed vectors (Vilnis and McCallum, 2015). Order embeddings (Vendrov et al., 2016) or their probabilistic version (POE) (Lai and Hockenmaier, 2017) are one technique suited for hierarchical modeling. However, OE can only handle binary entailment decisions, and POE cannot model negative correlations between types, a critical limitation in its use as a probabilistic model; these shortcomings directly led to the development of box embeddings. Hyperbolic embeddings (Nickel and Kiela, 2058 (a) (b) Figure 3: Edges of (a) the person box vs the actor box and (b) the person box vs t"
2021.acl-long.160,D17-1284,0,0.0156317,"003) are old problems in NLP. Recent work has focused chiefly on predicted fine-grained entity types (Ling and Weld, 2012; Gillick et al., 2014; Choi et al., 2018), as these convey significantly more information for downstream tasks. As a result, there is a challenge of scaling to large type inventories, which has inspired work on type embeddings (Ren et al., 2016a,b). Entity typing information has been used across a range of NLP tasks, including models for entity linking and coreference (Durrett and Klein, 2014). Typing has been shown to be useful for crossdomain entity linking specifically (Gupta et al., 2017; Onoe and Durrett, 2020a). It has also recently been applied to coreference resolution (Onoe and Durrett, 2020b; Khosla and Rose, 2020) and text generation (Dong et al., 2020), suggesting that it can be a useful intermediate layer even in pretrained neural models. 7 Conclusion In this paper, we investigated a box-based model for fine-grained entity typing. By representing entity types in a box embedding space and projecting entity mentions into the same space, we can naturally capture the hierarchy of and correlations between entity types. Our experiments showed several benefits of box embedd"
2021.acl-long.160,N06-2015,0,0.0143871,"observe state-of-the-art performance on several entity typing benchmarks. In addition to competitive typing performance, our box-based model shows better performance in prediction consistency (predicting a supertype and a subtype together) and confidence (i.e., calibration), demonstrating that the box-based model captures the latent type hierarchies better than the vector-based model does.1 1 Introduction The development of named entity recognition and entity typing has been characterized by a growth in the size and complexity of type sets: from 4 (Tjong Kim Sang and De Meulder, 2003) to 17 (Hovy et al., 2006) to hundreds (Weischedel and Brunstein, 2005; Ling and Weld, 2012) or thousands (Choi et al., 2018). These types follow some kind 1 The code is available at https://github.com/ yasumasaonoe/Box4Types. of hierarchical structure (Weischedel and Brunstein, 2005; Ling and Weld, 2012; Gillick et al., 2014; Murty et al., 2018), so effective models for these tasks frequently engage with this hierarchy explicitly. Prior systems incorporate this structure via hierarchical losses (Murty et al., 2018; Xu and Barbosa, 2018; Chen et al., 2020) or by embedding types into a high-dimensional Euclidean or hype"
2021.acl-long.160,2020.emnlp-main.21,1,0.845791,"Table 10 in Appendix C lists the 30 pairs. Robustness Entity typing datasets with very large ontologies like UFET are noisy; does our box-based model’s notion of hierarchy do a better job of handling intrinsic noise in a dataset? To test this in a controlled fashion, we synthetically create noisy labels by randomly dropping the gold labels with probability 13 .8 We derive two noisy training sets from the UFET training set: 1) adding noise to the coarse types and 2) adding noise to fine & ultra-fine types. We train on these noised datasets and evaluate on the standard UFET dev set. Calibration Desai and Durrett (2020) study calibration of pre-trained Transformers such as BERT and RoBERTa (Liu et al., 2019) on natural language inference, paraphrase detection, and commonsense reasoning. In a similar manner, we investigate if our box-based entity typing model is calibrated: do the probabilities assigned to types by the model match the empirical likelihoods of those types? Since models may naturally have different scales 8 If this causes the gold type set to be empty, we retain the original gold type(s); however, this case is rare. Model P R F1 Box Vector 52.8 38.8 44.8 53.0 36.3 43.1 Choi et al. (2018) Label"
2021.acl-long.160,N19-1423,0,0.0456209,"et D: ∑ L=− ) , − xM β +e k yM − β ) + (1 − tkgold ) · log(1 − pθ (tk |m, s)), . Following Dasgupta et al. (2020), we approximate the expected volume of a Gumbel box using a softplus function: ) ( ∏ xM,i − xm,i − 2γ , Vol(x) ≈ softplus β i where i is an index of each coordinate and γ ≈ 0.5772 is the Euler–Mascheroni constant,4 and softplus(x) = 1t log(1 + exp(xt)), with t as an inverse temperature value. Mention and Context Encoder We format the context words s and the mention span m as x = [CLS] m [SEP] s [SEP] and chunk into WordPiece tokens (Wu et al., 2016). Using pre-trained BERT5 (Devlin et al., 2019), we encode the whole sequence into a single vector by taking the hidden vector at the [CLS] token. A highway layer (Srivastava et al., 2015) projects down the hidden vector h[CLS] ∈ Rℓ to the R2d space, where ℓ is the hidden dimension of the encoder (BERT), and d is the dimension of the box space. This highway layer transforms representations in a vector space to the box space without impeding the gradient flow. We further split the hidden vector ¯ ∈ R2d into two vectors: the center point of the h box cx ∈ Rd and the offset from the maximum and minimum corners ox ∈ Rd . The minimum and maximu"
2021.acl-long.160,2020.codi-1.3,0,0.0312954,"t al., 2014; Choi et al., 2018), as these convey significantly more information for downstream tasks. As a result, there is a challenge of scaling to large type inventories, which has inspired work on type embeddings (Ren et al., 2016a,b). Entity typing information has been used across a range of NLP tasks, including models for entity linking and coreference (Durrett and Klein, 2014). Typing has been shown to be useful for crossdomain entity linking specifically (Gupta et al., 2017; Onoe and Durrett, 2020a). It has also recently been applied to coreference resolution (Onoe and Durrett, 2020b; Khosla and Rose, 2020) and text generation (Dong et al., 2020), suggesting that it can be a useful intermediate layer even in pretrained neural models. 7 Conclusion In this paper, we investigated a box-based model for fine-grained entity typing. By representing entity types in a box embedding space and projecting entity mentions into the same space, we can naturally capture the hierarchy of and correlations between entity types. Our experiments showed several benefits of box embeddings over the equivalent vector-based model, including typing performance, calibration, and robustness to noise. Acknowledgments Thanks"
2021.acl-long.160,E17-1068,0,0.127328,"t the person box almost contains this “effective” actor box. 6 Related Work Embeddings Embedding concepts/words into a high-dimensional vector space (Hinton, 1986) has a long history and has been an essential part of neural networks for language (Bengio et al., 2003; Collobert et al., 2011). There is similarly a long history of rethinking the semantics of these embedding spaces, such as treating words as regions using sparse count-based vectors (Erk, 2009a,b) or dense distributed vectors (Vilnis and McCallum, 2015). Order embeddings (Vendrov et al., 2016) or their probabilistic version (POE) (Lai and Hockenmaier, 2017) are one technique suited for hierarchical modeling. However, OE can only handle binary entailment decisions, and POE cannot model negative correlations between types, a critical limitation in its use as a probabilistic model; these shortcomings directly led to the development of box embeddings. Hyperbolic embeddings (Nickel and Kiela, 2058 (a) (b) Figure 3: Edges of (a) the person box vs the actor box and (b) the person box vs the minimum bounding box of the intersections between mention & context boxes and the actor box. 2017; L´opez and Strube, 2020) can also model hierarchical relationship"
2021.acl-long.160,D19-1641,0,0.56804,"-based model, the boxbased model improves primarily in macro-recall compared to macro-precision. Choi et al. (2018) is a LSTM-based model using GloVe (Pennington et al., 2014). On top of this model, Xiong et al. (2019) add a graph convolution layer to model type dependencies. Onoe and Durrett (2019) use ELMo (Peters et al., 2018) and apply denoising to fix label inconsistency in the distantly annotated data. Note that past work on this dataset has used BERT-base (Onoe and Durrett, 2019). Work on other datasets has used ELMo and observed that BERT-based models have surprisingly underperformed (Lin and Ji, 2019). Some of the gain from our vector-based model can be attributed to our use of BERT-Large; however, our box model still achieves stronger performance than the corresponding vector-based version which uses the same pretrained model. Table 2 breaks down the performance into the coarse, fine, and ultra-fine classes. Our box-based model consistently outperforms the vector-based model in macro-recall and F1 across the three classes. The largest gap in macro-recall is in the fine class, leading to the largest gap in macro-F1 within the three classes. We also list the numbers from prior work in Table"
2021.acl-long.160,2021.ccl-1.108,0,0.0482288,"Missing"
2021.acl-long.160,2020.findings-emnlp.42,0,0.039003,"Missing"
2021.acl-long.160,P18-1010,1,0.879896,"res the latent type hierarchies better than the vector-based model does.1 1 Introduction The development of named entity recognition and entity typing has been characterized by a growth in the size and complexity of type sets: from 4 (Tjong Kim Sang and De Meulder, 2003) to 17 (Hovy et al., 2006) to hundreds (Weischedel and Brunstein, 2005; Ling and Weld, 2012) or thousands (Choi et al., 2018). These types follow some kind 1 The code is available at https://github.com/ yasumasaonoe/Box4Types. of hierarchical structure (Weischedel and Brunstein, 2005; Ling and Weld, 2012; Gillick et al., 2014; Murty et al., 2018), so effective models for these tasks frequently engage with this hierarchy explicitly. Prior systems incorporate this structure via hierarchical losses (Murty et al., 2018; Xu and Barbosa, 2018; Chen et al., 2020) or by embedding types into a high-dimensional Euclidean or hyperbolic space (Yogatama et al., 2015; L´opez and Strube, 2020). However, the former approach requires prior knowledge of the type hierarchy, which is unsuitable for a recent class of large type sets where the hierarchy is not explicit (Choi et al., 2018; Onoe and Durrett, 2020a). The latter approaches, while leveraging th"
2021.acl-long.160,D15-1182,0,0.0665569,"Missing"
2021.acl-long.160,2020.findings-emnlp.54,1,0.735776,"2005; Ling and Weld, 2012; Gillick et al., 2014; Murty et al., 2018), so effective models for these tasks frequently engage with this hierarchy explicitly. Prior systems incorporate this structure via hierarchical losses (Murty et al., 2018; Xu and Barbosa, 2018; Chen et al., 2020) or by embedding types into a high-dimensional Euclidean or hyperbolic space (Yogatama et al., 2015; L´opez and Strube, 2020). However, the former approach requires prior knowledge of the type hierarchy, which is unsuitable for a recent class of large type sets where the hierarchy is not explicit (Choi et al., 2018; Onoe and Durrett, 2020a). The latter approaches, while leveraging the inductive bias of hyperbolic space to represent trees, lack a probabilistic interpretation of the embedding and do not naturally capture all of the complex type relationships beyond strict containment. In this paper, we describe an approach that represents entity types with box embeddings in a highdimensional space (Vilnis et al., 2018). We build an entity typing model that jointly embeds each entity mention and entity types into the same box space to determine the relation between them. Volumes of boxes correspond to probabilities and taking int"
2021.acl-long.160,D14-1162,0,0.10189,"Missing"
2021.acl-long.160,N18-1202,0,0.0651462,".2 67.1 38.4 40.7 46.2 46.6 39.4 42.2 43.8 40.4 Table 2: Macro-averaged P/R/F1 on the dev set for the entity typing task of Choi et al. (2018) comparing various systems. Our box-based model outperforms models from past work as well as our vector-based baseline. F1.9 Compared to the vector-based model, the boxbased model improves primarily in macro-recall compared to macro-precision. Choi et al. (2018) is a LSTM-based model using GloVe (Pennington et al., 2014). On top of this model, Xiong et al. (2019) add a graph convolution layer to model type dependencies. Onoe and Durrett (2019) use ELMo (Peters et al., 2018) and apply denoising to fix label inconsistency in the distantly annotated data. Note that past work on this dataset has used BERT-base (Onoe and Durrett, 2019). Work on other datasets has used ELMo and observed that BERT-based models have surprisingly underperformed (Lin and Ji, 2019). Some of the gain from our vector-based model can be attributed to our use of BERT-Large; however, our box model still achieves stronger performance than the corresponding vector-based version which uses the same pretrained model. Table 2 breaks down the performance into the coarse, fine, and ultra-fine classes."
2021.acl-long.160,D16-1144,0,0.591369,"tity types into the same box space to determine the relation between them. Volumes of boxes correspond to probabilities and taking intersections of boxes corresponds to computing joint distributions, which allows us to model mentiontype relations (what types does this mention exhibit?) and type-type relations (what is the type hierarchy?). Concretely, we can compute the conditional probability of a type given the entity mention with straightforward volume calculations, allowing us to construct a probabilistic type classification model. Compared to embedding types as points in Euclidean space (Ren et al., 2016a), the box space is expressive and suitable for representing entity types due to its geometric properties. Boxes can nest, overlap, or be completely disjoint to capture 2051 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2051–2064 August 1–6, 2021. ©2021 Association for Computational Linguistics … The Hunger Games, the first of 3 best selling books by Suzanne Collins. Figure 1: A mention (Suzanne Collins) and three entity types are embedded into a vector space (left) and"
2021.acl-long.160,E17-1119,0,0.0217716,"erforms the vector-based model on two benchmarks, Ultra-fine Entity Typing and OntoNotes, achieving state-ofthe-art-performance. In our other experiments, the box-based model also performs better at predicting supertypes and subtypes consistently and being robust against label noise, indicating that our approach is capable of capturing the latent hierarchical structure in entity types. 2 Motivation When predicting class labels like entity types that exhibit a hierarchical structure, we naturally want our model’s output layer to be sensitive to this structure. Previous work (Ren et al., 2016a; Shimaoka et al., 2017; Choi et al., 2018; Onoe and Durrett, 2019, inter alia) has fundamentally treated types as vectors, as shown in the left half of Figure 1. As is standard in multiclass or multi-label classification, the output layer of these models typically involves taking a dot product between a mention embedding and each possible type. A type could be more general and predicted on more examples by having higher norm,2 but it is hard for these representations to capture that a coarse type like Person will have many mutually orthogonal subtypes. By contrast, box embeddings naturally represent these kinds of"
2021.acl-long.160,N19-1250,1,0.893025,"hmarks, Ultra-fine Entity Typing and OntoNotes, achieving state-ofthe-art-performance. In our other experiments, the box-based model also performs better at predicting supertypes and subtypes consistently and being robust against label noise, indicating that our approach is capable of capturing the latent hierarchical structure in entity types. 2 Motivation When predicting class labels like entity types that exhibit a hierarchical structure, we naturally want our model’s output layer to be sensitive to this structure. Previous work (Ren et al., 2016a; Shimaoka et al., 2017; Choi et al., 2018; Onoe and Durrett, 2019, inter alia) has fundamentally treated types as vectors, as shown in the left half of Figure 1. As is standard in multiclass or multi-label classification, the output layer of these models typically involves taking a dot product between a mention embedding and each possible type. A type could be more general and predicted on more examples by having higher norm,2 but it is hard for these representations to capture that a coarse type like Person will have many mutually orthogonal subtypes. By contrast, box embeddings naturally represent these kinds of hierarchies as shown in the right half of F"
2021.acl-long.160,W03-0419,0,0.528764,"Missing"
2021.acl-long.160,P18-1025,1,0.921986,"z and Strube, 2020). However, the former approach requires prior knowledge of the type hierarchy, which is unsuitable for a recent class of large type sets where the hierarchy is not explicit (Choi et al., 2018; Onoe and Durrett, 2020a). The latter approaches, while leveraging the inductive bias of hyperbolic space to represent trees, lack a probabilistic interpretation of the embedding and do not naturally capture all of the complex type relationships beyond strict containment. In this paper, we describe an approach that represents entity types with box embeddings in a highdimensional space (Vilnis et al., 2018). We build an entity typing model that jointly embeds each entity mention and entity types into the same box space to determine the relation between them. Volumes of boxes correspond to probabilities and taking intersections of boxes corresponds to computing joint distributions, which allows us to model mentiontype relations (what types does this mention exhibit?) and type-type relations (what is the type hierarchy?). Concretely, we can compute the conditional probability of a type given the entity mention with straightforward volume calculations, allowing us to construct a probabilistic type"
2021.acl-long.160,N19-1084,0,0.117704,"he same pretrained model. Table 2 breaks down the performance into the coarse, fine, and ultra-fine classes. Our box-based model consistently outperforms the vector-based model in macro-recall and F1 across the three classes. The largest gap in macro-recall is in the fine class, leading to the largest gap in macro-F1 within the three classes. We also list the numbers from prior work in Table 2. HY XLarge (L´opez and Strube, 2020), a hyperbolic model designed to learn hierarchical structure in entity types, exceeds the performance of the models with similar sizes such as Choi et al. (2018) and Xiong et al. (2019) especially in macrorecall. In the ultra-fine class, both our box-based model and HY XLarge achieve higher macro-F1 compared to their vector-based counterparts. One possible reason for the higher recall of our 9 We omit the test number of L´opez and Strube (2020), since they report results broken down into coarse, fine, and ultra-fine types instead of an aggregated F1 value. However, based on the development results, their approach substantially underperforms the past work of Onoe and Durrett (2019) regardless. model is a stronger ability to model dependencies between types. Instead of failing"
2021.acl-long.160,N18-1002,0,0.0166108,"size and complexity of type sets: from 4 (Tjong Kim Sang and De Meulder, 2003) to 17 (Hovy et al., 2006) to hundreds (Weischedel and Brunstein, 2005; Ling and Weld, 2012) or thousands (Choi et al., 2018). These types follow some kind 1 The code is available at https://github.com/ yasumasaonoe/Box4Types. of hierarchical structure (Weischedel and Brunstein, 2005; Ling and Weld, 2012; Gillick et al., 2014; Murty et al., 2018), so effective models for these tasks frequently engage with this hierarchy explicitly. Prior systems incorporate this structure via hierarchical losses (Murty et al., 2018; Xu and Barbosa, 2018; Chen et al., 2020) or by embedding types into a high-dimensional Euclidean or hyperbolic space (Yogatama et al., 2015; L´opez and Strube, 2020). However, the former approach requires prior knowledge of the type hierarchy, which is unsuitable for a recent class of large type sets where the hierarchy is not explicit (Choi et al., 2018; Onoe and Durrett, 2020a). The latter approaches, while leveraging the inductive bias of hyperbolic space to represent trees, lack a probabilistic interpretation of the embedding and do not naturally capture all of the complex type relationships beyond strict con"
2021.acl-long.160,P15-2048,0,0.0625899,"Weischedel and Brunstein, 2005; Ling and Weld, 2012) or thousands (Choi et al., 2018). These types follow some kind 1 The code is available at https://github.com/ yasumasaonoe/Box4Types. of hierarchical structure (Weischedel and Brunstein, 2005; Ling and Weld, 2012; Gillick et al., 2014; Murty et al., 2018), so effective models for these tasks frequently engage with this hierarchy explicitly. Prior systems incorporate this structure via hierarchical losses (Murty et al., 2018; Xu and Barbosa, 2018; Chen et al., 2020) or by embedding types into a high-dimensional Euclidean or hyperbolic space (Yogatama et al., 2015; L´opez and Strube, 2020). However, the former approach requires prior knowledge of the type hierarchy, which is unsuitable for a recent class of large type sets where the hierarchy is not explicit (Choi et al., 2018; Onoe and Durrett, 2020a). The latter approaches, while leveraging the inductive bias of hyperbolic space to represent trees, lack a probabilistic interpretation of the embedding and do not naturally capture all of the complex type relationships beyond strict containment. In this paper, we describe an approach that represents entity types with box embeddings in a highdimensional"
2021.acl-long.160,S18-2022,0,0.600077,"Missing"
2021.acl-long.539,2020.findings-emnlp.301,0,0.086692,"Missing"
2021.acl-long.539,J05-3002,0,0.349974,"Missing"
2021.acl-long.539,2020.findings-emnlp.322,1,0.895988,"trides in recent years. These models demonstrate exciting new capabilities in terms of abstraction, but little is known about how these models work. In particular, do token generation decisions leverage the source text, and if so, which parts? Or do these decisions arise based primarily on knowledge from the language model (Jiang et al., 2020; Carlini et al., 2020), learned during pre-training or fine-tuning? Having tools to analyze these models is crucial to identifying and forestalling problems in generation, such as toxicity (Gehman et al., 2020) or factual errors (Kryscinski et al., 2020; Goyal and Durrett, 2020, 2021). Although interpreting classification models for NLP has been widely studied from perspectives like feature attribution (Ribeiro et al., 2016; Sundararajan et al., 2017) and influence functions (Koh and Liang, 2017; Han et al., 2020), summarization specifically introduces some additional elements that make these techniques hard to apply directly. First, summarization models make sequential decisions from a very large state space. Second, encoder-decoder models have a special structure, featuring a complex interaction of decoder-side and encoder-side computation to select the next word."
2021.acl-long.539,2020.blackboxnlp-1.14,0,0.0192058,"ition, along with several attribution methods from prior work. Occlusion (Zeiler and Fergus, 2014) involves iteratively masking every single token or remove each sentence in the document and measuring how the prediction probability of the target token changes. Although attention has been questioned (Jain and Wallace, 2019), it still has some value as an explanation technique (Wiegreffe and Pinter, 2019; Serrano and Smith, 2019). We pool the attention heads from the last layer of the Transformer inside our models, ignoring special tokens like SOS. Finally, we use two gradient-based techniques (Bastings and Filippova, 2020). Input Gradient is a saliency based approach taking the gradient of the target token with respect to the input and multiplying by the input feature values. Integrated Gradients Sundararajan et al. (2017) computes gradients of the model input at a number of points interpolated between a reference “baseline” (typically an all-MASK input) and the actual input. This computes a path integral of the gradient. 5 The full model is not a strict bound on this; restricting the model to only see salient content could actually increase the probability of what was generated. However, because we have limite"
2021.acl-long.539,2021.naacl-main.114,1,0.824875,"Missing"
2021.acl-long.539,2020.acl-main.492,0,0.0356674,"Missing"
2021.acl-long.539,2020.acl-main.494,0,0.0886603,"ces (s1 , · · · , sm ) and n tokens (w1 , w2 , · · · , wn ), then generates a sequence of tokens (y1 , · · · , yT ) as the summary. At each time step t in the generation phase, the model encodes the input document and the decoded summary prefix and predicts the distribution over tokens as p(yt |w1 , w2 , . . . , wm , y&lt;t ). 2.1 Target Models & Datasets We investigate the English-language CNN/DM (Hermann et al., 2015) and XSum (Narayan et al., 2018) datasets, which are commonly used to fine tune pre-trained language models like BART, PEGASUS and T5. As shown in past work (Narayan et al., 2018; Chen et al., 2020b; Xu et al., 2020a), XSum has significantly different properties from CNN/DM, so these datasets will show a range of model behaviors. We will primarily use the development sets for our analysis. We focus on BART (Lewis et al., 2020), a stateof-the-art pre-trained model for language modeling and text summarization. Specifically, we adopt ‘bart-large’ as the language model MLM , ‘bartlarge-xsum’ as the summarization model MSUM for XSum, and ‘bart-large-cnn’ for CNN/DM, made available by Wolf et al. (2019). BART features separate LM and summarization model sharing the same subword tokenization m"
2021.acl-long.539,2020.acl-main.386,0,0.0465284,"of a word, span, or sentence. This brings us to our second assumption: Assumption 2 In order to say that a span of the input or decoder context is important to the model’s prediction, it should be the case that this span is demonstrated to be important in counterfactual settings. That is, modified inputs to the model that include this span should yield closer predictions than those that don’t. This criterion depends on the set of counterfactuals that we use. Rather than just word removal (Ribeiro et al., 2016), we will use a more compre6927 CT-Hd hensive set of counterfactuals (Miller, 2019; Jacovi and Goldberg, 2020) to quantify the importance of input tokens. We describe this more in Section 5. 2.00 PT 1.75 2.4 Distance Metric 1.50 3 While our axes are very different here, our mapping concept loosely follows that of Swayamdipta et al. (2020). 1.00 0.75 LM 0.25 FT 0.00 0.0 0.5 1.0 d(LM , Sfull) 1.5 2.0 CT-Hd 2.00 PT 1.75 1.50 Ablation: Mapping Model Behavior Based on Assumption 1, we can take a first step towards understanding these models based on the partial models described in Section 2.3. Previous work (See et al., 2017; Song et al., 2020) has studied model behavior based on externally-visible propert"
2021.acl-long.539,W18-5408,0,0.0237426,"rse in the generation process), and could potentially be useful for fine-grained factuality evaluation using recent techniques (Tian et al., 2019; Kryscinski et al., 2020; Goyal and Durrett, 2020; Maynez et al., 2020). The majority of the “fusion” cases we investigated actually reflect content selection at the beginning of the generation. Other cases we observe fall more cleanly into classic sentence fusion or draw on coreference resolution. 7 Related Work Model interpretability for NLP has been intensively studied in the past few years (Ribeiro et al., 2016; Alvarez-Melis and Jaakkola, 2018; Jacovi et al., 2018; Chen et al., 2020a; Jacovi and Goldberg, 2020; DeYoung et al., 2020; Pruthi et al., 2020; Ye et al., 2021). However, many of these techniques are tailored to classification tasks like sentiment. For post-hoc interpretation of generation, most work has studied machine translation (Ma et al.; Li et al., 2020; Voita et al., 2020). Li et al. (2020) focus on evaluating explanations by finding surrogate models that are similar to the base MT model; this is similar to our evaluation approach in Section 5, but involves an extra distillation step. Compared to Voita et al. (2020), we are more interest"
2021.acl-long.539,N19-1357,0,0.0280801,"ore precise which helps locating the exact feature token, but the trade-off is that the input is not fully natural. 5.2 Methods We use two baseline methods: Random, which randomly selects tokens or sentences to display or remove, and Lead, which selects tokens or sentences according to document position, along with several attribution methods from prior work. Occlusion (Zeiler and Fergus, 2014) involves iteratively masking every single token or remove each sentence in the document and measuring how the prediction probability of the target token changes. Although attention has been questioned (Jain and Wallace, 2019), it still has some value as an explanation technique (Wiegreffe and Pinter, 2019; Serrano and Smith, 2019). We pool the attention heads from the last layer of the Transformer inside our models, ignoring special tokens like SOS. Finally, we use two gradient-based techniques (Bastings and Filippova, 2020). Input Gradient is a saliency based approach taking the gradient of the target token with respect to the input and multiplying by the input feature values. Integrated Gradients Sundararajan et al. (2017) computes gradients of the model input at a number of points interpolated between a referen"
2021.acl-long.539,2020.tacl-1.28,0,0.0887783,"Missing"
2021.acl-long.539,D18-1208,0,0.0548361,"Missing"
2021.acl-long.539,D19-5413,0,0.0431283,"Missing"
2021.acl-long.539,2020.acl-main.703,0,0.318279,"butions are truly important for the generation of the next token. While this machinery can be broadly useful even beyond summarization, we specifically demonstrate its capability to identify phrases the summarization model has memorized and determine where in the training pipeline this memorization happened, as well as study complex generation phenomena like sentence fusion on a per-instance basis. 1 Introduction Transformer-based neural summarization models (Liu and Lapata, 2019; Stiennon et al., 2020; Xu et al., 2020b; Desai et al., 2020), especially pretrained abstractive models like BART (Lewis et al., 2020) and PEGASUS (Zhang et al., 2020), have made great strides in recent years. These models demonstrate exciting new capabilities in terms of abstraction, but little is known about how these models work. In particular, do token generation decisions leverage the source text, and if so, which parts? Or do these decisions arise based primarily on knowledge from the language model (Jiang et al., 2020; Carlini et al., 2020), learned during pre-training or fine-tuning? Having tools to analyze these models is crucial to identifying and forestalling problems in generation, such as toxicity (Gehman et al."
2021.acl-long.539,2020.acl-main.35,0,0.107291,"input as either just the decoded summary so far or the summary and partial context. In practice, we see two things. First, when considering just the decoder context (i.e., behaving as an LM), the partial model may reproduce the full model’s behavior (e.g., Khan in Figure 1). We do not focus on explaining these cases in further detail. While conceivably the actual conditional model might internally be doing something different (a risk noted by Rudin (2019)), this proves the existence of a decoder-only proxy model that reproduces the full model’s results, which is a criterion used in past work (Li et al., 2020). Second, when considering partial inputs, the model frequently requires one or two specific sentences to reproduce the full model’s behavior, suggesting that the given contexts are both necessary and sufficient. Because these analyses involve using the model on data significantly different than that which it is trained on, we want another way to quantify the importance of a word, span, or sentence. This brings us to our second assumption: Assumption 2 In order to say that a span of the input or decoder context is important to the model’s prediction, it should be the case that this span is dem"
2021.acl-long.539,D19-1387,0,0.116227,"to select content and reconstruct the model’s predicted token from perturbations of the input, thus revealing whether highlighted attributions are truly important for the generation of the next token. While this machinery can be broadly useful even beyond summarization, we specifically demonstrate its capability to identify phrases the summarization model has memorized and determine where in the training pipeline this memorization happened, as well as study complex generation phenomena like sentence fusion on a per-instance basis. 1 Introduction Transformer-based neural summarization models (Liu and Lapata, 2019; Stiennon et al., 2020; Xu et al., 2020b; Desai et al., 2020), especially pretrained abstractive models like BART (Lewis et al., 2020) and PEGASUS (Zhang et al., 2020), have made great strides in recent years. These models demonstrate exciting new capabilities in terms of abstraction, but little is known about how these models work. In particular, do token generation decisions leverage the source text, and if so, which parts? Or do these decisions arise based primarily on knowledge from the language model (Jiang et al., 2020; Carlini et al., 2020), learned during pre-training or fine-tuning?"
2021.acl-long.539,2020.emnlp-main.746,0,0.0680805,"Missing"
2021.acl-long.539,N16-3020,0,0.679441,"particular, do token generation decisions leverage the source text, and if so, which parts? Or do these decisions arise based primarily on knowledge from the language model (Jiang et al., 2020; Carlini et al., 2020), learned during pre-training or fine-tuning? Having tools to analyze these models is crucial to identifying and forestalling problems in generation, such as toxicity (Gehman et al., 2020) or factual errors (Kryscinski et al., 2020; Goyal and Durrett, 2020, 2021). Although interpreting classification models for NLP has been widely studied from perspectives like feature attribution (Ribeiro et al., 2016; Sundararajan et al., 2017) and influence functions (Koh and Liang, 2017; Han et al., 2020), summarization specifically introduces some additional elements that make these techniques hard to apply directly. First, summarization models make sequential decisions from a very large state space. Second, encoder-decoder models have a special structure, featuring a complex interaction of decoder-side and encoder-side computation to select the next word. Third, pre-trained LMs blur the distinction between relying on implicit prior knowledge or explicit instance-dependent input. This paper aims to mor"
2021.acl-long.539,P17-1099,0,0.0702248,"se a more compre6927 CT-Hd hensive set of counterfactuals (Miller, 2019; Jacovi and Goldberg, 2020) to quantify the importance of input tokens. We describe this more in Section 5. 2.00 PT 1.75 2.4 Distance Metric 1.50 3 While our axes are very different here, our mapping concept loosely follows that of Swayamdipta et al. (2020). 1.00 0.75 LM 0.25 FT 0.00 0.0 0.5 1.0 d(LM , Sfull) 1.5 2.0 CT-Hd 2.00 PT 1.75 1.50 Ablation: Mapping Model Behavior Based on Assumption 1, we can take a first step towards understanding these models based on the partial models described in Section 2.3. Previous work (See et al., 2017; Song et al., 2020) has studied model behavior based on externally-visible properties of the model’s generation, such as identifying novel words, differentiating copy and generation, and prediction confidence, which provides some insight about model’s behavior (Xu et al., 2020a). However, these focus more on shallow comparison of the input document, the generated summary, and the reference summary, and do not focus as strongly on the model. We propose a new way of mapping the prediction space, with maps3 for XSum and CNN/DM shown in Figure 2. Each point in the map is a single subword token be"
2021.acl-long.539,P19-1282,0,0.0213144,"natural. 5.2 Methods We use two baseline methods: Random, which randomly selects tokens or sentences to display or remove, and Lead, which selects tokens or sentences according to document position, along with several attribution methods from prior work. Occlusion (Zeiler and Fergus, 2014) involves iteratively masking every single token or remove each sentence in the document and measuring how the prediction probability of the target token changes. Although attention has been questioned (Jain and Wallace, 2019), it still has some value as an explanation technique (Wiegreffe and Pinter, 2019; Serrano and Smith, 2019). We pool the attention heads from the last layer of the Transformer inside our models, ignoring special tokens like SOS. Finally, we use two gradient-based techniques (Bastings and Filippova, 2020). Input Gradient is a saliency based approach taking the gradient of the target token with respect to the input and multiplying by the input feature values. Integrated Gradients Sundararajan et al. (2017) computes gradients of the model input at a number of points interpolated between a reference “baseline” (typically an all-MASK input) and the actual input. This computes a path integral of the grad"
2021.acl-long.539,2020.emnlp-main.346,0,0.0792902,"Missing"
2021.acl-long.539,2020.tacl-1.48,0,0.099463,"Missing"
2021.acl-long.539,I13-1198,0,0.0760111,"Missing"
2021.acl-long.539,D19-1002,0,0.0172417,"that the input is not fully natural. 5.2 Methods We use two baseline methods: Random, which randomly selects tokens or sentences to display or remove, and Lead, which selects tokens or sentences according to document position, along with several attribution methods from prior work. Occlusion (Zeiler and Fergus, 2014) involves iteratively masking every single token or remove each sentence in the document and measuring how the prediction probability of the target token changes. Although attention has been questioned (Jain and Wallace, 2019), it still has some value as an explanation technique (Wiegreffe and Pinter, 2019; Serrano and Smith, 2019). We pool the attention heads from the last layer of the Transformer inside our models, ignoring special tokens like SOS. Finally, we use two gradient-based techniques (Bastings and Filippova, 2020). Input Gradient is a saliency based approach taking the gradient of the target token with respect to the input and multiplying by the input feature values. Integrated Gradients Sundararajan et al. (2017) computes gradients of the model input at a number of points interpolated between a reference “baseline” (typically an all-MASK input) and the actual input. This computes a"
2021.acl-long.539,2020.emnlp-main.508,1,0.82765,"Missing"
2021.acl-long.555,P19-1470,0,0.0290959,"Computational Linguistics Prior work has not combined traditional event cooccurrence with event temporality as we do. We propose a conditional generation model to tackle temporal event ordering and event infilling, and train it as a denoising autoencoder over outof-context temporal event sequences. As shown in Figure 1, the encoder of our TemporalBART model reads a temporally scrambled sequence of a subset of input events, obtained by corrupting a temporally-ordered sequence of events from a corpus. The decoder, which can be viewed as a conditional event language model (Kiyomaru et al., 2019; Bosselut et al., 2019; Madaan et al., 2020), then reconstructs the complete, temporally-ordered event sequence. Such denoising training has been successful exploited in many applications (Vincent et al., 2010; Lu et al., 2013; Lample et al., 2018; Lewis et al., 2020), and using seq2seq models to reorder and smooth inputs has been explored before (Goyal and Durrett, 2020), but to our knowledge we are the first to apply this in this temporal modeling setting. The conditional generation architecture of our model is flexible enough to address a variety of tasks, including our temporal ordering and event infilling task"
2021.acl-long.555,P08-1090,1,0.830954,"infilling goals. 2.2 Schema Induction Schema learning systems are often evaluated on their ability to predict unseen events. Initial work 7143 e1 e2 e3 e4 e5 Encoder Decoder e4 e2 e1 Autoencoder (reconstruct original sequence) e4 e5 e2 e1 e3 Event Deletion e1 e2 e3 e4 e5 Event Shuﬄing e1 e2 e3 e4 e5 Input events Figure 2: Our event-based denoising autoencoding training scheme used to encourage our model to learn temporal event knowledge. The input is corrupted by shuffling and deletion. attempted to use statistical methods to derive a library of schematic information (Mooney and DeJong, 1985; Chambers and Jurafsky, 2008; Jans et al., 2012). Another thread exploits event language modeling to learn the distributions over events (Pichotta and Mooney, 2016; Peng and Roth, 2016; Weber et al., 2018b), or focuses on learning event representations (Modi, 2016; Weber et al., 2018a) rather than writing down discrete schemas. However, most of this work only models the cooccurrence between events instead of directly considering temporal information, and only represent events as a small tuple of S-V-O headwords. Another line of work instead directly focuses on extracting coherent narratives from “story salads” (Wang et a"
2021.acl-long.555,P07-2044,1,0.650114,"d Work Learning temporal knowledge to order events and generate new events as part of schemas or stories are two problems that have received significant attention, but in contrast to our work, previous work typically focuses on each in isolation. 2.1 Temporal Event Ordering Closely related to the temporal ordering aspect of this paper is temporal relation extraction, which orders pairs of events in text in document context (Pustejovsky et al., 2003b; Cassidy et al., 2014; Ning et al., 2018b). This problem has been addressed as pairwise classification (Mani et al., 2006; Verhagen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008; Cheng and Miyao, 2017; Tourille et al., 2017; Goyal and Durrett, 2019) or as a structured learning problem to enforce constraints on the output (Do et al., 2012; Ning et al., 2017, 2018a; Leeuwenberg and Moens, 2017; Han et al., 2019a,b). However, even in these latter works, the models focus on pairwise relations. In contrast, our work here views temporal event ordering as a sequence generation problem, which provides models a stronger inductive bias to capture global temporal relations between events. One recent effort (Madaan and Yang, 2020) treats this task"
2021.acl-long.555,W16-1007,1,0.926305,"e training data we need. In these documents, discourse order is loosely assumed to reflect temporal order, so events extracted from this text can directly provide training data for our models. This use of automatic annotation allows us to use broad-domain data, giving us a strong domain-independent temporal model (Zhao et al., 2021). To evaluate how well our proposed models capture temporal knowledge and solve the two targeted tasks, we apply them on out-of domain test sets in a zero-shot manner. Specifically, for event ordering, we first extract test temporal event sequences from the CaTeRS (Mostafazadeh et al., 2016b) and MCTaco (Zhou et al., 2019) datasets, which include the annotations on temporal relations between events. We then compare the performance of our models with two baselines: a BERT-based pairwise model and a BERT-based pointer network. For event infilling, we use the test event sequences from CaTeRS and examine the ability of our models to order unseen events and generate infilled events in comparison with GPT-2 baselines from story generation. Our BART-based models significantly outperform the baseline models on the ordering settings we consider, and human evaluation verifies that our mod"
2021.acl-long.555,D17-1108,0,0.0202837,"focuses on each in isolation. 2.1 Temporal Event Ordering Closely related to the temporal ordering aspect of this paper is temporal relation extraction, which orders pairs of events in text in document context (Pustejovsky et al., 2003b; Cassidy et al., 2014; Ning et al., 2018b). This problem has been addressed as pairwise classification (Mani et al., 2006; Verhagen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008; Cheng and Miyao, 2017; Tourille et al., 2017; Goyal and Durrett, 2019) or as a structured learning problem to enforce constraints on the output (Do et al., 2012; Ning et al., 2017, 2018a; Leeuwenberg and Moens, 2017; Han et al., 2019a,b). However, even in these latter works, the models focus on pairwise relations. In contrast, our work here views temporal event ordering as a sequence generation problem, which provides models a stronger inductive bias to capture global temporal relations between events. One recent effort (Madaan and Yang, 2020) treats this task as a graph generation problem, and so is able to predict more complex structures, but it focuses solely on ordering and is not suitable for our event infilling goals. 2.2 Schema Induction Schema learning systems"
2021.acl-long.555,2020.emnlp-main.88,0,0.117498,"s about events without explicitly materializing a discrete schema library. The target tasks in this work are directly motivated by downstream applications of schema learning. Text generation tasks like story completion rely on understanding what makes narratives plausible and what events might be likely to happen before, after, and between other events (Jain et al., 2017; Yao et al., 2019), motivating our event infilling task. Answering questions about causes, effects, or what might happen next in a scenario requires knowing typical temporal orders of event sequences (Zhou et al., 2019, 2020; Ning et al., 2020), motivating our temporal ordering task. 7142 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 7142–7157 August 1–6, 2021. ©2021 Association for Computational Linguistics Prior work has not combined traditional event cooccurrence with event temporality as we do. We propose a conditional generation model to tackle temporal event ordering and event infilling, and train it as a denoising autoencoder over outof-context temporal event sequences. As shown in Figure 1, the encoder"
2021.acl-long.555,N18-1077,0,0.255309,"RT captures both temporal ordering and event cooccurrence to make various event-related inferences. ture temporal event knowledge broadly and support a wide range of inferences. We thus need a suitably general modeling framework to capture temporal knowledge about events, which in our case will be a BART-based (Lewis et al., 2020) model we call TemporalBART. Note that classic temporal relation extraction models, which model temporal ordering in context for a particular document, may chiefly learn how to use local discourse cues rather than generalizable event knowledge (Chambers et al., 2014; Ning et al., 2018b). Introduction This paper proposes a single model of events to support inferences in two seemingly different tasks: (1) temporal event ordering and (2) event infilling, or inferring unseen or unmentioned events occurring as part of a larger scenario. Figure 1 shows an example illustrating these two goals. Unlike prior approaches, we aim to address both with the same model architecture, rather than having to annotate data and build ad-hoc models for each task separately; our goal is to work towards models that capThe goals in this work relate to past work on learning narrative schemas (Mooney"
2021.acl-long.555,P18-1122,0,0.234314,"RT captures both temporal ordering and event cooccurrence to make various event-related inferences. ture temporal event knowledge broadly and support a wide range of inferences. We thus need a suitably general modeling framework to capture temporal knowledge about events, which in our case will be a BART-based (Lewis et al., 2020) model we call TemporalBART. Note that classic temporal relation extraction models, which model temporal ordering in context for a particular document, may chiefly learn how to use local discourse cues rather than generalizable event knowledge (Chambers et al., 2014; Ning et al., 2018b). Introduction This paper proposes a single model of events to support inferences in two seemingly different tasks: (1) temporal event ordering and (2) event infilling, or inferring unseen or unmentioned events occurring as part of a larger scenario. Figure 1 shows an example illustrating these two goals. Unlike prior approaches, we aim to address both with the same model architecture, rather than having to annotate data and build ad-hoc models for each task separately; our goal is to work towards models that capThe goals in this work relate to past work on learning narrative schemas (Mooney"
2021.acl-long.555,J05-1004,0,0.142026,"the scenario y: a partial set of unordered events. Our model should learn distributions over a true underlying order of events, without obvious gaps in the event sequence, given this incomplete information. By taking events out of context rather than in the context of a document, we are encouraging the model to encode temporal knowledge between events rather than superficial cues like surface textual order or discourse connectives that might determine their order. For the definition of events, we follow Chambers and Jurafsky (2008) where an event e is a predicate ve along with its arguments (Palmer et al., 2005). Our model can be formulated as a denoising autoencoder if x is created as a noised version of y. Specifically, given a temporal event sequence y as defined above, we first corrupt it to get the required input x by performing two transformation functions consecutively (see Figure 2): Event Shuffling We first perform a random shuffling of the events in y to produce x. To perfectly reconstruct the original sequence y, the model must capture the temporal relations between events. Event Deletion We randomly delete each event in y with probability p to produce x. This denoising scheme is similar t"
2021.acl-long.555,P16-1028,0,0.143115,"s a single model of events to support inferences in two seemingly different tasks: (1) temporal event ordering and (2) event infilling, or inferring unseen or unmentioned events occurring as part of a larger scenario. Figure 1 shows an example illustrating these two goals. Unlike prior approaches, we aim to address both with the same model architecture, rather than having to annotate data and build ad-hoc models for each task separately; our goal is to work towards models that capThe goals in this work relate to past work on learning narrative schemas (Mooney and DeJong, 1985; Chambers, 2013; Peng and Roth, 2016; Peng et al., 2017). Our approach particularly follows a recent line of work using distributed representations of schemas (Pichotta and Mooney, 2016; Weber et al., 2018b), which support inferences about events without explicitly materializing a discrete schema library. The target tasks in this work are directly motivated by downstream applications of schema learning. Text generation tasks like story completion rely on understanding what makes narratives plausible and what events might be likely to happen before, after, and between other events (Jain et al., 2017; Yao et al., 2019), motivating"
2021.acl-long.555,2020.emnlp-main.58,0,0.0778867,"Missing"
2021.acl-long.555,2020.emnlp-main.349,0,0.0593192,"Missing"
2021.acl-long.555,P17-2035,0,0.02266,"part of schemas or stories are two problems that have received significant attention, but in contrast to our work, previous work typically focuses on each in isolation. 2.1 Temporal Event Ordering Closely related to the temporal ordering aspect of this paper is temporal relation extraction, which orders pairs of events in text in document context (Pustejovsky et al., 2003b; Cassidy et al., 2014; Ning et al., 2018b). This problem has been addressed as pairwise classification (Mani et al., 2006; Verhagen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008; Cheng and Miyao, 2017; Tourille et al., 2017; Goyal and Durrett, 2019) or as a structured learning problem to enforce constraints on the output (Do et al., 2012; Ning et al., 2017, 2018a; Leeuwenberg and Moens, 2017; Han et al., 2019a,b). However, even in these latter works, the models focus on pairwise relations. In contrast, our work here views temporal event ordering as a sequence generation problem, which provides models a stronger inductive bias to capture global temporal relations between events. One recent effort (Madaan and Yang, 2020) treats this task as a graph generation problem, and so is able to predict more complex structu"
2021.acl-long.555,P19-1280,0,0.047847,"Missing"
2021.acl-long.555,S07-1014,0,0.0288069,"2 Background and Related Work Learning temporal knowledge to order events and generate new events as part of schemas or stories are two problems that have received significant attention, but in contrast to our work, previous work typically focuses on each in isolation. 2.1 Temporal Event Ordering Closely related to the temporal ordering aspect of this paper is temporal relation extraction, which orders pairs of events in text in document context (Pustejovsky et al., 2003b; Cassidy et al., 2014; Ning et al., 2018b). This problem has been addressed as pairwise classification (Mani et al., 2006; Verhagen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008; Cheng and Miyao, 2017; Tourille et al., 2017; Goyal and Durrett, 2019) or as a structured learning problem to enforce constraints on the output (Do et al., 2012; Ning et al., 2017, 2018a; Leeuwenberg and Moens, 2017; Han et al., 2019a,b). However, even in these latter works, the models focus on pairwise relations. In contrast, our work here views temporal event ordering as a sequence generation problem, which provides models a stronger inductive bias to capture global temporal relations between events. One recent effort (Madaan and Yang,"
2021.acl-long.555,C08-3012,0,0.0523325,"l knowledge to order events and generate new events as part of schemas or stories are two problems that have received significant attention, but in contrast to our work, previous work typically focuses on each in isolation. 2.1 Temporal Event Ordering Closely related to the temporal ordering aspect of this paper is temporal relation extraction, which orders pairs of events in text in document context (Pustejovsky et al., 2003b; Cassidy et al., 2014; Ning et al., 2018b). This problem has been addressed as pairwise classification (Mani et al., 2006; Verhagen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008; Cheng and Miyao, 2017; Tourille et al., 2017; Goyal and Durrett, 2019) or as a structured learning problem to enforce constraints on the output (Do et al., 2012; Ning et al., 2017, 2018a; Leeuwenberg and Moens, 2017; Han et al., 2019a,b). However, even in these latter works, the models focus on pairwise relations. In contrast, our work here views temporal event ordering as a sequence generation problem, which provides models a stronger inductive bias to capture global temporal relations between events. One recent effort (Madaan and Yang, 2020) treats this task as a graph generation problem,"
2021.acl-long.555,D19-1273,1,0.85049,"rn the distributions over events (Pichotta and Mooney, 2016; Peng and Roth, 2016; Weber et al., 2018b), or focuses on learning event representations (Modi, 2016; Weber et al., 2018a) rather than writing down discrete schemas. However, most of this work only models the cooccurrence between events instead of directly considering temporal information, and only represent events as a small tuple of S-V-O headwords. Another line of work instead directly focuses on extracting coherent narratives from “story salads” (Wang et al., 2018) or more broadly generating narratives given predefined scenarios (Wang et al., 2019; Qin et al., 2020). However, without considering temporal ordering, these systems are prone to learn discourse ordering of events instead of a strong representation of temporal knowledge. 3 3.1 Method Task Formulation and Model Our framework involves modeling a conditional distribution P (y |x) over temporal event sequences y = {e1 , · · · , el }, which are sequences of events taken out of context (i.e., not represented as spans in a document) which are part of the same scenario, involve shared actors, and are temporally ordered. The input of the model is a (not necessarily temporal) sequence"
2021.acl-long.555,D18-1175,1,0.821086,"sky, 2008; Jans et al., 2012). Another thread exploits event language modeling to learn the distributions over events (Pichotta and Mooney, 2016; Peng and Roth, 2016; Weber et al., 2018b), or focuses on learning event representations (Modi, 2016; Weber et al., 2018a) rather than writing down discrete schemas. However, most of this work only models the cooccurrence between events instead of directly considering temporal information, and only represent events as a small tuple of S-V-O headwords. Another line of work instead directly focuses on extracting coherent narratives from “story salads” (Wang et al., 2018) or more broadly generating narratives given predefined scenarios (Wang et al., 2019; Qin et al., 2020). However, without considering temporal ordering, these systems are prone to learn discourse ordering of events instead of a strong representation of temporal knowledge. 3 3.1 Method Task Formulation and Model Our framework involves modeling a conditional distribution P (y |x) over temporal event sequences y = {e1 , · · · , el }, which are sequences of events taken out of context (i.e., not represented as spans in a document) which are part of the same scenario, involve shared actors, and are"
2021.acl-long.555,D18-1413,1,0.810746,"events occurring as part of a larger scenario. Figure 1 shows an example illustrating these two goals. Unlike prior approaches, we aim to address both with the same model architecture, rather than having to annotate data and build ad-hoc models for each task separately; our goal is to work towards models that capThe goals in this work relate to past work on learning narrative schemas (Mooney and DeJong, 1985; Chambers, 2013; Peng and Roth, 2016; Peng et al., 2017). Our approach particularly follows a recent line of work using distributed representations of schemas (Pichotta and Mooney, 2016; Weber et al., 2018b), which support inferences about events without explicitly materializing a discrete schema library. The target tasks in this work are directly motivated by downstream applications of schema learning. Text generation tasks like story completion rely on understanding what makes narratives plausible and what events might be likely to happen before, after, and between other events (Jain et al., 2017; Yao et al., 2019), motivating our event infilling task. Answering questions about causes, effects, or what might happen next in a scenario requires knowing typical temporal orders of event sequences"
2021.acl-long.555,P18-1050,0,0.14193,"ampling from the model or using it to score sequences. Capitalizing on the success of recent pre-trained encoder-decoder transformers (Lewis et al., 2020; Raffel et al., 2020), our model itself is based on BART, consuming and producing predicate-argument structures rendered in surface order. Gathering large-scale high-quality labeled data with temporal annotations is often expensive and requires specially designed annotation schemes (Pustejovsky et al., 2003a; Cassidy et al., 2014; Ning et al., 2018b; Zhao et al., 2021). Here, we instead turn to a narrative documents corpus, EventsNarratives (Yao and Huang, 2018) and design an automatic method to extract the training data we need. In these documents, discourse order is loosely assumed to reflect temporal order, so events extracted from this text can directly provide training data for our models. This use of automatic annotation allows us to use broad-domain data, giving us a strong domain-independent temporal model (Zhao et al., 2021). To evaluate how well our proposed models capture temporal knowledge and solve the two targeted tasks, we apply them on out-of domain test sets in a zero-shot manner. Specifically, for event ordering, we first extract te"
2021.acl-long.555,2021.adaptnlp-1.20,1,0.773965,"ss a variety of tasks, including our temporal ordering and event infilling tasks, by either sampling from the model or using it to score sequences. Capitalizing on the success of recent pre-trained encoder-decoder transformers (Lewis et al., 2020; Raffel et al., 2020), our model itself is based on BART, consuming and producing predicate-argument structures rendered in surface order. Gathering large-scale high-quality labeled data with temporal annotations is often expensive and requires specially designed annotation schemes (Pustejovsky et al., 2003a; Cassidy et al., 2014; Ning et al., 2018b; Zhao et al., 2021). Here, we instead turn to a narrative documents corpus, EventsNarratives (Yao and Huang, 2018) and design an automatic method to extract the training data we need. In these documents, discourse order is loosely assumed to reflect temporal order, so events extracted from this text can directly provide training data for our models. This use of automatic annotation allows us to use broad-domain data, giving us a strong domain-independent temporal model (Zhao et al., 2021). To evaluate how well our proposed models capture temporal knowledge and solve the two targeted tasks, we apply them on out-o"
2021.acl-long.555,D19-1332,0,0.0911155,", which support inferences about events without explicitly materializing a discrete schema library. The target tasks in this work are directly motivated by downstream applications of schema learning. Text generation tasks like story completion rely on understanding what makes narratives plausible and what events might be likely to happen before, after, and between other events (Jain et al., 2017; Yao et al., 2019), motivating our event infilling task. Answering questions about causes, effects, or what might happen next in a scenario requires knowing typical temporal orders of event sequences (Zhou et al., 2019, 2020; Ning et al., 2020), motivating our temporal ordering task. 7142 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 7142–7157 August 1–6, 2021. ©2021 Association for Computational Linguistics Prior work has not combined traditional event cooccurrence with event temporality as we do. We propose a conditional generation model to tackle temporal event ordering and event infilling, and train it as a denoising autoencoder over outof-context temporal event sequences. As shown"
2021.acl-short.76,D13-1178,1,0.803289,"ly effective strategy for improving event language models by perturbing event sequences so we can relax model dependence on text order. Despite generating completely synthetic event orderings, we show that this technique improves the performance of the event language models on both applications and outof-domain events data. 1 Figure 1: Example of an event schema for which the discourse order is different from the temporal order. Introduction Event-level language models (LMs) provide a way to reason about events, and to approximate schematic and script-like knowledge (Schank and Abelson, 1977; Balasubramanian et al., 2013; Nguyen et al., 2015) about them (Modi and Titov, 2014; Pichotta and Mooney, 2016; Weber et al., 2018). These models aim to learn high-level representations of complex events (e.g., an arrest) and possibly their entity roles from raw text (e.g., a suspect). However, a major limitation is their reliance on the discourse order of event mentions when training the LM. Although powerful, these event LMs capture information we don’t want in true world knowledge. For instance, a script of events may be weakly ordered in real life, but the system instead learns to strongly rely on the text order in w"
2021.acl-short.76,P09-1068,1,0.665532,"events appeared in text (Manshadi et al., 2008). However, relying on discourse order may not be necessary and can potentially limit generalization of event LMs. For some event related tasks such as schema learning (Weber et al., 2018), the discourse order is not directly relevant. For other tasks such as event ordering (Pustejovsky et al., 2003; Chambers et al., 2014; Wang et al., 2018), temporal or logical order of events is most critical – discourse order, at best, is a noisy proxy. In fact, the first systems for schema learning were noticeably not language models (Mooney and DeJong, 1985; Chambers and Jurafsky, 2009, 2011). We introduce three simple perturbation techniques shown in Figure 2 that relax the reliance on discourse sequences. 2.1 Event Permutation One way to reduce reliance on discourse order is to expose the model to random permutations of the input sequences, as shown in Figure 2. Using all possible permutations of a sequence is impractical, so we introduce three specific shuffles that force the model to pay attention to long-term dependencies and avoid the over-reliance on local dependencies/order: • Reversed order: given a set of events as ABCD, the reverse of the sequence is created as D"
2021.acl-short.76,P11-1098,1,0.822527,"Missing"
2021.acl-short.76,K16-1008,0,0.0202393,"tions of the reverse order of the original sequence. The new sequence is: CADB Figure 2: Sequence perturbations strategies. 2 These shuffle patterns were selected to minimize the chance of repetition across permutations. Perturbing Discourse Sequences Event language modeling tasks are typically defined over sequences of events as they appear in text. The events can be represented either as a sequence of words annotated with predicateargument structure (e.g., semantic roles (Pichotta and Mooney, 2016), Open IE tuples (Weber et al., 2018; Rudinger et al., 2015) or with compositional embeddings (Modi, 2016). Generative models are trained to predict subsequent events in a sequence conditioning on previously observed events. Naturally, these models learn the order in which events appeared in text (Manshadi et al., 2008). However, relying on discourse order may not be necessary and can potentially limit generalization of event LMs. For some event related tasks such as schema learning (Weber et al., 2018), the discourse order is not directly relevant. For other tasks such as event ordering (Pustejovsky et al., 2003; Chambers et al., 2014; Wang et al., 2018), temporal or logical order of events is mo"
2021.acl-short.76,W14-1606,0,0.0146415,"rturbing event sequences so we can relax model dependence on text order. Despite generating completely synthetic event orderings, we show that this technique improves the performance of the event language models on both applications and outof-domain events data. 1 Figure 1: Example of an event schema for which the discourse order is different from the temporal order. Introduction Event-level language models (LMs) provide a way to reason about events, and to approximate schematic and script-like knowledge (Schank and Abelson, 1977; Balasubramanian et al., 2013; Nguyen et al., 2015) about them (Modi and Titov, 2014; Pichotta and Mooney, 2016; Weber et al., 2018). These models aim to learn high-level representations of complex events (e.g., an arrest) and possibly their entity roles from raw text (e.g., a suspect). However, a major limitation is their reliance on the discourse order of event mentions when training the LM. Although powerful, these event LMs capture information we don’t want in true world knowledge. For instance, a script of events may be weakly ordered in real life, but the system instead learns to strongly rely on the text order in which the events were described. Figure 1 shows an examp"
2021.acl-short.76,1985.tmi-1.17,0,0.119368,"learn the order in which events appeared in text (Manshadi et al., 2008). However, relying on discourse order may not be necessary and can potentially limit generalization of event LMs. For some event related tasks such as schema learning (Weber et al., 2018), the discourse order is not directly relevant. For other tasks such as event ordering (Pustejovsky et al., 2003; Chambers et al., 2014; Wang et al., 2018), temporal or logical order of events is most critical – discourse order, at best, is a noisy proxy. In fact, the first systems for schema learning were noticeably not language models (Mooney and DeJong, 1985; Chambers and Jurafsky, 2009, 2011). We introduce three simple perturbation techniques shown in Figure 2 that relax the reliance on discourse sequences. 2.1 Event Permutation One way to reduce reliance on discourse order is to expose the model to random permutations of the input sequences, as shown in Figure 2. Using all possible permutations of a sequence is impractical, so we introduce three specific shuffles that force the model to pay attention to long-term dependencies and avoid the over-reliance on local dependencies/order: • Reversed order: given a set of events as ABCD, the reverse of"
2021.acl-short.76,P15-1019,0,0.0481132,"Missing"
2021.acl-short.76,D14-1162,0,0.0870909,"Missing"
2021.acl-short.76,D15-1195,0,0.0543341,"Missing"
2021.acl-short.76,D12-1048,0,0.0118459,"ut procedures. 2.3 Event Masking When dropping events, we can provide additional information to the model about where events were dropped. This forces the model to capture longerterm dependencies among events in the sequence. We randomly select a number of event tuples and replace their tokens with a &lt;mask&gt; token (Masking in Figure 2). For each sequence in the training set, we generate its masked sequences with each having a fixed proportion of its events masked. 3 Experimental Setup Data We train event language models on the Annotated NYT corpus using Open IE event tuples extracted by Ollie (Schmitz et al., 2012). The dataset contains a total of around 1.8 million articles. After preprocessing steps, 1,467,366 articles are used as the training set, 6k articles as test set and 4k articles as the dev set. Each event is a 4-tuple (v,s,o,p) containing the verb, subject, object and preposition. We follow the same preprocessing steps outlined in Weber et al. (2018) to create event sequences. The components of the events (the verb, subject, etc.) are all individual tokens, and are treated like normal text. For example, the events (truck packed with explosives), (police arrested suspect), would be given to th"
2021.acl-short.76,D18-1175,1,0.841428,"r et al., 2015) or with compositional embeddings (Modi, 2016). Generative models are trained to predict subsequent events in a sequence conditioning on previously observed events. Naturally, these models learn the order in which events appeared in text (Manshadi et al., 2008). However, relying on discourse order may not be necessary and can potentially limit generalization of event LMs. For some event related tasks such as schema learning (Weber et al., 2018), the discourse order is not directly relevant. For other tasks such as event ordering (Pustejovsky et al., 2003; Chambers et al., 2014; Wang et al., 2018), temporal or logical order of events is most critical – discourse order, at best, is a noisy proxy. In fact, the first systems for schema learning were noticeably not language models (Mooney and DeJong, 1985; Chambers and Jurafsky, 2009, 2011). We introduce three simple perturbation techniques shown in Figure 2 that relax the reliance on discourse sequences. 2.1 Event Permutation One way to reduce reliance on discourse order is to expose the model to random permutations of the input sequences, as shown in Figure 2. Using all possible permutations of a sequence is impractical, so we introduce"
2021.acl-short.76,D18-1413,1,0.693357,"pendence on text order. Despite generating completely synthetic event orderings, we show that this technique improves the performance of the event language models on both applications and outof-domain events data. 1 Figure 1: Example of an event schema for which the discourse order is different from the temporal order. Introduction Event-level language models (LMs) provide a way to reason about events, and to approximate schematic and script-like knowledge (Schank and Abelson, 1977; Balasubramanian et al., 2013; Nguyen et al., 2015) about them (Modi and Titov, 2014; Pichotta and Mooney, 2016; Weber et al., 2018). These models aim to learn high-level representations of complex events (e.g., an arrest) and possibly their entity roles from raw text (e.g., a suspect). However, a major limitation is their reliance on the discourse order of event mentions when training the LM. Although powerful, these event LMs capture information we don’t want in true world knowledge. For instance, a script of events may be weakly ordered in real life, but the system instead learns to strongly rely on the text order in which the events were described. Figure 1 shows an example where discourse and actual temporal order are"
2021.acl-short.76,2020.emnlp-demos.6,0,0.0499494,"Missing"
2021.acl-short.76,P18-1050,0,0.137033,"equences in the training data to relax the model’s dependence on discourse order. By considering the next event based on shuffled sequences of events, we encourage the model to treat the input more as a set of events rather than strictly as a discourse sequence. Surprisingly, despite our disruption of discourse order, experiments show how perturbations can improve event language modeling of text, particularly when evaluating the model on other domains which present events in different orders (e.g., novels or blogs present data in more of a “narrative” fashion than news datasets common in NLP (Yao and Huang, 2018)). Our experiments evaluate accuracy on the Inverse Narrative Cloze task on in-domain newswire, as well as out-domain novels and blogs1 . 1 The code and data is available at https://github. com/StonyBrookNLP/elm-perturbations 599 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 599–604 August 1–6, 2021. ©2021 Association for Computational Linguistics • Concatenation of events in the odd positions followed by the even positions of the sequence: the permuted seq"
2021.adaptnlp-1.20,P14-2082,0,0.024178,"orcing a model trained on this data to learn other signals. We demonstrate that a pre-trained Transformer model is able to transfer from the automatically labeled examples to humanannotated benchmarks in both zero-shot and few-shot settings, and that the masking scheme is important in improving generalization.1 1 Introduction Temporal relation extraction has largely focused on identifying pairwise relationships between events in text. Past work on annotating temporal relations has struggled to devise annotations schemes which are both comprehensive and easy to judge (Pustejovsky et al., 2003; Cassidy et al., 2014). However, even simplified annotation schemes designed for crowdsourcing (Ning et al., 2018b; Vashishtha et al., 2019) can struggle to acquire high-accuracy judgments about nebulous phenomena, leading to a scarcity of high-quality labeled data. Compared to tasks like syntactic parsing (Bies et al., 2012) or natural language inference (Williams et al., 2018), there are thus fewer resources for temporal relation extraction in other domains. In this work, we present a method of automatically gathering distantly-labeled temporal relation examples. Unlike traditional distant supervision methods (Mi"
2021.adaptnlp-1.20,W04-3205,0,0.0659112,"focused on reporting verbs, but the distant data has a much flatter distribution. BeforeAfter features more light verbs whereas DistantTimex features events with more complex semantics; possibly the model can learn more regular and meaningful patterns from such data, or relevant cues from a more similar event distribution (than found in BeforeAfter). We present event-label tuples in Appendix D. 5 Related Work There is little direct prior work on using this kind of distant supervision for temporal relation extraction. Past work has studied automatic extraction of typical inter-event orderings (Chklovski and Pantel, 2004; Ning et al., 2018a; Yao and Huang, 2018) to aid downstream temporal tasks, but these approaches represent events as single words (predicates) taken out of context, so the knowledge they can capture is limited. The commonsense acquisition method of Zhou et al. (2020) learns more sophisticated information, but more about unary properties of events (typical time, duration) rather than relational knowledge. Lin et al. (2020) achieve a somewhat similar goal, but make a strong assumption about narrative-structured corpora and do not evaluate on in-context temporal relation extraction. Our techniqu"
2021.adaptnlp-1.20,P19-1442,0,0.0281005,"hambers et al., 2014). By then masking the explicit temporal indicators, a model trained on these examples can no longer learn trivial timex-based rules, but must instead attend to more general temporal context cues. We show that a pre-trained model fine-tuned on this data learns general, implicit cues that transfer more broadly to human-annotated benchmarks. This observation follows a trend of recent work showing pre-trained models’ ability to generalize from synthetic data to natural data (Xu et al., 2020; Marzoev et al., 2020). We implement our approach with pre-trained Transformer models (Devlin et al., 2019; Liu et al., 2019; Clark et al., 2020) similar to a state-of-theart temporal relation extraction model from the literature (Han et al., 2019). Our model is able to effectively transfer from a distantly-labeled dataset to the MATRES benchmark (Ning et al., 2018b) when used to supplement a small number of indomain or out-of-domain samples. 2 Classification Model Our base classification model consists of a pretrained Transformer (Vaswani et al., 2017) model with an appended linear classification layer, represented in Figure 1. For the majority of our experiments, we use RoBERTa (Liu et al., 2019"
2021.adaptnlp-1.20,N18-1077,0,0.609669,"ransformer model is able to transfer from the automatically labeled examples to humanannotated benchmarks in both zero-shot and few-shot settings, and that the masking scheme is important in improving generalization.1 1 Introduction Temporal relation extraction has largely focused on identifying pairwise relationships between events in text. Past work on annotating temporal relations has struggled to devise annotations schemes which are both comprehensive and easy to judge (Pustejovsky et al., 2003; Cassidy et al., 2014). However, even simplified annotation schemes designed for crowdsourcing (Ning et al., 2018b; Vashishtha et al., 2019) can struggle to acquire high-accuracy judgments about nebulous phenomena, leading to a scarcity of high-quality labeled data. Compared to tasks like syntactic parsing (Bies et al., 2012) or natural language inference (Williams et al., 2018), there are thus fewer resources for temporal relation extraction in other domains. In this work, we present a method of automatically gathering distantly-labeled temporal relation examples. Unlike traditional distant supervision methods (Mintz et al., 2009), we do not rely on a knowledge base, but instead on heuristic cues that 1"
2021.adaptnlp-1.20,P18-1122,0,0.576051,"ransformer model is able to transfer from the automatically labeled examples to humanannotated benchmarks in both zero-shot and few-shot settings, and that the masking scheme is important in improving generalization.1 1 Introduction Temporal relation extraction has largely focused on identifying pairwise relationships between events in text. Past work on annotating temporal relations has struggled to devise annotations schemes which are both comprehensive and easy to judge (Pustejovsky et al., 2003; Cassidy et al., 2014). However, even simplified annotation schemes designed for crowdsourcing (Ning et al., 2018b; Vashishtha et al., 2019) can struggle to acquire high-accuracy judgments about nebulous phenomena, leading to a scarcity of high-quality labeled data. Compared to tasks like syntactic parsing (Bies et al., 2012) or natural language inference (Williams et al., 2018), there are thus fewer resources for temporal relation extraction in other domains. In this work, we present a method of automatically gathering distantly-labeled temporal relation examples. Unlike traditional distant supervision methods (Mintz et al., 2009), we do not rely on a knowledge base, but instead on heuristic cues that 1"
2021.adaptnlp-1.20,P19-1280,0,0.195624,"Missing"
2021.eacl-srw.8,D17-1215,0,0.240329,"at are more likely to be correct in adversarial conditions. The impact of this is two-fold. First, our proposed method is model agnostic in that it can be applied post-hoc to any QA model that predicts probabilities of answer spans, without any retraining. Second but most important, we demonstrate that even this simple named entity based questionanswer matching technique can be surprisingly useful. We show that our method outperforms state-of-the-art but more complex adversarial defenses with both BiDAF (Seo et al., 2016) and BERT (Devlin et al., 2019) on two standard adversarial QA datasets (Jia and Liang, 2017; Wallace et al., 2019). The fact that such a straightforward technique works well calls into question how reliable current datasets are for evaluating actual robustness of QA models. While numerous methods have been proposed as defenses against adversarial examples in question answering (QA), these techniques are often model specific, require retraining of the model, and give only marginal improvements in performance over vanilla models. In this work, we present a simple model-agnostic approach to this problem that can be applied directly to any QA model without any retraining. Our method emp"
2021.eacl-srw.8,N19-1405,1,0.846511,"est within the text to look for the answer. On the contrary, we use a reranking mechanism that allows our QA model to ignore distractors in adversarial QA and can also provide model- and task-agnostic behavior unlike the commonly used learning-based (re)ranking mechanisms. 3 Approach Neural QA models are usually trained in a supervised fashion on labeled examples of contexts, questions, and answers to predict answer spans; we represent these as (s, e) tuples, where s represents the sentence and e the candidate span. Prior work (Lewis and Fan, 2019; Mudrakarta et al., 2018; Yeh and Chen, 2019; Chen and Durrett, 2019) has noted that the end-to-end paradigm can overfit superficial biases in the data causing learning to stop when simple correlations are sufficient for the model to answer a question confidently. By explicitly enforcing content relevance between the predicted answer-containing sentence and the question, we can combat this poor generalization. Specifically, we explicitly score the candidate sentences as per the word-level overlap in named entities common to both the question and a senIn yet another related line of research, (Chen et al., 2016; Kaushik and Lipton, 2018) reveal the simplistic nat"
2021.eacl-srw.8,P17-1147,0,0.0231354,"be applied directly to any QA model without any retraining. Our method employs an explicit answer candidate reranking mechanism that scores candidate answers on the basis of their content overlap with the question before making the final prediction. Combined with a strong base QA model, our method outperforms state-of-theart defense techniques, calling into question how well these techniques are actually doing and strong these adversarial testbeds are. 1 Introduction As reading comprehension datasets (Richardson et al., 2013; Weston et al., 2015; Hermann et al., 2015a; Rajpurkar et al., 2016; Joshi et al., 2017) and models (Sukhbaatar et al., 2015; Seo et al., 2016; Devlin et al., 2019) have advanced, QA research has increasingly focused on out-ofdistribution generalization (Khashabi et al., 2020; Talmor and Berant, 2019) and robustness. Jia and Liang (2017) and Wallace et al. (2019) show that appending unrelated distractors to contexts can easily confuse a deep QA model, calling into question the effectiveness of these models. Although these attacks do not necessarily reflect a real-world threat model, they serve as an additional testbed for generalization: models that perform better against such ad"
2021.eacl-srw.8,N19-1423,0,0.186609,"tities are shared with the question, we can extract answers that are more likely to be correct in adversarial conditions. The impact of this is two-fold. First, our proposed method is model agnostic in that it can be applied post-hoc to any QA model that predicts probabilities of answer spans, without any retraining. Second but most important, we demonstrate that even this simple named entity based questionanswer matching technique can be surprisingly useful. We show that our method outperforms state-of-the-art but more complex adversarial defenses with both BiDAF (Seo et al., 2016) and BERT (Devlin et al., 2019) on two standard adversarial QA datasets (Jia and Liang, 2017; Wallace et al., 2019). The fact that such a straightforward technique works well calls into question how reliable current datasets are for evaluating actual robustness of QA models. While numerous methods have been proposed as defenses against adversarial examples in question answering (QA), these techniques are often model specific, require retraining of the model, and give only marginal improvements in performance over vanilla models. In this work, we present a simple model-agnostic approach to this problem that can be applied di"
2021.eacl-srw.8,P19-1611,0,0.0141525,"rt performance, and the answers are often localized in the last few lines, even in very long passages, thus possibly allowing models to achieve very strong performance through learning trivial cues. Although we also question the efficacy of well-known adversarial QA datasets in this work, our core focus is on exposing certain issues specifically with the design of the adversarial distractors rather than the underlying datasets. Methods for (re)ranking of candidate passages/answers have often been explored in the context of information retrieval (Severyn and Moschitti, 2015), content-based QA (Kratzwald et al., 2019) and open-domain QA (Wang et al., 2018; Lee et al., 2018). Similar to our approach, these methods also exploit some measure of coverage of the query by the candidate answers or their supporting passages to decide the ranks. However, the main motive behind ranking in such cases is usually to narrow down the area of interest within the text to look for the answer. On the contrary, we use a reranking mechanism that allows our QA model to ignore distractors in adversarial QA and can also provide model- and task-agnostic behavior unlike the commonly used learning-based (re)ranking mechanisms. 3 App"
2021.eacl-srw.8,D18-1053,0,0.0178033,"few lines, even in very long passages, thus possibly allowing models to achieve very strong performance through learning trivial cues. Although we also question the efficacy of well-known adversarial QA datasets in this work, our core focus is on exposing certain issues specifically with the design of the adversarial distractors rather than the underlying datasets. Methods for (re)ranking of candidate passages/answers have often been explored in the context of information retrieval (Severyn and Moschitti, 2015), content-based QA (Kratzwald et al., 2019) and open-domain QA (Wang et al., 2018; Lee et al., 2018). Similar to our approach, these methods also exploit some measure of coverage of the query by the candidate answers or their supporting passages to decide the ranks. However, the main motive behind ranking in such cases is usually to narrow down the area of interest within the text to look for the answer. On the contrary, we use a reranking mechanism that allows our QA model to ignore distractors in adversarial QA and can also provide model- and task-agnostic behavior unlike the commonly used learning-based (re)ranking mechanisms. 3 Approach Neural QA models are usually trained in a supervise"
2021.eacl-srw.8,D19-5826,0,0.0180447,"the effectiveness of these models. Although these attacks do not necessarily reflect a real-world threat model, they serve as an additional testbed for generalization: models that perform better against such adversaries might be expected to generalize better in other ways, such as on contrastive examples (Gardner et al., 2020). In this paper, we propose a simple method for adversarial QA that explicitly reranks candidate 2 Related Work Over the years, various methods have been proposed for robustness in adversarial QA, the most prominent ones being adversarial training (Wang and Bansal, 2018; Lee et al., 2019; Yang et al., 2019b), data augmentation (Welbl et al., 2020) and posterior regularization (Zhou et al., 2019). Among these, we compare our method only with techniques that train on clean SQuAD (Wu et al., 2019; Yeh and Chen, 2019) for fairness. Wu et al. (2019) use a syntax-driven encoder to model the syntactic match between a question and an answer. Yeh and Chen (2019) use a prior approach (Hjelm et al., 2019) to maximize mutual information among contexts, questions, and answers to avoid overfitting to surface cues. In contrast, our technique is more 50 Proceedings of the 16th Conference of"
2021.eacl-srw.8,P18-1176,0,0.0177121,"s is usually to narrow down the area of interest within the text to look for the answer. On the contrary, we use a reranking mechanism that allows our QA model to ignore distractors in adversarial QA and can also provide model- and task-agnostic behavior unlike the commonly used learning-based (re)ranking mechanisms. 3 Approach Neural QA models are usually trained in a supervised fashion on labeled examples of contexts, questions, and answers to predict answer spans; we represent these as (s, e) tuples, where s represents the sentence and e the candidate span. Prior work (Lewis and Fan, 2019; Mudrakarta et al., 2018; Yeh and Chen, 2019; Chen and Durrett, 2019) has noted that the end-to-end paradigm can overfit superficial biases in the data causing learning to stop when simple correlations are sufficient for the model to answer a question confidently. By explicitly enforcing content relevance between the predicted answer-containing sentence and the question, we can combat this poor generalization. Specifically, we explicitly score the candidate sentences as per the word-level overlap in named entities common to both the question and a senIn yet another related line of research, (Chen et al., 2016; Kaushi"
2021.eacl-srw.8,P82-1020,0,0.785831,"Missing"
2021.eacl-srw.8,D19-1221,0,0.323426,"be correct in adversarial conditions. The impact of this is two-fold. First, our proposed method is model agnostic in that it can be applied post-hoc to any QA model that predicts probabilities of answer spans, without any retraining. Second but most important, we demonstrate that even this simple named entity based questionanswer matching technique can be surprisingly useful. We show that our method outperforms state-of-the-art but more complex adversarial defenses with both BiDAF (Seo et al., 2016) and BERT (Devlin et al., 2019) on two standard adversarial QA datasets (Jia and Liang, 2017; Wallace et al., 2019). The fact that such a straightforward technique works well calls into question how reliable current datasets are for evaluating actual robustness of QA models. While numerous methods have been proposed as defenses against adversarial examples in question answering (QA), these techniques are often model specific, require retraining of the model, and give only marginal improvements in performance over vanilla models. In this work, we present a simple model-agnostic approach to this problem that can be applied directly to any QA model without any retraining. Our method employs an explicit answer"
2021.eacl-srw.8,D14-1162,0,0.085183,"Missing"
2021.eacl-srw.8,D16-1264,0,0.340162,"o this problem that can be applied directly to any QA model without any retraining. Our method employs an explicit answer candidate reranking mechanism that scores candidate answers on the basis of their content overlap with the question before making the final prediction. Combined with a strong base QA model, our method outperforms state-of-theart defense techniques, calling into question how well these techniques are actually doing and strong these adversarial testbeds are. 1 Introduction As reading comprehension datasets (Richardson et al., 2013; Weston et al., 2015; Hermann et al., 2015a; Rajpurkar et al., 2016; Joshi et al., 2017) and models (Sukhbaatar et al., 2015; Seo et al., 2016; Devlin et al., 2019) have advanced, QA research has increasingly focused on out-ofdistribution generalization (Khashabi et al., 2020; Talmor and Berant, 2019) and robustness. Jia and Liang (2017) and Wallace et al. (2019) show that appending unrelated distractors to contexts can easily confuse a deep QA model, calling into question the effectiveness of these models. Although these attacks do not necessarily reflect a real-world threat model, they serve as an additional testbed for generalization: models that perform b"
2021.eacl-srw.8,D13-1020,0,0.0371022,"a models. In this work, we present a simple model-agnostic approach to this problem that can be applied directly to any QA model without any retraining. Our method employs an explicit answer candidate reranking mechanism that scores candidate answers on the basis of their content overlap with the question before making the final prediction. Combined with a strong base QA model, our method outperforms state-of-theart defense techniques, calling into question how well these techniques are actually doing and strong these adversarial testbeds are. 1 Introduction As reading comprehension datasets (Richardson et al., 2013; Weston et al., 2015; Hermann et al., 2015a; Rajpurkar et al., 2016; Joshi et al., 2017) and models (Sukhbaatar et al., 2015; Seo et al., 2016; Devlin et al., 2019) have advanced, QA research has increasingly focused on out-ofdistribution generalization (Khashabi et al., 2020; Talmor and Berant, 2019) and robustness. Jia and Liang (2017) and Wallace et al. (2019) show that appending unrelated distractors to contexts can easily confuse a deep QA model, calling into question the effectiveness of these models. Although these attacks do not necessarily reflect a real-world threat model, they serv"
2021.eacl-srw.8,N18-2091,0,0.0177133,"calling into question the effectiveness of these models. Although these attacks do not necessarily reflect a real-world threat model, they serve as an additional testbed for generalization: models that perform better against such adversaries might be expected to generalize better in other ways, such as on contrastive examples (Gardner et al., 2020). In this paper, we propose a simple method for adversarial QA that explicitly reranks candidate 2 Related Work Over the years, various methods have been proposed for robustness in adversarial QA, the most prominent ones being adversarial training (Wang and Bansal, 2018; Lee et al., 2019; Yang et al., 2019b), data augmentation (Welbl et al., 2020) and posterior regularization (Zhou et al., 2019). Among these, we compare our method only with techniques that train on clean SQuAD (Wu et al., 2019; Yeh and Chen, 2019) for fairness. Wu et al. (2019) use a syntax-driven encoder to model the syntactic match between a question and an answer. Yeh and Chen (2019) use a prior approach (Hjelm et al., 2019) to maximize mutual information among contexts, questions, and answers to avoid overfitting to surface cues. In contrast, our technique is more 50 Proceedings of the 1"
2021.eacl-srw.8,2020.findings-emnlp.103,0,0.0317883,"do not necessarily reflect a real-world threat model, they serve as an additional testbed for generalization: models that perform better against such adversaries might be expected to generalize better in other ways, such as on contrastive examples (Gardner et al., 2020). In this paper, we propose a simple method for adversarial QA that explicitly reranks candidate 2 Related Work Over the years, various methods have been proposed for robustness in adversarial QA, the most prominent ones being adversarial training (Wang and Bansal, 2018; Lee et al., 2019; Yang et al., 2019b), data augmentation (Welbl et al., 2020) and posterior regularization (Zhou et al., 2019). Among these, we compare our method only with techniques that train on clean SQuAD (Wu et al., 2019; Yeh and Chen, 2019) for fairness. Wu et al. (2019) use a syntax-driven encoder to model the syntactic match between a question and an answer. Yeh and Chen (2019) use a prior approach (Hjelm et al., 2019) to maximize mutual information among contexts, questions, and answers to avoid overfitting to surface cues. In contrast, our technique is more 50 Proceedings of the 16th Conference of the European Chapter of the Associationfor Computational Ling"
2021.eacl-srw.8,Q18-1021,0,0.0196763,"50–57 April 19 - 23, 2021. ©2021 Association for Computational Linguistics Figure 1: Our model agnostic answer reranking system (MAARS). Given each answer option (right column), we extract named entities and compare them to named entities in the question. The overlap is used as a reranking feature to choose the final answer. The ground truth answer containing sentence is highlighted in green, the ground truth answer is boxed and the distractor sentence is highlighted in red. closely related to retrieval-based methods for opendomain QA (Chen et al., 2017; Yang et al., 2019a) and multi-hop QA (Welbl et al., 2018; De Cao et al., 2019): we show that shallow matching can improve the reliability of deep models against adversaries in addition to these more complex settings. tensive study with multiple well-known QA benchmarks to show several troubling trends: basic model ablations, such as making the input questionor passage-only, can beat the state-of-the-art performance, and the answers are often localized in the last few lines, even in very long passages, thus possibly allowing models to achieve very strong performance through learning trivial cues. Although we also question the efficacy of well-known"
2021.eacl-srw.8,D19-5807,0,0.218214,"ersaries might be expected to generalize better in other ways, such as on contrastive examples (Gardner et al., 2020). In this paper, we propose a simple method for adversarial QA that explicitly reranks candidate 2 Related Work Over the years, various methods have been proposed for robustness in adversarial QA, the most prominent ones being adversarial training (Wang and Bansal, 2018; Lee et al., 2019; Yang et al., 2019b), data augmentation (Welbl et al., 2020) and posterior regularization (Zhou et al., 2019). Among these, we compare our method only with techniques that train on clean SQuAD (Wu et al., 2019; Yeh and Chen, 2019) for fairness. Wu et al. (2019) use a syntax-driven encoder to model the syntactic match between a question and an answer. Yeh and Chen (2019) use a prior approach (Hjelm et al., 2019) to maximize mutual information among contexts, questions, and answers to avoid overfitting to surface cues. In contrast, our technique is more 50 Proceedings of the 16th Conference of the European Chapter of the Associationfor Computational Linguistics: Student Research Workshop, pages 50–57 April 19 - 23, 2021. ©2021 Association for Computational Linguistics Figure 1: Our model agnostic ans"
2021.eacl-srw.8,P19-1485,0,0.0151947,"tion before making the final prediction. Combined with a strong base QA model, our method outperforms state-of-theart defense techniques, calling into question how well these techniques are actually doing and strong these adversarial testbeds are. 1 Introduction As reading comprehension datasets (Richardson et al., 2013; Weston et al., 2015; Hermann et al., 2015a; Rajpurkar et al., 2016; Joshi et al., 2017) and models (Sukhbaatar et al., 2015; Seo et al., 2016; Devlin et al., 2019) have advanced, QA research has increasingly focused on out-ofdistribution generalization (Khashabi et al., 2020; Talmor and Berant, 2019) and robustness. Jia and Liang (2017) and Wallace et al. (2019) show that appending unrelated distractors to contexts can easily confuse a deep QA model, calling into question the effectiveness of these models. Although these attacks do not necessarily reflect a real-world threat model, they serve as an additional testbed for generalization: models that perform better against such adversaries might be expected to generalize better in other ways, such as on contrastive examples (Gardner et al., 2020). In this paper, we propose a simple method for adversarial QA that explicitly reranks candidate"
2021.eacl-srw.8,N19-4013,0,0.0432282,"Missing"
2021.eacl-srw.8,D19-1333,0,0.185864,"expected to generalize better in other ways, such as on contrastive examples (Gardner et al., 2020). In this paper, we propose a simple method for adversarial QA that explicitly reranks candidate 2 Related Work Over the years, various methods have been proposed for robustness in adversarial QA, the most prominent ones being adversarial training (Wang and Bansal, 2018; Lee et al., 2019; Yang et al., 2019b), data augmentation (Welbl et al., 2020) and posterior regularization (Zhou et al., 2019). Among these, we compare our method only with techniques that train on clean SQuAD (Wu et al., 2019; Yeh and Chen, 2019) for fairness. Wu et al. (2019) use a syntax-driven encoder to model the syntactic match between a question and an answer. Yeh and Chen (2019) use a prior approach (Hjelm et al., 2019) to maximize mutual information among contexts, questions, and answers to avoid overfitting to surface cues. In contrast, our technique is more 50 Proceedings of the 16th Conference of the European Chapter of the Associationfor Computational Linguistics: Student Research Workshop, pages 50–57 April 19 - 23, 2021. ©2021 Association for Computational Linguistics Figure 1: Our model agnostic answer reranking system"
2021.emnlp-main.447,2020.emnlp-main.711,0,0.0336576,"Missing"
2021.emnlp-main.447,2020.acl-srw.16,0,0.017984,"to train the linear model: L IME computes the weights heuristically, whereas S HAP uses a procedure based on Shapley values. Integrated Gradient (I NT G RAD) (Sundararajan et al., 2017) computes an attribution for each token by integrating the gradients of the prediction with respect to the token embeddings over the path from a baseline input (typically mask or pad tokens) towards the designated input. Although a common technique, recent work has raised concern about the effectiveness of I NT G RAD methods for NLP tasks, as interpolated word embeddings do not correspond to real input values (Harbecke and Alt, 2020; Sanyal and Ren, 2021). Differentiable Mask (D IFF M ASK) (De Cao et al., 2020) learns to mask out a subsets of the input tokens for a given example while maintaining a distribution over answers as close to the original distribution as possible. This mask is learned in 5 A potentially even more powerful format would be a program approximating the model’s behavior, as has been explored in the context of reinforcement learning (Verma et al., 2018; Bastani et al., 2018). However, beyond limited versions of this (Ribeiro et al., 2018), prior work does not show how to effectively build this type o"
2021.emnlp-main.447,2020.acl-main.491,0,0.35553,"he explanation, we expect predictions to change in this way Figure 1: Our methodology. Given a base example, we can formulate a hypothesis about the model’s behavior, like a theory about how the model is using certain tokens. Next, we collect counterfactual examples that modify these tokens and profile the actual model behavior. Finally, we assess whether feature attributions suggest behavior consistent with what we observe, verifying whether our attributions actually enable meaningful statements about behavior on counterfactuals. to the computation of the original model (Wu and Mooney, 2019; Hase and Bansal, 2020; Wiegreffe et al., 2021; Jacovi and Goldberg, 2020) and as a result, they can potentially mislead users (Rudin, 2019). Furthermore, attributions do not have a con1 Introduction sistent and meaningful social attribution (Miller, 2019; Jacovi and Goldberg, 2021): that is, when Recent research in interpretability of neural moda user of the system looks at an explanation, they els (Lipton, 2018) has yielded numerous post-hoc do not necessarily draw a valid conclusion from it, explanation methods, including token attribution making it hard to use for downstream tasks. techniques (Ribeiro et al., 2"
2021.emnlp-main.447,2021.ccl-1.108,0,0.0630042,"Missing"
2021.emnlp-main.447,2021.emnlp-main.805,0,0.0376034,"el: L IME computes the weights heuristically, whereas S HAP uses a procedure based on Shapley values. Integrated Gradient (I NT G RAD) (Sundararajan et al., 2017) computes an attribution for each token by integrating the gradients of the prediction with respect to the token embeddings over the path from a baseline input (typically mask or pad tokens) towards the designated input. Although a common technique, recent work has raised concern about the effectiveness of I NT G RAD methods for NLP tasks, as interpolated word embeddings do not correspond to real input values (Harbecke and Alt, 2020; Sanyal and Ren, 2021). Differentiable Mask (D IFF M ASK) (De Cao et al., 2020) learns to mask out a subsets of the input tokens for a given example while maintaining a distribution over answers as close to the original distribution as possible. This mask is learned in 5 A potentially even more powerful format would be a program approximating the model’s behavior, as has been explored in the context of reinforcement learning (Verma et al., 2018; Bastani et al., 2018). However, beyond limited versions of this (Ribeiro et al., 2018), prior work does not show how to effectively build this type of explanation for QA at"
2021.emnlp-main.447,P19-1282,0,0.023605,"s nonadditive feature interaction. Similar to D IFF M ASK, A RCHIP is also implicitly based on unrealistic counterfactuals which remove tokens. Given a subset of tokens, A RCHIP defines the contribution of the interaction by the prediction obtained from masking out all the other tokens, only leaving a very small fraction of the input. Applying this definition to a complex task like QA can result in a nonsensical input. Attention Attribution (ATATTR) (Hao et al., 2021) uses attention specifically to derive pairwise explanations. However, it avoids the pitfalls of directly inspecting attention (Serrano and Smith, 2019; Wiegreffe and Pinter, 2019) by running an integrated gradients-like procedure over all the attention links within transformers, yielding attribution scores for each link. The attribution scores directly reflect the attribution of the particular attention links, making this model able to describe pairwise interactions. Concretely, define the h-head attention matrix over input D with n tokens as A = [A1 , ..., Al ], where Ai ∈ Rh×n×n is the attention scores for each layer. We can obtain the attribution score for each entry in the attention matrix A as: 4.3 … Feature Interaction-Based ATTR(A) ="
2021.emnlp-main.447,P19-1416,0,0.119208,"put points as in I NT G RAD, all of which “true” set of realistic counterfactuals is highly domain-specific, but nevertheless, a good explanation technique should work well on a range of counterfactuals like those considered here. violate assumptions about the input distribution. In RC, masking part of the question often makes it nonsensical and we may not have strong expectations about our model’s behavior in this case.3 Focusing on realistic counterfactuals, by contrast, illuminates fundamental problems with our RC models’ reasoning capabilities (Jia and Liang, 2017; Chen and Durrett, 2019; Min et al., 2019; Jiang and Bansal, 2019). This is the same motivation as that behind contrast sets (Gardner et al., 2020), but our work focuses on benchmarking explanations, not models themselves. 3 Behavior on Counterfactuals We seek to formalize the reasoning we undertook in Figure 2. Using the model’s explanation on a base data point, can we predict the model’s behavior on the perturbed instances of that point? Definitions Given an original example D0 (e.g., the top example in Figure 2), we construct a set of perturbations {D1 , ..., Dk } (e.g., the three counterfactual examples in Figure 2), which togeth"
2021.emnlp-main.447,W19-4807,0,0.012837,", the model always predicts “yes” for every example in the neighborhood, casting doubt on whether the model is following the right reasoning process. Although the pairwise attribution seemed at first glance much less plausible than that generated by the other techniques, it was actually better from the perspective of faithfully simulating the model’s behavior on these examples. Our main assumption in this work can be stated as follows: an explanation should describe model behavior with respect to realistic counterfactuals. Past work has evaluated along plausibility criteria (Lei et al., 2016; Strout et al., 2019; Thorne et al., 2019), but as we see from this example, faithful explanations (Subramanian et al., 2020; Jacovi and Goldberg, 2020, 2021) are better aligned with our goal of simulatability. We argue that a good explanation is one that aligns with the model’s high-level behavior, from which we can understand how the model generalizes to new data. How to interpret behavior from explanations is still an open question, but we take initial steps in this work with techniques based on assessing the attribution “mass” on perturbed tokens. Discussion: Realistic Counterfactuals Many counterfactual modi"
2021.emnlp-main.447,N18-1097,0,0.0264075,"avenue for future investigation. By first thinking about what kind of counterfactuals and what kind of behaviours we want to explain, we can motivate the development of new explanation techniques to serve these needs. 6 Related Work to predict behavior on contrast sets similar to this work; however, this cannot be done heuristically and larger datasets are needed to explore this. Other work considering how to evaluate explanations is primarily based on how explanations can assist humans in predicting model decisions for a given example (Doshi-Velez and Kim, 2017; Chandrasekaran et al., 2018; Nguyen, 2018; Hase and Bansal, 2020); We are the first to consider building contrast sets for this. Similar ideas have been used in other contexts (Kaushik et al., 2020; Gardner et al., 2020) but our work focuses on evaluation of explanations rather than general model evaluation. 7 Conclusion We have presented a new methodology using explanations to understand model behavior on realistic counterfactuals. We show explanations can indeed be connected to model behavior, and therefore we can compare explanations to understand which ones truly give us actionable insights about what our models are doing. We hav"
2021.emnlp-main.447,2020.acl-main.495,0,0.0120648,"he model is following the right reasoning process. Although the pairwise attribution seemed at first glance much less plausible than that generated by the other techniques, it was actually better from the perspective of faithfully simulating the model’s behavior on these examples. Our main assumption in this work can be stated as follows: an explanation should describe model behavior with respect to realistic counterfactuals. Past work has evaluated along plausibility criteria (Lei et al., 2016; Strout et al., 2019; Thorne et al., 2019), but as we see from this example, faithful explanations (Subramanian et al., 2020; Jacovi and Goldberg, 2020, 2021) are better aligned with our goal of simulatability. We argue that a good explanation is one that aligns with the model’s high-level behavior, from which we can understand how the model generalizes to new data. How to interpret behavior from explanations is still an open question, but we take initial steps in this work with techniques based on assessing the attribution “mass” on perturbed tokens. Discussion: Realistic Counterfactuals Many counterfactual modifications are possible: past work has looked at injecting non-meaningful triggers (Wallace et al., 2019)"
2021.emnlp-main.447,N19-1101,0,0.114831,"butions 2017; Guan et al., 2019; De Cao et al., 2020). Atmake faithful and meaningful statements about tributions are a flexible explanation format and model behavior? In this work, we show how to use can be applied to many domains, including sencounterfactual examples to evaluate attributions’ timent analysis (Guan et al., 2019; De Cao et al., ability to reveal the high-level behavior of mod2020), visual recognition (Simonyan et al., 2013), els. That is, rather than a vague statement like and natural language inference (Camburu et al., “this word was important,” we want attributions to 2018; Thorne et al., 2019). However, it is hard give concrete, testable conclusions like “the model to evaluate whether these explanations are faithful compared these two words to reach its decision;” 1 Code and data: https://github.com/xiye17/EvalQAExpl this statement can be evaluated for faithfulness and 5496 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5496–5512 c November 7–11, 2021. 2021 Association for Computational Linguistics it helps a user make important inferences about how the system behaves. We approach this evaluation from a perspective of simulatability (H"
2021.emnlp-main.447,W19-4812,0,0.0247232,"vior simulate Given the explanation, we expect predictions to change in this way Figure 1: Our methodology. Given a base example, we can formulate a hypothesis about the model’s behavior, like a theory about how the model is using certain tokens. Next, we collect counterfactual examples that modify these tokens and profile the actual model behavior. Finally, we assess whether feature attributions suggest behavior consistent with what we observe, verifying whether our attributions actually enable meaningful statements about behavior on counterfactuals. to the computation of the original model (Wu and Mooney, 2019; Hase and Bansal, 2020; Wiegreffe et al., 2021; Jacovi and Goldberg, 2020) and as a result, they can potentially mislead users (Rudin, 2019). Furthermore, attributions do not have a con1 Introduction sistent and meaningful social attribution (Miller, 2019; Jacovi and Goldberg, 2021): that is, when Recent research in interpretability of neural moda user of the system looks at an explanation, they els (Lipton, 2018) has yielded numerous post-hoc do not necessarily draw a valid conclusion from it, explanation methods, including token attribution making it hard to use for downstream tasks. techni"
2021.emnlp-main.447,D18-1259,0,0.417611,"Gardner et al., 2020) consisting of realistic counterfactuals.2 To showcase the kind of evaluation this method can enable, we investigate two paradigms of explanation techniques: token attribution-based (Simonyan et al., 2013; Ribeiro et al., 2016; De Cao et al., 2020) and feature interaction-based (Tsang et al., 2020; Hao et al., 2021), which attribute decisions to sets of tokens or pairwise token interactions. For both techniques, we devise methods to connect these explanations to our high-level hypotheses about behavior on counterfactual examples. On two types of questions from H OTPOT QA (Yang et al., 2018) and questions from adversarial S QUAD (Rajpurkar et al., 2016), we show that token-level attribution is not sufficient for analyz2 ing RC models, which naturally involves more comOne could argue that these counterfactuals are not entirely plex reasoning over multiple clues. We further pro- realistic: a romance film about smoking is unlikely. Generating suitable counterfactuals is a very hard problem (Qin et al., pose a modification to an existing interaction tech- 2019), requiring deep world knowledge of what scenarios nique from Hao et al. (2021) and show improved make sense or what properti"
2021.emnlp-main.447,2020.emnlp-main.14,0,0.0285219,"opular alternative, but it is difficult to evaluate these in our framework as it is References very difficult to bridge from a free-text explanation Anish Athalye, Nicholas Carlini, and David Wagner. to an approximation of a model’s computation. 2018. Obfuscated gradients give a false sense of Probing techniques aim to discover what intersecurity: Circumventing defenses to adversarial examples. In Proceedings of the International Confermediate representations have been learned in neural ence on Machine Learning (ICML). models (Tenney et al., 2019; Conneau et al., 2018; Hewitt and Liang, 2019; Voita and Titov, 2020). Osbert Bastani, Yewen Pu, and Armando SolarInternal representations could potentially be used Lezama. 2018. Verifiable reinforcement learning via 5504 policy extraction. In Proceedings of the Conference on Advances in Neural Information Processing Systems (NeurIPS). David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. 2017. Network dissection: Quantifying interpretability of deep visual representations. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR). Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-"
2021.emnlp-main.447,D19-1221,0,0.0278994,"ramanian et al., 2020; Jacovi and Goldberg, 2020, 2021) are better aligned with our goal of simulatability. We argue that a good explanation is one that aligns with the model’s high-level behavior, from which we can understand how the model generalizes to new data. How to interpret behavior from explanations is still an open question, but we take initial steps in this work with techniques based on assessing the attribution “mass” on perturbed tokens. Discussion: Realistic Counterfactuals Many counterfactual modifications are possible: past work has looked at injecting non-meaningful triggers (Wallace et al., 2019), deleting chunks of content (Ribeiro et al., 2016), or evaluating interpolated input points as in I NT G RAD, all of which “true” set of realistic counterfactuals is highly domain-specific, but nevertheless, a good explanation technique should work well on a range of counterfactuals like those considered here. violate assumptions about the input distribution. In RC, masking part of the question often makes it nonsensical and we may not have strong expectations about our model’s behavior in this case.3 Focusing on realistic counterfactuals, by contrast, illuminates fundamental problems with ou"
2021.emnlp-main.447,2021.emnlp-main.804,0,0.0551148,"Missing"
2021.emnlp-main.447,D19-1002,0,0.0179441,"eraction. Similar to D IFF M ASK, A RCHIP is also implicitly based on unrealistic counterfactuals which remove tokens. Given a subset of tokens, A RCHIP defines the contribution of the interaction by the prediction obtained from masking out all the other tokens, only leaving a very small fraction of the input. Applying this definition to a complex task like QA can result in a nonsensical input. Attention Attribution (ATATTR) (Hao et al., 2021) uses attention specifically to derive pairwise explanations. However, it avoids the pitfalls of directly inspecting attention (Serrano and Smith, 2019; Wiegreffe and Pinter, 2019) by running an integrated gradients-like procedure over all the attention links within transformers, yielding attribution scores for each link. The attribution scores directly reflect the attribution of the particular attention links, making this model able to describe pairwise interactions. Concretely, define the h-head attention matrix over input D with n tokens as A = [A1 , ..., Al ], where Ai ∈ Rh×n×n is the attention scores for each layer. We can obtain the attribution score for each entry in the attention matrix A as: 4.3 … Feature Interaction-Based ATTR(A) = A Layer Attentions Layer n ∫"
2021.emnlp-main.506,N18-2017,0,0.0592237,"Missing"
2021.emnlp-main.506,2020.scil-1.40,0,0.058389,"Missing"
2021.emnlp-main.506,W17-3408,0,0.0198691,"ralizations of NP substitution to other phrase categories, most likely learned as a side effect of paraphrastic data augmentation. 5 Related Work Natural Logic (Bernardi, 2002; Zamansky et al., 2006; MacCartney and Manning, 2009; Angeli et al., 2016) is related to our approach in that it provides a framework for logical reasoning about statements in natural language. Such systems recognize that there is a cat on the dresser entails there is an animal on the dresser because of the hypernymy relationship between cat and animal. These relationships can be formalized into a monotonicity calculus (Icard et al., 2017) and past work has grounded lexical inference tasks into such a formalism (Angeli et al., 2016; Hu et al., 2020). Instead of decomposing entailment into relationships between words, our models learn to map premises to conclusions at the sentence level, allowing our approach to handle relationships not captured by such a formalism. Generative Reasoning The generative nature of our models resembles generative models used for commonsense inference (Rajani et al., 2019; Latcinnik and Berant, 2020; Shwartz et al., 2020). However, these models do not strongly constrain the nature of what is generate"
2021.emnlp-main.506,2021.naacl-main.99,0,0.0323466,"captured by such a formalism. Generative Reasoning The generative nature of our models resembles generative models used for commonsense inference (Rajani et al., 2019; Latcinnik and Berant, 2020; Shwartz et al., 2020). However, these models do not strongly constrain the nature of what is generated. In contrast, our models reliably perform specific logical transformations, indicating that they can support sound inferences over longer reasoning chains in future work. Arabshahi et al. (2021) also explore generative reasoning in commonsense scenarios, but the domain of their approach is limited. Khot et al. (2021) use generative models to decompose a complex QA problem into a series of elementary steps that can be delegated to simpler models; this idea parallels the notion of decomposing reasoning into simple steps to be performed by generative operation models. Their results support the idea that such decomposition aids systematic generalization by enforcing separation of concerns. 6 Conclusion Building systems that use natural language as a medium for reasoning will require operations to logically combine and transform natural language statements. In this work, we present PARA PATTERN, a method for c"
2021.emnlp-main.506,2020.acl-main.703,0,0.451504,"s, we envision future reasoning systems factoring the deduction process into a set of common operations, analogous to proof rules. Modeling the reasoning process in this way would grant the ability to generalize systematically to any problem that could be decomposed in terms of available operations, among other desirable properties (Rudin, 2019). In this work, we describe a generative model for single-step deductive reasoning, building towards models capable of generating the range of logical transformations needed for the full reasoning process. We use a BART-based sequence-tosequence model (Lewis et al., 2020) to represent the distribution of valid conclusion statements conditioned on one or more premise statements. To make sound inferences, the model must be fine-tuned on well-formed training data. We describe a pipeline for crafting this data based on syntactic retrieval from Wikipedia, rule-based example construction, and automatic paraphrasing to increase diversity. Our hypothesis is that the logical regularities in the constructed examples will teach models to generate correct deductions, while paraphrasing coupled with the inductive bias from pretraining will regularize models, allowing them"
2021.emnlp-main.506,W09-3714,0,0.0599357,"ement living in an ecosystem. Humans building with our quantitative results, these outputs confirm homes in an ecosystem causes that ecosystem that PARA PATTERN generates sound inferences to change. even under domain shift. → Humans building homes in an ecosystem 6273 usually has a negative impact on an ecosystem / organisms living in an ecosystem. We hypothesize that these inferences reflect generalizations of NP substitution to other phrase categories, most likely learned as a side effect of paraphrastic data augmentation. 5 Related Work Natural Logic (Bernardi, 2002; Zamansky et al., 2006; MacCartney and Manning, 2009; Angeli et al., 2016) is related to our approach in that it provides a framework for logical reasoning about statements in natural language. Such systems recognize that there is a cat on the dresser entails there is an animal on the dresser because of the hypernymy relationship between cat and animal. These relationships can be formalized into a monotonicity calculus (Icard et al., 2017) and past work has grounded lexical inference tasks into such a formalism (Angeli et al., 2016; Hu et al., 2020). Instead of decomposing entailment into relationships between words, our models learn to map pre"
2021.emnlp-main.506,P19-1416,0,0.10159,"nguage reasoning has been comparatively slow. Today, ‘natural language inference’ usually means recognizing textual entailment (RTE), a pairwise sentence classification task. Models have saturated RTE benchmarks (Bowman et al., 2015; Williams et al., 2018) largely through surfacelevel heuristics (Gururangan et al., 2018; Poliak et al., 2018); hill-climbing on these benchmarks has failed to yield robust models (Naik et al., 2018) or systems capable of more complex reasoning. Following a line of work on multi-hop question answering (Welbl et al., 2018; Yang et al., 2018; Chen and Durrett, 2019; Min et al., 2019), the reading comprehension community has started to make inroads in the area of reasoning. Recent datasets have been explicitly designed to test 1 Introduction deduction ability (Liu et al., 2020; Yu et al., Developing models that can make useful inferences 2020; Holzenberger et al., 2020) and new types of from natural language premises has been a core models take inspiration from formal and informal goal in artificial intelligence since the field’s reasoning (Clark et al., 2020; Saha et al., 2020; early days (Bobrow, 1964; Winograd, 1971). Cartuyvels et al., 2020; Betz et al., 2021). Many Si"
2021.emnlp-main.506,C18-1198,0,0.0212136,"which we construct models. Note that conclusions involve both lexical inferences (X plays in the NFL → X is an NFL team, ¬[X lacks Y ] → X has Y ) and logical transformations. natural language reasoning has been comparatively slow. Today, ‘natural language inference’ usually means recognizing textual entailment (RTE), a pairwise sentence classification task. Models have saturated RTE benchmarks (Bowman et al., 2015; Williams et al., 2018) largely through surfacelevel heuristics (Gururangan et al., 2018; Poliak et al., 2018); hill-climbing on these benchmarks has failed to yield robust models (Naik et al., 2018) or systems capable of more complex reasoning. Following a line of work on multi-hop question answering (Welbl et al., 2018; Yang et al., 2018; Chen and Durrett, 2019; Min et al., 2019), the reading comprehension community has started to make inroads in the area of reasoning. Recent datasets have been explicitly designed to test 1 Introduction deduction ability (Liu et al., 2020; Yu et al., Developing models that can make useful inferences 2020; Holzenberger et al., 2020) and new types of from natural language premises has been a core models take inspiration from formal and informal goal in ar"
2021.emnlp-main.506,S18-2023,0,0.0606803,"Missing"
2021.emnlp-main.506,P19-1487,0,0.0253772,"because of the hypernymy relationship between cat and animal. These relationships can be formalized into a monotonicity calculus (Icard et al., 2017) and past work has grounded lexical inference tasks into such a formalism (Angeli et al., 2016; Hu et al., 2020). Instead of decomposing entailment into relationships between words, our models learn to map premises to conclusions at the sentence level, allowing our approach to handle relationships not captured by such a formalism. Generative Reasoning The generative nature of our models resembles generative models used for commonsense inference (Rajani et al., 2019; Latcinnik and Berant, 2020; Shwartz et al., 2020). However, these models do not strongly constrain the nature of what is generated. In contrast, our models reliably perform specific logical transformations, indicating that they can support sound inferences over longer reasoning chains in future work. Arabshahi et al. (2021) also explore generative reasoning in commonsense scenarios, but the domain of their approach is limited. Khot et al. (2021) use generative models to decompose a complex QA problem into a series of elementary steps that can be delegated to simpler models; this idea paralle"
2021.emnlp-main.506,2020.emnlp-main.9,0,0.0356977,"on multi-hop question answering (Welbl et al., 2018; Yang et al., 2018; Chen and Durrett, 2019; Min et al., 2019), the reading comprehension community has started to make inroads in the area of reasoning. Recent datasets have been explicitly designed to test 1 Introduction deduction ability (Liu et al., 2020; Yu et al., Developing models that can make useful inferences 2020; Holzenberger et al., 2020) and new types of from natural language premises has been a core models take inspiration from formal and informal goal in artificial intelligence since the field’s reasoning (Clark et al., 2020; Saha et al., 2020; early days (Bobrow, 1964; Winograd, 1971). Cartuyvels et al., 2020; Betz et al., 2021). Many Since then, there has been massive progress recent modeling efforts share a common motif in automated formal reasoning (De Moura and of using intermediate fact chains to support their Bjørner, 2011); in contrast, progress in automated final predictions, but a major shortcoming is that 1 https://github.com/alephic/ParaPattern these chains are either retrieved heuristically or 6266 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6266–6278 c November 7–11, 2"
2021.emnlp-main.506,2020.acl-main.704,0,0.0241702,"tated without knowledge of model identity to prevent rating bias. The last author reannotated a subset of QASC examples to validate the first author’s annotations; there were minor differences in interpretation of the divisions between non-valid categories, but the relative proportion of conclusions rated as valid remained consistent between annotators. We additionally compute the perplexity of reference conclusions under each model in order to assess the likelihood assigned to desired conclusions by each model’s output distribution. For the perturbation sets, we also report the BLEURT score (Sellam et al., 2020) of generated conclusions with respect to reference conclusions. 4 4.1 Results Results on Perturbation Sets Our first question is whether or not we have good generative models of natural language Valid: Conclusion is logically consistent with deductions. As Table 1 shows, PARA PATTERN premises but does not trivially repeat them. BART outperforms all baselines by a wide margin in terms of the likelihood of desired conclusions Valid with minor grammar errors: Conclusion (Ref. PPL), the similarity of its outputs to is valid but includes minor syntactic errors desired conclusions, and its overall"
2021.emnlp-main.506,P16-1009,0,0.031363,"r to be used in the surrounding lands, the rivers in the mountains must have their headwaters there. Conclusion: As such, rivers that do not provide water for irrigation in the surrounding lands do not have headwaters in the mountains. Premise: Dogs that are especially dirty or hungry are not able to participate in contests. Paraphrased: To participate in a contest, dogs that are dirty or hungry, must be turned away. Conclusion: Dogs that are able to participate in contests are not especially dirty or hungry. Figure 4: Examples produced by our data generation pipeline. in machine translation (Sennrich et al., 2016). Additional samples of the output of our data generation pipeline are shown in Figure 4. These examples demonstrate the ability of automatic paraphrasing to reduce both lexical and syntactic regularities in the original template outputs that can lead models to overfit to the template. In our subsequent experiments, we examine this overfitting by ablating Phase 3 of our data generation pipeline. experiments, we found that fine-tuning models for more than a single epoch always produced detrimental overfitting. Models are trained using a total batch size of 16 split across two NVidia Titan RTX G"
2021.emnlp-main.506,2020.emnlp-main.373,0,0.0424273,"Missing"
2021.emnlp-main.506,2021.findings-acl.317,0,0.221962,"logically consistent text for training language annotated data, indicating that our method is a models; however, there is little need for diversity or viable substitute for expensive direct supervision. naturalism in their data as it is exclusively used 6267 1 + pattern 2 Base sentences , , , Transformed deduction examples 3 , , , Paraphrased examples Figure 2: Schematic overview of the three phases of our data collection process: retrieval of base sentences from Wikipedia, expansion of these into reasoning examples, and paraphrasing. during pretraining for the purposes of transfer learning. Tafjord et al. (2021) use template-based natural language proofs to fine-tune transformer language models for reasoning; we include a model trained on their data as one of our baseline systems. 2.1 Data Collection Our proposed method, PARA PATTERN, combines scraping, template-based generation, and automatic paraphrasing in order to achieve sufficient data diversity and quality with very little manual effort. PARA PATTERN consists of three phases, as shown in Figures 2 and 3. matches are filtered based on a list of disallowed subject modifiers that would result in semantically invalid examples. After filtering, the"
2021.emnlp-main.506,P19-1618,0,0.0262292,"that PARA PATTERN yields operation models capable of generating consistent logical transformations over a diverse range of natural language inputs, matching the performance of models trained with in-domain human supervision. Multi-Hop Reasoning Combining facts to form a conclusion overlaps with the idea of multi-hop reasoning, which has been explored in reading comprehension settings (Welbl et al., 2018; Yang et al., 2018). However, training end-to-end models on these datasets does not necessarily teach models to combine facts (Chen and Durrett, 2019; Min et al., 2019). Systems like NLProlog (Weber et al., 2019) attempt to explicitly ground reasoning in logic, but this process still heavily relies on latent representations; in contrast, by Acknowledgments grounding reasoning directly in natural language, Thanks to Peter Clark for valuable discussion a system based on natural deduction operations and to the anonymous reviewers for their helpful like ours gains inherent faithful natural language comments. This work was partially supported by explanations and can build on the strengths of NSF awards # IIS-1814522 and # CCF-1918651, pretrained language models. by DARPA KAIROS award # FA8750-19-2-1003, Mo"
2021.emnlp-main.506,Q18-1021,0,0.159549,"X lacks Y ] → X has Y ) and logical transformations. natural language reasoning has been comparatively slow. Today, ‘natural language inference’ usually means recognizing textual entailment (RTE), a pairwise sentence classification task. Models have saturated RTE benchmarks (Bowman et al., 2015; Williams et al., 2018) largely through surfacelevel heuristics (Gururangan et al., 2018; Poliak et al., 2018); hill-climbing on these benchmarks has failed to yield robust models (Naik et al., 2018) or systems capable of more complex reasoning. Following a line of work on multi-hop question answering (Welbl et al., 2018; Yang et al., 2018; Chen and Durrett, 2019; Min et al., 2019), the reading comprehension community has started to make inroads in the area of reasoning. Recent datasets have been explicitly designed to test 1 Introduction deduction ability (Liu et al., 2020; Yu et al., Developing models that can make useful inferences 2020; Holzenberger et al., 2020) and new types of from natural language premises has been a core models take inspiration from formal and informal goal in artificial intelligence since the field’s reasoning (Clark et al., 2020; Saha et al., 2020; early days (Bobrow, 1964; Winogra"
2021.emnlp-main.506,2020.acl-main.773,0,0.0186621,"ch example are shown in turquoise. Grammatical errors are shown in orange. conditioned on their respective premises (MNLI BART). We train on all instances for which the gold label indicates entailment (≈103K examples) with the same training configuration as our other models, detailed in 2.2. We hypothesize that while this model may assign higher likelihood to valid conclusions than a general language model would, it will place much more probability mass on reemitting premise statements due to the fact that high word overlap tends to be a common feature of RTE examples labeled as ‘entailment’ (Zhou and Bansal, 2020). Our third baseline model is an instance of BART-Large fine-tuned to generate proof steps from the ProofWriter dataset (Tafjord et al., 2021) (ProofWriter BART). While this dataset contains a large number of English proof steps (≈135K), the language used in its proofs is automatically generated from a limited template library and is thus highly constrained. We hypothesize that this model will be unable to generalize as a result. 3.2 Perturbation sets First, in order to evaluate the accuracy of our models on the operations they were designed for, and to understand the degree to which they gene"
2021.emnlp-main.506,2020.lrec-1.671,0,0.0907104,"Missing"
2021.emnlp-main.506,D18-1259,0,0.154704,"Y ) and logical transformations. natural language reasoning has been comparatively slow. Today, ‘natural language inference’ usually means recognizing textual entailment (RTE), a pairwise sentence classification task. Models have saturated RTE benchmarks (Bowman et al., 2015; Williams et al., 2018) largely through surfacelevel heuristics (Gururangan et al., 2018; Poliak et al., 2018); hill-climbing on these benchmarks has failed to yield robust models (Naik et al., 2018) or systems capable of more complex reasoning. Following a line of work on multi-hop question answering (Welbl et al., 2018; Yang et al., 2018; Chen and Durrett, 2019; Min et al., 2019), the reading comprehension community has started to make inroads in the area of reasoning. Recent datasets have been explicitly designed to test 1 Introduction deduction ability (Liu et al., 2020; Yu et al., Developing models that can make useful inferences 2020; Holzenberger et al., 2020) and new types of from natural language premises has been a core models take inspiration from formal and informal goal in artificial intelligence since the field’s reasoning (Clark et al., 2020; Saha et al., 2020; early days (Bobrow, 1964; Winograd, 1971). Cartuyvel"
2021.findings-emnlp.146,P16-1002,0,0.015194,"ranslating NL to executable logical forms) a maximum of 5000 states. For beam search, we has been a long-standing research problem in the terminate search when the beam is filled up with NLP community (Zelle and Mooney, 1996; Price, complete programs or the size of partial programs 1990). Traditional grammar-based semantic parsers 1698 can construct database queries (Zelle and Mooney, 1996; Price, 1990), lambda calculus expressions (Zettlemoyer and Collins, 2005) and programs in other DSLs (Kushman and Barzilay, 2013; Wang et al., 2015). Recent advances in deep learning have explored seq2seq (Jia and Liang, 2016) or seq2tree models (Dong and Lapata, 2016) that directly translate the NL into a logical form, and syntax-based models (Yin and Neubig, 2017) can also inject syntactic constraints. Our approach relies on similar neural modeling to predict the distribution of target programs from NL. However, search is much more complex in our example-guided synthesis setting, whereas prior neural semantic parsers approximate the best solution using beam search (Dong and Lapata, 2016; Yin and Neubig, 2017). Optimal Synthesis with Examples Prior work on PBE considers various notions of optimality using cost fun"
2021.findings-emnlp.146,P16-1004,0,0.192746,"a complete program. ( cat ,V → 1 )n cat(V1, V1) 1 )n ( <0&gt;,V → <0&gt;)n cat(V1, V1) 1 2 3 ( V , Ø )n ( <1&gt;,V → <1&gt;)n 5 1 Figure 3: Example of an AST derivation of cat(cat(<0&gt;,<1&gt;),<0&gt;). Blue boxes represent symbols and yellow boxes represent productions. completion of P that satisfies the specification φ. 3 ( cat ,V → 1 4 1 5 Figure 4: Example of a partial program. n4 is a leaf node with non-terminal symbol V1 . ways we could factor and parameterize this distribution, including PCFGs, where the distribution depends only on the parent, or as sequence models over a pre-order traversal of the tree (Dong and Lapata, 2016; Yin and Neubig, 2017; Polosukhin and Skidanov, 2018). We choose the following factorization, similar to that used in Abstract Syntax Networks (ASN) (Rabinovich et al., 2017), where a production rule depends on the derivation path leading to that nonterminal: Y pθ (P |N ) = pθ (R(n) |π(P, n), N ) (2) n∈C(P ) The chief advantage of this factorization is that the score of a partial program is invariant to the derivation order of that program, assuming they were generated according to some topological ordering. Two derivations of the same tree P that differ only in the order that child branches"
2021.findings-emnlp.146,N13-1103,0,0.0147863,"atural Language to Logical Forms Semantic terminate O P S YNTH’s search after it has explored parsing (translating NL to executable logical forms) a maximum of 5000 states. For beam search, we has been a long-standing research problem in the terminate search when the beam is filled up with NLP community (Zelle and Mooney, 1996; Price, complete programs or the size of partial programs 1990). Traditional grammar-based semantic parsers 1698 can construct database queries (Zelle and Mooney, 1996; Price, 1990), lambda calculus expressions (Zettlemoyer and Collins, 2005) and programs in other DSLs (Kushman and Barzilay, 2013; Wang et al., 2015). Recent advances in deep learning have explored seq2seq (Jia and Liang, 2016) or seq2tree models (Dong and Lapata, 2016) that directly translate the NL into a logical form, and syntax-based models (Yin and Neubig, 2017) can also inject syntactic constraints. Our approach relies on similar neural modeling to predict the distribution of target programs from NL. However, search is much more complex in our example-guided synthesis setting, whereas prior neural semantic parsers approximate the best solution using beam search (Dong and Lapata, 2016; Yin and Neubig, 2017). Optima"
2021.findings-emnlp.146,D15-1166,0,0.128679,"Missing"
2021.findings-emnlp.146,H90-1020,0,0.625369,"hat performs beam search with varying beam sizes for Test-E. For the purposes of this experiment, we Natural Language to Logical Forms Semantic terminate O P S YNTH’s search after it has explored parsing (translating NL to executable logical forms) a maximum of 5000 states. For beam search, we has been a long-standing research problem in the terminate search when the beam is filled up with NLP community (Zelle and Mooney, 1996; Price, complete programs or the size of partial programs 1990). Traditional grammar-based semantic parsers 1698 can construct database queries (Zelle and Mooney, 1996; Price, 1990), lambda calculus expressions (Zettlemoyer and Collins, 2005) and programs in other DSLs (Kushman and Barzilay, 2013; Wang et al., 2015). Recent advances in deep learning have explored seq2seq (Jia and Liang, 2016) or seq2tree models (Dong and Lapata, 2016) that directly translate the NL into a logical form, and syntax-based models (Yin and Neubig, 2017) can also inject syntactic constraints. Our approach relies on similar neural modeling to predict the distribution of target programs from NL. However, search is much more complex in our example-guided synthesis setting, whereas prior neural se"
2021.findings-emnlp.146,P17-1105,0,0.239813,"constraints from niques for automatically generating programs from distinct sources? high-level expressions of user intent, such as inputThe core contribution of this paper is to formuoutput examples (Balog et al., 2017; Chen et al., late multimodal synthesis as an optimal synthesis 2019a; Devlin et al., 2017; Ellis et al., 2019; Kalyan task and propose an optimal synthesis algorithm to et al., 2018; Shin et al., 2018) and natural lan- solve it. The goal of optimal synthesis is to genguage (Yaghmazadeh et al., 2017; Dong and Lap- erate a program that satisfies any hard constraints ata, 2016; Rabinovich et al., 2017; Yin and Neu- provided by the user while also maximizing the 1 Code available: https://github.com/xiye17/OpSynth score under a learned neural network model that 1691 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1691–1704 November 7–11, 2021. ©2021 Association for Computational Linguistics captures noisy information, like that from natural language. In practice, there are many programs that satisfy the hard constraints, so this maximization is crucial to finding the user’s intended program: if our neural model is well-calibrated, a program that maximizes the sco"
2021.findings-emnlp.146,P15-1129,0,0.0106939,"orms Semantic terminate O P S YNTH’s search after it has explored parsing (translating NL to executable logical forms) a maximum of 5000 states. For beam search, we has been a long-standing research problem in the terminate search when the beam is filled up with NLP community (Zelle and Mooney, 1996; Price, complete programs or the size of partial programs 1990). Traditional grammar-based semantic parsers 1698 can construct database queries (Zelle and Mooney, 1996; Price, 1990), lambda calculus expressions (Zettlemoyer and Collins, 2005) and programs in other DSLs (Kushman and Barzilay, 2013; Wang et al., 2015). Recent advances in deep learning have explored seq2seq (Jia and Liang, 2016) or seq2tree models (Dong and Lapata, 2016) that directly translate the NL into a logical form, and syntax-based models (Yin and Neubig, 2017) can also inject syntactic constraints. Our approach relies on similar neural modeling to predict the distribution of target programs from NL. However, search is much more complex in our example-guided synthesis setting, whereas prior neural semantic parsers approximate the best solution using beam search (Dong and Lapata, 2016; Yin and Neubig, 2017). Optimal Synthesis with Exa"
2021.findings-emnlp.146,2020.acl-main.541,1,0.91199,". For the purposes of this paper, a partial program is an AST in which some of the nodes are labeled with non-terminal symbols in the grammar (see Figure 4). For a complete program, all node labels are terminal symbols. We use the notation E XPAND(P, l, r) to denote replacing leaf l with production r, which adds n nodes s1 , . . . , sn to the tree corresponding to the yield of r. We implement our method in a synthesizer Consistency with examples. In this paper, we called O P S YNTH and evaluate it on the challeng- focus on the multimodal synthesis problem where ing S TRUCTURED R EGEX dataset (Ye et al., 2020a) the user provides a logical specification φ and a for synthesizing regular expressions from linguis- natural language description. Specifically, we focus tically diverse natural language descriptions and on logical specifications in the form of positive and positive/negative examples. We compare our ap- negative examples of the program behavior. Each proach against a range of techniques from prior example is a pair (x, y) such that, for a positive work and ablations of our own method. O P S YNTH example, we have P (x) = y for the target program achieves substantial gain over past work by so"
2021.findings-emnlp.146,2020.tacl-1.44,1,0.905784,". For the purposes of this paper, a partial program is an AST in which some of the nodes are labeled with non-terminal symbols in the grammar (see Figure 4). For a complete program, all node labels are terminal symbols. We use the notation E XPAND(P, l, r) to denote replacing leaf l with production r, which adds n nodes s1 , . . . , sn to the tree corresponding to the yield of r. We implement our method in a synthesizer Consistency with examples. In this paper, we called O P S YNTH and evaluate it on the challeng- focus on the multimodal synthesis problem where ing S TRUCTURED R EGEX dataset (Ye et al., 2020a) the user provides a logical specification φ and a for synthesizing regular expressions from linguis- natural language description. Specifically, we focus tically diverse natural language descriptions and on logical specifications in the form of positive and positive/negative examples. We compare our ap- negative examples of the program behavior. Each proach against a range of techniques from prior example is a pair (x, y) such that, for a positive work and ablations of our own method. O P S YNTH example, we have P (x) = y for the target program achieves substantial gain over past work by so"
2021.findings-emnlp.324,D19-1418,0,0.0225728,"wering Modern QA systems often give incorrect answers in challenging settings that require generalization (Rajpurkar et al., 2018; Chen and Durrett, 2019; Wallace et al., 2019; Gardner et al., 2020; Kaushik et al., 2019). Models focusing on robustness and generalizability have been proposed in recent years: Wang and Bansal (2018); Khashabi et al. (2020); Liu et al. (2020) use perturbation based methods and adversarial training; Lewis and Fan (2018) propose generative QA to prevent the model from overfitting to simple patterns; Yeh and Chen (2019); Zhou et al. (2020) use advanced regularizers; Clark et al. (2019) debias the training set through ensemble-based training; and Chen and Durrett (2021) incorporate an explicit graph alignment procedure. Another line of work to make models more robust is by introducing answer verification (Hu 7 Related Work et al., 2019; Kamath et al., 2020; Wang et al., 2020; NLI for Downstream Tasks Welleck et al. Zhang et al., 2021) as a final step for question an(2019) proposed a dialogue-based NLI dataset and swering models. Our work is in the same vein, but the NLI model trained over it improved the con- has certain advantages from using an NLI model. sistency of a dial"
2021.findings-emnlp.324,N19-1423,0,0.0447416,"h Ted Danson is the right answer, an NLI model determines that the hypothesis is not entailed by the premise due to missing information. notion for when a hypothesis statement is entailed by a premise statement. By viewing the answer sentence in context as the premise, paired with the question and its proposed answer as a hypothesis (see Figure 1), we can use NLI systems to verify that the answer proposed by a QA model satisfies the entailment criterion (Harabagiu and Hickl, 2006; Richardson et al., 2013). Recent question answering systems perform well on benchmark datasets (Seo et al., 2017; Devlin et al., 2019; Guu et al., 2020), but these models Prior work has paved the way for this applicaoften lack the ability to verify whether an answer tion of NLI. Pieces of our pipeline like convertis correct or not; they can correctly reject some ing a question to a declarative sentence (Wang unanswerable questions (Rajpurkar et al., 2018; et al., 2018; Demszky et al., 2018) and reformulatKwiatkowski et al., 2019; Asai and Choi, 2021), ing an answer sentence to stand on its own (Choi but are not always well-calibrated to spot spurious et al., 2021) have been explored. Moreover, an answers under distribution"
2021.findings-emnlp.324,D19-5801,1,0.845925,"he utility of NLI for verifying answers primarily under distribution shifts, following recent work on selective question answering (Kamath et al., 2020). We transfer an NQ-trained QA model to a range of datasets and evaluate whether NLI improves answer confidence. Datasets We use five English-language spanextractive QA datasets: Natural Questions (Kwiatkowski et al., 2019, NQ), TriviaQA (Joshi et al., 2017), BioASQ (Tsatsaronis et al., 2015), Adversarial SQuAD (Jia and Liang, 2017, SQuADadv), and SQuAD 2.0 (Rajpurkar et al., 2018). For TriviaQA and BioASQ, we use processed versions from MRQA (Fisch et al., 2019). These datasets cover a wide range of domains including biology (BioASQ), trivia questions (TriviaQA), real user questions (NQ), and human-synthetic challenging sets (SQuAD2.0 and SQuAD-adv). For NQ, we filter out the examples in which the questions are narrative statements rather than questions by the rulebased system proposed by Demszky et al. (2018). We also exclude the examples based on tables because they are not compatible with the task formulation of NLI.3 3 After filtering, we have 191,022/4,855 examples for the training and development sets respectively. For comparison, the original"
2021.findings-emnlp.324,W18-2501,0,0.0245122,"Missing"
2021.findings-emnlp.324,P17-1147,1,0.67276,"ecode Sd from a concatenation of (Sa , C) pair, following the original work. More details about the models we discuss here can be found in Appendix B. 3 Experimental Settings Our experiments seek to validate the utility of NLI for verifying answers primarily under distribution shifts, following recent work on selective question answering (Kamath et al., 2020). We transfer an NQ-trained QA model to a range of datasets and evaluate whether NLI improves answer confidence. Datasets We use five English-language spanextractive QA datasets: Natural Questions (Kwiatkowski et al., 2019, NQ), TriviaQA (Joshi et al., 2017), BioASQ (Tsatsaronis et al., 2015), Adversarial SQuAD (Jia and Liang, 2017, SQuADadv), and SQuAD 2.0 (Rajpurkar et al., 2018). For TriviaQA and BioASQ, we use processed versions from MRQA (Fisch et al., 2019). These datasets cover a wide range of domains including biology (BioASQ), trivia questions (TriviaQA), real user questions (NQ), and human-synthetic challenging sets (SQuAD2.0 and SQuAD-adv). For NQ, we filter out the examples in which the questions are narrative statements rather than questions by the rulebased system proposed by Demszky et al. (2018). We also exclude the examples based"
2021.findings-emnlp.324,2020.acl-main.503,0,0.493188,"pplicaoften lack the ability to verify whether an answer tion of NLI. Pieces of our pipeline like convertis correct or not; they can correctly reject some ing a question to a declarative sentence (Wang unanswerable questions (Rajpurkar et al., 2018; et al., 2018; Demszky et al., 2018) and reformulatKwiatkowski et al., 2019; Asai and Choi, 2021), ing an answer sentence to stand on its own (Choi but are not always well-calibrated to spot spurious et al., 2021) have been explored. Moreover, an answers under distribution shifts (Jia and Liang, abundance of NLI datasets (Bowman et al., 2015; 2017; Kamath et al., 2020). Natural language in- Williams et al., 2018) and related fact verification ference (NLI) (Dagan et al., 2005; Bowman et al., datasets (Thorne et al., 2018) provide ample re2015) suggests one way to address this shortcom- sources to train reliable models. We draw on these ing: logical entailment provides a more rigorous tools to enable NLI models to verify the answers 3841 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3841–3854 November 7–11, 2021. ©2021 Association for Computational Linguistics from QA systems, and critically investigate the benefits and pitfall"
2021.findings-emnlp.324,2020.emnlp-main.12,0,0.20244,"LI dataset to aid other tasks such as fact-checking. Finally, the contrast we establish here allows us to conduct a thorough human analysis over the converted NLI data and show how the task specifications of NLI and QA are different (Section 6.2). Robust Question Answering Modern QA systems often give incorrect answers in challenging settings that require generalization (Rajpurkar et al., 2018; Chen and Durrett, 2019; Wallace et al., 2019; Gardner et al., 2020; Kaushik et al., 2019). Models focusing on robustness and generalizability have been proposed in recent years: Wang and Bansal (2018); Khashabi et al. (2020); Liu et al. (2020) use perturbation based methods and adversarial training; Lewis and Fan (2018) propose generative QA to prevent the model from overfitting to simple patterns; Yeh and Chen (2019); Zhou et al. (2020) use advanced regularizers; Clark et al. (2019) debias the training set through ensemble-based training; and Chen and Durrett (2021) incorporate an explicit graph alignment procedure. Another line of work to make models more robust is by introducing answer verification (Hu 7 Related Work et al., 2019; Kamath et al., 2020; Wang et al., 2020; NLI for Downstream Tasks Welleck et al."
2021.findings-emnlp.324,2021.acl-long.304,0,0.0294845,"ed to the ancient Chinese military strategist Sun Tzu ( “Master Sun”, also spelled Sunzi), is composed of 13 chapters. Full Context: The Art of War is an ancient Chinese military treatise dating from the Spring and Autumn period in 5th century BC. The work, which is attributed to the ancient Chinese military strategist Sun Tzu … Figure 7: Pipeline error examples from the NQ development set: the underlined text span denotes the answer predicted by the QA model. yields an unrelated statement. Adding a presupposition checking stage to the question converter could further improve its performance (Kim et al., 2021). (2) The question is long and syntactically complex; the question converter just copies a long question without answer replacement. For the decontextualization model, errors usually happen when the model fails to recall one of the required modifications. As shown in the example in Figure 7, the model fails to replace The work with its full entity name The Art of War. 6.2 Errors from the NLI Model ourselves if these really are errors. We categorize them into the following categories. Entailment These errors are truly mistakes by the entailment model: in our view, the pair of sentences should e"
2021.findings-emnlp.324,2021.ccl-1.108,0,0.0612437,"Missing"
2021.findings-emnlp.324,2021.naacl-main.104,0,0.487964,"data for passage and question reformulation. Richardson et al. (2013) explore a similar pipeline, but find that it works quite poorly, possibly due to the low performance of entailment systems at the time (Stern and Dagan, 2011). We believe that a combination of recent advances in natural language generation (Demszky et al., 2018; Choi et al., 2021) and strong models for NLI (Liu et al., 2019) equip us to re-evaluate this approach. Moreover, the focus of other recent work in this space has been on transforming QA datasets into NLI datasets, which is a different end. Demszky et al. (2018) and Mishra et al. (2021) argue that QA datasets feature more diverse reasoning and can lead to stronger NLI models, particularly those better suited to strong contexts, but less attention has been paid to whether this agrees with classic definitions of entailment (Dagan et al., 2005) or short-context NLI settings (Williams et al., 2018). Our work particularly aims to shed light on information sufficiency in question answering. Other work in this space has focused on validating answers to unanswerable questions (Rajpurkar et al., 2018; Kwiatkowski et al., 2019), but such questions may be nonsensical in context; these"
2021.findings-emnlp.324,W17-4504,0,0.0649675,"Missing"
2021.findings-emnlp.324,P18-2124,0,0.497747,"a hypothesis (see Figure 1), we can use NLI systems to verify that the answer proposed by a QA model satisfies the entailment criterion (Harabagiu and Hickl, 2006; Richardson et al., 2013). Recent question answering systems perform well on benchmark datasets (Seo et al., 2017; Devlin et al., 2019; Guu et al., 2020), but these models Prior work has paved the way for this applicaoften lack the ability to verify whether an answer tion of NLI. Pieces of our pipeline like convertis correct or not; they can correctly reject some ing a question to a declarative sentence (Wang unanswerable questions (Rajpurkar et al., 2018; et al., 2018; Demszky et al., 2018) and reformulatKwiatkowski et al., 2019; Asai and Choi, 2021), ing an answer sentence to stand on its own (Choi but are not always well-calibrated to spot spurious et al., 2021) have been explored. Moreover, an answers under distribution shifts (Jia and Liang, abundance of NLI datasets (Bowman et al., 2015; 2017; Kamath et al., 2020). Natural language in- Williams et al., 2018) and related fact verification ference (NLI) (Dagan et al., 2005; Bowman et al., datasets (Thorne et al., 2018) provide ample re2015) suggests one way to address this shortcom- source"
2021.findings-emnlp.324,D16-1264,0,0.0311568,"wering techniques in terms of spotting out-of-domain questions that the model is likely to get wrong rather than more general confidence estimation. What is missing in these threads of literature is a formal criterion like entailment: when is an answer truly sufficient and when are we confident that it addresses the question? 2.2 Our Approach Our pipeline consists of an answer candidate generator, a question converter, and a decontextualizer, 3842 which form the inputs to the final entailment model. Answer Generation In this work, we focus our attention on extractive QA (Hermann et al., 2015; Rajpurkar et al., 2016), for which we can get an answer candidate by running a pre-trained QA model.2 We use the Bert-joint model proposed by Alberti et al. (2019) for its simplicity and relatively high performance. Question Conversion Given a question q and an answer candidate a, our goal is to convert the (q, a) pair to a declarative answer sentence d which can be treated as the hypothesis in an NLI system (Demszky et al., 2018; Khot et al., 2018). While rulebased approaches have long been employed for this purpose (Cucerzan and Agichtein, 2005), the work of Demszky et al. (2018) showed a benefit from more sophist"
2021.findings-emnlp.324,N18-1074,0,0.0382967,"Missing"
2021.findings-emnlp.324,P19-1363,0,0.0402249,"Missing"
2021.findings-emnlp.324,N19-1302,0,0.0440849,"Missing"
2021.findings-emnlp.324,D19-1221,0,0.0244346,"cus on a single sentence. We also study whether the converted dataset is compatible with other off-the-shelf NLI datasets. By contrast, Mishra et al. (2021) use their converted NLI dataset to aid other tasks such as fact-checking. Finally, the contrast we establish here allows us to conduct a thorough human analysis over the converted NLI data and show how the task specifications of NLI and QA are different (Section 6.2). Robust Question Answering Modern QA systems often give incorrect answers in challenging settings that require generalization (Rajpurkar et al., 2018; Chen and Durrett, 2019; Wallace et al., 2019; Gardner et al., 2020; Kaushik et al., 2019). Models focusing on robustness and generalizability have been proposed in recent years: Wang and Bansal (2018); Khashabi et al. (2020); Liu et al. (2020) use perturbation based methods and adversarial training; Lewis and Fan (2018) propose generative QA to prevent the model from overfitting to simple patterns; Yeh and Chen (2019); Zhou et al. (2020) use advanced regularizers; Clark et al. (2019) debias the training set through ensemble-based training; and Chen and Durrett (2021) incorporate an explicit graph alignment procedure. Another line of wor"
2021.findings-emnlp.324,N18-1101,0,0.589492,"her an answer tion of NLI. Pieces of our pipeline like convertis correct or not; they can correctly reject some ing a question to a declarative sentence (Wang unanswerable questions (Rajpurkar et al., 2018; et al., 2018; Demszky et al., 2018) and reformulatKwiatkowski et al., 2019; Asai and Choi, 2021), ing an answer sentence to stand on its own (Choi but are not always well-calibrated to spot spurious et al., 2021) have been explored. Moreover, an answers under distribution shifts (Jia and Liang, abundance of NLI datasets (Bowman et al., 2015; 2017; Kamath et al., 2020). Natural language in- Williams et al., 2018) and related fact verification ference (NLI) (Dagan et al., 2005; Bowman et al., datasets (Thorne et al., 2018) provide ample re2015) suggests one way to address this shortcom- sources to train reliable models. We draw on these ing: logical entailment provides a more rigorous tools to enable NLI models to verify the answers 3841 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3841–3854 November 7–11, 2021. ©2021 Association for Computational Linguistics from QA systems, and critically investigate the benefits and pitfalls of such a formulation. Mapping QA to NLI en"
2021.findings-emnlp.324,D19-1333,0,0.0180138,"ions of NLI and QA are different (Section 6.2). Robust Question Answering Modern QA systems often give incorrect answers in challenging settings that require generalization (Rajpurkar et al., 2018; Chen and Durrett, 2019; Wallace et al., 2019; Gardner et al., 2020; Kaushik et al., 2019). Models focusing on robustness and generalizability have been proposed in recent years: Wang and Bansal (2018); Khashabi et al. (2020); Liu et al. (2020) use perturbation based methods and adversarial training; Lewis and Fan (2018) propose generative QA to prevent the model from overfitting to simple patterns; Yeh and Chen (2019); Zhou et al. (2020) use advanced regularizers; Clark et al. (2019) debias the training set through ensemble-based training; and Chen and Durrett (2021) incorporate an explicit graph alignment procedure. Another line of work to make models more robust is by introducing answer verification (Hu 7 Related Work et al., 2019; Kamath et al., 2020; Wang et al., 2020; NLI for Downstream Tasks Welleck et al. Zhang et al., 2021) as a final step for question an(2019) proposed a dialogue-based NLI dataset and swering models. Our work is in the same vein, but the NLI model trained over it improved the con-"
2021.findings-emnlp.324,2020.emnlp-main.660,0,0.0529312,"Missing"
2021.findings-emnlp.324,2021.findings-acl.172,1,0.805942,"; Liu et al. (2020) use perturbation based methods and adversarial training; Lewis and Fan (2018) propose generative QA to prevent the model from overfitting to simple patterns; Yeh and Chen (2019); Zhou et al. (2020) use advanced regularizers; Clark et al. (2019) debias the training set through ensemble-based training; and Chen and Durrett (2021) incorporate an explicit graph alignment procedure. Another line of work to make models more robust is by introducing answer verification (Hu 7 Related Work et al., 2019; Kamath et al., 2020; Wang et al., 2020; NLI for Downstream Tasks Welleck et al. Zhang et al., 2021) as a final step for question an(2019) proposed a dialogue-based NLI dataset and swering models. Our work is in the same vein, but the NLI model trained over it improved the con- has certain advantages from using an NLI model. sistency of a dialogue system; Pasunuru et al. First, the answer verification process is more ex3848 Entailment Error (NLI Prediction: Not Entail) Question: What were the results of the development of Florida&apos;s railroads? Predicted / Gold Answer: towns grew and farmland was cultivated / towns grew and farmland was cultivated Hypothesis: The results of the development of"
2021.naacl-main.114,2020.acl-main.656,0,0.0125026,"t (Cao et al., 2018), post-hoc correction (Dong et al., 2020), or constrained decoding (Song et al., 2020a; Mao et al., 2020). However, these techniques fundamentally struggle to handle the whole range of factual errors; factuality is a fuzzy notion and cannot be easily encapsulated into a set of discrete rules. Faithfulness and factuality have also been tackled in related tasks, including summarizing radiology reports (Zhang et al., 2020b) and data-to-text generation tasks (Tian et al., 2019). Another recent line of work has looked at fact verification (Thorne et al., 2018; Nie et al., 2019; Atanasova et al., 2020). In this literature, the claims are usually humanauthored and a straightforward statement of a fact, whereas generated summaries might feature claims buried in nominal modifiers like two-time winner. 8 Conclusion In this work, we showed that existing synthetic datasets are not well-suited to factuality evaluation of recent summarization models (like BART) in challenging domain (like XS UM). Models trained on human-annotated data, especially those that leverage fine-grained annotations, can enable training of more factual summarization models. We hope future work will explore better modeling a"
2021.naacl-main.114,P99-1071,0,0.174688,"ctual (score = 1) or non-factual (score = 0). An average score is computed for each summary by aggregating the 7 annotator scores. Table 6 reports the average summary scores for the 50 (article, summary) pairs across the 3 summarization models. The results show that the proposed approach outperforms both the baseline model and the loss truncation approach. This demonstrates that factuality models trained on a small number of annotated examples can be used to train factual summarization models, even when the underlying summarization dataset is noisy. 7 Related Work Earlier work on abstraction (Barzilay et al., 1999; Carenini and Cheung, 2008) and compression (Knight and Marcu, 2000; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Durrett et al., 2016) in summarization has typically focused evaluation on content selection and grammaticality, with little heed paid to factuality. Human evaluation similarly focused on content selection (Gillick and Liu, 2010). Methods such as Pyramid (Nenkova and Passonneau, 2004) that could have in principle been used to evaluate factuality were primarily used to understand content selection. Recent work has explored different methods for enforcing factuality: mo"
2021.naacl-main.114,P11-1049,0,0.0331753,"by aggregating the 7 annotator scores. Table 6 reports the average summary scores for the 50 (article, summary) pairs across the 3 summarization models. The results show that the proposed approach outperforms both the baseline model and the loss truncation approach. This demonstrates that factuality models trained on a small number of annotated examples can be used to train factual summarization models, even when the underlying summarization dataset is noisy. 7 Related Work Earlier work on abstraction (Barzilay et al., 1999; Carenini and Cheung, 2008) and compression (Knight and Marcu, 2000; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Durrett et al., 2016) in summarization has typically focused evaluation on content selection and grammaticality, with little heed paid to factuality. Human evaluation similarly focused on content selection (Gillick and Liu, 2010). Methods such as Pyramid (Nenkova and Passonneau, 2004) that could have in principle been used to evaluate factuality were primarily used to understand content selection. Recent work has explored different methods for enforcing factuality: modifying the model, such as encoding SRL structures in the input (Cao et al., 2018), post-hoc correc"
2021.naacl-main.114,2020.emnlp-main.506,0,0.557848,"t, while synthetic data generation applore both synthetic and human-labeled data proaches are specifically designed for factuality sources for training models to identify factual evaluation, do these align with actual errors errors in summarization, and study factuality at the word-, dependency-, and sentence-level. made by generation models? We find the anOur observations are threefold. First, exhibswer is no: techniques using surface-level data corited factual errors differ significantly across ruption (Kryscinski et al., 2020; Zhao et al., 2020; datasets, and commonly-used training sets of Cao et al., 2020) or paraphrasing (Goyal and Dursimple synthetic errors do not reflect errors rett, 2020a) target inherently different error distrimade on abstractive datasets like XS UM. Secbutions than those seen in actual model generations, ond, human-labeled data with fine-grained anand factuality models trained on these datasets pernotations provides a more effective training signal than sentence-level annotations or synform poorly in practice. Furthermore, we show thetic data. Finally, we show that our best that different summarization domains, CNN/Daily factuality detection model enables training of Mai"
2021.naacl-main.114,W08-1106,0,0.0298191,"n-factual (score = 0). An average score is computed for each summary by aggregating the 7 annotator scores. Table 6 reports the average summary scores for the 50 (article, summary) pairs across the 3 summarization models. The results show that the proposed approach outperforms both the baseline model and the loss truncation approach. This demonstrates that factuality models trained on a small number of annotated examples can be used to train factual summarization models, even when the underlying summarization dataset is noisy. 7 Related Work Earlier work on abstraction (Barzilay et al., 1999; Carenini and Cheung, 2008) and compression (Knight and Marcu, 2000; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Durrett et al., 2016) in summarization has typically focused evaluation on content selection and grammaticality, with little heed paid to factuality. Human evaluation similarly focused on content selection (Gillick and Liu, 2010). Methods such as Pyramid (Nenkova and Passonneau, 2004) that could have in principle been used to evaluate factuality were primarily used to understand content selection. Recent work has explored different methods for enforcing factuality: modifying the model, such as e"
2021.naacl-main.114,P07-1036,0,0.0701101,".2 Human Annotated Dataset Evaluation To investigate whether human annotated data is useful to train factuality models, we train our 3 factuality models on the remaining 2000 human an5 notated examples from XS UM -H UMAN. In order This techniques resembles posterior regularization (Ganchev et al., 2010); however, these constraints are ento train DAE model on this dataset, we use the span forced in a hard way on individual examples rather than in highlights to derive dependency-level gold annotaexpectation at the corpus level. It can also be viewed as an instance of constraint-driven learning (Chang et al., 2007). tions, using the same strategy from 2.3 (illustrated 1455 Model Balanced-Acc Sent-Factuality DAE DAE-Weak Model DAE DAE-Weak Localization of errors Our evaluation so far has focused on the sentencelevel performance of factuality models. Next, we evaluate the models’ ability to localize errors within the generated summary as well as show how such a capability can be leveraged to train less error-prone summarization models. F1 69.7 54.9 78.2 76.6 73.7 63.9 74.7 62.3 83.9 65.0 59.1 71.3 Word-level Table 4: Comparison of different factuality models when trained on human annotated data and evalua"
2021.naacl-main.114,2020.emnlp-main.750,0,0.256985,"paper, we aim to answer two main quesniques are succeeding and failing. We extions. First, while synthetic data generation applore both synthetic and human-labeled data proaches are specifically designed for factuality sources for training models to identify factual evaluation, do these align with actual errors errors in summarization, and study factuality at the word-, dependency-, and sentence-level. made by generation models? We find the anOur observations are threefold. First, exhibswer is no: techniques using surface-level data corited factual errors differ significantly across ruption (Kryscinski et al., 2020; Zhao et al., 2020; datasets, and commonly-used training sets of Cao et al., 2020) or paraphrasing (Goyal and Dursimple synthetic errors do not reflect errors rett, 2020a) target inherently different error distrimade on abstractive datasets like XS UM. Secbutions than those seen in actual model generations, ond, human-labeled data with fine-grained anand factuality models trained on these datasets pernotations provides a more effective training signal than sentence-level annotations or synform poorly in practice. Furthermore, we show thetic data. Finally, we show that our best that different"
2021.naacl-main.114,D19-5413,0,0.1714,"Missing"
2021.naacl-main.114,2020.acl-main.703,0,0.101048,"Missing"
2021.naacl-main.114,2020.eval4nlp-1.1,0,0.0849068,"Missing"
2021.naacl-main.114,2020.acl-main.173,0,0.167627,"r to realistic settings. We start by qualitatively analyzing the actual errors produced by summarization models to see how these align with the synthetic data, which helps us better understand this assumption. We identify four broad categories of errors (see Figure 3) that we will identify through manual inspection. Each of these categories is further divided into Intrinsic (errors that arise as a result of misinterpreting information from the source article) and Extrinsic (errors that hallucinate new information or facts not present in the source article), following the characterization from Maynez et al. (2020). dates, etc. Hallucination of new entities is an extrinsic error; incorrectly combining distinct entities from the source article is an intrinsic error (Paul Telstra in Figure 3). 2. Event-Related: errors with incorrect claims about events in the summary, such as predicates with arguments filled by incorrect entities. Hallucinations of new events (held a press conference in Figure 3) are extrinsic; mixed-up attributes from within the source article are intrinsic (apple lawyer never claimed in Figure 3, incorrect agent). 3. Noun Phrase-Related: errors related to noun phrases other than the ent"
2021.naacl-main.114,N18-1074,0,0.0611926,"Missing"
2021.naacl-main.114,2020.acl-main.450,0,0.0498461,"Missing"
2021.naacl-main.114,K16-1028,0,0.0355664,"nd Dursimple synthetic errors do not reflect errors rett, 2020a) target inherently different error distrimade on abstractive datasets like XS UM. Secbutions than those seen in actual model generations, ond, human-labeled data with fine-grained anand factuality models trained on these datasets pernotations provides a more effective training signal than sentence-level annotations or synform poorly in practice. Furthermore, we show thetic data. Finally, we show that our best that different summarization domains, CNN/Daily factuality detection model enables training of Mail (Hermann et al., 2015; Nallapati et al., 2016) more factual XS UM summarization models by and XSum (Narayan et al., 2018) (which differ in allowing us to identify non-factual tokens in the style of summaries and degree of abstraction), the training data.1 exhibit substantially different error distributions in generated summaries, and the same dataset creation 1 Introduction approach cannot be used across the board. Hallucination of unsupported or incorrect facts Second, we investigate the best approach for is a known shortcoming of current text genera- modeling and learning factuality, particularly for tion and summarization models (Cao e"
2021.naacl-main.114,D18-1206,0,0.0444385,"ently different error distrimade on abstractive datasets like XS UM. Secbutions than those seen in actual model generations, ond, human-labeled data with fine-grained anand factuality models trained on these datasets pernotations provides a more effective training signal than sentence-level annotations or synform poorly in practice. Furthermore, we show thetic data. Finally, we show that our best that different summarization domains, CNN/Daily factuality detection model enables training of Mail (Hermann et al., 2015; Nallapati et al., 2016) more factual XS UM summarization models by and XSum (Narayan et al., 2018) (which differ in allowing us to identify non-factual tokens in the style of summaries and degree of abstraction), the training data.1 exhibit substantially different error distributions in generated summaries, and the same dataset creation 1 Introduction approach cannot be used across the board. Hallucination of unsupported or incorrect facts Second, we investigate the best approach for is a known shortcoming of current text genera- modeling and learning factuality, particularly for tion and summarization models (Cao et al., 2018; highly abstractive summarization settings (Narayan Falke et al"
2021.naacl-main.114,D12-1022,0,0.0165421,"scores. Table 6 reports the average summary scores for the 50 (article, summary) pairs across the 3 summarization models. The results show that the proposed approach outperforms both the baseline model and the loss truncation approach. This demonstrates that factuality models trained on a small number of annotated examples can be used to train factual summarization models, even when the underlying summarization dataset is noisy. 7 Related Work Earlier work on abstraction (Barzilay et al., 1999; Carenini and Cheung, 2008) and compression (Knight and Marcu, 2000; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Durrett et al., 2016) in summarization has typically focused evaluation on content selection and grammaticality, with little heed paid to factuality. Human evaluation similarly focused on content selection (Gillick and Liu, 2010). Methods such as Pyramid (Nenkova and Passonneau, 2004) that could have in principle been used to evaluate factuality were primarily used to understand content selection. Recent work has explored different methods for enforcing factuality: modifying the model, such as encoding SRL structures in the input (Cao et al., 2018), post-hoc correction (Dong et al., 2020), o"
2021.naacl-main.114,N04-1019,0,0.203327,"ed on a small number of annotated examples can be used to train factual summarization models, even when the underlying summarization dataset is noisy. 7 Related Work Earlier work on abstraction (Barzilay et al., 1999; Carenini and Cheung, 2008) and compression (Knight and Marcu, 2000; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Durrett et al., 2016) in summarization has typically focused evaluation on content selection and grammaticality, with little heed paid to factuality. Human evaluation similarly focused on content selection (Gillick and Liu, 2010). Methods such as Pyramid (Nenkova and Passonneau, 2004) that could have in principle been used to evaluate factuality were primarily used to understand content selection. Recent work has explored different methods for enforcing factuality: modifying the model, such as encoding SRL structures in the input (Cao et al., 2018), post-hoc correction (Dong et al., 2020), or constrained decoding (Song et al., 2020a; Mao et al., 2020). However, these techniques fundamentally struggle to handle the whole range of factual errors; factuality is a fuzzy notion and cannot be easily encapsulated into a set of discrete rules. Faithfulness and factuality have also"
2021.naacl-main.129,2020.repl4nlp-1.10,0,0.0107468,"he classes. Training We split the data into 5 cross-validation folds, stratified by congressional hearing (to preserve the differing response distributions as seen in Figure 3). We reserve one fold for hyperparameter tuning and use the remaining 4 folds for cross-validation at test time.9 Baselines The A LL P OSITIVE baseline predicts 1 for all labels. This baseline easily outperforms a majority baseline that predicts the most frequent label (answer+direct). L OG R EGRESSION performs logistic regression with bag-of-words representations. CNN is a convolutional neural network as implemented in Adhikari et al. (2020). Other baselines performing lower than CNN are in Appendix C. Pretrained We experiment with several pretrained language models, and find RO BERTA (Liu et al., 2019) performs the best on the held-out development fold. We use the implementation from Hugging Face.10 We feed in the tokenized response text and truncate input to 512 word pieces (additional inputs used in the model variants we describe next are separated by the [SEP] token). Hierarchical We use two classifiers to mimic the hierarchy of our taxonomy: the first classifier predicts the conversation act while the second predicts the com"
2021.naacl-main.129,J08-4004,0,0.105177,"Missing"
2021.naacl-main.129,Q19-1035,0,0.0287432,"Missing"
2021.naacl-main.129,N03-2011,0,0.336484,"Missing"
2021.naacl-main.129,N19-1173,0,0.0125943,"ure 3). We reserve one fold for hyperparameter tuning and use the remaining 4 folds for cross-validation at test time.9 Baselines The A LL P OSITIVE baseline predicts 1 for all labels. This baseline easily outperforms a majority baseline that predicts the most frequent label (answer+direct). L OG R EGRESSION performs logistic regression with bag-of-words representations. CNN is a convolutional neural network as implemented in Adhikari et al. (2020). Other baselines performing lower than CNN are in Appendix C. Pretrained We experiment with several pretrained language models, and find RO BERTA (Liu et al., 2019) performs the best on the held-out development fold. We use the implementation from Hugging Face.10 We feed in the tokenized response text and truncate input to 512 word pieces (additional inputs used in the model variants we describe next are separated by the [SEP] token). Hierarchical We use two classifiers to mimic the hierarchy of our taxonomy: the first classifier predicts the conversation act while the second predicts the complete label (conversation act+intent). We train the classifiers independently, and condition the second classifier on the ground truth of the first classifier during"
2021.naacl-main.129,N19-1177,0,0.0258602,"speaker. Detection of deception is, unlike many other NLP tasks, challenging even for humans (Ott et al., 2011). Most datasets consist of instructed lies (where participants are told to lie). Our work contains naturally-occurring deception where we include not just lying but other more covert mechanisms such as being deliberately vague or evasive (Clementson, 2018), both frequent in political discourse (Bull, 2008). Argumentation mining analyzes non-cooperative conversations, but typically requires expert annotators. Recent work decomposes the task into intuitive questions for crowdsourcing (Miller et al., 2019), inspiring our annotation schemes that assume little to no training. Closer to our setting is argument persuasiveness, where Durmus and Cardie (2018) find prior beliefs of the audience play a strong role in their ability to be persuaded, which further motivates our focus on the annotator’s bias. 3 Dataset item question response #sents/ turn #toks/ total turn sents total toks total spkrs 4.1 2.6 81.5 47.0 82582 48831 91 20 4096 2634 Table 1: Statistics of our 20 U.S. congressional hearings. the response, and the data is plentiful.2 A dataset statement is in Appendix D. 3.1 Dataset creation Con"
2021.naacl-main.129,2020.emnlp-main.734,0,0.0351484,"bligations, such as responding to a question (Traum and Allen, 1994; Potts, 2008). For this reason, we explicitly separate judgments on conversation acts (that usually fulfill a specific obligation) from communicative intents, which can be perceived as deceptive (or sincere). Prior work examines how writer intentions are often misaligned with reader perceptions (Chang et al., 2020), which further motivates our focus on the reader (our annotator). While our work focuses on subjectivity, ambiguity is studied in many NLP tasks, including Natural Language Inference (Pavlick and Kwiatkowski, 2019; Nie et al., 2020), evaluation of NLG (Schoch et al., 2020), a recent SemEval 2021 shared task,1 as well as several discourse tasks (Asher and Lascarides, 2003; Versley, 2011; Webber and Joshi, 2012; Das et al., 2017; Poesio et al., 2019; Webber et al., 2019). Only one study strives to understand how these ambiguities are resolved: Scholman (2019) shows different interpretations of ambiguous coherence relations can be attributable to different cognitive biases. However, our work focuses more generally on subjecIn summary, the task together with the dataset present a valuable opportunity to understand perception"
2021.naacl-main.129,P11-1032,0,0.0605211,"at the utterance level. Classification models typically combine representations of linguistic units (word, utterance, conversation-level) (Chen et al., 2018). In our work, we employ a hierarchical model to account for the levels in our label taxonomy. Intent detection is traditionally applied to human-computer scenarios for task-specific goals such as booking a flight. Our conversation data is not task-oriented, and we thus define our intents more closely aligned with beliefs in the sincerity of the speaker. Detection of deception is, unlike many other NLP tasks, challenging even for humans (Ott et al., 2011). Most datasets consist of instructed lies (where participants are told to lie). Our work contains naturally-occurring deception where we include not just lying but other more covert mechanisms such as being deliberately vague or evasive (Clementson, 2018), both frequent in political discourse (Bull, 2008). Argumentation mining analyzes non-cooperative conversations, but typically requires expert annotators. Recent work decomposes the task into intuitive questions for crowdsourcing (Miller et al., 2019), inspiring our annotation schemes that assume little to no training. Closer to our setting"
2021.naacl-main.129,Q19-1043,0,0.0269729,"ons of dialogue, or discourse obligations, such as responding to a question (Traum and Allen, 1994; Potts, 2008). For this reason, we explicitly separate judgments on conversation acts (that usually fulfill a specific obligation) from communicative intents, which can be perceived as deceptive (or sincere). Prior work examines how writer intentions are often misaligned with reader perceptions (Chang et al., 2020), which further motivates our focus on the reader (our annotator). While our work focuses on subjectivity, ambiguity is studied in many NLP tasks, including Natural Language Inference (Pavlick and Kwiatkowski, 2019; Nie et al., 2020), evaluation of NLG (Schoch et al., 2020), a recent SemEval 2021 shared task,1 as well as several discourse tasks (Asher and Lascarides, 2003; Versley, 2011; Webber and Joshi, 2012; Das et al., 2017; Poesio et al., 2019; Webber et al., 2019). Only one study strives to understand how these ambiguities are resolved: Scholman (2019) shows different interpretations of ambiguous coherence relations can be attributable to different cognitive biases. However, our work focuses more generally on subjecIn summary, the task together with the dataset present a valuable opportunity to un"
2021.naacl-main.129,C16-1181,0,0.029574,"Missing"
2021.naacl-main.129,W12-3205,0,0.0272315,"a specific obligation) from communicative intents, which can be perceived as deceptive (or sincere). Prior work examines how writer intentions are often misaligned with reader perceptions (Chang et al., 2020), which further motivates our focus on the reader (our annotator). While our work focuses on subjectivity, ambiguity is studied in many NLP tasks, including Natural Language Inference (Pavlick and Kwiatkowski, 2019; Nie et al., 2020), evaluation of NLG (Schoch et al., 2020), a recent SemEval 2021 shared task,1 as well as several discourse tasks (Asher and Lascarides, 2003; Versley, 2011; Webber and Joshi, 2012; Das et al., 2017; Poesio et al., 2019; Webber et al., 2019). Only one study strives to understand how these ambiguities are resolved: Scholman (2019) shows different interpretations of ambiguous coherence relations can be attributable to different cognitive biases. However, our work focuses more generally on subjecIn summary, the task together with the dataset present a valuable opportunity to understand perceptions of discourse in a non-cooperative environment. More broadly, we show the need and value 1 for considering the subjectivity of NLP tasks. Our https://sites.google.com/view/ work i"
2021.naacl-main.129,W19-0411,0,0.0830517,". congressional hearings. In Figure 1, annotators give conflicting assessments of responses given by the witness Mark Zuckerberg (CEO of Facebook) who is being questioned by Congressman Eliot Engel. Discourse, like many uses of language, has inherent ambiguity, meaning it can have multiple, valid interpretations. Much work has focused on characTo make sense of our setting that has speakers terizing these “genuine disagreements” (Asher and (witness, politicians) and observers (annotators), Lascarides, 2003; Das et al., 2017; Poesio et al., we are inspired by the game-theoretic view of con2019; Webber et al., 2019) and incorporating their uncertainty through concurrent labels (Rohde et al., versation in Asher and Paul (2018). The players (witness, politicians) make certain discourse moves 2018) and underspecified structures (Hanneforth et al., 2003). However, prior work does not exam- in order to influence a third party, who is the judge of the game (the annotator). Importantly, the judge ine the subjectivity of discourse: how you resolve makes biased evaluations about the type of the an ambiguity by applying your personal beliefs and player (e.g., sincere vs. deceptive), which leads preferences. Our wo"
2021.naacl-main.98,W13-2322,0,0.0101674,"ary 7, 2016 … Figure 2: Example of our question-passage graph. Edges come from SRL, coreference (Super Bowl 50— the game), and postprocessing of predicates nested inside arguments (was—determine). The oracle alignment (Section 3.4) is shown with dotted lines. Blue nodes are predicates and orange ones are arguments. 2 QA as Graph Alignment Our approach critically relies on the ability to decompose questions and answers into a graph over text spans. Our model can in principle work for a range of syntactic and semantic structures, including dependency parsing, SRL (Palmer et al., 2005), and AMR (Banarescu et al., 2013). We use SRL in this work and augment it with coreference links, due to the high performance and flexibility of current SRL systems (Peters et al., 2018). Throughout this work, we use the BERT-based SRL system from Shi and Lin (2019) and the SpanBERT-based coreference system from Joshi et al. (2020). An example graph we construct is shown in Figure 2. Both the question and context are represented as graphs where the nodes consist of predicates and arguments. Edges are undirected and connect each predicate and its corresponding arguments. Since SRL only captures the predicateWe train our model"
2021.naacl-main.98,N19-1405,1,0.852104,"n rely on spurious patterns trustable (Lewis and Fan, 2018); if not, we suspect between the question and context rather than learn- that the model is making an incorrect prediction. ing the desired behavior. They may ignore the The sub-parts we use are predicates and arguments question entirely (Kaushik and Lipton, 2018), fo- from Semantic Role Labeling (Palmer et al., 2005), cus primarily on the answer type (Mudrakarta et al., which we found to be a good semantic represen2018), or otherwise bypass the “intended” mode tation for the types of questions we studied. We of reasoning for the task (Chen and Durrett, 2019; then view the question answering procedure as a Niven and Kao, 2019). Thus, these models are constrained graph alignment problem (Sachan and not robust to adversarial attacks (Jia and Liang, Xing, 2016), where the nodes represent the predi1251 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1251–1263 June 6–11, 2021. ©2021 Association for Computational Linguistics cates and arguments and the edges are formed by relations between them (e.g. predicate-argument relations and coreference relatio"
2021.naacl-main.98,2020.emnlp-main.21,1,0.890433,"Missing"
2021.naacl-main.98,N19-1423,0,0.523605,"in the end the context node aligned to the wh-span should ideally contain the answer. Then we can use a standard QA model to extract the answer. Figure 1 shows an adversarial example of SQuAD (Jia and Liang, 2017) where a standard BERT QA model predicts the wrong answer August 18, 1991. In order to choose the adversarial answer, our model must explicitly align Super Bowl 50 to Champ Bowl. Even if the model still makes this mistake, this error is now exposed directly, making it easier to interpret and subsequently patch. In our alignment model, each pair of aligned nodes is scored using BERT (Devlin et al., 2019). These alignment scores are then plugged into a beam search inference procedure to perform the constrained graph alignment. This structured alignment model can be trained as a structured support vector machine (SSVM) to minimize alignment error with heuristically-derived oracle alignments. The alignment scores are computed in a black-box way, so these individual decisions aren’t easily explainable (Jain and Wallace, 2019); however, the score of an answer is directly a sum of the score of each aligned piece, making this structured prediction phase of the model faithful by construction (Jain et"
2021.naacl-main.98,N19-1246,0,0.0353418,"Missing"
2021.naacl-main.98,P18-2006,0,0.0233302,"Dean, which is unrelated. Because we have alignments over the sub-parts of a question, we can inspect our model’s behavior in a way that the normal BERT QA model does not allow. We believe that this type of debuggability provides a path forward for building stronger QA systems in high-stakes settings. 6 Related Work Adversarial Attacks in NLP. Adversarial attacks in NLP may take the form of adding sentences like adversarial SQuAD (Jia and Liang, 2017), universal adversarial triggers (Wallace et al., 2019), or sentence perturbations: Ribeiro et al. (2018) propose deriving transformation rules, Ebrahimi et al. (2018) use character-level flips, and Iyyer et al. (2018) use controlled paraphrase generation. The highly structured nature of our approach makes it more robust to such attacks and provides hooks to constrain the system to improve performance further. In this section, we give several examples of the alignment and demonstrate how those scores can act as an explanation to the model’s behavior. Those examples are shown in Figure 6. As shown by the dashed arrows, all adversarial alignments contain at least one alignment with Neural module networks. Neural module netsignificantly lower alignment score."
2021.naacl-main.98,D19-5801,0,0.143722,"l., 2019): they can be fooled by surface-level distractor answers that follow the spurious patterns. Methods like adversarial training (Miyato et al., 2016; Wang and Bansal, 2018; Lee et al., 2019; Yang et al., 2019), data augmentation (Welbl et al., 2020), and posterior regularization (Pereyra et al., 2016; Zhou et al., 2019) have been proposed to improve robustness. However, these techniques often optimize for a certain type of error. We want models that can adapt to new types of adversarial examples and work under other distribution shifts, such as on questions from different text domains (Fisch et al., 2019). In this paper, we explore a model for text-based question answering through sub-part alignment. The core idea behind our method is that if every 1 Introduction aspect of the question is well supported by the anCurrent text-based question answering models swer context, then the answer produced should be learned end-to-end often rely on spurious patterns trustable (Lewis and Fan, 2018); if not, we suspect between the question and context rather than learn- that the model is making an incorrect prediction. ing the desired behavior. They may ignore the The sub-parts we use are predicates and arg"
2021.naacl-main.98,N18-1170,0,0.094827,"football game to determine the champion … The game was played on February 7, 2016 … Sentence with correct answer What day was Super Bowl 50 played on? Question The Champ Bowl was played on the day of August 18, 1991 Adversarial sentence Figure 1: A typical example on adversarial SQuAD. By breaking the question and context down into smaller units, we can expose the incorrect entity match and use explicit constraints to fix it. The solid lines denote edges from SRL and coreference, and the dotted lines denote the possible alignments between the arguments (desired in red, actual in black). 2017; Iyyer et al., 2018; Wallace et al., 2019): they can be fooled by surface-level distractor answers that follow the spurious patterns. Methods like adversarial training (Miyato et al., 2016; Wang and Bansal, 2018; Lee et al., 2019; Yang et al., 2019), data augmentation (Welbl et al., 2020), and posterior regularization (Pereyra et al., 2016; Zhou et al., 2019) have been proposed to improve robustness. However, these techniques often optimize for a certain type of error. We want models that can adapt to new types of adversarial examples and work under other distribution shifts, such as on questions from different"
2021.naacl-main.98,N19-1357,0,0.0303004,"this mistake, this error is now exposed directly, making it easier to interpret and subsequently patch. In our alignment model, each pair of aligned nodes is scored using BERT (Devlin et al., 2019). These alignment scores are then plugged into a beam search inference procedure to perform the constrained graph alignment. This structured alignment model can be trained as a structured support vector machine (SSVM) to minimize alignment error with heuristically-derived oracle alignments. The alignment scores are computed in a black-box way, so these individual decisions aren’t easily explainable (Jain and Wallace, 2019); however, the score of an answer is directly a sum of the score of each aligned piece, making this structured prediction phase of the model faithful by construction (Jain et al., 2020). Critically, this allows us to understand what parts of the alignment are responsible for a prediction, and if needed, constrain the behavior of the alignment to correct certain types of errors. We view this interpretability and extensibility with constraints as one of the principal advantages of our model. Question: What day was Super Bowl 50 played on? Super Bowl 50 Super Bowl 50 ARG1 coref played ARG-TMP ARG"
2021.naacl-main.98,2020.acl-main.409,0,0.0506978,"Missing"
2021.naacl-main.98,D17-1215,0,0.464722,"Association for Computational Linguistics: Human Language Technologies, pages 1251–1263 June 6–11, 2021. ©2021 Association for Computational Linguistics cates and arguments and the edges are formed by relations between them (e.g. predicate-argument relations and coreference relations). Our goal is to align each node in the question to a counterpart in the context, respecting some loose constraints, and in the end the context node aligned to the wh-span should ideally contain the answer. Then we can use a standard QA model to extract the answer. Figure 1 shows an adversarial example of SQuAD (Jia and Liang, 2017) where a standard BERT QA model predicts the wrong answer August 18, 1991. In order to choose the adversarial answer, our model must explicitly align Super Bowl 50 to Champ Bowl. Even if the model still makes this mistake, this error is now exposed directly, making it easier to interpret and subsequently patch. In our alignment model, each pair of aligned nodes is scored using BERT (Devlin et al., 2019). These alignment scores are then plugged into a beam search inference procedure to perform the constrained graph alignment. This structured alignment model can be trained as a structured suppor"
2021.naacl-main.98,D19-1455,0,0.0114579,"with Neural module networks. Neural module netsignificantly lower alignment score. The model is works are a class of models that decompose a task overconfident towards the other alignments with a into several sub-tasks, addressed by independent high lexical overlap as shown by the bold arrows. neural modules, which make the model more roThese overconfident alignments also show that the bust and interpretable (Andreas et al., 2016; Hu predicate alignment learned on SQuAD-1.1 is not et al., 2017; Cirik et al., 2018; Hudson and Manreliable. To further improve the quality of predicate ning, 2018; Jiang and Bansal, 2019). Like these, alignment, either a more powerful training set or a our model is trained end-to-end, but our approach 1258 uses structured prediction and a static network structure rather than dynamically assembling a network on the fly. Our approach could be further improved by devising additional modules with distinct parameters, particularly if these are trained on other datasets to integrate additional semantic constraints. Unanswerable questions Our approach rejects some questions as unanswerable. This is similar to the idea of unanswerable questions in SQuAD 2.0 (Rajpurkar et al., 2018), w"
2021.naacl-main.98,2020.tacl-1.5,0,0.0598355,"e arguments. 2 QA as Graph Alignment Our approach critically relies on the ability to decompose questions and answers into a graph over text spans. Our model can in principle work for a range of syntactic and semantic structures, including dependency parsing, SRL (Palmer et al., 2005), and AMR (Banarescu et al., 2013). We use SRL in this work and augment it with coreference links, due to the high performance and flexibility of current SRL systems (Peters et al., 2018). Throughout this work, we use the BERT-based SRL system from Shi and Lin (2019) and the SpanBERT-based coreference system from Joshi et al. (2020). An example graph we construct is shown in Figure 2. Both the question and context are represented as graphs where the nodes consist of predicates and arguments. Edges are undirected and connect each predicate and its corresponding arguments. Since SRL only captures the predicateWe train our model on the SQuAD-1.1 argument relations within one sentence, we add coreference edges as well: if two arguments are in dataset (Rajpurkar et al., 2016) and evaluate on SQuAD Adversarial (Jia and Liang, 2017), Univer- the same coreference cluster, we add an edge between them. Finally, in certain cases in"
2021.naacl-main.98,2020.acl-main.503,0,0.0270582,"or more deeply. This structure also allows us to add constraints to our model to prohibit certain behaviors, which can be used to adapt our model to adversarial settings. In this section, we explore how two types of constraints enable us to reject examples the model is less confident about. Hard constraints can enable us to reject questions where the model finds no admissible answers. Soft constraints allow us to set a calibration threshold for when to return our answer. We focus on evaluating our model’s accuracy at various coverage points, the so-called selective question answering setting (Kamath et al., 2020). Constraints on Entity Matches By examining addSent and addOneSent, we find the model is typically fooled when the nodes containing en3 tities in the question align to “adversarial” entity For the MRQA task, only the paragraph containing the short answer of NQ is provided as context, which eliminates nodes. An intuitive constraint we can place on the many distractors. In such cases, those NQ questions have alignment is that we require a hard entity match— a similar distribution as those in SQuAD-1.1, and similarly make no use of the global alignment. for each argument in the question, if it c"
2021.naacl-main.98,D18-1546,0,0.0206024,"e a model for text-based question answering through sub-part alignment. The core idea behind our method is that if every 1 Introduction aspect of the question is well supported by the anCurrent text-based question answering models swer context, then the answer produced should be learned end-to-end often rely on spurious patterns trustable (Lewis and Fan, 2018); if not, we suspect between the question and context rather than learn- that the model is making an incorrect prediction. ing the desired behavior. They may ignore the The sub-parts we use are predicates and arguments question entirely (Kaushik and Lipton, 2018), fo- from Semantic Role Labeling (Palmer et al., 2005), cus primarily on the answer type (Mudrakarta et al., which we found to be a good semantic represen2018), or otherwise bypass the “intended” mode tation for the types of questions we studied. We of reasoning for the task (Chen and Durrett, 2019; then view the question answering procedure as a Niven and Kao, 2019). Thus, these models are constrained graph alignment problem (Sachan and not robust to adversarial attacks (Jia and Liang, Xing, 2016), where the nodes represent the predi1251 Proceedings of the 2021 Conference of the North Americ"
2021.naacl-main.98,2020.emnlp-main.713,0,0.0180518,"maintaining strong performance. In this work, we presented a model for question answering through sub-part alignment. By structuring our model around explicit alignment scoring, we show that our approach can generalize better to other domains. Having alignments also makes it possible to filter out bad model predictions (through score constraints) and interpret the model’s behavior (by inspecting the scores). Acknowledgments Past work has also decomposed complex questions to answer them more effectively (Talmor and This work was partially supported by NSF Grant Berant, 2018; Min et al., 2019; Perez et al., 2020). IIS-1814522 and NSF Grant SHF-1762299. The Wolfson et al. (2020) further introduce a Question authors acknowledge the Texas Advanced ComputDecomposition Meaning Representation (QDMR) ing Center (TACC) at The University of Texas at to explicitly model this process. However, the ques- Austin for providing HPC resources used to contions they answer, such as those from HotpotQA duct this research. Results presented in this paper (Yang et al., 2018), are fundamentally designed were obtained using the Chameleon testbed supto be multi-part and so are easily decomposed, ported by the National Scienc"
2021.naacl-main.98,Q19-1026,0,0.0167282,"Experimental Settings For all experiments, we train our model only on the English SQuAD-1.1 dataset (Rajpurkar et al., 2016) and examine how well it can generalize to adversarial and out-of-domain settings with minimal modification, using no fine-tuning on new data and no data augmentation that would capture useful transformations. We evaluate on the addSent and addOneSent proposed by Jia and Liang (2017), and the Universal Triggers on SQuAD (Wallace et al., 2019). We also test the performance of our SQuAD-trained models in zeroshot adaptation to new English domains, namely Natural Questions (Kwiatkowski et al., 2019), NewsQA (Trischler et al., 2017), BioASQ (Tsatsaronis et al., 2015) and TextbookQA (Kembhavi et al., 2017), taken from the MRQA shared task (Fisch et al., 2019). Our motivation here was to focus on text from a variety of domains where transferred SQuAD models may at least behave credibly. We excluded, for example, HotpotQA (Yang et al., 2018) and DROP (Dua et al., 2019), since these are so far out-of-domain from the perspective of SQuAD that we do not see them as a realistic crossdomain target. wh word, we back off to the standard BERT QA system (results in Table 3). We set the beam size b ="
2021.naacl-main.98,D17-1018,0,0.101737,"er Bowl 50 … [SEP] Super Bowl 50 was an … Figure 3: Alignment scoring. Here the alignment score is computed by the dot product between span representations of question and context nodes. The final alignment score (not shown) is the sum of these edge scores. Scoring Our alignment scoring process is shown in Figure 3. We first concatenate the question text with the document text into T and then encode them using the pre-trained BERT encoder. We then compute a representation for each node in the question and context using a span extractor, which in our case is the self-attentive pooling layer of Lee et al. (2017). The node representation in the question can be computed in the same way. Then the score of a node pair is computed as a dot product S(q, c, T) = q · c. Answer Extraction Our model so far produces an alignment between question nodes and context nodes. We assume that one question node contains a wh-word and this node aligns to the context node containing the answer.1 Ideally, we can use this aligned node to extract the actual answer. However, in practice, the aligned context node may only contain part of the answer and in some cases answering the question only based the aligned context node ca"
2021.naacl-main.98,D19-5826,0,0.0360391,"Missing"
2021.naacl-main.98,P19-1613,0,0.0194481,"lity further while maintaining strong performance. In this work, we presented a model for question answering through sub-part alignment. By structuring our model around explicit alignment scoring, we show that our approach can generalize better to other domains. Having alignments also makes it possible to filter out bad model predictions (through score constraints) and interpret the model’s behavior (by inspecting the scores). Acknowledgments Past work has also decomposed complex questions to answer them more effectively (Talmor and This work was partially supported by NSF Grant Berant, 2018; Min et al., 2019; Perez et al., 2020). IIS-1814522 and NSF Grant SHF-1762299. The Wolfson et al. (2020) further introduce a Question authors acknowledge the Texas Advanced ComputDecomposition Meaning Representation (QDMR) ing Center (TACC) at The University of Texas at to explicitly model this process. However, the ques- Austin for providing HPC resources used to contions they answer, such as those from HotpotQA duct this research. Results presented in this paper (Yang et al., 2018), are fundamentally designed were obtained using the Chameleon testbed supto be multi-part and so are easily decomposed, ported b"
2021.naacl-main.98,P18-1176,0,0.0449755,"Missing"
2021.naacl-main.98,P19-1459,0,0.0169879,"suspect between the question and context rather than learn- that the model is making an incorrect prediction. ing the desired behavior. They may ignore the The sub-parts we use are predicates and arguments question entirely (Kaushik and Lipton, 2018), fo- from Semantic Role Labeling (Palmer et al., 2005), cus primarily on the answer type (Mudrakarta et al., which we found to be a good semantic represen2018), or otherwise bypass the “intended” mode tation for the types of questions we studied. We of reasoning for the task (Chen and Durrett, 2019; then view the question answering procedure as a Niven and Kao, 2019). Thus, these models are constrained graph alignment problem (Sachan and not robust to adversarial attacks (Jia and Liang, Xing, 2016), where the nodes represent the predi1251 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1251–1263 June 6–11, 2021. ©2021 Association for Computational Linguistics cates and arguments and the edges are formed by relations between them (e.g. predicate-argument relations and coreference relations). Our goal is to align each node in the question to a counterpart i"
2021.naacl-main.98,J05-1004,0,0.399318,"alignment. The core idea behind our method is that if every 1 Introduction aspect of the question is well supported by the anCurrent text-based question answering models swer context, then the answer produced should be learned end-to-end often rely on spurious patterns trustable (Lewis and Fan, 2018); if not, we suspect between the question and context rather than learn- that the model is making an incorrect prediction. ing the desired behavior. They may ignore the The sub-parts we use are predicates and arguments question entirely (Kaushik and Lipton, 2018), fo- from Semantic Role Labeling (Palmer et al., 2005), cus primarily on the answer type (Mudrakarta et al., which we found to be a good semantic represen2018), or otherwise bypass the “intended” mode tation for the types of questions we studied. We of reasoning for the task (Chen and Durrett, 2019; then view the question answering procedure as a Niven and Kao, 2019). Thus, these models are constrained graph alignment problem (Sachan and not robust to adversarial attacks (Jia and Liang, Xing, 2016), where the nodes represent the predi1251 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguis"
2021.naacl-main.98,N18-1202,0,0.0344078,"nested inside arguments (was—determine). The oracle alignment (Section 3.4) is shown with dotted lines. Blue nodes are predicates and orange ones are arguments. 2 QA as Graph Alignment Our approach critically relies on the ability to decompose questions and answers into a graph over text spans. Our model can in principle work for a range of syntactic and semantic structures, including dependency parsing, SRL (Palmer et al., 2005), and AMR (Banarescu et al., 2013). We use SRL in this work and augment it with coreference links, due to the high performance and flexibility of current SRL systems (Peters et al., 2018). Throughout this work, we use the BERT-based SRL system from Shi and Lin (2019) and the SpanBERT-based coreference system from Joshi et al. (2020). An example graph we construct is shown in Figure 2. Both the question and context are represented as graphs where the nodes consist of predicates and arguments. Edges are undirected and connect each predicate and its corresponding arguments. Since SRL only captures the predicateWe train our model on the SQuAD-1.1 argument relations within one sentence, we add coreference edges as well: if two arguments are in dataset (Rajpurkar et al., 2016) and e"
2021.naacl-main.98,P18-2124,0,0.0604327,"Missing"
2021.naacl-main.98,D16-1264,0,0.307635,"systems (Peters et al., 2018). Throughout this work, we use the BERT-based SRL system from Shi and Lin (2019) and the SpanBERT-based coreference system from Joshi et al. (2020). An example graph we construct is shown in Figure 2. Both the question and context are represented as graphs where the nodes consist of predicates and arguments. Edges are undirected and connect each predicate and its corresponding arguments. Since SRL only captures the predicateWe train our model on the SQuAD-1.1 argument relations within one sentence, we add coreference edges as well: if two arguments are in dataset (Rajpurkar et al., 2016) and evaluate on SQuAD Adversarial (Jia and Liang, 2017), Univer- the same coreference cluster, we add an edge between them. Finally, in certain cases involving versal Triggers on SQuAD (Wallace et al., 2019), and bal or clausal arguments, there might exist nested several out-of-domain datasets from MRQA (Fisch et al., 2019). Our framework allows us to incor- structures where an argument to one predicate contains a separate predicate-argument structure. In porate natural constraints on alignment scores to improve zero-shot performance under these distri- this case, we remove the larger argumen"
2021.naacl-main.98,P18-1079,0,0.0204938,"orces Luther’s 95 Theses to have no choice but align to Jeff Dean, which is unrelated. Because we have alignments over the sub-parts of a question, we can inspect our model’s behavior in a way that the normal BERT QA model does not allow. We believe that this type of debuggability provides a path forward for building stronger QA systems in high-stakes settings. 6 Related Work Adversarial Attacks in NLP. Adversarial attacks in NLP may take the form of adding sentences like adversarial SQuAD (Jia and Liang, 2017), universal adversarial triggers (Wallace et al., 2019), or sentence perturbations: Ribeiro et al. (2018) propose deriving transformation rules, Ebrahimi et al. (2018) use character-level flips, and Iyyer et al. (2018) use controlled paraphrase generation. The highly structured nature of our approach makes it more robust to such attacks and provides hooks to constrain the system to improve performance further. In this section, we give several examples of the alignment and demonstrate how those scores can act as an explanation to the model’s behavior. Those examples are shown in Figure 6. As shown by the dashed arrows, all adversarial alignments contain at least one alignment with Neural module ne"
2021.naacl-main.98,P15-1024,0,0.186214,"ge directly between the two predicates. This bution shifts, as well as explore coverage-accuracy is shown by the edge from was to determine (latradeoffs in these settings. Finally, our model’s alignments serve as “explanations” for its predic- beled as nested structure) in Figure 2). Breaking down such large arguments helps avoid ambiguity tion, allowing us to ask why certain predictions are made over others and examine scores for hypothet- during alignment. ical other answers the model could give. Aligning questions and contexts has proven 1252 useful for question answering in previous work (Sachan et al., 2015; Sachan and Xing, 2016; Khashabi et al., 2018). Our framework differs from theirs in that it incorporates a much stronger alignment model (BERT), allowing us to relax the alignment constraints and build a more flexible, highercoverage model. Alignment Constraints Once we have the constructed graph, we can align each node in the question to its counterpart in the context graph. In this work, we control the alignment behavior by placing explicit constraints on this process. We place a locality constraint on the alignment: adjacent pairs of question nodes must align no more than k nodes apart in"
2021.naacl-main.98,P16-2079,0,0.149065,"he two predicates. This bution shifts, as well as explore coverage-accuracy is shown by the edge from was to determine (latradeoffs in these settings. Finally, our model’s alignments serve as “explanations” for its predic- beled as nested structure) in Figure 2). Breaking down such large arguments helps avoid ambiguity tion, allowing us to ask why certain predictions are made over others and examine scores for hypothet- during alignment. ical other answers the model could give. Aligning questions and contexts has proven 1252 useful for question answering in previous work (Sachan et al., 2015; Sachan and Xing, 2016; Khashabi et al., 2018). Our framework differs from theirs in that it incorporates a much stronger alignment model (BERT), allowing us to relax the alignment constraints and build a more flexible, highercoverage model. Alignment Constraints Once we have the constructed graph, we can align each node in the question to its counterpart in the context graph. In this work, we control the alignment behavior by placing explicit constraints on this process. We place a locality constraint on the alignment: adjacent pairs of question nodes must align no more than k nodes apart in the context. k = 1 mea"
2021.naacl-main.98,N18-1059,0,0.0551082,"Missing"
2021.naacl-main.98,W17-2623,0,0.0238021,"iments, we train our model only on the English SQuAD-1.1 dataset (Rajpurkar et al., 2016) and examine how well it can generalize to adversarial and out-of-domain settings with minimal modification, using no fine-tuning on new data and no data augmentation that would capture useful transformations. We evaluate on the addSent and addOneSent proposed by Jia and Liang (2017), and the Universal Triggers on SQuAD (Wallace et al., 2019). We also test the performance of our SQuAD-trained models in zeroshot adaptation to new English domains, namely Natural Questions (Kwiatkowski et al., 2019), NewsQA (Trischler et al., 2017), BioASQ (Tsatsaronis et al., 2015) and TextbookQA (Kembhavi et al., 2017), taken from the MRQA shared task (Fisch et al., 2019). Our motivation here was to focus on text from a variety of domains where transferred SQuAD models may at least behave credibly. We excluded, for example, HotpotQA (Yang et al., 2018) and DROP (Dua et al., 2019), since these are so far out-of-domain from the perspective of SQuAD that we do not see them as a realistic crossdomain target. wh word, we back off to the standard BERT QA system (results in Table 3). We set the beam size b = 20 for the constrained alignment."
2021.naacl-main.98,D19-1221,0,0.286546,"ermine the champion … The game was played on February 7, 2016 … Sentence with correct answer What day was Super Bowl 50 played on? Question The Champ Bowl was played on the day of August 18, 1991 Adversarial sentence Figure 1: A typical example on adversarial SQuAD. By breaking the question and context down into smaller units, we can expose the incorrect entity match and use explicit constraints to fix it. The solid lines denote edges from SRL and coreference, and the dotted lines denote the possible alignments between the arguments (desired in red, actual in black). 2017; Iyyer et al., 2018; Wallace et al., 2019): they can be fooled by surface-level distractor answers that follow the spurious patterns. Methods like adversarial training (Miyato et al., 2016; Wang and Bansal, 2018; Lee et al., 2019; Yang et al., 2019), data augmentation (Welbl et al., 2020), and posterior regularization (Pereyra et al., 2016; Zhou et al., 2019) have been proposed to improve robustness. However, these techniques often optimize for a certain type of error. We want models that can adapt to new types of adversarial examples and work under other distribution shifts, such as on questions from different text domains (Fisch et"
2021.naacl-main.98,N18-2091,0,0.0240372,"the day of August 18, 1991 Adversarial sentence Figure 1: A typical example on adversarial SQuAD. By breaking the question and context down into smaller units, we can expose the incorrect entity match and use explicit constraints to fix it. The solid lines denote edges from SRL and coreference, and the dotted lines denote the possible alignments between the arguments (desired in red, actual in black). 2017; Iyyer et al., 2018; Wallace et al., 2019): they can be fooled by surface-level distractor answers that follow the spurious patterns. Methods like adversarial training (Miyato et al., 2016; Wang and Bansal, 2018; Lee et al., 2019; Yang et al., 2019), data augmentation (Welbl et al., 2020), and posterior regularization (Pereyra et al., 2016; Zhou et al., 2019) have been proposed to improve robustness. However, these techniques often optimize for a certain type of error. We want models that can adapt to new types of adversarial examples and work under other distribution shifts, such as on questions from different text domains (Fisch et al., 2019). In this paper, we explore a model for text-based question answering through sub-part alignment. The core idea behind our method is that if every 1 Introducti"
2021.naacl-main.98,2020.findings-emnlp.103,0,0.0271266,"dversarial SQuAD. By breaking the question and context down into smaller units, we can expose the incorrect entity match and use explicit constraints to fix it. The solid lines denote edges from SRL and coreference, and the dotted lines denote the possible alignments between the arguments (desired in red, actual in black). 2017; Iyyer et al., 2018; Wallace et al., 2019): they can be fooled by surface-level distractor answers that follow the spurious patterns. Methods like adversarial training (Miyato et al., 2016; Wang and Bansal, 2018; Lee et al., 2019; Yang et al., 2019), data augmentation (Welbl et al., 2020), and posterior regularization (Pereyra et al., 2016; Zhou et al., 2019) have been proposed to improve robustness. However, these techniques often optimize for a certain type of error. We want models that can adapt to new types of adversarial examples and work under other distribution shifts, such as on questions from different text domains (Fisch et al., 2019). In this paper, we explore a model for text-based question answering through sub-part alignment. The core idea behind our method is that if every 1 Introduction aspect of the question is well supported by the anCurrent text-based questi"
2021.naacl-main.98,2020.tacl-1.13,0,0.0117392,"del for question answering through sub-part alignment. By structuring our model around explicit alignment scoring, we show that our approach can generalize better to other domains. Having alignments also makes it possible to filter out bad model predictions (through score constraints) and interpret the model’s behavior (by inspecting the scores). Acknowledgments Past work has also decomposed complex questions to answer them more effectively (Talmor and This work was partially supported by NSF Grant Berant, 2018; Min et al., 2019; Perez et al., 2020). IIS-1814522 and NSF Grant SHF-1762299. The Wolfson et al. (2020) further introduce a Question authors acknowledge the Texas Advanced ComputDecomposition Meaning Representation (QDMR) ing Center (TACC) at The University of Texas at to explicitly model this process. However, the ques- Austin for providing HPC resources used to contions they answer, such as those from HotpotQA duct this research. Results presented in this paper (Yang et al., 2018), are fundamentally designed were obtained using the Chameleon testbed supto be multi-part and so are easily decomposed, ported by the National Science Foundation. Thanks whereas the questions we consider are not. Ou"
2021.naacl-main.98,D18-1259,0,0.0238107,"addSent and addOneSent proposed by Jia and Liang (2017), and the Universal Triggers on SQuAD (Wallace et al., 2019). We also test the performance of our SQuAD-trained models in zeroshot adaptation to new English domains, namely Natural Questions (Kwiatkowski et al., 2019), NewsQA (Trischler et al., 2017), BioASQ (Tsatsaronis et al., 2015) and TextbookQA (Kembhavi et al., 2017), taken from the MRQA shared task (Fisch et al., 2019). Our motivation here was to focus on text from a variety of domains where transferred SQuAD models may at least behave credibly. We excluded, for example, HotpotQA (Yang et al., 2018) and DROP (Dua et al., 2019), since these are so far out-of-domain from the perspective of SQuAD that we do not see them as a realistic crossdomain target. wh word, we back off to the standard BERT QA system (results in Table 3). We set the beam size b = 20 for the constrained alignment. We use BERT-base-uncased for all of our experiments, and fine-tune the model using Adam (Kingma and Ba, 2014) with learning rate set to 2e-5. Our preprocessing uses a SpanBERT-based coreference system (Joshi et al., 2020) and a BERT-based SRL system (Shi and Lin, 2019). We limit the length of the context to 51"
2021.teachingnlp-1.17,P17-1171,0,0.0133724,"ogradable fashion, and have collectively been tested on over 300 students across several semesters.1 1 A3. Hidden Markov Models and linear-chain conditional random fields (CRFs) for named entity recognition (NER) (Tjong Kim Sang and De Meulder, 2003), using features similar to those from Zhang and Johnson (2003). A4. Character-level RNN language modeling (Mikolov et al., 2010). A5. Semantic parsing with seq2seq models (Jia and Liang, 2016) on the GeoQuery dataset (Zelle and Mooney, 1996). A6. Reading comprehension on SQuAD (Rajpurkar et al., 2016) using a simplified version of the DrQA model (Chen et al., 2017), similar to BiDAF (Seo et al., 2016). A1-A5 come with autograders. These train each student’s model from scratch and evaluate performance on the development set of each task, verifying whether their code behaves as intended. The autograders are bundled to be deployable on Gradescope using their Docker framework.2 These coding assignments can also be supplemented with conceptual questions for hybrid assignments, though we do not distribute those as part of this release. Introduction This paper presents a series of assignments designed to give a survey of modern NLP through the lens of system-b"
2021.teachingnlp-1.17,N19-1423,0,0.00873045,"ation, and inference required for these different techniques. Neural architectures We cover feedforward networks (A2), recurrent neural encoders (A4, A5, A6), seq2seq models (A5), and attention (A5, A6). From these, Transformers (Vaswani et al., 2017) naturally emerge even though they are not explicitly implemented in an assignment. 2.2 Other Desiderata A major consideration in designing these assignments was to enable understanding without large-scale computational resources. Maintaining simplicity and tractability is the major reason we do not feature more exploration of pre-trained models (Devlin et al., 2019). These factors are also why we choose character-level language model2.1 Covering Model Types ing (rather than word-level) and seq2seq semantic There are far too many NLP tasks and models to cover in a single course. Rather than focus on ex- parsing (rather than translation): training large auposing students to the most important applications, toregressive models to perform well when output vocabularies are in the tens of thousands requires we instead designed these assignments to feature significant engineering expertise. While we teach a range of models along the following typological studen"
2021.teachingnlp-1.17,W19-3621,0,0.0123375,"n some cases have other hands-on components that engage with parsing, but students do not actually build a parser. Instead, sequence models are taken as an example of structured inference, and other classification tasks are used instead of transition systems. From a system-building perspective, the biggest omissions are pre-training and Transformers. These can be explored in the context of final projects, as we describe in the next section. Finally, our courses integrate additional discussion around ethics, with specific discussions surrounding bias in word embeddings (Bolukbasi et al., 2016; Gonen and Goldberg, 2019) and ethical considerations of pre-trained models (Bender et al., 2021), as well as an open-ended discussion surrounding social impact and ethical considerations of NLP, deep learning, and machine learning. These are not formally assessed at present, but we are considering this for future iterations of the course given these topics’ importance. 3 Deployment Assignment Eisenstein Jurafsky + Martin A1 A2 A3 A4 A5 A6 2, 4 3 7 6 12, 18 17.5 4, 5 7 8 7, 9 11, 15 23.2 Table 2: Book chapters associated with each assignment; gray indicates an imperfect match. Our courses use a combination of Eisenstei"
2021.teachingnlp-1.17,P15-1162,0,0.0674012,"Missing"
2021.teachingnlp-1.17,P16-1002,0,0.0581036,"Missing"
2021.teachingnlp-1.17,W05-0104,0,0.128531,"oder-decoder, attention QA, domain adaptation Binary Binary Tags Token seq Token seq Span Linear       Components FFNN Enc Dec Attn            Table 1: Breakdown of assignments. The concepts and model components in each are designed to build on one another. A gray square indicates partial engagement with a concept, typically when students are already given the needed component or it isn’t a focus of the assignment. also covers topics like linear classification, generative modeling (HMMs), and structured inference. Other hands-on courses discussed in prior Teaching NLP papers (Klein, 2005; Madnani and Dorr, 2008; Baldridge and Erk, 2008) make some similar choices about how to blend linguistics and CS concepts, but our desire to integrate deep learning as a primary (but not the sole) focus area guides us towards a different set of assignment topics. 2 Design Principles This set of assignments was designed after we asked ourselves, what should a student taking NLP know how to build? NLP draws on principles from machine learning, statistics, linguistics, algorithms, and more, and we set out to expose students to a range of ideas from these disciplines through the lens of implemen"
2021.teachingnlp-1.17,W08-0209,0,0.0606378,"attention QA, domain adaptation Binary Binary Tags Token seq Token seq Span Linear       Components FFNN Enc Dec Attn            Table 1: Breakdown of assignments. The concepts and model components in each are designed to build on one another. A gray square indicates partial engagement with a concept, typically when students are already given the needed component or it isn’t a focus of the assignment. also covers topics like linear classification, generative modeling (HMMs), and structured inference. Other hands-on courses discussed in prior Teaching NLP papers (Klein, 2005; Madnani and Dorr, 2008; Baldridge and Erk, 2008) make some similar choices about how to blend linguistics and CS concepts, but our desire to integrate deep learning as a primary (but not the sole) focus area guides us towards a different set of assignment topics. 2 Design Principles This set of assignments was designed after we asked ourselves, what should a student taking NLP know how to build? NLP draws on principles from machine learning, statistics, linguistics, algorithms, and more, and we set out to expose students to a range of ideas from these disciplines through the lens of implementation. This choice foll"
2021.teachingnlp-1.17,W02-1011,0,0.0274033,"ovide hands-on experience with concepts and implementation practices that we consider critical for students to master, ranging from linear feature-based models to cutting-edge deep learning approaches. The assignments are as follows: Other Courses and Materials Several other widely-publicized courses like Stanford CS224N and CMU CS 11-747 are much more “neural-first” views of NLP: their assignments delve more heavily into word embeddings and low-level neural implementation like backpropagation. By contrast, this course is designed to be a survey that A1. Sentiment analysis with linear models (Pang et al., 2002) on the Stanford Sentiment Treebank (Socher et al., 2013). ∗ Corresponding author. Subsequent authors listed alphabetically. 1 See https://cs.utexas.edu/~gdurrett for past offerings and static versions of these assignments; contact Greg Durrett for access to the repository with instructor solutions. 2 For the CRF and seq2seq modeling assignments, a custom framework must be used, as Gradescope autograders cannot handle these. We grade these in a batch fashion on a single instructional machine, which poses some logistical challenges. 99 Proceedings of the Fifth Workshop on Teaching NLP, pages 99"
2021.teachingnlp-1.17,D14-1162,0,0.08488,"Missing"
2021.teachingnlp-1.17,D16-1264,0,0.0252985,"tudents freedom to explore. All of them can be deployed in a fully autogradable fashion, and have collectively been tested on over 300 students across several semesters.1 1 A3. Hidden Markov Models and linear-chain conditional random fields (CRFs) for named entity recognition (NER) (Tjong Kim Sang and De Meulder, 2003), using features similar to those from Zhang and Johnson (2003). A4. Character-level RNN language modeling (Mikolov et al., 2010). A5. Semantic parsing with seq2seq models (Jia and Liang, 2016) on the GeoQuery dataset (Zelle and Mooney, 1996). A6. Reading comprehension on SQuAD (Rajpurkar et al., 2016) using a simplified version of the DrQA model (Chen et al., 2017), similar to BiDAF (Seo et al., 2016). A1-A5 come with autograders. These train each student’s model from scratch and evaluate performance on the development set of each task, verifying whether their code behaves as intended. The autograders are bundled to be deployable on Gradescope using their Docker framework.2 These coding assignments can also be supplemented with conceptual questions for hybrid assignments, though we do not distribute those as part of this release. Introduction This paper presents a series of assignments des"
2021.teachingnlp-1.17,D13-1170,0,0.0051608,"ion practices that we consider critical for students to master, ranging from linear feature-based models to cutting-edge deep learning approaches. The assignments are as follows: Other Courses and Materials Several other widely-publicized courses like Stanford CS224N and CMU CS 11-747 are much more “neural-first” views of NLP: their assignments delve more heavily into word embeddings and low-level neural implementation like backpropagation. By contrast, this course is designed to be a survey that A1. Sentiment analysis with linear models (Pang et al., 2002) on the Stanford Sentiment Treebank (Socher et al., 2013). ∗ Corresponding author. Subsequent authors listed alphabetically. 1 See https://cs.utexas.edu/~gdurrett for past offerings and static versions of these assignments; contact Greg Durrett for access to the repository with instructor solutions. 2 For the CRF and seq2seq modeling assignments, a custom framework must be used, as Gradescope autograders cannot handle these. We grade these in a batch fashion on a single instructional machine, which poses some logistical challenges. 99 Proceedings of the Fifth Workshop on Teaching NLP, pages 99–103 June 10–11, 2021. ©2021 Association for Computationa"
2021.teachingnlp-1.17,W03-0434,0,0.0216496,"ear models suitable for industry applications to state-of-theart deep learning models used in NLP research. The assignments are customizable, with constrained options to guide less experienced students or open-ended options giving advanced students freedom to explore. All of them can be deployed in a fully autogradable fashion, and have collectively been tested on over 300 students across several semesters.1 1 A3. Hidden Markov Models and linear-chain conditional random fields (CRFs) for named entity recognition (NER) (Tjong Kim Sang and De Meulder, 2003), using features similar to those from Zhang and Johnson (2003). A4. Character-level RNN language modeling (Mikolov et al., 2010). A5. Semantic parsing with seq2seq models (Jia and Liang, 2016) on the GeoQuery dataset (Zelle and Mooney, 1996). A6. Reading comprehension on SQuAD (Rajpurkar et al., 2016) using a simplified version of the DrQA model (Chen et al., 2017), similar to BiDAF (Seo et al., 2016). A1-A5 come with autograders. These train each student’s model from scratch and evaluate performance on the development set of each task, verifying whether their code behaves as intended. The autograders are bundled to be deployable on Gradescope using thei"
D12-1001,J93-2004,0,\N,Missing
D12-1001,N12-1052,0,\N,Missing
D12-1001,D10-1120,0,\N,Missing
D12-1001,D09-1086,0,\N,Missing
D12-1001,W06-2920,0,\N,Missing
D12-1001,N09-1009,0,\N,Missing
D12-1001,J03-4003,0,\N,Missing
D12-1001,J08-4003,0,\N,Missing
D12-1001,P10-1131,1,\N,Missing
D12-1001,P05-1012,0,\N,Missing
D12-1001,P11-1070,1,\N,Missing
D12-1001,P08-1068,0,\N,Missing
D12-1001,P11-1156,0,\N,Missing
D12-1001,P06-1111,1,\N,Missing
D12-1001,P06-1055,1,\N,Missing
D12-1001,P11-1061,0,\N,Missing
D12-1001,petrov-etal-2012-universal,0,\N,Missing
D12-1001,P09-1042,0,\N,Missing
D12-1001,D11-1006,0,\N,Missing
D12-1001,P04-1061,1,\N,Missing
D12-1001,D11-1005,0,\N,Missing
D12-1001,2005.mtsummit-papers.11,0,\N,Missing
D12-1001,D07-1096,0,\N,Missing
D12-1001,N06-1014,1,\N,Missing
D13-1203,P12-1041,1,0.723037,"Missing"
D13-1203,D08-1031,0,0.347489,"ses and NER tags for each document. All experiments use system mentions except where otherwise indicated. For each experiment, we report MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and CEAFe (Luo, 2005), as well as their average, the CoNLL metric. All metrics are computed using version 5 of the official CoNLL scorer.3 3 A Mention-Synchronous Framework We first present the basic architecture of our coreference system, independent of a feature set. Unlike binary classification-based coreference systems where independent binary decisions are made about each pair (Soon et al., 2001; Bengtson and Roth, 2008; Versley et al., 2008; Stoyanov et al., 2010), we use a log-linear model to select at most one antecedent for 2 This dataset is identical to the English portion of the CoNLL 2012 data, except for the absence of a small pivot text. 3 Note that this version of the scorer implements a modified version of B 3 , described in Cai and Strube (2010), that was used for the CoNLL shared tasks. The implementation of CEAFe is also not exactly as described in Luo et al. (2004), but for completeness we include this metric as well. 1972 each mention or determine that it begins a new cluster (Denis and Baldr"
D13-1203,P06-1005,0,0.836276,"ttom-left cell). As we were able to incorporate syntax with shallow features, so too might we hope to incorporate semantics. However, the semantic information contained even in a coreference corpus of thousands of documents is insufficient to generalize to unseen data,8 so system designers have turned to external resources such as semantic classes derived from WordNet (Soon et al., 2001), WordNet hypernymy or synonymy (Stoyanov et al., 2010), semantic similarity computed from online resources (Ponzetto and Strube, 2006), named entity type features, gender and number match using the dataset of Bergsma and Lin (2006), and features from unsupervised clusters (Hendrickx and Daelemans, 2007; Durrett et al., 2013). In this section, we consider the following subset of these information sources: included in the OntoNotes dataset due to choices in the annotation standard. Second, we divide mentions by their type, pronominal versus nominal/proper; we then further subdivide nominals and propers based on whether or not the head word of the mention has appeared as the head of a previous mention in the document. Table 4 shows the results of our analysis. In each cell, we show the fraction of mentions that we correctl"
D13-1203,W12-4503,0,0.397898,"Missing"
D13-1203,W11-1905,0,0.0610802,"Missing"
D13-1203,W10-4305,0,0.0182212,"Synchronous Framework We first present the basic architecture of our coreference system, independent of a feature set. Unlike binary classification-based coreference systems where independent binary decisions are made about each pair (Soon et al., 2001; Bengtson and Roth, 2008; Versley et al., 2008; Stoyanov et al., 2010), we use a log-linear model to select at most one antecedent for 2 This dataset is identical to the English portion of the CoNLL 2012 data, except for the absence of a small pivot text. 3 Note that this version of the scorer implements a modified version of B 3 , described in Cai and Strube (2010), that was used for the CoNLL shared tasks. The implementation of CEAFe is also not exactly as described in Luo et al. (2004), but for completeness we include this metric as well. 1972 each mention or determine that it begins a new cluster (Denis and Baldridge, 2008). In this mentionranking or mention-synchronous framework, features examine single mentions to evaluate whether or not they are anaphoric and pairs of mentions to evaluate whether or not they corefer. While other work has used this framework as a starting point for entity-level systems (Luo et al., 2004; Rahman and Ng, 2009; Haghig"
D13-1203,W12-4504,0,0.0141952,"impact pronoun resolution, and they allow speaker match to capture effects like I and you being coreferent when the speakers differ. features with regularization also means that we organically get distinctions among different mention types without having to choose a level of granularity a priori, unlike the distinct classifiers employed by Denis and Baldridge (2008). In terms of architecture, many coreference systems operate in a pipelined fashion, making partial decisions about coreference or pruning arcs before full resolution. Some systems use separate rule-based and learning-based passes (Chen and Ng, 2012; Fernandes et al., 2012), a series of learning-based passes (Bj¨orkelund and Farkas, 2012), or referentiality classifiers that prune the set of mentions before resolution (Rahman and Ng, 2009; Bj¨orkelund and Farkas, 2012; Recasens et al., 2013b). By contrast, our system resolves all mentions in one pass and does not need pruning: the S URFACE system can train in less than two hours without any subsampling of coreference arcs, and rule-based pruning of coreference arcs actually causes our system to perform less well, since our features can learn valuable information from these negative exampl"
D13-1203,D08-1069,0,0.672721,"and Roth, 2008; Versley et al., 2008; Stoyanov et al., 2010), we use a log-linear model to select at most one antecedent for 2 This dataset is identical to the English portion of the CoNLL 2012 data, except for the absence of a small pivot text. 3 Note that this version of the scorer implements a modified version of B 3 , described in Cai and Strube (2010), that was used for the CoNLL shared tasks. The implementation of CEAFe is also not exactly as described in Luo et al. (2004), but for completeness we include this metric as well. 1972 each mention or determine that it begins a new cluster (Denis and Baldridge, 2008). In this mentionranking or mention-synchronous framework, features examine single mentions to evaluate whether or not they are anaphoric and pairs of mentions to evaluate whether or not they corefer. While other work has used this framework as a starting point for entity-level systems (Luo et al., 2004; Rahman and Ng, 2009; Haghighi and Klein, 2010; Durrett et al., 2013), we will show that a mention-synchronous approach is sufficient to get state-of-the-art performance on its own. 3.1 Mention Detection Our system first identifies a set of predicted mentions from text annotated with parses and"
D13-1203,P13-1012,1,0.797376,"the CoNLL shared tasks. The implementation of CEAFe is also not exactly as described in Luo et al. (2004), but for completeness we include this metric as well. 1972 each mention or determine that it begins a new cluster (Denis and Baldridge, 2008). In this mentionranking or mention-synchronous framework, features examine single mentions to evaluate whether or not they are anaphoric and pairs of mentions to evaluate whether or not they corefer. While other work has used this framework as a starting point for entity-level systems (Luo et al., 2004; Rahman and Ng, 2009; Haghighi and Klein, 2010; Durrett et al., 2013), we will show that a mention-synchronous approach is sufficient to get state-of-the-art performance on its own. 3.1 Mention Detection Our system first identifies a set of predicted mentions from text annotated with parses and named entity tags. We extract three distinct types of mentions: proper mentions from all named entity chunks except for those labeled as QUANTITY, CARDINAL, or PERCENT, pronominal mentions from single words tagged with PRP or PRP$, and nominal mentions from all other maximal NP projections. These basic rules are similar to those of Lee et al. (2011), except that their sy"
D13-1203,N10-1112,0,0.0427848,"of its gold antecedents are among the set of system predicted mentions. Given t training examples of the form (xk , Ck∗ ), we write the following likelihood function:   t X X `(w) = log  P 0 (a|xk ) + λkwk1 k=1 a∈A(Ck∗ ) where P 0 (a|xk ) ∝ P (a|xk ) exp(l(a, Ck∗ )) with l(a, C ∗ ) being a real-valued loss function. The loss 4 Because of this marginalization over latent antecedent choices, our objective is non-convex. here plays an analogous role to the loss in structured max-margin objectives; incorporating it into a conditional likelihood objective is a technique called softmax-margin (Gimpel and Smith, 2010). Our loss function l(a, C ∗ ) is a weighted linear combination of three error types, examples of which are shown in Figure 1. A false anaphor (FA) error occurs when ai is chosen to be anaphoric when it should start a new cluster. A false new (FN) error occurs in the opposite case, when ai wrongly indicates a new cluster when it should be anaphoric. Finally, a wrong link (WL) error occurs when the antecedent chosen for ai is the wrong antecedent (but ai is indeed anaphoric). Our final parameterized loss function is a weighted sum of the counts of these three error types: l(a, C ∗ ) = αFA FA(a,"
D13-1203,J95-2003,0,0.399887,"able to substantially outperform the other two, the two best publicly-available English coreference systems. Bolded values are significant with p < 0.05 according to a bootstrap resampling test. ence features, just implicitly. For example, rather than having rules targeting person, number, gender, or animacy of mentions, we use conjunctions with pronoun identity, which contains this information. Rather than explicitly writing a feature targeting definiteness, our indicators on the first word of a mention will capture this and other effects. And finally, rather than targeting centering theory (Grosz et al., 1995) with rule-based features identifying syntactic positions (Stoyanov et al., 2010; Haghighi and Klein, 2010), our features on word context can identify configurational clues like whether a mention is preceded or followed by a verb, and therefore whether it is likely in subject or object position.5 Not only are data-driven features able to capture the same phenomena as heuristic-driven features, but they do so at a finer level of granularity, and can therefore model more patterns in the data. To contrast these two types of features, we experiment with three ablated versions of our system, where"
D13-1203,D09-1120,1,0.780034,"Missing"
D13-1203,N10-1061,1,0.953109,"(2010), that was used for the CoNLL shared tasks. The implementation of CEAFe is also not exactly as described in Luo et al. (2004), but for completeness we include this metric as well. 1972 each mention or determine that it begins a new cluster (Denis and Baldridge, 2008). In this mentionranking or mention-synchronous framework, features examine single mentions to evaluate whether or not they are anaphoric and pairs of mentions to evaluate whether or not they corefer. While other work has used this framework as a starting point for entity-level systems (Luo et al., 2004; Rahman and Ng, 2009; Haghighi and Klein, 2010; Durrett et al., 2013), we will show that a mention-synchronous approach is sufficient to get state-of-the-art performance on its own. 3.1 Mention Detection Our system first identifies a set of predicted mentions from text annotated with parses and named entity tags. We extract three distinct types of mentions: proper mentions from all named entity chunks except for those labeled as QUANTITY, CARDINAL, or PERCENT, pronominal mentions from single words tagged with PRP or PRP$, and nominal mentions from all other maximal NP projections. These basic rules are similar to those of Lee et al. (2011"
D13-1203,N06-2015,0,0.326909,"Missing"
D13-1203,P13-1049,0,0.0178986,"simple a feature set as possible, we are able to outperform even these sophisticated systems by a wide margin. 7 Related Work Many of the individual features we employ in the F I NAL feature set have appeared in other coreference systems (Bj¨orkelund and Nugues, 2011; Rahman and Ng, 2011b; Fernandes et al., 2012). However, other authors have often emphasized bilexical features on head pairs, whereas our features are heavily monolexical. For feature conjunctions, other authors have exploited three classes (Lee et al., 2011) or automatically learned conjunction schemes (Fernandes et al., 2012; Lassalle and Denis, 2013), but to our knowledge we are the first to do fine-grained modeling of every pronoun. Inclusion of a hierarchy of 10 Discrepancies between scores here and those printed in Pradhan et al. (2012) arise from two sources: improvements to the system of Lee et al. (2011) since the first CoNLL shared task, and a fix to the scoring of B 3 in the official scorer since results of the two CoNLL shared tasks were released. Unfortunately, because of this bug in the scoring program, direct comparison to the printed results of the other highest-scoring English systems, Fernandes et al. (2012) and Martschat e"
D13-1203,W11-1902,0,0.308117,"Missing"
D13-1203,P04-1018,0,0.0209192,"ary classification-based coreference systems where independent binary decisions are made about each pair (Soon et al., 2001; Bengtson and Roth, 2008; Versley et al., 2008; Stoyanov et al., 2010), we use a log-linear model to select at most one antecedent for 2 This dataset is identical to the English portion of the CoNLL 2012 data, except for the absence of a small pivot text. 3 Note that this version of the scorer implements a modified version of B 3 , described in Cai and Strube (2010), that was used for the CoNLL shared tasks. The implementation of CEAFe is also not exactly as described in Luo et al. (2004), but for completeness we include this metric as well. 1972 each mention or determine that it begins a new cluster (Denis and Baldridge, 2008). In this mentionranking or mention-synchronous framework, features examine single mentions to evaluate whether or not they are anaphoric and pairs of mentions to evaluate whether or not they corefer. While other work has used this framework as a starting point for entity-level systems (Luo et al., 2004; Rahman and Ng, 2009; Haghighi and Klein, 2010; Durrett et al., 2013), we will show that a mention-synchronous approach is sufficient to get state-of-the"
D13-1203,H05-1004,0,0.962662,"Missing"
D13-1203,W12-4511,0,0.0391712,"Missing"
D13-1203,N06-1025,0,0.241475,"Missing"
D13-1203,W11-1901,0,0.266008,"Missing"
D13-1203,W12-4501,0,0.623001,"ave appeared in other coreference systems (Bj¨orkelund and Nugues, 2011; Rahman and Ng, 2011b; Fernandes et al., 2012). However, other authors have often emphasized bilexical features on head pairs, whereas our features are heavily monolexical. For feature conjunctions, other authors have exploited three classes (Lee et al., 2011) or automatically learned conjunction schemes (Fernandes et al., 2012; Lassalle and Denis, 2013), but to our knowledge we are the first to do fine-grained modeling of every pronoun. Inclusion of a hierarchy of 10 Discrepancies between scores here and those printed in Pradhan et al. (2012) arise from two sources: improvements to the system of Lee et al. (2011) since the first CoNLL shared task, and a fix to the scoring of B 3 in the official scorer since results of the two CoNLL shared tasks were released. Unfortunately, because of this bug in the scoring program, direct comparison to the printed results of the other highest-scoring English systems, Fernandes et al. (2012) and Martschat et al. (2012), is impossible. 1979 Feature name Features of the S URFACE system Features on the current mention [ ANAPHORIC ] + [ CURRENT ANCESTRY ] Features on the antecedent [ ANTECEDENT ANCES"
D13-1203,D09-1101,0,0.457636,"ed in Cai and Strube (2010), that was used for the CoNLL shared tasks. The implementation of CEAFe is also not exactly as described in Luo et al. (2004), but for completeness we include this metric as well. 1972 each mention or determine that it begins a new cluster (Denis and Baldridge, 2008). In this mentionranking or mention-synchronous framework, features examine single mentions to evaluate whether or not they are anaphoric and pairs of mentions to evaluate whether or not they corefer. While other work has used this framework as a starting point for entity-level systems (Luo et al., 2004; Rahman and Ng, 2009; Haghighi and Klein, 2010; Durrett et al., 2013), we will show that a mention-synchronous approach is sufficient to get state-of-the-art performance on its own. 3.1 Mention Detection Our system first identifies a set of predicted mentions from text annotated with parses and named entity tags. We extract three distinct types of mentions: proper mentions from all named entity chunks except for those labeled as QUANTITY, CARDINAL, or PERCENT, pronominal mentions from single words tagged with PRP or PRP$, and nominal mentions from all other maximal NP projections. These basic rules are similar to"
D13-1203,P11-1082,0,0.158493,"herefore improve pronoun resolution substantially; however, these features generally only improve pronoun resolution. Full results for our S URFACE and F INAL feature sets are shown in Table 7. Again, we compare to Lee et al. (2011) and Bj¨orkelund and Farkas (2012).10 Despite our system’s emphasis on one-pass resolution with as simple a feature set as possible, we are able to outperform even these sophisticated systems by a wide margin. 7 Related Work Many of the individual features we employ in the F I NAL feature set have appeared in other coreference systems (Bj¨orkelund and Nugues, 2011; Rahman and Ng, 2011b; Fernandes et al., 2012). However, other authors have often emphasized bilexical features on head pairs, whereas our features are heavily monolexical. For feature conjunctions, other authors have exploited three classes (Lee et al., 2011) or automatically learned conjunction schemes (Fernandes et al., 2012; Lassalle and Denis, 2013), but to our knowledge we are the first to do fine-grained modeling of every pronoun. Inclusion of a hierarchy of 10 Discrepancies between scores here and those printed in Pradhan et al. (2012) arise from two sources: improvements to the system of Lee et al. (2011"
D13-1203,N13-1110,0,0.0377356,"ithout having to choose a level of granularity a priori, unlike the distinct classifiers employed by Denis and Baldridge (2008). In terms of architecture, many coreference systems operate in a pipelined fashion, making partial decisions about coreference or pruning arcs before full resolution. Some systems use separate rule-based and learning-based passes (Chen and Ng, 2012; Fernandes et al., 2012), a series of learning-based passes (Bj¨orkelund and Farkas, 2012), or referentiality classifiers that prune the set of mentions before resolution (Rahman and Ng, 2009; Bj¨orkelund and Farkas, 2012; Recasens et al., 2013b). By contrast, our system resolves all mentions in one pass and does not need pruning: the S URFACE system can train in less than two hours without any subsampling of coreference arcs, and rule-based pruning of coreference arcs actually causes our system to perform less well, since our features can learn valuable information from these negative examples. Prec. MUC Rec. F1 S TANFORD IMS S URFACE * F INAL * 61.62 66.67 68.42 68.97 59.34 58.20 60.80 63.47 60.46 62.15 64.39 66.10 S TANFORD IMS F INAL * 60.91 68.15 66.81 62.13 61.60 66.04 61.51 64.71 66.43 B3 CEAFe Prec. Rec. F1 Prec. Rec. CoNLL"
D13-1203,N13-1071,0,0.154885,"Missing"
D13-1203,J01-4004,0,0.982701,"ndard automatic parses and NER tags for each document. All experiments use system mentions except where otherwise indicated. For each experiment, we report MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and CEAFe (Luo, 2005), as well as their average, the CoNLL metric. All metrics are computed using version 5 of the official CoNLL scorer.3 3 A Mention-Synchronous Framework We first present the basic architecture of our coreference system, independent of a feature set. Unlike binary classification-based coreference systems where independent binary decisions are made about each pair (Soon et al., 2001; Bengtson and Roth, 2008; Versley et al., 2008; Stoyanov et al., 2010), we use a log-linear model to select at most one antecedent for 2 This dataset is identical to the English portion of the CoNLL 2012 data, except for the absence of a small pivot text. 3 Note that this version of the scorer implements a modified version of B 3 , described in Cai and Strube (2010), that was used for the CoNLL shared tasks. The implementation of CEAFe is also not exactly as described in Luo et al. (2004), but for completeness we include this metric as well. 1972 each mention or determine that it begins a new"
D13-1203,P09-1074,0,0.0552793,"Missing"
D13-1203,P10-2029,0,0.189469,"ents use system mentions except where otherwise indicated. For each experiment, we report MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and CEAFe (Luo, 2005), as well as their average, the CoNLL metric. All metrics are computed using version 5 of the official CoNLL scorer.3 3 A Mention-Synchronous Framework We first present the basic architecture of our coreference system, independent of a feature set. Unlike binary classification-based coreference systems where independent binary decisions are made about each pair (Soon et al., 2001; Bengtson and Roth, 2008; Versley et al., 2008; Stoyanov et al., 2010), we use a log-linear model to select at most one antecedent for 2 This dataset is identical to the English portion of the CoNLL 2012 data, except for the absence of a small pivot text. 3 Note that this version of the scorer implements a modified version of B 3 , described in Cai and Strube (2010), that was used for the CoNLL shared tasks. The implementation of CEAFe is also not exactly as described in Luo et al. (2004), but for completeness we include this metric as well. 1972 each mention or determine that it begins a new cluster (Denis and Baldridge, 2008). In this mentionranking or mention"
D13-1203,M95-1005,0,0.94123,"Missing"
D13-1203,W12-4502,0,\N,Missing
D13-1203,D08-1067,0,\N,Missing
D17-1275,P13-1035,0,0.049848,"Missing"
D17-1275,D14-1082,0,0.0202088,"evel Evaluation Another axis of variation in metrics comes from whether we consider token-level or phrase-level outputs. As noted in the previous section, we did not annotate noun phrases, but we may actually be interested in identifying them. In Figure 1, for example, extracting Backconnect bot is more useful than extracting bot in isolation, since bot is a less specific characterization of the product. We can convert our token-level annotations to phrase-level annotations by projecting our annotations to the noun phrase level based on the output of an automatic parser. We used the parser of Chen and Manning (2014) to parse all sentences of each post. For each annotated token that was given a nominal tag (N*), we projected that token to the largest NP containing it of length less than or equal to 7; most product NPs are shorter than this, and when the parser predicts a longer NP, our analysis found that it typically reflects a mistake. In Figure 1, the entire noun phrase Backconnect bot would be labeled as a product. For products realized as verbs (e.g., hack), we leave the annotation as the single token. Throughout the rest of this work, we will evaluate sometimes at the token-level and sometimes at 6"
D17-1275,P07-1033,0,0.259721,"Missing"
D17-1275,P05-1045,0,0.0405631,"ervised methods (Section 5). Baselines One approach takes the most frequent noun or verb in a post and classifies all occurrences of that word type as products. A more sophisticated lexical baseline is based on a product dictionary extracted from our training data: we tag the most frequent noun or verb in a post that also appears in this dictionary. This method fails primarily in that it prefers to extract common words like account and website even when they do not occur as products. The most relevant off-theshelf system is an NER tagging model; we retrain the Stanford NER system on our data (Finkel et al., 2005). Finally, we can tag the first noun phrase of the post as a product, which will often capture the product if it is mentioned in the title of the post.8 We also include human performance results. We averaged the results for annotators compared with the consensus annotations. For the phrase level evaluation, we apply the projection method described in Section 3.1. Binary classifier/CRF One learning-based approach to this task is to employ a binary SVM classifier for each token in isolation. We also experimented with a token-level CRF with a binary tagset, and found identical performance, so we"
D17-1275,N13-1014,0,0.134481,"on extraction perspective. Having annotated a dataset, we examine supervised and semi-supervised learning approaches to the product extraction problem. Binary or CRF classification of tokens as products is effective, but performance drops off precipitously when a system trained on one forum is applied to a different forum: in this sense, even two different cybercrime forums seem to represent different “finegrained domains.” Since we want to avoid having to annotate data for every new forum that might need to be analyzed, we explore several methods for adaptation, mixing type-level annotation (Garrette and Baldridge, 2013; Garrette et al., 2013), token-level annotation (Daume III, 2007), and semi-supervised approaches (Turian et al., 2010; Kshirsagar et al., 2015). We find little improvement from these methods and discuss why they fail to have a larger impact. Overall, our results characterize the challenges of our fine-grained domain adaptation problem in online marketplace data. We believe that this new dataset provides a useful testbed for additional inquiry and investigation into modeling of finegrained domain differences. 2 Dataset and Annotation We consider several forums that vary in the nature of produ"
D17-1275,P13-1057,0,0.123805,"ing annotated a dataset, we examine supervised and semi-supervised learning approaches to the product extraction problem. Binary or CRF classification of tokens as products is effective, but performance drops off precipitously when a system trained on one forum is applied to a different forum: in this sense, even two different cybercrime forums seem to represent different “finegrained domains.” Since we want to avoid having to annotate data for every new forum that might need to be analyzed, we explore several methods for adaptation, mixing type-level annotation (Garrette and Baldridge, 2013; Garrette et al., 2013), token-level annotation (Daume III, 2007), and semi-supervised approaches (Turian et al., 2010; Kshirsagar et al., 2015). We find little improvement from these methods and discuss why they fail to have a larger impact. Overall, our results characterize the challenges of our fine-grained domain adaptation problem in online marketplace data. We believe that this new dataset provides a useful testbed for additional inquiry and investigation into modeling of finegrained domain differences. 2 Dataset and Annotation We consider several forums that vary in the nature of products being traded: • Dark"
D17-1275,D15-1157,0,0.0604442,"Missing"
D17-1275,W10-2923,0,0.0239519,"oth slot-filling information extraction (with provenance information) as well as standard named-entity recognition (NER). Compared to NER, our task features a higher dependence on context: we only care about the specific product being bought or sold in a post, not other products that might be mentioned. Moreover, because we are operating over forums, the data is substantially messier than classical NER corpora like CoNLL (Tjong Kim Sang and De Meulder, 2003). While prior work has dealt with these messy characteristics for syntax (Kaljahi et al., 2015) and for discourse (Lui and Baldwin, 2010; Kim et al., 2010; Wang et al., 2011), our work is the first to tackle forum data (and marketplace forums specifically) from an information extraction perspective. Having annotated a dataset, we examine supervised and semi-supervised learning approaches to the product extraction problem. Binary or CRF classification of tokens as products is effective, but performance drops off precipitously when a system trained on one forum is applied to a different forum: in this sense, even two different cybercrime forums seem to represent different “finegrained domains.” Since we want to avoid having to annotate data for e"
D17-1275,D15-1032,1,0.800204,"es a useful form of prior knowledge, namely that each post has exactly one product in almost all cases. Our post-level system is formulated as an instance of a latent SVM (Yu and Joachims, 2009). The output space is the set of all tokens (or noun phrases, in the NP case) in the post. The latent variable is the choice of token/NP to select, since there may be multiple correct choices of product tokens. The features used on each token/NP are the same as in the token classifier. We trained all of the learned models by subgradient descent on the primal form of the objective (Ratliff et al., 2007; Kummerfeld et al., 2015). We use AdaGrad (Duchi et al., 2011) to speed convergence in the presence of a large weight vector with heterogeneous feature types. All product extractors in this section are trained for 5 iterations with `1 -regularization tuned on the development set. 2602 Freq Dict NER Binary Post Human∗ Freq Dict First NER Binary Post Human∗ P 41.9 57.9 59.7 62.4 82.4 86.9 P 61.8 57.9 73.1 63.6 67.0 87.6 87.6 Token Prediction Tokens Products R F1 P R F1 42.5 42.2 48.4 33.5 39.6 51.1 54.3 65.6 44.0 52.7 62.2 60.9 60.8 62.6 61.7 76.0 68.5 58.1 77.6 66.4 36.1 50.3 83.5 56.6 67.5 80.4 83.5 87.7 77.6 82.2 NP"
D17-1275,U10-1009,0,0.0262503,"k has similarities to both slot-filling information extraction (with provenance information) as well as standard named-entity recognition (NER). Compared to NER, our task features a higher dependence on context: we only care about the specific product being bought or sold in a post, not other products that might be mentioned. Moreover, because we are operating over forums, the data is substantially messier than classical NER corpora like CoNLL (Tjong Kim Sang and De Meulder, 2003). While prior work has dealt with these messy characteristics for syntax (Kaljahi et al., 2015) and for discourse (Lui and Baldwin, 2010; Kim et al., 2010; Wang et al., 2011), our work is the first to tackle forum data (and marketplace forums specifically) from an information extraction perspective. Having annotated a dataset, we examine supervised and semi-supervised learning approaches to the product extraction problem. Binary or CRF classification of tokens as products is effective, but performance drops off precipitously when a system trained on one forum is applied to a different forum: in this sense, even two different cybercrime forums seem to represent different “finegrained domains.” Since we want to avoid having to a"
D17-1275,P14-5010,0,0.00355307,"ed every post in the Darkode training, Hack Forums training, Blackhat test, and Nulled test sets; these annotations were then merged into a final annotation by majority vote. The development and test sets for Darkode and Hack Forums were annotated by additional team members (five for Darkode, one for Hack Forums), and then every disagreement was discussed and resolved to produce a final annotation. The authors, who are researchers in either NLP or computer security, did all of the annotation. We preprocessed the data using the tokenizer and sentence-splitter from the Stanford CoreNLP toolkit (Manning et al., 2014). Note that many sentences in the data are already delimited by line breaks, making the sentence-splitting task much easier. We performed annotation on the tokenized data so that annotations would be consistent with surrounding punctuation and hyphenated words. Our full annotation guide is available with our data release.3 Our basic annotation principle is 2 The table does not include additional posts that were labeled by all annotators in order to check agreement. 3 https://evidencebasedsecurity.org/ forums/annotation-guide.pdf to annotate tokens when they are either the product that will be"
D17-1275,P13-1108,0,0.0727509,"Missing"
D17-1275,P10-1040,0,0.780109,"uct extraction problem. Binary or CRF classification of tokens as products is effective, but performance drops off precipitously when a system trained on one forum is applied to a different forum: in this sense, even two different cybercrime forums seem to represent different “finegrained domains.” Since we want to avoid having to annotate data for every new forum that might need to be analyzed, we explore several methods for adaptation, mixing type-level annotation (Garrette and Baldridge, 2013; Garrette et al., 2013), token-level annotation (Daume III, 2007), and semi-supervised approaches (Turian et al., 2010; Kshirsagar et al., 2015). We find little improvement from these methods and discuss why they fail to have a larger impact. Overall, our results characterize the challenges of our fine-grained domain adaptation problem in online marketplace data. We believe that this new dataset provides a useful testbed for additional inquiry and investigation into modeling of finegrained domain differences. 2 Dataset and Annotation We consider several forums that vary in the nature of products being traded: • Darkode: Cybercriminal wares, including exploit kits, spam services, ransomware programs, and steal"
D17-1275,D11-1002,0,0.025689,"nformation extraction (with provenance information) as well as standard named-entity recognition (NER). Compared to NER, our task features a higher dependence on context: we only care about the specific product being bought or sold in a post, not other products that might be mentioned. Moreover, because we are operating over forums, the data is substantially messier than classical NER corpora like CoNLL (Tjong Kim Sang and De Meulder, 2003). While prior work has dealt with these messy characteristics for syntax (Kaljahi et al., 2015) and for discourse (Lui and Baldwin, 2010; Kim et al., 2010; Wang et al., 2011), our work is the first to tackle forum data (and marketplace forums specifically) from an information extraction perspective. Having annotated a dataset, we examine supervised and semi-supervised learning approaches to the product extraction problem. Binary or CRF classification of tokens as products is effective, but performance drops off precipitously when a system trained on one forum is applied to a different forum: in this sense, even two different cybercrime forums seem to represent different “finegrained domains.” Since we want to avoid having to annotate data for every new forum that"
D17-1275,Q17-1036,0,0.0298178,"to train models available at https://evidencebasedsecurity.org/forums/ 2013). One domain for which automated analysis is particularly useful is Internet security: researchers obtain large amounts of text data pertinent to active threats or ongoing cybercriminal activity, for which the ability to rapidly characterize that text and draw conclusions can reap major benefits (Krebs, 2013a,b). However, conducting automatic analysis is difficult because this data is outof-domain for conventional NLP models, which harms the performance of both discrete models (McClosky et al., 2010) and deep models (Zhang et al., 2017). Not only that, we show that data from one cybercrime forum is even out of domain with respect to another cybercrime forum, making this data especially challenging. In this work, we present the task of identifying products being bought and sold in the marketplace sections of these online cybercrime forums. 2598 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2598–2607 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Words Products Annotated Annotators Inter-annotator agreement Forum Posts per post per pos"
D17-1275,N10-1004,0,0.0237298,"013; O’Connor et al., 1 Dataset and code to train models available at https://evidencebasedsecurity.org/forums/ 2013). One domain for which automated analysis is particularly useful is Internet security: researchers obtain large amounts of text data pertinent to active threats or ongoing cybercriminal activity, for which the ability to rapidly characterize that text and draw conclusions can reap major benefits (Krebs, 2013a,b). However, conducting automatic analysis is difficult because this data is outof-domain for conventional NLP models, which harms the performance of both discrete models (McClosky et al., 2010) and deep models (Zhang et al., 2017). Not only that, we show that data from one cybercrime forum is even out of domain with respect to another cybercrime forum, making this data especially challenging. In this work, we present the task of identifying products being bought and sold in the marketplace sections of these online cybercrime forums. 2598 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2598–2607 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Words Products Annotated Annotators Inter-annotator a"
D18-1126,L18-1707,0,0.0153093,"le at https://github.com/davidandym/wikilinks-ned These approaches typically focus on aggregating information from a mix of sources, including long-range information from the textual context or other linked entities. While this approach is suitable for entity linking settings such as newswire (Bentivogli, 2010) and Wikipedia (Ratinov et al., 2011), we cannot always rely on this information in other settings like Twitter (Guo et al., 2013; Fang and Chang, 2014; Huang et al., 2014; Dredze et al., 2016), Snapchat (Moon et al., 2018), other web platforms (Eshel et al., 2017), or dialogue systems (Bowden et al., 2018). We need models that can make effective use of limited context windows in noisy settings. In this work, we investigate this problem of effectively using context in the setting of the WikilinksNED dataset from Eshel et al. (2017). The examples in this dataset, which consists of 3.2 million entity disambiguation examples derived from Wikilinks (Singh et al., 2012), have at most 20 words of context on either side and usually no other mentions of the entity being disambiguated. We build off a state-of-the-art attentive LSTM model from prior work (Eshel et al., 2017) and show that despite its good"
D18-1126,D13-1184,0,0.0492063,"Missing"
D18-1126,D07-1074,0,0.13586,"Missing"
D18-1126,W16-6204,0,0.0141881,"ticated neural architectures (Gupta et al., 2017; Sil et al., 2018). ∗ 1 Work done while at UT Austin. Code available at https://github.com/davidandym/wikilinks-ned These approaches typically focus on aggregating information from a mix of sources, including long-range information from the textual context or other linked entities. While this approach is suitable for entity linking settings such as newswire (Bentivogli, 2010) and Wikipedia (Ratinov et al., 2011), we cannot always rely on this information in other settings like Twitter (Guo et al., 2013; Fang and Chang, 2014; Huang et al., 2014; Dredze et al., 2016), Snapchat (Moon et al., 2018), other web platforms (Eshel et al., 2017), or dialogue systems (Bowden et al., 2018). We need models that can make effective use of limited context windows in noisy settings. In this work, we investigate this problem of effectively using context in the setting of the WikilinksNED dataset from Eshel et al. (2017). The examples in this dataset, which consists of 3.2 million entity disambiguation examples derived from Wikilinks (Singh et al., 2012), have at most 20 words of context on either side and usually no other mentions of the entity being disambiguated. We bu"
D18-1126,Q14-1037,1,0.934645,"Missing"
D18-1126,K17-1008,0,0.196952,"Work done while at UT Austin. Code available at https://github.com/davidandym/wikilinks-ned These approaches typically focus on aggregating information from a mix of sources, including long-range information from the textual context or other linked entities. While this approach is suitable for entity linking settings such as newswire (Bentivogli, 2010) and Wikipedia (Ratinov et al., 2011), we cannot always rely on this information in other settings like Twitter (Guo et al., 2013; Fang and Chang, 2014; Huang et al., 2014; Dredze et al., 2016), Snapchat (Moon et al., 2018), other web platforms (Eshel et al., 2017), or dialogue systems (Bowden et al., 2018). We need models that can make effective use of limited context windows in noisy settings. In this work, we investigate this problem of effectively using context in the setting of the WikilinksNED dataset from Eshel et al. (2017). The examples in this dataset, which consists of 3.2 million entity disambiguation examples derived from Wikilinks (Singh et al., 2012), have at most 20 words of context on either side and usually no other mentions of the entity being disambiguated. We build off a state-of-the-art attentive LSTM model from prior work (Eshel e"
D18-1126,D15-1166,0,0.0539975,"ld entity’s actual Wikipedia title. In these instances, the model fails to attend correctly to words that we, as human readers, would most likely see as disambiguating terms. For instance, in this example’s left context: ...known also for the B.P. Koirala Institute of Health Sciences, one of the biggest government hospital. The indigenous people of Dharan are Limbu ... Exploiting Context Cues Attention One way to improve over the basic GRU model is to use attention over the context based on the title under consideration. The attention we use is a modified version of the dot product attention (Luong et al., 2015) used by Eshel et al. (2017), allowing the model to weight the importance of the outputs of the GRU at each time step. Each context (left and right) has its own attention weights. For a given side of context and candidate t, the attention first computes a transformation of the entity embedding et as follows: qt = tanh(W et ). This allows the model to learn an attention query qt distinct from the candidate embedding et . The model then computes attention probabilities αi for each GRU output oi , normalized over the entire sequence of GRU outputs (of length n): αi = softmaxi (qt · oi ) a= n X th"
D18-1126,P16-1101,0,0.078552,"Missing"
D18-1126,P18-1186,0,0.0254983,"pta et al., 2017; Sil et al., 2018). ∗ 1 Work done while at UT Austin. Code available at https://github.com/davidandym/wikilinks-ned These approaches typically focus on aggregating information from a mix of sources, including long-range information from the textual context or other linked entities. While this approach is suitable for entity linking settings such as newswire (Bentivogli, 2010) and Wikipedia (Ratinov et al., 2011), we cannot always rely on this information in other settings like Twitter (Guo et al., 2013; Fang and Chang, 2014; Huang et al., 2014; Dredze et al., 2016), Snapchat (Moon et al., 2018), other web platforms (Eshel et al., 2017), or dialogue systems (Bowden et al., 2018). We need models that can make effective use of limited context windows in noisy settings. In this work, we investigate this problem of effectively using context in the setting of the WikilinksNED dataset from Eshel et al. (2017). The examples in this dataset, which consists of 3.2 million entity disambiguation examples derived from Wikilinks (Singh et al., 2012), have at most 20 words of context on either side and usually no other mentions of the entity being disambiguated. We build off a state-of-the-art att"
D18-1126,P11-1138,0,0.0986301,"Missing"
D18-1126,Q14-1021,0,0.0158716,"rancisLandau et al., 2016), or more sophisticated neural architectures (Gupta et al., 2017; Sil et al., 2018). ∗ 1 Work done while at UT Austin. Code available at https://github.com/davidandym/wikilinks-ned These approaches typically focus on aggregating information from a mix of sources, including long-range information from the textual context or other linked entities. While this approach is suitable for entity linking settings such as newswire (Bentivogli, 2010) and Wikipedia (Ratinov et al., 2011), we cannot always rely on this information in other settings like Twitter (Guo et al., 2013; Fang and Chang, 2014; Huang et al., 2014; Dredze et al., 2016), Snapchat (Moon et al., 2018), other web platforms (Eshel et al., 2017), or dialogue systems (Bowden et al., 2018). We need models that can make effective use of limited context windows in noisy settings. In this work, we investigate this problem of effectively using context in the setting of the WikilinksNED dataset from Eshel et al. (2017). The examples in this dataset, which consists of 3.2 million entity disambiguation examples derived from Wikilinks (Singh et al., 2012), have at most 20 words of context on either side and usually no other mention"
D18-1126,P16-1213,0,0.0240769,"Missing"
D18-1126,N16-1150,1,0.890278,"Missing"
D18-1126,N13-1122,0,0.0219774,"rks (Sun et al.; FrancisLandau et al., 2016), or more sophisticated neural architectures (Gupta et al., 2017; Sil et al., 2018). ∗ 1 Work done while at UT Austin. Code available at https://github.com/davidandym/wikilinks-ned These approaches typically focus on aggregating information from a mix of sources, including long-range information from the textual context or other linked entities. While this approach is suitable for entity linking settings such as newswire (Bentivogli, 2010) and Wikipedia (Ratinov et al., 2011), we cannot always rely on this information in other settings like Twitter (Guo et al., 2013; Fang and Chang, 2014; Huang et al., 2014; Dredze et al., 2016), Snapchat (Moon et al., 2018), other web platforms (Eshel et al., 2017), or dialogue systems (Bowden et al., 2018). We need models that can make effective use of limited context windows in noisy settings. In this work, we investigate this problem of effectively using context in the setting of the WikilinksNED dataset from Eshel et al. (2017). The examples in this dataset, which consists of 3.2 million entity disambiguation examples derived from Wikilinks (Singh et al., 2012), have at most 20 words of context on either side and us"
D18-1126,D17-1284,0,0.0751633,"task: in isolation, the mention Richard Wright could refer to three possible entities in Wikipedia’s knowledge base corresponding to an artist, a musician, or an author. Previous work in this area has distilled context information by exploiting tfidf features (Cucerzan, 2007; Milne and Witten, 2008; Ratinov et al., 2011), global link coherence (Hoffart et al.; Sil and Florian, 2016), cues from coreference (Cheng and Roth, 2013; Hajishirzi et al., 2013; Durrett and Klein, 2014), convolutional neural networks (Sun et al.; FrancisLandau et al., 2016), or more sophisticated neural architectures (Gupta et al., 2017; Sil et al., 2018). ∗ 1 Work done while at UT Austin. Code available at https://github.com/davidandym/wikilinks-ned These approaches typically focus on aggregating information from a mix of sources, including long-range information from the textual context or other linked entities. While this approach is suitable for entity linking settings such as newswire (Bentivogli, 2010) and Wikipedia (Ratinov et al., 2011), we cannot always rely on this information in other settings like Twitter (Guo et al., 2013; Fang and Chang, 2014; Huang et al., 2014; Dredze et al., 2016), Snapchat (Moon et al., 201"
D18-1126,D13-1029,0,0.0692993,"Missing"
D18-1126,P14-1036,0,0.0177367,"016), or more sophisticated neural architectures (Gupta et al., 2017; Sil et al., 2018). ∗ 1 Work done while at UT Austin. Code available at https://github.com/davidandym/wikilinks-ned These approaches typically focus on aggregating information from a mix of sources, including long-range information from the textual context or other linked entities. While this approach is suitable for entity linking settings such as newswire (Bentivogli, 2010) and Wikipedia (Ratinov et al., 2011), we cannot always rely on this information in other settings like Twitter (Guo et al., 2013; Fang and Chang, 2014; Huang et al., 2014; Dredze et al., 2016), Snapchat (Moon et al., 2018), other web platforms (Eshel et al., 2017), or dialogue systems (Bowden et al., 2018). We need models that can make effective use of limited context windows in noisy settings. In this work, we investigate this problem of effectively using context in the setting of the WikilinksNED dataset from Eshel et al. (2017). The examples in this dataset, which consists of 3.2 million entity disambiguation examples derived from Wikilinks (Singh et al., 2012), have at most 20 words of context on either side and usually no other mentions of the entity bein"
D18-1126,N16-1030,0,0.0629897,"Missing"
D18-1126,P14-2050,0,0.0190521,"ing on a separate candidate generation scheme. Our model places a distribution over titles P (t|m, cl , cr ), where t takes values in the set of candidate Wikipedia titles for that mention. This model, depicted in Figure 1, roughly follows that of Eshel et al. (2017), with some key differences, as we discuss in the rest of this section. Embedding contexts Given an example of the form (m, cl , cr ), our model first uses a GRU layer (Cho et al., 2014) over each context to convert cl Embedding entities We follow the method of Eshel et al. (2017) for generating entity embeddings, using word2vecf (Levy and Goldberg, 2014) to jointly train word and entity embeddings simultaneously using Wikipedia article text. Each title t is associated in turn with each content word w in the article, yielding a set of (w, t) pairs that are consumed by the training procedure. This yields a set of title embeddings et which we can treat as distributed representations of entities. Entity-context comparison We systematically compare the representations for l, r, and et as follows: [l · et , r · et , l − et , r − et , (l − et )2 , (r − et )2 ] where · denotes the conventional dot product and the other comparisons are elementwise. Th"
D18-1175,J08-1001,0,0.459268,"mble it into coherent narratives of what happened. This task is also the subject of an upcoming task at the Text Analysis Conference.1 Picking apart a story salad is a hard task that could in principle make use of arbitrary amounts of inference. But it is also a task in which coher1 https://tac.nist.gov/2018/SM-KBP/ index.html holgate@utexas.edu katrin.erk@mail.utexas.edu ence judgments could play a large role, the simplest being topical coherence, but also narrative coherence (Chambers and Jurafsky, 2008, 2009; Pichotta and Mooney, 2016; Mostafazadeh et al., 2017), overall textual coherence (Barzilay and Lapata, 2008; Logeswaran et al., 2018), and coherence in the description of entities. This makes it an attractive task for neural models. To make the task accessible to neural models, we propose a simple method for creating simulated story salad data at scale: we mix together sentences from different documents. Figure 1 shows an example mixture of two articles from Wikipedia, one on the Russia-Chechnya conflict and one on a conflict between the U.S. and Afghanistan. By controlling how similar the source documents are, we can flexibly adjust the difficulty of the task. In particular, as we show below, we c"
D18-1175,D08-1005,0,0.61264,"h in general. The task of picking apart story salads is related to the task of conversation disentanglement (Elsner and Charniak, 2008; Wang and Oard, 2009; Jiang et al., 2018), which is a clustering task of dividing a transcript into a set of distinct conversations. While superficially similar to our Story Salad task, conversation disentanglement focuses on dialogues and has many types of metadata available, such as time stamps, discourse information, and chat handles. Existing systems draw heavily on this metadata. Another related task is the distinction of on-topic and off-topic documents (Bekkerman and Crammer, 2008), which is defined in terms of topical relatedness. In comparison, the story salad task offers opportunities for more in-depth reasoning, as we show below. 3 Data Natural story salads arise when multiple messy narratives exist to describe the same event or outcome. Often this is because each contribution to the explanation only addresses a small aspect of the larger picture. We can directly simulate the confusion this kind of discourse creates by taking multiple narratives, cutting them into small pieces, and mixing them together. 4 The story salad task is more similar to multichoice narrative"
D18-1175,D15-1075,0,0.0434658,"ument mixture from which they are drawn. Second, we want to capture more in-depth interactions between sentences: our sentence embedding scheme for a sentence s1 should exploit its point 9 Experiments with convolutional encoders here yielded somewhat worse results. 10 Two stories may be on the same topic and still form clearly different narratives. For example, both narratives in Figure 1 are regarding military conflict. of comparison s2 and encode s1 with a view of similarities to and differences with s2 . This type of technique has been useful in tasks like natural language inference (NLI) (Bowman et al., 2015; Peters et al., 2018). To improve contextualization, we add a CNNbased context encoder to the BiLSTM classifier: the reader embeds the whole document salad at hand into a vector. Formally, we compute c = CNN(d), where in this case CNN denotes a single convolution layer with max pooling in the style of Kim (2014) and d is the concatenation of all sentences in the mixture. This component is shown in blue in Figure 3. The context vector c is then appended to z and fed into the bilinear layer. To capture the interaction between two sentences in a pair, we employ a mutual attention mechanism, whic"
D18-1175,P08-1090,0,0.390098,"le, messy narratives, a story salad. We would like to be able to automatically identify relevant information and assemble it into coherent narratives of what happened. This task is also the subject of an upcoming task at the Text Analysis Conference.1 Picking apart a story salad is a hard task that could in principle make use of arbitrary amounts of inference. But it is also a task in which coher1 https://tac.nist.gov/2018/SM-KBP/ index.html holgate@utexas.edu katrin.erk@mail.utexas.edu ence judgments could play a large role, the simplest being topical coherence, but also narrative coherence (Chambers and Jurafsky, 2008, 2009; Pichotta and Mooney, 2016; Mostafazadeh et al., 2017), overall textual coherence (Barzilay and Lapata, 2008; Logeswaran et al., 2018), and coherence in the description of entities. This makes it an attractive task for neural models. To make the task accessible to neural models, we propose a simple method for creating simulated story salad data at scale: we mix together sentences from different documents. Figure 1 shows an example mixture of two articles from Wikipedia, one on the Russia-Chechnya conflict and one on a conflict between the U.S. and Afghanistan. By controlling how similar"
D18-1175,P09-1068,0,0.179449,"h), confirming that the task requires more than just general topical similarity. But there is much room for improvement, in particular on salads generated to be more difficult, where performance is around 15 points lower than on arbitrary mixtures. 2 Related Work Building on early work in script learning (Schank and Abelson, 1977), Chambers and Jurafsky (2008) introduce narrative schema and propose the “narrative cloze” task where the modeling objective is to predict the event happening next. The topic has since seen many extensions and variants coupled with increasingly sophisticated models (Chambers and Jurafsky, 2009) including neural networks (Granroth-Wilding and Clark, 2016; Pichotta and Mooney, 2016; Mostafazadeh et al., 2017). This line of work is related to story salads in that our aim of separating entangled narratives in a document mixture also leverages withinnarrative coherence. Our work, however, is very different from narrative cloze: (i) we group sentences/events rather than predicting what happens next; (ii) crucially, the narrative coherence in story salads is in context, in that a narrative clustering is only meaningful with respect to a particular document mixture (see Section 5, 6), while"
D18-1175,N18-1076,1,0.839294,"rd and *- HARD mixtures are challenging for current models. Furthermore, our WIKI - HARD dataset contains salads featuring conflicting information and is an attractive setting for building models with deeper reasoning capabilities. 4 Models We treat the story salad task as a narrative clustering task where, in our dataset, each salad is comprised of two clusters. Accordingly, the first baselines we consider are standard clustering approaches. Baselines. Our first baseline is a simple uniform baseline (hereafter UNIF), where we assign 6 Event tuples are extracted via the extractor presented in Cheng and Erk (2018), and copular verbs are not treated as events, meaning that some sentences translate to null events. all sentences in a document mixture to a single cluster. Under UNIF the clustering accuracy is the percentage of the majority-cluster sentences, e.g. if a mixture has 7 sentences from one narrative and 3 from the other, then the accuracy is 0.7. Additionally, we explore a family of baselines that consist of clustering off-the-shelf sentence embeddings. We choose k-medoids7 (hereafter KM) as our clustering algorithm. For sentence embeddings, we experimented with (i) averaged 300D GloVe embedding"
D18-1175,N18-2031,0,0.0168924,"Under UNIF the clustering accuracy is the percentage of the majority-cluster sentences, e.g. if a mixture has 7 sentences from one narrative and 3 from the other, then the accuracy is 0.7. Additionally, we explore a family of baselines that consist of clustering off-the-shelf sentence embeddings. We choose k-medoids7 (hereafter KM) as our clustering algorithm. For sentence embeddings, we experimented with (i) averaged 300D GloVe embeddings (Pennington et al., 2014), which have been shown to produce surprisingly strong performance in a variety of text classification tasks (Iyyer et al., 2015; Coates and Bollegala, 2018); (ii) skip-thought embeddings (Kiros et al., 2015); and (iii) SCDV (Mekala et al., 2017), a multisense-aware sentence embedding algorithm which builds upon pretrained GloVe embeddings using a Gaussian mixture model. Averaged GloVe embeddings gave the best performance in our experiments; to avoid clutter, we only report those results henceforth. Neural supervised clustering. Our baselines work directly on sentence embeddings and therefore ignore the task-specific supervision available in our labeled training data. Inspired by the work in Bilenko et al. (2004) and Finley and Joachim (2005) on s"
D18-1175,P18-1198,0,0.025666,"ed model (i.e. BILSTM - MT- CTX ) than the models with a single component (i.e. BILSTM - MT and BILSTM - CTX). Overall, the large margin of KM and classifieraided models above the UNIF baseline indicates that separating story salads is a valid task where generalizable patterns can be exploited by machine learning techniques. Why would the mutual attention mechanism help? Plotting the attention weights of randomly selected samples, we see distributionally similar words being attended to in Figure 4a. Intuitively, a BiLSTM compresses a sentence into a single vector, leading to information loss (Conneau et al., 2018). Mutual attention enriches this representation by allowing us access to detailed information in sentences at word-level resolution by capturing lexical similarity. Even more interestingly, we observe a synergistic effect between mutual attention and contextualization: with the context reader added, we see high attention weights on words/phrases which bear little distributional similarity but are important for connecting/contrasting different narratives. For example, in Figure 4b, sega group and family station wagon are selected by the attention, despite not having similar words in the other s"
D18-1175,E17-1104,0,0.0158672,"LDC2003T05). level.4 Working with labeled story salad examples, we draw inspiration from previous work on supervised clustering (Bilenko et al., 2004; Finley and Joachim, 2005). We also take advantage of the recent success of deep learning in leveraging a continuous semantic space (Pennington et al., 2014; Kiros et al., 2015; Mekala et al., 2017; Wieting and Gimpel, 2017; Wieting et al., 2017) for word/sentence/event encoding; neural components for enhanced supervised clustering (Bilenko et al., 2004), in particular LSTMs (Hochreiter and Schmidhuber, 1997; Dai and Le, 2015), CNNs (Kim, 2014; Conneau et al., 2017), and attention mechanisms (Bahdanau et al., 2015; Hermann et al., 2015; Lin et al., 2017). By exploring our ability to pick apart story salads with these stateof-the-art NLP modeling tools, we attempt to (i) show the value of the story salad task as a new NLP task that warrants extensive research; (ii) understand the nature of the task and the challenges it sets forth for NLP research in general. The task of picking apart story salads is related to the task of conversation disentanglement (Elsner and Charniak, 2008; Wang and Oard, 2009; Jiang et al., 2018), which is a clustering task of divid"
D18-1175,P08-1095,0,0.39154,"cular LSTMs (Hochreiter and Schmidhuber, 1997; Dai and Le, 2015), CNNs (Kim, 2014; Conneau et al., 2017), and attention mechanisms (Bahdanau et al., 2015; Hermann et al., 2015; Lin et al., 2017). By exploring our ability to pick apart story salads with these stateof-the-art NLP modeling tools, we attempt to (i) show the value of the story salad task as a new NLP task that warrants extensive research; (ii) understand the nature of the task and the challenges it sets forth for NLP research in general. The task of picking apart story salads is related to the task of conversation disentanglement (Elsner and Charniak, 2008; Wang and Oard, 2009; Jiang et al., 2018), which is a clustering task of dividing a transcript into a set of distinct conversations. While superficially similar to our Story Salad task, conversation disentanglement focuses on dialogues and has many types of metadata available, such as time stamps, discourse information, and chat handles. Existing systems draw heavily on this metadata. Another related task is the distinction of on-topic and off-topic documents (Bekkerman and Crammer, 2008), which is defined in terms of topical relatedness. In comparison, the story salad task offers opportuniti"
D18-1175,P15-1162,0,0.0536316,"Missing"
D18-1175,N18-1164,0,0.214775,"ai and Le, 2015), CNNs (Kim, 2014; Conneau et al., 2017), and attention mechanisms (Bahdanau et al., 2015; Hermann et al., 2015; Lin et al., 2017). By exploring our ability to pick apart story salads with these stateof-the-art NLP modeling tools, we attempt to (i) show the value of the story salad task as a new NLP task that warrants extensive research; (ii) understand the nature of the task and the challenges it sets forth for NLP research in general. The task of picking apart story salads is related to the task of conversation disentanglement (Elsner and Charniak, 2008; Wang and Oard, 2009; Jiang et al., 2018), which is a clustering task of dividing a transcript into a set of distinct conversations. While superficially similar to our Story Salad task, conversation disentanglement focuses on dialogues and has many types of metadata available, such as time stamps, discourse information, and chat handles. Existing systems draw heavily on this metadata. Another related task is the distinction of on-topic and off-topic documents (Bekkerman and Crammer, 2008), which is defined in terms of topical relatedness. In comparison, the story salad task offers opportunities for more in-depth reasoning, as we show"
D18-1175,D14-1181,0,0.0607888,"vailable as LDC2003T05). level.4 Working with labeled story salad examples, we draw inspiration from previous work on supervised clustering (Bilenko et al., 2004; Finley and Joachim, 2005). We also take advantage of the recent success of deep learning in leveraging a continuous semantic space (Pennington et al., 2014; Kiros et al., 2015; Mekala et al., 2017; Wieting and Gimpel, 2017; Wieting et al., 2017) for word/sentence/event encoding; neural components for enhanced supervised clustering (Bilenko et al., 2004), in particular LSTMs (Hochreiter and Schmidhuber, 1997; Dai and Le, 2015), CNNs (Kim, 2014; Conneau et al., 2017), and attention mechanisms (Bahdanau et al., 2015; Hermann et al., 2015; Lin et al., 2017). By exploring our ability to pick apart story salads with these stateof-the-art NLP modeling tools, we attempt to (i) show the value of the story salad task as a new NLP task that warrants extensive research; (ii) understand the nature of the task and the challenges it sets forth for NLP research in general. The task of picking apart story salads is related to the task of conversation disentanglement (Elsner and Charniak, 2008; Wang and Oard, 2009; Jiang et al., 2018), which is a c"
D18-1175,D17-1069,0,0.158148,"l with respect to a particular document mixture (see Section 5, 6), while in narrative cloze the next event is predicted on a “global” salads are available for download directly and we have provided code to reconstruct the NYT salads from English Gigaword 5 (available as LDC2003T05). level.4 Working with labeled story salad examples, we draw inspiration from previous work on supervised clustering (Bilenko et al., 2004; Finley and Joachim, 2005). We also take advantage of the recent success of deep learning in leveraging a continuous semantic space (Pennington et al., 2014; Kiros et al., 2015; Mekala et al., 2017; Wieting and Gimpel, 2017; Wieting et al., 2017) for word/sentence/event encoding; neural components for enhanced supervised clustering (Bilenko et al., 2004), in particular LSTMs (Hochreiter and Schmidhuber, 1997; Dai and Le, 2015), CNNs (Kim, 2014; Conneau et al., 2017), and attention mechanisms (Bahdanau et al., 2015; Hermann et al., 2015; Lin et al., 2017). By exploring our ability to pick apart story salads with these stateof-the-art NLP modeling tools, we attempt to (i) show the value of the story salad task as a new NLP task that warrants extensive research; (ii) understand the nature"
D18-1175,W17-0906,0,0.152888,"to automatically identify relevant information and assemble it into coherent narratives of what happened. This task is also the subject of an upcoming task at the Text Analysis Conference.1 Picking apart a story salad is a hard task that could in principle make use of arbitrary amounts of inference. But it is also a task in which coher1 https://tac.nist.gov/2018/SM-KBP/ index.html holgate@utexas.edu katrin.erk@mail.utexas.edu ence judgments could play a large role, the simplest being topical coherence, but also narrative coherence (Chambers and Jurafsky, 2008, 2009; Pichotta and Mooney, 2016; Mostafazadeh et al., 2017), overall textual coherence (Barzilay and Lapata, 2008; Logeswaran et al., 2018), and coherence in the description of entities. This makes it an attractive task for neural models. To make the task accessible to neural models, we propose a simple method for creating simulated story salad data at scale: we mix together sentences from different documents. Figure 1 shows an example mixture of two articles from Wikipedia, one on the Russia-Chechnya conflict and one on a conflict between the U.S. and Afghanistan. By controlling how similar the source documents are, we can flexibly adjust the difficu"
D18-1175,N18-1202,0,0.0218138,"ich they are drawn. Second, we want to capture more in-depth interactions between sentences: our sentence embedding scheme for a sentence s1 should exploit its point 9 Experiments with convolutional encoders here yielded somewhat worse results. 10 Two stories may be on the same topic and still form clearly different narratives. For example, both narratives in Figure 1 are regarding military conflict. of comparison s2 and encode s1 with a view of similarities to and differences with s2 . This type of technique has been useful in tasks like natural language inference (NLI) (Bowman et al., 2015; Peters et al., 2018). To improve contextualization, we add a CNNbased context encoder to the BiLSTM classifier: the reader embeds the whole document salad at hand into a vector. Formally, we compute c = CNN(d), where in this case CNN denotes a single convolution layer with max pooling in the style of Kim (2014) and d is the concatenation of all sentences in the mixture. This component is shown in blue in Figure 3. The context vector c is then appended to z and fed into the bilinear layer. To capture the interaction between two sentences in a pair, we employ a mutual attention mechanism, which is similar to the at"
D18-1175,P16-1027,0,0.393947,". We would like to be able to automatically identify relevant information and assemble it into coherent narratives of what happened. This task is also the subject of an upcoming task at the Text Analysis Conference.1 Picking apart a story salad is a hard task that could in principle make use of arbitrary amounts of inference. But it is also a task in which coher1 https://tac.nist.gov/2018/SM-KBP/ index.html holgate@utexas.edu katrin.erk@mail.utexas.edu ence judgments could play a large role, the simplest being topical coherence, but also narrative coherence (Chambers and Jurafsky, 2008, 2009; Pichotta and Mooney, 2016; Mostafazadeh et al., 2017), overall textual coherence (Barzilay and Lapata, 2008; Logeswaran et al., 2018), and coherence in the description of entities. This makes it an attractive task for neural models. To make the task accessible to neural models, we propose a simple method for creating simulated story salad data at scale: we mix together sentences from different documents. Figure 1 shows an example mixture of two articles from Wikipedia, one on the Russia-Chechnya conflict and one on a conflict between the U.S. and Afghanistan. By controlling how similar the source documents are, we can"
D18-1175,D16-1264,0,0.033043,"scuss energy policy in a speech on Friday. (B) On Friday, Bush call Gore a “flip-flopper”, say UNK proposal to tap into the reserve be a political ploy. Figure 6: Part of a story salad that is impossible for a human to pick apart (source: NYT- HARD). “UNK” represents out-of-vocabulary tokens, and all the words are lemmatized. Both narratives, i.e. (A) and (B) involve the characters Al Gore and George Bush, and both are on the topic of energy, with strongly overlapping vocabulary. human performance sets the ceiling for the best achievable results (e.g. span-prediction based question answering (Rajpurkar et al., 2016), where all the information needed for the correct answer is available in the input), successfully picking apart narratives in a story salad may require consulting an external knowledge base, which affords machine learning models a clear advantage over humans. For example, recognizing that Commander Kamal is likely to be Afghani based on his name, which is not knowledge every reader possesses, would allow us to successfully cluster the sentence with the U.S.-Afghanistan narrative rather than the Russian-Chechnya narrative. 6 Conclusion We have presented a technique to generate Story Salads, mi"
D18-1175,N09-1023,0,0.0138351,"Schmidhuber, 1997; Dai and Le, 2015), CNNs (Kim, 2014; Conneau et al., 2017), and attention mechanisms (Bahdanau et al., 2015; Hermann et al., 2015; Lin et al., 2017). By exploring our ability to pick apart story salads with these stateof-the-art NLP modeling tools, we attempt to (i) show the value of the story salad task as a new NLP task that warrants extensive research; (ii) understand the nature of the task and the challenges it sets forth for NLP research in general. The task of picking apart story salads is related to the task of conversation disentanglement (Elsner and Charniak, 2008; Wang and Oard, 2009; Jiang et al., 2018), which is a clustering task of dividing a transcript into a set of distinct conversations. While superficially similar to our Story Salad task, conversation disentanglement focuses on dialogues and has many types of metadata available, such as time stamps, discourse information, and chat handles. Existing systems draw heavily on this metadata. Another related task is the distinction of on-topic and off-topic documents (Bekkerman and Crammer, 2008), which is defined in terms of topical relatedness. In comparison, the story salad task offers opportunities for more in-depth"
D18-1175,N18-2049,1,0.810213,"humans, looking at 20 mixtures each. Here, very interestingly, we find more mixtures that are impossible for humans in NYT- HARD (10 cases, example in Figure 6) than WIKI - HARD (3 cases). This presents a clear discrepancy between difficulty for humans and difficulty for models: the models do better on NYT- HARD which is harder for us. While we would not want to draw strong conclusions from a small sample, this hints at possibilities of future work where world knowledge, which is likely to be orthogonal to the information picked up by the models, can be introduced to improve performance (e.g. Wang et al. (2018)). Note that unlike many other NLP tasks where 1462 (A) The most basic question face the country on energy be how to keep supply and demand in line. The Democrats would say : “what can UNK do to make good use of what UNK have get?” (B) Oil price dominate the 31-minute news conference, hold here near pittsburgh. (B) Vice President Al Gore hold UNK first news conference in 67 day on Friday, defend UNK call for the release of oil from the government’s stockpile and and vow that UNK would “confront friend and foe alike” over the marketing of violent entertainment to child, despite the million in d"
D18-1175,P17-1190,0,0.0179378,"articular document mixture (see Section 5, 6), while in narrative cloze the next event is predicted on a “global” salads are available for download directly and we have provided code to reconstruct the NYT salads from English Gigaword 5 (available as LDC2003T05). level.4 Working with labeled story salad examples, we draw inspiration from previous work on supervised clustering (Bilenko et al., 2004; Finley and Joachim, 2005). We also take advantage of the recent success of deep learning in leveraging a continuous semantic space (Pennington et al., 2014; Kiros et al., 2015; Mekala et al., 2017; Wieting and Gimpel, 2017; Wieting et al., 2017) for word/sentence/event encoding; neural components for enhanced supervised clustering (Bilenko et al., 2004), in particular LSTMs (Hochreiter and Schmidhuber, 1997; Dai and Le, 2015), CNNs (Kim, 2014; Conneau et al., 2017), and attention mechanisms (Bahdanau et al., 2015; Hermann et al., 2015; Lin et al., 2017). By exploring our ability to pick apart story salads with these stateof-the-art NLP modeling tools, we attempt to (i) show the value of the story salad task as a new NLP task that warrants extensive research; (ii) understand the nature of the task and the challe"
D18-1175,D17-1026,0,0.0267835,"(see Section 5, 6), while in narrative cloze the next event is predicted on a “global” salads are available for download directly and we have provided code to reconstruct the NYT salads from English Gigaword 5 (available as LDC2003T05). level.4 Working with labeled story salad examples, we draw inspiration from previous work on supervised clustering (Bilenko et al., 2004; Finley and Joachim, 2005). We also take advantage of the recent success of deep learning in leveraging a continuous semantic space (Pennington et al., 2014; Kiros et al., 2015; Mekala et al., 2017; Wieting and Gimpel, 2017; Wieting et al., 2017) for word/sentence/event encoding; neural components for enhanced supervised clustering (Bilenko et al., 2004), in particular LSTMs (Hochreiter and Schmidhuber, 1997; Dai and Le, 2015), CNNs (Kim, 2014; Conneau et al., 2017), and attention mechanisms (Bahdanau et al., 2015; Hermann et al., 2015; Lin et al., 2017). By exploring our ability to pick apart story salads with these stateof-the-art NLP modeling tools, we attempt to (i) show the value of the story salad task as a new NLP task that warrants extensive research; (ii) understand the nature of the task and the challenges it sets forth for"
D18-1480,K16-1002,0,0.0667191,"g and bag-ofwords document modeling. An analysis of the properties of our vMF representations shows that they learn richer and more nuanced structures in their latent representations than their Gaussian counterparts.1 1 Introduction Recent work has established the effectiveness of deep generative models for a range of tasks in NLP, including text generation (Hu et al., 2017; Yu et al., 2017), machine translation (Zhang et al., 2016), and style transfer (Shen et al., 2017; Zhao et al., 2017a). Variational autoencoders, which have been explored in past work for text modeling (Miao et al., 2016; Bowman et al., 2016), 1 The code and dataset are available at: https:// github.com/jiacheng-xu/vmf_vae_nlp posit a continuous latent variable which is used to capture latent structure in the data. Typical VAE implementations assume the prior of this latent space is a multivariate Gaussian; during training, a Kullback-Leibler (KL) divergence term in loss function encourages the variational posterior to approximate the prior. One major limitation of this approach observed by past work is that the KL term may encourage the posterior distribution of the latent variable to “collapse” to the prior, effectively renderin"
D18-1480,Q18-1031,0,0.304372,"surface of the unit sphere. While κ can be predicted from the encoder network, we find experimentally that fixing it leads to more stable optimization and better performance. latent code, but the gains are limited and changing the decoder in this way requires ad hoc model engineering and careful tuning of various decoder capacity parameters. Our method is orthogonal to the choice of the decoder and can be combined with any of these approaches. Using vMF distributions in VAEs also leaves us the flexibility to modify the prior in other ways, such as using a product distribution with a uniform (Guu et al., 2018) or piecewise constant term (Serban et al., 2017a). We evaluate our approach in two generative modeling paradigms. For both RNN language modeling and bag-of-words document modeling, we find that vMF is more robust than a Gaussian prior, and our model learns to rely more on the latent variable while achieving better held-out data likelihoods. To better understand the contrast between these models, we design and conduct a series of experiments to understand the properties of the Gaussian and vMF latent code spaces, which make different structural assumptions. Unsurprisingly, these latent code di"
D18-1480,J93-2004,0,0.0611725,"on of Guu et al. (2018), we use the rejection sampling scheme of Wood (1994) to sample a “change magnitude” √ w. Our sample is then given by z = wµ + v 1 − w2 , where v is a randomly sampled unit vector tangent to the hypersphere at µ. Neither v nor w depends on µ, so we can now take gradients of z with respect to µ as required. 4 Experiments on Language Modeling We first evaluate our vMF approach in the NVRNN setting. We will return to this model and analyze its properties further in Sections 6 and 7 after showing experiments on document modeling. Dataset For NVRNN, we use the Penn Treebank (Marcus et al., 1993), also used in Bowman et al. (2016), and Yelp 2013 (Xu et al., 2016). Examples in the Yelp dataset are much longer and more diverse than those from PTB, requiring more understanding of high-level semantics to generate a coherent sequence. Yelp has a long tail of very long reviews, so we truncate the examples to a maximum length of 50 words; this still gives an average length over twice as long as in the PTB setting. Statistics about all datasets used in this paper are shown in Table 2. Settings We evaluate our NVRNN as in Bowman et al. (2016) and explore two different settings. In the Standard"
D18-1480,D17-1066,0,0.810852,"trying to flexibly learn κ, and a wide range of settings for fixed κ lead to good performance. Our model systematically achieves better log likelihoods than analogous Gaussian models while having higher KL divergence values, showing that it more successfully makes use of the latent variables at the end of training. Past work has suggested several other techniques for dealing with the KL collapse in the Gaussian case. Annealing the weight of KL term (Bowman et al., 2016) still leaves us with brittleness in the optimization process, as we show in Section 2. Other prior work (Yang et al., 2017; Semeniuta et al., 2017) focuses on using CNNs rather than RNNs as the decoder in order to weaken the model and encourage the use of the 4503 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4503–4513 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Linear Linear µ log( ) N (µ, ) Gaussian Enc Enc Enc Enc Linear Linear µ  &lt;latexit sha1_base64=&quot;gfs6aeX3qDUUDl4RruVxsg1XFMA=&quot;>AAAB6nicbVA9TwJBEJ3DL8Qv1NJmI9FYkTsbLUlsLDEKksCF7C17sGF377I7Z0Iu/AQbC42x9RfZ+W9c4AoFXzLJy3szmZkXpVJY9P1vr7S2vrG5Vd6u7Ozu7R9UD4/aNskM4y2WyMR0Imq5FJq3UKDkn"
D18-1480,D16-1172,1,0.839152,"994) to sample a “change magnitude” √ w. Our sample is then given by z = wµ + v 1 − w2 , where v is a randomly sampled unit vector tangent to the hypersphere at µ. Neither v nor w depends on µ, so we can now take gradients of z with respect to µ as required. 4 Experiments on Language Modeling We first evaluate our vMF approach in the NVRNN setting. We will return to this model and analyze its properties further in Sections 6 and 7 after showing experiments on document modeling. Dataset For NVRNN, we use the Penn Treebank (Marcus et al., 1993), also used in Bowman et al. (2016), and Yelp 2013 (Xu et al., 2016). Examples in the Yelp dataset are much longer and more diverse than those from PTB, requiring more understanding of high-level semantics to generate a coherent sequence. Yelp has a long tail of very long reviews, so we truncate the examples to a maximum length of 50 words; this still gives an average length over twice as long as in the PTB setting. Statistics about all datasets used in this paper are shown in Table 2. Settings We evaluate our NVRNN as in Bowman et al. (2016) and explore two different settings. In the Standard setting, the input to the RNN at each time step is the concatenatio"
D18-1480,D16-1050,0,0.0446574,"how that doing so not only averts the KL collapse, but consistently gives better likelihoods than Gaussians across a range of modeling conditions, including recurrent language modeling and bag-ofwords document modeling. An analysis of the properties of our vMF representations shows that they learn richer and more nuanced structures in their latent representations than their Gaussian counterparts.1 1 Introduction Recent work has established the effectiveness of deep generative models for a range of tasks in NLP, including text generation (Hu et al., 2017; Yu et al., 2017), machine translation (Zhang et al., 2016), and style transfer (Shen et al., 2017; Zhao et al., 2017a). Variational autoencoders, which have been explored in past work for text modeling (Miao et al., 2016; Bowman et al., 2016), 1 The code and dataset are available at: https:// github.com/jiacheng-xu/vmf_vae_nlp posit a continuous latent variable which is used to capture latent structure in the data. Typical VAE implementations assume the prior of this latent space is a multivariate Gaussian; during training, a Kullback-Leibler (KL) divergence term in loss function encourages the variational posterior to approximate the prior. One majo"
D18-1480,P16-5005,0,\N,Missing
D19-1070,D16-1032,0,0.0532428,". ing tasks we study, we perform additional analysis and find that these tasks still do not encourage transformers to form truly deep entity representations. Our performance gain is largely from better understanding of verb semantics in terms of associating process actions with entity the paragraph is conditioned on. The model also does not specialize in “tracking” composed entities per se, again using surface clues like verbs to identify the components involved in a new composition. We evaluate our models on two datasets specifically designed to invoke procedural understanding: (i) R ECIPES (Kiddon et al., 2016), and (ii) P RO PARA (Dalvi et al., 2018). For the R ECIPES dataset, we classify whether an ingredient was affected in a certain step, which requires understanding when ingredients are combined or the focus of the recipe shifts away from them. The P RO PARA dataset involves answering a more complex set of questions about physical state changes of components in scientific processes. To handle this more structured setting, our transformer produces potentials consumed by a conditional random field which predicts entity states over time. Using a unidirectional GPT-based architecture, we achieve st"
D19-1070,N18-1204,0,0.0223816,"ent of understanding such texts. We pose both of these procedural understanding tasks as classification problems, predicting the state of the entity at each timestep from a set of pre-defined classes. In Figure 1, these classes cor760 respond to either the presence (1) or absence (0) or the sequence of state changes create (C), move (M), destroy (D), exists (E), and none (O). State-of-the-art approaches on these tasks are inherently entity-centric. Separately, it has been shown that entity-centric language modeling in a continuous framework can lead to better performance for LM related tasks (Clark et al., 2018; Ji et al., 2017). Moreover, external data has shown to be useful for modeling process understanding tasks in prior work (Tandon et al., 2018; Bosselut et al., 2018), suggesting that pre-trained models may be effective. With such tasks in place, a strong model will ideally learn to form robust entity-centric representation at each time step instead of solely relying on extracting information from the local entity mentions. This expectation is primarily due to the evolving nature of the process domain where entities undergo complex interactions, form intermediate compositions, and are often ac"
D19-1070,D17-1018,0,0.0573032,"Missing"
D19-1070,N18-1144,0,0.308997,"ly local entity mentions (Wiseman et al., 2016; Lee et al., 2017; Peters et al., 2017) rather than forming truly global entity representations (Rahman and Ng, 2009; Lee et al., 2018). Thus, performance on these tasks does not form sufficient evidence that these representations strongly capture entity semantics. Better understanding the models’ capabilities requires testing them in domains involving complex entity interactions over longer texts. One such domain is that of procedural language, which is strongly focused on tracking the entities involved and their interactions (Mori et al., 2014; Dalvi et al., 2018; Bosselut et al., 2018). This paper investigates the question of how transformer-based models form entity representations and what these representations capture. We expect that after fine-tuning on a target task, a transformer’s output representations should somehow capture relevant entity properties, in the sense that these properties can be extracted by shallow classification either from entity tokens or from marker tokens. However, we observe that such “post-conditioning” approaches don’t perform significantly better than rule-based baselines on the tasks we study. We address this by propo"
D19-1070,N18-2108,0,0.0125616,"Gupta and Greg Durrett Department of Computer Science The University of Texas at Austin {agupta,gdurrett}@cs.utexas.edu Abstract the robust transfer of lexical semantics to downstream tasks. However, these models are still better at capturing syntax than they are at more entityfocused aspects like coreference (Tenney et al., 2019a,b); moreover, existing state-of-the-art architectures for such tasks often perform well looking at only local entity mentions (Wiseman et al., 2016; Lee et al., 2017; Peters et al., 2017) rather than forming truly global entity representations (Rahman and Ng, 2009; Lee et al., 2018). Thus, performance on these tasks does not form sufficient evidence that these representations strongly capture entity semantics. Better understanding the models’ capabilities requires testing them in domains involving complex entity interactions over longer texts. One such domain is that of procedural language, which is strongly focused on tracking the entities involved and their interactions (Mori et al., 2014; Dalvi et al., 2018; Bosselut et al., 2018). This paper investigates the question of how transformer-based models form entity representations and what these representations capture. W"
D19-1070,mori-etal-2014-flow,0,0.0262662,"well looking at only local entity mentions (Wiseman et al., 2016; Lee et al., 2017; Peters et al., 2017) rather than forming truly global entity representations (Rahman and Ng, 2009; Lee et al., 2018). Thus, performance on these tasks does not form sufficient evidence that these representations strongly capture entity semantics. Better understanding the models’ capabilities requires testing them in domains involving complex entity interactions over longer texts. One such domain is that of procedural language, which is strongly focused on tracking the entities involved and their interactions (Mori et al., 2014; Dalvi et al., 2018; Bosselut et al., 2018). This paper investigates the question of how transformer-based models form entity representations and what these representations capture. We expect that after fine-tuning on a target task, a transformer’s output representations should somehow capture relevant entity properties, in the sense that these properties can be extracted by shallow classification either from entity tokens or from marker tokens. However, we observe that such “post-conditioning” approaches don’t perform significantly better than rule-based baselines on the tasks we study. We a"
D19-1070,P17-1161,0,0.0625527,"Missing"
D19-1070,N19-1423,0,0.0525301,"be attained by restructuring the input to guide the transformer model to focus on a particular entity. Second, we assess the degree to which transformer networks capture the process dynamics, investigating such factors as merged entities and oblique entity references. On two different tasks, ingredient detection in recipes and QA over scientific processes, we achieve state-ofthe-art results, but our models still largely attend to shallow context clues and do not form complex representations of intermediate entity or process state.1 1 Introduction Transformer based pre-trained language models (Devlin et al., 2019; Radford et al., 2018, 2019; Joshi et al., 2019; Yang et al., 2019) have been shown to perform remarkably well on a range of tasks, including entity-related tasks like coreference resolution (Kantor and Globerson, 2019) and named entity recognition (Devlin et al., 2019). This performance has been generally attributed to 1 Code to reproduce experiments in this paper is available at https://github.com/aditya2211/ transformer-entity-tracking 759 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pr"
D19-1070,N18-1202,0,0.0215526,"(i) Majority Class, (ii) Exact Match of an ingredient e in recipe step st , and (iii) First Occurrence, where we predict the ingredient to be present in all steps following the first exact match. These latter two baselines capture natural modes of reasoning about the dataset: an ingredient is used when it is directly mentioned, or it is used in every step after it is mentioned, reflecting the assumption that a recipe is about incrementally adding ingredients to an ever-growing mixture. We also construct a LSTM baseline to evaluate the performance of ELMo embeddings (ELMotoken and ELMosent ) (Peters et al., 2018) compared to GPT. Table 2 compares the performance of the discussed models against the baselines, evaluating per-step entity prediction performance. Using the ground truth about ingredient’s state, we also report the uncombined (UR) and combined (CR) recalls, which are per-timestep ingredient recall distinguished by whether the ingredient is explicitly mentioned (uncombined) or part of a mixture (combined). Note that Exact Match and First Occ baselines represent high-precision and high-recall regimes for this task, respectively. As observed from the results, the postconditioning frameworks und"
D19-1070,W19-1502,1,0.917155,"rediction task of identifying state change sequences. Both require cross-sentence reasoning, such as knowing what components are in a mixture and understanding verb semantics like combine. Dalvi et al. (2018) introduced the P RO PARA dataset to probe understanding of scientific processes. The goal is to track the sequence of physical state changes (creation, destruction, and movement) entites undergo over long sequences of process steps. Past work involves both modeling entities across time (Das et al., 2019) and capturing structural constraints inherent in the processes (Tandon et al., 2018; Gupta and Durrett, 2019) Figure 1b shows an example of the dataset posed as a structured prediction task, as in (Gupta and Durrett, 2019). For such a domain, it is crucial to capture implicit event occurrences beyond explicit entity mentions. For example, in fuel goes into the generator. The generator converts mechanical energy into electrical energy”, the fuel is implicitly destroyed in the process. ing tasks we study, we perform additional analysis and find that these tasks still do not encourage transformers to form truly deep entity representations. Our performance gain is largely from better understanding of ver"
D19-1070,N19-1357,0,0.0232377,"gain is largely due to the increase in combined recall. One possible reason could be that external data leads to betCat-1 Model C ETBERT ETGP T M Cat-2 D C M D 78.51 61.60 71.50 76.68 54.12 58.62 79.82 56.27 73.83 77.24 50.82 56.27 Table 8: Results for each state change type. Performance on predicting creation and destruction are highest, partially due to the model’s ability to use verb semantics for these tasks. 766 Figure 4: Gradient of the classification loss of the gold class with respect to inputs when predicting the status of butter in the last sentence. We follow a similar approach as Jain and Wallace (2019) to compute associations. Exact matches of the entity receive high weight, as does a seemingly unrelated verb dredge, which often indicates that the butter has already been used and is therefore present. 6 Analysis Input The model’s performance on these challenging task cases suggests that even though it outperforms baselines, it may not be capturing deep reasoning about entities. To understand what the model actually does, we perform analysis of the model’s behavior with respect to the input to understand what cues it is picking up on. Accuracy Complete Process 84.59 w/o Other Ingredients w/o"
D19-1070,D17-1195,0,0.020907,"such texts. We pose both of these procedural understanding tasks as classification problems, predicting the state of the entity at each timestep from a set of pre-defined classes. In Figure 1, these classes cor760 respond to either the presence (1) or absence (0) or the sequence of state changes create (C), move (M), destroy (D), exists (E), and none (O). State-of-the-art approaches on these tasks are inherently entity-centric. Separately, it has been shown that entity-centric language modeling in a continuous framework can lead to better performance for LM related tasks (Clark et al., 2018; Ji et al., 2017). Moreover, external data has shown to be useful for modeling process understanding tasks in prior work (Tandon et al., 2018; Bosselut et al., 2018), suggesting that pre-trained models may be effective. With such tasks in place, a strong model will ideally learn to form robust entity-centric representation at each time step instead of solely relying on extracting information from the local entity mentions. This expectation is primarily due to the evolving nature of the process domain where entities undergo complex interactions, form intermediate compositions, and are often accompanied by impli"
D19-1070,D09-1101,0,0.0232202,"ntity Tracking Aditya Gupta and Greg Durrett Department of Computer Science The University of Texas at Austin {agupta,gdurrett}@cs.utexas.edu Abstract the robust transfer of lexical semantics to downstream tasks. However, these models are still better at capturing syntax than they are at more entityfocused aspects like coreference (Tenney et al., 2019a,b); moreover, existing state-of-the-art architectures for such tasks often perform well looking at only local entity mentions (Wiseman et al., 2016; Lee et al., 2017; Peters et al., 2017) rather than forming truly global entity representations (Rahman and Ng, 2009; Lee et al., 2018). Thus, performance on these tasks does not form sufficient evidence that these representations strongly capture entity semantics. Better understanding the models’ capabilities requires testing them in domains involving complex entity interactions over longer texts. One such domain is that of procedural language, which is strongly focused on tracking the entities involved and their interactions (Mori et al., 2014; Dalvi et al., 2018; Bosselut et al., 2018). This paper investigates the question of how transformer-based models form entity representations and what these represe"
D19-1070,P19-1066,0,0.012411,"actors as merged entities and oblique entity references. On two different tasks, ingredient detection in recipes and QA over scientific processes, we achieve state-ofthe-art results, but our models still largely attend to shallow context clues and do not form complex representations of intermediate entity or process state.1 1 Introduction Transformer based pre-trained language models (Devlin et al., 2019; Radford et al., 2018, 2019; Joshi et al., 2019; Yang et al., 2019) have been shown to perform remarkably well on a range of tasks, including entity-related tasks like coreference resolution (Kantor and Globerson, 2019) and named entity recognition (Devlin et al., 2019). This performance has been generally attributed to 1 Code to reproduce experiments in this paper is available at https://github.com/aditya2211/ transformer-entity-tracking 759 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 759–769, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics a) Binary Classification Task for Ingredient Detection (Recipes Dataset) sugar eggs flour Combine sugar, oil"
D19-1070,D18-1006,0,0.0816051,"ARA as a structured prediction task of identifying state change sequences. Both require cross-sentence reasoning, such as knowing what components are in a mixture and understanding verb semantics like combine. Dalvi et al. (2018) introduced the P RO PARA dataset to probe understanding of scientific processes. The goal is to track the sequence of physical state changes (creation, destruction, and movement) entites undergo over long sequences of process steps. Past work involves both modeling entities across time (Das et al., 2019) and capturing structural constraints inherent in the processes (Tandon et al., 2018; Gupta and Durrett, 2019) Figure 1b shows an example of the dataset posed as a structured prediction task, as in (Gupta and Durrett, 2019). For such a domain, it is crucial to capture implicit event occurrences beyond explicit entity mentions. For example, in fuel goes into the generator. The generator converts mechanical energy into electrical energy”, the fuel is implicitly destroyed in the process. ing tasks we study, we perform additional analysis and find that these tasks still do not encourage transformers to form truly deep entity representations. Our performance gain is largely from b"
D19-1070,P19-1452,0,0.0570811,"Missing"
D19-1070,N16-1114,0,0.0531432,"Missing"
D19-1070,D08-1067,0,\N,Missing
D19-1273,J08-1001,0,0.022779,"program,1 and in a recent Text Analysis Conference (TAC).2 We frame the task as query-based scenario discovery: given a topic (e.g., the disappearance of Jamal Khashoggi) and a query (e.g. Jamal Khashoggi was murdered), we want to retrieve 1 https://www.darpa.mil/program/activeinterpretation-of-disparate-alternatives 2 https://tac.nist.gov/2018/SM-KBP/ index.html a scenario, a set of compatible events, from the given reports. We formulate query-based scenario discovery as one-class clustering (Bekkerman and Crammer, 2008). We specifically focus on discovering a scenario of compatible events (Barzilay and Lapata, 2008; Chambers and Jurafsky, 2008, 2009; Mostafazadeh et al., 2017) in a collection of related and unrelated event-denoting sentences, which may contain conflicting and irrelevant information. We start with a query (see Figure 1) and then iteratively insert sentences into the “scenario-in-construction”. Sentences are chosen based on overall compatibility as well as the ease with which scenario sentences can be arranged into an order. We additionally use an adapted relation network (Santoro et al., 2017) to assess connections between words. For our evaluation, we collect a human-curated set of comp"
D19-1273,D08-1005,0,0.237795,"ual scenarios is also being considered in the Active Interpretation of Disparate Alternatives (AIDA) program,1 and in a recent Text Analysis Conference (TAC).2 We frame the task as query-based scenario discovery: given a topic (e.g., the disappearance of Jamal Khashoggi) and a query (e.g. Jamal Khashoggi was murdered), we want to retrieve 1 https://www.darpa.mil/program/activeinterpretation-of-disparate-alternatives 2 https://tac.nist.gov/2018/SM-KBP/ index.html a scenario, a set of compatible events, from the given reports. We formulate query-based scenario discovery as one-class clustering (Bekkerman and Crammer, 2008). We specifically focus on discovering a scenario of compatible events (Barzilay and Lapata, 2008; Chambers and Jurafsky, 2008, 2009; Mostafazadeh et al., 2017) in a collection of related and unrelated event-denoting sentences, which may contain conflicting and irrelevant information. We start with a query (see Figure 1) and then iteratively insert sentences into the “scenario-in-construction”. Sentences are chosen based on overall compatibility as well as the ease with which scenario sentences can be arranged into an order. We additionally use an adapted relation network (Santoro et al., 2017"
D19-1273,P08-1090,0,0.679188,"Text Analysis Conference (TAC).2 We frame the task as query-based scenario discovery: given a topic (e.g., the disappearance of Jamal Khashoggi) and a query (e.g. Jamal Khashoggi was murdered), we want to retrieve 1 https://www.darpa.mil/program/activeinterpretation-of-disparate-alternatives 2 https://tac.nist.gov/2018/SM-KBP/ index.html a scenario, a set of compatible events, from the given reports. We formulate query-based scenario discovery as one-class clustering (Bekkerman and Crammer, 2008). We specifically focus on discovering a scenario of compatible events (Barzilay and Lapata, 2008; Chambers and Jurafsky, 2008, 2009; Mostafazadeh et al., 2017) in a collection of related and unrelated event-denoting sentences, which may contain conflicting and irrelevant information. We start with a query (see Figure 1) and then iteratively insert sentences into the “scenario-in-construction”. Sentences are chosen based on overall compatibility as well as the ease with which scenario sentences can be arranged into an order. We additionally use an adapted relation network (Santoro et al., 2017) to assess connections between words. For our evaluation, we collect a human-curated set of competing scenarios for real-worl"
D19-1273,P09-1068,0,0.0527105,"datasets, which we show serve as an effective source of training data. (3) Comprehensive experiments and analysis that cast light on the properties of the task and data, as well as on the challenges. 2 Background Our work traces its roots to research in script (Schank and Abelson, 1977; Mooney and DeJong, 1985) and narrative schema learning (Chambers and Jurafsky, 2008; Pichotta and Mooney, 2016). Early work explored tasks such as script modeling (Mooney and DeJong, 1985). Recent work built on the idea that compatibility of events can be learned from corpus data, evaluated on narrative cloze (Chambers and Jurafsky, 2009) and predicting-next-events (Pichotta and Mooney, 2016; Mostafazadeh et al., 2017). We introduce a task with a more practical objective in mind: given a query or an information cue, extract the rest of the pieces to build a compatible scenario. The task is related to conversation disentanglement of multiple entangled conversations in a dialogue transcript (Elsner and Charniak, 2008, 2011; Jiang et al., 2018; Kummerfeld et al., 2019), and more closely to narrative clustering (Wang et al., 2018), i.e. identifying all the scenarios in an information source by grouping relevant sentences/events. U"
D19-1273,D13-1203,1,0.74865,"Missing"
D19-1273,P08-1095,0,0.0228647,"ooney, 2016). Early work explored tasks such as script modeling (Mooney and DeJong, 1985). Recent work built on the idea that compatibility of events can be learned from corpus data, evaluated on narrative cloze (Chambers and Jurafsky, 2009) and predicting-next-events (Pichotta and Mooney, 2016; Mostafazadeh et al., 2017). We introduce a task with a more practical objective in mind: given a query or an information cue, extract the rest of the pieces to build a compatible scenario. The task is related to conversation disentanglement of multiple entangled conversations in a dialogue transcript (Elsner and Charniak, 2008, 2011; Jiang et al., 2018; Kummerfeld et al., 2019), and more closely to narrative clustering (Wang et al., 2018), i.e. identifying all the scenarios in an information source by grouping relevant sentences/events. Unlike Wang et al. (2018), we do not attempt to identify all the scenarios in the source, but are guided by one particular user’s information need (e.g. the scenario about Khashoggi’s murder, as opposed to all the theories regarding his disappearance, like in Wang et al. (2018)). Further, we do not assume the number of scenarios is known a priori (as Wang et al. (2018) do). We phras"
D19-1273,P11-1118,0,0.0689189,"Missing"
D19-1273,N18-1164,0,0.0126357,"tasks such as script modeling (Mooney and DeJong, 1985). Recent work built on the idea that compatibility of events can be learned from corpus data, evaluated on narrative cloze (Chambers and Jurafsky, 2009) and predicting-next-events (Pichotta and Mooney, 2016; Mostafazadeh et al., 2017). We introduce a task with a more practical objective in mind: given a query or an information cue, extract the rest of the pieces to build a compatible scenario. The task is related to conversation disentanglement of multiple entangled conversations in a dialogue transcript (Elsner and Charniak, 2008, 2011; Jiang et al., 2018; Kummerfeld et al., 2019), and more closely to narrative clustering (Wang et al., 2018), i.e. identifying all the scenarios in an information source by grouping relevant sentences/events. Unlike Wang et al. (2018), we do not attempt to identify all the scenarios in the source, but are guided by one particular user’s information need (e.g. the scenario about Khashoggi’s murder, as opposed to all the theories regarding his disappearance, like in Wang et al. (2018)). Further, we do not assume the number of scenarios is known a priori (as Wang et al. (2018) do). We phrase query-based scenario con"
D19-1273,1985.tmi-1.17,0,0.0830579,"d: (1) A querybased scenario construction task, for which we introduce a model to iteratively build a scenario with compatible events, exploiting ordering. (2) A human-curated evaluation set consisting of multiple accounts of real-world new events, along with a collection of scalably-built synthetic simulation datasets, which we show serve as an effective source of training data. (3) Comprehensive experiments and analysis that cast light on the properties of the task and data, as well as on the challenges. 2 Background Our work traces its roots to research in script (Schank and Abelson, 1977; Mooney and DeJong, 1985) and narrative schema learning (Chambers and Jurafsky, 2008; Pichotta and Mooney, 2016). Early work explored tasks such as script modeling (Mooney and DeJong, 1985). Recent work built on the idea that compatibility of events can be learned from corpus data, evaluated on narrative cloze (Chambers and Jurafsky, 2009) and predicting-next-events (Pichotta and Mooney, 2016; Mostafazadeh et al., 2017). We introduce a task with a more practical objective in mind: given a query or an information cue, extract the rest of the pieces to build a compatible scenario. The task is related to conversation dis"
D19-1273,W17-0906,0,0.301187,"frame the task as query-based scenario discovery: given a topic (e.g., the disappearance of Jamal Khashoggi) and a query (e.g. Jamal Khashoggi was murdered), we want to retrieve 1 https://www.darpa.mil/program/activeinterpretation-of-disparate-alternatives 2 https://tac.nist.gov/2018/SM-KBP/ index.html a scenario, a set of compatible events, from the given reports. We formulate query-based scenario discovery as one-class clustering (Bekkerman and Crammer, 2008). We specifically focus on discovering a scenario of compatible events (Barzilay and Lapata, 2008; Chambers and Jurafsky, 2008, 2009; Mostafazadeh et al., 2017) in a collection of related and unrelated event-denoting sentences, which may contain conflicting and irrelevant information. We start with a query (see Figure 1) and then iteratively insert sentences into the “scenario-in-construction”. Sentences are chosen based on overall compatibility as well as the ease with which scenario sentences can be arranged into an order. We additionally use an adapted relation network (Santoro et al., 2017) to assess connections between words. For our evaluation, we collect a human-curated set of competing scenarios for real-world news topics. As collecting such"
D19-1273,H05-1115,0,0.13816,"Missing"
D19-1273,N18-1202,0,0.0196191,"Missing"
D19-1273,P16-1027,0,0.0179623,"ratively build a scenario with compatible events, exploiting ordering. (2) A human-curated evaluation set consisting of multiple accounts of real-world new events, along with a collection of scalably-built synthetic simulation datasets, which we show serve as an effective source of training data. (3) Comprehensive experiments and analysis that cast light on the properties of the task and data, as well as on the challenges. 2 Background Our work traces its roots to research in script (Schank and Abelson, 1977; Mooney and DeJong, 1985) and narrative schema learning (Chambers and Jurafsky, 2008; Pichotta and Mooney, 2016). Early work explored tasks such as script modeling (Mooney and DeJong, 1985). Recent work built on the idea that compatibility of events can be learned from corpus data, evaluated on narrative cloze (Chambers and Jurafsky, 2009) and predicting-next-events (Pichotta and Mooney, 2016; Mostafazadeh et al., 2017). We introduce a task with a more practical objective in mind: given a query or an information cue, extract the rest of the pieces to build a compatible scenario. The task is related to conversation disentanglement of multiple entangled conversations in a dialogue transcript (Elsner and C"
D19-1273,D18-1175,1,0.889076,"ant information. We start with a query (see Figure 1) and then iteratively insert sentences into the “scenario-in-construction”. Sentences are chosen based on overall compatibility as well as the ease with which scenario sentences can be arranged into an order. We additionally use an adapted relation network (Santoro et al., 2017) to assess connections between words. For our evaluation, we collect a human-curated set of competing scenarios for real-world news topics. As collecting such data is costly, we follow past work in training our model on synthetic data consisting of document mixtures (Wang et al., 2018) and compare our models directly to theirs. We show that training on such synthetic data yields a model that can substantially outper2712 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2712–2722, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Figure 2: Phase 2 (cf. Table 1) of generating human evaluation data Human100: given the topic Why did MH-17 crash? and the scenario MH-17 had a bomb on board. The annotator searches the web and fi"
D19-1324,P11-1049,0,0.390809,"ction Neural network approaches to document summarization have ranged from purely extractive (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018) to abstractive (Rush et al., 2015; Nallapati et al., 2016; Chopra et al., 2016; Tan et al., 2017; Gehrmann et al., 2018). Extractive systems are robust and straightforward to use. Abstractive systems are more flexible for varied summarization situations (Grusky et al., 2018), but can make factual errors (Cao et al., 2018; Li et al., 2018) or fall back on extraction in practice (See et al., 2017). Extractive and compressive systems (Berg-Kirkpatrick et al., 2011; Qian and Liu, 2013; Durrett et al., 2016) combine the strengths of both approaches; however, there has been little work studying neural network models in this vein, and the approaches that have been employed typically use seq2seq-based sentence compression (Chen and Bansal, 2018). In this work, we propose a model that can combine the high performance of neural extractive systems, additional flexibility from compression, and interpretability given by having discrete compression options. Our model first encodes the source document and its sentences and then sequentially selects a set of senten"
D19-1324,W01-1605,0,0.0145533,"ibuted to the parser.1 2 Compression in Summarization Sentence compression is a long-studied problem dealing with how to delete the least critical information in a sentence to make it shorter (Knight and Marcu, 2000, 2002; Martins and Smith, 2009; Cohn and Lapata, 2009; Wang et al., 2013; Li et al., 2014). Many of these approaches are syntax-driven, though end-to-end neural models have been proposed as well (Filippova et al., 2015; Wang et al., 2017). Past non-neural work on summarization has used both syntax-based (BergKirkpatrick et al., 2011; Woodsend and Lapata, 2011) and discourse-based (Carlson et al., 2001; Hirao et al., 2013; Li et al., 2016) compressions. Our approach follows in the syntax-driven vein. Our high-level approach to summarization is shown in Figure 1. In Section 3, we describe the models for extraction and compression. Our compression depends on having a discrete set of valid compression options that maintain the grammaticality of the underlying sentence, which we now proceed to describe. 1 The code, full model output, and the pre-trained model are available at https://github.com/ jiacheng-xu/neu-compression-sum Compression Rules We refer to the rules derived in Li et al. (2014),"
D19-1324,P18-1063,0,0.783333,"ractive systems are robust and straightforward to use. Abstractive systems are more flexible for varied summarization situations (Grusky et al., 2018), but can make factual errors (Cao et al., 2018; Li et al., 2018) or fall back on extraction in practice (See et al., 2017). Extractive and compressive systems (Berg-Kirkpatrick et al., 2011; Qian and Liu, 2013; Durrett et al., 2016) combine the strengths of both approaches; however, there has been little work studying neural network models in this vein, and the approaches that have been employed typically use seq2seq-based sentence compression (Chen and Bansal, 2018). In this work, we propose a model that can combine the high performance of neural extractive systems, additional flexibility from compression, and interpretability given by having discrete compression options. Our model first encodes the source document and its sentences and then sequentially selects a set of sentences to further compress. Each sentence has a set of compression options available that are selected to preserve meaning and grammaticality; these are derived from syntactic constituency parses and represent an expanded set of discrete options from prior work (Berg-Kirkpatrick et al"
D19-1324,P16-1046,0,0.0592373,"and New York Times datasets show that our model achieves strong performance (comparable to state-of-the-art systems) as evaluated by ROUGE. Moreover, our approach outperforms an off-theshelf compression module, and human and manual evaluation shows that our model’s output generally remains grammatical. Extraction Module Output Summary Ref Figure 1: Diagram of the proposed model. Extraction and compression are modularized but jointly trained with supervision derived from the reference summary. Introduction Neural network approaches to document summarization have ranged from purely extractive (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018) to abstractive (Rush et al., 2015; Nallapati et al., 2016; Chopra et al., 2016; Tan et al., 2017; Gehrmann et al., 2018). Extractive systems are robust and straightforward to use. Abstractive systems are more flexible for varied summarization situations (Grusky et al., 2018), but can make factual errors (Cao et al., 2018; Li et al., 2018) or fall back on extraction in practice (See et al., 2017). Extractive and compressive systems (Berg-Kirkpatrick et al., 2011; Qian and Liu, 2013; Durrett et al., 2016) combine the strengths of both approaches; h"
D19-1324,N16-1012,0,0.0787247,"d by ROUGE. Moreover, our approach outperforms an off-theshelf compression module, and human and manual evaluation shows that our model’s output generally remains grammatical. Extraction Module Output Summary Ref Figure 1: Diagram of the proposed model. Extraction and compression are modularized but jointly trained with supervision derived from the reference summary. Introduction Neural network approaches to document summarization have ranged from purely extractive (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018) to abstractive (Rush et al., 2015; Nallapati et al., 2016; Chopra et al., 2016; Tan et al., 2017; Gehrmann et al., 2018). Extractive systems are robust and straightforward to use. Abstractive systems are more flexible for varied summarization situations (Grusky et al., 2018), but can make factual errors (Cao et al., 2018; Li et al., 2018) or fall back on extraction in practice (See et al., 2017). Extractive and compressive systems (Berg-Kirkpatrick et al., 2011; Qian and Liu, 2013; Durrett et al., 2016) combine the strengths of both approaches; however, there has been little work studying neural network models in this vein, and the approaches that have been employed typ"
D19-1324,D18-1409,0,0.330547,"Missing"
D19-1324,P16-1188,1,0.925866,"zation have ranged from purely extractive (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018) to abstractive (Rush et al., 2015; Nallapati et al., 2016; Chopra et al., 2016; Tan et al., 2017; Gehrmann et al., 2018). Extractive systems are robust and straightforward to use. Abstractive systems are more flexible for varied summarization situations (Grusky et al., 2018), but can make factual errors (Cao et al., 2018; Li et al., 2018) or fall back on extraction in practice (See et al., 2017). Extractive and compressive systems (Berg-Kirkpatrick et al., 2011; Qian and Liu, 2013; Durrett et al., 2016) combine the strengths of both approaches; however, there has been little work studying neural network models in this vein, and the approaches that have been employed typically use seq2seq-based sentence compression (Chen and Bansal, 2018). In this work, we propose a model that can combine the high performance of neural extractive systems, additional flexibility from compression, and interpretability given by having discrete compression options. Our model first encodes the source document and its sentences and then sequentially selects a set of sentences to further compress. Each sentence has"
D19-1324,D15-1042,0,0.254175,"nvestigate the fluency and grammaticality of our compressed sentences. The human evaluation shows that our system yields generally grammatical output, with many remaining errors being attributed to the parser.1 2 Compression in Summarization Sentence compression is a long-studied problem dealing with how to delete the least critical information in a sentence to make it shorter (Knight and Marcu, 2000, 2002; Martins and Smith, 2009; Cohn and Lapata, 2009; Wang et al., 2013; Li et al., 2014). Many of these approaches are syntax-driven, though end-to-end neural models have been proposed as well (Filippova et al., 2015; Wang et al., 2017). Past non-neural work on summarization has used both syntax-based (BergKirkpatrick et al., 2011; Woodsend and Lapata, 2011) and discourse-based (Carlson et al., 2001; Hirao et al., 2013; Li et al., 2016) compressions. Our approach follows in the syntax-driven vein. Our high-level approach to summarization is shown in Figure 1. In Section 3, we describe the models for extraction and compression. Our compression depends on having a discrete set of valid compression options that maintain the grammaticality of the underlying sentence, which we now proceed to describe. 1 The co"
D19-1324,D18-1443,0,0.0786726,"erforms an off-theshelf compression module, and human and manual evaluation shows that our model’s output generally remains grammatical. Extraction Module Output Summary Ref Figure 1: Diagram of the proposed model. Extraction and compression are modularized but jointly trained with supervision derived from the reference summary. Introduction Neural network approaches to document summarization have ranged from purely extractive (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018) to abstractive (Rush et al., 2015; Nallapati et al., 2016; Chopra et al., 2016; Tan et al., 2017; Gehrmann et al., 2018). Extractive systems are robust and straightforward to use. Abstractive systems are more flexible for varied summarization situations (Grusky et al., 2018), but can make factual errors (Cao et al., 2018; Li et al., 2018) or fall back on extraction in practice (See et al., 2017). Extractive and compressive systems (Berg-Kirkpatrick et al., 2011; Qian and Liu, 2013; Durrett et al., 2016) combine the strengths of both approaches; however, there has been little work studying neural network models in this vein, and the approaches that have been employed typically use seq2seq-based sentence compress"
D19-1324,W09-1802,0,0.119449,"Missing"
D19-1324,C12-1059,0,0.0247622,"Missing"
D19-1324,N18-1065,0,0.158471,"Missing"
D19-1324,D13-1158,0,0.0664871,"Missing"
D19-1324,D18-1440,0,0.0824513,"JECS is the full Joint Extractive and Compressive Summarizer. We compare our model with various abstractive and extractive summarization models. NeuSum (Zhou et al., 2018) uses a seq2seq model to predict a sequence of sentences indices to be picked up from the document. Our extractive approach is most similar to this model. Refresh (Narayan et al., 2018), BanditSum (Dong et al., 2018) and LatSum (Zhang et al., 2018) are extractive summarization models for comparison. We also compare with some abstractive models including PointGenCov (See et al., 2017), FARS (Chen and Bansal, 2018) and CBDec (Jiang and Bansal, 2018). We also compare our joint model with a pipeline model with an off-the-shelf compression module. We implement a deletion-based BiLSTM model for sentence compression (Wang et al., 2017) and run the model on top of our extraction output.5 5 We reimplemented the authors’ model following their specification and matched their accuracy. For fair compariR-1 CNNDM R-2 R-L Lead (Ours) Refresh* (Narayan et al., 2018) NeuSum LatSum* (Zhang et al., 2018) LatSum w/ Compression BanditSum CBDec (Jiang and Bansal, 2018) FARS (Chen and Bansal, 2018) 40.3 40.0 41.6 41.0 36.7 41.5 40.7 40.9 17.6 18.1 19.0 18.8"
D19-1324,D14-1076,0,0.134669,"’s compression threshold is robust across a range of settings yet tunable to give differentlength summaries. Finally, we investigate the fluency and grammaticality of our compressed sentences. The human evaluation shows that our system yields generally grammatical output, with many remaining errors being attributed to the parser.1 2 Compression in Summarization Sentence compression is a long-studied problem dealing with how to delete the least critical information in a sentence to make it shorter (Knight and Marcu, 2000, 2002; Martins and Smith, 2009; Cohn and Lapata, 2009; Wang et al., 2013; Li et al., 2014). Many of these approaches are syntax-driven, though end-to-end neural models have been proposed as well (Filippova et al., 2015; Wang et al., 2017). Past non-neural work on summarization has used both syntax-based (BergKirkpatrick et al., 2011; Woodsend and Lapata, 2011) and discourse-based (Carlson et al., 2001; Hirao et al., 2013; Li et al., 2016) compressions. Our approach follows in the syntax-driven vein. Our high-level approach to summarization is shown in Figure 1. In Section 3, we describe the models for extraction and compression. Our compression depends on having a discrete set of v"
D19-1324,C18-1121,0,0.0188325,"and compression are modularized but jointly trained with supervision derived from the reference summary. Introduction Neural network approaches to document summarization have ranged from purely extractive (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018) to abstractive (Rush et al., 2015; Nallapati et al., 2016; Chopra et al., 2016; Tan et al., 2017; Gehrmann et al., 2018). Extractive systems are robust and straightforward to use. Abstractive systems are more flexible for varied summarization situations (Grusky et al., 2018), but can make factual errors (Cao et al., 2018; Li et al., 2018) or fall back on extraction in practice (See et al., 2017). Extractive and compressive systems (Berg-Kirkpatrick et al., 2011; Qian and Liu, 2013; Durrett et al., 2016) combine the strengths of both approaches; however, there has been little work studying neural network models in this vein, and the approaches that have been employed typically use seq2seq-based sentence compression (Chen and Bansal, 2018). In this work, we propose a model that can combine the high performance of neural extractive systems, additional flexibility from compression, and interpretability given by having discrete com"
D19-1324,W16-3617,0,0.063867,"mmarization Sentence compression is a long-studied problem dealing with how to delete the least critical information in a sentence to make it shorter (Knight and Marcu, 2000, 2002; Martins and Smith, 2009; Cohn and Lapata, 2009; Wang et al., 2013; Li et al., 2014). Many of these approaches are syntax-driven, though end-to-end neural models have been proposed as well (Filippova et al., 2015; Wang et al., 2017). Past non-neural work on summarization has used both syntax-based (BergKirkpatrick et al., 2011; Woodsend and Lapata, 2011) and discourse-based (Carlson et al., 2001; Hirao et al., 2013; Li et al., 2016) compressions. Our approach follows in the syntax-driven vein. Our high-level approach to summarization is shown in Figure 1. In Section 3, we describe the models for extraction and compression. Our compression depends on having a discrete set of valid compression options that maintain the grammaticality of the underlying sentence, which we now proceed to describe. 1 The code, full model output, and the pre-trained model are available at https://github.com/ jiacheng-xu/neu-compression-sum Compression Rules We refer to the rules derived in Li et al. (2014), Wang et al. (2013), and Durrett et al"
D19-1324,P14-5010,0,0.00881534,"Missing"
D19-1324,K16-1028,0,0.0502692,"art systems) as evaluated by ROUGE. Moreover, our approach outperforms an off-theshelf compression module, and human and manual evaluation shows that our model’s output generally remains grammatical. Extraction Module Output Summary Ref Figure 1: Diagram of the proposed model. Extraction and compression are modularized but jointly trained with supervision derived from the reference summary. Introduction Neural network approaches to document summarization have ranged from purely extractive (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018) to abstractive (Rush et al., 2015; Nallapati et al., 2016; Chopra et al., 2016; Tan et al., 2017; Gehrmann et al., 2018). Extractive systems are robust and straightforward to use. Abstractive systems are more flexible for varied summarization situations (Grusky et al., 2018), but can make factual errors (Cao et al., 2018; Li et al., 2018) or fall back on extraction in practice (See et al., 2017). Extractive and compressive systems (Berg-Kirkpatrick et al., 2011; Qian and Liu, 2013; Durrett et al., 2016) combine the strengths of both approaches; however, there has been little work studying neural network models in this vein, and the approaches that h"
D19-1324,N18-1158,0,0.436536,"achieves strong performance (comparable to state-of-the-art systems) as evaluated by ROUGE. Moreover, our approach outperforms an off-theshelf compression module, and human and manual evaluation shows that our model’s output generally remains grammatical. Extraction Module Output Summary Ref Figure 1: Diagram of the proposed model. Extraction and compression are modularized but jointly trained with supervision derived from the reference summary. Introduction Neural network approaches to document summarization have ranged from purely extractive (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018) to abstractive (Rush et al., 2015; Nallapati et al., 2016; Chopra et al., 2016; Tan et al., 2017; Gehrmann et al., 2018). Extractive systems are robust and straightforward to use. Abstractive systems are more flexible for varied summarization situations (Grusky et al., 2018), but can make factual errors (Cao et al., 2018; Li et al., 2018) or fall back on extraction in practice (See et al., 2017). Extractive and compressive systems (Berg-Kirkpatrick et al., 2011; Qian and Liu, 2013; Durrett et al., 2016) combine the strengths of both approaches; however, there has been little work studying neu"
D19-1324,N18-1202,0,0.00967174,"onality traits shared between artists and their felines. Document: ... Philadelphia-based artist and journalist Alison Nastasi has collated a collection of intimate portraits featuring well-known artists with their furry friends. ... Let Ci = {ci1 , · · · , cil } denote the possible compression spans derived from the rules described in Section 2. Let yi,c be a binary variable equal to 1 if we are deleting the cth option of the ith sentence. Our text compression module models p(yi,c |D, sˆt = si ) as described in the following section. Compression Encoder We use a contextualized encoder, ELMo (Peters et al., 2018) to compute contextualized word representations. We then use CNNs with max pooling to encode the sentence (shown in blue in Figure 4) and the candidate compression (shown in light green in Figure 4). The sentence representation vsent and the compression span representation vcomp are concatenated with the hidden state in sentence decoder hdec and the document representation vdoc . Compression Classifier We feed the concatenated representation to a feedforward neural network to predict whether the compression span should be deleted or kept, which is formulated as a binary classification problem."
D19-1324,D13-1156,0,0.0245872,"to document summarization have ranged from purely extractive (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018) to abstractive (Rush et al., 2015; Nallapati et al., 2016; Chopra et al., 2016; Tan et al., 2017; Gehrmann et al., 2018). Extractive systems are robust and straightforward to use. Abstractive systems are more flexible for varied summarization situations (Grusky et al., 2018), but can make factual errors (Cao et al., 2018; Li et al., 2018) or fall back on extraction in practice (See et al., 2017). Extractive and compressive systems (Berg-Kirkpatrick et al., 2011; Qian and Liu, 2013; Durrett et al., 2016) combine the strengths of both approaches; however, there has been little work studying neural network models in this vein, and the approaches that have been employed typically use seq2seq-based sentence compression (Chen and Bansal, 2018). In this work, we propose a model that can combine the high performance of neural extractive systems, additional flexibility from compression, and interpretability given by having discrete compression options. Our model first encodes the source document and its sentences and then sequentially selects a set of sentences to further compr"
D19-1324,W09-1801,0,0.351759,"the more compressed nature of CNN summaries. We show that our model’s compression threshold is robust across a range of settings yet tunable to give differentlength summaries. Finally, we investigate the fluency and grammaticality of our compressed sentences. The human evaluation shows that our system yields generally grammatical output, with many remaining errors being attributed to the parser.1 2 Compression in Summarization Sentence compression is a long-studied problem dealing with how to delete the least critical information in a sentence to make it shorter (Knight and Marcu, 2000, 2002; Martins and Smith, 2009; Cohn and Lapata, 2009; Wang et al., 2013; Li et al., 2014). Many of these approaches are syntax-driven, though end-to-end neural models have been proposed as well (Filippova et al., 2015; Wang et al., 2017). Past non-neural work on summarization has used both syntax-based (BergKirkpatrick et al., 2011; Woodsend and Lapata, 2011) and discourse-based (Carlson et al., 2001; Hirao et al., 2013; Li et al., 2016) compressions. Our approach follows in the syntax-driven vein. Our high-level approach to summarization is shown in Figure 1. In Section 3, we describe the models for extraction and compre"
D19-1324,D15-1044,0,0.0639506,"le to state-of-the-art systems) as evaluated by ROUGE. Moreover, our approach outperforms an off-theshelf compression module, and human and manual evaluation shows that our model’s output generally remains grammatical. Extraction Module Output Summary Ref Figure 1: Diagram of the proposed model. Extraction and compression are modularized but jointly trained with supervision derived from the reference summary. Introduction Neural network approaches to document summarization have ranged from purely extractive (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018) to abstractive (Rush et al., 2015; Nallapati et al., 2016; Chopra et al., 2016; Tan et al., 2017; Gehrmann et al., 2018). Extractive systems are robust and straightforward to use. Abstractive systems are more flexible for varied summarization situations (Grusky et al., 2018), but can make factual errors (Cao et al., 2018; Li et al., 2018) or fall back on extraction in practice (See et al., 2017). Extractive and compressive systems (Berg-Kirkpatrick et al., 2011; Qian and Liu, 2013; Durrett et al., 2016) combine the strengths of both approaches; however, there has been little work studying neural network models in this vein, a"
D19-1324,N19-1397,0,0.512457,"Missing"
D19-1324,D16-1031,0,0.0262582,"n Prior to the explosion of neural models for summarization, syntactic compression (Martins and Smith, 2009; Woodsend and Lapata, 2011) was relatively more common. Several systems explored the usage of constituency parses (Berg-Kirkpatrick et al., 2011; Wang et al., 2013; Li et al., 2014) as well as RSTbased approaches (Hirao et al., 2013; Durrett et al., 2016). Our approach follows in this vein but could be combined with more sophisticated neural text compression methods as well. Neural Text Compression Filippova et al. (2015) presented an LSTM approach to deletionbased sentence compression. Miao and Blunsom (2016) proposed a deep generative model for text compression. Zhang et al. (2018) explored the compression module after the extraction model but the separation of these two modules hurt the performance. For this work, we find that relying on syntax gives us more easily understandable and controllable compression options. Contemporaneously with our work, Mendes et al. (2019) explored an extractive and compressive approach using compression integrated into a sequential decoding process; however, their approach does not leverage explicit syntax and makes several different model design choices. 8 Conclu"
D19-1324,P17-1099,0,0.662184,"supervision derived from the reference summary. Introduction Neural network approaches to document summarization have ranged from purely extractive (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018) to abstractive (Rush et al., 2015; Nallapati et al., 2016; Chopra et al., 2016; Tan et al., 2017; Gehrmann et al., 2018). Extractive systems are robust and straightforward to use. Abstractive systems are more flexible for varied summarization situations (Grusky et al., 2018), but can make factual errors (Cao et al., 2018; Li et al., 2018) or fall back on extraction in practice (See et al., 2017). Extractive and compressive systems (Berg-Kirkpatrick et al., 2011; Qian and Liu, 2013; Durrett et al., 2016) combine the strengths of both approaches; however, there has been little work studying neural network models in this vein, and the approaches that have been employed typically use seq2seq-based sentence compression (Chen and Bansal, 2018). In this work, we propose a model that can combine the high performance of neural extractive systems, additional flexibility from compression, and interpretability given by having discrete compression options. Our model first encodes the source docum"
D19-1324,P17-1108,0,0.0295086,"our approach outperforms an off-theshelf compression module, and human and manual evaluation shows that our model’s output generally remains grammatical. Extraction Module Output Summary Ref Figure 1: Diagram of the proposed model. Extraction and compression are modularized but jointly trained with supervision derived from the reference summary. Introduction Neural network approaches to document summarization have ranged from purely extractive (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018) to abstractive (Rush et al., 2015; Nallapati et al., 2016; Chopra et al., 2016; Tan et al., 2017; Gehrmann et al., 2018). Extractive systems are robust and straightforward to use. Abstractive systems are more flexible for varied summarization situations (Grusky et al., 2018), but can make factual errors (Cao et al., 2018; Li et al., 2018) or fall back on extraction in practice (See et al., 2017). Extractive and compressive systems (Berg-Kirkpatrick et al., 2011; Qian and Liu, 2013; Durrett et al., 2016) combine the strengths of both approaches; however, there has been little work studying neural network models in this vein, and the approaches that have been employed typically use seq2seq"
D19-1324,P17-1127,0,0.0944682,"nd grammaticality of our compressed sentences. The human evaluation shows that our system yields generally grammatical output, with many remaining errors being attributed to the parser.1 2 Compression in Summarization Sentence compression is a long-studied problem dealing with how to delete the least critical information in a sentence to make it shorter (Knight and Marcu, 2000, 2002; Martins and Smith, 2009; Cohn and Lapata, 2009; Wang et al., 2013; Li et al., 2014). Many of these approaches are syntax-driven, though end-to-end neural models have been proposed as well (Filippova et al., 2015; Wang et al., 2017). Past non-neural work on summarization has used both syntax-based (BergKirkpatrick et al., 2011; Woodsend and Lapata, 2011) and discourse-based (Carlson et al., 2001; Hirao et al., 2013; Li et al., 2016) compressions. Our approach follows in the syntax-driven vein. Our high-level approach to summarization is shown in Figure 1. In Section 3, we describe the models for extraction and compression. Our compression depends on having a discrete set of valid compression options that maintain the grammaticality of the underlying sentence, which we now proceed to describe. 1 The code, full model outpu"
D19-1324,P13-1136,0,0.380986,"work, we propose a model that can combine the high performance of neural extractive systems, additional flexibility from compression, and interpretability given by having discrete compression options. Our model first encodes the source document and its sentences and then sequentially selects a set of sentences to further compress. Each sentence has a set of compression options available that are selected to preserve meaning and grammaticality; these are derived from syntactic constituency parses and represent an expanded set of discrete options from prior work (Berg-Kirkpatrick et al., 2011; Wang et al., 2013). The neural model additionally scores and chooses which compressions to apply given the context of the document, the sentence, and the decoder model’s recurrent state. A principal challenge of training an extractive and compressive model is constructing the oracle summary for supervision. We identify a set of high-quality sentences from the document with beam search and derive oracle compression labels in each sentence through an additional refinement process. Our model’s training objective combines these extractive and compressive components and learns them jointly. We conduct experiments on"
D19-1324,D11-1038,0,0.244061,"al output, with many remaining errors being attributed to the parser.1 2 Compression in Summarization Sentence compression is a long-studied problem dealing with how to delete the least critical information in a sentence to make it shorter (Knight and Marcu, 2000, 2002; Martins and Smith, 2009; Cohn and Lapata, 2009; Wang et al., 2013; Li et al., 2014). Many of these approaches are syntax-driven, though end-to-end neural models have been proposed as well (Filippova et al., 2015; Wang et al., 2017). Past non-neural work on summarization has used both syntax-based (BergKirkpatrick et al., 2011; Woodsend and Lapata, 2011) and discourse-based (Carlson et al., 2001; Hirao et al., 2013; Li et al., 2016) compressions. Our approach follows in the syntax-driven vein. Our high-level approach to summarization is shown in Figure 1. In Section 3, we describe the models for extraction and compression. Our compression depends on having a discrete set of valid compression options that maintain the grammaticality of the underlying sentence, which we now proceed to describe. 1 The code, full model output, and the pre-trained model are available at https://github.com/ jiacheng-xu/neu-compression-sum Compression Rules We refer"
D19-1324,D18-1088,0,0.46917,"w the “compressability” of these three datasets: how valuable various compression options seem to be from the standpoint of improving ROUGE. We found that CNN has significantly more positive compression options than the other two. Critically, CNN also has the shortest references (37 words on average, Learning Objective 4 Often, many oracle summaries achieve very similar ROUGE values. We therefore want to More details about the experimental setup, implementation details, and human evaluation are provided in the Appendix. 3296 R-1 CNN R-2 R-L Lead (Ours) Refresh* (Narayan et al., 2018) LatSum* (Zhang et al., 2018) BanditSum (Dong et al., 2018) 29.1 30.3 28.8 30.7 11.1 11.6 11.5 11.6 25.8 26.9 25.4 27.4 L EAD D EDUP L EAD C OMP E XTRACTION E XT LSTMD EL JECS 29.7 30.6 30.3 30.6 32.7 10.9 10.8 11.0 11.9 12.2 26.2 27.2 26.5 27.1 29.0 Model Model Table 3: Experimental results on the test sets of CNN. * indicates models evaluates with our own ROUGE metrics. Our model outperforms our extractive model and lead-based baselines, as well as prior work. compared to 61 for Daily Mail; see Appendix). In our experiments, we first focus on CNN and then evaluate on the other datasets. Models We present several variant"
D19-1324,P18-1061,0,0.169046,"N combination, shown in Figure 3 with orange blocks. Decoding The decoding stage selects a number of sentences given the document representation vdoc and sentences’ representations hi . This process is depicted in the right half of Figure 3. We use a sequential LSTM decoder where, at each time step, we take the representation h of the last selected sentence, the overall document vector vdoc , and the recurrent state dt−1 , and produce a distribution over all of the remaining sentences excluding those already selected. This approach resembles pointer network-style approaches used in past work (Zhou et al., 2018). Formally, we write this as: dt = LSTM(dt−1 , hk , vdoc ) scoret,i = Wm tanh(Wd dt + Wh hi ) Contextualized Encoder …… [ hdec Label where hk is the representation of the sentence selected at time step t − 1. dt−1 is the decoding hid; vsent Keep Del ; vdoc Binary Classification ; vcomp ] FFNN Figure 4: Text compression module. A neural classifier scores the compression option (with their furry friends) in the sentence and broader document context and decides whether or not to delete it. den state from last time step. Wd , Wh , Wm , and parameters in LSTM are learned. Once a sentence is selecte"
D19-1324,W04-1013,0,\N,Missing
D19-1324,W10-0722,0,\N,Missing
N13-1138,N07-1020,0,0.0212847,"Missing"
N13-1138,E12-1066,0,0.0743844,"Missing"
N13-1138,E12-1053,0,0.109772,"Missing"
N13-1138,D11-1057,0,0.654445,"nd attribute vector, we selected the first in linear order; applying the same principle, we also kept only the first inflection table when more than one was listed for a given base form. Furthermore, base forms and inflected forms separated by spaces, hyphens, or colons were discarded. As a result, we discarded German verbpreposition compounds such as ablehnen4 and Spanish reflexives such as lavarse. 6 Experiments We evaluate our model under two experimental conditions. First, we use the German verb lexicon in the CELEX lexical database (Baayen et al., 1995) with the same train/test splits as Dreyer and Eisner (2011). Second, we train on our Wiktionary data described in Section 5 and evaluate on held-out forms from this same dataset. In each case, we evaluate two variants of our model in order to examine the importance of jointly modeling the production of the entire inflection table. Our J OINT model is exactly as defined in Section 4. For our FACTORED model, the dictionary of rules is extracted separately for each setting of the attributes a; i.e., we run the entire procedure in Section 3 with only one inflected form at a time and forego the U NION S PANS step. A separate prediction model is trained for"
N13-1138,P02-1001,0,0.0111212,"rb denken aligning to its past participle gedacht, the initial d and g will be aligned and the following e’s will be aligned, preventing the algorithm from recognizing the addition of the prefix ge-. To solve this problem, we use a dynamic edit distance cost scheme in which I, D, and unmatched substitutions all have a cost of 0. Matched substitutions have a negative cost −ci , where i is the index in the base form and ci is the number of other inflected forms for which i is aligned to a matching character. The inflected forms are iteratively realigned with the base form until the ci converge (Eisner, 2002; Oncina and Sebban, 2006). This cost scheme encourages a single consistent analysis of the base form as it aligns to all of its inflected forms. Span Merging. From each aligned pair of words, the P ROJECT S PANS procedure identifies sequences of character edit operations with contiguous spans of the base form. We construct a set of changed spans Ca of b as follows: include the span (i, j) if and only if no characters between i and j were aligned to matching characters in Ta (b) and no smaller span captures the same set of changes. Projected spans for the inflected forms of schleichen are show"
N13-1138,J01-2001,0,0.357111,"se language-specific, curated resources. In overall setup, our work most closely resembles that of Dreyer and Eisner (2011), but they focus on incorporating large amounts of raw text data rather than using large training sets effectively. Broadly similar techniques are also employed in systems to filter candidate rules and aid in human annotation of paradigms (Zajac, 2001; Forsberg et al., 2006; D´etrez and Ranta, 2012) for resources such as Grammatical Framework (Ranta, 2011). Related Work Much of the past work on morphology has focused on concatenative morphology using unsupervised methods (Goldsmith, 2001; Creutz and Lagus, 2007; Monson, 2008; Poon et al., 2009; Goldwater et al., 2009) or weak forms of supervision (Snyder and Barzilay, 2008). These methods can handle aspects of derivational morphology that we cannot, such as compounding, but we can handle a much larger subset of inflectional morphology, including more complex prefix and suffix rules, stem changes, and irregular forms. Some unsupervised work has specifically targeted these sorts of phenomena by, for example, learning spelling rules for mildly nonconcatenative cases (Dasgupta and Ng, 2007; Naradowsky and Goldwater, 2009) or mini"
N13-1138,P08-1103,0,0.00742441,"odels to compute the gradient of pw , and at test time, the Viterbi algorithm can exactly find the best rule subset under the model: Aˆ = arg maxA pw (A|b). Features. The feature function φ captures contextual information in the base form surrounding the site of the anchored rule application. It is well understood that different morphological rules may require examining different amounts of context to apply correctly (Kohonen, 1986; Torkkola, 1993; Shalonova and Gol´enia, 2010); to this end, we will use local character n-gram features, which have been successfully applied to related problems (Jiampojamarn et al., 2008; Dinu et al., 2012). A sketch of our feature computation scheme is shown in Figure 2b. Our basic feature template is an indicator on a character n-gram with some offset from the rule application site, conjoined with the identity of the rule R being applied. Our features look at variable amounts of context: we include features on unigrams through 4-grams, starting up to five letters behind the anchored rule span and ending up to five letters past the anchored rule span. These features can model most hand-coded morphological rules, but are in many cases more numerous than necessary. However, we"
N13-1138,D12-1127,0,0.061659,"Missing"
N13-1138,W09-4615,0,0.0411817,"Missing"
N13-1138,N09-1024,0,0.0971816,"up, our work most closely resembles that of Dreyer and Eisner (2011), but they focus on incorporating large amounts of raw text data rather than using large training sets effectively. Broadly similar techniques are also employed in systems to filter candidate rules and aid in human annotation of paradigms (Zajac, 2001; Forsberg et al., 2006; D´etrez and Ranta, 2012) for resources such as Grammatical Framework (Ranta, 2011). Related Work Much of the past work on morphology has focused on concatenative morphology using unsupervised methods (Goldsmith, 2001; Creutz and Lagus, 2007; Monson, 2008; Poon et al., 2009; Goldwater et al., 2009) or weak forms of supervision (Snyder and Barzilay, 2008). These methods can handle aspects of derivational morphology that we cannot, such as compounding, but we can handle a much larger subset of inflectional morphology, including more complex prefix and suffix rules, stem changes, and irregular forms. Some unsupervised work has specifically targeted these sorts of phenomena by, for example, learning spelling rules for mildly nonconcatenative cases (Dasgupta and Ng, 2007; Naradowsky and Goldwater, 2009) or mining lemma-base form pairs from a corpus (Schone and Jurafs"
N13-1138,N01-1024,0,0.0323965,"Missing"
N13-1138,C10-1110,0,0.0312875,"Missing"
N13-1138,P08-1084,0,0.183299,"they focus on incorporating large amounts of raw text data rather than using large training sets effectively. Broadly similar techniques are also employed in systems to filter candidate rules and aid in human annotation of paradigms (Zajac, 2001; Forsberg et al., 2006; D´etrez and Ranta, 2012) for resources such as Grammatical Framework (Ranta, 2011). Related Work Much of the past work on morphology has focused on concatenative morphology using unsupervised methods (Goldsmith, 2001; Creutz and Lagus, 2007; Monson, 2008; Poon et al., 2009; Goldwater et al., 2009) or weak forms of supervision (Snyder and Barzilay, 2008). These methods can handle aspects of derivational morphology that we cannot, such as compounding, but we can handle a much larger subset of inflectional morphology, including more complex prefix and suffix rules, stem changes, and irregular forms. Some unsupervised work has specifically targeted these sorts of phenomena by, for example, learning spelling rules for mildly nonconcatenative cases (Dasgupta and Ng, 2007; Naradowsky and Goldwater, 2009) or mining lemma-base form pairs from a corpus (Schone and Jurafsky, 2001), but it is extremely difficult to make unsupervised methods perform as w"
N13-1138,P09-1055,0,0.060924,"ly beyond the capacity of a model based purely on orthography. Words ending in -e are commonly feminine, and none of our other training examples end in -we, so guessing that L¨owe follows a common feminine inflection pattern is reasonable (though L¨owe is, in fact, masculine). Disambiguating this case requires either features on observed genders, a more complex model of the German language, or observing the word in a large corpus. Generally, when the model fails, as in this case, it is because of a fundamental linguistic information source that it does not have access to. 7 2008) or decoding (Toutanova and Cherry, 2009) steps similar to those of our model, but none attempt to jointly predict a complete inflection table based on automatically extracted rules. Some previous work has addressed the joint analysis (Zajac, 2001; Monson, 2008) or prediction (Lind´en and Tuovila, 2009; Dinu et al., 2012) of whole inflection tables, as we do, but rarely are both aspects addressed simultaneously and most approaches are tuned to one particular language or use language-specific, curated resources. In overall setup, our work most closely resembles that of Dreyer and Eisner (2011), but they focus on incorporating large am"
N13-1138,W04-0109,0,0.0681234,"Missing"
N13-1138,P00-1027,0,0.707675,"Missing"
N13-1138,W01-0711,0,0.0107563,"rn is reasonable (though L¨owe is, in fact, masculine). Disambiguating this case requires either features on observed genders, a more complex model of the German language, or observing the word in a large corpus. Generally, when the model fails, as in this case, it is because of a fundamental linguistic information source that it does not have access to. 7 2008) or decoding (Toutanova and Cherry, 2009) steps similar to those of our model, but none attempt to jointly predict a complete inflection table based on automatically extracted rules. Some previous work has addressed the joint analysis (Zajac, 2001; Monson, 2008) or prediction (Lind´en and Tuovila, 2009; Dinu et al., 2012) of whole inflection tables, as we do, but rarely are both aspects addressed simultaneously and most approaches are tuned to one particular language or use language-specific, curated resources. In overall setup, our work most closely resembles that of Dreyer and Eisner (2011), but they focus on incorporating large amounts of raw text data rather than using large training sets effectively. Broadly similar techniques are also employed in systems to filter candidate rules and aid in human annotation of paradigms (Zajac, 2"
N13-1138,zesch-etal-2008-extracting,0,0.0155438,"Missing"
N15-1029,N01-1016,0,0.134289,"nson and Charniak, 2004), causing researchers to employ other approaches such as pipelines of sequence models (Qian and Liu, 2013) or incremental syntactic systems (Honnibal and Johnson, 2014). Second, human processing of spoken language is complex and mixes acoustic and syntactic indicators (Cutler et al., 1997), so an automatic system must employ features targeting all levels of the perceptual stack to achieve high performance. In spite of this, the primary thread of work in the NLP community has focused on identifying disfluencies based only on lexicosyntactic cues (Heeman and Allen, 1994; Charniak and Johnson, 2001; Snover et al., 2004; Rasooli and Tetreault, 2013). A separate line of work has therefore attempted to build systems that leverage prosody as well as lexical information (Shriberg et al., 1997; Liu et al., 2003; Kim et al., 2004; Liu et al., 2006), though often with mixed success. In this work, we present a model for disfluency detection that improves upon model structures used in past work and leverages additional prosodic information. Our model is a semi-Markov conditional random field that distinguishes disfluent chunks (to be deleted) from fluent chunks (everything else), as shown in Figu"
N15-1029,N10-1112,0,0.00949571,"input sentence. In our model we constrain the transitions so that fluent spans can only be followed by disfluent spans. For this task, the spans we are predicting correspond directly to the reparanda of disfluencies, since these are the parts of the input sentences that should be removed. Note that our feature function can jointly inspect both the beginning and ending of the disfluency; we will describe the features of this form more specifically in Section 3.2.2. To train our model, we maximize conditional log likelihood of the training data augmented with a loss function via softmax-margin (Gimpel and Smith, 2010). Specifically, during training, we maxPd 0 imize L(θ) = i=1 log pθ (¯ s|x), where p0θ (¯ s|x) = ∗ pθ (¯ s|x) exp (`(¯ s, s¯ )). We take the loss function Surrounding POS: (VB, WRB) Unigrams: determine, how, you Bigrams: (determine, how), (how, you) POS Unigrams: VB, WRB, PRP POS Bigrams: (VB, WRB), (WRB, PRP) O O B I E O O Beginning POS: (VB, WRB) O Fluent Disfluent Ending POS: (VBP, WRB) Fluent to determine how you address how you weigh… to determine how you address how you weigh… TO VB WRB PRP VBP WRB PRP VBP WRB PRP VBP Word duplicate length: 2 POS duplicate length: 3 Figure 3: Span featur"
N15-1029,P94-1041,0,0.0376611,"allel substructures (Johnson and Charniak, 2004), causing researchers to employ other approaches such as pipelines of sequence models (Qian and Liu, 2013) or incremental syntactic systems (Honnibal and Johnson, 2014). Second, human processing of spoken language is complex and mixes acoustic and syntactic indicators (Cutler et al., 1997), so an automatic system must employ features targeting all levels of the perceptual stack to achieve high performance. In spite of this, the primary thread of work in the NLP community has focused on identifying disfluencies based only on lexicosyntactic cues (Heeman and Allen, 1994; Charniak and Johnson, 2001; Snover et al., 2004; Rasooli and Tetreault, 2013). A separate line of work has therefore attempted to build systems that leverage prosody as well as lexical information (Shriberg et al., 1997; Liu et al., 2003; Kim et al., 2004; Liu et al., 2006), though often with mixed success. In this work, we present a model for disfluency detection that improves upon model structures used in past work and leverages additional prosodic information. Our model is a semi-Markov conditional random field that distinguishes disfluent chunks (to be deleted) from fluent chunks (everyt"
N15-1029,Q14-1011,0,0.136149,"anguage in that it contains frequent disfluencies, or parts of an utterance that are corrected by the speaker. Removing these disfluencies is desirable in order to clean the input for use in downstream NLP tasks. However, automatically identifying disfluencies is challenging for a number of reasons. First, disfluencies are a syntactic phenomenon, but defy standard context-free parsing models due to their parallel substructures (Johnson and Charniak, 2004), causing researchers to employ other approaches such as pipelines of sequence models (Qian and Liu, 2013) or incremental syntactic systems (Honnibal and Johnson, 2014). Second, human processing of spoken language is complex and mixes acoustic and syntactic indicators (Cutler et al., 1997), so an automatic system must employ features targeting all levels of the perceptual stack to achieve high performance. In spite of this, the primary thread of work in the NLP community has focused on identifying disfluencies based only on lexicosyntactic cues (Heeman and Allen, 1994; Charniak and Johnson, 2001; Snover et al., 2004; Rasooli and Tetreault, 2013). A separate line of work has therefore attempted to build systems that leverage prosody as well as lexical informa"
N15-1029,N10-1005,0,0.145169,"stics work also enables novel prosodic features that compute pauses and word duration based on alignments to the speech signal itself, allowing the model to capture acoustic cues like pauses and hesitations that have proven useful for disfluency detection in earlier work (Shriberg et al., 1997). Such information has been exploited by NLP systems in the past via ToBI break indices (Silverman et al., 1992), a mid-level prosodic abstraction that might be indicative of disfluencies. These have been incorporated into syntactic parsers with some success (Kahn et al., 2005; Dreyer and Shafran, 2007; Huang and Harper, 2010), but we find that using features on predicted breaks is ineffective compared to directly using acoustic indicators. Our implementation of a baseline CRF model already achieves results comparable to those of a highperformance system based on pipelined inference (Qian and Liu, 2013). Our semi-CRF with span features improves on this, and adding prosodic indicators gives additional gains. Our final system gets an F-score of 85.4, which is 1.3 F1 better than the best prior reported F-score on this dataset (Honnibal and Johnson, 2014). 2 Experimental Setup Throughout this work, we make use of the S"
N15-1029,P04-1005,0,0.0972961,"signal and predicts disfluencies as complete chunks using a semi-Markov conditional random field. Introduction Spoken language is fundamentally different from written language in that it contains frequent disfluencies, or parts of an utterance that are corrected by the speaker. Removing these disfluencies is desirable in order to clean the input for use in downstream NLP tasks. However, automatically identifying disfluencies is challenging for a number of reasons. First, disfluencies are a syntactic phenomenon, but defy standard context-free parsing models due to their parallel substructures (Johnson and Charniak, 2004), causing researchers to employ other approaches such as pipelines of sequence models (Qian and Liu, 2013) or incremental syntactic systems (Honnibal and Johnson, 2014). Second, human processing of spoken language is complex and mixes acoustic and syntactic indicators (Cutler et al., 1997), so an automatic system must employ features targeting all levels of the perceptual stack to achieve high performance. In spite of this, the primary thread of work in the NLP community has focused on identifying disfluencies based only on lexicosyntactic cues (Heeman and Allen, 1994; Charniak and Johnson, 20"
N15-1029,H05-1030,0,0.314468,"15. 2015 Association for Computational Linguistics work also enables novel prosodic features that compute pauses and word duration based on alignments to the speech signal itself, allowing the model to capture acoustic cues like pauses and hesitations that have proven useful for disfluency detection in earlier work (Shriberg et al., 1997). Such information has been exploited by NLP systems in the past via ToBI break indices (Silverman et al., 1992), a mid-level prosodic abstraction that might be indicative of disfluencies. These have been incorporated into syntactic parsers with some success (Kahn et al., 2005; Dreyer and Shafran, 2007; Huang and Harper, 2010), but we find that using features on predicted breaks is ineffective compared to directly using acoustic indicators. Our implementation of a baseline CRF model already achieves results comparable to those of a highperformance system based on pipelined inference (Qian and Liu, 2013). Our semi-CRF with span features improves on this, and adding prosodic indicators gives additional gains. Our final system gets an F-score of 85.4, which is 1.3 F1 better than the best prior reported F-score on this dataset (Honnibal and Johnson, 2014). 2 Experiment"
N15-1029,N04-1018,0,0.0409339,"s complex and mixes acoustic and syntactic indicators (Cutler et al., 1997), so an automatic system must employ features targeting all levels of the perceptual stack to achieve high performance. In spite of this, the primary thread of work in the NLP community has focused on identifying disfluencies based only on lexicosyntactic cues (Heeman and Allen, 1994; Charniak and Johnson, 2001; Snover et al., 2004; Rasooli and Tetreault, 2013). A separate line of work has therefore attempted to build systems that leverage prosody as well as lexical information (Shriberg et al., 1997; Liu et al., 2003; Kim et al., 2004; Liu et al., 2006), though often with mixed success. In this work, we present a model for disfluency detection that improves upon model structures used in past work and leverages additional prosodic information. Our model is a semi-Markov conditional random field that distinguishes disfluent chunks (to be deleted) from fluent chunks (everything else), as shown in Figure 1. By making chunk-level predictions, we can incorporate not only standard tokenlevel features but also features that can consider the entire reparandum and the start of the repair, enabling our model to easily capture paralle"
N15-1029,P06-1055,1,0.144346,"an F-score of 85.4, which is 1.3 F1 better than the best prior reported F-score on this dataset (Honnibal and Johnson, 2014). 2 Experimental Setup Throughout this work, we make use of the Switchboard corpus using the train/test splits specified by Johnson and Charniak (2004) and used in other work. We use the provided transcripts and gold alignments between the text and the speech signal. We follow the same preprocessing regimen as past work: we remove partial words, punctuation, and capitalization to make the input more realistic.2 Finally, we use predicted POS tags from the Berkeley parser (Petrov et al., 2006) trained on Switchboard. 3 Model Past work on disfluency detection has employed CRFs to predict disfluencies using a IOBES tag set (Qian and Liu, 2013). An example of this is shown in Figure 2. One major shortcoming of this model is that beginning and ending of a disfluency are not decided jointly: because features in the CRF are local 2 As described in Honnibal and Johnson (2014), we computed features over sentences with filler words (um and uh) and the phrases I mean and you know removed. 258 to emissions and transitions, features in this model cannot recognize that a proposed disfluency beg"
N15-1029,N13-1102,0,0.517586,"ken language is fundamentally different from written language in that it contains frequent disfluencies, or parts of an utterance that are corrected by the speaker. Removing these disfluencies is desirable in order to clean the input for use in downstream NLP tasks. However, automatically identifying disfluencies is challenging for a number of reasons. First, disfluencies are a syntactic phenomenon, but defy standard context-free parsing models due to their parallel substructures (Johnson and Charniak, 2004), causing researchers to employ other approaches such as pipelines of sequence models (Qian and Liu, 2013) or incremental syntactic systems (Honnibal and Johnson, 2014). Second, human processing of spoken language is complex and mixes acoustic and syntactic indicators (Cutler et al., 1997), so an automatic system must employ features targeting all levels of the perceptual stack to achieve high performance. In spite of this, the primary thread of work in the NLP community has focused on identifying disfluencies based only on lexicosyntactic cues (Heeman and Allen, 1994; Charniak and Johnson, 2001; Snover et al., 2004; Rasooli and Tetreault, 2013). A separate line of work has therefore attempted to"
N15-1029,D13-1013,0,0.159911,"employ other approaches such as pipelines of sequence models (Qian and Liu, 2013) or incremental syntactic systems (Honnibal and Johnson, 2014). Second, human processing of spoken language is complex and mixes acoustic and syntactic indicators (Cutler et al., 1997), so an automatic system must employ features targeting all levels of the perceptual stack to achieve high performance. In spite of this, the primary thread of work in the NLP community has focused on identifying disfluencies based only on lexicosyntactic cues (Heeman and Allen, 1994; Charniak and Johnson, 2001; Snover et al., 2004; Rasooli and Tetreault, 2013). A separate line of work has therefore attempted to build systems that leverage prosody as well as lexical information (Shriberg et al., 1997; Liu et al., 2003; Kim et al., 2004; Liu et al., 2006), though often with mixed success. In this work, we present a model for disfluency detection that improves upon model structures used in past work and leverages additional prosodic information. Our model is a semi-Markov conditional random field that distinguishes disfluent chunks (to be deleted) from fluent chunks (everything else), as shown in Figure 1. By making chunk-level predictions, we can inc"
N15-1029,N04-4040,0,0.220286,"using researchers to employ other approaches such as pipelines of sequence models (Qian and Liu, 2013) or incremental syntactic systems (Honnibal and Johnson, 2014). Second, human processing of spoken language is complex and mixes acoustic and syntactic indicators (Cutler et al., 1997), so an automatic system must employ features targeting all levels of the perceptual stack to achieve high performance. In spite of this, the primary thread of work in the NLP community has focused on identifying disfluencies based only on lexicosyntactic cues (Heeman and Allen, 1994; Charniak and Johnson, 2001; Snover et al., 2004; Rasooli and Tetreault, 2013). A separate line of work has therefore attempted to build systems that leverage prosody as well as lexical information (Shriberg et al., 1997; Liu et al., 2003; Kim et al., 2004; Liu et al., 2006), though often with mixed success. In this work, we present a model for disfluency detection that improves upon model structures used in past work and leverages additional prosodic information. Our model is a semi-Markov conditional random field that distinguishes disfluent chunks (to be deleted) from fluent chunks (everything else), as shown in Figure 1. By making chunk"
N16-1150,D13-1184,0,0.229046,"rforming the prior systems of Durrett and Klein (2014) and Nguyen et al. (2014).1 1 Introduction One of the major challenges of entity linking is resolving contextually polysemous mentions. For example, Germany may refer to a nation, to that nation’s government, or even to a soccer team. Past approaches to such cases have often focused on collective entity linking: nearby mentions in a document might be expected to link to topically-similar entities, which can give us clues about the identity of the mention currently being resolved (Ratinov et al., 2011; Hoffart et al., 2011; He et al., 2013; Cheng and Roth, 2013; Durrett and Klein, 2014). But an even simpler approach is to use context information from just the words in the source document itself to make sure the entity is being resolved sensibly in context. In past work, these approaches have typically relied on heuristics such as tf-idf (Ratinov et 1 Source available at github.com/matthewfl/nlp-entity-convnet al., 2011), but such heuristics are hard to calibrate and they capture structure in a coarser way than learning-based methods. In this work, we model semantic similarity between a mention’s source document context and its potential entity targe"
N16-1150,D07-1074,0,0.615559,"nd fed into a final logistic regression layer (maintaining end-to-end inference and learning of the filters). When trained with backpropagation, the convolutional networks should learn to map text into vector spaces that are informative about whether the document and entity are related or not. 2.2 Integrating with a Sparse Model The dense model presented in Section 2.1 is effective at capturing semantic topic similarity, but it is most effective when combined with other signals for entity linking. An important cue for resolving a mention is the use of link counts from hyperlinks in Wikipedia (Cucerzan, 2007; Milne and Witten, 2008; Ji and Grishman, 2011), which tell us how often a given mention was linked to each article on Wikipedia. This information can serve as a useful prior, but only if we can leverage it effectively by targeting the most salient part of a mention. For example, we may have never observed President Barack Obama as a linked string on Wikipedia, even though we have seen the substring Barack Obama and it unambiguously indicates the correct answer. Following Durrett and Klein (2014), we introduce a latent variable q to capture which subset of a mention (known as a query) we reso"
N16-1150,P15-1026,0,0.0377226,"df (Ratinov et 1 Source available at github.com/matthewfl/nlp-entity-convnet al., 2011), but such heuristics are hard to calibrate and they capture structure in a coarser way than learning-based methods. In this work, we model semantic similarity between a mention’s source document context and its potential entity targets using convolutional neural networks (CNNs). CNNs have been shown to be effective for sentence classification tasks (Kalchbrenner et al., 2014; Kim, 2014; Iyyer et al., 2015) and for capturing similarity in models for entity linking (Sun et al., 2015) and other related tasks (Dong et al., 2015; Shen et al., 2014), so we expect them to be effective at isolating the relevant topic semantics for entity linking. We show that convolutions over multiple granularities of the input document are useful for providing different notions of semantic context. Finally, we show how to integrate these networks with a preexisting entity linking system (Durrett and Klein, 2014). Through a combination of these two distinct methods into a single system that leverages their complementary strengths, we achieve state-ofthe-art performance across several datasets. 2 Model Our model focuses on two core idea"
N16-1150,Q14-1037,1,0.701957,"that might refer to different entities in different contexts. We present a model that uses convolutional neural networks to capture semantic correspondence between a mention’s context and a proposed target entity. These convolutional networks operate at multiple granularities to exploit various kinds of topic information, and their rich parameterization gives them the capacity to learn which n-grams characterize different topics. We combine these networks with a sparse linear model to achieve state-of-the-art performance on multiple entity linking datasets, outperforming the prior systems of Durrett and Klein (2014) and Nguyen et al. (2014).1 1 Introduction One of the major challenges of entity linking is resolving contextually polysemous mentions. For example, Germany may refer to a nation, to that nation’s government, or even to a soccer team. Past approaches to such cases have often focused on collective entity linking: nearby mentions in a document might be expected to link to topically-similar entities, which can give us clues about the identity of the mention currently being resolved (Ratinov et al., 2011; Hoffart et al., 2011; He et al., 2013; Cheng and Roth, 2013; Durrett and Klein, 2014). But an"
N16-1150,E14-1052,0,0.0588406,"mention, as well as standard lexical, POS, and named entity type features. fE mostly captures how likely the selected query is to correspond to a given entity based on factors like anchor text counts from Wikipedia, string match with proposed Wikipedia titles, and discretized cosine similarities of tf-idf vectors (Ratinov et al., 2011). Adding tf-idf indicators is the only modification we made to the features of Durrett and Klein (2014). 3 Experimental Results We performed experiments on 4 different entity linking datasets. • ACE (NIST, 2005; Bentivogli et al., 2010): This corpus was used in Fahrni and Strube (2014) and Durrett and Klein (2014). • CoNLL-YAGO (Hoffart et al., 2011): This corpus is based on the CoNLL 2003 dataset; the test set consists of 231 news articles and contains a number of rarer entities. • WP (Heath and Bizer, 2011): This dataset consists of short snippets from Wikipedia. • Wikipedia (Ratinov et al., 2011): This dataset consists of 10,000 randomly sampled Wikipedia articles, with the task being to resolve the links in each article.3 3 We do not compare to Ratinov et al. (2011) on this dataset because we do not have access to the original Wikipedia dump they used for their work and"
N16-1150,D13-1041,0,0.0395894,"g datasets, outperforming the prior systems of Durrett and Klein (2014) and Nguyen et al. (2014).1 1 Introduction One of the major challenges of entity linking is resolving contextually polysemous mentions. For example, Germany may refer to a nation, to that nation’s government, or even to a soccer team. Past approaches to such cases have often focused on collective entity linking: nearby mentions in a document might be expected to link to topically-similar entities, which can give us clues about the identity of the mention currently being resolved (Ratinov et al., 2011; Hoffart et al., 2011; He et al., 2013; Cheng and Roth, 2013; Durrett and Klein, 2014). But an even simpler approach is to use context information from just the words in the source document itself to make sure the entity is being resolved sensibly in context. In past work, these approaches have typically relied on heuristics such as tf-idf (Ratinov et 1 Source available at github.com/matthewfl/nlp-entity-convnet al., 2011), but such heuristics are hard to calibrate and they capture structure in a coarser way than learning-based methods. In this work, we model semantic similarity between a mention’s source document context and its"
N16-1150,D11-1072,0,0.786132,"Missing"
N16-1150,P15-1162,0,0.0617261,"Missing"
N16-1150,P11-1115,0,0.0504631,"layer (maintaining end-to-end inference and learning of the filters). When trained with backpropagation, the convolutional networks should learn to map text into vector spaces that are informative about whether the document and entity are related or not. 2.2 Integrating with a Sparse Model The dense model presented in Section 2.1 is effective at capturing semantic topic similarity, but it is most effective when combined with other signals for entity linking. An important cue for resolving a mention is the use of link counts from hyperlinks in Wikipedia (Cucerzan, 2007; Milne and Witten, 2008; Ji and Grishman, 2011), which tell us how often a given mention was linked to each article on Wikipedia. This information can serve as a useful prior, but only if we can leverage it effectively by targeting the most salient part of a mention. For example, we may have never observed President Barack Obama as a linked string on Wikipedia, even though we have seen the substring Barack Obama and it unambiguously indicates the correct answer. Following Durrett and Klein (2014), we introduce a latent variable q to capture which subset of a mention (known as a query) we resolve. Query generation includes potentially remov"
N16-1150,P14-1062,0,0.0334107,"e document itself to make sure the entity is being resolved sensibly in context. In past work, these approaches have typically relied on heuristics such as tf-idf (Ratinov et 1 Source available at github.com/matthewfl/nlp-entity-convnet al., 2011), but such heuristics are hard to calibrate and they capture structure in a coarser way than learning-based methods. In this work, we model semantic similarity between a mention’s source document context and its potential entity targets using convolutional neural networks (CNNs). CNNs have been shown to be effective for sentence classification tasks (Kalchbrenner et al., 2014; Kim, 2014; Iyyer et al., 2015) and for capturing similarity in models for entity linking (Sun et al., 2015) and other related tasks (Dong et al., 2015; Shen et al., 2014), so we expect them to be effective at isolating the relevant topic semantics for entity linking. We show that convolutions over multiple granularities of the input document are useful for providing different notions of semantic context. Finally, we show how to integrate these networks with a preexisting entity linking system (Durrett and Klein, 2014). Through a combination of these two distinct methods into a single system"
N16-1150,D14-1181,0,0.00426049,"ure the entity is being resolved sensibly in context. In past work, these approaches have typically relied on heuristics such as tf-idf (Ratinov et 1 Source available at github.com/matthewfl/nlp-entity-convnet al., 2011), but such heuristics are hard to calibrate and they capture structure in a coarser way than learning-based methods. In this work, we model semantic similarity between a mention’s source document context and its potential entity targets using convolutional neural networks (CNNs). CNNs have been shown to be effective for sentence classification tasks (Kalchbrenner et al., 2014; Kim, 2014; Iyyer et al., 2015) and for capturing similarity in models for entity linking (Sun et al., 2015) and other related tasks (Dong et al., 2015; Shen et al., 2014), so we expect them to be effective at isolating the relevant topic semantics for entity linking. We show that convolutions over multiple granularities of the input document are useful for providing different notions of semantic context. Finally, we show how to integrate these networks with a preexisting entity linking system (Durrett and Klein, 2014). Through a combination of these two distinct methods into a single system that levera"
N16-1150,P14-2050,0,0.0529462,"ectors derived from Google News vs. Wikipedia on development sets for each corpus. 3.2 Embedding Vectors We also explored two different sources of embedding vectors for the convolutions. Table 4 shows that word vectors trained on Wikipedia outperformed Google News word vectors trained on a larger corpus. Further investigation revealed that the Google News vectors had much higher out-of-vocabulary rates. For learning the vectors, we use the standard word2vec toolkit (Mikolov et al., 2013) with vector length set to 300, window set to 21 (larger windows produce more semantically-focused vectors (Levy and Goldberg, 2014)), 10 negative samples and 10 iterations through Wikipedia. We do not fine-tune word vectors during training of our model, as that was not found to improve performance. 3.3 Analysis of Learned Convolutions One downside of our system compared to its purely indicator-based variant is that its operation is less interpretable. However, one way we can inspect the learned system is by examining what causes high activations of the various convolutional filters (rows of the matrices Mg from Equation 1). Table 3 shows the n-grams in the ACE dataset leading to maximal activations of three of the filters"
N16-1150,P11-1138,0,0.931799,"he-art performance on multiple entity linking datasets, outperforming the prior systems of Durrett and Klein (2014) and Nguyen et al. (2014).1 1 Introduction One of the major challenges of entity linking is resolving contextually polysemous mentions. For example, Germany may refer to a nation, to that nation’s government, or even to a soccer team. Past approaches to such cases have often focused on collective entity linking: nearby mentions in a document might be expected to link to topically-similar entities, which can give us clues about the identity of the mention currently being resolved (Ratinov et al., 2011; Hoffart et al., 2011; He et al., 2013; Cheng and Roth, 2013; Durrett and Klein, 2014). But an even simpler approach is to use context information from just the words in the source document itself to make sure the entity is being resolved sensibly in context. In past work, these approaches have typically relied on heuristics such as tf-idf (Ratinov et 1 Source available at github.com/matthewfl/nlp-entity-convnet al., 2011), but such heuristics are hard to calibrate and they capture structure in a coarser way than learning-based methods. In this work, we model semantic similarity between a men"
N18-2049,K16-1002,0,0.0253448,"ectional preference. candy is selectionally preferred because it is distributionally common patient in the event man-swallow-*, as opposed to the bizarre and rarely seen (if at all) patient paintball. However both are semantically plausible according to our world knowledge: they are small-sized objects that are swallowable by a man. desk is both distributionally unlikely and implausible (i.e. oversized for swallowing). Semantic plausibility is pertinent and crucial in a multitude of interesting NLP tasks put forth previously, such as narrative schema (Chambers, 2013), narrative interpolation (Bowman et al., 2016), story understanding (Mostafazadeh et al., 2016), and paragraph reconstruction (Li and Jurafsky, 2017). Existing methods for these tasks, however, draw predominantly (if not only) on distributional data and produce rather weak performance. Semantic plausibility over subject-verbobject triples, while simpler than these other tasks, is a key building block that requires many of the same signals and encapsulates complex world knowledge in a binary prediction problem. Introduction Intuitively, a man can swallow a candy or paintball but not a desk. Equally so, one cannot plausibly eat a cake and t"
N18-2049,D13-1185,0,0.0206428,"tinguishing semantic plausibility from selectional preference. candy is selectionally preferred because it is distributionally common patient in the event man-swallow-*, as opposed to the bizarre and rarely seen (if at all) patient paintball. However both are semantically plausible according to our world knowledge: they are small-sized objects that are swallowable by a man. desk is both distributionally unlikely and implausible (i.e. oversized for swallowing). Semantic plausibility is pertinent and crucial in a multitude of interesting NLP tasks put forth previously, such as narrative schema (Chambers, 2013), narrative interpolation (Bowman et al., 2016), story understanding (Mostafazadeh et al., 2016), and paragraph reconstruction (Li and Jurafsky, 2017). Existing methods for these tasks, however, draw predominantly (if not only) on distributional data and produce rather weak performance. Semantic plausibility over subject-verbobject triples, while simpler than these other tasks, is a key building block that requires many of the same signals and encapsulates complex world knowledge in a binary prediction problem. Introduction Intuitively, a man can swallow a candy or paintball but not a desk. Eq"
N18-2049,N01-1013,0,0.274003,"Missing"
N18-2049,P10-2017,1,0.860967,"Missing"
N18-2049,P17-1025,0,0.408245,"y, semantic plausibility is sensitive to certain properties such as relative object size that are not explicitly encoded by selectional preferences (Bagherinezhad et al., 2016). Therefore, it is crucial that we learn to model these dimensions in addition to using classical distributional signals. In this work, we show that world knowledge injection is necessary and effective for the semantic plausibility task, for which we create a robust, high-agreement dataset (details in section 3). Employing methods inspired by the recent work on world knowledge propagation through distributional context (Forbes and Choi, 2017; Wang et al., 2017), we accomplish the goal with minimal effort in manual annotation. Finally, we perform an indepth error analysis to point to future directions of work on semantic plausibility. 303 Proceedings of NAACL-HLT 2018, pages 303–308 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2 Related Work (a) Have Turkers write down plausible or implausible S - V and V- O selections; (b) Randomly generate S - V- O triples from collected S - V and V- O pairs; (c) Send resulting S - V- O triples to Turkers to filter for ones with high agreement (by ma"
N18-2049,D17-1019,0,0.0336672,"n the event man-swallow-*, as opposed to the bizarre and rarely seen (if at all) patient paintball. However both are semantically plausible according to our world knowledge: they are small-sized objects that are swallowable by a man. desk is both distributionally unlikely and implausible (i.e. oversized for swallowing). Semantic plausibility is pertinent and crucial in a multitude of interesting NLP tasks put forth previously, such as narrative schema (Chambers, 2013), narrative interpolation (Bowman et al., 2016), story understanding (Mostafazadeh et al., 2016), and paragraph reconstruction (Li and Jurafsky, 2017). Existing methods for these tasks, however, draw predominantly (if not only) on distributional data and produce rather weak performance. Semantic plausibility over subject-verbobject triples, while simpler than these other tasks, is a key building block that requires many of the same signals and encapsulates complex world knowledge in a binary prediction problem. Introduction Intuitively, a man can swallow a candy or paintball but not a desk. Equally so, one cannot plausibly eat a cake and then hold it. What kinds of semantic knowledge are necessary for distinguishing a physically plausible e"
N18-2049,N16-1098,0,0.0736901,"preferred because it is distributionally common patient in the event man-swallow-*, as opposed to the bizarre and rarely seen (if at all) patient paintball. However both are semantically plausible according to our world knowledge: they are small-sized objects that are swallowable by a man. desk is both distributionally unlikely and implausible (i.e. oversized for swallowing). Semantic plausibility is pertinent and crucial in a multitude of interesting NLP tasks put forth previously, such as narrative schema (Chambers, 2013), narrative interpolation (Bowman et al., 2016), story understanding (Mostafazadeh et al., 2016), and paragraph reconstruction (Li and Jurafsky, 2017). Existing methods for these tasks, however, draw predominantly (if not only) on distributional data and produce rather weak performance. Semantic plausibility over subject-verbobject triples, while simpler than these other tasks, is a key building block that requires many of the same signals and encapsulates complex world knowledge in a binary prediction problem. Introduction Intuitively, a man can swallow a candy or paintball but not a desk. Equally so, one cannot plausibly eat a cake and then hold it. What kinds of semantic knowledge are"
N18-2049,P10-1045,0,0.22193,"Missing"
N18-2049,D16-1017,0,0.106709,"Missing"
N18-2049,D14-1004,0,0.273445,"Missing"
N18-2049,I17-1021,1,0.800029,"y is sensitive to certain properties such as relative object size that are not explicitly encoded by selectional preferences (Bagherinezhad et al., 2016). Therefore, it is crucial that we learn to model these dimensions in addition to using classical distributional signals. In this work, we show that world knowledge injection is necessary and effective for the semantic plausibility task, for which we create a robust, high-agreement dataset (details in section 3). Employing methods inspired by the recent work on world knowledge propagation through distributional context (Forbes and Choi, 2017; Wang et al., 2017), we accomplish the goal with minimal effort in manual annotation. Finally, we perform an indepth error analysis to point to future directions of work on semantic plausibility. 303 Proceedings of NAACL-HLT 2018, pages 303–308 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2 Related Work (a) Have Turkers write down plausible or implausible S - V and V- O selections; (b) Randomly generate S - V- O triples from collected S - V and V- O pairs; (c) Send resulting S - V- O triples to Turkers to filter for ones with high agreement (by majority vote). (a) en"
N18-2049,Q17-1027,0,0.0794419,"Missing"
N19-1250,D15-1162,0,0.0137203,"entation: mword = Attention(bi-LSTM(m0 )). Second, a character-level representation is computed for the mention. Each character is embedded and then a 1-D convolution (Collobert et al., 2011) is applied over the characters of the mention. This gives a character vector mchar . Finally, we take the contextualized word vector of the headword mhead as a third component of our representation. This can be seen as a residual connection (He et al., 2016) specific to the mention head word. We find the headwords in the mention spans by parsing those spans in isolation using the spaCy dependency parser (Honnibal and Johnson, 2015). Empirically, we found this to be useful on long spans, when the span attention would often focus on incorrect tokens. The final representation of the input x is a concatenation of the sentence, the word- & characterlevel mention, and mention headword repre the  word char head sentations, v = s; m ;m ;m ∈ RdΦ . Decoder We treat each label prediction as an independent binary classification problem. Thus, we compute a score for each type in the type vocabulary V t . Similar to the decoder of the relabeling function g, we compute e = σ (Ev), where E ∈ t t R|V |×dΦ and e ∈ R|V |. For the final"
N19-1250,D15-1103,0,0.155419,"performance. Adding distant data that has been denoised with our learned models gives further performance gains over this base model, outperforming models trained on raw distant data or heuristically-denoised distant data. 1 Introduction With the rise of data-hungry neural network models, system designers have turned increasingly to unlabeled and weakly-labeled data in order to scale up model training. For information extraction tasks such as relation extraction and entity typing, distant supervision (Mintz et al., 2009) is a powerful approach for adding more data, using a knowledge base (Del Corro et al., 2015; Rabinovich and Klein, 2017) or heuristics (Ratner et al., 2016; Hancock et al., 2018) to automatically label instances. One can treat this data just like any other supervised data, but it is noisy; more effective approaches employ specialized probabilistic models (Riedel et al., 2010; Ratner et al., 2018a), capturing its interaction with other supervision (Wang and Poon, 2018) or breaking down aspects of a task on which it is reliable (Ratner et al., 2018b). However, these approaches often require sophisticated probabilistic inference for training of the final model. Ideally, we want a techn"
N19-1250,N06-2015,0,0.056526,"enoising model is applied to the data, performance improves, and it improves more than heuristic denoising approaches tailored to this dataset. Our strongest denoising model gives a gain of 3 F1 absolute over the ELMo baseline, and a 4.4 F1 improvement over naive incorporation of distant data. This establishes a new state-ofthe-art on the test set, outperforming concurrently published work (Xiong et al., 2019) and matching the performance of a BERT model (Devlin et al., 2018) on this task. Finally, we show that denoising helps even when the label set is projected onto the OntoNotes label set (Hovy et al., 2006; Gillick et al., 2014), outperforming the method of Choi et al. (2018) in that setting as well. 2 Setup We consider the task of predicting a structured target y associated with an input x. Suppose we have high-quality labeled data tar of n (input, (1) (1) (n) (n) get) pairs D = { x , y , . . . , (x , y )}, and noisily labeled data of n0 (input, target) pairs 0 (1) (n0 ) D0 = {(x(1) , ynoisy ), . . . , (x(n ) , ynoisy )}. For our tasks, D is collected through manual annotation and D0 is collected by distant supervision. We use two models to denoise data from D0 : a filtering function f dispos"
N19-1250,P18-1175,0,0.0459494,"Missing"
N19-1250,D17-1018,0,0.0754912,"Missing"
N19-1250,C18-1036,0,0.030987,", currency, money, medium of exchange, dollar, monetary unit (c) ... Vittoria was influenced also by [Michelangelo] ... architect, sculptor, painter, poet person, artist, writer Figure 4: Examples of the noisy labels (left) and the denoised labels (right) for mentions (bold). The colors correspond to type classes: general (purple), finegrained (green), and ultra-fine (yellow). tic graphical modeling approaches have been used (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Takamatsu et al., 2012) as well as deep models (Lin et al., 2016; Feng et al., 2017; Luo et al., 2017; Lei et al., 2018; Han et al., 2018), though these often focus on incorporating signals from other sources as opposed to manually labeled data. 7 Conclusion In this work, we investigated the problem of denoising distant data for entity typing tasks. We trained a filtering function that discards examples from the distantly labeled data that are wholly unusable and a relabeling function that repairs noisy labels for the retained examples. When distant data is processed with our best denoising model, our final trained model achieves state-of-the-art performance on an ultra-fine entity typing task. Acknowledgments"
N19-1250,P16-1200,0,0.0220508,"re, multi-instance learning and probabilisdollar object, currency, money, medium of exchange, dollar, monetary unit (c) ... Vittoria was influenced also by [Michelangelo] ... architect, sculptor, painter, poet person, artist, writer Figure 4: Examples of the noisy labels (left) and the denoised labels (right) for mentions (bold). The colors correspond to type classes: general (purple), finegrained (green), and ultra-fine (yellow). tic graphical modeling approaches have been used (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Takamatsu et al., 2012) as well as deep models (Lin et al., 2016; Feng et al., 2017; Luo et al., 2017; Lei et al., 2018; Han et al., 2018), though these often focus on incorporating signals from other sources as opposed to manually labeled data. 7 Conclusion In this work, we investigated the problem of denoising distant data for entity typing tasks. We trained a filtering function that discards examples from the distantly labeled data that are wholly unusable and a relabeling function that repairs noisy labels for the retained examples. When distant data is processed with our best denoising model, our final trained model achieves state-of-the-art performan"
N19-1250,P17-1040,0,0.0147065,"bilisdollar object, currency, money, medium of exchange, dollar, monetary unit (c) ... Vittoria was influenced also by [Michelangelo] ... architect, sculptor, painter, poet person, artist, writer Figure 4: Examples of the noisy labels (left) and the denoised labels (right) for mentions (bold). The colors correspond to type classes: general (purple), finegrained (green), and ultra-fine (yellow). tic graphical modeling approaches have been used (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Takamatsu et al., 2012) as well as deep models (Lin et al., 2016; Feng et al., 2017; Luo et al., 2017; Lei et al., 2018; Han et al., 2018), though these often focus on incorporating signals from other sources as opposed to manually labeled data. 7 Conclusion In this work, we investigated the problem of denoising distant data for entity typing tasks. We trained a filtering function that discards examples from the distantly labeled data that are wholly unusable and a relabeling function that repairs noisy labels for the retained examples. When distant data is processed with our best denoising model, our final trained model achieves state-of-the-art performance on an ultra-fine entity typing tas"
N19-1250,P09-1113,0,0.113075,"f their model with pre-trained ELMo representations, which already achieves state-of-the-art performance. Adding distant data that has been denoised with our learned models gives further performance gains over this base model, outperforming models trained on raw distant data or heuristically-denoised distant data. 1 Introduction With the rise of data-hungry neural network models, system designers have turned increasingly to unlabeled and weakly-labeled data in order to scale up model training. For information extraction tasks such as relation extraction and entity typing, distant supervision (Mintz et al., 2009) is a powerful approach for adding more data, using a knowledge base (Del Corro et al., 2015; Rabinovich and Klein, 2017) or heuristics (Ratner et al., 2016; Hancock et al., 2018) to automatically label instances. One can treat this data just like any other supervised data, but it is noisy; more effective approaches employ specialized probabilistic models (Riedel et al., 2010; Ratner et al., 2018a), capturing its interaction with other supervision (Wang and Poon, 2018) or breaking down aspects of a task on which it is reliable (Ratner et al., 2018b). However, these approaches often require sop"
N19-1250,P18-1010,0,0.364385,"labeling function adds coarse types, {object, currency}, as well as specific types such as {medium of exchange, monetary unit}. In another EL example (c), the relabeling function tries to add coarse and fine types but struggles to assign multiple diverse ultra-fine types to the mention span Michelangelo, possibly because some of these types rarely cooccur (painter and poet). 6 location, place, city, country, area, region, township, town, municipality Related Work Past work on denoising data for entity typing has used multi-instance multi-label learning (Yaghoobzadeh and Sch¨utze, 2015, 2017; Murty et al., 2018). One view of these approaches is that they delete noisily-introduced labels, but they cannot add them, or filter bad examples. Other work focuses on learning type embeddings (Yogatama et al., 2015; Ren et al., 2016a,b); our approach goes beyond this in treating the label set in a structured way. The label set of Choi et al. (2018) is distinct in not being explicitly hierarchical, making past hierarchical approaches difficult to apply. Denoising techniques for distant supervision have been applied extensively to relation extraction. Here, multi-instance learning and probabilisdollar object, cu"
N19-1250,D14-1162,0,0.0849161,", the noisy types t1 , . . . , tm are embedded into type vectors {t1 , . . P . , tm }. The final embedding of the type set t = j tj . Type Definition Encoder Using trainable type embeddings exposes the denoising model to potential data sparsity issues, as some types appear only a few or zero times in the training data. Therefore, we also assign each type a vector based on its definition in WordNet (Miller, 1995). Even low-frequent types are therefore assigned a plausible embedding.1 Let wij denote the ith word of the jth type’s most common WordNet definition. Each wij is embedded using GloVe (Pennington et al., 2014). The resulting word embedding vectors wij are fed into a bi-LSTM (Hochreiter and Schmidhuber, 1997; Graves and Schmidhuber, 2005), and a concatenation of the last hidden states in both directions is used as the definition representation wj . The final representation of the definitions is the sum over these vectors for each type: w = 1 We found this technique to be more effective than using pretrained vectors from GloVe or ELMo. It gave small improvements on an intrinsic evaluation over not incorporating it; results are omitted due to space constraints. 2409 wk . Our final vt = [t; w], the con"
N19-1250,N18-1202,0,0.391806,"hampion, achiever, winner, contestant, person, athlete film, movie, show, art, entertainment, creation Figure 1: Examples selected from the Ultra-Fine Entity Typing dataset of Choi et al. (2018). (a) A manuallyannotated example. (b) The head word heuristic functioning correctly but missing types in (a). (c) Entity linking providing the wrong types. (d) Entity linking providing correct but incomplete types. tity typing scenario and use the same two distant supervision sources as them, based on entity linking and head words. On top of an adapted model from Choi et al. (2018) incorporating ELMo (Peters et al., 2018), na¨ıvely adding distant data actually hurts performance. However, when our learned denoising model is applied to the data, performance improves, and it improves more than heuristic denoising approaches tailored to this dataset. Our strongest denoising model gives a gain of 3 F1 absolute over the ELMo baseline, and a 4.4 F1 improvement over naive incorporation of distant data. This establishes a new state-ofthe-art on the test set, outperforming concurrently published work (Xiong et al., 2019) and matching the performance of a BERT model (Devlin et al., 2018) on this task. Finally, we show th"
N19-1250,P17-2052,0,0.140024,"distant data that has been denoised with our learned models gives further performance gains over this base model, outperforming models trained on raw distant data or heuristically-denoised distant data. 1 Introduction With the rise of data-hungry neural network models, system designers have turned increasingly to unlabeled and weakly-labeled data in order to scale up model training. For information extraction tasks such as relation extraction and entity typing, distant supervision (Mintz et al., 2009) is a powerful approach for adding more data, using a knowledge base (Del Corro et al., 2015; Rabinovich and Klein, 2017) or heuristics (Ratner et al., 2016; Hancock et al., 2018) to automatically label instances. One can treat this data just like any other supervised data, but it is noisy; more effective approaches employ specialized probabilistic models (Riedel et al., 2010; Ratner et al., 2018a), capturing its interaction with other supervision (Wang and Poon, 2018) or breaking down aspects of a task on which it is reliable (Ratner et al., 2018b). However, these approaches often require sophisticated probabilistic inference for training of the final model. Ideally, we want a technique that handles distant dat"
N19-1250,D12-1042,0,0.044622,"nt supervision have been applied extensively to relation extraction. Here, multi-instance learning and probabilisdollar object, currency, money, medium of exchange, dollar, monetary unit (c) ... Vittoria was influenced also by [Michelangelo] ... architect, sculptor, painter, poet person, artist, writer Figure 4: Examples of the noisy labels (left) and the denoised labels (right) for mentions (bold). The colors correspond to type classes: general (purple), finegrained (green), and ultra-fine (yellow). tic graphical modeling approaches have been used (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Takamatsu et al., 2012) as well as deep models (Lin et al., 2016; Feng et al., 2017; Luo et al., 2017; Lei et al., 2018; Han et al., 2018), though these often focus on incorporating signals from other sources as opposed to manually labeled data. 7 Conclusion In this work, we investigated the problem of denoising distant data for entity typing tasks. We trained a filtering function that discards examples from the distantly labeled data that are wholly unusable and a relabeling function that repairs noisy labels for the retained examples. When distant data is processed with our best denoising"
N19-1250,P12-1076,0,0.0272659,"n applied extensively to relation extraction. Here, multi-instance learning and probabilisdollar object, currency, money, medium of exchange, dollar, monetary unit (c) ... Vittoria was influenced also by [Michelangelo] ... architect, sculptor, painter, poet person, artist, writer Figure 4: Examples of the noisy labels (left) and the denoised labels (right) for mentions (bold). The colors correspond to type classes: general (purple), finegrained (green), and ultra-fine (yellow). tic graphical modeling approaches have been used (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Takamatsu et al., 2012) as well as deep models (Lin et al., 2016; Feng et al., 2017; Luo et al., 2017; Lei et al., 2018; Han et al., 2018), though these often focus on incorporating signals from other sources as opposed to manually labeled data. 7 Conclusion In this work, we investigated the problem of denoising distant data for entity typing tasks. We trained a filtering function that discards examples from the distantly labeled data that are wholly unusable and a relabeling function that repairs noisy labels for the retained examples. When distant data is processed with our best denoising model, our final trained"
N19-1250,D18-1215,0,0.0163464,"o scale up model training. For information extraction tasks such as relation extraction and entity typing, distant supervision (Mintz et al., 2009) is a powerful approach for adding more data, using a knowledge base (Del Corro et al., 2015; Rabinovich and Klein, 2017) or heuristics (Ratner et al., 2016; Hancock et al., 2018) to automatically label instances. One can treat this data just like any other supervised data, but it is noisy; more effective approaches employ specialized probabilistic models (Riedel et al., 2010; Ratner et al., 2018a), capturing its interaction with other supervision (Wang and Poon, 2018) or breaking down aspects of a task on which it is reliable (Ratner et al., 2018b). However, these approaches often require sophisticated probabilistic inference for training of the final model. Ideally, we want a technique that handles distant data just like supervised data, so we can treat our final model and its training procedure as black boxes. This paper tackles the problem of exploiting weakly-labeled data in a structured setting with a two-stage denoising approach. We can view a distant instance’s label as a noisy version of a true underlying label. We therefore learn a model to turn a"
N19-1250,N19-1084,0,0.461358,"tity linking and head words. On top of an adapted model from Choi et al. (2018) incorporating ELMo (Peters et al., 2018), na¨ıvely adding distant data actually hurts performance. However, when our learned denoising model is applied to the data, performance improves, and it improves more than heuristic denoising approaches tailored to this dataset. Our strongest denoising model gives a gain of 3 F1 absolute over the ELMo baseline, and a 4.4 F1 improvement over naive incorporation of distant data. This establishes a new state-ofthe-art on the test set, outperforming concurrently published work (Xiong et al., 2019) and matching the performance of a BERT model (Devlin et al., 2018) on this task. Finally, we show that denoising helps even when the label set is projected onto the OntoNotes label set (Hovy et al., 2006; Gillick et al., 2014), outperforming the method of Choi et al. (2018) in that setting as well. 2 Setup We consider the task of predicting a structured target y associated with an input x. Suppose we have high-quality labeled data tar of n (input, (1) (1) (n) (n) get) pairs D = { x , y , . . . , (x , y )}, and noisily labeled data of n0 (input, target) pairs 0 (1) (n0 ) D0 = {(x(1) , ynoisy"
N19-1250,D15-1083,0,0.17903,"Missing"
N19-1250,D16-1144,0,0.677906,"orms well but not as well as our model with augmented training data. One source of our improvements from data augmentation comes from additional data that is able to be used because some OntoNotes type can be derived. This is due to denoising doing a better job 6 One possible reason for this is identifying stray word senses; film can refer to the physical photosensitive object, among other things. Ours + ELMo w/o augmentation Ours + ELMo w augmentation Ours + ELMo w augmentation + filter & relabel Ours + ELMo w augmentation by Choi et al. (2018) BERT-Base, Uncased Shimaoka et al. (2017) AFET (Ren et al., 2016a) PLE (Ren et al., 2016b) Choi et al. (2018) L ABEL GCN (Xiong et al., 2019) Acc. Ma-F1 Mi-F1 42.7 59.3 63.9 72.7 76.5 84.5 66.7 70.7 78.9 64.9 84.5 79.2 51.8 76.6 69.1 51.7 55.1 57.2 59.5 59.6 70.9 71.1 71.5 76.8 77.8 64.9 64.7 66.1 71.8 72.2 Table 4: Test results on OntoNotes. Denoising helps substantially even in this reduced setting. Using fewer distant examples, we nearly match the performance using the data from Choi et al. (2018) (see text). of providing correct general types. In the EL setting, this yields 730k usable examples out of 1M (vs 540K for no denoising), and in HEAD, 640K ou"
N19-1250,E17-1055,0,0.0516839,"Missing"
N19-1250,P15-2048,0,0.0724902,"arse and fine types but struggles to assign multiple diverse ultra-fine types to the mention span Michelangelo, possibly because some of these types rarely cooccur (painter and poet). 6 location, place, city, country, area, region, township, town, municipality Related Work Past work on denoising data for entity typing has used multi-instance multi-label learning (Yaghoobzadeh and Sch¨utze, 2015, 2017; Murty et al., 2018). One view of these approaches is that they delete noisily-introduced labels, but they cannot add them, or filter bad examples. Other work focuses on learning type embeddings (Yogatama et al., 2015; Ren et al., 2016a,b); our approach goes beyond this in treating the label set in a structured way. The label set of Choi et al. (2018) is distinct in not being explicitly hierarchical, making past hierarchical approaches difficult to apply. Denoising techniques for distant supervision have been applied extensively to relation extraction. Here, multi-instance learning and probabilisdollar object, currency, money, medium of exchange, dollar, monetary unit (c) ... Vittoria was influenced also by [Michelangelo] ... architect, sculptor, painter, poet person, artist, writer Figure 4: Examples of t"
N19-1250,E17-1119,0,0.174011,"les in D and create a noised training set Ddrop , where a single training example is ((s, m, t0 ), t). g is trained on D0 drop with a binary classification loss function over types used in Choi et al. (2018), described in the next section. One can think of g as a type of denoising autoencoder (Vincent et al., 2008) whose reconstructed types t˜ are conditioned on v as well as t. 4 Typing Model In this section, we define the sentence and mention encoder Φm , which is use both in the denoising model as well as in the final prediction task. We extend previous attention-based models for this task (Shimaoka et al., 2017; Choi et al., 2018). At a high level, we have an instance encoder Φm that returns a vector vm ∈ RdΦ , then multiply the output of this encoding by a matrix and apply a sigmoid to get a binary prediction for each type as a probability of that type applying. Figure 3 outlines the overall architecture of our typing model. The encoder Φm consists of four vectors: a sentence representation s, a wordlevel mention representation mword , a characterlevel mention representation mchar , and a headword mention vector mhead . The first three of these were employed by Choi et al. (2018). We have modified"
N19-1349,D18-1431,0,0.0594287,"ur system is available at https://git. io/fjkDd. 2 Related work Generic responses is a recognized problem in dialogue generation. Li et al. (2016a) maximized mutual information in decoding or reranking, which practically looks like penalizing responses that are common under a language model. Zhou et al. (2017) promoted diversity by training latent embeddings to represent different response mechanisms. Shao et al. (2017) trained and reranked responses segment by segment with a glimpse model to inject diversity. Another angle is to promote prompt-response coherence using techniques such as LDA (Baheti et al., 2018; Xing et al., 2017). Cosine similarity between prompt and response has also been used for coherence (Xu et al., 2018b; Baheti et al., 2018). Wu et al. (2018) learn a small vocabulary of words that may be relevant during decoding and generates responses with this vocabulary. Several works tackle the problem by directly controlling response specificity in terms of word and response frequency. IDF and response frequency have been used as rewards in reinforcement learning (Yao et al., 2016; Li et al., 2016d). Some methods adjusted sample weights in the training data, using a dual encoding model ("
N19-1349,W05-0909,0,0.0745248,"entences from diversity decoding often have similar structure and phrases across candidates. We also experimented with reinforcement learning, using policy gradient with the reranking scores as reward. However, during development, we observed that this method produced shorter, less informative sentences compared to reranking. P 5 5.1 Experiments Evaluation metrics Automatic evaluation of dialogue generation systems is a known challenge. Prior work has shown that commonly used metrics for overall quality in other generation tasks such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005) and perplexity have poor correlations with human judgment (Liu et al., 2016; Tao et al., 2018)4 or are modeldependent (Liu et al., 2016). Therefore, we adopt several metrics that evaluate multiple aspects of responses, and also conduct human evaluation for each result we present. We use the following automatic evaluation metrics: (1) distinct-1 and distinct-2 (Li et al., 2016a), which evaluates response diversity. They respectively calculate the number of distinct unigrams and bigrams, divided by the total number of words in all responses; (2) linguistically-informed specificity (spec) (Ko et"
N19-1349,D12-1091,0,0.060406,"Missing"
N19-1349,J92-1002,0,0.460484,"ord frequency (NIWF) Used in Zhang et al. (2018b), NIWF is the maximum of the Inverse Word Frequency (IWF) of all the words in a response, normalized to 0-1: log(1 + |Y |) max(IW F ) = max fw   (3) where fw denotes the number of responses in the corpus that contain the word w, and |Y |is the number of responses in the corpus. Taking a maximum reflects the assumption that a response is specific as long as it has at least some infrequent word. Perplexity per word (PPW) Perplexity is the exponentiation of the entropy, which estimates the expected number of bits required to encode the sentence (Brown et al., 1992; Goodman, 2001). Thus perplexity is a direct measure of the amount of information in the sentence in information theory; it has also been used as a measure of linguistic complexity (Gorin et al., 2000). To compute perplexity, we train a neural language model (Mikolov et al., 2011) on all gold responses and calculate cross-entropy of each sentence. To represent the amount of information per-token and to prevent the model to simply generate long sentences, we normalize perplexity by sentence length. Linguistically-informed specificity We use the system developed by Ko et al. (2019), which estim"
N19-1349,D17-1070,0,0.0146628,"andomly selected function word in the gold response with another random function word of the same part-ofspeech. For pronouns and determiners, these negative sentences would likely be incohesive; with other word categories such as prepositions, this will target grammatically. One word or phrase is replaced in each synthetic sentence. We train one classifier θj , j ∈{1,2,3,4} for each of the categories above.3 The classifiers take word embeddings as input and predict if the response is real or generated. Each classifier consists of a bi-directional LSTM with a projection layer and max pooling (Conneau et al., 2017), followed by 3 fully connected layers. The posterior probabilities of these classifiers reflect how confident the classifiers are that the sentence is synthetic and prone to be implausible, hence we prefer sentences with lower posterior probabilities. During reranking, we feed each candidate sentence c into the classifiers and aggregate the posterior probabilities from these classifiers by taking the 3 We compare with using one classifier lumping all negative sentences in the experiments. 3460 mean 14 4k=1 P (synthetic|c, θk ). At test time, to encourage diversity, we repeat inference multipl"
N19-1349,P18-1152,0,0.012331,"stributional semantics largely fail to capture semantic plausibility, especially in terms of discrete properties (e.g., negation) (Kruszewski et al., 2016) and physical properties (Wang et al., 2018). Kruszewski et al. (2016) created a dataset building on synthetically generated sentences for negation plausibility. Methodology-wise, Li et al. (2016b) trained embeddings for different speakers jointly with the dialogue context. Huang et al. (2018) learned embeddings of emotions; we learn embeddings of specificity metrics. Targeting multiple factors this way is broadly similar to the approach of Holtzman et al. (2018), who used multiple cooperative discriminators to model repetition, entailment, rel3457 evance, and lexical style in generation. Our approach additionally leverages synthetic synthetic sentences targeting a range of plausibility issues and trains discriminators for reranking. 3 Generating specific responses Our main framework (Figure 1) is an attentionbased SEQ 2 SEQ model (Section 3.1) augmented with the ability to jointly learn embeddings from a target metric (e.g., specificity) with the response (Section 3.2). We then integrate frequencybased, information-theoretic and linguistic notions of"
N19-1349,N18-2008,0,0.0128396,"multiple phenomena, including referring expressions, concreteness of concepts, gradable adjectives, subjectivity and syntactic structure. Researchers have noticed that distributional semantics largely fail to capture semantic plausibility, especially in terms of discrete properties (e.g., negation) (Kruszewski et al., 2016) and physical properties (Wang et al., 2018). Kruszewski et al. (2016) created a dataset building on synthetically generated sentences for negation plausibility. Methodology-wise, Li et al. (2016b) trained embeddings for different speakers jointly with the dialogue context. Huang et al. (2018) learned embeddings of emotions; we learn embeddings of specificity metrics. Targeting multiple factors this way is broadly similar to the approach of Holtzman et al. (2018), who used multiple cooperative discriminators to model repetition, entailment, rel3457 evance, and lexical style in generation. Our approach additionally leverages synthetic synthetic sentences targeting a range of plausibility issues and trains discriminators for reranking. 3 Generating specific responses Our main framework (Figure 1) is an attentionbased SEQ 2 SEQ model (Section 3.1) augmented with the ability to jointly"
N19-1349,J16-4003,0,0.169626,"o discourse relations. Sentence specificity predictors have since been developed (Louis and Nenkova, 2011; Li and Nenkova, 2015; Lugini and Litman, 2017; Ko et al., 2019). Insights from these featurerich systems and hand-code analysis (Li et al., 2016e) showed that sentence specificity encompasses multiple phenomena, including referring expressions, concreteness of concepts, gradable adjectives, subjectivity and syntactic structure. Researchers have noticed that distributional semantics largely fail to capture semantic plausibility, especially in terms of discrete properties (e.g., negation) (Kruszewski et al., 2016) and physical properties (Wang et al., 2018). Kruszewski et al. (2016) created a dataset building on synthetically generated sentences for negation plausibility. Methodology-wise, Li et al. (2016b) trained embeddings for different speakers jointly with the dialogue context. Huang et al. (2018) learned embeddings of emotions; we learn embeddings of specificity metrics. Targeting multiple factors this way is broadly similar to the approach of Holtzman et al. (2018), who used multiple cooperative discriminators to model repetition, entailment, rel3457 evance, and lexical style in generation. Our"
N19-1349,N16-1014,0,0.440062,"model using linguistically motivated specificity and plausibility reranking improves the informativeness, reasonableness, and grammatically of responses. 1 Introduction Since the pioneering work in machine translation (Sutskever et al., 2014), sequence-tosequence (SEQ 2 SEQ) models have led much recent progress in open-domain dialogue generation, especially single-turn generation where the input is a prompt and the output is a response. However, SEQ 2 SEQ methods are known to favor universal responses, e.g., “I don’t know what you are talking about” (Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016a). These responses tend to be “safe” responses to many input queries, yet they usually fail to provide useful information. One promising line of research tackling this issue is to improve the specificity of responses, building on the intuition that generic responses frequently appear in the training data or consist of frequent words (Yao et al., 2016; Zhang et al., 2018b; Liu et al., 2018). However, past work in sentence specificity—the “quality of belonging or relating uniquely to a particular subject”1 — has shown that word frequency is only one aspect of specificity, and that specificity i"
N19-1349,W17-5546,0,0.0133682,"; Xing et al., 2017). Cosine similarity between prompt and response has also been used for coherence (Xu et al., 2018b; Baheti et al., 2018). Wu et al. (2018) learn a small vocabulary of words that may be relevant during decoding and generates responses with this vocabulary. Several works tackle the problem by directly controlling response specificity in terms of word and response frequency. IDF and response frequency have been used as rewards in reinforcement learning (Yao et al., 2016; Li et al., 2016d). Some methods adjusted sample weights in the training data, using a dual encoding model (Lison and Bibauw, 2017) or sentence length and frequency in the corpus (Liu et al., 2018). Zhang et al. (2018b) proposed a Gaussian mixture model using frequency-based specificity values. Their approach involves ensembling the context probability and a specificity probability, whereas our approach conditions on both in a single model. Prediction of sentence specificity following the dictionary definition and pragmatically cast as “level of detail” was first proposed by Louis and Nenkova (2011), who related specificity to discourse relations. Sentence specificity predictors have since been developed (Louis and Nenkov"
N19-1349,D16-1230,0,0.0621724,"idates. We also experimented with reinforcement learning, using policy gradient with the reranking scores as reward. However, during development, we observed that this method produced shorter, less informative sentences compared to reranking. P 5 5.1 Experiments Evaluation metrics Automatic evaluation of dialogue generation systems is a known challenge. Prior work has shown that commonly used metrics for overall quality in other generation tasks such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005) and perplexity have poor correlations with human judgment (Liu et al., 2016; Tao et al., 2018)4 or are modeldependent (Liu et al., 2016). Therefore, we adopt several metrics that evaluate multiple aspects of responses, and also conduct human evaluation for each result we present. We use the following automatic evaluation metrics: (1) distinct-1 and distinct-2 (Li et al., 2016a), which evaluates response diversity. They respectively calculate the number of distinct unigrams and bigrams, divided by the total number of words in all responses; (2) linguistically-informed specificity (spec) (Ko et al., 2019); (3) cosine similarity between input and response representation"
N19-1349,D18-1297,0,0.270373,"e input is a prompt and the output is a response. However, SEQ 2 SEQ methods are known to favor universal responses, e.g., “I don’t know what you are talking about” (Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016a). These responses tend to be “safe” responses to many input queries, yet they usually fail to provide useful information. One promising line of research tackling this issue is to improve the specificity of responses, building on the intuition that generic responses frequently appear in the training data or consist of frequent words (Yao et al., 2016; Zhang et al., 2018b; Liu et al., 2018). However, past work in sentence specificity—the “quality of belonging or relating uniquely to a particular subject”1 — has shown that word frequency is only one aspect of specificity, and that specificity involves a wide range of phenomena including word usage, sentence structure (Louis and Nenkova, 2011; Li and Nenkova, 2015; Lugini and Litman, 2017) and discourse context (Dixon, 1987; Lassonde and O’Brien, 2009). Frequency-based specificity also does not exactly capture “the amount of information” as an information-theoretic concept. Hence, in dialogue generation, we can potentially make pr"
N19-1349,I11-1068,0,0.439567,"hey usually fail to provide useful information. One promising line of research tackling this issue is to improve the specificity of responses, building on the intuition that generic responses frequently appear in the training data or consist of frequent words (Yao et al., 2016; Zhang et al., 2018b; Liu et al., 2018). However, past work in sentence specificity—the “quality of belonging or relating uniquely to a particular subject”1 — has shown that word frequency is only one aspect of specificity, and that specificity involves a wide range of phenomena including word usage, sentence structure (Louis and Nenkova, 2011; Li and Nenkova, 2015; Lugini and Litman, 2017) and discourse context (Dixon, 1987; Lassonde and O’Brien, 2009). Frequency-based specificity also does not exactly capture “the amount of information” as an information-theoretic concept. Hence, in dialogue generation, we can potentially make progress by incorporating more linguistically driven measures of specificity, as opposed to relying solely on frequency. We present a sequence-to-sequence dialogue model that factors out specificity and explicitly conditions on it when generating a response. The decoder takes as input categorized values of"
N19-1349,W17-5006,0,0.109906,"One promising line of research tackling this issue is to improve the specificity of responses, building on the intuition that generic responses frequently appear in the training data or consist of frequent words (Yao et al., 2016; Zhang et al., 2018b; Liu et al., 2018). However, past work in sentence specificity—the “quality of belonging or relating uniquely to a particular subject”1 — has shown that word frequency is only one aspect of specificity, and that specificity involves a wide range of phenomena including word usage, sentence structure (Louis and Nenkova, 2011; Li and Nenkova, 2015; Lugini and Litman, 2017) and discourse context (Dixon, 1987; Lassonde and O’Brien, 2009). Frequency-based specificity also does not exactly capture “the amount of information” as an information-theoretic concept. Hence, in dialogue generation, we can potentially make progress by incorporating more linguistically driven measures of specificity, as opposed to relying solely on frequency. We present a sequence-to-sequence dialogue model that factors out specificity and explicitly conditions on it when generating a response. The decoder takes as input categorized values of several specificity metrics, embeds them, and us"
N19-1349,P02-1040,0,0.104787,"different plausibility levels). On the contrary, sentences from diversity decoding often have similar structure and phrases across candidates. We also experimented with reinforcement learning, using policy gradient with the reranking scores as reward. However, during development, we observed that this method produced shorter, less informative sentences compared to reranking. P 5 5.1 Experiments Evaluation metrics Automatic evaluation of dialogue generation systems is a known challenge. Prior work has shown that commonly used metrics for overall quality in other generation tasks such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005) and perplexity have poor correlations with human judgment (Liu et al., 2016; Tao et al., 2018)4 or are modeldependent (Liu et al., 2016). Therefore, we adopt several metrics that evaluate multiple aspects of responses, and also conduct human evaluation for each result we present. We use the following automatic evaluation metrics: (1) distinct-1 and distinct-2 (Li et al., 2016a), which evaluates response diversity. They respectively calculate the number of distinct unigrams and bigrams, divided by the total number of words in all responses;"
N19-1349,P16-1094,0,0.256097,"model using linguistically motivated specificity and plausibility reranking improves the informativeness, reasonableness, and grammatically of responses. 1 Introduction Since the pioneering work in machine translation (Sutskever et al., 2014), sequence-tosequence (SEQ 2 SEQ) models have led much recent progress in open-domain dialogue generation, especially single-turn generation where the input is a prompt and the output is a response. However, SEQ 2 SEQ methods are known to favor universal responses, e.g., “I don’t know what you are talking about” (Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016a). These responses tend to be “safe” responses to many input queries, yet they usually fail to provide useful information. One promising line of research tackling this issue is to improve the specificity of responses, building on the intuition that generic responses frequently appear in the training data or consist of frequent words (Yao et al., 2016; Zhang et al., 2018b; Liu et al., 2018). However, past work in sentence specificity—the “quality of belonging or relating uniquely to a particular subject”1 — has shown that word frequency is only one aspect of specificity, and that specificity i"
N19-1349,D14-1162,0,0.082107,"122,458 prompt-response pairs for training and 14,602 pairs for testing. For validation, for reasons described in Section 5.1, we opt for human evaluation of overall response quality on a validation set of 60 prompt-response pairs from PersonaChat. Settings We use LSTMs with hidden layers of size 500, Adam optimizer (Kingma and Ba, 2015) with learning rate 0.001, β1 = 0.9, β2 = 0.999, dropout rate 0.2 for both training and testing, metric embedding dimension 300 and 5 training epochs. We train randomly initialized word embeddings of size 500 for the dialog model and use 300 dimentional GloVe (Pennington et al., 2014) embeddings for reranking classifiers. We generate 15 candidates for reranking per input sentence. To train the 4 reranking classifiers, we use 375,996 positive sentences on Opensubtitles and 110,221 on PersonaChat. We generate one negative sentence per word or phrase in the positive sentences. Since specificity is the focus of this study, during testing, we use the embedding of the highest specificity level (5) for NIWF and the linguistically informed specificity predictor. For PPW, we observe that the perplexity of generated sentences does not increase beyond the median level (3) during deve"
N19-1349,D16-1127,0,0.484362,"model using linguistically motivated specificity and plausibility reranking improves the informativeness, reasonableness, and grammatically of responses. 1 Introduction Since the pioneering work in machine translation (Sutskever et al., 2014), sequence-tosequence (SEQ 2 SEQ) models have led much recent progress in open-domain dialogue generation, especially single-turn generation where the input is a prompt and the output is a response. However, SEQ 2 SEQ methods are known to favor universal responses, e.g., “I don’t know what you are talking about” (Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016a). These responses tend to be “safe” responses to many input queries, yet they usually fail to provide useful information. One promising line of research tackling this issue is to improve the specificity of responses, building on the intuition that generic responses frequently appear in the training data or consist of frequent words (Yao et al., 2016; Zhang et al., 2018b; Liu et al., 2018). However, past work in sentence specificity—the “quality of belonging or relating uniquely to a particular subject”1 — has shown that word frequency is only one aspect of specificity, and that specificity i"
N19-1349,D17-1230,0,0.0282286,"ate multiple aspects of responses, and also conduct human evaluation for each result we present. We use the following automatic evaluation metrics: (1) distinct-1 and distinct-2 (Li et al., 2016a), which evaluates response diversity. They respectively calculate the number of distinct unigrams and bigrams, divided by the total number of words in all responses; (2) linguistically-informed specificity (spec) (Ko et al., 2019); (3) cosine similarity between input and response representations, which captures coherence (Zhang et al., 2018a). We follow standards from prior work for human evaluation (Li et al., 2017; Zhang et al., 2018a,b; Xu et al., 2018a). We select 250 prompt-response pairs, and asked 5 judges from MechanicalTurk to rate the responses for each prompt. We evaluate whether the responses are informative (Ko et al., 2019; Wu et al., 2018; Shao et al., 2017) and on topic with the prompt (Shen et al., 2018; Xu et al., 4 Although Tao et al. (2018) proposed an unspervised metric, their code is not available. 2018b; Xing et al., 2017), on a scale of 1-5. Average scores are reported. In addition, we evaluate plausibility by asking judges whether they think the given response sentence without th"
N19-1349,D17-1235,0,0.118753,"her improvement. Our plausibility reranking method not only successfully improved the semantic plausibility of responses, but also improved their informativeness, relevance, and grammaticality. Our system is available at https://git. io/fjkDd. 2 Related work Generic responses is a recognized problem in dialogue generation. Li et al. (2016a) maximized mutual information in decoding or reranking, which practically looks like penalizing responses that are common under a language model. Zhou et al. (2017) promoted diversity by training latent embeddings to represent different response mechanisms. Shao et al. (2017) trained and reranked responses segment by segment with a glimpse model to inject diversity. Another angle is to promote prompt-response coherence using techniques such as LDA (Baheti et al., 2018; Xing et al., 2017). Cosine similarity between prompt and response has also been used for coherence (Xu et al., 2018b; Baheti et al., 2018). Wu et al. (2018) learn a small vocabulary of words that may be relevant during decoding and generates responses with this vocabulary. Several works tackle the problem by directly controlling response specificity in terms of word and response frequency. IDF and r"
N19-1349,N16-1141,1,0.834858,"th. Linguistically-informed specificity We use the system developed by Ko et al. (2019), which estimates specificity as a real value. This system adopts a pragmatic notion of specificity—level of details in text—that is originally derived using sentence pairs connected via the INSTANTIATION discourse relation (Louis and Nenkova, 2011). With this relation, one sentence explains in further detail of the content in the other; the explanatory sentence is shown to demonstrate properties of specificity towards particular concepts, entities and objects, while the other sentence is much more general (Li and Nenkova, 2016). We use this particular system since other specificity predictors are trained on news with binary specificity labels (Li and Nenkova, 2015). Ko et al. (2019) is an unsupervised domain adaptation system that predicts continuous specificity values, and was evaluated to be close to human judgments across several domains. We retrain their system using the gold responses in our data as unlabeled sentences in the unsupervised domain adaptation component. Coherence Prior work has shown that the universal response problem can be mitigated by improving the coherence between prompt and response (Zhang"
N19-1349,N15-1020,0,0.0380746,"d factors. Experiments show that our final model using linguistically motivated specificity and plausibility reranking improves the informativeness, reasonableness, and grammatically of responses. 1 Introduction Since the pioneering work in machine translation (Sutskever et al., 2014), sequence-tosequence (SEQ 2 SEQ) models have led much recent progress in open-domain dialogue generation, especially single-turn generation where the input is a prompt and the output is a response. However, SEQ 2 SEQ methods are known to favor universal responses, e.g., “I don’t know what you are talking about” (Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016a). These responses tend to be “safe” responses to many input queries, yet they usually fail to provide useful information. One promising line of research tackling this issue is to improve the specificity of responses, building on the intuition that generic responses frequently appear in the training data or consist of frequent words (Yao et al., 2016; Zhang et al., 2018b; Liu et al., 2018). However, past work in sentence specificity—the “quality of belonging or relating uniquely to a particular subject”1 — has shown that word frequency is only one aspect"
N19-1349,L16-1620,1,0.907373,"Missing"
N19-1349,W04-1013,0,0.00982445,". On the contrary, sentences from diversity decoding often have similar structure and phrases across candidates. We also experimented with reinforcement learning, using policy gradient with the reranking scores as reward. However, during development, we observed that this method produced shorter, less informative sentences compared to reranking. P 5 5.1 Experiments Evaluation metrics Automatic evaluation of dialogue generation systems is a known challenge. Prior work has shown that commonly used metrics for overall quality in other generation tasks such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005) and perplexity have poor correlations with human judgment (Liu et al., 2016; Tao et al., 2018)4 or are modeldependent (Liu et al., 2016). Therefore, we adopt several metrics that evaluate multiple aspects of responses, and also conduct human evaluation for each result we present. We use the following automatic evaluation metrics: (1) distinct-1 and distinct-2 (Li et al., 2016a), which evaluates response diversity. They respectively calculate the number of distinct unigrams and bigrams, divided by the total number of words in all responses; (2) linguistically"
N19-1349,N18-2049,1,0.835989,"tors have since been developed (Louis and Nenkova, 2011; Li and Nenkova, 2015; Lugini and Litman, 2017; Ko et al., 2019). Insights from these featurerich systems and hand-code analysis (Li et al., 2016e) showed that sentence specificity encompasses multiple phenomena, including referring expressions, concreteness of concepts, gradable adjectives, subjectivity and syntactic structure. Researchers have noticed that distributional semantics largely fail to capture semantic plausibility, especially in terms of discrete properties (e.g., negation) (Kruszewski et al., 2016) and physical properties (Wang et al., 2018). Kruszewski et al. (2016) created a dataset building on synthetically generated sentences for negation plausibility. Methodology-wise, Li et al. (2016b) trained embeddings for different speakers jointly with the dialogue context. Huang et al. (2018) learned embeddings of emotions; we learn embeddings of specificity metrics. Targeting multiple factors this way is broadly similar to the approach of Holtzman et al. (2018), who used multiple cooperative discriminators to model repetition, entailment, rel3457 evance, and lexical style in generation. Our approach additionally leverages synthetic sy"
N19-1349,D18-1432,0,0.52716,"eration. Li et al. (2016a) maximized mutual information in decoding or reranking, which practically looks like penalizing responses that are common under a language model. Zhou et al. (2017) promoted diversity by training latent embeddings to represent different response mechanisms. Shao et al. (2017) trained and reranked responses segment by segment with a glimpse model to inject diversity. Another angle is to promote prompt-response coherence using techniques such as LDA (Baheti et al., 2018; Xing et al., 2017). Cosine similarity between prompt and response has also been used for coherence (Xu et al., 2018b; Baheti et al., 2018). Wu et al. (2018) learn a small vocabulary of words that may be relevant during decoding and generates responses with this vocabulary. Several works tackle the problem by directly controlling response specificity in terms of word and response frequency. IDF and response frequency have been used as rewards in reinforcement learning (Yao et al., 2016; Li et al., 2016d). Some methods adjusted sample weights in the training data, using a dual encoding model (Lison and Bibauw, 2017) or sentence length and frequency in the corpus (Liu et al., 2018). Zhang et al. (2018b) propo"
N19-1349,P18-1102,0,0.0752342,"n generation where the input is a prompt and the output is a response. However, SEQ 2 SEQ methods are known to favor universal responses, e.g., “I don’t know what you are talking about” (Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016a). These responses tend to be “safe” responses to many input queries, yet they usually fail to provide useful information. One promising line of research tackling this issue is to improve the specificity of responses, building on the intuition that generic responses frequently appear in the training data or consist of frequent words (Yao et al., 2016; Zhang et al., 2018b; Liu et al., 2018). However, past work in sentence specificity—the “quality of belonging or relating uniquely to a particular subject”1 — has shown that word frequency is only one aspect of specificity, and that specificity involves a wide range of phenomena including word usage, sentence structure (Louis and Nenkova, 2011; Li and Nenkova, 2015; Lugini and Litman, 2017) and discourse context (Dixon, 1987; Lassonde and O’Brien, 2009). Frequency-based specificity also does not exactly capture “the amount of information” as an information-theoretic concept. Hence, in dialogue generation, we can"
N19-1349,P18-1205,0,0.0832626,"n generation where the input is a prompt and the output is a response. However, SEQ 2 SEQ methods are known to favor universal responses, e.g., “I don’t know what you are talking about” (Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016a). These responses tend to be “safe” responses to many input queries, yet they usually fail to provide useful information. One promising line of research tackling this issue is to improve the specificity of responses, building on the intuition that generic responses frequently appear in the training data or consist of frequent words (Yao et al., 2016; Zhang et al., 2018b; Liu et al., 2018). However, past work in sentence specificity—the “quality of belonging or relating uniquely to a particular subject”1 — has shown that word frequency is only one aspect of specificity, and that specificity involves a wide range of phenomena including word usage, sentence structure (Louis and Nenkova, 2011; Li and Nenkova, 2015; Lugini and Litman, 2017) and discourse context (Dixon, 1987; Lassonde and O’Brien, 2009). Frequency-based specificity also does not exactly capture “the amount of information” as an information-theoretic concept. Hence, in dialogue generation, we can"
N19-1405,P16-1223,0,0.0299257,"g information from different parts of a passage to answer the question, leading to a number of models designed to do multi-hop reasoning. Two recent large-scale datasets have been specifically designed to test multi-hop reasoning: WikiHop (Welbl et al., 2018) and HotpotQA (Yang et al., 2018). In this paper, we seek to answer two main questions. First, although the two datasets are explicitly constructed for multi-hop reasoning, do models really need to do multi-hop reasoning to do well on them? Recent work has shown that largescale QA datasets often do not exhibit their advertised properties (Chen et al., 2016; Kaushik and Lipton, 2018). We devise a test setting to see whether multi-hop reasoning is necessary: can a model which treats each sentence independently select the sentence containing the answer? This provides a rough estimate of the fraction of questions solvable by a non-multi-hop system. Our results show that more than half of the questions in WikiHop and HotpotQA do not require multihop reasoning to solve. Surprisingly, we find that a simple baseline which ignores the passage and only uses the question and answer can achieve strong results on WikiHop and a modified version of HotpotQA,"
N19-1405,J81-4005,0,0.676719,"Missing"
N19-1405,W18-1703,0,0.0565125,"multi-hop reasoning is only needed in a few questions. Results in Table 1 indicate that although intentionally created for multi-hop reasoning, for more than half of questions in WikiHop and HotpotQA, we can figure out where the answer is without doing multi-hop reasoning. This result is initially surprising, but one reason it may be possible is suggested by the example from HotpotQA shown in Figure 1. We can see that the model could easily figure out the answer sentence without looking at the bridging entities using lexical cues alone. This observation is also in accordance with the work of Jansen (2018), which demonstrates that high performance for a simple baseline can be achieved in cases when passages have increasing lexical overlap with the question. We note that this method potentially overestimates performance of a non-multi-hop model on HotpotQA, since there are some examples where many plausible answers are in the same sentence and require other context to resolve. However, these still form a minority in the dataset (see Table 3 of Yang et al. (2018)). 3.2 Bi-linear Dot Figure 2: An example of question and candidates from WikiHop. Here we can see that among the candidates, only Natio"
N19-1405,L18-1433,0,0.0937759,"ve comparable performance to the other two more complex models, which indicates that these models still aren’t learning multihop reasoning in such a strengthened setting. Span-based training data is more powerful. To further understand the two different supervision signals, we conduct another experiment where we train using span-based supervision and evaluate on the multiple-choice setting. Specifically, during evaluation, we select all document spans that map onto some answer candidate, then Discussion and Conclusion There exist several other multi-hop reasoning datasets including WorldTree (Jansen et al., 2018), OpenBookQA (Mihaylov et al., 2018), and MultiRC (Khashabi et al., 2018). These datasets are more complex to analyze since the answers may not appear directly in the passage and may simply be entailed by passage content. We leave a detailed investigation of these for future work. For researchers working on the problem of multi-hop reasoning, we think the following points should be considered: (1) Prefer models using span-based supervision to avoid “cheating” by using the extra candidate information. (2) If using multiple-choice supervision, check the no-context baseline to see whether there a"
N19-1405,P17-1147,0,0.0648295,"ng models may not be learning as much multi-hop reasoning as previously thought. 1 Introduction Question answering from text (Richardson et al., 2013; Hill et al., 2015; Hermann et al., 2015; Rajpurkar et al., 2016) is a key challenge problem for NLP that tests whether models can extract information based on a query. However, even sophisticated models that perform well on QA benchmarks (Seo et al., 2017; Shen et al., 2017; Yu et al., 2018) may only be doing shallow pattern matching of the question against the supporting passage (Weissenborn et al., 2017). More recent work (Kumar et al., 2016; Joshi et al., 2017; Welbl et al., 2018) has emphasized gathering information from different parts of a passage to answer the question, leading to a number of models designed to do multi-hop reasoning. Two recent large-scale datasets have been specifically designed to test multi-hop reasoning: WikiHop (Welbl et al., 2018) and HotpotQA (Yang et al., 2018). In this paper, we seek to answer two main questions. First, although the two datasets are explicitly constructed for multi-hop reasoning, do models really need to do multi-hop reasoning to do well on them? Recent work has shown that largescale QA datasets often"
N19-1405,D18-1546,0,0.0451075,"different parts of a passage to answer the question, leading to a number of models designed to do multi-hop reasoning. Two recent large-scale datasets have been specifically designed to test multi-hop reasoning: WikiHop (Welbl et al., 2018) and HotpotQA (Yang et al., 2018). In this paper, we seek to answer two main questions. First, although the two datasets are explicitly constructed for multi-hop reasoning, do models really need to do multi-hop reasoning to do well on them? Recent work has shown that largescale QA datasets often do not exhibit their advertised properties (Chen et al., 2016; Kaushik and Lipton, 2018). We devise a test setting to see whether multi-hop reasoning is necessary: can a model which treats each sentence independently select the sentence containing the answer? This provides a rough estimate of the fraction of questions solvable by a non-multi-hop system. Our results show that more than half of the questions in WikiHop and HotpotQA do not require multihop reasoning to solve. Surprisingly, we find that a simple baseline which ignores the passage and only uses the question and answer can achieve strong results on WikiHop and a modified version of HotpotQA, further confirming this vie"
N19-1405,N18-1023,0,0.104008,"ndicates that these models still aren’t learning multihop reasoning in such a strengthened setting. Span-based training data is more powerful. To further understand the two different supervision signals, we conduct another experiment where we train using span-based supervision and evaluate on the multiple-choice setting. Specifically, during evaluation, we select all document spans that map onto some answer candidate, then Discussion and Conclusion There exist several other multi-hop reasoning datasets including WorldTree (Jansen et al., 2018), OpenBookQA (Mihaylov et al., 2018), and MultiRC (Khashabi et al., 2018). These datasets are more complex to analyze since the answers may not appear directly in the passage and may simply be entailed by passage content. We leave a detailed investigation of these for future work. For researchers working on the problem of multi-hop reasoning, we think the following points should be considered: (1) Prefer models using span-based supervision to avoid “cheating” by using the extra candidate information. (2) If using multiple-choice supervision, check the no-context baseline to see whether there are strong correlations between question and candidates. (3) When construc"
N19-1405,D16-1147,0,0.0296786,"etting. Our models only mildly outperform the no-context baseline in all settings. randomly select 9 entities in all of the documents as distractors, and add the answer to make a 10choice candidates set. To modify WikiHop to be span-based, we concatenate all documents and treat the first appearance of the answer mention as the gold span for training. Any answer occurrence is treated as correct for evaluation. 4.1 Systems to Compare MemNet Memory networks (Weston et al., 2015) define a generic model class which can gather information from different parts of the passage. Kumar et al. (2016) and Miller et al. (2016) have demonstrated its effectiveness in certain multi-hop settings. These models process a document over several timesteps. On the ith step, the model takes a question representation qi , attends to the context representation p, gets an attention distribution Pαi , computes a new memory cell value mi = αi pi , then forms an updated qi+1 = f (mi , qi ). The final memory cell mT is used to compute a score si = g(mT , cj ) with the jth candidate representation cj . We modify this architecture slightly using a standard hierarchical attention module (Li et al., 2015). We can also modify this archit"
N19-1405,D16-1264,0,0.377969,"o achieve high performance considering only the questions and answers. Finally, we investigate one key difference between these datasets, namely spanbased vs. multiple-choice formulations of the QA task. Multiple-choice versions of both datasets can be easily gamed, and two models we examine only marginally exceed a baseline in this setting. Overall, while these datasets are useful testbeds, high-performing models may not be learning as much multi-hop reasoning as previously thought. 1 Introduction Question answering from text (Richardson et al., 2013; Hill et al., 2015; Hermann et al., 2015; Rajpurkar et al., 2016) is a key challenge problem for NLP that tests whether models can extract information based on a query. However, even sophisticated models that perform well on QA benchmarks (Seo et al., 2017; Shen et al., 2017; Yu et al., 2018) may only be doing shallow pattern matching of the question against the supporting passage (Weissenborn et al., 2017). More recent work (Kumar et al., 2016; Joshi et al., 2017; Welbl et al., 2018) has emphasized gathering information from different parts of a passage to answer the question, leading to a number of models designed to do multi-hop reasoning. Two recent lar"
N19-1405,D13-1020,0,0.0557289,"relations in the unmasked version of WikiHop, which make it easy to achieve high performance considering only the questions and answers. Finally, we investigate one key difference between these datasets, namely spanbased vs. multiple-choice formulations of the QA task. Multiple-choice versions of both datasets can be easily gamed, and two models we examine only marginally exceed a baseline in this setting. Overall, while these datasets are useful testbeds, high-performing models may not be learning as much multi-hop reasoning as previously thought. 1 Introduction Question answering from text (Richardson et al., 2013; Hill et al., 2015; Hermann et al., 2015; Rajpurkar et al., 2016) is a key challenge problem for NLP that tests whether models can extract information based on a query. However, even sophisticated models that perform well on QA benchmarks (Seo et al., 2017; Shen et al., 2017; Yu et al., 2018) may only be doing shallow pattern matching of the question against the supporting passage (Weissenborn et al., 2017). More recent work (Kumar et al., 2016; Joshi et al., 2017; Welbl et al., 2018) has emphasized gathering information from different parts of a passage to answer the question, leading to a n"
N19-1405,Q18-1021,0,0.241494,"learning as much multi-hop reasoning as previously thought. 1 Introduction Question answering from text (Richardson et al., 2013; Hill et al., 2015; Hermann et al., 2015; Rajpurkar et al., 2016) is a key challenge problem for NLP that tests whether models can extract information based on a query. However, even sophisticated models that perform well on QA benchmarks (Seo et al., 2017; Shen et al., 2017; Yu et al., 2018) may only be doing shallow pattern matching of the question against the supporting passage (Weissenborn et al., 2017). More recent work (Kumar et al., 2016; Joshi et al., 2017; Welbl et al., 2018) has emphasized gathering information from different parts of a passage to answer the question, leading to a number of models designed to do multi-hop reasoning. Two recent large-scale datasets have been specifically designed to test multi-hop reasoning: WikiHop (Welbl et al., 2018) and HotpotQA (Yang et al., 2018). In this paper, we seek to answer two main questions. First, although the two datasets are explicitly constructed for multi-hop reasoning, do models really need to do multi-hop reasoning to do well on them? Recent work has shown that largescale QA datasets often do not exhibit their"
N19-1405,D18-1259,0,0.486465,"isticated models that perform well on QA benchmarks (Seo et al., 2017; Shen et al., 2017; Yu et al., 2018) may only be doing shallow pattern matching of the question against the supporting passage (Weissenborn et al., 2017). More recent work (Kumar et al., 2016; Joshi et al., 2017; Welbl et al., 2018) has emphasized gathering information from different parts of a passage to answer the question, leading to a number of models designed to do multi-hop reasoning. Two recent large-scale datasets have been specifically designed to test multi-hop reasoning: WikiHop (Welbl et al., 2018) and HotpotQA (Yang et al., 2018). In this paper, we seek to answer two main questions. First, although the two datasets are explicitly constructed for multi-hop reasoning, do models really need to do multi-hop reasoning to do well on them? Recent work has shown that largescale QA datasets often do not exhibit their advertised properties (Chen et al., 2016; Kaushik and Lipton, 2018). We devise a test setting to see whether multi-hop reasoning is necessary: can a model which treats each sentence independently select the sentence containing the answer? This provides a rough estimate of the fraction of questions solvable by a no"
N19-1405,P15-1107,0,\N,Missing
N19-1405,D18-1260,0,\N,Missing
N19-1405,P18-1078,0,\N,Missing
N19-1405,N19-1240,0,\N,Missing
N19-1405,K17-1028,0,\N,Missing
P11-2005,D08-1087,0,0.0593334,"Missing"
P11-2005,P10-2041,0,0.0409051,"racting from the count of each n-gram, is one of the core aspects of Kneser-Ney language modeling (Kneser and Ney, 1995). For all but the smallest n-gram counts, Kneser-Ney uses a single discount, one that does not grow with the ngram count, because such constant-discounting was seen in early experiments on held-out data (Church and Gale, 1991). However, due to increasing computational power and corpus sizes, language modeling today presents a different set of challenges than it did 20 years ago. In particular, modeling crossdomain effects has become increasingly more important (Klakow, 2000; Moore and Lewis, 2010), and deployed systems must frequently process data that is out-of-domain from the standpoint of the language model. In this work, we perform experiments on heldout data to evaluate how discounting behaves in the Discount Analysis Underlying discounting is the idea that n-grams will occur fewer times in test data than they do in training data. We investigate this quantitatively by conducting experiments similar in spirit to those of Church and Gale (1991). Suppose that we have collected counts on two corpora of the same size, which we will call our train and test corpora. For an n-gram w = (w1"
P11-2005,P09-2088,0,0.0309829,"m types exhibit similar discount relationships. from different years, and between the NYT and AFP newswire, discounts grow even more quickly. We observed these trends continuing steadily up into ngram counts in the hundreds, beyond which point it becomes difficult to robustly estimate discounts due to fewer n-gram types in this count range. This result is surprising in light of the constant discounts observed for the NYT95/NYT950 pair. Goodman (2001) proposes that discounts arise from document-level “burstiness” in a corpus, because language often repeats itself locally within a document, and Moore and Quirk (2009) suggest that discounting also corrects for quantization error due to estimating a continuous distribution using a discrete maximum likelihood estimator (MLE). Both of these factors are at play in the NYT95/NYT950 experiment, and yet only a small, constant discount is observed. Our growing discounts must therefore be caused by other, larger-scale phenomena, such as shifts in the subjects of news articles over time or in the style of the writing between newswire sources. The increasing rate of discount growth as the source changes and temporal divergence increases lends credence to this hypothe"
P11-2005,P06-1124,0,0.105458,"Missing"
P13-1012,P08-2012,0,0.139183,"our model: mentions manage their partial membership in various coreference chains, so that information about entity-level properties is decentralized and propagated across individual mentions, and we never need to explicitly instantiate entities. Exact inference in this factor graph is intractable, but efficient approximate inference can be carried out with belief propagation. Our model is the first discriminatively-trained model that both makes joint decisions over an entire document and models specific entity-level properties, rather than simply enforcing transitivity of pairwise decisions (Finkel and Manning, 2008; Song et al., 2012). We evaluate our system on the dataset from the CoNLL 2011 shared task using three different types of properties: synthetic oracle properties, entity phi features (number, gender, animacy, and NER type), and properties derived from unsupervised clusters targeting semantic type information. In all cases, our transitive model of entity properties equals or outperforms our pairwise system and our reimplementation of a previous entity-level system (Rahman and Ng, 2009). Our final system is competitive with the winner of the CoNLL 2011 shared task (Lee et al., 2011). Efficientl"
P13-1012,P12-1041,1,0.844664,"ARG1:approved ... ... ARG1:cause ARG2:following ARG1:reported ARG1:filed prices shares index rates ... ... ... ARG1:rose ARG1:fell ARG1:cut ARG1:closed ... ... Figure 4: Examples of clusters produced by the NAIVE BAYES model on SRL-tagged data with pronouns discarded. types of fine-grained semantic class information (Hendrickx and Daelemans, 2007; Ng, 2007; Rahman and Ng, 2010). Other approaches incorporate information from other sources (Ponzetto and Strube, 2006) or compute heuristic scores for realvalued features based on a large corpus or the web (Dagan and Itai, 1990; Yang et al., 2005; Bansal and Klein, 2012). We use four different clusterings in our experiments, each with twenty clusters: dependency-parse-derived NAIVE BAYES clusters, semantic-role-derived C ONDITIONAL clusters, SRL-derived NAIVE BAYES clusters generating a N OV ERB token when r cannot be determined, and SRL-derived NAIVE BAYES clusters with all pronoun tuples discarded. Examples of the latter clusters are shown in Figure 4. Each clustering is learned for 30 iterations of EM over English Gigaword (Graff et al., 2007), parsed with the Berkeley Parser (Petrov et al., 2006) and with SRL determined by Senna (Collobert et al., 2011)."
P13-1012,N10-1112,0,0.122795,"chi et al., 2011); we found this to be faster and give higher performance than L-BFGS using L2 regularization (Liu and Nocedal, 1989). Note that because of the marginalization over A(C i ), even the objective for the BASIC model is not convex. 5 l(a, C) = Inference Inference in the BASIC model is straightforward. Given a set of weights w, we can predict a∈A(C i ) where (xi , C i ) is the ith labeled training example. This optimizes for the 0-1 loss; however, we are much more interested in optimizing with respect to a coreference-specific loss function. To this end, we will use softmax-margin (Gimpel and Smith, 2010), which augments the probability of each example with a term proportional to its loss, pushing the model to assign less mass to highly incorrect examples. We modify Equation 1 to use a new probability distribution P 0 instead of P , where P 0 (a|xi ) ∝ P (a|xi ) exp (l(a, C)) and l(a, C) is a loss function. In order to perform inference efficiently, l(a, C) must decompose linearly across mentions: l(a, C) = Pn i=1 l(ai , C). Commonly-used coreference metrics such as MUC (Vilain et al., 1995) and B 3 (Bagga and Baldwin, 1998) do not have this property, so we instead make use of a parameterized"
P13-1012,D08-1031,0,0.348235,"s feature function can include pairwise features based on mention i and the chosen antecedent ai , since information about each mention is contained in x. Because the model factors completely over the individual ai , these feature functions fA can be expressed as unary factors Ai (see Figure 1), with Ai (j) ∝ exp wT fA (i, j, x) . Given a setting of w, we can determine a ˆ = arg maxa P (a|x) and then deterministically compute C(a), the final set of coreference chains. While the features of this model factor over coreference links, this approach differs from classical pairwise systems such as Bengtson and Roth (2008) or Stoyanov et al. (2010). Because potential antecedents compete with each other and with the non-anaphoric hypothesis, the choice of ai actually represents a joint decision about i−1 pairwise links, as opposed to systems that use a pairwise binary classifier and a separate agglomeration step, which consider one link at a time during learning. This approach is similar to the mentionranking model of Rahman and Ng (2009). Models We will first present our BASIC model (Section 3.1) and describe the features it incorporates (Section 3.2), then explain how to extend it to use transitive features (S"
P13-1012,P06-1005,0,0.0589705,"rameter settings give fairly weak oracle information: a document may have hundreds of clusters, so even in the absence of noise these oracle properties do not have high dis7.3 Phi Features As we have seen, our T RANSITIVE model can exploit high-quality entity-level features. How does it perform using real features that have been proposed for entity-level coreference? Here, we use hard phi feature determinations extracted from the system of Lee et al. (2011). Named-entity type and animacy are both computed based on the output of a named-entity tagger, while number and gender use the dataset of Bergsma and Lin (2006). Once this information is determined, the PAIR P ROPERTY and L EFTT O R IGHT systems can compute features over it directly. In the T RANSITIVE model, each of the Ri factors places 34 of its mass on the determined label and distributes the remainder uniformly among the possible options. Table 2 shows results when adding entity-level phi features on top of our BASIC pairwise system (which already contains pairwise features) and on top of an ablated BASIC system without pairwise 6 Using gold entities for training as in Rahman and Ng (2009) resulted in a lower-performing system. 7 We even do this"
P13-1012,N10-1061,1,0.85998,"nd-to-end discriminative probabilistic model for coreference that, along with standard pairwise features, enforces structural agreement constraints between specified properties of coreferent mentions. This model can be represented as a factor graph for each document that admits efficient inference via belief propagation. We show that our method can use entity-level information to outperform a basic pairwise system. 1 Introduction The inclusion of entity-level features has been a driving force behind the development of many coreference resolution systems (Luo et al., 2004; Rahman and Ng, 2009; Haghighi and Klein, 2010; Lee et al., 2011). There is no polynomial-time dynamic program for inference in a model with arbitrary entity-level features, so systems that use such features typically rely on making decisions in a pipelined manner and sticking with them, operating greedily in a left-to-right fashion (Rahman and Ng, 2009) or in a multi-pass, sieve-like manner (Raghunathan et al., 2010). However, such systems may be locked into bad coreference decisions and are difficult to directly optimize for standard evaluation metrics. In this work, we present a new structured model of entity-level information designed"
P13-1012,N12-1004,1,0.837515,"t takes the standard form of the gradient of a log-linear model, a difference of expected feature counts under the gold annotation and under no annotation. This requires computing marginals P 0 (ai |x) for each mention i, but because the model already factors this way, this step is easy. The T RANSITIVE model is more complex. Exact inference is intractable due to the E factors that couple all of the ai by way of the pi nodes. However, we can compute approximate marginals for the ai , pi , and ri using belief propagation. BP has been effectively used on other NLP tasks (Smith and Eisner, 2008; Burkett and Klein, 2012), and is effective in cases such as this where the model is largely driven by non-loopy factors (here, the Ai ). From marginals over each node, we can compute the necessary gradient and decode as before: [c1 I(K1 (ai , C)) + c2 I(K2 (ai , C)) a ˆ = arg max Pˆ (a|x) i=1 a + c3 I(K3 (ai , C))] 3 One could use ILP-based decoding in the style of Finkel and Manning (2008) and Song et al. (2012) to attempt to explicitly find the optimal C with choice of a marginalized out, but we did not explore this option. where c1 , c2 , and c3 are real-valued weights, K1 denotes the event that ai is falsely anap"
P13-1012,N06-2015,0,0.07396,"e, we use our BA SIC model to prune antecedent choices for each ai in order to reduce the size of the factor graph that we must instantiate. Specifically, we prune links between pairs of mentions that are of mention distance more than 100, as well as values for ai that fall below a particular odds ratio threshold with respect to the best setting of that ai in the BASIC model; that is, those for which   PBASIC (ai |x) log maxj PBASIC (ai = j|x) 7 We use the datasets, experimental setup, and scoring program from the CoNLL 2011 shared task (Pradhan et al., 2011), based on the OntoNotes corpus (Hovy et al., 2006). We use the standard automatic parses and NER tags for each document. Our mentions are those output by the system of Lee et al. (2011); we also use their postprocessing to remove appositives, predicate nominatives, and singletons before evaluation. For each experiment, we report MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and CEAFe (Luo, 2005), as well as their average. is below a cutoff γ. 6 Experiments Related Work Parameter settings. We take the regularization constant λ = 0.001 and the parameters of our surrogate loss (c1 , c2 , c3 ) = (0.15, 2.5, 1) for all models.4 All mod"
P13-1012,W11-1902,0,0.470878,"ons (Finkel and Manning, 2008; Song et al., 2012). We evaluate our system on the dataset from the CoNLL 2011 shared task using three different types of properties: synthetic oracle properties, entity phi features (number, gender, animacy, and NER type), and properties derived from unsupervised clusters targeting semantic type information. In all cases, our transitive model of entity properties equals or outperforms our pairwise system and our reimplementation of a previous entity-level system (Rahman and Ng, 2009). Our final system is competitive with the winner of the CoNLL 2011 shared task (Lee et al., 2011). Efficiently incorporating entity-level information is a challenge for coreference resolution systems due to the difficulty of exact inference over partitions. We describe an end-to-end discriminative probabilistic model for coreference that, along with standard pairwise features, enforces structural agreement constraints between specified properties of coreferent mentions. This model can be represented as a factor graph for each document that admits efficient inference via belief propagation. We show that our method can use entity-level information to outperform a basic pairwise system. 1 In"
P13-1012,C90-3063,0,0.0628431,"nd agreement ARG1:set ARG0:announced plan ARG1:approved ... ... ARG1:cause ARG2:following ARG1:reported ARG1:filed prices shares index rates ... ... ... ARG1:rose ARG1:fell ARG1:cut ARG1:closed ... ... Figure 4: Examples of clusters produced by the NAIVE BAYES model on SRL-tagged data with pronouns discarded. types of fine-grained semantic class information (Hendrickx and Daelemans, 2007; Ng, 2007; Rahman and Ng, 2010). Other approaches incorporate information from other sources (Ponzetto and Strube, 2006) or compute heuristic scores for realvalued features based on a large corpus or the web (Dagan and Itai, 1990; Yang et al., 2005; Bansal and Klein, 2012). We use four different clusterings in our experiments, each with twenty clusters: dependency-parse-derived NAIVE BAYES clusters, semantic-role-derived C ONDITIONAL clusters, SRL-derived NAIVE BAYES clusters generating a N OV ERB token when r cannot be determined, and SRL-derived NAIVE BAYES clusters with all pronoun tuples discarded. Examples of the latter clusters are shown in Figure 4. Each clustering is learned for 30 iterations of EM over English Gigaword (Graff et al., 2007), parsed with the Berkeley Parser (Petrov et al., 2006) and with SRL de"
P13-1012,N07-1030,0,0.043178,"n constant λ = 0.001 and the parameters of our surrogate loss (c1 , c2 , c3 ) = (0.15, 2.5, 1) for all models.4 All models are trained for 20 iterations. We take the pruning threshold γ = −2. Our BASIC model is a mention-ranking approach resembling models used by Denis and Baldridge (2008) and Rahman and Ng (2009), though it is trained using a novel parameterized loss function. It is also similar to the MLN-J OINT (BF) model of Song et al. (2012), but we enforce the singleparent constraint at a deeper structural level, allowing us to treat non-anaphoricity symmetrically with coreference as in Denis and Baldridge (2007) and Stoyanov and Eisner (2012). The model of Fernandes et al. (2012) also uses the single-parent constraint structurally, but with learning via latent perceptron and ILP-based one-best decoding rather than logistic regression and BP-based marginal computation. Our T RANSITIVE model is novel; while McCallum and Wellner (2004) proposed the idea of using attributes for mentions, they do not actually implement a model that does so. Other systems include entity-level information via handwritten rules (Raghunathan et al., 2010), induced rules (Yang et al., 2008), or features with learned weights (L"
P13-1012,P04-1018,0,0.553537,"rence over partitions. We describe an end-to-end discriminative probabilistic model for coreference that, along with standard pairwise features, enforces structural agreement constraints between specified properties of coreferent mentions. This model can be represented as a factor graph for each document that admits efficient inference via belief propagation. We show that our method can use entity-level information to outperform a basic pairwise system. 1 Introduction The inclusion of entity-level features has been a driving force behind the development of many coreference resolution systems (Luo et al., 2004; Rahman and Ng, 2009; Haghighi and Klein, 2010; Lee et al., 2011). There is no polynomial-time dynamic program for inference in a model with arbitrary entity-level features, so systems that use such features typically rely on making decisions in a pipelined manner and sticking with them, operating greedily in a left-to-right fashion (Rahman and Ng, 2009) or in a multi-pass, sieve-like manner (Raghunathan et al., 2010). However, such systems may be locked into bad coreference decisions and are difficult to directly optimize for standard evaluation metrics. In this work, we present a new struct"
P13-1012,D08-1069,0,0.594988,"use their postprocessing to remove appositives, predicate nominatives, and singletons before evaluation. For each experiment, we report MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and CEAFe (Luo, 2005), as well as their average. is below a cutoff γ. 6 Experiments Related Work Parameter settings. We take the regularization constant λ = 0.001 and the parameters of our surrogate loss (c1 , c2 , c3 ) = (0.15, 2.5, 1) for all models.4 All models are trained for 20 iterations. We take the pruning threshold γ = −2. Our BASIC model is a mention-ranking approach resembling models used by Denis and Baldridge (2008) and Rahman and Ng (2009), though it is trained using a novel parameterized loss function. It is also similar to the MLN-J OINT (BF) model of Song et al. (2012), but we enforce the singleparent constraint at a deeper structural level, allowing us to treat non-anaphoricity symmetrically with coreference as in Denis and Baldridge (2007) and Stoyanov and Eisner (2012). The model of Fernandes et al. (2012) also uses the single-parent constraint structurally, but with learning via latent perceptron and ILP-based one-best decoding rather than logistic regression and BP-based marginal computation. Ou"
P13-1012,H05-1004,0,0.417385,"SIC model; that is, those for which   PBASIC (ai |x) log maxj PBASIC (ai = j|x) 7 We use the datasets, experimental setup, and scoring program from the CoNLL 2011 shared task (Pradhan et al., 2011), based on the OntoNotes corpus (Hovy et al., 2006). We use the standard automatic parses and NER tags for each document. Our mentions are those output by the system of Lee et al. (2011); we also use their postprocessing to remove appositives, predicate nominatives, and singletons before evaluation. For each experiment, we report MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and CEAFe (Luo, 2005), as well as their average. is below a cutoff γ. 6 Experiments Related Work Parameter settings. We take the regularization constant λ = 0.001 and the parameters of our surrogate loss (c1 , c2 , c3 ) = (0.15, 2.5, 1) for all models.4 All models are trained for 20 iterations. We take the pruning threshold γ = −2. Our BASIC model is a mention-ranking approach resembling models used by Denis and Baldridge (2008) and Rahman and Ng (2009), though it is trained using a novel parameterized loss function. It is also similar to the MLN-J OINT (BF) model of Song et al. (2012), but we enforce the singlepa"
P13-1012,P07-1068,0,0.0115929,"has been noted previously (Luo et al., 2004). 7.4 C LUSTERS MUC B 3 CEAFe 61.96 70.66 47.30 62.88 70.71 47.45 61.98 70.19 45.77 63.34 70.89 46.88 ARG0:said way ARG1:signed ARG0:say law ARG1:announced ARG0:found agreement ARG1:set ARG0:announced plan ARG1:approved ... ... ARG1:cause ARG2:following ARG1:reported ARG1:filed prices shares index rates ... ... ... ARG1:rose ARG1:fell ARG1:cut ARG1:closed ... ... Figure 4: Examples of clusters produced by the NAIVE BAYES model on SRL-tagged data with pronouns discarded. types of fine-grained semantic class information (Hendrickx and Daelemans, 2007; Ng, 2007; Rahman and Ng, 2010). Other approaches incorporate information from other sources (Ponzetto and Strube, 2006) or compute heuristic scores for realvalued features based on a large corpus or the web (Dagan and Itai, 1990; Yang et al., 2005; Bansal and Klein, 2012). We use four different clusterings in our experiments, each with twenty clusters: dependency-parse-derived NAIVE BAYES clusters, semantic-role-derived C ONDITIONAL clusters, SRL-derived NAIVE BAYES clusters generating a N OV ERB token when r cannot be determined, and SRL-derived NAIVE BAYES clusters with all pronoun tuples discarded."
P13-1012,P06-1055,1,0.0610045,"pus or the web (Dagan and Itai, 1990; Yang et al., 2005; Bansal and Klein, 2012). We use four different clusterings in our experiments, each with twenty clusters: dependency-parse-derived NAIVE BAYES clusters, semantic-role-derived C ONDITIONAL clusters, SRL-derived NAIVE BAYES clusters generating a N OV ERB token when r cannot be determined, and SRL-derived NAIVE BAYES clusters with all pronoun tuples discarded. Examples of the latter clusters are shown in Figure 4. Each clustering is learned for 30 iterations of EM over English Gigaword (Graff et al., 2007), parsed with the Berkeley Parser (Petrov et al., 2006) and with SRL determined by Senna (Collobert et al., 2011). Table 3 shows results of modeling these cluster properties. As in the case of oracle features, the PAIR P ROPERTY and L EFT T O R IGHT systems use the modes of the cluster posteriors, and the T RAN SITIVE system uses the posteriors directly as the Ri . We see comparable performance from incorporating features in both an entity-level framework and a pairwise framework, though the T RANSI - Clustering Features Finally, we consider mention properties derived from unsupervised clusterings; these properties are designed to target semantic"
P13-1012,M95-1005,0,0.929899,"izing with respect to a coreference-specific loss function. To this end, we will use softmax-margin (Gimpel and Smith, 2010), which augments the probability of each example with a term proportional to its loss, pushing the model to assign less mass to highly incorrect examples. We modify Equation 1 to use a new probability distribution P 0 instead of P , where P 0 (a|xi ) ∝ P (a|xi ) exp (l(a, C)) and l(a, C) is a loss function. In order to perform inference efficiently, l(a, C) must decompose linearly across mentions: l(a, C) = Pn i=1 l(ai , C). Commonly-used coreference metrics such as MUC (Vilain et al., 1995) and B 3 (Bagga and Baldwin, 1998) do not have this property, so we instead make use of a parameterized loss function that does and fit the parameters to give good performance. Specifically, we take n X a∈A(C i ) a ˆ = arg max P (a|x) a We then report the corresponding chains C(a) as the system output.3 For learning, the gradient takes the standard form of the gradient of a log-linear model, a difference of expected feature counts under the gold annotation and under no annotation. This requires computing marginals P 0 (ai |x) for each mention i, but because the model already factors this way,"
P13-1012,N06-1025,0,0.081196,"2.88 70.71 47.45 61.98 70.19 45.77 63.34 70.89 46.88 ARG0:said way ARG1:signed ARG0:say law ARG1:announced ARG0:found agreement ARG1:set ARG0:announced plan ARG1:approved ... ... ARG1:cause ARG2:following ARG1:reported ARG1:filed prices shares index rates ... ... ... ARG1:rose ARG1:fell ARG1:cut ARG1:closed ... ... Figure 4: Examples of clusters produced by the NAIVE BAYES model on SRL-tagged data with pronouns discarded. types of fine-grained semantic class information (Hendrickx and Daelemans, 2007; Ng, 2007; Rahman and Ng, 2010). Other approaches incorporate information from other sources (Ponzetto and Strube, 2006) or compute heuristic scores for realvalued features based on a large corpus or the web (Dagan and Itai, 1990; Yang et al., 2005; Bansal and Klein, 2012). We use four different clusterings in our experiments, each with twenty clusters: dependency-parse-derived NAIVE BAYES clusters, semantic-role-derived C ONDITIONAL clusters, SRL-derived NAIVE BAYES clusters generating a N OV ERB token when r cannot be determined, and SRL-derived NAIVE BAYES clusters with all pronoun tuples discarded. Examples of the latter clusters are shown in Figure 4. Each clustering is learned for 30 iterations of EM over"
P13-1012,P05-1021,0,0.0314974,"ARG0:announced plan ARG1:approved ... ... ARG1:cause ARG2:following ARG1:reported ARG1:filed prices shares index rates ... ... ... ARG1:rose ARG1:fell ARG1:cut ARG1:closed ... ... Figure 4: Examples of clusters produced by the NAIVE BAYES model on SRL-tagged data with pronouns discarded. types of fine-grained semantic class information (Hendrickx and Daelemans, 2007; Ng, 2007; Rahman and Ng, 2010). Other approaches incorporate information from other sources (Ponzetto and Strube, 2006) or compute heuristic scores for realvalued features based on a large corpus or the web (Dagan and Itai, 1990; Yang et al., 2005; Bansal and Klein, 2012). We use four different clusterings in our experiments, each with twenty clusters: dependency-parse-derived NAIVE BAYES clusters, semantic-role-derived C ONDITIONAL clusters, SRL-derived NAIVE BAYES clusters generating a N OV ERB token when r cannot be determined, and SRL-derived NAIVE BAYES clusters with all pronoun tuples discarded. Examples of the latter clusters are shown in Figure 4. Each clustering is learned for 30 iterations of EM over English Gigaword (Graff et al., 2007), parsed with the Berkeley Parser (Petrov et al., 2006) and with SRL determined by Senna ("
P13-1012,W11-1901,0,0.171711,"w when a document contains over 200 mentions. Therefore, we use our BA SIC model to prune antecedent choices for each ai in order to reduce the size of the factor graph that we must instantiate. Specifically, we prune links between pairs of mentions that are of mention distance more than 100, as well as values for ai that fall below a particular odds ratio threshold with respect to the best setting of that ai in the BASIC model; that is, those for which   PBASIC (ai |x) log maxj PBASIC (ai = j|x) 7 We use the datasets, experimental setup, and scoring program from the CoNLL 2011 shared task (Pradhan et al., 2011), based on the OntoNotes corpus (Hovy et al., 2006). We use the standard automatic parses and NER tags for each document. Our mentions are those output by the system of Lee et al. (2011); we also use their postprocessing to remove appositives, predicate nominatives, and singletons before evaluation. For each experiment, we report MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and CEAFe (Luo, 2005), as well as their average. is below a cutoff γ. 6 Experiments Related Work Parameter settings. We take the regularization constant λ = 0.001 and the parameters of our surrogate loss (c1 ,"
P13-1012,P08-1096,0,0.0325606,"cally with coreference as in Denis and Baldridge (2007) and Stoyanov and Eisner (2012). The model of Fernandes et al. (2012) also uses the single-parent constraint structurally, but with learning via latent perceptron and ILP-based one-best decoding rather than logistic regression and BP-based marginal computation. Our T RANSITIVE model is novel; while McCallum and Wellner (2004) proposed the idea of using attributes for mentions, they do not actually implement a model that does so. Other systems include entity-level information via handwritten rules (Raghunathan et al., 2010), induced rules (Yang et al., 2008), or features with learned weights (Luo et al., 2004; Rahman and Ng, 2011), but all of these systems freeze past coreference decisions in order to compute their entities. Most similar to our entity-level approach is the system of Haghighi and Klein (2010), which 7.1 Systems Besides our BASIC and T RANSITIVE systems, we evaluate a strictly pairwise system that incorporates property information by way of indicator features on the current mention’s most likely property value and the proposed antecedent’s most likely property value. We call this system PAIR P ROP ERTY ; it is simply the BASIC syst"
P13-1012,D10-1048,0,0.288912,"ation to outperform a basic pairwise system. 1 Introduction The inclusion of entity-level features has been a driving force behind the development of many coreference resolution systems (Luo et al., 2004; Rahman and Ng, 2009; Haghighi and Klein, 2010; Lee et al., 2011). There is no polynomial-time dynamic program for inference in a model with arbitrary entity-level features, so systems that use such features typically rely on making decisions in a pipelined manner and sticking with them, operating greedily in a left-to-right fashion (Rahman and Ng, 2009) or in a multi-pass, sieve-like manner (Raghunathan et al., 2010). However, such systems may be locked into bad coreference decisions and are difficult to directly optimize for standard evaluation metrics. In this work, we present a new structured model of entity-level information designed to allow efficient inference. We use a log-linear model that can be expressed as a factor graph. Pairwise features appear in the model as unary factors, adjacent to nodes representing a choice of antecedent (or none) for each mention. Additional nodes model entity-level properties on a per-mention basis, and 2 Example We begin with an example motivating our use of entity-"
P13-1012,D11-1135,0,0.0273976,"ns the semantic role of n (or some approximation thereof) conjoined with its governor. Two different algorithms are used to cluster these pairs: a NAIVE BAYES model, where c generates n and r, and a C ONDITIONAL model, where c is generated conditioned on r and then n is generated from c. Parameters for each can be learned with the expectation maximization (EM) algorithm (Dempster et al., 1977), with symmetry broken by a small amount of random noise at initialization. Similar models have been used to learn subcategorization information (Rooth et al., 1999) or properties of verb argument slots (Yao et al., 2011). We choose this kind of clustering for its relative simplicity and because it allows pronouns to have more informed properties (from their verbal context) than would be possible using a model that makes type-level decisions about nominals only. Though these specific cluster features are novel to coreference, previous work has used similar 121 BASIC S TANFORD Prec. 69.99 61.49 MUC Rec. 55.59 59.59 PAIR P ROPERTY L EFT T O R IGHT T RANSITIVE 76.49 76.92 76.48 58.53 58.55 60.20 L EFT T O R IGHT T RANSITIVE 69.77 70.27 54.73 56.54 BASIC -P HI PAIR P ROPERTY L EFT T O R IGHT T RANSITIVE 67.04 70.2"
P13-1012,D09-1101,0,0.527408,"models specific entity-level properties, rather than simply enforcing transitivity of pairwise decisions (Finkel and Manning, 2008; Song et al., 2012). We evaluate our system on the dataset from the CoNLL 2011 shared task using three different types of properties: synthetic oracle properties, entity phi features (number, gender, animacy, and NER type), and properties derived from unsupervised clusters targeting semantic type information. In all cases, our transitive model of entity properties equals or outperforms our pairwise system and our reimplementation of a previous entity-level system (Rahman and Ng, 2009). Our final system is competitive with the winner of the CoNLL 2011 shared task (Lee et al., 2011). Efficiently incorporating entity-level information is a challenge for coreference resolution systems due to the difficulty of exact inference over partitions. We describe an end-to-end discriminative probabilistic model for coreference that, along with standard pairwise features, enforces structural agreement constraints between specified properties of coreferent mentions. This model can be represented as a factor graph for each document that admits efficient inference via belief propagation. We"
P13-1012,C10-1105,0,0.0107721,"oted previously (Luo et al., 2004). 7.4 C LUSTERS MUC B 3 CEAFe 61.96 70.66 47.30 62.88 70.71 47.45 61.98 70.19 45.77 63.34 70.89 46.88 ARG0:said way ARG1:signed ARG0:say law ARG1:announced ARG0:found agreement ARG1:set ARG0:announced plan ARG1:approved ... ... ARG1:cause ARG2:following ARG1:reported ARG1:filed prices shares index rates ... ... ... ARG1:rose ARG1:fell ARG1:cut ARG1:closed ... ... Figure 4: Examples of clusters produced by the NAIVE BAYES model on SRL-tagged data with pronouns discarded. types of fine-grained semantic class information (Hendrickx and Daelemans, 2007; Ng, 2007; Rahman and Ng, 2010). Other approaches incorporate information from other sources (Ponzetto and Strube, 2006) or compute heuristic scores for realvalued features based on a large corpus or the web (Dagan and Itai, 1990; Yang et al., 2005; Bansal and Klein, 2012). We use four different clusterings in our experiments, each with twenty clusters: dependency-parse-derived NAIVE BAYES clusters, semantic-role-derived C ONDITIONAL clusters, SRL-derived NAIVE BAYES clusters generating a N OV ERB token when r cannot be determined, and SRL-derived NAIVE BAYES clusters with all pronoun tuples discarded. Examples of the latte"
P13-1012,P99-1014,0,0.0200981,"pairs (n, r) of a noun head n and a string r which contains the semantic role of n (or some approximation thereof) conjoined with its governor. Two different algorithms are used to cluster these pairs: a NAIVE BAYES model, where c generates n and r, and a C ONDITIONAL model, where c is generated conditioned on r and then n is generated from c. Parameters for each can be learned with the expectation maximization (EM) algorithm (Dempster et al., 1977), with symmetry broken by a small amount of random noise at initialization. Similar models have been used to learn subcategorization information (Rooth et al., 1999) or properties of verb argument slots (Yao et al., 2011). We choose this kind of clustering for its relative simplicity and because it allows pronouns to have more informed properties (from their verbal context) than would be possible using a model that makes type-level decisions about nominals only. Though these specific cluster features are novel to coreference, previous work has used similar 121 BASIC S TANFORD Prec. 69.99 61.49 MUC Rec. 55.59 59.59 PAIR P ROPERTY L EFT T O R IGHT T RANSITIVE 76.49 76.92 76.48 58.53 58.55 60.20 L EFT T O R IGHT T RANSITIVE 69.77 70.27 54.73 56.54 BASIC -P H"
P13-1012,D08-1016,0,0.016988,"or learning, the gradient takes the standard form of the gradient of a log-linear model, a difference of expected feature counts under the gold annotation and under no annotation. This requires computing marginals P 0 (ai |x) for each mention i, but because the model already factors this way, this step is easy. The T RANSITIVE model is more complex. Exact inference is intractable due to the E factors that couple all of the ai by way of the pi nodes. However, we can compute approximate marginals for the ai , pi , and ri using belief propagation. BP has been effectively used on other NLP tasks (Smith and Eisner, 2008; Burkett and Klein, 2012), and is effective in cases such as this where the model is largely driven by non-loopy factors (here, the Ai ). From marginals over each node, we can compute the necessary gradient and decode as before: [c1 I(K1 (ai , C)) + c2 I(K2 (ai , C)) a ˆ = arg max Pˆ (a|x) i=1 a + c3 I(K3 (ai , C))] 3 One could use ILP-based decoding in the style of Finkel and Manning (2008) and Song et al. (2012) to attempt to explicitly find the optimal C with choice of a marginalized out, but we did not explore this option. where c1 , c2 , and c3 are real-valued weights, K1 denotes the eve"
P13-1012,D12-1114,0,0.183478,"their partial membership in various coreference chains, so that information about entity-level properties is decentralized and propagated across individual mentions, and we never need to explicitly instantiate entities. Exact inference in this factor graph is intractable, but efficient approximate inference can be carried out with belief propagation. Our model is the first discriminatively-trained model that both makes joint decisions over an entire document and models specific entity-level properties, rather than simply enforcing transitivity of pairwise decisions (Finkel and Manning, 2008; Song et al., 2012). We evaluate our system on the dataset from the CoNLL 2011 shared task using three different types of properties: synthetic oracle properties, entity phi features (number, gender, animacy, and NER type), and properties derived from unsupervised clusters targeting semantic type information. In all cases, our transitive model of entity properties equals or outperforms our pairwise system and our reimplementation of a previous entity-level system (Rahman and Ng, 2009). Our final system is competitive with the winner of the CoNLL 2011 shared task (Lee et al., 2011). Efficiently incorporating enti"
P13-1012,C12-1154,0,0.0766171,"rameters of our surrogate loss (c1 , c2 , c3 ) = (0.15, 2.5, 1) for all models.4 All models are trained for 20 iterations. We take the pruning threshold γ = −2. Our BASIC model is a mention-ranking approach resembling models used by Denis and Baldridge (2008) and Rahman and Ng (2009), though it is trained using a novel parameterized loss function. It is also similar to the MLN-J OINT (BF) model of Song et al. (2012), but we enforce the singleparent constraint at a deeper structural level, allowing us to treat non-anaphoricity symmetrically with coreference as in Denis and Baldridge (2007) and Stoyanov and Eisner (2012). The model of Fernandes et al. (2012) also uses the single-parent constraint structurally, but with learning via latent perceptron and ILP-based one-best decoding rather than logistic regression and BP-based marginal computation. Our T RANSITIVE model is novel; while McCallum and Wellner (2004) proposed the idea of using attributes for mentions, they do not actually implement a model that does so. Other systems include entity-level information via handwritten rules (Raghunathan et al., 2010), induced rules (Yang et al., 2008), or features with learned weights (Luo et al., 2004; Rahman and Ng,"
P13-1012,W12-4502,0,\N,Missing
P13-1012,P10-2029,0,\N,Missing
P13-1012,D08-1067,0,\N,Missing
P13-1021,koen-2004-pharaoh,0,0.0151054,"to each result. Bernoulli parameter for this pixel, we can write:  [XiGLYPH ]jk ∼ Bernoulli θ PIXEL (j, k, gi ; φei ) The interpolation process for a single row is depicted in Figure 5. We define a constant interpolation vector µ(gi , k) that is specific to the glyph box width gi and glyph box column k. Each µ(gi , k) is shaped according to a Gaussian centered at the relative column position in φei . The glyph pixel Bernoulli parameters are defined as follows: 4.2 The number of states in the dynamic programming lattice grows exponentially with the order of the language model (Jelinek, 1998; Koehn, 2004). As a result, inference can become slow when the language model order n is large. To remedy this, we take a coarse-to-fine approach to both learning and inference. On each iteration of EM, we perform two passes: a coarse pass using a low-order language model, and a fine pass using a high-order language model (Petrov et al., 2008; Zhang and Gildea, 2008). We use the marginals4 from the coarse pass to prune states from the dynamic program of the fine pass. θ PIXEL (j, k,gi ; φei ) = w h X i logistic µ(gi , k)k0 · [φei ]jk0 k0 =1 The fact that the parameterization is log-linear will ensure tha"
P13-1021,D11-1029,1,0.848221,"fully integrating a strong language model (Vamvakas et al., 2008; Kluzner et al., 2009; Kae et al., 2010; Kluzner et al., 2011). The most comparable work is that of Kopec and Lomelin (1996) and Kopec et al. (2001). They integrated typesetting models with language models, but did not model noise. In the NLP community, generative models have been developed specifically for correcting outputs of OCR systems (Kolak et al., 2003), but these do not deal directly with images. A closely related area of work is automatic decipherment (Ravi and Knight, 2008; Snyder et al., 2010; Ravi and Knight, 2011; Berg-Kirkpatrick and Klein, 2011). The fundamental problem is similar to our own: we are presented with a sequence of symbols, and we need to learn a correspondence between symbols and letters. Our approach is also similar in that we use a strong language model (in conjunction with the constraint that the correspondence be regular) to learn the correct mapping. However, the symbols are not noisy in decipherment problems and in our problem we face a grid of pixels for which the segmentation into symbols is unknown. In contrast, decipherment typically deals only with discrete symbols. P (E, T, R, X) = [Language model] P (E) [Ty"
P13-1021,N03-1018,0,0.0344936,"Missing"
P13-1021,N10-1083,1,0.467591,"Missing"
P13-1021,J04-4002,0,0.02125,"Missing"
P13-1021,D08-1012,1,0.899245,"Missing"
P13-1021,D08-1085,0,0.0125897,"rical documents has done so using a pipelined approach, and without fully integrating a strong language model (Vamvakas et al., 2008; Kluzner et al., 2009; Kae et al., 2010; Kluzner et al., 2011). The most comparable work is that of Kopec and Lomelin (1996) and Kopec et al. (2001). They integrated typesetting models with language models, but did not model noise. In the NLP community, generative models have been developed specifically for correcting outputs of OCR systems (Kolak et al., 2003), but these do not deal directly with images. A closely related area of work is automatic decipherment (Ravi and Knight, 2008; Snyder et al., 2010; Ravi and Knight, 2011; Berg-Kirkpatrick and Klein, 2011). The fundamental problem is similar to our own: we are presented with a sequence of symbols, and we need to learn a correspondence between symbols and letters. Our approach is also similar in that we use a strong language model (in conjunction with the constraint that the correspondence be regular) to learn the correct mapping. However, the symbols are not noisy in decipherment problems and in our problem we face a grid of pixels for which the segmentation into symbols is unknown. In contrast, decipherment typicall"
P13-1021,P11-1025,0,0.01315,"d approach, and without fully integrating a strong language model (Vamvakas et al., 2008; Kluzner et al., 2009; Kae et al., 2010; Kluzner et al., 2011). The most comparable work is that of Kopec and Lomelin (1996) and Kopec et al. (2001). They integrated typesetting models with language models, but did not model noise. In the NLP community, generative models have been developed specifically for correcting outputs of OCR systems (Kolak et al., 2003), but these do not deal directly with images. A closely related area of work is automatic decipherment (Ravi and Knight, 2008; Snyder et al., 2010; Ravi and Knight, 2011; Berg-Kirkpatrick and Klein, 2011). The fundamental problem is similar to our own: we are presented with a sequence of symbols, and we need to learn a correspondence between symbols and letters. Our approach is also similar in that we use a strong language model (in conjunction with the constraint that the correspondence be regular) to learn the correct mapping. However, the symbols are not noisy in decipherment problems and in our problem we face a grid of pixels for which the segmentation into symbols is unknown. In contrast, decipherment typically deals only with discrete symbols. P (E, T,"
P13-1021,P10-1107,0,0.0205837,"e so using a pipelined approach, and without fully integrating a strong language model (Vamvakas et al., 2008; Kluzner et al., 2009; Kae et al., 2010; Kluzner et al., 2011). The most comparable work is that of Kopec and Lomelin (1996) and Kopec et al. (2001). They integrated typesetting models with language models, but did not model noise. In the NLP community, generative models have been developed specifically for correcting outputs of OCR systems (Kolak et al., 2003), but these do not deal directly with images. A closely related area of work is automatic decipherment (Ravi and Knight, 2008; Snyder et al., 2010; Ravi and Knight, 2011; Berg-Kirkpatrick and Klein, 2011). The fundamental problem is similar to our own: we are presented with a sequence of symbols, and we need to learn a correspondence between symbols and letters. Our approach is also similar in that we use a strong language model (in conjunction with the constraint that the correspondence be regular) to learn the correct mapping. However, the symbols are not noisy in decipherment problems and in our problem we face a grid of pixels for which the segmentation into symbols is unknown. In contrast, decipherment typically deals only with dis"
P13-1021,P08-1025,0,0.0542564,"a Gaussian centered at the relative column position in φei . The glyph pixel Bernoulli parameters are defined as follows: 4.2 The number of states in the dynamic programming lattice grows exponentially with the order of the language model (Jelinek, 1998; Koehn, 2004). As a result, inference can become slow when the language model order n is large. To remedy this, we take a coarse-to-fine approach to both learning and inference. On each iteration of EM, we perform two passes: a coarse pass using a low-order language model, and a fine pass using a high-order language model (Petrov et al., 2008; Zhang and Gildea, 2008). We use the marginals4 from the coarse pass to prune states from the dynamic program of the fine pass. θ PIXEL (j, k,gi ; φei ) = w h X i logistic µ(gi , k)k0 · [φei ]jk0 k0 =1 The fact that the parameterization is log-linear will ensure that, during the unsupervised learning process, updating the shape parameters φc is simple and feasible. By varying the magnitude of µ we can change the level of smoothing in the logistic model and cause it to permit areas that are over-inked. This is the effect that di controls. By offsetting the rows of φc that we interpolate weights from, we change the v"
P14-1022,W13-4916,0,0.0731857,"Missing"
P14-1022,E93-1006,0,0.190874,"ead lexicalization (Eisner, 1996; Collins, 1997; Charniak, 1997), structural annotation (Johnson, 1998; Klein and Manning, 2003), and state-splitting (Matsuzaki et al., 2005; Petrov et al., 2006) are all designed to take coarse symbols like PP and decorate them with additional context. The underlying reason that such propagation is even needed is that PCFG parsers score trees based on local configurations only, and any information that is not threaded through the tree becomes inaccessible to the scoring function. There have been non-local approaches as well, such as tree-substitution parsers (Bod, 1993; Sima’an, 2000), neural net parsers (Henderson, 2003), and rerankers (Collins and Koo, 2005; Charniak and Johnson, 2005; Huang, 2008). These non-local approaches can actually go even further in enriching the grammar’s structural complexity by coupling larger domains in various ways, though their non-locality generally complicates inference. In this work, we instead try to minimize the structural complexity of the grammar by moving as much context as possible onto local surface features. We examine the position that grammars should not propagate any information that is available from surface s"
P14-1022,J92-4003,0,0.04521,"consistently head final. Structural contexts like those captured by parent annotation (Johnson, 1998) are more subtle. Parent annotation can capture, for instance, the difference in distribution in NPs that have S as a parent (that is, subjects) and NPs under VPs (objects). We try to capture some of this same intuition by introducing a feature on the length of a span. For instance, VPs embedded in NPs tend to be short, usually as embedded gerund phrases. Because constituents in the treebank can be quite long, we bin our length features into 8 buckets, of 1 Experiments with the Brown clusters (Brown et al., 1992) provided by Turian et al. (2010) in lieu of suffixes were not promising. Moreover, lowering this threshold did not improve performance. 231 PRN VP ( CEO of Enron ) said , “ Too bad , ” (XxX) x,“Xx,” VP → no VBP NNS VP VBP NNS no read messages in his inbox Figure 2: An example showing the utility of span context. The ambiguity about whether read is an adjective or a verb is resolved when we construct a VP and notice that the word proceeding it is unlikely. Figure 4: Computation of span shape features on two examples. Parentheticals, quotes, and other punctuation-heavy, short constituents benef"
P14-1022,P05-1022,0,0.0838469,"Klein and Manning, 2003), and state-splitting (Matsuzaki et al., 2005; Petrov et al., 2006) are all designed to take coarse symbols like PP and decorate them with additional context. The underlying reason that such propagation is even needed is that PCFG parsers score trees based on local configurations only, and any information that is not threaded through the tree becomes inaccessible to the scoring function. There have been non-local approaches as well, such as tree-substitution parsers (Bod, 1993; Sima’an, 2000), neural net parsers (Henderson, 2003), and rerankers (Collins and Koo, 2005; Charniak and Johnson, 2005; Huang, 2008). These non-local approaches can actually go even further in enriching the grammar’s structural complexity by coupling larger domains in various ways, though their non-locality generally complicates inference. In this work, we instead try to minimize the structural complexity of the grammar by moving as much context as possible onto local surface features. We examine the position that grammars should not propagate any information that is available from surface strings, since a discriminative parser can access that information directly. We therefore begin with a minimal grammar an"
P14-1022,J05-1003,0,0.0405921,"otation (Johnson, 1998; Klein and Manning, 2003), and state-splitting (Matsuzaki et al., 2005; Petrov et al., 2006) are all designed to take coarse symbols like PP and decorate them with additional context. The underlying reason that such propagation is even needed is that PCFG parsers score trees based on local configurations only, and any information that is not threaded through the tree becomes inaccessible to the scoring function. There have been non-local approaches as well, such as tree-substitution parsers (Bod, 1993; Sima’an, 2000), neural net parsers (Henderson, 2003), and rerankers (Collins and Koo, 2005; Charniak and Johnson, 2005; Huang, 2008). These non-local approaches can actually go even further in enriching the grammar’s structural complexity by coupling larger domains in various ways, though their non-locality generally complicates inference. In this work, we instead try to minimize the structural complexity of the grammar by moving as much context as possible onto local surface features. We examine the position that grammars should not propagate any information that is available from surface strings, since a discriminative parser can access that information directly. We therefore beg"
P14-1022,P97-1003,0,0.271872,"longer spans, we only use words sufficiently close to the span’s beginning and end. 232 Annotation v = 0, h = 0 v = 1, h = 0 v = 0, h = 1 v = 1, h = 1 Lexicalized Dev, len ≤ 40 90.1 90.5 90.2 90.9 90.3 Berkeley This work Test all 90.1 89.2 Table 3: Final Parseval results for the v = 1, h = 0 parser on Section 23 of the Penn Treebank. Table 2: Results for the Penn Treebank development set, sentences of length ≤ 40, for different annotation schemes implemented on top of the Xbar grammar. 5.2 Lexical Annotation Another commonly-used kind of structural annotation is lexicalization (Eisner, 1996; Collins, 1997; Charniak, 1997). By annotating grammar nonterminals with their headwords, the idea is to better model phenomena that depend heavily on the semantics of the words involved, such as coordination and PP attachment. Table 2 shows results from lexicalizing the Xbar grammar; it provides meager improvements. One probable reason for this is that our parser already includes monolexical features that inspect the first and last words of each span, which captures the syntactic or the semantic head in many cases or can otherwise provide information about what the constituent’s type may be and how it is l"
P14-1022,C96-1058,0,0.151043,"of one in2 For longer spans, we only use words sufficiently close to the span’s beginning and end. 232 Annotation v = 0, h = 0 v = 1, h = 0 v = 0, h = 1 v = 1, h = 1 Lexicalized Dev, len ≤ 40 90.1 90.5 90.2 90.9 90.3 Berkeley This work Test all 90.1 89.2 Table 3: Final Parseval results for the v = 1, h = 0 parser on Section 23 of the Penn Treebank. Table 2: Results for the Penn Treebank development set, sentences of length ≤ 40, for different annotation schemes implemented on top of the Xbar grammar. 5.2 Lexical Annotation Another commonly-used kind of structural annotation is lexicalization (Eisner, 1996; Collins, 1997; Charniak, 1997). By annotating grammar nonterminals with their headwords, the idea is to better model phenomena that depend heavily on the semantics of the words involved, such as coordination and PP attachment. Table 2 shows results from lexicalizing the Xbar grammar; it provides meager improvements. One probable reason for this is that our parser already includes monolexical features that inspect the first and last words of each span, which captures the syntactic or the semantic head in many cases or can otherwise provide information about what the constituent’s type may be"
P14-1022,P08-1109,0,0.0650238,"Missing"
P14-1022,W01-0521,0,0.0593252,"PP attachment. Table 2 shows results from lexicalizing the Xbar grammar; it provides meager improvements. One probable reason for this is that our parser already includes monolexical features that inspect the first and last words of each span, which captures the syntactic or the semantic head in many cases or can otherwise provide information about what the constituent’s type may be and how it is likely to combine. Lexicalization allows us to capture bilexical relationships along dependency arcs, but it has been previously shown that these add only marginal benefit to Collins’s model anyway (Gildea, 2001). Recall from Section 3 that every span feature is conjoined with indicators over rules and rule parents to produce features over anchored rule productions; when we consider adding an annotation layer to the grammar, what that does is refine the rule indicators that are conjoined with every span feature. While this is a powerful way of refining features, we show that common successful annotation schemes provide at best modest benefit on top of the base parser. 5.1 Test ≤ 40 90.6 89.9 Structural Annotation The most basic, well-understood kind of annotation on top of an X-bar grammar is structur"
P14-1022,D12-1105,1,0.819196,"across tasks to an extent. For example, Socher et al. (2013) demonstrates that sentiment analysis, which is usually approached as a flat classification task, can be viewed as tree-structured. In their work, they propagate real-valued vectors up a tree using neural tensor nets and see gains from their recursive approach. Our parser can be easily adapted to this task by replacing the X-bar grammar over treebank symbols with a grammar over the sentiment values to encode the output variables and then adding n-gram indicators to our feature set to capture the bulk of the lexical effects. When 229 Hall and Klein (2012) employed both kinds of annotations, along with lexicalized head word annotation. All of these past CRF parsers do also exploit span features, as did the structured margin parser of Taskar et al. (2004); the current work primarily differs in shifting the work from the grammar to the surface features. The problem with rich annotations is that they increase the state space of the grammar substantially. For example, adding parent annotation can square the number of symbols, and each subsequent annotation causes a multiplicative increase in the size of the state space. Hall and Klein (2012) attemp"
P14-1022,P08-1067,0,0.0124476,"nd state-splitting (Matsuzaki et al., 2005; Petrov et al., 2006) are all designed to take coarse symbols like PP and decorate them with additional context. The underlying reason that such propagation is even needed is that PCFG parsers score trees based on local configurations only, and any information that is not threaded through the tree becomes inaccessible to the scoring function. There have been non-local approaches as well, such as tree-substitution parsers (Bod, 1993; Sima’an, 2000), neural net parsers (Henderson, 2003), and rerankers (Collins and Koo, 2005; Charniak and Johnson, 2005; Huang, 2008). These non-local approaches can actually go even further in enriching the grammar’s structural complexity by coupling larger domains in various ways, though their non-locality generally complicates inference. In this work, we instead try to minimize the structural complexity of the grammar by moving as much context as possible onto local surface features. We examine the position that grammars should not propagate any information that is available from surface strings, since a discriminative parser can access that information directly. We therefore begin with a minimal grammar and iteratively"
P14-1022,P00-1008,0,0.0493641,"Missing"
P14-1022,J98-4004,0,0.0581003,"ble to us, namely, the identity of the first and last words of a span. Because heads of constituents are often at the beginning or the end of a span, these feature templates can (noisily) capture monolexical properties of heads without having to incur the inferential cost of lexicalized annotations. For example, in English, the syntactic head of a verb phrase is typically at the beginning of the span, while the head of a simple noun phrase is the last word. Other languages, like Korean or Japanese, are more consistently head final. Structural contexts like those captured by parent annotation (Johnson, 1998) are more subtle. Parent annotation can capture, for instance, the difference in distribution in NPs that have S as a parent (that is, subjects) and NPs under VPs (objects). We try to capture some of this same intuition by introducing a feature on the length of a span. For instance, VPs embedded in NPs tend to be short, usually as embedded gerund phrases. Because constituents in the treebank can be quite long, we bin our length features into 8 buckets, of 1 Experiments with the Brown clusters (Brown et al., 1992) provided by Turian et al. (2010) in lieu of suffixes were not promising. Moreover"
P14-1022,P03-1054,1,0.142646,"other annotations are less clearly beneficial. 5.3 English Evaluation Finally, Table 3 shows our final evaluation on Section 23 of the Penn Treebank. We use the v = 1, h = 0 grammar. While we do not do as well as the Berkeley parser, we will see in Section 6 that our parser does a substantially better job of generalizing to other languages. 6 Other Languages Historically, many annotation schemes for parsers have required language-specific engineering: for example, lexicalized parsers require a set of head rules and manually-annotated grammars require detailed analysis of the treebank itself (Klein and Manning, 2003). A key strength of a parser that does not rely heavily on an annotated grammar is that it may be more portable to other languages. We show that this is indeed the case: on nine languages, our system is competitive with or better than the Berkeley parser, which is the best single 3 We use v = 0 to indicate no annotation, diverging from the notation in Klein and Manning (2003). 233 Arabic Basque French German Berkeley Berkeley-Rep Our work 78.24 78.70 78.89 69.17 84.33 83.74 79.74 79.68 79.40 81.74 82.74 83.28 Berkeley Berkeley-Tags Our work 79.19 78.66 78.75 70.50 74.74 83.39 80.38 79.76 79.70"
P14-1022,D13-1170,0,0.376465,"s have surface reflexes, our system can still parse accurately with context-free backbones as minimal as Xbar grammars. Keeping the structural backbone simple and moving features to the surface also allows easy adaptation to new languages and even to new tasks. On the SPMRL 2013 multilingual constituency parsing shared task (Seddah et al., 2013), our system outperforms the top single parser system of Bj¨orkelund et al. (2013) on a range of languages. In addition, despite being designed for syntactic analysis, our system also achieves stateof-the-art numbers on the structural sentiment task of Socher et al. (2013). Finally, we show that, in both syntactic parsing and sentiment analysis, many broad linguistic trends can be captured via surface features. 1 Introduction Na¨ıve context-free grammars, such as those embodied by standard treebank annotations, do not parse well because their symbols have too little context to constrain their syntactic behavior. For example, to PPs usually attach to verbs and of PPs usually attach to nouns, but a context-free PP 228 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 228–237, c Baltimore, Maryland, USA, June 23-25 2014"
P14-1022,W04-3201,1,0.874709,"s inference. In this work, we instead try to minimize the structural complexity of the grammar by moving as much context as possible onto local surface features. We examine the position that grammars should not propagate any information that is available from surface strings, since a discriminative parser can access that information directly. We therefore begin with a minimal grammar and iteratively augment it with rich input features that do not enrich the context-free backbone. Previous work has also used surface features in their parsers, but the focus has been on machine learning methods (Taskar et al., 2004), latent annotations (Petrov and Klein, 2008a; Petrov and Klein, 2008b), or implementation (Finkel et al., 2008). By contrast, we investigate the extent to which We present a parser that relies primarily on extracting information directly from surface spans rather than on propagating information through enriched grammar structure. For example, instead of creating separate grammar symbols to mark the definiteness of an NP, our parser might instead capture the same information from the first word of the NP. Moving context out of the grammar and onto surface features can greatly simplify the stru"
P14-1022,P05-1010,0,0.142664,"Missing"
P14-1022,N03-1033,1,0.0577354,"Linguistics applied to this task, our system generally matches their accuracy overall and is able to outperform it on the overall sentence-level subtask. we need a grammar at all. As a thought experiment, consider a parser with no grammar, which functions by independently classifying each span (i, j) of a sentence as an NP, VP, and so on, or null if that span is a non-constituent. For example, spans that begin with the might tend to be NPs, while spans that end with of might tend to be non-constituents. An independent classification approach is actually very viable for part-of-speech tagging (Toutanova et al., 2003), but is problematic for parsing – if nothing else, parsing comes with a structural requirement that the output be a well-formed, nested tree. Our parser uses a minimal PCFG backbone grammar to ensure a basic level of structural well-formedness, but relies mostly on features of surface spans to drive accuracy. Formally, our model is a CRF where the features factor over anchored rules of a small backbone grammar, as shown in Figure 1. 2 Parsing Model In order to exploit non-independent surface features of the input, we use a discriminative formulation. Our model is a conditional random field (L"
P14-1022,P05-1015,0,0.0881148,"nford Sentiment Treebank which shows the utility of our span features for this task. The presence of “While” under this kind of rule tells us that the sentiment of the constituent to the right dominates the sentiment to the left. 7 Adapting to Sentiment Sentiment Analysis Finally, because the system is, at its core, a classifier of spans, it can be used equally well for tasks that do not normally use parsing algorithms. One example is sentiment analysis. While approaches to sentiment analysis often simply classify the sentence monolithically, treating it as a bag of ngrams (Pang et al., 2002; Pang and Lee, 2005; Wang and Manning, 2012), the recent dataset of Socher et al. (2013) imposes a layer of structure on the problem that we can exploit. They annotate every constituent in a number of training trees with an integer sentiment value from 1 (very negative) to 5 (very positive), opening the door for models such as ours to learn how syntax can structurally affect sentiment.7 Figure 5 shows an example that requires some analysis of sentence structure to correctly understand. The first constituent conveys positive sentiment with never lethargic and the second conveys negative sentiment with hindered, b"
P14-1022,W02-1011,0,0.0141368,"ntence from the Stanford Sentiment Treebank which shows the utility of our span features for this task. The presence of “While” under this kind of rule tells us that the sentiment of the constituent to the right dominates the sentiment to the left. 7 Adapting to Sentiment Sentiment Analysis Finally, because the system is, at its core, a classifier of spans, it can be used equally well for tasks that do not normally use parsing algorithms. One example is sentiment analysis. While approaches to sentiment analysis often simply classify the sentence monolithically, treating it as a bag of ngrams (Pang et al., 2002; Pang and Lee, 2005; Wang and Manning, 2012), the recent dataset of Socher et al. (2013) imposes a layer of structure on the problem that we can exploit. They annotate every constituent in a number of training trees with an integer sentiment value from 1 (very negative) to 5 (very positive), opening the door for models such as ours to learn how syntax can structurally affect sentiment.7 Figure 5 shows an example that requires some analysis of sentence structure to correctly understand. The first constituent conveys positive sentiment with never lethargic and the second conveys negative sentim"
P14-1022,P10-1040,0,0.0551624,"ral contexts like those captured by parent annotation (Johnson, 1998) are more subtle. Parent annotation can capture, for instance, the difference in distribution in NPs that have S as a parent (that is, subjects) and NPs under VPs (objects). We try to capture some of this same intuition by introducing a feature on the length of a span. For instance, VPs embedded in NPs tend to be short, usually as embedded gerund phrases. Because constituents in the treebank can be quite long, we bin our length features into 8 buckets, of 1 Experiments with the Brown clusters (Brown et al., 1992) provided by Turian et al. (2010) in lieu of suffixes were not promising. Moreover, lowering this threshold did not improve performance. 231 PRN VP ( CEO of Enron ) said , “ Too bad , ” (XxX) x,“Xx,” VP → no VBP NNS VP VBP NNS no read messages in his inbox Figure 2: An example showing the utility of span context. The ambiguity about whether read is an adjective or a verb is resolved when we construct a VP and notice that the word proceeding it is unlikely. Figure 4: Computation of span shape features on two examples. Parentheticals, quotes, and other punctuation-heavy, short constituents benefit from being explicitly modeled"
P14-1022,N07-1051,1,0.929923,"elopment set because neither the system nor test set values are publicly available. Berkeley-Tags is a version of the Berkeley parser run by the task organizers where tags are provided to the model, and is the best single parser submitted to the official task. In both cases, we match or outperform the baseline parsers in aggregate and on the majority of individual languages. parser4 for the majority of cases we consider. We evaluate on the constituency treebanks from the Statistical Parsing of Morphologically Rich Languages Shared Task (Seddah et al., 2013). We compare to the Berkeley parser (Petrov and Klein, 2007) as well as two variants. First, we use the “Replaced” system of Bj¨orkelund et al. (2013) (Berkeley-Rep), which is their best single parser.5 The “Replaced” system modifies the Berkeley parser by replacing rare words with morphological descriptors of those words computed using language-specific modules, which have been hand-crafted for individual languages or are trained with additional annotation layers in the treebanks that we do not exploit. Unfortunately, Bj¨orkelund et al. (2013) only report results on the development set for the Berkeley-Rep model; however, the task organizers also use"
P14-1022,P12-2018,0,0.0458306,"ll find that parent annotation is effective and otherwise additional annotation layers are not useful. One structural difference between sentiment analysis and syntactic parsing lies in where the relevant information is present in a span. Syntax is often driven by heads of constituents, which tend to be located at the beginning or the end, whereas sentiment is more likely to depend on modifiers such as adjectives, which are typically present in the middle of spans. Therefore, we augment our existing model with standard sentiment analysis features that look at unigrams and bigrams in the span (Wang and Manning, 2012). Moreover, the Stanford Sentiment Treebank is unique in that each constituent was annotated in isolation, meaning that context never affects sentiment and that every word always has the same tag. We exploit this by adding an additional feature template similar to our span shape feature from Section 4.4 which uses the (deterministic) tag for each word as its descriptor. 2 4 1 While “ Gangs ” is never lethargic , it is hindered by its plot . Figure 5: An example of a sentence from the Stanford Sentiment Treebank which shows the utility of our span features for this task. The presence of “While”"
P14-1022,D08-1091,1,0.84872,"to minimize the structural complexity of the grammar by moving as much context as possible onto local surface features. We examine the position that grammars should not propagate any information that is available from surface strings, since a discriminative parser can access that information directly. We therefore begin with a minimal grammar and iteratively augment it with rich input features that do not enrich the context-free backbone. Previous work has also used surface features in their parsers, but the focus has been on machine learning methods (Taskar et al., 2004), latent annotations (Petrov and Klein, 2008a; Petrov and Klein, 2008b), or implementation (Finkel et al., 2008). By contrast, we investigate the extent to which We present a parser that relies primarily on extracting information directly from surface spans rather than on propagating information through enriched grammar structure. For example, instead of creating separate grammar symbols to mark the definiteness of an NP, our parser might instead capture the same information from the first word of the NP. Moving context out of the grammar and onto surface features can greatly simplify the structural component of the parser: because so m"
P14-1022,P06-1055,1,0.750388,"Missing"
P14-1022,W13-4917,0,0.0218132,"Missing"
P14-1022,N03-1014,0,\N,Missing
P15-1030,W13-4916,0,0.0441349,"Missing"
P15-1030,W14-6110,0,0.198064,"Missing"
P15-1030,P05-1022,0,0.430767,"Missing"
P15-1030,W08-2102,0,0.0281083,"s in a conventional CRF, with the gradient of the network parameters naturally computed by backpropagating a difference of expected anchored rule counts through the network for each span and split point. Using dense learned features alone, the neural CRF model obtains high performance, outperforming the CRF parser of Hall et al. (2014). When sparse indicators are used in addition, the resulting model gets 91.1 F1 on section 23 of the Penn Treebank, outperforming the parser of Socher et al. (2013) as well as the Berkeley Parser (Petrov and Klein, 2007) and matching the discriminative parser of Carreras et al. (2008). The model also obtains the best single parser results on nine other languages, again outperforming the system of Hall et al. (2014). 2 PP j reflected the flip side of the Stoltzman personality . DT NNP VBZ NP The Fed issued … h r = NP ! NP PP 2.1 Anchored Rules The fundamental units that our parsing models consider are anchored rules. As shown in Figure 2, we define an anchored rule as a tuple (r, s), where r is an indicator of the rule’s identity and s = (i, j, k) indicates the span (i, k) and split point j of the rule.3 A tree T is simply a collection of anchored rules subject to the const"
P15-1030,D14-1082,0,0.690947,"split point. As input, it takes vector representations of words at the split point and span boundaries; it then outputs scores for anchored rules applied to that span and split point. These scores can be thought of as nonlinear potentials analogous to linear potentials in conventional CRFs. Crucially, while the network replicates are connected in a unified model, their computations factor along the same substructures as in standard CRFs. Prior work on parsing using neural network models has often sidestepped the problem of structured inference by making sequential decisions (Henderson, 2003; Chen and Manning, 2014; Tsuboi, 2014) or by doing reranking (Socher et al., 2013; Le and Zuidema, 2014); by contrast, our framework permits exact inference via CKY, since the model’s structured interactions are purely discrete and do not involve continuous hidden state. Therefore, we can exploit a neural net’s capacity to learn nonlinear features without modifying This paper describes a parsing model that combines the exact dynamic programming of CRF parsing with the rich nonlinear featurization of neural net approaches. Our model is structurally a CRF that factors over anchored rule productions, but instead of lin"
P15-1030,C14-1078,0,0.017442,"ters and a single feature extractor f that jointly inspects the surface and the rule. However, when the feature representation conjoins each rule r with surface properties of the sentence in a systematic way (an assumption that holds in our case as well as for standard CRF models for POS tagging and NER), this is equivalent to our formalism. 5 Embedding words allows us to use standard pre-trained vectors more easily and tying embeddings across word positions substantially reduces the number of model parameters. However, embedding features rather than words has also been shown to be effective (Chen et al., 2014). 304 more discriminating power. Also note that it is possible to use deeper networks or more sophisticated architectures here; we will return to this in Section 6. Our two models can be easily combined: increases the ability to overfit. Following Hall et al. (2014), we use grammars with very little annotation: we use no horizontal Markovization for any of experiments, and all of our English experiments with the neural CRF use no vertical Markovization (V = 0). This also has the benefit of making the system much faster, due to the smaller state space for dynamic programming. We do find that us"
P15-1030,P14-2133,1,0.856453,"continuous word representations themselves, we also experimented with vectors trained on just the text from the training set of the Penn Treebank using the skip-gram model with a window size of 1. While these vectors are somewhat lower performing on their own (f), they still provide a surprising and noticeable gain when stacked on top of sparse features (h), again suggesting that dense and sparse representations have complementary strengths. This result also reinforces the notion that the utility of word vectors does not come primarily from importing information about out-of-vocabulary words (Andreas and Klein, 2014). Since the neural features incorporate information from unlabeled data, we should provide the 6 Design Choices The neural net design space is large, so we wish to analyze the particular design choices we made for this system by examining the performance of several variants of the neural net architecture used in our system. Table 2 shows development results from potential alternate architectural choices, which we now discuss. Choice of nonlinearity The choice of nonlinearity g has been frequently discussed in the neural network literature. Our choice g(x) = max(x, 0), a rectified linear unit,"
P15-1030,P14-2131,0,0.174074,", and split point of the anchored rule (as shown in Figure 2) as well as on two other span properties, span length and span shape (an indicator of where capitalized words, numbers, and punctuation occur in the span). For our neural model, we take fw for all productions (preterminal and nonterminal) to be the words surrounding the beginning and end of a span and the split point, as shown in Figure 2; in particular, we look two words in either direction around each point of interest, meaning the neural net takes 12 words as input.7 For our word embeddings v, we use pre-trained word vectors from Bansal et al. (2014). We compare with other sources of word vectors in Section 5. Contrary to standard practice, we do not update these vectors during training; we found that doing so did not provide an accuracy benefit and slowed down training considerably. 2.4 Learning L(H, W ) = D X log P (Ti∗ |wi ; H, W ) i=1 Because we are using rectified linear units as our nonlinearity, our objective is not everywhere differentiable. The interaction of the parameters and the nonlinearity also makes the objective nonconvex. However, in spite of this, we can still follow subgradients to optimize this objective, as is standar"
P15-1030,Q14-1043,0,0.0115935,"tures, our system1 achieves 91.1 F1 on section 23 of the Penn Treebank, and more generally outperforms the best prior single parser results on a range of languages. 1 Introduction Neural network-based approaches to structured NLP tasks have both strengths and weaknesses when compared to more conventional models, such conditional random fields (CRFs). A key strength of neural approaches is their ability to learn nonlinear interactions between underlying features. In the case of unstructured output spaces, this capability has led to gains in problems ranging from syntax (Chen and Manning, 2014; Belinkov et al., 2014) to lexical semantics (Kalchbrenner et al., 2014; Kim, 2014). Neural methods are also powerful tools in the case of structured 1 System available at http://nlp.cs.berkeley.edu 302 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 302–312, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics NP S NP NP VP i fo W fw v(fw ) Structured inference (discrete) fw k reflected the side of personality . v(fw ) Feature extraction (continuous) fs [[PreviousWor"
P15-1030,P08-1109,0,0.133233,"Missing"
P15-1030,N07-1051,1,0.899028,"arse model. Our model can be trained by gradient descent exactly as in a conventional CRF, with the gradient of the network parameters naturally computed by backpropagating a difference of expected anchored rule counts through the network for each span and split point. Using dense learned features alone, the neural CRF model obtains high performance, outperforming the CRF parser of Hall et al. (2014). When sparse indicators are used in addition, the resulting model gets 91.1 F1 on section 23 of the Penn Treebank, outperforming the parser of Socher et al. (2013) as well as the Berkeley Parser (Petrov and Klein, 2007) and matching the discriminative parser of Carreras et al. (2008). The model also obtains the best single parser results on nine other languages, again outperforming the system of Hall et al. (2014). 2 PP j reflected the flip side of the Stoltzman personality . DT NNP VBZ NP The Fed issued … h r = NP ! NP PP 2.1 Anchored Rules The fundamental units that our parsing models consider are anchored rules. As shown in Figure 2, we define an anchored rule as a tuple (r, s), where r is an indicator of the rule’s identity and s = (i, j, k) indicates the span (i, k) and split point j of the rule.3 A tre"
P15-1030,P14-1022,1,0.115909,"zation of neural net approaches. Our model is structurally a CRF that factors over anchored rule productions, but instead of linear potential functions based on sparse features, we use nonlinear potentials computed via a feedforward neural network. Because potentials are still local to anchored rules, structured inference (CKY) is unchanged from the sparse case. Computing gradients during learning involves backpropagating an error signal formed from standard CRF sufficient statistics (expected rule counts). Using only dense features, our neural CRF already exceeds a strong baseline CRF model (Hall et al., 2014). In combination with sparse features, our system1 achieves 91.1 F1 on section 23 of the Penn Treebank, and more generally outperforms the best prior single parser results on a range of languages. 1 Introduction Neural network-based approaches to structured NLP tasks have both strengths and weaknesses when compared to more conventional models, such conditional random fields (CRFs). A key strength of neural approaches is their ability to learn nonlinear interactions between underlying features. In the case of unstructured output spaces, this capability has led to gains in problems ranging from"
P15-1030,N03-1014,0,0.25862,"Missing"
P15-1030,D08-1091,1,0.843811,"kes the standard form of log-linear models:   X ∂L = h(w, s; H)fo (r)>  − ∂W (r,s)∈T ∗   X X  P (T |w; H, W ) h(w, s; H)fo (r)>  T (r,s)∈T Note that the outer products give matrices of feature counts isomorphic to W . The second expression can be simplified to be in terms of expected feature counts. To update H, we use standard backpropagation by first computing:   X ∂L  = W fo (r) − ∂h ∗ (r,s)∈T   X X  P (T |w; H, W ) W fo (r) Grammar Refinements A recurring issue in discriminative constituency parsing is the granularity of annotation in the base grammar (Finkel et al., 2008; Petrov and Klein, 2008; Hall et al., 2014). Using finer-grained symbols in our rules r gives the model greater capacity, but also introduces more parameters into W and 6 The model actually uses the longest suffix of each word occurring at least 100 times in the training set, up to the entire word. Removing this abstraction of rare words harms performance. 7 The sparse model did not benefit from using this larger neighborhood, so improvements from the neural net are not simply due to considering more lexical context. T (r,s)∈T Since h is the output of the neural network, we can then apply the chain rule to compute g"
P15-1030,N10-1003,0,0.0196019,"unlabeled data. We further compare to the shiftreduce parser of Zhu et al. (2013), which uses unlabeled data in the form of Brown clusters. Our method achieves performance close to that of their parser. We also compare to the compositional vector grammar (CVG) parser of Socher et al. (2013) as well as the LSTM-based parser of Vinyals et al. (2014). The conditions these parsers are operating under are slightly different: the former is a reranker on top of the Stanford Parser (Klein and Manning, 2003) and the latter trains on much larger amounts of data parsed by a product of Berkeley parsers (Petrov, 2010). Regardless, we outperform the CVG parser as well as the single parser results from Vinyals et al. (2014). F1 all Single model, PTB only Hall et al. (2014) Berkeley Carreras et al. (2008) Shindo et al. (2012) single 89.2 90.1 91.1 91.1 Single model, PTB + vectors/clusters Zhu et al. (2013) This work* 91.3 91.1 Extended conditions Charniak and Johnson (2005) Socher et al. (2013) Vinyals et al. (2014) single Vinyals et al. (2014) ensemble Shindo et al. (2012) ensemble 91.5 90.4 90.5 91.6 92.4 Table 4: Test results on section 23 of the Penn Treebank. We compare to several categories of parsers f"
P15-1030,D14-1080,0,0.0135229,"Missing"
P15-1030,W13-4917,0,0.0183787,"Missing"
P15-1030,P14-1062,0,0.0135426,"n 23 of the Penn Treebank, and more generally outperforms the best prior single parser results on a range of languages. 1 Introduction Neural network-based approaches to structured NLP tasks have both strengths and weaknesses when compared to more conventional models, such conditional random fields (CRFs). A key strength of neural approaches is their ability to learn nonlinear interactions between underlying features. In the case of unstructured output spaces, this capability has led to gains in problems ranging from syntax (Chen and Manning, 2014; Belinkov et al., 2014) to lexical semantics (Kalchbrenner et al., 2014; Kim, 2014). Neural methods are also powerful tools in the case of structured 1 System available at http://nlp.cs.berkeley.edu 302 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 302–312, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics NP S NP NP VP i fo W fw v(fw ) Structured inference (discrete) fw k reflected the side of personality . v(fw ) Feature extraction (continuous) fs [[PreviousWord = reflected]], [[SpanLength = 7]], … Figure 1:"
P15-1030,D14-1181,0,0.0886277,"and more generally outperforms the best prior single parser results on a range of languages. 1 Introduction Neural network-based approaches to structured NLP tasks have both strengths and weaknesses when compared to more conventional models, such conditional random fields (CRFs). A key strength of neural approaches is their ability to learn nonlinear interactions between underlying features. In the case of unstructured output spaces, this capability has led to gains in problems ranging from syntax (Chen and Manning, 2014; Belinkov et al., 2014) to lexical semantics (Kalchbrenner et al., 2014; Kim, 2014). Neural methods are also powerful tools in the case of structured 1 System available at http://nlp.cs.berkeley.edu 302 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 302–312, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics NP S NP NP VP i fo W fw v(fw ) Structured inference (discrete) fw k reflected the side of personality . v(fw ) Feature extraction (continuous) fs [[PreviousWord = reflected]], [[SpanLength = 7]], … Figure 1: Neural CRF"
P15-1030,P03-1054,1,0.0651747,"ly, single parser data condition; we match their performance at 91.1 F1 , though we also use word vectors computed from unlabeled data. We further compare to the shiftreduce parser of Zhu et al. (2013), which uses unlabeled data in the form of Brown clusters. Our method achieves performance close to that of their parser. We also compare to the compositional vector grammar (CVG) parser of Socher et al. (2013) as well as the LSTM-based parser of Vinyals et al. (2014). The conditions these parsers are operating under are slightly different: the former is a reranker on top of the Stanford Parser (Klein and Manning, 2003) and the latter trains on much larger amounts of data parsed by a product of Berkeley parsers (Petrov, 2010). Regardless, we outperform the CVG parser as well as the single parser results from Vinyals et al. (2014). F1 all Single model, PTB only Hall et al. (2014) Berkeley Carreras et al. (2008) Shindo et al. (2012) single 89.2 90.1 91.1 91.1 Single model, PTB + vectors/clusters Zhu et al. (2013) This work* 91.3 91.1 Extended conditions Charniak and Johnson (2005) Socher et al. (2013) Vinyals et al. (2014) single Vinyals et al. (2014) ensemble Shindo et al. (2012) ensemble 91.5 90.4 90.5 91.6"
P15-1030,W14-6111,0,0.0334686,"Missing"
P15-1030,P08-1068,0,0.0446488,"lusters (c), and even word representations learned just on the Penn Treebank are surprisingly effective (f, h). F1 len ≤ 40 Table 2: Exploration of other implementation choices in the feedforward neural network on sentences of length ≤ 40 from section 22 of the Penn Treebank. Rectified linear units perform better than tanh or cubic units, a network with one hidden layer performs best, and embedding the output feature vector gives worse performance. sparse model with similar information for a true apples-to-apples comparison. Brown clusters have been shown to be effective vehicles in the past (Koo et al., 2008; Turian et al., 2010; Bansal et al., 2014). We can incorporate Brown clusters into the baseline CRF model in an analogous way to how embedding features are used in the dense model: surface features are fired on Brown cluster identities (we use prefixes of length 4 and 10) of key words. We use the Brown clusters from Koo et al. (2008), which are trained on the same data as the vectors of Bansal et al. (2014). However, Table 1 shows that these features provide no benefit to the baseline model, which suggests either that it is difficult to learn reliable weights for these as sparse features or t"
P15-1030,P12-1046,0,0.498905,"in the SPMRL 2013/2014 Shared Tasks; all values are F-scores for sentences of all lengths using the version of evalb distributed with the shared task. Our parser substantially outperforms the strongest single parser results on this dataset (Hall et al., 2014; Crabb´e and Seddah, 2014). Berkeley-Tags is an improved version of the Berkeley parser designed for the shared task (Seddah et al., 2013). 2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the best published numbers on this dataset (Bj¨orkelund et al., 2013; Bj¨orkelund et al., 2014). the single TSG parser of Shindo et al. (2012). To our knowledge, the latter two systems are the highest performing in this PTB-only, single parser data condition; we match their performance at 91.1 F1 , though we also use word vectors computed from unlabeled data. We further compare to the shiftreduce parser of Zhu et al. (2013), which uses unlabeled data in the form of Brown clusters. Our method achieves performance close to that of their parser. We also compare to the compositional vector grammar (CVG) parser of Socher et al. (2013) as well as the LSTM-based parser of Vinyals et al. (2014). The conditions these parsers are operating un"
P15-1030,P13-1045,0,0.749596,"rds at the split point and span boundaries; it then outputs scores for anchored rules applied to that span and split point. These scores can be thought of as nonlinear potentials analogous to linear potentials in conventional CRFs. Crucially, while the network replicates are connected in a unified model, their computations factor along the same substructures as in standard CRFs. Prior work on parsing using neural network models has often sidestepped the problem of structured inference by making sequential decisions (Henderson, 2003; Chen and Manning, 2014; Tsuboi, 2014) or by doing reranking (Socher et al., 2013; Le and Zuidema, 2014); by contrast, our framework permits exact inference via CKY, since the model’s structured interactions are purely discrete and do not involve continuous hidden state. Therefore, we can exploit a neural net’s capacity to learn nonlinear features without modifying This paper describes a parsing model that combines the exact dynamic programming of CRF parsing with the rich nonlinear featurization of neural net approaches. Our model is structurally a CRF that factors over anchored rule productions, but instead of linear potential functions based on sparse features, we use n"
P15-1030,D14-1081,0,0.0210564,"pan boundaries; it then outputs scores for anchored rules applied to that span and split point. These scores can be thought of as nonlinear potentials analogous to linear potentials in conventional CRFs. Crucially, while the network replicates are connected in a unified model, their computations factor along the same substructures as in standard CRFs. Prior work on parsing using neural network models has often sidestepped the problem of structured inference by making sequential decisions (Henderson, 2003; Chen and Manning, 2014; Tsuboi, 2014) or by doing reranking (Socher et al., 2013; Le and Zuidema, 2014); by contrast, our framework permits exact inference via CKY, since the model’s structured interactions are purely discrete and do not involve continuous hidden state. Therefore, we can exploit a neural net’s capacity to learn nonlinear features without modifying This paper describes a parsing model that combines the exact dynamic programming of CRF parsing with the rich nonlinear featurization of neural net approaches. Our model is structurally a CRF that factors over anchored rule productions, but instead of linear potential functions based on sparse features, we use nonlinear potentials com"
P15-1030,P14-1130,0,0.0178065,"ing substantially over linear features of the continuous input. We can use the embedding vector of an anchored span v(fw ) directly as input to a basic linear CRF, as shown in Figure 4a. Table 1 shows that the purely linear architecture (0 HL) performs surprisingly well, but is still less effective than the network with one hidden layer. This agrees with the results of Wang and Manning (2013), who noted that dense features typically benefit from nonlinear modeling. We also compare against a two-layer neural network, but find that this also performs worse than the one-layer architecture. work (Lei et al., 2014). This approach saves us from having to learn a separate row of W for every rule in the grammar; if rules are given similar embeddings, then they will behave similarly according to the model. We experimented with noe = 20 and show the results in Table 2. Unfortunately, this approach does not seem to work well for parsing. Learning the output representation was empirically very unstable, and it also required careful initialization. We tried Gaussian initialization (as in the rest of our model) and initializing the model by clustering rules either randomly or according to their parent symbol. Th"
P15-1030,D14-1101,0,0.00492609,"it takes vector representations of words at the split point and span boundaries; it then outputs scores for anchored rules applied to that span and split point. These scores can be thought of as nonlinear potentials analogous to linear potentials in conventional CRFs. Crucially, while the network replicates are connected in a unified model, their computations factor along the same substructures as in standard CRFs. Prior work on parsing using neural network models has often sidestepped the problem of structured inference by making sequential decisions (Henderson, 2003; Chen and Manning, 2014; Tsuboi, 2014) or by doing reranking (Socher et al., 2013; Le and Zuidema, 2014); by contrast, our framework permits exact inference via CKY, since the model’s structured interactions are purely discrete and do not involve continuous hidden state. Therefore, we can exploit a neural net’s capacity to learn nonlinear features without modifying This paper describes a parsing model that combines the exact dynamic programming of CRF parsing with the rich nonlinear featurization of neural net approaches. Our model is structurally a CRF that factors over anchored rule productions, but instead of linear potential f"
P15-1030,P14-2050,0,0.00645301,"hology for predicting tags of unknown words, which typically have regular inflection patterns. By contrast, the neural model must rely on the geometry of the vector space exposing useful regularities. At the same time, the strong performance of the combination of the two systems (g) indicates that not only are both featurization approaches highperforming on their own, but that they have complementary strengths. Unlabeled data Much attention has been paid to the choice of word vectors for various NLP tasks, notably whether they capture more syntactic or semantic phenomena (Bansal et al., 2014; Levy and Goldberg, 2014). We primarily use vectors from Bansal et al. (2014), who train the skipgram model of Mikolov et al. (2013) using contexts from dependency links; a similar approach was also suggested by Levy and Goldberg (2014). 8 One reason we did not choose to include the rule identity fo as an input to the network is that it requires computing an even larger number of network activations, since we cannot reuse them across rules over the same span and split point. 306 Sparse Neural V a b c Hall et al. (2014), V = 1 90.5 X X X 89.22 90.13 90.17 d e f g h Word Reps F1 len ≤ 40 F1 all X X 0 1 1 Brown 89.89 90."
P15-1030,P10-1040,0,0.0103985,"ven word representations learned just on the Penn Treebank are surprisingly effective (f, h). F1 len ≤ 40 Table 2: Exploration of other implementation choices in the feedforward neural network on sentences of length ≤ 40 from section 22 of the Penn Treebank. Rectified linear units perform better than tanh or cubic units, a network with one hidden layer performs best, and embedding the output feature vector gives worse performance. sparse model with similar information for a true apples-to-apples comparison. Brown clusters have been shown to be effective vehicles in the past (Koo et al., 2008; Turian et al., 2010; Bansal et al., 2014). We can incorporate Brown clusters into the baseline CRF model in an analogous way to how embedding features are used in the dense model: surface features are fired on Brown cluster identities (we use prefixes of length 4 and 10) of key words. We use the Brown clusters from Koo et al. (2008), which are trained on the same data as the vectors of Bansal et al. (2014). However, Table 1 shows that these features provide no benefit to the baseline model, which suggests either that it is difficult to learn reliable weights for these as sparse features or that different regular"
P15-1030,J93-2004,0,0.0507879,"important. 4 expensive. However, because only a small number of rules can apply to a given span and split point, fo is sparse and we can selectively compute the terms necessary for the final bilinear product. Our combined sparse and neural model trains on the Penn Treebank in 24 hours on a single machine with a parallelized CPU implementation. For reference, the purely sparse model with a parentannotated grammar (necessary for the best results) takes around 15 hours on the same machine. 5 System Ablations Table 1 shows results on section 22 (the development set) of the English Penn Treebank (Marcus et al., 1993), computed using evalb. Full test results and comparisons to other systems are shown in Table 4. We compare variants of our system along two axes: whether they use standard linear sparse features, nonlinear dense features from the neural net, or both, and whether any word representations (vectors or clusters) are used. Inference Our baseline and neural model both score anchored rule productions. We can use CKY in the standard fashion to compute either expected anchored rule counts EP (T |w) [(r, s)] or the Viterbi tree arg maxT P (T |w). We speed up inference by using a coarse pruning pass. We"
P15-1030,I13-1183,0,0.114938,"RFs that decompose over anchored rule productions and place a probability distribution over trees conditioned on a sentence w as follows:   X P (T |w) ∝ exp  φ(w, r, s) (r,s)∈T 2 Throughout this work, we will primarily consider two potential functions: linear functions of sparse indicators and nonlinear neural networks over dense, continuous features. Although other modeling choices are possible, these two points in the design space reflect common choices in NLP, and past work has suggested that nonlinear functions of indicators or linear functions of dense features may perform less well (Wang and Manning, 2013). 3 For simplicity of exposition, we ignore unary rules; however, they are easily supported in this framework by simply specifying a null value for the split point. Model Figure 1 shows our neural CRF model. The model decomposes over anchored rules, and it scores each of these with a potential function; in a standard CRF, these potentials are typically linear functions of sparse indicator features, whereas 303 where φ is a scoring function that considers the input sentence and the anchored rule in question. Figure 1 shows this scoring process schematically. As we will see, the module on the le"
P15-1030,P13-1043,0,0.0428738,"Berkeley-Tags is an improved version of the Berkeley parser designed for the shared task (Seddah et al., 2013). 2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the best published numbers on this dataset (Bj¨orkelund et al., 2013; Bj¨orkelund et al., 2014). the single TSG parser of Shindo et al. (2012). To our knowledge, the latter two systems are the highest performing in this PTB-only, single parser data condition; we match their performance at 91.1 F1 , though we also use word vectors computed from unlabeled data. We further compare to the shiftreduce parser of Zhu et al. (2013), which uses unlabeled data in the form of Brown clusters. Our method achieves performance close to that of their parser. We also compare to the compositional vector grammar (CVG) parser of Socher et al. (2013) as well as the LSTM-based parser of Vinyals et al. (2014). The conditions these parsers are operating under are slightly different: the former is a reranker on top of the Stanford Parser (Klein and Manning, 2003) and the latter trains on much larger amounts of data parsed by a product of Berkeley parsers (Petrov, 2010). Regardless, we outperform the CVG parser as well as the single pars"
P16-1188,P13-1020,0,0.0223492,"its antecedent. These pronoun rewrites are scored in the objective and introduced into the length constraint to make sure they do not cause our summary to be too long. Finally, constraints on these variables control when they are used and also require the model to include antecedents of pronouns when the model is not confident enough to rewrite them. 2.1 Grammaticality Constraints Following work on isolated sentence compression (McDonald, 2006; Clarke and Lapata, 2008) and compressive summarization (Lin, 2003; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013), we wish to be able to compress sentences so we can pack more information into a summary. During training, our model learns how to take advantage of available compression options and select content to match human generated summaries as closely possible.2 We explore two ways of deriving units for compression: the RST-based compressions of Hirao et al. (2013) and the syntactic compressions of Berg-Kirkpatrick et al. (2011). RST compressions Figure 2a shows how to derive compressions from Rhetorical Structure Theory (Mann and Thompson, 1988; Carlson et al., 2001). We show a sentence broken into"
P16-1188,J08-1001,0,0.0195037,"ROUGE score than either method. These results indicate that our model has the expressive capacity to extract important content, but is sufficiently constrained to ensure fluency is not sacrificed as a result. Past work has explored various kinds of structure for summarization. Some work has focused on improving content selection using discourse structure (Louis et al., 2010; Hirao et al., 2013), topical structure (Barzilay and Lee, 2004), or related techniques (Mithun and Kosseim, 2011). Other work has used structure primarily to reorder summaries and ensure coherence (Barzilay et al., 2001; Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Christensen et al., 2013) or to represent content for sentence fusion or abstraction (Thadani and McKeown, 2013; Pighin et al., 2014). Similar to these approaches, we appeal to structures from upstream NLP tasks (syntactic parsing, RST parsing, and coreference) to restrict our model’s capacity to generate. However, we go further by optimizing for ROUGE subject to these constraints with end-to-end learning. 2 Model Our model is shown in Figure 1. Broadly, our ILP takes a set of textual units u = (u1 , . . . , un ) from a document and finds the highest-scoring extracti"
P16-1188,N04-1015,0,0.0350411,"bjective and are incorporated into the length constraint. Yoshida et al. (2014) and approaching the clarity of a sentence-extractive baseline—and still achieves substantially higher ROUGE score than either method. These results indicate that our model has the expressive capacity to extract important content, but is sufficiently constrained to ensure fluency is not sacrificed as a result. Past work has explored various kinds of structure for summarization. Some work has focused on improving content selection using discourse structure (Louis et al., 2010; Hirao et al., 2013), topical structure (Barzilay and Lee, 2004), or related techniques (Mithun and Kosseim, 2011). Other work has used structure primarily to reorder summaries and ensure coherence (Barzilay et al., 2001; Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Christensen et al., 2013) or to represent content for sentence fusion or abstraction (Thadani and McKeown, 2013; Pighin et al., 2014). Similar to these approaches, we appeal to structures from upstream NLP tasks (syntactic parsing, RST parsing, and coreference) to restrict our model’s capacity to generate. However, we go further by optimizing for ROUGE subject to these constraints with e"
P16-1188,H01-1065,0,0.162869,"Missing"
P16-1188,P11-1049,1,0.934644,"damental units of extraction in our model. For a sentence-extractive model, these would be entire sentences, but for our compressive models we will have more fine-grained units, as shown in Figure 2 and described in Section 2.1. Textual units are scored according to features f and model parameters w learned on training data. Finally, the extraction process is subject to a length constraint of k words. This approach is similar in spirit to ILP formulations of multi-document summarization systems, though in those systems content is typically modeled in terms of bigrams (Gillick and Favre, 2009; Berg-Kirkpatrick et al., 2011; Hong and Nenkova, 2014; Li et al., 2015). For our model, type-level n-gram scoring only arises when we compute our loss function in maxmargin training (see Section 3). In Section 2.1, we discuss grammaticality constraints, which take the form of introducing dependencies between textual units, as shown in Figure 2. If one textual unit requires another, it cannot be included unless its prerequisite is. We will show that different sets of requirements can capture both syntactic and discourse-based compression schemes. Furthermore, we introduce anaphora constraints (Section 2.2) via a new set of"
P16-1188,W01-1605,0,0.0722609,"Woodsend and Lapata, 2012; Almeida and Martins, 2013), we wish to be able to compress sentences so we can pack more information into a summary. During training, our model learns how to take advantage of available compression options and select content to match human generated summaries as closely possible.2 We explore two ways of deriving units for compression: the RST-based compressions of Hirao et al. (2013) and the syntactic compressions of Berg-Kirkpatrick et al. (2011). RST compressions Figure 2a shows how to derive compressions from Rhetorical Structure Theory (Mann and Thompson, 1988; Carlson et al., 2001). We show a sentence broken into elemen2 The features in our model are actually rich enough to learn a sophisticated compression model, but the data we have (abstractive summaries) does not directly provide examples of correct compressions; past work has gotten around this with multi-task learning (Almeida and Martins, 2013), but we simply treat grammaticality as a constraint from upstream models. tary discourse units (EDUs) with RST relations between them. Units marked as S AME -U NIT must both be kept or both be deleted, but other nodes in the tree structure can be deleted as long as we do n"
P16-1188,N13-1136,0,0.0110846,"ate that our model has the expressive capacity to extract important content, but is sufficiently constrained to ensure fluency is not sacrificed as a result. Past work has explored various kinds of structure for summarization. Some work has focused on improving content selection using discourse structure (Louis et al., 2010; Hirao et al., 2013), topical structure (Barzilay and Lee, 2004), or related techniques (Mithun and Kosseim, 2011). Other work has used structure primarily to reorder summaries and ensure coherence (Barzilay et al., 2001; Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Christensen et al., 2013) or to represent content for sentence fusion or abstraction (Thadani and McKeown, 2013; Pighin et al., 2014). Similar to these approaches, we appeal to structures from upstream NLP tasks (syntactic parsing, RST parsing, and coreference) to restrict our model’s capacity to generate. However, we go further by optimizing for ROUGE subject to these constraints with end-to-end learning. 2 Model Our model is shown in Figure 1. Broadly, our ILP takes a set of textual units u = (u1 , . . . , un ) from a document and finds the highest-scoring extractive summary by optimizing over variables xUNIT = x1UN"
P16-1188,J10-3005,0,0.0598346,"ll trees that are not contained in other trees that are used in the augmentation process. This is broadly similar to the combined compression scheme in Kikuchi et al. (2014) but we use a different set of constraints that more strictly enforce grammaticality.3 2.2 Anaphora Constraints What kind of cross-sentential coherence do we need to ensure for the kinds of summaries our system produces? Many notions of coherence are useful, including centering theory (Grosz et al., 1995) and lexical cohesion (Nishikawa et al., 2014), but one of the most pressing phenomena to deal with is pronoun anaphora (Clarke and Lapata, 2010). Cases of pronouns being “orphaned” during extraction (their antecedents are deleted) are 3 We also differ from past work in that we do not use crosssentential RST constraints (Hirao et al., 2013; Yoshida et al., 2014). We experimented with these and found no improvement from using them, possibly because we have a featurebased model rather than a heuristic content selection procedure, and possibly because automatic discourse parsers are less good at recovering cross-sentence relations. u1 This hasn’t been Kellogg’s year . it No replacement necessary u2 Kellogg p1 p3 p2 it year The oat-bran cr"
P16-1188,P02-1057,0,0.146913,"Missing"
P16-1188,E14-4040,0,0.0180116,"as more difficult. Content selection is tricky without redundancy across multiple input documents as a guide and simple positional information is often hard to beat (Penn and Zhu, 2008). In this work, we tackle the single-document problem by training an expressive summarization model on a large nat1 Available at http://nlp.cs.berkeley.edu urally occurring corpus—the New York Times Annotated Corpus (Sandhaus, 2008) which contains around 100,000 news articles with abstractive summaries—learning to select important content with lexical features. This corpus has been explored in related contexts (Dunietz and Gillick, 2014; Hong and Nenkova, 2014), but to our knowledge it has not been directly used for singledocument summarization. To increase the expressive capacity of our model we allow more aggressive compression of individual sentences by combining two different formalisms—one syntactic and the other discursive. Additionally, we incorporate a model of anaphora resolution and give our system the ability rewrite pronominal mentions, further increasing expressivity. In order to guide the model, we incorporate (1) constraints from coreference ensuring that critical pronoun references are clear in the final summ"
P16-1188,D13-1203,1,0.744439,"is Kellogg. We explore two types of constraints for dealing with this: rewriting the pronoun explicitly, or constraining the summary to include the pronoun’s antecedent. 2.2.1 Pronoun Replacement One way of dealing with these pronoun reference issues is to explicitly replace the pronoun with what it refers to. This replacement allows us to maintain maximal extraction flexibility, since we 4 We focus on pronoun coreference because it is the most pressing manifestation of this problem and because existing coreference systems perform well on pronouns compared to harder instances of coreference (Durrett and Klein, 2013). 2001 can make an isolated textual unit meaningful even if it contains a pronoun. Figure 3 shows how this process works. We run the Berkeley Entity Resolution System (Durrett and Klein, 2014) and compute posteriors over possible links for the pronoun. If the coreference system is sufficiently confident in its prediction (i.e. maxi pi &gt; α for a specified threshold α &gt; 12 ), we allow ourselves to replace the pronoun with the first mention of the entity corresponding to the pronoun’s most likely antecedent. In Figure 3, if the system correctly determines that Kellogg is the correct antecedent wi"
P16-1188,Q14-1037,1,0.910185,"ment One way of dealing with these pronoun reference issues is to explicitly replace the pronoun with what it refers to. This replacement allows us to maintain maximal extraction flexibility, since we 4 We focus on pronoun coreference because it is the most pressing manifestation of this problem and because existing coreference systems perform well on pronouns compared to harder instances of coreference (Durrett and Klein, 2013). 2001 can make an isolated textual unit meaningful even if it contains a pronoun. Figure 3 shows how this process works. We run the Berkeley Entity Resolution System (Durrett and Klein, 2014) and compute posteriors over possible links for the pronoun. If the coreference system is sufficiently confident in its prediction (i.e. maxi pi &gt; α for a specified threshold α &gt; 12 ), we allow ourselves to replace the pronoun with the first mention of the entity corresponding to the pronoun’s most likely antecedent. In Figure 3, if the system correctly determines that Kellogg is the correct antecedent with high probability, we enable the first replacement shown there, which is used if u2 is included the summary without u1 .5 As shown in the ILP in Figure 1, we instantiate corresponding pronou"
P16-1188,W09-1802,0,0.312645,"n is licensed by compression rules; in our framework, these are implemented as dependencies between subsentential units of text. Anaphoricity constraints then improve cross-sentence coherence by guaranteeing that, for each pronoun included in the summary, the pronoun’s antecedent is included as well or the pronoun is rewritten as a full mention. When trained end-to-end, our final system1 outperforms prior work on both ROUGE as well as on human judgments of linguistic quality. 1 Introduction While multi-document summarization is wellstudied in the NLP literature (Carbonell and Goldstein, 1998; Gillick and Favre, 2009; Lin and Bilmes, 2011; Nenkova and McKeown, 2011), single-document summarization (McKeown et al., 1995; Marcu, 1998; Mani, 2001; Hirao et al., 2013) has received less attention in recent years and is generally viewed as more difficult. Content selection is tricky without redundancy across multiple input documents as a guide and simple positional information is often hard to beat (Penn and Zhu, 2008). In this work, we tackle the single-document problem by training an expressive summarization model on a large nat1 Available at http://nlp.cs.berkeley.edu urally occurring corpus—the New York Time"
P16-1188,W10-0722,0,0.076163,"hat removing either syntactic or EDU-based compressions decreases ROUGE. ument prefix summary. One reason for this is that many of the articles are longer-form pieces that begin with a relatively content-free lede of several sentences, which should be identifiable with lexicosyntactic indicators as are used in our discriminative model. 4.3 New York Times Results We evaluate our system along two axes: first, on content selection, using ROUGE9 (Lin and Hovy, 2003), and second, on clarity of language and referential structure, using annotators from Amazon Mechanical Turk. We follow the method of Gillick and Liu (2010) for this evaluation and ask Turkers to rate a summary on how grammatical it is using a 10-point Likert scale. Furthermore, we ask how many unclear pronouns references there were in the text. The Turkers do not see the original document or the reference summary, and rate each summary in isolation. Gillick and Liu (2010) showed that for linguistic quality judgments (as opposed to content judgments), Turkers reproduced the ranking of systems according to expert judgments. To speed up preprocessing and training time 9 We use the ROUGE 1.5.5 script with the following command line arguments: -n 2 -"
P16-1188,D13-1158,0,0.0503894,"then improve cross-sentence coherence by guaranteeing that, for each pronoun included in the summary, the pronoun’s antecedent is included as well or the pronoun is rewritten as a full mention. When trained end-to-end, our final system1 outperforms prior work on both ROUGE as well as on human judgments of linguistic quality. 1 Introduction While multi-document summarization is wellstudied in the NLP literature (Carbonell and Goldstein, 1998; Gillick and Favre, 2009; Lin and Bilmes, 2011; Nenkova and McKeown, 2011), single-document summarization (McKeown et al., 1995; Marcu, 1998; Mani, 2001; Hirao et al., 2013) has received less attention in recent years and is generally viewed as more difficult. Content selection is tricky without redundancy across multiple input documents as a guide and simple positional information is often hard to beat (Penn and Zhu, 2008). In this work, we tackle the single-document problem by training an expressive summarization model on a large nat1 Available at http://nlp.cs.berkeley.edu urally occurring corpus—the New York Times Annotated Corpus (Sandhaus, 2008) which contains around 100,000 news articles with abstractive summaries—learning to select important content with"
P16-1188,E14-1075,0,0.0534042,"selection is tricky without redundancy across multiple input documents as a guide and simple positional information is often hard to beat (Penn and Zhu, 2008). In this work, we tackle the single-document problem by training an expressive summarization model on a large nat1 Available at http://nlp.cs.berkeley.edu urally occurring corpus—the New York Times Annotated Corpus (Sandhaus, 2008) which contains around 100,000 news articles with abstractive summaries—learning to select important content with lexical features. This corpus has been explored in related contexts (Dunietz and Gillick, 2014; Hong and Nenkova, 2014), but to our knowledge it has not been directly used for singledocument summarization. To increase the expressive capacity of our model we allow more aggressive compression of individual sentences by combining two different formalisms—one syntactic and the other discursive. Additionally, we incorporate a model of anaphora resolution and give our system the ability rewrite pronominal mentions, further increasing expressivity. In order to guide the model, we incorporate (1) constraints from coreference ensuring that critical pronoun references are clear in the final summary and (2) constraints f"
P16-1188,P13-1048,0,0.015369,"Missing"
P16-1188,P14-2052,0,0.0168089,": Tcomb = (S ∪ Ssyn (kl) ∪ {(i, k), (l, j)}, πrst ∪ πsyn(kl) ∪ {(i, k) → (l, j), (l, j) → (i, k), (k, l) → (i, k)}) That is, we maintain the existing tree structure except for the EDU (i, j), which is broken into three parts: the outer two depend on each other (is a claims adjuster and . from Figure 2d) and the inner one depends on the others and preserves the tree structure from Tsyn . We augment Trst with all maximal subtrees of Tsyn , i.e. all trees that are not contained in other trees that are used in the augmentation process. This is broadly similar to the combined compression scheme in Kikuchi et al. (2014) but we use a different set of constraints that more strictly enforce grammaticality.3 2.2 Anaphora Constraints What kind of cross-sentential coherence do we need to ensure for the kinds of summaries our system produces? Many notions of coherence are useful, including centering theory (Grosz et al., 1995) and lexical cohesion (Nishikawa et al., 2014), but one of the most pressing phenomena to deal with is pronoun anaphora (Clarke and Lapata, 2010). Cases of pronouns being “orphaned” during extraction (their antecedents are deleted) are 3 We also differ from past work in that we do not use cros"
P16-1188,D15-1032,1,0.437982,"y that in general cannot be produced by our model. Specifically, we take: `(xNGRAM , y) = maxx∗ ∗ ROUGE -1(x , y) − ROUGE -1(xNGRAM , y) i.e. the gap between the hypothesis’s ROUGE score and the oracle ROUGE score achievable under the model (including constraints). Here xNGRAM are indicator variables that track, for each n-gram type in the reference summary, whether that n-gram is present in the system summary. These are the sufficient statistics for computing ROUGE. We train the model via stochastic subgradient descent on the primal form of the structured SVM objective (Ratliff et al., 2007; Kummerfeld et al., 2015). In order to compute the subgradient for a given training example, we need to find the most violated constraint on the given instance through a loss-augmented decode, which for a linear model takes the form arg maxx w&gt; f (x) + `(x, y). To do this decode at training time in the context of our model, we use an extended version of our ILP in Figure 1 that is augmented to explicitly track typelevel n-grams: &quot; Xh max xUNIT ,xREF ,xNGRAM UNIT xi i (w f (ui )) &gt; i  + Xh REF &gt; i xij (w f (rij )) − `(x NGRAM , y) (i,j) subject to all constraints from Figure 1, and xiNGRAM = 1 iff an included textual"
P16-1188,N15-1079,0,0.012553,"ce-extractive model, these would be entire sentences, but for our compressive models we will have more fine-grained units, as shown in Figure 2 and described in Section 2.1. Textual units are scored according to features f and model parameters w learned on training data. Finally, the extraction process is subject to a length constraint of k words. This approach is similar in spirit to ILP formulations of multi-document summarization systems, though in those systems content is typically modeled in terms of bigrams (Gillick and Favre, 2009; Berg-Kirkpatrick et al., 2011; Hong and Nenkova, 2014; Li et al., 2015). For our model, type-level n-gram scoring only arises when we compute our loss function in maxmargin training (see Section 3). In Section 2.1, we discuss grammaticality constraints, which take the form of introducing dependencies between textual units, as shown in Figure 2. If one textual unit requires another, it cannot be included unless its prerequisite is. We will show that different sets of requirements can capture both syntactic and discourse-based compression schemes. Furthermore, we introduce anaphora constraints (Section 2.2) via a new set of variables that capture the process of rew"
P16-1188,P11-1052,0,0.0300998,"ion rules; in our framework, these are implemented as dependencies between subsentential units of text. Anaphoricity constraints then improve cross-sentence coherence by guaranteeing that, for each pronoun included in the summary, the pronoun’s antecedent is included as well or the pronoun is rewritten as a full mention. When trained end-to-end, our final system1 outperforms prior work on both ROUGE as well as on human judgments of linguistic quality. 1 Introduction While multi-document summarization is wellstudied in the NLP literature (Carbonell and Goldstein, 1998; Gillick and Favre, 2009; Lin and Bilmes, 2011; Nenkova and McKeown, 2011), single-document summarization (McKeown et al., 1995; Marcu, 1998; Mani, 2001; Hirao et al., 2013) has received less attention in recent years and is generally viewed as more difficult. Content selection is tricky without redundancy across multiple input documents as a guide and simple positional information is often hard to beat (Penn and Zhu, 2008). In this work, we tackle the single-document problem by training an expressive summarization model on a large nat1 Available at http://nlp.cs.berkeley.edu urally occurring corpus—the New York Times Annotated Corpus (Sa"
P16-1188,J95-2003,0,0.469362,"Missing"
P16-1188,N03-1020,0,0.193323,"ally significant gains compared to No Anaphoricity and Tree Knapsack (respectively) with p &lt; 0.05 according to a bootstrap resampling test. We also see that removing either syntactic or EDU-based compressions decreases ROUGE. ument prefix summary. One reason for this is that many of the articles are longer-form pieces that begin with a relatively content-free lede of several sentences, which should be identifiable with lexicosyntactic indicators as are used in our discriminative model. 4.3 New York Times Results We evaluate our system along two axes: first, on content selection, using ROUGE9 (Lin and Hovy, 2003), and second, on clarity of language and referential structure, using annotators from Amazon Mechanical Turk. We follow the method of Gillick and Liu (2010) for this evaluation and ask Turkers to rate a summary on how grammatical it is using a 10-point Likert scale. Furthermore, we ask how many unclear pronouns references there were in the text. The Turkers do not see the original document or the reference summary, and rate each summary in isolation. Gillick and Liu (2010) showed that for linguistic quality judgments (as opposed to content judgments), Turkers reproduced the ranking of systems"
P16-1188,P14-1020,1,0.819066,"(Sandhaus, 2008). We also investigate its performance on the RST Discourse Treebank (Carlson et al., 2001), but because this dataset is only 30 documents it provides much less robust estimates of performance.8 Throughout this section, when we decode a document, we set the word budget for our summarizer to be the same as the number of words in the corresponding reference summary, following previous work (Hirao et al., 2013; Yoshida et al., 2014). 4.1 Preprocessing We preprocess all data using the Berkeley Parser (Petrov et al., 2006), specifically the GPUaccelerated version of the parser from Hall et al. (2014), and the Berkeley Entity Resolution System (Durrett and Klein, 2014). For RST discourse analysis, we segment text into EDUs using a semiMarkov CRF trained on the RST treebank with features on boundaries similar to those of Hernault et al. (2010), plus novel features on spans including span length and span identity for short spans. To follow the conditions of Yoshida et al. (2014) as closely as possible, we also build a discourse parser in the style of Hirao et al. (2013), since their parser is not publicly available. Specifically, 8 Tasks like DUC and TAC have focused on multidocument summari"
P16-1188,W03-1101,0,0.0302607,"explicit mentions. That is, xij = 1 if we should rewrite the jth pronoun in the ith unit with its antecedent. These pronoun rewrites are scored in the objective and introduced into the length constraint to make sure they do not cause our summary to be too long. Finally, constraints on these variables control when they are used and also require the model to include antecedents of pronouns when the model is not confident enough to rewrite them. 2.1 Grammaticality Constraints Following work on isolated sentence compression (McDonald, 2006; Clarke and Lapata, 2008) and compressive summarization (Lin, 2003; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013), we wish to be able to compress sentences so we can pack more information into a summary. During training, our model learns how to take advantage of available compression options and select content to match human generated summaries as closely possible.2 We explore two ways of deriving units for compression: the RST-based compressions of Hirao et al. (2013) and the syntactic compressions of Berg-Kirkpatrick et al. (2011). RST compressions Figure 2a shows how to derive compressions fr"
P16-1188,D12-1106,0,0.016443,"thod. These results indicate that our model has the expressive capacity to extract important content, but is sufficiently constrained to ensure fluency is not sacrificed as a result. Past work has explored various kinds of structure for summarization. Some work has focused on improving content selection using discourse structure (Louis et al., 2010; Hirao et al., 2013), topical structure (Barzilay and Lee, 2004), or related techniques (Mithun and Kosseim, 2011). Other work has used structure primarily to reorder summaries and ensure coherence (Barzilay et al., 2001; Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Christensen et al., 2013) or to represent content for sentence fusion or abstraction (Thadani and McKeown, 2013; Pighin et al., 2014). Similar to these approaches, we appeal to structures from upstream NLP tasks (syntactic parsing, RST parsing, and coreference) to restrict our model’s capacity to generate. However, we go further by optimizing for ROUGE subject to these constraints with end-to-end learning. 2 Model Our model is shown in Figure 1. Broadly, our ILP takes a set of textual units u = (u1 , . . . , un ) from a document and finds the highest-scoring extractive summary by optimizing"
P16-1188,W10-4327,0,0.0273313,"Missing"
P16-1188,W98-1124,0,0.0373498,"Anaphoricity constraints then improve cross-sentence coherence by guaranteeing that, for each pronoun included in the summary, the pronoun’s antecedent is included as well or the pronoun is rewritten as a full mention. When trained end-to-end, our final system1 outperforms prior work on both ROUGE as well as on human judgments of linguistic quality. 1 Introduction While multi-document summarization is wellstudied in the NLP literature (Carbonell and Goldstein, 1998; Gillick and Favre, 2009; Lin and Bilmes, 2011; Nenkova and McKeown, 2011), single-document summarization (McKeown et al., 1995; Marcu, 1998; Mani, 2001; Hirao et al., 2013) has received less attention in recent years and is generally viewed as more difficult. Content selection is tricky without redundancy across multiple input documents as a guide and simple positional information is often hard to beat (Penn and Zhu, 2008). In this work, we tackle the single-document problem by training an expressive summarization model on a large nat1 Available at http://nlp.cs.berkeley.edu urally occurring corpus—the New York Times Annotated Corpus (Sandhaus, 2008) which contains around 100,000 news articles with abstractive summaries—learning"
P16-1188,W09-1801,0,0.0266717,"entions. That is, xij = 1 if we should rewrite the jth pronoun in the ith unit with its antecedent. These pronoun rewrites are scored in the objective and introduced into the length constraint to make sure they do not cause our summary to be too long. Finally, constraints on these variables control when they are used and also require the model to include antecedents of pronouns when the model is not confident enough to rewrite them. 2.1 Grammaticality Constraints Following work on isolated sentence compression (McDonald, 2006; Clarke and Lapata, 2008) and compressive summarization (Lin, 2003; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013), we wish to be able to compress sentences so we can pack more information into a summary. During training, our model learns how to take advantage of available compression options and select content to match human generated summaries as closely possible.2 We explore two ways of deriving units for compression: the RST-based compressions of Hirao et al. (2013) and the syntactic compressions of Berg-Kirkpatrick et al. (2011). RST compressions Figure 2a shows how to derive compressions from Rhetorical Structure T"
P16-1188,P05-1012,0,0.0434139,"Missing"
P16-1188,E06-1038,0,0.0215508,"d) Process of augmenting a textual unit with syntactic compressions. REF explicit mentions. That is, xij = 1 if we should rewrite the jth pronoun in the ith unit with its antecedent. These pronoun rewrites are scored in the objective and introduced into the length constraint to make sure they do not cause our summary to be too long. Finally, constraints on these variables control when they are used and also require the model to include antecedents of pronouns when the model is not confident enough to rewrite them. 2.1 Grammaticality Constraints Following work on isolated sentence compression (McDonald, 2006; Clarke and Lapata, 2008) and compressive summarization (Lin, 2003; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013), we wish to be able to compress sentences so we can pack more information into a summary. During training, our model learns how to take advantage of available compression options and select content to match human generated summaries as closely possible.2 We explore two ways of deriving units for compression: the RST-based compressions of Hirao et al. (2013) and the syntactic compressions of Berg-Kirkpatrick et al. (20"
P16-1188,R11-1066,0,0.0174782,"nstraint. Yoshida et al. (2014) and approaching the clarity of a sentence-extractive baseline—and still achieves substantially higher ROUGE score than either method. These results indicate that our model has the expressive capacity to extract important content, but is sufficiently constrained to ensure fluency is not sacrificed as a result. Past work has explored various kinds of structure for summarization. Some work has focused on improving content selection using discourse structure (Louis et al., 2010; Hirao et al., 2013), topical structure (Barzilay and Lee, 2004), or related techniques (Mithun and Kosseim, 2011). Other work has used structure primarily to reorder summaries and ensure coherence (Barzilay et al., 2001; Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Christensen et al., 2013) or to represent content for sentence fusion or abstraction (Thadani and McKeown, 2013; Pighin et al., 2014). Similar to these approaches, we appeal to structures from upstream NLP tasks (syntactic parsing, RST parsing, and coreference) to restrict our model’s capacity to generate. However, we go further by optimizing for ROUGE subject to these constraints with end-to-end learning. 2 Model Our model is shown in"
P16-1188,D15-1182,0,0.0301045,"Missing"
P16-1188,P14-1084,0,0.0402219,"nsure fluency is not sacrificed as a result. Past work has explored various kinds of structure for summarization. Some work has focused on improving content selection using discourse structure (Louis et al., 2010; Hirao et al., 2013), topical structure (Barzilay and Lee, 2004), or related techniques (Mithun and Kosseim, 2011). Other work has used structure primarily to reorder summaries and ensure coherence (Barzilay et al., 2001; Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Christensen et al., 2013) or to represent content for sentence fusion or abstraction (Thadani and McKeown, 2013; Pighin et al., 2014). Similar to these approaches, we appeal to structures from upstream NLP tasks (syntactic parsing, RST parsing, and coreference) to restrict our model’s capacity to generate. However, we go further by optimizing for ROUGE subject to these constraints with end-to-end learning. 2 Model Our model is shown in Figure 1. Broadly, our ILP takes a set of textual units u = (u1 , . . . , un ) from a document and finds the highest-scoring extractive summary by optimizing over variables xUNIT = x1UNIT , . . . , xnUNIT , which are binary indicators of whether each unit is included. Textual units are contig"
P16-1188,N03-1030,0,0.042861,"Missing"
P16-1188,I13-1198,0,0.0176762,"fficiently constrained to ensure fluency is not sacrificed as a result. Past work has explored various kinds of structure for summarization. Some work has focused on improving content selection using discourse structure (Louis et al., 2010; Hirao et al., 2013), topical structure (Barzilay and Lee, 2004), or related techniques (Mithun and Kosseim, 2011). Other work has used structure primarily to reorder summaries and ensure coherence (Barzilay et al., 2001; Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Christensen et al., 2013) or to represent content for sentence fusion or abstraction (Thadani and McKeown, 2013; Pighin et al., 2014). Similar to these approaches, we appeal to structures from upstream NLP tasks (syntactic parsing, RST parsing, and coreference) to restrict our model’s capacity to generate. However, we go further by optimizing for ROUGE subject to these constraints with end-to-end learning. 2 Model Our model is shown in Figure 1. Broadly, our ILP takes a set of textual units u = (u1 , . . . , un ) from a document and finds the highest-scoring extractive summary by optimizing over variables xUNIT = x1UNIT , . . . , xnUNIT , which are binary indicators of whether each unit is included. Te"
P16-1188,D12-1022,0,0.0100531,"ronoun in the ith unit with its antecedent. These pronoun rewrites are scored in the objective and introduced into the length constraint to make sure they do not cause our summary to be too long. Finally, constraints on these variables control when they are used and also require the model to include antecedents of pronouns when the model is not confident enough to rewrite them. 2.1 Grammaticality Constraints Following work on isolated sentence compression (McDonald, 2006; Clarke and Lapata, 2008) and compressive summarization (Lin, 2003; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013), we wish to be able to compress sentences so we can pack more information into a summary. During training, our model learns how to take advantage of available compression options and select content to match human generated summaries as closely possible.2 We explore two ways of deriving units for compression: the RST-based compressions of Hirao et al. (2013) and the syntactic compressions of Berg-Kirkpatrick et al. (2011). RST compressions Figure 2a shows how to derive compressions from Rhetorical Structure Theory (Mann and Thompson, 1988; Carlson et al., 2001). We"
P16-1188,D14-1196,0,0.0482556,"By training our full system endto-end on a large-scale dataset, we are able to learn a high-capacity structured model of the summarization process, contrasting with past approaches to the single-document task which have typically been heuristic in nature (Daum´e and Marcu, 2002; Hirao et al., 2013). We focus our evaluation on the New York Times Annotated corpus (Sandhaus, 2008). According to ROUGE, our system outperforms a document prefix baseline, a bigram coverage baseline adapted from a strong multi-document system (Gillick and Favre, 2009), and a discourse-informed method from prior work (Yoshida et al., 2014). Imposing discursive and referential constraints improves human judgments of linguistic clarity and referential structure—outperforming the method of 1998 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1998–2008, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics max unit ref x ,x subject to h X ⇥ Extraction Score xunit (w&gt; f (ui )) + i i Grammaticality Constraints (Section 2.1) Anaphora Score &gt; xref ij (w f (rij )) (i,j) i ⇤ Length adjustment for explicit mention Length Constraint X xunit  xunit if ui requires"
P16-1188,C14-1156,0,0.0799128,"reserves the tree structure from Tsyn . We augment Trst with all maximal subtrees of Tsyn , i.e. all trees that are not contained in other trees that are used in the augmentation process. This is broadly similar to the combined compression scheme in Kikuchi et al. (2014) but we use a different set of constraints that more strictly enforce grammaticality.3 2.2 Anaphora Constraints What kind of cross-sentential coherence do we need to ensure for the kinds of summaries our system produces? Many notions of coherence are useful, including centering theory (Grosz et al., 1995) and lexical cohesion (Nishikawa et al., 2014), but one of the most pressing phenomena to deal with is pronoun anaphora (Clarke and Lapata, 2010). Cases of pronouns being “orphaned” during extraction (their antecedents are deleted) are 3 We also differ from past work in that we do not use crosssentential RST constraints (Hirao et al., 2013; Yoshida et al., 2014). We experimented with these and found no improvement from using them, possibly because we have a featurebased model rather than a heuristic content selection procedure, and possibly because automatic discourse parsers are less good at recovering cross-sentence relations. u1 This h"
P16-1188,P08-1054,0,0.0219105,"r work on both ROUGE as well as on human judgments of linguistic quality. 1 Introduction While multi-document summarization is wellstudied in the NLP literature (Carbonell and Goldstein, 1998; Gillick and Favre, 2009; Lin and Bilmes, 2011; Nenkova and McKeown, 2011), single-document summarization (McKeown et al., 1995; Marcu, 1998; Mani, 2001; Hirao et al., 2013) has received less attention in recent years and is generally viewed as more difficult. Content selection is tricky without redundancy across multiple input documents as a guide and simple positional information is often hard to beat (Penn and Zhu, 2008). In this work, we tackle the single-document problem by training an expressive summarization model on a large nat1 Available at http://nlp.cs.berkeley.edu urally occurring corpus—the New York Times Annotated Corpus (Sandhaus, 2008) which contains around 100,000 news articles with abstractive summaries—learning to select important content with lexical features. This corpus has been explored in related contexts (Dunietz and Gillick, 2014; Hong and Nenkova, 2014), but to our knowledge it has not been directly used for singledocument summarization. To increase the expressive capacity of our model"
P16-1188,P06-1055,1,0.129979,"n a roughly 3000-document evaluation set from the New York Times Annotated Corpus (Sandhaus, 2008). We also investigate its performance on the RST Discourse Treebank (Carlson et al., 2001), but because this dataset is only 30 documents it provides much less robust estimates of performance.8 Throughout this section, when we decode a document, we set the word budget for our summarizer to be the same as the number of words in the corresponding reference summary, following previous work (Hirao et al., 2013; Yoshida et al., 2014). 4.1 Preprocessing We preprocess all data using the Berkeley Parser (Petrov et al., 2006), specifically the GPUaccelerated version of the parser from Hall et al. (2014), and the Berkeley Entity Resolution System (Durrett and Klein, 2014). For RST discourse analysis, we segment text into EDUs using a semiMarkov CRF trained on the RST treebank with features on boundaries similar to those of Hernault et al. (2010), plus novel features on spans including span length and span identity for short spans. To follow the conditions of Yoshida et al. (2014) as closely as possible, we also build a discourse parser in the style of Hirao et al. (2013), since their parser is not publicly availabl"
P16-1188,W01-0100,0,\N,Missing
P19-1062,D14-1162,0,0.0846982,"dj ) (1) Models We present two models: one for text classification, and one for sentence ordering. Both are based on the L&L model, with a design change to cause stronger percolation of information up the tree (we also experiment without this change). ci = Text classification The left-hand side of Figure 1 presents an overview of the model: the model operates first at the sentence-level to create sentence representations, and then at the document-level to create a document representation from the previously created sentence representations. In more detail, the model composes GloVe embeddings (Pennington et al., 2014) into a sentence representation using structured attention (from which a tree can be derived), then sentence representations into a single document representation for class predicn X aik ek (3) k=1 where aik is the probability that k is the child of i, and ek is the semantic vector of the child. The children vectors are then passed through a non-linear function, resulting in the updated semantic vector e0i for parent node i. e0i = tanh(Wr [ei , ci ]) 3 (4) https://github.com/nlpyang/structured We found similar results for using both parents and children as well as using parents only. 4 647 Yel"
P19-1062,D13-1158,0,0.0651747,"Missing"
P19-1062,prasad-etal-2008-penn,0,0.0301692,"ically connected to each other. Taking into account this structure has shown to help many NLP end tasks, including summarization (Hirao et al., 2013; Durrett et al., 2016), machine translation (Joty et al., 2017), and sentiment analysis (Ji and Smith, 2017). However, annotating discourse requires considerable effort by trained experts and may not always yield a structure appropriate for the end task. As a result, having a model induce the discourse structure of a text is an attractive option. Our goal in this paper is to evaluate such an induced structure. 2 The Penn Discourse Treebank (PDTB; Prasad et al., 2008) captures lexically-grounded discourse for individual connectives and adjacent sentences, and does not span an entire document; Segmented Discourse Representation Theory (Lascarides and Asher, 2008) is a graph. 1 Code and data available at https://github.com/ elisaF/structured 646 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 646–653 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics document class document-level max-pooling structured+compose attention sentences sentence structured+compose attention word"
P19-1062,P17-1092,0,0.673652,"ations, the trees are still far from capturing discourse structure when compared to discourse dependency trees from an existing discourse parser. Finally, ablation studies show the structured attention provides little benefit, sometimes even hurting performance.1 1 Introduction Discourse describes how a document is organized, and how discourse units are rhetorically connected to each other. Taking into account this structure has shown to help many NLP end tasks, including summarization (Hirao et al., 2013; Durrett et al., 2016), machine translation (Joty et al., 2017), and sentiment analysis (Ji and Smith, 2017). However, annotating discourse requires considerable effort by trained experts and may not always yield a structure appropriate for the end task. As a result, having a model induce the discourse structure of a text is an attractive option. Our goal in this paper is to evaluate such an induced structure. 2 The Penn Discourse Treebank (PDTB; Prasad et al., 2008) captures lexically-grounded discourse for individual connectives and adjacent sentences, and does not span an entire document; Segmented Discourse Representation Theory (Lascarides and Asher, 2008) is a graph. 1 Code and data available"
P19-1062,D18-1548,0,0.0479993,"Missing"
P19-1062,P15-1098,0,0.0174817,"e, and the model from the timestep with highest development performance is chosen. We report accuracies on the test set, and tree analyses on the development set. Our implementation is built on the L&L released implementation, with changes as noted in Section 3. Preprocessing and training details are in Appendix A. We evaluate the model on four text classification tasks and one sentence order discrimination task. 4.1 Settings Datasets Details and statistics are included in Appendix A.5 Yelp (in L&L, 5-way classification) comprises customer reviews from the Yelp Dataset Challenge (collected by Tang et al. (2015)). Each review is labeled with a 1 to 5 rating (least to most positive). Debates (in L&L, binary classification) are transcribed debates on Congressional bills from the U.S. House of Representatives (compiled by Thomas et al. (2006), preprocessed by Yogatama 4.3 Results We report accuracy (as in prior work) in Table 1, and perform two ablations: removing the structured attention at the document level, and removing it at both document and sentence levels. Additionally, we run experiments on the original code 5 Of the document-level datasets used in L&L (SNLI was sentence-level), we omit IMDB an"
P19-1062,J93-2004,0,\N,Missing
P19-1062,W06-1639,0,\N,Missing
P19-1062,W01-1605,0,\N,Missing
P19-1062,P14-1074,0,\N,Missing
P19-1062,J08-1001,0,\N,Missing
P19-1062,Q13-1028,0,\N,Missing
P19-1062,J17-4001,0,\N,Missing
P19-1062,N19-1173,0,\N,Missing
P19-1062,Q18-1005,0,\N,Missing
P19-1062,D07-1015,0,\N,Missing
P19-1433,W06-1623,0,0.199651,"ith more frequent event-timex interactions.1 1 Introduction Understanding the temporal ordering of events in a document is an important component of document understanding and plays an integral role in tasks such as timeline creation (Do et al., 2012), temporal question answering (Llorens et al., 2015) and causality inference (Mostafazadeh et al., 2016; Ning et al., 2018a). Inferring temporal event order is challenging as it often disagrees with the narrative order in text. Past work on temporal relation extraction has exploited cues such as global constraints on the temporal graph structure (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Ning et al., 2017), world knowledge (Ning et al., 1 Data and code are available at https://github. com/tagoyal/Temporal-event-ordering 2018b), grouping of events (Tourille et al., 2017), or fusing these cues more effectively with deep models (Meng et al., 2017; Cheng and Miyao, 2017). One key component of temporal understanding is time expressions (timexes) that help anchor events to the time axis, but few recent systems effectively use the knowledge derivable from time expressions in their models. They either give timexes no special treatment (Ning et al., 2017)"
P19-1433,P14-2082,0,0.112324,"he dataset. We used the GloVe vectors with 840 billion tokens (largest available) to circumvent this issue and minimize the number of out of vocabulary instances, but still see low performance. 3.2 Event Temporal Ordering Next, we investigate the effectiveness of our timex embeddings in the context of our full event temporal ordering model. We evaluate on two event temporal ordering datasets, one real and one artificially constructed. 3.2.1 Evaluation on MATRES We evaluate on the MATRES dataset proposed in Ning et al. (2018c). This dataset is designed to be less ambiguous than TimeBank-Dense (Cassidy et al., 2014). MATRES contains temporal annotations for documents from the TimeBank (Pustejovsky et al., 2003), AQUAINT (Graff, 2002) and Platinum datasets (UzZaman et al., 2013). We follow standard practice and use TimeBank and AQUAINT (256 articles) for training and Platinum (20 articles) for testing. Table 2 outlines the performance of the proposed approach on MATRES. We implemented the model proposed by Cheng and Miyao (2017) and compare against it. We evaluate the models using both GloVe and ELMo embeddings. Our results show substantial improvement over this baseline model. Moreover, including time em"
P19-1433,Q14-1022,0,0.40503,"are available at https://github. com/tagoyal/Temporal-event-ordering 2018b), grouping of events (Tourille et al., 2017), or fusing these cues more effectively with deep models (Meng et al., 2017; Cheng and Miyao, 2017). One key component of temporal understanding is time expressions (timexes) that help anchor events to the time axis, but few recent systems effectively use the knowledge derivable from time expressions in their models. They either give timexes no special treatment (Ning et al., 2017) or rely on rule-based post-processing modules to remove inconsistencies with explicit timexes (Chambers et al., 2014; Meng et al., 2017). In this work, we address this shortcoming by introducing a framework for including rich representations of timexes in neural models. These models implicitly capture some information via word embeddings (Mikolov et al., 2013; Pennington et al., 2014) or contextualized embeddings such as ELMo (Peters et al., 2018). However, these embeddings do not encode the full richness of temporal information needed for this task. For example, these systems fail to infer the correct event relation in the following sentence: He visited France in 1992 and went to Germany in 1963. partially"
P19-1433,D08-1073,0,0.11608,"t-timex interactions.1 1 Introduction Understanding the temporal ordering of events in a document is an important component of document understanding and plays an integral role in tasks such as timeline creation (Do et al., 2012), temporal question answering (Llorens et al., 2015) and causality inference (Mostafazadeh et al., 2016; Ning et al., 2018a). Inferring temporal event order is challenging as it often disagrees with the narrative order in text. Past work on temporal relation extraction has exploited cues such as global constraints on the temporal graph structure (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Ning et al., 2017), world knowledge (Ning et al., 1 Data and code are available at https://github. com/tagoyal/Temporal-event-ordering 2018b), grouping of events (Tourille et al., 2017), or fusing these cues more effectively with deep models (Meng et al., 2017; Cheng and Miyao, 2017). One key component of temporal understanding is time expressions (timexes) that help anchor events to the time axis, but few recent systems effectively use the knowledge derivable from time expressions in their models. They either give timexes no special treatment (Ning et al., 2017) or rely on rule-based post-p"
P19-1433,P17-2001,0,0.299743,"lity inference (Mostafazadeh et al., 2016; Ning et al., 2018a). Inferring temporal event order is challenging as it often disagrees with the narrative order in text. Past work on temporal relation extraction has exploited cues such as global constraints on the temporal graph structure (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Ning et al., 2017), world knowledge (Ning et al., 1 Data and code are available at https://github. com/tagoyal/Temporal-event-ordering 2018b), grouping of events (Tourille et al., 2017), or fusing these cues more effectively with deep models (Meng et al., 2017; Cheng and Miyao, 2017). One key component of temporal understanding is time expressions (timexes) that help anchor events to the time axis, but few recent systems effectively use the knowledge derivable from time expressions in their models. They either give timexes no special treatment (Ning et al., 2017) or rely on rule-based post-processing modules to remove inconsistencies with explicit timexes (Chambers et al., 2014; Meng et al., 2017). In this work, we address this shortcoming by introducing a framework for including rich representations of timexes in neural models. These models implicitly capture some inform"
P19-1433,D12-1062,0,0.027208,"nsisting of pairs of timexes, then train a character LSTM to learn embeddings and classify the timexes’ temporal relation. We evaluate the utility of these embeddings in the context of a strong neural model for event temporal ordering, and show a small increase in performance on the MATRES dataset and more substantial gains on an automatically collected dataset with more frequent event-timex interactions.1 1 Introduction Understanding the temporal ordering of events in a document is an important component of document understanding and plays an integral role in tasks such as timeline creation (Do et al., 2012), temporal question answering (Llorens et al., 2015) and causality inference (Mostafazadeh et al., 2016; Ning et al., 2018a). Inferring temporal event order is challenging as it often disagrees with the narrative order in text. Past work on temporal relation extraction has exploited cues such as global constraints on the temporal graph structure (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Ning et al., 2017), world knowledge (Ning et al., 1 Data and code are available at https://github. com/tagoyal/Temporal-event-ordering 2018b), grouping of events (Tourille et al., 2017), or fusing the"
P19-1433,S15-2134,0,0.0178065,"acter LSTM to learn embeddings and classify the timexes’ temporal relation. We evaluate the utility of these embeddings in the context of a strong neural model for event temporal ordering, and show a small increase in performance on the MATRES dataset and more substantial gains on an automatically collected dataset with more frequent event-timex interactions.1 1 Introduction Understanding the temporal ordering of events in a document is an important component of document understanding and plays an integral role in tasks such as timeline creation (Do et al., 2012), temporal question answering (Llorens et al., 2015) and causality inference (Mostafazadeh et al., 2016; Ning et al., 2018a). Inferring temporal event order is challenging as it often disagrees with the narrative order in text. Past work on temporal relation extraction has exploited cues such as global constraints on the temporal graph structure (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Ning et al., 2017), world knowledge (Ning et al., 1 Data and code are available at https://github. com/tagoyal/Temporal-event-ordering 2018b), grouping of events (Tourille et al., 2017), or fusing these cues more effectively with deep models (Meng et a"
P19-1433,P14-5010,0,0.00448082,"ngs from ELMo, vp is a randomly initialized and trainable embedding of the part-of-speech tag, and vt corresponds to the timex embedding derived for time expressions (explained in Section 2.1). Contextual Encoding A biLSTM is used to obtain contextualized embeddings hk for each token xk in the two sentences, as shown in Figure 1. The parameters are shared between these lower biLSTMs for the two sentences. Prior work (Cheng and Miyao, 2017) does not include these lower biLSTMs and only leverages the dependency encoding, explained next. Dependency Encoding We use the Stanford Dependency Parser (Manning et al., 2014) to extract the dependency paths for both events to their lowest common ancestor. For inter-sentence event pairs, paths are extracted to the root of each sentence. Each vector along the dependency path is fed into an upper biLSTM to produce output hupper . Formally, for sentence s1 , h1upper = biLSTM([hk for k ∈ dep-path(e1 )]) Parameters are shared between the upper biLSTMs for the two sentences. Figure 1: Temporal relation extraction model. Here, peaked and remained are the two events under consideration. The sentences are passed through the lower LSTM, then the outputs corresponding to the"
P19-1433,P18-1049,0,0.0916745,"dataset of examples with explicit timexes that expose their temporal relation; we view the timexes as distant supervision for the event pairs. To identify such examples, we use two high precision classifiers proposed in Chambers et al. (2014): (a) an eventtimex classifier that identifies the temporal relation between adjacent verb and time expressions (precision = 0.92), (b) a timex-timex classifier that identifies the temporal relation between two time expressions (precision = 0.88). These classifiers can allow us to directly infer the time relation be5 In prior work (Cheng and Miyao, 2017; Meng and Rumshisky, 2018), machine learning classifiers are used to infer a wider range of event-timex links, which can potentially increase the informativeness of timexes. However, many of the links they target require complex inferences to determine, and as a result those works report relatively low performance for such classifiers. Hence, we do not compare to these methods in our experiments. 4403 2000 3000 4000 76.8 75.5 83.2 78.2 77.1 83.1 83.8 80.1 84.5 84.3 80.7 84.8 GloVe Ours w/o Timex Embed Ours w/ Masked Timex Ours w/ Timex Embed 74.0 73.9 81.6 ELMo Ours w/o Timex Embed Ours w/ Masked Timex Ours w/ Timex Em"
P19-1433,D17-1092,0,0.069897,"l., 2015) and causality inference (Mostafazadeh et al., 2016; Ning et al., 2018a). Inferring temporal event order is challenging as it often disagrees with the narrative order in text. Past work on temporal relation extraction has exploited cues such as global constraints on the temporal graph structure (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Ning et al., 2017), world knowledge (Ning et al., 1 Data and code are available at https://github. com/tagoyal/Temporal-event-ordering 2018b), grouping of events (Tourille et al., 2017), or fusing these cues more effectively with deep models (Meng et al., 2017; Cheng and Miyao, 2017). One key component of temporal understanding is time expressions (timexes) that help anchor events to the time axis, but few recent systems effectively use the knowledge derivable from time expressions in their models. They either give timexes no special treatment (Ning et al., 2017) or rely on rule-based post-processing modules to remove inconsistencies with explicit timexes (Chambers et al., 2014; Meng et al., 2017). In this work, we address this shortcoming by introducing a framework for including rich representations of timexes in neural models. These models implic"
P19-1433,W16-1007,0,0.0186704,"timexes’ temporal relation. We evaluate the utility of these embeddings in the context of a strong neural model for event temporal ordering, and show a small increase in performance on the MATRES dataset and more substantial gains on an automatically collected dataset with more frequent event-timex interactions.1 1 Introduction Understanding the temporal ordering of events in a document is an important component of document understanding and plays an integral role in tasks such as timeline creation (Do et al., 2012), temporal question answering (Llorens et al., 2015) and causality inference (Mostafazadeh et al., 2016; Ning et al., 2018a). Inferring temporal event order is challenging as it often disagrees with the narrative order in text. Past work on temporal relation extraction has exploited cues such as global constraints on the temporal graph structure (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Ning et al., 2017), world knowledge (Ning et al., 1 Data and code are available at https://github. com/tagoyal/Temporal-event-ordering 2018b), grouping of events (Tourille et al., 2017), or fusing these cues more effectively with deep models (Meng et al., 2017; Cheng and Miyao, 2017). One key component"
P19-1433,D17-1108,0,0.0894539,"oduction Understanding the temporal ordering of events in a document is an important component of document understanding and plays an integral role in tasks such as timeline creation (Do et al., 2012), temporal question answering (Llorens et al., 2015) and causality inference (Mostafazadeh et al., 2016; Ning et al., 2018a). Inferring temporal event order is challenging as it often disagrees with the narrative order in text. Past work on temporal relation extraction has exploited cues such as global constraints on the temporal graph structure (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Ning et al., 2017), world knowledge (Ning et al., 1 Data and code are available at https://github. com/tagoyal/Temporal-event-ordering 2018b), grouping of events (Tourille et al., 2017), or fusing these cues more effectively with deep models (Meng et al., 2017; Cheng and Miyao, 2017). One key component of temporal understanding is time expressions (timexes) that help anchor events to the time axis, but few recent systems effectively use the knowledge derivable from time expressions in their models. They either give timexes no special treatment (Ning et al., 2017) or rely on rule-based post-processing modules to"
P19-1433,P18-1212,0,0.660821,". We evaluate the utility of these embeddings in the context of a strong neural model for event temporal ordering, and show a small increase in performance on the MATRES dataset and more substantial gains on an automatically collected dataset with more frequent event-timex interactions.1 1 Introduction Understanding the temporal ordering of events in a document is an important component of document understanding and plays an integral role in tasks such as timeline creation (Do et al., 2012), temporal question answering (Llorens et al., 2015) and causality inference (Mostafazadeh et al., 2016; Ning et al., 2018a). Inferring temporal event order is challenging as it often disagrees with the narrative order in text. Past work on temporal relation extraction has exploited cues such as global constraints on the temporal graph structure (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Ning et al., 2017), world knowledge (Ning et al., 1 Data and code are available at https://github. com/tagoyal/Temporal-event-ordering 2018b), grouping of events (Tourille et al., 2017), or fusing these cues more effectively with deep models (Meng et al., 2017; Cheng and Miyao, 2017). One key component of temporal unders"
P19-1433,N18-1077,0,0.124017,". We evaluate the utility of these embeddings in the context of a strong neural model for event temporal ordering, and show a small increase in performance on the MATRES dataset and more substantial gains on an automatically collected dataset with more frequent event-timex interactions.1 1 Introduction Understanding the temporal ordering of events in a document is an important component of document understanding and plays an integral role in tasks such as timeline creation (Do et al., 2012), temporal question answering (Llorens et al., 2015) and causality inference (Mostafazadeh et al., 2016; Ning et al., 2018a). Inferring temporal event order is challenging as it often disagrees with the narrative order in text. Past work on temporal relation extraction has exploited cues such as global constraints on the temporal graph structure (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Ning et al., 2017), world knowledge (Ning et al., 1 Data and code are available at https://github. com/tagoyal/Temporal-event-ordering 2018b), grouping of events (Tourille et al., 2017), or fusing these cues more effectively with deep models (Meng et al., 2017; Cheng and Miyao, 2017). One key component of temporal unders"
P19-1433,P18-1122,0,0.699364,". We evaluate the utility of these embeddings in the context of a strong neural model for event temporal ordering, and show a small increase in performance on the MATRES dataset and more substantial gains on an automatically collected dataset with more frequent event-timex interactions.1 1 Introduction Understanding the temporal ordering of events in a document is an important component of document understanding and plays an integral role in tasks such as timeline creation (Do et al., 2012), temporal question answering (Llorens et al., 2015) and causality inference (Mostafazadeh et al., 2016; Ning et al., 2018a). Inferring temporal event order is challenging as it often disagrees with the narrative order in text. Past work on temporal relation extraction has exploited cues such as global constraints on the temporal graph structure (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Ning et al., 2017), world knowledge (Ning et al., 1 Data and code are available at https://github. com/tagoyal/Temporal-event-ordering 2018b), grouping of events (Tourille et al., 2017), or fusing these cues more effectively with deep models (Meng et al., 2017; Cheng and Miyao, 2017). One key component of temporal unders"
P19-1433,D14-1162,0,0.0935673,"time expressions (timexes) that help anchor events to the time axis, but few recent systems effectively use the knowledge derivable from time expressions in their models. They either give timexes no special treatment (Ning et al., 2017) or rely on rule-based post-processing modules to remove inconsistencies with explicit timexes (Chambers et al., 2014; Meng et al., 2017). In this work, we address this shortcoming by introducing a framework for including rich representations of timexes in neural models. These models implicitly capture some information via word embeddings (Mikolov et al., 2013; Pennington et al., 2014) or contextualized embeddings such as ELMo (Peters et al., 2018). However, these embeddings do not encode the full richness of temporal information needed for this task. For example, these systems fail to infer the correct event relation in the following sentence: He visited France in 1992 and went to Germany in 1963. partially because the dates 1992 and 1963 do not have temporally-informed embeddings. We devise a method for embedding timexes that more explicitly reflects their temporal status. Specifically, we sample pairs of time expressions from synthetic data, train character LSTM models t"
P19-1433,N18-1202,0,0.0670113,"but few recent systems effectively use the knowledge derivable from time expressions in their models. They either give timexes no special treatment (Ning et al., 2017) or rely on rule-based post-processing modules to remove inconsistencies with explicit timexes (Chambers et al., 2014; Meng et al., 2017). In this work, we address this shortcoming by introducing a framework for including rich representations of timexes in neural models. These models implicitly capture some information via word embeddings (Mikolov et al., 2013; Pennington et al., 2014) or contextualized embeddings such as ELMo (Peters et al., 2018). However, these embeddings do not encode the full richness of temporal information needed for this task. For example, these systems fail to infer the correct event relation in the following sentence: He visited France in 1992 and went to Germany in 1963. partially because the dates 1992 and 1963 do not have temporally-informed embeddings. We devise a method for embedding timexes that more explicitly reflects their temporal status. Specifically, we sample pairs of time expressions from synthetic data, train character LSTM models to encode these time expressions and classify their temporal orde"
P19-1433,P17-2035,0,0.0542307,"s timeline creation (Do et al., 2012), temporal question answering (Llorens et al., 2015) and causality inference (Mostafazadeh et al., 2016; Ning et al., 2018a). Inferring temporal event order is challenging as it often disagrees with the narrative order in text. Past work on temporal relation extraction has exploited cues such as global constraints on the temporal graph structure (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Ning et al., 2017), world knowledge (Ning et al., 1 Data and code are available at https://github. com/tagoyal/Temporal-event-ordering 2018b), grouping of events (Tourille et al., 2017), or fusing these cues more effectively with deep models (Meng et al., 2017; Cheng and Miyao, 2017). One key component of temporal understanding is time expressions (timexes) that help anchor events to the time axis, but few recent systems effectively use the knowledge derivable from time expressions in their models. They either give timexes no special treatment (Ning et al., 2017) or rely on rule-based post-processing modules to remove inconsistencies with explicit timexes (Chambers et al., 2014; Meng et al., 2017). In this work, we address this shortcoming by introducing a framework for incl"
P19-1433,S13-2001,0,0.117077,"t still see low performance. 3.2 Event Temporal Ordering Next, we investigate the effectiveness of our timex embeddings in the context of our full event temporal ordering model. We evaluate on two event temporal ordering datasets, one real and one artificially constructed. 3.2.1 Evaluation on MATRES We evaluate on the MATRES dataset proposed in Ning et al. (2018c). This dataset is designed to be less ambiguous than TimeBank-Dense (Cassidy et al., 2014). MATRES contains temporal annotations for documents from the TimeBank (Pustejovsky et al., 2003), AQUAINT (Graff, 2002) and Platinum datasets (UzZaman et al., 2013). We follow standard practice and use TimeBank and AQUAINT (256 articles) for training and Platinum (20 articles) for testing. Table 2 outlines the performance of the proposed approach on MATRES. We implemented the model proposed by Cheng and Miyao (2017) and compare against it. We evaluate the models using both GloVe and ELMo embeddings. Our results show substantial improvement over this baseline model. Moreover, including time embeddings as additional input to the improved models leads to a small improvement in the overall accuracy. However, we did not find the results to be statistically si"
Q14-1037,P12-1041,1,0.831013,"al., 2012). This corpus does not contain gold-standard entity links, so we cannot evaluate this portion of our model, though the model still exploits the information from Wikipedia to make coreference and named entity decisions. We will compare to prior coreference and named entity work in the system mentions setting. 6.1 ACE Evaluation We tokenize and sentence-split the ACE dataset using the tools bundled with Reconcile (Stoyanov et al., 2010) and parse it using the Berkeley Parser (Petrov et al., 2006). We use the train/test split from Stoyanov et al. (2009), Haghighi and Klein (2010), and Bansal and Klein (2012). FAHRNI I NDEP. J OINT ∆ over I NDEP. Prec. 81.15 80.26 83.26 +3.00 Non-NILS Rec. F1 78.10 79.60 76.30 78.23 77.67 80.37 +1.37 +2.14 NILS Prec. 41.25 33.39 35.19 +1.80 Rec. 61.10 54.47 65.42 +10.95 F1 49.25 41.40 45.77 +3.37 Accuracy 76.87 74.71 76.78 +2.07 Table 2: Detailed entity linking results on the ACE 2005 test set. We evaluate both our I NDEP. (task-specific factors only) and J OINT models and compare to the results of the FAHRNI model, a state-of-the-art entity linking system. We compare overall accuracy as well as performance at predicting NILS (mentions not in the knowledge base) a"
Q14-1037,P14-1005,0,0.243887,"Missing"
Q14-1037,N12-1004,1,0.849804,"Missing"
Q14-1037,D13-1057,0,0.0400117,"r I N DEP. and J OINT models compared to three strong systems: Durrett and Klein (2013), Fernandes et al. (2012) (the winner of the CoNLL shared task), and Bj¨orkelund and Kuhn (2014) (the best reported results on the dataset). Our J OINT method outperforms all three as well as the I NDEP. system.12 Next, we report results on named entity recognition. We use the same OntoNotes splits as for the coreference data; however, the New Testament (NT) 11 The NER-coreference portion of the model now resembles the skip-chain CRF from Finkel et al. (2005), though with soft coreference. 12 The systems of Chang et al. (2013) and Webster and Curran (2014) perform similarly to the F ERNANDES system; changes in the reference implementation of the metrics make exact comparison to printed numbers difficult. 486 I LLINOIS PASSOS I NDEP. J OINT ∆ over I NDEP. Prec. 82.00 − 83.79 85.22 +1.43 Rec. 84.95 − 81.53 82.89 +1.36 F1 83.45 82.24 82.64 84.04 +1.40 Table 5: Results for NER tagging on the OntoNotes 5.0 / CoNLL 2011 test set. We compare our systems to the Illinois system (Ratinov and Roth, 2009) and the system of Passos et al. (2014). Our model outperforms both other systems in terms of F1 , and once again joint mode"
Q14-1037,D13-1184,0,0.63446,"to the method of Durrett et al. (2013). Figure 1 shows an example of the effects such factors can capture. The non-locality of coreference factors make exact inference intractable, but we find that belief propagation is a suitable approximation technique and performs well. Our joint modeling of these three tasks is motivated by their heavy interdependencies, which have been noted in previous work (discussed more in Section 7). Entity linking has been employed for coreference resolution (Ponzetto and Strube, 2006; Rahman and Ng, 2011; Ratinov and Roth, 2012) and coreference for entity linking (Cheng and Roth, 2013) as part of pipelined systems. Past work has 477 Transactions of the Association for Computational Linguistics, vol. 2, pp. 477–490, 2014. Action Editor: Jason Eisner. c Submitted 8/2014; Revised 10/2014; Published November 1, 2014. 2014 Association for Computational Linguistics. en.wikipedia.org/wiki/Dell Infobox type: company en.wikipedia.org/wiki/Michael_Dell Infobox type: person ORGANIZATION PERSON Revenues of $14.5 billion were posted by Dell1. The company1 ... Figure 1: Coreference can help resolve ambiguous cases of semantic types or entity links: propagating information across corefere"
Q14-1037,D07-1074,0,0.511529,"How do we characterize the collection of entities present in a document? Two broad threads exist in the literature. The first is coreference resolution (Soon et al., 2001; Ng, 2010; Pradhan et al., 2011), which identifies clusters of mentions in a document referring to the same entity. This process gives us access to useful information about the referents of pronouns and nominal expressions, but because clusters are local to each document, it is often hard to situate document entities in a broader context. A separate line of work has considered the problem of entity linking or “Wikification” (Cucerzan, 2007; Milne and Witten, 2008; Ji and Grishman, 2011), where mentions are linked to entries in a given knowledge 1 System available at http://nlp.cs.berkeley.edu In this paper, we describe a joint model of coreference, entity linking, and semantic typing (named entity recognition) using a structured conditional random field. Variables in the model capture decisions about antecedence, semantic type, and entity links for each mention. Unary factors on these variables incorporate features that are commonly employed when solving each task in isolation. Binary and higher-order factors capture interactio"
Q14-1037,H05-1013,0,0.129353,"Missing"
Q14-1037,D08-1069,0,0.0764432,"mplement high-performing models for each task. State-of-the-art approaches to coreference (Durrett and Klein, 2013) and entity linking (Ratinov et al., 2011) already have this independent structure and Ratinov and Roth (2009) note that it is a reasonable assumption to make for NER as well.6 In this section, we describe the features present in the task-specific factors of each type (which also serve as our three separate baseline systems). 3.1.1 Coreference Our modeling of the coreference output space (as antecedents chosen for each mention) follows the mention-ranking approach to coreference (Denis and Baldridge, 2008; Durrett and Klein, 2013). Our feature set is that of Durrett and Klein, targeting surface properties of mentions: for each mention, we examine the first word, head word, last word, context words, the mention’s length, and whether the mention is nominal, proper or pronominal. Anaphoricity features examine each of these properties in turn; coreference features conjoin various properties between mention pairs and also use properties of the mention pair itself, such as the distance between the mentions and whether their heads match. Note that this baseline does not rely on having access to named"
Q14-1037,C10-1032,0,0.056639,"e clusters from Koo et al. (2008), and 6) common bigrams of word shape and word identity. 6 Pairwise potentials in sequence-based NER are useful for producing coherent output (e.g. prohibiting configurations like O I - PER ), but since we have so far defined the task as operating over fixed mentions, this structural constraint does not come into play for our system. 480 3.1.3 Entity Linking Our entity linking system diverges more substantially from past work than the coreference or NER systems. Most entity linking systems operate in two distinct phases (Cucerzan, 2007; Milne and Witten, 2008; Dredze et al., 2010; Ratinov et al., 2011). First, in the candidate generation phase, a system generates a ranked set of possible candidates for a given mention by querying Wikipedia. The standard approach for doing so is to collect all hyperlinks in Wikipedia and associate each hyperlinked span of text (e.g. Michael Jordan) with a distribution over titles of Wikipedia articles it is observed to link to (Michael Jordan, Michael I. Jordan, etc.). Second, in the disambiguation phase, a learned model selects the correct candidate from the set of possibilities. As noted by Hachey et al. (2013) and Guo et al. (2013),"
Q14-1037,D13-1203,1,0.830037,"with standard factor graph notation; features over a particular set of output variables (and x) are associated with factors connected to those variables. Figure 3 shows the task-specific factors in the model, discussed next in Section 3.1. Higher-order factors coupling variables between tasks are discussed in Section 3.2. 3.1 Independent Model Figure 3 shows a version of the model with only task-specific factors. Though this framework is structurally simple, it is nevertheless powerful enough for us to implement high-performing models for each task. State-of-the-art approaches to coreference (Durrett and Klein, 2013) and entity linking (Ratinov et al., 2011) already have this independent structure and Ratinov and Roth (2009) note that it is a reasonable assumption to make for NER as well.6 In this section, we describe the features present in the task-specific factors of each type (which also serve as our three separate baseline systems). 3.1.1 Coreference Our modeling of the coreference output space (as antecedents chosen for each mention) follows the mention-ranking approach to coreference (Denis and Baldridge, 2008; Durrett and Klein, 2013). Our feature set is that of Durrett and Klein, targeting surfac"
Q14-1037,P13-1012,1,0.864729,"type, and entity links for each mention. Unary factors on these variables incorporate features that are commonly employed when solving each task in isolation. Binary and higher-order factors capture interactions between pairs of tasks. For entity linking and NER, factors capture a mapping between NER’s semantic types and Wikipedia’s semantics as described by infoboxes, categories, and article text. Coreference interacts with the other tasks in a more complex way, via factors that softly encourage consistency of semantic types and entity links across coreference arcs, similar to the method of Durrett et al. (2013). Figure 1 shows an example of the effects such factors can capture. The non-locality of coreference factors make exact inference intractable, but we find that belief propagation is a suitable approximation technique and performs well. Our joint modeling of these three tasks is motivated by their heavy interdependencies, which have been noted in previous work (discussed more in Section 7). Entity linking has been employed for coreference resolution (Ponzetto and Strube, 2006; Rahman and Ng, 2011; Ratinov and Roth, 2012) and coreference for entity linking (Cheng and Roth, 2013) as part of pipel"
Q14-1037,E14-1052,0,0.169282,"addressed in past joint modeling efforts (Daum´e and Marcu, 2005; Li and Ji, 2014), but that is outside the scope of the current work. 478 models. Finally, as a structured CRF, it is conceptually no more complex than its component models and its behavior can be understood using the same intuition. We apply our model to two datasets, ACE 2005 and OntoNotes, with different mention standards and layers of annotation. In both settings, our joint model outperforms our independent baseline models. On ACE, we achieve state-of-the-art entity linking results, matching the performance of the system of Fahrni and Strube (2014). On OntoNotes, we match the performance of the best published coreference system (Bj¨orkelund and Kuhn, 2014) and outperform two strong NER systems (Ratinov and Roth, 2009; Passos et al., 2014). 2 Motivating Examples We first present two examples to motivate our approach. Figure 1 shows an example of a case where coreference is beneficial for named entity recognition and entity linking. The company is clearly coreferent to Dell by virtue of the lack of other possible antecedents; this in turn indicates that Dell refers to the corporation rather than to Michael Dell. This effect can be capture"
Q14-1037,N09-1037,0,0.0137602,"s (2013) jointly model NER and entity linking in such a way that they maintain uncertainty over mention boundaries, allowing information from Wikipedia to inform segmentation choices. We could strengthen our model by integrating this capability; however, the primary cause of errors for mention detection on OntoNotes is parsing ambiguities rather than named entity ambiguities, so we would be unlikely to see improvements in the experiments presented here. Beyond maintaining uncertainty over mention boundaries, we might also consider maintaining uncertainty over the entire parse structure, as in Finkel and Manning (2009), who consider parsing and named entity recognition together with a PCFG. 8 Conclusion We return to our initial motivation for joint modeling, namely that the three tasks we address have the potential to influence one another. Table 3 shows 487 that failing to exploit any of the pairwise interactions between the tasks causes lower performance on at least one of them. Therefore, any pipelined system would necessarily underperform a joint model on whatever task came first in the pipeline, which is undesirable given the importance of these tasks. The trend towards broader and deeper NLP pipelines"
Q14-1037,P05-1045,0,0.187866,"ATION PERSON Revenues of $14.5 billion were posted by Dell1. The company1 ... Figure 1: Coreference can help resolve ambiguous cases of semantic types or entity links: propagating information across coreference arcs can inform us that, in this context, Dell is an organization and should therefore link to the article on Dell in Wikipedia. shown that tighter integration of coreference and entity linking is promising (Hajishirzi et al., 2013; Zheng et al., 2013); we extend these approaches and model the entire process more holistically. Named entity recognition is improved by simple coreference (Finkel et al., 2005; Ratinov and Roth, 2009) and knowledge from Wikipedia (Kazama and Torisawa, 2007; Ratinov and Roth, 2009; Nothman et al., 2013; Sil and Yates, 2013). Joint models of coreference and NER have been proposed in Haghighi and Klein (2010) and Durrett et al. (2013), but in neither case was supervised data used for both tasks. Technically, our model is most closely related to that of Singh et al. (2013), who handle coreference, named entity recognition, and relation extraction.2 Our system is novel in three ways: the choice of tasks to model jointly, the fact that we maintain uncertainty about all d"
Q14-1037,N10-1112,0,0.039656,"latent structure has been employed in prior work as well (Fernandes et al., 2012; Durrett and Klein, 2013). We adapt this objective to exploit parameterized loss functions for each task by modifying the distribution as follows: p0 (a, t, e|x; θ) ∝ p(a, t, e, x) exp [αc `c (a, C ∗ ) +αt `t (t, t∗ ) + αe `e (e, e∗ )] where `c , `t , and `e are task-specific loss functions with weight parameters α. This technique, softmaxmargin, allows us to shape the distribution learned by the model and encourage the model to move probability mass away from outputs that are bad according to our loss functions (Gimpel and Smith, 2010). As in Durrett and Klein (2013), we take αc = 1 and use `c as defined there, penalizing the model by αc,FA = 0.1 for linking up a mention that should have been nonanaphoric, by αc,FN = 3 for calling nonanaphoric a mention that should have an antecedent, and by αc,WL = 1 for picking the wrong antecedent for an anaphoric mention. `t and `e are simply Hamming distance, with αt = 3 and αe = 0 for all experiments. We found that the outcome of learning was not particularly sensitive to these parameters.7 We optimize our objective using AdaGrad (Duchi et al., 2011) with L1 regularization and λ = 0.0"
Q14-1037,D09-1120,1,0.347203,"ities. Integration with the rest of the model, learning, and inference would remain unchanged. However, while such features have been employed in past entity linking systems (Ratinov et al., 2011; Hoffart et al., 2011), Ratinov et al. found them to be of limited utility, so we omit them from the present work. 3.2 Cross-task Interaction Factors We now add factors that tie the predictions of multiple output variables in a feature-based way. Figure 4 shows the general structure of these factors. Each 481 • Copula in the first sentence (is a British politician); used for coreference previously in Haghighi and Klein (2009) We fire features that conjoin the information from the selected Wikipedia article with the selected NER type. Because these types of information from Wikipedia are of a moderate granularity, we should be able to learn a mapping between them and NER types and exploit Wikipedia as a soft gazetteer. 3.2.2 Coreference and NER Coreference can improve NER by ensuring consistent semantic type predictions across coreferent mentions; likewise, NER can help coreference by encouraging the system to link up mentions of the same type. The factors we implement for these purposes closely resemble the factor"
Q14-1037,N10-1061,1,0.736533,"s that, in this context, Dell is an organization and should therefore link to the article on Dell in Wikipedia. shown that tighter integration of coreference and entity linking is promising (Hajishirzi et al., 2013; Zheng et al., 2013); we extend these approaches and model the entire process more holistically. Named entity recognition is improved by simple coreference (Finkel et al., 2005; Ratinov and Roth, 2009) and knowledge from Wikipedia (Kazama and Torisawa, 2007; Ratinov and Roth, 2009; Nothman et al., 2013; Sil and Yates, 2013). Joint models of coreference and NER have been proposed in Haghighi and Klein (2010) and Durrett et al. (2013), but in neither case was supervised data used for both tasks. Technically, our model is most closely related to that of Singh et al. (2013), who handle coreference, named entity recognition, and relation extraction.2 Our system is novel in three ways: the choice of tasks to model jointly, the fact that we maintain uncertainty about all decisions throughout inference (rather than using a greedy approach), and the feature sets we deploy for cross-task interactions. In designing a joint model, we would like to preserve the modularity, efficiency, and structural simplici"
Q14-1037,D13-1029,0,0.607243,"ed November 1, 2014. 2014 Association for Computational Linguistics. en.wikipedia.org/wiki/Dell Infobox type: company en.wikipedia.org/wiki/Michael_Dell Infobox type: person ORGANIZATION PERSON Revenues of $14.5 billion were posted by Dell1. The company1 ... Figure 1: Coreference can help resolve ambiguous cases of semantic types or entity links: propagating information across coreference arcs can inform us that, in this context, Dell is an organization and should therefore link to the article on Dell in Wikipedia. shown that tighter integration of coreference and entity linking is promising (Hajishirzi et al., 2013; Zheng et al., 2013); we extend these approaches and model the entire process more holistically. Named entity recognition is improved by simple coreference (Finkel et al., 2005; Ratinov and Roth, 2009) and knowledge from Wikipedia (Kazama and Torisawa, 2007; Ratinov and Roth, 2009; Nothman et al., 2013; Sil and Yates, 2013). Joint models of coreference and NER have been proposed in Haghighi and Klein (2010) and Durrett et al. (2013), but in neither case was supervised data used for both tasks. Technically, our model is most closely related to that of Singh et al. (2013), who handle coreferenc"
Q14-1037,D11-1072,0,0.661577,"Missing"
Q14-1037,N06-2015,0,0.289643,"the latter is much more computationally difficult to find and would be largely the same, since the posterior distributions of the ai are quite peaked. 6 Experiments We present results on two corpora. First, we use the ACE 2005 corpus (NIST, 2005): this corpus annotates mentions complete with coreference, semantic types (per mention), and entity links (also per mention) later added by Bentivogli et al. (2010). We evaluate on gold mentions in this setting for comparability with prior work on entity linking; we lift this restriction in Section 6.3. Second, we evaluate on the OntoNotes 5 corpus (Hovy et al., 2006) as used in the CoNLL 2012 coreference shared task (Pradhan et al., 2012). This corpus does not contain gold-standard entity links, so we cannot evaluate this portion of our model, though the model still exploits the information from Wikipedia to make coreference and named entity decisions. We will compare to prior coreference and named entity work in the system mentions setting. 6.1 ACE Evaluation We tokenize and sentence-split the ACE dataset using the tools bundled with Reconcile (Stoyanov et al., 2010) and parse it using the Berkeley Parser (Petrov et al., 2006). We use the train/test spli"
Q14-1037,P11-1115,0,0.00946756,"entities present in a document? Two broad threads exist in the literature. The first is coreference resolution (Soon et al., 2001; Ng, 2010; Pradhan et al., 2011), which identifies clusters of mentions in a document referring to the same entity. This process gives us access to useful information about the referents of pronouns and nominal expressions, but because clusters are local to each document, it is often hard to situate document entities in a broader context. A separate line of work has considered the problem of entity linking or “Wikification” (Cucerzan, 2007; Milne and Witten, 2008; Ji and Grishman, 2011), where mentions are linked to entries in a given knowledge 1 System available at http://nlp.cs.berkeley.edu In this paper, we describe a joint model of coreference, entity linking, and semantic typing (named entity recognition) using a structured conditional random field. Variables in the model capture decisions about antecedence, semantic type, and entity links for each mention. Unary factors on these variables incorporate features that are commonly employed when solving each task in isolation. Binary and higher-order factors capture interactions between pairs of tasks. For entity linking an"
Q14-1037,D07-1073,0,0.183991,".. Figure 1: Coreference can help resolve ambiguous cases of semantic types or entity links: propagating information across coreference arcs can inform us that, in this context, Dell is an organization and should therefore link to the article on Dell in Wikipedia. shown that tighter integration of coreference and entity linking is promising (Hajishirzi et al., 2013; Zheng et al., 2013); we extend these approaches and model the entire process more holistically. Named entity recognition is improved by simple coreference (Finkel et al., 2005; Ratinov and Roth, 2009) and knowledge from Wikipedia (Kazama and Torisawa, 2007; Ratinov and Roth, 2009; Nothman et al., 2013; Sil and Yates, 2013). Joint models of coreference and NER have been proposed in Haghighi and Klein (2010) and Durrett et al. (2013), but in neither case was supervised data used for both tasks. Technically, our model is most closely related to that of Singh et al. (2013), who handle coreference, named entity recognition, and relation extraction.2 Our system is novel in three ways: the choice of tasks to model jointly, the fact that we maintain uncertainty about all decisions throughout inference (rather than using a greedy approach), and the feat"
Q14-1037,P08-1068,0,0.00800239,"concatenation of standard NER surface features associated with each token in that chunk. We use surface token features similar to those from previous work (Zhang and Johnson, 2003; Ratinov and Roth, 2009; Passos et al., 2014): for tokens at offsets of {−2, −1, 0, 1, 2} from the current token, we fire features based on 1) word identity, 2) POS tag, 3) word class (based on capitalization, presence of numbers, suffixes, etc.), 4) word shape (based on the pattern of uppercase and lowercase letters, digits, and punctuation), 5) Brown cluster prefixes of length 4, 6, 10, 20 using the clusters from Koo et al. (2008), and 6) common bigrams of word shape and word identity. 6 Pairwise potentials in sequence-based NER are useful for producing coherent output (e.g. prohibiting configurations like O I - PER ), but since we have so far defined the task as operating over fixed mentions, this structural constraint does not come into play for our system. 480 3.1.3 Entity Linking Our entity linking system diverges more substantially from past work than the coreference or NER systems. Most entity linking systems operate in two distinct phases (Cucerzan, 2007; Milne and Witten, 2008; Dredze et al., 2010; Ratinov et a"
Q14-1037,W11-1902,0,0.0242695,"Missing"
Q14-1037,P14-1038,0,0.0214376,"ke to preserve the modularity, efficiency, and structural simplicity of pipelined approaches. Our model’s feature-based structure permits improvement of features specific to a particular task or to a pair of tasks. By pruning variable domains with a coarse model and using approximate inference via belief propagation, we maintain efficiency and our model is only a factor of two slower than the union of the individual 2 Our model could potentially be extended to handle relation extraction or mention detection, which has also been addressed in past joint modeling efforts (Daum´e and Marcu, 2005; Li and Ji, 2014), but that is outside the scope of the current work. 478 models. Finally, as a structured CRF, it is conceptually no more complex than its component models and its behavior can be understood using the same intuition. We apply our model to two datasets, ACE 2005 and OntoNotes, with different mention standards and layers of annotation. In both settings, our joint model outperforms our independent baseline models. On ACE, we achieve state-of-the-art entity linking results, matching the performance of the system of Fahrni and Strube (2014). On OntoNotes, we match the performance of the best publis"
Q14-1037,H05-1004,0,0.380812,".07 Table 2: Detailed entity linking results on the ACE 2005 test set. We evaluate both our I NDEP. (task-specific factors only) and J OINT models and compare to the results of the FAHRNI model, a state-of-the-art entity linking system. We compare overall accuracy as well as performance at predicting NILS (mentions not in the knowledge base) and nonNILS . The J OINT model roughly matches the performance of FAHRNI and gives strong gains over the I NDEP. system. Table 1 shows our results. Coreference results are reported using MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and CEAFe (Luo, 2005), as well as their average, the CoNLL metric, all computed from the reference implementation of the CoNLL scorer (Pradhan et al., 2014). We see that the joint model improves all three tasks compared to the individual task models in the baseline. More in-depth entity linking results are shown in Table 2. We both evaluate on overall accuracy (how many mentions are correctly linked) as well as two more specific criteria: precision/recall/F1 of nonNIL9 predictions, and precision/recall/F1 of NIL predictions. This latter measure may be important if a system designer is trying to identify new entiti"
Q14-1037,P10-1142,0,0.0120988,"ry factors encode local features from strong baselines for each task. We then add binary and ternary factors to capture cross-task interactions, such as the constraint that coreferent mentions have the same semantic type. On the ACE 2005 and OntoNotes datasets, we achieve state-of-theart results for all three tasks. Moreover, joint modeling improves performance on each task over strong independent baselines.1 1 Introduction How do we characterize the collection of entities present in a document? Two broad threads exist in the literature. The first is coreference resolution (Soon et al., 2001; Ng, 2010; Pradhan et al., 2011), which identifies clusters of mentions in a document referring to the same entity. This process gives us access to useful information about the referents of pronouns and nominal expressions, but because clusters are local to each document, it is often hard to situate document entities in a broader context. A separate line of work has considered the problem of entity linking or “Wikification” (Cucerzan, 2007; Milne and Witten, 2008; Ji and Grishman, 2011), where mentions are linked to entries in a given knowledge 1 System available at http://nlp.cs.berkeley.edu In this p"
Q14-1037,W14-1609,0,0.0617995,"o more complex than its component models and its behavior can be understood using the same intuition. We apply our model to two datasets, ACE 2005 and OntoNotes, with different mention standards and layers of annotation. In both settings, our joint model outperforms our independent baseline models. On ACE, we achieve state-of-the-art entity linking results, matching the performance of the system of Fahrni and Strube (2014). On OntoNotes, we match the performance of the best published coreference system (Bj¨orkelund and Kuhn, 2014) and outperform two strong NER systems (Ratinov and Roth, 2009; Passos et al., 2014). 2 Motivating Examples We first present two examples to motivate our approach. Figure 1 shows an example of a case where coreference is beneficial for named entity recognition and entity linking. The company is clearly coreferent to Dell by virtue of the lack of other possible antecedents; this in turn indicates that Dell refers to the corporation rather than to Michael Dell. This effect can be captured for entity linking by a feature tying the lexical item company to the fact that C OMPANY is in the Wikipedia infobox for Dell,3 thereby helping the linker make the correct decision. This would"
Q14-1037,P06-1055,1,0.154267,"te on the OntoNotes 5 corpus (Hovy et al., 2006) as used in the CoNLL 2012 coreference shared task (Pradhan et al., 2012). This corpus does not contain gold-standard entity links, so we cannot evaluate this portion of our model, though the model still exploits the information from Wikipedia to make coreference and named entity decisions. We will compare to prior coreference and named entity work in the system mentions setting. 6.1 ACE Evaluation We tokenize and sentence-split the ACE dataset using the tools bundled with Reconcile (Stoyanov et al., 2010) and parse it using the Berkeley Parser (Petrov et al., 2006). We use the train/test split from Stoyanov et al. (2009), Haghighi and Klein (2010), and Bansal and Klein (2012). FAHRNI I NDEP. J OINT ∆ over I NDEP. Prec. 81.15 80.26 83.26 +3.00 Non-NILS Rec. F1 78.10 79.60 76.30 78.23 77.67 80.37 +1.37 +2.14 NILS Prec. 41.25 33.39 35.19 +1.80 Rec. 61.10 54.47 65.42 +10.95 F1 49.25 41.40 45.77 +3.37 Accuracy 76.87 74.71 76.78 +2.07 Table 2: Detailed entity linking results on the ACE 2005 test set. We evaluate both our I NDEP. (task-specific factors only) and J OINT models and compare to the results of the FAHRNI model, a state-of-the-art entity linking sys"
Q14-1037,N06-1025,0,0.731424,"ctors that softly encourage consistency of semantic types and entity links across coreference arcs, similar to the method of Durrett et al. (2013). Figure 1 shows an example of the effects such factors can capture. The non-locality of coreference factors make exact inference intractable, but we find that belief propagation is a suitable approximation technique and performs well. Our joint modeling of these three tasks is motivated by their heavy interdependencies, which have been noted in previous work (discussed more in Section 7). Entity linking has been employed for coreference resolution (Ponzetto and Strube, 2006; Rahman and Ng, 2011; Ratinov and Roth, 2012) and coreference for entity linking (Cheng and Roth, 2013) as part of pipelined systems. Past work has 477 Transactions of the Association for Computational Linguistics, vol. 2, pp. 477–490, 2014. Action Editor: Jason Eisner. c Submitted 8/2014; Revised 10/2014; Published November 1, 2014. 2014 Association for Computational Linguistics. en.wikipedia.org/wiki/Dell Infobox type: company en.wikipedia.org/wiki/Michael_Dell Infobox type: person ORGANIZATION PERSON Revenues of $14.5 billion were posted by Dell1. The company1 ... Figure 1: Coreference can"
Q14-1037,W11-1901,0,0.0278341,"encode local features from strong baselines for each task. We then add binary and ternary factors to capture cross-task interactions, such as the constraint that coreferent mentions have the same semantic type. On the ACE 2005 and OntoNotes datasets, we achieve state-of-theart results for all three tasks. Moreover, joint modeling improves performance on each task over strong independent baselines.1 1 Introduction How do we characterize the collection of entities present in a document? Two broad threads exist in the literature. The first is coreference resolution (Soon et al., 2001; Ng, 2010; Pradhan et al., 2011), which identifies clusters of mentions in a document referring to the same entity. This process gives us access to useful information about the referents of pronouns and nominal expressions, but because clusters are local to each document, it is often hard to situate document entities in a broader context. A separate line of work has considered the problem of entity linking or “Wikification” (Cucerzan, 2007; Milne and Witten, 2008; Ji and Grishman, 2011), where mentions are linked to entries in a given knowledge 1 System available at http://nlp.cs.berkeley.edu In this paper, we describe a joi"
Q14-1037,W12-4501,0,0.593459,"e largely the same, since the posterior distributions of the ai are quite peaked. 6 Experiments We present results on two corpora. First, we use the ACE 2005 corpus (NIST, 2005): this corpus annotates mentions complete with coreference, semantic types (per mention), and entity links (also per mention) later added by Bentivogli et al. (2010). We evaluate on gold mentions in this setting for comparability with prior work on entity linking; we lift this restriction in Section 6.3. Second, we evaluate on the OntoNotes 5 corpus (Hovy et al., 2006) as used in the CoNLL 2012 coreference shared task (Pradhan et al., 2012). This corpus does not contain gold-standard entity links, so we cannot evaluate this portion of our model, though the model still exploits the information from Wikipedia to make coreference and named entity decisions. We will compare to prior coreference and named entity work in the system mentions setting. 6.1 ACE Evaluation We tokenize and sentence-split the ACE dataset using the tools bundled with Reconcile (Stoyanov et al., 2010) and parse it using the Berkeley Parser (Petrov et al., 2006). We use the train/test split from Stoyanov et al. (2009), Haghighi and Klein (2010), and Bansal and"
Q14-1037,P14-2006,0,0.14136,"bjective from the different tasks, addressing Singh et al. (2013)’s objection to single objectives for joint models. Dev MUC I NDEP. J OINT ∆ 77.95 79.41 +1.46 B3 74.81 75.56 +0.75 CEAFe 71.84 73.34 +1.50 Avg. 74.87 76.10 +1.23 NER 83.04 85.94 +2.90 Link 73.07 75.69 +2.62 Test MUC 81.03 81.41 +0.42 B3 74.89 74.70 −0.19 CEAFe 72.56 72.93 +0.37 Avg. 76.16 76.35 +0.19 NER 82.35 85.60 +3.25 Link 74.71 76.78 +2.07 Table 1: Results on the ACE 2005 dev and test sets for the I NDEP. (task-specific factors only) and J OINT models. Coreference metrics are computed using their reference implementations (Pradhan et al., 2014). We report accuracy on NER because the set of mentions is fixed and all mentions have named entity types. Coreference and NER are compared to prior work in a more standard setting in Section 6.3. Finally, we also report accuracy of our entity linker (including links to NIL); entity linking is analyzed more thoroughly in Table 2. Bolded values represent statistically significant improvements with p < 0.05 according to a bootstrap resampling test. 5 Inference For both learning and decoding, inference consists of computing marginals for individual variables or for sets of variables adjacent to a"
Q14-1037,D10-1048,0,0.0112298,"ing baselines from the literature: the Illinois NER system of Ratinov and Roth (2009) and the results of Passos et al. (2014). Table 5 shows that we outperform both prior systems in terms of F1 , though the I LLINOIS system features higher recall while our system features higher precision. 7 Related Work There are two closely related threads of prior work: those that address the tasks we consider in a different way and those that propose joint models for other related sets of tasks. In the first category, Hajishirzi et al. (2013) integrate entity linking into a sieve-based coreference system (Raghunathan et al., 2010), the aim being to propagate link decisions throughout coreference chains, block coreference links between different entities, and use semantic information to make additional coreference links. Zheng et al. (2013) build coreference clusters greedily left-to-right and maintain entity link information for each cluster, namely a list of possible targets in the knowledge base as well as a current best link target that is used to extract features (though that might not be the target that is chosen by the end of inference). Cheng and Roth (2013) use coreference as a preprocessing step for entity lin"
Q14-1037,P11-1082,0,0.0414084,"consistency of semantic types and entity links across coreference arcs, similar to the method of Durrett et al. (2013). Figure 1 shows an example of the effects such factors can capture. The non-locality of coreference factors make exact inference intractable, but we find that belief propagation is a suitable approximation technique and performs well. Our joint modeling of these three tasks is motivated by their heavy interdependencies, which have been noted in previous work (discussed more in Section 7). Entity linking has been employed for coreference resolution (Ponzetto and Strube, 2006; Rahman and Ng, 2011; Ratinov and Roth, 2012) and coreference for entity linking (Cheng and Roth, 2013) as part of pipelined systems. Past work has 477 Transactions of the Association for Computational Linguistics, vol. 2, pp. 477–490, 2014. Action Editor: Jason Eisner. c Submitted 8/2014; Revised 10/2014; Published November 1, 2014. 2014 Association for Computational Linguistics. en.wikipedia.org/wiki/Dell Infobox type: company en.wikipedia.org/wiki/Michael_Dell Infobox type: person ORGANIZATION PERSON Revenues of $14.5 billion were posted by Dell1. The company1 ... Figure 1: Coreference can help resolve ambiguo"
Q14-1037,W09-1119,0,0.677451,"of $14.5 billion were posted by Dell1. The company1 ... Figure 1: Coreference can help resolve ambiguous cases of semantic types or entity links: propagating information across coreference arcs can inform us that, in this context, Dell is an organization and should therefore link to the article on Dell in Wikipedia. shown that tighter integration of coreference and entity linking is promising (Hajishirzi et al., 2013; Zheng et al., 2013); we extend these approaches and model the entire process more holistically. Named entity recognition is improved by simple coreference (Finkel et al., 2005; Ratinov and Roth, 2009) and knowledge from Wikipedia (Kazama and Torisawa, 2007; Ratinov and Roth, 2009; Nothman et al., 2013; Sil and Yates, 2013). Joint models of coreference and NER have been proposed in Haghighi and Klein (2010) and Durrett et al. (2013), but in neither case was supervised data used for both tasks. Technically, our model is most closely related to that of Singh et al. (2013), who handle coreference, named entity recognition, and relation extraction.2 Our system is novel in three ways: the choice of tasks to model jointly, the fact that we maintain uncertainty about all decisions throughout infer"
Q14-1037,D12-1113,0,0.229189,"tic types and entity links across coreference arcs, similar to the method of Durrett et al. (2013). Figure 1 shows an example of the effects such factors can capture. The non-locality of coreference factors make exact inference intractable, but we find that belief propagation is a suitable approximation technique and performs well. Our joint modeling of these three tasks is motivated by their heavy interdependencies, which have been noted in previous work (discussed more in Section 7). Entity linking has been employed for coreference resolution (Ponzetto and Strube, 2006; Rahman and Ng, 2011; Ratinov and Roth, 2012) and coreference for entity linking (Cheng and Roth, 2013) as part of pipelined systems. Past work has 477 Transactions of the Association for Computational Linguistics, vol. 2, pp. 477–490, 2014. Action Editor: Jason Eisner. c Submitted 8/2014; Revised 10/2014; Published November 1, 2014. 2014 Association for Computational Linguistics. en.wikipedia.org/wiki/Dell Infobox type: company en.wikipedia.org/wiki/Michael_Dell Infobox type: person ORGANIZATION PERSON Revenues of $14.5 billion were posted by Dell1. The company1 ... Figure 1: Coreference can help resolve ambiguous cases of semantic type"
Q14-1037,P11-1138,0,0.476003,"over a particular set of output variables (and x) are associated with factors connected to those variables. Figure 3 shows the task-specific factors in the model, discussed next in Section 3.1. Higher-order factors coupling variables between tasks are discussed in Section 3.2. 3.1 Independent Model Figure 3 shows a version of the model with only task-specific factors. Though this framework is structurally simple, it is nevertheless powerful enough for us to implement high-performing models for each task. State-of-the-art approaches to coreference (Durrett and Klein, 2013) and entity linking (Ratinov et al., 2011) already have this independent structure and Ratinov and Roth (2009) note that it is a reasonable assumption to make for NER as well.6 In this section, we describe the features present in the task-specific factors of each type (which also serve as our three separate baseline systems). 3.1.1 Coreference Our modeling of the coreference output space (as antecedents chosen for each mention) follows the mention-ranking approach to coreference (Denis and Baldridge, 2008; Durrett and Klein, 2013). Our feature set is that of Durrett and Klein, targeting surface properties of mentions: for each mention"
Q14-1037,D08-1016,0,0.00918921,"Missing"
Q14-1037,J01-4004,0,0.574465,"l random field. Unary factors encode local features from strong baselines for each task. We then add binary and ternary factors to capture cross-task interactions, such as the constraint that coreferent mentions have the same semantic type. On the ACE 2005 and OntoNotes datasets, we achieve state-of-theart results for all three tasks. Moreover, joint modeling improves performance on each task over strong independent baselines.1 1 Introduction How do we characterize the collection of entities present in a document? Two broad threads exist in the literature. The first is coreference resolution (Soon et al., 2001; Ng, 2010; Pradhan et al., 2011), which identifies clusters of mentions in a document referring to the same entity. This process gives us access to useful information about the referents of pronouns and nominal expressions, but because clusters are local to each document, it is often hard to situate document entities in a broader context. A separate line of work has considered the problem of entity linking or “Wikification” (Cucerzan, 2007; Milne and Witten, 2008; Ji and Grishman, 2011), where mentions are linked to entries in a given knowledge 1 System available at http://nlp.cs.berkeley.edu"
Q14-1037,P09-1074,0,0.0101264,"d in the CoNLL 2012 coreference shared task (Pradhan et al., 2012). This corpus does not contain gold-standard entity links, so we cannot evaluate this portion of our model, though the model still exploits the information from Wikipedia to make coreference and named entity decisions. We will compare to prior coreference and named entity work in the system mentions setting. 6.1 ACE Evaluation We tokenize and sentence-split the ACE dataset using the tools bundled with Reconcile (Stoyanov et al., 2010) and parse it using the Berkeley Parser (Petrov et al., 2006). We use the train/test split from Stoyanov et al. (2009), Haghighi and Klein (2010), and Bansal and Klein (2012). FAHRNI I NDEP. J OINT ∆ over I NDEP. Prec. 81.15 80.26 83.26 +3.00 Non-NILS Rec. F1 78.10 79.60 76.30 78.23 77.67 80.37 +1.37 +2.14 NILS Prec. 41.25 33.39 35.19 +1.80 Rec. 61.10 54.47 65.42 +10.95 F1 49.25 41.40 45.77 +3.37 Accuracy 76.87 74.71 76.78 +2.07 Table 2: Detailed entity linking results on the ACE 2005 test set. We evaluate both our I NDEP. (task-specific factors only) and J OINT models and compare to the results of the FAHRNI model, a state-of-the-art entity linking system. We compare overall accuracy as well as performance a"
Q14-1037,P10-2029,0,0.00810508,"ing; we lift this restriction in Section 6.3. Second, we evaluate on the OntoNotes 5 corpus (Hovy et al., 2006) as used in the CoNLL 2012 coreference shared task (Pradhan et al., 2012). This corpus does not contain gold-standard entity links, so we cannot evaluate this portion of our model, though the model still exploits the information from Wikipedia to make coreference and named entity decisions. We will compare to prior coreference and named entity work in the system mentions setting. 6.1 ACE Evaluation We tokenize and sentence-split the ACE dataset using the tools bundled with Reconcile (Stoyanov et al., 2010) and parse it using the Berkeley Parser (Petrov et al., 2006). We use the train/test split from Stoyanov et al. (2009), Haghighi and Klein (2010), and Bansal and Klein (2012). FAHRNI I NDEP. J OINT ∆ over I NDEP. Prec. 81.15 80.26 83.26 +3.00 Non-NILS Rec. F1 78.10 79.60 76.30 78.23 77.67 80.37 +1.37 +2.14 NILS Prec. 41.25 33.39 35.19 +1.80 Rec. 61.10 54.47 65.42 +10.95 F1 49.25 41.40 45.77 +3.37 Accuracy 76.87 74.71 76.78 +2.07 Table 2: Detailed entity linking results on the ACE 2005 test set. We evaluate both our I NDEP. (task-specific factors only) and J OINT models and compare to the resul"
Q14-1037,M95-1005,0,0.733428,"+10.95 F1 49.25 41.40 45.77 +3.37 Accuracy 76.87 74.71 76.78 +2.07 Table 2: Detailed entity linking results on the ACE 2005 test set. We evaluate both our I NDEP. (task-specific factors only) and J OINT models and compare to the results of the FAHRNI model, a state-of-the-art entity linking system. We compare overall accuracy as well as performance at predicting NILS (mentions not in the knowledge base) and nonNILS . The J OINT model roughly matches the performance of FAHRNI and gives strong gains over the I NDEP. system. Table 1 shows our results. Coreference results are reported using MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and CEAFe (Luo, 2005), as well as their average, the CoNLL metric, all computed from the reference implementation of the CoNLL scorer (Pradhan et al., 2014). We see that the joint model improves all three tasks compared to the individual task models in the baseline. More in-depth entity linking results are shown in Table 2. We both evaluate on overall accuracy (how many mentions are correctly linked) as well as two more specific criteria: precision/recall/F1 of nonNIL9 predictions, and precision/recall/F1 of NIL predictions. This latter measure may be important"
Q14-1037,C14-1201,0,0.0160604,"dels compared to three strong systems: Durrett and Klein (2013), Fernandes et al. (2012) (the winner of the CoNLL shared task), and Bj¨orkelund and Kuhn (2014) (the best reported results on the dataset). Our J OINT method outperforms all three as well as the I NDEP. system.12 Next, we report results on named entity recognition. We use the same OntoNotes splits as for the coreference data; however, the New Testament (NT) 11 The NER-coreference portion of the model now resembles the skip-chain CRF from Finkel et al. (2005), though with soft coreference. 12 The systems of Chang et al. (2013) and Webster and Curran (2014) perform similarly to the F ERNANDES system; changes in the reference implementation of the metrics make exact comparison to printed numbers difficult. 486 I LLINOIS PASSOS I NDEP. J OINT ∆ over I NDEP. Prec. 82.00 − 83.79 85.22 +1.43 Rec. 84.95 − 81.53 82.89 +1.36 F1 83.45 82.24 82.64 84.04 +1.40 Table 5: Results for NER tagging on the OntoNotes 5.0 / CoNLL 2011 test set. We compare our systems to the Illinois system (Ratinov and Roth, 2009) and the system of Passos et al. (2014). Our model outperforms both other systems in terms of F1 , and once again joint modeling gives substantial improve"
Q14-1037,W03-0434,0,0.0128739,"perties between mention pairs and also use properties of the mention pair itself, such as the distance between the mentions and whether their heads match. Note that this baseline does not rely on having access to named entity chunks. 3.1.2 Named Entity Recognition Our NER model places a distribution over possible semantic types for each mention, which corresponds to a fixed span of the input text. We define the features of a span to be the concatenation of standard NER surface features associated with each token in that chunk. We use surface token features similar to those from previous work (Zhang and Johnson, 2003; Ratinov and Roth, 2009; Passos et al., 2014): for tokens at offsets of {−2, −1, 0, 1, 2} from the current token, we fire features based on 1) word identity, 2) POS tag, 3) word class (based on capitalization, presence of numbers, suffixes, etc.), 4) word shape (based on the pattern of uppercase and lowercase letters, digits, and punctuation), 5) Brown cluster prefixes of length 4, 6, 10, 20 using the clusters from Koo et al. (2008), and 6) common bigrams of word shape and word identity. 6 Pairwise potentials in sequence-based NER are useful for producing coherent output (e.g. prohibiting con"
Q14-1037,W13-3517,0,0.227496,"Association for Computational Linguistics. en.wikipedia.org/wiki/Dell Infobox type: company en.wikipedia.org/wiki/Michael_Dell Infobox type: person ORGANIZATION PERSON Revenues of $14.5 billion were posted by Dell1. The company1 ... Figure 1: Coreference can help resolve ambiguous cases of semantic types or entity links: propagating information across coreference arcs can inform us that, in this context, Dell is an organization and should therefore link to the article on Dell in Wikipedia. shown that tighter integration of coreference and entity linking is promising (Hajishirzi et al., 2013; Zheng et al., 2013); we extend these approaches and model the entire process more holistically. Named entity recognition is improved by simple coreference (Finkel et al., 2005; Ratinov and Roth, 2009) and knowledge from Wikipedia (Kazama and Torisawa, 2007; Ratinov and Roth, 2009; Nothman et al., 2013; Sil and Yates, 2013). Joint models of coreference and NER have been proposed in Haghighi and Klein (2010) and Durrett et al. (2013), but in neither case was supervised data used for both tasks. Technically, our model is most closely related to that of Singh et al. (2013), who handle coreference, named entity recog"
Q14-1037,W12-4502,0,\N,Missing
Q14-1037,W10-3503,0,\N,Missing
W19-1502,N18-1204,0,0.0392884,"oots absorb water from soil s1 the water flows to the s2 leaf CO2 enters leaf s3 Figure 1: Task and our proposed model. Top: raw text descriptions are annotated with entity-state change information; we modify this in a rule-based way for our model. Bottom: our model. Entity mention and verb information is aggregated in per-entity LSTMs (right). A CRF layer then predicts entity state. A separate sentence-level LSTM (left) tracks each entity-location pair using the combined entity and location mention information. thetic (Weston et al., 2015), or continuous entitycentric neural language models (Clark et al., 2018; Ji et al., 2017). For process understanding specifically, past work has effectively captured global information (Dalvi et al., 2018) and temporal characteristics (Das et al., 2019). However, these models do not leverage the structure constraints of the problem, or only handle them heuristically (Tandon et al., 2018). We find that our model outperforms these past approaches on the P RO PARA dataset of Dalvi et al. (2018) with a significant boost in questions concerning entity state, regardless of the location. Our model, as depicted in Fig. 1, consists of two core modules: (i) state tracking,"
W19-1502,P15-1030,1,0.84681,"tured global information (Dalvi et al., 2018) and temporal characteristics (Das et al., 2019). However, these models do not leverage the structure constraints of the problem, or only handle them heuristically (Tandon et al., 2018). We find that our model outperforms these past approaches on the P RO PARA dataset of Dalvi et al. (2018) with a significant boost in questions concerning entity state, regardless of the location. Our model, as depicted in Fig. 1, consists of two core modules: (i) state tracking, and (ii) location tracking. We follow past work on neural CRFs (Collobert et al., 2011; Durrett and Klein, 2015; Lample et al., 2016), leveraging continuous LSTMs to distill information and a discrete CRF layer for prediction. 2 Contextual Embeddings Our model first computes contextual embeddings for each word in the paragraph using a single layered bidirectional LSTM. Each token word wi is encoded as a vector xi = [emb(wi ); vi ] which serves as input to the LSTM. Here, emb(wi ) ∈ Rd1 is an embedding for the word produced by either pre-trained GloVe (Pennington et al., 2014) or ELMo (Peters et al., 2018) embeddings and vi is a scalar binary indicator if the current word is a verb. We denote by hi = LS"
W19-1502,D17-1195,0,0.0238397,"om soil s1 the water flows to the s2 leaf CO2 enters leaf s3 Figure 1: Task and our proposed model. Top: raw text descriptions are annotated with entity-state change information; we modify this in a rule-based way for our model. Bottom: our model. Entity mention and verb information is aggregated in per-entity LSTMs (right). A CRF layer then predicts entity state. A separate sentence-level LSTM (left) tracks each entity-location pair using the combined entity and location mention information. thetic (Weston et al., 2015), or continuous entitycentric neural language models (Clark et al., 2018; Ji et al., 2017). For process understanding specifically, past work has effectively captured global information (Dalvi et al., 2018) and temporal characteristics (Das et al., 2019). However, these models do not leverage the structure constraints of the problem, or only handle them heuristically (Tandon et al., 2018). We find that our model outperforms these past approaches on the P RO PARA dataset of Dalvi et al. (2018) with a significant boost in questions concerning entity state, regardless of the location. Our model, as depicted in Fig. 1, consists of two core modules: (i) state tracking, and (ii) location"
W19-1502,Q18-1021,0,0.0156469,"explicitly capture constraints on entity states over time, enforcing that, for example, an entity cannot move to a location after it is destroyed. We evaluate the performance of our proposed model on QA tasks over process paragraphs in the P RO PARA dataset (Dalvi et al., 2018) and find that our model achieves state-of-the-art results. 1 Introduction Many reading comprehension question answering tasks (Richardson et al., 2013; Rajpurkar et al., 2016; Joshi et al., 2017) require looking at primarily one point in the passage to answer each question, or sometimes two or three (Yang et al., 2018; Welbl et al., 2018). As a result, modeling surfacelevel correspondences can work well (Seo et al., 2017) and holistic passage comprehension is not necessary. However, certain QA settings require deeper analysis by focusing specifically on entities, asking questions about their states over time (Weston et al., 2015; Long et al., 2016), combina7 Proceedings of 3rd Workshop on Structured Prediction for NLP, pages 7–12 c Minneapolis, Minnesota, June 7, 2019. 2019 Association for Computational Linguistics Raw Turker Annotations Process Text (Input) EVENTS Roots absorb water from soil The water flows to the leaf CO2 e"
W19-1502,P17-1147,0,0.0129917,"The global discrete state structure is explicitly modelled with a neural CRF over the changing hidden representation of the entity. This CRF can explicitly capture constraints on entity states over time, enforcing that, for example, an entity cannot move to a location after it is destroyed. We evaluate the performance of our proposed model on QA tasks over process paragraphs in the P RO PARA dataset (Dalvi et al., 2018) and find that our model achieves state-of-the-art results. 1 Introduction Many reading comprehension question answering tasks (Richardson et al., 2013; Rajpurkar et al., 2016; Joshi et al., 2017) require looking at primarily one point in the passage to answer each question, or sometimes two or three (Yang et al., 2018; Welbl et al., 2018). As a result, modeling surfacelevel correspondences can work well (Seo et al., 2017) and holistic passage comprehension is not necessary. However, certain QA settings require deeper analysis by focusing specifically on entities, asking questions about their states over time (Weston et al., 2015; Long et al., 2016), combina7 Proceedings of 3rd Workshop on Structured Prediction for NLP, pages 7–12 c Minneapolis, Minnesota, June 7, 2019. 2019 Associatio"
W19-1502,N16-1030,0,0.0510615,"(Dalvi et al., 2018) and temporal characteristics (Das et al., 2019). However, these models do not leverage the structure constraints of the problem, or only handle them heuristically (Tandon et al., 2018). We find that our model outperforms these past approaches on the P RO PARA dataset of Dalvi et al. (2018) with a significant boost in questions concerning entity state, regardless of the location. Our model, as depicted in Fig. 1, consists of two core modules: (i) state tracking, and (ii) location tracking. We follow past work on neural CRFs (Collobert et al., 2011; Durrett and Klein, 2015; Lample et al., 2016), leveraging continuous LSTMs to distill information and a discrete CRF layer for prediction. 2 Contextual Embeddings Our model first computes contextual embeddings for each word in the paragraph using a single layered bidirectional LSTM. Each token word wi is encoded as a vector xi = [emb(wi ); vi ] which serves as input to the LSTM. Here, emb(wi ) ∈ Rd1 is an embedding for the word produced by either pre-trained GloVe (Pennington et al., 2014) or ELMo (Peters et al., 2018) embeddings and vi is a scalar binary indicator if the current word is a verb. We denote by hi = LSTM([xi ]) the LSTM’s o"
W19-1502,D18-1259,0,0.0179837,"tity. This CRF can explicitly capture constraints on entity states over time, enforcing that, for example, an entity cannot move to a location after it is destroyed. We evaluate the performance of our proposed model on QA tasks over process paragraphs in the P RO PARA dataset (Dalvi et al., 2018) and find that our model achieves state-of-the-art results. 1 Introduction Many reading comprehension question answering tasks (Richardson et al., 2013; Rajpurkar et al., 2016; Joshi et al., 2017) require looking at primarily one point in the passage to answer each question, or sometimes two or three (Yang et al., 2018; Welbl et al., 2018). As a result, modeling surfacelevel correspondences can work well (Seo et al., 2017) and holistic passage comprehension is not necessary. However, certain QA settings require deeper analysis by focusing specifically on entities, asking questions about their states over time (Weston et al., 2015; Long et al., 2016), combina7 Proceedings of 3rd Workshop on Structured Prediction for NLP, pages 7–12 c Minneapolis, Minnesota, June 7, 2019. 2019 Association for Computational Linguistics Raw Turker Annotations Process Text (Input) EVENTS Roots absorb water from soil The water fl"
W19-1502,P16-1138,0,0.113315,"Missing"
W19-1502,D14-1162,0,0.111622,"of two core modules: (i) state tracking, and (ii) location tracking. We follow past work on neural CRFs (Collobert et al., 2011; Durrett and Klein, 2015; Lample et al., 2016), leveraging continuous LSTMs to distill information and a discrete CRF layer for prediction. 2 Contextual Embeddings Our model first computes contextual embeddings for each word in the paragraph using a single layered bidirectional LSTM. Each token word wi is encoded as a vector xi = [emb(wi ); vi ] which serves as input to the LSTM. Here, emb(wi ) ∈ Rd1 is an embedding for the word produced by either pre-trained GloVe (Pennington et al., 2014) or ELMo (Peters et al., 2018) embeddings and vi is a scalar binary indicator if the current word is a verb. We denote by hi = LSTM([xi ]) the LSTM’s output for the ith token in w. 2.1 State Tracking This part of the model is charged with modeling each entity’s state over time. Our model places a distribution over state sequences y given a passage w and an entity e: P (y|w, e). Model We propose a structured neural model for the process paragraph comprehension task of Dalvi et al. (2018). An example from their dataset is shown in Figure 1. It consists of annotation over a process paragraph w ="
W19-1502,N18-1202,0,0.017416,"acking, and (ii) location tracking. We follow past work on neural CRFs (Collobert et al., 2011; Durrett and Klein, 2015; Lample et al., 2016), leveraging continuous LSTMs to distill information and a discrete CRF layer for prediction. 2 Contextual Embeddings Our model first computes contextual embeddings for each word in the paragraph using a single layered bidirectional LSTM. Each token word wi is encoded as a vector xi = [emb(wi ); vi ] which serves as input to the LSTM. Here, emb(wi ) ∈ Rd1 is an embedding for the word produced by either pre-trained GloVe (Pennington et al., 2014) or ELMo (Peters et al., 2018) embeddings and vi is a scalar binary indicator if the current word is a verb. We denote by hi = LSTM([xi ]) the LSTM’s output for the ith token in w. 2.1 State Tracking This part of the model is charged with modeling each entity’s state over time. Our model places a distribution over state sequences y given a passage w and an entity e: P (y|w, e). Model We propose a structured neural model for the process paragraph comprehension task of Dalvi et al. (2018). An example from their dataset is shown in Figure 1. It consists of annotation over a process paragraph w = {wi }Pi=1 of P tokens describe"
W19-1502,D16-1264,0,0.0374167,"vant state information. The global discrete state structure is explicitly modelled with a neural CRF over the changing hidden representation of the entity. This CRF can explicitly capture constraints on entity states over time, enforcing that, for example, an entity cannot move to a location after it is destroyed. We evaluate the performance of our proposed model on QA tasks over process paragraphs in the P RO PARA dataset (Dalvi et al., 2018) and find that our model achieves state-of-the-art results. 1 Introduction Many reading comprehension question answering tasks (Richardson et al., 2013; Rajpurkar et al., 2016; Joshi et al., 2017) require looking at primarily one point in the passage to answer each question, or sometimes two or three (Yang et al., 2018; Welbl et al., 2018). As a result, modeling surfacelevel correspondences can work well (Seo et al., 2017) and holistic passage comprehension is not necessary. However, certain QA settings require deeper analysis by focusing specifically on entities, asking questions about their states over time (Weston et al., 2015; Long et al., 2016), combina7 Proceedings of 3rd Workshop on Structured Prediction for NLP, pages 7–12 c Minneapolis, Minnesota, June 7,"
W19-1502,D13-1020,0,0.0158916,"each step to contain relevant state information. The global discrete state structure is explicitly modelled with a neural CRF over the changing hidden representation of the entity. This CRF can explicitly capture constraints on entity states over time, enforcing that, for example, an entity cannot move to a location after it is destroyed. We evaluate the performance of our proposed model on QA tasks over process paragraphs in the P RO PARA dataset (Dalvi et al., 2018) and find that our model achieves state-of-the-art results. 1 Introduction Many reading comprehension question answering tasks (Richardson et al., 2013; Rajpurkar et al., 2016; Joshi et al., 2017) require looking at primarily one point in the passage to answer each question, or sometimes two or three (Yang et al., 2018; Welbl et al., 2018). As a result, modeling surfacelevel correspondences can work well (Seo et al., 2017) and holistic passage comprehension is not necessary. However, certain QA settings require deeper analysis by focusing specifically on entities, asking questions about their states over time (Weston et al., 2015; Long et al., 2016), combina7 Proceedings of 3rd Workshop on Structured Prediction for NLP, pages 7–12 c Minneapo"
W19-1502,D18-1006,0,\N,Missing
