2020.acl-demos.8,P19-3022,0,0.0484637,"atements (Lu, 2011; Ren et al., 2017; Shen et al., 2018). With the results from those search engines, scientists still need to read a large number of retrieved documents to find specific statements as textual evidence to validate the input query. This textual evidence is key to tasks such as developing new hypotheses, designing informative experiments, or comparing and validating new findings against previous knowledge. While the last several years have witnessed substantial growth in interests and efforts in evidence mining (Lippi and Torroni, 2016; Wachsmuth et al., 2017; Stab et al., 2018; Chen et al., 2019; Majithia et al., 2019; Chernodub et al., 2019; Allot et al., 2019), little work has been done for evidence mining system development in the scientific literature. A significant difference between evidence in the scientific literature and evidence in other corpora (e.g., the online debate corpus) is that scientific evidence usually does not have a strong sentiment (i.e., positive, negative or neutral) in the opinion it holds. Most scientific evidence sentences are objective statements reflecting how strongly they support a query statement. Therefore, if scientists are interested in finding te"
2020.acl-demos.8,D18-1230,1,0.839843,".g., PubMed) are designed for document 1 A brief demo of E VIDENCE M INER is available at https://youtu.be/iYuQ6gsr--I. 2 https://www.ncbi.nlm.nih.gov/pubmed/ 3 https://www.ncbi.nlm.nih.gov/pmc/ 56 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 56–62 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics novel data-driven methods for distantly supervised named entity recognition and open information extraction. E VIDENCE M INER relies on external knowledge bases to provide distant supervision for named entity recognition (NER) (Shang et al., 2018b; Wang et al., 2018b, 2019). Based on the entity annotation results, it automatically extracts informative meta-patterns (textual patterns containing entity types, e.g., CHEMICAL inhibit DISEASE) from sentences in the background corpora. (Jiang et al., 2017; Wang et al., 2018a; Li et al., 2018a,b). Sentences with meta-patterns that better match the query statement is more likely to be textual evidence. The entities and patterns are precomputed and indexed offline to support fast online evidence retrieval. The annotation results are also highlighted in the original document for better visualiz"
2020.acl-demos.8,P19-3031,0,0.04245,"et al., 2018). With the results from those search engines, scientists still need to read a large number of retrieved documents to find specific statements as textual evidence to validate the input query. This textual evidence is key to tasks such as developing new hypotheses, designing informative experiments, or comparing and validating new findings against previous knowledge. While the last several years have witnessed substantial growth in interests and efforts in evidence mining (Lippi and Torroni, 2016; Wachsmuth et al., 2017; Stab et al., 2018; Chen et al., 2019; Majithia et al., 2019; Chernodub et al., 2019; Allot et al., 2019), little work has been done for evidence mining system development in the scientific literature. A significant difference between evidence in the scientific literature and evidence in other corpora (e.g., the online debate corpus) is that scientific evidence usually does not have a strong sentiment (i.e., positive, negative or neutral) in the opinion it holds. Most scientific evidence sentences are objective statements reflecting how strongly they support a query statement. Therefore, if scientists are interested in finding textual evidence for “melanoma is treated with ni"
2020.acl-demos.8,N18-5005,0,0.0452739,"Missing"
2020.acl-demos.8,W17-5110,0,0.0338118,"imPortal (Majithia et al., 2019) is an integrated infrastructure for searching and checking factual claims on Twitter. TARGER (Chernodub et al., 2019) is an argument mining framework for tagging arguments in the free input text and keyword-based retrieval of arguments from the argument-tagged corpus. Most of these tools rely on fully supervised methods that require human-annotated training data. It is difficult to directly apply these systems to other domains, such as life sciences since it is non-trivial to retrieve the set of human-annotated articles and the annotations are prone to errors (Levy et al., 2017). 1. We build E VIDENCE M INER, a web-based system for textual evidence discovery for life sciences. E VIDENCE M INER is supported by novel methods for distantly supervised named entity recognition and pattern-based open information extraction. 2. The retrieved evidence sentences can be easily located in the original text. The entity and relation annotation results are also highlighted in the original document for better visualization. 3. Analytic functionalities are included such as finding the most frequent entities/relations for given entity/relation types and finding the most frequent enti"
2020.acl-demos.8,W17-5106,0,0.0262794,"o not allow direct retrieval of specific statements (Lu, 2011; Ren et al., 2017; Shen et al., 2018). With the results from those search engines, scientists still need to read a large number of retrieved documents to find specific statements as textual evidence to validate the input query. This textual evidence is key to tasks such as developing new hypotheses, designing informative experiments, or comparing and validating new findings against previous knowledge. While the last several years have witnessed substantial growth in interests and efforts in evidence mining (Lippi and Torroni, 2016; Wachsmuth et al., 2017; Stab et al., 2018; Chen et al., 2019; Majithia et al., 2019; Chernodub et al., 2019; Allot et al., 2019), little work has been done for evidence mining system development in the scientific literature. A significant difference between evidence in the scientific literature and evidence in other corpora (e.g., the online debate corpus) is that scientific evidence usually does not have a strong sentiment (i.e., positive, negative or neutral) in the opinion it holds. Most scientific evidence sentences are objective statements reflecting how strongly they support a query statement. Therefore, if s"
2020.acl-demos.8,P19-3026,0,0.144794,"Ren et al., 2017; Shen et al., 2018). With the results from those search engines, scientists still need to read a large number of retrieved documents to find specific statements as textual evidence to validate the input query. This textual evidence is key to tasks such as developing new hypotheses, designing informative experiments, or comparing and validating new findings against previous knowledge. While the last several years have witnessed substantial growth in interests and efforts in evidence mining (Lippi and Torroni, 2016; Wachsmuth et al., 2017; Stab et al., 2018; Chen et al., 2019; Majithia et al., 2019; Chernodub et al., 2019; Allot et al., 2019), little work has been done for evidence mining system development in the scientific literature. A significant difference between evidence in the scientific literature and evidence in other corpora (e.g., the online debate corpus) is that scientific evidence usually does not have a strong sentiment (i.e., positive, negative or neutral) in the opinion it holds. Most scientific evidence sentences are objective statements reflecting how strongly they support a query statement. Therefore, if scientists are interested in finding textual evidence for “mel"
2020.acl-demos.8,P17-4010,1,0.833375,"1 , Enyi Jiang1 , Qi Li3 , David Liem4 , Dibakar Sigdel4 , J. Harry Caufield4 , Peipei Ping4 , Jiawei Han1 1 Department of Computer Science, University of Illinois at Urbana-Champaign School of Information Sciences, University of Illinois at Urbana-Champaign 3 Department of Computer Science, Iowa State University 4 School of Medicine, University of California, Los Angeles 1,2 {xwang174,yingjun2,weilil2,aabhasc2,enyij2,hanj}@illinois.edu, 3 qli@iastate.edu, 4 {dliem,dsigdel,jcaufield,pping}@mednet.ucla.edu 2 Abstract retrieval and do not allow direct retrieval of specific statements (Lu, 2011; Ren et al., 2017; Shen et al., 2018). With the results from those search engines, scientists still need to read a large number of retrieved documents to find specific statements as textual evidence to validate the input query. This textual evidence is key to tasks such as developing new hypotheses, designing informative experiments, or comparing and validating new findings against previous knowledge. While the last several years have witnessed substantial growth in interests and efforts in evidence mining (Lippi and Torroni, 2016; Wachsmuth et al., 2017; Stab et al., 2018; Chen et al., 2019; Majithia et al.,"
2020.acl-main.445,P19-1330,0,0.0146475,"se them as similarity measures instead of the ROUGE-based approaches tested in Sec. 4.1 for automatic FAM creation (i.e., finding support sentences for each facet by the scores of embedding-based metrics). Such similarity measures are especially beneficial when the facet and its support sentences are not similar at the lexical level. Reflections on Text Summarization. There has been increasing attention and critique to the issues of existing summarization metrics (Schluter, 2017), methods (Kedzie et al., 2018; Shapira et al., 2018), and datasets (Jung et al., 2019). Notably, Kryscinski et al. (2019) conducted a comprehensive critical evaluation for summarization from various aspects. Zopf et al. (2018) investigated sentence regression approaches in a manner similar to ours but they could only evaluate them approximately against ROUGE as no ground-truth labels (FAMs) existed. Annotation and Analysis. Many recent studies conduct human annotation or evaluation on text summarization and other NLP tasks to gain useful insights. Hardy et al. (2019) annotated 50 documents to demonstrate the benefits of highlightbased summarization evaluation. Recent summarization methods (Paulus et al., 2017; N"
2020.acl-main.445,P18-1013,0,0.118756,"Mj } is a support group, where I1 , I2 , ..., IMj are the indices of support sentences and Mj is the number of support sentences in Sji . One illustrative example is presented in Fig. 1. The support sentences are likely to be verbose, but we consider whether the support sentences express the semantics of the facet regardless of their length.3 The reason is that we believe extractive summarization should focus on information coverage since it cannot alter the original sentences and once salient sentences are extracted, one can then compress them in an abstractive manner (Chen and Bansal, 2018; Hsu et al., 2018). since existing datasets do not provide extractive labels but only abstractive references. Our assumption that each reference sentence corresponds to one facet is similar to that during the creation of extractive labels. The major differences are that (1) We allow an arbitrary number of support sentences while extractive labels usually limit to one support sentence for each reference sentence, i.e., we do not specify Mj . For example, we would put two support sentences to one support group if they are complementary and only combining them can cover the facet. (2) We try to find multiple suppo"
2020.acl-main.445,D19-1327,0,0.0117924,"rporate embeddingbased metrics into FAR is to use them as similarity measures instead of the ROUGE-based approaches tested in Sec. 4.1 for automatic FAM creation (i.e., finding support sentences for each facet by the scores of embedding-based metrics). Such similarity measures are especially beneficial when the facet and its support sentences are not similar at the lexical level. Reflections on Text Summarization. There has been increasing attention and critique to the issues of existing summarization metrics (Schluter, 2017), methods (Kedzie et al., 2018; Shapira et al., 2018), and datasets (Jung et al., 2019). Notably, Kryscinski et al. (2019) conducted a comprehensive critical evaluation for summarization from various aspects. Zopf et al. (2018) investigated sentence regression approaches in a manner similar to ours but they could only evaluate them approximately against ROUGE as no ground-truth labels (FAMs) existed. Annotation and Analysis. Many recent studies conduct human annotation or evaluation on text summarization and other NLP tasks to gain useful insights. Hardy et al. (2019) annotated 50 documents to demonstrate the benefits of highlightbased summarization evaluation. Recent summarizat"
2020.acl-main.445,D18-1208,0,0.108378,"Missing"
2020.acl-main.445,D19-1051,0,0.0203738,"Missing"
2020.acl-main.445,W04-1013,0,0.203031,"celona in 1997 where they enjoyed a successful relationship at the Camp Nou. (ROUGE Recall/F1=0, no lexical overlap at all) Table 1: Lexical overlap — finding the document sentence with the highest ROUGE against one reference sentence — could be misleading. Examples are from the CNN/Daily Mail dataset (Nallapati et al., 2016). Introduction Text summarization has enjoyed increasing popularity due to its wide applications, whereas the evaluation of text summarization remains challenging and controversial. The most commonly used evaluation metric of summarization is lexical overlap, i.e., ROUGE (Lin, 2004), which regards the system and reference summaries as sequences of tokens and measures their n-gram overlap. However, recent studies (Paulus et al., 2017; Schluter, 2017; Kryscinski et al., 2019) reveal the limitations of ROUGE and find that in many cases, it fails to reach consensus with human judgment. Since lexical overlap only captures information 1 Data can be found at https://github.com/ morningmoni/FAR. coverage at the surface (token) level, ROUGE favors system summaries that share more tokens with the reference summaries. Nevertheless, such summaries may not always convey the desired s"
2020.acl-main.445,K16-1028,0,0.103317,"Missing"
2020.acl-main.445,P18-1188,0,0.156245,"upport groups (N &gt; 1), as there could be more than one set of support sentences that cover the same facet. In contrast, there is no notion of support group in extractive labels as they inherently form one such group (N = 1). Also, we allow N = 0 if such a mapping cannot be found even by humans. (3) The FAMs are more accurate as they are created by human annotators while extractive methods use sentence regression approaches (which we evaluate in Sec. 4.1) to obtain extractive labels approximately. Relation w. Extractive Labels. Extractive methods (Nallapati et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018c) typically require binary labels of every document sentence indicating whether it should be extracted during model training. Such labels are called extractive labels and usually created heuristically based on reference summaries Comparison w. SCUs. Some may mistake FAMs for Summarization Content Units (SCUs) in Pyramid (Nenkova and Passonneau, 2004), but they are different in that (1) FAMs utilize both the documents and reference summaries while SCUs ignore the documents; (2) FAMs are at the sentence level and can thus be used to automatically evaluate extractive methods once created — simpl"
2020.acl-main.445,D18-1206,0,0.383606,"upport groups (N &gt; 1), as there could be more than one set of support sentences that cover the same facet. In contrast, there is no notion of support group in extractive labels as they inherently form one such group (N = 1). Also, we allow N = 0 if such a mapping cannot be found even by humans. (3) The FAMs are more accurate as they are created by human annotators while extractive methods use sentence regression approaches (which we evaluate in Sec. 4.1) to obtain extractive labels approximately. Relation w. Extractive Labels. Extractive methods (Nallapati et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018c) typically require binary labels of every document sentence indicating whether it should be extracted during model training. Such labels are called extractive labels and usually created heuristically based on reference summaries Comparison w. SCUs. Some may mistake FAMs for Summarization Content Units (SCUs) in Pyramid (Nenkova and Passonneau, 2004), but they are different in that (1) FAMs utilize both the documents and reference summaries while SCUs ignore the documents; (2) FAMs are at the sentence level and can thus be used to automatically evaluate extractive methods once created — simpl"
2020.acl-main.445,N18-1158,0,0.270366,"upport groups (N &gt; 1), as there could be more than one set of support sentences that cover the same facet. In contrast, there is no notion of support group in extractive labels as they inherently form one such group (N = 1). Also, we allow N = 0 if such a mapping cannot be found even by humans. (3) The FAMs are more accurate as they are created by human annotators while extractive methods use sentence regression approaches (which we evaluate in Sec. 4.1) to obtain extractive labels approximately. Relation w. Extractive Labels. Extractive methods (Nallapati et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018c) typically require binary labels of every document sentence indicating whether it should be extracted during model training. Such labels are called extractive labels and usually created heuristically based on reference summaries Comparison w. SCUs. Some may mistake FAMs for Summarization Content Units (SCUs) in Pyramid (Nenkova and Passonneau, 2004), but they are different in that (1) FAMs utilize both the documents and reference summaries while SCUs ignore the documents; (2) FAMs are at the sentence level and can thus be used to automatically evaluate extractive methods once created — simpl"
2020.acl-main.445,N04-1019,0,0.395762,"ed by human annotators while extractive methods use sentence regression approaches (which we evaluate in Sec. 4.1) to obtain extractive labels approximately. Relation w. Extractive Labels. Extractive methods (Nallapati et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018c) typically require binary labels of every document sentence indicating whether it should be extracted during model training. Such labels are called extractive labels and usually created heuristically based on reference summaries Comparison w. SCUs. Some may mistake FAMs for Summarization Content Units (SCUs) in Pyramid (Nenkova and Passonneau, 2004), but they are different in that (1) FAMs utilize both the documents and reference summaries while SCUs ignore the documents; (2) FAMs are at the sentence level and can thus be used to automatically evaluate extractive methods once created — simply by matching sentence indices we can know how many facets are covered, while SCUs have to be manually annotated for each system (refer to Appendix B Fig. 4). 3 We ignore coreference (e.g., “he” vs. “the writer”) and short fragments when considering the semantics of one facet, as we found that the wording of the reference summaries regarding such choi"
2020.acl-main.445,D15-1222,0,0.0179456,"ture work. Method FAR AutoFAR AutoFAR-L FAR vs. AutoFAR(-L) BanditSum Lead-3 FastRL(E) NeuSum Refresh UnifiedSum(E) 44.7 50.6 50.8 51.2 51.3 54.8 44.8 51.3 51.0 49.9 51.7 54.5 44.7 45.6 43.1 44.3 46.2 46.9 Pearson’s r 97.6 (42.9) Spearman’s ρ 77.1 (54.3) Kendall’s τ 60.0 (46.7) Table 8: FAR prediction via linear regression. AutoFAR(-L) denotes the results on the humanannotated subset (entire CNN/Daily Mail dataset). 5 Related Work Evaluation Metrics for Text Summarization. ROUGE (Lin, 2004) is the most widely used evaluation metric for text summarization. Extensions of ROUGE include ROUGE-WE (Ng and Abrecht, 2015) that incorporated word embedding into ROUGE, ROUGE 2.0 (Ganesan, 2018) that considered synonyms, and ROUGE-G (ShafieiBavani et al., 2018) that applied graph analysis to WordNet for lexical and semantic matching. Nevertheless, these extensions did not draw enough attention as the original ROUGE and recent advances (Gu et al., 2020; Zhang et al., 2019a) are still primarily evaluated by the vanilla ROUGE. Another popular branch is Pyramid-based metrics (Nenkova and Passonneau, 2004; Yang et al., 2016), which annotate and compare the Summarization Content Units (SCUs) in the summaries. 7 The raw"
2020.acl-main.445,E17-2007,0,0.0719641,"cument sentence with the highest ROUGE against one reference sentence — could be misleading. Examples are from the CNN/Daily Mail dataset (Nallapati et al., 2016). Introduction Text summarization has enjoyed increasing popularity due to its wide applications, whereas the evaluation of text summarization remains challenging and controversial. The most commonly used evaluation metric of summarization is lexical overlap, i.e., ROUGE (Lin, 2004), which regards the system and reference summaries as sequences of tokens and measures their n-gram overlap. However, recent studies (Paulus et al., 2017; Schluter, 2017; Kryscinski et al., 2019) reveal the limitations of ROUGE and find that in many cases, it fails to reach consensus with human judgment. Since lexical overlap only captures information 1 Data can be found at https://github.com/ morningmoni/FAR. coverage at the surface (token) level, ROUGE favors system summaries that share more tokens with the reference summaries. Nevertheless, such summaries may not always convey the desired semantics. For example, in Table 1, the document sentence with the highest ROUGE score has more lexical overlap but expresses rather different semantic meaning. In contra"
2020.acl-main.445,P17-1099,0,0.256857,"Missing"
2020.acl-main.445,D18-1085,0,0.0176168,"1.2 51.3 54.8 44.8 51.3 51.0 49.9 51.7 54.5 44.7 45.6 43.1 44.3 46.2 46.9 Pearson’s r 97.6 (42.9) Spearman’s ρ 77.1 (54.3) Kendall’s τ 60.0 (46.7) Table 8: FAR prediction via linear regression. AutoFAR(-L) denotes the results on the humanannotated subset (entire CNN/Daily Mail dataset). 5 Related Work Evaluation Metrics for Text Summarization. ROUGE (Lin, 2004) is the most widely used evaluation metric for text summarization. Extensions of ROUGE include ROUGE-WE (Ng and Abrecht, 2015) that incorporated word embedding into ROUGE, ROUGE 2.0 (Ganesan, 2018) that considered synonyms, and ROUGE-G (ShafieiBavani et al., 2018) that applied graph analysis to WordNet for lexical and semantic matching. Nevertheless, these extensions did not draw enough attention as the original ROUGE and recent advances (Gu et al., 2020; Zhang et al., 2019a) are still primarily evaluated by the vanilla ROUGE. Another popular branch is Pyramid-based metrics (Nenkova and Passonneau, 2004; Yang et al., 2016), which annotate and compare the Summarization Content Units (SCUs) in the summaries. 7 The raw estimated FAR scores are provided in Appendix B Fig. 5 in the interest of space. 4948 FAR is related to Pyramid and HighRES (Hardy et al.,"
2020.acl-main.445,P18-1061,0,0.0144846,"all the support sentences of one document-summary pair to one single support set and define the Support-Aware Recall (SAR) as follows. SAR is used in Sec. 3.4 for the comparative analysis of extractive methods. SAR = | Automatic Evaluation with FAR By utilizing the low abstraction category on the extractive CNN/Daily Mail dataset, we revisit extractive methods to evaluate how they perform on information coverage. Specifically, we compare Lead-3 (that extracts the first three document sentences), FastRL(E) (E for extractive only) (Chen and Bansal, 2018), BanditSum (Dong et al., 2018), NeuSum (Zhou et al., 2018), Refresh (Narayan et al., 2018c), and UnifiedSum(E) (Hsu et al., 2018) using both ROUGE and FAR. For a fair comparison, each method extracts three sentences (|E |= 3).5 Results on Neural Extractive Methods. As shown in Table 3, there is almost no discrimination among the last four methods under ROUGE-1 F1, and the rankings under ROUGE-1/2/L often contradict with each other. The observations on ROUGE Precision/Recall are similar. We provide them as well as more comparative analysis under facet-aware evaluation in Sec. 3.4. For facet coverage, the upper bound of FAR by extracting 3 sentences (O"
2020.acl-main.445,N18-1161,0,0.0523793,"Missing"
2020.acl-main.445,D19-1618,0,0.017828,"based metrics (Nenkova and Passonneau, 2004; Yang et al., 2016), which annotate and compare the Summarization Content Units (SCUs) in the summaries. 7 The raw estimated FAR scores are provided in Appendix B Fig. 5 in the interest of space. 4948 FAR is related to Pyramid and HighRES (Hardy et al., 2019) in that Pyramid employs the summaries to annotate SCUs and HighRES highlights salient text fragments in the documents, while FAR considers both the summaries and documents. Beyond lexical overlap, embedding-based evaluation metrics (Zhang et al., 2019b; Zhao et al., 2019; Sun and Nenkova, 2019; Xenouleas et al., 2019) are gaining more traction along with the dominance of pre-trained language models. One straightforward way to incorporate embeddingbased metrics into FAR is to use them as similarity measures instead of the ROUGE-based approaches tested in Sec. 4.1 for automatic FAM creation (i.e., finding support sentences for each facet by the scores of embedding-based metrics). Such similarity measures are especially beneficial when the facet and its support sentences are not similar at the lexical level. Reflections on Text Summarization. There has been increasing attention and critique to the issues of e"
2020.acl-main.445,D18-1197,0,0.0127752,"urs but they could only evaluate them approximately against ROUGE as no ground-truth labels (FAMs) existed. Annotation and Analysis. Many recent studies conduct human annotation or evaluation on text summarization and other NLP tasks to gain useful insights. Hardy et al. (2019) annotated 50 documents to demonstrate the benefits of highlightbased summarization evaluation. Recent summarization methods (Paulus et al., 2017; Narayan et al., 2018c; Chen and Bansal, 2018) generally sampled 50 to 100 documents for human evaluation in addition to ROUGE in light of its limitations. Chen et al. (2016); Yavuz et al. (2018) inspected 100 samples and analyzed their category breakdown for reading comprehension and semantic parsing, respectively. We observed similar trends when analyzing different subsets of the FAMs, indicating that our findings are relatively stable. We thus conjecture that our sample size is sufficient to verify our hypotheses and benefit future research. 6 Conclusion and Future Work We propose a facet-aware evaluation setup for better assessment of information coverage in extractive summarization. We construct an extractive summarization dataset and demonstrate the effectiveness of facet-aware"
2020.acl-main.725,D08-1099,0,\N,Missing
2020.acl-main.725,C92-2082,0,\N,Missing
2020.acl-main.725,U08-1013,0,\N,Missing
2020.acl-main.725,D18-2004,0,\N,Missing
2020.acl-main.725,N19-1423,0,\N,Missing
2020.acl-main.725,D19-1028,0,\N,Missing
2020.acl-main.725,D09-1098,0,\N,Missing
2020.acl-main.725,P19-1421,0,\N,Missing
2020.emnlp-main.22,N10-1084,0,0.161071,"estfeld and Pfitzmann, 1999; bin Mohamed Amin et al., 2003; Chang and Clark, 2014). One useful cover signal for steganography is natural language text because of its prevalence and innocuity in daily life. Traditional linguistic steganography methods are mostly edit-based, i.e., they try to directly edit the secret message and transform it into an innocent text that will not raise the eavesdropper’s suspicious eyes. Typical strategies include synonym 1 Code and datasets are available at https://github. com/mickeystroller/StegaText. substitution (Topkara et al., 2006), paraphrase substitution (Chang and Clark, 2010), and syntactic transformation (Safaka et al., 2016), applied to various text media such as Email (Tutuncu and Hassan, 2015) and Twitter (Wilson et al., 2014). Although being able to maintain the grammatical correctness of output text, those edit-based methods cannot encode information efficiently. For example, the popular CoverTweet system (Wilson and Ker, 2016) can only encode two bits of information in each tweet on average. Recent advances in neural language models (LMs) (Józefowicz et al., 2016; Radford et al., 2019; Yang et al., 2019a) have enabled a diagram shift from edit-based methods"
2020.emnlp-main.22,J14-2006,0,0.126979,"COVID-19 lockdown rules Bob Let’s meet in (receiver) Room 9112A at decrypt decode Figure 1: Linguistic steganography pipeline. Introduction Privacy is central to modern communication systems such as email services and online social networks. To protect privacy, two research fields are established: (1) cryptography which encrypts secret messages into codes such that an eavesdropper is unable to decrypt, and (2) steganography which encodes messages into cover signals such that an eavesdropper is not even aware a secret message exists (Westfeld and Pfitzmann, 1999; bin Mohamed Amin et al., 2003; Chang and Clark, 2014). One useful cover signal for steganography is natural language text because of its prevalence and innocuity in daily life. Traditional linguistic steganography methods are mostly edit-based, i.e., they try to directly edit the secret message and transform it into an innocent text that will not raise the eavesdropper’s suspicious eyes. Typical strategies include synonym 1 Code and datasets are available at https://github. com/mickeystroller/StegaText. substitution (Topkara et al., 2006), paraphrase substitution (Chang and Clark, 2010), and syntactic transformation (Safaka et al., 2016), applie"
2020.emnlp-main.22,P19-1422,0,0.464294,"cover text, this encoder function f must be both deterministic and invertible. Moreover, this encoder f , together with the ciphertext distribution and the input LM, implicitly define a distribution of cover text y which we denote as Q(y). When cover texts are transmitted in the public channel, this distribution Q(y) is what an eavesdropper would observe. Imperceptibility. To avoid raising eavesdropper’s suspicion, we want the cover text distribution Q to be similar to the true natural language distribution (i.e., what this eavesdropper would expect to see in this public channel). Following (Dai and Cai, 2019), we formulate “imperceptibility” using the total variation distance (TVD) as follows: TVD(P∗LM , Q) = 1 kQ − P∗LM k1 , 2 (1) where P∗LM denotes the true language distribution. As we approximate P∗LM using a LM PLM (e.g., OpenAI GPT-2 (Radford et al., 2019)), we further decompose TVD(P∗LM , Q) as follows: TVD(P∗LM , Q) ≤ 1 ∗ 1 kPLM − PLM k1 + kPLM − Qk1 , 2 2 (2) where the first term measures how good this LM is and the second term, that is the main focus of this study, indicates the gap induced by the steganography encoder. Even without knowing the first term, we can still obtain a relative i"
2020.emnlp-main.22,P17-3017,0,0.599167,"014). Although being able to maintain the grammatical correctness of output text, those edit-based methods cannot encode information efficiently. For example, the popular CoverTweet system (Wilson and Ker, 2016) can only encode two bits of information in each tweet on average. Recent advances in neural language models (LMs) (Józefowicz et al., 2016; Radford et al., 2019; Yang et al., 2019a) have enabled a diagram shift from edit-based methods to generation-based methods which directly output a cover text by encoding the message reversibly in the choices of tokens. Various encoding algorithms (Fang et al., 2017; Yang et al., 2019b; Ziegler et al., 2019) have been proposed to leverage neural LMs to generate high-quality cover texts in terms of both fluency and information hiding capacity. However, most of the existing methods do not provide explicit guarantees on the imperceptibility of generated cover text (i.e., to what extent the cover text is indistinguishable from natural texts without hidden messages). One recent exception is the work (Dai 303 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 303–313, c November 16–20, 2020. 2020 Association for Compu"
2020.emnlp-main.22,W18-4203,1,0.926519,"es a pre-defined per-step imperceptibility guarantee. Specifically, the sender can set a small per-step imperceptibility gap δ  1 and at time step t, we set the Kt as: (8) This result shows our proposed SAAC algorithm is near-imperceptible for linguistic steganography. DKL (Q(yt |y<t )kPLM (yt |y<t )) = − log ZK , X ZK = PLM (y 0 |y<t ), (6) X Drug (5) where TK (y<t ) = argtopKy0 PLM (y 0 |y<t ). Accordingly, we have the imperceptibility of one generation step to be: Kt = min({K| Dataset Experiment Setups Datasets. We conduct our experiments on four datasets from different domains: (1) Drug (Ji and Knight, 2018), which contains a set of Reddit comments related to drugs, (2) News, which includes a subset of news articles in the CNN/DailyMail dataset (Hermann et al., 2015), (3) COVID-19, which is a subset of research papers related to COVID-19 in the CORD-19 dataset (Wang et al., 2020), and (4) Random, which is a collection of uniformly sampled bit sequences. The first three datasets contain natural language texts and we convert them into bit sequences5 following the same process in Ziegler et al. (2019). Table 1 summarizes the dataset statistics. PLM (y 0 |y<t ) ≥ 2−δ }). (7) y 0 ∈TK (y<t ) This selec"
2020.emnlp-main.22,2020.nlpcovid19-acl.1,0,0.0220763,"Missing"
2020.emnlp-main.22,P14-2115,1,0.817354,"Missing"
2020.emnlp-main.22,P15-1057,1,0.863635,"Missing"
2020.emnlp-main.22,D19-1115,0,0.207619,"Missing"
2020.emnlp-main.463,N19-4009,0,0.0767189,"Missing"
2020.emnlp-main.463,P19-1558,0,0.130843,"Missing"
2020.emnlp-main.568,D19-1468,0,0.234736,"ilable at https://github. com/teapot123/JASen. covered in the review, whereas the latter decides its sentiment polarity. Various methods have been proposed for the task. Neural network models (Liu et al., 2015; Xu et al., 2018) have outperformed rule-based models (Hu and Liu, 2004; Zhuang et al., 2006), but they require large-scale fine-grained labeled data to train, which can be difficult to obtain. Some other studies leverage word embeddings to solve the aspect extraction problem in an unsupervised (He et al., 2017; Liao et al., 2019) or weaklysupervised setting (Angelidis and Lapata, 2018; Karamanolakis et al., 2019), without using any annotated documents. In this work, we study the weakly-supervised setting, where only a few keywords are provided for each aspect and sentiment. We show two sample restaurant reviews in Fig. 1 together with their expected output—aspect and sentiment labels. With a closer look at these two example reviews, we observe that S2 includes a general opinion word “good” and a pure aspect word “seafood”, which are separate hints for sentiment and aspect classification respectively. S1, on the other hand, does not address the target with plain and general words, but instead use more"
2020.emnlp-main.568,D19-1465,0,0.052273,"classification. The former identifies the aspect 1 Our code and data are available at https://github. com/teapot123/JASen. covered in the review, whereas the latter decides its sentiment polarity. Various methods have been proposed for the task. Neural network models (Liu et al., 2015; Xu et al., 2018) have outperformed rule-based models (Hu and Liu, 2004; Zhuang et al., 2006), but they require large-scale fine-grained labeled data to train, which can be difficult to obtain. Some other studies leverage word embeddings to solve the aspect extraction problem in an unsupervised (He et al., 2017; Liao et al., 2019) or weaklysupervised setting (Angelidis and Lapata, 2018; Karamanolakis et al., 2019), without using any annotated documents. In this work, we study the weakly-supervised setting, where only a few keywords are provided for each aspect and sentiment. We show two sample restaurant reviews in Fig. 1 together with their expected output—aspect and sentiment labels. With a closer look at these two example reviews, we observe that S2 includes a general opinion word “good” and a pure aspect word “seafood”, which are separate hints for sentiment and aspect classification respectively. S1, on the other"
2020.emnlp-main.568,N10-1122,0,0.0605301,"2.2). 2.1 Aspect Extraction Early studies towards aspect extraction are mainly based on manually defined rules (Hu and Liu, 2004; Zhuang et al., 2006), which have been outperformed by supervised neural approaches that do not need labor-intensive feature engineering. While CNN (Xu et al., 2018) and RNN (Liu et al., 2015) based models have shown the powerful expressiveness of neural models, they can easily consume thousands of labeled documents thus suffer from the label scarcity bottleneck. Various unsupervised approaches are proposed to model different aspects automatically. LDAbased methods (Brody and Elhadad, 2010; Chen et al., 2014) model each document as a mixture of aspects (topics) and output a word distribution for each aspect. Recently, neural models have shown to extract more coherent topics. ABAE (He 6990 et al., 2017) uses an autoencoder to reconstruct sentences through aspect embedding and removes irrelevant words through attention mechanisms. CAt (Tulkens and van Cranenburgh, 2020) introduces a single head attention calculated by a Radial Basis Function (RBF) kernel to be the sentence summary. The unsupervised nature of these algorithms is hindered by the fact that the learned aspects often"
2020.emnlp-main.568,D15-1168,0,0.0673054,"elp, aspect-based sentiment analysis, which extracts opinions about certain facets of entities from text, becomes increasingly essential and benefits a wide range of downstream applications (Bauman et al., 2017; Nguyen et al., 2015). Aspect-based sentiment analysis contains two sub-tasks: Aspect extraction and sentiment polarity classification. The former identifies the aspect 1 Our code and data are available at https://github. com/teapot123/JASen. covered in the review, whereas the latter decides its sentiment polarity. Various methods have been proposed for the task. Neural network models (Liu et al., 2015; Xu et al., 2018) have outperformed rule-based models (Hu and Liu, 2004; Zhuang et al., 2006), but they require large-scale fine-grained labeled data to train, which can be difficult to obtain. Some other studies leverage word embeddings to solve the aspect extraction problem in an unsupervised (He et al., 2017; Liao et al., 2019) or weaklysupervised setting (Angelidis and Lapata, 2018; Karamanolakis et al., 2019), without using any annotated documents. In this work, we study the weakly-supervised setting, where only a few keywords are provided for each aspect and sentiment. We show two sampl"
2020.emnlp-main.568,J92-4003,0,0.432377,"Missing"
2020.emnlp-main.568,P14-1033,0,0.138704,"on Early studies towards aspect extraction are mainly based on manually defined rules (Hu and Liu, 2004; Zhuang et al., 2006), which have been outperformed by supervised neural approaches that do not need labor-intensive feature engineering. While CNN (Xu et al., 2018) and RNN (Liu et al., 2015) based models have shown the powerful expressiveness of neural models, they can easily consume thousands of labeled documents thus suffer from the label scarcity bottleneck. Various unsupervised approaches are proposed to model different aspects automatically. LDAbased methods (Brody and Elhadad, 2010; Chen et al., 2014) model each document as a mixture of aspects (topics) and output a word distribution for each aspect. Recently, neural models have shown to extract more coherent topics. ABAE (He 6990 et al., 2017) uses an autoencoder to reconstruct sentences through aspect embedding and removes irrelevant words through attention mechanisms. CAt (Tulkens and van Cranenburgh, 2020) introduces a single head attention calculated by a Radial Basis Function (RBF) kernel to be the sentence summary. The unsupervised nature of these algorithms is hindered by the fact that the learned aspects often do not well align wi"
2020.emnlp-main.568,N19-1423,0,0.0298348,"(He et al., 2017): An attention-based model to unsupervisedly extract aspects. An autoencoder is trained to reconstruct sentences through aspect embeddings. The learned topics need to be manually mapped to aspects. • CAt (Tulkens and van Cranenburgh, 2020): A recent method for unsupervised aspect extraction. A single head attention is calculated by a Radio Basis Function kernel to be the sentence summary. • W2VLDA (Garc´ıa-Pablos et al., 2018): A stateof-the-art topic modeling based method that leverages keywords for each aspect/sentiment to jointly do aspect/sentiment classification. • BERT (Devlin et al., 2019): A recent proposed deep language model. We utilize the pre-trained BERT (12-layer, 768 dimension, uncased) and implement a simple weakly-supervised method that fine-tunes the model by providing pseudo 6994 Methods Accuracy CosSim ABAE(He et al., 2017) CAt(Tulkens and van Cranenburgh, 2020) W2VLDA(Garc´ıa-Pablos et al., 2018) BERT(Devlin et al., 2019) JASen w/o joint JASen w/o self train JASen 61.43 67.34 66.30 70.75 72.98 81.03 82.90 83.83 Restaurant Precision Recall 50.12 46.63 49.20 58.82 58.20 61.66 63.15 64.73 50.26 50.79 50.61 57.44 74.63 65.91 72.51 72.95 macro-F1 Accuracy 42.31 45.31 4"
2020.emnlp-main.568,2020.emnlp-main.724,1,0.889829,"ed to map topics to certain aspects, not to mention some topics are irrelevant of interested aspects. Several weakly-supervised methods address this problem by using a few keywords per aspect as supervision to guide the learning process. MATE (Angelidis and Lapata, 2018) extends ABAE by initializing aspect embedding using weighted average of keyword embeddings from each aspect. ISWD (Karamanolakis et al., 2019) co-trains a bagof-word classifier and an embedding-based neural classifier to generalize the keyword supervision. Other text classification methods leverage pre-trained language model (Meng et al., 2020b) to learn the semantics of label names or metadata (Zhang et al., 2020) to propagate document labels. The above methods do not take aspect-specific opinion words into consideration. The semantic meaning captured by a hsentiment, aspecti joint topic preserves more fine-grained information to imply the aspect of a sentence and thus can be used to improve the performance of aspect extraction. 2.2 Joint Extraction of Aspect and Sentiment Most previous studies that jointly perform aspect and sentiment extraction are LDA-based methods. Zhao et al. (2010) include aspect-specific opinion models alon"
2020.emnlp-main.568,P17-1036,0,0.603521,"ntiment polarity classification. The former identifies the aspect 1 Our code and data are available at https://github. com/teapot123/JASen. covered in the review, whereas the latter decides its sentiment polarity. Various methods have been proposed for the task. Neural network models (Liu et al., 2015; Xu et al., 2018) have outperformed rule-based models (Hu and Liu, 2004; Zhuang et al., 2006), but they require large-scale fine-grained labeled data to train, which can be difficult to obtain. Some other studies leverage word embeddings to solve the aspect extraction problem in an unsupervised (He et al., 2017; Liao et al., 2019) or weaklysupervised setting (Angelidis and Lapata, 2018; Karamanolakis et al., 2019), without using any annotated documents. In this work, we study the weakly-supervised setting, where only a few keywords are provided for each aspect and sentiment. We show two sample restaurant reviews in Fig. 1 together with their expected output—aspect and sentiment labels. With a closer look at these two example reviews, we observe that S2 includes a general opinion word “good” and a pure aspect word “seafood”, which are separate hints for sentiment and aspect classification respectivel"
2020.emnlp-main.568,P18-2092,0,0.0190604,"e. The embedding-based prediction is effectively leveraged by neural models to generalize on unlabeled data via self-training. (3) We demonstrate that JASen generates high-quality joint topics and outperforms baselines significantly on two benchmark datasets. 2 Related Work The problem of aspect-based sentiment analysis can be decomposed into two sub-tasks: aspect extraction and sentiment polarity classification. Most previous studies deal with them individually. There are various related efforts on aspect extraction (He et al., 2017), which can be followed by sentiment classification models (He et al., 2018). Other methods (Garc´ıa-Pablos et al., 2018) jointly solve these two sub-tasks by first separating target words from opinion words and then learning joint topic distributions over words. Below we first review relevant work on aspect extraction (Sec 2.1) and then turn to studies that jointly extract aspects and sentiment polarity (Sec 2.2). 2.1 Aspect Extraction Early studies towards aspect extraction are mainly based on manually defined rules (Hu and Liu, 2004; Zhuang et al., 2006), which have been outperformed by supervised neural approaches that do not need labor-intensive feature engineeri"
2020.emnlp-main.568,S15-2082,0,0.0874436,"ment tips manager waitress servers service warranty coverage replace windows ios mac system screen led monitor resolution life charge last power hp toshiba dell lenovo touch track button pad programs apps itunes photoshop key space type keys Table 2: Keywords of each aspect. 5.1 Experimental Setup Datasets: The following two datasets are used for evaluation: • Restaurant: For in-domain training corpus, we collect 17,027 unlabeled reviews from Yelp Dataset Challenge2 . For evaluation, we use the benchmark dataset in the restaurant domain in SemEval-2016 (Pontiki et al., 2016) and SemEval-2015 (Pontiki et al., 2015), where each sentence is labeled with aspect and sentiment polarity. We remove sentences with multiple labels or with a neutral sentiment polarity to simplify the problem (otherwise a set of keywords can be added to describe it). • Laptop: We leverage 14,683 unlabeled Amazon reviews under the laptop category collected by (He and McAuley, 2016) as in-domain training corpus. We also use the benchmark dataset in the laptop domain in SemEval-2016 and SemEval-2015 for evaluation. Detailed statistics of both datasets are listed in Table 1, and the aspects along with their keywords are in Table 2. Pr"
2020.emnlp-main.568,2020.acl-main.290,0,0.288473,"Missing"
2020.emnlp-main.568,P15-1060,0,0.333942,"Missing"
2020.emnlp-main.568,P18-2094,0,0.120755,"sentiment analysis, which extracts opinions about certain facets of entities from text, becomes increasingly essential and benefits a wide range of downstream applications (Bauman et al., 2017; Nguyen et al., 2015). Aspect-based sentiment analysis contains two sub-tasks: Aspect extraction and sentiment polarity classification. The former identifies the aspect 1 Our code and data are available at https://github. com/teapot123/JASen. covered in the review, whereas the latter decides its sentiment polarity. Various methods have been proposed for the task. Neural network models (Liu et al., 2015; Xu et al., 2018) have outperformed rule-based models (Hu and Liu, 2004; Zhuang et al., 2006), but they require large-scale fine-grained labeled data to train, which can be difficult to obtain. Some other studies leverage word embeddings to solve the aspect extraction problem in an unsupervised (He et al., 2017; Liao et al., 2019) or weaklysupervised setting (Angelidis and Lapata, 2018; Karamanolakis et al., 2019), without using any annotated documents. In this work, we study the weakly-supervised setting, where only a few keywords are provided for each aspect and sentiment. We show two sample restaurant revie"
2020.emnlp-main.568,D10-1006,0,0.270234,"020 Association for Computational Linguistics is hard for models that are solely trained for one sub-task. If a model can automatically learn the semantics of each joint topic of hsentiment, aspecti, it will be able to identify representative terms of the joint topics such as “semi-private” which provide information for aspect and sentiment simultaneously, and will consequently benefit both aspect extraction and sentiment classification. Therefore, leveraging more fine-grained information by coupling the two subtasks will enhance both. Several LDA-based methods consider learning joint topics (Zhao et al., 2010; Wang et al., 2015; Xu et al., 2012), but they rely on external resources such as part-of-speech (POS) tagging or opinion word lexicons. A recent LDA-based model (Garc´ıaPablos et al., 2018) uses pre-trained word embedding to bias the prior in topic models to jointly model aspect words and opinion words. Though working fairly well, topic models are generative models and do not enforce topic distinctiveness— topic-word distribution can largely overlap among different topics, allowing topics to resemble each other. Besides, topic models yield unstable results, causing large variance in classifi"
2020.emnlp-main.666,P19-1421,0,0.351222,"set of entities that belong to the same semantic class (i.e., Country). Entity synonym discovery (ESD) intends to group all terms in a vocabulary that refer to the same realworld entity (e.g., “America” and “USA” refer to the same country) into a synonym set (hence called a synset). Those discovered entities and synsets include rich knowledge and can benefit many downstream applications such as semantic search (Xiong Contributions. TX xiangren@usc.edu derives Land of Lincoln Introduction * Equal IL Vocabulary V 4 et al., 2017), taxonomy construction (Shen et al., 2018a), and online education (Yu et al., 2019a). Previous studies regard ESE and ESD as two independent tasks. Many ESE methods (Mamou et al., 2018b; Yan et al., 2019; Huang et al., 2020; Zhang et al., 2020; Zhu et al., 2020) are developed to iteratively select and add the most confident entities into the set. A core challenge for ESE is to find those infrequent long-tail entities in the target semantic class (e.g., “Lone Star State” in the class US_States) while filtering out false positive entities from other related classes (e.g., “Austin” and “Dallas” in the class City) as they will cause semantic shift to the set. Meanwhile, various"
2020.emnlp-main.666,2020.acl-main.725,1,0.906301,"o the same realworld entity (e.g., “America” and “USA” refer to the same country) into a synonym set (hence called a synset). Those discovered entities and synsets include rich knowledge and can benefit many downstream applications such as semantic search (Xiong Contributions. TX xiangren@usc.edu derives Land of Lincoln Introduction * Equal IL Vocabulary V 4 et al., 2017), taxonomy construction (Shen et al., 2018a), and online education (Yu et al., 2019a). Previous studies regard ESE and ESD as two independent tasks. Many ESE methods (Mamou et al., 2018b; Yan et al., 2019; Huang et al., 2020; Zhang et al., 2020; Zhu et al., 2020) are developed to iteratively select and add the most confident entities into the set. A core challenge for ESE is to find those infrequent long-tail entities in the target semantic class (e.g., “Lone Star State” in the class US_States) while filtering out false positive entities from other related classes (e.g., “Austin” and “Dallas” in the class City) as they will cause semantic shift to the set. Meanwhile, various ESD methods (Qu et al., 2017; Ustalov et al., 2017a; Wang et al., 2019; Shen et al., 2019) combine stringlevel features with embedding features to find a query"
2020.emnlp-main.724,2020.acl-main.194,0,0.0998556,"on large-scale labeled documents (usually over tens of thousands), thanks to their strong representation learning power that effectively captures the high-order, long-range semantic dependency in text sequences for accurate classification. Recently, increasing attention has been paid to semi-supervised text classification which requires a much smaller amount of labeled data. The success of semi-supervised methods stems from the usage of abundant unlabeled data: Unlabeled documents provide natural regularization for constraining the model predictions to be invariant to small changes in input (Chen et al., 2020; Miyato et al., 2017; Xie et al., 2019), thus improving the generalization ability of the model. Despite mitigating the annotation burden, semi-supervised methods still require manual efforts from domain experts, which might be difficult or expensive to obtain especially when the number of classes is large. Contrary to existing supervised and semisupervised models which learn from labeled documents, a human expert will just need to understand the label name (i.e., a single or a few representative words) of each class to classify documents. For example, we can easily classify news articles whe"
2020.emnlp-main.724,N19-1423,0,0.532004,"will just need to understand the label name (i.e., a single or a few representative words) of each class to classify documents. For example, we can easily classify news articles when given the label names such as “sports”, “business”, and “politics” because we are able to understand these topics based on prior knowledge. In this paper, we study the problem of weaklysupervised text classification where only the label name of each class is provided to train a classifier on purely unlabeled data. We propose a language model self-training approach wherein a pre-trained neural language model (LM) (Devlin et al., 2019; Peters et al., 2018; Radford et al., 2018; Yang et al., 2019) is used as both the general knowledge source for category understanding and feature representation learning model for classification. The LM 9006 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 9006–9017, c November 16–20, 2020. 2020 Association for Computational Linguistics creates contextualized word-level category supervision from unlabeled data to train itself, and then generalizes to document-level classification via a self-training objective. Specifically, we propose the LOTClass"
2020.emnlp-main.724,2020.emnlp-main.568,1,0.741729,"loss. Test Acc. (20, 0.822) 0.15 Loss Test Acc. Test Acc. 0.84 LOTClass BERT w. simple match 0.90 Acc. Loss 0.86 (50, 0.867) 0.85 0.80 0.75 0 200 400 600 Steps 800 (c) LOTClass vs. BERT w. simple match during self-training. Figure 2: (On AG News dataset.) (a) The performance of LOTClass is close to that of Supervised BERT with 48 labeled documents per class. (b) The self-training loss of LOTClass decreases in a period of 50 steps; the performance of LOTClass gradually improves. (c) BERT w. simple match does not benefit from self-training. tity recognition and aspect-based sentiment analysis (Huang et al., 2020). Sometimes a label name could be too generic to interpret (e.g., “person”, “time”, etc). To apply similar methods as introduced in this paper to these scenarios, one may consider instantiating the label names with more concrete example terms like specific person names. Limitation of weakly-supervised classification. There are difficult cases where label names are not sufficient to teach the model for correct classification. For example, some review texts implicitly express sentiment polarity that goes beyond wordlevel understanding: “I find it sad that just because Edward Norton did not want"
2020.emnlp-main.724,D14-1181,0,0.00335127,"ocuments but learning from unlabeled data supervised by at most 3 words (1 in most cases) per class as the label name1 . 1 Introduction Text classification is a classic and fundamental task in Natural Language Processing (NLP) with a wide spectrum of applications such as question answering (Rajpurkar et al., 2016), spam detection (Jindal and Liu, 2007) and sentiment analysis (Pang et al., 2002). Building an automatic text classification model has been viewed as a task of training machine learning models from human-labeled documents. Indeed, many deep learning-based classifiers including CNNs (Kim, 2014; Zhang et al., 2015) and RNNs (Tang et al., 2015a; Yang et al., 1 Source code can be found at https://github.com/ yumeng5/LOTClass. 2016) have been developed and achieved great success when trained on large-scale labeled documents (usually over tens of thousands), thanks to their strong representation learning power that effectively captures the high-order, long-range semantic dependency in text sequences for accurate classification. Recently, increasing attention has been paid to semi-supervised text classification which requires a much smaller amount of labeled data. The success of semi-sup"
2020.emnlp-main.724,2020.acl-main.703,0,0.0161316,"l category prediction task that trains LM to predict the implied category of a word using its contexts. The LM so trained generalizes well to document-level classification upon self-training on unlabeled corpus. • On four benchmark datasets, LOTClass outperforms significantly weakly-supervised models and has comparable performance to strong semisupervised and supervised models. 2 2.1 Related Work Neural Language Models ELMo (Peters et al., 2018), GPT (Radford et al., 2018) and XLNet (Yang et al., 2019) and autoencoding LMs such as BERT (Devlin et al., 2019) and its variants (Lan et al., 2020; Lewis et al., 2020; Liu et al., 2019b), has brought astonishing performance improvement to a wide range of NLP tasks, mainly for two reasons: (1) LMs are pre-trained on largescale text corpora, which allow the models to learn generic linguistic features (Tenney et al., 2019) and serve as knowledge bases (Petroni et al., 2019); and (2) LMs enjoy strong feature representation learning power of capturing high-order, long-range dependency in texts thanks to the Transformer architecture (Vaswani et al., 2017). 2.2 For semi-supervised text classification, two lines of framework are developed to leverage unlabeled dat"
2020.emnlp-main.724,D19-1486,0,0.0800171,"n task that trains LM to predict the implied category of a word using its contexts. The LM so trained generalizes well to document-level classification upon self-training on unlabeled corpus. • On four benchmark datasets, LOTClass outperforms significantly weakly-supervised models and has comparable performance to strong semisupervised and supervised models. 2 2.1 Related Work Neural Language Models ELMo (Peters et al., 2018), GPT (Radford et al., 2018) and XLNet (Yang et al., 2019) and autoencoding LMs such as BERT (Devlin et al., 2019) and its variants (Lan et al., 2020; Lewis et al., 2020; Liu et al., 2019b), has brought astonishing performance improvement to a wide range of NLP tasks, mainly for two reasons: (1) LMs are pre-trained on largescale text corpora, which allow the models to learn generic linguistic features (Tenney et al., 2019) and serve as knowledge bases (Petroni et al., 2019); and (2) LMs enjoy strong feature representation learning power of capturing high-order, long-range dependency in texts thanks to the Transformer architecture (Vaswani et al., 2017). 2.2 For semi-supervised text classification, two lines of framework are developed to leverage unlabeled data. Augmentation-ba"
2020.emnlp-main.724,2021.ccl-1.108,0,0.107411,"Missing"
2020.emnlp-main.724,P11-1015,0,0.192289,",000 7,600 70,000 25,000 400,000 Table 5: Dataset statistics. Supervised models are trained on the entire training set. Semi-supervised models use 10 labeled documents per class from the training set and the rest as unlabeled data. Weakly-supervised models are trained by using the entire training set as unlabeled data. All models are evaluated on the test set. 4 4.1 Experiments unlabeled data. All methods are evaluated on the test set. Datasets Weakly-supervised methods: We use four benchmark datasets for text classification: AG News (Zhang et al., 2015), DBPedia (Lehmann et al., 2015), IMDB (Maas et al., 2011) and Amazon (McAuley and Leskovec, 2013). The dataset statistics are shown in Table 5. All datasets are in English language. 4.2 Compared Methods We compare LOTClass with a wide range of weakly-supervised methods and also state-of-theart semi-supervised and supervised methods. The label names used as supervision on each dataset for the weakly-supervised methods are shown in Tables 2, 3, 4 and 9. (Table 9 can be found in Appendix A.) Fully supervised methods use the entire training set for model training. Semi-supervised method UDA uses 10 labeled documents per class from the training set and t"
2020.emnlp-main.724,2020.acl-main.30,0,0.360439,"label name semantics and derive documentconcept relevance via explicit semantic analysis (Gabrilovich and Markovitch, 2007). Since the classifier is learned purely from general knowledge without even requiring any unlabeled domainspecific data, these methods are called dataless classification (Chang et al., 2008; Song and Roth, 2014; Yin et al., 2019). Later, topic models (Chen et al., 2015; Li et al., 2016) are exploited for seedguided classification to learn seed word-aware topics by biasing the Dirichlet priors and to infer posterior document-topic assignment. Recently, neural approaches (Mekala and Shang, 2020; Meng et al., 2018, 2019) have been developed for weaklysupervised text classification. They assign documents pseudo labels to train a neural classifier by either generating pseudo documents or using LMs to detect category-indicative words. While achieving inspiring performance, these neural approaches train classifiers from scratch on the local corpus and fail to take advantage of the general knowledge source used by dataless classification. In this paper, we build our method upon pre-trained LMs, which are used both as general linguistic knowledge sources for understanding the semantics of"
2020.emnlp-main.724,W02-1011,0,0.0394697,"Missing"
2020.emnlp-main.724,D14-1162,0,0.0820586,"Missing"
2020.emnlp-main.724,N18-1202,0,0.335119,"erstand the label name (i.e., a single or a few representative words) of each class to classify documents. For example, we can easily classify news articles when given the label names such as “sports”, “business”, and “politics” because we are able to understand these topics based on prior knowledge. In this paper, we study the problem of weaklysupervised text classification where only the label name of each class is provided to train a classifier on purely unlabeled data. We propose a language model self-training approach wherein a pre-trained neural language model (LM) (Devlin et al., 2019; Peters et al., 2018; Radford et al., 2018; Yang et al., 2019) is used as both the general knowledge source for category understanding and feature representation learning model for classification. The LM 9006 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 9006–9017, c November 16–20, 2020. 2020 Association for Computational Linguistics creates contextualized word-level category supervision from unlabeled data to train itself, and then generalizes to document-level classification via a self-training objective. Specifically, we propose the LOTClass model for Label-Name"
2020.emnlp-main.724,D19-1250,0,0.0584868,"Missing"
2020.emnlp-main.724,N16-1174,0,0.188184,"Missing"
2020.emnlp-main.724,D19-1404,0,0.155237,"ication Weakly-supervised text classification aims to categorize text documents based only on word-level descriptions of each category, eschewing the need of any labeled documents. Early attempts rely on distant supervision such as Wikipedia to interpret 9007 the label name semantics and derive documentconcept relevance via explicit semantic analysis (Gabrilovich and Markovitch, 2007). Since the classifier is learned purely from general knowledge without even requiring any unlabeled domainspecific data, these methods are called dataless classification (Chang et al., 2008; Song and Roth, 2014; Yin et al., 2019). Later, topic models (Chen et al., 2015; Li et al., 2016) are exploited for seedguided classification to learn seed word-aware topics by biasing the Dirichlet priors and to infer posterior document-topic assignment. Recently, neural approaches (Mekala and Shang, 2020; Meng et al., 2018, 2019) have been developed for weaklysupervised text classification. They assign documents pseudo labels to train a neural classifier by either generating pseudo documents or using LMs to detect category-indicative words. While achieving inspiring performance, these neural approaches train classifiers from scra"
2020.emnlp-main.724,D18-1352,0,0.0367154,"al., 2015b; Zhang et al., 2020) build text networks with words, documents and labels and propagate labeling information along the graph via embedding learning (Tang et al., 2015c) or graph neural networks (Kipf and Welling, 2017). Zero-shot text classification generalizes the classifier trained on a known label set to an unknown one without using any new labeled documents. Transferring knowledge from seen classes to unseen ones typically relies on semantic attributes and descriptions of all classes (Liu et al., 2019a; Pushp and Srivastava, 2017; Xia et al., 2018), correlations among classes (Rios and Kavuluru, 2018; Zhang et al., 2019) or joint embeddings of classes and documents (Nam et al., 2016). However, zero-shot learning still requires labeled data for the seen label set and cannot be applied to cases where no labeled documents for any class is available. 2.3 Pre-training deep neural models for language modeling, including autoregressive LMs such as 2 Other semi-supervised/weakly-supervised methods usually take advantage of distant supervision like Wikipedia dump (Chang et al., 2008), or augmentation systems like trained back translation models (Xie et al., 2019). Semi-Supervised and Zero-Shot Tex"
2020.emnlp-main.724,P16-1009,0,0.318549,"t al., 2019) and serve as knowledge bases (Petroni et al., 2019); and (2) LMs enjoy strong feature representation learning power of capturing high-order, long-range dependency in texts thanks to the Transformer architecture (Vaswani et al., 2017). 2.2 For semi-supervised text classification, two lines of framework are developed to leverage unlabeled data. Augmentation-based methods generate new instances and regularize the model’s predictions to be invariant to small changes in input. The augmented instances can be either created as real text sequences (Xie et al., 2019) via back translation (Sennrich et al., 2016) or in the hidden states of the model via perturbations (Miyato et al., 2017) or interpolations (Chen et al., 2020). Graph-based methods (Tang et al., 2015b; Zhang et al., 2020) build text networks with words, documents and labels and propagate labeling information along the graph via embedding learning (Tang et al., 2015c) or graph neural networks (Kipf and Welling, 2017). Zero-shot text classification generalizes the classifier trained on a known label set to an unknown one without using any new labeled documents. Transferring knowledge from seen classes to unseen ones typically relies on se"
2020.emnlp-main.724,D15-1167,0,0.257446,"supervised by at most 3 words (1 in most cases) per class as the label name1 . 1 Introduction Text classification is a classic and fundamental task in Natural Language Processing (NLP) with a wide spectrum of applications such as question answering (Rajpurkar et al., 2016), spam detection (Jindal and Liu, 2007) and sentiment analysis (Pang et al., 2002). Building an automatic text classification model has been viewed as a task of training machine learning models from human-labeled documents. Indeed, many deep learning-based classifiers including CNNs (Kim, 2014; Zhang et al., 2015) and RNNs (Tang et al., 2015a; Yang et al., 1 Source code can be found at https://github.com/ yumeng5/LOTClass. 2016) have been developed and achieved great success when trained on large-scale labeled documents (usually over tens of thousands), thanks to their strong representation learning power that effectively captures the high-order, long-range semantic dependency in text sequences for accurate classification. Recently, increasing attention has been paid to semi-supervised text classification which requires a much smaller amount of labeled data. The success of semi-supervised methods stems from the usage of abundant"
2020.emnlp-main.724,N19-1108,0,0.0189471,", 2020) build text networks with words, documents and labels and propagate labeling information along the graph via embedding learning (Tang et al., 2015c) or graph neural networks (Kipf and Welling, 2017). Zero-shot text classification generalizes the classifier trained on a known label set to an unknown one without using any new labeled documents. Transferring knowledge from seen classes to unseen ones typically relies on semantic attributes and descriptions of all classes (Liu et al., 2019a; Pushp and Srivastava, 2017; Xia et al., 2018), correlations among classes (Rios and Kavuluru, 2018; Zhang et al., 2019) or joint embeddings of classes and documents (Nam et al., 2016). However, zero-shot learning still requires labeled data for the seen label set and cannot be applied to cases where no labeled documents for any class is available. 2.3 Pre-training deep neural models for language modeling, including autoregressive LMs such as 2 Other semi-supervised/weakly-supervised methods usually take advantage of distant supervision like Wikipedia dump (Chang et al., 2008), or augmentation systems like trained back translation models (Xie et al., 2019). Semi-Supervised and Zero-Shot Text Classification Weak"
2020.emnlp-main.724,D18-1348,0,0.017091,"hen et al., 2020). Graph-based methods (Tang et al., 2015b; Zhang et al., 2020) build text networks with words, documents and labels and propagate labeling information along the graph via embedding learning (Tang et al., 2015c) or graph neural networks (Kipf and Welling, 2017). Zero-shot text classification generalizes the classifier trained on a known label set to an unknown one without using any new labeled documents. Transferring knowledge from seen classes to unseen ones typically relies on semantic attributes and descriptions of all classes (Liu et al., 2019a; Pushp and Srivastava, 2017; Xia et al., 2018), correlations among classes (Rios and Kavuluru, 2018; Zhang et al., 2019) or joint embeddings of classes and documents (Nam et al., 2016). However, zero-shot learning still requires labeled data for the seen label set and cannot be applied to cases where no labeled documents for any class is available. 2.3 Pre-training deep neural models for language modeling, including autoregressive LMs such as 2 Other semi-supervised/weakly-supervised methods usually take advantage of distant supervision like Wikipedia dump (Chang et al., 2008), or augmentation systems like trained back translation models"
2021.acl-long.316,2020.emnlp-main.550,0,0.0418601,"Missing"
2021.acl-long.316,P17-1171,0,0.530685,"ves state-of-the-art performance on Natural Questions and TriviaQA datasets under the extractive QA setup when equipped with an extractive reader, and consistently outperforms other retrieval methods when the same generative reader is used.1 1 Introduction Open-domain question answering (OpenQA) aims to answer factoid questions without a pre-specified domain and has numerous real-world applications. In OpenQA, a large collection of documents (e.g., Wikipedia) are often used to seek information pertaining to the questions. One of the most common approaches uses a retriever-reader architecture (Chen et al., 2017), which first retrieves a small subset of documents using the question as the query and then reads the retrieved documents to extract (or generate) an answer. The retriever is crucial as it is infeasible to examine every piece of information in the entire document collection (e.g., millions of Wikipedia passages) and the retrieval accuracy bounds the performance of the (extractive) reader. ∗ Work was done during internship at Microsoft Azure AI. Our code is available at https://github.com/ morningmoni/GAR. 1 Early OpenQA systems (Chen et al., 2017) use classic retrieval methods such as TF-IDF"
2021.acl-long.316,Q19-1026,0,0.0800737,"2seq learning with the question as the input and various freely accessible in-domain contexts as the output such as the answer, the sentence where the answer belongs to, and the title of a passage that contains the answer. We then append the generated contexts to the question as the generationaugmented query for retrieval. We demonstrate that using multiple contexts from diverse generation targets is beneficial as fusing the retrieval results of different generation-augmented queries consistently yields better retrieval accuracy. We conduct extensive experiments on the Natural Questions (NQ) (Kwiatkowski et al., 2019) and TriviaQA (Trivia) (Joshi et al., 2017) datasets. The results reveal four major advantages of G AR: (1) G AR, combined with BM25, achieves significant gains over the same BM25 model that uses the original queries or existing unsupervised query expansion (QE) methods. (2) G AR with sparse representations (BM25) achieves comparable or even better performance than the current state-of-the-art retrieval methods, such as DPR (Karpukhin et al., 2020), that use dense representations. (3) Since G AR uses sparse representations to measure lexical overlap2 , it is complementary to dense representati"
2021.acl-long.316,N19-1423,0,0.184595,"cient (including the cost of the generation model), closing the gap between sparse and dense retrieval methods. reader design of the major baselines for a fair comparison, while virtually any existing QA reader can be used with G AR. 4.1 For the extractive setup, we largely follow the design of the extractive reader in DPR (Karpukhin et al., 2020). Let D = [d1 , d2 , ..., dk ] denote the list of retrieved passages with passage relevance scores D. Let Si = [s1 , s2 , ..., sN ] denote the top N text spans in passage di ranked by span relevance scores Si . Briefly, the DPR reader uses BERT-base (Devlin et al., 2019) for representation learning, where it estimates the passage relevance score Dk for each retrieved passage dk based on the [CLS] tokens of all retrieved passages D, and assigns span relevance scores Si for each candidate span based on the representations of its start and end tokens. Finally, the span with the highest span relevance score from the passage with the highest passage relevance score is chosen as the answer. We refer the readers to Karpukhin et al. (2020) for more details. Passage-level Span Voting. Many extractive QA methods (Chen et al., 2017; Min et al., 2019b; Guu et al., 2020;"
2021.acl-long.316,P17-1147,0,0.271232,"various freely accessible in-domain contexts as the output such as the answer, the sentence where the answer belongs to, and the title of a passage that contains the answer. We then append the generated contexts to the question as the generationaugmented query for retrieval. We demonstrate that using multiple contexts from diverse generation targets is beneficial as fusing the retrieval results of different generation-augmented queries consistently yields better retrieval accuracy. We conduct extensive experiments on the Natural Questions (NQ) (Kwiatkowski et al., 2019) and TriviaQA (Trivia) (Joshi et al., 2017) datasets. The results reveal four major advantages of G AR: (1) G AR, combined with BM25, achieves significant gains over the same BM25 model that uses the original queries or existing unsupervised query expansion (QE) methods. (2) G AR with sparse representations (BM25) achieves comparable or even better performance than the current state-of-the-art retrieval methods, such as DPR (Karpukhin et al., 2020), that use dense representations. (3) Since G AR uses sparse representations to measure lexical overlap2 , it is complementary to dense representations: by fusing the retrieval results of G A"
2021.acl-long.316,P19-1612,0,0.194727,"ieval for OpenQA, respectively. For query expansion, we re-emphasize that G AR is the first QE approach designed for OpenQA and most of the recent approaches are not applicable or efficient enough for OpenQA since they have task-specific objectives, require external supervision that was shown to transfer poorly to OpenQA, or take many days to train (Sec. 2). We thus compare with a classic unsupervised QE method RM3 (Abdul-Jaleel et al., 2004) that does not need external resources for a fair comparison. For passage reading, we compare with both extractive (Min et al., 2019a; Asai et al., 2019; Lee et al., 2019; Min et al., 2019b; Guu et al., 2020; Karpukhin et al., 2020) and generative (Brown et al., 2020; Roberts et al., 2020; Min et al., 2020; Lewis et al., 2020a; Izacard and Grave, 2020) methods when equipping G AR with the corresponding reader. 5.4 Implementation Details Retriever. We use Anserini (Yang et al., 2017) for text retrieval of BM25 and G AR with its default parameters. We conduct grid search for the QE baseline RM3 (Abdul-Jaleel et al., 2004). Generator. We use BART-large (Lewis et al., 2019) to generate query contexts in G AR. When there are multiple desired targets (such as multip"
2021.acl-long.316,2020.acl-main.703,0,0.0842252,"Missing"
2021.acl-long.316,W15-4640,0,0.0161974,"on both the ranking and score of the passages. As the generator and retriever are largely independent now, it is also interesting to study how to jointly or iteratively optimize generation and retrieval such that the generator is aware of the retriever and generates query contexts more beneficial for the retrieval stage. Last but not least, it is very likely that better results can be obtained by more extensive hyper-parameter tuning. Applicability to other tasks. Beyond OpenQA, G AR also has great potentials for other tasks that involve text matching such as conversation utterance selection (Lowe et al., 2015; Dinan et al., 2020) or information retrieval (Nguyen et al., 2016; Craswell et al., 2020). The default generation target is always available for supervised tasks. For example, for conversation utterance selection one can use the reference utterance as the default target and then match the concatenation of the conversation history and the generated utterance with the provided utterance candidates. For article search, the default target could be (part of) the ground-truth article itself. Other generation targets are more taskspecific and can be designed as long as they can be fetched from the"
2021.acl-long.316,D19-1284,0,0.509075,"uses BERT-base (Devlin et al., 2019) for representation learning, where it estimates the passage relevance score Dk for each retrieved passage dk based on the [CLS] tokens of all retrieved passages D, and assigns span relevance scores Si for each candidate span based on the representations of its start and end tokens. Finally, the span with the highest span relevance score from the passage with the highest passage relevance score is chosen as the answer. We refer the readers to Karpukhin et al. (2020) for more details. Passage-level Span Voting. Many extractive QA methods (Chen et al., 2017; Min et al., 2019b; Guu et al., 2020; Karpukhin et al., 2020) measure the probability of span extraction in different retrieved passages independently, despite that their collective signals may provide more evidence in determining the correct answer. We propose a simple yet effective passage-level span voting mechanism, which aggregates the predictions of the spans in the same surface form from different retrieved passages. Intuitively, if a text span is considered as the answer multiple times in different passages, it is more likely to be the correct answer. Specifically, G AR calculates a normalized score p("
2021.acl-long.316,2020.emnlp-main.466,0,0.715239,"ormation of the questions. G AR extends to contexts relevant to the questions by extracting information inside PLMs and helps sparse methods achieve comparable or better performance than dense methods (Guu et al., 2020; Karpukhin et al., 2020), while enjoying the simplicity and efficiency of sparse representations. G AR can also be used with dense representations to seek for even better performance, which we leave as future work. Generative QA. Generative QA generates answers through seq2seq learning instead of extracting answer spans. Recent studies on generative OpenQA (Lewis et al., 2020a; Min et al., 2020; Izacard and Grave, 2020) are orthogonal to G AR in that they focus on improving the reading stage and directly reuse DPR (Karpukhin et al., 2020) as the retriever. Unlike generative QA, the goal of G AR is not to generate perfect answers to the questions but pertinent contexts that are helpful for retrieval. Another line in generative QA learns to generate answers without relevant passages as the evidence but solely the question itself using PLMs (Roberts et al., 2020; Brown et al., 2020). G AR further confirms that one can extract factual knowledge from PLMs, which is not limited to the ans"
2021.acl-long.316,D17-1061,0,0.408758,"There have been some recent studies on query reformulation with text generation for other retrieval tasks, which, for example, rewrite the queries to context-independent (Yu et al., 2020; Lin et al., 2020; Vakulenko et al., 2020) or well-formed (Liu et al., 2019) ones. However, these methods require either task-specific data (e.g., conversational contexts, ill-formed queries) or external resources such as paraphrase data (Zaiem and Sadat, 2019; Wang et al., 2020) that cannot or do not transfer well to OpenQA. Also, some rely on timeconsuming training process like reinforcement learning (RL) (Nogueira and Cho, 2017; Liu et al., 2019; Wang et al., 2020) that is not efficient enough for OpenQA (more discussions in Sec. 2). In this paper, we propose GenerationAugmented Retrieval (G AR), which augments a query through text generation of a pre-trained language model (PLM). Different from prior studies that reformulate queries, G AR does not require external resources or downstream feedback via RL as supervision, because it does not rewrite the query but expands it with heuristically discov4089 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International J"
2021.acl-long.316,2020.emnlp-main.437,0,0.0256691,"Missing"
2021.emnlp-main.413,2021.naacl-main.384,0,0.113457,"the tokens in a sequence equally important, it is unclear how many of the * Equal contribution 1 important input concepts can be (or have been) Our code is available at https://github.com/ morningmoni/EDE preserved. Existing attempts to alleviate the above 5063 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5063–5074 c November 7–11, 2021. 2021 Association for Computational Linguistics issue use soft constraints, such as copy mechanism or additional attention, to focus on (certain parts of) the source input (See et al., 2017; Dinan et al., 2019; Dou et al., 2021). Nevertheless, they still lack explicit guidance and resort to seq2seq learning itself to figure out what is important, without any guarantee of the model output or evaluation on the input concept preservation. Explicit guidance for text generation can be achieved by lexically (hard) constrained generation (LCGen), which specifies lexical constraints (tokens) that must be included in the model output. However, to what extent guiding text-totext generation with lexical constraints works in general remains unknown, as existing studies on LCGen (Hokamp and Liu, 2017; Post and Vilar, 2018; Zhang"
2021.emnlp-main.413,P17-1123,0,0.052269,"Missing"
2021.emnlp-main.413,2020.acl-main.703,0,0.0181177,"We take two widely used news sum- model needs to focus on. We use copy mechamarization datasets, CNN/Daily Mail (CNN/DM) nism as a representative example for this purpose. (Nallapati et al., 2016) and XSum (Narayan et al., Copy mechanism estimates the importance of to2018), for evaluation, where CNN/DM is more kens in the source input and learns to copy them extractive (Mao et al., 2020a) and XSum more ab- when appropriate, which is useful for preserving stractive (Maynez et al., 2020) in nature. the important concepts in the input, especially rare 5065 2.4 Experimental Settings We use BART (Lewis et al., 2020) as the major base model for analysis. We use spaCy (Honnibal and Montani, 2017) to extract the entities from the target output as the gold concepts, the quality of which is shown to be reasonably good for the source-target alignment in summarization (Nan et al., 2021). We mainly use exact matching to be consistent with the current automatic metrics that generally consider lexical overlap, while also manually analyzing the missing concepts to address the limitation of exact matching. More implementation details can be found in App. A. 2.5 Results and Analysis Concept Availability. We first exa"
2021.emnlp-main.413,D19-1387,0,0.0226509,"Missing"
2021.emnlp-main.413,2020.acl-main.445,1,0.928399,"nce-to-sequence (seq2seq) learning – this is particularly the case for the recent pre-trained language models (PLMs) (Lewis et al., 1 Introduction 2020; Raffel et al., 2020), where seq2seq learning is Text-to-text generation is an important research expected to identify what to attend to in the source problem with a broad set of applications, such as input and what to include in the model output, with dialog response generation (Dinan et al., 2019), access to only parallel training data. headline generation (Gu et al., 2020), and sumHowever, as seq2seq learning does not explicitly marization (Mao et al., 2020b). A distinct feature focus on key concepts (e.g., named entities) and of text-to-text generation (vs. free-form text gen- commonly used evaluation metrics (e.g., BLEU eration) is that it is often desired to preserve the and ROUGE) also treat all the tokens in a sequence equally important, it is unclear how many of the * Equal contribution 1 important input concepts can be (or have been) Our code is available at https://github.com/ morningmoni/EDE preserved. Existing attempts to alleviate the above 5063 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pa"
2021.emnlp-main.413,2020.emnlp-main.136,1,0.930371,"nce-to-sequence (seq2seq) learning – this is particularly the case for the recent pre-trained language models (PLMs) (Lewis et al., 1 Introduction 2020; Raffel et al., 2020), where seq2seq learning is Text-to-text generation is an important research expected to identify what to attend to in the source problem with a broad set of applications, such as input and what to include in the model output, with dialog response generation (Dinan et al., 2019), access to only parallel training data. headline generation (Gu et al., 2020), and sumHowever, as seq2seq learning does not explicitly marization (Mao et al., 2020b). A distinct feature focus on key concepts (e.g., named entities) and of text-to-text generation (vs. free-form text gen- commonly used evaluation metrics (e.g., BLEU eration) is that it is often desired to preserve the and ROUGE) also treat all the tokens in a sequence equally important, it is unclear how many of the * Equal contribution 1 important input concepts can be (or have been) Our code is available at https://github.com/ morningmoni/EDE preserved. Existing attempts to alleviate the above 5063 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pa"
2021.emnlp-main.413,2020.acl-main.123,0,0.0432268,"Missing"
2021.emnlp-main.413,2020.acl-main.173,0,0.287024,"ow Hollywood got its name? It's a pretty funny story! [Headline Generation] [Source Input] Israel's air forces on Monday launched a third missile strike on Gaza city in less than four hours, Palestinian witnesses said. [Target Output] Israel launches third airstrike on Gaza city. Figure 1: Examples of text-to-text generation tasks where preserving important input concepts is crucial for producing satisfactory results. concepts in the source input (see Fig. 1 for an illustration). On one hand, concept preservation is crucial for maintaining the factual consistency between the input and output (Maynez et al., 2020; Nan et al., 2021). On the other hand, encouraging the model to focus on important input concepts may also improve its generation quality (Yao et al., 2019; Li et al., 2020). Mainstream text-to-text generation methods are mostly data-driven, which “hope” to learn meaningful mappings between source input and target output via sequence-to-sequence (seq2seq) learning – this is particularly the case for the recent pre-trained language models (PLMs) (Lewis et al., 1 Introduction 2020; Raffel et al., 2020), where seq2seq learning is Text-to-text generation is an important research expected to ident"
2021.emnlp-main.413,P17-1054,0,0.0170284,"pts or the dataset involves abundant extrinsic information. 4.3 4.4 Evaluation on Concept Preservation As sequence-level metrics consider all the tokens equally important and can only measure overall generation quality, we further conduct conceptlevel evaluation to measure model performance on concept preservation in particular. We first examine the quality of extracted concepts (automatic constraints) in Table 7. We observe that the F1 scores on different datasets are largely in the range of 0.4 to 0.5, which is on par with the state-of-the-art performance on keyphrase extraction benchmarks (Meng et al., 2017). Task SQuAD WoW Gigaword CNN/DM XSum Precision Recall F1 29.1 35.4 41.1 52.0 38.5 62.9 53.1 47.8 50.1 58.3 39.8 42.5 44.2 51.0 46.4 Table 7: Quality of automatically extracted concepts (constraints) on different text-to-text generation tasks. Takeaways From the analysis on concept preservation and model performance when automatic constraints are used, we can see that EDE is likely to improve overall generation quality when the upper bound performance using available gold concepts is high. EDE may not be very effective when many gold concepts cannot be found from the source input, which is an"
2021.emnlp-main.413,K16-1028,0,0.0676565,"Missing"
2021.emnlp-main.413,W12-3018,0,0.0514483,"Missing"
2021.emnlp-main.413,D18-1206,0,0.0265979,"Missing"
2021.emnlp-main.413,N18-1119,0,0.190733,"al., 2019; Dou et al., 2021). Nevertheless, they still lack explicit guidance and resort to seq2seq learning itself to figure out what is important, without any guarantee of the model output or evaluation on the input concept preservation. Explicit guidance for text generation can be achieved by lexically (hard) constrained generation (LCGen), which specifies lexical constraints (tokens) that must be included in the model output. However, to what extent guiding text-totext generation with lexical constraints works in general remains unknown, as existing studies on LCGen (Hokamp and Liu, 2017; Post and Vilar, 2018; Zhang et al., 2020b) focus on scenarios where gold (ground-truth) constraints are given (e.g., generate a story using user-specified keywords), while in generic text-to-text generation tasks the constraints (target output) are unavailable. In this paper, we present a systematic analysis on generic text-to-text generation to understand (1) the abilities of seq2seq models, especially the PLMs, for preserving important input concepts and (2) whether more explicit guidance that uses important input concepts as lexical constraints can complement seq2seq learning. We select four representative tas"
2021.emnlp-main.413,N19-1269,0,0.0204546,"on and also preserves the input concepts better on Gigaword. Table 5: Performance comparison of abstractive summarization on CNN/DM and XSum. counterpart on both datasets. Better performance may be achieved when the constraint extraction method is improved. Apart from sequence-level evaluation (ROUGE), we observe that EDE is consistently better at concept preservation in conceptlevel evaluation (Sec. 4.3). 4.2 Human Evaluation 300k training 10k training In addition to automatic evaluation, we further conduct human evaluation with the following three aspects: closeness, relevancy, and fluency (Prabhumoye et al., 2019). Closeness measures the similarity between the model output and target output. Relevancy considers the quality of model output directly with the source input, since there are usually more valid outputs than the target output for generation tasks. Fluency of the model output is on a scale of 1 (unreadable) to 4 (perfect). Method R-1 R-2 R-L We randomly sample 50 examples on each task Transformer (Vaswani et al., 2017) 10.97 2.23 10.42 and conduct pairwise comparisons between BART, MASS (Song et al., 2019) 25.03 9.48 23.48 UniLMlarge (Dong et al., 2019) 32.96 14.68 30.56 EDE (DBA), and EDE (DDB"
2021.emnlp-main.413,D16-1264,0,0.0205961,"the enforced task of generating a question given a passage and constraints are noisy (inappropriate). We choose the corresponding answer. Ideally, a seq2seq model from LCGen methods that enforce constraints by would learn to focus on the relevant information constraining beam search (Hokamp and Liu, 2017; surrounding the answer span and reuse some of Post and Vilar, 2018), as they function at the inthe concepts in the source input as part of the gen- ference stage and can be easily combined with diferated question. We use the SQuAD 1.1 dataset ferent seq2seq models, making our analysis more (Rajpurkar et al., 2016), which is repurposed by Du generalizable. In contrast, sampling-based and et al. (2017) for question generation. insertion-based approaches are not easily applicaKnowledge-Grounded Dialog. Knowledge- ble to generic text-to-text generation, as they typgrounded dialog involves utterances with ground- ically involve specialized training schemes or do not accept other inputs than the constraints (Miao ings to specific knowledge sources. It is used as et al., 2019; Zhang et al., 2020b; Sha, 2020). another test case for our analysis as the model is Specifically, we take dynamic beam allocation supp"
2021.emnlp-main.413,D15-1044,0,0.0468808,"ith the training set. output, as the <EOS> token is only allowed when Headline Generation. Headline generation fits all the constraints are met. At each decoding step, our analysis as a headline usually consists of the there are three sources of candidates: (1) the top-k most important concepts in the input article. We tokens across all hypotheses as in standard beam use the English Gigaword dataset (Napoles et al., search; (2) all unfulfilled constraint tokens for each 2012) for evaluation. As training on the full train- hypothesis; and (3) the single-best token for each ing set of Gigaword (Rush et al., 2015) is compu- hypothesis. The banks are trimmed by the sequence tationally prohibitive, we adopt two low-resource probability if the total number of candidate tokens settings where the 10k training examples in Dong is beyond capacity (beam size). et al. (2019) and the first 300k of the 3.8M training Soft Constrained Generation. We additionally examples are used. examine the effectiveness of soft constrained genAbstractive Summarization. Abstractive summa- eration for comparison. There are various types rization is used as another testbed as a summary of soft constraints and we particularly consid"
2021.emnlp-main.413,D19-1253,0,0.0503546,"Missing"
2021.emnlp-main.413,P17-1099,0,0.317824,"preserve the and ROUGE) also treat all the tokens in a sequence equally important, it is unclear how many of the * Equal contribution 1 important input concepts can be (or have been) Our code is available at https://github.com/ morningmoni/EDE preserved. Existing attempts to alleviate the above 5063 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5063–5074 c November 7–11, 2021. 2021 Association for Computational Linguistics issue use soft constraints, such as copy mechanism or additional attention, to focus on (certain parts of) the source input (See et al., 2017; Dinan et al., 2019; Dou et al., 2021). Nevertheless, they still lack explicit guidance and resort to seq2seq learning itself to figure out what is important, without any guarantee of the model output or evaluation on the input concept preservation. Explicit guidance for text generation can be achieved by lexically (hard) constrained generation (LCGen), which specifies lexical constraints (tokens) that must be included in the model output. However, to what extent guiding text-totext generation with lexical constraints works in general remains unknown, as existing studies on LCGen (Hokamp and"
2021.emnlp-main.413,2020.emnlp-main.698,0,0.0298321,"Missing"
2021.emnlp-main.413,2020.emnlp-main.701,0,0.182136,"d question. We use the SQuAD 1.1 dataset ferent seq2seq models, making our analysis more (Rajpurkar et al., 2016), which is repurposed by Du generalizable. In contrast, sampling-based and et al. (2017) for question generation. insertion-based approaches are not easily applicaKnowledge-Grounded Dialog. Knowledge- ble to generic text-to-text generation, as they typgrounded dialog involves utterances with ground- ically involve specialized training schemes or do not accept other inputs than the constraints (Miao ings to specific knowledge sources. It is used as et al., 2019; Zhang et al., 2020b; Sha, 2020). another test case for our analysis as the model is Specifically, we take dynamic beam allocation supposed to extract important concepts from the (DBA) (Post and Vilar, 2018) for our analysis, as grounded knowledge when appropriate. We use the Wizard of Wikipedia (WoW) dataset (Dinan et al., it has higher efficiency than other LCGen methods that constrain beam search (Hokamp and Liu, 2019) for evaluation. As we focus on generation 2017). DBA revises the process of beam search by instead of knowledge retrieval, we adopt the gold dividing the beam into a number of groups (named knowledge settin"
2021.emnlp-main.413,2020.acl-main.125,0,0.0685332,"Missing"
2021.emnlp-main.422,D13-1185,0,0.0147787,"on all evaluation tasks, since it ignores the coreferential arguments and their relations, but 6 Related Work relies solely on the overly simplistic temporal order to connect events. This is especially apparent from The definition of a complex event schema septhe instance graph perplexity in Table 3. arates us from related lines of work, namely Learning Corpus Size. An average of 113 in- schema induction and script learning. Previous stance graphs is used for each complex event type work on schema induction aims to characterize 5210 event triggers and participants of individual atomic events (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015; Sha et al., 2016; Yuan et al., 2018), ignoring inter-event relations. Work on script learning, on the other hand, originally limited attention to event chains with a single protagonist (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015; Jans et al., 2012; Granroth-Wilding and Clark, 2016) and later extended to multiple participants (Pichotta and Mooney, 2014, 2016; Weber et al., 2018). Recent efforts rely on distributed representations encoded from the compositional nature of events (Modi, 2016; Granroth-Wilding and Clark, 2016; Weber et al.,"
2021.emnlp-main.422,P08-1090,0,0.742122,"event schema septhe instance graph perplexity in Table 3. arates us from related lines of work, namely Learning Corpus Size. An average of 113 in- schema induction and script learning. Previous stance graphs is used for each complex event type work on schema induction aims to characterize 5210 event triggers and participants of individual atomic events (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015; Sha et al., 2016; Yuan et al., 2018), ignoring inter-event relations. Work on script learning, on the other hand, originally limited attention to event chains with a single protagonist (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015; Jans et al., 2012; Granroth-Wilding and Clark, 2016) and later extended to multiple participants (Pichotta and Mooney, 2014, 2016; Weber et al., 2018). Recent efforts rely on distributed representations encoded from the compositional nature of events (Modi, 2016; Granroth-Wilding and Clark, 2016; Weber et al., 2018, 2020; Zhang et al., 2020), and language modeling (Rudinger et al., 2015; Pichotta and Mooney, 2016; Peng and Roth, 2016). All of these methods still assume that events follow linear order in a single chain. They also overlook the relations between par"
2021.emnlp-main.422,chambers-jurafsky-2010-database,0,0.0434858,"Missing"
2021.emnlp-main.422,N13-1104,0,0.0305441,"n tasks, since it ignores the coreferential arguments and their relations, but 6 Related Work relies solely on the overly simplistic temporal order to connect events. This is especially apparent from The definition of a complex event schema septhe instance graph perplexity in Table 3. arates us from related lines of work, namely Learning Corpus Size. An average of 113 in- schema induction and script learning. Previous stance graphs is used for each complex event type work on schema induction aims to characterize 5210 event triggers and participants of individual atomic events (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015; Sha et al., 2016; Yuan et al., 2018), ignoring inter-event relations. Work on script learning, on the other hand, originally limited attention to event chains with a single protagonist (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015; Jans et al., 2012; Granroth-Wilding and Clark, 2016) and later extended to multiple participants (Pichotta and Mooney, 2014, 2016; Weber et al., 2018). Recent efforts rely on distributed representations encoded from the compositional nature of events (Modi, 2016; Granroth-Wilding and Clark, 2016; Weber et al., 2018, 2020; Zhang et"
2021.emnlp-main.422,P16-1154,0,0.011558,"ained the new event ei is an A RREST event, so we add LDC Schema Learning Ontology. three argument nodes for D ETAINEE, JAILOR, and 5 Compared to (Liao et al., 2019), we do not use the posiP LACE respectively. The edges between these ar- tional embedding mask because the newly generated nodes guments and event ei are also added into the graph. have distinct roles. 5206 3.5 Coreferential Argument Generation After updating the node representations, we detect the entity type of each argument, and also predict whether the argument is coreferential to existing entities. Inspired by copy mechanism (Gu et al., 2016), we classify each argument node vj to either a new entity with entity type φ(vj ), or an existing entity node in the previous graph G&lt;i . For example, in Figure 2, the D ETAINEE should be classified to the existing ATTACKER node, while JAILOR node is classified as P ERSON. Namely, p(hei , aj , vj i|ei , aj ) ( p(hei , aj , vj i, g|ei , aj ) if vj is new, = p(hei , aj , vj i, c|ei , aj ) otherwise, where p(hei , aj , vj i, g|ei , aj ) is the generation probability, classifying the new node to its entity type φ(vj ):  p(hei , aj , vj i, g|ei , aj ) = exp(W φ(vj ) v j ) Z The copy probability p"
2021.emnlp-main.422,E12-1034,0,0.0677415,"Missing"
2021.emnlp-main.422,D19-6014,0,0.0172787,"lso overlook the relations between participants which are critical for understanding the complex event. However, we induce a comprehensive event graph schema, capturing both the temporal dependency and the multi-hop argument dependency across events. Recent work on event graph schema induction (Li et al., 2020) only considers the connections between a pair of two events. Similarly, their event prediction task is designed to automatically generate a missing event (e.g., a word sequence) given a single or a sequence of prerequisite events (Nguyen et al., 2017; Hu et al., 2017; Li et al., 2018b; Kiyomaru et al., 2019; Lv et al., 2019), or predict a pre-condition event given the current events (Kwon et al., 2020). In contrast, we leverage the automatically discovered temporal event schema as guidance to forecast the future events. Existing script annotations (Chambers and Jurafsky, 2008, 2010; Modi et al., 2016; Wanzare et al., 2016; Mostafazadeh et al., 2016a,b; Kwon et al., 2020) cannot support a comprehensive graph schema induction due to the missing of critical event graph structures, such as argument relations. Furthermore, in real-world applications, complex event schemas are expected to be induced f"
2021.emnlp-main.422,2020.findings-emnlp.340,0,0.0275404,"nt. However, we induce a comprehensive event graph schema, capturing both the temporal dependency and the multi-hop argument dependency across events. Recent work on event graph schema induction (Li et al., 2020) only considers the connections between a pair of two events. Similarly, their event prediction task is designed to automatically generate a missing event (e.g., a word sequence) given a single or a sequence of prerequisite events (Nguyen et al., 2017; Hu et al., 2017; Li et al., 2018b; Kiyomaru et al., 2019; Lv et al., 2019), or predict a pre-condition event given the current events (Kwon et al., 2020). In contrast, we leverage the automatically discovered temporal event schema as guidance to forecast the future events. Existing script annotations (Chambers and Jurafsky, 2008, 2010; Modi et al., 2016; Wanzare et al., 2016; Mostafazadeh et al., 2016a,b; Kwon et al., 2020) cannot support a comprehensive graph schema induction due to the missing of critical event graph structures, such as argument relations. Furthermore, in real-world applications, complex event schemas are expected to be induced from large-scale historical data, which is not feasible to annotate manually. We propose a data-dr"
2021.emnlp-main.422,2021.naacl-main.274,1,0.713586,"complex event type, such as car-bombing, we construct a set of instance graphs, where each instance graph is about one complex event, such as Kabul ambulance bombing. We first identify a cluster of documents that describes the same complex event. In this paper, we treat all documents linked to a single Wikipedia page as belonging to the same complex event, detailed in §4.1. We use OneIE, a state-of-the-art Information Extraction system (Lin et al., 2020), to extract entities, relations and events, and then perform crossdocument entity (Pan et al., 2015, 2017) and event coreference resolution (Lai et al., 2021) over the document cluster of each complex event. We further conduct event-event temporal relation extraction (Ning et al., 2019; Wen et al., 2021b) to determine the order of event pairs. We run the entire 2 For simplification purposes, we mention “schema graphs” as “schemas”, and “events” in schemas are only “event types”. pipeline following (Wen et al., 2021a) 3 , and the detailed extraction performance is reported in the paper. After extraction, we construct one instance graph for each complex event, where coreferential events or entities are merged. We consider the isolated events as irrel"
2021.emnlp-main.422,2020.emnlp-main.50,1,0.80141,"tional nature of events (Modi, 2016; Granroth-Wilding and Clark, 2016; Weber et al., 2018, 2020; Zhang et al., 2020), and language modeling (Rudinger et al., 2015; Pichotta and Mooney, 2016; Peng and Roth, 2016). All of these methods still assume that events follow linear order in a single chain. They also overlook the relations between participants which are critical for understanding the complex event. However, we induce a comprehensive event graph schema, capturing both the temporal dependency and the multi-hop argument dependency across events. Recent work on event graph schema induction (Li et al., 2020) only considers the connections between a pair of two events. Similarly, their event prediction task is designed to automatically generate a missing event (e.g., a word sequence) given a single or a sequence of prerequisite events (Nguyen et al., 2017; Hu et al., 2017; Li et al., 2018b; Kiyomaru et al., 2019; Lv et al., 2019), or predict a pre-condition event given the current events (Kwon et al., 2020). In contrast, we leverage the automatically discovered temporal event schema as guidance to forecast the future events. Existing script annotations (Chambers and Jurafsky, 2008, 2010; Modi et a"
2021.emnlp-main.422,2021.naacl-main.69,1,0.668757,"ns between arguments, so we only compute this metric for the IED dataset. IED Schema Learning Corpus: The same type of complex events may have many variants, which depends on the different types of conditions and participants. In order to evaluate our model’s capability at capturing uncertainty and multiple hypotheses, we decided to dive deeper into one scenario and chose the improvised explosive device (IED) as our case study. We first collected Wikipedia articles that describe 4 types of complex events, i.e., Car-bombing IED, Drone Strikes IED, Suicide IED and General IED. Then we followed (Li et al., 2021) to exploit the external links to collect the additional news documents with the corresponding complex event type. The ground-truth schemas for this IED corpus are created manually, through a schema curation 4.3 Instance Graph Perplexity Evaluation tool (Mishra et al., 2021). Only one human schema graph was created for each complex event type, To evaluate our temporal event graph model, we resulting in 4 schemas. In detail, for each com- compute the instance graph perplexity by predictplex event type, we presented example instance ing the instance graphs in the test set, graphs and the ranked"
2021.emnlp-main.422,L16-1555,0,0.0194662,"l., 2020) only considers the connections between a pair of two events. Similarly, their event prediction task is designed to automatically generate a missing event (e.g., a word sequence) given a single or a sequence of prerequisite events (Nguyen et al., 2017; Hu et al., 2017; Li et al., 2018b; Kiyomaru et al., 2019; Lv et al., 2019), or predict a pre-condition event given the current events (Kwon et al., 2020). In contrast, we leverage the automatically discovered temporal event schema as guidance to forecast the future events. Existing script annotations (Chambers and Jurafsky, 2008, 2010; Modi et al., 2016; Wanzare et al., 2016; Mostafazadeh et al., 2016a,b; Kwon et al., 2020) cannot support a comprehensive graph schema induction due to the missing of critical event graph structures, such as argument relations. Furthermore, in real-world applications, complex event schemas are expected to be induced from large-scale historical data, which is not feasible to annotate manually. We propose a data-driven schema induction approach, and choose to use IE systems instead of using manual annotation, to induce schemas that are robust and can tolerate extraction errors. modeling and generation of graphs ("
2021.emnlp-main.422,2020.acl-main.713,1,0.793931,"rded as a summary abstraction of instance graphs, capturing the reoccurring structures. 3 3.1 Our Approach Instance Graph Construction To induce schemas for a complex event type, such as car-bombing, we construct a set of instance graphs, where each instance graph is about one complex event, such as Kabul ambulance bombing. We first identify a cluster of documents that describes the same complex event. In this paper, we treat all documents linked to a single Wikipedia page as belonging to the same complex event, detailed in §4.1. We use OneIE, a state-of-the-art Information Extraction system (Lin et al., 2020), to extract entities, relations and events, and then perform crossdocument entity (Pan et al., 2015, 2017) and event coreference resolution (Lai et al., 2021) over the document cluster of each complex event. We further conduct event-event temporal relation extraction (Ning et al., 2019; Wen et al., 2021b) to determine the order of event pairs. We run the entire 2 For simplification purposes, we mention “schema graphs” as “schemas”, and “events” in schemas are only “event types”. pipeline following (Wen et al., 2021a) 3 , and the detailed extraction performance is reported in the paper. After"
2021.emnlp-main.422,K16-1008,0,0.0141284,"and participants of individual atomic events (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015; Sha et al., 2016; Yuan et al., 2018), ignoring inter-event relations. Work on script learning, on the other hand, originally limited attention to event chains with a single protagonist (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015; Jans et al., 2012; Granroth-Wilding and Clark, 2016) and later extended to multiple participants (Pichotta and Mooney, 2014, 2016; Weber et al., 2018). Recent efforts rely on distributed representations encoded from the compositional nature of events (Modi, 2016; Granroth-Wilding and Clark, 2016; Weber et al., 2018, 2020; Zhang et al., 2020), and language modeling (Rudinger et al., 2015; Pichotta and Mooney, 2016; Peng and Roth, 2016). All of these methods still assume that events follow linear order in a single chain. They also overlook the relations between participants which are critical for understanding the complex event. However, we induce a comprehensive event graph schema, capturing both the temporal dependency and the multi-hop argument dependency across events. Recent work on event graph schema induction (Li et al., 2020) only considers the"
2021.emnlp-main.422,W16-1007,0,0.0187693,"between a pair of two events. Similarly, their event prediction task is designed to automatically generate a missing event (e.g., a word sequence) given a single or a sequence of prerequisite events (Nguyen et al., 2017; Hu et al., 2017; Li et al., 2018b; Kiyomaru et al., 2019; Lv et al., 2019), or predict a pre-condition event given the current events (Kwon et al., 2020). In contrast, we leverage the automatically discovered temporal event schema as guidance to forecast the future events. Existing script annotations (Chambers and Jurafsky, 2008, 2010; Modi et al., 2016; Wanzare et al., 2016; Mostafazadeh et al., 2016a,b; Kwon et al., 2020) cannot support a comprehensive graph schema induction due to the missing of critical event graph structures, such as argument relations. Furthermore, in real-world applications, complex event schemas are expected to be induced from large-scale historical data, which is not feasible to annotate manually. We propose a data-driven schema induction approach, and choose to use IE systems instead of using manual annotation, to induce schemas that are robust and can tolerate extraction errors. modeling and generation of graphs (Li et al., 2018a; Jin et al., 2018; Grover et al."
2021.emnlp-main.422,I17-2007,0,0.0262425,"hat events follow linear order in a single chain. They also overlook the relations between participants which are critical for understanding the complex event. However, we induce a comprehensive event graph schema, capturing both the temporal dependency and the multi-hop argument dependency across events. Recent work on event graph schema induction (Li et al., 2020) only considers the connections between a pair of two events. Similarly, their event prediction task is designed to automatically generate a missing event (e.g., a word sequence) given a single or a sequence of prerequisite events (Nguyen et al., 2017; Hu et al., 2017; Li et al., 2018b; Kiyomaru et al., 2019; Lv et al., 2019), or predict a pre-condition event given the current events (Kwon et al., 2020). In contrast, we leverage the automatically discovered temporal event schema as guidance to forecast the future events. Existing script annotations (Chambers and Jurafsky, 2008, 2010; Modi et al., 2016; Wanzare et al., 2016; Mostafazadeh et al., 2016a,b; Kwon et al., 2020) cannot support a comprehensive graph schema induction due to the missing of critical event graph structures, such as argument relations. Furthermore, in real-world applic"
2021.emnlp-main.422,P15-1019,0,0.0538438,"Missing"
2021.emnlp-main.422,D19-1642,0,0.0347696,"Missing"
2021.emnlp-main.422,N15-1119,1,0.748927,"roach Instance Graph Construction To induce schemas for a complex event type, such as car-bombing, we construct a set of instance graphs, where each instance graph is about one complex event, such as Kabul ambulance bombing. We first identify a cluster of documents that describes the same complex event. In this paper, we treat all documents linked to a single Wikipedia page as belonging to the same complex event, detailed in §4.1. We use OneIE, a state-of-the-art Information Extraction system (Lin et al., 2020), to extract entities, relations and events, and then perform crossdocument entity (Pan et al., 2015, 2017) and event coreference resolution (Lai et al., 2021) over the document cluster of each complex event. We further conduct event-event temporal relation extraction (Ning et al., 2019; Wen et al., 2021b) to determine the order of event pairs. We run the entire 2 For simplification purposes, we mention “schema graphs” as “schemas”, and “events” in schemas are only “event types”. pipeline following (Wen et al., 2021a) 3 , and the detailed extraction performance is reported in the paper. After extraction, we construct one instance graph for each complex event, where coreferential events or en"
2021.emnlp-main.422,P17-1178,1,0.891563,"Missing"
2021.emnlp-main.422,L16-1556,0,0.0245624,"iders the connections between a pair of two events. Similarly, their event prediction task is designed to automatically generate a missing event (e.g., a word sequence) given a single or a sequence of prerequisite events (Nguyen et al., 2017; Hu et al., 2017; Li et al., 2018b; Kiyomaru et al., 2019; Lv et al., 2019), or predict a pre-condition event given the current events (Kwon et al., 2020). In contrast, we leverage the automatically discovered temporal event schema as guidance to forecast the future events. Existing script annotations (Chambers and Jurafsky, 2008, 2010; Modi et al., 2016; Wanzare et al., 2016; Mostafazadeh et al., 2016a,b; Kwon et al., 2020) cannot support a comprehensive graph schema induction due to the missing of critical event graph structures, such as argument relations. Furthermore, in real-world applications, complex event schemas are expected to be induced from large-scale historical data, which is not feasible to annotate manually. We propose a data-driven schema induction approach, and choose to use IE systems instead of using manual annotation, to induce schemas that are robust and can tolerate extraction errors. modeling and generation of graphs (Li et al., 2018a; Jin"
2021.emnlp-main.422,P16-1028,0,0.0157009,"ations. Work on script learning, on the other hand, originally limited attention to event chains with a single protagonist (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015; Jans et al., 2012; Granroth-Wilding and Clark, 2016) and later extended to multiple participants (Pichotta and Mooney, 2014, 2016; Weber et al., 2018). Recent efforts rely on distributed representations encoded from the compositional nature of events (Modi, 2016; Granroth-Wilding and Clark, 2016; Weber et al., 2018, 2020; Zhang et al., 2020), and language modeling (Rudinger et al., 2015; Pichotta and Mooney, 2016; Peng and Roth, 2016). All of these methods still assume that events follow linear order in a single chain. They also overlook the relations between participants which are critical for understanding the complex event. However, we induce a comprehensive event graph schema, capturing both the temporal dependency and the multi-hop argument dependency across events. Recent work on event graph schema induction (Li et al., 2020) only considers the connections between a pair of two events. Similarly, their event prediction task is designed to automatically generate a missing event (e.g., a word sequence) given a single o"
2021.emnlp-main.422,E14-1024,0,0.0227831,"duction and script learning. Previous stance graphs is used for each complex event type work on schema induction aims to characterize 5210 event triggers and participants of individual atomic events (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015; Sha et al., 2016; Yuan et al., 2018), ignoring inter-event relations. Work on script learning, on the other hand, originally limited attention to event chains with a single protagonist (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015; Jans et al., 2012; Granroth-Wilding and Clark, 2016) and later extended to multiple participants (Pichotta and Mooney, 2014, 2016; Weber et al., 2018). Recent efforts rely on distributed representations encoded from the compositional nature of events (Modi, 2016; Granroth-Wilding and Clark, 2016; Weber et al., 2018, 2020; Zhang et al., 2020), and language modeling (Rudinger et al., 2015; Pichotta and Mooney, 2016; Peng and Roth, 2016). All of these methods still assume that events follow linear order in a single chain. They also overlook the relations between participants which are critical for understanding the complex event. However, we induce a comprehensive event graph schema, capturing both the temporal depen"
2021.emnlp-main.422,2020.emnlp-main.612,0,0.0293622,"Missing"
2021.emnlp-main.422,2021.naacl-demos.16,1,0.847555,"Missing"
2021.emnlp-main.422,D15-1195,0,0.0450596,"Missing"
2021.emnlp-main.422,N16-1049,0,0.0199393,"l arguments and their relations, but 6 Related Work relies solely on the overly simplistic temporal order to connect events. This is especially apparent from The definition of a complex event schema septhe instance graph perplexity in Table 3. arates us from related lines of work, namely Learning Corpus Size. An average of 113 in- schema induction and script learning. Previous stance graphs is used for each complex event type work on schema induction aims to characterize 5210 event triggers and participants of individual atomic events (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015; Sha et al., 2016; Yuan et al., 2018), ignoring inter-event relations. Work on script learning, on the other hand, originally limited attention to event chains with a single protagonist (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015; Jans et al., 2012; Granroth-Wilding and Clark, 2016) and later extended to multiple participants (Pichotta and Mooney, 2014, 2016; Weber et al., 2018). Recent efforts rely on distributed representations encoded from the compositional nature of events (Modi, 2016; Granroth-Wilding and Clark, 2016; Weber et al., 2018, 2020; Zhang et al., 2020), and language modeling (Rudi"
2021.emnlp-main.422,2021.naacl-main.6,1,0.815069,"Missing"
2021.emnlp-main.422,2020.emnlp-main.374,0,0.0992327,"Missing"
2021.emnlp-main.424,Q16-1026,0,0.0447106,", “coupling reactions”). In the chemistry doneeds to study dozens to hundreds of dismain, previous NER studies are mostly focused tinct, fine-grained entity types, making conon one coarse-grained entity type (i.e., chemicals) sistent and accurate annotation difficult even for crowds of domain experts. On the other (Krallinger et al., 2015; He et al., 2020; Watanabe hand, domain-specific ontologies and knowlet al., 2019) and rely on large amounts of manuallyedge bases (KBs) can be easily accessed, conannotated data for training deep learning models structed, or integrated, which makes distant (Chiu and Nichols, 2016; Ma and Hovy, 2016; Lamsupervision realistic for fine-grained chemple et al., 2016; Wang et al., 2019b; Devlin et al., istry NER. In distant supervision, training la2019; Liu et al., 2019). bels are generated by matching mentions in a document with the concepts in the knowlIn real-world applications, it is important to edge bases (KBs). However, this kind of recognize chemistry entities on diverse and fineKB-matching suffers from two major chalgrained types (e.g., “inorganic phophorus comlenges: incomplete annotation and noisy annopounds”, “coupling reactions” and “catalysts”) to tation. We p"
2021.emnlp-main.424,P18-1009,0,0.0354785,"Missing"
2021.emnlp-main.424,D15-1103,0,0.0259435,"idate entity in the text to a concept identifier Corpus Collection. For this study, we collected a in the knowledge bases. However, entity linking chemistry literature corpus from PubChem2 . This cannot deal with new entities that do not exist in corpus contains 4,608 papers, among which 319 the background knowledge bases. Another similar papers have the full-text and all have the title and task is fine-grained entity typing (FET) (Hoffart et al., 2011; Yosef et al., 2012; Ling and Weld, abstract. There are 71,406 sentences in this corpus. Type Ontology and Dictionary Collection. We 2012; Del Corro et al., 2015; Ren et al., 2015; Choi collected a fine-grained chemistry type ontology et al., 2018) that has been extensively studied in from Wikipedia categories rooted under the Chemthe general domain. FET aims at classifying an istry category3 . We treat the Wikipedia category entity mention into a wide range of entity types by pages as types and the titles of the pages associated disambiguating the pre-identified entity mentions into a set of candidate entity types. It is formulated 2 https://pubchem.ncbi.nlm.nih.gov/ 3 as a multi-class, multi-label classification problem https://en.wikipedia.org/wiki"
2021.emnlp-main.424,N19-1423,0,0.0187581,"Missing"
2021.emnlp-main.424,N16-1150,0,0.0341458,"Missing"
2021.emnlp-main.424,D17-1284,0,0.0528205,"Missing"
2021.emnlp-main.424,D11-1072,0,0.0373632,"., 2016; Gupta et al., 2017; Raiman and chemistry type ontology and associated entity dicRaiman, 2018; Le and Titov, 2018) that maps a tionaries for each type. candidate entity in the text to a concept identifier Corpus Collection. For this study, we collected a in the knowledge bases. However, entity linking chemistry literature corpus from PubChem2 . This cannot deal with new entities that do not exist in corpus contains 4,608 papers, among which 319 the background knowledge bases. Another similar papers have the full-text and all have the title and task is fine-grained entity typing (FET) (Hoffart et al., 2011; Yosef et al., 2012; Ling and Weld, abstract. There are 71,406 sentences in this corpus. Type Ontology and Dictionary Collection. We 2012; Del Corro et al., 2015; Ren et al., 2015; Choi collected a fine-grained chemistry type ontology et al., 2018) that has been extensively studied in from Wikipedia categories rooted under the Chemthe general domain. FET aims at classifying an istry category3 . We treat the Wikipedia category entity mention into a wide range of entity types by pages as types and the titles of the pages associated disambiguating the pre-identified entity mentions into a set of"
2021.emnlp-main.424,N16-1030,0,0.0499501,"Missing"
2021.emnlp-main.424,P18-1148,0,0.016074,"C HEM NER, an ontology-guided distantly-supervised NER method for fine-grained chemistry NER (Figure 2). It includes distant label generation (entity span detection, flexible KBmatching, and ontology-guided multi-type disambiguation) and sequence labeling model training. 3.1 Data Preparation Other Related Tasks. One similar task to fineThe input to C HEM NER includes two parts: (1) a grained NER is entity linking (Francis-Landau chemistry literature corpus, and (2) a fine-grained et al., 2016; Gupta et al., 2017; Raiman and chemistry type ontology and associated entity dicRaiman, 2018; Le and Titov, 2018) that maps a tionaries for each type. candidate entity in the text to a concept identifier Corpus Collection. For this study, we collected a in the knowledge bases. However, entity linking chemistry literature corpus from PubChem2 . This cannot deal with new entities that do not exist in corpus contains 4,608 papers, among which 319 the background knowledge bases. Another similar papers have the full-text and all have the title and task is fine-grained entity typing (FET) (Hoffart et al., 2011; Yosef et al., 2012; Ling and Weld, abstract. There are 71,406 sentences in this corpus. Type Ontolog"
2021.emnlp-main.424,2021.ccl-1.108,0,0.0530935,"Missing"
2021.emnlp-main.424,P16-1101,0,0.126946,"In the chemistry doneeds to study dozens to hundreds of dismain, previous NER studies are mostly focused tinct, fine-grained entity types, making conon one coarse-grained entity type (i.e., chemicals) sistent and accurate annotation difficult even for crowds of domain experts. On the other (Krallinger et al., 2015; He et al., 2020; Watanabe hand, domain-specific ontologies and knowlet al., 2019) and rely on large amounts of manuallyedge bases (KBs) can be easily accessed, conannotated data for training deep learning models structed, or integrated, which makes distant (Chiu and Nichols, 2016; Ma and Hovy, 2016; Lamsupervision realistic for fine-grained chemple et al., 2016; Wang et al., 2019b; Devlin et al., istry NER. In distant supervision, training la2019; Liu et al., 2019). bels are generated by matching mentions in a document with the concepts in the knowlIn real-world applications, it is important to edge bases (KBs). However, this kind of recognize chemistry entities on diverse and fineKB-matching suffers from two major chalgrained types (e.g., “inorganic phophorus comlenges: incomplete annotation and noisy annopounds”, “coupling reactions” and “catalysts”) to tation. We propose C HEM NER, a"
2021.emnlp-main.424,P19-1231,0,0.0298842,"Missing"
2021.emnlp-main.424,D18-1230,1,0.896597,"Missing"
2021.emnlp-main.424,C12-2133,0,0.0723148,"Missing"
2021.emnlp-main.424,H05-1059,0,0.083012,"each type. We further remove irrelevant types and merge some fine-grained types to their coarse-grained parent types based on their term frequencies in the corpus. We also expand the entity dictionaries with synonyms collected from the PubChem knowledge base. Finally, we obtained a fine-grained chemistry entity type ontology with 62 types and its associated dictionaries with 10,551 entities. Figure 3 shows a subset of our chemistry type ontology. The complete fine-grained chemistry type ontology with 62 types can be found in Appendix A.1. 3.2 Flexible KB-Matching Cole, 2016) and Genia Tagger (Tsuruoka and Tsujii, 2005), to generate candidate entity spans in the input corpus (e.g., in Figure 2 sentence S2, the phrase chunking tools find “Suzuki-Miyaura crosscoupling reactions” as a candidate entity span.) Based on the detected candidate entity spans, we develop a flexible KB-matching method with TFIDF-based majority voting to resolve the incomplete annotation problem. The flexible KB-matching method can match long and complex chemistry entities (e.g., chemical compounds) that do not exist in the KBs. Specifically, we label each candidate entity span by letting each word token in the entity span vote for seve"
2021.emnlp-main.424,D19-1648,0,0.0309058,"Missing"
2021.emnlp-main.441,N16-1049,0,0.0251969,"ies on event schema induction adopt rule-based approaches (Lehnert et al., 1992; Chinchor et al., 1993) and classification-based methods (Chieu et al., 2003; Bunescu and Mooney, 2004) to induce templates from labeled corpus. Later, unsupervised methods are proposed to leverage relation patterns (Sekine, 2006; Qiu et al., 2008) and coreference chains (Chambers and Jurafsky, 2011) for event schema induction. Typical approaches use probabilistic generative models (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015; Li et al., 2020, 2021) or ad-hoc clustering algorithms (Huang et al., 2016; Sha et al., 2016) to induce predicate and argument clusters. In particular, (Liu et al., 2019) takes an entity-centric view toward event schema induction. It clusters entities into semantic slots and finds predicates for entity clusters in a post-processing step. (Yuan et al., 2018) studies the event profiling task and includes one module that leverages a Bayesian generative model to cluster hpredicate:role:labeli triplets into event types. These methods typically rely on discrete hand-crafted features derived from bag-of-word text representations and impose strong statistics assumptions; whereas our method us"
2021.emnlp-main.441,P19-1276,0,0.0220825,"92; Chinchor et al., 1993) and classification-based methods (Chieu et al., 2003; Bunescu and Mooney, 2004) to induce templates from labeled corpus. Later, unsupervised methods are proposed to leverage relation patterns (Sekine, 2006; Qiu et al., 2008) and coreference chains (Chambers and Jurafsky, 2011) for event schema induction. Typical approaches use probabilistic generative models (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015; Li et al., 2020, 2021) or ad-hoc clustering algorithms (Huang et al., 2016; Sha et al., 2016) to induce predicate and argument clusters. In particular, (Liu et al., 2019) takes an entity-centric view toward event schema induction. It clusters entities into semantic slots and finds predicates for entity clusters in a post-processing step. (Yuan et al., 2018) studies the event profiling task and includes one module that leverages a Bayesian generative model to cluster hpredicate:role:labeli triplets into event types. These methods typically rely on discrete hand-crafted features derived from bag-of-word text representations and impose strong statistics assumptions; whereas our method uses pre-trained language models to reduce the feature generation complexity an"
2021.emnlp-main.441,2020.emnlp-main.666,1,0.802433,"Missing"
2021.emnlp-main.441,E17-1010,0,0.0251289,"Missing"
2021.emnlp-main.441,P15-1019,0,0.0197179,"“immunize” and “vaccinate” for Vaccinate type). 6 Related Work Event Schema Induction. Early studies on event schema induction adopt rule-based approaches (Lehnert et al., 1992; Chinchor et al., 1993) and classification-based methods (Chieu et al., 2003; Bunescu and Mooney, 2004) to induce templates from labeled corpus. Later, unsupervised methods are proposed to leverage relation patterns (Sekine, 2006; Qiu et al., 2008) and coreference chains (Chambers and Jurafsky, 2011) for event schema induction. Typical approaches use probabilistic generative models (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015; Li et al., 2020, 2021) or ad-hoc clustering algorithms (Huang et al., 2016; Sha et al., 2016) to induce predicate and argument clusters. In particular, (Liu et al., 2019) takes an entity-centric view toward event schema induction. It clusters entities into semantic slots and finds predicates for entity clusters in a post-processing step. (Yuan et al., 2018) studies the event profiling task and includes one module that leverages a Bayesian generative model to cluster hpredicate:role:labeli triplets into event types. These methods typically rely on discrete hand-crafted features derived from b"
2021.emnlp-main.441,W16-5706,0,0.0431728,"Missing"
2021.emnlp-main.441,J05-1004,0,0.245902,"et al., 2021) utilize bag-of-word text representations and imassume a set of predefined event types and their pose strong statistical assumptions. Huang et al. corresponding annotations are curated by human (2016) relax those restrictions using a pipelined experts. This annotation process is expensive and approach that leverages extensive lexical and setime-consuming. Besides, those manually-defined mantic resources (e.g., FrameNet (Baker et al., event types often fail to generalize to new do1998), VerbNet (Schuler and Palmer, 2005), and mains. For example, the widely used ACE 2005 PropBank (Palmer et al., 2005)) to discover event event schemas2 do not contain any event type schemas. While being effective, this method is 1 The programs, data and resources are publicly availlimited by the scope of external resources and acable for research purpose at https://github.com/ curacies of its preprocessing tools. Recently, some mickeystroller/ETypeClus. 2 studies (Huang et al., 2018; Lai and Nguyen, 2019; https://www.ldc.upenn.edu/ collaborations/past-projects/ace Huang and Ji, 2020) have used transfer learning to 5427 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pa"
2021.emnlp-main.441,W15-0812,0,0.0241755,", ET YPE C LUS clusters the remaining salient P-O pairs into event types using a latent space generative model. This model jointly embeds P-O pairs into a latent spherical space and performs clustering within this space. By doing so, we can guide the latent space learning with the clustering objective and enable the clustering process to benefit from the well-separated structure of the latent space. We show our ET YPE C LUS framework can save annotation cost and output corpus-specific event types on three datasets. The first two are benchmark datasets ACE 2005 and ERE (Entity Relation Event) (Song et al., 2015). ET YPE C LUS can successfully recover predefined types and identify new event types such as Build in ACE and Bombing in ERE. Furthermore, to test the performance of ET YPE C LUS in new domains, we collect a corpus about the disease outbreak scenario. Results show that ET YPE C LUS can identify many interesting fine-grained event types (e.g., Vaccinate, Test) that align well with human annotations. Contributions. The major contributions of this paper are summarized as follows: (1) A new event type representation is created as a cluster of hpredicate sense, object headi tuples; (2) a novel eve"
2021.emnlp-main.441,P18-2010,0,0.131353,"he men’s suicide bomber training in 2011 in Somalia and association with Beledi, who prosecutors said bombed a government checkpoint in Mogadishu that year. Table 3: Example outputs of ET YPE C LUS discovered event types with their associated sentences in ACE and ERE datasets. The first two types come from ACE and the remaining two are from ERE. The event types with superscript “∇ ” originally do not exist in human-labeled schemas and are discovered by ET YPE C LUS framework. Predicates are in bold and object heads are underlined and in italics. ACE Methods Kmeans sp-Kmeans AggClus Triframes (Ustalov et al., 2018) JCSC (Huang et al., 2016) ET YPE C LUS ERE ARI (std) NMI (std) ACC (std) BCubed-F1 (std) ARI (std) NMI (std) ACC (std) BCubed-F1 (std) 26.27 (1.60) 26.06 (2.12) 24.45 (0.00) 19.35 (6.60) 36.10 (4.96) 40.78 (3.20) 48.02 (1.55) 47.30 (1.65) 45.71 (0.00) 36.38 (4.91) 49.50 (2.70) 57.57 (2.40) 41.57 (3.07) 40.41 (2.46) 41.00 (0.00) — 46.17 (3.64) 48.35 (2.55) 41.33 (1.75) 39.52 (1.42) 40.20 (0.00) 38.91 (2.36) 43.83 (3.17) 51.58 (2.50) 11.17 (1.83) 13.62 (2.14) 6.07 (0.00) 10.89 (2.51) 17.07 (4.40) 24.09 (1.93) 35.10 (2.36) 37.33 (2.25) 29.62 (0.00) 34.94 (2.54) 39.50 (3.97) 49.40 (1.37) 31.65 (1"
2021.emnlp-main.441,D19-1027,0,0.0259244,"w keywords. These methods reduce the annotation efforts but still require all target new types to be given. Recently, some studies (Huang 15 More example outputs are in Appendix Section H. et al., 2018; Lai and Nguyen, 2019; Huang and Ji, 2020) use transfer learning techniques to extend traditional event extraction models to new types without explicitly deriving schemas of new event types. Compared to our study, these methods still require many annotations for a set of seen types and their resulting vector-based event type representations are less human interpretable. Another related work by (Wang et al., 2019) uses GAN to extract events from an open domain corpus. It clusters hentity:location:keyword:datei quadruples related to the same event rather than finds event types. 7 Conclusions and Future Work In this paper, we study the event type induction problem that aims to automatically generate salient event types for a given corpus. We define a novel event type representation as a hpredicate sense, object headi cluster, and propose ET YPE C LUS that can extract and select salient predicates and object heads, disambiguate predicate senses, and jointly embed and cluster P-O pairs in a latent space. E"
2021.emnlp-main.441,2020.emnlp-demos.6,0,0.0185926,"Missing"
2021.emnlp-main.810,W09-3302,0,0.0491436,"1 to M do 8 θ k ← Train with Eq. (5); 9 {wi ← 1 (fi,yi (x; θ k ) &gt; τ )}ni=1 ; 10 θ ENS ← Train with Eq. (6); 11 // Augmentation; 0 12 {x } ← Eq. (7); 13 // Self-training; (0) 14 θ ← θ ENS ; 15 // Train for T iterations; 16 for t ← 0 to T − 1 do 17 y (t+1) ← Eq. (8); 18 θ (t+1) ← Train with Eq. (9); (T ) 19 Return θ = θ ; Dataset # Types # Train # Test CoNLL03 OntoNotes5.0 Wikigold 4 18 4 14,041 59,924 1,142 3,453 8,262 274 Table 1: Dataset statistics with the number of entity types and the number of training/test sequences. follow the pre-processing of (Chiu and Nichols, 2016), and Wikigold (Balasuriya et al., 2009). The dataset statistics are shown in Table 1. All datasets are in English language. 3.2 Compared Methods We compare with a wide range of state-of-the-art distantly-supervised methods and supervised methods. Fully supervised methods use the ground truth training set for model training. Distantlysupervised methods use the distantly-labeled training set obtained as in (Liang et al., 2020). All methods are evaluated on the test set. the distant supervision quality (i.e., compares distantly-labeled results with the ground truth). • Distant RoBERTa: We fine-tune a pre-trained RoBERTa model on dista"
2021.emnlp-main.810,L18-1707,0,0.0232011,", location and organization names) from texts, can be matched to multiple types in the knowledge is a fundamental task in natural language process- bases—such ambiguity cannot be resolved by the ing with a wide range of applications, including context-free matching process. Figure 1 shows that question answering (Khalid et al., 2008), knowl- some “person” mentions may be partially labeled edge base construction (Etzioni et al., 2005), text (or not labeled at all in other cases), and some summarization (Aramaki et al., 2009) and dialog entities with multiple possible types may be mislasystems (Bowden et al., 2018). In recent years, beled. deep neural models (Devlin et al., 2019; Huang Due to the existence of such noise, straightforet al., 2015; Lample et al., 2016; Ma and Hovy, ward application of supervised learning to distantly2016) have achieved enormous success for NER, labeled data will yield deteriorated performance, thanks to their strong representation learning power because neural models have the strong capacity to that accurately captures the entity semantics in tex- fit to the given (noisy) data. Some previous studies tual contexts. However, a common bottleneck of on distantly-supervised NER"
2021.emnlp-main.810,2020.acl-main.194,0,0.0210362,"truth), or require an additional set of manually-labeled data to train a denoising model. Our method addresses the label noise with a noise-robust learning scheme and a self-training step for better generalization, without using any ground truth data. Our study is also related to data augmentation techniques. In NLP, data augmentation is well developed for text classification, by either creating real text sequences (Xie et al., 2020) via back translation (Sennrich et al., 2016) or in the hidden states of the model via perturbations (Miyato et al., 2017) or interpolations by mixing up labels (Chen et al., 2020). However, these techniques cannot be readily used for the NER task. (Dai and Adel, 2020) study a set of simple augmentation methods for the NER task, like synonym replacement, mention replacement or segment shuffling. Nevertheless, these augmentations are context-free which may generate unreasonable sequences or require additional sources like the WordNet. Our proposed augmentation method is unsupervised and contextualized, generating high-quality sequences thanks to the pre-trained knowledge of PLMs and reliably improving model generalization. 5 Conclusion and Future Work function and a nois"
2021.emnlp-main.810,Q16-1026,0,0.033396,"6 // Train for M iterations; 7 for m ← 1 to M do 8 θ k ← Train with Eq. (5); 9 {wi ← 1 (fi,yi (x; θ k ) &gt; τ )}ni=1 ; 10 θ ENS ← Train with Eq. (6); 11 // Augmentation; 0 12 {x } ← Eq. (7); 13 // Self-training; (0) 14 θ ← θ ENS ; 15 // Train for T iterations; 16 for t ← 0 to T − 1 do 17 y (t+1) ← Eq. (8); 18 θ (t+1) ← Train with Eq. (9); (T ) 19 Return θ = θ ; Dataset # Types # Train # Test CoNLL03 OntoNotes5.0 Wikigold 4 18 4 14,041 59,924 1,142 3,453 8,262 274 Table 1: Dataset statistics with the number of entity types and the number of training/test sequences. follow the pre-processing of (Chiu and Nichols, 2016), and Wikigold (Balasuriya et al., 2009). The dataset statistics are shown in Table 1. All datasets are in English language. 3.2 Compared Methods We compare with a wide range of state-of-the-art distantly-supervised methods and supervised methods. Fully supervised methods use the ground truth training set for model training. Distantlysupervised methods use the distantly-labeled training set obtained as in (Liang et al., 2020). All methods are evaluated on the test set. the distant supervision quality (i.e., compares distantly-labeled results with the ground truth). • Distant RoBERTa: We fine-t"
2021.emnlp-main.810,2020.coling-main.343,0,0.228163,"original ones. To further enforce the labelpreserving constraint of the augmented sequence, we (1) sample x0i only from the top-5 terms given by pMLM (ˆ x; θ PRE ) to avoid low-quality replacements, i and (2) require x0i to have the same capitalization and tokenization with xi (i.e., if xi is capitalized or is a subword, so should x0i ). Using PLMs to perform augmentation for NER has the major benefit of being unsupervised and contextualized. Without PLMs, one may still perform augmentation by replacing an entity in the sequence with another of the same type in the distant supervision source (Dai and Adel, 2020). However, such an approach requires prior knowledge about the entity type in the sequence (i.e., it does not work for non-entities or entities not matched with distant labels), and the augmentation is context-free, which may create low-quality and invalid sequences (e.g., it does not fit the context to replace a technology company with a news agency although they both belong to the “organization” entity type). Contextualized Augmentations with PLMs. Many PLMs (Devlin et al., 2019; Lan et al., 2020; Liu et al., 2019) are pre-trained with the masked language modeling (MLM) task on large-scale t"
2021.emnlp-main.810,N19-1423,0,0.195265,"ultiple types in the knowledge is a fundamental task in natural language process- bases—such ambiguity cannot be resolved by the ing with a wide range of applications, including context-free matching process. Figure 1 shows that question answering (Khalid et al., 2008), knowl- some “person” mentions may be partially labeled edge base construction (Etzioni et al., 2005), text (or not labeled at all in other cases), and some summarization (Aramaki et al., 2009) and dialog entities with multiple possible types may be mislasystems (Bowden et al., 2018). In recent years, beled. deep neural models (Devlin et al., 2019; Huang Due to the existence of such noise, straightforet al., 2015; Lample et al., 2016; Ma and Hovy, ward application of supervised learning to distantly2016) have achieved enormous success for NER, labeled data will yield deteriorated performance, thanks to their strong representation learning power because neural models have the strong capacity to that accurately captures the entity semantics in tex- fit to the given (noisy) data. Some previous studies tual contexts. However, a common bottleneck of on distantly-supervised NER directly treat distant applying deep learning models is the acqu"
2021.emnlp-main.810,2021.emnlp-main.813,1,0.652733,"owledge transfer from high rethe second term in Eq. (9)) on the Wikigold dataset source languages to low resource languages (Feng and show the results in Figure 3(c). Even with- et al., 2018; Ni et al., 2017; Xie et al., 2018), agout augmentations, the self-training improves the gregating multiple weak labeling functions (Lison model by using high-confidence predictions for et al., 2020; Safranchik et al., 2020) or leveragself-refinement; with augmentations, the model is ing sentence-level labels (Kruengkrai et al., 2020). trained with more data and eventually generalizes Few-shot approaches (Huang et al., 2021) have also better with higher test set performance. Two con- been explored to leverage very few labeled data for crete augmentation examples are shown in Table 5. NER model training. Our work is more closely related to distantly3.8 Case Study supervised NER which uses external gazetteers Finally, we perform case study to understand the or knowledge bases to automatically derive enadvantage of RoSTER with a concrete example tity labels. Along this line, different methods in Table 6. We show the prediction of AutoNER, have been proposed to leverage the distant superBOND and RoSTER on a training"
2021.emnlp-main.810,2020.tacl-1.28,0,0.0232953,"the sequence (i.e., it does not work for non-entities or entities not matched with distant labels), and the augmentation is context-free, which may create low-quality and invalid sequences (e.g., it does not fit the context to replace a technology company with a news agency although they both belong to the “organization” entity type). Contextualized Augmentations with PLMs. Many PLMs (Devlin et al., 2019; Lan et al., 2020; Liu et al., 2019) are pre-trained with the masked language modeling (MLM) task on large-scale text corpora carrying general knowledge like the Wikipedia. Previous studies (Jiang et al., 2020; Petroni et al., 2019) have shown that entity-related knowledge can be extracted from a PLM (without any fine-tuning) by querying it via cloze templates Self-Training. The goals of self-training (ST) and gathering the PLM’s MLM outputs. are two-fold: (1) use the model’s high-confidence 10370 ··· O 0 <latexit sha1_base64=&quot;8G0nGLSTs3/pcwh69xFGY55W9fQ=&quot;&gt;AAAB+HicbVC7TsMwFL0pr1IeDTCyWFQIpipBRTBWYmEsEn1IbVQ5jtNadZzIdhAl6pewMIAQK5/Cxt/gtBmg5UiWj865Vz4+fsKZ0o7zbZXW1jc2t8rblZ3dvf2qfXDYUXEqCW2TmMey52NFORO0rZnmtJdIiiOf064/ucn97gOVisXiXk8T6kV4JFjICNZGGtrVgR/zQE0jc2WPs7OhXXPqzhxolbgFqUGB1tD+GgQxSSMqNOFYqb"
2021.emnlp-main.810,2020.acl-main.523,0,0.0248361,"evious studies have explored ing the augmentations (i.e., including or excluding cross lingual knowledge transfer from high rethe second term in Eq. (9)) on the Wikigold dataset source languages to low resource languages (Feng and show the results in Figure 3(c). Even with- et al., 2018; Ni et al., 2017; Xie et al., 2018), agout augmentations, the self-training improves the gregating multiple weak labeling functions (Lison model by using high-confidence predictions for et al., 2020; Safranchik et al., 2020) or leveragself-refinement; with augmentations, the model is ing sentence-level labels (Kruengkrai et al., 2020). trained with more data and eventually generalizes Few-shot approaches (Huang et al., 2021) have also better with higher test set performance. Two con- been explored to leverage very few labeled data for crete augmentation examples are shown in Table 5. NER model training. Our work is more closely related to distantly3.8 Case Study supervised NER which uses external gazetteers Finally, we perform case study to understand the or knowledge bases to automatically derive enadvantage of RoSTER with a concrete example tity labels. Along this line, different methods in Table 6. We show the predictio"
2021.emnlp-main.810,N16-1030,0,0.0456277,"such ambiguity cannot be resolved by the ing with a wide range of applications, including context-free matching process. Figure 1 shows that question answering (Khalid et al., 2008), knowl- some “person” mentions may be partially labeled edge base construction (Etzioni et al., 2005), text (or not labeled at all in other cases), and some summarization (Aramaki et al., 2009) and dialog entities with multiple possible types may be mislasystems (Bowden et al., 2018). In recent years, beled. deep neural models (Devlin et al., 2019; Huang Due to the existence of such noise, straightforet al., 2015; Lample et al., 2016; Ma and Hovy, ward application of supervised learning to distantly2016) have achieved enormous success for NER, labeled data will yield deteriorated performance, thanks to their strong representation learning power because neural models have the strong capacity to that accurately captures the entity semantics in tex- fit to the given (noisy) data. Some previous studies tual contexts. However, a common bottleneck of on distantly-supervised NER directly treat distant applying deep learning models is the acquisition labels as if ground truth for model training and 1 rely on simple tricks such as"
2021.emnlp-main.810,2020.acl-main.139,0,0.0435576,"Missing"
2021.emnlp-main.810,2021.ccl-1.108,0,0.0539633,"Missing"
2021.emnlp-main.810,P16-1101,0,0.293987,"the created augmentations improve the model’s generalization ability. • On three benchmark datasets, RoSTER outperforms existing distantly-supervised NER approaches by significant margins. 2 Method In this section, we (1) briefly describe how to obtain distantly-labeled data, (2) introduce our noiserobust learning scheme and (3) propose a selftraining method with a new contextualized augmentation generation technique. We assume the pre-trained RoBERTa (Liu et al., 2019) model is used as our backbone model, but our proposed methods can be integrated with other architectures (e.g., LSTM-based (Ma and Hovy, 2016)) as well. 2020) for this step: (1) potential entities are determined via POS tagging and hand-crafted rules, (2) their types are acquired by querying Wikidata using SPARQL (Vrandeˇci´c and Krötzsch, 2014), and (3) additional gazetteers from multiple online resources are used for matching more entities in the corpus. 2.2 Noise-Robust Learning We first overview the common setup for training NER models, and then propose two designs that work jointly for distantly-supervised NER, motivated by the challenges of learning with noisy labels: (1) a new loss function, and (2) noisy label removal. Final"
2021.emnlp-main.810,2020.emnlp-main.724,1,0.862818,"Missing"
2021.emnlp-main.810,P17-1135,0,0.0243666,"man annotation burden when applying deep modWe study the effectiveness of the generated con- els, several studies propose to train NER models textualized augmentations for the self-training step. with weakly/distantly-labeled data. For weaklyWe run the self-training step with and without us- supervised NER, previous studies have explored ing the augmentations (i.e., including or excluding cross lingual knowledge transfer from high rethe second term in Eq. (9)) on the Wikigold dataset source languages to low resource languages (Feng and show the results in Figure 3(c). Even with- et al., 2018; Ni et al., 2017; Xie et al., 2018), agout augmentations, the self-training improves the gregating multiple weak labeling functions (Lison model by using high-confidence predictions for et al., 2020; Safranchik et al., 2020) or leveragself-refinement; with augmentations, the model is ing sentence-level labels (Kruengkrai et al., 2020). trained with more data and eventually generalizes Few-shot approaches (Huang et al., 2021) have also better with higher test set performance. Two con- been explored to leverage very few labeled data for crete augmentation examples are shown in Table 5. NER model training. Our w"
2021.emnlp-main.810,N19-1250,0,0.0222651,"Missing"
2021.emnlp-main.810,P19-1231,0,0.019775,"Automobile Corporation]ORG and [Ek Chor China Motorcycle]ORG . AutoNER: Shanghai-Ek [Chor]PER is jointly owned by the Shanghai Automobile Corporation and [Ek Chor]PER [China]LOC Motorcycle. BOND: [Shanghai-Ek Chor]PER is jointly owned by the [Shanghai]LOC [Automobile Corporation]ORG and [Ek Chor]PER [China Motorcycle]ORG . RoSTER: [Shanghai-Ek Chor]ORG is jointly owned by the [Shanghai Automobile Corporation]ORG and [Ek Chor China Motorcycle]ORG . Table 6: Case study with RoSTER and baselines. The sentence is from CoNLL03. 2019), formulating the task as a positive-unlabeled learning problem (Peng et al., 2019), and adopting early stopping to prevent the model from overfitting to distant labels (Liang et al., 2020). However, previous methods either do not explicitly address the noise in the distantly-labeled data (i.e., treating them as if they are ground truth), or require an additional set of manually-labeled data to train a denoising model. Our method addresses the label noise with a noise-robust learning scheme and a self-training step for better generalization, without using any ground truth data. Our study is also related to data augmentation techniques. In NLP, data augmentation is well devel"
2021.emnlp-main.810,D19-1250,0,0.149452,"on have been exploited via noise-robust learning, but some tokens may have not been fully leveraged by the model since they are excluded by the noisy label removal step. The self-training step aims to bootstrap on all tokens using the model’s own predictions to improve its generalization ability. Similar selftraining ideas have been explored in classification tasks (Meng et al., 2018, 2019, 2020). (2) The pretrained language model (PLM) has only been used to initialize the NER model for fine-tuning, while PLMs (without fine-tuning) encode factual and relational knowledge through pre-training (Petroni et al., 2019) that may complement the NER model training. The self-training step thus also brings additional pre-trained knowledge for better model generalization by creating contextualized augmentations using a PLM. Figure 2 shows an overview. Given that the MLM task shares high similarity with the NER task (i.e., both leverage the contextual information within the sequence for token-level classification) and that the MLM outputs contain general knowledge acquired during pre-training, we propose to use the pre-trained RoBERTa model (without fine-tuning) θ PRE for creating label-preserving augmentations (i"
2021.emnlp-main.810,P16-1009,0,0.0395807,"wever, previous methods either do not explicitly address the noise in the distantly-labeled data (i.e., treating them as if they are ground truth), or require an additional set of manually-labeled data to train a denoising model. Our method addresses the label noise with a noise-robust learning scheme and a self-training step for better generalization, without using any ground truth data. Our study is also related to data augmentation techniques. In NLP, data augmentation is well developed for text classification, by either creating real text sequences (Xie et al., 2020) via back translation (Sennrich et al., 2016) or in the hidden states of the model via perturbations (Miyato et al., 2017) or interpolations by mixing up labels (Chen et al., 2020). However, these techniques cannot be readily used for the NER task. (Dai and Adel, 2020) study a set of simple augmentation methods for the NER task, like synonym replacement, mention replacement or segment shuffling. Nevertheless, these augmentations are context-free which may generate unreasonable sequences or require additional sources like the WordNet. Our proposed augmentation method is unsupervised and contextualized, generating high-quality sequences th"
2021.emnlp-main.810,D18-1034,0,0.0188475,"rden when applying deep modWe study the effectiveness of the generated con- els, several studies propose to train NER models textualized augmentations for the self-training step. with weakly/distantly-labeled data. For weaklyWe run the self-training step with and without us- supervised NER, previous studies have explored ing the augmentations (i.e., including or excluding cross lingual knowledge transfer from high rethe second term in Eq. (9)) on the Wikigold dataset source languages to low resource languages (Feng and show the results in Figure 3(c). Even with- et al., 2018; Ni et al., 2017; Xie et al., 2018), agout augmentations, the self-training improves the gregating multiple weak labeling functions (Lison model by using high-confidence predictions for et al., 2020; Safranchik et al., 2020) or leveragself-refinement; with augmentations, the model is ing sentence-level labels (Kruengkrai et al., 2020). trained with more data and eventually generalizes Few-shot approaches (Huang et al., 2021) have also better with higher test set performance. Two con- been explored to leverage very few labeled data for crete augmentation examples are shown in Table 5. NER model training. Our work is more closely"
2021.emnlp-main.813,D18-1398,0,0.0285564,"ty-aware self-attention. 10414 These methods are designed for standard supervised learning, and have a limited generalization ability in few-shot settings, as empirically shown in (Fritzler et al., 2019). Prototype-based methods recently become popular few-shot learning approaches in machine learning community. It was firstly studied in the context of image classification (Vinyals et al., 2016; Sung et al., 2018; Zhao et al., 2020), and has recently been adapted to different NLP tasks such as text classification (Wang et al., 2018; Geng et al., 2019; Bansal et al., 2020), machine translation (Gu et al., 2018) and relation classification (Han et al., 2018). The work closest related to ours is (Fritzler et al., 2019) which explores prototypical network on fewshot NER, but only utilizes RNNs as the backbone model and does not leverage the power of largescale Transformer-based architectures for word representations. Our work is similar to (Ziyadi et al., 2020; Wiseman and Stratos, 2019) in that all of them utilize the nearest neighbor criterion to assign the entity type, but differs in that (Ziyadi et al., 2020; Wiseman and Stratos, 2019) consider every individual token instance for nearest neighbor c"
2021.emnlp-main.813,D18-1514,0,0.0225138,"re designed for standard supervised learning, and have a limited generalization ability in few-shot settings, as empirically shown in (Fritzler et al., 2019). Prototype-based methods recently become popular few-shot learning approaches in machine learning community. It was firstly studied in the context of image classification (Vinyals et al., 2016; Sung et al., 2018; Zhao et al., 2020), and has recently been adapted to different NLP tasks such as text classification (Wang et al., 2018; Geng et al., 2019; Bansal et al., 2020), machine translation (Gu et al., 2018) and relation classification (Han et al., 2018). The work closest related to ours is (Fritzler et al., 2019) which explores prototypical network on fewshot NER, but only utilizes RNNs as the backbone model and does not leverage the power of largescale Transformer-based architectures for word representations. Our work is similar to (Ziyadi et al., 2020; Wiseman and Stratos, 2019) in that all of them utilize the nearest neighbor criterion to assign the entity type, but differs in that (Ziyadi et al., 2020; Wiseman and Stratos, 2019) consider every individual token instance for nearest neighbor comparison, while ours considers prototypes for"
2021.emnlp-main.813,P19-1524,0,0.294934,"both few-shot and training-free settings compared with existing methods. 1 Prototype Methods Linear Classifier Fine-tuning 2 Noisy Supervised Pretraining 3 Self-Training Figure 1: An overview of methods studied in our paper. Linear classifier fine-tuning is a default baseline that updates an NER model from pre-trained Roberta/BERT. We study three orthogonal strategies to improve NER models in the limited labeled data settings. has shown remarkable success in NER in recent years, especially with self-supervised pre-trained language models (PLMs) such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019c). State-of-the-art (SoTA) NER models are often initialized with PLM weights, fine-tuned with standard supervised learning. One classic approach is to add a linear classifier on top of the representations provided by PLMs, and fine-tune the entire model using a cross-entropy objective on domainspecific labels (Devlin et al., 2019). Despite its simplicity, the approach generally results in good performance on benchmarks and serves as a strong baseline in this study. 1 Introduction Unfortunately, even with these PLMs, building Named Entity Recognition (NER) involves pro- NER systems still remai"
2021.emnlp-main.813,2020.findings-emnlp.17,1,0.896572,"s persons, organizations, loca- to achieve a reasonable accuracy. However, this is tions, medical codes, dates and quantities. NER in contrast to the real-world application scenarios, serves as an important first component for tasks where only very small amounts of labeled data are such as information extraction (Ritter et al., 2012), available for new domains, such as medical (Hofer information retrieval (Guo et al., 2009), question et al., 2018) domain. The cost of building NER sysanswering (Moll´a et al., 2006), task-oriented dia- tems at scale with rich annotations (i.e., hundreds logues (Peng et al., 2020a; Gao et al., 2019) and of different enterprise use-cases/domains) can be other language understanding applications (Nadeau prohibitively expensive. This draws attentions to and Sekine, 2007; Shaalan, 2014). Deep learning a challenging but practical research problem: fewshot NER. ∗ Work performed during an internship at Microsoft † Corresponding author To deal with the challenge of few-shot learning, 10408 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10408–10423 c November 7–11, 2021. 2021 Association for Computational Linguistics we focus on i"
2021.emnlp-main.813,N18-1202,0,0.0399813,"Missing"
2021.emnlp-main.813,N18-1109,0,0.0253222,"ypes are related but different (e.g., Musician and Artist are more fine-grained types of Person in the downstream task). The associated types on each token can be noisy. (d) Self-training: An NER system (teacher model) trained on a small labeled dataset is used to predict soft labels for sentences in a large unlabeled dataset. The joint of the predicted dataset and original dataset is used to train a student model. 3.1 Prototype-based Methods Meta-learning (Ravi and Larochelle, 2017) has shown promising results for few-shot image classification (Tian et al., 2020) and sentence classification (Yu et al., 2018; Geng et al., 2019). It is natural to adapt this idea to few-shot NER. The core idea is to use episodic classification paradigm to simulate few-shot settings during model training. Specifically in each episode, M entity types (usually M &lt; |Y|) are randomly sampled from DL , containing a ×K support set S = {(Xi , Yi }M (K sentences per i=1 ˆ i, Y ˆ i }M ×K 0 (K 0 type) and a query set Q = {(X i=1 sentences per type). We build our method based on prototypical network (Snell et al., 2017), which introduces the notion of prototypes, representing entity types as vectors in the same representation"
2021.findings-acl.219,2020.emnlp-main.20,0,0.328752,"X Empirically, we find that using the contextualized repR n resentations for the MWS task (i.e., {hMWS D (x )i }|i=1 ) can achieve better fine-tuning performance. 2478 Task-agnostic Representation Task 1 Task 2 Head Head Task 1 Task 2 Head Head Representation for Task 1 Task 1 Task 2 Head Head Task-agnostic Layers Token 1 Token 2 Token 3 Representation for Task 2 Attention-based Task 1 Head to 33 Billion tokens by including data from Gigaword (Parker et al., 2011), ClueWeb (Callan et al., 2009), and CommonCrawl (Crawl, 2019). The same dataset is used in XLNet (Yang et al., 2019) and ELECTRA (Clark et al., 2020a). Attention-based Task 2 Head Task-agnostic Layers Token 1 Token 2 Token 3 Figure 2: Architectures for transforming task-agnostic representations to task-specific representations. (Left) Adding task-specific heads on each token separately. (Right) Using task-specific attention heads capture all token information holistically. cies in sequences. Particularly, we design this attention head to be one transformer layer (i.e., a self-attention block followed by a fully connected network with one hidden layer). Since our discriminator also uses a transformer model to obtain each token’s task-agnos"
2021.findings-acl.219,I05-5002,0,0.133678,"Missing"
2021.findings-acl.219,W07-1401,0,0.111964,"Missing"
2021.findings-acl.219,P19-1441,0,0.310824,"erentiate ground truth tokens from other negative non-original ones. At the same time, the second task, with reduced task complex2475 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2475–2486 August 1–6, 2021. ©2021 Association for Computational Linguistics ity, keeps the discriminator to achieve the same sample efficiency as ELECTRA. To further improve the performance and efficiency of our method, we introduce two techniques. The first one is using attention-based task-specific heads for discriminator multi-task pre-training. Different from previous studies (Liu et al., 2019a; Sun et al., 2020) that pass the last encoder layer outputs to different task heads, our method directly incorporates task-specific attention layers into the discriminator encoder. Such a design offers higher flexibility in capturing task-specific token dependencies in sequence and leads to significant performance boost. The second technique is to share the bottom layers of the generator and the discriminator. This technique reduces the number of parameters, saves computes, and serves as a form of regularization that stabilizes the training and helps the generalization. Combining above novel"
2021.findings-acl.219,2021.ccl-1.108,0,0.0380087,"Missing"
2021.findings-acl.219,D19-1387,0,0.0274217,"Missing"
2021.findings-acl.219,N18-1202,0,0.0443031,"Missing"
2021.findings-acl.219,N18-1101,0,0.0306678,"Missing"
2021.findings-acl.219,D13-1170,0,0.00609564,"Missing"
2021.findings-acl.29,N19-1423,0,0.0291568,"answers, we first take the predictions of the generative reader (G) in Sec. 2.3, which is trained on the passages without reranking and used for final passage reading in R3. It represents an apple-to-apple comparison to R2 without any additional information but higher-quality input. We also experiment with an extractive reader (E) that has access to all retrieved passages, where the goal is to study whether we can rerank passages via other signals and further improve G such that it outperforms both G and E when they are in R2. We use the extractive reader in Mao et al. (2020) with BERT-base (Devlin et al., 2019) representation and span voting. For the generative reader, we either take its top-1 prediction with greedy decoding or sample 10 answers with decoding parameters as follows. We set sampling temperature to 5/2 and the top probability in nucleus sampling to 0.5/0.5 on NQ/Trivia, Quality of Reranking Signals We first analyze the EM of the top-N reader predictions A[:N ] . We consider a question correctly answered as long as one of the top-N predictions matches the ground-truth answer. The standard EM is a special case with N = 1. As listed in Table 3, the reader EM can be improved by up to 24.0"
2021.findings-acl.29,P17-1147,0,0.338917,"the top predicted answers of the reader before reranking. Intuitively, the top predictions of the reader are closely related to the ground-truth answer and even if the predicted answers are partially correct or incorrect, they may still provide useful signals suggesting which passages may contain the correct answer (Mao et al., 2020). We conduct experiments on the Natural Questions (NQ) (Kwiatkowski et al., 2019) and Triv344 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 344–350 August 1–6, 2021. ©2021 Association for Computational Linguistics iaQA (Trivia) (Joshi et al., 2017) datasets. We demonstrate that R3 with R IDER, without any additional training, achieves 10 to 20 absolute gains in top-1 retrieval accuracy, and 1 to 4 gains in Exact Match (EM) compared to the R2 architecture. R IDER also outperforms two state-of-theart transformer-based supervised reranking models that require expensive training and inference. Notably, using only 1,024 tokens (7.8 passages on average) as the input of a generative reader, R IDER achieves EM=47.5/63.5 on NQ/Trivia when the predictions of the same generative reader (EM=45.3/62.2 in R2) are used for reranking, and EM=48.3/66.4"
2021.findings-acl.29,2020.emnlp-main.550,0,0.0956417,"Missing"
2021.findings-acl.29,Q19-1026,0,0.20036,"ropose a simple and effective passage reranking method, named Reader-guIDEd Reranker (R IDER), which does not require any training and reranks the retrieved passages solely based on their lexical overlap with the top predicted answers of the reader before reranking. Intuitively, the top predictions of the reader are closely related to the ground-truth answer and even if the predicted answers are partially correct or incorrect, they may still provide useful signals suggesting which passages may contain the correct answer (Mao et al., 2020). We conduct experiments on the Natural Questions (NQ) (Kwiatkowski et al., 2019) and Triv344 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 344–350 August 1–6, 2021. ©2021 Association for Computational Linguistics iaQA (Trivia) (Joshi et al., 2017) datasets. We demonstrate that R3 with R IDER, without any additional training, achieves 10 to 20 absolute gains in top-1 retrieval accuracy, and 1 to 4 gains in Exact Match (EM) compared to the R2 architecture. R IDER also outperforms two state-of-theart transformer-based supervised reranking models that require expensive training and inference. Notably, using only 1,024 tokens (7.8 passages o"
2021.findings-acl.29,D18-1053,0,0.210592,"rank the most relevant passages at the very top. One line of work (Mao et al., 2020; Karpukhin et al., 2020) aims to improve the retriever and shows that significantly better QA performance can be achieved when the retrieval results are improved. ∗ Work was done during internship at Microsoft Azure AI. Our code is available at https://github.com/ morningmoni/GAR. 1 An alternative solution is to rerank the initial retrieval results via a reranker, which is widely used in information retrieval (Nogueira and Cho, 2019; Qiao et al., 2019) and explored in early OpenQA systems (Wang et al., 2018a; Lee et al., 2018). However, current state-of-the-art OpenQA systems (Karpukhin et al., 2020; Izacard and Grave, 2020b) do not distinguish the order of the retrieved passages and instead equally consider a large number of retrieved passages (e.g., 100), which could be computationally prohibitive as the model size of the readers becomes larger (Izacard and Grave, 2020b). We argue that a Retriever-Reranker-Reader (R3) architecture is beneficial in terms of both model effectiveness and efficiency: passage reranking improves the retrieval accuracy of the retriever at top positions and allows the reader to achieve c"
2021.findings-acl.29,P19-1612,0,0.105441,"Missing"
2021.findings-acl.29,2020.acl-main.703,0,0.0970835,"Missing"
2021.findings-acl.29,2020.acl-tutorials.8,0,0.55013,"atural Questions dataset and 66.4 EM on the TriviaQA dataset when only 1,024 tokens (7.8 passages on average) are used as the reader input after passage reranking.1 1 Introduction Current open-domain question answering (OpenQA) systems often follow a RetrieverReader (R2) architecture, where the retriever first retrieves relevant passages and the reader then reads the retrieved passages to form an answer. Since the retriever retrieves passages from a large candidate pool (e.g., millions of Wikipedia passages), it often fails to rank the most relevant passages at the very top. One line of work (Mao et al., 2020; Karpukhin et al., 2020) aims to improve the retriever and shows that significantly better QA performance can be achieved when the retrieval results are improved. ∗ Work was done during internship at Microsoft Azure AI. Our code is available at https://github.com/ morningmoni/GAR. 1 An alternative solution is to rerank the initial retrieval results via a reranker, which is widely used in information retrieval (Nogueira and Cho, 2019; Qiao et al., 2019) and explored in early OpenQA systems (Wang et al., 2018a; Lee et al., 2018). However, current state-of-the-art OpenQA systems (Karpukhin et al"
2021.findings-acl.29,D19-1284,0,0.224656,"Missing"
2021.findings-acl.29,2020.emnlp-main.466,0,0.746661,"ng itself despite the noise in reader predictions. 2.3 Passage Reading We consider a scenario where the number of passages that can be used for QA is limited (sometimes deliberately) due to reasons such as insufficient computational resources, the limit of model input length, or requirement for faster responses. We use a generative reader initialized by BARTlarge (Lewis et al., 2019), which concatenates the question and top-10 retrieved passages, trims them to 1,024 tokens (7.8 passages are left on average) as the input, and learns to generate the answer in a seq2seq manner (Mao et al., 2020; Min et al., 2020). We further add a simple shuffle strategy during reader training, which randomly shuffles the top retrieved passages before concatenation. In this way, the reader appears to be more robust to the reranked passages during inference and achieves better performance after reranking. Passage Reranking Given an initially retrieved passage list R and topN predictions of the reader A[:N ] , R IDER forms a reranked passage list R0 as follows. R IDER scans R from the beginning of the list and appends to R0 every passage p ∈ R if p contains any reader prediction a ∈ A[:N ] after string normalization (re"
2021.findings-acl.29,2020.findings-emnlp.63,0,0.119668,"ess to much more passages. 4.4 Table 4: End-to-end QA comparison of state-of-theart methods. R IDER results in up to 4.2 EM gains. Top-1 Comparison w. Supervised Reranking Finally, we compare R IDER with two state-of-theart supervised reranking models. The first reranker is a BERT-base cross-encoder (Nogueira and Cho, 2019), which is popularly used for passage reranking in information retrieval. The cross-encoder concatenates the query and passage, and makes a binary relevance decision for each query-passage pair. The second one generates relevance labels as target tokens in a seq2seq manner (Nogueira et al., 2020). We use BART-large as the base model and “YES/NO” as the target tokens. As listed in Table 6, R IDER, without any train347 ing, outperforms the two transformer-based supervised rerankers on retrieval accuracy. Also, for QA performance, the best EM we obtain using the supervised rerankers is merely 46.3 on NQ. Such results further demonstrate the effectiveness of R IDER, which has the advantage of utilizing information from multiple passages (when the reader makes predictions), while the other rerankers consider query-passage pairs independently. 4.5 Runtime Efficiency The reranking step of R"
2021.findings-acl.29,2020.emnlp-main.437,0,0.155175,"Missing"
2021.naacl-demos.16,D14-1148,0,0.0290568,". We have made the dockerlized system publicly available for research purpose at GitHub1 , with a demo video2 . 1 Introduction Event extraction and tracking technologies can help us understand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: http://blender.cs.illinois.edu/softwa re/resin/resin.mp4 the United States will be able to answer the question “Which city is Columbia University located in?”, but very fe"
2021.naacl-demos.16,C16-1201,0,0.0607333,"Missing"
2021.naacl-demos.16,N13-1073,0,0.043681,"luster, we apply these patterns as highprecision patterns before two statistical temporal ordering models separately. The schema matching algorithm will select the best matching from two graphs as the final instantiated schema results. Because the annotation for non-English data can be expensive and time-consuming, the temporal event tracking component has only been trained on English input. To extend the temporal event tracking capability to cross-lingual setting, we apply Google Cloud neural machine translation 6 to translate Spanish documents into English and apply the FastAlign algorithm (Dyer et al., 2013) to obtain word alignment. 2.6 Cross-media Information Grounding and Fusion Visual event and argument role extraction: Our goal is to extract visual events along with their argument roles from visual data, i.e., images and videos. In order to train event extractor from visual data, we have collected a new dataset called Video M2E2 which contains 1,500 video-article pairs by searching over YouTube news channels using 18 event primitives related to visual concepts as search keywords. We have extensively annotated the the videos and sampled key frames for annotating bounding boxes of argument rol"
2021.naacl-demos.16,2020.acl-main.718,0,0.068856,"Missing"
2021.naacl-demos.16,D19-5102,0,0.0208286,"arch purpose at GitHub1 , with a demo video2 . 1 Introduction Event extraction and tracking technologies can help us understand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: http://blender.cs.illinois.edu/softwa re/resin/resin.mp4 the United States will be able to answer the question “Which city is Columbia University located in?”, but very few people can give a complete answer to “Who died from COVID-19?”. Pr"
2021.naacl-demos.16,glavas-etal-2014-hieve,0,0.0312373,"l cross-media event extraction, coreference resolution and temporal event tracking; (2) using human curated event schema library to match and enhance the extraction output. We have made the dockerlized system publicly available for research purpose at GitHub1 , with a demo video2 . 1 Introduction Event extraction and tracking technologies can help us understand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: ht"
2021.naacl-demos.16,R13-2011,0,0.025761,"ross-document cross-lingual cross-media event extraction, coreference resolution and temporal event tracking; (2) using human curated event schema library to match and enhance the extraction output. We have made the dockerlized system publicly available for research purpose at GitHub1 , with a demo video2 . 1 Introduction Event extraction and tracking technologies can help us understand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeli"
2021.naacl-demos.16,D19-1041,0,0.0480812,"Missing"
2021.naacl-demos.16,N19-1085,0,0.0191037,"oend Information Extraction (IE) systems (Wadden et al., 2019; Li et al., 2020b; Lin et al., 2020; Li et al., 2019) mainly focus on extracting entities, events and entity relations from individual sentences. In contrast, we extract and infer arguments over the global document context. Furthermore, our IE system is guided by a schema repository. The extracted graph will be used to instantiate a schema graph, which can be applied to predict future events. Coreference Resolution. Previous neural models for event coreference resolution use noncontextual (Nguyen et al., 2016; Choubey et al., 2020; Huang et al., 2019) or contextual word representations (Lu et al., 2020; Yu et al., 2020). We incorporate a wide range of symbolic features (Chen and Ji, 2009; Chen et al., 2009; Sammons et al., 2015; Lu and Ng, 2016, 2017; Duncan et al., 2017), such as event attributes and types, into our event coreference resolution module using a contextdependent gate mechanism. Temporal Event Ordering. Temporal relations between events are extracted for neighbor events in one sentence (Ning et al., 2017, 2018a, 2019; Multimedia Information Extraction. Previous Han et al., 2019), ignoring the temporal dependenmultimedia IE sy"
2021.naacl-demos.16,W18-3101,0,0.0195422,"ly available for research purpose at GitHub1 , with a demo video2 . 1 Introduction Event extraction and tracking technologies can help us understand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: http://blender.cs.illinois.edu/softwa re/resin/resin.mp4 the United States will be able to answer the question “Which city is Columbia University located in?”, but very few people can give a complete answer to “Who di"
2021.naacl-demos.16,N16-1056,0,0.030098,"nologies can help us understand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: http://blender.cs.illinois.edu/softwa re/resin/resin.mp4 the United States will be able to answer the question “Which city is Columbia University located in?”, but very few people can give a complete answer to “Who died from COVID-19?”. Progress in natural language understanding and computer vision has helped automate some parts of even"
2021.naacl-demos.16,W18-5620,0,0.0245812,"ts described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: http://blender.cs.illinois.edu/softwa re/resin/resin.mp4 the United States will be able to answer the question “Which city is Columbia University located in?”, but very few people can give a complete answer to “Who died from COVID-19?”. Progress in natural language understanding and computer vision has helped automate some parts of event understanding but the current, first-generati"
2021.naacl-demos.16,L16-1545,0,0.0202604,"rstand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: http://blender.cs.illinois.edu/softwa re/resin/resin.mp4 the United States will be able to answer the question “Which city is Columbia University located in?”, but very few people can give a complete answer to “Who died from COVID-19?”. Progress in natural language understanding and computer vision has helped automate some parts of event understanding but th"
2021.naacl-demos.16,2021.naacl-main.274,1,0.542222,"e postprocessing procedure and find the matching text span closest to the corresponding event trigger. Given N hybrid English and Spanish input documents, we create N (N2−1) pairs of documents and treat each pair as a single “mega-document”. We apply our model to each mega-document and, at the end, aggregate the predictions across all megadocuments to extract the coreference clusters. Finally, we also apply a simple heuristic rule that prevents two entity mentions from being merged together if they are linked to different entities with high confidence. Our event coreference resolution method (Lai et al., 2021) is similar to entity coreference resolution, while incorporating additional symbolic features such as the event type information. If the input documents are all about one specific complex event, we apply some schema-guided heuristic rules to further refine the predictions of the neural event coreference resolution model. For example, in a bombing schema, there is typically only one bombing event. Therefore, in a document cluster, if there are two event mentions of type bombing and they have several arguments in common, these two mentions will be considered as coreferential. 2.5 Cross-document"
2021.naacl-demos.16,D17-1018,0,0.0120578,"5 model and finetune it on MATRES (Ning et al., 2018b) and use it as the system for temporal event ordering. We 2.4 Cross-document Cross-lingual Entity and perform pair-wise temporal relation classification Event Coreference Resolution for all event mention pairs in a documents. After extracting all mentions of entities and events, We further train an alternative model from finewe apply our cross-document cross-lingual entity tuning RoBERTa (Liu et al., 2019) on MATRES coreference resolution model, which is an exten(Ning et al., 2018b). This model has also been sucsion of the e2e-coref model (Lee et al., 2017). cessfully applied for event time prediction (Wen We use the multilingual XLM-RoBERTa (XLMet al., 2021; Li et al., 2020a). We only consider R) Transformer model (Conneau et al., 2020) so event mention pairs which are within neighboring that our coreference resolution model can handle sentences, or can be connected by shared argunon-English data. Second, we port the e2e-coref ments. model to the cross-lingual cross-document setting. Besides model prediction, we also learn high 5 https://www.wikidata.org/ confident patterns from the schema repository. We 135 consider temporal relations that app"
2021.naacl-demos.16,2020.acl-main.703,0,0.0204692,"racted arguments from both models as the final output. We formulate the argument extraction problem as conditional text generation. Our model can easily handle the case of missing arguments and multiple arguments in the same role without the need of tuning thresholds and can extract all arguments in a single pass. The condition consists of the original document and a blank event template. For example, the template for Transportation event type is arg1 transported arg2 in arg3 from arg4 place to arg5 place. The desired output is a filled template with the arguments. Our model is based on BART (Lewis et al., 2020), which is an encoder-decoder language model. To utilize the encoder-decoder LM for argument extraction, we construct an input sequence of hsi template hsih/sidocument h/si. All argument names (arg1, arg2 etc.) in the template are replaced by a special placeholder token hargi. This model is trained in an end-to-end fashion by directly optimizing the generation probability. To align the extracted arguments back to the document, we adopt a simple postprocessing procedure and find the matching text span closest to the corresponding event trigger. Given N hybrid English and Spanish input documents"
2021.naacl-demos.16,N19-4019,1,0.796479,"ence Detainee ArrestJailDetain ReleaseParole Defendant Convict ReleaseParole Figure 2: The visualization of schema matching results from extracted graph and schema. The unmatched portions for both extracted graph and schema are blurred. can see that our system can extract events, entities and relations and align them well with the selected schema. The final instantiated schema is the hybrid of two graphs from merging the matched elements. 4 Related Work Text Information Extraction. Existing end-toend Information Extraction (IE) systems (Wadden et al., 2019; Li et al., 2020b; Lin et al., 2020; Li et al., 2019) mainly focus on extracting entities, events and entity relations from individual sentences. In contrast, we extract and infer arguments over the global document context. Furthermore, our IE system is guided by a schema repository. The extracted graph will be used to instantiate a schema graph, which can be applied to predict future events. Coreference Resolution. Previous neural models for event coreference resolution use noncontextual (Nguyen et al., 2016; Choubey et al., 2020; Huang et al., 2019) or contextual word representations (Lu et al., 2020; Yu et al., 2020). We incorporate a wide ra"
2021.naacl-demos.16,2020.acl-demos.11,1,0.922974,"document Cross-lingual Entity and perform pair-wise temporal relation classification Event Coreference Resolution for all event mention pairs in a documents. After extracting all mentions of entities and events, We further train an alternative model from finewe apply our cross-document cross-lingual entity tuning RoBERTa (Liu et al., 2019) on MATRES coreference resolution model, which is an exten(Ning et al., 2018b). This model has also been sucsion of the e2e-coref model (Lee et al., 2017). cessfully applied for event time prediction (Wen We use the multilingual XLM-RoBERTa (XLMet al., 2021; Li et al., 2020a). We only consider R) Transformer model (Conneau et al., 2020) so event mention pairs which are within neighboring that our coreference resolution model can handle sentences, or can be connected by shared argunon-English data. Second, we port the e2e-coref ments. model to the cross-lingual cross-document setting. Besides model prediction, we also learn high 5 https://www.wikidata.org/ confident patterns from the schema repository. We 135 consider temporal relations that appear very frequently as our prior knowledge. For each given document cluster, we apply these patterns as highprecision pa"
2021.naacl-demos.16,2020.emnlp-main.50,1,0.880674,"document Cross-lingual Entity and perform pair-wise temporal relation classification Event Coreference Resolution for all event mention pairs in a documents. After extracting all mentions of entities and events, We further train an alternative model from finewe apply our cross-document cross-lingual entity tuning RoBERTa (Liu et al., 2019) on MATRES coreference resolution model, which is an exten(Ning et al., 2018b). This model has also been sucsion of the e2e-coref model (Lee et al., 2017). cessfully applied for event time prediction (Wen We use the multilingual XLM-RoBERTa (XLMet al., 2021; Li et al., 2020a). We only consider R) Transformer model (Conneau et al., 2020) so event mention pairs which are within neighboring that our coreference resolution model can handle sentences, or can be connected by shared argunon-English data. Second, we port the e2e-coref ments. model to the cross-lingual cross-document setting. Besides model prediction, we also learn high 5 https://www.wikidata.org/ confident patterns from the schema repository. We 135 consider temporal relations that appear very frequently as our prior knowledge. For each given document cluster, we apply these patterns as highprecision pa"
2021.naacl-demos.16,2021.naacl-main.69,1,0.764758,"chemas 4 https://aws.amazon.com/transcribe/ global score. After we extract these mentions, we 134 apply a syntactic parser (Honnibal et al., 2020) to extend mention head words to their extents. Then we apply a cross-lingual entity linker (Pan et al., 2017) to link entity mentions to WikiData (Vrandeˇci´c and Krötzsch, 2014)5 . 2.3 Document-level Event Argument Extraction The previous module can only operate on the sentence level. In particular, event arguments can often be found in neighboring sentences. To make up for this, we further develop a document-level event argument extraction model (Li et al., 2021) and use the union of the extracted arguments from both models as the final output. We formulate the argument extraction problem as conditional text generation. Our model can easily handle the case of missing arguments and multiple arguments in the same role without the need of tuning thresholds and can extract all arguments in a single pass. The condition consists of the original document and a blank event template. For example, the template for Transportation event type is arg1 transported arg2 in arg3 from arg4 place to arg5 place. The desired output is a filled template with the arguments."
2021.naacl-demos.16,2020.acl-main.713,1,0.86722,"und the extracted knowledge elements onto our extracted graph via cross-media event coreference resolution (Section 2.6). Finally, our system selects the schema from a schema repository that best matches the extracted IE graph and merges these two graphs (Section 2.7). Our system can extract 24 types of entities, 46 types of relations and 67 types of events as defined in the DARPA KAIROS3 ontology. times for each detected words, as well as potential alternative transcriptions. Then from the speech recognition results and text input, we extract entity, relation, and event mentions using OneIE (Lin et al., 2020), a stateof-the-art joint neural model for sentence-level information extraction. Given a sentence, the goal of this module is to extract an information graph G = (V, E), where V is the node set containing entity mentions and event triggers and E is the edge set containing entity relations and event-argument links. We use a pre-trained BERT encoder (Devlin et al., 2018) to obtain contextualized word representations for the input sentence. Next, we adopt separate conditional random field-based taggers to identify entity mention and event trigger spans from the sentence. We represent each span,"
2021.naacl-demos.16,2021.ccl-1.108,0,0.0312676,"Missing"
2021.naacl-demos.16,L16-1631,0,0.0207744,"ences. In contrast, we extract and infer arguments over the global document context. Furthermore, our IE system is guided by a schema repository. The extracted graph will be used to instantiate a schema graph, which can be applied to predict future events. Coreference Resolution. Previous neural models for event coreference resolution use noncontextual (Nguyen et al., 2016; Choubey et al., 2020; Huang et al., 2019) or contextual word representations (Lu et al., 2020; Yu et al., 2020). We incorporate a wide range of symbolic features (Chen and Ji, 2009; Chen et al., 2009; Sammons et al., 2015; Lu and Ng, 2016, 2017; Duncan et al., 2017), such as event attributes and types, into our event coreference resolution module using a contextdependent gate mechanism. Temporal Event Ordering. Temporal relations between events are extracted for neighbor events in one sentence (Ning et al., 2017, 2018a, 2019; Multimedia Information Extraction. Previous Han et al., 2019), ignoring the temporal dependenmultimedia IE systems (Li et al., 2020b; Yazici cies between events across sentences. We perform et al., 2018) only include cross-media entity coref- document-level event ordering and propagate temerence resolutio"
2021.naacl-demos.16,P17-1009,0,0.0621125,"Missing"
2021.naacl-demos.16,2020.findings-emnlp.253,0,0.0765544,"Missing"
2021.naacl-demos.16,2020.findings-emnlp.344,0,0.0324985,"match and enhance the extraction output. We have made the dockerlized system publicly available for research purpose at GitHub1 , with a demo video2 . 1 Introduction Event extraction and tracking technologies can help us understand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: http://blender.cs.illinois.edu/softwa re/resin/resin.mp4 the United States will be able to answer the question “Which city is Columb"
2021.naacl-demos.16,D17-1108,1,0.83481,"on. Previous neural models for event coreference resolution use noncontextual (Nguyen et al., 2016; Choubey et al., 2020; Huang et al., 2019) or contextual word representations (Lu et al., 2020; Yu et al., 2020). We incorporate a wide range of symbolic features (Chen and Ji, 2009; Chen et al., 2009; Sammons et al., 2015; Lu and Ng, 2016, 2017; Duncan et al., 2017), such as event attributes and types, into our event coreference resolution module using a contextdependent gate mechanism. Temporal Event Ordering. Temporal relations between events are extracted for neighbor events in one sentence (Ning et al., 2017, 2018a, 2019; Multimedia Information Extraction. Previous Han et al., 2019), ignoring the temporal dependenmultimedia IE systems (Li et al., 2020b; Yazici cies between events across sentences. We perform et al., 2018) only include cross-media entity coref- document-level event ordering and propagate temerence resolution by grounding the extracted visual poral attributes through shared arguments. Furtherentities to text. We are the first to perform cross- more, we take advantage of the schema repository media joint event extraction and coreference reso- knowledge by using the frequent temporal"
2021.naacl-demos.16,P18-1212,1,0.895436,"Missing"
2021.naacl-demos.16,D19-1642,1,0.844444,"Missing"
2021.naacl-demos.16,P18-1122,1,0.846572,"oss-document Temporal Event Ordering Based on the event coreference resolution component described above, we group all mentions into clusters. Next we aim to order events along a timeline. We follow Zhou et al. (2020) to design a component for temporal event ordering. Specifically, we further pre-train a T5 model (Raffel et al., 2020) with distant temporal ordering supervision signals. These signals are acquired through two set of syntactic patterns: 1) before/after keywords in text and 2) explicit date and time mentions. We take such a pre-trained temporal T5 model and finetune it on MATRES (Ning et al., 2018b) and use it as the system for temporal event ordering. We 2.4 Cross-document Cross-lingual Entity and perform pair-wise temporal relation classification Event Coreference Resolution for all event mention pairs in a documents. After extracting all mentions of entities and events, We further train an alternative model from finewe apply our cross-document cross-lingual entity tuning RoBERTa (Liu et al., 2019) on MATRES coreference resolution model, which is an exten(Ning et al., 2018b). This model has also been sucsion of the e2e-coref model (Lee et al., 2017). cessfully applied for event time"
2021.naacl-demos.16,P17-1178,1,0.849941,"he audio signal. It cific global feature. We compute the global feature returns the transcription with starting and ending score as uf , where u is a learnable weight vec3 https://www.darpa.mil/program/knowledge-di tor. Finally, we use a beam search-based decoder rected-artificial-intelligence-reasoning-overto generate the information graph with the highest schemas 4 https://aws.amazon.com/transcribe/ global score. After we extract these mentions, we 134 apply a syntactic parser (Honnibal et al., 2020) to extend mention head words to their extents. Then we apply a cross-lingual entity linker (Pan et al., 2017) to link entity mentions to WikiData (Vrandeˇci´c and Krötzsch, 2014)5 . 2.3 Document-level Event Argument Extraction The previous module can only operate on the sentence level. In particular, event arguments can often be found in neighboring sentences. To make up for this, we further develop a document-level event argument extraction model (Li et al., 2021) and use the union of the extracted arguments from both models as the final output. We formulate the argument extraction problem as conditional text generation. Our model can easily handle the case of missing arguments and multiple argument"
2021.naacl-demos.16,W15-0812,0,0.0200332,"11 139 1,213 Videos 31 Table 4: Data statistics for schema matching corpus (LDC2020E39). Schema-guided Information Extraction. The Category Extracted Schema Instantiated Events Steps Steps performance of each component is shown in Table 3. We evaluate the end-to-end perfor# 3,180 1,738 958 mance of our system on a complex event corTable 5: Results of schema matching. pus (LDC2020E39), which contains multi-lingual multi-media document clusters. The data statistics are shown in Table 4. We train our mention ex3.3 Qualitative Analysis traction component on ACE 2005 (Walker et al., 2006) and ERE (Song et al., 2015); document- Figure 2 illustrates a subset of examples for the best matched results from our end-to-end system. We level argument exraction on ACE 2005 (Walker 7 et al., 2006) and RAMS (Ebner et al., 2020); corefLDC2017E03 8 erence component on ACE 2005 (Walker et al., LDC2017E52 137 Extracted Graph Old Bailey A court in British legal history Max Hill Manchester Communicator Place JudgeCourt JudgeCourt Place JudgeCourt Broadcast ReleaseParole Resident ... ... ChargeIndict TrialHearing Defendant Sentence ArrestJailDetain Defendant Defendant Detainee Defendant ReleaseParole Defendant Salman Abedi"
2021.naacl-demos.16,W02-2024,0,0.567788,"Missing"
2021.naacl-demos.16,D19-1585,0,0.0252013,".. ... ChargeIndict TrialHearing Defendant Defendant Sentence Detainee ArrestJailDetain ReleaseParole Defendant Convict ReleaseParole Figure 2: The visualization of schema matching results from extracted graph and schema. The unmatched portions for both extracted graph and schema are blurred. can see that our system can extract events, entities and relations and align them well with the selected schema. The final instantiated schema is the hybrid of two graphs from merging the matched elements. 4 Related Work Text Information Extraction. Existing end-toend Information Extraction (IE) systems (Wadden et al., 2019; Li et al., 2020b; Lin et al., 2020; Li et al., 2019) mainly focus on extracting entities, events and entity relations from individual sentences. In contrast, we extract and infer arguments over the global document context. Furthermore, our IE system is guided by a schema repository. The extracted graph will be used to instantiate a schema graph, which can be applied to predict future events. Coreference Resolution. Previous neural models for event coreference resolution use noncontextual (Nguyen et al., 2016; Choubey et al., 2020; Huang et al., 2019) or contextual word representations (Lu et"
2021.naacl-demos.16,W12-4501,0,0.0999034,"Missing"
2021.naacl-demos.16,2021.naacl-main.6,1,0.690677,"Missing"
2021.naacl-demos.16,P18-4009,0,0.0278418,"lized system publicly available for research purpose at GitHub1 , with a demo video2 . 1 Introduction Event extraction and tracking technologies can help us understand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: http://blender.cs.illinois.edu/softwa re/resin/resin.mp4 the United States will be able to answer the question “Which city is Columbia University located in?”, but very few people can give a compl"
2021.naacl-demos.16,N18-5009,1,0.831121,"Missing"
2021.naacl-demos.8,W19-1909,0,0.0217165,"Missing"
2021.naacl-demos.8,2020.emnlp-demos.18,0,0.241329,"ung cancer. Severe Acute Respiratory Syndrome lopinavir-ritonavir drug combination Q1 Q4 Q6 Q8 Table 1: Example Answers for Questions in Drug Repurposing Reports Manandhar and Yuret, 2013; Bui et al., 2014; Peng et al., 2016; Wei et al., 2015; Peng et al., 2017; Luo et al., 2017; Wei et al., 2019; Li and Ji, 2019; Peng et al., 2019, 2020), and events (Ananiadou et al., 2010; Van Landeghem et al., 2013; Nédellec et al., 2013; Deléger et al., 2016; Wei et al., 2019; Li et al., 2019; ShafieiBavani et al., 2020) from biomedical literature, with the most recent work focused on COVID-19 literature (Hope et al., 2020; Ilievski et al., 2020; Wolinski, 2020; Ahamed and Samad, 2020). Most of the recent biomedical QA work (Yang et al., 2015, 2016; Chandu et al., 2017; Kraus et al., 2017) is driven by the BioASQ initiative (Tsatsaronis et al., 2015), and many live QA systems, including COVIDASK11 and AUEB12 , and search enCOVID-19 Coronavirus Infections cathepsin D Figure 9: Connections Involving Coronavirus Related Diseases 5 Example Answers Drug Class angiotensin-converting enzyme (ACE) inhibitors Disease hypertension [PMID:32314699 (PMC7253125)] Past medical history was significant for hypertension, treated"
2021.naacl-demos.8,D19-1371,0,0.0246983,"89178)] To address the role of angiotensin in lung injury, there is an ongoing clinical trial to examine whether losartan treatment affects outcomes in COVID-19 associated ARDS (NCT04312009). [PMID:32439915 (PMC7242178)] Losartan was also the molecule chosen in two trials recently started in the United States by the University of Minnesota to treat patients with COVID-19 (clinical trials.gov NCT04311177 and NCT 104312009). Related Work Extensive prior research work has focused on extracting biomedical entities (Zheng et al., 2014; Habibi et al., 2017; Crichton et al., 2017; Wang et al., 2018; Beltagy et al., 2019; Alsentzer et al., 2019; Wei et al., 2019; Wang et al., 2020c), relations (Uzuner et al., 2011; Krallinger et al., 2011; 11 10 http://blender.cs.illinois.edu/ covid19/DrugRe-purposingReport_V2.0.docx 12 71 https://covidask.korea.ac.kr/ http://cslab241.cs.aueb.gr:5000/ gines (Kricka et al., 2020; Esteva et al., 2020; Hope et al., 2020; Taub Tabib et al., 2020) have been developed. Our work is an application and extension of our recently developed multimedia knowledge extraction system for the news domain (Li et al., 2020a,b). Similar to the news domain, the knowledge elements extracted from te"
2021.naacl-demos.8,W17-2307,0,0.0224213,"posing Reports Manandhar and Yuret, 2013; Bui et al., 2014; Peng et al., 2016; Wei et al., 2015; Peng et al., 2017; Luo et al., 2017; Wei et al., 2019; Li and Ji, 2019; Peng et al., 2019, 2020), and events (Ananiadou et al., 2010; Van Landeghem et al., 2013; Nédellec et al., 2013; Deléger et al., 2016; Wei et al., 2019; Li et al., 2019; ShafieiBavani et al., 2020) from biomedical literature, with the most recent work focused on COVID-19 literature (Hope et al., 2020; Ilievski et al., 2020; Wolinski, 2020; Ahamed and Samad, 2020). Most of the recent biomedical QA work (Yang et al., 2015, 2016; Chandu et al., 2017; Kraus et al., 2017) is driven by the BioASQ initiative (Tsatsaronis et al., 2015), and many live QA systems, including COVIDASK11 and AUEB12 , and search enCOVID-19 Coronavirus Infections cathepsin D Figure 9: Connections Involving Coronavirus Related Diseases 5 Example Answers Drug Class angiotensin-converting enzyme (ACE) inhibitors Disease hypertension [PMID:32314699 (PMC7253125)] Past medical history was significant for hypertension, treated with Evidence amlodipine and benazepril, and chronic back pain. Sentences [PMID:32081428 (PMC7092824)] On the other hand, many ACE inhibitors are cu"
2021.naacl-demos.8,W13-2001,0,0.0375856,"sults, the scientists also indicated that many results were worth further investigation. For example, in Figure 3 we can see that Lusartan is connected to tumor protein p53 which is related to lung cancer. Severe Acute Respiratory Syndrome lopinavir-ritonavir drug combination Q1 Q4 Q6 Q8 Table 1: Example Answers for Questions in Drug Repurposing Reports Manandhar and Yuret, 2013; Bui et al., 2014; Peng et al., 2016; Wei et al., 2015; Peng et al., 2017; Luo et al., 2017; Wei et al., 2019; Li and Ji, 2019; Peng et al., 2019, 2020), and events (Ananiadou et al., 2010; Van Landeghem et al., 2013; Nédellec et al., 2013; Deléger et al., 2016; Wei et al., 2019; Li et al., 2019; ShafieiBavani et al., 2020) from biomedical literature, with the most recent work focused on COVID-19 literature (Hope et al., 2020; Ilievski et al., 2020; Wolinski, 2020; Ahamed and Samad, 2020). Most of the recent biomedical QA work (Yang et al., 2015, 2016; Chandu et al., 2017; Kraus et al., 2017) is driven by the BioASQ initiative (Tsatsaronis et al., 2015), and many live QA systems, including COVIDASK11 and AUEB12 , and search enCOVID-19 Coronavirus Infections cathepsin D Figure 9: Connections Involving Coronavirus Related Disease"
2021.naacl-demos.8,N19-1145,1,0.895006,"Missing"
2021.naacl-demos.8,Q17-1008,0,0.0252127,"s Factor, and Interleukin-10. We see all of these connections in our results, such as the examples shown in Figure 3 and Figure 9. With further checks on these results, the scientists also indicated that many results were worth further investigation. For example, in Figure 3 we can see that Lusartan is connected to tumor protein p53 which is related to lung cancer. Severe Acute Respiratory Syndrome lopinavir-ritonavir drug combination Q1 Q4 Q6 Q8 Table 1: Example Answers for Questions in Drug Repurposing Reports Manandhar and Yuret, 2013; Bui et al., 2014; Peng et al., 2016; Wei et al., 2015; Peng et al., 2017; Luo et al., 2017; Wei et al., 2019; Li and Ji, 2019; Peng et al., 2019, 2020), and events (Ananiadou et al., 2010; Van Landeghem et al., 2013; Nédellec et al., 2013; Deléger et al., 2016; Wei et al., 2019; Li et al., 2019; ShafieiBavani et al., 2020) from biomedical literature, with the most recent work focused on COVID-19 literature (Hope et al., 2020; Ilievski et al., 2020; Wolinski, 2020; Ahamed and Samad, 2020). Most of the recent biomedical QA work (Yang et al., 2015, 2016; Chandu et al., 2017; Kraus et al., 2017) is driven by the BioASQ initiative (Tsatsaronis et al., 2015), and many l"
2021.naacl-demos.8,2020.bionlp-1.22,0,0.0299439,"Missing"
2021.naacl-demos.8,D19-6204,1,0.833702,"ctions in our results, such as the examples shown in Figure 3 and Figure 9. With further checks on these results, the scientists also indicated that many results were worth further investigation. For example, in Figure 3 we can see that Lusartan is connected to tumor protein p53 which is related to lung cancer. Severe Acute Respiratory Syndrome lopinavir-ritonavir drug combination Q1 Q4 Q6 Q8 Table 1: Example Answers for Questions in Drug Repurposing Reports Manandhar and Yuret, 2013; Bui et al., 2014; Peng et al., 2016; Wei et al., 2015; Peng et al., 2017; Luo et al., 2017; Wei et al., 2019; Li and Ji, 2019; Peng et al., 2019, 2020), and events (Ananiadou et al., 2010; Van Landeghem et al., 2013; Nédellec et al., 2013; Deléger et al., 2016; Wei et al., 2019; Li et al., 2019; ShafieiBavani et al., 2020) from biomedical literature, with the most recent work focused on COVID-19 literature (Hope et al., 2020; Ilievski et al., 2020; Wolinski, 2020; Ahamed and Samad, 2020). Most of the recent biomedical QA work (Yang et al., 2015, 2016; Chandu et al., 2017; Kraus et al., 2017) is driven by the BioASQ initiative (Tsatsaronis et al., 2015), and many live QA systems, including COVIDASK11 and AUEB12 , and"
2021.naacl-demos.8,2020.acl-demos.11,1,0.877148,"2014; Habibi et al., 2017; Crichton et al., 2017; Wang et al., 2018; Beltagy et al., 2019; Alsentzer et al., 2019; Wei et al., 2019; Wang et al., 2020c), relations (Uzuner et al., 2011; Krallinger et al., 2011; 11 10 http://blender.cs.illinois.edu/ covid19/DrugRe-purposingReport_V2.0.docx 12 71 https://covidask.korea.ac.kr/ http://cslab241.cs.aueb.gr:5000/ gines (Kricka et al., 2020; Esteva et al., 2020; Hope et al., 2020; Taub Tabib et al., 2020) have been developed. Our work is an application and extension of our recently developed multimedia knowledge extraction system for the news domain (Li et al., 2020a,b). Similar to the news domain, the knowledge elements extracted from text and images in literature are complementary. Our framework advances state-of-the-art by extending the knowledge elements to more fine-grained types, incorporating image analysis and cross-media knowledge grounding, and KG matching into QA. 6 would be too time-consuming for manual human effort. Accordingly, the tool would be useful for stakeholders (e.g., biomedical scientists) to identify specific drug candidates and molecular targets that are relevant in their biomedical and clinical research aims. The use of our know"
2021.naacl-demos.8,W19-5006,0,0.0162201,"ults, such as the examples shown in Figure 3 and Figure 9. With further checks on these results, the scientists also indicated that many results were worth further investigation. For example, in Figure 3 we can see that Lusartan is connected to tumor protein p53 which is related to lung cancer. Severe Acute Respiratory Syndrome lopinavir-ritonavir drug combination Q1 Q4 Q6 Q8 Table 1: Example Answers for Questions in Drug Repurposing Reports Manandhar and Yuret, 2013; Bui et al., 2014; Peng et al., 2016; Wei et al., 2015; Peng et al., 2017; Luo et al., 2017; Wei et al., 2019; Li and Ji, 2019; Peng et al., 2019, 2020), and events (Ananiadou et al., 2010; Van Landeghem et al., 2013; Nédellec et al., 2013; Deléger et al., 2016; Wei et al., 2019; Li et al., 2019; ShafieiBavani et al., 2020) from biomedical literature, with the most recent work focused on COVID-19 literature (Hope et al., 2020; Ilievski et al., 2020; Wolinski, 2020; Ahamed and Samad, 2020). Most of the recent biomedical QA work (Yang et al., 2015, 2016; Chandu et al., 2017; Kraus et al., 2017) is driven by the BioASQ initiative (Tsatsaronis et al., 2015), and many live QA systems, including COVIDASK11 and AUEB12 , and search enCOVID-19"
2021.naacl-demos.8,2020.acl-main.230,1,0.885325,"Missing"
2021.naacl-demos.8,D19-1410,0,0.0136688,"nowledge elements in each path in the KG. Each edge is assigned a salience score by aggregating the scores of paths passing through it. In addition to knowledge elements, we also present related sentences and source information as evidence. We use BioBert (Lee et al., 2020), a pre-trained language model to represent each sentence along with its left and right neighboring sentences as local contexts. Using the same architecture computed on all respective sentences and the user query, we aggregate the sequence embedding layer, the last hidden layer in the BERT architecture with average pooling (Reimers and Gurevych, 2019). We use the similarity between the embedding representations of each sentence and each query to identify and extract the most relevant sentences as evidence. Another common category of queries includes entity types, rather than entity instances, and requires extracting evidence sentences based on type or pattern matching. We have developed E VI DENCE M INER (Wang et al., 2020a,b), a web-based system that allows for the user’s query as a natural language statement or an inquiry about a relationship at the meta-symbol level (e.g., CHEMICAL, PROTEIN) and then automatically retrieves textual evid"
2021.naacl-demos.8,2020.bionlp-1.21,0,0.0201708,"igation. For example, in Figure 3 we can see that Lusartan is connected to tumor protein p53 which is related to lung cancer. Severe Acute Respiratory Syndrome lopinavir-ritonavir drug combination Q1 Q4 Q6 Q8 Table 1: Example Answers for Questions in Drug Repurposing Reports Manandhar and Yuret, 2013; Bui et al., 2014; Peng et al., 2016; Wei et al., 2015; Peng et al., 2017; Luo et al., 2017; Wei et al., 2019; Li and Ji, 2019; Peng et al., 2019, 2020), and events (Ananiadou et al., 2010; Van Landeghem et al., 2013; Nédellec et al., 2013; Deléger et al., 2016; Wei et al., 2019; Li et al., 2019; ShafieiBavani et al., 2020) from biomedical literature, with the most recent work focused on COVID-19 literature (Hope et al., 2020; Ilievski et al., 2020; Wolinski, 2020; Ahamed and Samad, 2020). Most of the recent biomedical QA work (Yang et al., 2015, 2016; Chandu et al., 2017; Kraus et al., 2017) is driven by the BioASQ initiative (Tsatsaronis et al., 2015), and many live QA systems, including COVIDASK11 and AUEB12 , and search enCOVID-19 Coronavirus Infections cathepsin D Figure 9: Connections Involving Coronavirus Related Diseases 5 Example Answers Drug Class angiotensin-converting enzyme (ACE) inhibitors Disease"
2021.naacl-demos.8,D18-1230,1,0.819154,"Missing"
2021.naacl-demos.8,2020.bionlp-1.3,0,0.0196518,"h COVID-19 (clinical trials.gov NCT04311177 and NCT 104312009). Related Work Extensive prior research work has focused on extracting biomedical entities (Zheng et al., 2014; Habibi et al., 2017; Crichton et al., 2017; Wang et al., 2018; Beltagy et al., 2019; Alsentzer et al., 2019; Wei et al., 2019; Wang et al., 2020c), relations (Uzuner et al., 2011; Krallinger et al., 2011; 11 10 http://blender.cs.illinois.edu/ covid19/DrugRe-purposingReport_V2.0.docx 12 71 https://covidask.korea.ac.kr/ http://cslab241.cs.aueb.gr:5000/ gines (Kricka et al., 2020; Esteva et al., 2020; Hope et al., 2020; Taub Tabib et al., 2020) have been developed. Our work is an application and extension of our recently developed multimedia knowledge extraction system for the news domain (Li et al., 2020a,b). Similar to the news domain, the knowledge elements extracted from text and images in literature are complementary. Our framework advances state-of-the-art by extending the knowledge elements to more fine-grained types, incorporating image analysis and cross-media knowledge grounding, and KG matching into QA. 6 would be too time-consuming for manual human effort. Accordingly, the tool would be useful for stakeholders (e.g., bio"
2021.naacl-demos.8,W16-3104,0,0.0689349,"Missing"
2021.naacl-demos.8,P19-1191,1,0.889692,"Missing"
2021.naacl-demos.8,C14-1149,1,0.831567,"Missing"
2021.naacl-demos.8,2020.acl-demos.8,1,0.900567,"cal contexts. Using the same architecture computed on all respective sentences and the user query, we aggregate the sequence embedding layer, the last hidden layer in the BERT architecture with average pooling (Reimers and Gurevych, 2019). We use the similarity between the embedding representations of each sentence and each query to identify and extract the most relevant sentences as evidence. Another common category of queries includes entity types, rather than entity instances, and requires extracting evidence sentences based on type or pattern matching. We have developed E VI DENCE M INER (Wang et al., 2020a,b), a web-based system that allows for the user’s query as a natural language statement or an inquiry about a relationship at the meta-symbol level (e.g., CHEMICAL, PROTEIN) and then automatically retrieves textual evidence from a background corpora of COVID-19. 4 4.1 BM1_00870 BM1_06175 BM1_16375 BM1_17125 BM1_22385 BM1_30360 BM1_33735 BM1_56245 BM1_56735 BM1_00870 BM1_06175 BM1_16375 BM1_17125 BM1_22385 BM1_30360 BM1_33735 BM1_56245 BM1_56735 CATB-10270 CATB-1418 CATB-1674 CATB-16A CATB-16D2 CATB-1852 CATB1874 CATB-2744 CATB-3098 CATB-348 CATB-3483 CATB-5880 CATB-84 CATB912 CATD CATHY CATK"
2021.naacl-main.335,P19-1590,0,0.0185525,"s. Here, if we are able to pinpoint this document’s most essential classes, crafted cereal and baby cereal, as core classes, we can check their ancestor classes in the taxonomy and recover all the true classes. success, people find that applying these methods to many real-world scenarios remains challenging as the human labeling process is often too timeconsuming and expensive. Recently, more studies have been developed to address text classification using smaller amount of 1 Introduction labeled data. First, several semi-supervised methHierarchical multi-label text classification (HMTC) ods (Gururangan et al., 2019; Berthelot et al., 2019) aims to assign each text document to a set of rel- propose to use abundant unlabeled documents to evant classes from a class taxonomy. As a funda- assist model training on labeled dataset. Although mental task in NLP, HMTC has many applications mitigating the human annotation burden, these such as product categorization (Goumy and Mejri, methods still require a labeled dataset that covers 2018), semantic indexing (Li et al., 2019), and fine- all classes, which could be too expensive to obtain grained entity typing (Xu and Barbosa, 2018). when we have a large number of"
2021.naacl-main.335,D14-1181,0,0.0104706,"d entity typing (Xu and Barbosa, 2018). when we have a large number of classes in HMTC. Most existing methods address HMTC in a super- Second, some weakly-supervised models exploit vised fashion — they first ask humans to provide class indicative keywords (Meng et al., 2018; Zeng many labeled documents and then train a text clas- et al., 2019; Mekala and Shang, 2020) or class sursifier for prediction. Many classifiers have been face names (Meng et al., 2020; Wang et al., 2020) developed with different deep learning architec- to derive pseudo-labeled data for model training. tures such as CNN (Kim, 2014), RNN (You et al., Nevertheless, these models all assume each docu2019), Attention Network (Huang et al., 2019), and ment has only one class and all class surface names achieved decent performance when trained on mas- (or class indicative keywords) must appear in the sive human-labeled documents. Despite such a corpus, which are too restrictive for HMTC. 4239 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4239–4249 June 6–11, 2021. ©2021 Association for Computational Linguistics In this paper"
2021.naacl-main.335,2020.emnlp-main.724,1,0.949213,"still require a labeled dataset that covers 2018), semantic indexing (Li et al., 2019), and fine- all classes, which could be too expensive to obtain grained entity typing (Xu and Barbosa, 2018). when we have a large number of classes in HMTC. Most existing methods address HMTC in a super- Second, some weakly-supervised models exploit vised fashion — they first ask humans to provide class indicative keywords (Meng et al., 2018; Zeng many labeled documents and then train a text clas- et al., 2019; Mekala and Shang, 2020) or class sursifier for prediction. Many classifiers have been face names (Meng et al., 2020; Wang et al., 2020) developed with different deep learning architec- to derive pseudo-labeled data for model training. tures such as CNN (Kim, 2014), RNN (You et al., Nevertheless, these models all assume each docu2019), Attention Network (Huang et al., 2019), and ment has only one class and all class surface names achieved decent performance when trained on mas- (or class indicative keywords) must appear in the sive human-labeled documents. Despite such a corpus, which are too restrictive for HMTC. 4239 Proceedings of the 2021 Conference of the North American Chapter of the Association for C"
2021.naacl-main.335,D18-1352,0,0.04719,"Missing"
2021.naacl-main.335,P18-1029,0,0.0654361,"Missing"
2021.naacl-main.335,2020.acl-main.30,1,0.882062,"ating the human annotation burden, these such as product categorization (Goumy and Mejri, methods still require a labeled dataset that covers 2018), semantic indexing (Li et al., 2019), and fine- all classes, which could be too expensive to obtain grained entity typing (Xu and Barbosa, 2018). when we have a large number of classes in HMTC. Most existing methods address HMTC in a super- Second, some weakly-supervised models exploit vised fashion — they first ask humans to provide class indicative keywords (Meng et al., 2018; Zeng many labeled documents and then train a text clas- et al., 2019; Mekala and Shang, 2020) or class sursifier for prediction. Many classifiers have been face names (Meng et al., 2020; Wang et al., 2020) developed with different deep learning architec- to derive pseudo-labeled data for model training. tures such as CNN (Kim, 2014), RNN (You et al., Nevertheless, these models all assume each docu2019), Attention Network (Huang et al., 2019), and ment has only one class and all class surface names achieved decent performance when trained on mas- (or class indicative keywords) must appear in the sive human-labeled documents. Despite such a corpus, which are too restrictive for HMTC. 42"
2021.naacl-main.335,2021.naacl-main.84,0,0.0935152,"Missing"
2021.naacl-main.335,N18-1002,0,0.0220281,"xt classification (HMTC) ods (Gururangan et al., 2019; Berthelot et al., 2019) aims to assign each text document to a set of rel- propose to use abundant unlabeled documents to evant classes from a class taxonomy. As a funda- assist model training on labeled dataset. Although mental task in NLP, HMTC has many applications mitigating the human annotation burden, these such as product categorization (Goumy and Mejri, methods still require a labeled dataset that covers 2018), semantic indexing (Li et al., 2019), and fine- all classes, which could be too expensive to obtain grained entity typing (Xu and Barbosa, 2018). when we have a large number of classes in HMTC. Most existing methods address HMTC in a super- Second, some weakly-supervised models exploit vised fashion — they first ask humans to provide class indicative keywords (Meng et al., 2018; Zeng many labeled documents and then train a text clas- et al., 2019; Mekala and Shang, 2020) or class sursifier for prediction. Many classifiers have been face names (Meng et al., 2020; Wang et al., 2020) developed with different deep learning architec- to derive pseudo-labeled data for model training. tures such as CNN (Kim, 2014), RNN (You et al., Neverthel"
2021.naacl-main.335,D19-1404,0,0.347132,"lso be tagged. Taking the document in Fig. 1 as an example, humans can quickly identify this review text is clearly about “baby cereal” and “crafted cereal”, which are the core classes. After assigning these two most essential classes to the document, people continue to check the core classes’ ancestor classes and find “feeding” as well as “baby food” should be tagged. Motivated by the above human labeling process, we propose TaxoClass, a weakly-supervised HMTC framework including four major steps. First, we calculate the document-class similarity using a pre-trained textual entailment model (Yin et al., 2019). Second, we identify each document’s core classes by (1) selecting candidate core classes that are most similar to the document at each level in a top-down fashion, and (2) choosing hdocument, candidate core classi pairs that are salient across the whole unlabeled corpus. Third, we derive training data from document core classes and use them to train a text classifier. This classifier includes a document encoder based on pre-trained BERT (Devlin et al., 2019), a class encoder capturing class taxonomy structure, and a text matching network computing the probability of a document being tagged w"
2021.naacl-main.406,2020.emnlp-main.463,1,0.699932,"Missing"
2021.naacl-main.406,N18-1202,0,0.0510568,"arge models respectively, while achieving in growing a model to different capacities, just like comparable performances1 . its importance in deciding network architectures under specific budgets (Tan and Le, 2019). Here, 1 Introduction we show that growing a Transformer from both Thanks to the rapid increase of computing power, dimensions leads to better performance with less large-scale pre-training has been breaking the glass training cost, which verifies our intuition and shows ceiling for natural language processing tasks (Liu the potential of using compound growth operators et al., 2018; Peters et al., 2018; Devlin et al., 2019; in progressive BERT training. Liu et al., 2019; Brown et al., 2020). However, with Further, we explore the potential choices of great power comes great challenges: the required growth operators on each dimension. We conduct excessive computational consumption significantly controlled experiments and comprehensive analyimpedes the efficient iteration of both research ex- ses to compare various available solutions. These ploration and industrial application. To lower the analyses further guide the design of effective comtraining cost, many attempts have been made to pound"
2021.naacl-main.406,N19-1423,0,0.190031,"for progressive training. In each training stage t, the corresponding growth operator gt grows the model f . Then, f is updated by the optimizer opt before entering the next training step. Correspondingly, our goal is to maximize the final model performance after all training stages, which can be formulated as minimizing the empirical loss L over dataset D: min L(fT ) s.t. ft = opt (gt (ft−1 ), D) gt ∈G Compound Effect. Existing progressive training methods only focus on one model dimension. For example, Gong et al. (2019) conduct Transformer growth by gradually increasing the network depth. Devlin et al. (2019) use shorter input sequence length at early stages. However, as studies in network architecture search have revealed (Tan and Le, 2019), growth operators that balance different model dimensions can achieve better performance than single-dimensional operators under the same budget. Note that our objective (Equation 1) is close to the objective of EfficientNet (Tan and Le, 2019), which aims to find the optimal network architecture by maximizing the model accuracy for a given resource budget: d,w,r s.t. Resource_cost(N ) ≤ target_budget, where N (d, w, r) is a CNN network, d, w, r are coefficient"
2021.naacl-main.406,2020.emnlp-main.72,0,0.0619559,"Missing"
2021.naacl-main.406,P18-2124,0,0.0327256,"Missing"
2021.naacl-main.406,W18-5446,0,0.0482834,"Missing"
2021.naacl-main.69,2020.acl-main.703,0,0.400601,"final output. that describes the event with hargi placeholders. The generated output is a filled template where placeholders are replaced by concrete arguments. An example of the unfilled template from the ontology and the filled template for the event type Transaction.ExchangeBuySell 5 can be seen in Figure 2. Notably, one template per event type is given in the ontology, and does not require further human curation as opposed to the question designing process in question answering (QA) models (Du and Cardie, 2020; Feng et al., 2020). Our base model is an encoder-decoder language model (BART (Lewis et al., 2020), T5 (Raffel et al., 2020). The generation process models the conditional probability of selecting a new token given the previous tokens and the input to the encoder. p(x |c) = |x| Y p (xi |x&lt;i , c) (1) ground truth sequence is the filled template where the placeholder token is replaced by the argument span whenever possible. In the case where there are multiple arguments for the same slot, we connect the arguments with the word “and&quot;. The generation probability is computed by taking the dot product between the decoder output and the embeddings of tokens from the input.  p(xi = w|x&lt;i , c, t)"
2021.naacl-main.69,2020.emnlp-main.724,1,0.736281,"labeling and our model is an adaptation of TapNet (Yoon et al., 2019; Hou et al., 2020), which was designed for few-shot classification and later extended to Conditional Random Field (CRF) models. Compared with (Hou et al., 2020), we do not collapse the entries of the transition matrix, making it possible for our model to learn different probabilities for each event type. Since our model takes class keywords as input, we refer to this model as TAP K EY. For each event type, we first obtain a class representation vector ck based on given keywords using the masked category prediction method in (Meng et al., 2020). This class representation vector is an average over the BERT vector representations of the keywords, with some filtering applied to remove ambiguous occurrences. Details of the filtering process are included in Appendix A. Following the linear-chain CRF model, the probability of a tagged sequence is: log p(y|h; θ) ∝ X i ϕ(yi |hi ) + X ψ(yi |yi−1 , hi ) (5) i hi is the output of the embedding network (in our case, BERT-large) corresponding to xi . The label space for yi is the set of IO tags. We choose to use this simplified tagging scheme because it has fewer parameters and the fact that con"
2021.naacl-main.69,2020.emnlp-main.373,0,0.0345908,"Missing"
2021.naacl-main.69,N19-1423,0,0.0433191,"Missing"
2021.naacl-main.69,2020.acl-main.713,1,0.894878,"USTICE. 13 known and only annotation for these event types will be seen. We used two settings for selecting known types: 10 most frequent events types and 8 event types, one from each parent type of the event ontology. The evaluation is done on the complete set of event types. We refer the reader to Appendix C for implementation details and hyperparameter settings. 4.1 Datasets In addition to our dataset W IKI E VENTS, we also report the performance on the Automatic Content Extraction (ACE) 2005 dataset10 and the Roles Across Multiple Sentences (RAMS) dataset11 . We follow preprocessing from (Lin et al., 2020; Wadden et al., 2019) for the ACE dataset. 12 Statistics of the ACE data splits can be found in Table 3. RAMS (Ebner et al., 2020) is a recently released dataset with cross-sentence argument annotation. A 5-sentence window is provided for each event trigger and the closest argument span is annotated for each role. We follow the official data splits from Version 1.0. 4.2 Our experiments fall under three settings: (1) document-level event argument extraction; (2) document-level informative argument extraction and (3) zero-shot event extraction. For document-level event argument extraction we fo"
2021.naacl-srw.9,D18-2029,0,0.0202166,"on retriever but use Google USE for the passage retriever and the individual question retriever. We record the answer ai associated to the top-ranked question set qi as Answer 3. The pipeline in Figure 2 that goes from Input Corpus to Candidate Answers, QA Space, {Q}A Space and finally Answer 3 is a valid reader-retriever workflow. We denote this workflow as Reader-Retriever-{Q}A-Space. 3.2.2 Passage Retriever and QA Reader Given a query, the passage retriever uses the dot product of the query embedding and passage embedding vectors generated by Google Universal Sentence Encoder (Google USE) (Cer et al., 2018) to retrieve from the corpus a passage that is semantically most similar to the query. We then use BERT (Devlin et al., 2019), fine-tuned on SQuAD, to read the retrieved passage, predict the answer, and record the predicted answer as Answer 1. The pipeline in Figure 2 that goes from Input Corpus 3.2.5 Answer Aggregator Now that we have Answer 1, {Answer 2}, and Answer 3, the last step is to aggregate them into one single answer to return to the user. Our answer aggregation works as follows: if Answer 1 appears in the set {Answer 2}, then accept Answer 1 and return it; otherwise reject Answer 1"
2021.naacl-srw.9,P17-1171,0,0.130857,"One family, namely retriever-readers (Fig. 1, left branch), first retrieves from the corpus some documents or paragraphs that are likely to be relevant to the question, and then uses neural networks to read the retrieved passages and locate the answer. Another line of work, namely question answering using knowledge bases (abbreviated as QA using KB in this paper; Fig. 1, middle branch), first constructs a knowledge 2 2.1 Related Work Retriever-Readers Retriever-readers solve OpenQA by converting it to easier single-passage QA tasks. Examples of popular algorithms in this family include DrQA (Chen et al., 2017), which has a TF-IDF retriever followed by a recurrent neural network reader, and BERTserini (Yang et al., 2019), which consists of a BM25 retriever and a BERT reader. All retriever-readers face a trade-off between efficiency and accuracy. When the retriever module is 61 Proceedings of NAACL-HLT 2021: Student Research Workshop, pages 61–67 June 6–11, 2021. ©2021 Association for Computational Linguistics Figure 1: Retriever-readers (left), QA using KB (middle), and reader-retrievers (right). computationally efficient, the retrieved results are not very reliable, and the performance of the subse"
2021.naacl-srw.9,D16-1264,0,0.0347826,"agree at all. We denote the complete workflow depicted in Figure 2 as R6 . 4 greatest extent, so that we can make sure we correctly reproduce others’ work and do not put their models into disadvantages when comparing them with ours. More experimental details are available in Section 4.2. Although not critical to this study, using different datasets for training and testing has one additional benefit that it shows the ability of the systems to adapt to new corpora. Experiments We evaluate the OpenQA performance of our proposed method R6 and baseline methods using two public QA datasets, SQuAD (Rajpurkar et al., 2016) and TriviaQA (Joshi et al., 2017). We adopt a rather challenging setting that all trainable components of the models are trained on SQuAD, while the final models are tested on TriviaQA. Furthermore, we use TriviaQA in an open-domain setting by removing all annotated associations between questions and documents and enforcing the systems to answer every question with the entire corpus. We write TriviaQA-Open to distinguish such an opendomain setting from those officially adopted by TriviaQA. One may wonder why we choose to use different datasets for training and testing. Because our goal of the"
2021.naacl-srw.9,N19-1423,0,0.0156048,"ssociated to the top-ranked question set qi as Answer 3. The pipeline in Figure 2 that goes from Input Corpus to Candidate Answers, QA Space, {Q}A Space and finally Answer 3 is a valid reader-retriever workflow. We denote this workflow as Reader-Retriever-{Q}A-Space. 3.2.2 Passage Retriever and QA Reader Given a query, the passage retriever uses the dot product of the query embedding and passage embedding vectors generated by Google Universal Sentence Encoder (Google USE) (Cer et al., 2018) to retrieve from the corpus a passage that is semantically most similar to the query. We then use BERT (Devlin et al., 2019), fine-tuned on SQuAD, to read the retrieved passage, predict the answer, and record the predicted answer as Answer 1. The pipeline in Figure 2 that goes from Input Corpus 3.2.5 Answer Aggregator Now that we have Answer 1, {Answer 2}, and Answer 3, the last step is to aggregate them into one single answer to return to the user. Our answer aggregation works as follows: if Answer 1 appears in the set {Answer 2}, then accept Answer 1 and return it; otherwise reject Answer 1 and return Answer 3. In other words, the answer aggregator checks the consistency between the retrieverreader results and th"
2021.naacl-srw.9,N19-1237,0,0.0141348,"gator Given a corpus, a named entity recognition (NER) tool called TAGME (Ferragina and Scaiella, 2010, 2012) is applied to detect named entities from the corpus and link the entities to Wikipedia titles. Those entities form the set of candidate answers A in Definition 1. Then a question-generating (QG) reader is applied to the set of candidate answers to generate a question for each answer based on the local context. This reader features an encoderdecoder model structure with a question-answering reward and a question fluency reward tuned with policy gradient optimization (Yuan et al., 2017; Hosking and Riedel, 2019). Then we use a question aggregator to build the {Q}A Space by putting together all the questions with the same answer entity. Given a query, the aggregated question retriever uses the BM25 score (Robertson and Zaragoza, 2009) to retrieve from the {Q}A space the answer whose associated set of questions is most similar to the given query. We query the {Q}A Space by treating each qi as a single document which contains qi,j for all j as sentences. In practice, we observe that BM25 works better for long documents and Google USE works better for short passages. That is why we use BM25 as the aggreg"
2021.naacl-srw.9,N19-4013,0,0.0314758,"Missing"
2021.naacl-srw.9,N18-4017,0,0.012718,"lowed by a recurrent neural network reader, and BERTserini (Yang et al., 2019), which consists of a BM25 retriever and a BERT reader. All retriever-readers face a trade-off between efficiency and accuracy. When the retriever module is 61 Proceedings of NAACL-HLT 2021: Student Research Workshop, pages 61–67 June 6–11, 2021. ©2021 Association for Computational Linguistics Figure 1: Retriever-readers (left), QA using KB (middle), and reader-retrievers (right). computationally efficient, the retrieved results are not very reliable, and the performance of the subsequent reader is also constrained (Htut et al., 2018). On the other hand, there exist systems such as R3 (Wang et al., 2018) and DS-QA (Lin et al., 2018) that have sophisticated retrievers jointly trained with the readers, but they are computationally expensive and thus not scalable to large corpora (Das et al., 2019). the KB construction step to the graph query step, and how to handle questions whose answers do not fall within the KB schema. Due to those complexities, the community is observing a recent trend that retriever-readers are dominating the leaderboards of public QA datasets but KB-based methods are not. Therefore, we choose to focus"
2021.naacl-srw.9,W17-2603,0,0.0211891,"and Question Aggregator Given a corpus, a named entity recognition (NER) tool called TAGME (Ferragina and Scaiella, 2010, 2012) is applied to detect named entities from the corpus and link the entities to Wikipedia titles. Those entities form the set of candidate answers A in Definition 1. Then a question-generating (QG) reader is applied to the set of candidate answers to generate a question for each answer based on the local context. This reader features an encoderdecoder model structure with a question-answering reward and a question fluency reward tuned with policy gradient optimization (Yuan et al., 2017; Hosking and Riedel, 2019). Then we use a question aggregator to build the {Q}A Space by putting together all the questions with the same answer entity. Given a query, the aggregated question retriever uses the BM25 score (Robertson and Zaragoza, 2009) to retrieve from the {Q}A space the answer whose associated set of questions is most similar to the given query. We query the {Q}A Space by treating each qi as a single document which contains qi,j for all j as sentences. In practice, we observe that BM25 works better for long documents and Google USE works better for short passages. That is wh"
2021.naacl-srw.9,P17-1147,0,0.0260436,"orkflow depicted in Figure 2 as R6 . 4 greatest extent, so that we can make sure we correctly reproduce others’ work and do not put their models into disadvantages when comparing them with ours. More experimental details are available in Section 4.2. Although not critical to this study, using different datasets for training and testing has one additional benefit that it shows the ability of the systems to adapt to new corpora. Experiments We evaluate the OpenQA performance of our proposed method R6 and baseline methods using two public QA datasets, SQuAD (Rajpurkar et al., 2016) and TriviaQA (Joshi et al., 2017). We adopt a rather challenging setting that all trainable components of the models are trained on SQuAD, while the final models are tested on TriviaQA. Furthermore, we use TriviaQA in an open-domain setting by removing all annotated associations between questions and documents and enforcing the systems to answer every question with the entire corpus. We write TriviaQA-Open to distinguish such an opendomain setting from those officially adopted by TriviaQA. One may wonder why we choose to use different datasets for training and testing. Because our goal of the experiments is to compare the eff"
2021.repl4nlp-1.31,C18-1250,0,0.02166,"n et al., 2020), constrastive learning is used to learn better sentence representation (Giorgi et al., 2020) and pre-trained language model (Wu et al., 2020). Deep Network Memory Reduction Many existing techniques deal with large and deep models. The gradient checkpoint method attempts to emulate training deep networks by training shallower layers and connecting them with gradient checkpoints and re-computation (Chen et al., 2016). Some methods also use reversible activation functions, allowing internal activation in the network to be recovered throughout back propagation (Gomez et al., 2017; MacKay et al., 2018). However, their effectiveness as part of contrastive encoders has not been confirmed. Recent work also attempts to remove the redundancy in optimizer tracked parameters on each GPU (Rajbhandari et al., 2020). Compared with the aforementioned methods, our method is designed for scaling over the batch size dimension for contrastive learning. 3 L=− 1 X exp(f (si )> g(tri )/τ ) log P | |S| tj ∈T exp(f (si ) g(tj )/τ ) si ∈S (1) where each summation term depends on the entire set T and requires fitting all of them into memory. We set temperature τ = 1 in the following discussion for simplicity as"
2021.repl4nlp-1.31,N19-1423,0,0.10938,"Missing"
2021.repl4nlp-1.31,2020.emnlp-main.550,0,0.462797,"he batch at a time, leading to almost constant memory usage. 1 1 Introduction Contrastive learning learns to encode data into an embedding space such that related data points have closer representations and unrelated ones have further apart ones. Recent works in NLP adopt deep neural nets as encoders and use unsupervised contrastive learning on sentence representation (Giorgi et al., 2020), text retrieval (Lee et al., 2019), and language model pre-training tasks (Wu et al., 2020). Supervised contrastive learning (Khosla et al., 2020) has also been shown effective in training dense retrievers (Karpukhin et al., 2020; Qu et al., 2020). These works typically use batch-wise contrastive loss, sharing target texts as in-batch negatives. With such a technique, previous works have empirically shown that larger batches help learn better representations. However, computing loss and updating model parameters with respect 1 to a big batch require encoding all batch data and storing all activation, so batch size is limited by total available GPU memory. This limits application and research of contrastive learning methods under memory limited setup, e.g. academia. For example, Lee et al. (2019) pre-train a BERT (Devl"
2021.repl4nlp-1.31,Q19-1026,0,0.0284695,"- BSZ = 512 68.6 68.3 79.3 79.9 86.0 86.6 Time / Update (seconds) 150 100 50 0 1000 Table 1: Retrieval: We compare top-5/20/100 hit accuracy of small batch update (Sequential), accumulated small batch (Accumulation) and gradient cache (Cache) systems with DPR reference. Experiments To examine the reliability and computation cost of our method, we implement our method into dense passage retriever (DPR; Karpukhin et al. (2020))2 . We use gradient cache to compute DPR’s supervised contrastive loss on a single GPU. Following DPR paper, we measure top hit accuracy on the Natural Question Dataset (Kwiatkowski et al., 2019) for different methods. We then examine the training speed of various batch sizes. 4.1 Retrieval Accuracy Compared Systems 1) DPR: the reference number taken from the original paper trained on 8 GPUs, 2) Sequential: update with max batch size that fits into 1 GPU, 3) Accumulation: similar to Sequential but accumulate gradients and update until number of examples matches DPR setup, 4) Cache: training with DPR setup using our gradient cache on 1 GPU. We attempted to run with gradient checkpointing but found it cannot scale to standard DPR batch size on our hardware. Implementations All runs star"
2021.repl4nlp-1.31,P19-1612,0,0.193522,"backpropagation between contrastive loss and the encoder, removing encoder backward pass data dependency along the batch dimension. As a result, gradients can be computed for one subset of the batch at a time, leading to almost constant memory usage. 1 1 Introduction Contrastive learning learns to encode data into an embedding space such that related data points have closer representations and unrelated ones have further apart ones. Recent works in NLP adopt deep neural nets as encoders and use unsupervised contrastive learning on sentence representation (Giorgi et al., 2020), text retrieval (Lee et al., 2019), and language model pre-training tasks (Wu et al., 2020). Supervised contrastive learning (Khosla et al., 2020) has also been shown effective in training dense retrievers (Karpukhin et al., 2020; Qu et al., 2020). These works typically use batch-wise contrastive loss, sharing target texts as in-batch negatives. With such a technique, previous works have empirically shown that larger batches help learn better representations. However, computing loss and updating model parameters with respect 1 to a big batch require encoding all batch data and storing all activation, so batch size is limited b"
C10-1039,J98-3005,0,0.0119468,"ve summary such as ‘iPhone’s battery is cheap, lasts long but is bulky’ is a more complete summary, conveying all the necessary information. Extractive methods also tend to be verbose and this is especially problematic when the summaries need to be viewed on smaller screens like on a PDA. Thus, an informative and concise abstractive summary would be a better solution. Unfortunately, abstractive summarization is known to be difficult. Existing work in abstractive summarization has been quite limited and can be categorized into two categories: (1) approaches using prior knowledge (Radev and McKeown, 1998) (Finley and Harabagiu, 2002) (DeJong, 1982) and (2) approaches using Natural Language Generation (NLG) systems (Saggion and Lapalme, 2002) (Jing and McKeown, 2000). The first line of work requires considerable amount of manual effort to define schemas such as frames and templates that can be filled with the use of information extraction techniques. These systems were mainly used to summarize news articles. The second category of work uses deeper NLP analysis with special techniques for text regeneration. Both approaches either heavily rely on manual effort or are domain dependent. In this pap"
C10-1039,A00-2024,0,0.0118456,"also tend to be verbose and this is especially problematic when the summaries need to be viewed on smaller screens like on a PDA. Thus, an informative and concise abstractive summary would be a better solution. Unfortunately, abstractive summarization is known to be difficult. Existing work in abstractive summarization has been quite limited and can be categorized into two categories: (1) approaches using prior knowledge (Radev and McKeown, 1998) (Finley and Harabagiu, 2002) (DeJong, 1982) and (2) approaches using Natural Language Generation (NLG) systems (Saggion and Lapalme, 2002) (Jing and McKeown, 2000). The first line of work requires considerable amount of manual effort to define schemas such as frames and templates that can be filled with the use of information extraction techniques. These systems were mainly used to summarize news articles. The second category of work uses deeper NLP analysis with special techniques for text regeneration. Both approaches either heavily rely on manual effort or are domain dependent. In this paper, we propose a novel flexible summarization framework, Opinosis, that uses graphs to produce abstractive summaries of highly redundant opinions. In contrast with"
C10-1039,J02-4005,0,0.00661139,"tion. Extractive methods also tend to be verbose and this is especially problematic when the summaries need to be viewed on smaller screens like on a PDA. Thus, an informative and concise abstractive summary would be a better solution. Unfortunately, abstractive summarization is known to be difficult. Existing work in abstractive summarization has been quite limited and can be categorized into two categories: (1) approaches using prior knowledge (Radev and McKeown, 1998) (Finley and Harabagiu, 2002) (DeJong, 1982) and (2) approaches using Natural Language Generation (NLG) systems (Saggion and Lapalme, 2002) (Jing and McKeown, 2000). The first line of work requires considerable amount of manual effort to define schemas such as frames and templates that can be filled with the use of information extraction techniques. These systems were mainly used to summarize news articles. The second category of work uses deeper NLP analysis with special techniques for text regeneration. Both approaches either heavily rely on manual effort or are domain dependent. In this paper, we propose a novel flexible summarization framework, Opinosis, that uses graphs to produce abstractive summaries of highly redundant op"
C10-1039,E09-1059,0,0.0226584,"Missing"
C10-1039,N03-1020,0,0.0159995,"Missing"
C10-1039,W04-1013,0,0.173687,"rious products1 . Based on these reviews, 2 humans were asked to construct ‘opinion seeking’ queries which would consist of an entity name and a topic of interest. Example of such queries are: Amazon Kindle:buttons, Holiday Inn, Chicago: staff, and so on. We compiled a set of 51 such queries. We create one review document per query by collecting all review sentences that contain the query words for the given entity. Each review document thus consists of a set of unordered, redundant review sentences related to the query. There are approximately 100 sentences per review document. We use ROUGE (Lin, 2004b) to quantitatively assess the agreement of Opinosis summaries with human composed summaries. ROUGE is based on an n-gram co-occurrence between machine summaries and human summaries and is a widely accepted standard for evaluation of summarization tasks. In our experiments, we use ROUGE-1, ROUGE-2 and ROUGE-SU4 measures. ROUGE1 and ROUGE-2 have been shown to have most correlation with human summaries (Lin and Hovy, 2003) and higher order ROUGE-N scores (N &gt; 1) estimate the fluency of summaries. We use multiple reference (human) summaries in our evaluation since it can achieve better correlati"
C10-1039,W04-3252,0,\N,Missing
C10-1039,J02-4001,0,\N,Missing
C10-1039,P04-1035,0,\N,Missing
C10-1039,W00-0403,0,\N,Missing
C10-1039,N03-1003,0,\N,Missing
C10-1039,P08-1036,0,\N,Missing
C10-1039,C10-1037,0,\N,Missing
C10-1039,W02-1011,0,\N,Missing
C10-1039,N07-1038,0,\N,Missing
C12-1076,C10-1034,0,0.0603871,"a predefined threshold δ t t . Given its success when applied to sentence ranking for the task of extractive document summarization (Mihalcea, 2004), we choose TextRank as the baseline method to compute ranking scores in 1241 tweet-only networks where edges between tweets are determined by their cosine similarity. 2.3 Redundancy Removal Since users on Twitter can be tweeting similar information obliviously, and retweet and reply others’ tweets, redundancy has been shown to be a pervasive phenomenon (Zanzotto et al., 2011). This issue has not been considered in previous works on tweet ranking (Duan et al., 2010; Huang et al., 2011). In this work, we perform a redundancy removal step to diversify top ranked tweets. To do so, we adopt the widely used greedy procedure (Carterette and Chandar, 2009; McDonald, 2007) to apply redundancy removal after the completion of each ranking method, as follows: tweet t i in position i is removed when its cosine similarity with tweets t j ∈ [t 1 , t i−1 ] in more highly-ranked positions exceeds or equals a predefined threshold δ r ed 3 3 Motivations and Hypotheses Next, we describe the motivational aspects and hypotheses in this work, which we aim to prove. Hypothesi"
C12-1076,I11-1042,0,0.0868099,"old δ t t . Given its success when applied to sentence ranking for the task of extractive document summarization (Mihalcea, 2004), we choose TextRank as the baseline method to compute ranking scores in 1241 tweet-only networks where edges between tweets are determined by their cosine similarity. 2.3 Redundancy Removal Since users on Twitter can be tweeting similar information obliviously, and retweet and reply others’ tweets, redundancy has been shown to be a pervasive phenomenon (Zanzotto et al., 2011). This issue has not been considered in previous works on tweet ranking (Duan et al., 2010; Huang et al., 2011). In this work, we perform a redundancy removal step to diversify top ranked tweets. To do so, we adopt the widely used greedy procedure (Carterette and Chandar, 2009; McDonald, 2007) to apply redundancy removal after the completion of each ranking method, as follows: tweet t i in position i is removed when its cosine similarity with tweets t j ∈ [t 1 , t i−1 ] in more highly-ranked positions exceeds or equals a predefined threshold δ r ed 3 3 Motivations and Hypotheses Next, we describe the motivational aspects and hypotheses in this work, which we aim to prove. Hypothesis 1: Informative twee"
C12-1076,P04-3020,0,0.0190467,"homogeneous networks, which is defined as follows: s(vi ) = (1 − d) + d ∗ X v j ∈I n(vi ) w ji s(v j ) X w jk (1) vk ∈Out(v j ) where vi is a vertex with s(vi ) as the ranking score, I n(vi ) as the set of incoming edges, and Out(vi ) as the set of outgoing edges; w i j is the weight for the edge between two vertices vi and v j . An edge exists between two vertices that represent text units when their computed shared content (cosine similarity) exceeds or equals a predefined threshold δ t t . Given its success when applied to sentence ranking for the task of extractive document summarization (Mihalcea, 2004), we choose TextRank as the baseline method to compute ranking scores in 1241 tweet-only networks where edges between tweets are determined by their cosine similarity. 2.3 Redundancy Removal Since users on Twitter can be tweeting similar information obliviously, and retweet and reply others’ tweets, redundancy has been shown to be a pervasive phenomenon (Zanzotto et al., 2011). This issue has not been considered in previous works on tweet ranking (Duan et al., 2010; Huang et al., 2011). In this work, we perform a redundancy removal step to diversify top ranked tweets. To do so, we adopt the wi"
C12-1076,D11-1061,0,0.0804207,"represent text units when their computed shared content (cosine similarity) exceeds or equals a predefined threshold δ t t . Given its success when applied to sentence ranking for the task of extractive document summarization (Mihalcea, 2004), we choose TextRank as the baseline method to compute ranking scores in 1241 tweet-only networks where edges between tweets are determined by their cosine similarity. 2.3 Redundancy Removal Since users on Twitter can be tweeting similar information obliviously, and retweet and reply others’ tweets, redundancy has been shown to be a pervasive phenomenon (Zanzotto et al., 2011). This issue has not been considered in previous works on tweet ranking (Duan et al., 2010; Huang et al., 2011). In this work, we perform a redundancy removal step to diversify top ranked tweets. To do so, we adopt the widely used greedy procedure (Carterette and Chandar, 2009; McDonald, 2007) to apply redundancy removal after the completion of each ranking method, as follows: tweet t i in position i is removed when its cosine similarity with tweets t j ∈ [t 1 , t i−1 ] in more highly-ranked positions exceeds or equals a predefined threshold δ r ed 3 3 Motivations and Hypotheses Next, we descr"
C12-1076,W04-3252,0,\N,Missing
C12-1076,J92-4003,0,\N,Missing
C14-1149,R13-1051,0,0.0579518,"Missing"
C14-1149,P14-1038,1,0.758714,"rases and their links. It must contain one query entity node and one or more slot filler nodes. The annotation of a node includes its entity type, subtype, mention type, referent entities, and semantic category (though not every node has each type of annotation). The annotation of a link includes a dependency label and/or a semantic relation between the two linked nodes. The knowledge graph is constructed using the following procedure. First, we annotate the evidence text using dependency parsing (Marneffe et al., 2006) and Information Extraction (entity, relation and event) (Li et al., 2013; Li and Ji, 2014). Two nodes are linked if they are deemed related by one of the annotation methods (e.g., [Mays, 50] is labeled with the dependency type amod, and [home, Tampa] is labeled with the semantic relation located in). The annotation output is often in terms of syntactic heads. Thus, we extend the boundaries of entity, time, and value mentions (e.g., people’s titles) to include an entire phrase where possible. We then enrich each node with annotation for entity type, subtype and mention type. Entity type and subtype refer to the role played by the entity in the world, the latter being more fine-grain"
C14-1149,P13-1008,1,0.422759,"tity mentions, phrases and their links. It must contain one query entity node and one or more slot filler nodes. The annotation of a node includes its entity type, subtype, mention type, referent entities, and semantic category (though not every node has each type of annotation). The annotation of a link includes a dependency label and/or a semantic relation between the two linked nodes. The knowledge graph is constructed using the following procedure. First, we annotate the evidence text using dependency parsing (Marneffe et al., 2006) and Information Extraction (entity, relation and event) (Li et al., 2013; Li and Ji, 2014). Two nodes are linked if they are deemed related by one of the annotation methods (e.g., [Mays, 50] is labeled with the dependency type amod, and [home, Tampa] is labeled with the semantic relation located in). The annotation output is often in terms of syntactic heads. Thus, we extend the boundaries of entity, time, and value mentions (e.g., people’s titles) to include an entire phrase where possible. We then enrich each node with annotation for entity type, subtype and mention type. Entity type and subtype refer to the role played by the entity in the world, the latter bei"
C14-1149,de-marneffe-etal-2006-generating,0,0.00418774,"nd [Mays, per: age, 50]. Formally, a knowledge graph is an annotated graph of entity mentions, phrases and their links. It must contain one query entity node and one or more slot filler nodes. The annotation of a node includes its entity type, subtype, mention type, referent entities, and semantic category (though not every node has each type of annotation). The annotation of a link includes a dependency label and/or a semantic relation between the two linked nodes. The knowledge graph is constructed using the following procedure. First, we annotate the evidence text using dependency parsing (Marneffe et al., 2006) and Information Extraction (entity, relation and event) (Li et al., 2013; Li and Ji, 2014). Two nodes are linked if they are deemed related by one of the annotation methods (e.g., [Mays, 50] is labeled with the dependency type amod, and [home, Tampa] is labeled with the semantic relation located in). The annotation output is often in terms of syntactic heads. Thus, we extend the boundaries of entity, time, and value mentions (e.g., people’s titles) to include an entire phrase where possible. We then enrich each node with annotation for entity type, subtype and mention type. Entity type and su"
C14-1149,P04-3020,0,0.0192256,"between ri and tk when response ri is provided by system tk . Credibility Initialization Each source is represented as a combination of publication venue and genre. The credibility scores of sources S are initialized uniformly as n1 , where n is the number of sources. Given the set of systems T = {t1 , . . . , tl }, we initialize their credibility scores c0 (t) based on their interactions on the predicted responses. Suppose each system ti generates a set of responses Rti . The similarity between two systems ti and tj is defined as similarity(ti , tj ) = |Rti ∩Rtj | log (|Rti |)+log (|Rtj |) (Mihalcea, 2004). Then we construct a weighted undirected graph G = hT, Ei, where T (G) = {t1 , . . . , tl } and E(G) = {hti , tj i}, hti , tj i = similarity(ti , tj ), and apply the TextRank algorithm (Mihalcea, 2004) on G to obtain c0 (t). We got negative results by initializing system credibility scores uniformly. We also got negative results by initializing system credibility scores using system metadata, such as the algorithms and resources the system used at each step, its previous performance in benchmark tests, and the confidence values it produced for its responses. We found the quality of an SF syst"
C14-1149,P09-1113,0,0.0716101,"Mays amod nsubj {GPE.Population-Center.NAM, FL-USA} 【 Per:place_of_death】 aux Tampa {Death-Trigger} prep_in had located_in nn prep_of died sleep prep_at home poss {FAC.Building-Grounds.NOM} poss June,28 his {PER.Individual.PRO, Mays} {06/28/2009, TIME-WITHIN} 【 per:date_of_death】 Figure 2: Knowledge Graph Example. number of trigger phrases for each slot type by mapping various knowledge bases, including Wikipedia Infoboxes, Freebase (Bollacker et al., 2008), DBPedia (Auer et al., 2007) and YAGO (Suchanek et al., 2007), into the Gigaword corpus3 and Wikipedia articles via distant supervision (Mintz et al., 2009)4 . Each intermediate node in the knowledge graph that matches a trigger phrase is then assigned a corresponding semantic category. For example, “died” in Fig. 2 is labeled a Death-Trigger. 4.3 Knowledge Graph-Based Verification We design linguistic indicators in terms of the properties of nodes and paths that are likely to be bear on the response’s veracity. Formally, a path consists of the list of nodes and links that must be traversed along a route from a query node to a slot filler node. Node indicators contribute information about a query entity or slot filler node in isolation, that may"
C14-1149,C10-1099,0,0.021436,"Missing"
C16-1224,radev-etal-2002-evaluating,0,0.176867,"Missing"
C16-1224,D13-1044,0,0.0766924,"Missing"
C16-1224,C16-1038,0,0.0246464,"ss the efficiency issue, ranging from developing fast training hardwares (Chilimbi et al., 2014) to parameter sharing and simplifying model structures, in comparison to standard simple IR approaches (Tao et al., 2007; Bendersky et al., 2010; Buttcher et al., 2006), the training efficiency and scalability of these methods still significantly lack behind. This issue is particularly significant when we have to adapt the model repeatedly for new domains. While domain-adaptation techniques have been proposed for many deep learning models (Chopra et al., 2013; Zhou et al., 2012; Ganin et al., 2015; Sun et al., 2016), the focus and starting point of these work have largely been on how to adjust the models for accuracy, rather than from an accuracy and scalability point of view. 2379 Figure 1: Model architecture for simplified deep learning model (a.k.a. fast model) 3 Methods In this section, we describe our proposed hybrid model, FastHybrid, for effective and efficient answer selection. It combines a fast (deep) model1 with an initial IR model to create an efficient and effective overall structure for this task. A major area that distinguishes our work from the past deep learning models is that we directl"
C16-1224,P16-1044,1,0.903559,"ral QA datasets. Experimental results show that although the hybrid uses no training data, its accuracy is often on-par with supervised deep learning techniques, while significantly reducing training and tuning costs across different domains. 1 Introduction Open-domain question answering (QA) aims to serve a user’s information request by returning a list of direct answers. This problem has been receiving an increasingly amount of attention in the NLP and machine learning communities in the recent years (Ferrucci et al., 2012; Etzioni et al., 2011). Answer sentence selection (Yih et al., 2013; Tan et al., 2016; Yu et al., 2014; Severyn et al., 2013), which, given a user question, returns the correct sentences that contain the exact answer, is a core component in QA systems. The performance of QA systems critically depends on choosing the right candidate sentences which facilitate the extraction of final answers. To be successful, in addition to accuracy, real-world systems must be scalable and select the most accurate answer sentences in a short amount of time. However, accuracy and speed/scalability are competing forces that often counteract each other. It is often the case that methods developed"
C16-1224,voorhees-tice-2000-trec,0,0.22128,"Missing"
C16-1224,P15-2116,0,0.0408237,"Missing"
C16-1224,D15-1237,0,0.0166246,"l or return the IR results. It is beyond the scope of our paper to detail the QPP approach; we refer interested readers to (He et al., 2005) for more details about query performance prediction. Finally, we would like to point out the entire pipeline is hyper-parameter free, relieving the need for expensive hyper-parameters tuning. This property allows our model to work with new domains and datasets more seamlessly – reducing the workload in model tuning. 4 Experiments In this section we present a comprehensive set of experiments over three QA datasets: WikiQA, TrecQA, and InsuranceQA. WikiQA (Yang et al., 2015) is an open domain question-answering dataset. We use the subtask that assumes that there is at least one correct answer for a question. The TrecQA dataset was created based on TREC QA task (8-13) data (Voorhees et al., 2000). We follow the exact approach of train/dev/test question selections as in (Wang et al., 2015). InsuranceQA(v2)2 is a recently released 2 git clone https://github.com/shuzi/insuranceQA.git 2382 Train (# questions) Dev (# questions) Test (# questions) Avg # cand answers WikiQA 873 126 243 9 TrecQA 1162 65 68 38 InsuranceQA(v2) 12,889 2000 2000 500 Table 1: Dataset statistic"
C16-1224,P13-1171,0,0.193766,"ectiveness on several QA datasets. Experimental results show that although the hybrid uses no training data, its accuracy is often on-par with supervised deep learning techniques, while significantly reducing training and tuning costs across different domains. 1 Introduction Open-domain question answering (QA) aims to serve a user’s information request by returning a list of direct answers. This problem has been receiving an increasingly amount of attention in the NLP and machine learning communities in the recent years (Ferrucci et al., 2012; Etzioni et al., 2011). Answer sentence selection (Yih et al., 2013; Tan et al., 2016; Yu et al., 2014; Severyn et al., 2013), which, given a user question, returns the correct sentences that contain the exact answer, is a core component in QA systems. The performance of QA systems critically depends on choosing the right candidate sentences which facilitate the extraction of final answers. To be successful, in addition to accuracy, real-world systems must be scalable and select the most accurate answer sentences in a short amount of time. However, accuracy and speed/scalability are competing forces that often counteract each other. It is often the case that"
D16-1144,D15-1103,0,0.175276,"em as multi-class classification following the type mutual exclusion assumption (i.e., one type per mention) (Nadeau and Sekine, 2007). Recent work has focused on a much larger set of fine-grained types (Yosef et al., 2012; Ling and Weld, 2012). As the type mutual exclusion assumption no longer holds, they cast the problem as multilabel multi-class (hierarchical) classification problems (Gillick et al., 2014; Yosef et al., 2012; Ling and Weld, 2012). Embedding techniques are also recently applied to jointly learn feature and type representations (Yogatama et al., 2015; Dong et al., 2015). Del Corro et al. (2015) proposed an unsupervised method to generate context-aware candidates types, and subsequently select the most appropriate type. Gillick et al. (2014) discuss the label noise issue in fine-grained typing and propose three pruning heuristics. However, these heuristics aggressively delete training examples and may suffer from low recall (see Table. 4). In the context of distant supervision, label noise issue has been studied for other information extraction tasks such as relation extraction (Takamatsu et al., 2012). In relation extraction, label noise is introduced by the false positive textual m"
D16-1144,E14-4040,0,0.0180677,"Missing"
D16-1144,P05-1045,0,0.0126774,"D consists of a set of extracted entity mentions M = {mi }N i=1 , the context (e.g., sentence, paragraph) of each mention {ci }N i=1 , and the candiN date type sets {Yi }i=1 for each mention. We repre N sent D using a set of triples D = (mi , ci , Yi ) i=1 . Problem Description. For each test mention, we aim to predict the correct type-path in Y based on the mention’s context. More specifically, the test set T is defined as a set of mention-context pairs (m, c), where mentions in T (denoted as Mt ) are extracted from their sentences using existing extractors such as named entity recognizer (Finkel et al., 2005). We denote the gold type-path for a test mention m as Y ∗ . This work focuses on learning a typing model from the noisy training corpus D, and estimating Y ∗ from Y for each test mention m (in set Mt ), based on mention m, its context c, and the learned model. Framework Overview. At a high level, the AFET framework (see also Fig. 2) learns low-dimensional representations for entity types and text features, and infers type-paths for test mentions using the learned embeddings. It consists of the following steps: Knowledge Base 1. Extract text features for entity mentions in training set M and t"
D16-1144,P08-1030,1,0.634711,".. author actor Candidate Type Set (Sub-tree) singer ... Knowledge Bases Distant Supervision Entity: Arnold Schwarzenegger Figure 1: Current systems may detect Arnold Schwarzenegger in sentences S1-S3 and assign the same types to all (listed within braces), when only some types are correct for context (blue labels within braces). Assigning types (e.g., person, organization) to mentions of entities in context is an important task in natural language processing (NLP). The extracted entity type information can serve as primitives for relation extraction (Mintz et al., 2009) and event extraction (Ji and Grishman, 2008), and assists a wide range of downstream applications including knowledge base (KB) completion (Dong et al., 2014), question answering (Lin et al., 2012) and entity recommendation (Yu et al., 2014). While ∗ person ... Noisy Training Examples Introduction 1 product Equal contribution. Codes and datasets used in this paper can be downloaded at https://github.com/shanzhenren/AFET. traditional named entity recognition systems (Ratinov and Roth, 2009; Nadeau and Sekine, 2007) focus on a small set of coarse types (typically fewer than 10), recent studies (Ling and Weld, 2012; Yosef et al., 2012) wor"
D16-1144,D12-1082,0,0.0335026,"Missing"
D16-1144,P14-5010,0,0.0110603,"o similar sets of entities should be more related to each other than those assigned to quite different entities (Jiang et al., 2015) (e.g., actor is Feature Head Token POS Character Word Shape Length Context Brown Cluster Dependency Description Syntactic head token of the mention Tokens in the mention Part-of-Speech tag of tokens in the mention All character trigrams in the head of the mention Word shape of the tokens in the mention Number of tokens in the mention Unigrams/bigrams before and after the mention Brown cluster ID for the head token (learned using D) Stanford syntactic dependency (Manning et al., 2014) associated with the head token Example “HEAD Turing” “Turing”, “Machine” “NN” “:tu”, “tur”, ..., “ng:” “Aa” for “Turing” “2” “CXT B:Maserati ,”, “CXT A:and the” “4 1100”, “8 1101111”, “12 111011111111” “GOV:nn”, “GOV:turing” Table 2: Text features used in this paper. “Turing Machine” is used as an example mention from “The band’s former drummer Jerry Fuchs—who was also a member of Maserati, Turing Machine and The Juan MacLean—died after falling down an elevator shaft.”. more related to director than to author in the left column of Fig. 3). Thus, type correlation between yk and yk0 (denoted as"
D16-1144,P09-1113,0,0.0309695,"thete politician artist ... business man ... author actor Candidate Type Set (Sub-tree) singer ... Knowledge Bases Distant Supervision Entity: Arnold Schwarzenegger Figure 1: Current systems may detect Arnold Schwarzenegger in sentences S1-S3 and assign the same types to all (listed within braces), when only some types are correct for context (blue labels within braces). Assigning types (e.g., person, organization) to mentions of entities in context is an important task in natural language processing (NLP). The extracted entity type information can serve as primitives for relation extraction (Mintz et al., 2009) and event extraction (Ji and Grishman, 2008), and assists a wide range of downstream applications including knowledge base (KB) completion (Dong et al., 2014), question answering (Lin et al., 2012) and entity recommendation (Yu et al., 2014). While ∗ person ... Noisy Training Examples Introduction 1 product Equal contribution. Codes and datasets used in this paper can be downloaded at https://github.com/shanzhenren/AFET. traditional named entity recognition systems (Ratinov and Roth, 2009; Nadeau and Sekine, 2007) focus on a small set of coarse types (typically fewer than 10), recent studies"
D16-1144,W09-1119,0,0.0432782,"age processing (NLP). The extracted entity type information can serve as primitives for relation extraction (Mintz et al., 2009) and event extraction (Ji and Grishman, 2008), and assists a wide range of downstream applications including knowledge base (KB) completion (Dong et al., 2014), question answering (Lin et al., 2012) and entity recommendation (Yu et al., 2014). While ∗ person ... Noisy Training Examples Introduction 1 product Equal contribution. Codes and datasets used in this paper can be downloaded at https://github.com/shanzhenren/AFET. traditional named entity recognition systems (Ratinov and Roth, 2009; Nadeau and Sekine, 2007) focus on a small set of coarse types (typically fewer than 10), recent studies (Ling and Weld, 2012; Yosef et al., 2012) work on a much larger set of fine-grained types (usually over 100) which form a tree-structured hierarchy (see the blue region of Fig. 1). Fine-grained typing allows one mention to have multiple types, which together constitute a type-path (not necessarily ending in a leaf node) in the given type hierarchy, depending on the local context (e.g., sentence). Consider the example in Fig. 1, “Arnold Schwarzenegger” could be labeled as {person, businessm"
D16-1144,P12-1076,0,0.047099,"learn feature and type representations (Yogatama et al., 2015; Dong et al., 2015). Del Corro et al. (2015) proposed an unsupervised method to generate context-aware candidates types, and subsequently select the most appropriate type. Gillick et al. (2014) discuss the label noise issue in fine-grained typing and propose three pruning heuristics. However, these heuristics aggressively delete training examples and may suffer from low recall (see Table. 4). In the context of distant supervision, label noise issue has been studied for other information extraction tasks such as relation extraction (Takamatsu et al., 2012). In relation extraction, label noise is introduced by the false positive textual matches of entity pairs. In entity typing, however, label noise comes from the assignment of types to entity mentions without considering their contexts. The forms 1377 of distant supervision are different in these two problems. Recently, (Ren et al., 2016b) has tackled the problem of label noise in fine-grained entity typing, but focused on how to generate a clean training set instead of doing entity typing. Partial label learning (PLL) (Zhang, 2014; Nguyen and Caruana, 2008; Cour et al., 2011) deals with the pr"
D16-1144,P15-2048,0,0.476766,"didate type set of each mention, all KB types of its KB-linked entity. However, existing distant supervision methods encounter the following limitations when doing automatic fine-grained typing. • Noisy Training Labels. Current practice of distant supervision may introduce label noise to training data since it fails to take a mention’s local contexts into account when assigning type labels (e.g., see Fig. 1). Many previous studies ignore the label noises which appear in a majority of training mentions (see Table. 1, row (1)), and assume all types obtained by distant supervision are “correct” (Yogatama et al., 2015; Ling and Weld, 2012). The noisy labels may mislead the trained models and cause negative effect. A few systems try to denoise the training corpora using simple pruning heuristics such as deleting mentions with conflicting types (Gillick et al., 2014). However, such strategies significantly reduce the size of training set (Table 1, rows (2a-c)) and lead to performance degradation (later shown in our experiments). The larger the target type set, the more severe the loss. • Type Correlation. Most existing methods (Yogatama et al., 2015; Ling and Weld, 2012) treat every type label in a training"
D16-1144,C12-2133,0,0.474884,"(Ji and Grishman, 2008), and assists a wide range of downstream applications including knowledge base (KB) completion (Dong et al., 2014), question answering (Lin et al., 2012) and entity recommendation (Yu et al., 2014). While ∗ person ... Noisy Training Examples Introduction 1 product Equal contribution. Codes and datasets used in this paper can be downloaded at https://github.com/shanzhenren/AFET. traditional named entity recognition systems (Ratinov and Roth, 2009; Nadeau and Sekine, 2007) focus on a small set of coarse types (typically fewer than 10), recent studies (Ling and Weld, 2012; Yosef et al., 2012) work on a much larger set of fine-grained types (usually over 100) which form a tree-structured hierarchy (see the blue region of Fig. 1). Fine-grained typing allows one mention to have multiple types, which together constitute a type-path (not necessarily ending in a leaf node) in the given type hierarchy, depending on the local context (e.g., sentence). Consider the example in Fig. 1, “Arnold Schwarzenegger” could be labeled as {person, businessman} in S3 (investment). But he could also be labeled as {person, politician} in S1 or {person, artist, actor} in S2. Such fine-grained type represe"
D17-1005,P14-1091,0,0.0127696,"d on reliabilities of labeling functions, which are calculated with their proficient subsets’ representations. Then, these inferred true labels would serve as supervision for all components, including context representation, true label discovery and relation extraction. Besides, the context representation bridges relation extraction with true label dis2 Preliminaries In this section, we would formally define relation extraction and heterogeneous supervision, including the format of labeling functions. 47 fc vi zc li 2.1 Relation Extraction Here we conduct relation extraction in sentencelevel (Bao et al., 2014). For a sentence d, an entity mention is a token span in d which represents an entity, and a relation mention is a triple (e1 , e2 , d) which consists of an ordered entity pair (e1 , e2 ) and d. And the relation extraction task is to categorize relation mentions into a given set of relation types R, or Not-Target-Type (None) which means the type of the relation mention does not belong to R. oc,i o∗c ρc,i Si sc,i ti Table 1: Notation Table. into a set of relation types. Intuitively, errors of annotations (O) come from mismatch of contexts, e.g., in Fig. 1, λ1 annotates c1 and c2 with ’true’ lab"
D17-1005,J92-4003,0,0.206259,"scovery model by maximizing JT . 50 features and conduct the text features’ representation learning. After calculating the representation of c, we would infer its true label o∗c based on our true label discovery model, and finally update model parameters based on o∗c . Kind Pattern KB Wiki-KBP #Types #LF 13 147 7 7 NYT #Types #LF 16 115 25 26 Table 4: Number of labeling functions and the relation types they can annotated w.r.t. two kinds of information 3.5 Relation Type Inference CoreNLP tool (Manning et al., 2014) to generate entity mentions and get POS tags for both datasets. Brown clusters(Brown et al., 1992) are derived for each corpus using public implementation2 . All these features are shared with all compared methods in our experiments. We now discuss the strategy of performing type inference for Cu . As shown in Table 3, the proportion of None in Cu is usually much larger than in Cl . Additionally, not like other relation types in R, None does not have a coherent semantic meaning. Similar to (Ren et al., 2016), we introduce a heuristic rule: identifying a relation mention as None when (1) our relation extractor predict it as None, or (2) the entropy of p(.|zc ) over R exceeds a pre-defined t"
D17-1005,P07-1073,0,0.0521594,"he discriminative module in label level, while the discriminative module influences the true label discovery module by selecting a feature subset. Although delicately designed, it fails to make full use of the connection between these modules, i.e., not refine the context representation for classifier. Thus, its discriminative module might suffer from the overwhelming size of text features. 5.1 Relation Extraction Relation extraction aims to detect and categorize semantic relations between a pair of entities. To alleviate the dependency of annotations given by human experts, weak supervision (Bunescu and Mooney, 2007; Etzioni et al., 2004) and distant supervision (Ren et al., 2016) have been employed to automatically generate annotations based on knowledge base (or seed patterns/instances). Universal Schemas (Riedel et al., 2013; Verga et al., 2015; Toutanova et al., 2015) has been proposed to unify patterns and knowledge base, but it’s designed for document-level relation extraction, i.e., not to categorize relation types based on a specific context, but based on the whole corpus. Thus, it allows one relation mention to have multiple true relation types; and does not fit our scenario very well, which is"
D17-1005,P14-5010,0,0.00210619,"f ρc,i = δ(oc,i = o∗c ). Thus, we would first infer o∗c = argmaxo∗c JT , then train the true label discovery model by maximizing JT . 50 features and conduct the text features’ representation learning. After calculating the representation of c, we would infer its true label o∗c based on our true label discovery model, and finally update model parameters based on o∗c . Kind Pattern KB Wiki-KBP #Types #LF 13 147 7 7 NYT #Types #LF 16 115 25 26 Table 4: Number of labeling functions and the relation types they can annotated w.r.t. two kinds of information 3.5 Relation Type Inference CoreNLP tool (Manning et al., 2014) to generate entity mentions and get POS tags for both datasets. Brown clusters(Brown et al., 1992) are derived for each corpus using public implementation2 . All these features are shared with all compared methods in our experiments. We now discuss the strategy of performing type inference for Cu . As shown in Table 3, the proportion of None in Cu is usually much larger than in Cl . Additionally, not like other relation types in R, None does not have a coherent semantic meaning. Similar to (Ren et al., 2016), we introduce a heuristic rule: identifying a relation mention as None when (1) our r"
D17-1005,P09-1113,0,0.942466,"belongs to λi ’s proficient subset relation type embedding for ri ∈ R 2. Text feature embeddings are utilized to calculate relation mention embeddings (see Fig. 2); 3. With relation mention embeddings, true labels are inferred by calculating labeling functions’ reliabilities in a context-aware manner (see Fig. 1); 4. Inferred true labels would ‘supervise’ all components to learn model parameters (see Fig. 1). We now proceed by introducing these components of the model in further details. 3.1 Modeling Relation Mention As shown in Table 2, we extract abundant lexical features (Ren et al., 2016; Mintz et al., 2009) to characterize relation mentions. However, this abundance also results in the gigantic dimension of original text features (∼ 107 in our case). In The REH ESSION Framework Here, we present REH ESSION, a novel framework to infer true labels from automatically generated noisy labels, and categorize unlabeled instances 48 Feature Entity mention (EM) head Entity Mention Token Tokens between two EMs Part-of-speech (POS) tag Collocations Entity mention order Entity mention distance Body entity mentions numbers Entity mention context Brown cluster (learned on D) Description Syntactic head token of"
D17-1005,C10-1099,0,0.0117953,"or machine; DSL (Mintz et al., 2009) trains a multi-class logistic classifier4 on the training data; MultiR (Hoffmann et al., 2011) models training label noise by multi-instance multi-label learning; FCM (Gormley et al., 2015) performs compositional embedding by neural language model. CoType-RM (Ren et al., 2016) adopts partial-label loss to handle label noise and train the extractor. Moreover, two different strategies are adopted to feed heterogeneous supervision to these methods. The first is to keep all noisy labels, marked as ‘NL’. Alternatively, a true label discovery method, Investment (Pasternack and Roth, 2010), is applied to resolve conflicts, which is based on the source consistency assumption and iteratively updates inferred true labels and label functions’ reliabilities. Then, the second strategy is to only feed the inferred true labels, referred as ‘TD’. For relation classification task, which excludes None type from training / testing, we use the classification accuracy (Acc) for evaluation, and for relation extraction task, precision (Prec), recall (Rec) and F1 score (Bunescu and Mooney, 2005; Bach and Badaskar, 2007) are employed. Notice that both relation extraction and relation classificat"
D17-1005,D15-1205,0,0.0199289,"nts, and Universal Schemas is listed as a baseline in Sec. 4.4. Indeed, it performs similarly to the Investment method. Table 6: Number of relation mentions (RM), relation mentions annotated as None, relation mentions with conflicting annotations and conflicts involving None learning with Perceptron algorithm. BFK (Bunescu and Mooney, 2005) applies bag-offeature kernel to train a support vector machine; DSL (Mintz et al., 2009) trains a multi-class logistic classifier4 on the training data; MultiR (Hoffmann et al., 2011) models training label noise by multi-instance multi-label learning; FCM (Gormley et al., 2015) performs compositional embedding by neural language model. CoType-RM (Ren et al., 2016) adopts partial-label loss to handle label noise and train the extractor. Moreover, two different strategies are adopted to feed heterogeneous supervision to these methods. The first is to keep all noisy labels, marked as ‘NL’. Alternatively, a true label discovery method, Investment (Pasternack and Roth, 2010), is applied to resolve conflicts, which is based on the source consistency assumption and iteratively updates inferred true labels and label functions’ reliabilities. Then, the second strategy is to"
D17-1005,P11-1055,0,0.170271,"well. Due to the constraint of space, we only compared our method to Investment in most experiments, and Universal Schemas is listed as a baseline in Sec. 4.4. Indeed, it performs similarly to the Investment method. Table 6: Number of relation mentions (RM), relation mentions annotated as None, relation mentions with conflicting annotations and conflicts involving None learning with Perceptron algorithm. BFK (Bunescu and Mooney, 2005) applies bag-offeature kernel to train a support vector machine; DSL (Mintz et al., 2009) trains a multi-class logistic classifier4 on the training data; MultiR (Hoffmann et al., 2011) models training label noise by multi-instance multi-label learning; FCM (Gormley et al., 2015) performs compositional embedding by neural language model. CoType-RM (Ren et al., 2016) adopts partial-label loss to handle label noise and train the extractor. Moreover, two different strategies are adopted to feed heterogeneous supervision to these methods. The first is to keep all noisy labels, marked as ‘NL’. Alternatively, a true label discovery method, Investment (Pasternack and Roth, 2010), is applied to resolve conflicts, which is based on the source consistency assumption and iteratively up"
D17-1005,C14-1220,0,0.0507984,"on to have multiple true relation types; and does not fit our scenario very well, which is sentence-level relation extraction and assumes one instance has only one relation type. Here we propose a more general framework to consolidate heterogeneous information and further refine the true label from noisy labels, which gives the relation extractor potential to detect more types of relations in a more precise way. Word embedding has demonstrated great potential in capturing semantic meaning (Mikolov et al., 2013), and achieved great success in a wide range of NLP tasks like relation extraction (Zeng et al., 2014; Takase and Inui, 2016; Nguyen and Grishman, 2015). In our model, we employed the embedding techniques to represent context information, and reduce the dimension of text features, which allows our model to generalize better. 6 Conclusion and Future Work In this paper, we propose REH ESSION, an embedding framework to extract relation under heterogeneous supervision. When dealing with heterogeneous supervisions, one unique challenge is how to resolve conflicts generated by different labeling functions. Accordingly, we go beyond the “source consistency assumption” in prior works and leverage con"
D17-1005,N13-1008,0,0.165874,"0.3325 0.3360 0.1964 0.5645 0.2914 0.3499 0.3923 0.3107 0.5368 0.3879 0.5726 0.4792 0.3677 0.4933 0.4208 Relation Classification NYT Wiki-KBP Accuracy Accuracy 0.6598 0.6226 0.6905 0.5000 0.7954 0.6355 0.7059 0.6484 0.7033 0.5419 0.6485 0.6935 0.7059 0.6355 0.6292 0.5032 0.7570 0.6452 0.6061 0.6613 0.6803 0.5645 0.6409 0.6890 0.8381 0.7277 Table 5: Performance comparison of relation extraction and relation classification Dataset Total Number of RM RM annotated as None RM with conflicts Conflicts involving None Wiki-KBP 225977 100521 32008 30559 NYT 530767 356497 58198 38756 Universal Schemas (Riedel et al., 2013) is proposed to unify different information by calculating a low-rank approximation of the annotations O. It can serve as an alternative of the Investment method, i.e., selecting the relation type with highest score in the low-rank approximation as the true type. But it doesnt explicitly model noise and not fit our scenario very well. Due to the constraint of space, we only compared our method to Investment in most experiments, and Universal Schemas is listed as a baseline in Sec. 4.4. Indeed, it performs similarly to the Investment method. Table 6: Number of relation mentions (RM), relation m"
D17-1005,D15-1174,0,0.0208116,"he context representation for classifier. Thus, its discriminative module might suffer from the overwhelming size of text features. 5.1 Relation Extraction Relation extraction aims to detect and categorize semantic relations between a pair of entities. To alleviate the dependency of annotations given by human experts, weak supervision (Bunescu and Mooney, 2007; Etzioni et al., 2004) and distant supervision (Ren et al., 2016) have been employed to automatically generate annotations based on knowledge base (or seed patterns/instances). Universal Schemas (Riedel et al., 2013; Verga et al., 2015; Toutanova et al., 2015) has been proposed to unify patterns and knowledge base, but it’s designed for document-level relation extraction, i.e., not to categorize relation types based on a specific context, but based on the whole corpus. Thus, it allows one relation mention to have multiple true relation types; and does not fit our scenario very well, which is sentence-level relation extraction and assumes one instance has only one relation type. Here we propose a more general framework to consolidate heterogeneous information and further refine the true label from noisy labels, which gives the relation extractor pot"
D17-1291,P16-2087,0,0.0125454,"mean direction µt of each vMF distribution is generated from a prior vMF distribution vMF(·|µ0 , C0 ), while the concentration parameter κt is generated from a log-normal prior logNormal(·|m0 , σ02 ). A similar design is also adopted in (Gopal and Yang, 2014). Parameter inference. We infer the parameters by Gibbs sampling. Because both the von MisesFisher distribution and the Dirichlet distribution 2750 have conjugate priors, we can integrate out parameters µt and π i and develop a collapsed Gibbs sampler of zij : 3.2 Discussions. We notice that there are some topic models (Das et al., 2015; Batmanghelich et al., 2016) proposed for similar data, where words are represented as embedding vectors. Our model is proposed independently for the purpose of identifying semantic focuses, which serves the task of outlier detection. Existing models may lack signals for the following outlier detection steps and hence cannot be directly plugged in. However, it is possible to adapt certain models to the outlier detection task. 3.3 Identifying Semantic Focuses The semantic regions learned from the Embedded vMF Allocation model provide a set of candidates frequently mentioned by documents in the corpus. P (zij = t|Z−ij , X,"
D17-1291,P15-1077,0,\N,Missing
D18-1153,Q16-1026,0,0.0229967,"84 after pruning on the CoNLL03 dataset. It 1 outperforms TagLM (4) and R-ELMo (7), whose performances are 91.62 and 91.54. Besides, we trained small LMs of the same size as the pruned LMs (1-layer densely connected LSTMs). Its perplexity is 69 and its performance on the CoNLL03 dataset is 91.55 ± 0.19. 5 Related Work Sequence labeling. Linguistic sequence labeling is one of the fundamental tasks in NLP, encompassing various applications including POS tagging, chunking, and NER. Many attempts have been made to conduct end-to-end learning and build reliable models without handcrafted features (Chiu and Nichols, 2016; Lample et al., 2016; Ma and Hovy, 2016). Language modeling. Language modeling is a core task in NLP. Many attempts have been paid to develop better neural language models (Zilly et al., 2017; Inan et al., 2016; Godin et al., 2017; Melis et al., 2017). Specifically, with extensive corpora, language models can be well trained to generate high-quality sentences from scratch (J´ozefowicz et al., 2016; Grave et al., 2017; Li et al., 2018; Shazeer et al., 2017). Meanwhile, initial attempts have been made to improve the performance of other tasks with these methods. Some methods treat the language"
D18-1153,W17-2622,0,0.0634627,"Missing"
D18-1153,C16-1087,0,0.0358238,"predicting the next word. the Input of the Recurrent Unit: input input (a) (b) Figure 3: Layer-wise dropout conducted on a 4-layer densely connected RNN. (a) is the remained RNN. (b) is the original densely connected RNN. In other words, this dropout is only applied to the input of recurrent layers, which aims to imitate the pruned input without totally removing any layers. 3 Sequence Labeling In this section, we will introduce our sequence labeling architecture, which is augmented with the contextualized representations. 3.1 Neural Architecture Following the recent studies (Liu et al., 2018; Kuru et al., 2016), we construct the neural architecture as in Fig. 4. Given the input sequence {x1 , x2 , · · · , xT }, for tth token (xt ), we assume its word embedding is wt , its label is yt , and its character-level input is {ci,1 , ci,2 , · · · , ci, }, where ci, is the space character following xt . The character-level representations have become the required components for most of the state-of-the-art. Following the recent study (Liu et al., 2018), we employ LSTMs to take the character-level input in a context-aware manner, and mark its output for xt as ct . Similar to the contextualized representation,"
D18-1153,N16-1030,0,0.597778,"beling Pierre … … concatenate … … … … … … … … [] fixed lstm lstm Vinken w2 … … … … Pierre Pierre , … … embedding Vinken ␣ c1, V i n k e n c2,0 c2,1 c2,2 c2,3 c2,4 c2,5 ␣ c2, Vinken , , Forward LM Figure 4: The proposed sequence labeling architecture with contextualized representations. After projections, these vectors are concatenated as vt = [c∗t ; rt ; wt ], ∀i ∈ [1, T ] and further fed into the word-level LSTMs. We refer to their output as U = {u1 , · · · , uT }. To ensure the model to predict valid label sequences, we append a first-order conditional random field (CRF) layer to the model (Lample et al., 2016). Specifically, the model defines the generation probability of y = {y1 , · · · , yT } as QT t=1 φ(yt−1 , yt , ut ) QT yt−1 , yˆt , ut ) ˆ ∈Y(U) t=1 φ(ˆ y p(y|U) = P (8) ˆ = {ˆ where y y1 , . . . , yˆT } is a generic label sequence, Y(U) is the set of all generic label sequences for U and φ(yt−1 , yt , ut ) is the potential function. In our model, φ(yt−1 , yt , ut ) is defined as exp(Wyt ut + byt−1 ,yt ), where Wyt and byt−1 ,yt are the weight and bias parameters. 3.2 Model Training and Inference We use the following negative log-likelihood as the empirical risk. X L=− log p(y|U) (9) U For tes"
D18-1153,P16-1101,0,0.0755163,"outperforms TagLM (4) and R-ELMo (7), whose performances are 91.62 and 91.54. Besides, we trained small LMs of the same size as the pruned LMs (1-layer densely connected LSTMs). Its perplexity is 69 and its performance on the CoNLL03 dataset is 91.55 ± 0.19. 5 Related Work Sequence labeling. Linguistic sequence labeling is one of the fundamental tasks in NLP, encompassing various applications including POS tagging, chunking, and NER. Many attempts have been made to conduct end-to-end learning and build reliable models without handcrafted features (Chiu and Nichols, 2016; Lample et al., 2016; Ma and Hovy, 2016). Language modeling. Language modeling is a core task in NLP. Many attempts have been paid to develop better neural language models (Zilly et al., 2017; Inan et al., 2016; Godin et al., 2017; Melis et al., 2017). Specifically, with extensive corpora, language models can be well trained to generate high-quality sentences from scratch (J´ozefowicz et al., 2016; Grave et al., 2017; Li et al., 2018; Shazeer et al., 2017). Meanwhile, initial attempts have been made to improve the performance of other tasks with these methods. Some methods treat the language modeling as an additional supervision, an"
D18-1153,P17-1161,0,0.457217,"o the averaged perplexity of the forward and the backward LMs. We can observe that, for those models taking word embedding as the input, embedding composes the vast majority of model parameters. However, embedding can be embodied as a “sparse” layer which is computationally efficient. Instead, the intense calculations are conducted in 1219 Network 8192-1024 (J´ozefowicz et al., 2016) Ind. # Hid. # 1 8192 Layer # Param.# (·107 ) PPL RNN Others 2 15.1] 163] 30.6 89] 30.0 CNN-8192-1024 (J´ozefowicz et al., 2016) 2 8192 2 15.1] CNN-4096-512 (Peters et al., 2018) 3 4096 2 3.8] 40.6] 39.7 2048-512 (Peters et al., 2017) 4 2048 1 0.9] 40.6] 47.50 2048-Adaptive (Grave et al., 2017) 5 2048 2 5.2† 26.5† 39.8 25.6† vanilla LSTM 6 7 2048 1600 2 2 5.3† 3.2† 24.2† 40.27 48.85 LD-Net without Layer-wise Dropout 8 300 10 2.3† 24.2† 45.14 LD-Net with Layer-wise Dropout 9 300 10 2.3† 24.2† 50.06 Table 1: Performance comparison of language models. Models marked with† adopted adaptive softmax and the vanilla LSTMs, which has less softmax parameters. Models marked with] employed sampled softmax LSTMs w. projection, which results in less RNN parameters w.r.t. the size of hidden states. RNN layers and softmax layer for langua"
D18-1153,N18-1202,0,0.458912,"., 2017), slim word embedding (Li et al., 2018), the sampled softmax and the noise contrastive estimation (J´ozefowicz et al., 2016). Since the major focus of our paper does not lie in the language modeling task, we choose the adaptive softmax because of its practical efficiency when accelerated with GPUs. 2.3 Contextualized Representations As pre-trained LMs can describe the text generation accurately, they can be utilized to extract information and construct features for other tasks. These features, referred as contextualized representations, have been demonstrated to be essentially useful (Peters et al., 2018). To capture information from both directions, we utilized not only forward LMs, but also backward LMs. Backward LMs are based on Eqn. 4 instead of Eqn. 2. Similar to forward LMs, backward LMs approach p(xt |xt+1 , · · · , xT ) with NNs. For reference, the output of the RNN in backward LMs for xt is recorded as hrt . p(x1 , · · · , xn ) = T Y p(xt |xt+1 , · · · , xT ) (4) t=1 Ideally, the final output of LMs (e.g., h∗t ) would be the same as the representation of the target word (e.g., xt+1 ); therefore, it may not contain much context information. Meanwhile, the output of the densely connecte"
D18-1153,W09-1119,0,0.118852,"Following TagLM (Peters et al., 2017), we evaluate our methods in two benchmark datasets, the CoNLL03 NER task (Tjong Kim Sang and De Meulder, 2003) and the CoNLL00 Chunking task (Tjong Kim Sang and Buchholz, 2000). CoNLL03 NER has four entity types and includes the standard training, development and test sets. CoNLL00 chunking defines eleven syntactic chunk types (e.g., NP and VP) in addition to Other. Since it only includes training and test sets, we sampled 1000 sentences from training set as a held-out development set (Peters et al., 2017). In both cases, we use the BIOES labeling scheme (Ratinov and Roth, 2009) and use the micro-averaged F1 as the evaluation metric. Based on the analysis conducted in the development set, we set λ0 = 0.05 for the NER task, and λ0 = 0.5 for the Chunking task. As discussed before, we conduct optimization with the stochastic gradient descent with momentum. We set the batch size, the momentum, and the learning rate to 10, 0.9, η0 and ηt = 1+ρt respectively. Here, η0 = 0.015 is the initial learning rate and ρ = 0.05 is the decay ratio. Dropout is applied in our model, and its ratio is set to 0.5. For a better stability, we use gradient clipping of 5.0. Furthermore, we emp"
D18-1153,P17-1194,0,0.0218824,"een paid to develop better neural language models (Zilly et al., 2017; Inan et al., 2016; Godin et al., 2017; Melis et al., 2017). Specifically, with extensive corpora, language models can be well trained to generate high-quality sentences from scratch (J´ozefowicz et al., 2016; Grave et al., 2017; Li et al., 2018; Shazeer et al., 2017). Meanwhile, initial attempts have been made to improve the performance of other tasks with these methods. Some methods treat the language modeling as an additional supervision, and conduct co-training for knowledge transfer (Dai and Le, 2015; Liu et al., 2018; Rei, 2017). Others, including this paper, aim to construct additional features (referred as contextualized representations) with the pre-trained language models (Peters et al., 2017, 2018). Neural Network Acceleration. There are mainly three kinds of NN acceleration methods, i.e., prune network into smaller sizes (Han et al., 2015; Wen et al., 2016), converting float operation into customized low precision arithmetic (Hubara et al., 2018; Courbariaux et al., 2016), and using shallower networks to mimic the output of deeper ones (Hinton et al., 2015; Romero et al., 2014). However, most of them require co"
D18-1153,W00-0726,0,0.662905,"Missing"
D18-1153,W03-0419,0,0.71951,"Missing"
D18-1230,P05-1045,0,0.681709,"ate that AutoNER achieves the best performance when only using dictionaries with no additional human effort, and delivers competitive results with state-of-the-art supervised benchmarks. 1 Introduction Recently, extensive efforts have been made on building reliable named entity recognition (NER) models without handcrafting features (Liu et al., 2018; Ma and Hovy, 2016; Lample et al., 2016). However, most existing methods require large amounts of manually annotated sentences for training supervised models (e.g., neural sequence models) (Liu et al., 2018; Ma and Hovy, 2016; Lample et al., 2016; Finkel et al., 2005). This is particularly challenging in specific do∗ Equal contribution. ‡ teng.ren@cootek.cn mains, where domain-expert annotation is expensive and/or slow to obtain. To alleviate human effort, distant supervision has been applied to automatically generate labeled data, and has gained successes in various natural language processing tasks, including phrase mining (Shang et al., 2018), entity recognition (Ren et al., 2015; Fries et al., 2017; He, 2017), aspect term extraction (Giannakopoulos et al., 2017), and relation extraction (Mintz et al., 2009). Meanwhile, open knowledge bases (or dictiona"
D18-1230,W17-5224,0,0.308462,"ed models (e.g., neural sequence models) (Liu et al., 2018; Ma and Hovy, 2016; Lample et al., 2016; Finkel et al., 2005). This is particularly challenging in specific do∗ Equal contribution. ‡ teng.ren@cootek.cn mains, where domain-expert annotation is expensive and/or slow to obtain. To alleviate human effort, distant supervision has been applied to automatically generate labeled data, and has gained successes in various natural language processing tasks, including phrase mining (Shang et al., 2018), entity recognition (Ren et al., 2015; Fries et al., 2017; He, 2017), aspect term extraction (Giannakopoulos et al., 2017), and relation extraction (Mintz et al., 2009). Meanwhile, open knowledge bases (or dictionaries) are becoming increasingly popular, such as WikiData and YAGO in the general domain, as well as MeSH and CTD in the biomedical domain. The existence of such dictionaries makes it possible to generate training data for NER at a large scale without additional human effort. Existing distantly supervised NER models usually tackle the entity span detection problem by heuristic matching rules, such as POS tag-based regular expressions (Ren et al., 2015; Fries et al., 2017) and exact string matching (Gian"
D18-1230,W12-3016,0,0.0556348,"Missing"
D18-1230,P16-1101,0,0.68106,"nal framework and propose a novel, more effective neural model AutoNER with a new Tie or Break scheme. In addition, we discuss how to refine distant supervision for better NER performance. Extensive experiments on three benchmark datasets demonstrate that AutoNER achieves the best performance when only using dictionaries with no additional human effort, and delivers competitive results with state-of-the-art supervised benchmarks. 1 Introduction Recently, extensive efforts have been made on building reliable named entity recognition (NER) models without handcrafting features (Liu et al., 2018; Ma and Hovy, 2016; Lample et al., 2016). However, most existing methods require large amounts of manually annotated sentences for training supervised models (e.g., neural sequence models) (Liu et al., 2018; Ma and Hovy, 2016; Lample et al., 2016; Finkel et al., 2005). This is particularly challenging in specific do∗ Equal contribution. ‡ teng.ren@cootek.cn mains, where domain-expert annotation is expensive and/or slow to obtain. To alleviate human effort, distant supervision has been applied to automatically generate labeled data, and has gained successes in various natural language processing tasks, including"
D18-1230,P09-1113,0,0.809382,"018; Ma and Hovy, 2016; Lample et al., 2016; Finkel et al., 2005). This is particularly challenging in specific do∗ Equal contribution. ‡ teng.ren@cootek.cn mains, where domain-expert annotation is expensive and/or slow to obtain. To alleviate human effort, distant supervision has been applied to automatically generate labeled data, and has gained successes in various natural language processing tasks, including phrase mining (Shang et al., 2018), entity recognition (Ren et al., 2015; Fries et al., 2017; He, 2017), aspect term extraction (Giannakopoulos et al., 2017), and relation extraction (Mintz et al., 2009). Meanwhile, open knowledge bases (or dictionaries) are becoming increasingly popular, such as WikiData and YAGO in the general domain, as well as MeSH and CTD in the biomedical domain. The existence of such dictionaries makes it possible to generate training data for NER at a large scale without additional human effort. Existing distantly supervised NER models usually tackle the entity span detection problem by heuristic matching rules, such as POS tag-based regular expressions (Ren et al., 2015; Fries et al., 2017) and exact string matching (Giannakopoulos et al., 2017; He, 2017). In these m"
D18-1230,S14-2004,0,0.0744032,"Missing"
D18-1230,W09-1119,0,0.808465,"me surface name in the dictionary. To address these challenges, we propose and compare two neural architectures with customized tagging schemes. We start with adjusting models under the traditional sequence labeling framework. Typically, NER models are built upon conditional random 2054 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2054–2064 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics fields (CRF) with the IOB or IOBES tagging scheme (Liu et al., 2018; Ma and Hovy, 2016; Lample et al., 2016; Ratinov and Roth, 2009; Finkel et al., 2005). However, such design cannot deal with multi-label tokens. Therefore, we customize the conventional CRF layer in LSTMCRF (Lample et al., 2016) into a Fuzzy CRF layer, which allows each token to have multiple labels without sacrificing computing efficiency. To adapt to imperfect labels generated by distant supervision, we go beyond the traditional sequence labeling framework and propose a new prediction model. Specifically, instead of predicting the label of each single token, we propose to predict whether two adjacent tokens are tied in the same entity mention or not (i."
D18-1230,N16-1030,0,0.858046,"ropose a novel, more effective neural model AutoNER with a new Tie or Break scheme. In addition, we discuss how to refine distant supervision for better NER performance. Extensive experiments on three benchmark datasets demonstrate that AutoNER achieves the best performance when only using dictionaries with no additional human effort, and delivers competitive results with state-of-the-art supervised benchmarks. 1 Introduction Recently, extensive efforts have been made on building reliable named entity recognition (NER) models without handcrafting features (Liu et al., 2018; Ma and Hovy, 2016; Lample et al., 2016). However, most existing methods require large amounts of manually annotated sentences for training supervised models (e.g., neural sequence models) (Liu et al., 2018; Ma and Hovy, 2016; Lample et al., 2016; Finkel et al., 2005). This is particularly challenging in specific do∗ Equal contribution. ‡ teng.ren@cootek.cn mains, where domain-expert annotation is expensive and/or slow to obtain. To alleviate human effort, distant supervision has been applied to automatically generate labeled data, and has gained successes in various natural language processing tasks, including phrase mining (Shang"
D18-1230,P16-1209,0,0.0234109,"leave further improvements of AutoNER to the future work. 6 Related Work The task of supervised named entity recognition (NER) is typically embodied as a sequence labeling problem. Conditional random fields (CRF) models built upon human annotations and handcrafted features are the standard (Finkel et al., 2005; Settles, 2004; Leaman and Gonzalez, 2008). Recent advances in neural models have freed do2061 main experts from handcrafting features for NER tasks. (Lample et al., 2016; Ma and Hovy, 2016; Liu et al., 2018). Such neural models are increasingly common in the domain-specific NER tasks (Sahu and Anand, 2016; Dernoncourt et al., 2017; Wang et al., 2018). Semi-supervised methods have been explored to further improve the accuracy by either augmenting labeled datasets with word embeddings or bootstrapping techniques in tasks like gene name recognition (Kuksa and Qi, 2010; Tang et al., 2014; Vlachos and Gasperin, 2006). Unlike these existing approaches, our study focuses on the distantly supervised setting without any expert-curated training data. Distant supervision has attracted many attentions to alleviate human efforts. Originally, it was proposed to leverage knowledge bases to supervise relation"
D18-1230,W04-1221,0,0.182675,"o without sufficient human annotations. Still, we observe that, when the supervised benchmark is trained with all annotations, it achieves the performance better than AutoNER. We conjugate that this is because AutoNER lacks more advanced techniques to handle distant supervision, and we leave further improvements of AutoNER to the future work. 6 Related Work The task of supervised named entity recognition (NER) is typically embodied as a sequence labeling problem. Conditional random fields (CRF) models built upon human annotations and handcrafted features are the standard (Finkel et al., 2005; Settles, 2004; Leaman and Gonzalez, 2008). Recent advances in neural models have freed do2061 main experts from handcrafting features for NER tasks. (Lample et al., 2016; Ma and Hovy, 2016; Liu et al., 2018). Such neural models are increasingly common in the domain-specific NER tasks (Sahu and Anand, 2016; Dernoncourt et al., 2017; Wang et al., 2018). Semi-supervised methods have been explored to further improve the accuracy by either augmenting labeled datasets with word embeddings or bootstrapping techniques in tasks like gene name recognition (Kuksa and Qi, 2010; Tang et al., 2014; Vlachos and Gasperin,"
D18-1230,W06-3328,0,0.0544292,", 2005; Settles, 2004; Leaman and Gonzalez, 2008). Recent advances in neural models have freed do2061 main experts from handcrafting features for NER tasks. (Lample et al., 2016; Ma and Hovy, 2016; Liu et al., 2018). Such neural models are increasingly common in the domain-specific NER tasks (Sahu and Anand, 2016; Dernoncourt et al., 2017; Wang et al., 2018). Semi-supervised methods have been explored to further improve the accuracy by either augmenting labeled datasets with word embeddings or bootstrapping techniques in tasks like gene name recognition (Kuksa and Qi, 2010; Tang et al., 2014; Vlachos and Gasperin, 2006). Unlike these existing approaches, our study focuses on the distantly supervised setting without any expert-curated training data. Distant supervision has attracted many attentions to alleviate human efforts. Originally, it was proposed to leverage knowledge bases to supervise relation extraction tasks (Craven et al., 1999; Mintz et al., 2009). AutoPhrase has demonstrated powers in extracting high-quality phrases from domain-specific corpora like scientific papers and business reviews (Shang et al., 2018) but it cannot categorize phrases into typed entities in a contextaware manner. We incorp"
D19-1042,N18-1150,0,0.0126189,"l probability distribution pFlat of all the labels on the hierarchy: pFlat = σ(WFlat ed ). The combination of the base model and the flat component functions the same as a flat model and ensures that the object representation ed has the capability of flat classification. We denote the flat loss that measures the binary the labels P entropy over all P cross Flat (l) + (1 − L (l) × logp by Of = − N i i=1 l∈L i (l)). Combining the flat Li (l)) × log(1 − pFlat i and local losses, the supervised loss in HiLAP-SL is defined as OSL = λOf + (1 − λ)Ol , where λ ∈ [0, 1] is the mixing ratio. Similar to Celikyilmaz et al. (2018), we also found that mixing a proportion of the supervised loss is beneficial to the learning process of HiLAP. Further combining the global information Og (i.e., ORL ), the total loss of HiLAP is defined as O = ORL + αOSL , where α is a scaling factor accounting for the difference in magnitude between ORL and OSL . While we do not use the flat component during inference, it helps the representation learning of the base model and improves the performance of both HiLAP-SL and HiLAP (see Sec. 4.5). 4 Dataset Evaluation Metrics. We use standard metrics (Johnson and Zhang, 2014; Meng et al., 2018;"
D19-1042,P18-1229,1,0.848397,"ormulate HTC as Rewards. The agent receives scalar rewards as feedback for its actions. Different from exist447 ing work where each label of one example2 is treated independently, HiLAP measures the quality of all the labels assigned to each example xi by rewarding the agent with the Example-based F1 (see Sec. 4.1 for details of this metric). Intuitively, the agent would realize how similar the assigned and the ground-truth labels of one example are. Instead of waiting until the end of the label assignment process and comparing the predicted labels with the gold labels, we use reward shaping (Mao et al., 2018), i.e., giving intermediate rewards at each time step, to accelerate the learning process. Specifically, we set the reward r of xi at time step t to be the difference of Example-based F1 scores between current and the last time step: i rtxi = F1xt i − F1xt−1 . If current F1 is better than that at the last time step, the reward would be positive, and vice versa. The cumulative reward from current time step to the end of an episode would cancel the intermediate rewards and thus reflect whether the current action improves the holistic label assignment or not. As a result, the learned policy would"
D19-1042,D14-1162,0,0.0817881,"Missing"
D19-1042,P09-2042,0,0.073075,"Missing"
D19-1042,D14-1181,0,0.00689806,"Missing"
D19-1042,D16-1076,0,0.0360859,"Missing"
D19-1042,N16-1174,0,0.496145,"dy shows that HiLAP is especially beneficial to those unpopular labels at the bottom levels. been predicted positive. One critical issue is that the number of local classifiers depends on the size of the label hierarchy, making local approaches infeasible to scale. Global approaches use one single classifier and model the label hierarchy more explicitly. Traditional global approaches (Wang et al., 2001; Silla Jr and Freitas, 2009) are largely based on specific flat models and often make unrealistic assumptions (Cai and Hofmann, 2004) as in flat approaches. Recent neural approaches (Kim, 2014; Yang et al., 2016) mainly focus on flat classification while their performance in HTC is relatively less studied. Even if the classification is supposed to be hierarchical, prior work (Gopal and Yang, 2013; Johnson and Zhang, 2014; Peng et al., 2018) still make flat and independent predictions or utilize simple constraints without considering the holistic quality of label assignment. One recent framework (Wehrmann et al., 2018) attempts to leverage both local and global information but it uses static features as input and its inference process is still flat. In this paper, we formulate HTC as a Markov decision"
D19-1519,W99-0606,0,0.331962,"the weight of this sentence as mistakes. [Hapoel Jerusalem]{ORD}, [Maccabi Tel Aviv]{ORG} Action Flair w/ CrossWeight Result Table 11: Case Study on the CoNLL03 dataset. Errors are marked with red tion to the training set is the same as our mistake estimation module, except that we have an entity disjoint filtering step. Experiments in Table 4 show that this step is crucial to our performance gain. The choice of ten folds also stems from cross validation (Kohavi, 1995). Another similar thread of work is boosting, such as Adaboost (Freund et al., 1999; Schapire and Singer, 1999). For example, Abney et al. (1999) has applied Adaboost on the Penn Treebank POS tagging dataset and gained encouraging results on model performance. In boosting algorithms, the training data is assumed to be perfect. Therefore, it trains models using the full training set and then increases the weights of training instances that fails the current model in the next round of learning. In contrast, we decrease the weights of sentences that differ from the model built upon the entity disjoint training set. More importantly, our framework is a better fit for neural models, because they can likely overfit the training data and thus"
D19-1519,N19-1078,0,0.102981,"Missing"
D19-1519,C18-1139,0,0.303713,"l., 2016) incorporates long short term memory (LSTM) neural network with conditional random field (CRF). It also uses a word-wise character LSTM. • LSTM-CNNs-CRF (Ma and Hovy, 2016) has a similar structure as LSTM-CRF, but captures character-level information through a convolutional neural network (CNN) over the character embedding. • VanillaNER (Liu et al., 2018a) also extends LSTM-CRF and LSTM-CNNs-CRF by using a sentence-wise character LSTM. • ELMo (Peters et al., 2018) extends LSTM-CRF and leverages pre-trained word-level language models for better contextualized representations. • Flair (Akbik et al., 2018) also aims for contextualized representations, utilizing pretrained character level language models. • Pooled-Flair (Akbik et al., 2018) extends Flair and maintains an embedding pool for each word to bring in dataset-level word embedding. We use the implementation released by the authors for each algorithm and report the performance on original test set and corrected test set averaging 5 runs. Results & Discussions. We re-evaluate the performance of the NER algorithms on the corrected test set. Their performance on the original test set is also listed for the reference. From the results in Tab"
D19-1519,C02-1101,0,0.121926,"Missing"
D19-1519,P17-1161,0,0.0245859,"Missing"
D19-1519,N18-1202,0,0.206866,"dard deviations on both original and corrected datasets. The results are based on 5 different runs. lar NER algorithms: • LSTM-CRF (Lample et al., 2016) incorporates long short term memory (LSTM) neural network with conditional random field (CRF). It also uses a word-wise character LSTM. • LSTM-CNNs-CRF (Ma and Hovy, 2016) has a similar structure as LSTM-CRF, but captures character-level information through a convolutional neural network (CNN) over the character embedding. • VanillaNER (Liu et al., 2018a) also extends LSTM-CRF and LSTM-CNNs-CRF by using a sentence-wise character LSTM. • ELMo (Peters et al., 2018) extends LSTM-CRF and leverages pre-trained word-level language models for better contextualized representations. • Flair (Akbik et al., 2018) also aims for contextualized representations, utilizing pretrained character level language models. • Pooled-Flair (Akbik et al., 2018) extends Flair and maintains an embedding pool for each word to bring in dataset-level word embedding. We use the implementation released by the authors for each algorithm and report the performance on original test set and corrected test set averaging 5 runs. Results & Discussions. We re-evaluate the performance of the"
D19-1519,helgadottir-etal-2014-correcting,0,0.0723348,"Missing"
D19-1519,P17-1107,0,0.118599,"Missing"
D19-1519,N16-1030,0,0.151532,"ied that there is indeed a vessel called “Seagrand Ace” (“Seagramd ace” might be a typo). 2.2 CoNLL03 Re-Evaluation NER Algorithms. We re-evaluate following popuMethod LSTM-CRF LSTM-CNNs-CRF VanillaNER Elmo Flair Pooled Flair Original Corrected 90.64 (±0.23) 90.65 (±0.57) 91.44 (±0.16) 92.28 (±0.19) 92.87 (±0.08) 93.14 (±0.14) 91.47 (±0.15) 91.87 (±0.50) 92.32 (±0.16) 93.42 (±0.15) 93.89 (±0.06) 94.13 (±0.11) Table 2: CoNLL03 Re-Evaluation: Test F1 scores and standard deviations on both original and corrected datasets. The results are based on 5 different runs. lar NER algorithms: • LSTM-CRF (Lample et al., 2016) incorporates long short term memory (LSTM) neural network with conditional random field (CRF). It also uses a word-wise character LSTM. • LSTM-CNNs-CRF (Ma and Hovy, 2016) has a similar structure as LSTM-CRF, but captures character-level information through a convolutional neural network (CNN) over the character embedding. • VanillaNER (Liu et al., 2018a) also extends LSTM-CRF and LSTM-CNNs-CRF by using a sentence-wise character LSTM. • ELMo (Peters et al., 2018) extends LSTM-CRF and leverages pre-trained word-level language models for better contextualized representations. • Flair (Akbik et"
D19-1519,W03-0419,0,0.117016,"Missing"
D19-1519,D18-1153,1,0.913545,"32 (±0.16) 93.42 (±0.15) 93.89 (±0.06) 94.13 (±0.11) Table 2: CoNLL03 Re-Evaluation: Test F1 scores and standard deviations on both original and corrected datasets. The results are based on 5 different runs. lar NER algorithms: • LSTM-CRF (Lample et al., 2016) incorporates long short term memory (LSTM) neural network with conditional random field (CRF). It also uses a word-wise character LSTM. • LSTM-CNNs-CRF (Ma and Hovy, 2016) has a similar structure as LSTM-CRF, but captures character-level information through a convolutional neural network (CNN) over the character embedding. • VanillaNER (Liu et al., 2018a) also extends LSTM-CRF and LSTM-CNNs-CRF by using a sentence-wise character LSTM. • ELMo (Peters et al., 2018) extends LSTM-CRF and leverages pre-trained word-level language models for better contextualized representations. • Flair (Akbik et al., 2018) also aims for contextualized representations, utilizing pretrained character level language models. • Pooled-Flair (Akbik et al., 2018) extends Flair and maintains an embedding pool for each word to bring in dataset-level word embedding. We use the implementation released by the authors for each algorithm and report the performance on original"
D19-1519,P16-1101,0,0.417598,"RF LSTM-CNNs-CRF VanillaNER Elmo Flair Pooled Flair Original Corrected 90.64 (±0.23) 90.65 (±0.57) 91.44 (±0.16) 92.28 (±0.19) 92.87 (±0.08) 93.14 (±0.14) 91.47 (±0.15) 91.87 (±0.50) 92.32 (±0.16) 93.42 (±0.15) 93.89 (±0.06) 94.13 (±0.11) Table 2: CoNLL03 Re-Evaluation: Test F1 scores and standard deviations on both original and corrected datasets. The results are based on 5 different runs. lar NER algorithms: • LSTM-CRF (Lample et al., 2016) incorporates long short term memory (LSTM) neural network with conditional random field (CRF). It also uses a word-wise character LSTM. • LSTM-CNNs-CRF (Ma and Hovy, 2016) has a similar structure as LSTM-CRF, but captures character-level information through a convolutional neural network (CNN) over the character embedding. • VanillaNER (Liu et al., 2018a) also extends LSTM-CRF and LSTM-CNNs-CRF by using a sentence-wise character LSTM. • ELMo (Peters et al., 2018) extends LSTM-CRF and leverages pre-trained word-level language models for better contextualized representations. • Flair (Akbik et al., 2018) also aims for contextualized representations, utilizing pretrained character level language models. • Pooled-Flair (Akbik et al., 2018) extends Flair and maintai"
D19-1519,D18-1230,1,0.846964,", our framework is a better fit for neural models, because they can likely overfit the training data and thus being bad choices as weak classifiers in boosting. 6.3 NER Algorithms Neural models have been widely used for Named Entity Recognition, and the state-of-the-art models integrate LSTMs, conditional random field and language models (Lample et al., 2016; Ma and Hovy, 2016; Liu et al., 2018b; Peters et al., 2018; Akbik et al., 2018). In this paper, we focus on improving the annotation quality for NER, and our method has a big potential to help other methods, especially for noisy datasets (Shang et al., 2018). 7 Conclusion & Future work In this paper, we explore and correct the label mistakes in the CoNLL03 NER dataset. Based on the corrected test set, we re-evaluate most of recent NER models. We further propose a novel framework, CrossWeigh, that is able to detect label mistakes in the training set and then train a more robust NER model accordingly. Extensive experiments demonstrate the effectiveness of CrossWeigh on three datasets and also indicate the potentials of using CrossWeigh to improve the annotation quality during the label curation process. In future, we plan to extend our framework in"
D19-1519,D10-1017,0,0.0478383,"Missing"
N16-3015,P15-1056,1,0.400174,"ructured data in different modalities (e.g., texts, images and videos) is posted online for ready viewing. Complex event extraction and recommendation is critical for many information distillation tasks, including tracking current events, providing alerts, and predicting possible changes, as related to topics of ongoing concern. State-of-the-art Information Extraction (IE) technologies focus on extracting events from a single data modality and ignore cross-media fusion. More importantly, users are presented with extracted events in a passive way (e.g., in a temporally ordered event chronicle (Ge et al., 2015)). Such technologies do not leverage user behavior to identify the event 1 The system demo is available at: http://nlp.cs. rpi.edu/multimedia/event/navigation_dark. html properties of interest to them in selecting new scenarios for presentation. In this paper we present a novel event extraction and recommendation system that incorporates advances in extracting events across multiple sources with data in diverse modalities and so yields a more comprehensive understanding of collective events, their importance, and their inter-connections. The novel capabilities of our system include: • Event Ex"
N16-3015,P12-1038,0,0.0307608,"of sentences and apply spectral clustering to find several clustering centers (i.e., representative sentences including the most important phrases) as the summary. The user is also provided two options to show the original documents and the document containing the summary. 4.2 Visual Information Extraction For each event, we retrieve the most representative video/image online using the key-phrases such as date and entities as queries. Videos and images are often more impressive and efficient at conveying information. We first apply a pretrained convolutional neural network (CNN) architecture (Kuznetsova et al., 2012) to extract visual concepts from each video key frame based on the EventNet concept library (Ye et al., 2015). For example, the extracted visual concepts “crowd on street, riot, demonstration or protest, people marching” appear when the user’s mouse is over the video of the primary event (Figure 3). Then we adopt the approach described in (Li et al., 2015) which applies CNN and association rule mining technique to generate visual patterns and extract semantically meaningful relations between visual and textual information to name the patterns. Figure 3: Recommendation Interface. 5 6 Conclusion"
N16-3015,D14-1198,1,0.888744,"Missing"
N19-1145,W06-0901,0,0.0269479,". In contrast, by incorporating KB concept embeddings, especially 1427 the information from the function description positive regulation of transcription, DNA-templated for Tax, our approach successfully promotes the probability of E1 being predicted as the Theme of E2. 4 Related Work As a crucial task in information extraction, event extraction has gained a lot of interest. In general news domain, previous work on event extraction can be divided into two main categories. The first is feature-based methods which mainly focus on feature design, leveraging local features (Grishman et al., 2005; Ahn, 2006) and global features (Ji and Grishman, 2008; Liao and Grishman, 2011; Huang and Riloff, 2012) to improve the performance. Some studies proposed joint models to overcome the error propagation problem (Poon and Vanderwende, 2010; Riedel et al., 2009; Li et al., 2013; Venugopal et al., 2014; Li et al., 2014). The second category includes distributional representation based methods which have been applied into event extraction extensively. Most of these approaches are based on the standard Convolutional Neural Networks (CNNs) (Chen et al., 2015; Nguyen and Grishman, 2015, 2016), Recurrent Neural N"
N19-1145,W11-1828,0,0.208157,"Missing"
N19-1145,W13-2003,0,0.0422846,"Missing"
N19-1145,W18-2311,0,0.291907,"Missing"
N19-1145,W13-2014,0,0.0391315,"Missing"
N19-1145,P17-1038,0,0.0125929,"nt extraction extensively. Most of these approaches are based on the standard Convolutional Neural Networks (CNNs) (Chen et al., 2015; Nguyen and Grishman, 2015, 2016), Recurrent Neural Networks (RNNs) (Nguyen et al., 2016), generative adversarial networks (Hong et al., 2018), zero-shot learning (Huang et al., 2017) and advanced attention mechanisms (Liu et al., 2018b; Chen et al., 2018). Our work is also related to the studies which leverage the external knowledge base for information extraction. Liu et al. (2017) takes advantage of external resources, such as FrameNet, to label events while Chen et al. (2017) adopts distance supervision to augment the training data. Liu et al. (2018a) develops an attention-based model for event extraction. What’s more, shortest dependency path is broadly explored for information extraction, especially for relation classification (Xu et al., 2015; Miwa and Bansal, 2016) and shows promising benefits. Biomedical event extraction task part of the BioNLP Shared Task series (Kim et al., 2009, 2011; N´edellec et al., 2013). Previous studies mainly explore local and global features with SVM model (Miwa et al., 2010, 2012; Bj¨orne and Salakoski, 2013; Majumder et al., 2016"
N19-1145,P15-1017,0,0.0283369,"re design, leveraging local features (Grishman et al., 2005; Ahn, 2006) and global features (Ji and Grishman, 2008; Liao and Grishman, 2011; Huang and Riloff, 2012) to improve the performance. Some studies proposed joint models to overcome the error propagation problem (Poon and Vanderwende, 2010; Riedel et al., 2009; Li et al., 2013; Venugopal et al., 2014; Li et al., 2014). The second category includes distributional representation based methods which have been applied into event extraction extensively. Most of these approaches are based on the standard Convolutional Neural Networks (CNNs) (Chen et al., 2015; Nguyen and Grishman, 2015, 2016), Recurrent Neural Networks (RNNs) (Nguyen et al., 2016), generative adversarial networks (Hong et al., 2018), zero-shot learning (Huang et al., 2017) and advanced attention mechanisms (Liu et al., 2018b; Chen et al., 2018). Our work is also related to the studies which leverage the external knowledge base for information extraction. Liu et al. (2017) takes advantage of external resources, such as FrameNet, to label events while Chen et al. (2017) adopts distance supervision to augment the training data. Liu et al. (2018a) develops an attention-based model for"
N19-1145,D18-1158,0,0.0117435,"ation problem (Poon and Vanderwende, 2010; Riedel et al., 2009; Li et al., 2013; Venugopal et al., 2014; Li et al., 2014). The second category includes distributional representation based methods which have been applied into event extraction extensively. Most of these approaches are based on the standard Convolutional Neural Networks (CNNs) (Chen et al., 2015; Nguyen and Grishman, 2015, 2016), Recurrent Neural Networks (RNNs) (Nguyen et al., 2016), generative adversarial networks (Hong et al., 2018), zero-shot learning (Huang et al., 2017) and advanced attention mechanisms (Liu et al., 2018b; Chen et al., 2018). Our work is also related to the studies which leverage the external knowledge base for information extraction. Liu et al. (2017) takes advantage of external resources, such as FrameNet, to label events while Chen et al. (2017) adopts distance supervision to augment the training data. Liu et al. (2018a) develops an attention-based model for event extraction. What’s more, shortest dependency path is broadly explored for information extraction, especially for relation classification (Xu et al., 2015; Miwa and Bansal, 2016) and shows promising benefits. Biomedical event extraction task part of t"
N19-1145,W09-1407,0,0.0102761,"develops an attention-based model for event extraction. What’s more, shortest dependency path is broadly explored for information extraction, especially for relation classification (Xu et al., 2015; Miwa and Bansal, 2016) and shows promising benefits. Biomedical event extraction task part of the BioNLP Shared Task series (Kim et al., 2009, 2011; N´edellec et al., 2013). Previous studies mainly explore local and global features with SVM model (Miwa et al., 2010, 2012; Bj¨orne and Salakoski, 2013; Majumder et al., 2016). Riedel and McCallum (2011) develop a joint model with dual decomposition. Cohen et al. (2009), Kilicoglu and Bergler (2011) and Bui et al. (2013) develop rule-based methods and achieve high precision. Venugopal et al. (2014) leverage Markov logic networks for joint inference. Rao et al. (2017) uses the Abstract Meaning Representations (AMR) to extract events based on the assumption that an event structure can be derived from an AMR subgraph. Recently, some representationbased models (Jagannatha and Yu, 2016; Rao et al., 2017; Bj¨orne and Salakoski, 2018) have been proposed while most of them adopt the widely used CNNs and RNNs with features derived from the biomedical text. Lim et al."
N19-1145,D17-1070,0,0.0143517,"litate the explicit pattern learning for argument role labeling, for example, the gene expression event pattern (Theme: Protein, Trigger: transduced) is more popular than (Theme: Tax, Trigger: transduced) in Figure 1. The gene ontology function can provide implicit clues to determine the trigger type as aforementioned in Section 1. As shown in Figure 1, we assign a word embedding which pretrained on PubMed and PMC texts (Moen and Ananiadou, 2013) to represent each entity type. For each gene ontology function which is usually a long phrase, we use a stateof-the-art sentence embedding approach (Conneau et al., 2017) to automatically learn a vector representation. We then concatenate these two types of KB property representations as the final KB concept embedding. 1423 2.3 Event Trigger Extraction After obtaining the KB concept embeddings, we further incorporate them into the Tree-LSTM to leverage the domain-specific knowledge. Given a sentence, for example the sentence shown in Figure 3, we first perform the dependency parsing with the Stanford dependency parser (Chen and Manning) and obtain a dependency tree structure. For each node j in the tree structure, C(j) is the set of children nodes of node j an"
N19-1145,E12-1029,0,0.011886,"mation from the function description positive regulation of transcription, DNA-templated for Tax, our approach successfully promotes the probability of E1 being predicted as the Theme of E2. 4 Related Work As a crucial task in information extraction, event extraction has gained a lot of interest. In general news domain, previous work on event extraction can be divided into two main categories. The first is feature-based methods which mainly focus on feature design, leveraging local features (Grishman et al., 2005; Ahn, 2006) and global features (Ji and Grishman, 2008; Liao and Grishman, 2011; Huang and Riloff, 2012) to improve the performance. Some studies proposed joint models to overcome the error propagation problem (Poon and Vanderwende, 2010; Riedel et al., 2009; Li et al., 2013; Venugopal et al., 2014; Li et al., 2014). The second category includes distributional representation based methods which have been applied into event extraction extensively. Most of these approaches are based on the standard Convolutional Neural Networks (CNNs) (Chen et al., 2015; Nguyen and Grishman, 2015, 2016), Recurrent Neural Networks (RNNs) (Nguyen et al., 2016), generative adversarial networks (Hong et al., 2018), ze"
N19-1145,N16-1056,0,0.0219326,"global features with SVM model (Miwa et al., 2010, 2012; Bj¨orne and Salakoski, 2013; Majumder et al., 2016). Riedel and McCallum (2011) develop a joint model with dual decomposition. Cohen et al. (2009), Kilicoglu and Bergler (2011) and Bui et al. (2013) develop rule-based methods and achieve high precision. Venugopal et al. (2014) leverage Markov logic networks for joint inference. Rao et al. (2017) uses the Abstract Meaning Representations (AMR) to extract events based on the assumption that an event structure can be derived from an AMR subgraph. Recently, some representationbased models (Jagannatha and Yu, 2016; Rao et al., 2017; Bj¨orne and Salakoski, 2018) have been proposed while most of them adopt the widely used CNNs and RNNs with features derived from the biomedical text. Lim et al. (2018) implements a binary Tree-LSTM architecture for biomedical relation extraction. Compared with these methods, our approach only requires pretrained distributed word representations as input features and incorporates meaningful KB information into a Tree-LSTM. 5 Conclusions and Future Work In this paper, we show the effectiveness of using a KB-driven tree-structured LSTM for event extraction in biomedical domai"
N19-1145,P08-1030,1,0.652212,"KB concept embeddings, especially 1427 the information from the function description positive regulation of transcription, DNA-templated for Tax, our approach successfully promotes the probability of E1 being predicted as the Theme of E2. 4 Related Work As a crucial task in information extraction, event extraction has gained a lot of interest. In general news domain, previous work on event extraction can be divided into two main categories. The first is feature-based methods which mainly focus on feature design, leveraging local features (Grishman et al., 2005; Ahn, 2006) and global features (Ji and Grishman, 2008; Liao and Grishman, 2011; Huang and Riloff, 2012) to improve the performance. Some studies proposed joint models to overcome the error propagation problem (Poon and Vanderwende, 2010; Riedel et al., 2009; Li et al., 2013; Venugopal et al., 2014; Li et al., 2014). The second category includes distributional representation based methods which have been applied into event extraction extensively. Most of these approaches are based on the standard Convolutional Neural Networks (CNNs) (Chen et al., 2015; Nguyen and Grishman, 2015, 2016), Recurrent Neural Networks (RNNs) (Nguyen et al., 2016), gener"
N19-1145,W11-1827,0,0.0217041,"n-based model for event extraction. What’s more, shortest dependency path is broadly explored for information extraction, especially for relation classification (Xu et al., 2015; Miwa and Bansal, 2016) and shows promising benefits. Biomedical event extraction task part of the BioNLP Shared Task series (Kim et al., 2009, 2011; N´edellec et al., 2013). Previous studies mainly explore local and global features with SVM model (Miwa et al., 2010, 2012; Bj¨orne and Salakoski, 2013; Majumder et al., 2016). Riedel and McCallum (2011) develop a joint model with dual decomposition. Cohen et al. (2009), Kilicoglu and Bergler (2011) and Bui et al. (2013) develop rule-based methods and achieve high precision. Venugopal et al. (2014) leverage Markov logic networks for joint inference. Rao et al. (2017) uses the Abstract Meaning Representations (AMR) to extract events based on the assumption that an event structure can be derived from an AMR subgraph. Recently, some representationbased models (Jagannatha and Yu, 2016; Rao et al., 2017; Bj¨orne and Salakoski, 2018) have been proposed while most of them adopt the widely used CNNs and RNNs with features derived from the biomedical text. Lim et al. (2018) implements a binary Tr"
N19-1145,W09-1401,0,0.110226,"the studies which leverage the external knowledge base for information extraction. Liu et al. (2017) takes advantage of external resources, such as FrameNet, to label events while Chen et al. (2017) adopts distance supervision to augment the training data. Liu et al. (2018a) develops an attention-based model for event extraction. What’s more, shortest dependency path is broadly explored for information extraction, especially for relation classification (Xu et al., 2015; Miwa and Bansal, 2016) and shows promising benefits. Biomedical event extraction task part of the BioNLP Shared Task series (Kim et al., 2009, 2011; N´edellec et al., 2013). Previous studies mainly explore local and global features with SVM model (Miwa et al., 2010, 2012; Bj¨orne and Salakoski, 2013; Majumder et al., 2016). Riedel and McCallum (2011) develop a joint model with dual decomposition. Cohen et al. (2009), Kilicoglu and Bergler (2011) and Bui et al. (2013) develop rule-based methods and achieve high precision. Venugopal et al. (2014) leverage Markov logic networks for joint inference. Rao et al. (2017) uses the Abstract Meaning Representations (AMR) to extract events based on the assumption that an event structure can be"
N19-1145,W11-1802,0,0.646021,"are manually annotated and given as part of the input. We evaluate our results on the test set using the official online tool provided by the Genia task organizers.1 Following previous studies (Bj¨orne and Salakoski, 2011; Venugopal et al., 2014; Rao et al., 2017; Bj¨orne and Salakoski, 2018), we report scores obtained by the approximate span (allowing trigger spans to differ from gold spans by single words). As we only focus on matching core arguments, we use recursive matching criterion for evaluation which not requires matching of additional arguments for events referred from other events (Kim et al., 2011). We use the word embedding pretrained on PubMed and PMC texts (Moen and Ananiadou, 1425 1 http://bionlp-st.dbcls.jp/GE/2011/eval-test/ System 2013) for word and type embeddings. The hyperparameters are tuned on the development set and listed in Table 2. Word representations are updated during training with an initial learning rate of 0.1. Parameter Word embedding size Type embedding size Sentence embedding size Tree-LSTM hidden size Batch size Epoch size Dropout rate Learning rate Initial embedding learning rate Optimizer KB-driven Tree-LSTM Value 200 200 4096 100 25 30 0.5 0.05 0.1 AdaGrad T"
N19-1145,P13-1008,1,0.861127,"heme of E2. 4 Related Work As a crucial task in information extraction, event extraction has gained a lot of interest. In general news domain, previous work on event extraction can be divided into two main categories. The first is feature-based methods which mainly focus on feature design, leveraging local features (Grishman et al., 2005; Ahn, 2006) and global features (Ji and Grishman, 2008; Liao and Grishman, 2011; Huang and Riloff, 2012) to improve the performance. Some studies proposed joint models to overcome the error propagation problem (Poon and Vanderwende, 2010; Riedel et al., 2009; Li et al., 2013; Venugopal et al., 2014; Li et al., 2014). The second category includes distributional representation based methods which have been applied into event extraction extensively. Most of these approaches are based on the standard Convolutional Neural Networks (CNNs) (Chen et al., 2015; Nguyen and Grishman, 2015, 2016), Recurrent Neural Networks (RNNs) (Nguyen et al., 2016), generative adversarial networks (Hong et al., 2018), zero-shot learning (Huang et al., 2017) and advanced attention mechanisms (Liu et al., 2018b; Chen et al., 2018). Our work is also related to the studies which leverage the"
N19-1145,D14-1198,1,0.870375,"ask in information extraction, event extraction has gained a lot of interest. In general news domain, previous work on event extraction can be divided into two main categories. The first is feature-based methods which mainly focus on feature design, leveraging local features (Grishman et al., 2005; Ahn, 2006) and global features (Ji and Grishman, 2008; Liao and Grishman, 2011; Huang and Riloff, 2012) to improve the performance. Some studies proposed joint models to overcome the error propagation problem (Poon and Vanderwende, 2010; Riedel et al., 2009; Li et al., 2013; Venugopal et al., 2014; Li et al., 2014). The second category includes distributional representation based methods which have been applied into event extraction extensively. Most of these approaches are based on the standard Convolutional Neural Networks (CNNs) (Chen et al., 2015; Nguyen and Grishman, 2015, 2016), Recurrent Neural Networks (RNNs) (Nguyen et al., 2016), generative adversarial networks (Hong et al., 2018), zero-shot learning (Huang et al., 2017) and advanced attention mechanisms (Liu et al., 2018b; Chen et al., 2018). Our work is also related to the studies which leverage the external knowledge base for information ex"
N19-1145,R11-1002,0,0.0142687,"especially 1427 the information from the function description positive regulation of transcription, DNA-templated for Tax, our approach successfully promotes the probability of E1 being predicted as the Theme of E2. 4 Related Work As a crucial task in information extraction, event extraction has gained a lot of interest. In general news domain, previous work on event extraction can be divided into two main categories. The first is feature-based methods which mainly focus on feature design, leveraging local features (Grishman et al., 2005; Ahn, 2006) and global features (Ji and Grishman, 2008; Liao and Grishman, 2011; Huang and Riloff, 2012) to improve the performance. Some studies proposed joint models to overcome the error propagation problem (Poon and Vanderwende, 2010; Riedel et al., 2009; Li et al., 2013; Venugopal et al., 2014; Li et al., 2014). The second category includes distributional representation based methods which have been applied into event extraction extensively. Most of these approaches are based on the standard Convolutional Neural Networks (CNNs) (Chen et al., 2015; Nguyen and Grishman, 2015, 2016), Recurrent Neural Networks (RNNs) (Nguyen et al., 2016), generative adversarial network"
N19-1145,P17-1164,0,0.0122005,"nd category includes distributional representation based methods which have been applied into event extraction extensively. Most of these approaches are based on the standard Convolutional Neural Networks (CNNs) (Chen et al., 2015; Nguyen and Grishman, 2015, 2016), Recurrent Neural Networks (RNNs) (Nguyen et al., 2016), generative adversarial networks (Hong et al., 2018), zero-shot learning (Huang et al., 2017) and advanced attention mechanisms (Liu et al., 2018b; Chen et al., 2018). Our work is also related to the studies which leverage the external knowledge base for information extraction. Liu et al. (2017) takes advantage of external resources, such as FrameNet, to label events while Chen et al. (2017) adopts distance supervision to augment the training data. Liu et al. (2018a) develops an attention-based model for event extraction. What’s more, shortest dependency path is broadly explored for information extraction, especially for relation classification (Xu et al., 2015; Miwa and Bansal, 2016) and shows promising benefits. Biomedical event extraction task part of the BioNLP Shared Task series (Kim et al., 2009, 2011; N´edellec et al., 2013). Previous studies mainly explore local and global fe"
N19-1145,D18-1156,0,0.14784,"me the error propagation problem (Poon and Vanderwende, 2010; Riedel et al., 2009; Li et al., 2013; Venugopal et al., 2014; Li et al., 2014). The second category includes distributional representation based methods which have been applied into event extraction extensively. Most of these approaches are based on the standard Convolutional Neural Networks (CNNs) (Chen et al., 2015; Nguyen and Grishman, 2015, 2016), Recurrent Neural Networks (RNNs) (Nguyen et al., 2016), generative adversarial networks (Hong et al., 2018), zero-shot learning (Huang et al., 2017) and advanced attention mechanisms (Liu et al., 2018b; Chen et al., 2018). Our work is also related to the studies which leverage the external knowledge base for information extraction. Liu et al. (2017) takes advantage of external resources, such as FrameNet, to label events while Chen et al. (2017) adopts distance supervision to augment the training data. Liu et al. (2018a) develops an attention-based model for event extraction. What’s more, shortest dependency path is broadly explored for information extraction, especially for relation classification (Xu et al., 2015; Miwa and Bansal, 2016) and shows promising benefits. Biomedical event extr"
N19-1145,W16-6308,0,0.425834,".24 82.31 87.50 87.28 80.28 85.95 53.16 53.61 57.90 52.39 55.73 67.01 83.41 48.72 53.54 64.56 78.75 43.05 53.81 62.18 F1 80.28 75.39 60.87 84.36 68.47 78.73 44.10 43.52 48.26 49.02 47.72 58.65 76.83 40.62 45.64 56.53 73.03 40.65 44.30 54.46 Table 3: Precision (Prec), recall (Rec) and F-score (F1) results achieved by the KB-driven Tree-LSTM model on the test set of BioNLP Genia 2011, evaluated on approximate span and recursive criteria. System TEES(Bj¨orne and Salakoski, 2011) FAUST(Riedel and McCallum, 2011) EventMine-CR(Miwa et al., 2012) BioMLN(Venugopal et al., 2014) Stacked Generalization(Majumder et al., 2016) CNN(Bj¨orne and Salakoski, 2018) Rec Prec F1 49.56 57.65 53.30 49.41 64.75 56.04 53.35 63.48 57.98 53.42 63.61 58.07 48.96 66.46 56.38 49.94 69.45 58.07 Table 4: State-of-the-art system results evaluated on BioNLP Genia 2011 test dataset with approximate span and recursive criteria. ing event extraction is more challenging since it usually has multiple arguments. For example, Figure 4 shows two sentences which are chosen from the output of the development data set. There are two Binding event mentions in the first sentence: E1 (Trigger: interacting, Type: Binding, Theme: RUNX1, Theme2: p3000)"
N19-1145,P16-1105,0,0.0197849,"g (Huang et al., 2017) and advanced attention mechanisms (Liu et al., 2018b; Chen et al., 2018). Our work is also related to the studies which leverage the external knowledge base for information extraction. Liu et al. (2017) takes advantage of external resources, such as FrameNet, to label events while Chen et al. (2017) adopts distance supervision to augment the training data. Liu et al. (2018a) develops an attention-based model for event extraction. What’s more, shortest dependency path is broadly explored for information extraction, especially for relation classification (Xu et al., 2015; Miwa and Bansal, 2016) and shows promising benefits. Biomedical event extraction task part of the BioNLP Shared Task series (Kim et al., 2009, 2011; N´edellec et al., 2013). Previous studies mainly explore local and global features with SVM model (Miwa et al., 2010, 2012; Bj¨orne and Salakoski, 2013; Majumder et al., 2016). Riedel and McCallum (2011) develop a joint model with dual decomposition. Cohen et al. (2009), Kilicoglu and Bergler (2011) and Bui et al. (2013) develop rule-based methods and achieve high precision. Venugopal et al. (2014) leverage Markov logic networks for joint inference. Rao et al. (2017) u"
N19-1145,D16-1085,0,0.0273283,"Missing"
N19-1145,N10-1123,0,0.218195,"es the probability of E1 being predicted as the Theme of E2. 4 Related Work As a crucial task in information extraction, event extraction has gained a lot of interest. In general news domain, previous work on event extraction can be divided into two main categories. The first is feature-based methods which mainly focus on feature design, leveraging local features (Grishman et al., 2005; Ahn, 2006) and global features (Ji and Grishman, 2008; Liao and Grishman, 2011; Huang and Riloff, 2012) to improve the performance. Some studies proposed joint models to overcome the error propagation problem (Poon and Vanderwende, 2010; Riedel et al., 2009; Li et al., 2013; Venugopal et al., 2014; Li et al., 2014). The second category includes distributional representation based methods which have been applied into event extraction extensively. Most of these approaches are based on the standard Convolutional Neural Networks (CNNs) (Chen et al., 2015; Nguyen and Grishman, 2015, 2016), Recurrent Neural Networks (RNNs) (Nguyen et al., 2016), generative adversarial networks (Hong et al., 2018), zero-shot learning (Huang et al., 2017) and advanced attention mechanisms (Liu et al., 2018b; Chen et al., 2018). Our work is also rela"
N19-1145,W17-2315,0,0.547653,"Missing"
N19-1145,W09-1406,0,0.0272412,"ng predicted as the Theme of E2. 4 Related Work As a crucial task in information extraction, event extraction has gained a lot of interest. In general news domain, previous work on event extraction can be divided into two main categories. The first is feature-based methods which mainly focus on feature design, leveraging local features (Grishman et al., 2005; Ahn, 2006) and global features (Ji and Grishman, 2008; Liao and Grishman, 2011; Huang and Riloff, 2012) to improve the performance. Some studies proposed joint models to overcome the error propagation problem (Poon and Vanderwende, 2010; Riedel et al., 2009; Li et al., 2013; Venugopal et al., 2014; Li et al., 2014). The second category includes distributional representation based methods which have been applied into event extraction extensively. Most of these approaches are based on the standard Convolutional Neural Networks (CNNs) (Chen et al., 2015; Nguyen and Grishman, 2015, 2016), Recurrent Neural Networks (RNNs) (Nguyen et al., 2016), generative adversarial networks (Hong et al., 2018), zero-shot learning (Huang et al., 2017) and advanced attention mechanisms (Liu et al., 2018b; Chen et al., 2018). Our work is also related to the studies wh"
N19-1145,W11-1807,0,0.0792817,"opts distance supervision to augment the training data. Liu et al. (2018a) develops an attention-based model for event extraction. What’s more, shortest dependency path is broadly explored for information extraction, especially for relation classification (Xu et al., 2015; Miwa and Bansal, 2016) and shows promising benefits. Biomedical event extraction task part of the BioNLP Shared Task series (Kim et al., 2009, 2011; N´edellec et al., 2013). Previous studies mainly explore local and global features with SVM model (Miwa et al., 2010, 2012; Bj¨orne and Salakoski, 2013; Majumder et al., 2016). Riedel and McCallum (2011) develop a joint model with dual decomposition. Cohen et al. (2009), Kilicoglu and Bergler (2011) and Bui et al. (2013) develop rule-based methods and achieve high precision. Venugopal et al. (2014) leverage Markov logic networks for joint inference. Rao et al. (2017) uses the Abstract Meaning Representations (AMR) to extract events based on the assumption that an event structure can be derived from an AMR subgraph. Recently, some representationbased models (Jagannatha and Yu, 2016; Rao et al., 2017; Bj¨orne and Salakoski, 2018) have been proposed while most of them adopt the widely used CNNs"
N19-1145,P15-1150,0,0.0463187,"−2 fj−2 fj−2 xj ij cj ˜ cj oj hj xj ij cj ˜ fj−1 fj−1 hj−1 oj cj cj−1 hj−1 μ j−1 ˜ j−1 A. a Tree-LSTM unit gj cj−1 KB concept of x hj μj ˜ KB concept of x j B. a KB-driven Tree-LSTM unit Figure 2: (A): a Tree-LSTM unit. (B): a KB-driven Tree-LSTM unit. The yellow circles with µ e notations denote external KB concept embeddings. We first introduce the Tree-LSTM framework, and then describe the construction of KB concept embedding for each entity. Finally we incorporate the KB concept embedding into a Tree-LSTM and apply it for event trigger and argument extraction. 2.1 Tree-LSTM The Tree-LSTM (Tai et al., 2015) is a variation of LSTM (Hochreiter and Schmidhuber, 1997) to a tree-structured network topology. It shows improvement in representing sentence semantic meaning compared to sequential LSTM such as Bidirectional LSTM (BiLSTM) (Graves et al., 2013). The main difference between sequential LSTM and Tree-LSTM is, at each time step, the former calculates its hidden state from the input at the current time step and the hidden state from previous step, while Tree-LSTM computes its hidden state from the input token and the hidden states of all its children nodes from the tree structure. A Tree-LSTM red"
N19-1145,D14-1090,0,0.604672,"al., 2013). The Genia task defines 9 fine-grained event types as shown in Table 1. Note that a Binding event may take more than one protein as its Theme arguments. A Regulation event may take one protein or event as its Theme argument and also optionally take one Experimental Setup We apply our KB-driven Tree-LSTM model on Genia 2011 data set. The entities in Genia data set are manually annotated and given as part of the input. We evaluate our results on the test set using the official online tool provided by the Genia task organizers.1 Following previous studies (Bj¨orne and Salakoski, 2011; Venugopal et al., 2014; Rao et al., 2017; Bj¨orne and Salakoski, 2018), we report scores obtained by the approximate span (allowing trigger spans to differ from gold spans by single words). As we only focus on matching core arguments, we use recursive matching criterion for evaluation which not requires matching of additional arguments for events referred from other events (Kim et al., 2011). We use the word embedding pretrained on PubMed and PMC texts (Moen and Ananiadou, 1425 1 http://bionlp-st.dbcls.jp/GE/2011/eval-test/ System 2013) for word and type embeddings. The hyperparameters are tuned on the development"
N19-1145,D15-1206,0,0.0349407,"zero-shot learning (Huang et al., 2017) and advanced attention mechanisms (Liu et al., 2018b; Chen et al., 2018). Our work is also related to the studies which leverage the external knowledge base for information extraction. Liu et al. (2017) takes advantage of external resources, such as FrameNet, to label events while Chen et al. (2017) adopts distance supervision to augment the training data. Liu et al. (2018a) develops an attention-based model for event extraction. What’s more, shortest dependency path is broadly explored for information extraction, especially for relation classification (Xu et al., 2015; Miwa and Bansal, 2016) and shows promising benefits. Biomedical event extraction task part of the BioNLP Shared Task series (Kim et al., 2009, 2011; N´edellec et al., 2013). Previous studies mainly explore local and global features with SVM model (Miwa et al., 2010, 2012; Bj¨orne and Salakoski, 2013; Majumder et al., 2016). Riedel and McCallum (2011) develop a joint model with dual decomposition. Cohen et al. (2009), Kilicoglu and Bergler (2011) and Bui et al. (2013) develop rule-based methods and achieve high precision. Venugopal et al. (2014) leverage Markov logic networks for joint infere"
N19-1145,W13-2001,0,0.256189,"Missing"
N19-1145,N16-1034,0,0.0384089,"ures (Ji and Grishman, 2008; Liao and Grishman, 2011; Huang and Riloff, 2012) to improve the performance. Some studies proposed joint models to overcome the error propagation problem (Poon and Vanderwende, 2010; Riedel et al., 2009; Li et al., 2013; Venugopal et al., 2014; Li et al., 2014). The second category includes distributional representation based methods which have been applied into event extraction extensively. Most of these approaches are based on the standard Convolutional Neural Networks (CNNs) (Chen et al., 2015; Nguyen and Grishman, 2015, 2016), Recurrent Neural Networks (RNNs) (Nguyen et al., 2016), generative adversarial networks (Hong et al., 2018), zero-shot learning (Huang et al., 2017) and advanced attention mechanisms (Liu et al., 2018b; Chen et al., 2018). Our work is also related to the studies which leverage the external knowledge base for information extraction. Liu et al. (2017) takes advantage of external resources, such as FrameNet, to label events while Chen et al. (2017) adopts distance supervision to augment the training data. Liu et al. (2018a) develops an attention-based model for event extraction. What’s more, shortest dependency path is broadly explored for informati"
N19-1145,P15-2060,0,0.0467616,"ng local features (Grishman et al., 2005; Ahn, 2006) and global features (Ji and Grishman, 2008; Liao and Grishman, 2011; Huang and Riloff, 2012) to improve the performance. Some studies proposed joint models to overcome the error propagation problem (Poon and Vanderwende, 2010; Riedel et al., 2009; Li et al., 2013; Venugopal et al., 2014; Li et al., 2014). The second category includes distributional representation based methods which have been applied into event extraction extensively. Most of these approaches are based on the standard Convolutional Neural Networks (CNNs) (Chen et al., 2015; Nguyen and Grishman, 2015, 2016), Recurrent Neural Networks (RNNs) (Nguyen et al., 2016), generative adversarial networks (Hong et al., 2018), zero-shot learning (Huang et al., 2017) and advanced attention mechanisms (Liu et al., 2018b; Chen et al., 2018). Our work is also related to the studies which leverage the external knowledge base for information extraction. Liu et al. (2017) takes advantage of external resources, such as FrameNet, to label events while Chen et al. (2017) adopts distance supervision to augment the training data. Liu et al. (2018a) develops an attention-based model for event extraction. What’s m"
P13-1107,W08-0336,0,0.0614196,"Missing"
P13-1107,P98-1069,0,0.0611667,"m well on morph resolution. In this paper we exploit cross-genre information and social correlation to measure semantic similarity. (Yang et al., 2011; Huang et al., 2012) also showed the effectiveness of exploiting information from formal web documents to enhance tweet summarization and tweet ranking. Other similar research lines are the TAC-KBP Entity Linking (EL) (Ji et al., 2010; Ji et al., 2011), which links a named entity in news and web documents to an appropriate knowledge base (KB) entry, the task of mining name translation pairs from comparable corpora (Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Hassan et al., 2007) and the link prediction problem (Adamic and Adar, 2001; LibenNowell and Kleinberg, 2003; Sun et al., 2011b; Hasan et al., 2006; Wang et al., 2007; Sun et al., 2011a). Most of the work focused on unstructured or structured data with clean and rich relations (e.g. DBLP). In contrast, our work constructs heterogeneous information networks from unstructured, noisy multi-genre text without explicit entity attributes. 7 Conclusion and Future Work To the best of our knowledge, this is the first work of resolving implicit information morphs from th"
P13-1107,P08-1030,1,0.799097,"than one type of links. In order to construct the information networks for morphs, we apply the Standford Chinese word 1085 segmenter with Chinese Penn Treebank segmentation standard (Chang et al., 2008) and Stanford part-of-speech tagger (Toutanova et al., 2003) to process each sentence in the comparable data sets. Then we apply a hierarchical Hidden Markov Model (HMM) based Chinese lexical analyzer ICTCLAS (Zhang et al., 2003) to extract named entities, noun phrases and events. We have also attempted using the results from Dependency Parsing, Relation Extraction and Event Extraction tools (Ji and Grishman, 2008) to enrich the link types. Unfortunately the stateof-the-art techniques for these tasks still perform poorly on social media in terms of both accuracy and coverage of important information, these sophisticated semantic links all produced negative impact on the target ranking performance. Therefore we limited the types of vertices into: Morph (M), Entity(E), which includes target candidates, Event (EV), and Non-Entity Noun Phrases (NP); and used co-occurrence as the edge type. We extract entities, events, and non-entity noun phrases that occur in more than one tweet as neighbors. And for two ve"
P13-1107,W09-3107,1,0.850868,"not perform well on morph resolution. In this paper we exploit cross-genre information and social correlation to measure semantic similarity. (Yang et al., 2011; Huang et al., 2012) also showed the effectiveness of exploiting information from formal web documents to enhance tweet summarization and tweet ranking. Other similar research lines are the TAC-KBP Entity Linking (EL) (Ji et al., 2010; Ji et al., 2011), which links a named entity in news and web documents to an appropriate knowledge base (KB) entry, the task of mining name translation pairs from comparable corpora (Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Hassan et al., 2007) and the link prediction problem (Adamic and Adar, 2001; LibenNowell and Kleinberg, 2003; Sun et al., 2011b; Hasan et al., 2006; Wang et al., 2007; Sun et al., 2011a). Most of the work focused on unstructured or structured data with clean and rich relations (e.g. DBLP). In contrast, our work constructs heterogeneous information networks from unstructured, noisy multi-genre text without explicit entity attributes. 7 Conclusion and Future Work To the best of our knowledge, this is the first work of resolving implicit inform"
P13-1107,P10-1142,0,0.0237712,"candidate of m if and only if, for each tmi ∈ Tm (i = 1, 2, ..., Zm ), there exist a j ∈ {1, 2, ..., Ze } such that tmi − tej ≤ δ, where δ is a threshold value (in this paper we set the threshold to 7 days, which is optimized from a development set). For comparison we also attempted topic modeling approach to detect target candidates, as shown in section 5.3. 4 Target Candidate Ranking 4.2.1 Surface Features We first extract surface features between the morph and the candidate based on measuring orthographic similarity measures which were commonly used in entity coreference resolution (e.g. (Ng, 2010; Hsiung et al., 2005)). The measures we use include “string edit distance”, “normalized string edit distance” (Wagner and Fischer, 1974) and “longest common subsequence” (Hirschberg, 1977). Semantic Features Motivations Fortunately, although a morph and its target may have very different orthographic forms, they tend to be embedded in similar semantic contexts which involve similar topics and events. Figure 2 presents some example messages under censorship (Weibo) and not under censorship (Twitter and Chinese Daily). We can see that they include similar topics, events (e.g., “fell from power”"
P13-1107,P99-1067,0,0.0682673,"lution. In this paper we exploit cross-genre information and social correlation to measure semantic similarity. (Yang et al., 2011; Huang et al., 2012) also showed the effectiveness of exploiting information from formal web documents to enhance tweet summarization and tweet ranking. Other similar research lines are the TAC-KBP Entity Linking (EL) (Ji et al., 2010; Ji et al., 2011), which links a named entity in news and web documents to an appropriate knowledge base (KB) entry, the task of mining name translation pairs from comparable corpora (Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Hassan et al., 2007) and the link prediction problem (Adamic and Adar, 2001; LibenNowell and Kleinberg, 2003; Sun et al., 2011b; Hasan et al., 2006; Wang et al., 2007; Sun et al., 2011a). Most of the work focused on unstructured or structured data with clean and rich relations (e.g. DBLP). In contrast, our work constructs heterogeneous information networks from unstructured, noisy multi-genre text without explicit entity attributes. 7 Conclusion and Future Work To the best of our knowledge, this is the first work of resolving implicit information morphs from the data under"
P13-1107,C04-1089,0,0.0294334,"his paper we exploit cross-genre information and social correlation to measure semantic similarity. (Yang et al., 2011; Huang et al., 2012) also showed the effectiveness of exploiting information from formal web documents to enhance tweet summarization and tweet ranking. Other similar research lines are the TAC-KBP Entity Linking (EL) (Ji et al., 2010; Ji et al., 2011), which links a named entity in news and web documents to an appropriate knowledge base (KB) entry, the task of mining name translation pairs from comparable corpora (Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Hassan et al., 2007) and the link prediction problem (Adamic and Adar, 2001; LibenNowell and Kleinberg, 2003; Sun et al., 2011b; Hasan et al., 2006; Wang et al., 2007; Sun et al., 2011a). Most of the work focused on unstructured or structured data with clean and rich relations (e.g. DBLP). In contrast, our work constructs heterogeneous information networks from unstructured, noisy multi-genre text without explicit entity attributes. 7 Conclusion and Future Work To the best of our knowledge, this is the first work of resolving implicit information morphs from the data under active censorship."
P13-1107,N03-1033,0,0.0200878,"(e) ∈ R. If two links belong to the same relation type, then they share the same starting object type as well as the same ending object type. An information network is homogeneous if and only if there is only one type for both objects and links, and an information network is heterogeneous when the objects are from multiple distinct types or there exist more than one type of links. In order to construct the information networks for morphs, we apply the Standford Chinese word 1085 segmenter with Chinese Penn Treebank segmentation standard (Chang et al., 2008) and Stanford part-of-speech tagger (Toutanova et al., 2003) to process each sentence in the comparable data sets. Then we apply a hierarchical Hidden Markov Model (HMM) based Chinese lexical analyzer ICTCLAS (Zhang et al., 2003) to extract named entities, noun phrases and events. We have also attempted using the results from Dependency Parsing, Relation Extraction and Event Extraction tools (Ji and Grishman, 2008) to enrich the link types. Unfortunately the stateof-the-art techniques for these tasks still perform poorly on social media in terms of both accuracy and coverage of important information, these sophisticated semantic links all produced nega"
P13-1107,E09-1091,0,0.0220577,"tection methods did not perform well on morph resolution. In this paper we exploit cross-genre information and social correlation to measure semantic similarity. (Yang et al., 2011; Huang et al., 2012) also showed the effectiveness of exploiting information from formal web documents to enhance tweet summarization and tweet ranking. Other similar research lines are the TAC-KBP Entity Linking (EL) (Ji et al., 2010; Ji et al., 2011), which links a named entity in news and web documents to an appropriate knowledge base (KB) entry, the task of mining name translation pairs from comparable corpora (Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Hassan et al., 2007) and the link prediction problem (Adamic and Adar, 2001; LibenNowell and Kleinberg, 2003; Sun et al., 2011b; Hasan et al., 2006; Wang et al., 2007; Sun et al., 2011a). Most of the work focused on unstructured or structured data with clean and rich relations (e.g. DBLP). In contrast, our work constructs heterogeneous information networks from unstructured, noisy multi-genre text without explicit entity attributes. 7 Conclusion and Future Work To the best of our knowledge, this is the first work of resolving impli"
P13-1107,W03-1730,0,0.0181433,"eneous if and only if there is only one type for both objects and links, and an information network is heterogeneous when the objects are from multiple distinct types or there exist more than one type of links. In order to construct the information networks for morphs, we apply the Standford Chinese word 1085 segmenter with Chinese Penn Treebank segmentation standard (Chang et al., 2008) and Stanford part-of-speech tagger (Toutanova et al., 2003) to process each sentence in the comparable data sets. Then we apply a hierarchical Hidden Markov Model (HMM) based Chinese lexical analyzer ICTCLAS (Zhang et al., 2003) to extract named entities, noun phrases and events. We have also attempted using the results from Dependency Parsing, Relation Extraction and Event Extraction tools (Ji and Grishman, 2008) to enrich the link types. Unfortunately the stateof-the-art techniques for these tasks still perform poorly on social media in terms of both accuracy and coverage of important information, these sophisticated semantic links all produced negative impact on the target ranking performance. Therefore we limited the types of vertices into: Morph (M), Entity(E), which includes target candidates, Event (EV), and N"
P13-1107,C98-1066,0,\N,Missing
P13-1107,C12-1076,1,\N,Missing
P14-2115,P13-2041,0,0.0216675,"sformable, we substitute the part of e with t to form a new morph. For example, we can substitute the characters of “比尔 盖茨 (Bill Gates) [Bi Er Gai Ci]” with “鼻耳 (Nose and ear) [Bi Er]” and “盖子 (Lid) [Gai Zi]” to form new morph “鼻耳 盖子 (Nose and ear Lid) [Bi Er Gai Zi]”. We rank the candidates based on the following two criteria: (1) If the morph includes more negative words (based on a gazetteer including 11,729 negative words derived from HowNet (Dong and Dong, 1999), it’s more humorous (Valitutti et al., 2013). (2) If the morph includes rarer terms with low frequency, it is more interesting (Petrovic and Matthews, 2013). 2.3 2.5 M4: Translation and Transliteration Given an entity e, we search its English translation EN(e) based on 94,015 name translation pairs (Ji et al., 2009). Then, if any name component in EN(e) is a common English word, we search for its Chinese translation based on a 94,966 word translation pairs (Zens and Ney, 2004), and use the Chinese translation to replace the corresponding characters in e. For example, we create a morph “拉里 鸟儿 (Larry bird)” for “拉里 伯德 (Larry Bird)” by replacing the last name “伯德 (Bird)” with its Chinese translation “鸟儿 (bird)”. 2.6 M5: Semantic Interpretation For e"
P14-2115,P13-2044,0,0.0208414,") are mutually transformable. If a part of pinyin(e) and pinyin(t) are identical or their initials are transformable, we substitute the part of e with t to form a new morph. For example, we can substitute the characters of “比尔 盖茨 (Bill Gates) [Bi Er Gai Ci]” with “鼻耳 (Nose and ear) [Bi Er]” and “盖子 (Lid) [Gai Zi]” to form new morph “鼻耳 盖子 (Nose and ear Lid) [Bi Er Gai Zi]”. We rank the candidates based on the following two criteria: (1) If the morph includes more negative words (based on a gazetteer including 11,729 negative words derived from HowNet (Dong and Dong, 1999), it’s more humorous (Valitutti et al., 2013). (2) If the morph includes rarer terms with low frequency, it is more interesting (Petrovic and Matthews, 2013). 2.3 2.5 M4: Translation and Transliteration Given an entity e, we search its English translation EN(e) based on 94,015 name translation pairs (Ji et al., 2009). Then, if any name component in EN(e) is a common English word, we search for its Chinese translation based on a 94,966 word translation pairs (Zens and Ney, 2004), and use the Chinese translation to replace the corresponding characters in e. For example, we create a morph “拉里 鸟儿 (Larry bird)” for “拉里 伯德 (Larry Bird)” by rep"
P14-2115,P13-1107,1,0.575788,"ng morphs. One main purpose of encoding morphs is to disseminate them widely so they can become part of the new Internet language. Therefore morphs should be interesting, fun, intuitive and easy to remember. (5) Morphs rapidly evolve over time, as some morphs are discovered and blocked by censorship and newly created morphs emerge. We propose a brand new and challenging research problem - can we automatically encode morphs for any given entity to help users communicate in an appropriate and fun way? Introduction One of the most innovative linguistic forms in social media is Information Morph (Huang et al., 2013). Morph is a special case of alias to hide the original objects (e.g., sensitive entities and events) for different purposes, including avoiding censorship (Bamman et al., 2012; Chen et al., 2013), expressing strong sentiment, emotion or sarcasm, and making descriptions more vivid. Morphs are widely used in Chinese social media. Here is an example morphs: “由于瓜爹的事情，方便面与 天线摊牌. (Because of Gua Dad’s issue, Instant Noodles faces down with Antenna.)”, where • “瓜爹 (Gua Dad)” refers to “薄熙来 (Bo Xilai)” because it shares one character “瓜 (Gua)” with “薄瓜瓜 (Bo Guagua)” who is the son of “薄熙 来 (Bo Xilai)"
P14-2115,P13-1072,0,0.0337907,"Missing"
P14-2115,P11-1115,1,0.782334,"taring at sea and listening to surf)” as a present when he visited China. In the films Ma Jingtao starred, he always used exaggerated roaring to express various emotions. The morph derives from Ma Yingjiu’s political position on cross-strait relations. Table 2: Morph Examples Categorized based on Human Generation Methods based on their semantic contexts. For example, this approach generates a morph “太祖 (the First Emperor)” for “毛泽东 (Mao Zedong)” who is the first chairman of P. R. China and “高祖 (the Second Emperor )” for “邓小平 (Deng Xiaoping )” who succeeded Mao. 2.8 al., 2010; Ji et al., 2011; Ji and Grishman, 2011) which include 3 million news and web documents, and DARPA BOLT program’s discussion forum corpora with 300k threads. Given an entity e, we compute the semantic relationship between e and each word from these corpora. We then rank the words by: (1) cosine similarity, (2) the same criteria as in section 2.6. Finally we append the top ranking word to the entity’s last name to obtain a new morph. Using this method, we are able to generate many vivid morphs such as “姚 奇才 (Yao Wizard)” for “姚 明 (Yao Ming)”. M7: Characteristics Modeling Finally, we propose a novel approach to automatically generate"
P14-2115,I13-1015,0,0.0216269,"Missing"
P14-2115,W06-2808,0,0.0198146,"Missing"
P14-2115,I05-3013,0,0.0556604,"Missing"
P14-2115,P06-1125,0,0.0446686,"Missing"
P14-2115,N04-1033,0,0.0106403,"If the morph includes more negative words (based on a gazetteer including 11,729 negative words derived from HowNet (Dong and Dong, 1999), it’s more humorous (Valitutti et al., 2013). (2) If the morph includes rarer terms with low frequency, it is more interesting (Petrovic and Matthews, 2013). 2.3 2.5 M4: Translation and Transliteration Given an entity e, we search its English translation EN(e) based on 94,015 name translation pairs (Ji et al., 2009). Then, if any name component in EN(e) is a common English word, we search for its Chinese translation based on a 94,966 word translation pairs (Zens and Ney, 2004), and use the Chinese translation to replace the corresponding characters in e. For example, we create a morph “拉里 鸟儿 (Larry bird)” for “拉里 伯德 (Larry Bird)” by replacing the last name “伯德 (Bird)” with its Chinese translation “鸟儿 (bird)”. 2.6 M5: Semantic Interpretation For each character ck in the first name of a given entity name e, we search its semantic interpretation sentence from the Xinhua Chinese character dictionary including 20,894 entries 3 . If a word in the sentence contains ck , we append the word with the last name of e to form a new morph. Similarly to M1, we prefer positive, ne"
P14-2115,D08-1108,0,0.0445781,"Missing"
P14-2115,J98-4003,1,\N,Missing
P15-1057,W08-0336,0,0.0275193,"riginal meanings. 2 Problem Formulation Following the recent work on morphs (Huang et al., 2013; Zhang et al., 2014), we use Chinese Weibo tweets for experiments. Our goal is to develop an end-to-end system that automatically extract morph mentions and resolve them to their target entities. Given a corpus of tweets D = {d1 , d2 , ..., d|D |}, we define a candidate morph mi as a unique term tj in T , where T = {t1 , t2 , ..., t|T |} is the set of unique terms in D. To extract T , we first apply several well-developed Natural Language Processing tools, including Stanford Chinese word segmenter (Chang et al., 2008), Stanford part-ofspeech tagger (Toutanova et al., 2003) and Chinese lexical analyzer ICTCLAS (Zhang et al., 2003), to process the tweets and identify noun phrases. Then we define a morph mention mpi of mi as the p-th occurrence of mi in a specific document dj . Note that a mention with the same surface form as mi but referring to its original entity is not considered as a morph mention. For instance, the “平西 王 (Conquer West King)” in d1 and d3 in Figure 1 are morph mentions since they refer to the modern politician “薄熙来 (Bo Xilai)”, while the one in d4 is not a morph mention since it refers t"
P15-1057,P06-1017,0,0.020688,"Missing"
P15-1057,P13-2006,0,0.0430093,"Missing"
P15-1057,W13-0908,0,0.0277964,"d malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wider range of semantic categories and can include either abstractive or concrete information. Some common features for detect"
P15-1057,P13-1107,1,0.928563,"ecoding humangenerated morphs in text is critical for downstream deep language understanding tasks such as entity linking and event argument extraction. However, even for human, it is difficult to decode many morphs without certain historical, cultural, or political background knowledge (Zhang et al., 2014). For example, “The Hutt” can be used to refer to a fictional alien entity in the Star Wars universe (“The Hutt stayed and established himself as ruler of Nam Chorios”), or the governor of New Jersey, Chris Christie (“The Hutt announced a bid for a seat in the New Jersey General Assembly”). Huang et al. (2013) did a pioneering pilot study on morph resolution, but their approach assumed the entity morphs were already extracted and used a large amount of labeled data. In fact, they resolved morphs on corpus-level instead of mention-level and thus their approach was context-independent. A practical morph decoder, as depicted in Figure 1, consists of two problems: (1) Morph Extraction: given a corpus, extract morph mentions; and (2). Morph Resolution: For each morph mention, figure out the entity that it refers to. In this paper, we aim to solve the fundamental research problem of end-to-end morph deco"
P15-1057,P14-1036,1,0.900239,"Missing"
P15-1057,D14-1067,0,0.0149744,"(e.g., named entities), while morphs tend to be informal and convey implicit information. Morph mention detection is also related to malware detection (e.g., (Firdausi et al., 2010; Chandola et al., 2009; Firdausi et al., 2010; Christodorescu and Jha, 2003)) which discovers abnormal behavior in code and malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. A"
P15-1057,W13-0906,0,0.0253293,"avior in code and malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wider range of semantic categories and can include either abstractive or concrete information. Some comm"
P15-1057,P14-1038,1,0.800559,"2010; Christodorescu and Jha, 2003)) which discovers abnormal behavior in code and malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wider range of semantic categories and ca"
P15-1057,D14-1071,0,0.0150722,"s), while morphs tend to be informal and convey implicit information. Morph mention detection is also related to malware detection (e.g., (Firdausi et al., 2010; Chandola et al., 2009; Firdausi et al., 2010; Christodorescu and Jha, 2003)) which discovers abnormal behavior in code and malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in secti"
P15-1057,P95-1026,0,0.257659,"s in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wider range of semantic categories and can include either abstractive or concrete information. Some common features for detecting metaphors (e.g. (Tsvetkov, 593 cial to keep the genre"
P15-1057,P14-2105,0,0.0142968,"nd formal entities (e.g., named entities), while morphs tend to be informal and convey implicit information. Morph mention detection is also related to malware detection (e.g., (Firdausi et al., 2010; Chandola et al., 2009; Firdausi et al., 2010; Christodorescu and Jha, 2003)) which discovers abnormal behavior in code and malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to t"
P15-1057,N07-1025,0,0.0387675,"ntext. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wider range of semantic categories and can include either abstractive or concrete information. Some common features for detecting metaphors (e.g. (Tsvetkov, 593 cial to keep the genres consistent bet"
P15-1057,W03-1730,0,0.00977908,"4), we use Chinese Weibo tweets for experiments. Our goal is to develop an end-to-end system that automatically extract morph mentions and resolve them to their target entities. Given a corpus of tweets D = {d1 , d2 , ..., d|D |}, we define a candidate morph mi as a unique term tj in T , where T = {t1 , t2 , ..., t|T |} is the set of unique terms in D. To extract T , we first apply several well-developed Natural Language Processing tools, including Stanford Chinese word segmenter (Chang et al., 2008), Stanford part-ofspeech tagger (Toutanova et al., 2003) and Chinese lexical analyzer ICTCLAS (Zhang et al., 2003), to process the tweets and identify noun phrases. Then we define a morph mention mpi of mi as the p-th occurrence of mi in a specific document dj . Note that a mention with the same surface form as mi but referring to its original entity is not considered as a morph mention. For instance, the “平西 王 (Conquer West King)” in d1 and d3 in Figure 1 are morph mentions since they refer to the modern politician “薄熙来 (Bo Xilai)”, while the one in d4 is not a morph mention since it refers to the original entity, who was king “吴三桂 (Wu Sangui)”. For each morph mention, we discover a list of target candid"
P15-1057,P14-2115,1,0.48139,"{zhangb8,huangh9,panx2,jih,yener}@rpi.edu, 2 lisujian@pku.edu.cn, 3 cyl@microsoft.com 4 hanj@illinois.edu, 5 zhenwen@us.ibm.com, 6 yzsun@ccs.neu.edu, 7 hanj@illinois.edu Abstract a morph “Su-tooth” was created to refer to the Uruguay striker “Luis Suarez” for his habit of biting other players. Automatically decoding humangenerated morphs in text is critical for downstream deep language understanding tasks such as entity linking and event argument extraction. However, even for human, it is difficult to decode many morphs without certain historical, cultural, or political background knowledge (Zhang et al., 2014). For example, “The Hutt” can be used to refer to a fictional alien entity in the Star Wars universe (“The Hutt stayed and established himself as ruler of Nam Chorios”), or the governor of New Jersey, Chris Christie (“The Hutt announced a bid for a seat in the New Jersey General Assembly”). Huang et al. (2013) did a pioneering pilot study on morph resolution, but their approach assumed the entity morphs were already extracted and used a large amount of labeled data. In fact, they resolved morphs on corpus-level instead of mention-level and thus their approach was context-independent. A practic"
P15-1057,P05-1049,0,0.0177967,"Missing"
P15-1057,D08-1063,0,0.0286454,"2010; Chandola et al., 2009; Firdausi et al., 2010; Christodorescu and Jha, 2003)) which discovers abnormal behavior in code and malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wide"
P15-1057,W12-4304,0,0.0148793,"; Firdausi et al., 2010; Christodorescu and Jha, 2003)) which discovers abnormal behavior in code and malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wider range of semantic"
P15-1057,N03-1033,0,0.00839243,"e recent work on morphs (Huang et al., 2013; Zhang et al., 2014), we use Chinese Weibo tweets for experiments. Our goal is to develop an end-to-end system that automatically extract morph mentions and resolve them to their target entities. Given a corpus of tweets D = {d1 , d2 , ..., d|D |}, we define a candidate morph mi as a unique term tj in T , where T = {t1 , t2 , ..., t|T |} is the set of unique terms in D. To extract T , we first apply several well-developed Natural Language Processing tools, including Stanford Chinese word segmenter (Chang et al., 2008), Stanford part-ofspeech tagger (Toutanova et al., 2003) and Chinese lexical analyzer ICTCLAS (Zhang et al., 2003), to process the tweets and identify noun phrases. Then we define a morph mention mpi of mi as the p-th occurrence of mi in a specific document dj . Note that a mention with the same surface form as mi but referring to its original entity is not considered as a morph mention. For instance, the “平西 王 (Conquer West King)” in d1 and d3 in Figure 1 are morph mentions since they refer to the modern politician “薄熙来 (Bo Xilai)”, while the one in d4 is not a morph mention since it refers to the original entity, who was king “吴三桂 (Wu Sangui)”. F"
P15-5001,P13-1107,1,0.867799,"Missing"
P15-5001,P14-1036,1,0.885699,"Missing"
P15-5001,C14-1149,1,0.832877,"Missing"
P16-1025,C96-1079,0,0.793923,"ccording to predefined event types. The extraction quality of new event types is also promising. 1 E1. Two Soldiers were killed and one injured in the close-quarters fighting in Kut. E2. Bill Bennet’s glam gambling loss changed my opinion. Introduction E3. Gen. Vincent Brooks announced the capture of Barzan Ibrahim Hasan al-Tikriti, telling reporters he was an adviser to Saddam. Event extraction aims at identifying and typing trigger words and participants (arguments). It remains a challenging and costly task. The first question is what to extract? The TIPSTER (Onyshkevych et al., 1993), MUC (Grishman and Sundheim, 1996), CoNLL (Tjong et al., 2003; Pradhan et al., 2011), ACE 1 and TACKBP (Ji and Grishman, 2011) programs found that it was feasible to manually define an event schema based on the needs of potential users. An ACE event schema example is shown in Figure 1. This process is very expensive because consumers and 1 E4. This was the Italian ship that was captured by Palestinian terrorists back in 1985. E5. Ayman Sabawi Ibrahim was arrested in Tikrit and was sentenced to life in prison. We seek to cluster the event triggers and event arguments so that each cluster represents a type. We rely on distributi"
P16-1025,P03-2030,0,0.054262,"er and argument representations are then passed to a joint constraint clustering framework. Finally, we name each cluster of triggers, and name each trigger’s arguments using mappings between the meaning representation and semantic role descriptions in FrameNet, VerbNet (Kipper et al., 2008) and Propbank (Palmer et al., 2005). We compare settings in which semantic relations connecting triggers to context words are derived from three meaning representations: Abstract Meaning Representation (AMR) (Banarescu et al., 2013), Stanford Typed Dependencies (Marie-Catherine et al., 2006), and FrameNet (Baker and Sato, 2003). We derive semantic relations automatically for these three representations using CAMR (Wang et al., 2015a), Stanford’s dependency parser (Manning, 2003), and SEMAFOR (Das et al., 2014), respectively. 2.2 which is used to combine multiple sentences into one AMR graph.3 When FrameNet is the meaning representation we allow all frame relations to identify arguments. For dependencies, we manually mapped dependency relations to AMR relations and use Table 2. Categories Core roles Non-core roles Temporal Spatial Relations ARG0, ARG1, ARG2, ARG3, ARG4 mod, location, poss, manner, topic, medium, inst"
P16-1025,W13-2322,0,0.0832222,"over vectors in the embedding space. Argument representations are generated as a by-product. Trigger and argument representations are then passed to a joint constraint clustering framework. Finally, we name each cluster of triggers, and name each trigger’s arguments using mappings between the meaning representation and semantic role descriptions in FrameNet, VerbNet (Kipper et al., 2008) and Propbank (Palmer et al., 2005). We compare settings in which semantic relations connecting triggers to context words are derived from three meaning representations: Abstract Meaning Representation (AMR) (Banarescu et al., 2013), Stanford Typed Dependencies (Marie-Catherine et al., 2006), and FrameNet (Baker and Sato, 2003). We derive semantic relations automatically for these three representations using CAMR (Wang et al., 2015a), Stanford’s dependency parser (Manning, 2003), and SEMAFOR (Das et al., 2014), respectively. 2.2 which is used to combine multiple sentences into one AMR graph.3 When FrameNet is the meaning representation we allow all frame relations to identify arguments. For dependencies, we manually mapped dependency relations to AMR relations and use Table 2. Categories Core roles Non-core roles Tempora"
P16-1025,P13-1088,0,0.0611697,"Missing"
P16-1025,N07-4013,0,0.0723425,"Missing"
P16-1025,P11-1113,0,0.0548102,"r = O(Ccurr , Ccurr ) if Ocurr < Omin T A ∗ Omin = Ocurr , C T = Ccurr , C A = Ccurr – while iterate time ≤ 10 T T A ∗ Ccurr = spectral(T, EgT , ER , KT , Ccurr ) A A T ∗ Ccurr = spectral(A, Eg , KA , Ccurr ) T A ∗ Ocurr = O(Ccurr , Ccurr ) ∗ if Ocurr < Omin T A · Omin =Ocurr , C T = Ccurr , C A = Ccurr Table 4: None-Core Role Mapping. compare the impact of perfect AMR and system generated AMR. To compare with state-of-the-art event extraction on Automatic Content Extraction (ACE2005) data, we follow the same evaluation setting in previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011) and use 40 newswire documents as our test set. • return Omin , C T , C A ; 3.2 AMR Core Role fire.1 ARG0 fire.1 ARG1 extrude.1 ARG0 extrude.1 ARG1 extrude.1 ARG2 blood.1 ARG0 blood.1 ARG1 Table 5 compares the coverage of event schema discovered by our approach, using AMR as meaning representation, with the predefined ACE and ERE event schemas. Besides the types defined in ACE and ERE, this approach discovers many new event types such as Build and Threaten as displayed in Figure 6. Our approach can also discover new argument roles for a given event type. For example, for Attack events, besides"
P16-1025,P08-1004,0,0.117222,"Missing"
P16-1025,E06-1018,0,0.0144014,"Missing"
P16-1025,N06-2015,0,0.0484569,"sed on Hypothesis 1, we learn sense-based embeddings from a large data set, using the Continuous Skip-gram model (Mikolov et al., 2013). Specifically, we first apply WSD to link each word to its sense in WordNet using a state-of-the-art tool (Zhong and Ng, 2010), and map WordNet sense output to OntoNotes senses. 4 We map each trigger candidate to its OntoNotes sense and learn a distinct embedding for each sense. We use general lexical embeddings for arguments. Candidate Trigger and Argument Identification Given a sentence, we consider all noun and verb concepts that are assigned an OntoNotes (Hovy et al., 2006) sense by WSD as candidate event triggers. Any remaining concepts that match both a verbal and a nominal lexical unit in the FrameNet corpus are considered candidate event triggers as well. This mainly helps to identify more nominal triggers like “pickpocket” and “sin”.2 For each candidate event trigger, we consider as candidate arguments all concepts for which one of a manually-selected set of semantic relations holds between it and the event trigger. For the setting in which AMR serves as our meaning representation, we selected a subset of all AMR relations that specify event arguments, as s"
P16-1025,P08-1030,1,0.847292,"KT ) A A Ccurr = spectral(A, Eg , KA ) T A Ocurr = O(Ccurr , Ccurr ) if Ocurr < Omin T A ∗ Omin = Ocurr , C T = Ccurr , C A = Ccurr – while iterate time ≤ 10 T T A ∗ Ccurr = spectral(T, EgT , ER , KT , Ccurr ) A A T ∗ Ccurr = spectral(A, Eg , KA , Ccurr ) T A ∗ Ocurr = O(Ccurr , Ccurr ) ∗ if Ocurr < Omin T A · Omin =Ocurr , C T = Ccurr , C A = Ccurr Table 4: None-Core Role Mapping. compare the impact of perfect AMR and system generated AMR. To compare with state-of-the-art event extraction on Automatic Content Extraction (ACE2005) data, we follow the same evaluation setting in previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011) and use 40 newswire documents as our test set. • return Omin , C T , C A ; 3.2 AMR Core Role fire.1 ARG0 fire.1 ARG1 extrude.1 ARG0 extrude.1 ARG1 extrude.1 ARG2 blood.1 ARG0 blood.1 ARG1 Table 5 compares the coverage of event schema discovered by our approach, using AMR as meaning representation, with the predefined ACE and ERE event schemas. Besides the types defined in ACE and ERE, this approach discovers many new event types such as Build and Threaten as displayed in Figure 6. Our approach can also discover new argument roles for a given event"
P16-1025,E09-1013,0,0.0124643,"Missing"
P16-1025,P11-1115,1,0.769669,"E1. Two Soldiers were killed and one injured in the close-quarters fighting in Kut. E2. Bill Bennet’s glam gambling loss changed my opinion. Introduction E3. Gen. Vincent Brooks announced the capture of Barzan Ibrahim Hasan al-Tikriti, telling reporters he was an adviser to Saddam. Event extraction aims at identifying and typing trigger words and participants (arguments). It remains a challenging and costly task. The first question is what to extract? The TIPSTER (Onyshkevych et al., 1993), MUC (Grishman and Sundheim, 1996), CoNLL (Tjong et al., 2003; Pradhan et al., 2011), ACE 1 and TACKBP (Ji and Grishman, 2011) programs found that it was feasible to manually define an event schema based on the needs of potential users. An ACE event schema example is shown in Figure 1. This process is very expensive because consumers and 1 E4. This was the Italian ship that was captured by Palestinian terrorists back in 1985. E5. Ayman Sabawi Ibrahim was arrested in Tikrit and was sentenced to life in prison. We seek to cluster the event triggers and event arguments so that each cluster represents a type. We rely on distributional similarity for our clustering distance metric. The distributional hypothesis (Harris, 1"
P16-1025,P11-1098,0,0.270072,"Missing"
P16-1025,D13-1185,0,0.0229869,"Missing"
P16-1025,C12-1033,0,0.0174421,"Missing"
P16-1025,P13-1008,1,0.853732,"Missing"
P16-1025,P15-1017,0,0.247756,"Missing"
P16-1025,P10-1081,0,0.0298077,"ral(A, Eg , KA ) T A Ocurr = O(Ccurr , Ccurr ) if Ocurr < Omin T A ∗ Omin = Ocurr , C T = Ccurr , C A = Ccurr – while iterate time ≤ 10 T T A ∗ Ccurr = spectral(T, EgT , ER , KT , Ccurr ) A A T ∗ Ccurr = spectral(A, Eg , KA , Ccurr ) T A ∗ Ocurr = O(Ccurr , Ccurr ) ∗ if Ocurr < Omin T A · Omin =Ocurr , C T = Ccurr , C A = Ccurr Table 4: None-Core Role Mapping. compare the impact of perfect AMR and system generated AMR. To compare with state-of-the-art event extraction on Automatic Content Extraction (ACE2005) data, we follow the same evaluation setting in previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011) and use 40 newswire documents as our test set. • return Omin , C T , C A ; 3.2 AMR Core Role fire.1 ARG0 fire.1 ARG1 extrude.1 ARG0 extrude.1 ARG1 extrude.1 ARG2 blood.1 ARG0 blood.1 ARG1 Table 5 compares the coverage of event schema discovered by our approach, using AMR as meaning representation, with the predefined ACE and ERE event schemas. Besides the types defined in ACE and ERE, this approach discovers many new event types such as Build and Threaten as displayed in Figure 6. Our approach can also discover new argument roles for a given event type. For example, for At"
P16-1025,J14-1002,0,0.0287898,"n the meaning representation and semantic role descriptions in FrameNet, VerbNet (Kipper et al., 2008) and Propbank (Palmer et al., 2005). We compare settings in which semantic relations connecting triggers to context words are derived from three meaning representations: Abstract Meaning Representation (AMR) (Banarescu et al., 2013), Stanford Typed Dependencies (Marie-Catherine et al., 2006), and FrameNet (Baker and Sato, 2003). We derive semantic relations automatically for these three representations using CAMR (Wang et al., 2015a), Stanford’s dependency parser (Manning, 2003), and SEMAFOR (Das et al., 2014), respectively. 2.2 which is used to combine multiple sentences into one AMR graph.3 When FrameNet is the meaning representation we allow all frame relations to identify arguments. For dependencies, we manually mapped dependency relations to AMR relations and use Table 2. Categories Core roles Non-core roles Temporal Spatial Relations ARG0, ARG1, ARG2, ARG3, ARG4 mod, location, poss, manner, topic, medium, instrument, duration, prep-X year, duration, decade, weekday, time destination, path, location Table 2: Event-Related AMR Relations. In E1, for example, “killed”, “injured” and “fighting” ar"
P16-1025,C10-2087,0,0.399724,"Missing"
P16-1025,N06-1039,0,0.0802104,"Missing"
P16-1025,de-marneffe-etal-2006-generating,0,0.102939,"Missing"
P16-1025,D11-1014,0,0.0551092,"Reconstruct: (X’ga,Y’l)=Z1W’mod+b’ T Z1=fmod(Wmod,Xga,Yl)=X gaWmodYl+b lose Figure 4: Partial AMR and Event Structure for E2. E(VI , VO ) = 1 ||VI − VO ||2 2 For each pair of words X and Y , the reconstruction error back-propagates from its output layer to input layer through parameters Θr = 0 0 (Wr , br , Wr , br ). Let δO be the residual error of the output layer, and δH be the error of the hidden layer: 0 O lated words for the event trigger with sense “lose1” and construct the event structure for the whole event, as shown in Figure 4. We design a Tensor based Recursive AutoEncoder (TRAE) (Socher et al., 2011) framework to utilize a tensor based composition function for each of a subset of the AMR semantic relations and compose the event structure representation based on multiple functional applications. This subset was manually selected by the authors as the set of relations that link a trigger to concepts that help to determine its type. Similarly, we selected a subset of dependency and FrameNet relations using the same criteria for experiments using those meaning representations. Figure 4 shows an instance of a TRAE applied to an event structure to generate its representation. For each semantic"
P16-1025,P11-1163,0,0.0337219,"Missing"
P16-1025,D09-1013,0,0.086859,"Missing"
P16-1025,D13-1170,0,0.00276702,"Missing"
P16-1025,W03-0419,0,0.104049,"he extraction quality of new event types is also promising. 1 E1. Two Soldiers were killed and one injured in the close-quarters fighting in Kut. E2. Bill Bennet’s glam gambling loss changed my opinion. Introduction E3. Gen. Vincent Brooks announced the capture of Barzan Ibrahim Hasan al-Tikriti, telling reporters he was an adviser to Saddam. Event extraction aims at identifying and typing trigger words and participants (arguments). It remains a challenging and costly task. The first question is what to extract? The TIPSTER (Onyshkevych et al., 1993), MUC (Grishman and Sundheim, 1996), CoNLL (Tjong et al., 2003; Pradhan et al., 2011), ACE 1 and TACKBP (Ji and Grishman, 2011) programs found that it was feasible to manually define an event schema based on the needs of potential users. An ACE event schema example is shown in Figure 1. This process is very expensive because consumers and 1 E4. This was the Italian ship that was captured by Palestinian terrorists back in 1985. E5. Ayman Sabawi Ibrahim was arrested in Tikrit and was sentenced to life in prison. We seek to cluster the event triggers and event arguments so that each cluster represents a type. We rely on distributional similarity for our clu"
P16-1025,P11-1148,0,0.0424776,"Missing"
P16-1025,P15-2060,0,0.0603427,"Missing"
P16-1025,P15-2141,0,0.0160239,"ach cluster of triggers, and name each trigger’s arguments using mappings between the meaning representation and semantic role descriptions in FrameNet, VerbNet (Kipper et al., 2008) and Propbank (Palmer et al., 2005). We compare settings in which semantic relations connecting triggers to context words are derived from three meaning representations: Abstract Meaning Representation (AMR) (Banarescu et al., 2013), Stanford Typed Dependencies (Marie-Catherine et al., 2006), and FrameNet (Baker and Sato, 2003). We derive semantic relations automatically for these three representations using CAMR (Wang et al., 2015a), Stanford’s dependency parser (Manning, 2003), and SEMAFOR (Das et al., 2014), respectively. 2.2 which is used to combine multiple sentences into one AMR graph.3 When FrameNet is the meaning representation we allow all frame relations to identify arguments. For dependencies, we manually mapped dependency relations to AMR relations and use Table 2. Categories Core roles Non-core roles Temporal Spatial Relations ARG0, ARG1, ARG2, ARG3, ARG4 mod, location, poss, manner, topic, medium, instrument, duration, prep-X year, duration, decade, weekday, time destination, path, location Table 2: Event-"
P16-1025,P15-1019,0,0.0988925,"Missing"
P16-1025,Q15-1005,0,0.0118071,"ach cluster of triggers, and name each trigger’s arguments using mappings between the meaning representation and semantic role descriptions in FrameNet, VerbNet (Kipper et al., 2008) and Propbank (Palmer et al., 2005). We compare settings in which semantic relations connecting triggers to context words are derived from three meaning representations: Abstract Meaning Representation (AMR) (Banarescu et al., 2013), Stanford Typed Dependencies (Marie-Catherine et al., 2006), and FrameNet (Baker and Sato, 2003). We derive semantic relations automatically for these three representations using CAMR (Wang et al., 2015a), Stanford’s dependency parser (Manning, 2003), and SEMAFOR (Das et al., 2014), respectively. 2.2 which is used to combine multiple sentences into one AMR graph.3 When FrameNet is the meaning representation we allow all frame relations to identify arguments. For dependencies, we manually mapped dependency relations to AMR relations and use Table 2. Categories Core roles Non-core roles Temporal Spatial Relations ARG0, ARG1, ARG2, ARG3, ARG4 mod, location, poss, manner, topic, medium, instrument, duration, prep-X year, duration, decade, weekday, time destination, path, location Table 2: Event-"
P16-1025,X93-1013,0,0.717501,"a large amount of data labeled according to predefined event types. The extraction quality of new event types is also promising. 1 E1. Two Soldiers were killed and one injured in the close-quarters fighting in Kut. E2. Bill Bennet’s glam gambling loss changed my opinion. Introduction E3. Gen. Vincent Brooks announced the capture of Barzan Ibrahim Hasan al-Tikriti, telling reporters he was an adviser to Saddam. Event extraction aims at identifying and typing trigger words and participants (arguments). It remains a challenging and costly task. The first question is what to extract? The TIPSTER (Onyshkevych et al., 1993), MUC (Grishman and Sundheim, 1996), CoNLL (Tjong et al., 2003; Pradhan et al., 2011), ACE 1 and TACKBP (Ji and Grishman, 2011) programs found that it was feasible to manually define an event schema based on the needs of potential users. An ACE event schema example is shown in Figure 1. This process is very expensive because consumers and 1 E4. This was the Italian ship that was captured by Palestinian terrorists back in 1985. E5. Ayman Sabawi Ibrahim was arrested in Tikrit and was sentenced to life in prison. We seek to cluster the event triggers and event arguments so that each cluster repre"
P16-1025,W12-3022,0,0.0652492,"Missing"
P16-1025,J05-1004,0,0.0379358,"ts. For each event trigger, we apply a series of compositional functions to generate that trigger’s event structure representation. Each function is specific to a semantic relation, and operates over vectors in the embedding space. Argument representations are generated as a by-product. Trigger and argument representations are then passed to a joint constraint clustering framework. Finally, we name each cluster of triggers, and name each trigger’s arguments using mappings between the meaning representation and semantic role descriptions in FrameNet, VerbNet (Kipper et al., 2008) and Propbank (Palmer et al., 2005). We compare settings in which semantic relations connecting triggers to context words are derived from three meaning representations: Abstract Meaning Representation (AMR) (Banarescu et al., 2013), Stanford Typed Dependencies (Marie-Catherine et al., 2006), and FrameNet (Baker and Sato, 2003). We derive semantic relations automatically for these three representations using CAMR (Wang et al., 2015a), Stanford’s dependency parser (Manning, 2003), and SEMAFOR (Das et al., 2014), respectively. 2.2 which is used to combine multiple sentences into one AMR graph.3 When FrameNet is the meaning repres"
P16-1025,P12-1059,0,0.056824,"Missing"
P16-1025,S07-1096,0,0.0753062,"Missing"
P16-1025,P10-4014,0,0.0661832,"e 2: Event-Related AMR Relations. In E1, for example, “killed”, “injured” and “fighting” are identified as candidate triggers, and three concept sets are identified as candidate arguments using AMR relations: “{Two Soldiers, very large missile}”, “{one, Kut}” and “{Two Soldiers, Kut}”, as shown in Figure 3. 2.3 Trigger Sense and Argument Representation Based on Hypothesis 1, we learn sense-based embeddings from a large data set, using the Continuous Skip-gram model (Mikolov et al., 2013). Specifically, we first apply WSD to link each word to its sense in WordNet using a state-of-the-art tool (Zhong and Ng, 2010), and map WordNet sense output to OntoNotes senses. 4 We map each trigger candidate to its OntoNotes sense and learn a distinct embedding for each sense. We use general lexical embeddings for arguments. Candidate Trigger and Argument Identification Given a sentence, we consider all noun and verb concepts that are assigned an OntoNotes (Hovy et al., 2006) sense by WSD as candidate event triggers. Any remaining concepts that match both a verbal and a nominal lexical unit in the FrameNet corpus are considered candidate event triggers as well. This mainly helps to identify more nominal triggers li"
P16-1025,W11-1901,0,0.0129852,"y of new event types is also promising. 1 E1. Two Soldiers were killed and one injured in the close-quarters fighting in Kut. E2. Bill Bennet’s glam gambling loss changed my opinion. Introduction E3. Gen. Vincent Brooks announced the capture of Barzan Ibrahim Hasan al-Tikriti, telling reporters he was an adviser to Saddam. Event extraction aims at identifying and typing trigger words and participants (arguments). It remains a challenging and costly task. The first question is what to extract? The TIPSTER (Onyshkevych et al., 1993), MUC (Grishman and Sundheim, 1996), CoNLL (Tjong et al., 2003; Pradhan et al., 2011), ACE 1 and TACKBP (Ji and Grishman, 2011) programs found that it was feasible to manually define an event schema based on the needs of potential users. An ACE event schema example is shown in Figure 1. This process is very expensive because consumers and 1 E4. This was the Italian ship that was captured by Palestinian terrorists back in 1985. E5. Ayman Sabawi Ibrahim was arrested in Tikrit and was sentenced to life in prison. We seek to cluster the event triggers and event arguments so that each cluster represents a type. We rely on distributional similarity for our clustering distance metric"
P16-1025,D11-1001,0,0.0699272,"Missing"
P16-1025,P06-2094,0,0.0223593,"Missing"
P16-1025,Q14-1017,0,\N,Missing
P16-1025,S10-1011,0,\N,Missing
P17-4010,P16-4004,0,0.136194,"whether there exist other drugs that can also treat breast cancer. Introduction Scientific literature is an important resource in facilitating life science research, and a primary medium for communicating novel research results. However, even though vast amounts of biomedical textual information are available online (e.g., publications in PubMed, encyclopedic articles in Wikipedia, ontologies on genes, drugs, etc.), there exists only limited support of exploring and analyzing relevant factual knowledge in the massive • Previous Efforts and Limitations. In life sciences domain, recent studies (Ernst et al., 2016; Szklarczyk et al., 2014; Thomas et al., 2012; 55 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics-System Demonstrations, pages 55–60 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-4010 Kim et al., 2008) rely on biomedical entity information associated with the documents to support entity-centric literature search. Most existing information retrieval systems exploit either the MeSH terms manually annotated for each PubMed article (Kim et al., 2008) or textual mentions of biome"
P17-4010,P05-1061,0,0.00983957,"given entity types, and generating hypothetical facts to assist literaturebased knowledge discovery (e.g., drug target prediction). 1 CCR4 MDC1 Kawasaki Disease Aspirin May treat Type Path May prevent Coexpression Physical Interaction BRCA1 Drug NSAID Type Path Disease Associated with RAP2A Pathway Breast Cancer Genetic Interaction Neoplasms Breast Neoplasms May treat May treat May treat BRAF Gene Drug Target Tafinlar CBDCA CDDP Disease Figure 1: A snapshot of the structured network in Life-iNet. literature (Tao et al., 2014), or of gaining new insights from the existing factual information (McDonald et al., 2005; Riedel and McCallum, 2011). Users typically search PubMed using keywords and Medical Subject Headings (MeSH) terms, and then rely on Google and external biomedical ontologies for everything else. Such an approach, however, might not work well on capturing different entity relationships (i.e., facts), or identifying publications related to facts of interest. For example, a biologist who is interested in cancer might need to check what specific diseases belong to the category of breast neoplasms (e.g., breast cancer) and what genes (e.g., BRCA1) and drugs (e.g., Aspirin, Tafinlar) are related"
P17-4010,P09-1113,0,0.00513912,"the network con1 https://www.ncbi.nlm.nih.gov/pubmed/ https://www.ncbi.nlm.nih.gov/pmc/ 3 https://en.wikipedia.org/wiki/Portal:Health_and_ fitness 2 57 text segmentation algorithm, SegPhrase (Liu et al., 2015), to extract high-quality words/phrases as entity candidates. SegPhrase uses entity names from KBs as positive examples to train a quality classifier, and then efficiently segments the corpus by maximizing the joint probability based on the trained classifier. Table 1 shows the statistics of detected entity mentions for the corpora. Distant Supervision Generation.. Distant supervisions (Mintz et al., 2009; Ren et al., 2017, 2016a) leverages the information overlap between external KBs and given corpora to automatically generate large amounts of training data. A typical workflow is as follows: (1) map detected entity mentions to entities in KB, (2) assign, to the entity type set of each entity mention, KB types of its KB-mapped entity, and (3) assign, to the relation type set of each entity mention pair, KB relations between their KB-mapped entities. Such a label generation process may introduce noisy type labels (Ren et al., 2017). Our network construction pipeline faithfully incorporates the"
P17-4010,D16-1144,1,0.911494,"answering questions such as “which genes are distinctively related to the given disease type under GeneDiseaseAssociation relation?”). To systematically incorporate these ideas, LifeiNet leverages the novel distantly-supervised information extraction techniques (Ren et al., 2017, 2016a, 2015) to implement an effort-light network construction framework (see Fig. 2). Specially, it relies on distant supervision in conjunction with external knowledge bases to (1) detect quality entity mentions (Ren et al., 2015), (2) label entity mentions with fine-grained entity types in a given type hierarchy (Ren et al., 2016a), and (3) identify relationships of different types between entities (Ren et al., 2017). In particular, we design specialized loss functions to faithfully model “appropriate” labels and remove “false positive” la• Lack of Factual Structures: Most existing entity-centric systems compute the document/corpus-level co-occurrence statistics between two biomedical entities to capture the relations between them, but cannot identify the semantic relation types between two entities based on the textual evidence in a specific sentence. For example, in Fig. 1, relations between gene entities should be"
P17-4010,D11-1001,0,0.0166053,"d generating hypothetical facts to assist literaturebased knowledge discovery (e.g., drug target prediction). 1 CCR4 MDC1 Kawasaki Disease Aspirin May treat Type Path May prevent Coexpression Physical Interaction BRCA1 Drug NSAID Type Path Disease Associated with RAP2A Pathway Breast Cancer Genetic Interaction Neoplasms Breast Neoplasms May treat May treat May treat BRAF Gene Drug Target Tafinlar CBDCA CDDP Disease Figure 1: A snapshot of the structured network in Life-iNet. literature (Tao et al., 2014), or of gaining new insights from the existing factual information (McDonald et al., 2005; Riedel and McCallum, 2011). Users typically search PubMed using keywords and Medical Subject Headings (MeSH) terms, and then rely on Google and external biomedical ontologies for everything else. Such an approach, however, might not work well on capturing different entity relationships (i.e., facts), or identifying publications related to facts of interest. For example, a biologist who is interested in cancer might need to check what specific diseases belong to the category of breast neoplasms (e.g., breast cancer) and what genes (e.g., BRCA1) and drugs (e.g., Aspirin, Tafinlar) are related to breast cancer, and might"
P18-1229,D14-1088,0,0.106185,"-structured data. To deal with the issue of incomplete coverage, some works (Liu et al., 2012; Dong et al., 2014; Panchenko et al., 2016; Kozareva and Hovy, 2010) utilize data from domain-specific resources or the Web. Panchenko et al. (2016) extract hypernyms by patterns from general purpose corpora and domain-specific corpora bootstrapped from the input vocabulary. Kozareva and Hovy (2010) harvest new terms from the Web by employing Hearst-like lexico-syntactic patterns and validate the learned is-a relations by a web-based concept positioning procedure. Many works (Kozareva and Hovy, 2010; Anh et al., 2014; Velardi et al., 2013; Bansal et al., 2014; Zhang et al., 2016; Panchenko et al., 2016; Gupta et al., 2017) cast the task of hypernymy organization as a graph optimization problem. Kozareva and Hovy (2010) begin with a set of root terms and leaf terms and aim to generate intermediate terms by deriving the longest path from the root to leaf in a noisy hypernym graph. Velardi et al. (2013) induct a taxonomy from the hypernym graph via optimal branching and a weighting policy. Bansal et al. (2014) regard the induction of a taxonomy as a structured learning problem by building a factor graph to m"
P18-1229,P14-1113,0,0.322134,"TaxoRL a set of terms into a taxonomy based on relevant resources such as text corpora. Prior studies on automatic taxonomy induction (Gupta et al., 2017; Camacho-Collados, 2017) often divide the problem into two sequential subtasks: (1) hypernymy detection (i.e., extracting term pairs of “is-a” relation); and (2) hypernymy organization (i.e., organizing is-a term pairs into a tree-structured hierarchy). Methods developed for hypernymy detection either harvest new terms (Yamada et al., 2009; Kozareva and Hovy, 2010) or presume a vocabulary is given and study term semantics (Snow et al., 2005; Fu et al., 2014; Tuan et al., 2016; Shwartz et al., 2016). The hypernymy pairs extracted in the first subtask form a noisy hypernym graph, which is then transformed into a tree-structured taxonomy in the hypernymy organization subtask, using different graph pruning methods including maximum spanning tree (MST) (Bansal et al., 2014; Zhang et al., 2016), minimum-cost flow (MCF) (Gupta et al., 2017) and other pruning heuristics (Kozareva and Hovy, 2010; Velardi et al., 2013; Faralli et al., 2015; Panchenko et al., 2016). However, these two-phase methods encounter two major limitations. First, most of them ignor"
P18-1229,P14-1098,0,0.189792,"Missing"
P18-1229,W11-2501,0,0.148077,"Missing"
P18-1229,S13-1005,0,0.0516518,"Missing"
P18-1229,S16-1168,0,0.0322842,"Missing"
P18-1229,C92-2082,0,0.294133,"ormation which are complementary to each other. 6 Related Work 6.1 Hypernymy Detection Finding high-quality hypernyms is of great importance since it serves as the first step of taxonomy induction. In previous works, there are mainly two categories of approaches for hypernymy detection, namely pattern-based and distributional methods. Pattern-based methods consider lexicosyntactic patterns between the joint occurrences of term pairs for hypernymy detection. They generally achieve high precision but suffer from low recall. Typical methods that leverage patterns for hypernym extraction include (Hearst, 1992; Snow et al., 2005; Kozareva and Hovy, 2010; Panchenko et al., 2016; Nakashole et al., 2012). Distributional methods leverage the contexts of each term separately. The co-occurrence of term pairs is hence unnecessary. Some distributional methods are developed in an unsupervised manner. Measures such as symmetric similarity (Lin et al., 1998) and those based on distributional inclusion hypothesis (Weeds et al., 2004; Chang et al., 2017) were proposed. Supervised methods, on the other hand, usually have better performance than unsupervised methods for hypernymy detection. Recent works towards t"
P18-1229,D16-1146,0,0.062882,"Missing"
P18-1229,P14-1089,0,0.125954,"terminates at this point, we call it a partial induction. We can also continue the induction by restoring the original action space at this moment so that all the terms in V are eventually attached to the taxonomy. We call this setting a full induction. In this experiment, we use the English environment and science taxonomies in the SemEval-2016 task 13 (TExEval2) (Bordea et al., 2016). Each taxonomy is composed of hundreds of terms, which is much larger than the WordNet taxonomies. The taxonomies are aggregated from existing resources such as WordNet, Eurovoc6 , and the Wikipedia Bitaxonomy (Flati et al., 2014). Since this dataset provides no training data, we train our model using the WordNet dataset in the first experiment. To avoid possible overlap between these two sources, we exclude those taxonomies constructed from WordNet. In both experiments, we combine three public corpora – the latest Wikipedia dump, the UMBC web-based corpus (Han et al., 2013) and the One Billion Word Language Modeling Benchmark (Chelba et al., 2013). Only sentences where term pairs co-occur are reserved, which results in 2467 6 http://eurovoc.europa.eu/drupal/ Model TAXI HypeNET HypeNET+MST TaxoRL (RE) TaxoRL (NR) Bansa"
P18-1229,N15-1169,0,0.0567274,"Missing"
P18-1229,D10-1108,0,0.394473,"Missing"
P18-1229,P16-1200,0,0.0146951,"eral or too specific. The surface, frequency, and generality features are binned and their embeddings are concatenated as a part of the term pair representation. In summary, the final term pair representation Rxy has the following form: Rxy = [PP(x,y) , Vwx , Vwy , VF (x,y) ], where PP(x,y) , Vwx , Vwy , VF (x,y) denote the path representation, the word embedding of x and y, and the feature embeddings, respectively. Our approach is general and can be flexibly extended to incorporate different feature representation components introduced by other relation extraction models (Zhang et al., 2017; Lin et al., 2016; Shwartz et al., 2016). We leave in-depth discussion of the design choice of hypernymy relation representation components as future work. 3 Reinforcement Learning for End-to-End Taxonomy Induction |Vt |+ |Tt |= |V0 |. The episode terminates when all the terms are attached to the taxonomy, which makes the length of one episode equal to |V0 |. A remaining issue is how to select the first term when no terms are on the taxonomy. One approach that we tried is to add a virtual node as root and consider it as if a real node. The root embedding is randomly initialized and updated with other parameter"
P18-1229,D12-1104,0,0.038867,"tion Finding high-quality hypernyms is of great importance since it serves as the first step of taxonomy induction. In previous works, there are mainly two categories of approaches for hypernymy detection, namely pattern-based and distributional methods. Pattern-based methods consider lexicosyntactic patterns between the joint occurrences of term pairs for hypernymy detection. They generally achieve high precision but suffer from low recall. Typical methods that leverage patterns for hypernym extraction include (Hearst, 1992; Snow et al., 2005; Kozareva and Hovy, 2010; Panchenko et al., 2016; Nakashole et al., 2012). Distributional methods leverage the contexts of each term separately. The co-occurrence of term pairs is hence unnecessary. Some distributional methods are developed in an unsupervised manner. Measures such as symmetric similarity (Lin et al., 1998) and those based on distributional inclusion hypothesis (Weeds et al., 2004; Chang et al., 2017) were proposed. Supervised methods, on the other hand, usually have better performance than unsupervised methods for hypernymy detection. Recent works towards this direction include (Fu et al., 2014; Rimell, 2014; Yu et al., 2015; Tuan et al., 2016; Shw"
P18-1229,S16-1206,0,0.141148,"Missing"
P18-1229,D14-1162,0,0.0811758,"e and give the agent the difference between the real reward and the baseline reward instead of feeding the real reward directly. We use a moving average of the reward as the baseline for simplicity. 5 We tried to encode induction history by feeding representations of previously selected term pairs into an LSTM, and leveraging the output of the LSTM as history representation (concatenating it with current term pair representations or passing it to a feed-forward network). However, we didn’t observe clear performance change. 2466 3.5 Implementation Details We use pre-trained GloVe word vectors (Pennington et al., 2014) with dimensionality 50 as word embeddings. We limit the maximum number of dependency paths between each term pair to be 200 because some term pairs containing general terms may have too many dependency paths. We run with different random seeds and hyperparameters and use the validation set to pick the best model. We use an Adam optimizer with initial learning rate 10−3 . We set the discounting factor γ to 0.4 as it is shown that using a smaller discount factor than defined can be viewed as regularization (Jiang et al., 2015). Since the parameter updates are performed at the end of each episod"
P18-1229,E14-1054,0,0.0188916,"2010; Panchenko et al., 2016; Nakashole et al., 2012). Distributional methods leverage the contexts of each term separately. The co-occurrence of term pairs is hence unnecessary. Some distributional methods are developed in an unsupervised manner. Measures such as symmetric similarity (Lin et al., 1998) and those based on distributional inclusion hypothesis (Weeds et al., 2004; Chang et al., 2017) were proposed. Supervised methods, on the other hand, usually have better performance than unsupervised methods for hypernymy detection. Recent works towards this direction include (Fu et al., 2014; Rimell, 2014; Yu et al., 2015; Tuan et al., 2016; Shwartz et al., 2016). 6.2 Taxonomy Induction There are many lines of work for taxonomy induction in the prior literature. One line of works (Snow et al., 2005; Yang and Callan, 2009; Shen et al., 2012; Jurgens and Pilehvar, 2015) aims to complete existing taxonomies by attaching new terms in an incremental way. Snow et al. (2005) enrich WordNet by maximizing the probability of an extended taxonomy given evidence 2469 of relations from text corpora. Shen et al. (2012) determine whether an entity is on the taxonomy and either attach it to the right category"
P18-1229,P16-1226,0,0.0570691,"Missing"
P18-1229,D16-1039,0,0.324173,"erms into a taxonomy based on relevant resources such as text corpora. Prior studies on automatic taxonomy induction (Gupta et al., 2017; Camacho-Collados, 2017) often divide the problem into two sequential subtasks: (1) hypernymy detection (i.e., extracting term pairs of “is-a” relation); and (2) hypernymy organization (i.e., organizing is-a term pairs into a tree-structured hierarchy). Methods developed for hypernymy detection either harvest new terms (Yamada et al., 2009; Kozareva and Hovy, 2010) or presume a vocabulary is given and study term semantics (Snow et al., 2005; Fu et al., 2014; Tuan et al., 2016; Shwartz et al., 2016). The hypernymy pairs extracted in the first subtask form a noisy hypernym graph, which is then transformed into a tree-structured taxonomy in the hypernymy organization subtask, using different graph pruning methods including maximum spanning tree (MST) (Bansal et al., 2014; Zhang et al., 2016), minimum-cost flow (MCF) (Gupta et al., 2017) and other pruning heuristics (Kozareva and Hovy, 2010; Velardi et al., 2013; Faralli et al., 2015; Panchenko et al., 2016). However, these two-phase methods encounter two major limitations. First, most of them ignore the taxonomy stru"
P18-1229,J13-3007,0,0.148813,"r harvest new terms (Yamada et al., 2009; Kozareva and Hovy, 2010) or presume a vocabulary is given and study term semantics (Snow et al., 2005; Fu et al., 2014; Tuan et al., 2016; Shwartz et al., 2016). The hypernymy pairs extracted in the first subtask form a noisy hypernym graph, which is then transformed into a tree-structured taxonomy in the hypernymy organization subtask, using different graph pruning methods including maximum spanning tree (MST) (Bansal et al., 2014; Zhang et al., 2016), minimum-cost flow (MCF) (Gupta et al., 2017) and other pruning heuristics (Kozareva and Hovy, 2010; Velardi et al., 2013; Faralli et al., 2015; Panchenko et al., 2016). However, these two-phase methods encounter two major limitations. First, most of them ignore the taxonomy structure when estimating the probability that a term pair holds the hypernymy relation. They estimate the probability of different term pairs independently and the learned term pair representations are fixed during hypernymy organization. In consequence, there is no feedback from the second phase to the first phase and possibly wrong representations cannot be rectified based on the results of hypernymy organization, which causes the error p"
P18-1229,C04-1146,0,0.162442,"Missing"
P18-1229,D09-1097,0,0.0796389,"Missing"
P18-1229,P09-1031,0,0.0184214,"developed in an unsupervised manner. Measures such as symmetric similarity (Lin et al., 1998) and those based on distributional inclusion hypothesis (Weeds et al., 2004; Chang et al., 2017) were proposed. Supervised methods, on the other hand, usually have better performance than unsupervised methods for hypernymy detection. Recent works towards this direction include (Fu et al., 2014; Rimell, 2014; Yu et al., 2015; Tuan et al., 2016; Shwartz et al., 2016). 6.2 Taxonomy Induction There are many lines of work for taxonomy induction in the prior literature. One line of works (Snow et al., 2005; Yang and Callan, 2009; Shen et al., 2012; Jurgens and Pilehvar, 2015) aims to complete existing taxonomies by attaching new terms in an incremental way. Snow et al. (2005) enrich WordNet by maximizing the probability of an extended taxonomy given evidence 2469 of relations from text corpora. Shen et al. (2012) determine whether an entity is on the taxonomy and either attach it to the right category or link it to an existing one based on the results. Another line of works (Suchanek et al., 2007; Ponzetto and Strube, 2008; Flati et al., 2014) focuses on the taxonomy induction of existing encyclopedias (e.g., Wikiped"
P18-1229,D17-1004,0,0.0280064,"t are either too general or too specific. The surface, frequency, and generality features are binned and their embeddings are concatenated as a part of the term pair representation. In summary, the final term pair representation Rxy has the following form: Rxy = [PP(x,y) , Vwx , Vwy , VF (x,y) ], where PP(x,y) , Vwx , Vwy , VF (x,y) denote the path representation, the word embedding of x and y, and the feature embeddings, respectively. Our approach is general and can be flexibly extended to incorporate different feature representation components introduced by other relation extraction models (Zhang et al., 2017; Lin et al., 2016; Shwartz et al., 2016). We leave in-depth discussion of the design choice of hypernymy relation representation components as future work. 3 Reinforcement Learning for End-to-End Taxonomy Induction |Vt |+ |Tt |= |V0 |. The episode terminates when all the terms are attached to the taxonomy, which makes the length of one episode equal to |V0 |. A remaining issue is how to select the first term when no terms are on the taxonomy. One approach that we tried is to add a virtual node as root and consider it as if a real node. The root embedding is randomly initialized and updated wi"
P19-1016,C18-1139,0,0.0697448,"essfully identifies “Zheng Chenggong” as a person, it is not able to connect this name with “Koxinga” based on the expression “also known as” to further infer that “Koxinga” should also be a person. Table 5: Name tagging result comparison between the baseline model and our model. 171 4 Related Work bution of word frequency, embedding vectors usually have inconsistent reliability, and such inconsistency has been long overlooked. Meanwhile, language models such as ELMo, Flair, and BERT have shown their effectiveness on constructing representations in a context-aware manner (Peters et al., 2018; Akbik et al., 2018; Devlin et al., 2018). These models are designed to better capture the context information by pre-training, while our model dynamically composes representations in a reliability-aware manner. Therefore, our model and these efforts have the potential to mutually enhance each other. In addition, (Kim et al., 2016) and (Rei et al., 2016) also mix word- and character-level representations using gating mechanisms. They use a single gate to balance the representations in a reliability-agnostic way. Name Tagging Models Most existing methods treat name tagging as a sequence labeling task. Traditional"
P19-1016,K18-1028,0,0.026665,"o recognize names but still absent from the current model. Word Representation Models Acknowledgments Recent advances on representation learning allow us to capture textual signals in a data-driven manner. Based on the distributional hypothesis (i.e., “a word is characterized by the company it keeps” (Harris, 1954)), embedding methods represent each word as a dense vector, while preserving their syntactic and semantic information in a context-agnostic manner (Mikolov et al., 2013; Pennington et al., 2014). Recent work shows that word embeddings can cover textual information of various levels (Artetxe et al., 2018) and improve name tagging performance significantly (Cherry and Guo, 2015). Still, due to the long-tail distriThis work was supported by the U.S. DARPA AIDA Program No. FA8750-18-2-0014, LORELEI Program No. HR0011-15-C-0115, Air Force No. FA8650-17-C-7715, U.S. ARL NS-CTA No. W911NF-09-2-0053, and Tencent AI Lab Rhino-Bird Gift Fund. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied of the U.S. Government. The U.S. Government is authorized to reproduce and distribute r"
P19-1016,D14-1082,0,0.0601337,"Missing"
P19-1016,N15-1075,0,0.0303189,"tion Models Acknowledgments Recent advances on representation learning allow us to capture textual signals in a data-driven manner. Based on the distributional hypothesis (i.e., “a word is characterized by the company it keeps” (Harris, 1954)), embedding methods represent each word as a dense vector, while preserving their syntactic and semantic information in a context-agnostic manner (Mikolov et al., 2013; Pennington et al., 2014). Recent work shows that word embeddings can cover textual information of various levels (Artetxe et al., 2018) and improve name tagging performance significantly (Cherry and Guo, 2015). Still, due to the long-tail distriThis work was supported by the U.S. DARPA AIDA Program No. FA8750-18-2-0014, LORELEI Program No. HR0011-15-C-0115, Air Force No. FA8650-17-C-7715, U.S. ARL NS-CTA No. W911NF-09-2-0053, and Tencent AI Lab Rhino-Bird Gift Fund. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied of the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation"
P19-1016,N16-1030,0,0.0148324,"and employ conditional random fields (CRF) to model label dependencies (Finkel et al., 2005; Settles, 2004; Leaman et al., 2008). Bi-LSTM-CRF (Huang et al., 2015) combines word embedding and handcrafted features, integrates neural networks with CRF, and shows performance boost over previous methods. LSTMCNN further utilizes CNN and illustrates the potential of capturing character-level signals (Chiu and Nichols, 2016). LSTM-CRF and LSTMCNNs-CRF are proposed to get rid of hand-crafted features and demonstrate the feasibility to fully rely on representation learning to capture textual features (Lample et al., 2016; Ma and Hovy, 2016; Liu et al., 2018b). Recently, language modeling methods are proven effective as the representation module for name tagging (Liu et al., 2018a; Peters et al., 2018; Akbik et al., 2018). At the same time, there has been extensive research about cross-genre (Peng and Dredze, 2017), crossdomain (Pan et al., 2013; He and Sun, 2017), cross-time (Mota and Grishman, 2008), crosstask (Søgaard and Goldberg, 2016; Liu et al., 2018b), and cross-lingual (Yang et al., 2017; Lin et al., 2018) adaptation for name tagging training. Unlike these models, although we also aim to enhance the p"
P19-1016,Q16-1026,0,0.519455,"el. The basis of this dynamic composition mechanism is the reliability signals that inform the model of the quality of each word embedding. Specifically, we assume that if a word occurs more frequently, its word embedding will be more fully trained as it has richer contexts and its embedding is updated more often during training. Thus, we design a set of reliability signals based on word frequency in the embedding training corpus and name tagging training corpus. 2.1 Baseline Model We adopt a state-of-the-art name tagging model LSTM-CNN (Long-short Term Memory - Convolutional Neural Network) (Chiu and Nichols, 2016) as our base model. In this architecture, the input sentence is represented as a sequence of vectors X = {x1 , ..., xL }, where xi is the vector representation of the i-th word, and L is the length of the sequence. Generally, xi is a concatenation of word embedding and character-level representation generated with a group of convolutional neural networks (CNNs) with various filter sizes from compositional character embeddings of the word. Next, the sequence X is fed into a bi-directional Recurrent Neural Network (RNN) with Longshort Term Memory (LSTM) units (Hochreiter and Schmidhuber, 1997)."
P19-1016,P18-1074,1,0.833856,"onstrate the feasibility to fully rely on representation learning to capture textual features (Lample et al., 2016; Ma and Hovy, 2016; Liu et al., 2018b). Recently, language modeling methods are proven effective as the representation module for name tagging (Liu et al., 2018a; Peters et al., 2018; Akbik et al., 2018). At the same time, there has been extensive research about cross-genre (Peng and Dredze, 2017), crossdomain (Pan et al., 2013; He and Sun, 2017), cross-time (Mota and Grishman, 2008), crosstask (Søgaard and Goldberg, 2016; Liu et al., 2018b), and cross-lingual (Yang et al., 2017; Lin et al., 2018) adaptation for name tagging training. Unlike these models, although we also aim to enhance the performance on new data, we achieve this by improving the generalization capability of the model so that it can work better on unknown new data instead of transferring it to a known target setting. 5 Conclusions and Future Work We propose a name tagging model that is able to dynamically compose features depending on the quality of input word embeddings. Experiments on the benchmark data sets in both within-genre and cross-genre settings demonstrate the effectiveness of our model and verify our intui"
P19-1016,P15-1033,0,0.026624,"Missing"
P19-1016,D18-1153,1,0.841269,"F) to model label dependencies (Finkel et al., 2005; Settles, 2004; Leaman et al., 2008). Bi-LSTM-CRF (Huang et al., 2015) combines word embedding and handcrafted features, integrates neural networks with CRF, and shows performance boost over previous methods. LSTMCNN further utilizes CNN and illustrates the potential of capturing character-level signals (Chiu and Nichols, 2016). LSTM-CRF and LSTMCNNs-CRF are proposed to get rid of hand-crafted features and demonstrate the feasibility to fully rely on representation learning to capture textual features (Lample et al., 2016; Ma and Hovy, 2016; Liu et al., 2018b). Recently, language modeling methods are proven effective as the representation module for name tagging (Liu et al., 2018a; Peters et al., 2018; Akbik et al., 2018). At the same time, there has been extensive research about cross-genre (Peng and Dredze, 2017), crossdomain (Pan et al., 2013; He and Sun, 2017), cross-time (Mota and Grishman, 2008), crosstask (Søgaard and Goldberg, 2016; Liu et al., 2018b), and cross-lingual (Yang et al., 2017; Lin et al., 2018) adaptation for name tagging training. Unlike these models, although we also aim to enhance the performance on new data, we achieve th"
P19-1016,P05-1045,0,0.00980259,"omposes representations in a reliability-aware manner. Therefore, our model and these efforts have the potential to mutually enhance each other. In addition, (Kim et al., 2016) and (Rei et al., 2016) also mix word- and character-level representations using gating mechanisms. They use a single gate to balance the representations in a reliability-agnostic way. Name Tagging Models Most existing methods treat name tagging as a sequence labeling task. Traditional methods leverage handcrafted features to capture textual signals and employ conditional random fields (CRF) to model label dependencies (Finkel et al., 2005; Settles, 2004; Leaman et al., 2008). Bi-LSTM-CRF (Huang et al., 2015) combines word embedding and handcrafted features, integrates neural networks with CRF, and shows performance boost over previous methods. LSTMCNN further utilizes CNN and illustrates the potential of capturing character-level signals (Chiu and Nichols, 2016). LSTM-CRF and LSTMCNNs-CRF are proposed to get rid of hand-crafted features and demonstrate the feasibility to fully rely on representation learning to capture textual features (Lample et al., 2016; Ma and Hovy, 2016; Liu et al., 2018b). Recently, language modeling met"
P19-1016,P16-1101,0,0.0223755,"l random fields (CRF) to model label dependencies (Finkel et al., 2005; Settles, 2004; Leaman et al., 2008). Bi-LSTM-CRF (Huang et al., 2015) combines word embedding and handcrafted features, integrates neural networks with CRF, and shows performance boost over previous methods. LSTMCNN further utilizes CNN and illustrates the potential of capturing character-level signals (Chiu and Nichols, 2016). LSTM-CRF and LSTMCNNs-CRF are proposed to get rid of hand-crafted features and demonstrate the feasibility to fully rely on representation learning to capture textual features (Lample et al., 2016; Ma and Hovy, 2016; Liu et al., 2018b). Recently, language modeling methods are proven effective as the representation module for name tagging (Liu et al., 2018a; Peters et al., 2018; Akbik et al., 2018). At the same time, there has been extensive research about cross-genre (Peng and Dredze, 2017), crossdomain (Pan et al., 2013; He and Sun, 2017), cross-time (Mota and Grishman, 2008), crosstask (Søgaard and Goldberg, 2016; Liu et al., 2018b), and cross-lingual (Yang et al., 2017; Lin et al., 2018) adaptation for name tagging training. Unlike these models, although we also aim to enhance the performance on new d"
P19-1016,P14-1146,0,0.117179,"Missing"
P19-1016,mota-grishman-2008-ne,0,0.0329385,"-level signals (Chiu and Nichols, 2016). LSTM-CRF and LSTMCNNs-CRF are proposed to get rid of hand-crafted features and demonstrate the feasibility to fully rely on representation learning to capture textual features (Lample et al., 2016; Ma and Hovy, 2016; Liu et al., 2018b). Recently, language modeling methods are proven effective as the representation module for name tagging (Liu et al., 2018a; Peters et al., 2018; Akbik et al., 2018). At the same time, there has been extensive research about cross-genre (Peng and Dredze, 2017), crossdomain (Pan et al., 2013; He and Sun, 2017), cross-time (Mota and Grishman, 2008), crosstask (Søgaard and Goldberg, 2016; Liu et al., 2018b), and cross-lingual (Yang et al., 2017; Lin et al., 2018) adaptation for name tagging training. Unlike these models, although we also aim to enhance the performance on new data, we achieve this by improving the generalization capability of the model so that it can work better on unknown new data instead of transferring it to a known target setting. 5 Conclusions and Future Work We propose a name tagging model that is able to dynamically compose features depending on the quality of input word embeddings. Experiments on the benchmark dat"
P19-1016,W17-2612,0,0.0123742,"ods. LSTMCNN further utilizes CNN and illustrates the potential of capturing character-level signals (Chiu and Nichols, 2016). LSTM-CRF and LSTMCNNs-CRF are proposed to get rid of hand-crafted features and demonstrate the feasibility to fully rely on representation learning to capture textual features (Lample et al., 2016; Ma and Hovy, 2016; Liu et al., 2018b). Recently, language modeling methods are proven effective as the representation module for name tagging (Liu et al., 2018a; Peters et al., 2018; Akbik et al., 2018). At the same time, there has been extensive research about cross-genre (Peng and Dredze, 2017), crossdomain (Pan et al., 2013; He and Sun, 2017), cross-time (Mota and Grishman, 2008), crosstask (Søgaard and Goldberg, 2016; Liu et al., 2018b), and cross-lingual (Yang et al., 2017; Lin et al., 2018) adaptation for name tagging training. Unlike these models, although we also aim to enhance the performance on new data, we achieve this by improving the generalization capability of the model so that it can work better on unknown new data instead of transferring it to a known target setting. 5 Conclusions and Future Work We propose a name tagging model that is able to dynamically compose feat"
P19-1016,N16-1174,0,0.0614333,"Missing"
P19-1016,D14-1162,0,0.0917685,"l knowledge and common sense as additional signals into our architecture as they are important for human readers to recognize names but still absent from the current model. Word Representation Models Acknowledgments Recent advances on representation learning allow us to capture textual signals in a data-driven manner. Based on the distributional hypothesis (i.e., “a word is characterized by the company it keeps” (Harris, 1954)), embedding methods represent each word as a dense vector, while preserving their syntactic and semantic information in a context-agnostic manner (Mikolov et al., 2013; Pennington et al., 2014). Recent work shows that word embeddings can cover textual information of various levels (Artetxe et al., 2018) and improve name tagging performance significantly (Cherry and Guo, 2015). Still, due to the long-tail distriThis work was supported by the U.S. DARPA AIDA Program No. FA8750-18-2-0014, LORELEI Program No. HR0011-15-C-0115, Air Force No. FA8650-17-C-7715, U.S. ARL NS-CTA No. W911NF-09-2-0053, and Tencent AI Lab Rhino-Bird Gift Fund. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, eith"
P19-1016,N18-1202,0,0.29,"though our model successfully identifies “Zheng Chenggong” as a person, it is not able to connect this name with “Koxinga” based on the expression “also known as” to further infer that “Koxinga” should also be a person. Table 5: Name tagging result comparison between the baseline model and our model. 171 4 Related Work bution of word frequency, embedding vectors usually have inconsistent reliability, and such inconsistency has been long overlooked. Meanwhile, language models such as ELMo, Flair, and BERT have shown their effectiveness on constructing representations in a context-aware manner (Peters et al., 2018; Akbik et al., 2018; Devlin et al., 2018). These models are designed to better capture the context information by pre-training, while our model dynamically composes representations in a reliability-aware manner. Therefore, our model and these efforts have the potential to mutually enhance each other. In addition, (Kim et al., 2016) and (Rei et al., 2016) also mix word- and character-level representations using gating mechanisms. They use a single gate to balance the representations in a reliability-agnostic way. Name Tagging Models Most existing methods treat name tagging as a sequence labeli"
P19-1016,W13-3516,0,0.0320936,"Missing"
P19-1016,C16-1030,0,0.0599161,"Missing"
P19-1016,W04-1221,0,0.0386504,"ns in a reliability-aware manner. Therefore, our model and these efforts have the potential to mutually enhance each other. In addition, (Kim et al., 2016) and (Rei et al., 2016) also mix word- and character-level representations using gating mechanisms. They use a single gate to balance the representations in a reliability-agnostic way. Name Tagging Models Most existing methods treat name tagging as a sequence labeling task. Traditional methods leverage handcrafted features to capture textual signals and employ conditional random fields (CRF) to model label dependencies (Finkel et al., 2005; Settles, 2004; Leaman et al., 2008). Bi-LSTM-CRF (Huang et al., 2015) combines word embedding and handcrafted features, integrates neural networks with CRF, and shows performance boost over previous methods. LSTMCNN further utilizes CNN and illustrates the potential of capturing character-level signals (Chiu and Nichols, 2016). LSTM-CRF and LSTMCNNs-CRF are proposed to get rid of hand-crafted features and demonstrate the feasibility to fully rely on representation learning to capture textual features (Lample et al., 2016; Ma and Hovy, 2016; Liu et al., 2018b). Recently, language modeling methods are proven"
P19-1016,P16-2038,0,0.0178532,"6). LSTM-CRF and LSTMCNNs-CRF are proposed to get rid of hand-crafted features and demonstrate the feasibility to fully rely on representation learning to capture textual features (Lample et al., 2016; Ma and Hovy, 2016; Liu et al., 2018b). Recently, language modeling methods are proven effective as the representation module for name tagging (Liu et al., 2018a; Peters et al., 2018; Akbik et al., 2018). At the same time, there has been extensive research about cross-genre (Peng and Dredze, 2017), crossdomain (Pan et al., 2013; He and Sun, 2017), cross-time (Mota and Grishman, 2008), crosstask (Søgaard and Goldberg, 2016; Liu et al., 2018b), and cross-lingual (Yang et al., 2017; Lin et al., 2018) adaptation for name tagging training. Unlike these models, although we also aim to enhance the performance on new data, we achieve this by improving the generalization capability of the model so that it can work better on unknown new data instead of transferring it to a known target setting. 5 Conclusions and Future Work We propose a name tagging model that is able to dynamically compose features depending on the quality of input word embeddings. Experiments on the benchmark data sets in both within-genre and cross-g"
W18-1202,E17-2068,0,0.0137694,"s for great cross-domain performance in identifying semantically meaningful subwords. We then investigate utilizing the mined subwords within the FastText embedding model and compare performance of the learned representations in a downstream language modeling task. 1 Figure 1: Hierarchical segmentation of words spatial, spatiotemporal, temporally into subwords. been shown to capture syntactic and semantic regularities and have been shown to boost the performance in tasks such as text classification, sequential classification, sentiment analysis, and machine translation (Mikolov et al., 2013c; Joulin et al., 2017; Huang et al., 2015; Tang et al., 2014; Zou et al., 2013). Many different methods have been proposed to derive these continuous representations from large, unlabeled, text corpora (Collobert and Weston, 2008; Mikolov et al., 2013a,b). While distributed continuous representations have helped push the state-of-the-art in a variety of NLP tasks, because most text corpora have long-tail distributions, embeddings in the longtail are often of poor quality due to infrequency. This is even worse for out-of-vocabulary words which are all given the same constant embedding vector because no information"
W18-1202,N06-2001,0,0.0546922,"subwords for neural machine translation tasks (Sennrich et al., 2015). Both these methods construct a fixed-size subword vocabulary to construct each word as a sequence of subwords. Many attempts have been proposed to address data-sparsity when learning distributed word representations. These methods posit that individual words have semantically meaningful attributes that are shared among other words allowing for parameter sharing between vocabulary words. One such method proposes a factored neural language model where words are represented as a set of features including subword information (Alexandrescu and Kirchhoff, 2006). Another method attempts to incorporate morphological information into the word embeddings by adding morphological similarity features into a neural network along with the context features (Cui et al., 2015). This method while similarly motivated, does not leverage subword structure but instead utilizes the embeddings of “morphologically similar” words in the embedding process. While this may seem appealing, identifying morphologically similar words can be an expensive process as it requires a search over the entire vocabulary which may be prohibitive during on-thefly computation out-of-word"
W18-1202,Q17-1010,0,0.108015,"methods apply a maximum likelihood approach to identifying subwords p∈wx which equates to a simple summation over subword embedding vectors. 4 Related Works There have been many attempts at automatic substructure extraction from words. These techniques generally fall into one of three families: scoring based on segment predictability, identifying subwords based on discovering similar and dissimilar word parts, and optimization methods. In morphological analysis of relating phonemes to morphemes, segment predictability has been suggested as a potential identifying characteristic 17 formation (Bojanowski et al., 2017). This method extends the standard skip-gram model but utilizes character-ngram subword embeddings for parameter sharing. The major differences between SubwordMine and this method is that FastText embedding utilizes all character n-grams of userspecified lengths for subword embedding and performs a simple sum over their representations while SubwordMine performs unsupervised segmentation and applies a novel attention mechanism to combine the subword representations into word representations. Finally, utilizing subword information was shown to improve performance in machine translation (Sennric"
W18-1202,N13-1090,0,0.0204465,"We show that this allows for great cross-domain performance in identifying semantically meaningful subwords. We then investigate utilizing the mined subwords within the FastText embedding model and compare performance of the learned representations in a downstream language modeling task. 1 Figure 1: Hierarchical segmentation of words spatial, spatiotemporal, temporally into subwords. been shown to capture syntactic and semantic regularities and have been shown to boost the performance in tasks such as text classification, sequential classification, sentiment analysis, and machine translation (Mikolov et al., 2013c; Joulin et al., 2017; Huang et al., 2015; Tang et al., 2014; Zou et al., 2013). Many different methods have been proposed to derive these continuous representations from large, unlabeled, text corpora (Collobert and Weston, 2008; Mikolov et al., 2013a,b). While distributed continuous representations have helped push the state-of-the-art in a variety of NLP tasks, because most text corpora have long-tail distributions, embeddings in the longtail are often of poor quality due to infrequency. This is even worse for out-of-vocabulary words which are all given the same constant embedding vector b"
W18-1202,W02-0604,0,0.122798,"eturn output rithm terminates if the word cannot be further segmented. Otherwise, the word is segmented with the dynamic programming parsimonious segmentation algorithm. Each subword is then treated as a word and recursively segmented by the algorithm, and the collection of all subwords from segmentation are outputted. 3.4 Word Embedding Deviating from predictability-based methods, several subword detection methods have been proposed for detecting subwords by comparing words and identifying similar and dissimilar parts. One such method performs alignment from the left and right edge of words (Neuvel and Fulop, 2002) identifying common subwords. Another method adds words to a trie in correct order and reverse order to identify leading and trailing frequent subwords (Schone and Jurafsky, 2001). Unfortunately both these methods can only identify prefix and suffix subwords, ignoring many internal subwords. Unlike these methods, our subword segmentation is position insensitive and can identify subwords that occur in any position in a word. To efficiently utilize our mined subwords to improve upon word embeddings, we modify the FastText model for word embeddings to use our extracted subwords (Bojanowski et al."
W18-1202,N15-1140,0,0.118858,"Missing"
W18-1202,C14-1015,0,0.0983188,"lar” words in the embedding process. While this may seem appealing, identifying morphologically similar words can be an expensive process as it requires a search over the entire vocabulary which may be prohibitive during on-thefly computation out-of-word vocabulary. In addition, this method may miss important morphological cues such as negation subwords. When extended to use subword information, this model assumes subwords are already provided, this requires manual identification of subwords which can be an expensive human-powered task, especially in domain-specific settings or new languages (Qiu et al., 2014). Along similar motivation, a method has been proposed where given an input of morphologically annotated data, logbilinear models are trained to jointly predict context words and its morphological tag (Cotterell and Sch¨utze, 2015). Despite displaying superior embedding performance on German corpora, this method once again requires human-labeling for tagging words. This limits applicability to domain-specific corpora and new languages where labeled data is scarce or expensive to obtain. The method that is is closest to our approach is the extension of FastText enriched with subword in5 Experim"
W18-1202,W02-0603,0,0.12137,"modify the FastText model for word embeddings to use our extracted subwords (Bojanowski et al., 2016). FastText utilizes the skip-gram objective with negative sampling yielding the following objective (for simplicity, `(x) = log(1 + exp(−x))): W hX X x=1 `(s(wx , wc )) + c∈Cx X t∈Nx,c i `(−s(wx , t)) (4) The scoring function is then adapted to incorporate subword information as follows: X s(wx , wc ) = z|p vc (5) Opting for an optimization over a scoring perspective, a variety of methods have been proposed. One such method models segmentation through the minimum description length principle (Creutz and Lagus, 2002). This method attempts to minimize both the vocabulary while maintaining the likelihood of the corpus data.This method was successfully applied to languages such as Turkish (Sak et al., 2010). Unfortunately, unlike methods that take the vocabulary as input, these family of optimization methods must make several passes over the corpus. This not only adds significant runtime and may discourage use as a preprocessing step before embedding, but can also be intractable for large text corpora. Other methods apply a maximum likelihood approach to identifying subwords p∈wx which equates to a simple su"
W18-1202,W98-1239,0,0.476371,"Missing"
W18-1202,N01-1024,0,0.0425715,"ubword is then treated as a word and recursively segmented by the algorithm, and the collection of all subwords from segmentation are outputted. 3.4 Word Embedding Deviating from predictability-based methods, several subword detection methods have been proposed for detecting subwords by comparing words and identifying similar and dissimilar parts. One such method performs alignment from the left and right edge of words (Neuvel and Fulop, 2002) identifying common subwords. Another method adds words to a trie in correct order and reverse order to identify leading and trailing frequent subwords (Schone and Jurafsky, 2001). Unfortunately both these methods can only identify prefix and suffix subwords, ignoring many internal subwords. Unlike these methods, our subword segmentation is position insensitive and can identify subwords that occur in any position in a word. To efficiently utilize our mined subwords to improve upon word embeddings, we modify the FastText model for word embeddings to use our extracted subwords (Bojanowski et al., 2016). FastText utilizes the skip-gram objective with negative sampling yielding the following objective (for simplicity, `(x) = log(1 + exp(−x))): W hX X x=1 `(s(wx , wc )) + c"
W18-1202,P14-1146,0,0.0464961,"dentifying semantically meaningful subwords. We then investigate utilizing the mined subwords within the FastText embedding model and compare performance of the learned representations in a downstream language modeling task. 1 Figure 1: Hierarchical segmentation of words spatial, spatiotemporal, temporally into subwords. been shown to capture syntactic and semantic regularities and have been shown to boost the performance in tasks such as text classification, sequential classification, sentiment analysis, and machine translation (Mikolov et al., 2013c; Joulin et al., 2017; Huang et al., 2015; Tang et al., 2014; Zou et al., 2013). Many different methods have been proposed to derive these continuous representations from large, unlabeled, text corpora (Collobert and Weston, 2008; Mikolov et al., 2013a,b). While distributed continuous representations have helped push the state-of-the-art in a variety of NLP tasks, because most text corpora have long-tail distributions, embeddings in the longtail are often of poor quality due to infrequency. This is even worse for out-of-vocabulary words which are all given the same constant embedding vector because no information is available to infer a meaningful repr"
W18-1202,1983.tc-1.13,0,0.422354,"Missing"
W18-1202,P16-1162,0,0.0382766,", 2017). This method extends the standard skip-gram model but utilizes character-ngram subword embeddings for parameter sharing. The major differences between SubwordMine and this method is that FastText embedding utilizes all character n-grams of userspecified lengths for subword embedding and performs a simple sum over their representations while SubwordMine performs unsupervised segmentation and applies a novel attention mechanism to combine the subword representations into word representations. Finally, utilizing subword information was shown to improve performance in machine translation (Sennrich et al., 2016). Another spectrum of approaches address word sparsity through the use of characters as the base unit for embedding. Some approaches treat each word as a sequence of characters. and apply recurrent neural networks to the task of language modeling (Bojanowski et al., 2015; Sutskever et al., 2011). Other related models apply convolutional neural networks directly on characters (Kim et al., 2016). (also called wordpieces) and has been successfully applied to a variety of NLP tasks (Wu et al., 2016; Schuster and Nakajima, 2012). And similarly the byte-pair compression algorithm has been used to id"
W18-1202,D13-1141,0,0.0429797,"ally meaningful subwords. We then investigate utilizing the mined subwords within the FastText embedding model and compare performance of the learned representations in a downstream language modeling task. 1 Figure 1: Hierarchical segmentation of words spatial, spatiotemporal, temporally into subwords. been shown to capture syntactic and semantic regularities and have been shown to boost the performance in tasks such as text classification, sequential classification, sentiment analysis, and machine translation (Mikolov et al., 2013c; Joulin et al., 2017; Huang et al., 2015; Tang et al., 2014; Zou et al., 2013). Many different methods have been proposed to derive these continuous representations from large, unlabeled, text corpora (Collobert and Weston, 2008; Mikolov et al., 2013a,b). While distributed continuous representations have helped push the state-of-the-art in a variety of NLP tasks, because most text corpora have long-tail distributions, embeddings in the longtail are often of poor quality due to infrequency. This is even worse for out-of-vocabulary words which are all given the same constant embedding vector because no information is available to infer a meaningful representation. To addr"
W19-4607,N16-3003,0,0.0258882,"odel is an effective representation module for NER (Peters et al., 2017, 2018; Liu et al., 2018b; Akbik et al., 2018; Liu et al., 2018a). At the same time, many approaches have been proposed specifically to solve the NER task in Arabic. Traditional Arabic NER models are mostly rule-basedmodels (Shaalan, 2014). Recently, people have started to attach this task with machine learning methods (Helwe and Elbassuoni, 2017; Gridach, 2016). To further improve the performance, attempts have been made to combine both rule-based and learning-based approaches into a unified framework (Pasha et al., 2014; Abdelali et al., 2016). Besides, incorporating additional supervision from other domains or languages has been explored as well (Darwish, 2013). 3 3.2 The Arabic NER challenge uses the public Arabic NER benchmark dataset (i.e., the AQMAR dataset) (Mohit et al., 2012). Its annotated entities are classified into four types (i.e., “Person”, “Location”, “Organization” and “Miscellaneous”). This dataset contains 28 hand-annotated Arabic Wikipedia articles, 14 articles are used as the training set, 7 articles are used as the development set, and 7 articles are used as the test set. Data cleaning is further conducted on t"
W19-4607,N16-1030,0,0.0301902,"uence labeling task. Before deep learning demonstrated its effectiveness, traditional methods rely on handcrafted features (e.g., features based on POS tags) and language-specific resources (e.g., gazetteers) to capture textual signals. Machine learning models like conditional random field (CRF) and hidden Markov model (HMM) are employed to capture the label dependency (Lafferty et al., 2001; Florian et al., 2003; Chieu and Ng, 2002). Many attempts have been made to reduce the reliance on feature engineering or other human endeavors, which makes the NER task be solved in an end-to-end manner (Lample et al., 2016; Ma and Hovy, 2016; Shang et al., 2018). Recent studies have revealed that language model is an effective representation module for NER (Peters et al., 2017, 2018; Liu et al., 2018b; Akbik et al., 2018; Liu et al., 2018a). At the same time, many approaches have been proposed specifically to solve the NER task in Arabic. Traditional Arabic NER models are mostly rule-basedmodels (Shaalan, 2014). Recently, people have started to attach this task with machine learning methods (Helwe and Elbassuoni, 2017; Gridach, 2016). To further improve the performance, attempts have been made to combine both r"
W19-4607,C18-1139,0,0.0645678,"eers) to capture textual signals. Machine learning models like conditional random field (CRF) and hidden Markov model (HMM) are employed to capture the label dependency (Lafferty et al., 2001; Florian et al., 2003; Chieu and Ng, 2002). Many attempts have been made to reduce the reliance on feature engineering or other human endeavors, which makes the NER task be solved in an end-to-end manner (Lample et al., 2016; Ma and Hovy, 2016; Shang et al., 2018). Recent studies have revealed that language model is an effective representation module for NER (Peters et al., 2017, 2018; Liu et al., 2018b; Akbik et al., 2018; Liu et al., 2018a). At the same time, many approaches have been proposed specifically to solve the NER task in Arabic. Traditional Arabic NER models are mostly rule-basedmodels (Shaalan, 2014). Recently, people have started to attach this task with machine learning methods (Helwe and Elbassuoni, 2017; Gridach, 2016). To further improve the performance, attempts have been made to combine both rule-based and learning-based approaches into a unified framework (Pasha et al., 2014; Abdelali et al., 2016). Besides, incorporating additional supervision from other domains or languages has been explo"
W19-4607,D18-1153,1,0.88105,"urces (e.g., gazetteers) to capture textual signals. Machine learning models like conditional random field (CRF) and hidden Markov model (HMM) are employed to capture the label dependency (Lafferty et al., 2001; Florian et al., 2003; Chieu and Ng, 2002). Many attempts have been made to reduce the reliance on feature engineering or other human endeavors, which makes the NER task be solved in an end-to-end manner (Lample et al., 2016; Ma and Hovy, 2016; Shang et al., 2018). Recent studies have revealed that language model is an effective representation module for NER (Peters et al., 2017, 2018; Liu et al., 2018b; Akbik et al., 2018; Liu et al., 2018a). At the same time, many approaches have been proposed specifically to solve the NER task in Arabic. Traditional Arabic NER models are mostly rule-basedmodels (Shaalan, 2014). Recently, people have started to attach this task with machine learning methods (Helwe and Elbassuoni, 2017; Gridach, 2016). To further improve the performance, attempts have been made to combine both rule-based and learning-based approaches into a unified framework (Pasha et al., 2014; Abdelali et al., 2016). Besides, incorporating additional supervision from other domains or lan"
W19-4607,K18-1028,0,0.0140103,"labelled as j and k ˆ n }. Also, we calculate Bi (j) = |{n|ˆ in {Y yn,i = j}|, which is the times of i-th token being labelled as j. Then the integrated label sequence is calculated with dynamic programming: ˆ = arg max Y TX −1 t=1 Rt (ˆ yt , yˆt+1 ) + T X 5.1 Word Embedding Based on the distributional hypothesis (i.e., “a word is characterized by the company it keeps” (Harris, 1954)), word embedding methods aim to learn the distributed representations by analyzing their contexts (Mikolov et al., 2013). Recent work shows that word embedding could uncover textual information of various levels (Artetxe et al., 2018). Hence, we leverage word embedding as a part of the word representation. Due to the limited size of the training set, we fix the pre-trained word embedding during the training of NER models. When the pre-trained embedding has a high dimension, we will add a linear projection to further project them to a relatively low dimension. 5.2 Bt (ˆ yt ) Contextualized Representation Contextualized representations have been widely adopted in the state-of-the-art sequence labeling models. Typically, they rely on bidirectional neural language models to capture the local contextual information before and a"
W19-4607,P16-1101,0,0.0238661,"deep learning demonstrated its effectiveness, traditional methods rely on handcrafted features (e.g., features based on POS tags) and language-specific resources (e.g., gazetteers) to capture textual signals. Machine learning models like conditional random field (CRF) and hidden Markov model (HMM) are employed to capture the label dependency (Lafferty et al., 2001; Florian et al., 2003; Chieu and Ng, 2002). Many attempts have been made to reduce the reliance on feature engineering or other human endeavors, which makes the NER task be solved in an end-to-end manner (Lample et al., 2016; Ma and Hovy, 2016; Shang et al., 2018). Recent studies have revealed that language model is an effective representation module for NER (Peters et al., 2017, 2018; Liu et al., 2018b; Akbik et al., 2018; Liu et al., 2018a). At the same time, many approaches have been proposed specifically to solve the NER task in Arabic. Traditional Arabic NER models are mostly rule-basedmodels (Shaalan, 2014). Recently, people have started to attach this task with machine learning methods (Helwe and Elbassuoni, 2017; Gridach, 2016). To further improve the performance, attempts have been made to combine both rule-based and learn"
W19-4607,Q17-1010,0,0.0190113,"construct contextualized representations for the downstream task, whose input would be space separated, we conduct further preprocessing. Specifically, we first tokenize the text, then concatenate the token sequence by space. To demonstrate the importance of pre-processing, we trained two kinds of language models, one with pre-processing, and the other without. For pre-trained word embedding, we adopt two sets of pre-trained embedding. One is trained with the word2vec model (Mikolov et al., 2013). It has 100 dimensions and is public available 6 . The other is trained with the Fasttext model (Bojanowski et al., 2017), which is released together with 156 other languages 7 . It has 300 dimensions and would be projected to 100 dimensions before concatenating with other vectors. Language Model Integration. Using the bidirectional character-level language models, we construct contextualized representations for each word. Specifically, we feed the input character sequence C to language models, and then concatenate the hidden state of the forward language model at ci, and the hidden state of the backward language model at ci−1, as the representations for xi . We refer these two hidden states as hi and hri . Due"
W19-4607,C02-1025,0,0.0736613,"character right after xi . Then, the goal of NER becomes to predict the label yi for each token xi in the input sequence X. Typically, named entity recognition is conducted as a sequence labeling task. Before deep learning demonstrated its effectiveness, traditional methods rely on handcrafted features (e.g., features based on POS tags) and language-specific resources (e.g., gazetteers) to capture textual signals. Machine learning models like conditional random field (CRF) and hidden Markov model (HMM) are employed to capture the label dependency (Lafferty et al., 2001; Florian et al., 2003; Chieu and Ng, 2002). Many attempts have been made to reduce the reliance on feature engineering or other human endeavors, which makes the NER task be solved in an end-to-end manner (Lample et al., 2016; Ma and Hovy, 2016; Shang et al., 2018). Recent studies have revealed that language model is an effective representation module for NER (Peters et al., 2017, 2018; Liu et al., 2018b; Akbik et al., 2018; Liu et al., 2018a). At the same time, many approaches have been proposed specifically to solve the NER task in Arabic. Traditional Arabic NER models are mostly rule-basedmodels (Shaalan, 2014). Recently, people hav"
W19-4607,E12-1017,0,0.0303099,"Arabic NER models are mostly rule-basedmodels (Shaalan, 2014). Recently, people have started to attach this task with machine learning methods (Helwe and Elbassuoni, 2017; Gridach, 2016). To further improve the performance, attempts have been made to combine both rule-based and learning-based approaches into a unified framework (Pasha et al., 2014; Abdelali et al., 2016). Besides, incorporating additional supervision from other domains or languages has been explored as well (Darwish, 2013). 3 3.2 The Arabic NER challenge uses the public Arabic NER benchmark dataset (i.e., the AQMAR dataset) (Mohit et al., 2012). Its annotated entities are classified into four types (i.e., “Person”, “Location”, “Organization” and “Miscellaneous”). This dataset contains 28 hand-annotated Arabic Wikipedia articles, 14 articles are used as the training set, 7 articles are used as the development set, and 7 articles are used as the test set. Data cleaning is further conducted on this dataset. Specifically, we observed that the label sequence is encoded in a noisy manner. For example, some entities are labelled as {B-, O, I-}, while the legit label sequence should be {B-, I-, I-}; Some entities are labelled as {B-T0 , I-T"
W19-4607,P13-1153,0,0.0285752,"2018a). At the same time, many approaches have been proposed specifically to solve the NER task in Arabic. Traditional Arabic NER models are mostly rule-basedmodels (Shaalan, 2014). Recently, people have started to attach this task with machine learning methods (Helwe and Elbassuoni, 2017; Gridach, 2016). To further improve the performance, attempts have been made to combine both rule-based and learning-based approaches into a unified framework (Pasha et al., 2014; Abdelali et al., 2016). Besides, incorporating additional supervision from other domains or languages has been explored as well (Darwish, 2013). 3 3.2 The Arabic NER challenge uses the public Arabic NER benchmark dataset (i.e., the AQMAR dataset) (Mohit et al., 2012). Its annotated entities are classified into four types (i.e., “Person”, “Location”, “Organization” and “Miscellaneous”). This dataset contains 28 hand-annotated Arabic Wikipedia articles, 14 articles are used as the training set, 7 articles are used as the development set, and 7 articles are used as the test set. Data cleaning is further conducted on this dataset. Specifically, we observed that the label sequence is encoded in a noisy manner. For example, some entities a"
W19-4607,pasha-etal-2014-madamira,0,0.0923791,"Missing"
W19-4607,W03-0425,0,0.0963751,"i and ci, is the space character right after xi . Then, the goal of NER becomes to predict the label yi for each token xi in the input sequence X. Typically, named entity recognition is conducted as a sequence labeling task. Before deep learning demonstrated its effectiveness, traditional methods rely on handcrafted features (e.g., features based on POS tags) and language-specific resources (e.g., gazetteers) to capture textual signals. Machine learning models like conditional random field (CRF) and hidden Markov model (HMM) are employed to capture the label dependency (Lafferty et al., 2001; Florian et al., 2003; Chieu and Ng, 2002). Many attempts have been made to reduce the reliance on feature engineering or other human endeavors, which makes the NER task be solved in an end-to-end manner (Lample et al., 2016; Ma and Hovy, 2016; Shang et al., 2018). Recent studies have revealed that language model is an effective representation module for NER (Peters et al., 2017, 2018; Liu et al., 2018b; Akbik et al., 2018; Liu et al., 2018a). At the same time, many approaches have been proposed specifically to solve the NER task in Arabic. Traditional Arabic NER models are mostly rule-basedmodels (Shaalan, 2014)."
W19-4607,W16-3703,0,0.0192764,"an endeavors, which makes the NER task be solved in an end-to-end manner (Lample et al., 2016; Ma and Hovy, 2016; Shang et al., 2018). Recent studies have revealed that language model is an effective representation module for NER (Peters et al., 2017, 2018; Liu et al., 2018b; Akbik et al., 2018; Liu et al., 2018a). At the same time, many approaches have been proposed specifically to solve the NER task in Arabic. Traditional Arabic NER models are mostly rule-basedmodels (Shaalan, 2014). Recently, people have started to attach this task with machine learning methods (Helwe and Elbassuoni, 2017; Gridach, 2016). To further improve the performance, attempts have been made to combine both rule-based and learning-based approaches into a unified framework (Pasha et al., 2014; Abdelali et al., 2016). Besides, incorporating additional supervision from other domains or languages has been explored as well (Darwish, 2013). 3 3.2 The Arabic NER challenge uses the public Arabic NER benchmark dataset (i.e., the AQMAR dataset) (Mohit et al., 2012). Its annotated entities are classified into four types (i.e., “Person”, “Location”, “Organization” and “Miscellaneous”). This dataset contains 28 hand-annotated Arabic"
W19-4607,P17-1161,0,0.0200534,"and language-specific resources (e.g., gazetteers) to capture textual signals. Machine learning models like conditional random field (CRF) and hidden Markov model (HMM) are employed to capture the label dependency (Lafferty et al., 2001; Florian et al., 2003; Chieu and Ng, 2002). Many attempts have been made to reduce the reliance on feature engineering or other human endeavors, which makes the NER task be solved in an end-to-end manner (Lample et al., 2016; Ma and Hovy, 2016; Shang et al., 2018). Recent studies have revealed that language model is an effective representation module for NER (Peters et al., 2017, 2018; Liu et al., 2018b; Akbik et al., 2018; Liu et al., 2018a). At the same time, many approaches have been proposed specifically to solve the NER task in Arabic. Traditional Arabic NER models are mostly rule-basedmodels (Shaalan, 2014). Recently, people have started to attach this task with machine learning methods (Helwe and Elbassuoni, 2017; Gridach, 2016). To further improve the performance, attempts have been made to combine both rule-based and learning-based approaches into a unified framework (Pasha et al., 2014; Abdelali et al., 2016). Besides, incorporating additional supervision f"
W19-4607,N18-1202,0,0.060796,"Missing"
W19-4607,W09-1119,0,0.055275,"NER Challenge 4 Model Framework As visualized in Figure 1, we design a heterogeneous framework, which incorporates various techniques: (1) It employs representation learning and sequence labeling as the basic sequence labeling model; (2) It leverages ensemble learning to combine outputs from different NER models; and (3) It further incorporates a dictionary-based string matching model. Sequence Labeling In the sequence labeling framework, NER problems are usually annotated following the labeling schemes like BIO and IOBES. These labeling schemes help us encode the information about entities (Ratinov and Roth, 2009). For example, in the BIO scheme, when a token sequence is identified as a named entity, its starting token and middle/end tokens are labeled as B- and I- followed by the type; and all other words are labeled as 4 https://github.com/LiyuanLucasLiu/ ArabicNER 61 Translation: Indeed, no European country has sent a team until the two months before the start of the competition. ا@نافسة . Contextualized Representation لبداية =السابق الشهرين حتى فريقا أوروبية دولة لم ترسل ، وبالفعل Word Embedding Feature Engineering Building Dictionary-based Named Entity Recognition Model O O O O"
W19-4607,J14-2008,0,0.0261933,"an et al., 2003; Chieu and Ng, 2002). Many attempts have been made to reduce the reliance on feature engineering or other human endeavors, which makes the NER task be solved in an end-to-end manner (Lample et al., 2016; Ma and Hovy, 2016; Shang et al., 2018). Recent studies have revealed that language model is an effective representation module for NER (Peters et al., 2017, 2018; Liu et al., 2018b; Akbik et al., 2018; Liu et al., 2018a). At the same time, many approaches have been proposed specifically to solve the NER task in Arabic. Traditional Arabic NER models are mostly rule-basedmodels (Shaalan, 2014). Recently, people have started to attach this task with machine learning methods (Helwe and Elbassuoni, 2017; Gridach, 2016). To further improve the performance, attempts have been made to combine both rule-based and learning-based approaches into a unified framework (Pasha et al., 2014; Abdelali et al., 2016). Besides, incorporating additional supervision from other domains or languages has been explored as well (Darwish, 2013). 3 3.2 The Arabic NER challenge uses the public Arabic NER benchmark dataset (i.e., the AQMAR dataset) (Mohit et al., 2012). Its annotated entities are classified int"
W19-4607,D18-1230,1,0.835416,"g demonstrated its effectiveness, traditional methods rely on handcrafted features (e.g., features based on POS tags) and language-specific resources (e.g., gazetteers) to capture textual signals. Machine learning models like conditional random field (CRF) and hidden Markov model (HMM) are employed to capture the label dependency (Lafferty et al., 2001; Florian et al., 2003; Chieu and Ng, 2002). Many attempts have been made to reduce the reliance on feature engineering or other human endeavors, which makes the NER task be solved in an end-to-end manner (Lample et al., 2016; Ma and Hovy, 2016; Shang et al., 2018). Recent studies have revealed that language model is an effective representation module for NER (Peters et al., 2017, 2018; Liu et al., 2018b; Akbik et al., 2018; Liu et al., 2018a). At the same time, many approaches have been proposed specifically to solve the NER task in Arabic. Traditional Arabic NER models are mostly rule-basedmodels (Shaalan, 2014). Recently, people have started to attach this task with machine learning methods (Helwe and Elbassuoni, 2017; Gridach, 2016). To further improve the performance, attempts have been made to combine both rule-based and learning-based approaches"
W19-4610,D17-1098,0,0.0299594,"we demonstrate how the templatic pattern-based word formation process that transforms the root to the original word can be used for further morphological decomposition. Our root extraction method differentiates itself from other methods in three 89 character-level input space as does our own method, they ignore the sequential nature in the target class. Closely related to our model, constrained sequence-to-sequence models have been used for sentence simplification forcing the model to select simple words (Zhang et al., 2017). Similar approaches have been used for constrained image captioning (Anderson et al., 2017). Our model differs in that it constrains not only on specific vocabulary, but on specific sequences. This method, however, may incorrectly remove many letters that are part of the root. Another of these models achieves high accuracy by incorporating sentence-level context and inferred syntactic categories into a parametric Bayesian model (Lee et al., 2011). Our model forgoes these context features as it attempts to identify the root solely on the word itself. Additionally, this method cannot model non-contiguous roots, of which Semitic languages have many. Other unsupervised methods utilize d"
W19-4610,W11-0301,0,0.0318201,"related to our model, constrained sequence-to-sequence models have been used for sentence simplification forcing the model to select simple words (Zhang et al., 2017). Similar approaches have been used for constrained image captioning (Anderson et al., 2017). Our model differs in that it constrains not only on specific vocabulary, but on specific sequences. This method, however, may incorrectly remove many letters that are part of the root. Another of these models achieves high accuracy by incorporating sentence-level context and inferred syntactic categories into a parametric Bayesian model (Lee et al., 2011). Our model forgoes these context features as it attempts to identify the root solely on the word itself. Additionally, this method cannot model non-contiguous roots, of which Semitic languages have many. Other unsupervised methods utilize dictionaries to select the characters from within words (Darwish, 2002; Boudlal et al., 2011; Alhanini and Ab Aziz, 2011). Another line of research leverages the templatic nature for human-constructed rule-based constraints (Elghamry, 2005; Rodrigues and Cavar, 2007; Choueka, 1990). Finally, methods have been proposed that utilize both a root dictionary and"
W19-4610,D15-1166,0,0.0105536,"the non-contiguous nature of Semitic roots, they fail to leverage the sequential structure of the root label space. We show that such methods that forgo the sequential structure in the label space underperform on words with rare roots. Additionally, these methods are only applied to triconsonantal leaving out many biconsonantal and quadriliteral roots. Sequence-to-sequence models have been utilized for learning to map sequences to other sequences and predominantly applied to machine translation (Sutskever et al., 2014), with later variations of these models enhanced with attention mechanisms (Luong et al., 2015). While LSTM variants have been dominant, previous work has shown that GRU-based models perform comparably to LSTM-based models with superior train time (Chung et al., 2014). More recent work has investigated character-level language models in order to handle the many outof-vocabulary (OOV) words in morphologically rich languages (Gerz et al., 2018). Such methods have shown large improvements in language modeling across many morphologically rich languages. While such methods share the same 3 Root Extraction Framework We introduce a framework for extracting the root from templatic words within"
W19-4610,W02-0506,0,0.0228147,"ecific vocabulary, but on specific sequences. This method, however, may incorrectly remove many letters that are part of the root. Another of these models achieves high accuracy by incorporating sentence-level context and inferred syntactic categories into a parametric Bayesian model (Lee et al., 2011). Our model forgoes these context features as it attempts to identify the root solely on the word itself. Additionally, this method cannot model non-contiguous roots, of which Semitic languages have many. Other unsupervised methods utilize dictionaries to select the characters from within words (Darwish, 2002; Boudlal et al., 2011; Alhanini and Ab Aziz, 2011). Another line of research leverages the templatic nature for human-constructed rule-based constraints (Elghamry, 2005; Rodrigues and Cavar, 2007; Choueka, 1990). Finally, methods have been proposed that utilize both a root dictionary and rule-based templatic constraints (Yaseen and Hmeidi, 2014). Supervised methods have been developed for identifying Hebrew roots by combining various multiclass classification models with Hebrewspecific linguistic constraints (Daya et al., 2004). This same technique was extended to extract both Arabic and Hebr"
W19-4610,W04-3246,0,0.0441307,"thods utilize dictionaries to select the characters from within words (Darwish, 2002; Boudlal et al., 2011; Alhanini and Ab Aziz, 2011). Another line of research leverages the templatic nature for human-constructed rule-based constraints (Elghamry, 2005; Rodrigues and Cavar, 2007; Choueka, 1990). Finally, methods have been proposed that utilize both a root dictionary and rule-based templatic constraints (Yaseen and Hmeidi, 2014). Supervised methods have been developed for identifying Hebrew roots by combining various multiclass classification models with Hebrewspecific linguistic constraints (Daya et al., 2004). This same technique was extended to extract both Arabic and Hebrew roots (Daya et al., 2008). While these supervised methods effectively address the non-contiguous nature of Semitic roots, they fail to leverage the sequential structure of the root label space. We show that such methods that forgo the sequential structure in the label space underperform on words with rare roots. Additionally, these methods are only applied to triconsonantal leaving out many biconsonantal and quadriliteral roots. Sequence-to-sequence models have been utilized for learning to map sequences to other sequences an"
W19-4610,Q17-1010,0,0.103511,"Missing"
W19-4610,J08-3005,0,0.0374915,"et al., 2011; Alhanini and Ab Aziz, 2011). Another line of research leverages the templatic nature for human-constructed rule-based constraints (Elghamry, 2005; Rodrigues and Cavar, 2007; Choueka, 1990). Finally, methods have been proposed that utilize both a root dictionary and rule-based templatic constraints (Yaseen and Hmeidi, 2014). Supervised methods have been developed for identifying Hebrew roots by combining various multiclass classification models with Hebrewspecific linguistic constraints (Daya et al., 2004). This same technique was extended to extract both Arabic and Hebrew roots (Daya et al., 2008). While these supervised methods effectively address the non-contiguous nature of Semitic roots, they fail to leverage the sequential structure of the root label space. We show that such methods that forgo the sequential structure in the label space underperform on words with rare roots. Additionally, these methods are only applied to triconsonantal leaving out many biconsonantal and quadriliteral roots. Sequence-to-sequence models have been utilized for learning to map sequences to other sequences and predominantly applied to machine translation (Sutskever et al., 2014), with later variations"
W19-4610,W18-1202,1,0.901188,"Missing"
W19-4610,P17-2072,0,0.100663,"an and Pearson rank correlation coefficients. As seen in Table 5, enriching the embedding vectors with the template-based extracted 5.3 Word Analogy Evaluation Given our comprehensive dataset of Arabic roots and human-curated evaluation set of Arabic word embeddings, we show the effectiveness of enriching Arabic word embeddings with their morphological decompositions via a word analogy task. The goal of said task is to identify the best value for D in analogies of the form “A is to B as C is to D”. After training each embedding model on the Arabic Wikipedia dataset, we use an analogy dataset (Elrazzaz et al., 2017) curated for methodological evaluation of Arabic word embeddings. 94 Embedding Model SkipGram FastText ISRI-RootVec BiGRU-RootVec S2S-RootVec CS2S-RootVec ISRI-TemplaticVec Class-TemplaticVec S2S-TemplaticVec CS2S-TemplaticVec Pearson 0.496 0.459 0.491 0.492 0.508 0.507 0.482 0.474 0.514 0.512 Spearman 0.520 0.468 0.518 0.510 0.516 0.514 0.501 0.491 0.529 0.533 Embedding Model SkipGram FastText ISRI-RootVec BiGRU-RootVec S2S-RootVec CS2S-RootVec ISRI-TemplaticVec Class-TemplaticVec S2S-TemplaticVec CS2S-TemplaticVec Table 5: Word Similarity Table 6: Language Modeling morphemes substantially im"
W19-4610,Q18-1032,0,0.0566188,"Missing"
W19-4610,D14-1181,0,0.00439462,"Missing"
