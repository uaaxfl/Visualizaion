2020.acl-main.739,P17-1008,0,0.0131869,"mple tweet with text and an image. The author of the tweet possesses the cup for a few weeks or months. The tweet does not indicate a co-possession. Introduction Relation extraction is a core problem in natural language processing. Extracting relations is generally defined as linking two text chunks with a label. For example, relations such as PRESI DENT OF and MARRIED TO are common in information extraction (Angeli et al., 2015). Within computational semantics, relations capture spatial and temporal knowledge (Kordjamshidi et al., 2018; McDowell et al., 2017), as well as many other meanings (Abend and Rappoport, 2017). Approaches to relation extraction usually only determine the right label—often referred to as relation name or type—between two text chunks. Relation labels are certainly useful, but there is almost always complementary information that can be extracted. For example, relation labels do not give any hint about for how long the relation holds true or whether the relation is one-to-one or oneto-many. Many relations would benefit from having this additional information available, including LOCATED AT (people have many locations over time) and AGENT (some events are carried out by ∗ Work done at"
2020.acl-main.739,P15-1034,0,0.0357843,"se a binary label. Cohen’s kappa coefficients indicate substantial agreement, and experimental results show that text is more useful than the image for solving these tasks. 1 Figure 1: Sample tweet with text and an image. The author of the tweet possesses the cup for a few weeks or months. The tweet does not indicate a co-possession. Introduction Relation extraction is a core problem in natural language processing. Extracting relations is generally defined as linking two text chunks with a label. For example, relations such as PRESI DENT OF and MARRIED TO are common in information extraction (Angeli et al., 2015). Within computational semantics, relations capture spatial and temporal knowledge (Kordjamshidi et al., 2018; McDowell et al., 2017), as well as many other meanings (Abend and Rappoport, 2017). Approaches to relation extraction usually only determine the right label—often referred to as relation name or type—between two text chunks. Relation labels are certainly useful, but there is almost always complementary information that can be extracted. For example, relation labels do not give any hint about for how long the relation holds true or whether the relation is one-to-one or oneto-many. Many"
2020.acl-main.739,J08-4004,0,0.294892,"Missing"
2020.acl-main.739,L18-1242,0,0.0199727,"ns that are rarely explicitly stated. Previous works on extracting possession relations primarily fall under efforts to extract large relation inventories. The goal of these efforts is to identify which relation—out of a predefined inventory—holds between two arguments. For example, Tratz and Hovy (2013) investigate semantic relations realized by English possessive constructions, both Nakov and Hearst (2013) and Tratz and Hovy (2010) consider relations realized by noun compounds such as family estate, and Badulescu and Moldovan (2009) extract relations realized by English genitives. Recently, Blodgett and Schneider (2018) present a corpus of web reviews in which the s-genitive and of-genitive are annotated with semantic labels (or supersenses). Regardless of the lexico-syntactic pattern, possession relations are a minority of the relations targeted by these previous works (other relations include THEME, QUANTITY, CAUSE, ORIGINATOR, EXPERIENCER , etc.). In addition, they do not target possession duration or co-possession. 1 Available at http://dhivyachinnappa.com To the best of our knowledge, there are three previous works on extracting possession relations. All of them introduce their own annotations and prese"
2020.acl-main.739,N18-1046,1,0.900416,"-genitive and of-genitive are annotated with semantic labels (or supersenses). Regardless of the lexico-syntactic pattern, possession relations are a minority of the relations targeted by these previous works (other relations include THEME, QUANTITY, CAUSE, ORIGINATOR, EXPERIENCER , etc.). In addition, they do not target possession duration or co-possession. 1 Available at http://dhivyachinnappa.com To the best of our knowledge, there are three previous works on extracting possession relations. All of them introduce their own annotations and present experimental results. In our previous work (Chinnappa and Blanco, 2018), we consider possession relations between individuals (named entity person and personal pronouns) and concrete objects mentioned within the same sentence in the OntoNotes corpus. Regarding time, we indicate whether the possession held true before, during or after the event in the sentence. Banea and Mihalcea (2018) consider possessions between the author of a weblog (i.e., the possessor is fixed) and the possessees identified in the weblog. Regarding time, they exclusively target possessions that held true when the weblog was written—not before or after. More recently, we investigate the prob"
2020.acl-main.739,D19-1061,1,0.787486,"within the same sentence in the OntoNotes corpus. Regarding time, we indicate whether the possession held true before, during or after the event in the sentence. Banea and Mihalcea (2018) consider possessions between the author of a weblog (i.e., the possessor is fixed) and the possessees identified in the weblog. Regarding time, they exclusively target possessions that held true when the weblog was written—not before or after. More recently, we investigate the problem of determining whether authors of tweets possess the objects they tweet about, and use tweets consisting of text and images (Chinnappa et al., 2019). All of these previous efforts target possession existence (i.e., whether a possession relation holds true) and very limited temporal information. Unlike them, we go beyond possession existence and target possession duration and co-possession. Finally, we note that theoretical works consider having temporary control of something as a type of possession (Tham, 2004). For example, ship captains and plane pilots have control possession of the ships and planes under their command, but usually not ownership or alienable possession. Similarly, office workers have control possession of their work de"
2020.acl-main.739,J11-4005,0,0.20187,"ple, ship captains and plane pilots have control possession of the ships and planes under their command, but usually not ownership or alienable possession. Similarly, office workers have control possession of their work desk and computer, but they do not own them. According to this definition, control possessions indicate co-possession. We note, however, that control possessions are only a subset of possessions thus they are insufficient to determine co-possession. Event Durations. Our methodology to annotate possession durations is heavily inspired by previous work targeting event durations (Pan et al., 2011). The main difference is that we do not target events (e.g., How long did met in John met his advisor on Thursday last?) but possession relations. As we shall see, we derive sound time intervals for possession durations from lower and upper temporal bounds. To the best of our knowledge, we are the first to target the duration in which a semantic relation holds true. Not surprisingly, we find that possession durations tend to be longer than events. For example, events may last only a few seconds 8333 (e.g., turn on a car), but possessions last at least a few minutes and many last over a year. d"
2020.acl-main.739,D14-1162,0,0.0823732,"Missing"
2020.acl-main.739,P10-1070,0,0.0418961,"ue. There are, however, some exceptions that assign temporal information to relations (Ji et al., 2011; McClosky and Manning, 2012). Unlike these previous efforts, we work with durations that are rarely explicitly stated. Previous works on extracting possession relations primarily fall under efforts to extract large relation inventories. The goal of these efforts is to identify which relation—out of a predefined inventory—holds between two arguments. For example, Tratz and Hovy (2013) investigate semantic relations realized by English possessive constructions, both Nakov and Hearst (2013) and Tratz and Hovy (2010) consider relations realized by noun compounds such as family estate, and Badulescu and Moldovan (2009) extract relations realized by English genitives. Recently, Blodgett and Schneider (2018) present a corpus of web reviews in which the s-genitive and of-genitive are annotated with semantic labels (or supersenses). Regardless of the lexico-syntactic pattern, possession relations are a minority of the relations targeted by these previous works (other relations include THEME, QUANTITY, CAUSE, ORIGINATOR, EXPERIENCER , etc.). In addition, they do not target possession duration or co-possession."
2020.acl-main.739,P13-1037,0,0.0269122,"ons. 2 Related Work Most previous work on relation extraction does not identify the temporal bounds during which a relation holds true. There are, however, some exceptions that assign temporal information to relations (Ji et al., 2011; McClosky and Manning, 2012). Unlike these previous efforts, we work with durations that are rarely explicitly stated. Previous works on extracting possession relations primarily fall under efforts to extract large relation inventories. The goal of these efforts is to identify which relation—out of a predefined inventory—holds between two arguments. For example, Tratz and Hovy (2013) investigate semantic relations realized by English possessive constructions, both Nakov and Hearst (2013) and Tratz and Hovy (2010) consider relations realized by noun compounds such as family estate, and Badulescu and Moldovan (2009) extract relations realized by English genitives. Recently, Blodgett and Schneider (2018) present a corpus of web reviews in which the s-genitive and of-genitive are annotated with semantic labels (or supersenses). Regardless of the lexico-syntactic pattern, possession relations are a minority of the relations targeted by these previous works (other relations inc"
2020.acl-main.739,D12-1080,0,0.073476,"Missing"
2020.acl-main.739,I17-1085,0,0.0258799,"l than the image for solving these tasks. 1 Figure 1: Sample tweet with text and an image. The author of the tweet possesses the cup for a few weeks or months. The tweet does not indicate a co-possession. Introduction Relation extraction is a core problem in natural language processing. Extracting relations is generally defined as linking two text chunks with a label. For example, relations such as PRESI DENT OF and MARRIED TO are common in information extraction (Angeli et al., 2015). Within computational semantics, relations capture spatial and temporal knowledge (Kordjamshidi et al., 2018; McDowell et al., 2017), as well as many other meanings (Abend and Rappoport, 2017). Approaches to relation extraction usually only determine the right label—often referred to as relation name or type—between two text chunks. Relation labels are certainly useful, but there is almost always complementary information that can be extracted. For example, relation labels do not give any hint about for how long the relation holds true or whether the relation is one-to-one or oneto-many. Many relations would benefit from having this additional information available, including LOCATED AT (people have many locations over tim"
2020.acl-main.743,W12-3808,0,0.069544,"Missing"
2020.acl-main.743,L16-1597,0,0.0374931,"Missing"
2020.acl-main.743,D16-1025,0,0.0172497,"nalysis benefits from processing negation (Wiegand et al., 2010). For example, like generally carries positive sentiment, but not when modified by a negation cue (e.g., don’t like). Wilson et al. (2005) introduce the idea of contextual polarity, and note that negation may intensify rather than change polarity (e.g., not good vs. not only good but amazing). Jia et al. (2009) present a set of heuristic rules to determine sentiment when negation is present, and Councill et al. (2010) show that information about the scope of negation is beneficial to predict sentiment. Outside sentiment analysis, Bentivogli et al. (2016) point out that neural machine translation struggles translating negation, and point to focus detection as a possible solution. Neural networks are hard to interpret, but there is evidence that they learn to process negation—to a certain degree—when trained to predict sentiment analysis. Li et al. (2016) visually show that neural networks are capable of meaning composition in the presence of, among others, negation and intensification. Wang et al. (2015) show that an LSTM architecture is capable of determining sentiment of sequences containing negation such as not good and not bad. These previ"
2020.acl-main.743,P11-1059,1,0.876938,"(e.g., There is no friend like [. . . ]), pronouns (e.g., [. . . ] has yielded nothing to a careful search), affixes (e.g., The inexplicable tangle seemed [. . . ]), and others. Other corpora annotating scope in English include efforts with biomedical texts (Vincze et al., 2008) and working with reviews (Councill et al., 2010; Konstantinova et al., 2012). Corpora Annotating Focus. Although focus of negation is defined as a subset of the scope, there is no corpus annotating both of them in the same texts. We work with PB-FOC, the largest publicly available corpus annotating focus of negation (Blanco and Moldovan, 2011). PB-FOC annotates the focus of the negations marked with M - NEG role in PropBank (Palmer et al., 2005), which in turn annotates semantic roles on top of the Penn TreeBank (Taylor et al., 2003). As a result, PB-FOC annotates the focus of 3,544 verbal negations (i.e., when a negation cue such as never or not syntactically modifies a verb). As per the authors, the annotation process consisted of selecting the semantic role most likely to be the focus. Therefore, focus annotations in PB-FOC are always all the tokens corresponding to a semantic role of the (negated) verb. Finally, M - NEG role is"
2020.acl-main.743,W05-0620,0,0.384248,"Missing"
2020.acl-main.743,W10-3110,0,0.598777,"per role, percentages of negated verbs having each role, and percentage of each role being the focus. of which contain a negation. CD-SCO annotates all negations, including verbs (e.g., I fail to see how you could have done more), adverbs (e.g., It was never proved that [. . . ]), determiners (e.g., There is no friend like [. . . ]), pronouns (e.g., [. . . ] has yielded nothing to a careful search), affixes (e.g., The inexplicable tangle seemed [. . . ]), and others. Other corpora annotating scope in English include efforts with biomedical texts (Vincze et al., 2008) and working with reviews (Councill et al., 2010; Konstantinova et al., 2012). Corpora Annotating Focus. Although focus of negation is defined as a subset of the scope, there is no corpus annotating both of them in the same texts. We work with PB-FOC, the largest publicly available corpus annotating focus of negation (Blanco and Moldovan, 2011). PB-FOC annotates the focus of the negations marked with M - NEG role in PropBank (Palmer et al., 2005), which in turn annotates semantic roles on top of the Penn TreeBank (Taylor et al., 2003). As a result, PB-FOC annotates the focus of 3,544 verbal negations (i.e., when a negation cue such as never"
2020.acl-main.743,P19-1508,0,0.0208876,"ng that is in some way opposed to e—a semantic and highly ambiguous undertaking that comes naturally to humans in everyday communication. Negation is generally understood to carry positive meaning, or in other words, to suggest an affirmative alternative. For example, John didn’t leave the house implicates that John stayed inside the house. Hasson and Glucksberg (2006) show that comprehending negation involves considering the representation of affirmative alternatives. While not fully understood, there is evidence that negation involves reduced access to the affirmative mental representation (Djokic et al., 2019). Orenes et al. (2014) provide evidence that humans switch to the affirmative alternative in binary scenarios (e.g., from not red to green when processing The figure could be red or green. The figure is not red). In such multary scenarios, however, humans keep the negated representation unless the affirmative interpretation is obvious from context (e.g., humans keep not red when processing The figure is red, green, yellow or blue. The figure is not red.). From a linguistic perspective, negation is understood in terms of scope and focus (Section 2). The scope is the part of the meaning that is"
2020.acl-main.743,P16-1047,0,0.118667,"get focus of negation detection—and the resulting affirmative alternatives—and work with task-independent negations. Scope Identification. Compared to focus identification, scope identification has received substantially more attention. The first proposals (Morante and Daelemans, 2009) were trained in the biomedical domain with BioScope (Szarvas et al., 2008). The *SEM-2012 Shared Task (Morante and Blanco, 2012) included scope identification with CD-SCO (Section 2), and the winner proposed an SVM-based ranking of syntactic constituents to identify the scope (Read et al., 2012). More recently, Fancellu et al. (2016) present neural networks for this task, and Packard et al. (2014) present a complementary approach that operates over semantic representations obtained with an offthe-shelf parser. Finally, Fancellu et al. (2017) present an error analysis showing that scope is much easier to identify when delimited by punctuation. In this paper, we use a scope detector trained with CD-SCO to predict the focus of negation. While we only incorporate small modifications to previously proposed architectures, our scope detector outperforms previous work (Section 4). Focus Identification. Although focus is part of t"
2020.acl-main.743,E17-2010,0,0.012047,"antially more attention. The first proposals (Morante and Daelemans, 2009) were trained in the biomedical domain with BioScope (Szarvas et al., 2008). The *SEM-2012 Shared Task (Morante and Blanco, 2012) included scope identification with CD-SCO (Section 2), and the winner proposed an SVM-based ranking of syntactic constituents to identify the scope (Read et al., 2012). More recently, Fancellu et al. (2016) present neural networks for this task, and Packard et al. (2014) present a complementary approach that operates over semantic representations obtained with an offthe-shelf parser. Finally, Fancellu et al. (2017) present an error analysis showing that scope is much easier to identify when delimited by punctuation. In this paper, we use a scope detector trained with CD-SCO to predict the focus of negation. While we only incorporate small modifications to previously proposed architectures, our scope detector outperforms previous work (Section 4). Focus Identification. Although focus is part of the scope, state-of-the-art approaches to identify the focus of negation ignore information about scope. Possible reasons are that (a) existing corpora annotating scope and focus contain substantially different te"
2020.acl-main.743,2020.cl-1.5,0,0.0193241,"Missing"
2020.acl-main.743,konstantinova-etal-2012-review,0,0.0744004,"Missing"
2020.acl-main.743,N16-1082,0,0.0276705,".g., not good vs. not only good but amazing). Jia et al. (2009) present a set of heuristic rules to determine sentiment when negation is present, and Councill et al. (2010) show that information about the scope of negation is beneficial to predict sentiment. Outside sentiment analysis, Bentivogli et al. (2016) point out that neural machine translation struggles translating negation, and point to focus detection as a possible solution. Neural networks are hard to interpret, but there is evidence that they learn to process negation—to a certain degree—when trained to predict sentiment analysis. Li et al. (2016) visually show that neural networks are capable of meaning composition in the presence of, among others, negation and intensification. Wang et al. (2015) show that an LSTM architecture is capable of determining sentiment of sequences containing negation such as not good and not bad. These previous works train a model for a particular task (i.e., sentiment analysis) and then investigate whether the model learnt anything related to negation that is useful for that task. Unlike them, we target focus of negation detection—and the resulting affirmative alternatives—and work with task-independent ne"
2020.acl-main.743,S12-1035,1,0.856583,". These previous works train a model for a particular task (i.e., sentiment analysis) and then investigate whether the model learnt anything related to negation that is useful for that task. Unlike them, we target focus of negation detection—and the resulting affirmative alternatives—and work with task-independent negations. Scope Identification. Compared to focus identification, scope identification has received substantially more attention. The first proposals (Morante and Daelemans, 2009) were trained in the biomedical domain with BioScope (Szarvas et al., 2008). The *SEM-2012 Shared Task (Morante and Blanco, 2012) included scope identification with CD-SCO (Section 2), and the winner proposed an SVM-based ranking of syntactic constituents to identify the scope (Read et al., 2012). More recently, Fancellu et al. (2016) present neural networks for this task, and Packard et al. (2014) present a complementary approach that operates over semantic representations obtained with an offthe-shelf parser. Finally, Fancellu et al. (2017) present an error analysis showing that scope is much easier to identify when delimited by punctuation. In this paper, we use a scope detector trained with CD-SCO to predict the foc"
2020.acl-main.743,W09-1105,0,0.0501164,"15) show that an LSTM architecture is capable of determining sentiment of sequences containing negation such as not good and not bad. These previous works train a model for a particular task (i.e., sentiment analysis) and then investigate whether the model learnt anything related to negation that is useful for that task. Unlike them, we target focus of negation detection—and the resulting affirmative alternatives—and work with task-independent negations. Scope Identification. Compared to focus identification, scope identification has received substantially more attention. The first proposals (Morante and Daelemans, 2009) were trained in the biomedical domain with BioScope (Szarvas et al., 2008). The *SEM-2012 Shared Task (Morante and Blanco, 2012) included scope identification with CD-SCO (Section 2), and the winner proposed an SVM-based ranking of syntactic constituents to identify the scope (Read et al., 2012). More recently, Fancellu et al. (2016) present neural networks for this task, and Packard et al. (2014) present a complementary approach that operates over semantic representations obtained with an offthe-shelf parser. Finally, Fancellu et al. (2017) present an error analysis showing that scope is muc"
2020.acl-main.743,morante-daelemans-2012-conandoyle,0,0.250939,"Missing"
2020.acl-main.743,P14-1007,0,0.0135182,"rnatives—and work with task-independent negations. Scope Identification. Compared to focus identification, scope identification has received substantially more attention. The first proposals (Morante and Daelemans, 2009) were trained in the biomedical domain with BioScope (Szarvas et al., 2008). The *SEM-2012 Shared Task (Morante and Blanco, 2012) included scope identification with CD-SCO (Section 2), and the winner proposed an SVM-based ranking of syntactic constituents to identify the scope (Read et al., 2012). More recently, Fancellu et al. (2016) present neural networks for this task, and Packard et al. (2014) present a complementary approach that operates over semantic representations obtained with an offthe-shelf parser. Finally, Fancellu et al. (2017) present an error analysis showing that scope is much easier to identify when delimited by punctuation. In this paper, we use a scope detector trained with CD-SCO to predict the focus of negation. While we only incorporate small modifications to previously proposed architectures, our scope detector outperforms previous work (Section 4). Focus Identification. Although focus is part of the scope, state-of-the-art approaches to identify the focus of ne"
2020.acl-main.743,J05-1004,0,0.133998,"affixes (e.g., The inexplicable tangle seemed [. . . ]), and others. Other corpora annotating scope in English include efforts with biomedical texts (Vincze et al., 2008) and working with reviews (Councill et al., 2010; Konstantinova et al., 2012). Corpora Annotating Focus. Although focus of negation is defined as a subset of the scope, there is no corpus annotating both of them in the same texts. We work with PB-FOC, the largest publicly available corpus annotating focus of negation (Blanco and Moldovan, 2011). PB-FOC annotates the focus of the negations marked with M - NEG role in PropBank (Palmer et al., 2005), which in turn annotates semantic roles on top of the Penn TreeBank (Taylor et al., 2003). As a result, PB-FOC annotates the focus of 3,544 verbal negations (i.e., when a negation cue such as never or not syntactically modifies a verb). As per the authors, the annotation process consisted of selecting the semantic role most likely to be the focus. Therefore, focus annotations in PB-FOC are always all the tokens corresponding to a semantic role of the (negated) verb. Finally, M - NEG role is chosen when the focus is the verb. The annotations in PB-FOC were carried out taking into account the p"
2020.acl-main.743,N18-1202,0,0.0469745,"ture The network architecture (Fig. 1) consists of a base NN (all components except those inside dotted shapes) plus additional components to include information about the scope and context of negation. Base NN. The base network is inspired by Huang et al. (2015) and Reimers and Gurevych (2017). It is a 3-layer Bidirectional Long Short-Term Memory (BiLSTM) network with a Conditional Random Field (CRF) layer. The network takes as input the sentence containing the negation whose focus is to be predicted, where each word is represented with the concatenation of (a) its pre-trained ELMo embedding Peters et al. (2018), (b) a specialized embedding indicating whether a token is the negated verb (not the negation cue), and (c) a specialized embedding indicating semantic roles (one per role label). The specialized embeddings are trained from scratch as part of the tuning of the network. Scope Information. We add an extra input at the token level indicating whether a token belongs to the scope of the negation whose focus is to be predicted. This new input is then mapped to a third specialized embedding (two values: inside or outside the scope), and concatenated to the word representation prior to feeding it to"
2020.acl-main.743,S12-1041,0,0.02697,"for that task. Unlike them, we target focus of negation detection—and the resulting affirmative alternatives—and work with task-independent negations. Scope Identification. Compared to focus identification, scope identification has received substantially more attention. The first proposals (Morante and Daelemans, 2009) were trained in the biomedical domain with BioScope (Szarvas et al., 2008). The *SEM-2012 Shared Task (Morante and Blanco, 2012) included scope identification with CD-SCO (Section 2), and the winner proposed an SVM-based ranking of syntactic constituents to identify the scope (Read et al., 2012). More recently, Fancellu et al. (2016) present neural networks for this task, and Packard et al. (2014) present a complementary approach that operates over semantic representations obtained with an offthe-shelf parser. Finally, Fancellu et al. (2017) present an error analysis showing that scope is much easier to identify when delimited by punctuation. In this paper, we use a scope detector trained with CD-SCO to predict the focus of negation. While we only incorporate small modifications to previously proposed architectures, our scope detector outperforms previous work (Section 4). Focus Iden"
2020.acl-main.743,S12-1039,0,0.0140588,"(Section 4). Focus Identification. Although focus is part of the scope, state-of-the-art approaches to identify the focus of negation ignore information about scope. Possible reasons are that (a) existing corpora annotating scope and focus contain substantially different texts (Section 2), and (b) incorporating scope information is not straightforward with traditional machine learning and manually defined features. The initial proposals obtain modest results and only consider the sentence containing the negation (Blanco and Moldovan, 2011), including scope information in a rule-based system (Rosenberg and Bergler, 2012). Zou et al. (2014, 2015) propose 8391 N_F N_F N_F F F CRF Layer context, previous context, next FC ap FC ap an FC an ap FC an ap FC an an ap 3 layer BiLSTM 2 layer BiLSTM 2 layer BiLSTM scope information Scope emb. StatesWest Nevada The SRL emb. (previous sentence) guarantees (next sentence) Neg. verb emb. Word emb. (ELMo) (current sentence) (neg. verb, roles, scope) The (N, A0, I_S) turned (Y, V, I_S) not (N, AM-NEG, O_S) a (N, A1, I_S) profit (N, A1, I_S) Figure 1: Neural network to predict the focus of negation. The core of the architecture (NN, all components except those inside dotted sh"
2020.acl-main.743,D16-1119,1,0.864561,"Missing"
2020.acl-main.743,E17-1081,1,0.858173,"Missing"
2020.acl-main.743,D19-1230,0,0.0473769,"A1, I_S) profit (N, A1, I_S) Figure 1: Neural network to predict the focus of negation. The core of the architecture (NN, all components except those inside dotted shapes) takes as input the sentence containing the negation, and each word is represented with its word embedding and specialized embeddings for the negated verb and semantic roles. The additional components inside dotted shapes incorporate information about (a) the scope and (b) context (previous and next sentences). graph-based models that incorporate discourse information and obtain improvements over previous works. In addition, Shen et al. (2019) present a neural model that leverages word-level and topic-level attention mechanisms to utilize contextual information. We compare our results and theirs in Section 4.2. In this paper, we show that (a) neural networks considering the scope of negation obtain the best results to date and (b) context is not beneficial if scope is available (Section 4). 4 Predicting the Focus of Negation We approach the task of predicting focus of negation as a sequence labeling task with a neural network. We first describe the network architecture, and then present quantitative results. Section 5 presents a de"
2020.acl-main.743,W08-0606,0,0.0744822,"s containing negation such as not good and not bad. These previous works train a model for a particular task (i.e., sentiment analysis) and then investigate whether the model learnt anything related to negation that is useful for that task. Unlike them, we target focus of negation detection—and the resulting affirmative alternatives—and work with task-independent negations. Scope Identification. Compared to focus identification, scope identification has received substantially more attention. The first proposals (Morante and Daelemans, 2009) were trained in the biomedical domain with BioScope (Szarvas et al., 2008). The *SEM-2012 Shared Task (Morante and Blanco, 2012) included scope identification with CD-SCO (Section 2), and the winner proposed an SVM-based ranking of syntactic constituents to identify the scope (Read et al., 2012). More recently, Fancellu et al. (2016) present neural networks for this task, and Packard et al. (2014) present a complementary approach that operates over semantic representations obtained with an offthe-shelf parser. Finally, Fancellu et al. (2017) present an error analysis showing that scope is much easier to identify when delimited by punctuation. In this paper, we use a"
2020.acl-main.743,P15-1130,0,0.0319357,"ouncill et al. (2010) show that information about the scope of negation is beneficial to predict sentiment. Outside sentiment analysis, Bentivogli et al. (2016) point out that neural machine translation struggles translating negation, and point to focus detection as a possible solution. Neural networks are hard to interpret, but there is evidence that they learn to process negation—to a certain degree—when trained to predict sentiment analysis. Li et al. (2016) visually show that neural networks are capable of meaning composition in the presence of, among others, negation and intensification. Wang et al. (2015) show that an LSTM architecture is capable of determining sentiment of sequences containing negation such as not good and not bad. These previous works train a model for a particular task (i.e., sentiment analysis) and then investigate whether the model learnt anything related to negation that is useful for that task. Unlike them, we target focus of negation detection—and the resulting affirmative alternatives—and work with task-independent negations. Scope Identification. Compared to focus identification, scope identification has received substantially more attention. The first proposals (Mor"
2020.acl-main.743,W10-3111,0,0.10326,"Missing"
2020.acl-main.743,H05-1044,0,0.0788373,"), use dependency trees instead of roles (Sarabi and Blanco, 2016), target non-verbal negations (Sarabi and Blanco, 2017), and work with tutorial dialogues (Banjade and Rus, 2016). 3 Previous Work In addition to identifying negation cues and resolving the scope and focus of negation, there is work showing that processing negation is important for natural language understanding in general. In particular, sentiment analysis benefits from processing negation (Wiegand et al., 2010). For example, like generally carries positive sentiment, but not when modified by a negation cue (e.g., don’t like). Wilson et al. (2005) introduce the idea of contextual polarity, and note that negation may intensify rather than change polarity (e.g., not good vs. not only good but amazing). Jia et al. (2009) present a set of heuristic rules to determine sentiment when negation is present, and Councill et al. (2010) show that information about the scope of negation is beneficial to predict sentiment. Outside sentiment analysis, Bentivogli et al. (2016) point out that neural machine translation struggles translating negation, and point to focus detection as a possible solution. Neural networks are hard to interpret, but there i"
2020.acl-main.743,N16-1174,0,0.0589814,"g the scope of negation: 79.41 F1 (vs. 77.77 F1). We do not elaborate on the scope detector as we only leverage it to predict focus. Context. We also experiment with an additional component to add contextual information (previous and next sentences), as previous work has shown empirically that doing so is beneficial (Zou et al., 2014). While we tried many strategies (e.g., concatenating sentence embeddings to the representations from the 3-layer BiLSTM), we present only the one yielding the best results. Specifically, we use 2-layer Bi-LSTMs with an attention mechanism (Bahdanau et al., 2014; Yang et al., 2016). The attention weights (ap and an for the previous 8392 Zou et al. (2014) Zou et al. (2015) Shen et al. (2019) NN (baseline) NN + S NN + Cntxt NN + S + Cntxt P 71.67 n/a n/a 72.14 75.92 73.69 74.15 R 67.43 n/a n/a 71.63 75.7 73.17 73.74 F1 69.49 n/a n/a 71.88 75.81 73.43 73.94 Acc 67.1 69.4 70.5 71.6 75.7 73.2 73.7 ARG 0 ARG 1 ARG 2 ARG 3 ARG 4 M - NEG M - TMP M - MNR M - ADV Table 2: Focus prediction results of the best performing previous works and our neural network (baseline network and adding components). S and Cntxt refer to Scope and Context, respectively. Note that Zou et al. (2014) d"
2020.acl-main.743,P14-1049,0,0.100283,"tion. Although focus is part of the scope, state-of-the-art approaches to identify the focus of negation ignore information about scope. Possible reasons are that (a) existing corpora annotating scope and focus contain substantially different texts (Section 2), and (b) incorporating scope information is not straightforward with traditional machine learning and manually defined features. The initial proposals obtain modest results and only consider the sentence containing the negation (Blanco and Moldovan, 2011), including scope information in a rule-based system (Rosenberg and Bergler, 2012). Zou et al. (2014, 2015) propose 8391 N_F N_F N_F F F CRF Layer context, previous context, next FC ap FC ap an FC an ap FC an ap FC an an ap 3 layer BiLSTM 2 layer BiLSTM 2 layer BiLSTM scope information Scope emb. StatesWest Nevada The SRL emb. (previous sentence) guarantees (next sentence) Neg. verb emb. Word emb. (ELMo) (current sentence) (neg. verb, roles, scope) The (N, A0, I_S) turned (Y, V, I_S) not (N, AM-NEG, O_S) a (N, A1, I_S) profit (N, A1, I_S) Figure 1: Neural network to predict the focus of negation. The core of the architecture (NN, all components except those inside dotted shapes) takes as inp"
2020.acl-main.743,D15-1187,0,0.0265037,"Missing"
2020.coling-main.60,J08-4004,0,0.11716,"Missing"
2020.coling-main.60,P98-1013,0,0.15433,"Missing"
2020.coling-main.60,M93-1007,0,0.715195,"Missing"
2020.coling-main.60,N19-1423,0,0.0364389,"Missing"
2020.coling-main.60,W12-1610,0,0.0752223,"Missing"
2020.coling-main.60,W13-0402,0,0.0492516,"Missing"
2020.coling-main.60,N16-1056,0,0.0681383,"Missing"
2020.coling-main.60,2020.lrec-1.170,0,0.0554427,"Missing"
2020.coling-main.60,W18-5908,0,0.0609026,"Missing"
2020.coling-main.60,W09-1119,0,0.201933,"Missing"
2020.coling-main.60,D19-1492,0,0.0354018,"Missing"
2020.emnlp-main.732,C04-1180,0,0.192699,"Missing"
2020.emnlp-main.732,D15-1075,0,0.0466415,"entailment or no entailment (Dagan et al., 2006), and a newer formulation considers three labels: entailment, contradiction or neutral (Giampiccolo et al., 2007). For example, the text “A person on a horse jumps over an airplane” entails hypothesis “A person is outdoors, on a horse,” contradicts “A person is at a diner, ordering an omelette,” and is neutral with respect to “A person is training his horse for a competition.” We work with three existing benchmarks: a collection of RTE datasets (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009), SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018). The RTE datasets are smaller (5,767 text-hypothesis pairs) than SNLI and MNLI (569,033 and 431,997 pairs). MNLI is more challenging than RTE and SNLI: texts are longer and were selected from 10 genres including fiction and non-fiction as well as conversation transcripts. On the other hand, the texts in SNLI were selected from image captions. The hypotheses in SNLI and MNLI were crowdsourced, i.e., manually generated by non-experts. Tables 2 and 4 show examples in the RTE, SNLI and MNLI benchmarks. We work with the formatted versions of these datasets in the G"
2020.emnlp-main.732,W10-3110,0,0.0769832,"Missing"
2020.emnlp-main.732,P09-1053,0,0.116199,"Missing"
2020.emnlp-main.732,N19-1423,0,0.0488832,"gation plays an important role. We also show that state-of-the-art transformers struggle making inference judgments with the new pairs. 1 Introduction Natural language understanding remains an elusive goal except in limited scenarios. It is arguably the ultimate problem in natural language processing: to empower machines to understand language as generated by humans. The state of the art has seen tremendous progress in recent years, and has moved from symbolic representations (Bos et al., 2004; Artzi and Zettlemoyer, 2013) to distributional representations often learned from massive datasets (Devlin et al., 2019). Recognizing entailments (Dagan et al., 2006), identifying paraphrases (Das and Smith, 2009), determining semantic textual similarity (Agirre et al., 2012), and sentiment analysis (Pang and Lee, 2008) are but a few problems that require natural language understanding to a lesser or greater degree. There are many benchmarks targeting the problems above, and they usually cast them as classification problems. A couple of popular evaluation platforms, GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019), aggregate benchmarks for some of the problems above and provide a single score for many"
2020.emnlp-main.732,C04-1051,0,0.15017,"Missing"
2020.emnlp-main.732,W07-1401,0,0.125087,"s formers trained with the original benchmarks are not robust when negation is present. 4. We provide empirical evidence that transformers may be unable to learn the intricacies of negation in the most challenging benchmark, which includes longer texts from many genres. 2 Background The task of natural language inference or recognizing textual entailment consists in determining whether a hypothesis is true given a text. The original task considers two labels: entailment or no entailment (Dagan et al., 2006), and a newer formulation considers three labels: entailment, contradiction or neutral (Giampiccolo et al., 2007). For example, the text “A person on a horse jumps over an airplane” entails hypothesis “A person is outdoors, on a horse,” contradicts “A person is at a diner, ordering an omelette,” and is neutral with respect to “A person is training his horse for a competition.” We work with three existing benchmarks: a collection of RTE datasets (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009), SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018). The RTE datasets are smaller (5,767 text-hypothesis pairs) than SNLI and MNLI (569,033 and 431,997 pairs)."
2020.emnlp-main.732,P18-2103,0,0.037755,"Missing"
2020.emnlp-main.732,N18-2017,0,0.0435477,"Missing"
2020.emnlp-main.732,2020.acl-main.743,1,0.75562,"Missing"
2020.emnlp-main.732,N06-2015,0,0.107565,"Missing"
2020.emnlp-main.732,N18-1170,0,0.0548482,"Missing"
2020.emnlp-main.732,R19-1067,1,0.692787,"Missing"
2020.emnlp-main.732,E14-3011,0,0.020485,"Missing"
2020.emnlp-main.732,2021.ccl-1.108,0,0.0380573,"Missing"
2020.emnlp-main.732,P11-1015,0,0.0850636,"Missing"
2020.emnlp-main.732,S12-1035,1,0.800709,"t challenging benchmark, MNLI. 4 Negation in English and Natural Language Inference Benchmarks Negation is pervasive in English (Morante and Sporleder, 2012), although there is limited empirical evidence from previous work (Councill et al., 2010; Elkin et al., 2005). In order to conduct a large-scale analysis and compare how often negation is present in English and existing natural language inference benchmarks, we employ a negation cue detector using a Bi-LSTM neural architecture with an additional CRF layer (Hossain et al., 2020). Trained and tested with CD-SCO, a corpus publicly available (Morante and Blanco, 2012), it obtains 0.92 F1. The supplemental materials provide more details regarding the architecture of the negation cue detector and the negation cues it detects. Table 1 details the percentage of sentences with at least one negation in several large generalpurpose English corpora. We work with online reviews (Wan et al., 2019; Maas et al., 2011), conversations (Chang et al., 2019), Wikipedia (50,000 pages with at least 20 views), 500 books from Project Gutenberg (Lahiri, 2014), and OntoNotes (Hovy et al., 2006) as released by Pradhan et al. (2011). The percentage of sentences containing negation"
2020.emnlp-main.732,P09-1034,0,0.0713823,"Missing"
2020.emnlp-main.732,W17-4504,0,0.0555041,"Missing"
2020.emnlp-main.732,D14-1162,0,0.084944,"Missing"
2020.emnlp-main.732,S18-2023,0,0.0538435,"Missing"
2020.emnlp-main.732,W11-1901,0,0.100591,"Missing"
2020.emnlp-main.732,morante-daelemans-2012-conandoyle,0,0.0730694,"Missing"
2020.emnlp-main.732,P19-1561,0,0.021367,"Missing"
2020.emnlp-main.732,J12-2001,0,0.0619106,"Missing"
2020.emnlp-main.732,C18-1198,0,0.0672068,"Missing"
2020.emnlp-main.732,P18-1079,0,0.0614338,"Missing"
2020.emnlp-main.732,2020.acl-main.442,0,0.0272863,"Missing"
2020.emnlp-main.732,N19-1302,0,0.041835,"Missing"
2020.emnlp-main.732,D19-1221,0,0.0342763,"Missing"
2020.emnlp-main.732,P19-1248,0,0.037457,"Missing"
2020.emnlp-main.732,W18-5446,0,0.119226,"s (Bos et al., 2004; Artzi and Zettlemoyer, 2013) to distributional representations often learned from massive datasets (Devlin et al., 2019). Recognizing entailments (Dagan et al., 2006), identifying paraphrases (Das and Smith, 2009), determining semantic textual similarity (Agirre et al., 2012), and sentiment analysis (Pang and Lee, 2008) are but a few problems that require natural language understanding to a lesser or greater degree. There are many benchmarks targeting the problems above, and they usually cast them as classification problems. A couple of popular evaluation platforms, GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019), aggregate benchmarks for some of the problems above and provide a single score for many tasks under the umbrella of natural language inference. State-of-the-art models are close to or even surpass human performance (Wang et al., 2019). This fact, however, is true only when eduardo.blanco@unt.edu evaluating models and humans with existing benchmarks. Indeed, researchers have pointed out weaknesses in benchmarks suggesting that we are evaluating models with examples that are much simpler than what humans are capable of (Section 3). Source text selection, annot"
2020.emnlp-main.732,N18-1101,0,0.0584134,"agan et al., 2006), and a newer formulation considers three labels: entailment, contradiction or neutral (Giampiccolo et al., 2007). For example, the text “A person on a horse jumps over an airplane” entails hypothesis “A person is outdoors, on a horse,” contradicts “A person is at a diner, ordering an omelette,” and is neutral with respect to “A person is training his horse for a competition.” We work with three existing benchmarks: a collection of RTE datasets (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009), SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018). The RTE datasets are smaller (5,767 text-hypothesis pairs) than SNLI and MNLI (569,033 and 431,997 pairs). MNLI is more challenging than RTE and SNLI: texts are longer and were selected from 10 genres including fiction and non-fiction as well as conversation transcripts. On the other hand, the texts in SNLI were selected from image captions. The hypotheses in SNLI and MNLI were crowdsourced, i.e., manually generated by non-experts. Tables 2 and 4 show examples in the RTE, SNLI and MNLI benchmarks. We work with the formatted versions of these datasets in the GLUE (Wang et al., 2018) and Super"
2020.emnlp-main.732,N19-1131,0,0.0389695,"Missing"
2020.findings-emnlp.214,P13-1025,0,0.0340216,"0.78 and 0.86). For the indicators of business success, either the SVM (F-measures = 0.59, 0.38 and 0.67 ) or the MTL (F-measures = 0.63, 0.51 and 0.65) model outperforms the majority baseline. 5 Related Work There have been a few studies analyzing language usage when people communicate. For example, researchers have studied power (or hierarchical) relationships in online communities (DanescuNiculescu-Mizil et al., 2012), emails (Prabhakaran and Rambow, 2014), and social networks (Bramsen et al., 2011). Some have studied how roles of Wikipedia editors affect their success (Maki et al., 2017). Danescu-Niculescu-Mizil et al. (2013) an2369 alyze politeness in online forums using structural and linguistic features derived from the communications between two individuals. Katerenchuk and Rosenberg (2016) develop an algorithm to predict user influence levels in online communities. Rashid and Blanco (2018) characterize interactions between people with dimensions and produce a dataset annotating dimensions on TV scripts. VegaRedondo et al. (2019) annotate business relevance and sentiment on online chat interactions among aspiring entrepreneurs. In contrast, we annotate the communicative styles cooperativeness, motivational, ad"
2020.findings-emnlp.214,D16-1057,0,0.0229225,"per indicator of business success to predict the different labels. Feature Set. After basic preprocessing (removal of stop words), tokenization, and parsing (to get the root verb) using spaCy, we extract features from the chat interactions and sentiment lexica. The feature set relies only on language usage. We extract the first word in a chat interaction, the bag-of-words representations (binary flags and tf-idf scores) of the chat interaction and features from sentiment lexica. Specifically, we extract flags indicating whether the turn has a positive, negative or neutral word in the list by Hamilton et al. (2016), the sentiment score of the chat interaction (summation of sentiment scores per token over number of tokens), and a flag indicating whether the interaction contains a negative word from the list by Hu and Liu (2004). We also extract other features, which include (a) the root verb (b) binary flags indicating the presence of exclamation, question marks and negation cues from Morante and Daelemans (2012). 4.2 Model Cooperative Motivation Equal Advice HAS BUSINESS BUSINESS EVER BUSINESS PROPOSAL P R F majority SVM majority SVM majority SVM majority SVM 0.25 0.77 0.66 0.90 0.60 0.78 0.74 0.86 0.50"
2020.findings-emnlp.214,N13-1132,1,0.841047,"Missing"
2020.findings-emnlp.214,I17-1103,0,0.048899,"Missing"
2020.findings-emnlp.214,N13-1090,0,0.0142672,"L majority SVM MTL 0.51 0.58 0.61 0.20 0.54 0.52 0.57 0.66 0.65 0.71 0.68 0.66 0.44 0.47 0.51 0.75 0.69 0.75 0.59 0.59 0.63 0.27 0.38 0.51 0.64 0.67 0.65 Table 4: Results for predicting styles of interactions and three indicators of business success. The Fmeasures are the weighted averages of the F-measures of the two labels. Multitask Learning (MTL) setup We use a standard Convolutional Neural Network over word-embeddings, with one output per task. We preprocess the data (convert to lowercase, removed URLs and stop-words, converted numbers to 0’s etc.) and learn a skip-gram embeddings model (Mikolov et al., 2013) trained for 50 epochs. We use an embedding size of 512, choosing a power of 2 for memory efficiency. In the CNN, the input layer has the word indices of the text, converted via the embedding matrix into word embeddings. We convolve two parallel channels with max-pooling layers, and convolutional window sizes 4 and 8 over the input. The two window sizes account for both short and relatively long patterns in the texts. In both channels, the initial number of filters is 128 for the first convolution, and 256 in the second one. We join the convolutional channels’ output and pass it through an att"
2020.findings-emnlp.214,morante-daelemans-2012-conandoyle,0,0.0181801,"Missing"
2020.findings-emnlp.214,P14-2056,0,0.0231418,"ns as well as the business success indicators. Our SVM model does much better than the majority baseline for all the styles of interactions (F-measures = 0.77, 0.89, 0.78 and 0.86). For the indicators of business success, either the SVM (F-measures = 0.59, 0.38 and 0.67 ) or the MTL (F-measures = 0.63, 0.51 and 0.65) model outperforms the majority baseline. 5 Related Work There have been a few studies analyzing language usage when people communicate. For example, researchers have studied power (or hierarchical) relationships in online communities (DanescuNiculescu-Mizil et al., 2012), emails (Prabhakaran and Rambow, 2014), and social networks (Bramsen et al., 2011). Some have studied how roles of Wikipedia editors affect their success (Maki et al., 2017). Danescu-Niculescu-Mizil et al. (2013) an2369 alyze politeness in online forums using structural and linguistic features derived from the communications between two individuals. Katerenchuk and Rosenberg (2016) develop an algorithm to predict user influence levels in online communities. Rashid and Blanco (2018) characterize interactions between people with dimensions and produce a dataset annotating dimensions on TV scripts. VegaRedondo et al. (2019) annotate"
2020.findings-emnlp.214,D18-1470,1,0.92104,"s they produce an active exchange of ideas. People are usually assumed to be altruistic in networks like online social forums. They cooperate with and help one another with answers, advice, and ideas. The motivations behind helping a peer include, but are not limited to, getting pure pleasure from helping, self-advancement, building a reputation, developing relationships, or sheer entertainment (Tausczik and Pennebaker, 2012). When people interact with each other, their interactions vary along various communicative styles, such as showing cooperativeness, equality, business orientation, etc. (Rashid and Blanco, 2018). Varying these communication styles provides tools to achieve communicative goals. For example, someone trying to build a reputation will tend to use a more cooperative style. Someone who tries to be helpful may use more words of advice in their interactions. The usage of relationshipestablishing styles is more prevalent in certain personalities (Cheng, 2011) and in specific settings. Business-oriented people communicate more independence, tolerance of ambiguity, risk-taking propensity, innovativeness, and leadership qualities (Wagener et al., 2010). The impact of these styles is, therefore,"
2020.findings-emnlp.214,N16-1174,0,0.0200264,"via the embedding matrix into word embeddings. We convolve two parallel channels with max-pooling layers, and convolutional window sizes 4 and 8 over the input. The two window sizes account for both short and relatively long patterns in the texts. In both channels, the initial number of filters is 128 for the first convolution, and 256 in the second one. We join the convolutional channels’ output and pass it through an attention mechanism (Bahdanau et al., 2014; Vaswani et al., 2017) to emphasize the weight of any meaningful pattern recognized by the convolutions. We use the implementation of Yang et al. (2016). The output consists of 7 independent, fullyconnected layers for the predictions, respectively in the form of discrete labels for classification of one of the business success indicators of a person (HAS BUSINESS , BUSINESS EVER or BUSINESS PRO as the target task, and the styles of interactions (business, sentiment, cooperativeness, motivational, advice, equality) as the auxiliary tasks. We trained one model per business success indicator. POSAL ) 4.3 Results Table 4 compares the results of the different systems to predict the styles of interactions as well as the business success indicators."
2020.findings-emnlp.345,L16-1100,0,0.028889,"point of claiming human parity in some high-resource language pairs and in-domain settings (Hassan et al., 2018),1 fine-grained semantic differences become 1 We direct the reader to (Läubli et al., 2018) and (Toral antonis@gmu.edu increasingly important. Negation in particular, with its property of logical reversal, has the potential to cause loss of (or mis-)information if mistranslated. Other linguistic phenomena and analysis axes have gathered significant attention in NMT evaluation studies, including anaphora resolution (Hardmeier et al., 2014; Voita et al., 2018) and pronoun translation (Guillou and Hardmeier, 2016), modality (Baker et al., 2012), ellipsis and deixis (Voita et al., 2019), word sense disambiguation (Tang et al., 2018), and morphological competence (Burlot and Yvon, 2017). Nevertheless, the last comprehensive study of the effect of negation in MT pertains to older, phrase-based models (Fancellu and Webber, 2015). In this work, we set out to study the effect of negation in modern NMT systems. Specifically, we explore: 1. Whether negation affects the quality of the produced translations (it does); 2. Whether the typically-used evaluation datasets include a significant amount of negated examp"
2020.findings-emnlp.345,E17-2002,0,0.0128272,"ine-Y afrl-sys online-A afrl-ewc TartuNLP-u online-X.0 0.161 0.143 0.134 0.133 0.126 0.125 0.108 0.091 0.024 0.022 -0.030 -0.039 -0.096 Q4: Is Translating Negation between Similar Languages Easier? Intuition may lead us to believe that it is easier to translate negation between similar languages. We show the correlation between language similarity and relative differences in Z-scores with and without negation in Figure 2. To calculate similarity between two languages, we follow Zhang and Toral (2019) and Berzak et al. (2017). Briefly, we obtain feature vectors for each language from lang2vec (Littell et al., 2017), and define the similarity between two languages as the cosine similarity between their feature vectors. More specifically, we concatenate 103 morphosyntactic features and 87 language family features (only those relevant to the languages we work with) from the URIEL typological database. We conclude that similarity between languages is only a weak indicator of how difficult it is to translate negation. We revisit this question in Section 5 with an in-depth linguistic discussion. (Z(w/ neg.) - Z(w/o neg.)) / Z(all sentences) Table 3: Rankings of all submissions translating from Russian to Engl"
2020.findings-emnlp.345,W19-6623,0,0.0130524,"ed evaluation datasets include a significant amount of negated examples (they don’t); 3. Whether different systems quantifiably handle negation differently across different settings (they do); and 4. Whether there is a linguistics-grounded explanation of our findings (there is). Our conclusion is that indeed negation still poses an issue for NMT systems in several language pairs, an issue which should be tackled in future NMT systems development. Negation should be taken into consideration especially when deploying realworld systems which might produce incredibly fluent but inadequate output (Martindale et al., 2019). 2 Negation 101 Negation at its most straightforward—simple negation of declarative sentences—involves reversing et al., 2018) for further examination of such claims. 3869 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3869–3885 c November 16 - 20, 2020. 2020 Association for Computational Linguistics the truth value of a sentence. Skies are blue, for example, becomes Skies are not blue. Clausal negation of an existing utterance, defined roughly as negation of the entire clause rather than a subpart, produces a second utterance whose meaning contradicts that of th"
2020.findings-emnlp.345,W17-4770,0,0.0479203,"Missing"
2020.findings-emnlp.345,W18-6319,0,0.0173336,"igating the role of negation WMT competitions, and are preferred to au- in machine translation by only looking at English tomated metrics to assess the quality of MT. negations likely misses valid insights. For examNevertheless, most of the MT community still ple, a Spanish sentence containing negation (e.g., relies on automated metrics for development “El ladrón no estaba preocupado hasta que vino la and system comparisons. Thus, we also work policía”) can be translated into English either with with three automated metrics, in particular negation (“The thief was not worried until the poBLEU (Post, 2018), chrF++ (Popovi´c, 2017), and lice arrived”), or without negation (“The thief only METEOR (Denkowski and Lavie, 2011). worried when the police arrived”). We reserve for future work a more thorough analysis of corresponIn the remainder of the paper we present two dences between negation in source sentences and complementary analyses. First, we investigate the role of negation in machine translation with an em- negation in English reference translations. phasis on numeric evaluation (Section 4). Second, 4 Quantitative Analysis we investigate from a linguistic perspective what makes translating"
2020.findings-emnlp.359,W19-3412,0,0.0411462,"gregation are important problems (Imran et al., 2015). In this paper, we work with mundane events (cooking and baking) described in one tweet, and study the degree to which they resulted in their desired outcomes. Event detection from social media has received considerable attention, in particular, pinpointing important life events (Li et al., 2014; Dickinson et al., 2015). Previous research shows that people often tweet about events they do not participate in (Sanagavarapu et al., 2017), targets recurring events (Kunneman and Van den Bosch, 2015), and summarizes tweet streams about TV shows (Andy et al., 2019). The work presented here is not concerned with event detection, our selection criteria virtually guarantees that we only work with tweets about cooking or baking (Section 3). Determining the degree to which an event results in its desired outcome is distantly related to assessing factuality and other event properties. Previous efforts working with social media target event factuality (Soni et al., 2014), identify controversial events (Popescu and Pennacchiotti, 2010) and credible eyewitnesses (Doggett and Cantarero, 2016), and work with arguably more challenging properties such as rumors (Zub"
2020.findings-emnlp.359,J08-4004,0,0.139677,"Missing"
2020.findings-emnlp.359,P19-1239,0,0.0348303,"Missing"
2020.findings-emnlp.359,D19-1061,1,0.885682,"Missing"
2020.findings-emnlp.359,R15-1043,0,0.0685813,"Missing"
2020.findings-emnlp.359,D15-1189,0,0.0255536,"events occur (or happen, or take place) at a time and location. People share in social media a deluge of information including events they care about. These events range from mundane events such as eating or watching TV to important life events such as getting married and graduating from college (Li et al., 2014). Twitter is one of the most popular social networks with 166 million daily active users (Twitter, 2020). An important property of events is whether they actually occurred. The literature has studied this property under different terms, e.g., factuality (Saur´ı and Pustejovsky, 2009; Lee et al., 2015) and veridicality (de Marneffe et al., 2012). Other related tasks have studied the level of commitment a speaker or writer has towards a proposition (Werner et al., 2015; Jiang and de Marneffe, 2019). Assessing the degree to which an event occurred or is believed to be true is critical to make inferences and information extraction. Even when an event is guaranteed to have occurred, however, it is not necessarily the case that the desired outcome came to fruition. For example, people make phone ⇤ Currently at Thomson Reuters.Work done while at University of North Texas. Figure 1: Tweet discussi"
2020.findings-emnlp.359,D14-1162,0,0.0820631,"Missing"
2020.lrec-1.140,L16-1592,0,0.0190746,"oth Nakov and Hearst (2013) and Tratz and Hovy (2010) consider possession expressed by noun compounds, such as “family estate.” Badulescu and Moldovan (2009) extract possession as one of the many semantic relations expressed by English genetives. Blodgett and Schneider (2018) present a corpus of web reviews annotating genitives with adpositional supersenses, finding that this inventory works well for canonical possessives. We consider all expressions of possession, whether phrasal, clausal, sentential, or even inter-sentential. The non-restrictive approach presented here is similar to that of Banea et al. (2016), who annotate possessions of particular bloggers at the time of utterance. In our previous work, we (Chinnappa and Blanco, 2018a) first extract possessions from a sentence using a deterministic procedure and then identify the types and temporal anchors of possession. In a latter work, we (Chinnappa and Blanco, 2018b) work with the same Wikipedia articles about artworks as the current work, and extract their possessors. We match these possessors to their temporal information with respect to the years explicitly mentioned (before, during or after). Unlike the current work, we limit the temporal"
2020.lrec-1.140,L18-1242,0,0.0168755,"capable of changing hands. We are interested in tracking change of possession and thus restrict our work to alienable possessions. Previous work on automatic extraction of possession has mostly focused on particular syntactic constructions. Tratz and Hovy (2013) investigate various semantic relations realized by English possessive constructions, and both Nakov and Hearst (2013) and Tratz and Hovy (2010) consider possession expressed by noun compounds, such as “family estate.” Badulescu and Moldovan (2009) extract possession as one of the many semantic relations expressed by English genetives. Blodgett and Schneider (2018) present a corpus of web reviews annotating genitives with adpositional supersenses, finding that this inventory works well for canonical possessives. We consider all expressions of possession, whether phrasal, clausal, sentential, or even inter-sentential. The non-restrictive approach presented here is similar to that of Banea et al. (2016), who annotate possessions of particular bloggers at the time of utterance. In our previous work, we (Chinnappa and Blanco, 2018a) first extract possessions from a sentence using a deterministic procedure and then identify the types and temporal anchors of"
2020.lrec-1.140,D15-1075,0,0.0420045,"cores to each possessor and each temporal relation; and e) assemble individual possession events into a global possession timeline. In addition to the corpus, we release evaluation scripts and a baseline model for the task. Keywords: possession, timeline, wikipossesions 1. Temporally-oriented possession Research in machine reading, text comprehension, and natural language understanding continues to proceed at a rapid pace. Each time a new approach claims to surpass human performance on benchmark evaluation data sets, a new data set comes along, offering a new twist on the previous challenges (Bowman et al., 2015; Rajpurkar et al., 2016; Dua et al., 2019, for example). At the same time, the community has questioned whether these data sets in fact require comprehension of reading texts by the machine (Kaushik and Lipton, 2018). With this paper, we offer a new benchmark evaluation data set which combines several related tasks requiring different degrees of inferential complexity. Rather than focusing on question-answer pairs linked to particular text passages, the unit of analysis for this data set is a complete Wikipedia article. As a comparison, the average text length in our evaluation data set is an"
2020.lrec-1.140,N18-1046,1,0.835807,"tate.” Badulescu and Moldovan (2009) extract possession as one of the many semantic relations expressed by English genetives. Blodgett and Schneider (2018) present a corpus of web reviews annotating genitives with adpositional supersenses, finding that this inventory works well for canonical possessives. We consider all expressions of possession, whether phrasal, clausal, sentential, or even inter-sentential. The non-restrictive approach presented here is similar to that of Banea et al. (2016), who annotate possessions of particular bloggers at the time of utterance. In our previous work, we (Chinnappa and Blanco, 2018a) first extract possessions from a sentence using a deterministic procedure and then identify the types and temporal anchors of possession. In a latter work, we (Chinnappa and Blanco, 2018b) work with the same Wikipedia articles about artworks as the current work, and extract their possessors. We match these possessors to their temporal information with respect to the years explicitly mentioned (before, during or after). Unlike the current work, we limit the temporal information to be a year within a Wikipedia section. We do not capture any other possession attributes such as certainty or ord"
2020.lrec-1.140,D18-1251,1,0.831571,"tate.” Badulescu and Moldovan (2009) extract possession as one of the many semantic relations expressed by English genetives. Blodgett and Schneider (2018) present a corpus of web reviews annotating genitives with adpositional supersenses, finding that this inventory works well for canonical possessives. We consider all expressions of possession, whether phrasal, clausal, sentential, or even inter-sentential. The non-restrictive approach presented here is similar to that of Banea et al. (2016), who annotate possessions of particular bloggers at the time of utterance. In our previous work, we (Chinnappa and Blanco, 2018a) first extract possessions from a sentence using a deterministic procedure and then identify the types and temporal anchors of possession. In a latter work, we (Chinnappa and Blanco, 2018b) work with the same Wikipedia articles about artworks as the current work, and extract their possessors. We match these possessors to their temporal information with respect to the years explicitly mentioned (before, during or after). Unlike the current work, we limit the temporal information to be a year within a Wikipedia section. We do not capture any other possession attributes such as certainty or ord"
2020.lrec-1.140,N19-1246,0,0.0216997,"ation; and e) assemble individual possession events into a global possession timeline. In addition to the corpus, we release evaluation scripts and a baseline model for the task. Keywords: possession, timeline, wikipossesions 1. Temporally-oriented possession Research in machine reading, text comprehension, and natural language understanding continues to proceed at a rapid pace. Each time a new approach claims to surpass human performance on benchmark evaluation data sets, a new data set comes along, offering a new twist on the previous challenges (Bowman et al., 2015; Rajpurkar et al., 2016; Dua et al., 2019, for example). At the same time, the community has questioned whether these data sets in fact require comprehension of reading texts by the machine (Kaushik and Lipton, 2018). With this paper, we offer a new benchmark evaluation data set which combines several related tasks requiring different degrees of inferential complexity. Rather than focusing on question-answer pairs linked to particular text passages, the unit of analysis for this data set is a complete Wikipedia article. As a comparison, the average text length in our evaluation data set is an order of magnitude longer than the averag"
2020.lrec-1.140,D18-1546,0,0.0230909,"or the task. Keywords: possession, timeline, wikipossesions 1. Temporally-oriented possession Research in machine reading, text comprehension, and natural language understanding continues to proceed at a rapid pace. Each time a new approach claims to surpass human performance on benchmark evaluation data sets, a new data set comes along, offering a new twist on the previous challenges (Bowman et al., 2015; Rajpurkar et al., 2016; Dua et al., 2019, for example). At the same time, the community has questioned whether these data sets in fact require comprehension of reading texts by the machine (Kaushik and Lipton, 2018). With this paper, we offer a new benchmark evaluation data set which combines several related tasks requiring different degrees of inferential complexity. Rather than focusing on question-answer pairs linked to particular text passages, the unit of analysis for this data set is a complete Wikipedia article. As a comparison, the average text length in our evaluation data set is an order of magnitude longer than the average passage in the DROP corpus (Dua et al., 2019), which was developed specifically to demand reasoning across longer text spans than previous reading comprehension benchmarks."
2020.lrec-1.140,D16-1264,0,0.0623639,"Missing"
2020.lrec-1.140,P10-1070,0,0.0298767,"possession, in which such separation is not possible (Aikhenvald and Dixon, 2012; Heine, 1997, among others). Unlike inalienable possessions, which are permanent, alienable possessions are temporary and, therefore, capable of changing hands. We are interested in tracking change of possession and thus restrict our work to alienable possessions. Previous work on automatic extraction of possession has mostly focused on particular syntactic constructions. Tratz and Hovy (2013) investigate various semantic relations realized by English possessive constructions, and both Nakov and Hearst (2013) and Tratz and Hovy (2010) consider possession expressed by noun compounds, such as “family estate.” Badulescu and Moldovan (2009) extract possession as one of the many semantic relations expressed by English genetives. Blodgett and Schneider (2018) present a corpus of web reviews annotating genitives with adpositional supersenses, finding that this inventory works well for canonical possessives. We consider all expressions of possession, whether phrasal, clausal, sentential, or even inter-sentential. The non-restrictive approach presented here is similar to that of Banea et al. (2016), who annotate possessions of part"
2020.lrec-1.140,P13-1037,0,0.0252925,"rature makes a conceptual distinction between alienable possession, in which possessees can be separated from their possessors, and inalienable possession, in which such separation is not possible (Aikhenvald and Dixon, 2012; Heine, 1997, among others). Unlike inalienable possessions, which are permanent, alienable possessions are temporary and, therefore, capable of changing hands. We are interested in tracking change of possession and thus restrict our work to alienable possessions. Previous work on automatic extraction of possession has mostly focused on particular syntactic constructions. Tratz and Hovy (2013) investigate various semantic relations realized by English possessive constructions, and both Nakov and Hearst (2013) and Tratz and Hovy (2010) consider possession expressed by noun compounds, such as “family estate.” Badulescu and Moldovan (2009) extract possession as one of the many semantic relations expressed by English genetives. Blodgett and Schneider (2018) present a corpus of web reviews annotating genitives with adpositional supersenses, finding that this inventory works well for canonical possessives. We consider all expressions of possession, whether phrasal, clausal, sentential, o"
2020.lrec-1.140,S13-2001,0,0.037591,"2: Corpus statistics for WikiPossessions. Table 1: Statistics for all marked possessors (2) It was loaned to the Ashmolean Museum in early 1900s, its whereabouts after this are unknown; it was rediscovered in a Battersea home in the early 1960s, boxed in over a chimney. [Flaming June] 4. Stage Two: Temporal Anchoring, Temporal Relations, and Certainty. The second step takes as input a text passage and an extracted (artifact, possessor) relation; output is a temporal anchor for the possession relation. This task corresponds to the TLINK task in the TempEval shared tasks (Verhagen et al., 2010; UzZaman et al., 2013), limiting the range to only possession-type events. In our case, we allow three types of anchors: a year, a range of years, or a major historical event (as in ex. 3). (3) After the victory of Francisco Franco in Spain, the painting was sent to the United States to raise funds and support for Spanish refugees. [Guernica] Once the temporal anchor has been identified, the next task is temporal relation identification (Verhagen et al., 2010; UzZaman et al., 2013). Specifically, the system should indicate whether the possession event held BEFORE the temporal anchor, DURING the temporal anchor, or"
2020.lrec-1.853,J12-2006,0,0.0109648,"iwatts Marketing Group, 31 December 2017, accessed 20 February 2019. URL: https: //www.internetworldstats.com/stats7.htm 2 In the examples we mark in bold negation cues and enclose negation scopes between square brackets. 2. Related Work Negation is a well-studied phenomenon from a theoretical perspective (Horn, 2010; Horn, 1989). However, its computational treatment has not been extensively studied for 6902 languages other than English. Its automatic detection and treatment is relevant in a wide range of applications, such as information extraction (Savova et al., 2010), machine translation (Baker et al., 2012) or sentiment analysis (Liu, 2015), where it is crucial to detect when a fragment of text expresses a different meaning due to the presence of negation. The first attempts to process negation in English were mostly rule-based and focused on finding negated terms in clinical texts (Chapman et al., 2001; Mutalik et al., 2001; Goldin and Chapman, 2003; Auerbuch et al., 2004; Elkin et al., 2005; Boytcheva et al., 2005; Goryachev et al., ; Sanchez-Graillet and Poesio, 2007; Huang and Lowe, 2007; Rokach et al., 2008). The task of detecting negation scopes was introduced in 2008 as a machine learning"
2020.lrec-1.853,W16-2921,0,0.101754,"t al., 2005; Goryachev et al., ; Sanchez-Graillet and Poesio, 2007; Huang and Lowe, 2007; Rokach et al., 2008). The task of detecting negation scopes was introduced in 2008 as a machine learning sequence labelling task (Morante et al., 2008). Subsequently, three main types of approaches have been applied to processing negation: (i) rule-based systems, in an attempt to improve the NegEx algorithm, such as ConText (Harkema et al., 2009), DEEPEN (Mehrabi et al., 2015), and NegMiner (Elazhary, 2017); (ii) machine learning techniques (Agarwal and Yu, 2010; Li et al., 2010; Cruz D´ıaz et al., 2012; Cotik et al., 2016); and (iii) deep learning approaches (Qian et al., 2016; Ren et al., 2018; Lazib et al., 2018). Several shared tasks have addressed negation processing: the BioNLP’09 Shared Task 3 (Kim et al., 2009), the CoNLL2010 shared task (Farkas et al., 2010), the i2b2 NLP Challenge (Uzuner et al., 2011), the *SEM 2012 Shared Task (Morante and Blanco, 2012) and the ShARe/CLEF eHealth Evaluation Lab 2014 Task 2 (Mowery et al., 2014). Previous work has incorporated negation processing in sentiment analysis systems. Some systems use rules, but do not evaluate the processing of negation (Das and Chen, 2001;"
2020.lrec-1.853,W10-3110,0,0.0459921,"the *SEM 2012 Shared Task (Morante and Blanco, 2012) and the ShARe/CLEF eHealth Evaluation Lab 2014 Task 2 (Mowery et al., 2014). Previous work has incorporated negation processing in sentiment analysis systems. Some systems use rules, but do not evaluate the processing of negation (Das and Chen, 2001; Polanyi and Zaenen, 2006; Kennedy and Inkpen, 2006; Jia et al., 2009). Other systems employ a lexicon of negation cues and predict the scope with CRFs using as features lowercased token strings, token PoS tags, token-wise distances from explicit negation cues and dependency syntax information (Councill et al., 2010), or a rich set of lexical and syntactic features, together with cue-dependant information (Lapponi et al., 2012a). Cruz et al. (2016) use SVM and lexical, syntactic and dependency features. They test the system in the SFU Review corpus (Konstantinova et al., 2012). Work on processing negation in Spanish is relatively recent. Costumero et al. (2014), Stricker et al. (2015) and Cotik et al. (2016) develop systems for the identification of negation in clinical texts by adapting the NegEx algorithm (Chapman et al., 2001). Regarding product reviews, there are some works that treat negation as a su"
2020.lrec-1.853,W10-3001,0,0.113629,"Missing"
2020.lrec-1.853,W09-1401,0,0.0283826,"ce labelling task (Morante et al., 2008). Subsequently, three main types of approaches have been applied to processing negation: (i) rule-based systems, in an attempt to improve the NegEx algorithm, such as ConText (Harkema et al., 2009), DEEPEN (Mehrabi et al., 2015), and NegMiner (Elazhary, 2017); (ii) machine learning techniques (Agarwal and Yu, 2010; Li et al., 2010; Cruz D´ıaz et al., 2012; Cotik et al., 2016); and (iii) deep learning approaches (Qian et al., 2016; Ren et al., 2018; Lazib et al., 2018). Several shared tasks have addressed negation processing: the BioNLP’09 Shared Task 3 (Kim et al., 2009), the CoNLL2010 shared task (Farkas et al., 2010), the i2b2 NLP Challenge (Uzuner et al., 2011), the *SEM 2012 Shared Task (Morante and Blanco, 2012) and the ShARe/CLEF eHealth Evaluation Lab 2014 Task 2 (Mowery et al., 2014). Previous work has incorporated negation processing in sentiment analysis systems. Some systems use rules, but do not evaluate the processing of negation (Das and Chen, 2001; Polanyi and Zaenen, 2006; Kennedy and Inkpen, 2006; Jia et al., 2009). Other systems employ a lexicon of negation cues and predict the scope with CRFs using as features lowercased token strings, toke"
2020.lrec-1.853,konstantinova-etal-2012-review,0,0.0425358,"Missing"
2020.lrec-1.853,S12-1042,0,0.167483,"y et al., 2014). Previous work has incorporated negation processing in sentiment analysis systems. Some systems use rules, but do not evaluate the processing of negation (Das and Chen, 2001; Polanyi and Zaenen, 2006; Kennedy and Inkpen, 2006; Jia et al., 2009). Other systems employ a lexicon of negation cues and predict the scope with CRFs using as features lowercased token strings, token PoS tags, token-wise distances from explicit negation cues and dependency syntax information (Councill et al., 2010), or a rich set of lexical and syntactic features, together with cue-dependant information (Lapponi et al., 2012a). Cruz et al. (2016) use SVM and lexical, syntactic and dependency features. They test the system in the SFU Review corpus (Konstantinova et al., 2012). Work on processing negation in Spanish is relatively recent. Costumero et al. (2014), Stricker et al. (2015) and Cotik et al. (2016) develop systems for the identification of negation in clinical texts by adapting the NegEx algorithm (Chapman et al., 2001). Regarding product reviews, there are some works that treat negation as a subtask of sentiment analysis (Taboada et al., 2011; Vilares et al., 2013; Vilares et al., 2015; Jim´enez-Zafra et"
2020.lrec-1.853,C10-1076,0,0.0355942,"al., 2004; Elkin et al., 2005; Boytcheva et al., 2005; Goryachev et al., ; Sanchez-Graillet and Poesio, 2007; Huang and Lowe, 2007; Rokach et al., 2008). The task of detecting negation scopes was introduced in 2008 as a machine learning sequence labelling task (Morante et al., 2008). Subsequently, three main types of approaches have been applied to processing negation: (i) rule-based systems, in an attempt to improve the NegEx algorithm, such as ConText (Harkema et al., 2009), DEEPEN (Mehrabi et al., 2015), and NegMiner (Elazhary, 2017); (ii) machine learning techniques (Agarwal and Yu, 2010; Li et al., 2010; Cruz D´ıaz et al., 2012; Cotik et al., 2016); and (iii) deep learning approaches (Qian et al., 2016; Ren et al., 2018; Lazib et al., 2018). Several shared tasks have addressed negation processing: the BioNLP’09 Shared Task 3 (Kim et al., 2009), the CoNLL2010 shared task (Farkas et al., 2010), the i2b2 NLP Challenge (Uzuner et al., 2011), the *SEM 2012 Shared Task (Morante and Blanco, 2012) and the ShARe/CLEF eHealth Evaluation Lab 2014 Task 2 (Mowery et al., 2014). Previous work has incorporated negation processing in sentiment analysis systems. Some systems use rules, but do not evaluate th"
2020.lrec-1.853,S12-1035,1,0.907207,"ed systems, in an attempt to improve the NegEx algorithm, such as ConText (Harkema et al., 2009), DEEPEN (Mehrabi et al., 2015), and NegMiner (Elazhary, 2017); (ii) machine learning techniques (Agarwal and Yu, 2010; Li et al., 2010; Cruz D´ıaz et al., 2012; Cotik et al., 2016); and (iii) deep learning approaches (Qian et al., 2016; Ren et al., 2018; Lazib et al., 2018). Several shared tasks have addressed negation processing: the BioNLP’09 Shared Task 3 (Kim et al., 2009), the CoNLL2010 shared task (Farkas et al., 2010), the i2b2 NLP Challenge (Uzuner et al., 2011), the *SEM 2012 Shared Task (Morante and Blanco, 2012) and the ShARe/CLEF eHealth Evaluation Lab 2014 Task 2 (Mowery et al., 2014). Previous work has incorporated negation processing in sentiment analysis systems. Some systems use rules, but do not evaluate the processing of negation (Das and Chen, 2001; Polanyi and Zaenen, 2006; Kennedy and Inkpen, 2006; Jia et al., 2009). Other systems employ a lexicon of negation cues and predict the scope with CRFs using as features lowercased token strings, token PoS tags, token-wise distances from explicit negation cues and dependency syntax information (Councill et al., 2010), or a rich set of lexical and"
2020.lrec-1.853,D08-1075,1,0.429593,"u, 2015), where it is crucial to detect when a fragment of text expresses a different meaning due to the presence of negation. The first attempts to process negation in English were mostly rule-based and focused on finding negated terms in clinical texts (Chapman et al., 2001; Mutalik et al., 2001; Goldin and Chapman, 2003; Auerbuch et al., 2004; Elkin et al., 2005; Boytcheva et al., 2005; Goryachev et al., ; Sanchez-Graillet and Poesio, 2007; Huang and Lowe, 2007; Rokach et al., 2008). The task of detecting negation scopes was introduced in 2008 as a machine learning sequence labelling task (Morante et al., 2008). Subsequently, three main types of approaches have been applied to processing negation: (i) rule-based systems, in an attempt to improve the NegEx algorithm, such as ConText (Harkema et al., 2009), DEEPEN (Mehrabi et al., 2015), and NegMiner (Elazhary, 2017); (ii) machine learning techniques (Agarwal and Yu, 2010; Li et al., 2010; Cruz D´ıaz et al., 2012; Cotik et al., 2016); and (iii) deep learning approaches (Qian et al., 2016; Ren et al., 2018; Lazib et al., 2018). Several shared tasks have addressed negation processing: the BioNLP’09 Shared Task 3 (Kim et al., 2009), the CoNLL2010 shared"
2020.lrec-1.853,padro-stanilovsky-2012-freeling,0,0.0931654,"Missing"
2020.lrec-1.853,D16-1078,0,0.0191586,"sio, 2007; Huang and Lowe, 2007; Rokach et al., 2008). The task of detecting negation scopes was introduced in 2008 as a machine learning sequence labelling task (Morante et al., 2008). Subsequently, three main types of approaches have been applied to processing negation: (i) rule-based systems, in an attempt to improve the NegEx algorithm, such as ConText (Harkema et al., 2009), DEEPEN (Mehrabi et al., 2015), and NegMiner (Elazhary, 2017); (ii) machine learning techniques (Agarwal and Yu, 2010; Li et al., 2010; Cruz D´ıaz et al., 2012; Cotik et al., 2016); and (iii) deep learning approaches (Qian et al., 2016; Ren et al., 2018; Lazib et al., 2018). Several shared tasks have addressed negation processing: the BioNLP’09 Shared Task 3 (Kim et al., 2009), the CoNLL2010 shared task (Farkas et al., 2010), the i2b2 NLP Challenge (Uzuner et al., 2011), the *SEM 2012 Shared Task (Morante and Blanco, 2012) and the ShARe/CLEF eHealth Evaluation Lab 2014 Task 2 (Mowery et al., 2014). Previous work has incorporated negation processing in sentiment analysis systems. Some systems use rules, but do not evaluate the processing of negation (Das and Chen, 2001; Polanyi and Zaenen, 2006; Kennedy and Inkpen, 2006; Jia"
2020.lrec-1.853,W15-2914,0,0.0414316,"Missing"
2020.lrec-1.853,J11-2001,0,0.0452492,"d syntactic features, together with cue-dependant information (Lapponi et al., 2012a). Cruz et al. (2016) use SVM and lexical, syntactic and dependency features. They test the system in the SFU Review corpus (Konstantinova et al., 2012). Work on processing negation in Spanish is relatively recent. Costumero et al. (2014), Stricker et al. (2015) and Cotik et al. (2016) develop systems for the identification of negation in clinical texts by adapting the NegEx algorithm (Chapman et al., 2001). Regarding product reviews, there are some works that treat negation as a subtask of sentiment analysis (Taboada et al., 2011; Vilares et al., 2013; Vilares et al., 2015; Jim´enez-Zafra et al., 2015; Miranda et al., 2016; Amores et al., 2016; Jim´enez-Zafra et al., 2019c). The first systems that detect negation cues were developed in the framework of the 2018 and 2019 editions of NEGES, the Workshop on Negation in Spanish (Jim´enezZafra et al., 2019a; Jim´enez-Zafra et al., 2019b) and were trained on the SFU ReviewSP -NEG corpus (Jim´enez-Zafra et al., 2018). Participants (Fabregat et al., 2018; Loharja et al., 2018; Giudice, 2019; Beltr´an and Gonz´alez, 2019; Dom´ınguez-Mas et al., 2019; Fabregat et al., 2019) add"
2020.lrec-1.853,taboada-etal-2006-methods,0,0.0889954,"Missing"
2021.findings-emnlp.355,P11-1078,0,0.0431585,"ed light into which characteristics make a justification more and less credible. 2 Previous Work outcomes (Schneuwly et al., 2019), and personality traits (Schwartz et al., 2013). In this paper, we do not profile forecasters. Instead, we build models to call questions based on forecasts by the crowd without knowledge of who submitted what. Previous research has also studied the language people use to communicate depending on the relationship between the parties. For example, the language people use when they are in positions of power (e.g., more seniority) has been studied in social networks (Bramsen et al., 2011), online communities (Danescu-Niculescu-Mizil et al., 2012), and corporate emails (Prabhakaran and Rambow, 2014). Similarly, Rashid and Blanco (2018) study how language provides clues about the interactions and relationships between people. Regarding language form and functions, prior research has analyzed politeness (DanescuNiculescu-Mizil et al., 2013), empathy (Sharma et al., 2020), advice (Govindarajan et al., 2020), condolences (Zhou and Jurgens, 2020) usefulness (Momeni et al., 2013), and deception (Soldner et al., 2019). More related to the problem we work with, Maki et al. (2017) analy"
2021.findings-emnlp.355,P13-1025,0,0.0172863,"Missing"
2021.findings-emnlp.355,N19-1423,0,0.00666158,"above and disdepicted in Figure 4. The network has three main abling several components. Specifically, we excomponents: a component to obtain a representa- periment with representing a forecast taking into tion of the question, a component to obtain a repre- account different information: sentation of a forecast, and an LSTM (Hochreiter • the prediction; and Schmidhuber, 1997) to process the sequence • the prediction and the representation of the of forecasts and call the question. question; We obtain the representation of a question using • the prediction and the representation of the BERT (Devlin et al., 2019) followed by a fully conjustification; and nected layer with 256 neurons, ReLU activation, • the prediction, the representation of the quesand 0.5 dropout (Srivastava et al., 2014). We obtion, and the representation of the justification. tain the representation of a forecast concatenating Implementation and Training Details In order three elements: (a) a binary flag indicating whether to implement the models,2 we use the Transformthe forecast was submitted in the day the question ers library by HuggingFace (Wolf et al., 2020) and is being called or in the past, (b) the prediction (a number ran"
2021.findings-emnlp.355,2020.emnlp-main.427,0,0.0172953,"pending on the relationship between the parties. For example, the language people use when they are in positions of power (e.g., more seniority) has been studied in social networks (Bramsen et al., 2011), online communities (Danescu-Niculescu-Mizil et al., 2012), and corporate emails (Prabhakaran and Rambow, 2014). Similarly, Rashid and Blanco (2018) study how language provides clues about the interactions and relationships between people. Regarding language form and functions, prior research has analyzed politeness (DanescuNiculescu-Mizil et al., 2013), empathy (Sharma et al., 2020), advice (Govindarajan et al., 2020), condolences (Zhou and Jurgens, 2020) usefulness (Momeni et al., 2013), and deception (Soldner et al., 2019). More related to the problem we work with, Maki et al. (2017) analyze the influence of Wikipedia editors, and Katerenchuk and Rosenberg (2016) study influence levels in online communities. Persuasion has also been studied from a computational perspective (Wei et al., 2016; Yang et al., 2019), including dialogue systems (Wang et al., 2019). The work presented here complements these works. We are interested in identifying credible justifications in order to aggregate crowdsourced forecas"
2021.findings-emnlp.355,J17-1004,0,0.0261923,"ang et al., 2019), including dialogue systems (Wang et al., 2019). The work presented here complements these works. We are interested in identifying credible justifications in order to aggregate crowdsourced forecasts, and we do so without explicitly targeting any of the above characteristics. Within computational linguistics, the previous task that is perhaps the closest to our goal is argumentation: a good justification for a forecast is arguably a good supporting argument. Previous work includes identifying argument components such as claims, premises, backings, rebuttals, and refutations (Habernal and Gurevych, 2017), and mining supporting and opposing arguments for a claim (Stab et al., 2018). Notwithstanding these works, we found that crowdsourced justifications rarely fall into these argumentation frameworks despite the former are useful to aggregate forecasts. The language people use is indicative of several attributes. Previous work includes both predictive models (input: language samples, output: some attribute about the author) and models that yield useful insights (input: language samples and attributes of the authors, output: differentiating language features depending on the attributes). Among F"
2021.findings-emnlp.355,P14-1105,0,0.0387782,"Missing"
2021.findings-emnlp.355,C16-1197,0,0.0268125,"y fall into these argumentation frameworks despite the former are useful to aggregate forecasts. The language people use is indicative of several attributes. Previous work includes both predictive models (input: language samples, output: some attribute about the author) and models that yield useful insights (input: language samples and attributes of the authors, output: differentiating language features depending on the attributes). Among Finally, there are a few works on forecasting many others, previous research has studied gen- that use the same or very similar corpora than we der and age (Li et al., 2016; Nguyen et al., 2014; do. From a psychology perspective, Mellers et al. Peersman et al., 2011), political ideology (Iyyer (2015) present strategies to improve forecasting acet al., 2014; Preo¸tiuc-Pietro et al., 2017), health curacy (using top forecasters, i.e., superforecasters) 4207 Figure 2: Average number of daily and active forecasts available per question (bottom) and average number of questions the majority forecast gets correct (top) over the life of the question (x-axis). There is a tiny peak of forecasts submitted soon after a question is published and then a roughly uniform amount"
2021.findings-emnlp.355,I17-1103,0,0.0197821,"s (Bramsen et al., 2011), online communities (Danescu-Niculescu-Mizil et al., 2012), and corporate emails (Prabhakaran and Rambow, 2014). Similarly, Rashid and Blanco (2018) study how language provides clues about the interactions and relationships between people. Regarding language form and functions, prior research has analyzed politeness (DanescuNiculescu-Mizil et al., 2013), empathy (Sharma et al., 2020), advice (Govindarajan et al., 2020), condolences (Zhou and Jurgens, 2020) usefulness (Momeni et al., 2013), and deception (Soldner et al., 2019). More related to the problem we work with, Maki et al. (2017) analyze the influence of Wikipedia editors, and Katerenchuk and Rosenberg (2016) study influence levels in online communities. Persuasion has also been studied from a computational perspective (Wei et al., 2016; Yang et al., 2019), including dialogue systems (Wang et al., 2019). The work presented here complements these works. We are interested in identifying credible justifications in order to aggregate crowdsourced forecasts, and we do so without explicitly targeting any of the above characteristics. Within computational linguistics, the previous task that is perhaps the closest to our goal"
2021.findings-emnlp.355,morante-daelemans-2012-conandoyle,0,0.0622861,"Missing"
2021.findings-emnlp.355,C14-1184,0,0.049316,"Missing"
2021.findings-emnlp.355,P14-2056,0,0.0241935,"s (Schneuwly et al., 2019), and personality traits (Schwartz et al., 2013). In this paper, we do not profile forecasters. Instead, we build models to call questions based on forecasts by the crowd without knowledge of who submitted what. Previous research has also studied the language people use to communicate depending on the relationship between the parties. For example, the language people use when they are in positions of power (e.g., more seniority) has been studied in social networks (Bramsen et al., 2011), online communities (Danescu-Niculescu-Mizil et al., 2012), and corporate emails (Prabhakaran and Rambow, 2014). Similarly, Rashid and Blanco (2018) study how language provides clues about the interactions and relationships between people. Regarding language form and functions, prior research has analyzed politeness (DanescuNiculescu-Mizil et al., 2013), empathy (Sharma et al., 2020), advice (Govindarajan et al., 2020), condolences (Zhou and Jurgens, 2020) usefulness (Momeni et al., 2013), and deception (Soldner et al., 2019). More related to the problem we work with, Maki et al. (2017) analyze the influence of Wikipedia editors, and Katerenchuk and Rosenberg (2016) study influence levels in online com"
2021.findings-emnlp.355,P17-1068,0,0.0254233,"Missing"
2021.findings-emnlp.355,D18-1470,1,0.854808,"y traits (Schwartz et al., 2013). In this paper, we do not profile forecasters. Instead, we build models to call questions based on forecasts by the crowd without knowledge of who submitted what. Previous research has also studied the language people use to communicate depending on the relationship between the parties. For example, the language people use when they are in positions of power (e.g., more seniority) has been studied in social networks (Bramsen et al., 2011), online communities (Danescu-Niculescu-Mizil et al., 2012), and corporate emails (Prabhakaran and Rambow, 2014). Similarly, Rashid and Blanco (2018) study how language provides clues about the interactions and relationships between people. Regarding language form and functions, prior research has analyzed politeness (DanescuNiculescu-Mizil et al., 2013), empathy (Sharma et al., 2020), advice (Govindarajan et al., 2020), condolences (Zhou and Jurgens, 2020) usefulness (Momeni et al., 2013), and deception (Soldner et al., 2019). More related to the problem we work with, Maki et al. (2017) analyze the influence of Wikipedia editors, and Katerenchuk and Rosenberg (2016) study influence levels in online communities. Persuasion has also been st"
2021.findings-emnlp.355,W19-3210,0,0.0199688,"t worth taking into account forecasts submitted in previous days? (it is); • Does calling a question benefit from taking into account the question and the justifications submitted with the forecasts? (it does); • Is it easier to call a question towards the end of its life? (it is); and • Is it true that the worse the crowd predictions the more useful the justifications? (it is). In addition, we also present an analysis of the justifications submitted with correct and wrong forecasts to shed light into which characteristics make a justification more and less credible. 2 Previous Work outcomes (Schneuwly et al., 2019), and personality traits (Schwartz et al., 2013). In this paper, we do not profile forecasters. Instead, we build models to call questions based on forecasts by the crowd without knowledge of who submitted what. Previous research has also studied the language people use to communicate depending on the relationship between the parties. For example, the language people use when they are in positions of power (e.g., more seniority) has been studied in social networks (Bramsen et al., 2011), online communities (Danescu-Niculescu-Mizil et al., 2012), and corporate emails (Prabhakaran and Rambow, 20"
2021.findings-emnlp.355,P19-1566,0,0.0208194,"uage form and functions, prior research has analyzed politeness (DanescuNiculescu-Mizil et al., 2013), empathy (Sharma et al., 2020), advice (Govindarajan et al., 2020), condolences (Zhou and Jurgens, 2020) usefulness (Momeni et al., 2013), and deception (Soldner et al., 2019). More related to the problem we work with, Maki et al. (2017) analyze the influence of Wikipedia editors, and Katerenchuk and Rosenberg (2016) study influence levels in online communities. Persuasion has also been studied from a computational perspective (Wei et al., 2016; Yang et al., 2019), including dialogue systems (Wang et al., 2019). The work presented here complements these works. We are interested in identifying credible justifications in order to aggregate crowdsourced forecasts, and we do so without explicitly targeting any of the above characteristics. Within computational linguistics, the previous task that is perhaps the closest to our goal is argumentation: a good justification for a forecast is arguably a good supporting argument. Previous work includes identifying argument components such as claims, premises, backings, rebuttals, and refutations (Habernal and Gurevych, 2017), and mining supporting and opposing"
2021.findings-emnlp.355,P16-2032,0,0.022767,"the interactions and relationships between people. Regarding language form and functions, prior research has analyzed politeness (DanescuNiculescu-Mizil et al., 2013), empathy (Sharma et al., 2020), advice (Govindarajan et al., 2020), condolences (Zhou and Jurgens, 2020) usefulness (Momeni et al., 2013), and deception (Soldner et al., 2019). More related to the problem we work with, Maki et al. (2017) analyze the influence of Wikipedia editors, and Katerenchuk and Rosenberg (2016) study influence levels in online communities. Persuasion has also been studied from a computational perspective (Wei et al., 2016; Yang et al., 2019), including dialogue systems (Wang et al., 2019). The work presented here complements these works. We are interested in identifying credible justifications in order to aggregate crowdsourced forecasts, and we do so without explicitly targeting any of the above characteristics. Within computational linguistics, the previous task that is perhaps the closest to our goal is argumentation: a good justification for a forecast is arguably a good supporting argument. Previous work includes identifying argument components such as claims, premises, backings, rebuttals, and refutation"
2021.findings-emnlp.355,D17-1250,0,0.014007,"oughly uniform amount through the life of the question. The majority of the forecasts, unsurprisingly, is less reliable towards the first half of the life of a question. and analyze the characteristics of superforecaster performance, which can be used for cultivating better forecasters. Mellers et al. (2014) discuss explanations of what makes forecasters better. These works aim at identifying superforecasters and do not take into account the written justifications. Unlike them, we build models to call questions without using any information about forecasters. Within computational linguistics, Schwartz et al. (2017) assess the language of quality justifications (rating, benefit, and influence). Zong et al. (2020) is perhaps the closest experiment to ours. They build models to predict forecaster skill using the text justifications of forecasts from Good Judgment Open data, and they also use another dataset, Company Earnings Reports, to individually predict which forecasts are more likely to be correct predictions. Unlike us, none of these works aim at calling the question throughout its life. 3 Dataset We work with data from the Good Judgment Open,1 a website where questions are posted and people submit f"
2021.findings-emnlp.355,2020.emnlp-main.425,0,0.0169989,"e people use to communicate depending on the relationship between the parties. For example, the language people use when they are in positions of power (e.g., more seniority) has been studied in social networks (Bramsen et al., 2011), online communities (Danescu-Niculescu-Mizil et al., 2012), and corporate emails (Prabhakaran and Rambow, 2014). Similarly, Rashid and Blanco (2018) study how language provides clues about the interactions and relationships between people. Regarding language form and functions, prior research has analyzed politeness (DanescuNiculescu-Mizil et al., 2013), empathy (Sharma et al., 2020), advice (Govindarajan et al., 2020), condolences (Zhou and Jurgens, 2020) usefulness (Momeni et al., 2013), and deception (Soldner et al., 2019). More related to the problem we work with, Maki et al. (2017) analyze the influence of Wikipedia editors, and Katerenchuk and Rosenberg (2016) study influence levels in online communities. Persuasion has also been studied from a computational perspective (Wei et al., 2016; Yang et al., 2019), including dialogue systems (Wang et al., 2019). The work presented here complements these works. We are interested in identifying credible justifications in ord"
2021.findings-emnlp.355,N19-1175,0,0.0250699,"f power (e.g., more seniority) has been studied in social networks (Bramsen et al., 2011), online communities (Danescu-Niculescu-Mizil et al., 2012), and corporate emails (Prabhakaran and Rambow, 2014). Similarly, Rashid and Blanco (2018) study how language provides clues about the interactions and relationships between people. Regarding language form and functions, prior research has analyzed politeness (DanescuNiculescu-Mizil et al., 2013), empathy (Sharma et al., 2020), advice (Govindarajan et al., 2020), condolences (Zhou and Jurgens, 2020) usefulness (Momeni et al., 2013), and deception (Soldner et al., 2019). More related to the problem we work with, Maki et al. (2017) analyze the influence of Wikipedia editors, and Katerenchuk and Rosenberg (2016) study influence levels in online communities. Persuasion has also been studied from a computational perspective (Wei et al., 2016; Yang et al., 2019), including dialogue systems (Wang et al., 2019). The work presented here complements these works. We are interested in identifying credible justifications in order to aggregate crowdsourced forecasts, and we do so without explicitly targeting any of the above characteristics. Within computational linguist"
2021.findings-emnlp.355,D18-1402,0,0.0199224,"re complements these works. We are interested in identifying credible justifications in order to aggregate crowdsourced forecasts, and we do so without explicitly targeting any of the above characteristics. Within computational linguistics, the previous task that is perhaps the closest to our goal is argumentation: a good justification for a forecast is arguably a good supporting argument. Previous work includes identifying argument components such as claims, premises, backings, rebuttals, and refutations (Habernal and Gurevych, 2017), and mining supporting and opposing arguments for a claim (Stab et al., 2018). Notwithstanding these works, we found that crowdsourced justifications rarely fall into these argumentation frameworks despite the former are useful to aggregate forecasts. The language people use is indicative of several attributes. Previous work includes both predictive models (input: language samples, output: some attribute about the author) and models that yield useful insights (input: language samples and attributes of the authors, output: differentiating language features depending on the attributes). Among Finally, there are a few works on forecasting many others, previous research ha"
2021.findings-emnlp.355,N19-1364,0,0.0188269,"and relationships between people. Regarding language form and functions, prior research has analyzed politeness (DanescuNiculescu-Mizil et al., 2013), empathy (Sharma et al., 2020), advice (Govindarajan et al., 2020), condolences (Zhou and Jurgens, 2020) usefulness (Momeni et al., 2013), and deception (Soldner et al., 2019). More related to the problem we work with, Maki et al. (2017) analyze the influence of Wikipedia editors, and Katerenchuk and Rosenberg (2016) study influence levels in online communities. Persuasion has also been studied from a computational perspective (Wei et al., 2016; Yang et al., 2019), including dialogue systems (Wang et al., 2019). The work presented here complements these works. We are interested in identifying credible justifications in order to aggregate crowdsourced forecasts, and we do so without explicitly targeting any of the above characteristics. Within computational linguistics, the previous task that is perhaps the closest to our goal is argumentation: a good justification for a forecast is arguably a good supporting argument. Previous work includes identifying argument components such as claims, premises, backings, rebuttals, and refutations (Habernal and Gure"
2021.findings-emnlp.355,2020.emnlp-main.45,0,0.0286742,"arties. For example, the language people use when they are in positions of power (e.g., more seniority) has been studied in social networks (Bramsen et al., 2011), online communities (Danescu-Niculescu-Mizil et al., 2012), and corporate emails (Prabhakaran and Rambow, 2014). Similarly, Rashid and Blanco (2018) study how language provides clues about the interactions and relationships between people. Regarding language form and functions, prior research has analyzed politeness (DanescuNiculescu-Mizil et al., 2013), empathy (Sharma et al., 2020), advice (Govindarajan et al., 2020), condolences (Zhou and Jurgens, 2020) usefulness (Momeni et al., 2013), and deception (Soldner et al., 2019). More related to the problem we work with, Maki et al. (2017) analyze the influence of Wikipedia editors, and Katerenchuk and Rosenberg (2016) study influence levels in online communities. Persuasion has also been studied from a computational perspective (Wei et al., 2016; Yang et al., 2019), including dialogue systems (Wang et al., 2019). The work presented here complements these works. We are interested in identifying credible justifications in order to aggregate crowdsourced forecasts, and we do so without explicitly ta"
2021.findings-emnlp.355,2020.acl-main.473,0,0.0375384,"is less reliable towards the first half of the life of a question. and analyze the characteristics of superforecaster performance, which can be used for cultivating better forecasters. Mellers et al. (2014) discuss explanations of what makes forecasters better. These works aim at identifying superforecasters and do not take into account the written justifications. Unlike them, we build models to call questions without using any information about forecasters. Within computational linguistics, Schwartz et al. (2017) assess the language of quality justifications (rating, benefit, and influence). Zong et al. (2020) is perhaps the closest experiment to ours. They build models to predict forecaster skill using the text justifications of forecasts from Good Judgment Open data, and they also use another dataset, Company Earnings Reports, to individually predict which forecasts are more likely to be correct predictions. Unlike us, none of these works aim at calling the question throughout its life. 3 Dataset We work with data from the Good Judgment Open,1 a website where questions are posted and people submit forecasts. Questions are about geopolitics and include topics such as domestic and international pol"
blanco-etal-2008-causal,A00-2031,0,\N,Missing
blanco-etal-2008-causal,W04-2609,1,\N,Missing
blanco-etal-2008-causal,W04-2610,1,\N,Missing
blanco-etal-2008-causal,P98-1013,0,\N,Missing
blanco-etal-2008-causal,C98-1013,0,\N,Missing
blanco-etal-2008-causal,P05-2006,0,\N,Missing
blanco-etal-2008-causal,J06-1005,1,\N,Missing
blanco-etal-2008-causal,P06-1015,0,\N,Missing
blanco-etal-2008-causal,W03-1210,0,\N,Missing
blanco-etal-2008-causal,P00-1043,0,\N,Missing
blanco-etal-2008-causal,J02-3001,0,\N,Missing
blanco-etal-2008-causal,P02-1047,0,\N,Missing
blanco-etal-2008-causal,N03-1011,1,\N,Missing
C10-2009,S07-1045,0,0.0336058,"Missing"
C10-2009,P07-2040,0,0.184953,"Moreover, the semantics of a given label depends on the verb. For example, ARG 2 is used for INSTRUMENT and VALUE. Copious work has been done lately on semantic roles (M`arquez et al., 2008). Approaches to detect semantic relations usually focus on particular lexical and syntactic patterns or kind of arguments. There are both unsupervised (Turney, 2006) and supervised approaches. The SemEval2007 Task 4 (Girju et al., 2007) focused on relations between nominals. Work has been done on detecting relations between noun phrases (Davidov and Rappoport, 2008; Moldovan et al., 2004), named entities (Hirano et al., 2007), and clauses (Szpakowicz et al., 1995). There have been pro1 Introduction Semantic representation of text facilitates inferences, reasoning, and greatly improves the performance of Question Answering, Information Extraction, Machine Translation and other NLP applications. Broadly speaking, semantic relations are unidirectional underlying connections between concepts. For example, the noun phrase the car engine encodes a PART- WHOLE relation: the engine is a part of the car. Semantic relations are the building blocks for creating a semantic structure of a sentence. There is a growing interest"
C10-2009,P98-1013,0,0.421017,"t and the motivation for this work. This paper presents a framework for combining semantic relations extracted from text to reveal even more semantics that otherwise would be missed. A set of 26 relations is introduced, with their arguments defined on an ontology of sorts. A semantic parser is used to extract these relations from noun phrases and verb argument structures. The method was successfully used in two applications: rapid customization of semantic relations to arbitrary domains and recognizing entailments. 2 Related Work In Computational Linguistics, WordNet (Miller, 1995), FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005) are probably the most used semantic resources. Like our approach and unlike PropBank, FrameNet annotates semantics between concepts regardless of their position in a parse tree. Unlike us, they use a predefined set of frames to be filled. PropBank adds semantic annotation on top of the Penn TreeBank and it contains only annotations between a verb and its arguments. Moreover, the semantics of a given label depends on the verb. For example, ARG 2 is used for INSTRUMENT and VALUE. Copious work has been done lately on semantic roles (M`arquez et al., 2008). Appr"
C10-2009,bethard-etal-2008-building,0,0.0182228,"d n new relations, resulting in a richer semantic representation (Figure 2). (2) REA−1 (x, y ) ◦ GOA(y, z ) → PRP(x, z ): events have as their purpose the effects of their goals. This is a strong relation. For example: Since they have a better view, they can see the mountain range. They cut the tree to have a better view. Therefore, they cut the tree to see the mountain range. P P The axioms have been evaluated using manually annotated data. PropBank CAU and PNC are used as reason and goal. Reason annotation is further collected from a corpus which adds causal annotation to the Penn TreeBank (Bethard et al., 2008). A total of 5 and 29 instances for axioms 3 and 4 were found. For all of them, the axioms yield a valid inference. For example, Buick [approached]y American express about [a joint promotion]x because [its card holders generally have a good credit history]z . PropBank annotation states GOA(x, y ) and REA−1 (y, z ), axiom 4 makes the implicit relation IFL−1 (x, z ) explicit. 6 Case Study: Reason and Goal P GOA (get there faster, crossed carelessly ) REA(crossed carelessly, got run over ) IFL (get there faster, got run over ) −1 REA (see the mountain range, better view ) GOA (better view, cut th"
C10-2009,J08-2001,0,0.108476,"Missing"
C10-2009,W04-2609,1,0.832612,"ations between a verb and its arguments. Moreover, the semantics of a given label depends on the verb. For example, ARG 2 is used for INSTRUMENT and VALUE. Copious work has been done lately on semantic roles (M`arquez et al., 2008). Approaches to detect semantic relations usually focus on particular lexical and syntactic patterns or kind of arguments. There are both unsupervised (Turney, 2006) and supervised approaches. The SemEval2007 Task 4 (Girju et al., 2007) focused on relations between nominals. Work has been done on detecting relations between noun phrases (Davidov and Rappoport, 2008; Moldovan et al., 2004), named entities (Hirano et al., 2007), and clauses (Szpakowicz et al., 1995). There have been pro1 Introduction Semantic representation of text facilitates inferences, reasoning, and greatly improves the performance of Question Answering, Information Extraction, Machine Translation and other NLP applications. Broadly speaking, semantic relations are unidirectional underlying connections between concepts. For example, the noun phrase the car engine encodes a PART- WHOLE relation: the engine is a part of the car. Semantic relations are the building blocks for creating a semantic structure of a"
C10-2009,P01-1019,0,0.0435144,"The work reported in this paper aims at extracting as many semantic relations from text as possi72 Coling 2010: Poster Volume, pages 72–80, Beijing, August 2010 posals to detect a particular relation, e.g., CAUSE (Chang and Choi, 2006), INTENT (Tatu, 2005) and PART- WHOLE (Girju et al., 2006). Researchers have also worked on combining semantic relations. Harabagiu and Moldovan (1998) combine WordNet relations and Helbig (2005) transforms chains of relations into theoretical axioms. Some use logic as the underlying formalism (Lakoff, 1970; S´anchez Valencia, 1991), more ideas can be found in (Copestake et al., 2001). MAIN(R ); and (iii) R ANGE(R ). Stating restrictions for D OMAIN and R ANGE has several advantages: it (i) helps distinguishing between relations, e.g., [tall]ql and [John]aco can be linked through VALUE, but not POSSESSION; (ii) helps discarding potential relations that do not hold, e.g., abstract objects do not have INTENT; and (iii) helps combining semantic relations (Section 5). Ontology of Sorts In order to define D OMAIN(R) and R ANGE(R), we use a customized ontology of sorts (Figure 1) modified from (Helbig, 2005). The root corresponds to entities, which refers to all things about whi"
C10-2009,J05-1004,0,0.285104,". This paper presents a framework for combining semantic relations extracted from text to reveal even more semantics that otherwise would be missed. A set of 26 relations is introduced, with their arguments defined on an ontology of sorts. A semantic parser is used to extract these relations from noun phrases and verb argument structures. The method was successfully used in two applications: rapid customization of semantic relations to arbitrary domains and recognizing entailments. 2 Related Work In Computational Linguistics, WordNet (Miller, 1995), FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005) are probably the most used semantic resources. Like our approach and unlike PropBank, FrameNet annotates semantics between concepts regardless of their position in a parse tree. Unlike us, they use a predefined set of frames to be filled. PropBank adds semantic annotation on top of the Penn TreeBank and it contains only annotations between a verb and its arguments. Moreover, the semantics of a given label depends on the verb. For example, ARG 2 is used for INSTRUMENT and VALUE. Copious work has been done lately on semantic roles (M`arquez et al., 2008). Approaches to detect semantic relations"
C10-2009,P08-1027,0,0.0123445,"nk and it contains only annotations between a verb and its arguments. Moreover, the semantics of a given label depends on the verb. For example, ARG 2 is used for INSTRUMENT and VALUE. Copious work has been done lately on semantic roles (M`arquez et al., 2008). Approaches to detect semantic relations usually focus on particular lexical and syntactic patterns or kind of arguments. There are both unsupervised (Turney, 2006) and supervised approaches. The SemEval2007 Task 4 (Girju et al., 2007) focused on relations between nominals. Work has been done on detecting relations between noun phrases (Davidov and Rappoport, 2008; Moldovan et al., 2004), named entities (Hirano et al., 2007), and clauses (Szpakowicz et al., 1995). There have been pro1 Introduction Semantic representation of text facilitates inferences, reasoning, and greatly improves the performance of Question Answering, Information Extraction, Machine Translation and other NLP applications. Broadly speaking, semantic relations are unidirectional underlying connections between concepts. For example, the noun phrase the car engine encodes a PART- WHOLE relation: the engine is a part of the car. Semantic relations are the building blocks for creating a"
C10-2009,P04-1055,0,0.03674,"ine, car ) MAK(cars, BMW ) POS(Ford F-150, John ) MNR(quick, delivery ) RCP(Mary, gave ) SYN (a dozen, twelve ) AT- L (party, John’s house ) AT- T (party, last Saturday ) PRO(height, John ) QNT(a dozen, eggs ) Table 1: The set of 26 relations clustered and classified with their properties (reflexive, symmetric, transitive) and examples. An asterisk indicates that the property holds under certain conditions. 4.1 includes relations present in WordNet (Miller, 1995), such as IS - A , PART- WHOLE and CAUSE. Szpakowicz et al. (1995) proposed a set of nine relations and Turney (2006) a set of five. Rosario and Hearst (2004) proposed a set of 38 relations including standard case roles and a set of specific relations for medical domain. Helbig (2005) proposed a set of 89 relations, including ANTONYMY and several TEMPORAL relations, e.g. SUCCES SION , EXTENSION , END . Our set clusters some of the previous proposals (e.g. we only consider AT- TIME ) and discards relations proposed elsewhere when they did not occur frequently enough in our experiments. For example, even though ANTONYMY and ENTAIL MENT are semantically grounded, they are very infrequent and we do not deal with them. Our pragmatic goal is to capture a"
C10-2009,W07-1401,0,0.0113818,"infer IS - EMPLOYER, IS - COWORKER, IS PARAMOUR, IS - INTERPRETER, WAS - ASSASSIN , ATTENDS - SCHOOL - AT , JAILED - AT, COHABITS WITH , AFFILIATED - TO , MARRIED - TO , RENTED BY, KIDNAPPED- BY and the relations in Table 4 were defined. Note that a relation can be inferred by several axioms. This customization effort to add 37 new specialized relations took a person only a few days and without modifying the SP. 7.2 Textual Entailment Problem An application of CSR is recognizing entailments. Given text T and hypothesis H, the task consists on determining whether or not H can be inferred by T (Giampiccolo et al., 2007). CSR axioms Several examples of the RTE3 challenge can be solved by applying CSR (Table 5). The rest of this section depicts the axioms involved in detecting entailment for each pair. Pair 113 is a simple one. A perfect match for H in T can be obtained by an axiom reading all concepts inherit the semantic relations of their hypernyms. Formally, ISA(x, y ) ◦ THM(y, z ) → THM(x, z ), T 2 and T 4 are the premises and the conclusion matches H2. T 1 matches H1. Pair 429 can be solved by an axiom reading agents are values for their themes. Formally, AGT(x, y ) ◦ THM−1 (y, z ) → VAL(x, z ); T 1 and"
C10-2009,J06-1005,1,0.880656,"ctured text into structured knowledge. More and more enterprises and academic organizations have adopted the World Wide Web Consortium (W3C) Resource Description Framework (RDF) specification as a standard representation of text knowledge. This is based on semantic triples, which can be used to represent semantic relations. The work reported in this paper aims at extracting as many semantic relations from text as possi72 Coling 2010: Poster Volume, pages 72–80, Beijing, August 2010 posals to detect a particular relation, e.g., CAUSE (Chang and Choi, 2006), INTENT (Tatu, 2005) and PART- WHOLE (Girju et al., 2006). Researchers have also worked on combining semantic relations. Harabagiu and Moldovan (1998) combine WordNet relations and Helbig (2005) transforms chains of relations into theoretical axioms. Some use logic as the underlying formalism (Lakoff, 1970; S´anchez Valencia, 1991), more ideas can be found in (Copestake et al., 2001). MAIN(R ); and (iii) R ANGE(R ). Stating restrictions for D OMAIN and R ANGE has several advantages: it (i) helps distinguishing between relations, e.g., [tall]ql and [John]aco can be linked through VALUE, but not POSSESSION; (ii) helps discarding potential relations th"
C10-2009,W07-1404,1,0.898013,"Missing"
C10-2009,S07-1003,0,0.236658,"Missing"
C10-2009,P05-2006,0,0.146997,"at aim at transforming unstructured text into structured knowledge. More and more enterprises and academic organizations have adopted the World Wide Web Consortium (W3C) Resource Description Framework (RDF) specification as a standard representation of text knowledge. This is based on semantic triples, which can be used to represent semantic relations. The work reported in this paper aims at extracting as many semantic relations from text as possi72 Coling 2010: Poster Volume, pages 72–80, Beijing, August 2010 posals to detect a particular relation, e.g., CAUSE (Chang and Choi, 2006), INTENT (Tatu, 2005) and PART- WHOLE (Girju et al., 2006). Researchers have also worked on combining semantic relations. Harabagiu and Moldovan (1998) combine WordNet relations and Helbig (2005) transforms chains of relations into theoretical axioms. Some use logic as the underlying formalism (Lakoff, 1970; S´anchez Valencia, 1991), more ideas can be found in (Copestake et al., 2001). MAIN(R ); and (iii) R ANGE(R ). Stating restrictions for D OMAIN and R ANGE has several advantages: it (i) helps distinguishing between relations, e.g., [tall]ql and [John]aco can be linked through VALUE, but not POSSESSION; (ii) he"
C10-2009,P06-1040,0,0.526264,"tics between concepts regardless of their position in a parse tree. Unlike us, they use a predefined set of frames to be filled. PropBank adds semantic annotation on top of the Penn TreeBank and it contains only annotations between a verb and its arguments. Moreover, the semantics of a given label depends on the verb. For example, ARG 2 is used for INSTRUMENT and VALUE. Copious work has been done lately on semantic roles (M`arquez et al., 2008). Approaches to detect semantic relations usually focus on particular lexical and syntactic patterns or kind of arguments. There are both unsupervised (Turney, 2006) and supervised approaches. The SemEval2007 Task 4 (Girju et al., 2007) focused on relations between nominals. Work has been done on detecting relations between noun phrases (Davidov and Rappoport, 2008; Moldovan et al., 2004), named entities (Hirano et al., 2007), and clauses (Szpakowicz et al., 1995). There have been pro1 Introduction Semantic representation of text facilitates inferences, reasoning, and greatly improves the performance of Question Answering, Information Extraction, Machine Translation and other NLP applications. Broadly speaking, semantic relations are unidirectional underl"
C10-2009,C98-1013,0,\N,Missing
D10-1031,P98-1013,0,0.102676,"Missing"
D10-1031,C10-2009,1,0.818952,"ctic parsers achieve a good performance, they make mistakes and the less our models rely on them, the better. 9 Composing MANNER with PURPOSE MANNER can combine with other semantic relations in order to reveal implicit relations that otherwise would be missed. The basic idea is to compose MANNER with other relations in order to infer another MANNER. A necessary condition for combining MANNER with another relation R is the compatibility of R ANGE(MNR) with D OMAIN(R) or R ANGE(R) with D OMAIN(MNR). The extended definition (Section 3) allows to quickly determine if two relations are compatible (Blanco et al., 2010). The new MANNER is automatically inferred by humans when reading, but computers need an explicit representation. Consider the following example: [. . . ] the traders [place]y orders [via computers]MNR [to buy the basket of stocks . . . ]PRP (wsj 0118, 48). PropBank states the basic annotation between brackets: via computers is the MANNER and to buy the basket [. . . ] the PURPOSE of the place orders event. We propose to combine these two relations in order to come up with the new relation MNR(via computers, buy the basket [. . . ] ). This relation is obvious when reading the sentence, so it i"
D10-1031,W05-0620,0,0.114188,"Missing"
D10-1031,P08-1079,0,0.0394071,"Missing"
D10-1031,P07-1030,0,0.0225366,"growing very slowly if at all and He started the company on his own. Consider the following example: The company said Mr. Stronach will personally direct the restructuring assisted by Manfred Gingl, [. . . ]1 . There are two MANNER relations in this sentence: the underlined chunks of text encode the way in which Mr. Stronach will direct the restructuring. 2 Previous Work The extraction of semantic relations in general has caught the attention of several researchers. Approaches to detect semantic relations usually focus on particular lexical and syntactic patterns. There are both unsupervised (Davidov et al., 2007; Turney, 2006) and supervised approaches. The SemEval2007 Task 04 (Girju et al., 2007) aimed at relations between nominals. Work has been done on detecting relations within noun phrases (Nulty, 2007), 1 Penn TreeBank, file wsj 0027, sentence 10. 315 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 315–324, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics named entities (Hirano et al., 2007), clauses (Szpakowicz et al., 1995) and syntax-based comma resolution (Srikumar et al., 2008). There have been propos"
D10-1031,W03-1207,1,0.313621,"Missing"
D10-1031,J06-1005,1,0.898066,"Missing"
D10-1031,S07-1003,0,0.0678697,"Missing"
D10-1031,P06-1117,0,0.0431454,"Missing"
D10-1031,C92-2082,0,0.298859,"Missing"
D10-1031,P07-2040,0,0.0627118,"Missing"
D10-1031,J08-2001,0,0.0272255,"Missing"
D10-1031,P07-3014,0,0.0131917,". There are two MANNER relations in this sentence: the underlined chunks of text encode the way in which Mr. Stronach will direct the restructuring. 2 Previous Work The extraction of semantic relations in general has caught the attention of several researchers. Approaches to detect semantic relations usually focus on particular lexical and syntactic patterns. There are both unsupervised (Davidov et al., 2007; Turney, 2006) and supervised approaches. The SemEval2007 Task 04 (Girju et al., 2007) aimed at relations between nominals. Work has been done on detecting relations within noun phrases (Nulty, 2007), 1 Penn TreeBank, file wsj 0027, sentence 10. 315 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 315–324, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics named entities (Hirano et al., 2007), clauses (Szpakowicz et al., 1995) and syntax-based comma resolution (Srikumar et al., 2008). There have been proposals to detect a particular relation, e.g., CAUSE (Chang and Choi, 2006), INTENT (Tatu, 2005), PART- WHOLE (Girju et al., 2006) and IS - A (Hearst, 1992). MANNER is a frequent relation, but besides the"
D10-1031,J05-1004,0,0.0148818,"Missing"
D10-1031,P08-1117,0,0.035175,"Missing"
D10-1031,P05-2006,0,0.0742562,"Missing"
D10-1031,P06-1040,0,0.0215597,"at all and He started the company on his own. Consider the following example: The company said Mr. Stronach will personally direct the restructuring assisted by Manfred Gingl, [. . . ]1 . There are two MANNER relations in this sentence: the underlined chunks of text encode the way in which Mr. Stronach will direct the restructuring. 2 Previous Work The extraction of semantic relations in general has caught the attention of several researchers. Approaches to detect semantic relations usually focus on particular lexical and syntactic patterns. There are both unsupervised (Davidov et al., 2007; Turney, 2006) and supervised approaches. The SemEval2007 Task 04 (Girju et al., 2007) aimed at relations between nominals. Work has been done on detecting relations within noun phrases (Nulty, 2007), 1 Penn TreeBank, file wsj 0027, sentence 10. 315 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 315–324, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics named entities (Hirano et al., 2007), clauses (Szpakowicz et al., 1995) and syntax-based comma resolution (Srikumar et al., 2008). There have been proposals to detect a"
D10-1031,C98-1013,0,\N,Missing
D10-1031,J02-3001,0,\N,Missing
D13-1123,S12-1051,0,0.164379,"ity Eduardo Blanco and Dan Moldovan Lymba Corporation Richardson, TX 75080 USA {eduardo,moldovan}@lymba.com Abstract man a This paper presents a novel approach to determine textual similarity. A layered methodology to transform text into logic forms is proposed, and semantic features are derived from a logic prover. Experimental results show that incorporating the semantic structure of sentences is beneficial. When training data is unavailable, scores obtained from the logic prover in an unsupervised manner outperform supervised methods. 1 Introduction The task of Semantic Textual Similarity (Agirre et al., 2012) measures the degree of semantic equivalence between two sentences. Unlike textual entailment (Giampiccolo et al., 2007), textual similarity is symmetric, and unlike both textual entailment and paraphrasing (Dolan and Brockett, 2005), textual similarity is modeled using a graded score rather than a binary decision. For example, sentence pair (1) below is very similar [5 out of 5], (2) is somewhat similar [3 out of 5] and (3) is not similar at all [0 out of 5]: 1. Someone is removing the scales from the fish. A person is descaling a fish. 2. A woman is chopping an herb. A man is finely chopping"
D13-1123,P98-1013,0,0.0248229,"he only required modification would be the LFT component (Figure 3) so that it accounts for the subtleties of the representation of choice. The named entity recognizer extracts 35 finegrained types organized in a taxonomy (date, language, city, instrument, etc.) and was first developed for a question answering system (Moldovan et al., 2002). The implementation uses publicly available gazetteers as well as machine learning. Semantic relations are extracted with Polaris (Moldovan and Blanco, 2012), a semantic parser that given text extracts semantic relations. Polaris is trained using FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), NomBank (Meyers et al., 2004), several SemEval corpora (Girju et al., 2007; MSRpar (36/35) [750/750] MSRvid (13/13), [750/750] SMTeuroparl (56/21), [734/459] surprise.OnWN (–/16), [–/750] surprise.SMTnews (–/24), [–/399] Score 2.600 0.000 4.250 1.500 3.000 Sentence Pair The unions also staged a five-day strike in March that forced all but one of Yale’s dining halls to close. The unions also staged a five-day strike in March; strikes have preceded eight of the last 10 contracts. A woman is swimming underwater. A man is slicing some carrots. Then perhaps we coul"
D13-1123,S12-1094,0,0.551478,"‘outside’: missing this information does not carry loss of meaning. 2 Related Work Determining similarity between text snippets is relevant to information retrieval (Hatzivassiloglou et al., 1999), paraphrase recognition (Madnani and Dorr, 2010), grading answers to questions (Mohler et al., 2011) and many others. We focus on recent work and emphasize the differences from our approach. The SemEval 2012 Task 6: A Pilot on Semantic Textual Similarity (Agirre et al., 2012) brought together 35 teams that competed against each other. ˇ c et The top 3 performers (B¨ar et al., 2012; Sari´ al., 2012; Banea et al., 2012), followed a maSentences Logic Form Transformation Pairwise Similarity Measures LFT-based scores logic forms Logic Prover features Machine Learning score pairwise word similarity scores Figure 3: Main components of our system to determine textual similarity. chine learning approach with features that do not take into account the semantic structure of sentences, e.g., n-grams, word overlap, evaluation measures for machine translation, pairwise word similarities, syntactic dependencies. All three used WordNet, Wikipedia and other large corpora. In particular, Banea et al. (2012) obtained models"
D13-1123,S12-1059,0,0.0968318,"Missing"
D13-1123,C04-1180,0,0.0261863,"to calculate some ad-hoc similarity score. In contrast, our approach derives features from semantic representations encoded using logic, and combine these features using machine learning. Moreover, we use three logic form transformations capturing different levels of knowledge, from only content words to semantic structure. In turn, this allows us to boost performance by relying on semantics when simpler shallow methods fail. A few logic-based approaches to recognize textual entailment are similar to the work presented here. Bos and Markert (2006) extract semantic representations with Boxer (Bos et al., 2004) and incorporate background knowledge from external re1 2 http://www.wiktionary.org/ A third team, spirin2, submitted results but a description paper could not be found in the ACL anthology. sources. They use a standard theorem prover and extract 8 features that are later combined using machine learning. Raina et al. (2005) use a logic form transformation derived from dependency parses and named entities. They use abductive reasoning and define an assumption cost model to account for partial entailments. Unlike them, we define three logic from transformations, use a modified resolution step an"
D13-1123,I05-5002,0,0.0288994,"text into logic forms is proposed, and semantic features are derived from a logic prover. Experimental results show that incorporating the semantic structure of sentences is beneficial. When training data is unavailable, scores obtained from the logic prover in an unsupervised manner outperform supervised methods. 1 Introduction The task of Semantic Textual Similarity (Agirre et al., 2012) measures the degree of semantic equivalence between two sentences. Unlike textual entailment (Giampiccolo et al., 2007), textual similarity is symmetric, and unlike both textual entailment and paraphrasing (Dolan and Brockett, 2005), textual similarity is modeled using a graded score rather than a binary decision. For example, sentence pair (1) below is very similar [5 out of 5], (2) is somewhat similar [3 out of 5] and (3) is not similar at all [0 out of 5]: 1. Someone is removing the scales from the fish. A person is descaling a fish. 2. A woman is chopping an herb. A man is finely chopping a green substance. 3. A cat is playing with a watermelon on a floor. A man is pouring oil into a pan. State-of-the-art systems to determine textual simˇ c et al., 2012; Banea ilarity (B¨ar et al., 2012; Sari´ et al., 2012) do not ac"
D13-1123,W07-1401,0,0.0113594,"t man a This paper presents a novel approach to determine textual similarity. A layered methodology to transform text into logic forms is proposed, and semantic features are derived from a logic prover. Experimental results show that incorporating the semantic structure of sentences is beneficial. When training data is unavailable, scores obtained from the logic prover in an unsupervised manner outperform supervised methods. 1 Introduction The task of Semantic Textual Similarity (Agirre et al., 2012) measures the degree of semantic equivalence between two sentences. Unlike textual entailment (Giampiccolo et al., 2007), textual similarity is symmetric, and unlike both textual entailment and paraphrasing (Dolan and Brockett, 2005), textual similarity is modeled using a graded score rather than a binary decision. For example, sentence pair (1) below is very similar [5 out of 5], (2) is somewhat similar [3 out of 5] and (3) is not similar at all [0 out of 5]: 1. Someone is removing the scales from the fish. A person is descaling a fish. 2. A woman is chopping an herb. A man is finely chopping a green substance. 3. A cat is playing with a watermelon on a floor. A man is pouring oil into a pan. State-of-the-art"
D13-1123,S07-1003,0,0.0437229,"Missing"
D13-1123,S12-1079,0,0.105039,"al. (2012) obtained models from 6 million Wikipedia articles and more than 9.5 million hyperlinks; B¨ar et al. (2012) used Wiktionary1 , ˇ c et which contains over 3 million entries; and Sari´ al. (2012) used The New York Times Annotated Corpus (Sandhaus, 2008), which contains over 1.8 million news articles, and Google n-grams (Lin et al., 2012), which consists of approximately 24GB of compressed text files. Our approach only uses WordNet, by far the smallest external resource with less than 120,000 synsets. Participants that incorporated information about the semantic structure of sentences (Glinos, 2012; Rios et al., 2012)2 did not perform at the top. Out of 88 runs, they were ranked 16, 36 and 64. We believe this is because they use semantic relations to calculate some ad-hoc similarity score. In contrast, our approach derives features from semantic representations encoded using logic, and combine these features using machine learning. Moreover, we use three logic form transformations capturing different levels of knowledge, from only content words to semantic structure. In turn, this allows us to boost performance by relying on semantics when simpler shallow methods fail. A few logic-based"
D13-1123,W99-0625,0,0.0653822,"y the same meaning. Sentence 2(c) A woman is applying cosmetics to her face and 2(d) A woman is putting on makeup are highly similar even though the latter specifies neither the LOCATION where the ‘makeup’ is applied nor the fact that a PART of the ‘woman’ is her ‘face’. Similarly, sentences 2(e) A woman is dancing in the rain and 2(f) A woman dances in the rain outside are semantically equivalent since ‘rain’ always has LOCATION ‘outside’: missing this information does not carry loss of meaning. 2 Related Work Determining similarity between text snippets is relevant to information retrieval (Hatzivassiloglou et al., 1999), paraphrase recognition (Madnani and Dorr, 2010), grading answers to questions (Mohler et al., 2011) and many others. We focus on recent work and emphasize the differences from our approach. The SemEval 2012 Task 6: A Pilot on Semantic Textual Similarity (Agirre et al., 2012) brought together 35 teams that competed against each other. ˇ c et The top 3 performers (B¨ar et al., 2012; Sari´ al., 2012; Banea et al., 2012), followed a maSentences Logic Form Transformation Pairwise Similarity Measures LFT-based scores logic forms Logic Prover features Machine Learning score pairwise word similarity"
D13-1123,S10-1006,0,0.0259352,"Missing"
D13-1123,N06-2015,0,0.0348913,"ber of instances) in the train and test splits. Pustejovsky and Verhagen, 2009; Hendrickx et al., 2010) and in-house annotations. 4.1 Corpora We use the corpora released by SemEval 2012 Task 06: A Pilot on Semantic Textual Similarity5 (Agirre et al., 2012). These corpora consist of pairs of sentences labeled with their semantic similarity score, ranging from 0.0 to 5.0. Sentence pairs come from five sources: (1) MSRpar, a corpus of paraphrases; (2) MSRvid, short video descriptions; (3) SMTeuroparl, output of machine translation systems and reference translations; (4) surprise.OnWN, OntoNotes (Hovy et al., 2006) and WordNet (Miller, 1995) glosses; and (5) surprise.SMTnews, output of machine translation systems in the news domain and gold translations. Examples can be found in Table 4, for more details refer to the aforementioned citation. 4.2 Results and Error Analysis Results are reported using the same train and test splits provided by the organization of SemEval 2012 Task 6. For surprise.OnWn and surprise.SMTnews, only test data is available and supervised machine learning is not an option. Table 5 shows results obtained with the test split not dropping and dropping unbound predicates. For compari"
D13-1123,O97-1002,0,0.187296,"Missing"
D13-1123,P12-3029,0,0.0230766,"Missing"
D13-1123,J10-3003,0,0.025096,"smetics to her face and 2(d) A woman is putting on makeup are highly similar even though the latter specifies neither the LOCATION where the ‘makeup’ is applied nor the fact that a PART of the ‘woman’ is her ‘face’. Similarly, sentences 2(e) A woman is dancing in the rain and 2(f) A woman dances in the rain outside are semantically equivalent since ‘rain’ always has LOCATION ‘outside’: missing this information does not carry loss of meaning. 2 Related Work Determining similarity between text snippets is relevant to information retrieval (Hatzivassiloglou et al., 1999), paraphrase recognition (Madnani and Dorr, 2010), grading answers to questions (Mohler et al., 2011) and many others. We focus on recent work and emphasize the differences from our approach. The SemEval 2012 Task 6: A Pilot on Semantic Textual Similarity (Agirre et al., 2012) brought together 35 teams that competed against each other. ˇ c et The top 3 performers (B¨ar et al., 2012; Sari´ al., 2012; Banea et al., 2012), followed a maSentences Logic Form Transformation Pairwise Similarity Measures LFT-based scores logic forms Logic Prover features Machine Learning score pairwise word similarity scores Figure 3: Main components of our system t"
D13-1123,P11-1076,0,0.0131218,"up are highly similar even though the latter specifies neither the LOCATION where the ‘makeup’ is applied nor the fact that a PART of the ‘woman’ is her ‘face’. Similarly, sentences 2(e) A woman is dancing in the rain and 2(f) A woman dances in the rain outside are semantically equivalent since ‘rain’ always has LOCATION ‘outside’: missing this information does not carry loss of meaning. 2 Related Work Determining similarity between text snippets is relevant to information retrieval (Hatzivassiloglou et al., 1999), paraphrase recognition (Madnani and Dorr, 2010), grading answers to questions (Mohler et al., 2011) and many others. We focus on recent work and emphasize the differences from our approach. The SemEval 2012 Task 6: A Pilot on Semantic Textual Similarity (Agirre et al., 2012) brought together 35 teams that competed against each other. ˇ c et The top 3 performers (B¨ar et al., 2012; Sari´ al., 2012; Banea et al., 2012), followed a maSentences Logic Form Transformation Pairwise Similarity Measures LFT-based scores logic forms Logic Prover features Machine Learning score pairwise word similarity scores Figure 3: Main components of our system to determine textual similarity. chine learning appro"
D13-1123,moldovan-blanco-2012-polaris,1,0.896368,"Missing"
D13-1123,J05-1004,0,0.00755777,"ould be the LFT component (Figure 3) so that it accounts for the subtleties of the representation of choice. The named entity recognizer extracts 35 finegrained types organized in a taxonomy (date, language, city, instrument, etc.) and was first developed for a question answering system (Moldovan et al., 2002). The implementation uses publicly available gazetteers as well as machine learning. Semantic relations are extracted with Polaris (Moldovan and Blanco, 2012), a semantic parser that given text extracts semantic relations. Polaris is trained using FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), NomBank (Meyers et al., 2004), several SemEval corpora (Girju et al., 2007; MSRpar (36/35) [750/750] MSRvid (13/13), [750/750] SMTeuroparl (56/21), [734/459] surprise.OnWN (–/16), [–/750] surprise.SMTnews (–/24), [–/399] Score 2.600 0.000 4.250 1.500 3.000 Sentence Pair The unions also staged a five-day strike in March that forced all but one of Yale’s dining halls to close. The unions also staged a five-day strike in March; strikes have preceded eight of the last 10 contracts. A woman is swimming underwater. A man is slicing some carrots. Then perhaps we could have avoided a catastrophe. We"
D13-1123,D09-1001,0,0.0303832,"SemRels Full woman N(x1 ) & dance V(x2 ) & rain N(x3 ) & outside M(x4 ) woman N(x1 ) & dance V(x2 ) & A G E N T SR(x2 , x1 ) & rain N(x3 ) & L O C A T I O N SR(x2 , x3 ) woman N(x1 ) & dance V(x2 ) & A G E N T SR(x2 , x1 ) & rain N(x3 ) & L O C A T I O N SR(x2 , x3 ) & outside M(x4 ) Table 1: Examples of logic from transformation using modes Basic, SemRels and Full. 3.1 Logic Form Transformation The logic form transformation (LFT) of a sentence is derived from the concepts in it, the semantic relations linking them and named entities. Unlike other LFT proposals (Zettlemoyer and Collins, 2005; Poon and Domingos, 2009), transforming sentences into logic forms is a straightforward step, the quality of the logic forms is determined by the output of standard NLP tools. We distinguish six types of predicates: • N for nouns, e.g., woman: woman N(x1 ). • V for verbs, e.g., dances: dance V(x2 ). • M for adjectives and adverbs, e.g., outside: outside M(x3 ). • O for concepts encoded by other POS tags. • NE for named entities, e.g., guitar: guitar N(x4 ) & instrument NE(x4 ). • SR for semantic relations, e.g., A woman dances: woman N(x1 ) & dance V(x2 ) & A G E N T SR(x2 , x1 ). In order to overcome semantic relatio"
D13-1123,W09-2418,0,0.0267092,"Missing"
D13-1123,S12-1100,0,0.0312894,"12; Banea et al., 2012), followed a maSentences Logic Form Transformation Pairwise Similarity Measures LFT-based scores logic forms Logic Prover features Machine Learning score pairwise word similarity scores Figure 3: Main components of our system to determine textual similarity. chine learning approach with features that do not take into account the semantic structure of sentences, e.g., n-grams, word overlap, evaluation measures for machine translation, pairwise word similarities, syntactic dependencies. All three used WordNet, Wikipedia and other large corpora. In particular, Banea et al. (2012) obtained models from 6 million Wikipedia articles and more than 9.5 million hyperlinks; B¨ar et al. (2012) used Wiktionary1 , ˇ c et which contains over 3 million entries; and Sari´ al. (2012) used The New York Times Annotated Corpus (Sandhaus, 2008), which contains over 1.8 million news articles, and Google n-grams (Lin et al., 2012), which consists of approximately 24GB of compressed text files. Our approach only uses WordNet, by far the smallest external resource with less than 120,000 synsets. Participants that incorporated information about the semantic structure of sentences (Glinos, 20"
D13-1123,H05-1047,1,0.763697,"nal re1 2 http://www.wiktionary.org/ A third team, spirin2, submitted results but a description paper could not be found in the ACL anthology. sources. They use a standard theorem prover and extract 8 features that are later combined using machine learning. Raina et al. (2005) use a logic form transformation derived from dependency parses and named entities. They use abductive reasoning and define an assumption cost model to account for partial entailments. Unlike them, we define three logic from transformations, use a modified resolution step and extract hundreds of features from the proofs. Tatu and Moldovan (2005) use a modified logic prover that drops predicates when a proof cannot be found. Unlike us, they do not drop unbound predicates and use a single logic form transformation. Another key difference is that they assign fixed weights to predicates a priori instead of using machine learning to determine them. 3 Approach Our approach to determine textual similarity (Figure 3) is grounded on using semantic features derived from a logic prover that are later combined in a standard supervised machine learning framework. First, sentences are transformed into logic forms (lft1 , lft2 ). Then, a modified l"
D13-1123,S12-1060,0,0.106804,"Missing"
D13-1123,C98-1013,0,\N,Missing
D13-1123,P94-1019,0,\N,Missing
D13-1123,meyers-etal-2004-annotating,0,\N,Missing
D16-1118,S12-1051,0,0.036005,"Missing"
D16-1118,N16-1169,1,0.66259,"ently change meaning). We also add all tokens to the left of verb until we find the first token whose part-of-speech tag does not start with VB, MD, RB or EX (verbs, modals, adverbs and existential there). 5. Generate additional normalizations. If verb is 1101 followed by TO + verb2 (e.g., want to go, like to play, intend to pass), we generate an additional normalization for verb2 after merging the semantic roles of verb and verb2 . Table 1 exemplifies the automatic normalization step by step with 2 modal constructions. Generating Potential Interpretations in Plain Text. Inspired by the rules Blanco and Sarabi (2016) used to generate interpretations from negation, we generate potential interpretations from modal constructions by toggling off combinations of roles in sem roles. We consider numbered roles (ARG0 – ARG 5 ), and argument modifiers (ARGM -) ending in LOC , TMP, MNR , PRP, CAU , EXT, PRD or DIR . Table 1 lists some potential interpretations generated from a sample modal construction. The total number of potential interpretations for the 324 selected modal construction is 1,756 (average: 5.4). We recognize that our procedure to generate implicit interpretations is unable to generate some useful i"
D16-1118,W13-0303,0,0.0419395,"nd linguistics have studied modality for decades (Palmer, 2001; Jespersen, 1992). Morante and Sporleder (2012) summarize some of these works and related phenomena, e.g., evidentiality, certainty, factuality, subjectivity. There are several expressions that have modal meanings (Fintel, 2006), including auxiliaries (must, should, etc.), adverbs (perhaps, possibly, etc.) nouns (possibility, chance, etc.) adjectives (necessary, possible, etc.) and conditionals (e.g., If the light is on, Sandy is home). Most previous works in computational linguistics target modal adverbs (Rubinstein et al., 2013; Carretero and Zamorano-Mansilla, 2013; de Waard and Maat, 2012), and some also target other modal triggers such as reporting verbs (e.g., The evidence suggests that he caused the fire), references, or all verbs (Diab et al., 2009). Following these previous works, we focus on modal adverbs. Beyond theoretical works, there are many proposals to annotate modality. Doing so has proven challenging: following different annotations schemas on the same source text yields little overlap (Vincze et al., 2011), and Carretero and Zamorano-Mansilla (2013) present an analysis of disagreements when targeting modal adverbs. Annotation schemas ty"
D16-1118,J12-2003,0,0.0374697,"Missing"
D16-1118,W12-4306,0,0.0255007,"Missing"
D16-1118,W09-3012,0,0.0165447,"ectivity. There are several expressions that have modal meanings (Fintel, 2006), including auxiliaries (must, should, etc.), adverbs (perhaps, possibly, etc.) nouns (possibility, chance, etc.) adjectives (necessary, possible, etc.) and conditionals (e.g., If the light is on, Sandy is home). Most previous works in computational linguistics target modal adverbs (Rubinstein et al., 2013; Carretero and Zamorano-Mansilla, 2013; de Waard and Maat, 2012), and some also target other modal triggers such as reporting verbs (e.g., The evidence suggests that he caused the fire), references, or all verbs (Diab et al., 2009). Following these previous works, we focus on modal adverbs. Beyond theoretical works, there are many proposals to annotate modality. Doing so has proven challenging: following different annotations schemas on the same source text yields little overlap (Vincze et al., 2011), and Carretero and Zamorano-Mansilla (2013) present an analysis of disagreements when targeting modal adverbs. Annotation schemas typically include 3 tasks: identifying modality triggers, their scopes, and sources (Quaresma et al., 2014; S´anchez and Vogel, 2015). Many also classify the modality into several types (epistemi"
D16-1118,J02-3001,0,0.0107184,"ature combinations. 6.1 Feature Selection The full set of features is detailed in Table 5. Baseline features are simple features characterizing adverb and verb and we do not elaborate on them. Adverb and verb features are extracted from the modal construction (constituent tree and semantic roles) and provide additional information about the modal construction. Interpretation features characterize the potential interpretation whose factuality is being scored, and are also derived from the constituent tree and semantic roles. Most adverb and verb features are standard in semantic role labeling (Gildea and Jurafsky, 2002). We include the part-of-speech tags of the parent, and left and right siblings of adverb and verb, as well as their subcategorization, i.e., the concatenation of the sibling’s part-of-speech tags. We also include syntactic path between adverb and verb, and its length. Additionally, we include the common ancestor, i.e., the syntactic node of the lowest common node that is an ancestor of both adverb and verb, and use binary features to indicate whether each semantic role is present in the modal construction. Finally, interpretation features characterize the semantic roles toggled off to generat"
D16-1118,N06-2015,0,0.0316457,"3 Terminology and Background We use the term modal construction to refer to verbargument structures modified by a modal adverb (possibly, probably, etc.). We use the term implicit interpretation, or interpretation to save space, to refer to meaning intuitively understood by humans when reading a modal construction. Potential interpretations are interpretations automatically generated whose factuality has yet to be determined. The factuality of an interpretation is a score indicating its likelihood—whether it is true, false or unknown given the modal construction. We work on top of OntoNotes (Hovy et al., 2006) because it includes text from several genres (news, broadcast and telephone conversations, weblogs, etc.) and includes part-of-speech tags, parse trees, PropBank-style semantic roles and other linguistic information.3 Very briefly, PropBank (Palmer et al., 2005) has two kinds of semantic roles: numbered roles (ARG0 , ARG1 , etc.), which are defined in verb-specific framesets, and argument modifiers (ARGM - TMP, ARGM - LOC, etc.), we refer the reader to the aforementioned reference, and the guidelines and framesets4 for more details. We transformed the parse trees in OntoNotes into syntactic d"
D16-1118,D15-1189,0,0.0122501,"lity is being annotated. Instead, we present an automated approach: we manipulate semantic roles and syntactic dependencies deterministically to generate several potential interpretations per modal construction, and then assess their factuality. Many other efforts expand on FactBank using crowdsourced annotations, different annotation schemas (usually simpler) or other domains. Prabhakaran et al. (2012) use crowdsourcing to classify propositions into 5 modalities: ability, effort, intention, success and want. Soni et al. (2014) target the factuality of quotes (direct and indirect) in Twitter. Lee et al. (2015) detect events and assess factuality using easy-to-understand short instructions to crowdsource annotations. Unlike us, they annotate factuality at the individual token level, where annotated tokens are deemed events by annotators. Prabhakaran et al. (2015) define and annotate propositional heads with four categories: (1) non-belief propositions, or (2) committed, non-committed or reported belief. Instead of assessing factuality only for propositional heads (usually verbs, one assessment per proposition), we do so for potential interpretations automatically generated by manipulating verbs and"
D16-1118,W04-3103,0,0.132146,"Missing"
D16-1118,N06-1006,0,0.0221325,"Missing"
D16-1118,P14-5010,0,0.00289208,"al genres (news, broadcast and telephone conversations, weblogs, etc.) and includes part-of-speech tags, parse trees, PropBank-style semantic roles and other linguistic information.3 Very briefly, PropBank (Palmer et al., 2005) has two kinds of semantic roles: numbered roles (ARG0 , ARG1 , etc.), which are defined in verb-specific framesets, and argument modifiers (ARGM - TMP, ARGM - LOC, etc.), we refer the reader to the aforementioned reference, and the guidelines and framesets4 for more details. We transformed the parse trees in OntoNotes into syntactic dependencies using Stanford CoreNLP (Manning et al., 2014). 4 Corpus Creation We define a two-step procedure to create a corpus of modal constructions and the implicit interpretations intuitively understood by humans when reading them. First, we automatically generate potential interpretations from modal constructions by manipulating syntactic dependencies and semantic roles. Second, we manually score potential interpretations according to their likelihood. These interpretations and scores are later used to learn how to score potential interpretations automatically (Section 6). 4.1 Generating Potential Interpretations Selecting Modal Constructions. O"
D16-1118,J12-2001,0,0.0224185,"contributions of 1098 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1098–1107, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics this paper are: (1) procedure to automatically generate potential interpretations from modal constructions; (2) annotations assessing the factuality of potential interpretations generated from OntoNotes;1 and (3) experimental results using several features. 2 Previous Work Theoretical works in philosophy and linguistics have studied modality for decades (Palmer, 2001; Jespersen, 1992). Morante and Sporleder (2012) summarize some of these works and related phenomena, e.g., evidentiality, certainty, factuality, subjectivity. There are several expressions that have modal meanings (Fintel, 2006), including auxiliaries (must, should, etc.), adverbs (perhaps, possibly, etc.) nouns (possibility, chance, etc.) adjectives (necessary, possible, etc.) and conditionals (e.g., If the light is on, Sandy is home). Most previous works in computational linguistics target modal adverbs (Rubinstein et al., 2013; Carretero and Zamorano-Mansilla, 2013; de Waard and Maat, 2012), and some also target other modal triggers suc"
D16-1118,W13-0501,0,0.0129377,"many proposals to annotate modality. Doing so has proven challenging: following different annotations schemas on the same source text yields little overlap (Vincze et al., 2011), and Carretero and Zamorano-Mansilla (2013) present an analysis of disagreements when targeting modal adverbs. Annotation schemas typically include 3 tasks: identifying modality triggers, their scopes, and sources (Quaresma et al., 2014; S´anchez and Vogel, 2015). Many also classify the modality into several types (epistemic, circumstantial, ability, deontic, etc.) or a fine-grained taxonomy (Rubinstein et al., 2013; Nissim et al., 2013). In this paper, we are not concerned with modeling modality per se, or classifying instances of modality into predefined classes or hierarchies. Instead, we extract implicit interpretations from modal constructions in order to mirror intuitive readings. FactBank is probably the best-known corpus for event factuality (Saur´ı and Pustejovsky, 2009). It was created following carefully crafted annotation 1 Available at www.sanders.tech 1099 guidelines and examples comprising 34 pages.2 The guidelines detail a manual normalization step to “identify the full event that needs to be assessed in terms"
D16-1118,J05-1004,0,0.0817901,"understood by humans when reading a modal construction. Potential interpretations are interpretations automatically generated whose factuality has yet to be determined. The factuality of an interpretation is a score indicating its likelihood—whether it is true, false or unknown given the modal construction. We work on top of OntoNotes (Hovy et al., 2006) because it includes text from several genres (news, broadcast and telephone conversations, weblogs, etc.) and includes part-of-speech tags, parse trees, PropBank-style semantic roles and other linguistic information.3 Very briefly, PropBank (Palmer et al., 2005) has two kinds of semantic roles: numbered roles (ARG0 , ARG1 , etc.), which are defined in verb-specific framesets, and argument modifiers (ARGM - TMP, ARGM - LOC, etc.), we refer the reader to the aforementioned reference, and the guidelines and framesets4 for more details. We transformed the parse trees in OntoNotes into syntactic dependencies using Stanford CoreNLP (Manning et al., 2014). 4 Corpus Creation We define a two-step procedure to create a corpus of modal constructions and the implicit interpretations intuitively understood by humans when reading them. First, we automatically gene"
D16-1118,W12-3807,0,0.0387819,"Missing"
D16-1118,S15-1009,0,0.0169455,"y other efforts expand on FactBank using crowdsourced annotations, different annotation schemas (usually simpler) or other domains. Prabhakaran et al. (2012) use crowdsourcing to classify propositions into 5 modalities: ability, effort, intention, success and want. Soni et al. (2014) target the factuality of quotes (direct and indirect) in Twitter. Lee et al. (2015) detect events and assess factuality using easy-to-understand short instructions to crowdsource annotations. Unlike us, they annotate factuality at the individual token level, where annotated tokens are deemed events by annotators. Prabhakaran et al. (2015) define and annotate propositional heads with four categories: (1) non-belief propositions, or (2) committed, non-committed or reported belief. Instead of assessing factuality only for propositional heads (usually verbs, one assessment per proposition), we do so for potential interpretations automatically generated by manipulating verbs and their arguments deterministically. All works cited in the previous two paragraphs either manually normalize text prior to assessing factuality—making automation from plain text impossible—or assess factuality for tokens deemed events (ordered, delay, agreed"
D16-1118,W11-1901,0,0.0553373,"tial interpretations from modal constructions by manipulating syntactic dependencies and semantic roles. Second, we manually score potential interpretations according to their likelihood. These interpretations and scores are later used to learn how to score potential interpretations automatically (Section 6). 4.1 Generating Potential Interpretations Selecting Modal Constructions. OntoNotes is a large corpus containing 63,918 sentences. Creating a corpus of interpretations for all modal constructions is outside the scope of this paper. In order 3 We use the CoNLL-2011 Shared Task distribution (Pradhan et al., 2011), http://conll.cemantix.org/2011/ 4 http://propbank.github.io/ 1100 to alleviate the annotation effort, we focus on selected modal constructions. Specifically, we select verb-argument structures that have one ARGM - ADV or ARGM - MNR role, and that role is one of the following modal adverbs: certainly, clearly, definitely, likely, obviously, possibly, probably, surely, or unlikely. These adverbs are the most frequent that satisfy the above filter. Additionally, we discard verbargument structures with to be as the main verb. These rules retrieve 324 modal constructions. Automatic Normalization."
D16-1118,W13-0306,0,0.0296176,"cal works in philosophy and linguistics have studied modality for decades (Palmer, 2001; Jespersen, 1992). Morante and Sporleder (2012) summarize some of these works and related phenomena, e.g., evidentiality, certainty, factuality, subjectivity. There are several expressions that have modal meanings (Fintel, 2006), including auxiliaries (must, should, etc.), adverbs (perhaps, possibly, etc.) nouns (possibility, chance, etc.) adjectives (necessary, possible, etc.) and conditionals (e.g., If the light is on, Sandy is home). Most previous works in computational linguistics target modal adverbs (Rubinstein et al., 2013; Carretero and Zamorano-Mansilla, 2013; de Waard and Maat, 2012), and some also target other modal triggers such as reporting verbs (e.g., The evidence suggests that he caused the fire), references, or all verbs (Diab et al., 2009). Following these previous works, we focus on modal adverbs. Beyond theoretical works, there are many proposals to annotate modality. Doing so has proven challenging: following different annotations schemas on the same source text yields little overlap (Vincze et al., 2011), and Carretero and Zamorano-Mansilla (2013) present an analysis of disagreements when targeti"
D16-1118,W15-0302,0,0.0273526,"Missing"
D16-1118,N06-1005,0,0.02945,"Missing"
D16-1118,P14-2068,0,0.0227904,"et al. (2012), rely on manual normalization to identify the eventuality whose factuality is being annotated. Instead, we present an automated approach: we manipulate semantic roles and syntactic dependencies deterministically to generate several potential interpretations per modal construction, and then assess their factuality. Many other efforts expand on FactBank using crowdsourced annotations, different annotation schemas (usually simpler) or other domains. Prabhakaran et al. (2012) use crowdsourcing to classify propositions into 5 modalities: ability, effort, intention, success and want. Soni et al. (2014) target the factuality of quotes (direct and indirect) in Twitter. Lee et al. (2015) detect events and assess factuality using easy-to-understand short instructions to crowdsource annotations. Unlike us, they annotate factuality at the individual token level, where annotated tokens are deemed events by annotators. Prabhakaran et al. (2015) define and annotate propositional heads with four categories: (1) non-belief propositions, or (2) committed, non-committed or reported belief. Instead of assessing factuality only for propositional heads (usually verbs, one assessment per proposition), we do"
D16-1118,W08-0606,0,0.0510893,"Missing"
D16-1119,S12-1043,0,0.0328511,"Missing"
D16-1119,S13-1004,0,0.0223569,"n this paper, we focus on verbal negation, i.e., when the negation mark—usually an adverb such as never and not—is grammatically associated with a verb. Positive Interpretations. In philosophy and linguistics, it is generally accepted that negation conveys positive meaning (Horn, 1989). This positive meaning ranges from implicatures, i.e., what is suggested in an utterance even though neither expressed nor strictly implied (Blackburn, 2008), to entailments. Other terms used in the literature include implied meanings (Mitkov, 2005), implied alternatives (Rooth, 1985) and semantically similars (Agirre et al., 2013). We do not strictly fit into any of this terminology, we reveal positive interpretations as intuitively done by humans when reading text. 2.1 Scope and Focus From a theoretical perspective, it is accepted that negation has scope and focus, and that the focus— not just the scope—yields positive interpretations (Horn, 1989; Rooth, 1992; Taglicht, 1984). Scope is “the part of the meaning that is negated” and focus “the part of the scope that is most prominently or explicitly negated” (Huddleston and Pullum, 2002). Consider the following statement in the context of the recent refugee crisis: (2)"
D16-1119,W12-3808,0,0.266182,"Missing"
D16-1119,P98-1013,0,0.0489611,"is preferable, as it yields a more specific interpretation and it entails the former: if some jobs last for decades, then something lasts for decades, but not the other way around. We use the term coarse-grained focus to refer to foci that include all tokens belonging to an argument of a verb (e.g., Most Jobs above), and fine-grained focus to refer to foci that do not (e.g., Most above). 3 Previous Work Within computational linguistics, approaches to process negation are shallow, or target scope and focus detection. Popular semantic representations such as semantic roles (Palmer et al., 2005; Baker et al., 1998) or AMR (Banarescu et al., 2013) do not reveal the positive interpretations we target in this paper. Shallow approaches are usually application-specific. In sentiment and opinion analysis, negation has been reduced to marking as negated all words between a negation cue and the first punctuation mark (Pang et al., 2002), or within a five-word window of a negation cue (Hu and Liu, 2004). The examples throughout this paper show that these techniques are insufficient to reveal implicit positive interpretations. 3.1 Scope Annotations and Detection Scope of negation detection has received a lot of a"
D16-1119,W13-2322,0,0.0164611,"a more specific interpretation and it entails the former: if some jobs last for decades, then something lasts for decades, but not the other way around. We use the term coarse-grained focus to refer to foci that include all tokens belonging to an argument of a verb (e.g., Most Jobs above), and fine-grained focus to refer to foci that do not (e.g., Most above). 3 Previous Work Within computational linguistics, approaches to process negation are shallow, or target scope and focus detection. Popular semantic representations such as semantic roles (Palmer et al., 2005; Baker et al., 1998) or AMR (Banarescu et al., 2013) do not reveal the positive interpretations we target in this paper. Shallow approaches are usually application-specific. In sentiment and opinion analysis, negation has been reduced to marking as negated all words between a negation cue and the first punctuation mark (Pang et al., 2002), or within a five-word window of a negation cue (Hu and Liu, 2004). The examples throughout this paper show that these techniques are insufficient to reveal implicit positive interpretations. 3.1 Scope Annotations and Detection Scope of negation detection has received a lot of attention, mostly using two corpo"
D16-1119,P11-1059,1,0.934677,"Missing"
D16-1119,N16-1169,1,0.275538,"Missing"
D16-1119,W10-3110,0,0.369438,"Missing"
D16-1119,P16-1047,0,0.122159,"vas et al., 2008) and CDSCO (Morante and Daelemans, 2012). BioScope annotates negation cues and linguistic scopes exclusively in biomedical texts. CD-SCO annotates negation cues, scopes, and negated events or properties in selected Conan Doyle stories. There have been several supervised proposals to detect the scope of negation using BioScope and ¨ ur and Radev, 2009; Øvrelid et al., CD-SCO (Ozg¨ 2010). Automatic approaches are mature (AbuJbara and Radev, 2012): F-scores are 0.96 for negation cue detection, and 0.89 for negation cue and scope detection (Velldal et al., 2012; Li et al., 2010). Fancellu et al. (2016) present the best results to date using CD-SCO, and analyze the main sources of errors. Outside BioScope and CD-SCO, Reitan et al. (2015) present a negation scope detector for tweets, and show that it improves sentiment analysis. As shown in Section 2, scope detection is insufficient to reveal positive interpretations from negation. 3.2 Focus Annotation and Detection While focus of negation has been studied for decades in philosophy and linguistics (Section 2), corpora and automated tools are scarce. Blanco and Moldovan (2011) annotate focus of negation in the 3,993 negations marked with ARGM"
D16-1119,N06-2015,0,0.0216114,"l negations by manipulating syntactic dependencies (Section 4.1). Second, we ask annotators to score potential positive interpretations (Section 4.2). Positive interpretations and their scores are later used to learn models to rank potential interpretations automatically (Section 6). Generating potential interpretations deterministically prior to scoring them proved very beneficial. After pilot experiments, it became clear that asking annotators to propose positive interpretations complicates the annotation effort (lower agreements) as well as learning. We decided to work on top of OntoNotes (Hovy et al., 2006)2 instead of plain text or other corpora for several reasons. First, OntoNotes includes gold linguistic annotations such as part-of-speech tags, parse trees and semantic roles. Second, unlike BioScope, CD-SCO and PB-FOC (Section 3.2), OntoNotes includes sentences from several genres, e.g., newswire, broadcast news and conversations, magazines, the web. We transformed the parse trees in OntoNotes into syntactic dependencies using Stanford CoreNLP (Manning et al., 2014). 4.1 Manipulating Syntactic Dependencies to Generate Potential Positive Interpretations OntoNotes contains 63,918 sentences. An"
D16-1119,C10-1076,0,0.135941,"dical domain (Szarvas et al., 2008) and CDSCO (Morante and Daelemans, 2012). BioScope annotates negation cues and linguistic scopes exclusively in biomedical texts. CD-SCO annotates negation cues, scopes, and negated events or properties in selected Conan Doyle stories. There have been several supervised proposals to detect the scope of negation using BioScope and ¨ ur and Radev, 2009; Øvrelid et al., CD-SCO (Ozg¨ 2010). Automatic approaches are mature (AbuJbara and Radev, 2012): F-scores are 0.96 for negation cue detection, and 0.89 for negation cue and scope detection (Velldal et al., 2012; Li et al., 2010). Fancellu et al. (2016) present the best results to date using CD-SCO, and analyze the main sources of errors. Outside BioScope and CD-SCO, Reitan et al. (2015) present a negation scope detector for tweets, and show that it improves sentiment analysis. As shown in Section 2, scope detection is insufficient to reveal positive interpretations from negation. 3.2 Focus Annotation and Detection While focus of negation has been studied for decades in philosophy and linguistics (Section 2), corpora and automated tools are scarce. Blanco and Moldovan (2011) annotate focus of negation in the 3,993 neg"
D16-1119,P14-5010,0,0.00568046,"interpretations complicates the annotation effort (lower agreements) as well as learning. We decided to work on top of OntoNotes (Hovy et al., 2006)2 instead of plain text or other corpora for several reasons. First, OntoNotes includes gold linguistic annotations such as part-of-speech tags, parse trees and semantic roles. Second, unlike BioScope, CD-SCO and PB-FOC (Section 3.2), OntoNotes includes sentences from several genres, e.g., newswire, broadcast news and conversations, magazines, the web. We transformed the parse trees in OntoNotes into syntactic dependencies using Stanford CoreNLP (Manning et al., 2014). 4.1 Manipulating Syntactic Dependencies to Generate Potential Positive Interpretations OntoNotes contains 63,918 sentences. Annotating all positive interpretations from all negations is outside the scope of this paper. Instead, we target selected representative negations. Selecting Negations. We first select all verbal negations by retrieving all tokens whose syntactic head is a verb and dependency type neg.3 Then, we discard negations from sentences that contain two negations, conditionals, commas or questions. Finally, we dis2 We use the CoNLL-2011 Shared Task distribution (Pradhan et al.,"
D16-1119,S12-1035,1,0.916659,"Missing"
D16-1119,morante-daelemans-2012-conandoyle,0,0.240128,"Shallow approaches are usually application-specific. In sentiment and opinion analysis, negation has been reduced to marking as negated all words between a negation cue and the first punctuation mark (Pang et al., 2002), or within a five-word window of a negation cue (Hu and Liu, 2004). The examples throughout this paper show that these techniques are insufficient to reveal implicit positive interpretations. 3.1 Scope Annotations and Detection Scope of negation detection has received a lot of attention, mostly using two corpora: BioScope in the medical domain (Szarvas et al., 2008) and CDSCO (Morante and Daelemans, 2012). BioScope annotates negation cues and linguistic scopes exclusively in biomedical texts. CD-SCO annotates negation cues, scopes, and negated events or properties in selected Conan Doyle stories. There have been several supervised proposals to detect the scope of negation using BioScope and ¨ ur and Radev, 2009; Øvrelid et al., CD-SCO (Ozg¨ 2010). Automatic approaches are mature (AbuJbara and Radev, 2012): F-scores are 0.96 for negation cue detection, and 0.89 for negation cue and scope detection (Velldal et al., 2012; Li et al., 2010). Fancellu et al. (2016) present the best results to date u"
D16-1119,J12-2001,0,0.136731,"Missing"
D16-1119,C10-1155,0,0.0599682,"Missing"
D16-1119,J05-1004,0,0.551616,"rgue that the latter is preferable, as it yields a more specific interpretation and it entails the former: if some jobs last for decades, then something lasts for decades, but not the other way around. We use the term coarse-grained focus to refer to foci that include all tokens belonging to an argument of a verb (e.g., Most Jobs above), and fine-grained focus to refer to foci that do not (e.g., Most above). 3 Previous Work Within computational linguistics, approaches to process negation are shallow, or target scope and focus detection. Popular semantic representations such as semantic roles (Palmer et al., 2005; Baker et al., 1998) or AMR (Banarescu et al., 2013) do not reveal the positive interpretations we target in this paper. Shallow approaches are usually application-specific. In sentiment and opinion analysis, negation has been reduced to marking as negated all words between a negation cue and the first punctuation mark (Pang et al., 2002), or within a five-word window of a negation cue (Hu and Liu, 2004). The examples throughout this paper show that these techniques are insufficient to reveal implicit positive interpretations. 3.1 Scope Annotations and Detection Scope of negation detection ha"
D16-1119,W02-1011,0,0.0242707,"d fine-grained focus to refer to foci that do not (e.g., Most above). 3 Previous Work Within computational linguistics, approaches to process negation are shallow, or target scope and focus detection. Popular semantic representations such as semantic roles (Palmer et al., 2005; Baker et al., 1998) or AMR (Banarescu et al., 2013) do not reveal the positive interpretations we target in this paper. Shallow approaches are usually application-specific. In sentiment and opinion analysis, negation has been reduced to marking as negated all words between a negation cue and the first punctuation mark (Pang et al., 2002), or within a five-word window of a negation cue (Hu and Liu, 2004). The examples throughout this paper show that these techniques are insufficient to reveal implicit positive interpretations. 3.1 Scope Annotations and Detection Scope of negation detection has received a lot of attention, mostly using two corpora: BioScope in the medical domain (Szarvas et al., 2008) and CDSCO (Morante and Daelemans, 2012). BioScope annotates negation cues and linguistic scopes exclusively in biomedical texts. CD-SCO annotates negation cues, scopes, and negated events or properties in selected Conan Doyle stor"
D16-1119,W11-1901,0,0.119489,"g et al., 2014). 4.1 Manipulating Syntactic Dependencies to Generate Potential Positive Interpretations OntoNotes contains 63,918 sentences. Annotating all positive interpretations from all negations is outside the scope of this paper. Instead, we target selected representative negations. Selecting Negations. We first select all verbal negations by retrieving all tokens whose syntactic head is a verb and dependency type neg.3 Then, we discard negations from sentences that contain two negations, conditionals, commas or questions. Finally, we dis2 We use the CoNLL-2011 Shared Task distribution (Pradhan et al., 2011), http://conll.cemantix.org/2011/ 3 The Stanford manual describes and exemplifies all syntactic dependencies (de Marneffe and Manning, 2008). 1111 card negations if the negated verb is to be or it does not have a subject (dependency nsubj or nsubjpass). Converting Negated Statements into their positive counterparts. We apply 3 steps inspired after the grammatical rules to form negation detailed by Huddleston and Pullum (2002, Ch. 9): 1. Remove the negation mark by deleting the token with syntactic dependency neg. 2. Remove auxiliaries, expand contractions, and fix third-person singular and pas"
D16-1119,W15-2914,0,0.114889,"Missing"
D16-1119,S12-1039,0,0.168449,"Missing"
D16-1119,W08-0606,0,0.212996,"tations we target in this paper. Shallow approaches are usually application-specific. In sentiment and opinion analysis, negation has been reduced to marking as negated all words between a negation cue and the first punctuation mark (Pang et al., 2002), or within a five-word window of a negation cue (Hu and Liu, 2004). The examples throughout this paper show that these techniques are insufficient to reveal implicit positive interpretations. 3.1 Scope Annotations and Detection Scope of negation detection has received a lot of attention, mostly using two corpora: BioScope in the medical domain (Szarvas et al., 2008) and CDSCO (Morante and Daelemans, 2012). BioScope annotates negation cues and linguistic scopes exclusively in biomedical texts. CD-SCO annotates negation cues, scopes, and negated events or properties in selected Conan Doyle stories. There have been several supervised proposals to detect the scope of negation using BioScope and ¨ ur and Radev, 2009; Øvrelid et al., CD-SCO (Ozg¨ 2010). Automatic approaches are mature (AbuJbara and Radev, 2012): F-scores are 0.96 for negation cue detection, and 0.89 for negation cue and scope detection (Velldal et al., 2012; Li et al., 2010). Fancellu et al. ("
D16-1119,J12-2005,0,0.149431,"ra: BioScope in the medical domain (Szarvas et al., 2008) and CDSCO (Morante and Daelemans, 2012). BioScope annotates negation cues and linguistic scopes exclusively in biomedical texts. CD-SCO annotates negation cues, scopes, and negated events or properties in selected Conan Doyle stories. There have been several supervised proposals to detect the scope of negation using BioScope and ¨ ur and Radev, 2009; Øvrelid et al., CD-SCO (Ozg¨ 2010). Automatic approaches are mature (AbuJbara and Radev, 2012): F-scores are 0.96 for negation cue detection, and 0.89 for negation cue and scope detection (Velldal et al., 2012; Li et al., 2010). Fancellu et al. (2016) present the best results to date using CD-SCO, and analyze the main sources of errors. Outside BioScope and CD-SCO, Reitan et al. (2015) present a negation scope detector for tweets, and show that it improves sentiment analysis. As shown in Section 2, scope detection is insufficient to reveal positive interpretations from negation. 3.2 Focus Annotation and Detection While focus of negation has been studied for decades in philosophy and linguistics (Section 2), corpora and automated tools are scarce. Blanco and Moldovan (2011) annotate focus of negatio"
D16-1119,P14-1049,0,0.282515,"Missing"
D16-1119,C98-1013,0,\N,Missing
D17-1244,W14-2907,0,0.0218769,"Missing"
D17-1244,P15-1034,0,0.0252207,"anizations and locations is the core goal of the task of information extraction. A few competitions have served as evaluation benchmarks (Grishman and Sundheim, 1996; Doddington et al., 2004; Kulick et al., 2014; Surdeanu and Heng, 2014), and include interpersonal relationships such as BUSINESS, SPOUSE and CHILDREN. Aguilar et al. (2014) compare several evaluations, and automated approaches to relation extraction—also referred to as link prediction and knowledge base completion— include (Yu and Lam, 2010; Nguyen et al., 2016; West et al., 2014). Open information extraction (Wu and Weld, 2010; Angeli et al., 2015) has emerged as an unsupervised domain-independent approach to extract relations. Regardless of details, all these previous efforts extract explicit relations, and do not attempt to characterize instances of relations with dimensions. Besides extracting relations per se, there have been efforts within computational linguistics involving interpersonal relationships. Voskarides et al. (2015) extract human-readable descriptions of relations in a knowledge graph by ranking sentences that justify the relations. Iyyer et al. (2016) propose an unsupervised algorithm to extract relationship trajectori"
D17-1244,W12-3626,0,0.692157,"all these previous efforts extract explicit relations, and do not attempt to characterize instances of relations with dimensions. Besides extracting relations per se, there have been efforts within computational linguistics involving interpersonal relationships. Voskarides et al. (2015) extract human-readable descriptions of relations in a knowledge graph by ranking sentences that justify the relations. Iyyer et al. (2016) propose an unsupervised algorithm to extract relationship trajectories of fictional characters, i.e., how interpersonal relationships evolve over time in fictional stories. Bracewell et al. (2012) introduce 9 social acts (e.g., agreement, undermining) designed to characterize relationships between individuals exhibiting adversarial and collegial behavior (similar to our cooperative vs. competitive dimension). Researchers have studied from a computational perspective how people communicate with each other. For example, Danescu-Niculescu-Mizil et al. (2012) study how power differences affects language style in online communities, and Prabhakaran and Rambow (2014) present a classifier to detect power relationships in email threads. Similarly, Gilbert (2012) explores how people in hierarch"
D17-1244,P11-1078,0,0.122542,"ning) designed to characterize relationships between individuals exhibiting adversarial and collegial behavior (similar to our cooperative vs. competitive dimension). Researchers have studied from a computational perspective how people communicate with each other. For example, Danescu-Niculescu-Mizil et al. (2012) study how power differences affects language style in online communities, and Prabhakaran and Rambow (2014) present a classifier to detect power relationships in email threads. Similarly, Gilbert (2012) explores how people in hierarchical relationships communicate through email, and Bramsen et al. (2011) focus on identifying power relationships in social networks. Politeness in online forums has also been studied (Danescu-Niculescu-Mizil et al., 2013). While power (similar to our equal vs. hierarchical dimension, Section 3) and politeness could be considered dimensions, these works exploit structural and linguistic features derived from communications between two individuals. Unlike all of them, we extract 9 dimensions of interpersonal relationships from sentences describing an event involving two people, and without needing language samples from them. 3 Dimensions of Interpersonal Relationsh"
D17-1244,P13-1025,0,0.0607717,"tive vs. competitive dimension). Researchers have studied from a computational perspective how people communicate with each other. For example, Danescu-Niculescu-Mizil et al. (2012) study how power differences affects language style in online communities, and Prabhakaran and Rambow (2014) present a classifier to detect power relationships in email threads. Similarly, Gilbert (2012) explores how people in hierarchical relationships communicate through email, and Bramsen et al. (2011) focus on identifying power relationships in social networks. Politeness in online forums has also been studied (Danescu-Niculescu-Mizil et al., 2013). While power (similar to our equal vs. hierarchical dimension, Section 3) and politeness could be considered dimensions, these works exploit structural and linguistic features derived from communications between two individuals. Unlike all of them, we extract 9 dimensions of interpersonal relationships from sentences describing an event involving two people, and without needing language samples from them. 3 Dimensions of Interpersonal Relationships Dimensions of interpersonal relationships have been studied for decades outside of computational linguistics, mostly in psychology and social scie"
D17-1244,doddington-etal-2004-automatic,0,0.311162,"mental results show that given a pair of people, values to dimensions can be assigned automatically. 1 Introduction The task of information extraction (IE) consists in creating structured representations from unstructured text. These representations usually consist of relations explicitly stated in text, and involve two or more arguments. For example, IE systems would extract SPOUSE(John, Mary ) or MAR RIED (John, Mary, 1994 ) from John and Mary have been married since 1994. IE systems have a long history, and became popular after evaluations such as MUC (Grishman and Sundheim, 1996) and ACE (Doddington et al., 2004). Traditional IE systems are supervised and extract relations defined before training takes place (Peng and McCallum, 2004). More recently, open IE systems have been proposed to extract all relations explicitly stated in text in an unsupervised manner and without defining relations a priori (Mausam et al., 2012). Regarding interpersonal relations—relations that take as arguments two people—both IE approaches extract relations such as RELATIVE, FRIEND and COMMU NICATES WITH . Open IE systems are domain independent and would extract, in principle, relations such as CLASSMATES and ADVISOR from st"
D17-1244,C96-1079,0,0.759471,"ions can be annotated reliably. Experimental results show that given a pair of people, values to dimensions can be assigned automatically. 1 Introduction The task of information extraction (IE) consists in creating structured representations from unstructured text. These representations usually consist of relations explicitly stated in text, and involve two or more arguments. For example, IE systems would extract SPOUSE(John, Mary ) or MAR RIED (John, Mary, 1994 ) from John and Mary have been married since 1994. IE systems have a long history, and became popular after evaluations such as MUC (Grishman and Sundheim, 1996) and ACE (Doddington et al., 2004). Traditional IE systems are supervised and extract relations defined before training takes place (Peng and McCallum, 2004). More recently, open IE systems have been proposed to extract all relations explicitly stated in text in an unsupervised manner and without defining relations a priori (Mausam et al., 2012). Regarding interpersonal relations—relations that take as arguments two people—both IE approaches extract relations such as RELATIVE, FRIEND and COMMU NICATES WITH . Open IE systems are domain independent and would extract, in principle, relations such"
D17-1244,N06-2015,0,0.0447896,"ationship is spatially distant (or distant). 4 Building a Corpus of Dimensions of Interpersonal Relationships Existing corpora annotating relations (Section 2) only consider selected interpersonal relationships and do not target dimensions. Our goal is to target dimensions of interpersonal relationships between any two individuals, from weak links (e.g., journalists interviewing celebrities) to strong ties (e.g., close friends). Thus, we create a corpus1 by first retrieving pairs of people, and then annotating dimensions for their relationships. We decided to add our annotations to OntoNotes (Hovy et al., 2006). Doing so has several advantages. First, OntoNotes contains texts from several domains and genres (e.g., conversational telephone speech, weblogs, broadcast), thus we not only work with newspaper articles. Second, OntoNotes includes part-ofspeech tags, named entities and coreference chains, three annotation layers that allow us to streamline the corpus creation process. 4.1 Retrieving Pairs of People We retrieve pairs of people within each sentence in OntoNotes following four steps: 2309 1 Available at http://hilt.cse.unt.edu/ Figure 1: Frequencies of the top 20 most frequent verbs after retr"
D17-1244,N16-1180,0,0.161141,"Missing"
D17-1244,W14-2904,0,0.168925,"detached Ref. [1] [1] [1] [1] [2] [3] [4] New New Table 1: Dimensions of interpersonal relationships targeted in this paper. [1] stands for (Wish et al., 1976), [2] for (Kelley, 2013), [3] for (Adamopoulos, 2012), and [4] for (Deutsch, 2011). New indicates a dimension discovered after analyzing several examples and pilot annotations. 2 Related Work Extracting relations between entities such as people, organizations and locations is the core goal of the task of information extraction. A few competitions have served as evaluation benchmarks (Grishman and Sundheim, 1996; Doddington et al., 2004; Kulick et al., 2014; Surdeanu and Heng, 2014), and include interpersonal relationships such as BUSINESS, SPOUSE and CHILDREN. Aguilar et al. (2014) compare several evaluations, and automated approaches to relation extraction—also referred to as link prediction and knowledge base completion— include (Yu and Lam, 2010; Nguyen et al., 2016; West et al., 2014). Open information extraction (Wu and Weld, 2010; Angeli et al., 2015) has emerged as an unsupervised domain-independent approach to extract relations. Regardless of details, all these previous efforts extract explicit relations, and do not attempt to character"
D17-1244,D12-1048,0,0.0185442,"two or more arguments. For example, IE systems would extract SPOUSE(John, Mary ) or MAR RIED (John, Mary, 1994 ) from John and Mary have been married since 1994. IE systems have a long history, and became popular after evaluations such as MUC (Grishman and Sundheim, 1996) and ACE (Doddington et al., 2004). Traditional IE systems are supervised and extract relations defined before training takes place (Peng and McCallum, 2004). More recently, open IE systems have been proposed to extract all relations explicitly stated in text in an unsupervised manner and without defining relations a priori (Mausam et al., 2012). Regarding interpersonal relations—relations that take as arguments two people—both IE approaches extract relations such as RELATIVE, FRIEND and COMMU NICATES WITH . Open IE systems are domain independent and would extract, in principle, relations such as CLASSMATES and ADVISOR from students’ diaries or biographies of scientists. While useful for applications such as question answering (Yao and Van Durme, 2014), these dyadic relations only provide a generic understanding of the relationship between two people. For example, COMMUNICATES WITH may relate people who have an intense or superficial"
D17-1244,N16-1054,0,0.0635289,"lot annotations. 2 Related Work Extracting relations between entities such as people, organizations and locations is the core goal of the task of information extraction. A few competitions have served as evaluation benchmarks (Grishman and Sundheim, 1996; Doddington et al., 2004; Kulick et al., 2014; Surdeanu and Heng, 2014), and include interpersonal relationships such as BUSINESS, SPOUSE and CHILDREN. Aguilar et al. (2014) compare several evaluations, and automated approaches to relation extraction—also referred to as link prediction and knowledge base completion— include (Yu and Lam, 2010; Nguyen et al., 2016; West et al., 2014). Open information extraction (Wu and Weld, 2010; Angeli et al., 2015) has emerged as an unsupervised domain-independent approach to extract relations. Regardless of details, all these previous efforts extract explicit relations, and do not attempt to characterize instances of relations with dimensions. Besides extracting relations per se, there have been efforts within computational linguistics involving interpersonal relationships. Voskarides et al. (2015) extract human-readable descriptions of relations in a knowledge graph by ranking sentences that justify the relations"
D17-1244,P14-1090,0,0.0482144,"Missing"
D17-1244,C10-2160,0,0.103934,"al examples and pilot annotations. 2 Related Work Extracting relations between entities such as people, organizations and locations is the core goal of the task of information extraction. A few competitions have served as evaluation benchmarks (Grishman and Sundheim, 1996; Doddington et al., 2004; Kulick et al., 2014; Surdeanu and Heng, 2014), and include interpersonal relationships such as BUSINESS, SPOUSE and CHILDREN. Aguilar et al. (2014) compare several evaluations, and automated approaches to relation extraction—also referred to as link prediction and knowledge base completion— include (Yu and Lam, 2010; Nguyen et al., 2016; West et al., 2014). Open information extraction (Wu and Weld, 2010; Angeli et al., 2015) has emerged as an unsupervised domain-independent approach to extract relations. Regardless of details, all these previous efforts extract explicit relations, and do not attempt to characterize instances of relations with dimensions. Besides extracting relations per se, there have been efforts within computational linguistics involving interpersonal relationships. Voskarides et al. (2015) extract human-readable descriptions of relations in a knowledge graph by ranking sentences that"
D17-1244,N04-1042,0,0.0479822,"k of information extraction (IE) consists in creating structured representations from unstructured text. These representations usually consist of relations explicitly stated in text, and involve two or more arguments. For example, IE systems would extract SPOUSE(John, Mary ) or MAR RIED (John, Mary, 1994 ) from John and Mary have been married since 1994. IE systems have a long history, and became popular after evaluations such as MUC (Grishman and Sundheim, 1996) and ACE (Doddington et al., 2004). Traditional IE systems are supervised and extract relations defined before training takes place (Peng and McCallum, 2004). More recently, open IE systems have been proposed to extract all relations explicitly stated in text in an unsupervised manner and without defining relations a priori (Mausam et al., 2012). Regarding interpersonal relations—relations that take as arguments two people—both IE approaches extract relations such as RELATIVE, FRIEND and COMMU NICATES WITH . Open IE systems are domain independent and would extract, in principle, relations such as CLASSMATES and ADVISOR from students’ diaries or biographies of scientists. While useful for applications such as question answering (Yao and Van Durme,"
D17-1244,P14-2056,0,0.134758,"ract relationship trajectories of fictional characters, i.e., how interpersonal relationships evolve over time in fictional stories. Bracewell et al. (2012) introduce 9 social acts (e.g., agreement, undermining) designed to characterize relationships between individuals exhibiting adversarial and collegial behavior (similar to our cooperative vs. competitive dimension). Researchers have studied from a computational perspective how people communicate with each other. For example, Danescu-Niculescu-Mizil et al. (2012) study how power differences affects language style in online communities, and Prabhakaran and Rambow (2014) present a classifier to detect power relationships in email threads. Similarly, Gilbert (2012) explores how people in hierarchical relationships communicate through email, and Bramsen et al. (2011) focus on identifying power relationships in social networks. Politeness in online forums has also been studied (Danescu-Niculescu-Mizil et al., 2013). While power (similar to our equal vs. hierarchical dimension, Section 3) and politeness could be considered dimensions, these works exploit structural and linguistic features derived from communications between two individuals. Unlike all of them, we"
D17-1244,P15-1055,0,0.230033,"Missing"
D17-1244,P10-1013,0,0.0234118,"such as people, organizations and locations is the core goal of the task of information extraction. A few competitions have served as evaluation benchmarks (Grishman and Sundheim, 1996; Doddington et al., 2004; Kulick et al., 2014; Surdeanu and Heng, 2014), and include interpersonal relationships such as BUSINESS, SPOUSE and CHILDREN. Aguilar et al. (2014) compare several evaluations, and automated approaches to relation extraction—also referred to as link prediction and knowledge base completion— include (Yu and Lam, 2010; Nguyen et al., 2016; West et al., 2014). Open information extraction (Wu and Weld, 2010; Angeli et al., 2015) has emerged as an unsupervised domain-independent approach to extract relations. Regardless of details, all these previous efforts extract explicit relations, and do not attempt to characterize instances of relations with dimensions. Besides extracting relations per se, there have been efforts within computational linguistics involving interpersonal relationships. Voskarides et al. (2015) extract human-readable descriptions of relations in a knowledge graph by ranking sentences that justify the relations. Iyyer et al. (2016) propose an unsupervised algorithm to extract r"
D18-1251,J08-4004,0,0.184738,"Missing"
D18-1251,L16-1592,0,0.129219,"Missing"
D18-1251,N18-1046,1,0.810154,"y propose a set of 18 relations, e.g. temporal (e.g., [today]x ’s [rates]y ), extent (e.g., [6 hours]y ’ [drive]x ). Their controller / owner / user relation (one relation with three aliases) is the closest relation to the possession relations we target in this paper. Extracting semantic relations between noun compounds (Nakov and Hearst, 2013; Tratz and Hovy, 2010) usually includes extracting possession relations, e.g., [family]x [estate]y . These previous works extract all semantic relations—including possessions— between arguments that follow a syntactic construction. In our previous work (Chinnappa and Blanco, 2018), we identify possession relations between a deterministically chosen person (possessor) and a concrete object (possessee) within a sentence. If a possession relation exists, we also identify the possession type (alienable or control). Finally, we temporally anchor the possession relation with respect to the verb of which the possessor is the subject. In this paper, we take a complementary approach. We start with text relevant to the possessee of interest—specifically, its Wikipedia article— and then extract its possessors without any restrictions beyond considering as possessors only named en"
D18-1251,P14-1135,0,0.0320012,"d in the form of years. 240 210 180 150 120 90 60 30 0 Before During After 400 350 300 250 200 150 100 50 0 4 All Figure 6: Distribution of yes label per article for all possessors and years. 3.5 Second, we only consider four digits within a named entity as temporal information. This means that temporal information encoded in relative dates (e.g., four years later) or historical events (e.g. after World War II) is disregarded. Additionally, we cannot distinguish between several possessors within a year, finer-grained times would be required to do so. To address these issues, temporal parsers (Lee et al., 2014; Str¨otgen and Gertz, 2015) and anchoring events in time (Reimers et al., 2016) are required. DATE Limitations While the proposed procedure successfully identifies possession relations over time, we acknowledge limitations in both the possessors and temporal information considered. First, we only consider named entities as potential possessors, so it is possible we miss some possessors (e.g., pronouns, the artist, his son). Because of the source documents we work with (Wikipedia articles about artwork) and the fact that we pair all potential possessors and years within a section, this is not"
D18-1251,P14-5010,0,0.00256281,"content of the corresponding Wikipedia articles. Some of the selected artworks are The Third of May 1808, Philosopher in Meditation, and Saturn Devouring His Son. The final corpus has 88 articles because we discarded articles if we could not select at least three (potential possessor, year) pairs (see below). Selecting Potential Possessors and Years Once possessees and their Wikipedia articles were selected, we identified potential possessors and years following the five steps below for each section in each Wikipedia article: 1. Run the named entity recognizers in spaCy3 and Stanford CoreNLP (Manning et al., 2014). 2 http://en.most-famous-paintings.com, http://remliel.com/2016/07/08/100greatest-paintings-of-all-time 3 https://spacy.io/ 120 80 160 20 0 the annotation process (is the potential possessor an actual possessor with respect to the years?) and analyze the resulting corpus. 3.2 640 100 Table 1: Counts of potential possessors (x) and years (y), and (x, y) pairs selected for annotation. 3.1 140 80 Unique Unique possessors (x) years (y) 0 Unique pairs (x, y) Figure 2: Distribution of unique potential possessors, unique years and unique pairs per article. Each boxplot displays the minimum, first qu"
D18-1251,D14-1162,0,0.0812285,"Missing"
D18-1251,W11-1901,0,0.0921247,"Missing"
D18-1251,P16-1207,0,0.0271507,"400 350 300 250 200 150 100 50 0 4 All Figure 6: Distribution of yes label per article for all possessors and years. 3.5 Second, we only consider four digits within a named entity as temporal information. This means that temporal information encoded in relative dates (e.g., four years later) or historical events (e.g. after World War II) is disregarded. Additionally, we cannot distinguish between several possessors within a year, finer-grained times would be required to do so. To address these issues, temporal parsers (Lee et al., 2014; Str¨otgen and Gertz, 2015) and anchoring events in time (Reimers et al., 2016) are required. DATE Limitations While the proposed procedure successfully identifies possession relations over time, we acknowledge limitations in both the possessors and temporal information considered. First, we only consider named entities as potential possessors, so it is possible we miss some possessors (e.g., pronouns, the artist, his son). Because of the source documents we work with (Wikipedia articles about artwork) and the fact that we pair all potential possessors and years within a section, this is not a big issue: most Wikipedia sections do not have mentions that cannot be resolve"
D18-1251,D15-1063,0,0.0643539,"Missing"
D18-1251,P10-1070,0,0.273313,"uistics Within computational linguistics, possession relations have been mostly studied as one of the many relations encoded in a given syntactic construction. For example, Tratz and Hovy (2013) extract semantic relations within English possessives. They propose a set of 18 relations, e.g. temporal (e.g., [today]x ’s [rates]y ), extent (e.g., [6 hours]y ’ [drive]x ). Their controller / owner / user relation (one relation with three aliases) is the closest relation to the possession relations we target in this paper. Extracting semantic relations between noun compounds (Nakov and Hearst, 2013; Tratz and Hovy, 2010) usually includes extracting possession relations, e.g., [family]x [estate]y . These previous works extract all semantic relations—including possessions— between arguments that follow a syntactic construction. In our previous work (Chinnappa and Blanco, 2018), we identify possession relations between a deterministically chosen person (possessor) and a concrete object (possessee) within a sentence. If a possession relation exists, we also identify the possession type (alienable or control). Finally, we temporally anchor the possession relation with respect to the verb of which the possessor is"
D18-1251,P13-1037,0,0.380976,"ters. For example, Stassen (2009) considers two parameters (permanent contact and control) and Heine (1997) defines five parameters (human possessor, concrete possessee, spatial proximity, temporal permanence, and control). While we do not closely follow any of these previous works, we borrow from them the broad definition of possession relations, and the motivation to work with possessions over time. 2.2 Computational Linguistics Within computational linguistics, possession relations have been mostly studied as one of the many relations encoded in a given syntactic construction. For example, Tratz and Hovy (2013) extract semantic relations within English possessives. They propose a set of 18 relations, e.g. temporal (e.g., [today]x ’s [rates]y ), extent (e.g., [6 hours]y ’ [drive]x ). Their controller / owner / user relation (one relation with three aliases) is the closest relation to the possession relations we target in this paper. Extracting semantic relations between noun compounds (Nakov and Hearst, 2013; Tratz and Hovy, 2010) usually includes extracting possession relations, e.g., [family]x [estate]y . These previous works extract all semantic relations—including possessions— between arguments t"
D18-1470,P15-1034,0,0.0265995,"ate the task using only information derived from language usage. Information extraction systems target, among others, relationships between people. There have been many evaluations (Grishman and Sundheim, 1996; Doddington et al., 2004; Kulick et al., 2014; Surdeanu and Heng, 2014), and there are two main approaches. Traditionally, relationships are defined before training takes place (e.g., PAR ENT , FRIENDS ), and systems are trained using supervised machine learning (Yu and Lam, 2010; Nguyen et al., 2016; West et al., 2014). On the other hand, open information extraction (Wu and Weld, 2010; Angeli et al., 2015) has emerged as an unsupervised domain-independent approach to extract relations. Regardless of details, these previous works extract explicit relationships and do not attempt to characterize instances of relationships with dimensions. Additionally, they do not distinguish between interactions and relationships. 3 Interpersonal Interactions and Relationships In this paper, we work with transcripts of conversations and define interaction and relationship as follows. An interaction between two people x and y exists for each conversation turn by either x or y referring to the other person. A rela"
D18-1470,J08-4004,0,0.207726,"Missing"
D18-1470,W12-3626,0,0.0289003,"pages 4395–4404 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics 2 Previous Work Within natural language processing, there have been several recent efforts working with relationships between people. Voskarides et al. (2015) extract human-readable descriptions of relations in a knowledge graph. Unlike the work presented here, they experiment with a proprietary knowledge graph and rely heavily on features extracted from the graph. Iyyer et al. (2016) propose an unsupervised algorithm to extract relationship trajectories of fictional characters. Bracewell et al. (2012) introduce social acts (e.g., agreement, undermining) designed to characterize relationships exhibiting adversarial and collegial behavior (similar to our cooperative vs. competitive dimension). None of these works distinguish between interactions and relationships, characterize interactions and relationships with dimensions, or consider all interactions between two people. In our previous work, we characterize interpersonal relationships with dimensions (Rashid and Blanco, 2017). In this paper, we improve upon our previous effort as follows. First, we distinguish between interpersonal interac"
D18-1470,P11-1078,0,0.067358,"us effort as follows. First, we distinguish between interpersonal interactions and relations. Second, we work with dialogues thus the same people interact with each other many times. There have been a few studies on analyzing language usage when people communicate. For example, Danescu-Niculescu-Mizil et al. (2012) study how power differences affect language style in online communities, and Prabhakaran and Rambow (2014) present a classifier to detect power relationships in email threads. Similarly, Gilbert (2012) explores how people in hierarchical relationships communicate through email, and Bramsen et al. (2011) focus on identifying power relationships in social networks. Politeness in online forums has also been studied (Danescu-NiculescuMizil et al., 2013). While power (similar to our equal vs. hierarchical dimension, Section 3) and politeness could be considered dimensions, these works exploit structural and linguistic features derived from communications between two individuals. Unlike all of them, we distinguish between and characterize interactions and relationships, and automate the task using only information derived from language usage. Information extraction systems target, among others, re"
D18-1470,W16-3612,0,0.0643609,"relationship is intimate if both people are emotionally close and warm to each other. Otherwise, the relationship in unintimate. Temporary vs. Enduring. A relationship is temporary if it lasts less than a day. A relationship is enduring if it last over a month. Otherwise (if it lasts more than a day and less than a month), this dimension is undefined. Annotating Dimensions of Interactions and Relationships Annotating dimensions of interactions and relationships requires a corpus in which the same people interact several times. We augment an existing corpus of scripts from the TV show Friends (Chen and Choi, 2016). More specifically, we work with the 24 episodes from Season 1 because they: • contain a large number of conversation turns (9,168, see counts per episode in Table 2); • involve many characters (42 characters speak at least 100 conservation turns, see the characters that interact the most in Table 2, and the full list in the supplementary materials); 4397 4.2 Annotation Process The annotations were done one episode at a time. Annotators were presented with the full transcript of the episode including speaker informa1 https://github.com/emorynlp/ character-mining 2 http://www.cse.unt.edu/˜blan"
D18-1470,P13-1025,0,0.159285,"Missing"
D18-1470,C96-1079,0,0.464792,"Politeness in online forums has also been studied (Danescu-NiculescuMizil et al., 2013). While power (similar to our equal vs. hierarchical dimension, Section 3) and politeness could be considered dimensions, these works exploit structural and linguistic features derived from communications between two individuals. Unlike all of them, we distinguish between and characterize interactions and relationships, and automate the task using only information derived from language usage. Information extraction systems target, among others, relationships between people. There have been many evaluations (Grishman and Sundheim, 1996; Doddington et al., 2004; Kulick et al., 2014; Surdeanu and Heng, 2014), and there are two main approaches. Traditionally, relationships are defined before training takes place (e.g., PAR ENT , FRIENDS ), and systems are trained using supervised machine learning (Yu and Lam, 2010; Nguyen et al., 2016; West et al., 2014). On the other hand, open information extraction (Wu and Weld, 2010; Angeli et al., 2015) has emerged as an unsupervised domain-independent approach to extract relations. Regardless of details, these previous works extract explicit relationships and do not attempt to characteri"
D18-1470,D16-1057,0,0.029026,"s for all labels, recall that the label distribution is biased (Figure 1). Feature Set. We use a combination of features extracted directly from the conversation turn, sentiment lexica and context. Specifically, we extract (a) the first word in the conversation turn, (b) bag-of-words features (binary flags and tf-idf scores), and (c) the root verb, and flags indicating the presence of exclamation, question marks and negation cues from (Morante and Daelemans, 2012) (other). Regarding sentiment, we extract flags indicating whether the turn has a positive, negative or neutral word in the list by Hamilton et al. (2016), the sentiment score of the turn (summation of sentiment scores per token over number of tokens in the turn), and a flag indicating whether the turn contains a negative word from the list by Hu and Liu (2004). Regarding context, we extract bag-of-words features from the previous conversation turn in which the same people interact (not necessarily the preceding turn). 6.1 Results Table 6 shows the overall results (average of all dimensions) obtained with the majority baseline and several feature combinations. All feature combinations outperform the baseline. Sentiment features are not benefici"
D18-1470,N16-1180,0,0.0696757,"Missing"
D18-1470,C04-1188,0,0.039366,"Missing"
D18-1470,W14-2904,0,0.0236209,"escu-NiculescuMizil et al., 2013). While power (similar to our equal vs. hierarchical dimension, Section 3) and politeness could be considered dimensions, these works exploit structural and linguistic features derived from communications between two individuals. Unlike all of them, we distinguish between and characterize interactions and relationships, and automate the task using only information derived from language usage. Information extraction systems target, among others, relationships between people. There have been many evaluations (Grishman and Sundheim, 1996; Doddington et al., 2004; Kulick et al., 2014; Surdeanu and Heng, 2014), and there are two main approaches. Traditionally, relationships are defined before training takes place (e.g., PAR ENT , FRIENDS ), and systems are trained using supervised machine learning (Yu and Lam, 2010; Nguyen et al., 2016; West et al., 2014). On the other hand, open information extraction (Wu and Weld, 2010; Angeli et al., 2015) has emerged as an unsupervised domain-independent approach to extract relations. Regardless of details, these previous works extract explicit relationships and do not attempt to characterize instances of relationships with dimensions."
D18-1470,morante-daelemans-2012-conandoyle,0,0.381986,"Missing"
D18-1470,N16-1054,0,0.017506,"duals. Unlike all of them, we distinguish between and characterize interactions and relationships, and automate the task using only information derived from language usage. Information extraction systems target, among others, relationships between people. There have been many evaluations (Grishman and Sundheim, 1996; Doddington et al., 2004; Kulick et al., 2014; Surdeanu and Heng, 2014), and there are two main approaches. Traditionally, relationships are defined before training takes place (e.g., PAR ENT , FRIENDS ), and systems are trained using supervised machine learning (Yu and Lam, 2010; Nguyen et al., 2016; West et al., 2014). On the other hand, open information extraction (Wu and Weld, 2010; Angeli et al., 2015) has emerged as an unsupervised domain-independent approach to extract relations. Regardless of details, these previous works extract explicit relationships and do not attempt to characterize instances of relationships with dimensions. Additionally, they do not distinguish between interactions and relationships. 3 Interpersonal Interactions and Relationships In this paper, we work with transcripts of conversations and define interaction and relationship as follows. An interaction betwee"
D18-1470,P14-2056,0,0.163408,"sions, or consider all interactions between two people. In our previous work, we characterize interpersonal relationships with dimensions (Rashid and Blanco, 2017). In this paper, we improve upon our previous effort as follows. First, we distinguish between interpersonal interactions and relations. Second, we work with dialogues thus the same people interact with each other many times. There have been a few studies on analyzing language usage when people communicate. For example, Danescu-Niculescu-Mizil et al. (2012) study how power differences affect language style in online communities, and Prabhakaran and Rambow (2014) present a classifier to detect power relationships in email threads. Similarly, Gilbert (2012) explores how people in hierarchical relationships communicate through email, and Bramsen et al. (2011) focus on identifying power relationships in social networks. Politeness in online forums has also been studied (Danescu-NiculescuMizil et al., 2013). While power (similar to our equal vs. hierarchical dimension, Section 3) and politeness could be considered dimensions, these works exploit structural and linguistic features derived from communications between two individuals. Unlike all of them, we"
D18-1470,H01-1054,0,0.13325,"ple interact with each other and as a result form relationships. These relationships range from weak (e.g., John talking to a waiter to order a drink) to strong (e.g., John and his best friend discussing career options). Traditionally, information extraction systems target, among others, relationships between people, e.g., PARENT , SIBLING , OTHER - PERSONAL, OTHER PROFESSIONAL (Doddington et al., 2004). Extracting a label describing the general relationship between two entities—often called relation type—is useful for tasks such as summarization (Jijkoun et al., 2004) and question answering (White et al., 2001). Only assigning a relation type, however, does not account for nuances in the relationship between two individuals. First, a relationship can be characterized beyond a relation type. For example, people having a PRO FESSIONAL relationship may be spatially near or distant (working at the same or different offices), and have an equal or hierarchical relationship (two software developers or a developer and the CEO). Second, relationships are defined by multiple interactions, and the fine-grained characteristics of interactions do not necessarily mirror the characteristics of the corresponding re"
D18-1470,P10-1013,0,0.0363136,"ionships, and automate the task using only information derived from language usage. Information extraction systems target, among others, relationships between people. There have been many evaluations (Grishman and Sundheim, 1996; Doddington et al., 2004; Kulick et al., 2014; Surdeanu and Heng, 2014), and there are two main approaches. Traditionally, relationships are defined before training takes place (e.g., PAR ENT , FRIENDS ), and systems are trained using supervised machine learning (Yu and Lam, 2010; Nguyen et al., 2016; West et al., 2014). On the other hand, open information extraction (Wu and Weld, 2010; Angeli et al., 2015) has emerged as an unsupervised domain-independent approach to extract relations. Regardless of details, these previous works extract explicit relationships and do not attempt to characterize instances of relationships with dimensions. Additionally, they do not distinguish between interactions and relationships. 3 Interpersonal Interactions and Relationships In this paper, we work with transcripts of conversations and define interaction and relationship as follows. An interaction between two people x and y exists for each conversation turn by either x or y referring to th"
D18-1470,C10-2160,0,0.0265243,"between two individuals. Unlike all of them, we distinguish between and characterize interactions and relationships, and automate the task using only information derived from language usage. Information extraction systems target, among others, relationships between people. There have been many evaluations (Grishman and Sundheim, 1996; Doddington et al., 2004; Kulick et al., 2014; Surdeanu and Heng, 2014), and there are two main approaches. Traditionally, relationships are defined before training takes place (e.g., PAR ENT , FRIENDS ), and systems are trained using supervised machine learning (Yu and Lam, 2010; Nguyen et al., 2016; West et al., 2014). On the other hand, open information extraction (Wu and Weld, 2010; Angeli et al., 2015) has emerged as an unsupervised domain-independent approach to extract relations. Regardless of details, these previous works extract explicit relationships and do not attempt to characterize instances of relationships with dimensions. Additionally, they do not distinguish between interactions and relationships. 3 Interpersonal Interactions and Relationships In this paper, we work with transcripts of conversations and define interaction and relationship as follows."
D18-1470,D17-1244,1,0.640787,"Iyyer et al. (2016) propose an unsupervised algorithm to extract relationship trajectories of fictional characters. Bracewell et al. (2012) introduce social acts (e.g., agreement, undermining) designed to characterize relationships exhibiting adversarial and collegial behavior (similar to our cooperative vs. competitive dimension). None of these works distinguish between interactions and relationships, characterize interactions and relationships with dimensions, or consider all interactions between two people. In our previous work, we characterize interpersonal relationships with dimensions (Rashid and Blanco, 2017). In this paper, we improve upon our previous effort as follows. First, we distinguish between interpersonal interactions and relations. Second, we work with dialogues thus the same people interact with each other many times. There have been a few studies on analyzing language usage when people communicate. For example, Danescu-Niculescu-Mizil et al. (2012) study how power differences affect language style in online communities, and Prabhakaran and Rambow (2014) present a classifier to detect power relationships in email threads. Similarly, Gilbert (2012) explores how people in hierarchical re"
D18-1470,P15-1055,0,0.0592161,"Missing"
D19-1061,J08-4004,0,0.115434,"Missing"
D19-1061,L18-1242,0,0.169563,"events identified in an image as an additional textual input. This allows us to leverage pretrained word embeddings and recurrent neural networks, a strategy that we prove beneficial. Previous Work Possession relations have primarily been studied in efforts targeting large relation repositories between arguments connected with some lexicosyntactic pattern. Tratz and Hovy (2013) work with 17 semantic relations realized by possessive constructions, Badulescu and Moldovan (2009) with 36 relations realized by genitives, and Nakov and Hearst (2013) and Tratz and Hovy (2010) target noun compounds. Blodgett and Schneider (2018) annotate 50 supersenses (including roles and relations between entities) for possessives. These efforts extract possessions from text, and target possessors and possessees connected by specific patterns. Unlike them, we extract possessions using both text and images. In addition to possession existence, we also extract types, temporal anchors, and interest in the possessee. Two recent efforts target possession relation extraction from text without strict syntactic constraints. In our previous work, we extract intrasentential possessions from OntoNotes (Chinnappa and Blanco, 2018). In the work"
D19-1061,N18-1046,1,0.842437,"compounds. Blodgett and Schneider (2018) annotate 50 supersenses (including roles and relations between entities) for possessives. These efforts extract possessions from text, and target possessors and possessees connected by specific patterns. Unlike them, we extract possessions using both text and images. In addition to possession existence, we also extract types, temporal anchors, and interest in the possessee. Two recent efforts target possession relation extraction from text without strict syntactic constraints. In our previous work, we extract intrasentential possessions from OntoNotes (Chinnappa and Blanco, 2018). In the work described here, we use the list of synsets from our previous work to select possessees (Section 3). Banea and Mihalcea (2018) work with blogs and annotate possession existence at the time of utterance. Unlike these previous works, we (a) leverage both 1 3 A Corpus of Possession Relations We start with a collection of English tweets consisting of text and images (Hu et al., 2018). First, we discard tweets that do not contain I, me, my, or mine in order to maximize the amount of tweets published by individuals and avoid tweets by organizations as well as advertisements. Second, we"
D19-1061,W16-6208,0,0.0524645,"Missing"
D19-1061,N18-1078,0,0.0472119,"tain more substantial improvements incorporating the objects and events identified in an image as an additional textual input and leveraging word embeddings. 2 text and images, (b) work with informal tweets (instead of standard English), (c) temporally anchor possessions before, during and after the tweet timestamp, and (d) also extract whether somebody has an interest in a concrete object regardless of possession existence. Using multiple modalities (e.g., text and images) to better solve some task is not new. Among many others, Specia et al. (2016) propose multimodal machine translation and Moon et al. (2018) show that named entity recognition benefits from taking into account both text and images. Our innovation is twofold. First, we show that humans understand more possession information when they have access to the image accompanying a text, as opposed to only reporting improvements on (automatically) solving some task. Second, our neural image component includes two subcomponents. The first one—weights from InceptionNet—is common in previous work, but the second one is novel. Specifically, the second component considers the objects and events identified in an image as an additional textual inp"
D19-1061,D14-1162,0,0.0852497,"e or control— into before yes or before no, during yes or during no, and after yes or after no. Finally, the sixth classifier predicts interest in the possessee (interest yes or interest no).2 5.1 Neural Network Architecture Figure 4 shows the neural network architecture, which includes components for the text and image (above and below dotted line respectively). Text Component. The text component is an LSTM (Hochreiter and Schmidhuber, 1997). Each token is represented with the concatenation of three embeddings. The first two are Glove word embeddings pretrained with Common Crawl and Twitter (Pennington et al., 2014). The third embedding only takes two possible values (dark grey: possessee, white: non-possessee) and it is used to indicate the potential possessee. Only the addi2 3 Code available at dhivyachinnappa.com 669 https://cloud.google.com/vision/ yes never unk Macro Avg. alienable control Macro Avg. interest yes interest no Macro Avg. Maj. baseline P R F1 .46 1.0 .63 .00 .00 .00 .00 .00 .00 .15 .33 .21 .84 1.0 .91 .00 .00 .00 .52 .50 .46 .59 1.0 .74 .00 .00 .00 .30 .50 .37 NN only text P R F1 .70 .69 .69 .57 .55 .56 .57 .55 .56 .61 .60 .60 .84 .94 .89 .19 .07 .10 .52 .51 .50 .54 .39 .45 .64 .77 .70"
D19-1061,W16-2346,0,0.0699977,"Missing"
D19-1061,P10-1070,0,0.0770758,"he second component considers the objects and events identified in an image as an additional textual input. This allows us to leverage pretrained word embeddings and recurrent neural networks, a strategy that we prove beneficial. Previous Work Possession relations have primarily been studied in efforts targeting large relation repositories between arguments connected with some lexicosyntactic pattern. Tratz and Hovy (2013) work with 17 semantic relations realized by possessive constructions, Badulescu and Moldovan (2009) with 36 relations realized by genitives, and Nakov and Hearst (2013) and Tratz and Hovy (2010) target noun compounds. Blodgett and Schneider (2018) annotate 50 supersenses (including roles and relations between entities) for possessives. These efforts extract possessions from text, and target possessors and possessees connected by specific patterns. Unlike them, we extract possessions using both text and images. In addition to possession existence, we also extract types, temporal anchors, and interest in the possessee. Two recent efforts target possession relation extraction from text without strict syntactic constraints. In our previous work, we extract intrasentential possessions fro"
D19-1061,P13-1037,0,0.203031,"solving some task. Second, our neural image component includes two subcomponents. The first one—weights from InceptionNet—is common in previous work, but the second one is novel. Specifically, the second component considers the objects and events identified in an image as an additional textual input. This allows us to leverage pretrained word embeddings and recurrent neural networks, a strategy that we prove beneficial. Previous Work Possession relations have primarily been studied in efforts targeting large relation repositories between arguments connected with some lexicosyntactic pattern. Tratz and Hovy (2013) work with 17 semantic relations realized by possessive constructions, Badulescu and Moldovan (2009) with 36 relations realized by genitives, and Nakov and Hearst (2013) and Tratz and Hovy (2010) target noun compounds. Blodgett and Schneider (2018) annotate 50 supersenses (including roles and relations between entities) for possessives. These efforts extract possessions from text, and target possessors and possessees connected by specific patterns. Unlike them, we extract possessions using both text and images. In addition to possession existence, we also extract types, temporal anchors, and i"
E14-1016,J08-4004,0,0.0364456,"Missing"
E14-1016,P98-1013,0,0.477901,"Missing"
E14-1016,J12-4003,0,0.186285,"Missing"
E14-1016,bethard-etal-2008-building,0,0.064119,"Missing"
E14-1016,P11-1146,1,0.716211,"Missing"
E14-1016,W05-0620,0,0.651863,"Missing"
E14-1016,N10-1030,0,0.0696558,"Missing"
E14-1016,S10-1059,0,0.0276925,"Missing"
E14-1016,P10-1160,0,0.533142,"Missing"
E14-1016,S07-1003,0,0.0769354,"Missing"
E14-1016,W09-2415,0,0.0157265,"les of ‘introduced ’ indicate that NP1 is the THEME and ‘[in the 1930s]PP ’ the TIME. In this case, there is no connection beTable 3: Counts of selected PropBank semantic roles. Total number of predicates is 112,917. verb by verb basis in each frameset. For example, ARG2 is used to indicate “created-from, thing changed” with verb make and “entity exempted from” with verb exempt (Table 1). Unlike numbered arguments, modifiers share a common meaning across verbs (Table 2). Some modifiers are arguably not a semantic relation and are not present in most relation inventories (Tratz and Hovy, 2010; Hendrickx et al., 2009). For example, AM - NEG and AM - MOD signal the presence of negation and modals, e.g., [wo]AM - MOD [n’t]AM - NEG [go]v . For more information about PropBank annotations and examples, refer to the annotation guidelines.3 Inspecting PropBank annotations one can easily conclude that numbered arguments dominate the annotations and only a few modifiers are an3 http://verbs.colorado.edu/˜mpalmer/projects/ace/ PBguidelines.pdf 147 rs = {TIME , LOCATION, MANNER, CAUSE , PURPOSE }; foreach semantic role R′ (x′ , y) such that R′ ∈ rs do foreach verb x in the same sentence do generate potential implicit"
E14-1016,D08-1008,0,0.0224012,"Missing"
E14-1016,W05-0625,0,0.0391924,"Missing"
E14-1016,P13-1116,0,0.810702,"Missing"
E14-1016,W13-0114,0,0.0227615,"Missing"
E14-1016,meyers-etal-2004-annotating,0,0.130215,"Missing"
E14-1016,moldovan-blanco-2012-polaris,1,0.846444,"lapping sem rel ARG1 feature 51, overlapping head resigned feature 52, overlapping direct true Table 8: PropBank roles and values for features (50–52) when predicting potential implicit relation R (said, to pursue other interests ), labeled N . ARG 1 Hibben false LIBSVM (Chang and Lin, 2011). Parameters α and γ were tuned by grid search using 10-fold cross validation over training instances. Results are reported using features extracted from gold and automatic annotations. Gold annotations are taken directly from the Penn TreeBank and PropBank. Automatic annotations are obtained with Polaris (Moldovan and Blanco, 2012), a semantic parser that among others is trained with PropBank. Results using gold (automatic) annotations are obtained with a model trained with gold (automatic) annotations. Table 7: Feature values when deciding if R (succeeds, last summer ) can be inferred from the verb-argument structures in Figure 1. these features, detailed descriptions and examples are provided by Gildea and Jurafsky (2002). Features (17–52) are derived from the predicate structures of x and y and specially defined to infer implicit semantic relations. Features (17–31, 35– 49) are flags indicating the presence of semant"
E14-1016,P86-1004,0,0.340548,"Missing"
E14-1016,J05-1004,0,0.847625,"USE of the man undergoing some ‘change ’. A question answering system would benefit from detecting this relation when answering Why did he change? Extracting all semantic relations from text is a monumental task and is at the core of language understanding. In recent years, approaches that aim at extracting a subset of all relations have achieved great success. In particular, previous research (Carreras and M`arquez, 2005; Punyakanok et al., 2008; Che et al., 2010; Zapirain et al., 2010) focused on verb-argument structures, i.e., relations between a verb and its syntactic arguments. PropBank (Palmer et al., 2005) is the corpus of reference for verb-argument relations. However, relations between a verb and its syntactic arguments are only a fraction of the relations present in texts. Consider the statement [Mr. Brown]NP1 succeeds [Joseph W. Hibben, who retired last August]NP2 and its parse tree (Figure 1). Verbargument relations encode that NP1 is the AGENT and NP2 is the THEME of verb ‘succeeds ’ (PropBank uses labels ARG0 and ARG1 ). Any semantic relation between ‘succeeds ’ and concepts dominated in the parse tree by one of its syntactic arguments NP1 or NP2 , e.g., ‘succeeds ’ occurred after ‘last"
E14-1016,J08-2005,0,0.0529507,"Missing"
E14-1016,S10-1065,0,0.0259311,"Missing"
E14-1016,P10-1070,0,0.0263912,"d S-ADV the MANNER; roles of ‘introduced ’ indicate that NP1 is the THEME and ‘[in the 1930s]PP ’ the TIME. In this case, there is no connection beTable 3: Counts of selected PropBank semantic roles. Total number of predicates is 112,917. verb by verb basis in each frameset. For example, ARG2 is used to indicate “created-from, thing changed” with verb make and “entity exempted from” with verb exempt (Table 1). Unlike numbered arguments, modifiers share a common meaning across verbs (Table 2). Some modifiers are arguably not a semantic relation and are not present in most relation inventories (Tratz and Hovy, 2010; Hendrickx et al., 2009). For example, AM - NEG and AM - MOD signal the presence of negation and modals, e.g., [wo]AM - MOD [n’t]AM - NEG [go]v . For more information about PropBank annotations and examples, refer to the annotation guidelines.3 Inspecting PropBank annotations one can easily conclude that numbered arguments dominate the annotations and only a few modifiers are an3 http://verbs.colorado.edu/˜mpalmer/projects/ace/ PBguidelines.pdf 147 rs = {TIME , LOCATION, MANNER, CAUSE , PURPOSE }; foreach semantic role R′ (x′ , y) such that R′ ∈ rs do foreach verb x in the same sentence do ge"
E14-1016,S13-2001,0,0.0646688,"Missing"
E14-1016,P91-1003,0,0.836088,"Missing"
E14-1016,N10-1058,0,0.0256065,"Missing"
E14-1016,W09-2417,0,0.529564,"Missing"
E14-1016,S12-1001,0,0.116343,"Missing"
E14-1016,J93-2004,0,\N,Missing
E14-1016,H86-1011,0,\N,Missing
E14-1016,S10-1008,0,\N,Missing
E14-1016,C98-1013,0,\N,Missing
E14-1016,J02-3001,0,\N,Missing
E14-1016,S10-1006,0,\N,Missing
E17-1081,W12-3808,0,0.321916,"ified by a negation cue is 1,866 and 979. 4 Focus of Negation and Positive Interpretations. Identifying the focus of negation is equivalent to revealing positive interpretations—everything but the focus is actually positive. The definition of focus does not specify annotation guidelines, and most existing efforts are grounded on semantic roles. Blanco and Moldovan (2011) annotate focus on the negations marked with ARGM - NEG role in PropBank (Palmer et al., 2005). They select a single focus per negation, specifically, they select the role that reveals the “most useful [positive] information.” Anand and Martell (2012) refine these annotations and differentiate positive interpretations arising from focus identification, scalar implicature and neg-raising predicates. Blanco and Sarabi (2016) propose a similar approach that scores the likelihood of several potential foci per negation. The main limitations of all these previous works is that selecting as focus a semantic role is only suitable when the negation cue modifies a predicate, and roles often yield coarse-grained interpretations. Sarabi and Blanco (2016) bypass these drawbacks by working with syntactic dependencies to refine coarse-grained interpretat"
E17-1081,P16-1231,0,0.0252873,"he training set. 6.1 7 Experimental Results We perform two kinds of experiments. First, we score all potential positive interpretations automatically generated (Section 7.1). Second, we identify interpretations scored with the highest score, 5 out 5 (Section 7.2). We always build separate models for nouns and adjectives, and train with gold linguistic information (POS tags and dependencies). We report results on the test set using both gold and predicted linguistic information. For gold, we use the annotations provided with the CoNLL-2011 release, and for auto, we use the output of SyntaxNet (Andor et al., 2016). Feature Selection Table 4 lists the full feature set. We extract features from the negated token (noun or adjective), part-of-speech tags and dependency tree. Basic features are straightforward. They include the negation cue, and the word form and part-of-speech tag of the negated token. Path features are derived from the syntactic path between the subgraph selected as focus and the negated token or closest verb. If the negated token is a noun, we extract the path between the subgraph and the negated token in Scenario (1), and between the subgraph and the closest verb in Scenario (2) (Sectio"
E17-1081,S12-1040,0,0.0166785,"ut corpora with focus annotations are restricted to verbal negation, i.e., when the negation cue is grammatically associated with a verb. Scope of Negation. There are two main corpora with scope of negation annotations: BioScope in the medical domain (Szarvas et al., 2008) and CD-SCO (Morante and Daelemans, 2012). The annotations schemas differ substantially; CDSCO annotates negation cues, their scopes, and the negated events or properties. There have been several supervised proposals to detect the scope of negation using BioScope and CD-SCO (Morante and Daelemans, 2009; Velldal et al., 2012; Basile et al., 2012). Fancellu et al. (2016) present the best results to date with CD-SCO using neural networks. They also perform out-of-domain evaluation with new annotations on Wikipedia, and analyze the main sources of errors. Outside BioScope and CD-SCO, Reitan et al. (2015) present a scope detector for negation in tweets, and use it for sentiment analysis. As the examples throughout this paper show (e.g., Section 2), detecting the scope of negation is insufficient to reveal the positive interpretations we target in this work. 861 Figure 1: Most frequent nouns (left) and adjectives (right) tokens that are ne"
E17-1081,N06-2015,0,0.0367392,"Missing"
E17-1081,P11-1059,1,0.919575,"negation is insufficient to reveal the positive interpretations we target in this work. 861 Figure 1: Most frequent nouns (left) and adjectives (right) tokens that are negated (neg dependency) in OntoNotes. Total number of noun and adjective tokens modified by a negation cue is 1,866 and 979. 4 Focus of Negation and Positive Interpretations. Identifying the focus of negation is equivalent to revealing positive interpretations—everything but the focus is actually positive. The definition of focus does not specify annotation guidelines, and most existing efforts are grounded on semantic roles. Blanco and Moldovan (2011) annotate focus on the negations marked with ARGM - NEG role in PropBank (Palmer et al., 2005). They select a single focus per negation, specifically, they select the role that reveals the “most useful [positive] information.” Anand and Martell (2012) refine these annotations and differentiate positive interpretations arising from focus identification, scalar implicature and neg-raising predicates. Blanco and Sarabi (2016) propose a similar approach that scores the likelihood of several potential foci per negation. The main limitations of all these previous works is that selecting as focus a s"
E17-1081,P14-5010,0,0.00458363,"Missing"
E17-1081,N16-1169,1,0.795817,"ns—everything but the focus is actually positive. The definition of focus does not specify annotation guidelines, and most existing efforts are grounded on semantic roles. Blanco and Moldovan (2011) annotate focus on the negations marked with ARGM - NEG role in PropBank (Palmer et al., 2005). They select a single focus per negation, specifically, they select the role that reveals the “most useful [positive] information.” Anand and Martell (2012) refine these annotations and differentiate positive interpretations arising from focus identification, scalar implicature and neg-raising predicates. Blanco and Sarabi (2016) propose a similar approach that scores the likelihood of several potential foci per negation. The main limitations of all these previous works is that selecting as focus a semantic role is only suitable when the negation cue modifies a predicate, and roles often yield coarse-grained interpretations. Sarabi and Blanco (2016) bypass these drawbacks by working with syntactic dependencies to refine coarse-grained interpretations. Corpus Creation We create a corpus of negations and their positive interpretations following three steps. First, we select negations whose negation cue syntactically mod"
E17-1081,W09-1105,0,0.12689,"ith scope annotations for all types of negations, but corpora with focus annotations are restricted to verbal negation, i.e., when the negation cue is grammatically associated with a verb. Scope of Negation. There are two main corpora with scope of negation annotations: BioScope in the medical domain (Szarvas et al., 2008) and CD-SCO (Morante and Daelemans, 2012). The annotations schemas differ substantially; CDSCO annotates negation cues, their scopes, and the negated events or properties. There have been several supervised proposals to detect the scope of negation using BioScope and CD-SCO (Morante and Daelemans, 2009; Velldal et al., 2012; Basile et al., 2012). Fancellu et al. (2016) present the best results to date with CD-SCO using neural networks. They also perform out-of-domain evaluation with new annotations on Wikipedia, and analyze the main sources of errors. Outside BioScope and CD-SCO, Reitan et al. (2015) present a scope detector for negation in tweets, and use it for sentiment analysis. As the examples throughout this paper show (e.g., Section 2), detecting the scope of negation is insufficient to reveal the positive interpretations we target in this work. 861 Figure 1: Most frequent nouns (lef"
E17-1081,morante-daelemans-2012-conandoyle,0,0.248766,"a were (probably) allowed in a press area outside (but not far from) the venue. The former positive inter3 Previous Work Within computational linguistics, most approaches to process negation target scope or focus detection. Generally speaking, there are corpora with scope annotations for all types of negations, but corpora with focus annotations are restricted to verbal negation, i.e., when the negation cue is grammatically associated with a verb. Scope of Negation. There are two main corpora with scope of negation annotations: BioScope in the medical domain (Szarvas et al., 2008) and CD-SCO (Morante and Daelemans, 2012). The annotations schemas differ substantially; CDSCO annotates negation cues, their scopes, and the negated events or properties. There have been several supervised proposals to detect the scope of negation using BioScope and CD-SCO (Morante and Daelemans, 2009; Velldal et al., 2012; Basile et al., 2012). Fancellu et al. (2016) present the best results to date with CD-SCO using neural networks. They also perform out-of-domain evaluation with new annotations on Wikipedia, and analyze the main sources of errors. Outside BioScope and CD-SCO, Reitan et al. (2015) present a scope detector for nega"
E17-1081,W10-3110,0,0.295221,"Missing"
E17-1081,J12-2001,0,0.0574202,"Missing"
E17-1081,J05-1004,0,0.0557036,"1: Most frequent nouns (left) and adjectives (right) tokens that are negated (neg dependency) in OntoNotes. Total number of noun and adjective tokens modified by a negation cue is 1,866 and 979. 4 Focus of Negation and Positive Interpretations. Identifying the focus of negation is equivalent to revealing positive interpretations—everything but the focus is actually positive. The definition of focus does not specify annotation guidelines, and most existing efforts are grounded on semantic roles. Blanco and Moldovan (2011) annotate focus on the negations marked with ARGM - NEG role in PropBank (Palmer et al., 2005). They select a single focus per negation, specifically, they select the role that reveals the “most useful [positive] information.” Anand and Martell (2012) refine these annotations and differentiate positive interpretations arising from focus identification, scalar implicature and neg-raising predicates. Blanco and Sarabi (2016) propose a similar approach that scores the likelihood of several potential foci per negation. The main limitations of all these previous works is that selecting as focus a semantic role is only suitable when the negation cue modifies a predicate, and roles often yiel"
E17-1081,P16-1047,0,0.0696961,"annotations are restricted to verbal negation, i.e., when the negation cue is grammatically associated with a verb. Scope of Negation. There are two main corpora with scope of negation annotations: BioScope in the medical domain (Szarvas et al., 2008) and CD-SCO (Morante and Daelemans, 2012). The annotations schemas differ substantially; CDSCO annotates negation cues, their scopes, and the negated events or properties. There have been several supervised proposals to detect the scope of negation using BioScope and CD-SCO (Morante and Daelemans, 2009; Velldal et al., 2012; Basile et al., 2012). Fancellu et al. (2016) present the best results to date with CD-SCO using neural networks. They also perform out-of-domain evaluation with new annotations on Wikipedia, and analyze the main sources of errors. Outside BioScope and CD-SCO, Reitan et al. (2015) present a scope detector for negation in tweets, and use it for sentiment analysis. As the examples throughout this paper show (e.g., Section 2), detecting the scope of negation is insufficient to reveal the positive interpretations we target in this work. 861 Figure 1: Most frequent nouns (left) and adjectives (right) tokens that are negated (neg dependency) i"
E17-1081,W11-1901,0,0.0202908,"Missing"
E17-1081,W15-2914,0,0.0317089,"Missing"
E17-1081,D16-1119,1,0.648068,"egation, specifically, they select the role that reveals the “most useful [positive] information.” Anand and Martell (2012) refine these annotations and differentiate positive interpretations arising from focus identification, scalar implicature and neg-raising predicates. Blanco and Sarabi (2016) propose a similar approach that scores the likelihood of several potential foci per negation. The main limitations of all these previous works is that selecting as focus a semantic role is only suitable when the negation cue modifies a predicate, and roles often yield coarse-grained interpretations. Sarabi and Blanco (2016) bypass these drawbacks by working with syntactic dependencies to refine coarse-grained interpretations. Corpus Creation We create a corpus of negations and their positive interpretations following three steps. First, we select negations whose negation cue syntactically modifies either a noun or adjective. Second, we automatically generate potential positive from those negations by manipulating syntactic dependencies and part-of-speech tags. Third, we gather manual annotations to validate and score potential interpretations. While asking annotators to suggest positive interpretations would pot"
E17-1081,W08-0606,0,0.082898,"ed inside the venue, and that media were (probably) allowed in a press area outside (but not far from) the venue. The former positive inter3 Previous Work Within computational linguistics, most approaches to process negation target scope or focus detection. Generally speaking, there are corpora with scope annotations for all types of negations, but corpora with focus annotations are restricted to verbal negation, i.e., when the negation cue is grammatically associated with a verb. Scope of Negation. There are two main corpora with scope of negation annotations: BioScope in the medical domain (Szarvas et al., 2008) and CD-SCO (Morante and Daelemans, 2012). The annotations schemas differ substantially; CDSCO annotates negation cues, their scopes, and the negated events or properties. There have been several supervised proposals to detect the scope of negation using BioScope and CD-SCO (Morante and Daelemans, 2009; Velldal et al., 2012; Basile et al., 2012). Fancellu et al. (2016) present the best results to date with CD-SCO using neural networks. They also perform out-of-domain evaluation with new annotations on Wikipedia, and analyze the main sources of errors. Outside BioScope and CD-SCO, Reitan et al."
E17-1081,J12-2005,0,0.0224344,"types of negations, but corpora with focus annotations are restricted to verbal negation, i.e., when the negation cue is grammatically associated with a verb. Scope of Negation. There are two main corpora with scope of negation annotations: BioScope in the medical domain (Szarvas et al., 2008) and CD-SCO (Morante and Daelemans, 2012). The annotations schemas differ substantially; CDSCO annotates negation cues, their scopes, and the negated events or properties. There have been several supervised proposals to detect the scope of negation using BioScope and CD-SCO (Morante and Daelemans, 2009; Velldal et al., 2012; Basile et al., 2012). Fancellu et al. (2016) present the best results to date with CD-SCO using neural networks. They also perform out-of-domain evaluation with new annotations on Wikipedia, and analyze the main sources of errors. Outside BioScope and CD-SCO, Reitan et al. (2015) present a scope detector for negation in tweets, and use it for sentiment analysis. As the examples throughout this paper show (e.g., Section 2), detecting the scope of negation is insufficient to reveal the positive interpretations we target in this work. 861 Figure 1: Most frequent nouns (left) and adjectives (rig"
E17-1081,W10-3001,0,\N,Missing
L16-1604,P98-1013,0,0.519205,"derable tention meaning in the lastfrom decade. In particular, semantic roleatlatention in the decade. In on particular, semantic role corlabeling and last efforts focused spatial meaning—both beling and efforts focused on spatial meaning—both pora development and automatic tools—have become corpopporaular. development tools—have Semantic and rolesautomatic capture semantic links become between poppredular.icates Semantic rolesarguments; capture semantic linkswho between pred-to and their they capture did what icates and their arguments; they capture who did what to whom, how, when and where (Baker et al., 1998; Palmer whom, how, when and where (Baker et al., 1998; Palmer et al., 2005). Efforts targeting spatial meaning use specialrelations suchtargeting as TRAJECTOR LANDMARK (Kordet al.,ized 2005). Efforts spatialand meaning use specialal., as 2011; Kolomiyets al., 2013), or(Korddeﬁne ized jamshidi relationsetsuch TRAJECTOR andetLANDMARK subtasks such as identifying spatial elements spatial jamshidi et al., 2011; Kolomiyets et al., 2013), and or define signals (Pustejovsky et al., 2015). subtasks such as identifying spatial elements and spatial There are several etcorpora with semantic role annotat"
L16-1604,W11-0106,1,0.903407,"Missing"
L16-1604,P11-1146,1,0.87267,"Missing"
L16-1604,E14-1016,1,0.85885,"Missing"
L16-1604,N15-1048,1,0.298525,"ny restriction is 44,997. Table 4 shows the number of pairs (x, y) generated using several combinations of restrictions. After enforcing all restrictions, we generate 1,732 pairs; for each pair, we generate 3 questions to gather temporally-anchored spatial knowledge: • Is x located at y the day before yverb ? • Is x located at y during yverb ? • Is x located at y the day after yverb ? 3.2. #pairs 685 437 161 306 143 1,732 Crowdsourcing Annotations Once potential additional spatial knowledge is generated via simple plain English questions, it is time to gather answers. After pilot annotations (Blanco and Vempala, 2015), it became clear that it is suboptimal to force annotators to answer YES, NO or UNKNOWN—often times there is evidence that something is (or is not) located somewhere, but it is difficult to fully commit. Inspired by previous work (Saur´ı and Pustejovsky, 2012), we considered 6 labels: • certYES: I am certain that the answer is yes. • probYES: The answer is probably yes, but I am unsure. • certNO: I am certain that the answer is no. • probNO: The answer is probably no, but I am unsure. • UNK: There is not enough information to choose one of the labels above. • INV: The question is invalid, I c"
L16-1604,W05-0620,0,0.0315671,"Missing"
L16-1604,P12-1012,0,0.0446597,"Missing"
L16-1604,P10-1160,0,0.0390872,"Missing"
L16-1604,J02-3001,0,0.262372,"Missing"
L16-1604,W09-1201,0,0.0862537,"Missing"
L16-1604,N06-2015,0,0.207828,",ized 2005). Efforts spatialand meaning use specialal., as 2011; Kolomiyets al., 2013), or(Korddeﬁne ized jamshidi relationsetsuch TRAJECTOR andetLANDMARK subtasks such as identifying spatial elements spatial jamshidi et al., 2011; Kolomiyets et al., 2013), and or define signals (Pustejovsky et al., 2015). subtasks such as identifying spatial elements and spatial There are several etcorpora with semantic role annotations, signals (Pustejovsky al., 2015). e.g., (Baker et al., semantic 1998), PropBank (Palmer et There areFrameNet several corpora with role annotations, al., 2005), and OntoNotes (Hovy et al., 2006). While see.g., FrameNet (Baker et al., 1998), PropBank (Palmer et mantic roles are useful, there is much more meaning in all al., 2005), and OntoNotes (Hovy et al., 2006). While sebut the simplest statements. Consider the sentence John mantic roles are useful, there is much more meaning in all drove to San Francisco for a doctor’s appointment and the but the simplest sentence John semantic rolesstatements. annotated in Consider OntoNotesthe (Figure 1, solid ardrove to San Francisco for a doctor’s appointment and rows). On top of these valuable semantic roles, one the can semantic rolesJohn an"
L16-1604,S13-2044,0,0.0714649,"Missing"
L16-1604,P13-1116,0,0.0267058,"Missing"
L16-1604,W04-2705,0,0.106491,"Missing"
L16-1604,J05-1004,0,0.196967,"Missing"
L16-1604,W11-1901,0,0.0714461,"Missing"
L16-1604,W09-2417,0,0.0612603,"Missing"
L16-1604,J12-2002,0,0.0529234,"Missing"
L16-1604,S10-1008,0,\N,Missing
L16-1604,C98-1013,0,\N,Missing
L18-1052,P16-1231,0,0.0180661,"code basic information regarding argument x, location y, x verb and y verb. NE features categorize the argument x and location y based on their named entity types. Syntax features capture dependency structure of x and y. Semantic features add information regarding spatial and temporal roles. 6. Results We performed experiments using gold-standard linguistic annotations as well as predicted linguistic annotations. The gold POS tags, parse-trees, semantic roles, dependencies and named entities are taken from the CoNLL release and the predicted linguistic information is obtained using SyntaxNet (Andor et al., 2016). The baseline systems predict the most frequent label per temporal anchor and obtain an overall F-score of 0.46 (0.29 for before, 0.49 for during and 0.29 for after). Results with coarse-grained and clustered fine-grained labels obtained with all features per temporal anchor using gold standard linguistic information are presented in Table 6. Models trained with all features perform best with respect to all temporal anchors. In general, results with before and during are better than results with after. Results with coarse-grained and clustered fine-grained labels obtained with all features pe"
L18-1052,N06-2015,0,0.0726113,"hodology to extract temporally-anchored spatial knowledge by manipulating syntactic dependencies, and a (2) crowdsourced corpus annotated with temporally-anchored spatial knowledge. The work presented here extends our previous work (Vempala and Blanco, 2016), which only manipulated semantic roles. We show that additional temporally-anchored spatial knowledge can be extracted by leveraging syntactic dependencies. We release a new corpus that annotates how long entities are and are not located somewhere, and temporally anchor this spatial information.1 2. Background We work on top of OntoNotes (Hovy et al., 2006) as it is a well known corpus with text from various domains. 1 Available at https://alakanandav.bitbucket.io/ Ontonotes contains over 64,000 sentences. It annotates, among other linguistic information, part-of-speech tags, parse trees, named entities and co-reference chains. We use the CoNLL- 2011 Shared Task distribution (Pradhan et al., 2011), and transform the gold parse trees into syntactic dependencies using Stanford CoreNLP (Manning et al., 2014). De Marneffe and Manning (2008) present and exemplify the Stanford dependencies, and Weischedel and Brunstein (2005) the named entity types us"
L18-1052,W15-1527,0,0.0311478,"anguage understanding. Efforts focused on spatial meaning—both corpora development and automatic tools—have become popular. Existing approaches to extract spatial knowledge usually focus on extracting locations of events, someone or something. For example, semantic role labeling (Palmer et al., 2005) determines who did what to whom, when and where, e.g., Thelma Gutierrez [went]verb [inside the forensic laboratory where scientist are trying to solve this mystery]ARG4 , where ARG4 indicates the END POINT of event went. Efforts targetting locations of entities include geo-locating Twitter users (Liu and Inkpen, 2015), and pairing companies with the location of their headquarters (Mintz et al., 2009) e.g., [IBM’s]company headquarters in [New York]location . Determining the temporal span where the spatial knowledge holds is not extensively researched. From the sentence John parked Jamie’s car at the Highland Garage, we can infer that John and the car are certainly located at the Highland Garage minutes before and during parking, and that John will leave shortly after parking whereas the car will be at the garage for a few days but not months. We can also infer that Jamie will probably be at the Highland Gar"
L18-1052,P14-5010,0,0.00354398,"how long entities are and are not located somewhere, and temporally anchor this spatial information.1 2. Background We work on top of OntoNotes (Hovy et al., 2006) as it is a well known corpus with text from various domains. 1 Available at https://alakanandav.bitbucket.io/ Ontonotes contains over 64,000 sentences. It annotates, among other linguistic information, part-of-speech tags, parse trees, named entities and co-reference chains. We use the CoNLL- 2011 Shared Task distribution (Pradhan et al., 2011), and transform the gold parse trees into syntactic dependencies using Stanford CoreNLP (Manning et al., 2014). De Marneffe and Manning (2008) present and exemplify the Stanford dependencies, and Weischedel and Brunstein (2005) the named entity types used in OntoNotes. We use the term temporally-anchored spatial knowledge to refer to information regarding whether a given x is or is not located at some location y, and for how long with respect to an event. We use the notation LOCATION(x, y) to indicate the spatial relation between x and y. We use the term potential spatial knowledge to refer to spatial relations LO CATION (x, y) that are yet to be validated. There are 2 types of relations LOCATION(x, y"
L18-1052,P09-1113,0,0.128182,"Missing"
L18-1052,J05-1004,0,0.109131,"s information. Crowdsourcing experiments show that spatial inferences are ubiquitous and intuitive, and experimental results show that they can be done automatically. Keywords: Semantics, Information extraction, Spatial knowledge 1. Introduction Extracting spatial meaning from text is of utmost importance in natural language understanding. Efforts focused on spatial meaning—both corpora development and automatic tools—have become popular. Existing approaches to extract spatial knowledge usually focus on extracting locations of events, someone or something. For example, semantic role labeling (Palmer et al., 2005) determines who did what to whom, when and where, e.g., Thelma Gutierrez [went]verb [inside the forensic laboratory where scientist are trying to solve this mystery]ARG4 , where ARG4 indicates the END POINT of event went. Efforts targetting locations of entities include geo-locating Twitter users (Liu and Inkpen, 2015), and pairing companies with the location of their headquarters (Mintz et al., 2009) e.g., [IBM’s]company headquarters in [New York]location . Determining the temporal span where the spatial knowledge holds is not extensively researched. From the sentence John parked Jamie’s car"
L18-1052,pan-etal-2006-annotated,0,0.0239534,"Missing"
L18-1052,W11-1901,0,0.0345089,"patial knowledge can be extracted by leveraging syntactic dependencies. We release a new corpus that annotates how long entities are and are not located somewhere, and temporally anchor this spatial information.1 2. Background We work on top of OntoNotes (Hovy et al., 2006) as it is a well known corpus with text from various domains. 1 Available at https://alakanandav.bitbucket.io/ Ontonotes contains over 64,000 sentences. It annotates, among other linguistic information, part-of-speech tags, parse trees, named entities and co-reference chains. We use the CoNLL- 2011 Shared Task distribution (Pradhan et al., 2011), and transform the gold parse trees into syntactic dependencies using Stanford CoreNLP (Manning et al., 2014). De Marneffe and Manning (2008) present and exemplify the Stanford dependencies, and Weischedel and Brunstein (2005) the named entity types used in OntoNotes. We use the term temporally-anchored spatial knowledge to refer to information regarding whether a given x is or is not located at some location y, and for how long with respect to an event. We use the notation LOCATION(x, y) to indicate the spatial relation between x and y. We use the term potential spatial knowledge to refer to"
L18-1052,P16-1142,1,0.92589,"ohn and the car are certainly located at the Highland Garage minutes before and during parking, and that John will leave shortly after parking whereas the car will be at the garage for a few days but not months. We can also infer that Jamie will probably be at the Highland Garage at some point after parking to pick up his car. This paper presents (1) a two-step methodology to extract temporally-anchored spatial knowledge by manipulating syntactic dependencies, and a (2) crowdsourced corpus annotated with temporally-anchored spatial knowledge. The work presented here extends our previous work (Vempala and Blanco, 2016), which only manipulated semantic roles. We show that additional temporally-anchored spatial knowledge can be extracted by leveraging syntactic dependencies. We release a new corpus that annotates how long entities are and are not located somewhere, and temporally anchor this spatial information.1 2. Background We work on top of OntoNotes (Hovy et al., 2006) as it is a well known corpus with text from various domains. 1 Available at https://alakanandav.bitbucket.io/ Ontonotes contains over 64,000 sentences. It annotates, among other linguistic information, part-of-speech tags, parse trees, nam"
L18-1280,P14-5010,0,0.00414322,"e downloaded over one million tweets published from California along with their metadata using the Twitter API.2 Then, we discarded tweets (a) consisting of less than 3 tokens, or (b) not containing at least one pronoun.3 We decided to work with these temporal tags because people usually tweet about what is happening, about what has happened recently, or about what is about to happen (Sanagavarapu et al., 2017). We allow annotators to choose from six labels inspired by previous work on factuality (Saur´ı and Pustejovsky, 2012): 2. We extracted named entities using spaCy4 and Stanford CoreNLP (Manning et al., 2014) after removing emoticons, URLs and newline characters from the original tweets. The corpus contains both the original tweet and the preprocessed version. • Certainly Yes (CY): I am certain that the author is located in the given location at the specified time. • Probably Yes (PY): I am not certain that the author is located in the given location at the specified time, but it is probably the case. 3. We identified locations in tweets if both Spacy and Stanford CoreNLP recognized a LOC or GPE named entity (location and geopolitical named entities respectively) spanning exactly the same tokens."
L18-1280,P17-2101,1,0.697519,"ting (After &lt; 24)? Selecting Tweets and Locations We collected tweets containing at least one location named entity following 4 steps: 5. at any point of time later than 24 hours after tweeting (After &gt; 24)? 1. We downloaded over one million tweets published from California along with their metadata using the Twitter API.2 Then, we discarded tweets (a) consisting of less than 3 tokens, or (b) not containing at least one pronoun.3 We decided to work with these temporal tags because people usually tweet about what is happening, about what has happened recently, or about what is about to happen (Sanagavarapu et al., 2017). We allow annotators to choose from six labels inspired by previous work on factuality (Saur´ı and Pustejovsky, 2012): 2. We extracted named entities using spaCy4 and Stanford CoreNLP (Manning et al., 2014) after removing emoticons, URLs and newline characters from the original tweets. The corpus contains both the original tweet and the preprocessed version. • Certainly Yes (CY): I am certain that the author is located in the given location at the specified time. • Probably Yes (PY): I am not certain that the author is located in the given location at the specified time, but it is probably th"
L18-1280,J12-2002,0,0.0709694,"Missing"
moldovan-blanco-2012-polaris,S07-1017,0,\N,Missing
moldovan-blanco-2012-polaris,S07-1008,0,\N,Missing
moldovan-blanco-2012-polaris,W08-2222,0,\N,Missing
moldovan-blanco-2012-polaris,H05-1112,1,\N,Missing
moldovan-blanco-2012-polaris,W08-2227,0,\N,Missing
moldovan-blanco-2012-polaris,H05-1047,1,\N,Missing
moldovan-blanco-2012-polaris,W09-2415,0,\N,Missing
moldovan-blanco-2012-polaris,W05-0620,0,\N,Missing
moldovan-blanco-2012-polaris,W03-1210,0,\N,Missing
moldovan-blanco-2012-polaris,D09-1001,0,\N,Missing
moldovan-blanco-2012-polaris,J02-3001,0,\N,Missing
moldovan-blanco-2012-polaris,S07-1003,0,\N,Missing
moldovan-blanco-2012-polaris,P10-1070,0,\N,Missing
moldovan-blanco-2012-polaris,P11-1146,1,\N,Missing
moldovan-blanco-2012-polaris,W09-2417,0,\N,Missing
moldovan-blanco-2012-polaris,balakrishna-etal-2010-semi,1,\N,Missing
N12-1050,P98-1013,0,0.0858955,"g corpora, BioScope annotates negation marks and linguistic scopes exclusively on biomedical texts. It does not annotate focus and it purposely disregards negations such as the reactions in NK3.3 cells are not always identical (Vincze et al., 2008), which carry the kind of positive meaning we aim at extracting (the reactions in NK3.3 cells are sometimes identical). Recently, Morante et al. (2011) present scope annotation in two Conan Doyle works, but they dismiss focus and positive meaning extraction. As stated before, PropBank (Palmer et al., 2005) treats negation superficially and FrameNet (Baker et al., 1998) regrettably disregards negation. Blanco and Moldovan (2011) introduce a semantic representation of negation using focus detection. They target verbal negation and work on top of PropBank, selecting as focus the role that corresponds to the focus of negation. Simply put, they propose that all roles but the one corresponding to the focus are actually positive. Their approach, however, has a major drawback: selecting the whole role often yields too coarse of a focus and the positive implicit meaning is not fully specified (Section 3.1). Focus-Sensitive Phenomena. The literature uses the term foc"
N12-1050,P11-1059,1,0.8915,"uistic scopes exclusively on biomedical texts. It does not annotate focus and it purposely disregards negations such as the reactions in NK3.3 cells are not always identical (Vincze et al., 2008), which carry the kind of positive meaning we aim at extracting (the reactions in NK3.3 cells are sometimes identical). Recently, Morante et al. (2011) present scope annotation in two Conan Doyle works, but they dismiss focus and positive meaning extraction. As stated before, PropBank (Palmer et al., 2005) treats negation superficially and FrameNet (Baker et al., 1998) regrettably disregards negation. Blanco and Moldovan (2011) introduce a semantic representation of negation using focus detection. They target verbal negation and work on top of PropBank, selecting as focus the role that corresponds to the focus of negation. Simply put, they propose that all roles but the one corresponding to the focus are actually positive. Their approach, however, has a major drawback: selecting the whole role often yields too coarse of a focus and the positive implicit meaning is not fully specified (Section 3.1). Focus-Sensitive Phenomena. The literature uses the term focus for widely distinct phenomena; space permits only a curso"
N12-1050,W10-3001,0,0.0842809,"Missing"
N12-1050,D09-1103,0,0.0505812,"Missing"
N12-1050,W00-0730,0,0.038542,"hin coarse-grained focus {affected, go, . . . } {VB, VBN, . . . } predicate text predicate POS tag 19 pred-word 20 pred-POS 21 sem-role 22 coarse-chunk (SBAR) {ARG1, MLOC, . . . } {B-CFG, I-CFG, O} semantic role this token belongs to wrt coarse-verb coarse-grained annotation using BIO Table 6: Feature set used to predict fine-grained focus of negation. If a feature is especially useful for a particular syntactic node, we indicate so between parenthesis in the right hand side of column 1 (otherwise it is useful for all). 5.2 Experiments and Results We have carried our experiments using Yamcha (Kudoh and Matsumoto, 2000), a generic, customizable, and open source text chunker1 implemented using TinySVM2 . Following Yamcha’s design, we distinguish between static and dynamic features. Static features are the ones depicted in Table 6 for a fixed size window. Dynamic features are the predicted classes for a fixed set of previous instances. Whereas values for static features are considered correct, values for dynamic features are predictions of previous instances and therefore may contain errors. Varying window size effectively varies the number of features considered, the larger the window the more local context i"
N12-1050,C10-1076,0,0.0785113,"Computational Linguistics ments and negative concords; concepts such as intra and inter-domain negation and strength of negation (Ladusaw, 1996), syntactic and semantic negation (L¨obner, 2000) have been discussed in the extensive literature, although we do not use them. In computational linguistics, negation has mainly drawn attention in sentiment analysis (Wilson et al., 2009; Wiegand et al., 2010) and the biomedical domain. Recently, two events (Morante and Sporleder, 2010; Farkas et al., 2010) targeted negation mostly on those subfields. Among many others, Morante and Daelemans (2009) and Li et al. (2010) propose scope detectors using the BioScope corpus. Considering scope is indeed a step forward, but focus must also be taken into account to represent negated statements and detect their positive implicit meanings. Regarding corpora, BioScope annotates negation marks and linguistic scopes exclusively on biomedical texts. It does not annotate focus and it purposely disregards negations such as the reactions in NK3.3 cells are not always identical (Vincze et al., 2008), which carry the kind of positive meaning we aim at extracting (the reactions in NK3.3 cells are sometimes identical). Recently,"
N12-1050,P11-1060,0,0.0273526,"knesses, shallow representations have been proven useful for several tasks, e.g., coreference resolution (Kong et al., 2009), machine translation (Wu and Fung, 2009). Consider statement (1) The company won’t ship the new product to the United States until next year. Existing approaches to represent the meaning of (1) either indicate that the verb ship is negated or disregard the negation altogether. Semantic role labelers trained over PropBank would link n’t to ship with MNEG (i.e., negate the verb); any system based on FrameNet and more recent unsupervised proposals (Poon and Domingos, 2009; Liang et al., 2011; Titov and Klementiev, 2011) ignore negation. In order to represent the meaning of (1), one must first ascertain that the negation mark n’t is actually negating the TEMPORAL context linked to ship and not the verb per se; more specifically, n’t is negating exclusively the preposition until. Only doing so one can aim at representing the actual meaning of (1): The company will ship the new product to the United States during next year. Note that the verb ship, and its AGENT, THEME and LOCATION (i.e., The company, the new product and to the United States) are positive, as well as the temporal an"
N12-1050,W09-1304,0,0.0273848,"3-8, 2012. 2012 Association for Computational Linguistics ments and negative concords; concepts such as intra and inter-domain negation and strength of negation (Ladusaw, 1996), syntactic and semantic negation (L¨obner, 2000) have been discussed in the extensive literature, although we do not use them. In computational linguistics, negation has mainly drawn attention in sentiment analysis (Wilson et al., 2009; Wiegand et al., 2010) and the biomedical domain. Recently, two events (Morante and Sporleder, 2010; Farkas et al., 2010) targeted negation mostly on those subfields. Among many others, Morante and Daelemans (2009) and Li et al. (2010) propose scope detectors using the BioScope corpus. Considering scope is indeed a step forward, but focus must also be taken into account to represent negated statements and detect their positive implicit meanings. Regarding corpora, BioScope annotates negation marks and linguistic scopes exclusively on biomedical texts. It does not annotate focus and it purposely disregards negations such as the reactions in NK3.3 cells are not always identical (Vincze et al., 2008), which carry the kind of positive meaning we aim at extracting (the reactions in NK3.3 cells are sometimes"
N12-1050,J05-1004,0,0.0368834,"statements and detect their positive implicit meanings. Regarding corpora, BioScope annotates negation marks and linguistic scopes exclusively on biomedical texts. It does not annotate focus and it purposely disregards negations such as the reactions in NK3.3 cells are not always identical (Vincze et al., 2008), which carry the kind of positive meaning we aim at extracting (the reactions in NK3.3 cells are sometimes identical). Recently, Morante et al. (2011) present scope annotation in two Conan Doyle works, but they dismiss focus and positive meaning extraction. As stated before, PropBank (Palmer et al., 2005) treats negation superficially and FrameNet (Baker et al., 1998) regrettably disregards negation. Blanco and Moldovan (2011) introduce a semantic representation of negation using focus detection. They target verbal negation and work on top of PropBank, selecting as focus the role that corresponds to the focus of negation. Simply put, they propose that all roles but the one corresponding to the focus are actually positive. Their approach, however, has a major drawback: selecting the whole role often yields too coarse of a focus and the positive implicit meaning is not fully specified (Section 3"
N12-1050,D09-1001,0,0.0122931,"taphor. Despite these weaknesses, shallow representations have been proven useful for several tasks, e.g., coreference resolution (Kong et al., 2009), machine translation (Wu and Fung, 2009). Consider statement (1) The company won’t ship the new product to the United States until next year. Existing approaches to represent the meaning of (1) either indicate that the verb ship is negated or disregard the negation altogether. Semantic role labelers trained over PropBank would link n’t to ship with MNEG (i.e., negate the verb); any system based on FrameNet and more recent unsupervised proposals (Poon and Domingos, 2009; Liang et al., 2011; Titov and Klementiev, 2011) ignore negation. In order to represent the meaning of (1), one must first ascertain that the negation mark n’t is actually negating the TEMPORAL context linked to ship and not the verb per se; more specifically, n’t is negating exclusively the preposition until. Only doing so one can aim at representing the actual meaning of (1): The company will ship the new product to the United States during next year. Note that the verb ship, and its AGENT, THEME and LOCATION (i.e., The company, the new product and to the United States) are positive, as wel"
N12-1050,P11-1145,0,0.0178955,"resentations have been proven useful for several tasks, e.g., coreference resolution (Kong et al., 2009), machine translation (Wu and Fung, 2009). Consider statement (1) The company won’t ship the new product to the United States until next year. Existing approaches to represent the meaning of (1) either indicate that the verb ship is negated or disregard the negation altogether. Semantic role labelers trained over PropBank would link n’t to ship with MNEG (i.e., negate the verb); any system based on FrameNet and more recent unsupervised proposals (Poon and Domingos, 2009; Liang et al., 2011; Titov and Klementiev, 2011) ignore negation. In order to represent the meaning of (1), one must first ascertain that the negation mark n’t is actually negating the TEMPORAL context linked to ship and not the verb per se; more specifically, n’t is negating exclusively the preposition until. Only doing so one can aim at representing the actual meaning of (1): The company will ship the new product to the United States during next year. Note that the verb ship, and its AGENT, THEME and LOCATION (i.e., The company, the new product and to the United States) are positive, as well as the temporal anchor next year. Regardless of"
N12-1050,W00-0726,0,0.160365,"Missing"
N12-1050,W08-0606,0,0.126036,"orleder, 2010; Farkas et al., 2010) targeted negation mostly on those subfields. Among many others, Morante and Daelemans (2009) and Li et al. (2010) propose scope detectors using the BioScope corpus. Considering scope is indeed a step forward, but focus must also be taken into account to represent negated statements and detect their positive implicit meanings. Regarding corpora, BioScope annotates negation marks and linguistic scopes exclusively on biomedical texts. It does not annotate focus and it purposely disregards negations such as the reactions in NK3.3 cells are not always identical (Vincze et al., 2008), which carry the kind of positive meaning we aim at extracting (the reactions in NK3.3 cells are sometimes identical). Recently, Morante et al. (2011) present scope annotation in two Conan Doyle works, but they dismiss focus and positive meaning extraction. As stated before, PropBank (Palmer et al., 2005) treats negation superficially and FrameNet (Baker et al., 1998) regrettably disregards negation. Blanco and Moldovan (2011) introduce a semantic representation of negation using focus detection. They target verbal negation and work on top of PropBank, selecting as focus the role that corresp"
N12-1050,W10-3111,0,0.0177985,"of negative ele456 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 456–465, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics ments and negative concords; concepts such as intra and inter-domain negation and strength of negation (Ladusaw, 1996), syntactic and semantic negation (L¨obner, 2000) have been discussed in the extensive literature, although we do not use them. In computational linguistics, negation has mainly drawn attention in sentiment analysis (Wilson et al., 2009; Wiegand et al., 2010) and the biomedical domain. Recently, two events (Morante and Sporleder, 2010; Farkas et al., 2010) targeted negation mostly on those subfields. Among many others, Morante and Daelemans (2009) and Li et al. (2010) propose scope detectors using the BioScope corpus. Considering scope is indeed a step forward, but focus must also be taken into account to represent negated statements and detect their positive implicit meanings. Regarding corpora, BioScope annotates negation marks and linguistic scopes exclusively on biomedical texts. It does not annotate focus and it purposely disregards negations"
N12-1050,J09-3003,0,0.0474498,"the position and form of negative ele456 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 456–465, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics ments and negative concords; concepts such as intra and inter-domain negation and strength of negation (Ladusaw, 1996), syntactic and semantic negation (L¨obner, 2000) have been discussed in the extensive literature, although we do not use them. In computational linguistics, negation has mainly drawn attention in sentiment analysis (Wilson et al., 2009; Wiegand et al., 2010) and the biomedical domain. Recently, two events (Morante and Sporleder, 2010; Farkas et al., 2010) targeted negation mostly on those subfields. Among many others, Morante and Daelemans (2009) and Li et al. (2010) propose scope detectors using the BioScope corpus. Considering scope is indeed a step forward, but focus must also be taken into account to represent negated statements and detect their positive implicit meanings. Regarding corpora, BioScope annotates negation marks and linguistic scopes exclusively on biomedical texts. It does not annotate focus and it purpose"
N12-1050,N09-2004,0,0.0145766,"ng using focus of negation are presented. The concept of granularity of focus is introduced and justified. New annotation and features to detect fine-grained focus are discussed and results reported. 1 Introduction Semantic representation of text is an important step towards text understanding. Current approaches are based on relatively shallow representations and ignore pervasive linguistic phenomena such as negation and metaphor. Despite these weaknesses, shallow representations have been proven useful for several tasks, e.g., coreference resolution (Kong et al., 2009), machine translation (Wu and Fung, 2009). Consider statement (1) The company won’t ship the new product to the United States until next year. Existing approaches to represent the meaning of (1) either indicate that the verb ship is negated or disregard the negation altogether. Semantic role labelers trained over PropBank would link n’t to ship with MNEG (i.e., negate the verb); any system based on FrameNet and more recent unsupervised proposals (Poon and Domingos, 2009; Liang et al., 2011; Titov and Klementiev, 2011) ignore negation. In order to represent the meaning of (1), one must first ascertain that the negation mark n’t is act"
N12-1050,J03-4003,0,\N,Missing
N12-1050,C98-1013,0,\N,Missing
N15-1048,J08-4004,0,0.0179996,"Missing"
N15-1048,P98-1013,0,0.863701,"pBank-style semantic roles. We believe this is an advantage since PropBank is wellknown in the field and several tools to predict PropBank roles are documented and publicly available.3 The work presented here could be incorporated into any NLP pipeline after role labeling without modifications to other components. 2.1 PropBank and OntoNotes PropBank (Palmer et al., 2005) adds semantic role annotations on top of the parse trees of the Penn 1 Available at http://hilt.cse.unt.edu/ We use semantic role to refer to PropBank-style (verbal) semantic roles. NomBank (Meyers et al., 2004) and FrameNet (Baker et al., 1998) also annotate semantic roles. 3 E.g., http://cogcomp.cs.illinois.edu/page/ software, http://ml.nec-labs.com/senna/; 2 453 [Mr. Cray]ARG0 [will]ARGM-MOD [work]verb [for the Colorado Springs CO company]ARG2 [as an independent contractor]ARG1 . [I]ARG0 ’d [slept]verb [through my only previous brush with natural disaster]ARG2 , [. . . ] Table 1: Examples of PropBank annotations. ARGM - LOC : location extent ARGM - DIS: discourse connective ARGM - ADV : general-purpose ARGM - NEG : negation marker ARGM - MOD : modal verb ARGM - EXT : ARGM - CAU : cause time ARGM - PNC : purpose ARGM - MNR : manner"
N15-1048,P11-1146,1,0.65433,"Missing"
N15-1048,E14-1016,1,0.844097,"Missing"
N15-1048,W05-0620,0,0.412947,"Missing"
N15-1048,P12-1012,0,0.292108,"Missing"
N15-1048,P10-1160,0,0.392415,"Missing"
N15-1048,J12-4003,0,0.0435832,"Missing"
N15-1048,J02-3001,0,0.791903,"Missing"
N15-1048,J06-1005,0,0.0323404,"t the core of text understanding. Semantic relations encode semantic connections between words. For example, from (1) Bill couldn’t handle the pressure and quit yesterday, one could extract that the CAUSE of quit was the pressure. Doing so would help answering question Why did Bill quit? and determining that the pressure started before Bill quit. In the past years, computational semantics has received a significant boost. But extracting all semantic relations in text—even in single sentences—is still an elusive goal. Most existing approaches target either a single relation, e.g., PART- WHOLE (Girju et al., 2006), or relations that hold between arguments following some syntactic construction, e.g., possessives (Tratz and Hovy, 2013). Among the latter kind, the task of verbal semantic role labeling focuses on extracting semantic links exclusively between verbs and their arguments. PropBank (Palmer et al., 2005) is a popular corpus for this task, and tools to extract verbal semantic roles have been proposed for years (Carreras and M`arquez, 2005). Some semantic relations hold forever, e.g., the CAUSE of event quit in example (1) above is pressure. Discussing when this CAUSE holds is somewhat artificial:"
N15-1048,N06-2015,0,0.515143,"Missing"
N15-1048,S13-2044,0,0.267047,"Missing"
N15-1048,N10-1137,0,0.0549152,"Missing"
N15-1048,P13-1116,0,0.623398,"Missing"
N15-1048,W04-2705,0,0.5719,"nferring spatial knowledge from PropBank-style semantic roles. We believe this is an advantage since PropBank is wellknown in the field and several tools to predict PropBank roles are documented and publicly available.3 The work presented here could be incorporated into any NLP pipeline after role labeling without modifications to other components. 2.1 PropBank and OntoNotes PropBank (Palmer et al., 2005) adds semantic role annotations on top of the parse trees of the Penn 1 Available at http://hilt.cse.unt.edu/ We use semantic role to refer to PropBank-style (verbal) semantic roles. NomBank (Meyers et al., 2004) and FrameNet (Baker et al., 1998) also annotate semantic roles. 3 E.g., http://cogcomp.cs.illinois.edu/page/ software, http://ml.nec-labs.com/senna/; 2 453 [Mr. Cray]ARG0 [will]ARGM-MOD [work]verb [for the Colorado Springs CO company]ARG2 [as an independent contractor]ARG1 . [I]ARG0 ’d [slept]verb [through my only previous brush with natural disaster]ARG2 , [. . . ] Table 1: Examples of PropBank annotations. ARGM - LOC : location extent ARGM - DIS: discourse connective ARGM - ADV : general-purpose ARGM - NEG : negation marker ARGM - MOD : modal verb ARGM - EXT : ARGM - CAU : cause time ARGM -"
N15-1048,J05-1004,0,0.94685,"ng that the pressure started before Bill quit. In the past years, computational semantics has received a significant boost. But extracting all semantic relations in text—even in single sentences—is still an elusive goal. Most existing approaches target either a single relation, e.g., PART- WHOLE (Girju et al., 2006), or relations that hold between arguments following some syntactic construction, e.g., possessives (Tratz and Hovy, 2013). Among the latter kind, the task of verbal semantic role labeling focuses on extracting semantic links exclusively between verbs and their arguments. PropBank (Palmer et al., 2005) is a popular corpus for this task, and tools to extract verbal semantic roles have been proposed for years (Carreras and M`arquez, 2005). Some semantic relations hold forever, e.g., the CAUSE of event quit in example (1) above is pressure. Discussing when this CAUSE holds is somewhat artificial: at some point Bill quit, and he did so because of the pressure. But LOCATION and other semantic relations often do not hold forever. For example, while buildings typically have one location during their existence, people and objects such as cars and books do not: they participate in events and as a re"
N15-1048,W09-2417,0,0.56256,"Missing"
N15-1048,J12-2002,0,0.0811736,"Missing"
N15-1048,P13-1037,0,0.025256,"ll couldn’t handle the pressure and quit yesterday, one could extract that the CAUSE of quit was the pressure. Doing so would help answering question Why did Bill quit? and determining that the pressure started before Bill quit. In the past years, computational semantics has received a significant boost. But extracting all semantic relations in text—even in single sentences—is still an elusive goal. Most existing approaches target either a single relation, e.g., PART- WHOLE (Girju et al., 2006), or relations that hold between arguments following some syntactic construction, e.g., possessives (Tratz and Hovy, 2013). Among the latter kind, the task of verbal semantic role labeling focuses on extracting semantic links exclusively between verbs and their arguments. PropBank (Palmer et al., 2005) is a popular corpus for this task, and tools to extract verbal semantic roles have been proposed for years (Carreras and M`arquez, 2005). Some semantic relations hold forever, e.g., the CAUSE of event quit in example (1) above is pressure. Discussing when this CAUSE holds is somewhat artificial: at some point Bill quit, and he did so because of the pressure. But LOCATION and other semantic relations often do not ho"
N15-1048,C98-1013,0,\N,Missing
N15-1048,W09-1201,0,\N,Missing
N15-1048,S12-1048,0,\N,Missing
N16-1169,S12-1043,0,0.222231,"Missing"
N16-1169,S13-1004,0,0.0279289,"atement, e.g., That TV is not small, it is tiny. In this paper, we target verbal, analytic, clausal and both ordinary and metalinguistic negation. 2.1 Positive Interpretations. In philosophy and linguistics, it is generally accepted that negation conveys positive meanings (Horn, 1989). These positive meanings range from implicatures, i.e., what is suggested in an utterance even though neither expressed nor strictly implied (Blackburn, 2008), to entailments. Other terms used in the literature include implied meanings (Mitkov, 2005), implied alternatives (Rooth, 1985) and semantically similars (Agirre et al., 2013). We do not strictly fit into any of this terminology, we reveal positive interpretations as intuitively done by humans when reading text. 1432 Scope and Focus. From a theoretical perspective, it is accepted that negation has scope and focus, and that the focus— not just the scope—yields positive interpretations (Horn, 1989; Rooth, 1992; Taglicht, 1984). Scope is “the part of the meaning that is negated” and focus “the part of the scope that is most prominently or explicitly negated” (Huddleston and Pullum, 2002). Consider the following statement in the context of the recent refuge crisis: (3)"
N16-1169,W12-3808,0,0.226549,"Missing"
N16-1169,P98-1013,0,0.0678154,"oci yield unlikely interpretations, e.g., Somebody was looking for heaven in Europe, but not Mr. Haile (3b, AGENT), Mr. Haile was looking for heaven somewhere, but not in Europe (3d, LOCA TION ). Note that (1) scope on its own does not yield positive interpretations, and (2) some negated statements convey several likely positive interpretations, e.g., statement (1) in Section 5, Table 3. 3 Previous Work Within computational linguistics, approaches to process negation are shallow, or target scope and focus detection. Popular semantic representations such as semantic roles (Palmer et al., 2005; Baker et al., 1998) or AMR (Banarescu et al., 2013) do not reveal the positive interpretations we target in this paper. Shallow approaches are usually application-specific. In sentiment and opinion analysis, negation has been reduced to marking as negated all words between a negation cue and the first punctuation mark (Pang et al., 2002), or within a five-word window of a negation cue (Hu and Liu, 2004). The examples throughout this paper show that these techniques are insufficient to reveal implicit positive interpretations. 3.1 Scope Annotation and Detection Scope of negation detection has received a lot of at"
N16-1169,W13-2322,0,0.0299809,"ations, e.g., Somebody was looking for heaven in Europe, but not Mr. Haile (3b, AGENT), Mr. Haile was looking for heaven somewhere, but not in Europe (3d, LOCA TION ). Note that (1) scope on its own does not yield positive interpretations, and (2) some negated statements convey several likely positive interpretations, e.g., statement (1) in Section 5, Table 3. 3 Previous Work Within computational linguistics, approaches to process negation are shallow, or target scope and focus detection. Popular semantic representations such as semantic roles (Palmer et al., 2005; Baker et al., 1998) or AMR (Banarescu et al., 2013) do not reveal the positive interpretations we target in this paper. Shallow approaches are usually application-specific. In sentiment and opinion analysis, negation has been reduced to marking as negated all words between a negation cue and the first punctuation mark (Pang et al., 2002), or within a five-word window of a negation cue (Hu and Liu, 2004). The examples throughout this paper show that these techniques are insufficient to reveal implicit positive interpretations. 3.1 Scope Annotation and Detection Scope of negation detection has received a lot of attention, mostly using two corpor"
N16-1169,P11-1059,1,0.922391,"are mature (AbuJbara and Radev, 2012): F-scores are 0.96 for negation cue detection, and 0.89 for negation cue and scope detection (Velldal et al., 2012; Li et al., 2010). Outside BioScope and CD-SCO, Reitan et al. (2015) present a negation scope detector for tweets, and show that it improves sentiment analysis. As shown in Section 2, scope detection is insufficient to reveal positive interpretations from negated statements. 3.2 Focus Annotation and Detection While focus of negation has been studied for decades in philosophy and linguistics (Section 2), corpora and automated tools are scarce. Blanco and Moldovan (2011) annotate focus of negation in the 3,993 negations marked with ARGM - NEG semantic role in PropBank (Palmer et al., 2005). Their annotations, PB-FOC, were used in the *SEM-2012 Shared Task (Morante and Blanco, 2012). Their guidelines require annotators to choose as focus the 1433 semantic role that “is most prominently negated” or the verb. If several roles may be the focus, they prioritize “the one that yields the most meaningful implicit [positive] information”, but do not specify what most meaningful means. Consider again statement (1) John doesn’t eat meat. Their approach would determine t"
N16-1169,N12-1050,1,0.838711,"Missing"
N16-1169,W10-3110,0,0.2939,"Missing"
N16-1169,J02-3001,0,0.0236354,"s from the negated verb or semantic role (sem role) used to generate the positive interpretation, from both of them (verb-sem role) and from the verb-argument structure of verb (verbargstruct), i.e., all semantic roles of verb. Verb features are straightforward and account for the verb word form and part-of-speech tag. Sem role features include the label of the semantic role from which the positive interpretation was generated (sem role label), its length (number of tokens), and the word form and part-of-speech tag of its head. Additionally, we add standard features in semantic role labeling (Gildea and Jurafsky, 2002): 1437 Experimental Results We report results obtained with several combinations of features in Table 5. We detail results obtained with features extracted from gold-standard and predicted linguistic annotations (part-of-speech tags, parse trees, semantic roles, etc.) as annotated in the gold and auto files from the CoNLL-2011 Shared Task release of OntoNotes (Pradhan et al., 2011). All models are trained with gold-standard linguistic annotations, and tested with either goldstandard or predicted linguistic annotations. Testing with gold-standard linguistic annotations. Using only the label of"
N16-1169,N06-2015,0,0.163353,"egated statements and their positive interpretations as intuitively understood by humans. We put a strong emphasis on automation. First, given a negated statement, we automatically generate plausible positive interpretations following a battery of linguistically motivated deterministic rules (Section 4.1). Second, we collect manual annotation to score the plausible positive interpretations according to their likelihood (Section 4.2). We then use these manually obtained scores to learn models that automatically score positive interpretations (Section 6). We decided to work on top of OntoNotes (Hovy et al., 2006) instead of plain text or other corpora for several reasons. First, OntoNotes includes gold linguistic annotations such as part-of-speech tags, parse trees and semantic roles. Second, state-ofthe-art role labelers trained with Propbank achieve F-measures of 0.835 (Lewis et al., 2015), and we use semantic roles to generate positive interpretations. Third, unlike BioScope, CD-SCO and PBFOC (Section 2), OntoNotes includes sentences from several genres, e.g., newswire, broadcast news and conversations, magazines, the web. 4.1 Generating Positive Interpretations OntoNotes2 is a large corpus contain"
N16-1169,D15-1169,0,0.0162081,"tic rules (Section 4.1). Second, we collect manual annotation to score the plausible positive interpretations according to their likelihood (Section 4.2). We then use these manually obtained scores to learn models that automatically score positive interpretations (Section 6). We decided to work on top of OntoNotes (Hovy et al., 2006) instead of plain text or other corpora for several reasons. First, OntoNotes includes gold linguistic annotations such as part-of-speech tags, parse trees and semantic roles. Second, state-ofthe-art role labelers trained with Propbank achieve F-measures of 0.835 (Lewis et al., 2015), and we use semantic roles to generate positive interpretations. Third, unlike BioScope, CD-SCO and PBFOC (Section 2), OntoNotes includes sentences from several genres, e.g., newswire, broadcast news and conversations, magazines, the web. 4.1 Generating Positive Interpretations OntoNotes2 is a large corpus containing 63,918 sentences. Annotating all positive interpretations from all negations is outside of the scope of this paper. Instead, we target selected representative negations. Selecting Negated Statements. We first selected all verbs negated with ARGM - NEG semantic role and obtained 6"
N16-1169,C10-1076,0,0.0561819,"dical domain (Szarvas et al., 2008) and CDSCO (Morante and Daelemans, 2012). BioScope annotates negation cues and linguistic scopes exclusively in biomedical texts. CD-SCO annotates negation cues, scopes, and negated events or properties in selected Conan Doyle stories. There have been several supervised proposals to detect the scope of negation using BioScope and ¨ ur and Radev, 2009; Øvrelid et al., CD-SCO (Ozg¨ 2010). Automatic approaches are mature (AbuJbara and Radev, 2012): F-scores are 0.96 for negation cue detection, and 0.89 for negation cue and scope detection (Velldal et al., 2012; Li et al., 2010). Outside BioScope and CD-SCO, Reitan et al. (2015) present a negation scope detector for tweets, and show that it improves sentiment analysis. As shown in Section 2, scope detection is insufficient to reveal positive interpretations from negated statements. 3.2 Focus Annotation and Detection While focus of negation has been studied for decades in philosophy and linguistics (Section 2), corpora and automated tools are scarce. Blanco and Moldovan (2011) annotate focus of negation in the 3,993 negations marked with ARGM - NEG semantic role in PropBank (Palmer et al., 2005). Their annotations, PB"
N16-1169,matsuyoshi-etal-2014-annotating,0,0.0239964,"Missing"
N16-1169,S12-1035,1,0.906424,"2015) present a negation scope detector for tweets, and show that it improves sentiment analysis. As shown in Section 2, scope detection is insufficient to reveal positive interpretations from negated statements. 3.2 Focus Annotation and Detection While focus of negation has been studied for decades in philosophy and linguistics (Section 2), corpora and automated tools are scarce. Blanco and Moldovan (2011) annotate focus of negation in the 3,993 negations marked with ARGM - NEG semantic role in PropBank (Palmer et al., 2005). Their annotations, PB-FOC, were used in the *SEM-2012 Shared Task (Morante and Blanco, 2012). Their guidelines require annotators to choose as focus the 1433 semantic role that “is most prominently negated” or the verb. If several roles may be the focus, they prioritize “the one that yields the most meaningful implicit [positive] information”, but do not specify what most meaningful means. Consider again statement (1) John doesn’t eat meat. Their approach would determine that the focus is the THEME of eat, meat, because it arguably yields the “most meaningful implicit [positive] information” (using our terminology, positive interpretation): John eats something other than meat. By des"
N16-1169,morante-daelemans-2012-conandoyle,0,0.290018,"Shallow approaches are usually application-specific. In sentiment and opinion analysis, negation has been reduced to marking as negated all words between a negation cue and the first punctuation mark (Pang et al., 2002), or within a five-word window of a negation cue (Hu and Liu, 2004). The examples throughout this paper show that these techniques are insufficient to reveal implicit positive interpretations. 3.1 Scope Annotation and Detection Scope of negation detection has received a lot of attention, mostly using two corpora: BioScope in the medical domain (Szarvas et al., 2008) and CDSCO (Morante and Daelemans, 2012). BioScope annotates negation cues and linguistic scopes exclusively in biomedical texts. CD-SCO annotates negation cues, scopes, and negated events or properties in selected Conan Doyle stories. There have been several supervised proposals to detect the scope of negation using BioScope and ¨ ur and Radev, 2009; Øvrelid et al., CD-SCO (Ozg¨ 2010). Automatic approaches are mature (AbuJbara and Radev, 2012): F-scores are 0.96 for negation cue detection, and 0.89 for negation cue and scope detection (Velldal et al., 2012; Li et al., 2010). Outside BioScope and CD-SCO, Reitan et al. (2015) present"
N16-1169,J12-2001,0,0.459062,"Missing"
N16-1169,C10-1155,0,0.336893,"Missing"
N16-1169,J05-1004,0,0.74038,"h noting that other foci yield unlikely interpretations, e.g., Somebody was looking for heaven in Europe, but not Mr. Haile (3b, AGENT), Mr. Haile was looking for heaven somewhere, but not in Europe (3d, LOCA TION ). Note that (1) scope on its own does not yield positive interpretations, and (2) some negated statements convey several likely positive interpretations, e.g., statement (1) in Section 5, Table 3. 3 Previous Work Within computational linguistics, approaches to process negation are shallow, or target scope and focus detection. Popular semantic representations such as semantic roles (Palmer et al., 2005; Baker et al., 1998) or AMR (Banarescu et al., 2013) do not reveal the positive interpretations we target in this paper. Shallow approaches are usually application-specific. In sentiment and opinion analysis, negation has been reduced to marking as negated all words between a negation cue and the first punctuation mark (Pang et al., 2002), or within a five-word window of a negation cue (Hu and Liu, 2004). The examples throughout this paper show that these techniques are insufficient to reveal implicit positive interpretations. 3.1 Scope Annotation and Detection Scope of negation detection has"
N16-1169,W02-1011,0,0.0183816,"positive interpretations, e.g., statement (1) in Section 5, Table 3. 3 Previous Work Within computational linguistics, approaches to process negation are shallow, or target scope and focus detection. Popular semantic representations such as semantic roles (Palmer et al., 2005; Baker et al., 1998) or AMR (Banarescu et al., 2013) do not reveal the positive interpretations we target in this paper. Shallow approaches are usually application-specific. In sentiment and opinion analysis, negation has been reduced to marking as negated all words between a negation cue and the first punctuation mark (Pang et al., 2002), or within a five-word window of a negation cue (Hu and Liu, 2004). The examples throughout this paper show that these techniques are insufficient to reveal implicit positive interpretations. 3.1 Scope Annotation and Detection Scope of negation detection has received a lot of attention, mostly using two corpora: BioScope in the medical domain (Szarvas et al., 2008) and CDSCO (Morante and Daelemans, 2012). BioScope annotates negation cues and linguistic scopes exclusively in biomedical texts. CD-SCO annotates negation cues, scopes, and negated events or properties in selected Conan Doyle stori"
N16-1169,W11-1901,0,0.500231,"t news and conversations, magazines, the web. 4.1 Generating Positive Interpretations OntoNotes2 is a large corpus containing 63,918 sentences. Annotating all positive interpretations from all negations is outside of the scope of this paper. Instead, we target selected representative negations. Selecting Negated Statements. We first selected all verbs negated with ARGM - NEG semantic role and obtained 6,617 verbal negations. After examining the negated verbs, it became clear that negation is not uniformly distributed across verbs in OntoNotes, 2 We use the CoNLL-2011 Shared Task distribution (Pradhan et al., 2011), http://conll.cemantix.org/2011/ 1434 it roughly follows Zipf’s law. In order to alleviate the annotation effort while accounting for all negated verbs in OntoNotes, we randomly selected up to 5 negations for each verb. The number of negated statements selected is 600. Converting Negated Statements into Their Positive Counterparts. We apply 3 steps inspired after the grammatical rules to form negation detailed by Huddleston and Pullum (2002, Ch. 9): 1. Remove the negation mark by removing the tokens within ARGM - NEG semantic role. 2. Remove auxiliaries, expand contractions, and fix third-per"
N16-1169,W15-2914,0,0.268311,"Missing"
N16-1169,S12-1039,0,0.443723,"Missing"
N16-1169,W08-0606,0,0.221574,"etations we target in this paper. Shallow approaches are usually application-specific. In sentiment and opinion analysis, negation has been reduced to marking as negated all words between a negation cue and the first punctuation mark (Pang et al., 2002), or within a five-word window of a negation cue (Hu and Liu, 2004). The examples throughout this paper show that these techniques are insufficient to reveal implicit positive interpretations. 3.1 Scope Annotation and Detection Scope of negation detection has received a lot of attention, mostly using two corpora: BioScope in the medical domain (Szarvas et al., 2008) and CDSCO (Morante and Daelemans, 2012). BioScope annotates negation cues and linguistic scopes exclusively in biomedical texts. CD-SCO annotates negation cues, scopes, and negated events or properties in selected Conan Doyle stories. There have been several supervised proposals to detect the scope of negation using BioScope and ¨ ur and Radev, 2009; Øvrelid et al., CD-SCO (Ozg¨ 2010). Automatic approaches are mature (AbuJbara and Radev, 2012): F-scores are 0.96 for negation cue detection, and 0.89 for negation cue and scope detection (Velldal et al., 2012; Li et al., 2010). Outside BioScope"
N16-1169,J12-2005,0,0.132392,"ra: BioScope in the medical domain (Szarvas et al., 2008) and CDSCO (Morante and Daelemans, 2012). BioScope annotates negation cues and linguistic scopes exclusively in biomedical texts. CD-SCO annotates negation cues, scopes, and negated events or properties in selected Conan Doyle stories. There have been several supervised proposals to detect the scope of negation using BioScope and ¨ ur and Radev, 2009; Øvrelid et al., CD-SCO (Ozg¨ 2010). Automatic approaches are mature (AbuJbara and Radev, 2012): F-scores are 0.96 for negation cue detection, and 0.89 for negation cue and scope detection (Velldal et al., 2012; Li et al., 2010). Outside BioScope and CD-SCO, Reitan et al. (2015) present a negation scope detector for tweets, and show that it improves sentiment analysis. As shown in Section 2, scope detection is insufficient to reveal positive interpretations from negated statements. 3.2 Focus Annotation and Detection While focus of negation has been studied for decades in philosophy and linguistics (Section 2), corpora and automated tools are scarce. Blanco and Moldovan (2011) annotate focus of negation in the 3,993 negations marked with ARGM - NEG semantic role in PropBank (Palmer et al., 2005). The"
N16-1169,P14-1049,0,0.268816,"Missing"
N16-1169,C98-1013,0,\N,Missing
N18-1046,J08-4004,0,0.573787,"Missing"
N18-1046,L16-1592,0,0.102335,"Missing"
N18-1046,J02-3001,0,0.0320837,"Missing"
N18-1046,J06-1005,0,0.126851,"Missing"
N18-1046,N06-2015,0,0.0192812,"queaker.n.01, strip.n.01, vehicle.n.01 Figure 1: WordNet synsets used to restrict possessees (y) when generating pairs (x, y). lemma.n.y indicates the yth synset of noun lemma. 4 A Corpus of Possession Relations We create a corpus1 following two steps. First, we generate intrasentential pairs (x, y) of potential possessors (x) and possessees (y). Second, we annotate whether a possession exists, and if so, the type and temporal anchors. Generating pairs a priori proved more effective than giving annotators plain text and asking them to annotate possessions. We add our annotations to OntoNotes (Hovy et al., 2006). Doing so has several advantages. First, OntoNotes contains texts from several domains and genres (e.g., conversational telephone speech, weblogs, broadcast), thus we not only work with newswire. Second, OntoNotes includes part-of-speech tags, named entities and parse trees, three annotation layers that allow us to streamline the corpus creation process. 4.1 All 369 219 192 160 13 11 5 4 2 4 979 Table 1: Counts of pairs (x, y) generated per type of potential possessor (x) and possessee (c). 3. For each possessor, collect as potential possessees all nouns reachable from verbx in the dependency"
N18-1046,D14-1162,0,0.0816612,"oken is the potential possessor x, possessee y, verbx , or none of them. Unlike the other two networks, LSTMsent has access to the full sentence, and we expect that the memory update mechanism (i.e., the input, output and forget gates) will learn the context most relevant for our task. regarding who is the potential possessor, possessee and verbx . 6.2 Neural Networks We experiment with feedforward and Long ShortTerm Memory networks, and use the implementations in Keras (Chollet et al., 2015) using TensorFlow backend (Abadi et al., 2015). All networks use GloVe embeddings with 100 dimensions (Pennington et al., 2014) and the Adam optimizer (Kingma and Ba, 2014). Regarding input, we experiment with the potential possessor x, possessee y, verbx , and the rest of the sentence. The three architectures are depicted in Figure 4. Feedforward Neural Network. The feedforward neural network takes as input the embeddings of the potential possessor x, possessee y and verbx . It has a fully connected hidden layer with 50 neurons and uses softmax in the output layer of size 3 for predicting possession existence (yes, never and unk) or size 2 for predicting possession type (alienable and control) and temporal anchors (y"
N18-1046,P10-1070,0,0.554913,"lations, e.g. temporal (e.g., [today]x ’s [rates]y ), extent (e.g., [6 hours]y ’ [drive]x ). Their controller / owner / user relation (one relation with three aliases) is the closest relation to the alienable and control possessions we target in this paper. Unlike them, we distinguish between alienable and control possessions, and assign temporal anchors to possessions. Additionally, we are not restricted to possessive constructions. Instead, we start by pairing potential possessors and possessees within a sentence. Extracting semantic relations between noun compounds (Nakov and Hearst, 2013; Tratz and Hovy, 2010) usually includes extracting possession relations, e.g., [family]x [estate]y . Because they target noun compounds, they disregard numerous possessions encoded in text at the clause or sentence level. Although they do extract many relations from noun compounds beyond possessions, they do not distinguish between alienable and control possessions, or temporally anchor relations with respect to events in which the possessor participates. To the best of our knowledge, the work by Banea et al. (2016) is the only one on extracting possession relations without imposing syntactic constraints. They buil"
N18-1046,P13-1037,0,0.232988,"inalienable, abstract, inanimate inalienable and inanimate alienable possession. Most influential to the work presented here, Tham (2004) presents four types of possession: (a) inalienable (e.g., John has a daughter), (b) alienable (e.g., John has a car), (c) control (e.g., John has the car (for the weekend)), and (d) focus (e.g., John has the window (to clean)). In this paper, we target alienable and control posses3 Previous Work Within computational linguistics, possession relations have been mostly studied as one of the many relations encoded in a given syntactic construction. For example, Tratz and Hovy (2013) extract semantic relations within English possessives. They propose a set of 18 relations, e.g. temporal (e.g., [today]x ’s [rates]y ), extent (e.g., [6 hours]y ’ [drive]x ). Their controller / owner / user relation (one relation with three aliases) is the closest relation to the alienable and control possessions we target in this paper. Unlike them, we distinguish between alienable and control possessions, and assign temporal anchors to possessions. Additionally, we are not restricted to possessive constructions. Instead, we start by pairing potential possessors and possessees within a sente"
N18-2026,J11-4005,0,0.294218,"r example, if John Doe started his drive to work at 8:00am, it is reasonable to expect him to start working by 9:00am because commuting took him (most likely) between a few minutes to an hour. Related Work TimeBank (Pustejovsky et al., 2006) is the corpus of reference for temporal information. The annotations follow TimeML (Pustejovsky et al., 2010) and include events, temporal expressions (e.g., last Friday), temporal signals (e.g., when, during), and links encoding relations. TimeBank does not annotate the expected duration of events. Annotating and learning event durations was pioneered by Pan et al. (2011), who annotated the events in TimeBank with their expected durations. Gusev et al. (2011) use query patterns in an unsupervised approach to predict the duration of events. The work presented here builds upon these previous works: we introduce additional features and an LSTM ensemble that obtains the best results to date. The new features are inspired by previous work on assigning situation entity (SE) type labels to clauses (Friedrich et al., 2016). SE types are a linguistic categorization of semantic clause type, whereby each clause is labeled according to the type of situation it introduces"
N18-2026,D14-1162,0,0.0771133,"ct and object in the WordNet taxonomy countability from WebCelex of the subject and object number of modifiers in the sentence adverbial degree of the sentence whether the sentence contains an adverb flags indicating the Brown clusters present in the sentence Table 1: Feature set to predict the expected duration of events with SVM. Features 1–25 were previously proposed for the same task. Features 26–700 are inspired by previous work assigning situation entity types to clauses (2016). 3 Corpus with TensorFlow backend (Abadi et al., 2015). All networks use GloVe embeddings with 300 dimensions (Pennington et al., 2014) and the Adam optimizer (Kingma and Ba, 2014). We use grid search and 5-fold cross-validation to tune hyperparameters (C and γ for SVM, and batch size, dropout rate, etc. for neural networks). We use the corpus by Pan et al. (2011), who annotated the events in TimeBank (Pustejovsky et al., 2003) with their expected durations by specifying upper and lower bounds. The authors clustered these bounds into two labels: less than a day (<day) and a day or longer (≥day), and the corpus contains 2,354 events (<day: 958, ≥day: 1,396). The same event predicate may have different durations depending on co"
N18-2026,P14-2082,0,0.0637745,"extual understanding requires identifying events and temporal relations between them. Beyond event participants, a crucial piece of information regarding events is their duration, an attribute rarely mentioned explicitly. For example, taking a shower lasts a few minutes (not days), and a vacation lasts a few days (not years). Core tasks such as temporal understanding and reasoning, as well as applications such as temporal question answering (Llorens et al., 2015) would benefit from knowing the expected duration of events. Consider a system that extracts temporal relations such as IS INCLUDED (Cassidy et al., 2014, among others). When deciding whether a relation holds between an event and a temporal expression, such a system would benefit from knowing the duration of the event at hand. For example, argument y of IS INCLUDED(built a house, y) must be a temporal span ranging from a few weeks to a year—the expected duration of built a house. Thus relation candidates such as IS INCLUDED(built a house, 4/5/2016 ) could be discarded right away. Similarly, event durations combined with event ordering and temporal anchoring would help to determine the time of subsequent events. For example, if John Doe started"
N18-2026,pustejovsky-etal-2010-iso,0,0.0187439,"expected duration of built a house. Thus relation candidates such as IS INCLUDED(built a house, 4/5/2016 ) could be discarded right away. Similarly, event durations combined with event ordering and temporal anchoring would help to determine the time of subsequent events. For example, if John Doe started his drive to work at 8:00am, it is reasonable to expect him to start working by 9:00am because commuting took him (most likely) between a few minutes to an hour. Related Work TimeBank (Pustejovsky et al., 2006) is the corpus of reference for temporal information. The annotations follow TimeML (Pustejovsky et al., 2010) and include events, temporal expressions (e.g., last Friday), temporal signals (e.g., when, during), and links encoding relations. TimeBank does not annotate the expected duration of events. Annotating and learning event durations was pioneered by Pan et al. (2011), who annotated the events in TimeBank with their expected durations. Gusev et al. (2011) use query patterns in an unsupervised approach to predict the duration of events. The work presented here builds upon these previous works: we introduce additional features and an LSTM ensemble that obtains the best results to date. The new fea"
N18-2026,P16-1166,1,0.913302,"ring), and links encoding relations. TimeBank does not annotate the expected duration of events. Annotating and learning event durations was pioneered by Pan et al. (2011), who annotated the events in TimeBank with their expected durations. Gusev et al. (2011) use query patterns in an unsupervised approach to predict the duration of events. The work presented here builds upon these previous works: we introduce additional features and an LSTM ensemble that obtains the best results to date. The new features are inspired by previous work on assigning situation entity (SE) type labels to clauses (Friedrich et al., 2016). SE types are a linguistic categorization of semantic clause type, whereby each clause is labeled according to the type of situation it introduces to a discourse (STATE, EVENT, GENERIC, and GENERALIZING SENTENCE (also known as habituals)). Other related works include efforts modeling event durations in social media (Williams and Katz, 2012), and temporal anchoring of, among others, durative events (Reimers et al., 2016). 164 Proceedings of NAACL-HLT 2018, pages 164–168 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Pan et al. Gusev et al. Aspectual"
N18-2026,P16-1207,0,0.0173499,"eatures and an LSTM ensemble that obtains the best results to date. The new features are inspired by previous work on assigning situation entity (SE) type labels to clauses (Friedrich et al., 2016). SE types are a linguistic categorization of semantic clause type, whereby each clause is labeled according to the type of situation it introduces to a discourse (STATE, EVENT, GENERIC, and GENERALIZING SENTENCE (also known as habituals)). Other related works include efforts modeling event durations in social media (Williams and Katz, 2012), and temporal anchoring of, among others, durative events (Reimers et al., 2016). 164 Proceedings of NAACL-HLT 2018, pages 164–168 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Pan et al. Gusev et al. Aspectual features inspired by situation entities (Friedrich et al.) # 1-3 4-9 10-18 19-20 21 22-25 26 27 28 29 30 31 32-40 41-43 44-46 47 48 49 50 51-700 Description event token, lemma and POS tag head word, lemma and POS tags of the syntactic subject and object of the event three closest hypernyms of the event, subject and object named entity types of the syntactic subject and object of the event flag indicating if the event is"
N18-2026,W11-0116,0,0.579031,"him to start working by 9:00am because commuting took him (most likely) between a few minutes to an hour. Related Work TimeBank (Pustejovsky et al., 2006) is the corpus of reference for temporal information. The annotations follow TimeML (Pustejovsky et al., 2010) and include events, temporal expressions (e.g., last Friday), temporal signals (e.g., when, during), and links encoding relations. TimeBank does not annotate the expected duration of events. Annotating and learning event durations was pioneered by Pan et al. (2011), who annotated the events in TimeBank with their expected durations. Gusev et al. (2011) use query patterns in an unsupervised approach to predict the duration of events. The work presented here builds upon these previous works: we introduce additional features and an LSTM ensemble that obtains the best results to date. The new features are inspired by previous work on assigning situation entity (SE) type labels to clauses (Friedrich et al., 2016). SE types are a linguistic categorization of semantic clause type, whereby each clause is labeled according to the type of situation it introduces to a discourse (STATE, EVENT, GENERIC, and GENERALIZING SENTENCE (also known as habituals"
N18-2026,P10-1040,0,0.0084426,"the tokens after the event (top right). is episodic, habitual, or static. It is primarily these criteria which features 26-50 aim to capture. For example, bare plural subjects with a simple present tense verb (e.g., Bats eat mosquitos) are a hallmark of GENERIC clauses. Although situation entity types do not directly map onto the duration labels (<day or ≥day), the criteria which contribute to determining them clearly influence aspectual interpretation, thus influencing understanding of the duration of events. Regarding Brown clusters, we use freely available clusters trained on news data by Turian et al. (2010) using the implementation by Liang (2005). We include one feature per cluster and set it to true if any word in the sentence belongs to the cluster. 4.2 Pan et al. Pan et al. + Gusev et al. Pan et al. + Gusev et al. + Situation Entities Feed-forward neural network LSTM ensemble Feed-Forward Neural Network The first neural network we experiment with is a one-hidden-layer feed-forward neural network that takes as input the event embedding. The tuning process revealed that the size of the hidden layer is not important, thus we report results using a hidden layer with 5 neurons. Intuitively, this"
N18-2026,S15-2134,0,0.159294,"and temporal structure of the clause provide useful clues, and that an LSTM ensemble captures relevant context around the event. 1 Introduction 2 Robust textual understanding requires identifying events and temporal relations between them. Beyond event participants, a crucial piece of information regarding events is their duration, an attribute rarely mentioned explicitly. For example, taking a shower lasts a few minutes (not days), and a vacation lasts a few days (not years). Core tasks such as temporal understanding and reasoning, as well as applications such as temporal question answering (Llorens et al., 2015) would benefit from knowing the expected duration of events. Consider a system that extracts temporal relations such as IS INCLUDED (Cassidy et al., 2014, among others). When deciding whether a relation holds between an event and a temporal expression, such a system would benefit from knowing the duration of the event at hand. For example, argument y of IS INCLUDED(built a house, y) must be a temporal span ranging from a few weeks to a year—the expected duration of built a house. Thus relation candidates such as IS INCLUDED(built a house, 4/5/2016 ) could be discarded right away. Similarly, ev"
N18-2026,P12-2044,0,0.249696,"The work presented here builds upon these previous works: we introduce additional features and an LSTM ensemble that obtains the best results to date. The new features are inspired by previous work on assigning situation entity (SE) type labels to clauses (Friedrich et al., 2016). SE types are a linguistic categorization of semantic clause type, whereby each clause is labeled according to the type of situation it introduces to a discourse (STATE, EVENT, GENERIC, and GENERALIZING SENTENCE (also known as habituals)). Other related works include efforts modeling event durations in social media (Williams and Katz, 2012), and temporal anchoring of, among others, durative events (Reimers et al., 2016). 164 Proceedings of NAACL-HLT 2018, pages 164–168 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Pan et al. Gusev et al. Aspectual features inspired by situation entities (Friedrich et al.) # 1-3 4-9 10-18 19-20 21 22-25 26 27 28 29 30 31 32-40 41-43 44-46 47 48 49 50 51-700 Description event token, lemma and POS tag head word, lemma and POS tags of the syntactic subject and object of the event three closest hypernyms of the event, subject and object named entity types"
N19-1214,L16-1626,0,0.0318366,"e state of the art uses neural networks and word embeddings. Baziotis et al. (2017) propose a stack of two BiLSTMs at the word level and do not use any lexicons. Cliche (2017) presents a CNN and BiLSTM ensemble and experiment with three pretrained embeddings. Rouvier (2017) also presents a CNN and BiLSTM ensemble but incorporates manually defined features (e.g., word presence in emotion lexicons, all-caps). The strategy presented here to incorporate emojis outperforms all these systems (Table 4). Within natural language processing and social media, emojis have received considerable attention. Barbieri et al. (2016) train emoji embeddings with word2vec and discover that the closest words are sound (e.g., : coffee, roasters, caffeine, latte). Eisner et al. (2016) propose a complementary approach to train emoji embeddings (Section 3). Emojis have also been used as labels for distant supervision to improve tweet classification (Felbo et al., 2017). The strategy presented here to incorporate emojis is simpler and more effective than previous ones, does not require additional pretraining or domain specific corpora, and can be used with any neural architecture that takes text as input without any modifications"
N19-1214,S17-2126,0,0.276007,"on taking as input an embedding for the input text (average of word embeddings) as well as manually crafted lexical, syntactic, semantic and polarity features. Our strategy to incorporate emojis outperforms all of them (Table 3). Sentiment analysis in tweets has been studied for years (Nakov et al., 2013). At its core, it is the task of classifying a tweet into expressing positive, neutral or negative sentiment (Rosenthal et al., 2017). Initial systems were primarily based on sentiment lexicons and manually extracted features, but the state of the art uses neural networks and word embeddings. Baziotis et al. (2017) propose a stack of two BiLSTMs at the word level and do not use any lexicons. Cliche (2017) presents a CNN and BiLSTM ensemble and experiment with three pretrained embeddings. Rouvier (2017) also presents a CNN and BiLSTM ensemble but incorporates manually defined features (e.g., word presence in emotion lexicons, all-caps). The strategy presented here to incorporate emojis outperforms all these systems (Table 4). Within natural language processing and social media, emojis have received considerable attention. Barbieri et al. (2016) train emoji embeddings with word2vec and discover that the c"
N19-1214,P06-4018,0,0.0116264,"TMobile tomorrow and I’m getting a new number. Evaluation Metrics. We follow the metrics used by previous work. Regarding irony detection, we report accuracy and macro-average F1 (all labels weighted equal regardless of frequency). Regarding sentiment analysis, we report accuracy, average recall and F1. Following previous work, we calculate accuracy and average recall using all labels (positive, negative and neutral) but F1 using only positive and negative instances. Preprocessing. We preprocess the input text following standard steps. Specifically, we tokenize with the NLTK’s TweetTokenizer (Bird, 2006), lowercase all text, and use regular expressions to remove stop words, numbers, urls, consecutive repeated words and Twitter users (i.e., tokens whose first character is ‘@’). We also expand hashtags (e.g., #PickANewSong: Pick a new song) with ekphrasis (Baziotis et al., 2017). Regarding emojis, we either (a) do nothing special and use pretrained emoji embeddings (EMJ - EMBED strategy), or (b) replace emojis with their textual description and use pretrained word embeddings for the words in their descriptions (EMJ - DESC strategy). Let us consider the following tweet: “@Paul OConnor187 hi we g"
N19-1214,S17-2094,0,0.188529,"rafted lexical, syntactic, semantic and polarity features. Our strategy to incorporate emojis outperforms all of them (Table 3). Sentiment analysis in tweets has been studied for years (Nakov et al., 2013). At its core, it is the task of classifying a tweet into expressing positive, neutral or negative sentiment (Rosenthal et al., 2017). Initial systems were primarily based on sentiment lexicons and manually extracted features, but the state of the art uses neural networks and word embeddings. Baziotis et al. (2017) propose a stack of two BiLSTMs at the word level and do not use any lexicons. Cliche (2017) presents a CNN and BiLSTM ensemble and experiment with three pretrained embeddings. Rouvier (2017) also presents a CNN and BiLSTM ensemble but incorporates manually defined features (e.g., word presence in emotion lexicons, all-caps). The strategy presented here to incorporate emojis outperforms all these systems (Table 4). Within natural language processing and social media, emojis have received considerable attention. Barbieri et al. (2016) train emoji embeddings with word2vec and discover that the closest words are sound (e.g., : coffee, roasters, caffeine, latte). Eisner et al. (2016) pro"
N19-1214,P15-1033,0,0.0124387,"eddings (EMJ - EMBED strategy), or (b) replace emojis with their textual description and use pretrained word embeddings for the words in their descriptions (EMJ - DESC strategy). Let us consider the following tweet: “@Paul OConnor187 hi we going to see ted 2 at the Odeon cinemas at Glasgow on Wednesday ”. After preprocessing, we transform it into “hi we going see ted odeon cinemas glasgow wednesday ” or “hi we going see ted odeon cinemas glasgow wednesday smiling face” (EMJ - EMBED and EMJ - DESC strategies respectively). Neural Network Architecture. We experiment with a stack of two BiLSTMs (Dyer et al., 2015) with attention (Zhou et al., 2016) to generate distributed representations of the input, and a softmax layer as the output layer. This architecture is simpler than previous proposals, but as we shall see, 2098 Previous Work (Top 3) This paper Vu et al. (2018) Wu et al. (2018) Baziotis et al. (2018) EMJ - EMBED EMJ - DESC Task A Acc. F1 0.7015 0.6476 0.7347 0.7054 0.7883 0.7856 0.7864 0.7814 0.8056 0.8031 Task B Acc. F1 0.6594 0.4437 0.6046 0.4947 0.6888 0.5358 0.6940 0.5434 0.7187 0.5565 Table 3: Results on irony detection (Accuracy and Macro F1). Task A is a binary classification (yes / no)"
N19-1214,W16-6208,0,0.231499,"rt uses neural networks and word embeddings. Baziotis et al. (2017) propose a stack of two BiLSTMs at the word level and do not use any lexicons. Cliche (2017) presents a CNN and BiLSTM ensemble and experiment with three pretrained embeddings. Rouvier (2017) also presents a CNN and BiLSTM ensemble but incorporates manually defined features (e.g., word presence in emotion lexicons, all-caps). The strategy presented here to incorporate emojis outperforms all these systems (Table 4). Within natural language processing and social media, emojis have received considerable attention. Barbieri et al. (2016) train emoji embeddings with word2vec and discover that the closest words are sound (e.g., : coffee, roasters, caffeine, latte). Eisner et al. (2016) propose a complementary approach to train emoji embeddings (Section 3). Emojis have also been used as labels for distant supervision to improve tweet classification (Felbo et al., 2017). The strategy presented here to incorporate emojis is simpler and more effective than previous ones, does not require additional pretraining or domain specific corpora, and can be used with any neural architecture that takes text as input without any modifications"
N19-1214,D17-1169,0,0.0392374,"ly defined features (e.g., word presence in emotion lexicons, all-caps). The strategy presented here to incorporate emojis outperforms all these systems (Table 4). Within natural language processing and social media, emojis have received considerable attention. Barbieri et al. (2016) train emoji embeddings with word2vec and discover that the closest words are sound (e.g., : coffee, roasters, caffeine, latte). Eisner et al. (2016) propose a complementary approach to train emoji embeddings (Section 3). Emojis have also been used as labels for distant supervision to improve tweet classification (Felbo et al., 2017). The strategy presented here to incorporate emojis is simpler and more effective than previous ones, does not require additional pretraining or domain specific corpora, and can be used with any neural architecture that takes text as input without any modifications. Simply put, we replace emojis with their textual descriptions and leverage existing pretrained word embeddings. 3 Strategies to Incorporate Emojis Neural networks that take as input text usually transform the input tokens into pretrained embeddings. When the input text are tweets, it is common to use embeddings pretrained with larg"
N19-1214,S18-1005,0,0.177306,"o tweet classification tasks: irony detection and sentiment analysis. 2 Related Work Irony is closely related to sarcasm. The Oxford Dictionary defines irony as “The expression of one’s meaning by using language that normally signifies the opposite, typically for humorous or emphatic effect”, and sarcasm as “The use of irony to mock or convey contempt.” Given these definitions, it is not surprising that many researchers do not distinguish between them (Maynard and Greenwood, 2014). The top-3 systems to detect irony are built with neural networks and pretrained word embeddings. Baziotis et al. (2018) build an ensemble of two stacks of BiLSTMs (word and character level) with attention. Wu et al. (2018) propose a BiLSTM and a multitask learning framework (hashtag, irony presence and irony type prediction), and complement the input text with sentiment features extracted from lexicons. Vu et al. (2018) propose a multilayer perceptron taking as input an embedding for the input text (average of word embeddings) as well as manually crafted lexical, syntactic, semantic and polarity features. Our strategy to incorporate emojis outperforms all of them (Table 3). Sentiment analysis in tweets has bee"
N19-1214,maynard-greenwood-2014-cares,0,0.0232791,"ace them with their textual description. Second, we show that this strategy outperforms previous methods and yields a new stateof-the-art in two tweet classification tasks: irony detection and sentiment analysis. 2 Related Work Irony is closely related to sarcasm. The Oxford Dictionary defines irony as “The expression of one’s meaning by using language that normally signifies the opposite, typically for humorous or emphatic effect”, and sarcasm as “The use of irony to mock or convey contempt.” Given these definitions, it is not surprising that many researchers do not distinguish between them (Maynard and Greenwood, 2014). The top-3 systems to detect irony are built with neural networks and pretrained word embeddings. Baziotis et al. (2018) build an ensemble of two stacks of BiLSTMs (word and character level) with attention. Wu et al. (2018) propose a BiLSTM and a multitask learning framework (hashtag, irony presence and irony type prediction), and complement the input text with sentiment features extracted from lexicons. Vu et al. (2018) propose a multilayer perceptron taking as input an embedding for the input text (average of word embeddings) as well as manually crafted lexical, syntactic, semantic and pola"
N19-1214,P16-2034,0,0.0181887,"(b) replace emojis with their textual description and use pretrained word embeddings for the words in their descriptions (EMJ - DESC strategy). Let us consider the following tweet: “@Paul OConnor187 hi we going to see ted 2 at the Odeon cinemas at Glasgow on Wednesday ”. After preprocessing, we transform it into “hi we going see ted odeon cinemas glasgow wednesday ” or “hi we going see ted odeon cinemas glasgow wednesday smiling face” (EMJ - EMBED and EMJ - DESC strategies respectively). Neural Network Architecture. We experiment with a stack of two BiLSTMs (Dyer et al., 2015) with attention (Zhou et al., 2016) to generate distributed representations of the input, and a softmax layer as the output layer. This architecture is simpler than previous proposals, but as we shall see, 2098 Previous Work (Top 3) This paper Vu et al. (2018) Wu et al. (2018) Baziotis et al. (2018) EMJ - EMBED EMJ - DESC Task A Acc. F1 0.7015 0.6476 0.7347 0.7054 0.7883 0.7856 0.7864 0.7814 0.8056 0.8031 Task B Acc. F1 0.6594 0.4437 0.6046 0.4947 0.6888 0.5358 0.6940 0.5434 0.7187 0.5565 Table 3: Results on irony detection (Accuracy and Macro F1). Task A is a binary classification (yes / no) and Task B is a four-way classifica"
N19-1214,S13-2052,0,0.104781,"Missing"
N19-1214,D14-1162,0,0.0836341,"tive than previous ones, does not require additional pretraining or domain specific corpora, and can be used with any neural architecture that takes text as input without any modifications. Simply put, we replace emojis with their textual descriptions and leverage existing pretrained word embeddings. 3 Strategies to Incorporate Emojis Neural networks that take as input text usually transform the input tokens into pretrained embeddings. When the input text are tweets, it is common to use embeddings pretrained with large collections of tweets as opposed to general purpose text (Li et al., 2017; Pennington et al., 2014). Emojis as Regular Tokens. The simplest option to incorporate emojis into a neural network is to consider them as any other token in the input text (Barbieri et al., 2016). This strategy relies on having seen enough instances of each emoji in the texts with which embeddings were pretrained— otherwise the embeddings will not capture the semantics of emoji tokens properly. Emoji Embeddings. Another strategy is to use separate embeddings for emojis. Eisner et al. (2016) pretrain emoji embeddings using positive and negative (randomly sampled) emoji descriptions. Descriptions are transformed into"
N19-1214,S17-2088,0,0.0510555,"Missing"
N19-1214,S17-2128,0,0.158122,"erforms all of them (Table 3). Sentiment analysis in tweets has been studied for years (Nakov et al., 2013). At its core, it is the task of classifying a tweet into expressing positive, neutral or negative sentiment (Rosenthal et al., 2017). Initial systems were primarily based on sentiment lexicons and manually extracted features, but the state of the art uses neural networks and word embeddings. Baziotis et al. (2017) propose a stack of two BiLSTMs at the word level and do not use any lexicons. Cliche (2017) presents a CNN and BiLSTM ensemble and experiment with three pretrained embeddings. Rouvier (2017) also presents a CNN and BiLSTM ensemble but incorporates manually defined features (e.g., word presence in emotion lexicons, all-caps). The strategy presented here to incorporate emojis outperforms all these systems (Table 4). Within natural language processing and social media, emojis have received considerable attention. Barbieri et al. (2016) train emoji embeddings with word2vec and discover that the closest words are sound (e.g., : coffee, roasters, caffeine, latte). Eisner et al. (2016) propose a complementary approach to train emoji embeddings (Section 3). Emojis have also been used as"
N19-1214,S18-1085,0,0.0331583,"Missing"
N19-1214,S18-1006,0,0.0403849,"Missing"
P11-1059,P98-1013,0,0.0225897,"s (Bos and Markert, 2005). Regarding corpora, the BioScope corpus annotates negation marks and linguistic scopes exclusively on biomedical texts. It does not annotate focus and it purposely ignores negations such as (talk582 ing about the reaction of certain elements) in NK3.3 cells is not always identical (Vincze et al., 2008), which carry the kind of positive meaning this work aims at extracting (in NK3.3 cells is often identical). PropBank (Palmer et al., 2005) only indicates the verb to which a negation mark attaches; it does not provide any information about the scope or focus. FrameNet (Baker et al., 1998) does not consider negation and FactBank (Saur´ı and Pustejovsky, 2009) only annotates degrees of factuality for events. None of the above references aim at detecting or annotating the focus of negation in natural language. Neither do they aim at carefully representing the meaning of negated statements nor extracting implicit positive meaning from them. 3 Negation in Natural Language Simply put, negation is a process that turns a statement into its opposite. Unlike affirmative statements, negation is marked by words (e.g., not, no, never) or affixes (e.g., -n’t, un-). Negation can interact wit"
P11-1059,H05-1079,0,0.0100444,"Wiegand et al., 2010) and the biomedical domain. Recently, the Negation and Speculation in NLP Workshop (Morante and Sporleder, 2010) and the CoNLL-2010 Shared Task (Farkas et al., 2010) targeted negation mostly on those subfields. Morante and Daelemans (2009) and ¨ ur and Radev (2009) propose scope detectors Ozg¨ using the BioScope corpus. Councill et al. (2010) present a supervised scope detector using their own annotation. Some NLP applications deal indirectly with negation, e.g., machine translation (van Munster, 1988), text classification (Rose et al., 2003) and recognizing entailments (Bos and Markert, 2005). Regarding corpora, the BioScope corpus annotates negation marks and linguistic scopes exclusively on biomedical texts. It does not annotate focus and it purposely ignores negations such as (talk582 ing about the reaction of certain elements) in NK3.3 cells is not always identical (Vincze et al., 2008), which carry the kind of positive meaning this work aims at extracting (in NK3.3 cells is often identical). PropBank (Palmer et al., 2005) only indicates the verb to which a negation mark attaches; it does not provide any information about the scope or focus. FrameNet (Baker et al., 1998) does"
P11-1059,choi-etal-2010-propbank-instance,0,0.0207324,"Missing"
P11-1059,W10-3110,0,0.171327,"th (1992)). In this paper, we follow the insights on scope and focus of negation by Huddleston and Pullum (2002) rather than Rooth’s (1985). Within natural language processing, negation has drawn attention mainly in sentiment analysis (Wilson et al., 2009; Wiegand et al., 2010) and the biomedical domain. Recently, the Negation and Speculation in NLP Workshop (Morante and Sporleder, 2010) and the CoNLL-2010 Shared Task (Farkas et al., 2010) targeted negation mostly on those subfields. Morante and Daelemans (2009) and ¨ ur and Radev (2009) propose scope detectors Ozg¨ using the BioScope corpus. Councill et al. (2010) present a supervised scope detector using their own annotation. Some NLP applications deal indirectly with negation, e.g., machine translation (van Munster, 1988), text classification (Rose et al., 2003) and recognizing entailments (Bos and Markert, 2005). Regarding corpora, the BioScope corpus annotates negation marks and linguistic scopes exclusively on biomedical texts. It does not annotate focus and it purposely ignores negations such as (talk582 ing about the reaction of certain elements) in NK3.3 cells is not always identical (Vincze et al., 2008), which carry the kind of positive meani"
P11-1059,W10-3001,0,0.0781501,"Missing"
P11-1059,W09-1304,0,0.0240154,"ve concords. Rooth (1985) presented a theory of focus in his dissertation and posterior publications (e.g., Rooth (1992)). In this paper, we follow the insights on scope and focus of negation by Huddleston and Pullum (2002) rather than Rooth’s (1985). Within natural language processing, negation has drawn attention mainly in sentiment analysis (Wilson et al., 2009; Wiegand et al., 2010) and the biomedical domain. Recently, the Negation and Speculation in NLP Workshop (Morante and Sporleder, 2010) and the CoNLL-2010 Shared Task (Farkas et al., 2010) targeted negation mostly on those subfields. Morante and Daelemans (2009) and ¨ ur and Radev (2009) propose scope detectors Ozg¨ using the BioScope corpus. Councill et al. (2010) present a supervised scope detector using their own annotation. Some NLP applications deal indirectly with negation, e.g., machine translation (van Munster, 1988), text classification (Rose et al., 2003) and recognizing entailments (Bos and Markert, 2005). Regarding corpora, the BioScope corpus annotates negation marks and linguistic scopes exclusively on biomedical texts. It does not annotate focus and it purposely ignores negations such as (talk582 ing about the reaction of certain eleme"
P11-1059,J05-1004,0,0.409408,"cations deal indirectly with negation, e.g., machine translation (van Munster, 1988), text classification (Rose et al., 2003) and recognizing entailments (Bos and Markert, 2005). Regarding corpora, the BioScope corpus annotates negation marks and linguistic scopes exclusively on biomedical texts. It does not annotate focus and it purposely ignores negations such as (talk582 ing about the reaction of certain elements) in NK3.3 cells is not always identical (Vincze et al., 2008), which carry the kind of positive meaning this work aims at extracting (in NK3.3 cells is often identical). PropBank (Palmer et al., 2005) only indicates the verb to which a negation mark attaches; it does not provide any information about the scope or focus. FrameNet (Baker et al., 1998) does not consider negation and FactBank (Saur´ı and Pustejovsky, 2009) only annotates degrees of factuality for events. None of the above references aim at detecting or annotating the focus of negation in natural language. Neither do they aim at carefully representing the meaning of negated statements nor extracting implicit positive meaning from them. 3 Negation in Natural Language Simply put, negation is a process that turns a statement into"
P11-1059,W03-0210,0,0.0127702,"inly in sentiment analysis (Wilson et al., 2009; Wiegand et al., 2010) and the biomedical domain. Recently, the Negation and Speculation in NLP Workshop (Morante and Sporleder, 2010) and the CoNLL-2010 Shared Task (Farkas et al., 2010) targeted negation mostly on those subfields. Morante and Daelemans (2009) and ¨ ur and Radev (2009) propose scope detectors Ozg¨ using the BioScope corpus. Councill et al. (2010) present a supervised scope detector using their own annotation. Some NLP applications deal indirectly with negation, e.g., machine translation (van Munster, 1988), text classification (Rose et al., 2003) and recognizing entailments (Bos and Markert, 2005). Regarding corpora, the BioScope corpus annotates negation marks and linguistic scopes exclusively on biomedical texts. It does not annotate focus and it purposely ignores negations such as (talk582 ing about the reaction of certain elements) in NK3.3 cells is not always identical (Vincze et al., 2008), which carry the kind of positive meaning this work aims at extracting (in NK3.3 cells is often identical). PropBank (Palmer et al., 2005) only indicates the verb to which a negation mark attaches; it does not provide any information about the"
P11-1059,C88-2090,0,0.641311,"Missing"
P11-1059,W08-0606,0,0.360703,"ctors Ozg¨ using the BioScope corpus. Councill et al. (2010) present a supervised scope detector using their own annotation. Some NLP applications deal indirectly with negation, e.g., machine translation (van Munster, 1988), text classification (Rose et al., 2003) and recognizing entailments (Bos and Markert, 2005). Regarding corpora, the BioScope corpus annotates negation marks and linguistic scopes exclusively on biomedical texts. It does not annotate focus and it purposely ignores negations such as (talk582 ing about the reaction of certain elements) in NK3.3 cells is not always identical (Vincze et al., 2008), which carry the kind of positive meaning this work aims at extracting (in NK3.3 cells is often identical). PropBank (Palmer et al., 2005) only indicates the verb to which a negation mark attaches; it does not provide any information about the scope or focus. FrameNet (Baker et al., 1998) does not consider negation and FactBank (Saur´ı and Pustejovsky, 2009) only annotates degrees of factuality for events. None of the above references aim at detecting or annotating the focus of negation in natural language. Neither do they aim at carefully representing the meaning of negated statements nor ex"
P11-1059,W10-3111,0,0.104716,"over 60 pages to it. Negation interacts with quantifiers and anaphora (Hintikka, 2002), and influences reasoning (Dowty, 1994; S´anchez Valencia, 1991). Zeijlstra (2007) analyzes the position and form of negative elements and negative concords. Rooth (1985) presented a theory of focus in his dissertation and posterior publications (e.g., Rooth (1992)). In this paper, we follow the insights on scope and focus of negation by Huddleston and Pullum (2002) rather than Rooth’s (1985). Within natural language processing, negation has drawn attention mainly in sentiment analysis (Wilson et al., 2009; Wiegand et al., 2010) and the biomedical domain. Recently, the Negation and Speculation in NLP Workshop (Morante and Sporleder, 2010) and the CoNLL-2010 Shared Task (Farkas et al., 2010) targeted negation mostly on those subfields. Morante and Daelemans (2009) and ¨ ur and Radev (2009) propose scope detectors Ozg¨ using the BioScope corpus. Councill et al. (2010) present a supervised scope detector using their own annotation. Some NLP applications deal indirectly with negation, e.g., machine translation (van Munster, 1988), text classification (Rose et al., 2003) and recognizing entailments (Bos and Markert, 2005)"
P11-1059,J09-3003,0,0.022372,"llum (2002) dedicate over 60 pages to it. Negation interacts with quantifiers and anaphora (Hintikka, 2002), and influences reasoning (Dowty, 1994; S´anchez Valencia, 1991). Zeijlstra (2007) analyzes the position and form of negative elements and negative concords. Rooth (1985) presented a theory of focus in his dissertation and posterior publications (e.g., Rooth (1992)). In this paper, we follow the insights on scope and focus of negation by Huddleston and Pullum (2002) rather than Rooth’s (1985). Within natural language processing, negation has drawn attention mainly in sentiment analysis (Wilson et al., 2009; Wiegand et al., 2010) and the biomedical domain. Recently, the Negation and Speculation in NLP Workshop (Morante and Sporleder, 2010) and the CoNLL-2010 Shared Task (Farkas et al., 2010) targeted negation mostly on those subfields. Morante and Daelemans (2009) and ¨ ur and Radev (2009) propose scope detectors Ozg¨ using the BioScope corpus. Councill et al. (2010) present a supervised scope detector using their own annotation. Some NLP applications deal indirectly with negation, e.g., machine translation (van Munster, 1988), text classification (Rose et al., 2003) and recognizing entailments"
P11-1059,D09-1145,0,\N,Missing
P11-1059,C98-1013,0,\N,Missing
P11-1146,P08-2045,0,0.0608037,"R(x’, z ) is already known. We use the following heuristic in order to improve accuracy: do not instantiate an axiom R1 (x, y) ◦ R2 (y, z) → R3 (x, z) if a relation of the form R3 (x’, z) is already known. This simple heuristic has increased the accuracy of the inferences at the cost of lowering their productivity. The last three columns in Table 6 show results when using the heuristic. 1462 6 Comparison with Previous Work There have been many proposals to detect semantic relations from text without composition. Researches have targeted particular relations (e.g., CAUSE (Chang and Choi, 2006; Bethard and Martin, 2008)), relations within noun phrases (Nulty, 2007), named entities (Hirano et al., 2007) or clauses (Szpakowicz et al., 1995). Competitions include (Litkowski, 2004; Carreras and M`arquez, 2005; Girju et al., 2007; Hendrickx et al., 2009). Two recent efforts (Ruppenhofer et al., 2009; Gerber and Chai, 2010) are similar to CSR in their goal (i.e., extract meaning ignored by current semantic parsers), but completely differ in their means. Their merit relies on annotating and extracting semantic connections not originally contemplated (e.g., between concepts from two different sentences) using an alr"
P11-1146,W11-0106,1,0.783078,"of y id. id. op. id. id. id. id. op. op. [1] [1] [2] [3] [3] - Table 1: List of semantic primitives. In the fourth column, [1] stands for (Winston et al., 1987), [2] for (Cohen and Losielle, 1988) and [3] for (Huhns and Stephens, 1989). lations. The conclusion of an axiom is identified using an algebra for composing semantic primitives. We name this framework Composition of Semantic Relations (CSR). The extended definition, set of primitives, algebra to compose primitives and CSR algorithm are independent of any particular set of relations. We first presented CSR and used it over PropBank in (Blanco and Moldovan, 2011). In this paper, we extend that work using a different set of primitives and relations. Seventy eight inference axioms are obtained and an empirical evaluation shows that inferred relations have high accuracies. 2 Semantic Relations Semantic relations are underlying relations between concepts. In general, they are defined by a textual definition accompanied by a few examples. For example, Chklovski and Pantel (2004) loosely define ENABLEMENT as a relation that holds between two verbs V1 and V2 when the pair can be glossed as V1 is accomplished by V2 and gives two examples: assess::review and a"
P11-1146,W05-0620,0,0.713667,"Missing"
P11-1146,W04-3205,0,0.0467592,"finition, set of primitives, algebra to compose primitives and CSR algorithm are independent of any particular set of relations. We first presented CSR and used it over PropBank in (Blanco and Moldovan, 2011). In this paper, we extend that work using a different set of primitives and relations. Seventy eight inference axioms are obtained and an empirical evaluation shows that inferred relations have high accuracies. 2 Semantic Relations Semantic relations are underlying relations between concepts. In general, they are defined by a textual definition accompanied by a few examples. For example, Chklovski and Pantel (2004) loosely define ENABLEMENT as a relation that holds between two verbs V1 and V2 when the pair can be glossed as V1 is accomplished by V2 and gives two examples: assess::review and accomplish::complete. We find this widespread kind of definition weak and prone to confusion. Following (Helbig, 2005), we propose an extended definition for semantic relations, including semantic restrictions for its arguments. For example, AGENT(x, y ) holds between an animate concrete object x and a situation y. Moreover, we propose to characterize relations by semantic primitives. Primitives indicate whether a pr"
P11-1146,P01-1019,0,0.0435576,"entailments associated with certain predicates and arguments (Dowty, 2001). There has not been much work on composing relations in the field of computational linguistics. The term compositional semantics is used in conjunction with the principle of compositionality, i.e., the meaning of a complex expression is determined from the meanings of its parts, and the way in which those parts are combined. These approaches are usually formal and use a potentially infinite set of predicates to represent semantics. Ge and Mooney (2009) extracts semantic representations using syntactic structures while Copestake et al. (2001) develops algebras for semantic construction within grammars. Logic approaches include (Lakoff, 1970; S´anchez Valencia, 1991; MacCartney and Manning, 2009). Composition of Semantic Relations is complimentary to Compositional Semantics. Previous research has manually extracted plausible inference axioms for WordNet relations (Harabagiu and Moldovan, 1998) and transformed chains of relations into theoretical axioms (Helbig, 2005). The CSR algorithm proposed here automatically obtains inference axioms. Composing relations has been proposed before within knowledge bases. Cohen and Losielle (1988)"
P11-1146,P09-1069,0,0.0287744,"mes to perform linguistic analysis. Dowty (2006) studies compositionality and identifies entailments associated with certain predicates and arguments (Dowty, 2001). There has not been much work on composing relations in the field of computational linguistics. The term compositional semantics is used in conjunction with the principle of compositionality, i.e., the meaning of a complex expression is determined from the meanings of its parts, and the way in which those parts are combined. These approaches are usually formal and use a potentially infinite set of predicates to represent semantics. Ge and Mooney (2009) extracts semantic representations using syntactic structures while Copestake et al. (2001) develops algebras for semantic construction within grammars. Logic approaches include (Lakoff, 1970; S´anchez Valencia, 1991; MacCartney and Manning, 2009). Composition of Semantic Relations is complimentary to Compositional Semantics. Previous research has manually extracted plausible inference axioms for WordNet relations (Harabagiu and Moldovan, 1998) and transformed chains of relations into theoretical axioms (Helbig, 2005). The CSR algorithm proposed here automatically obtains inference axioms. Com"
P11-1146,P10-1160,0,0.128766,"uctivity. The last three columns in Table 6 show results when using the heuristic. 1462 6 Comparison with Previous Work There have been many proposals to detect semantic relations from text without composition. Researches have targeted particular relations (e.g., CAUSE (Chang and Choi, 2006; Bethard and Martin, 2008)), relations within noun phrases (Nulty, 2007), named entities (Hirano et al., 2007) or clauses (Szpakowicz et al., 1995). Competitions include (Litkowski, 2004; Carreras and M`arquez, 2005; Girju et al., 2007; Hendrickx et al., 2009). Two recent efforts (Ruppenhofer et al., 2009; Gerber and Chai, 2010) are similar to CSR in their goal (i.e., extract meaning ignored by current semantic parsers), but completely differ in their means. Their merit relies on annotating and extracting semantic connections not originally contemplated (e.g., between concepts from two different sentences) using an already known and fixed relation set. Unlike CSR, they are dependent on the relation inventory, require annotation and do not reason or manipulate relations. In contrast to all the above references and the state of the art, the proposed framework obtains axioms that take as input semantic relations produce"
P11-1146,S07-1003,0,0.241537,"Missing"
P11-1146,W09-2415,0,0.143166,"Missing"
P11-1146,P07-2040,0,0.0291223,"do not instantiate an axiom R1 (x, y) ◦ R2 (y, z) → R3 (x, z) if a relation of the form R3 (x’, z) is already known. This simple heuristic has increased the accuracy of the inferences at the cost of lowering their productivity. The last three columns in Table 6 show results when using the heuristic. 1462 6 Comparison with Previous Work There have been many proposals to detect semantic relations from text without composition. Researches have targeted particular relations (e.g., CAUSE (Chang and Choi, 2006; Bethard and Martin, 2008)), relations within noun phrases (Nulty, 2007), named entities (Hirano et al., 2007) or clauses (Szpakowicz et al., 1995). Competitions include (Litkowski, 2004; Carreras and M`arquez, 2005; Girju et al., 2007; Hendrickx et al., 2009). Two recent efforts (Ruppenhofer et al., 2009; Gerber and Chai, 2010) are similar to CSR in their goal (i.e., extract meaning ignored by current semantic parsers), but completely differ in their means. Their merit relies on annotating and extracting semantic connections not originally contemplated (e.g., between concepts from two different sentences) using an already known and fixed relation set. Unlike CSR, they are dependent on the relation in"
P11-1146,W04-0803,0,0.0163488,"e form R3 (x’, z) is already known. This simple heuristic has increased the accuracy of the inferences at the cost of lowering their productivity. The last three columns in Table 6 show results when using the heuristic. 1462 6 Comparison with Previous Work There have been many proposals to detect semantic relations from text without composition. Researches have targeted particular relations (e.g., CAUSE (Chang and Choi, 2006; Bethard and Martin, 2008)), relations within noun phrases (Nulty, 2007), named entities (Hirano et al., 2007) or clauses (Szpakowicz et al., 1995). Competitions include (Litkowski, 2004; Carreras and M`arquez, 2005; Girju et al., 2007; Hendrickx et al., 2009). Two recent efforts (Ruppenhofer et al., 2009; Gerber and Chai, 2010) are similar to CSR in their goal (i.e., extract meaning ignored by current semantic parsers), but completely differ in their means. Their merit relies on annotating and extracting semantic connections not originally contemplated (e.g., between concepts from two different sentences) using an already known and fixed relation set. Unlike CSR, they are dependent on the relation inventory, require annotation and do not reason or manipulate relations. In co"
P11-1146,W09-3714,0,0.0218556,"tational linguistics. The term compositional semantics is used in conjunction with the principle of compositionality, i.e., the meaning of a complex expression is determined from the meanings of its parts, and the way in which those parts are combined. These approaches are usually formal and use a potentially infinite set of predicates to represent semantics. Ge and Mooney (2009) extracts semantic representations using syntactic structures while Copestake et al. (2001) develops algebras for semantic construction within grammars. Logic approaches include (Lakoff, 1970; S´anchez Valencia, 1991; MacCartney and Manning, 2009). Composition of Semantic Relations is complimentary to Compositional Semantics. Previous research has manually extracted plausible inference axioms for WordNet relations (Harabagiu and Moldovan, 1998) and transformed chains of relations into theoretical axioms (Helbig, 2005). The CSR algorithm proposed here automatically obtains inference axioms. Composing relations has been proposed before within knowledge bases. Cohen and Losielle (1988) combines a set of nine fairly specific relations (e.g., 1463 FOCUS - OF, PRODUCT- OF, SETTING - OF). The key to determine plausibility is the transitivity"
P11-1146,P07-3014,0,0.0290883,"in order to improve accuracy: do not instantiate an axiom R1 (x, y) ◦ R2 (y, z) → R3 (x, z) if a relation of the form R3 (x’, z) is already known. This simple heuristic has increased the accuracy of the inferences at the cost of lowering their productivity. The last three columns in Table 6 show results when using the heuristic. 1462 6 Comparison with Previous Work There have been many proposals to detect semantic relations from text without composition. Researches have targeted particular relations (e.g., CAUSE (Chang and Choi, 2006; Bethard and Martin, 2008)), relations within noun phrases (Nulty, 2007), named entities (Hirano et al., 2007) or clauses (Szpakowicz et al., 1995). Competitions include (Litkowski, 2004; Carreras and M`arquez, 2005; Girju et al., 2007; Hendrickx et al., 2009). Two recent efforts (Ruppenhofer et al., 2009; Gerber and Chai, 2010) are similar to CSR in their goal (i.e., extract meaning ignored by current semantic parsers), but completely differ in their means. Their merit relies on annotating and extracting semantic connections not originally contemplated (e.g., between concepts from two different sentences) using an already known and fixed relation set. Unlike CSR,"
P11-1146,J05-1004,0,0.172045,"in the box) or temporal (tmp, e.g., yesterday, last month) context of an entity. This simplified ontology does not aim at defining domains and ranges for any relation set; it is a simplification to fit the eight relations we work with. 5 Evaluation An evaluation was performed to estimate the validity of the 78 axioms. Because the number of axioms is large we have focused on a subset of them (Table 6). The 31 axioms having SYN as premise are intuitively correct: since synonymous concepts are interchangeable, given veracious annotation they perform valid inferences. We use PropBank annotation (Palmer et al., 2005) to instantiate the premises of each axiom. First, all instantiations of axiom PRP ◦ MNR−1 → MNR−1 were manually checked. This axiom yields 237 new MANNER, 189 of which are valid (Accuracy 0.80). Second, we evaluated axioms 1–7 (Table 6). Since PropBank is a large corpus, we restricted this phase to the first 1,000 sentences in which there is an instantiation of any axiom. These sentences contain 1,412 instantiations and are found in the first 31,450 sentences of PropBank. Table 6 depicts the total number of instantiations for each axiom and its accuracy (columns 3 and 4). Accuracies range fro"
P11-1146,W09-2417,0,0.286512,"ost of lowering their productivity. The last three columns in Table 6 show results when using the heuristic. 1462 6 Comparison with Previous Work There have been many proposals to detect semantic relations from text without composition. Researches have targeted particular relations (e.g., CAUSE (Chang and Choi, 2006; Bethard and Martin, 2008)), relations within noun phrases (Nulty, 2007), named entities (Hirano et al., 2007) or clauses (Szpakowicz et al., 1995). Competitions include (Litkowski, 2004; Carreras and M`arquez, 2005; Girju et al., 2007; Hendrickx et al., 2009). Two recent efforts (Ruppenhofer et al., 2009; Gerber and Chai, 2010) are similar to CSR in their goal (i.e., extract meaning ignored by current semantic parsers), but completely differ in their means. Their merit relies on annotating and extracting semantic connections not originally contemplated (e.g., between concepts from two different sentences) using an already known and fixed relation set. Unlike CSR, they are dependent on the relation inventory, require annotation and do not reason or manipulate relations. In contrast to all the above references and the state of the art, the proposed framework obtains axioms that take as input se"
P11-1146,S10-1006,0,\N,Missing
P16-1142,P98-1013,0,0.019763,"-anchored spatial knowledge. Extracting additional meaning on top of popular corpora is by no means a new problem. Ger2 We use the CoNLL 2011 Shared Task release (Pradhan et al., 2011), http://conll.cemantix.org/2011/ 3 https://catalog.ldc.upenn.edu/LDC2013T19 ber and Chai (2010) augmented NomBank (Meyers et al., 2004) annotations with additional numbered arguments appearing in the same or previous sentences, and Laparra and Rigau (2013) presented an improved algorithm for the same task. The SemEval-2010 Task 10 (Ruppenhofer et al., 2009) targeted cross-sentence missing arguments in FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005). Silberer and Frank (2012) casted the SemEval task as an anaphora resolution task. We have previously proposed an unsupervised framework to compose semantic relations out of previously extracted relations (Blanco and Moldovan, 2011), and a supervised approach to infer additional argument modifiers (ARGM) for verbs in PropBank (Blanco and Moldovan, 2014). Unlike the current work, these previous efforts (1) improve the semantic representation of verbal and nominal predicates, or (2) infer relations between arguments of the same predicate. More recently, we sho"
P16-1142,P08-2045,0,0.0322903,"for how long (seconds, hours, days, etc.). Crowdsourced annotations show that this additional knowledge is intuitive to humans and can be annotated by non-experts. Experimental results show that the task can be automated. drove DESTINATION to Berlin to pick up a package Figure 1: Semantic roles (solid arrows) and additional spatial knowledge (dashed arrow). 1 Introduction Extracting meaning from text is crucial for true text understanding and an important component of several natural language processing systems. Among many others, previous efforts have focused on extracting causal relations (Bethard and Martin, 2008), semantic relations between nominals (Hendrickx et al., 2010), spatial relations (Kordjamshidi et al., 2011) and temporal relations (Pustejovsky et al., 2003; Chambers et al., 2014). In terms of corpora development and automated approaches, semantic roles are one of the most studied semantic representations (Toutanova et al., 2005; M`arquez et al., 2008). They have been proven useful for, among others, coreference resolution (Ponzetto and Strube, 2006) and question answering (Shen and Lapata, 2007). While semantic roles provide a useful semantic layer, they capture a portion of the meaning en"
P16-1142,P11-1146,1,0.828334,"ber and Chai (2010) augmented NomBank (Meyers et al., 2004) annotations with additional numbered arguments appearing in the same or previous sentences, and Laparra and Rigau (2013) presented an improved algorithm for the same task. The SemEval-2010 Task 10 (Ruppenhofer et al., 2009) targeted cross-sentence missing arguments in FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005). Silberer and Frank (2012) casted the SemEval task as an anaphora resolution task. We have previously proposed an unsupervised framework to compose semantic relations out of previously extracted relations (Blanco and Moldovan, 2011), and a supervised approach to infer additional argument modifiers (ARGM) for verbs in PropBank (Blanco and Moldovan, 2014). Unlike the current work, these previous efforts (1) improve the semantic representation of verbal and nominal predicates, or (2) infer relations between arguments of the same predicate. More recently, we showed that spatial relations can be inferred from PropBank-style semantic roles (Blanco and Vempala, 2015; Vempala and Blanco, 2016). In this paper, we expand on this idea as follows. First, we not only extract whether “x has LOCATION y ” before, during or after an even"
P16-1142,E14-1016,1,0.800561,"same or previous sentences, and Laparra and Rigau (2013) presented an improved algorithm for the same task. The SemEval-2010 Task 10 (Ruppenhofer et al., 2009) targeted cross-sentence missing arguments in FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005). Silberer and Frank (2012) casted the SemEval task as an anaphora resolution task. We have previously proposed an unsupervised framework to compose semantic relations out of previously extracted relations (Blanco and Moldovan, 2011), and a supervised approach to infer additional argument modifiers (ARGM) for verbs in PropBank (Blanco and Moldovan, 2014). Unlike the current work, these previous efforts (1) improve the semantic representation of verbal and nominal predicates, or (2) infer relations between arguments of the same predicate. More recently, we showed that spatial relations can be inferred from PropBank-style semantic roles (Blanco and Vempala, 2015; Vempala and Blanco, 2016). In this paper, we expand on this idea as follows. First, we not only extract whether “x has LOCATION y ” before, during or after an event, but also specify for how long before and after (seconds, minutes, hours, days, weeks, months, years, etc.). Second, we r"
P16-1142,N15-1048,1,0.74164,"al task as an anaphora resolution task. We have previously proposed an unsupervised framework to compose semantic relations out of previously extracted relations (Blanco and Moldovan, 2011), and a supervised approach to infer additional argument modifiers (ARGM) for verbs in PropBank (Blanco and Moldovan, 2014). Unlike the current work, these previous efforts (1) improve the semantic representation of verbal and nominal predicates, or (2) infer relations between arguments of the same predicate. More recently, we showed that spatial relations can be inferred from PropBank-style semantic roles (Blanco and Vempala, 2015; Vempala and Blanco, 2016). In this paper, we expand on this idea as follows. First, we not only extract whether “x has LOCATION y ” before, during or after an event, but also specify for how long before and after (seconds, minutes, hours, days, weeks, months, years, etc.). Second, we release crowdsourced annotations for 1,732 potential additional spatial relations. Third, we experiment with both gold and predicted linguistic information. Spatial semantics has received considerable attention in the last decade. The task of spatial role labeling (Kordjamshidi et al., 2011; Kolomiyets et al., 2"
P16-1142,W05-0620,0,0.150968,"Missing"
P16-1142,Q14-1022,0,0.014399,"show that the task can be automated. drove DESTINATION to Berlin to pick up a package Figure 1: Semantic roles (solid arrows) and additional spatial knowledge (dashed arrow). 1 Introduction Extracting meaning from text is crucial for true text understanding and an important component of several natural language processing systems. Among many others, previous efforts have focused on extracting causal relations (Bethard and Martin, 2008), semantic relations between nominals (Hendrickx et al., 2010), spatial relations (Kordjamshidi et al., 2011) and temporal relations (Pustejovsky et al., 2003; Chambers et al., 2014). In terms of corpora development and automated approaches, semantic roles are one of the most studied semantic representations (Toutanova et al., 2005; M`arquez et al., 2008). They have been proven useful for, among others, coreference resolution (Ponzetto and Strube, 2006) and question answering (Shen and Lapata, 2007). While semantic roles provide a useful semantic layer, they capture a portion of the meaning encoded in all but the simplest statements. Consider the sentence in Figure 1 and the semantic roles of drove (solid arrows). In addition to these roles, humans intuitively understand"
P16-1142,P10-1160,0,0.0683689,"Missing"
P16-1142,J02-3001,0,0.0171846,"n and development sets, and results are calculated using the test set. During the tuning process, we discovered that it is beneficial to train one SVM per temporal anchor instead of a single model for the 3 temporal anchors. 5.1 for extracting temporally-anchored spatial knowledge from semantic roles. In order to determine whether x is (or is not) located at y and for how long, we extract features from x and y, the verbs to which they attach (x verb and y verb ) and all semantic roles of x verb and y verb (x roles and y roles ). Basic, lexical and heads features are standard in role labeling (Gildea and Jurafsky, 2002). Basic features are the word form and part-of-speech of x verb and y verb . Lexical features capture the first and last words of x and y and their part-of-speech tags, as well as a binary flag indicating whether x occurs before or after y. Heads features capture the heads of x and y and their part-of-speech tags, as well as their named entity types, if any. Semantic features include features 20–27. Feature 20 indicates the semantic role linking x and x verb (ARG0 , ARG1 , ARG2 , etc.); recall that the semantic role between y and yverb is always ARGM LOC (Section 4.1). Features 21–24 are count"
P16-1142,S10-1006,0,0.0606231,"Missing"
P16-1142,N06-2015,0,0.0678388,"d y is an argument of x. Generally speaking, semantic roles capture who did what to whom, how, when and where. We use the term additional spatial knowledge to refer to spatial knowledge not captured with semantic roles, i.e., spatial meaning between x and y where (1) x is not a predicate or (2) x is a predicate and y is not an argument of x. As we shall see, we go beyond extracting “x has LOCATION y ” with plain LOCATION(x, y ) relations. We extract where entities are and are not located, and for how long they are located (and not located) somewhere. 2.1 Semantic Roles in OntoNotes OntoNotes (Hovy et al., 2006) is large corpus (≈64K sentences) that includes verbal semantic role annotations, i.e., the first argument x of any role R(x, y ) is a verb.2 OntoNotes semantic roles follow PropBank framesets (Palmer et al., 2005). It uses a set of numbered arguments (ARG0 –ARG5 ) whose meanings are verb-dependent, e.g., ARG2 is used for “employer” with verb work.01 and “expected terminus of sleep” with verb sleep.01. Additionally, it uses argument modifiers which share a common meaning across verbs (ARGM LOC , ARGM - TMP, ARGM - PRP, ARGM - CAU , etc.). For a detailed description of OntoNotes semantic roles,"
P16-1142,S15-1006,0,0.0160787,"d considerable attention in the last decade. The task of spatial role labeling (Kordjamshidi et al., 2011; Kolomiyets et al., 2013) aims at representing spatial information with so-called spatial roles, e.g., trajector, landmark, spatial and motion indicators, etc. Unlike us, spatial role labeling does not aim at extracting where entities are not located or temporally-anchored spatial information. But doing so is intuitive to humans, as the examples and crowdsourced annotations in this paper show. Spatial knowledge is intuitively associated with motion events, e.g., drive, go, fly, walk, run. Hwang and Palmer (2015) presented a classifier to detect caused motion constructions triggered by non-motion verbs, e.g., The crowd laughed the clown off the stage (i.e., the crowd made the clown leave the stage). Our work does not target motion verbs or motion constructions, 1503 as the examples in Table 3 show, non-motion constructions triggered by non-motion verbs also allow us to infer temporally-anchored spatial meaning, e.g., played, honored, taught, fighting. 4 Corpus Creation and Analysis Our goal is to complement semantic role representations with additional spatial knowledge. Specifically, our goal is to i"
P16-1142,S13-2044,0,0.0229178,"nd M`arquez, 2005), state-of-the-art tools sobtain Fmeasures of 83.5 (Lewis et al., 2015). In this paper, we complement semantic role representations with temporally-anchored spatial knowledge. Extracting additional meaning on top of popular corpora is by no means a new problem. Ger2 We use the CoNLL 2011 Shared Task release (Pradhan et al., 2011), http://conll.cemantix.org/2011/ 3 https://catalog.ldc.upenn.edu/LDC2013T19 ber and Chai (2010) augmented NomBank (Meyers et al., 2004) annotations with additional numbered arguments appearing in the same or previous sentences, and Laparra and Rigau (2013) presented an improved algorithm for the same task. The SemEval-2010 Task 10 (Ruppenhofer et al., 2009) targeted cross-sentence missing arguments in FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005). Silberer and Frank (2012) casted the SemEval task as an anaphora resolution task. We have previously proposed an unsupervised framework to compose semantic relations out of previously extracted relations (Blanco and Moldovan, 2011), and a supervised approach to infer additional argument modifiers (ARGM) for verbs in PropBank (Blanco and Moldovan, 2014). Unlike the current work, thes"
P16-1142,P13-1116,0,0.0142603,"years (Carreras and M`arquez, 2005), state-of-the-art tools sobtain Fmeasures of 83.5 (Lewis et al., 2015). In this paper, we complement semantic role representations with temporally-anchored spatial knowledge. Extracting additional meaning on top of popular corpora is by no means a new problem. Ger2 We use the CoNLL 2011 Shared Task release (Pradhan et al., 2011), http://conll.cemantix.org/2011/ 3 https://catalog.ldc.upenn.edu/LDC2013T19 ber and Chai (2010) augmented NomBank (Meyers et al., 2004) annotations with additional numbered arguments appearing in the same or previous sentences, and Laparra and Rigau (2013) presented an improved algorithm for the same task. The SemEval-2010 Task 10 (Ruppenhofer et al., 2009) targeted cross-sentence missing arguments in FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005). Silberer and Frank (2012) casted the SemEval task as an anaphora resolution task. We have previously proposed an unsupervised framework to compose semantic relations out of previously extracted relations (Blanco and Moldovan, 2011), and a supervised approach to infer additional argument modifiers (ARGM) for verbs in PropBank (Blanco and Moldovan, 2014). Unlike the current work, thes"
P16-1142,D15-1169,0,0.0148001,"nd “expected terminus of sleep” with verb sleep.01. Additionally, it uses argument modifiers which share a common meaning across verbs (ARGM LOC , ARGM - TMP, ARGM - PRP, ARGM - CAU , etc.). For a detailed description of OntoNotes semantic roles, we refer the reader to the LDC catalog3 and PropBank (Palmer et al., 2005). To improve readability, we often rename numbered arguments, e.g., AGENT instead of ARG0 in Figure 1. 3 Related Work Approaches to extract PropBank-style semantic roles have been studied for years (Carreras and M`arquez, 2005), state-of-the-art tools sobtain Fmeasures of 83.5 (Lewis et al., 2015). In this paper, we complement semantic role representations with temporally-anchored spatial knowledge. Extracting additional meaning on top of popular corpora is by no means a new problem. Ger2 We use the CoNLL 2011 Shared Task release (Pradhan et al., 2011), http://conll.cemantix.org/2011/ 3 https://catalog.ldc.upenn.edu/LDC2013T19 ber and Chai (2010) augmented NomBank (Meyers et al., 2004) annotations with additional numbered arguments appearing in the same or previous sentences, and Laparra and Rigau (2013) presented an improved algorithm for the same task. The SemEval-2010 Task 10 (Ruppe"
P16-1142,J08-2001,0,0.0398629,"Missing"
P16-1142,meyers-etal-2004-annotating,0,0.0607301,"instead of ARG0 in Figure 1. 3 Related Work Approaches to extract PropBank-style semantic roles have been studied for years (Carreras and M`arquez, 2005), state-of-the-art tools sobtain Fmeasures of 83.5 (Lewis et al., 2015). In this paper, we complement semantic role representations with temporally-anchored spatial knowledge. Extracting additional meaning on top of popular corpora is by no means a new problem. Ger2 We use the CoNLL 2011 Shared Task release (Pradhan et al., 2011), http://conll.cemantix.org/2011/ 3 https://catalog.ldc.upenn.edu/LDC2013T19 ber and Chai (2010) augmented NomBank (Meyers et al., 2004) annotations with additional numbered arguments appearing in the same or previous sentences, and Laparra and Rigau (2013) presented an improved algorithm for the same task. The SemEval-2010 Task 10 (Ruppenhofer et al., 2009) targeted cross-sentence missing arguments in FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005). Silberer and Frank (2012) casted the SemEval task as an anaphora resolution task. We have previously proposed an unsupervised framework to compose semantic relations out of previously extracted relations (Blanco and Moldovan, 2011), and a supervised approach to in"
P16-1142,J05-1004,0,0.768265,"roles, i.e., spatial meaning between x and y where (1) x is not a predicate or (2) x is a predicate and y is not an argument of x. As we shall see, we go beyond extracting “x has LOCATION y ” with plain LOCATION(x, y ) relations. We extract where entities are and are not located, and for how long they are located (and not located) somewhere. 2.1 Semantic Roles in OntoNotes OntoNotes (Hovy et al., 2006) is large corpus (≈64K sentences) that includes verbal semantic role annotations, i.e., the first argument x of any role R(x, y ) is a verb.2 OntoNotes semantic roles follow PropBank framesets (Palmer et al., 2005). It uses a set of numbered arguments (ARG0 –ARG5 ) whose meanings are verb-dependent, e.g., ARG2 is used for “employer” with verb work.01 and “expected terminus of sleep” with verb sleep.01. Additionally, it uses argument modifiers which share a common meaning across verbs (ARGM LOC , ARGM - TMP, ARGM - PRP, ARGM - CAU , etc.). For a detailed description of OntoNotes semantic roles, we refer the reader to the LDC catalog3 and PropBank (Palmer et al., 2005). To improve readability, we often rename numbered arguments, e.g., AGENT instead of ARG0 in Figure 1. 3 Related Work Approaches to extract"
P16-1142,N06-1025,0,0.0494271,"tant component of several natural language processing systems. Among many others, previous efforts have focused on extracting causal relations (Bethard and Martin, 2008), semantic relations between nominals (Hendrickx et al., 2010), spatial relations (Kordjamshidi et al., 2011) and temporal relations (Pustejovsky et al., 2003; Chambers et al., 2014). In terms of corpora development and automated approaches, semantic roles are one of the most studied semantic representations (Toutanova et al., 2005; M`arquez et al., 2008). They have been proven useful for, among others, coreference resolution (Ponzetto and Strube, 2006) and question answering (Shen and Lapata, 2007). While semantic roles provide a useful semantic layer, they capture a portion of the meaning encoded in all but the simplest statements. Consider the sentence in Figure 1 and the semantic roles of drove (solid arrows). In addition to these roles, humans intuitively understand that (dashed arrow) (1) John was not located in Berlin before or during drove, (2) he was located in Berlin after drove for a short period of time (presumably, until he was done picking up the package, i.e., for a few minutes to an hour), and then left Berlin and thus (3) wa"
P16-1142,W11-1901,0,0.487924,"e reader to the LDC catalog3 and PropBank (Palmer et al., 2005). To improve readability, we often rename numbered arguments, e.g., AGENT instead of ARG0 in Figure 1. 3 Related Work Approaches to extract PropBank-style semantic roles have been studied for years (Carreras and M`arquez, 2005), state-of-the-art tools sobtain Fmeasures of 83.5 (Lewis et al., 2015). In this paper, we complement semantic role representations with temporally-anchored spatial knowledge. Extracting additional meaning on top of popular corpora is by no means a new problem. Ger2 We use the CoNLL 2011 Shared Task release (Pradhan et al., 2011), http://conll.cemantix.org/2011/ 3 https://catalog.ldc.upenn.edu/LDC2013T19 ber and Chai (2010) augmented NomBank (Meyers et al., 2004) annotations with additional numbered arguments appearing in the same or previous sentences, and Laparra and Rigau (2013) presented an improved algorithm for the same task. The SemEval-2010 Task 10 (Ruppenhofer et al., 2009) targeted cross-sentence missing arguments in FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005). Silberer and Frank (2012) casted the SemEval task as an anaphora resolution task. We have previously proposed an unsupervised fr"
P16-1142,W09-2417,0,0.0430623,"Missing"
P16-1142,D07-1002,0,0.0232251,"ng systems. Among many others, previous efforts have focused on extracting causal relations (Bethard and Martin, 2008), semantic relations between nominals (Hendrickx et al., 2010), spatial relations (Kordjamshidi et al., 2011) and temporal relations (Pustejovsky et al., 2003; Chambers et al., 2014). In terms of corpora development and automated approaches, semantic roles are one of the most studied semantic representations (Toutanova et al., 2005; M`arquez et al., 2008). They have been proven useful for, among others, coreference resolution (Ponzetto and Strube, 2006) and question answering (Shen and Lapata, 2007). While semantic roles provide a useful semantic layer, they capture a portion of the meaning encoded in all but the simplest statements. Consider the sentence in Figure 1 and the semantic roles of drove (solid arrows). In addition to these roles, humans intuitively understand that (dashed arrow) (1) John was not located in Berlin before or during drove, (2) he was located in Berlin after drove for a short period of time (presumably, until he was done picking up the package, i.e., for a few minutes to an hour), and then left Berlin and thus (3) was not located there anymore. Some of this addit"
P16-1142,S12-1001,0,0.013065,"ng on top of popular corpora is by no means a new problem. Ger2 We use the CoNLL 2011 Shared Task release (Pradhan et al., 2011), http://conll.cemantix.org/2011/ 3 https://catalog.ldc.upenn.edu/LDC2013T19 ber and Chai (2010) augmented NomBank (Meyers et al., 2004) annotations with additional numbered arguments appearing in the same or previous sentences, and Laparra and Rigau (2013) presented an improved algorithm for the same task. The SemEval-2010 Task 10 (Ruppenhofer et al., 2009) targeted cross-sentence missing arguments in FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005). Silberer and Frank (2012) casted the SemEval task as an anaphora resolution task. We have previously proposed an unsupervised framework to compose semantic relations out of previously extracted relations (Blanco and Moldovan, 2011), and a supervised approach to infer additional argument modifiers (ARGM) for verbs in PropBank (Blanco and Moldovan, 2014). Unlike the current work, these previous efforts (1) improve the semantic representation of verbal and nominal predicates, or (2) infer relations between arguments of the same predicate. More recently, we showed that spatial relations can be inferred from PropBank-style"
P16-1142,P05-1073,0,0.0363176,"nowledge (dashed arrow). 1 Introduction Extracting meaning from text is crucial for true text understanding and an important component of several natural language processing systems. Among many others, previous efforts have focused on extracting causal relations (Bethard and Martin, 2008), semantic relations between nominals (Hendrickx et al., 2010), spatial relations (Kordjamshidi et al., 2011) and temporal relations (Pustejovsky et al., 2003; Chambers et al., 2014). In terms of corpora development and automated approaches, semantic roles are one of the most studied semantic representations (Toutanova et al., 2005; M`arquez et al., 2008). They have been proven useful for, among others, coreference resolution (Ponzetto and Strube, 2006) and question answering (Shen and Lapata, 2007). While semantic roles provide a useful semantic layer, they capture a portion of the meaning encoded in all but the simplest statements. Consider the sentence in Figure 1 and the semantic roles of drove (solid arrows). In addition to these roles, humans intuitively understand that (dashed arrow) (1) John was not located in Berlin before or during drove, (2) he was located in Berlin after drove for a short period of time (pre"
P16-1142,C98-1013,0,\N,Missing
P17-2101,P16-1166,0,0.0614771,"Missing"
P17-2101,D14-1108,0,0.0609625,"Missing"
P17-2101,D14-1214,0,0.0187563,"Doe getting married, having a baby, being promoted). Extracting events of general importance often includes extracting the entities involved, date and location, and classifying events into classes such as trial, product launch or death (Ritter et al., 2012). Exploiting redundancy in tweets to extract events is common (Zhou et al., 2014), as well as spatio-temporal information (Cheng and Wicks, 2014), i.e., when and where tweets originate from. Extracting major life events consists on pinpointing significant events from mundane events (e.g., having lunch, exercising) (Di Eugenio et al., 2013; Li et al., 2014; Dickinson et al., 2015), and determining whether significant events are relevant to Twitter users (e.g., Why doesn’t John marry Mary already? [not relevant to the author]). Unlike these previous efforts, the work proposed here determines whether people participate in the events they tweet about, and specifies when with respect to tweet timestamps. As a result, we target past events, ongoing events, and events likely to occur in the future. Additionally, we target all events regardless of importance. Introduction Twitter has quickly become one of the most popular social media sites: it has 31"
P17-2101,H92-1116,0,0.588583,"Missing"
P17-2101,N13-1039,0,0.0885116,"Missing"
P17-2101,D11-1141,0,0.129429,"Missing"
P17-2101,P14-2114,0,0.0189177,"mentions, and experimental results showing that the task is challenging. 1 2 Previous Work Most previous efforts on detecting events from Twitter focus on events of general importance (e.g., death of a celebrity, natural disasters) or major life events of individuals (e.g. John Doe getting married, having a baby, being promoted). Extracting events of general importance often includes extracting the entities involved, date and location, and classifying events into classes such as trial, product launch or death (Ritter et al., 2012). Exploiting redundancy in tweets to extract events is common (Zhou et al., 2014), as well as spatio-temporal information (Cheng and Wicks, 2014), i.e., when and where tweets originate from. Extracting major life events consists on pinpointing significant events from mundane events (e.g., having lunch, exercising) (Di Eugenio et al., 2013; Li et al., 2014; Dickinson et al., 2015), and determining whether significant events are relevant to Twitter users (e.g., Why doesn’t John marry Mary already? [not relevant to the author]). Unlike these previous efforts, the work proposed here determines whether people participate in the events they tweet about, and specifies when with r"
S12-1035,P11-1059,1,0.233104,"he GENIA tagger (Tsuruoka and Tsujii, 2005), version 3.0.1, with the ’-nt’ command line option. GENIA PoS tags are complemented with TnT PoS tags for increased compatibility with the original PTB. • Parsing with the Charniak and Johnson (2005) reranking parser.5 For compatibility with PTB conventions, the top-level nodes in parse trees (‘S1’), were removed. The conversion of PTB-style syntax trees into CoNLL-style format was performed using the CoNLL 2005 Shared Task software.6 PB-FOC: Focus Annotation We have adapted the only previous annotation effort targeting focus of negation for PB-FOC (Blanco and Moldovan, 2011). This corpus provides focus annotation on top of PropBank. It targets exclusively verbal negations marked with MNEG in PropBank and selects as focus the semantic role containing the most likely focus. The motivation behind their approach, annotation guidelines and examples can be found in the aforementioned paper. We gathered all negations from sections 02–21, 23 and 24 and discarded negations for which the focus or PropBank annotations were not sound, leaving 3,544 instances.7 For each verbal negation, PBFOC provides the current sentence, and the previous and next sentences as context. For e"
S12-1035,H92-1022,0,0.0728387,"ntic role containing the most likely focus. The motivation behind their approach, annotation guidelines and examples can be found in the aforementioned paper. We gathered all negations from sections 02–21, 23 and 24 and discarded negations for which the focus or PropBank annotations were not sound, leaving 3,544 instances.7 For each verbal negation, PBFOC provides the current sentence, and the previous and next sentences as context. For each sentence, along with the gold focus annotations, PB-FOC contains the following additional annotations: • Token number; • POS tags using the Brill tagger (Brill, 1992); • Named Entities using the Stanford named entity recognizer recognizer (Finkel et al., 2005); • Chunks using the chunker by Phan (2006); • Syntactic tree using the Charniak parser (Charniak, 2000); • Dependency tree derived from the syntactic tree (de Marneffe et al., 2006); ErgTokenization, http://moin.delph-in.net/ ReppTop 5 November 2009 release available from Brown University. 6 http://www.lsi.upc.edu/˜srlconll/ srlconll-1.1.tgz 7 The original focus annotation targeted the 3,993 negations marked with MNEG in the whole PropBank. 269 1 role 2 roles 3 roles All A1 AM-NEG AM-TMP AM-MNR A2 A0"
S12-1035,P05-1022,0,0.0141545,"ocessed at the University of Oslo. Tokenization was obtained by the PTBcompliant tokenizer that is part of the LinGO English Resource Grammar. 4 4 268 Training 65,450 3644 848 23.27 984 30 887 616 http://moin.delph-in.net/ Apart from the gold annotations, the corpus was provided to participants with additional annotations: 3.2 Semantic roles focus belongs to • Lemmatization using the GENIA tagger (Tsuruoka and Tsujii, 2005), version 3.0.1, with the ’-nt’ command line option. GENIA PoS tags are complemented with TnT PoS tags for increased compatibility with the original PTB. • Parsing with the Charniak and Johnson (2005) reranking parser.5 For compatibility with PTB conventions, the top-level nodes in parse trees (‘S1’), were removed. The conversion of PTB-style syntax trees into CoNLL-style format was performed using the CoNLL 2005 Shared Task software.6 PB-FOC: Focus Annotation We have adapted the only previous annotation effort targeting focus of negation for PB-FOC (Blanco and Moldovan, 2011). This corpus provides focus annotation on top of PropBank. It targets exclusively verbal negations marked with MNEG in PropBank and selects as focus the semantic role containing the most likely focus. The motivation"
S12-1035,A00-2018,0,0.00565266,"ons 02–21, 23 and 24 and discarded negations for which the focus or PropBank annotations were not sound, leaving 3,544 instances.7 For each verbal negation, PBFOC provides the current sentence, and the previous and next sentences as context. For each sentence, along with the gold focus annotations, PB-FOC contains the following additional annotations: • Token number; • POS tags using the Brill tagger (Brill, 1992); • Named Entities using the Stanford named entity recognizer recognizer (Finkel et al., 2005); • Chunks using the chunker by Phan (2006); • Syntactic tree using the Charniak parser (Charniak, 2000); • Dependency tree derived from the syntactic tree (de Marneffe et al., 2006); ErgTokenization, http://moin.delph-in.net/ ReppTop 5 November 2009 release available from Brown University. 6 http://www.lsi.upc.edu/˜srlconll/ srlconll-1.1.tgz 7 The original focus annotation targeted the 3,993 negations marked with MNEG in the whole PropBank. 269 1 role 2 roles 3 roles All A1 AM-NEG AM-TMP AM-MNR A2 A0 None AM-ADV C-A1 AM-PNC AM-LOC A4 R-A1 Other Train 2,210 89 3 2,302 980 592 161 127 112 94 88 78 46 33 25 11 10 40 Devel 515 15 0 530 222 138 35 27 28 23 19 23 6 8 4 2 2 8 Test 672 38 2 712 309 172"
S12-1035,de-marneffe-etal-2006-generating,0,0.00947387,"Missing"
S12-1035,W10-3001,0,0.147806,"Missing"
S12-1035,P05-1045,0,0.00178046,"tation guidelines and examples can be found in the aforementioned paper. We gathered all negations from sections 02–21, 23 and 24 and discarded negations for which the focus or PropBank annotations were not sound, leaving 3,544 instances.7 For each verbal negation, PBFOC provides the current sentence, and the previous and next sentences as context. For each sentence, along with the gold focus annotations, PB-FOC contains the following additional annotations: • Token number; • POS tags using the Brill tagger (Brill, 1992); • Named Entities using the Stanford named entity recognizer recognizer (Finkel et al., 2005); • Chunks using the chunker by Phan (2006); • Syntactic tree using the Charniak parser (Charniak, 2000); • Dependency tree derived from the syntactic tree (de Marneffe et al., 2006); ErgTokenization, http://moin.delph-in.net/ ReppTop 5 November 2009 release available from Brown University. 6 http://www.lsi.upc.edu/˜srlconll/ srlconll-1.1.tgz 7 The original focus annotation targeted the 3,993 negations marked with MNEG in the whole PropBank. 269 1 role 2 roles 3 roles All A1 AM-NEG AM-TMP AM-MNR A2 A0 None AM-ADV C-A1 AM-PNC AM-LOC A4 R-A1 Other Train 2,210 89 3 2,302 980 592 161 127 112 94 88"
S12-1035,W09-1105,1,0.708954,"ince clinical reports and discharge summaries must be reliably interpreted and indexed. The annotation of negation and hedge cues and their scope in the BioScope corpus (Vincze et al., 2008) represented a pioneering effort. This corpus boosted research on scope resolution, especially since it was used in the CoNLL 2010 Shared Task (CoNLL ST 2010) on hedge detection (Farkas et al., 2010). Negation has also been studied in sentiment analysis (Wiegand et al., 2010) as a means to determine the polarity of sentiments and opinions. Whereas several scope detectors have been developed using BioScope (Morante and Daelemans, 2009; Velldal et al., 2012), there is a lack of corpora and tools to process negation in general domain texts. This is why we have prepared new corpora for scope and focus detection. Scope is annotated in Conan Doyle stories (CD-SCO corpus). For each negation, the cue, its scope and the negated event, if any, are marked as shown in example (1a). Focus is annotated on top of PropBank, which uses the WSJ section of the Penn TreeBank (PB-FOC corpus). Focus annotation is restricted to verbal negations annotated with MNEG in PropBank, and all the words belonging to a semantic role are selected as focus"
S12-1035,morante-daelemans-2012-conandoyle,1,0.60973,"Missing"
S12-1035,J12-2001,1,0.718204,"of negation. This paper presents the specifications, datasets and evaluation criteria of the task. An overview of participating systems is provided and their results are summarized. 1 Introduction Semantic representation of text has received considerable attention these past years. While early shallow approaches have been proven useful for several natural language processing applications (Wu and Fung, 2009; Surdeanu et al., 2003; Shen and Lapata, 2007), the field is moving towards analyzing and processing complex linguistic phenomena, such as metaphor (Shutova, 2010) or modality and negation (Morante and Sporleder, 2012). The *SEM 2012 Shared Task is devoted to negation, specifically, to resolving its scope and focus. Negation is a grammatical category that comprises devices used to reverse the truth value of propositions. Broadly speaking, scope is the part of the meaning that is negated and focus the part of the scope that is most prominently or explicitly negated (Huddleston and Pullum, 2002). Although negation is a very relevant and complex semantic aspect of language, current proposals to annotate meaning either dismiss negation or only treat it in a partial manner. The interest in automatically processi"
S12-1035,J08-2005,0,0.00730407,"inal focus annotation targeted the 3,993 negations marked with MNEG in the whole PropBank. 269 1 role 2 roles 3 roles All A1 AM-NEG AM-TMP AM-MNR A2 A0 None AM-ADV C-A1 AM-PNC AM-LOC A4 R-A1 Other Train 2,210 89 3 2,302 980 592 161 127 112 94 88 78 46 33 25 11 10 40 Devel 515 15 0 530 222 138 35 27 28 23 19 23 6 8 4 2 2 8 Test 672 38 2 712 309 172 46 38 36 31 35 26 16 12 10 5 2 16 Table 2: Basic numeric analysis for PB-FOC. The first 4 rows indicate the number of unique roles each negation belongs to, the rest indicate the counts for each role. • Semantic roles using the labeler described by (Punyakanok et al., 2008); and • Verbal negation, indicates with ‘N’ if that token correspond to a verbal negation for which focus must be predicted. Figure 2 provides a sample of PB-FOC. Knowing that the original focus annotations were done on top of PropBank and that focus corresponds to a single role, semantic role information is key to predict the focus. In Table 2, we show some basic numeric analysis regarding focus annotation and the automatically obtained semantic role labels. Most instances of focus belong to a single role in the three splits and the most common role focus belongs to is A1, followed by AM-NEG,"
S12-1035,D07-1002,0,0.00978739,"cs (*SEM) each year hosts a shared task on semantic related topics. In its first edition held in 2012, the shared task was dedicated to resolving the scope and focus of negation. This paper presents the specifications, datasets and evaluation criteria of the task. An overview of participating systems is provided and their results are summarized. 1 Introduction Semantic representation of text has received considerable attention these past years. While early shallow approaches have been proven useful for several natural language processing applications (Wu and Fung, 2009; Surdeanu et al., 2003; Shen and Lapata, 2007), the field is moving towards analyzing and processing complex linguistic phenomena, such as metaphor (Shutova, 2010) or modality and negation (Morante and Sporleder, 2012). The *SEM 2012 Shared Task is devoted to negation, specifically, to resolving its scope and focus. Negation is a grammatical category that comprises devices used to reverse the truth value of propositions. Broadly speaking, scope is the part of the meaning that is negated and focus the part of the scope that is most prominently or explicitly negated (Huddleston and Pullum, 2002). Although negation is a very relevant and com"
S12-1035,P10-1071,0,0.0153104,"dicated to resolving the scope and focus of negation. This paper presents the specifications, datasets and evaluation criteria of the task. An overview of participating systems is provided and their results are summarized. 1 Introduction Semantic representation of text has received considerable attention these past years. While early shallow approaches have been proven useful for several natural language processing applications (Wu and Fung, 2009; Surdeanu et al., 2003; Shen and Lapata, 2007), the field is moving towards analyzing and processing complex linguistic phenomena, such as metaphor (Shutova, 2010) or modality and negation (Morante and Sporleder, 2012). The *SEM 2012 Shared Task is devoted to negation, specifically, to resolving its scope and focus. Negation is a grammatical category that comprises devices used to reverse the truth value of propositions. Broadly speaking, scope is the part of the meaning that is negated and focus the part of the scope that is most prominently or explicitly negated (Huddleston and Pullum, 2002). Although negation is a very relevant and complex semantic aspect of language, current proposals to annotate meaning either dismiss negation or only treat it in a"
S12-1035,P03-1002,0,0.0130037,"d Computational Semantics (*SEM) each year hosts a shared task on semantic related topics. In its first edition held in 2012, the shared task was dedicated to resolving the scope and focus of negation. This paper presents the specifications, datasets and evaluation criteria of the task. An overview of participating systems is provided and their results are summarized. 1 Introduction Semantic representation of text has received considerable attention these past years. While early shallow approaches have been proven useful for several natural language processing applications (Wu and Fung, 2009; Surdeanu et al., 2003; Shen and Lapata, 2007), the field is moving towards analyzing and processing complex linguistic phenomena, such as metaphor (Shutova, 2010) or modality and negation (Morante and Sporleder, 2012). The *SEM 2012 Shared Task is devoted to negation, specifically, to resolving its scope and focus. Negation is a grammatical category that comprises devices used to reverse the truth value of propositions. Broadly speaking, scope is the part of the meaning that is negated and focus the part of the scope that is most prominently or explicitly negated (Huddleston and Pullum, 2002). Although negation is"
S12-1035,W08-2121,0,0.0715168,"Missing"
S12-1035,H05-1059,0,0.00829332,"presented in Table 1. More information about the annotation guidelines is provided by Morante et al. (2011) and Morante and Daelemans (2012), including inter-annotator agreement. The corpus was preprocessed at the University of Oslo. Tokenization was obtained by the PTBcompliant tokenizer that is part of the LinGO English Resource Grammar. 4 4 268 Training 65,450 3644 848 23.27 984 30 887 616 http://moin.delph-in.net/ Apart from the gold annotations, the corpus was provided to participants with additional annotations: 3.2 Semantic roles focus belongs to • Lemmatization using the GENIA tagger (Tsuruoka and Tsujii, 2005), version 3.0.1, with the ’-nt’ command line option. GENIA PoS tags are complemented with TnT PoS tags for increased compatibility with the original PTB. • Parsing with the Charniak and Johnson (2005) reranking parser.5 For compatibility with PTB conventions, the top-level nodes in parse trees (‘S1’), were removed. The conversion of PTB-style syntax trees into CoNLL-style format was performed using the CoNLL 2005 Shared Task software.6 PB-FOC: Focus Annotation We have adapted the only previous annotation effort targeting focus of negation for PB-FOC (Blanco and Moldovan, 2011). This corpus pro"
S12-1035,J12-2005,0,0.419944,"charge summaries must be reliably interpreted and indexed. The annotation of negation and hedge cues and their scope in the BioScope corpus (Vincze et al., 2008) represented a pioneering effort. This corpus boosted research on scope resolution, especially since it was used in the CoNLL 2010 Shared Task (CoNLL ST 2010) on hedge detection (Farkas et al., 2010). Negation has also been studied in sentiment analysis (Wiegand et al., 2010) as a means to determine the polarity of sentiments and opinions. Whereas several scope detectors have been developed using BioScope (Morante and Daelemans, 2009; Velldal et al., 2012), there is a lack of corpora and tools to process negation in general domain texts. This is why we have prepared new corpora for scope and focus detection. Scope is annotated in Conan Doyle stories (CD-SCO corpus). For each negation, the cue, its scope and the negated event, if any, are marked as shown in example (1a). Focus is annotated on top of PropBank, which uses the WSJ section of the Penn TreeBank (PB-FOC corpus). Focus annotation is restricted to verbal negations annotated with MNEG in PropBank, and all the words belonging to a semantic role are selected as focus. An annotated example"
S12-1035,W08-0606,0,0.689454,"rt of the meaning that is negated and focus the part of the scope that is most prominently or explicitly negated (Huddleston and Pullum, 2002). Although negation is a very relevant and complex semantic aspect of language, current proposals to annotate meaning either dismiss negation or only treat it in a partial manner. The interest in automatically processing negation originated in the medical domain (Chapman et al., 2001), since clinical reports and discharge summaries must be reliably interpreted and indexed. The annotation of negation and hedge cues and their scope in the BioScope corpus (Vincze et al., 2008) represented a pioneering effort. This corpus boosted research on scope resolution, especially since it was used in the CoNLL 2010 Shared Task (CoNLL ST 2010) on hedge detection (Farkas et al., 2010). Negation has also been studied in sentiment analysis (Wiegand et al., 2010) as a means to determine the polarity of sentiments and opinions. Whereas several scope detectors have been developed using BioScope (Morante and Daelemans, 2009; Velldal et al., 2012), there is a lack of corpora and tools to process negation in general domain texts. This is why we have prepared new corpora for scope and f"
S12-1035,W10-3111,0,0.13446,"negation or only treat it in a partial manner. The interest in automatically processing negation originated in the medical domain (Chapman et al., 2001), since clinical reports and discharge summaries must be reliably interpreted and indexed. The annotation of negation and hedge cues and their scope in the BioScope corpus (Vincze et al., 2008) represented a pioneering effort. This corpus boosted research on scope resolution, especially since it was used in the CoNLL 2010 Shared Task (CoNLL ST 2010) on hedge detection (Farkas et al., 2010). Negation has also been studied in sentiment analysis (Wiegand et al., 2010) as a means to determine the polarity of sentiments and opinions. Whereas several scope detectors have been developed using BioScope (Morante and Daelemans, 2009; Velldal et al., 2012), there is a lack of corpora and tools to process negation in general domain texts. This is why we have prepared new corpora for scope and focus detection. Scope is annotated in Conan Doyle stories (CD-SCO corpus). For each negation, the cue, its scope and the negated event, if any, are marked as shown in example (1a). Focus is annotated on top of PropBank, which uses the WSJ section of the Penn TreeBank (PB-FOC"
S12-1035,N09-2004,0,0.00938611,"rence on Lexical and Computational Semantics (*SEM) each year hosts a shared task on semantic related topics. In its first edition held in 2012, the shared task was dedicated to resolving the scope and focus of negation. This paper presents the specifications, datasets and evaluation criteria of the task. An overview of participating systems is provided and their results are summarized. 1 Introduction Semantic representation of text has received considerable attention these past years. While early shallow approaches have been proven useful for several natural language processing applications (Wu and Fung, 2009; Surdeanu et al., 2003; Shen and Lapata, 2007), the field is moving towards analyzing and processing complex linguistic phenomena, such as metaphor (Shutova, 2010) or modality and negation (Morante and Sporleder, 2012). The *SEM 2012 Shared Task is devoted to negation, specifically, to resolving its scope and focus. Negation is a grammatical category that comprises devices used to reverse the truth value of propositions. Broadly speaking, scope is the part of the meaning that is negated and focus the part of the scope that is most prominently or explicitly negated (Huddleston and Pullum, 2002"
S13-1042,baccianella-etal-2010-sentiwordnet,0,0.0344651,"Missing"
S13-1042,P97-1023,0,0.216743,"Missing"
S13-1042,W06-1642,0,0.0696434,"Missing"
S13-1042,C04-1200,0,0.301213,"Missing"
S13-1042,lin-etal-2010-new,0,0.0408064,"Missing"
S13-1042,P04-1036,0,0.121164,"Missing"
S13-1042,H93-1061,0,0.118574,"nstead we leverage part-of-speech and word sense data to help us determine which words are lexically ambiguous. 297 Figure 1: The relationship between text expressing positive emotion (POSEMO) and text containing LIWC terms for POSEMO. Our approach of eliminating ambiguous words increases the precision at the expense of recall, a reasonable trade-off in social media where we are working with millions or even billions of word instances. Additionally, it is minimally-supervised, in that we do not require training data on human-state; instead we use existing hand-labeled corpora, such as SemCor (Miller et al., 1993), for word sense information. Not requiring training data also means our refinement is flexible; it can be applied to multiple domains and lexica, it makes few assumptions that might introduce problems of over-fitting, and it is parsimonious in that it merely improves an established approach. This paper makes two primary contributions: (1) an analysis of the types of errors common for the word count approach (Section 3), and (2) a general method for refining psychosocial lexica based on the ambiguity of words (Section 4). Before describing these contributions, we discuss related work, making t"
S13-1042,W02-1011,0,0.0176418,"Missing"
S13-1042,J09-3003,0,0.0396365,"Missing"
S13-1042,H93-1052,0,0.317146,"Missing"
S19-1017,S13-1004,0,0.0132074,", 1997). In this paper, we focus on verbal negations, i.e., when the negation mark—usually an adverb such as never and not—is grammatically associated with a verb. Positive Interpretations. In philosophy and linguistics, it is accepted that negation conveys positive meaning (Horn, 1989). This positive meaning ranges from implicatures, i.e., what is suggested in an utterance even though neither expressed nor strictly implied (Blackburn, 2008), to entailments. Other terms used in the literature include implied meanings (Mitkov, 2005), implied alternatives (Rooth, 1985) and semantically similar (Agirre et al., 2013). We do not strictly fit into any of this terminology, we reveal positive interpretations as intuitively done by humans when reading text. Note that a positive interpretation is a statement that does not contain negation, not a statement that conveys positive sentiment. For example, The seller didn’t ship the right parts implicitly conveys The seller shipped the wrong parts, which has negative sentiment. Potential Positive Interpretations. Given a sentence containing negation, we use the term potential positive interpretation to refer to positive interpretations that are automatically generate"
S19-1017,W12-3808,0,0.075973,"Missing"
S19-1017,D15-1162,0,0.0338223,"Missing"
S19-1017,D16-1025,0,0.0226121,"Missing"
S19-1017,C10-1076,0,0.0728592,"Missing"
S19-1017,D15-1075,0,0.02611,"ential positive interpretations become actual positive interpretations. Negation and natural language understanding. Generating positive interpretations from negation has several potential applications. First, while neural machine translation is in general superior to phrase-based methods, that is not the case when translating negation (Bentivogli et al., 2016). Since our positive interpretations effectively rewrite negation-containing sentences to remove the negation, we argue that they have the potential to help machine translation. Second, current benchmarks for natural language inference (Bowman et al., 2015), do not include challenging examples with negation. As a result, state-of-the-art approaches (Chen et al., 3 Previous Work From a theoretical perspective, it is accepted that negation has scope and focus, and that the focus yields positive interpretations (Horn, 1989; Rooth, 1992). Scope is “the part of the meaning that is negated” and focus “the part of the scope that is most prominently or explicitly negated” (Huddleston and Pullum, 2002). Scope of negation detection has received a lot ¨ ur and Radev, 2009; Packard of attention (Ozg¨ et al., 2014), mostly using two corpora: BioScope (Szarva"
S19-1017,D15-1166,0,0.0215435,"etation, however, reveals that He has the intention of getting more money. Context, which is not shown in Table 8, support the correctness and validation scores (e.g., He is wealthy). 6 Experiments The task of generating positive interpretations from a sentence containing negation can be approached with sequence-to-sequence (seq2seq) models (input: sentence containing negation, output: positive interpretation). In this section, we present baseline results with existing seq2seq models. Specifically, we experiment with a basic seq2seq model (Cho et al., 2014), two seq2seq models with attention (Luong et al., 2015; Bahdanau et al., 2014), and Google’s neural machine translation (NMT) system (Wu et al., 2016), which is also seq2seq model with attention and arguably the most complex. We acknowledge that these systems are usually trained with orders of magnitude more examples, and comparing them when trained with our fairly small corpus may be unfair because they were designed for other tasks. Our goal is not to obtain the best results possible, but rather provide baseline results for our task and corpus. Figure 3: Distribution of correctness (top) and novelty (bottom) scores in our corpus. Interpretation"
S19-1017,W17-5307,0,0.0471949,"Missing"
S19-1017,D14-1179,0,0.0436641,"Missing"
S19-1017,S12-1035,1,0.882943,"Missing"
S19-1017,P16-1047,0,0.037469,"Missing"
S19-1017,P14-1007,0,0.0530314,"Missing"
S19-1017,J05-1004,0,0.248144,"Missing"
S19-1017,D16-1119,1,0.869512,"Missing"
S19-1017,W08-0606,0,0.0553874,"Missing"
S19-1017,J12-2005,0,0.0651736,"Missing"
W11-0106,P08-2045,0,0.0667426,", all inferences are correct. 6 Comparison with Previous Work There have been abundant proposals to detect semantic relations without taking into account composition of relations. All these approaches, regardless of their particular details, take as their input text and output the relations found in it. In contrast, the framework proposed in this article obtains axioms that take as their input relations found in text and output more relations previously ignored. Generally, efforts to extract semantic relations have concentrated on particular sets of relations or a single relation, e.g. CAUSE (Bethard and Martin, 2008; Chang and Choi, 2006) and PART- WHOLE (Girju et al., 2006). Automatic detection of semantic roles has received a lot of attention lately (M`arquez et al., 2008; Carreras and M`arquez, 2005). The SemEval-2007 Task 04 (Girju et al., 2007) and SemEval-2010 Task 08 (Hendrickx et al., 2009) aimed at relations between nominals. There has been work on detecting relations within noun phrases (Moldovan et al., 2004; Nulty, 2007), clauses (Szpakowicz et al., 1995) and syntax-based comma resolution (Srikumar et al., 2008). Previous research has exploited the idea of using semantic primitives to define"
W11-0106,W05-0620,0,0.458193,"Missing"
W11-0106,J06-1005,1,0.815317,"re have been abundant proposals to detect semantic relations without taking into account composition of relations. All these approaches, regardless of their particular details, take as their input text and output the relations found in it. In contrast, the framework proposed in this article obtains axioms that take as their input relations found in text and output more relations previously ignored. Generally, efforts to extract semantic relations have concentrated on particular sets of relations or a single relation, e.g. CAUSE (Bethard and Martin, 2008; Chang and Choi, 2006) and PART- WHOLE (Girju et al., 2006). Automatic detection of semantic roles has received a lot of attention lately (M`arquez et al., 2008; Carreras and M`arquez, 2005). The SemEval-2007 Task 04 (Girju et al., 2007) and SemEval-2010 Task 08 (Hendrickx et al., 2009) aimed at relations between nominals. There has been work on detecting relations within noun phrases (Moldovan et al., 2004; Nulty, 2007), clauses (Szpakowicz et al., 1995) and syntax-based comma resolution (Srikumar et al., 2008). Previous research has exploited the idea of using semantic primitives to define and classify semantic relations under different names. Among"
W11-0106,S07-1003,0,0.435452,"uistics for decades. They are unidirectional underlying connections between concepts. For example, the sentence The construction slowed down the traffic encodes a CAUSE and detecting it would help answer the question Why is traffic slower? In Computational Linguistics, there have been several proposals to detect semantic relations. Current approaches focus on a particular set of relations and given a text they output relations. There have been competitions aiming at detecting semantic roles (i.e., relations between a verb and its arguments) (Carreras and M`arquez, 2005), and between nominals (Girju et al., 2007; Hendrickx et al., 2009). In this paper, we propose a model to compose semantic relations to extract previously ignored relations. The model allows us to automatically obtain inference axioms given a set of relations and is not coupled to any particular set. Axioms take as their input semantic relations and yield a new semantic relation as their conclusion. Consider the sentence John went to the shop to buy flowers. Figure 1 shows semantic role annotation with solid arrows. By composing this basic annotation with inference axioms, one can obtain the relations shown with discontinuous arrows:"
W11-0106,J08-2001,0,0.0220263,"Missing"
W11-0106,W04-2609,1,0.767188,"n text and output more relations previously ignored. Generally, efforts to extract semantic relations have concentrated on particular sets of relations or a single relation, e.g. CAUSE (Bethard and Martin, 2008; Chang and Choi, 2006) and PART- WHOLE (Girju et al., 2006). Automatic detection of semantic roles has received a lot of attention lately (M`arquez et al., 2008; Carreras and M`arquez, 2005). The SemEval-2007 Task 04 (Girju et al., 2007) and SemEval-2010 Task 08 (Hendrickx et al., 2009) aimed at relations between nominals. There has been work on detecting relations within noun phrases (Moldovan et al., 2004; Nulty, 2007), clauses (Szpakowicz et al., 1995) and syntax-based comma resolution (Srikumar et al., 2008). Previous research has exploited the idea of using semantic primitives to define and classify semantic relations under different names. Among others, the literature uses relation elements, deep structure, aspects and primitives. To the best of our knowledge, the first effort on describing semantic relations 52 using primitives was made by Chaffin and Herrmann (1987). They introduce Relation Element Theory, and differentiate relations by relation elements. The authors describe a set of 31"
W11-0106,P07-3014,0,0.185972,"relations previously ignored. Generally, efforts to extract semantic relations have concentrated on particular sets of relations or a single relation, e.g. CAUSE (Bethard and Martin, 2008; Chang and Choi, 2006) and PART- WHOLE (Girju et al., 2006). Automatic detection of semantic roles has received a lot of attention lately (M`arquez et al., 2008; Carreras and M`arquez, 2005). The SemEval-2007 Task 04 (Girju et al., 2007) and SemEval-2010 Task 08 (Hendrickx et al., 2009) aimed at relations between nominals. There has been work on detecting relations within noun phrases (Moldovan et al., 2004; Nulty, 2007), clauses (Szpakowicz et al., 1995) and syntax-based comma resolution (Srikumar et al., 2008). Previous research has exploited the idea of using semantic primitives to define and classify semantic relations under different names. Among others, the literature uses relation elements, deep structure, aspects and primitives. To the best of our knowledge, the first effort on describing semantic relations 52 using primitives was made by Chaffin and Herrmann (1987). They introduce Relation Element Theory, and differentiate relations by relation elements. The authors describe a set of 31 relations clu"
W11-0106,J05-1004,0,0.0554931,"N(Rj ) = ∅, break 2. Primitives composition Using the algebra for composing semantic primitives, calculate PRi ◦ PRj 3. Conclusion match Repeat for R3 ∈ R If D OMAIN(R3 ) ∩ D OMAIN(Ri ) 6= ∅ and R ANGE(R3 ) ∩ R ANGE(Rj ) 6= ∅ and consistent(PR3 , PRi ◦ PRj ), then inf erence axioms += Ri (x, y) ◦ Rj (y, z) → R3 (x, z) The method consistent(P1 , P2 ) is a simple procedure that compares the values assigned to each primitive one by one. Two values for the same primitive are compatible unless they have different opposites or either value is ‘×’ (i.e., prohibited). 5 Case Study: PropBank PropBank (Palmer et al., 2005) adds a layer of predicate-argument information, or semantic role labels, on top of the syntactic trees provided by the Penn TreeBank. Along with FrameNet, it is the resource most widely used for semantic role annotation. PropBank uses a series of numeric core roles (ARG 0 - ARG 5) and a set of more general roles, ARGMs (e.g. MTMP, MLOC, MMNR). The interpretation of the numeric roles is determined by a verb-specific framesets, although ARG 0 and ARG 1 usually correspond to the prototypical AGENT and THEME. On the other hand, the meaning of AGRMs generalize across verbs. An example of PropBank"
W11-0106,P08-1117,0,0.0185363,"concentrated on particular sets of relations or a single relation, e.g. CAUSE (Bethard and Martin, 2008; Chang and Choi, 2006) and PART- WHOLE (Girju et al., 2006). Automatic detection of semantic roles has received a lot of attention lately (M`arquez et al., 2008; Carreras and M`arquez, 2005). The SemEval-2007 Task 04 (Girju et al., 2007) and SemEval-2010 Task 08 (Hendrickx et al., 2009) aimed at relations between nominals. There has been work on detecting relations within noun phrases (Moldovan et al., 2004; Nulty, 2007), clauses (Szpakowicz et al., 1995) and syntax-based comma resolution (Srikumar et al., 2008). Previous research has exploited the idea of using semantic primitives to define and classify semantic relations under different names. Among others, the literature uses relation elements, deep structure, aspects and primitives. To the best of our knowledge, the first effort on describing semantic relations 52 using primitives was made by Chaffin and Herrmann (1987). They introduce Relation Element Theory, and differentiate relations by relation elements. The authors describe a set of 31 relations clustered in five groups (CONTRAST, SIMILARS, CLASS INCLUSION, CASE - RELATIONS, PART- WHOLE), a"
W11-0106,W09-2415,0,\N,Missing
