2020.acl-demos.9,N19-1371,1,0.935324,"stics particular intervention, comparator, and outcome measure (which we describe as an ICO triplet). Because the end-to-end task combines NLP subtasks that are supported by different datasets, we collected new development and test sets — 160 abstracts in all, exhaustively annotated — in order to evaluate the overall performance of our system. Two medical doctors3 annotated these documents with the all of the expressed entities, their mentions in the text, the relations between them, the conclusions reported for each ICO triplet and the sentence that contains the supporting evidence for this (Lehman et al., 2019). We were unable to obtain normalized concept labels for the ICO triplets due to the excessive difficulty of the task for the annotators. Modeling decisions were informed by the 60 document development set, and we present evaluations of the first four information extraction modules with regard to the 100 documents in the unseen test set. ploration of the primary literature. In this work we describe an open-source prototype that enables evidence mapping, using NLP to generate interactive overviews and visualizations of all RCT reports indexed by MEDLINE (and accessible via PubMed). When mapping"
2020.acl-demos.9,P18-1019,1,0.882233,"articular intervention for a given condition? In the remainder of this paper we describe a prototype system that facilitates interactive exploration and mapping of the evidence base, with an emphasis on answering the above questions. The Trialstreamer mapping interface allows structured search over study populations, interventions/comparators, and outcomes — collectively referred to as PICO elements (Huang et al., 2006). It then displays key clinical attributes automatically extracted from the set of retrieved trials. This is made possible via NLP modules trained on recently released corpora (Nye et al., 2018; Lehman et al., 2019), described below. 2 2.1 Preprocessing Enabling search over RCT reports requires first compiling and indexing all such studies. This is, perhaps surprisingly, non-trivial. One may rely on “Publication Type” (PT) tags that codify study designs of articles, but these are manually applied by staff at the National Library of Medicine. Consequently, there is a lag between when a new study is published and when a PT tag is applied. Relying on these tags may thus hinder access to the most up-to-date evidence available. Therefore, we instead use an automated tagging system that u"
2020.acl-main.408,2020.acl-main.386,0,0.013867,"make a prediction. We refer to rationales that correspond to the inputs most relied upon to come to a disposition as faithful. Most automatic evaluations of faithfulness measure the impact of perturbing or erasing words or tokens identified as important on model output (Arras et al., 2017; Montavon et al., 2017; Serrano and Smith, 2019; Samek et al., 2016; Jain and Wallace, 2019). We build upon these methods in Section 4. Finally, we note that a recent article urges the community to evaluate faithfulness on a continuous scale of acceptability, rather than viewing this as a binary proposition (Jacovi and Goldberg, 2020). 3 Datasets in ERASER For all datasets in ERASER we distribute both reference labels and rationales marked by humans as supporting these in a standardized format. We 4445 delineate train, validation, and test splits for all corpora (see Appendix A for processing details). We ensure that these splits comprise disjoint sets of source documents to avoid contamination.3 We have made the decision to distribute the test sets publicly,4 in part because we do not view the ‘correct’ metrics to use as settled. We plan to acquire additional human annotations on held-out portions of some of the included"
2020.acl-main.408,N19-1357,1,0.918768,"uring training. However, such direct supervision will not always be available, motivating work on methods that can explain (or “rationalize”) model predictions using only instance-level supervision. In the context of modern neural models for text classification, one might use variants of attention (Bahdanau et al., 2015) to extract rationales. Attention mechanisms learn to assign soft weights to (usually contextualized) token representations, and so one can extract highly weighted tokens as rationales. However, attention weights do not in general provide faithful explanations for predictions (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019; Zhong et al., 2019; Pruthi et al., 2020; Brunner et al., 2020; Moradi et al., 2019; Vashishth et al., 2019). This likely owes to encoders entangling inputs, complicating the interpretation of attention weights on inputs over contextualized representations of the same.2 By contrast, hard attention mechanisms discretely extract snippets from the input to pass to the classifier, by construction providing faithful explanations. Recent work has proposed hard attention mechanisms as a means of providing explanations. Lei et al. (2016) proposed i"
2020.acl-main.408,2020.acl-main.409,1,0.78141,"g the interpretation of attention weights on inputs over contextualized representations of the same.2 By contrast, hard attention mechanisms discretely extract snippets from the input to pass to the classifier, by construction providing faithful explanations. Recent work has proposed hard attention mechanisms as a means of providing explanations. Lei et al. (2016) proposed instantiating two models with their own parameters; one to extract rationales, and one that consumes these to make a prediction. They trained these models jointly via REINFORCE (Williams, 1992) style optimization. Recently, Jain et al. (2020) proposed a variant of this two-model setup that uses heuristic feature scores to derive pseudo-labels on tokens comprising rationales; one model can then be used to perform hard extraction in this way, while a second (independent) model can make predictions on the basis of these. Elsewhere, Chang et al. (2019) introduced the notion of classwise rationales that explains support for different output classes using a game theoretic framework. Finally, other recent work has proposed using a differentiable binary mask over inputs, which also avoids recourse to REINFORCE (Bastings et al., 2019). Pos"
2020.acl-main.408,N18-1023,0,0.0239798,"for BoolQ, wherein source documents in the original train and validation set were not disjoint and we preserve this structure in our dataset. Questions, of course, are disjoint. 4 Consequently, for datasets that have been part of previous benchmarks with other aims (namely, GLUE/superGLUE) but which we have re-purposed for work on rationales in ERASER, e.g., BoolQ (Clark et al., 2019), we have carved out for release test sets from the original validation sets. 5 Annotation details are in Appendix B. texts. We take a subset of this dataset, including only supported and refuted claims. MultiRC (Khashabi et al., 2018). A reading comprehension dataset composed of questions with multiple correct answers that by construction depend on information from multiple sentences. Here each rationale is associated with a question, while answers are independent of one another. We convert each rationale/question/answer triplet into an instance within our dataset. Each answer candidate then has a label of True or False. Commonsense Explanations (CoS-E) (Rajani et al., 2019). This corpus comprises multiplechoice questions and answers from (Talmor et al., 2019) along with supporting rationales. The rationales in this case c"
2020.acl-main.408,D19-1002,0,0.0200855,"will not always be available, motivating work on methods that can explain (or “rationalize”) model predictions using only instance-level supervision. In the context of modern neural models for text classification, one might use variants of attention (Bahdanau et al., 2015) to extract rationales. Attention mechanisms learn to assign soft weights to (usually contextualized) token representations, and so one can extract highly weighted tokens as rationales. However, attention weights do not in general provide faithful explanations for predictions (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019; Zhong et al., 2019; Pruthi et al., 2020; Brunner et al., 2020; Moradi et al., 2019; Vashishth et al., 2019). This likely owes to encoders entangling inputs, complicating the interpretation of attention weights on inputs over contextualized representations of the same.2 By contrast, hard attention mechanisms discretely extract snippets from the input to pass to the classifier, by construction providing faithful explanations. Recent work has proposed hard attention mechanisms as a means of providing explanations. Lei et al. (2016) proposed instantiating two models with their own parameters; on"
2020.acl-main.408,D19-1420,0,0.0131407,"in the test set in Table 3. 4.2 Measuring faithfulness As discussed above, a model may provide rationales that are plausible (agreeable to humans) but that it did not rely on for its output. In many settings one may want rationales that actually explain model predictions, i.e., rationales extracted for an instance in this case ought to have meaningfully influenced its prediction for the same. We call these faithful rationales. How best to measure rationale faithfulness is an open question. In this first version of ERASER we propose simple metrics motivated by prior work (Zaidan et al., 2007; Yu et al., 2019). In particular, following Yu et al. (2019) we define metrics intended to measure the comprehensiveness (were all features needed to make a prediction selected?) and sufficiency (do the extracted rationales contain enough signal to come to a disposition?) of rationales, respectively. Comprehensiveness. To calculate rationale comprehensiveness we create contrast examples (Zaidan et al., 2007): We construct a contrast example for xi , x ˜i , which is xi with the predicted rationales ri removed. Assuming a classification setting, let m(xi )j be the original prediction provided by a model m for th"
2020.acl-main.408,N07-1033,0,0.340523,"cy in terms of model performance realized given a fixed amount of annotator effort (Zaidan and Eisner, 2008). In particular, recent work by McDonnell et al. (2017, 2016) has observed that at least for some tasks, asking annotators to provide rationales justifying their categorizations does not impose much additional effort. Combining rationale annotation with active learning (Settles, 2012) is another promising direction (Wallace et al., 2010; Sharma et al., 2015). Learning from rationales. Work on learning from rationales marked by annotators for text classification dates back over a decade (Zaidan et al., 2007). Earlier efforts proposed extending standard discriminative models like Support Vector Machines (SVMs) with regularization terms that penalized parameter estimates which disagreed with provided rationales (Zaidan et al., 2007; Small et al., 2011). Other efforts have attempted to specify generative models of rationales (Zaidan and Eisner, 2008). More recent work has aimed to exploit rationales in training neural text classifiers. Zhang et al. (2016) proposed a rationale-augmented Convolutional Neural Network (CNN) for text classification, explicitly trained to identify sentences supporting cat"
2020.acl-main.408,D08-1004,0,0.182552,"aviors (Feng et al., 2018). Gradients of course assume model differentiability. Other methods do not require any model properties. Examples include LIME (Ribeiro et al., 2016) and Alvarez-Melis and Jaakkola (2017); these methods approximate model behavior locally by having it repeatedly make predictions over perturbed inputs and fitting a simple, explainable model over the outputs. Acquiring rationales. Aside from interpretability considerations, collecting rationales from annotators may afford greater efficiency in terms of model performance realized given a fixed amount of annotator effort (Zaidan and Eisner, 2008). In particular, recent work by McDonnell et al. (2017, 2016) has observed that at least for some tasks, asking annotators to provide rationales justifying their categorizations does not impose much additional effort. Combining rationale annotation with active learning (Settles, 2012) is another promising direction (Wallace et al., 2010; Sharma et al., 2015). Learning from rationales. Work on learning from rationales marked by annotators for text classification dates back over a decade (Zaidan et al., 2007). Earlier efforts proposed extending standard discriminative models like Support Vector"
2020.acl-main.408,D16-1076,1,0.90058,"Missing"
2020.acl-main.409,P19-1284,0,0.102288,"d . This is just like the image of the sign above . Light rays strike flat shiny surfaces and are reflected . The reflections are reversed . Figure 1: Contiguous rationales extracted using Lei et al. (2016) and FRESH models for an example from the MultiRC dataset. We also show the reference rationale associated with this example (top). supervision (i.e., without token labels). This has necessitated training the extraction module via reinforcement learning — namely REINFORCE (Williams, 1992) — which exhibits high variance and is particularly sensitive to choice of hyperparameters. Recent work (Bastings et al., 2019) has proposed a differentiable mechanism to perform binary token selection, but this relies on the reparameterization trick, which similarly complicates training. Methods using the reparameterization trick tend to zero out token embeddings, which may adversely affect training in transformer-based models, especially when one is not fine-tuning lower layers of the model due to resource constraints, as in our experiments. To avoid the complexity inherent to training under a remote supervision signal, we introduce Faithful Rationale Extraction from Saliency tHresholding (FRESH), which disconnects"
2020.acl-main.409,D19-1371,0,0.0391837,"Missing"
2020.acl-main.409,N19-1423,0,0.0842793,"Missing"
2020.acl-main.409,D19-1224,0,0.0237474,"Missing"
2020.acl-main.409,D18-1407,0,0.0850364,"Missing"
2020.acl-main.409,W18-2501,0,0.0705869,"Missing"
2020.acl-main.409,N19-1357,1,0.837436,"y specific prediction. Concretely, in a bidirectional RNN or Transformer model, the contextual embedding for a word at position j in instance x may encode information from any or all of the tokens at positions 1 to j-1 and j+1 to |x|. Consequently, continuous scores such as attention weights (Bahdanau et al., 2015) induced over these contextualized embeddings reflect the importance not of individual inputs, but rather of unknown interactions between all input tokens. This makes it misleading to present heatmaps of these scores over the original token inputs as an explanation for a prediction (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Serrano and Smith, 2019). The key missing property here is faithfulness (Lipton, 2018): An explanation provided by a model is faithful if it reflects the information actually used by said model to come to a disposition. In some settings the ability of a model to provide faithful explanations may be paramount. For example, without faithful explanations, we cannot know whether a model is exploiting sensitive features such as gender (Pruthi et al., 2020). We propose an approach to neural text classification that provides faithful explanations for predictions by const"
2020.acl-main.409,N18-1023,0,0.0436889,"Missing"
2020.acl-main.409,N19-1371,1,0.89456,"Missing"
2020.acl-main.409,D16-1011,0,0.258807,"Missing"
2020.acl-main.409,D19-1523,0,0.0752594,"Missing"
2020.acl-main.409,N16-1082,0,0.0883956,"from Saliency tHresholding (FRESH), which disconnects the training regimes of the extractor and predictor networks, allowing each to be trained separately. We still assume only instance-level supervision; the trick is to define a method of selecting snippets from inputs — rationales (Zaidan et al., 2007) — that can be used to support prediction. Here we propose using arbitrary feature importance scoring techniques to do so. Notably, these need not satisfy the ‘faithfulness’ criterion. In this paper we evaluate variants of FRESH that use attention (Bahdanau et al., 2015) and gradient methods (Li et al., 2016; Simonyan et al., 2014) as illustrative feature scoring mechanisms. These provide continuous scores for features; we derive discrete rationales from them using simple heuristics. An independent network then uses only the extracted rationales to make predictions. Disconnecting the training tie between the independent rationale extractor and prediction modules means that FRESH is faithful by construction: The snippet that is ultimately used to inform a prediction can be presented as a faithful explanation because this was the only text available to the predictor. In contrast to prior discrete r"
2020.acl-main.409,2021.ccl-1.108,0,0.144711,"Missing"
2020.acl-main.409,2020.acl-main.432,0,0.214041,"isleading to present heatmaps of these scores over the original token inputs as an explanation for a prediction (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Serrano and Smith, 2019). The key missing property here is faithfulness (Lipton, 2018): An explanation provided by a model is faithful if it reflects the information actually used by said model to come to a disposition. In some settings the ability of a model to provide faithful explanations may be paramount. For example, without faithful explanations, we cannot know whether a model is exploiting sensitive features such as gender (Pruthi et al., 2020). We propose an approach to neural text classification that provides faithful explanations for predictions by construction. Following prior work in this direction (Lei et al., 2016), we decompose our model into independent extraction and prediction modules, such that the latter uses only inputs selected by the former. This discrete selection over inputs allows one to use an arbitrarily complex prediction network while still being able to guarantee that it uses only the extracted input features to inform its output. The main drawback to this rationalization approach has been the difficulty of t"
2020.acl-main.409,N16-3020,0,0.682468,"Missing"
2020.acl-main.409,P19-1282,0,0.0491752,"l RNN or Transformer model, the contextual embedding for a word at position j in instance x may encode information from any or all of the tokens at positions 1 to j-1 and j+1 to |x|. Consequently, continuous scores such as attention weights (Bahdanau et al., 2015) induced over these contextualized embeddings reflect the importance not of individual inputs, but rather of unknown interactions between all input tokens. This makes it misleading to present heatmaps of these scores over the original token inputs as an explanation for a prediction (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Serrano and Smith, 2019). The key missing property here is faithfulness (Lipton, 2018): An explanation provided by a model is faithful if it reflects the information actually used by said model to come to a disposition. In some settings the ability of a model to provide faithful explanations may be paramount. For example, without faithful explanations, we cannot know whether a model is exploiting sensitive features such as gender (Pruthi et al., 2020). We propose an approach to neural text classification that provides faithful explanations for predictions by construction. Following prior work in this direction (Lei e"
2020.acl-main.409,D13-1170,0,0.0107913,"Missing"
2020.acl-main.409,W19-4807,0,0.0305802,"Missing"
2020.acl-main.409,P19-1355,0,0.0410294,"Missing"
2020.acl-main.409,D19-1002,1,0.844437,"oncretely, in a bidirectional RNN or Transformer model, the contextual embedding for a word at position j in instance x may encode information from any or all of the tokens at positions 1 to j-1 and j+1 to |x|. Consequently, continuous scores such as attention weights (Bahdanau et al., 2015) induced over these contextualized embeddings reflect the importance not of individual inputs, but rather of unknown interactions between all input tokens. This makes it misleading to present heatmaps of these scores over the original token inputs as an explanation for a prediction (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Serrano and Smith, 2019). The key missing property here is faithfulness (Lipton, 2018): An explanation provided by a model is faithful if it reflects the information actually used by said model to come to a disposition. In some settings the ability of a model to provide faithful explanations may be paramount. For example, without faithful explanations, we cannot know whether a model is exploiting sensitive features such as gender (Pruthi et al., 2020). We propose an approach to neural text classification that provides faithful explanations for predictions by construction. Following prior wor"
2020.acl-main.409,D19-1420,0,0.314997,"is is in contrast to stanby REINFORCE, we introduce in which penalized as long as a predefined desired rationale The guaranteed outcome of system, this approach is thatFRESH, y2Thei approach 332 372 345 395 end-to-end classification thisoriginal difference can become yineural classifiers that induce soft dard proposed by Lei et al. (2016) was very rewe decompose the prediction task into a newbeen movie … passed: length d has InAmazing not pred — the model ultimately to make pre346 acting 396 373 Amazing importance vital as humansused takeAmazing substantially longer time to cently extended in Yu et al. (2019b), in which a third333 composcores over contextualized (hence enacting acting three sub-components, each fitted by its own inand script. … and script. nent (in addition to gen0 and was introduced in part, 0 0 0 enc) … dictionsto,— is faithful This model 334 397 read by fullconstruction. documents and andscript. produce predictions than … 347 tangled) representations of inputs; as discussed 1 1 1 1 1 1of … extracted rationales. Howdependent model. These are the support 374 model encourage comprehensiveness … X 335 the theif text 348 398 provided rationales, and their time tends to cost only co"
2020.acl-main.409,N07-1033,0,0.140798,"beddings, which may adversely affect training in transformer-based models, especially when one is not fine-tuning lower layers of the model due to resource constraints, as in our experiments. To avoid the complexity inherent to training under a remote supervision signal, we introduce Faithful Rationale Extraction from Saliency tHresholding (FRESH), which disconnects the training regimes of the extractor and predictor networks, allowing each to be trained separately. We still assume only instance-level supervision; the trick is to define a method of selecting snippets from inputs — rationales (Zaidan et al., 2007) — that can be used to support prediction. Here we propose using arbitrary feature importance scoring techniques to do so. Notably, these need not satisfy the ‘faithfulness’ criterion. In this paper we evaluate variants of FRESH that use attention (Bahdanau et al., 2015) and gradient methods (Li et al., 2016; Simonyan et al., 2014) as illustrative feature scoring mechanisms. These provide continuous scores for features; we derive discrete rationales from them using simple heuristics. An independent network then uses only the extracted rationales to make predictions. Disconnecting the training"
2020.acl-main.409,D08-1004,0,0.772836,"Missing"
2020.acl-main.409,D16-1076,1,0.924397,"Missing"
2020.acl-main.492,Q19-1004,0,0.0259235,"tic coefficient: +0.55 × 10−3 . Figure 5: Influence-artifact distribution for an original and negated HANS example. (P: The lawyers saw the professor behind the bankers. H: The lawyers saw / did not see the professor.) Original Negated Lexical overlap coef Negation coef +3.05 × 10−3 +0.53 × 10−3 −1.13 × 10−3 +0.27 × 10−3 Table 6: Average quadratic coefficients of the influence-artifact distribution for all original HANS examples and all negated HANS examples. 6 Related Work Interpreting NLP model predictions by constructing importance scores over the input tokens is a widely adopted approach (Belinkov and Glass, 2019). Since the appearance and rise of attentionbased models, many work naturally inspect attention scores and interpret with them. However, we are aware of the recent discussion over whether attention is a kind of faithful explanation (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). Using vanilla attention as interpretation could be more problematic in now ubiquitous deep transformerbased models, such as we use here. Gradient-based saliency maps are locally ‘faithful’ by construction. Other than the vanilla gradients (Simonyan et al., 2014) and the “gradient × input” method (Shrikumar et al."
2020.acl-main.492,D19-1415,0,0.0225552,"can explain any model’s decision by fitting a sparse linear model to the local region of the input example. The main focus of this work is the applicability of influence functions (Koh and Liang, 2017) as an interpretation method in NLP tasks, and to highlight the possibility of using this to surface annotation artifacts. Other methods that can trace the model’s decision back into the training examples include deep weighted averaging classifiers (Card et al., 2019), which make decisions based on the labels of training examples that are most similar to the test input by some distance metrics. Croce et al. (2019) use kernel-based deep architectures 5560 that project test inputs to a space determined by a group of sampled training examples and make explanations through the most activated training instances. While these methods can similarly identify the ‘influential’ training examples, they require special designs or modifications to the model and could sacrifice the model’s performance and generalizability. Other general methods for model interpretability include adversarial-attack approaches that identify that part of input texts can lead to drastically different model decisions when minimally edited"
2020.acl-main.492,N19-1423,0,0.222138,"-based saliency maps (left) and influence functions (right). Note that this example is classified incorrectly by the model. Positive saliency tokens and highly influential examples may suggest why the model makes the wrong decision; tokens and examples with negative saliency or influence scores may decrease the model’s confidence in making that decision. clude answering the following research questions. RQ 1 We empirically assess whether the approximation to the influence functions (Koh and Liang, 2017) can be reliably used to interpret decisions of deep transformer-based models such as BERT (Devlin et al., 2019). RQ 2 We investigate the degree to which results from the influence function are consistent with insights gleaned from gradient-based saliency scores for representative NLP tasks. RQ 3 We explore the application of influence functions as a mechanism to reveal artifacts (or confounds) in training data that might be exploited by models. To the best of our knowledge, this is the first work in NLP to compare interpretation methods that construct saliency maps over inputs with methods that explain predictions via influential training examples. We also propose a new quantitative measurement for the"
2020.acl-main.492,P18-2006,0,0.0229107,"se kernel-based deep architectures 5560 that project test inputs to a space determined by a group of sampled training examples and make explanations through the most activated training instances. While these methods can similarly identify the ‘influential’ training examples, they require special designs or modifications to the model and could sacrifice the model’s performance and generalizability. Other general methods for model interpretability include adversarial-attack approaches that identify that part of input texts can lead to drastically different model decisions when minimally edited (Ebrahimi et al., 2018; Ribeiro et al., 2018), probing approaches that test internal representations of models for certain tasks and properties (Liu et al., 2019b; Hewitt and Liang, 2019), and generative approaches that make the model jointly extract or generate natural language explanations to support predictions (Lei et al., 2016; Camburu et al., 2018; Liu et al., 2019a; Rajani et al., 2019). Specific to the NLI task, Gururangan et al. (2018) recognize and define some possible artifacts within NLI annotations. McCoy et al. (2019) create a diagnostic dataset that we use in this work and suggest that the model coul"
2020.acl-main.492,P19-1334,0,0.341383,"nfluence function are consistent with insights gleaned from gradient-based saliency scores for representative NLP tasks. RQ 3 We explore the application of influence functions as a mechanism to reveal artifacts (or confounds) in training data that might be exploited by models. To the best of our knowledge, this is the first work in NLP to compare interpretation methods that construct saliency maps over inputs with methods that explain predictions via influential training examples. We also propose a new quantitative measurement for the effect of hypothesized artifacts (Gururangan et al., 2018; McCoy et al., 2019) on the model’s prediction using influence functions. 2 Explaining Black-box Model Predictions Machine learning models in NLP depend on two factors when making predictions: the input text and the model parameters. Prior attempts to interpret opaque NLP models have typically focused on the input text. Our work investigates the complementary approach of interpreting predictions by analyzing the influence of examples in training data. Saliency maps aim to provide interpretability by highlighting parts of the input text, whereas influence functions seek clues in the model parameters, eventually lo"
2020.acl-main.492,D18-1407,0,0.0345731,"d of faithful explanation (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). Using vanilla attention as interpretation could be more problematic in now ubiquitous deep transformerbased models, such as we use here. Gradient-based saliency maps are locally ‘faithful’ by construction. Other than the vanilla gradients (Simonyan et al., 2014) and the “gradient × input” method (Shrikumar et al., 2017) we use in this work, there are some variants that aim to make gradient-based attributions robust to potential noise in the input (Sundararajan et al., 2017; Smilkov et al., 2017). We also note that Feng et al. (2018) find that gradient-based methods sometimes yield counter-intuitive results when iterative input reductions are performed. Other token-level interpretations include input perturbation (Li et al., 2016b) which measure a token’s importance by the effect of removing it, and LIME (Ribeiro et al., 2016) which can explain any model’s decision by fitting a sparse linear model to the local region of the input example. The main focus of this work is the applicability of influence functions (Koh and Liang, 2017) as an interpretation method in NLP tasks, and to highlight the possibility of using this to"
2020.acl-main.492,N18-1146,0,0.0689281,"model jointly extract or generate natural language explanations to support predictions (Lei et al., 2016; Camburu et al., 2018; Liu et al., 2019a; Rajani et al., 2019). Specific to the NLI task, Gururangan et al. (2018) recognize and define some possible artifacts within NLI annotations. McCoy et al. (2019) create a diagnostic dataset that we use in this work and suggest that the model could be exploiting some artifacts in training data based on its poor performance on the diagnostic set. Beyond NLI, the negative influence of artifacts in data was explored in other text classification tasks (Pryzant et al., 2018; Kumar et al., 2019; Landeiro et al., 2019), focusing on approaches to adversarial learning to demote the artifacts. 7 Conclusion We compared two complementary interpretation methods—gradient-based saliency maps and influence functions—in two text classification tasks: sentiment analysis and NLI. We first validated the reliability of influence functions when used with deep transformer-based models. We found that in a lexicon-driven sentiment analysis task, saliency maps and influence functions are largely consistent with each other. They are not consistent, however, on the task of NLI. We pos"
2020.acl-main.492,N18-2017,0,0.193708,"which results from the influence function are consistent with insights gleaned from gradient-based saliency scores for representative NLP tasks. RQ 3 We explore the application of influence functions as a mechanism to reveal artifacts (or confounds) in training data that might be exploited by models. To the best of our knowledge, this is the first work in NLP to compare interpretation methods that construct saliency maps over inputs with methods that explain predictions via influential training examples. We also propose a new quantitative measurement for the effect of hypothesized artifacts (Gururangan et al., 2018; McCoy et al., 2019) on the model’s prediction using influence functions. 2 Explaining Black-box Model Predictions Machine learning models in NLP depend on two factors when making predictions: the input text and the model parameters. Prior attempts to interpret opaque NLP models have typically focused on the input text. Our work investigates the complementary approach of interpreting predictions by analyzing the influence of examples in training data. Saliency maps aim to provide interpretability by highlighting parts of the input text, whereas influence functions seek clues in the model para"
2020.acl-main.492,P19-1487,0,0.020794,"nce and generalizability. Other general methods for model interpretability include adversarial-attack approaches that identify that part of input texts can lead to drastically different model decisions when minimally edited (Ebrahimi et al., 2018; Ribeiro et al., 2018), probing approaches that test internal representations of models for certain tasks and properties (Liu et al., 2019b; Hewitt and Liang, 2019), and generative approaches that make the model jointly extract or generate natural language explanations to support predictions (Lei et al., 2016; Camburu et al., 2018; Liu et al., 2019a; Rajani et al., 2019). Specific to the NLI task, Gururangan et al. (2018) recognize and define some possible artifacts within NLI annotations. McCoy et al. (2019) create a diagnostic dataset that we use in this work and suggest that the model could be exploiting some artifacts in training data based on its poor performance on the diagnostic set. Beyond NLI, the negative influence of artifacts in data was explored in other text classification tasks (Pryzant et al., 2018; Kumar et al., 2019; Landeiro et al., 2019), focusing on approaches to adversarial learning to demote the artifacts. 7 Conclusion We compared two c"
2020.acl-main.492,D19-1275,0,0.0195319,"st activated training instances. While these methods can similarly identify the ‘influential’ training examples, they require special designs or modifications to the model and could sacrifice the model’s performance and generalizability. Other general methods for model interpretability include adversarial-attack approaches that identify that part of input texts can lead to drastically different model decisions when minimally edited (Ebrahimi et al., 2018; Ribeiro et al., 2018), probing approaches that test internal representations of models for certain tasks and properties (Liu et al., 2019b; Hewitt and Liang, 2019), and generative approaches that make the model jointly extract or generate natural language explanations to support predictions (Lei et al., 2016; Camburu et al., 2018; Liu et al., 2019a; Rajani et al., 2019). Specific to the NLI task, Gururangan et al. (2018) recognize and define some possible artifacts within NLI annotations. McCoy et al. (2019) create a diagnostic dataset that we use in this work and suggest that the model could be exploiting some artifacts in training data based on its poor performance on the diagnostic set. Beyond NLI, the negative influence of artifacts in data was expl"
2020.acl-main.492,N16-3020,0,0.897788,"to interpreting black box NLP model predictions, i.e., 1 Code is available at https://github.com/ xhan77/influence-function-analysis. Yulia Tsvetkov Carnegie Mellon University ytsvetko@cs.cmu.edu indicating specific input tokens as being particularly influential for a given prediction. This in turn facilitates the construction of saliency maps over texts, in which words are highlighted with intensity proportional to continuous ‘importance’ scores. Prominent examples of the latter include gradient-based attribution (Simonyan et al., 2014; Sundararajan et al., 2017; Smilkov et al., 2017), LIME (Ribeiro et al., 2016), and attention-based (Xu et al., 2015) heatmaps. While widely used and potentially useful for some lexicon-driven tasks (e.g., sentiment analysis), we argue that by virtue of being constrained to highlighting individual input tokens, saliency maps will necessarily fail to explain predictions in more complex semantic tasks involving reasoning, such as natural language inference (NLI), where fine-grained interactions between multiple words or spans are key (Camburu et al., 2018). Moreover, saliency maps are inherently limited as a model debugging tool; they may tell us which inputs the model fo"
2020.acl-main.492,2020.acl-main.386,0,0.0302862,"2016). We are interested in why the model made a particular prediction. We therefore define a loss Lyˆ with respect to the prediction yˆi that the model actually made, rather than the ground truth yi . For each token t ∈ xi , we define a saliency score −∇e(t) Lyˆ · e(t), where e(t) is the embedding of t. This is also referred as the “gradient × input” method in Shrikumar et al. (2017). The “gradient” ∇e(t) Lyˆ captures the sensitivity of the loss to the change in the input embedding, and the “input” 2 Here we focus on interpretability approaches which are faithful (Wiegreffe and Pinter, 2019; Jacovi and Goldberg, 2020; Jain et al., 2020) by construction; other approaches are discussed in §6. 5554 e(t) leverages the sign and magnitude of the input. The final saliency score of each token t would be L1-normalized across all tokens in xi . Unlike Simonyan et al. (2014) and Li et al. (2016a), when scoring features for importance, we do not take the absolute value of the saliency score, as this encodes whether a token is positively influencing the prediction (i.e., providing support the prediction) or negatively influencing the prediction (highlighting counter-evidence). We show an example in the left part of Fi"
2020.acl-main.492,P18-1079,0,0.0276486,"chitectures 5560 that project test inputs to a space determined by a group of sampled training examples and make explanations through the most activated training instances. While these methods can similarly identify the ‘influential’ training examples, they require special designs or modifications to the model and could sacrifice the model’s performance and generalizability. Other general methods for model interpretability include adversarial-attack approaches that identify that part of input texts can lead to drastically different model decisions when minimally edited (Ebrahimi et al., 2018; Ribeiro et al., 2018), probing approaches that test internal representations of models for certain tasks and properties (Liu et al., 2019b; Hewitt and Liang, 2019), and generative approaches that make the model jointly extract or generate natural language explanations to support predictions (Lei et al., 2016; Camburu et al., 2018; Liu et al., 2019a; Rajani et al., 2019). Specific to the NLI task, Gururangan et al. (2018) recognize and define some possible artifacts within NLI annotations. McCoy et al. (2019) create a diagnostic dataset that we use in this work and suggest that the model could be exploiting some ar"
2020.acl-main.492,N19-1357,1,0.86226,"egation coef +3.05 × 10−3 +0.53 × 10−3 −1.13 × 10−3 +0.27 × 10−3 Table 6: Average quadratic coefficients of the influence-artifact distribution for all original HANS examples and all negated HANS examples. 6 Related Work Interpreting NLP model predictions by constructing importance scores over the input tokens is a widely adopted approach (Belinkov and Glass, 2019). Since the appearance and rise of attentionbased models, many work naturally inspect attention scores and interpret with them. However, we are aware of the recent discussion over whether attention is a kind of faithful explanation (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). Using vanilla attention as interpretation could be more problematic in now ubiquitous deep transformerbased models, such as we use here. Gradient-based saliency maps are locally ‘faithful’ by construction. Other than the vanilla gradients (Simonyan et al., 2014) and the “gradient × input” method (Shrikumar et al., 2017) we use in this work, there are some variants that aim to make gradient-based attributions robust to potential noise in the input (Sundararajan et al., 2017; Smilkov et al., 2017). We also note that Feng et al. (2018) find that gradient-based metho"
2020.acl-main.492,2020.acl-main.409,1,0.827447,"why the model made a particular prediction. We therefore define a loss Lyˆ with respect to the prediction yˆi that the model actually made, rather than the ground truth yi . For each token t ∈ xi , we define a saliency score −∇e(t) Lyˆ · e(t), where e(t) is the embedding of t. This is also referred as the “gradient × input” method in Shrikumar et al. (2017). The “gradient” ∇e(t) Lyˆ captures the sensitivity of the loss to the change in the input embedding, and the “input” 2 Here we focus on interpretability approaches which are faithful (Wiegreffe and Pinter, 2019; Jacovi and Goldberg, 2020; Jain et al., 2020) by construction; other approaches are discussed in §6. 5554 e(t) leverages the sign and magnitude of the input. The final saliency score of each token t would be L1-normalized across all tokens in xi . Unlike Simonyan et al. (2014) and Li et al. (2016a), when scoring features for importance, we do not take the absolute value of the saliency score, as this encodes whether a token is positively influencing the prediction (i.e., providing support the prediction) or negatively influencing the prediction (highlighting counter-evidence). We show an example in the left part of Figure 1. 2.2 Influenc"
2020.acl-main.492,D19-1425,1,0.859481,"or generate natural language explanations to support predictions (Lei et al., 2016; Camburu et al., 2018; Liu et al., 2019a; Rajani et al., 2019). Specific to the NLI task, Gururangan et al. (2018) recognize and define some possible artifacts within NLI annotations. McCoy et al. (2019) create a diagnostic dataset that we use in this work and suggest that the model could be exploiting some artifacts in training data based on its poor performance on the diagnostic set. Beyond NLI, the negative influence of artifacts in data was explored in other text classification tasks (Pryzant et al., 2018; Kumar et al., 2019; Landeiro et al., 2019), focusing on approaches to adversarial learning to demote the artifacts. 7 Conclusion We compared two complementary interpretation methods—gradient-based saliency maps and influence functions—in two text classification tasks: sentiment analysis and NLI. We first validated the reliability of influence functions when used with deep transformer-based models. We found that in a lexicon-driven sentiment analysis task, saliency maps and influence functions are largely consistent with each other. They are not consistent, however, on the task of NLI. We posit that influence fu"
2020.acl-main.492,D16-1011,0,0.0269157,"tions to the model and could sacrifice the model’s performance and generalizability. Other general methods for model interpretability include adversarial-attack approaches that identify that part of input texts can lead to drastically different model decisions when minimally edited (Ebrahimi et al., 2018; Ribeiro et al., 2018), probing approaches that test internal representations of models for certain tasks and properties (Liu et al., 2019b; Hewitt and Liang, 2019), and generative approaches that make the model jointly extract or generate natural language explanations to support predictions (Lei et al., 2016; Camburu et al., 2018; Liu et al., 2019a; Rajani et al., 2019). Specific to the NLI task, Gururangan et al. (2018) recognize and define some possible artifacts within NLI annotations. McCoy et al. (2019) create a diagnostic dataset that we use in this work and suggest that the model could be exploiting some artifacts in training data based on its poor performance on the diagnostic set. Beyond NLI, the negative influence of artifacts in data was explored in other text classification tasks (Pryzant et al., 2018; Kumar et al., 2019; Landeiro et al., 2019), focusing on approaches to adversarial l"
2020.acl-main.492,D13-1170,0,0.00867916,"Missing"
2020.acl-main.492,N16-1082,0,0.633017,"hlighting parts of the input text, whereas influence functions seek clues in the model parameters, eventually locating interpretations within the training examples that influenced these estimates. In this section we explain the two interpretation methods in detail.2 2.1 Gradient-based saliency maps As a standard, illustrative ‘explanation-by-inputfeatures’ method, we focus on gradient-based saliency maps, in which the gradient of the loss L is computed with respect to each token t in the input text, and the magnitude of the gradient serves as a feature importance score (Simonyan et al., 2014; Li et al., 2016a). Gradients have the advantage of being locally ‘faithful’ by construction: they tell us how much the loss would change, were we to perturb a token by a small amount. Gradient-based attributions are also agnostic with respect to the model, as long as it is differentiable with respect to inputs. Finally, calculating gradients is computationally efficient, especially compared to methods that require post-hoc input perturbation and function fitting, like LIME (Ribeiro et al., 2016). We are interested in why the model made a particular prediction. We therefore define a loss Lyˆ with respect to t"
2020.acl-main.492,D19-3002,0,0.0355319,"Missing"
2020.acl-main.492,D19-1002,0,0.237298,"like LIME (Ribeiro et al., 2016). We are interested in why the model made a particular prediction. We therefore define a loss Lyˆ with respect to the prediction yˆi that the model actually made, rather than the ground truth yi . For each token t ∈ xi , we define a saliency score −∇e(t) Lyˆ · e(t), where e(t) is the embedding of t. This is also referred as the “gradient × input” method in Shrikumar et al. (2017). The “gradient” ∇e(t) Lyˆ captures the sensitivity of the loss to the change in the input embedding, and the “input” 2 Here we focus on interpretability approaches which are faithful (Wiegreffe and Pinter, 2019; Jacovi and Goldberg, 2020; Jain et al., 2020) by construction; other approaches are discussed in §6. 5554 e(t) leverages the sign and magnitude of the input. The final saliency score of each token t would be L1-normalized across all tokens in xi . Unlike Simonyan et al. (2014) and Li et al. (2016a), when scoring features for importance, we do not take the absolute value of the saliency score, as this encodes whether a token is positively influencing the prediction (i.e., providing support the prediction) or negatively influencing the prediction (highlighting counter-evidence). We show an exa"
2020.acl-main.492,P19-1560,0,0.0212169,"ions through the most activated training instances. While these methods can similarly identify the ‘influential’ training examples, they require special designs or modifications to the model and could sacrifice the model’s performance and generalizability. Other general methods for model interpretability include adversarial-attack approaches that identify that part of input texts can lead to drastically different model decisions when minimally edited (Ebrahimi et al., 2018; Ribeiro et al., 2018), probing approaches that test internal representations of models for certain tasks and properties (Liu et al., 2019b; Hewitt and Liang, 2019), and generative approaches that make the model jointly extract or generate natural language explanations to support predictions (Lei et al., 2016; Camburu et al., 2018; Liu et al., 2019a; Rajani et al., 2019). Specific to the NLI task, Gururangan et al. (2018) recognize and define some possible artifacts within NLI annotations. McCoy et al. (2019) create a diagnostic dataset that we use in this work and suggest that the model could be exploiting some artifacts in training data based on its poor performance on the diagnostic set. Beyond NLI, the negative influence of"
2020.acl-main.492,N19-1112,0,0.0365371,"ions through the most activated training instances. While these methods can similarly identify the ‘influential’ training examples, they require special designs or modifications to the model and could sacrifice the model’s performance and generalizability. Other general methods for model interpretability include adversarial-attack approaches that identify that part of input texts can lead to drastically different model decisions when minimally edited (Ebrahimi et al., 2018; Ribeiro et al., 2018), probing approaches that test internal representations of models for certain tasks and properties (Liu et al., 2019b; Hewitt and Liang, 2019), and generative approaches that make the model jointly extract or generate natural language explanations to support predictions (Lei et al., 2016; Camburu et al., 2018; Liu et al., 2019a; Rajani et al., 2019). Specific to the NLI task, Gururangan et al. (2018) recognize and define some possible artifacts within NLI annotations. McCoy et al. (2019) create a diagnostic dataset that we use in this work and suggest that the model could be exploiting some artifacts in training data based on its poor performance on the diagnostic set. Beyond NLI, the negative influence of"
2020.acl-main.492,N18-1101,0,0.101455,"Missing"
2020.bionlp-1.13,W19-5034,0,0.0277081,"T-based (Devlin et al., 2018) pipeline comprising two independent models, as depicted in Figure 1. The first identifies evidence bearing sentences within an article for a given ICO. The second model then classifies the reported findings for an ICO prompt using the evidence extracted by this first model. These models place a dense layer on top of representations yielded from (Gururangan et al., 2020), 7 a variant of RoBERTa (Liu et al., 2019) pre-trained over scientific corpora,8 followed by a Softmax. Specifically, we first perform sentence segmentation over full-text articles using ScispaCy (Neumann et al., 2019). We use this segmentation to recover evidence bearing sentences. We train an evidence identifier by learning to discriminate between evidence bearing sentences and randomly sampled non-evidence sentences.9 We then train an evidence classifier over the evidence bearing sentences to characterize the trial’s finding as reporting that the Intervention significantly decreased, did not significantly change, or significantly increased the Outcome compared to the Comparator in an ICO. When making a prediction for an (ICO, document) pair we use the highest scoring evidence sentence from the identifier"
2020.bionlp-1.13,P18-1019,1,0.810501,"te the systematic review task (Marshall and Wallace, 2019). However, efforts at fully integrating the PICO framework into this process have been limited (Eriksen and Frandsen, 2018). What if we could build a database of Participants,2 Interventions, Comparisons, and Outcomes studied in these trials, and the findings reported concerning these? If done accurately, this would provide direct access to which treatments the evidence supports. In the near-term, such technologies may mitigate the tedious work necessary for manual synthesis. Recent efforts in this direction include the EBMNLP project (Nye et al., 2018), and Evidence Inference (Lehman et al., 2019), both of which comprise annotations collected on reports of Randomized Control Trials (RCTs) from PubMed.3 Here we build upon the latter, which tasks systems with inferring findings in full-text reports of RCTs with respect to particular interventions and outcomes, and extracting evidence snippets supporting these. We expand the Evidence Inference dataset and evaluate transformer-based models (Vaswani et al., 2017; Devlin et al., 2018) on the task. Concretely, our contributions are: Abstract How do we most effectively treat a disease or condition?"
2020.bionlp-1.13,2020.acl-main.740,0,\N,Missing
2021.emnlp-main.408,2020.bionlp-1.13,1,0.839143,"views are considered positive. Three star reviews are not included. For each dataset we simulate a pool of unlabeled data by hiding the annotations of the training set. We then create five distinct sets of labeled data (ten for sequence tagging tasks) by revealing the annotations for a random subset of the pool, forming a labeled training set L and an unlabeled training set U. For classification tasks, we sample ten examples per class to form L, while for CoNLL and EBMNLP, we sample two hundred examples. For the smaller TAC dataset, we use one hundred. Evidence Inference (Lehman et al., 2019; DeYoung et al., 2020) We construct a classification dataset derived from the evidence inference dataset, a biomedical corpus in which the task is to infer the effect of an intervention on an outcome from an article describing a randomized controlled trial. The classes correspond to the intervention leading to a significant increase, significant decrease, or no significant change in outcome. In the original task, the model must first extract relevant evidence sentences from the full text article, and then make a prediction based on this. We evaluate in the ‘oracle’ setting, in which the model must only classify giv"
2021.emnlp-main.408,D18-1045,0,0.0701193,"Missing"
2021.emnlp-main.408,N19-1371,1,0.821321,"Four and five star reviews are considered positive. Three star reviews are not included. For each dataset we simulate a pool of unlabeled data by hiding the annotations of the training set. We then create five distinct sets of labeled data (ten for sequence tagging tasks) by revealing the annotations for a random subset of the pool, forming a labeled training set L and an unlabeled training set U. For classification tasks, we sample ten examples per class to form L, while for CoNLL and EBMNLP, we sample two hundred examples. For the smaller TAC dataset, we use one hundred. Evidence Inference (Lehman et al., 2019; DeYoung et al., 2020) We construct a classification dataset derived from the evidence inference dataset, a biomedical corpus in which the task is to infer the effect of an intervention on an outcome from an article describing a randomized controlled trial. The classes correspond to the intervention leading to a significant increase, significant decrease, or no significant change in outcome. In the original task, the model must first extract relevant evidence sentences from the full text article, and then make a prediction based on this. We evaluate in the ‘oracle’ setting, in which the model"
2021.emnlp-main.408,D19-1371,0,0.0208289,"sentences The finetuned weights are produced by training on in the training set, ∼3, 000 in the test set), labeled BERT’s masked language model task with L ∪ U with entity categories person, organization, loca- as the training data. tion, and miscellaneous. In our classification experiments, we use a linear model on top of BERT (Devlin et al., 2018) TAC (Schmitt et al., 2018) comprises annotated as a classifier. For sequence tagging, we follow “materials and methods” sections from PubMed architecture and hyperparameter choices in prior Central articles (∼5, 500 sentences in the training work (Beltagy et al., 2019), adding a conditional set, ∼6, 500 in the test set). Labels are available random field (Lafferty et al., 2001) on top of BERT for 24 entity classes, of which we consider the two representations. We train all models using Adam best represented: end point and test article. (Kingma and Ba, 2015) with a learning rate of 2e-5 EBMNLP (Nye et al., 2018) is a corpus of anno- for classification and 1e-3 for sequence tagging. tated abstracts drawn from medical articles describIn exploratory experiments we observed that 4995 model performance is relatively robust to the choice of λ (the weight assigned"
2021.emnlp-main.408,P18-1019,1,0.88395,"AC (Schmitt et al., 2018) comprises annotated as a classifier. For sequence tagging, we follow “materials and methods” sections from PubMed architecture and hyperparameter choices in prior Central articles (∼5, 500 sentences in the training work (Beltagy et al., 2019), adding a conditional set, ∼6, 500 in the test set). Labels are available random field (Lafferty et al., 2001) on top of BERT for 24 entity classes, of which we consider the two representations. We train all models using Adam best represented: end point and test article. (Kingma and Ba, 2015) with a learning rate of 2e-5 EBMNLP (Nye et al., 2018) is a corpus of anno- for classification and 1e-3 for sequence tagging. tated abstracts drawn from medical articles describIn exploratory experiments we observed that 4995 model performance is relatively robust to the choice of λ (the weight assigned to the consistency loss term) when large quantities of unlabeled data are available. We therefore set λ to 1 in all of our semisupervised experiments. In our supervised learning only experiments, we found it necessary to compensate for the lack of unlabeled examples and the corresponding change in the relative weightings of standard and consistenc"
2021.emnlp-main.408,P16-1009,0,0.0376603,"icularly acute in domains such as information extraction from scientific documents, where unlaBy contrast, in NLP, there is less consensus beled in-domain data is plentiful but labeled data is about which perturbation models can be applied rare and requires significant annotator experience with confidence that they will not change the apto produce. The cost of acquiring data in such do- plicability of the original label. To apply UDA in mains has spurred interest in developing models NLP, researchers have primarily focused on backthat can achieve greater extraction accuracy, even translation (Sennrich et al., 2016; Edunov et al., when the available labelled corpora are small (Nye 2018), generating paraphrases by applying a maet al., 2018; Maharana et al., 2018). chine translation model to map a document into 4992 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4992–5001 c November 7–11, 2021. 2021 Association for Computational Linguistics a pivot language and then back into the original language. In practice, this process produces augmentations of varying quality. Another problem is that back-translation is slow, and performance may depend on arbitrary choi"
2021.emnlp-main.408,P19-1355,0,0.0489683,"Missing"
2021.emnlp-main.408,D18-1100,0,0.0192984,"ormance may depend on arbitrary choices concerning both the translation model and pivot language. Incorporating large quantities of unlabeled data in the training process is also computationally expensive. Given these limitations—and to better characterize why and when UDA helps—we investigate whether the benefits of UDA on NLP tasks can be achieved using less unlabeled data and/or simpler input perturbations. To this end, we investigate uniform random word replacement as an augmentation method. Random substitution for augmentation has been considered previously in the context of translation (Wang et al., 2018), and the UDA paper (Xie et al., 2019) gives preliminary results using random replacement for a single dataset, where it slightly underperforms back-translation. Here we deepen this analysis, showing that, surprisingly, random replacement is generally competitive with backtranslation. Furthermore, we find that significant increases in performance can be achieved by applying consistency loss on only small labeled data sets (although large volumes of in-domain unlabeled data provide further gains). As an additional contribution, we adapt UDA to sequence tagging tasks, which are common in NLP. Ba"
2021.emnlp-main.60,D19-1662,0,0.0210716,"e explicit motivation of fairness (Locatello et al., 2019a; Creager et al., 2019), which disentanglement may help to facilitate. different tasks. Recent work has proposed learning distinct vectors coding for semantic and syntactic properties of text (Chen et al., 2019; Ravfogel et al., 2020); these serve as baseline models in our experiments. Finally, while not explicitly framed in terms of disentanglement, efforts to ‘de-bias’ representations of text are related to our aims. Some of this work has used adversarial training to attempt to remove sensitive information (Elazar and Goldberg, 2018; Barrett et al., 2019). Network pruning. A final thread of relevant work concerns selective pruning of neural networks. This has often been done in the interest of model compression (Han et al., 2015a,b). Recent intriguing work has considered pruning from a different perspective: Identifying small subnetworks — winning ‘lottery tickets’ (Frankle and Carbin, 2019) — that, trained in isolation with the right initialization, can match the performance of the original networks from which they were extracted. Very recent work has demonstrated that winning tickets exist within BERT (Chen et al., 2020). 5 Discussion We hav"
2021.emnlp-main.60,N19-1254,0,0.360194,"ly clear independent factors of variation such as size, position, and orientation, which have physical grounding and can be formalized in terms of actions of symmetry subgroups (Higgins et al., 2018). A challenge in learning disentangled representations of text is that it is less clear which factors of variation should admit invariance. Still, we may hope to disentangle particular properties for certain applications — e.g., protected demographic information (Elazar and Goldberg, 2018) — and there are general properties of language that we might hope to disentangle, e.g., syntax and semantics (Chen et al., 2019). 2 Methods pects of interest. In such cases, we build triplets using these labels, defining (x0 , x1 , x2 ) such that (a) (a) (a) (b) (b) (b) y0 = y1 6= y2 and y0 = y2 6= y1 . 2.1 Masking Weights and Hidden Activations Figure 1 illustrates the two forms of masking that we consider in our approach (we depict only a single linear layer of the model). Here h = (h(a) , h(b) ) are input activations, W are the weights in the pretrained model,1 and h0 = (h0(a) , h0(b) ) are output activations. We augment each layer of the original network with two (binary) masks M = (M (a) , M (b) ), applied in one"
2021.emnlp-main.60,N19-1423,0,0.0391275,"mapping inputs onto a single vector that captures arbitrary combinations of features, our aim is to extract a representation that factorizes into distinct, complementary properties of inputs. Explicitly factorizing representations aids interpretability, in the sense that it becomes more straightforward to determine which factors of variation inform predictions in downstream tasks. A general motivation for learning disentangled representations is to try and minimize — or at least Large pretrained models such as ELMo (Peters expose — model reliance on spurious correlations, et al., 2018), BERT (Devlin et al., 2019), and XL- i.e., relationships between (potentially sensitive) Net (Yang et al., 2019) have come to dominate mod- attributes and labels that exist in the training data ern NLP. Such models rely on self-supervision over but which are not causally linked (Kaushik et al., large datasets to learn general-purpose representa- 2020). This is particularly important for large pretions of text that achieve strong predictive perfor- trained models like BERT, as we do not know what mance across a spectrum of downstream tasks (Liu the representations produced by such models enet al., 2019). A downside of su"
2021.emnlp-main.60,D18-1002,0,0.115459,"computer vision (Locatello et al., 2019b; Kulkarni et al., 2015; Chen et al., 2016; Higgins et al., 2017), where there exist comparatively clear independent factors of variation such as size, position, and orientation, which have physical grounding and can be formalized in terms of actions of symmetry subgroups (Higgins et al., 2018). A challenge in learning disentangled representations of text is that it is less clear which factors of variation should admit invariance. Still, we may hope to disentangle particular properties for certain applications — e.g., protected demographic information (Elazar and Goldberg, 2018) — and there are general properties of language that we might hope to disentangle, e.g., syntax and semantics (Chen et al., 2019). 2 Methods pects of interest. In such cases, we build triplets using these labels, defining (x0 , x1 , x2 ) such that (a) (a) (a) (b) (b) (b) y0 = y1 6= y2 and y0 = y2 6= y1 . 2.1 Masking Weights and Hidden Activations Figure 1 illustrates the two forms of masking that we consider in our approach (we depict only a single linear layer of the model). Here h = (h(a) , h(b) ) are input activations, W are the weights in the pretrained model,1 and h0 = (h0(a) , h0(b) ) ar"
2021.emnlp-main.60,D18-1497,1,0.90384,"Missing"
2021.emnlp-main.60,2021.ccl-1.108,0,0.0610824,"Missing"
2021.emnlp-main.60,P19-1041,0,0.0278647,"main task, we train an adversarial clas- tribution of the four groups is unequal in the train sifier to predict the non-target attribute, and the en- set, we expect that models will perform better on attribute combinations that are over-represented coder is trained to mitigate the adversaries’ ability in this set, and worse on those that are underto do so. We implement this via gradient-reversal represented, suggesting that the model is implicitly (Ganin and Lempitsky, 2015). We also compare exploiting the correlation between these attributes. to two variational autoencoder baselines: DRLST (John et al., 2019) is a VAE model with multi-task We report both the average and worst perforloss and adversarial loss; and DRLST-BERT is the mance on the four subgroups; the latter is a proxy to same model, except we use BERT as the encoder measure robustness when subgroup compositions 781 Masked Hidden Adversarial Finetuned Genre Sentiment Masked Weights Figure 3: t-SNE projection of sentiment and genre representations of different models. Marker colors denote sentiment (blue for positive and yellow for negative); marker shapes denote genre (× for drama and • for horror). In the upper row we expect the points"
2021.emnlp-main.60,N19-1049,0,0.0281914,"existing approaches (e.g., a varational Much of the prior work on disentanglement for NLP that does exist has focused on using such rep- auto-encoder on top of BERT), which have the benresentations to facilitate controlled generation, e.g., efit of finetuning all model parameters. manipulating sentiment (Larsson et al., 2017). Our experiments demonstrate the potential benA related notion is that of style transfer, for ex- efits of this approach. In Section 3.1 we showed ample, separating style from content in language that disentanglement via masking can yield repremodels Shen et al. (2017); Mir et al. (2019). There sentations that are comparatively robust to shifts has also been prior work on learning representa- in correlations between (potentially sensitive) attions of particular aspects to facilitate domain adap- tributes and target labels. Aside from increasing tation (Zhang et al., 2017), and aspect-specific in- robustness, finding sparse subnetworks that induce formation retrieval (Jain et al., 2018). Esmaeili et al. disentangled representations constitutes a new di(2019) focus on disentangling user and item repre- rection to pursue in service of providing at least one sentations for produc"
2021.emnlp-main.60,S13-2052,0,0.0437277,"ut in which we update all model parameters (as opposed to only estimating mask parameters). To evaluate learned representations with respect to the semantic and syntactic information that they encode, we evaluate them on four tasks. Two of these depend predominantly on semantic information, while the other two depend more heavily on syntax.3 For the semantics tasks we use: (i) A word content (WC) (Conneau et al., 2018) task in which we probe sentence representations to assess whether the corresponding sentence contains a particular word; and (ii) A semantic textual similarity (STS) benchmark (Nakov et al., 2013), which includes human provided similarity scores between pairs of sentences. We evaluate the former in terms of accuracy; for the latter (a ranking task) we use Spearman correlation. To evaluate whether representations encode syntax, we use: (i) A task in which the aim is to predict the length of the longest path in a sentence’s parse tree from its embedding (Depth) (Conneau et al., 2018); and (ii) A task in which we probe sentence representations for the type of their top constituents immediately below the S node (TopConst).4 Figure 5 shows the signed differences between the performance achi"
2021.emnlp-main.60,N18-1202,0,0.11781,"Missing"
2021.emnlp-main.60,P19-1452,0,0.0245688,"., semantics) while only weakly encoding others (e.g., syntax). Moreover, despite only learning masks, disentanglement-via-masking performs as well as — and often better than — previously proposed methods based on variational autoencoders and adversarial training. h’ M W Figure 1: Masking weights and hidden activations in BERT. We show a linear layer with weights W , inputs h, and outputs h0 . We learn a mask for each disentangled factor, which is either applied to the weights W or to intermediate representations h. encoded in learned representations (Rogers et al., 2020; Linzen et al., 2019; Tenney et al., 2019). In this paper we investigate whether we can uncover disentangled representations from pretrained models. That is, rather than mapping inputs onto a single vector that captures arbitrary combinations of features, our aim is to extract a representation that factorizes into distinct, complementary properties of inputs. Explicitly factorizing representations aids interpretability, in the sense that it becomes more straightforward to determine which factors of variation inform predictions in downstream tasks. A general motivation for learning disentangled representations is to try and minimize —"
2021.emnlp-main.60,P18-1042,0,0.0246056,"iment, we follow prior work in attempting to disentangle semantic from syntactic information encoded in learned (BERT) representations of text. Because we have proposed exploiting triplet-loss, we first construct triplets (x0 , x1 , x2 ) such that x0 and x1 are similar semantically but differ in syntax, while x0 and x2 are syntactically similar but encode different semantic information. We follow prior work (Chen et al., 2019; Ravfogel et al., 2020) in deriving these triplets. Specifically, we obtain x0 , x1 from 3 This is a (very) simplified view of ‘semantics’ / ‘syntax’. 4 the ParaNMT-50M (Wieting and Gimpel, 2018) See (Conneau et al., 2018) for more details regarding WC, dataset. Here x1 is obtained by applying back- Depth, and TopConst tasks. 783 TPR TNR TNR True Positive Rate White TPR TPR TPR TNR White TPR True Positive Rate White TNR Black Black True Negative Rate White White White TPR Diﬀ TNR True Negative RateTPR Diﬀ TNR Diﬀ Black Black True Positive Rate Black White White BlackWhite White White White TNR BlackTNR Black TPR TNR True Positive Rate True Negative Rate White White TPR TNR White White More likely to Black White White White True PositiveWhite Rate RateTPRBlack Black True Negative TPR T"
2021.emnlp-main.60,Q17-1036,0,0.0282437,"ulating sentiment (Larsson et al., 2017). Our experiments demonstrate the potential benA related notion is that of style transfer, for ex- efits of this approach. In Section 3.1 we showed ample, separating style from content in language that disentanglement via masking can yield repremodels Shen et al. (2017); Mir et al. (2019). There sentations that are comparatively robust to shifts has also been prior work on learning representa- in correlations between (potentially sensitive) attions of particular aspects to facilitate domain adap- tributes and target labels. Aside from increasing tation (Zhang et al., 2017), and aspect-specific in- robustness, finding sparse subnetworks that induce formation retrieval (Jain et al., 2018). Esmaeili et al. disentangled representations constitutes a new di(2019) focus on disentangling user and item repre- rection to pursue in service of providing at least one sentations for product reviews. Moradshahi et al. type of model interpretability for NLP. Finally, we (2019) combine BERT with Tensor-Product Rep- note that sparse masking (which does not mutate resentations to improve its transferability across the underlying transformer parameters) may offer 785 efficiency a"
2021.emnlp-main.60,2020.emnlp-main.174,0,0.0196653,"(h0(a) , h0(b) ) are output activations. We augment each layer of the original network with two (binary) masks M = (M (a) , M (b) ), applied in one of two ways: 1. Masking Weights Here masks M (a) and M (b) have the same shape as weights W , and outputs are computed using the masked weights tensor h0 = h · (W ◦ M ). (1) 2. Masking Hidden Activations In this case masks M (a) and M (b) have the same shape as the intermediate (hidden) activations h(a) and h(b) . Output activations are computed by applying the original weights W to masked inputs h0 = (h ◦ M ) · W. (2) In both methods, we follow (Zhao et al., 2020) and only mask the last several layers of BERT, leaving bottom layers unchanged.2 2.2 Triplet Loss We are interested in learning a disentangled repTo learn masks, we assume that we have access resentation that maps inputs x (text) onto vectors to supervision in the form of triplets, as introz (a) and z (b) that encode two distinct factors of duced above. Passing (x0 , x1 , x2 ) through our variation. To do so, we will learn two sets of masks model yields two representations for each instance: M (a) and M (b) that can be applied to either the (a) (b) (a) (b) (a) (b) (z0 , z0 ), (z1 , z1 ), (z2"
2021.emnlp-main.60,2020.blackboxnlp-1.9,0,0.549935,"in the fairness literature. We report the TPR and TNR of each model achieved over white and Black individuals, respectively, as well as the difference across the two groups in Figure 4. We observe that the proposed model variants achieve a smaller TPR and TNR gap across the two races (see rightmost subplots), indicating that performance is more equitable across the groups, compared to baselines. 3.3 Disentangling Semantics from Syntax translation to x0 , i.e., by translating x0 from English to Czech and then back into English. To derive x2 we keep all function words (from a list introduced in Ravfogel et al. 2020) in x0 , and replace content words by masking each in turn, running the resultant input forward through BERT, and randomly selecting one of the top predictions (that differs from the original word) as a replacement. We compare our disentanglement-via-masking strategies against models that represent state-of-theart approaches to disentangling syntax and semantics. In particular, we compare against VGVAE (Chen et al., 2019), though we implement this on top of BERT-base to allow fair comparison. Following prior work that has used triplet loss for disentanglement, we also compare against a model i"
2021.emnlp-main.60,2020.tacl-1.54,0,0.0264642,"at strongly encode particular aspects (e.g., semantics) while only weakly encoding others (e.g., syntax). Moreover, despite only learning masks, disentanglement-via-masking performs as well as — and often better than — previously proposed methods based on variational autoencoders and adversarial training. h’ M W Figure 1: Masking weights and hidden activations in BERT. We show a linear layer with weights W , inputs h, and outputs h0 . We learn a mask for each disentangled factor, which is either applied to the weights W or to intermediate representations h. encoded in learned representations (Rogers et al., 2020; Linzen et al., 2019; Tenney et al., 2019). In this paper we investigate whether we can uncover disentangled representations from pretrained models. That is, rather than mapping inputs onto a single vector that captures arbitrary combinations of features, our aim is to extract a representation that factorizes into distinct, complementary properties of inputs. Explicitly factorizing representations aids interpretability, in the sense that it becomes more straightforward to determine which factors of variation inform predictions in downstream tasks. A general motivation for learning disentangle"
2021.emnlp-main.60,P19-1163,0,0.367776,"uction and Motivation Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 778–791 c November 7–11, 2021. 2021 Association for Computational Linguistics As one example that we explore in this paper, consider the task of identifying Tweets that contain hate speech (Founta et al., 2018). Recent work shows that models trained over Tweets annotated on a toxicity scale exhibit a racial bias: They have a tendency to over-predict that Tweets written by users who self-identify as Black are “toxic”, owing to the use of African American Vernacular English (AAVE; Sap et al. 2019). In principal, disentangled representations would allow us to isolate relevant signal from irrelevant or spurious factors (such as, in this case, the particular English dialect used), which might in turn reveal and allow us to mitigate unwanted system biases, and increase robustness. To date, most research on disentangled representations has focused on applications in computer vision (Locatello et al., 2019b; Kulkarni et al., 2015; Chen et al., 2016; Higgins et al., 2017), where there exist comparatively clear independent factors of variation such as size, position, and orientation, which hav"
2021.mrqa-1.3,W19-5039,0,0.332061,"ges 28–41 Nov 10th, 2021 ©2021 Association for Computational Linguistics sis of the latest evidence. This has motivated development of QA systems and associated medical QA datasets used to train them. For example, BioASQ (Tsatsaronis et al., 2015) and PubMedQA (Jin et al., 2019) have been created to train and evaluate systems that answer clinicians’ questions based on medical research literature, while emrQA (Pampari et al., 2018), emrKBQA (Raghavan et al., 2021) and why-QA (Fan, 2019) were constructed using queries concerning patient data from electronic health records (EHRs). MEDIQA-QA (Ben Abacha et al., 2019) and LiveQA-Medical (Abacha et al., 2017) are datasets designed for systems that answer consumer (patient) queries. MEDIQA-AnS (Savery et al., 2020) accompanies the answers from MEDIQA-QA with summaries that consumers would understand more easily. Systems for QA over EHRs aim to answer questions about the medical history or prior care of individual patients. By contrast, our focus here is on systems that can provide general evidence-based guidance in response to queries; we therefore omit emrQA, emrKBQA and why-QA from our discussion. does not allow the possibility of changing practice, or pro"
2021.mrqa-1.3,W19-1913,0,0.0846102,"ih.gov https://www.cochranelibrary.com 28 Proceedings of the 3rd Workshop on Machine Reading for Question Answering , pages 28–41 Nov 10th, 2021 ©2021 Association for Computational Linguistics sis of the latest evidence. This has motivated development of QA systems and associated medical QA datasets used to train them. For example, BioASQ (Tsatsaronis et al., 2015) and PubMedQA (Jin et al., 2019) have been created to train and evaluate systems that answer clinicians’ questions based on medical research literature, while emrQA (Pampari et al., 2018), emrKBQA (Raghavan et al., 2021) and why-QA (Fan, 2019) were constructed using queries concerning patient data from electronic health records (EHRs). MEDIQA-QA (Ben Abacha et al., 2019) and LiveQA-Medical (Abacha et al., 2017) are datasets designed for systems that answer consumer (patient) queries. MEDIQA-AnS (Savery et al., 2020) accompanies the answers from MEDIQA-QA with summaries that consumers would understand more easily. Systems for QA over EHRs aim to answer questions about the medical history or prior care of individual patients. By contrast, our focus here is on systems that can provide general evidence-based guidance in response to que"
2021.mrqa-1.3,D19-1259,0,0.257599,"inicians would search for answers to such questions with reference to high-quality studies and up-to-date evidence syntheses, typically indexed in medical databases such as PubMed1 and the 1 2 https://pubmed.ncbi.nlm.nih.gov https://www.cochranelibrary.com 28 Proceedings of the 3rd Workshop on Machine Reading for Question Answering , pages 28–41 Nov 10th, 2021 ©2021 Association for Computational Linguistics sis of the latest evidence. This has motivated development of QA systems and associated medical QA datasets used to train them. For example, BioASQ (Tsatsaronis et al., 2015) and PubMedQA (Jin et al., 2019) have been created to train and evaluate systems that answer clinicians’ questions based on medical research literature, while emrQA (Pampari et al., 2018), emrKBQA (Raghavan et al., 2021) and why-QA (Fan, 2019) were constructed using queries concerning patient data from electronic health records (EHRs). MEDIQA-QA (Ben Abacha et al., 2019) and LiveQA-Medical (Abacha et al., 2017) are datasets designed for systems that answer consumer (patient) queries. MEDIQA-AnS (Savery et al., 2020) accompanies the answers from MEDIQA-QA with summaries that consumers would understand more easily. Systems for"
2021.mrqa-1.3,N19-1371,1,0.833159,"ing reviews (criterion 5). In addition, the clinician is able to investigate the contradictory snippets further by clicking on &quot;Conflicting Snippets&quot; which would show the snippets in Figure 6. Criterion 4 is inapplicable in this case as no answer was retrieved from the documents. One promising direction which may permit improved handling of contradictory evidence involves use of argumentation-based logic to “reason” about multiple potentially conflicting inputs (Chapman et al., 2019; Cyras et al., 2018), perhaps after explicitly inferring the reported findings concerning treatment efficacies (Lehman et al., 2019; Nye et al., 2020). An alternative (more audacious) direction 5 Conclusions We have introduced criteria for assessing the transparency of medical question answering systems. These have been guided by the following question: What would be needed for clinicians to trust, and act upon answers from a QA system? In part we have argued that these systems should be explicitly informed by principles of EBM. The adequacy of existing medical systems and datasets, including BioASQ, PubMedQA, MEDIQA-QA, MEDIQAAnS, LiveQA-Medical, MEANS, AskHERMES, CLINIQA and MedQA, was assessed using the transparency cr"
2021.mrqa-1.3,2021.bionlp-1.7,0,0.135798,"d the 1 2 https://pubmed.ncbi.nlm.nih.gov https://www.cochranelibrary.com 28 Proceedings of the 3rd Workshop on Machine Reading for Question Answering , pages 28–41 Nov 10th, 2021 ©2021 Association for Computational Linguistics sis of the latest evidence. This has motivated development of QA systems and associated medical QA datasets used to train them. For example, BioASQ (Tsatsaronis et al., 2015) and PubMedQA (Jin et al., 2019) have been created to train and evaluate systems that answer clinicians’ questions based on medical research literature, while emrQA (Pampari et al., 2018), emrKBQA (Raghavan et al., 2021) and why-QA (Fan, 2019) were constructed using queries concerning patient data from electronic health records (EHRs). MEDIQA-QA (Ben Abacha et al., 2019) and LiveQA-Medical (Abacha et al., 2017) are datasets designed for systems that answer consumer (patient) queries. MEDIQA-AnS (Savery et al., 2020) accompanies the answers from MEDIQA-QA with summaries that consumers would understand more easily. Systems for QA over EHRs aim to answer questions about the medical history or prior care of individual patients. By contrast, our focus here is on systems that can provide general evidence-based guid"
2021.mrqa-1.3,D18-1258,0,0.161417,"cal databases such as PubMed1 and the 1 2 https://pubmed.ncbi.nlm.nih.gov https://www.cochranelibrary.com 28 Proceedings of the 3rd Workshop on Machine Reading for Question Answering , pages 28–41 Nov 10th, 2021 ©2021 Association for Computational Linguistics sis of the latest evidence. This has motivated development of QA systems and associated medical QA datasets used to train them. For example, BioASQ (Tsatsaronis et al., 2015) and PubMedQA (Jin et al., 2019) have been created to train and evaluate systems that answer clinicians’ questions based on medical research literature, while emrQA (Pampari et al., 2018), emrKBQA (Raghavan et al., 2021) and why-QA (Fan, 2019) were constructed using queries concerning patient data from electronic health records (EHRs). MEDIQA-QA (Ben Abacha et al., 2019) and LiveQA-Medical (Abacha et al., 2017) are datasets designed for systems that answer consumer (patient) queries. MEDIQA-AnS (Savery et al., 2020) accompanies the answers from MEDIQA-QA with summaries that consumers would understand more easily. Systems for QA over EHRs aim to answer questions about the medical history or prior care of individual patients. By contrast, our focus here is on systems that can pr"
2021.naacl-main.395,W14-1207,0,0.030246,"rk Recent efforts on data-driven text simplification methods have tended to rely on two resources: the Wikipedia-Simple Wikipedia aligned corpus (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011) and the Newsela simplification corpus (Xu et al., 2015). Yet, there is an urgent need to simplify medical texts due to health literacy levels (World Health Organization, 2013). However, due to a lack of resources with which to train model-based simplification systems in this domain, past work has tended to focus on lexical simplification (Damay et al., 2006; Kandula et al., 2010; Abrahamsson et al., 2014; Mukherjee et al., 2017). Recently, Adduru et al. (2018) and Van den Bercken et al. (2019) introduced sentence-aligned corpora at the scale of thousands of sentence pairs. In contrast to our corpus, these datasets were automatically derived using paraphrase mining or monolingual alignment processes. Furthermore, as these are exclusively sentence corpora, they limit the set of potential approaches to just those that operate over sentences. Grabar and Cardon (2018) created a simplification corpus for medical texts in French, in which a small subset of the text pairs are manually sentence-aligne"
2021.naacl-main.395,2020.cl-1.4,0,0.012795,"l stay. We found no evidence suggesting that giving infants high volumes of milk causes feeding or gut problems, but this finding is not certain. Table 1: Sample excerpts from a technical abstract (top) and corresponding plain-language summary (bottom) from the Cochrane Library. One potential solution to this problem is text simplification, i.e., editing documents such that they are accessible to a wider audience, while preserving the key information that they contain. Although manual simplification is too expensive to feasibly apply at scale, automatic text simplification (Siddharthan, 2014; Alva-Manchego et al., 2020) provides a potential means of rendering a large volume of specialist knowledge more accessible. Large-scale data-driven simplification systems have mostly been trained on Wikipedia (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011) and news (Xu et al., 2015), and focus on sentence simplification (Wubben et al., 2012; Wang et al., 2016; Xu et al., 2016; Zhang and Lapata, 2017; Kriz et al., 2019; Dong et al., 2019; Alva-Manchego et al., 2020); on the other hand, medical text simplification is resource poor. Recent work has involved constructing sentencealigned data automati"
2021.naacl-main.395,N18-3011,0,0.0260975,"l., 2019). is pretrained on technical literature. A paired t-test Capitalizing on this intuition, we consider two confirms that these observed differences between large-scale pre-trained masked language modthe abstracts and PLS distributions are statistically els: (1) BERT (Devlin et al., 2019) trained on significant (with p &lt; 0.01). BooksCorpus (Zhu et al., 2015) and English Wikipedia; and (2) SciBERT (Beltagy et al., 2019), Which metric discriminates better? To better trained on a sample of 1.14 million technical pa- determine how well the proposed masked probabilpers from Semantic Scholar (Ammar et al., 2018) ity outputs discriminate between technical abstracts (mostly biomedical and computer science articles). and PLS, we plot receiver operating characteristic 4975 Figure 1: BERT (left) vs SciBERT (right) probabilities of technical abstracts (blue) and PLS (red). 1.0 True Positive Rate 0.8 0.6 0.4 0.2 0.0 0.0 Flesch-Kincaid (area = 0.68) ARI (area = 0.57) General BERT (area = 0.66) SciBERT (area = 0.70) 0.2 0.4 0.6 False Positive Rate 0.8 1.0 Figure 2: ROC Curves for Readability Metrics. (ROC) curves for the outputs of BERT, SciBERT, Flesch-Kincaid and ARI, coding technical and PLS abstracts as 0"
2021.naacl-main.395,D19-1371,0,0.0177855,"red to a model trained technical abstracts than for those in the plain lanover general lay corpora, as in the original BERT guage versions, as we might expect given that this model (Devlin et al., 2019). is pretrained on technical literature. A paired t-test Capitalizing on this intuition, we consider two confirms that these observed differences between large-scale pre-trained masked language modthe abstracts and PLS distributions are statistically els: (1) BERT (Devlin et al., 2019) trained on significant (with p &lt; 0.01). BooksCorpus (Zhu et al., 2015) and English Wikipedia; and (2) SciBERT (Beltagy et al., 2019), Which metric discriminates better? To better trained on a sample of 1.14 million technical pa- determine how well the proposed masked probabilpers from Semantic Scholar (Ammar et al., 2018) ity outputs discriminate between technical abstracts (mostly biomedical and computer science articles). and PLS, we plot receiver operating characteristic 4975 Figure 1: BERT (left) vs SciBERT (right) probabilities of technical abstracts (blue) and PLS (red). 1.0 True Positive Rate 0.8 0.6 0.4 0.2 0.0 0.0 Flesch-Kincaid (area = 0.68) ARI (area = 0.57) General BERT (area = 0.66) SciBERT (area = 0.70) 0.2 0"
2021.naacl-main.395,P11-2117,0,0.238037,"om the Cochrane Library. One potential solution to this problem is text simplification, i.e., editing documents such that they are accessible to a wider audience, while preserving the key information that they contain. Although manual simplification is too expensive to feasibly apply at scale, automatic text simplification (Siddharthan, 2014; Alva-Manchego et al., 2020) provides a potential means of rendering a large volume of specialist knowledge more accessible. Large-scale data-driven simplification systems have mostly been trained on Wikipedia (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011) and news (Xu et al., 2015), and focus on sentence simplification (Wubben et al., 2012; Wang et al., 2016; Xu et al., 2016; Zhang and Lapata, 2017; Kriz et al., 2019; Dong et al., 2019; Alva-Manchego et al., 2020); on the other hand, medical text simplification is resource poor. Recent work has involved constructing sentencealigned data automatically using monolingual text alignment methods (Adduru et al., 2018; Van den Bercken et al., 2019), but this process is noisy and constrains the task to sentence-level simplification. The need for accessible medical information has never been greater. A"
2021.naacl-main.395,N19-1423,0,0.0115063,"’ of text. In particular, when such models induce distributions over instances from the respecare trained on specialized or technical language tive sets that are clearly different. For example, (e.g., scientific articles) we would expect the likeSciBERT (which yields sharper differences) outlihoods subsequently assigned to ‘jargon’ tokens puts higher likelihoods for tokens comprising the to be relatively high compared to a model trained technical abstracts than for those in the plain lanover general lay corpora, as in the original BERT guage versions, as we might expect given that this model (Devlin et al., 2019). is pretrained on technical literature. A paired t-test Capitalizing on this intuition, we consider two confirms that these observed differences between large-scale pre-trained masked language modthe abstracts and PLS distributions are statistically els: (1) BERT (Devlin et al., 2019) trained on significant (with p &lt; 0.01). BooksCorpus (Zhu et al., 2015) and English Wikipedia; and (2) SciBERT (Beltagy et al., 2019), Which metric discriminates better? To better trained on a sample of 1.14 million technical pa- determine how well the proposed masked probabilpers from Semantic Scholar (Ammar et"
2021.naacl-main.395,P19-1331,0,0.0671541,"rmation that they contain. Although manual simplification is too expensive to feasibly apply at scale, automatic text simplification (Siddharthan, 2014; Alva-Manchego et al., 2020) provides a potential means of rendering a large volume of specialist knowledge more accessible. Large-scale data-driven simplification systems have mostly been trained on Wikipedia (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011) and news (Xu et al., 2015), and focus on sentence simplification (Wubben et al., 2012; Wang et al., 2016; Xu et al., 2016; Zhang and Lapata, 2017; Kriz et al., 2019; Dong et al., 2019; Alva-Manchego et al., 2020); on the other hand, medical text simplification is resource poor. Recent work has involved constructing sentencealigned data automatically using monolingual text alignment methods (Adduru et al., 2018; Van den Bercken et al., 2019), but this process is noisy and constrains the task to sentence-level simplification. The need for accessible medical information has never been greater. A Pew Research survey of American’s online health habits in 2013 revealed that “one in three American adults have gone online to figure out a medical condition” (Fox and Duggan, 2013)."
2021.naacl-main.395,W18-7002,0,0.0204112,"fication systems in this domain, past work has tended to focus on lexical simplification (Damay et al., 2006; Kandula et al., 2010; Abrahamsson et al., 2014; Mukherjee et al., 2017). Recently, Adduru et al. (2018) and Van den Bercken et al. (2019) introduced sentence-aligned corpora at the scale of thousands of sentence pairs. In contrast to our corpus, these datasets were automatically derived using paraphrase mining or monolingual alignment processes. Furthermore, as these are exclusively sentence corpora, they limit the set of potential approaches to just those that operate over sentences. Grabar and Cardon (2018) created a simplification corpus for medical texts in French, in which a small subset of the text pairs are manually sentence-aligned, resulting in 663 sentence pairs, 112 of which are also from Cochrane. With respect to modeling, recent work has focused on sentence simplification, treating it as a monolingual machine translation task (Wubben et al., 2012; Wang et al., 2016; Xu et al., 2016) using encoder-decoder models (Zhang and Lapata, 2017; Kriz et al., 2019; Dong et al., 2019). In the medical domain, existing systems tend to adopt lexical and syntactic simplification (Damay et al., 2006;"
2021.naacl-main.395,P19-1356,0,0.0213978,"een abstracts and generated PLS are statistically significant; so are differences in FK and ARI between UL models and No-UL (p &lt; 0.01, paired t-test). puts, however, are at the late-high school/early college levels. This could reflect the relatively small differences in readability scores between abstracts and PLS in general (Section 3.2). 5.2 Style In Section 3.2 we showed that SciBERT masked probability scores are more useful as a discriminator between technical abstracts and PLS than the standard readability metrics, which use surfacelevel cues like word and sentence counts. Experiments by Jawahar et al. (2019) suggest that BERTstyle masked language models encode a wide array of syntactic and semantic features of language, which they then employs for downstream tasks. For this reason, we use SciBERT masked probability scores as our notion of style, with lower scores corresponding to simpler, less technical language. To explore the extent to which the generated summaries stylistically resemble the PLS, we computed the average of the SciBERT masked probability scores of the generated texts for each model. The results are shown in Table 5 along with the readability scores. We see that every model produ"
2021.naacl-main.395,C10-1062,0,0.0511234,"rastically different in length, by keeping only instances where the length ratio between the two falls between 0.2 and 1.3. Our final dataset comprises 4459 pairs of technical abstracts and PLS, all containing ≤1024 tokens (so that they can be fed into the BART model in their entirety). 3.2 Characterizing readability differences Readability metrics. Designing metrics that reliably capture readability remains an open topic of research. In recent years, a host of metrics have been developed that use a wide variety of linguistic features to assess readability in a supervised manner. For example, Kate et al. (2010) developed a metric based on syntactical, semantic, and language model-based features, and Vajjala and Luˇci´c (2018) developed a new readability corpus, on which they trained support vector machines to predict text readability. For this medical text simplification task, however, we considered a couple 1. Background, Objectives, Search Methods, Selection Criestablished heuristics-based readability metrics due teria, Data Collection and Analysis, Main Results, Auto clear domain differences between our Cochrane thors’ Conclusions corpus and those used to train supervised read2. Background, Objec"
2021.naacl-main.395,N19-1317,0,0.0678952,"erving the key information that they contain. Although manual simplification is too expensive to feasibly apply at scale, automatic text simplification (Siddharthan, 2014; Alva-Manchego et al., 2020) provides a potential means of rendering a large volume of specialist knowledge more accessible. Large-scale data-driven simplification systems have mostly been trained on Wikipedia (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011) and news (Xu et al., 2015), and focus on sentence simplification (Wubben et al., 2012; Wang et al., 2016; Xu et al., 2016; Zhang and Lapata, 2017; Kriz et al., 2019; Dong et al., 2019; Alva-Manchego et al., 2020); on the other hand, medical text simplification is resource poor. Recent work has involved constructing sentencealigned data automatically using monolingual text alignment methods (Adduru et al., 2018; Van den Bercken et al., 2019), but this process is noisy and constrains the task to sentence-level simplification. The need for accessible medical information has never been greater. A Pew Research survey of American’s online health habits in 2013 revealed that “one in three American adults have gone online to figure out a medical condition” (Fox"
2021.naacl-main.395,2020.acl-main.703,0,0.518224,"75) and Automated Readability Index (Senter and Smith, 1967), are small. Instead, the differences are better captured using large-scale pretrained masked language models, and this reveals that there is more to the language difference than the shallow cues such as sentence and word lengths that traditional readability metrics focus on. We present baseline methods for automatic text simplification over this data and perform analyses that highlight the challenges of this important simplification task. We find that when naively finetuned for the task, existing encoder-decoder models such as BART (Lewis et al., 2020) tend to prefer deletion over paraphrasing or explaining, and are prone to generating technical words. We propose a new approach to try and mitigate the latter issue by imposing a variant of unlikelihood loss (Welleck et al., 2019) that explicitly penalizes the decoder for production of ‘technical’ tokens. We show that this yields improvements in terms of readability with only a minor tradeoff with content quality. lihood training to explicitly penalize models for producing jargon. We release our code and data at https://github.com/AshOlogn/Paragraphlevel-Simplification-of-Medical-Texts. 2 Rel"
2021.naacl-main.395,2020.acl-main.428,0,0.0383659,"obability distribution constructed by removing the ‘tail’ of probability mass from BART’s output distribution and then renormalizing. This strategy mitigates the awkward repetition typical of greedy methods like beam search while still avoiding incoherence by truncating the unlikely tail in the original model distribution. 4.2 Unlikelihood training As an additional mechanism to encourage simple terminology in the PLS generated by our model, we propose a new method in which we explicitly penalize the model for producing seemingly technical words via unlikelihood training (Welleck et al., 2019; Li et al., 2020). The idea is to add a term to the objective that encourages the model to decrease the probability mass assigned to some set of tokens S. This is realized by adding a term to the (log) P|S| loss: U L = j=1 − log(1−pθ (sj |y&lt;t , x)), where x is the technical abstract input to the encoder, y&lt;t is the prefix of the target summary y input to the decoder at time t, and pθ (sj |y&lt;t , x) is the probability assigned to token sj in the distribution output by BART (with model parameters θ) at time t. This 1 We also considered starting from a checkpoint corresponding to training over CNN/Daily News but p"
2021.naacl-main.395,L16-1505,0,0.0204471,"ion corpus for medical texts in French, in which a small subset of the text pairs are manually sentence-aligned, resulting in 663 sentence pairs, 112 of which are also from Cochrane. With respect to modeling, recent work has focused on sentence simplification, treating it as a monolingual machine translation task (Wubben et al., 2012; Wang et al., 2016; Xu et al., 2016) using encoder-decoder models (Zhang and Lapata, 2017; Kriz et al., 2019; Dong et al., 2019). In the medical domain, existing systems tend to adopt lexical and syntactic simplification (Damay et al., 2006; Kandula et al., 2010; Llanos et al., 2016). Research on document simplification has been sparse; In sum, this work takes a step towards paragraph- to the best of our knowledge, the few prior works on level simplification of medical texts by: (1) intro- this in English have focused on analysis (Petersen ducing a sizable new dataset, (2) proposing and and Ostendorf, 2007), sentence deletion (Woodvalidating a new masked language model (MLM)- send and Lapata, 2011; Zhong et al., 2020), and based metric for scoring the technicality of texts, localized explanation generation (Srikanth and (3) analyzing and understanding the style of plain L"
2021.naacl-main.395,D18-1206,0,0.0600654,"Missing"
2021.naacl-main.395,P02-1040,0,0.109933,"Missing"
2021.naacl-main.395,W18-0535,0,0.0461471,"Missing"
2021.naacl-main.395,2020.emnlp-demos.6,0,0.0854253,"Missing"
2021.naacl-main.395,D11-1038,0,0.258409,"anguage summary (bottom) from the Cochrane Library. One potential solution to this problem is text simplification, i.e., editing documents such that they are accessible to a wider audience, while preserving the key information that they contain. Although manual simplification is too expensive to feasibly apply at scale, automatic text simplification (Siddharthan, 2014; Alva-Manchego et al., 2020) provides a potential means of rendering a large volume of specialist knowledge more accessible. Large-scale data-driven simplification systems have mostly been trained on Wikipedia (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011) and news (Xu et al., 2015), and focus on sentence simplification (Wubben et al., 2012; Wang et al., 2016; Xu et al., 2016; Zhang and Lapata, 2017; Kriz et al., 2019; Dong et al., 2019; Alva-Manchego et al., 2020); on the other hand, medical text simplification is resource poor. Recent work has involved constructing sentencealigned data automatically using monolingual text alignment methods (Adduru et al., 2018; Van den Bercken et al., 2019), but this process is noisy and constrains the task to sentence-level simplification. The need for accessible medical informatio"
2021.naacl-main.395,P12-1107,0,0.0732422,"Missing"
2021.naacl-main.395,Q15-1021,0,0.345603,"al solution to this problem is text simplification, i.e., editing documents such that they are accessible to a wider audience, while preserving the key information that they contain. Although manual simplification is too expensive to feasibly apply at scale, automatic text simplification (Siddharthan, 2014; Alva-Manchego et al., 2020) provides a potential means of rendering a large volume of specialist knowledge more accessible. Large-scale data-driven simplification systems have mostly been trained on Wikipedia (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011) and news (Xu et al., 2015), and focus on sentence simplification (Wubben et al., 2012; Wang et al., 2016; Xu et al., 2016; Zhang and Lapata, 2017; Kriz et al., 2019; Dong et al., 2019; Alva-Manchego et al., 2020); on the other hand, medical text simplification is resource poor. Recent work has involved constructing sentencealigned data automatically using monolingual text alignment methods (Adduru et al., 2018; Van den Bercken et al., 2019), but this process is noisy and constrains the task to sentence-level simplification. The need for accessible medical information has never been greater. A Pew Research survey of Ame"
2021.naacl-main.395,Q16-1029,0,0.255393,"ccessible to a wider audience, while preserving the key information that they contain. Although manual simplification is too expensive to feasibly apply at scale, automatic text simplification (Siddharthan, 2014; Alva-Manchego et al., 2020) provides a potential means of rendering a large volume of specialist knowledge more accessible. Large-scale data-driven simplification systems have mostly been trained on Wikipedia (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011) and news (Xu et al., 2015), and focus on sentence simplification (Wubben et al., 2012; Wang et al., 2016; Xu et al., 2016; Zhang and Lapata, 2017; Kriz et al., 2019; Dong et al., 2019; Alva-Manchego et al., 2020); on the other hand, medical text simplification is resource poor. Recent work has involved constructing sentencealigned data automatically using monolingual text alignment methods (Adduru et al., 2018; Van den Bercken et al., 2019), but this process is noisy and constrains the task to sentence-level simplification. The need for accessible medical information has never been greater. A Pew Research survey of American’s online health habits in 2013 revealed that “one in three American adults have gone onli"
2021.naacl-main.395,D17-1062,0,0.0978497,"der audience, while preserving the key information that they contain. Although manual simplification is too expensive to feasibly apply at scale, automatic text simplification (Siddharthan, 2014; Alva-Manchego et al., 2020) provides a potential means of rendering a large volume of specialist knowledge more accessible. Large-scale data-driven simplification systems have mostly been trained on Wikipedia (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011) and news (Xu et al., 2015), and focus on sentence simplification (Wubben et al., 2012; Wang et al., 2016; Xu et al., 2016; Zhang and Lapata, 2017; Kriz et al., 2019; Dong et al., 2019; Alva-Manchego et al., 2020); on the other hand, medical text simplification is resource poor. Recent work has involved constructing sentencealigned data automatically using monolingual text alignment methods (Adduru et al., 2018; Van den Bercken et al., 2019), but this process is noisy and constrains the task to sentence-level simplification. The need for accessible medical information has never been greater. A Pew Research survey of American’s online health habits in 2013 revealed that “one in three American adults have gone online to figure out a medic"
2021.naacl-main.395,C10-1152,0,0.231048,"responding plain-language summary (bottom) from the Cochrane Library. One potential solution to this problem is text simplification, i.e., editing documents such that they are accessible to a wider audience, while preserving the key information that they contain. Although manual simplification is too expensive to feasibly apply at scale, automatic text simplification (Siddharthan, 2014; Alva-Manchego et al., 2020) provides a potential means of rendering a large volume of specialist knowledge more accessible. Large-scale data-driven simplification systems have mostly been trained on Wikipedia (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011) and news (Xu et al., 2015), and focus on sentence simplification (Wubben et al., 2012; Wang et al., 2016; Xu et al., 2016; Zhang and Lapata, 2017; Kriz et al., 2019; Dong et al., 2019; Alva-Manchego et al., 2020); on the other hand, medical text simplification is resource poor. Recent work has involved constructing sentencealigned data automatically using monolingual text alignment methods (Adduru et al., 2018; Van den Bercken et al., 2019), but this process is noisy and constrains the task to sentence-level simplification. The need for ac"
2021.naacl-main.73,W19-1909,0,0.0265968,"Missing"
2021.naacl-main.75,D19-1410,0,0.0142101,"presentation from the penultimate network layer). To quantify the importance of training point xi on the prediction for target sample xt , we calculate the similarity in embedding space induced by the model.1 To measure similarity we consider three measures: Euclidean distance, Dot product, and Cosine similarity. Specifically, we define similarity-based attribution scores as: NN EUC = −kft − fi k2 , NN COS = cos(ft , fi ), and NN DOT = hft , fi i. To investigate the effect of fine-tuning on these similarity measures, we also derive rankings based on similarities between untuned sentence-BERT (Reimers et al., 2019) representations. Gradient Based Attribution Influence Functions (IFs) were proposed in the context of neural models by Koh and Liang (2017) to quantify the contribution made by individual training points on specific test predictions. Denoting model parameter ˆ the IF approximates the effect that estimates by θ, upweighting instance i by a small amount—i — would have on the parameter estimates (here H is the Hessian of the loss function with respect to our dθˆ ˆ This esparameters): d = −H ˆ−1 ∇θ L(xi , yi , θ). i θ timate can in turn be used to derive the effect on a ˆ T · dθˆ . specific tes"
2021.naacl-main.75,N16-3020,1,0.604182,"methods provide an appealing mechanism to identify sources that led to specific predictions (which may reveal potentially problematic training examples), they have not yet been widely adopted, at least in part because even approximating influence functions (Koh and Liang, 2017)—arguably the most principled attribution method—can be prohibitively expensive in terms of compute. Is such complexity neces1 Introduction sary to identify ‘important’ training points? Or Interpretability methods are intended to help users do simpler methods (e.g., attribution scores based understand model predictions (Ribeiro et al., 2016; on similarity measures between train and test inLundberg and Lee, 2017; Sundararajan et al., 2017; stances) yield comparable results? In this paper, Gilpin et al., 2018). In machine learning broadly we set out to evaluate and compare instance attriand NLP specifically, such methods have focused bution methods, including relatively simple and on feature-based explanations that highlight parts efficient approaches (Rajani et al., 2020) in the conof inputs ‘responsible for’ the specific prediction. text of NLP (Figure 1). We design qualitative evalFeature attribution, however, does not communi-"
2021.naacl-main.75,D13-1170,0,0.00310653,"ution meth∗ Equal contribution ods (assessing the quality of more efficient approx967 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 967–975 June 6–11, 2021. ©2021 Association for Computational Linguistics imations)? (2) What is the quality of explanations in similarity methods compared to gradient-based ones (clarifying the necessity of adopting more complex methods)? We evaluate instance-based attribution methods on two datasets: binarized version of the Stanford Sentiment Treebank (SST-2; Socher et al. 2013) and the Multi-Genre NLI (MNLI) dataset (Williams et al., 2018). We investigate the correlation of more complex attribution methods with simpler approximations and variants (with and without use of the Hessian). Comparing explanation quality of gradient-based methods against simple similarity retrieval using leave-one-out (Basu et al., 2020) and randomized-test (Hanawa et al., 2021) analyses, we show that simpler methods are fairly competitive. Finally, using the HANS dataset (McCoy et al., 2019), we show the ability of similarity-based methods to surface artifacts in training data. 2 Attribut"
2021.naacl-main.75,N18-1101,0,0.0146252,"more efficient approx967 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 967–975 June 6–11, 2021. ©2021 Association for Computational Linguistics imations)? (2) What is the quality of explanations in similarity methods compared to gradient-based ones (clarifying the necessity of adopting more complex methods)? We evaluate instance-based attribution methods on two datasets: binarized version of the Stanford Sentiment Treebank (SST-2; Socher et al. 2013) and the Multi-Genre NLI (MNLI) dataset (Williams et al., 2018). We investigate the correlation of more complex attribution methods with simpler approximations and variants (with and without use of the Hessian). Comparing explanation quality of gradient-based methods against simple similarity retrieval using leave-one-out (Basu et al., 2020) and randomized-test (Hanawa et al., 2021) analyses, we show that simpler methods are fairly competitive. Finally, using the HANS dataset (McCoy et al., 2019), we show the ability of similarity-based methods to surface artifacts in training data. 2 Attribution Methods Similarity Based Attribution Consider a text classi"
D13-1182,W04-2328,0,0.0320294,"dstein et al., 2006). Cohen et al. (2004) demonstrated that good performance can be achieved for this task via existing text classification technologies. Elsewhere, researchers have explored automatically inferring “speech acts” in various other online social mediums, including message board posts (Qadir and Riloff, 2011), Wikipedia talk pages (Ferschke et al., 2012) and Twitter (Zhang et al., 2012). A separate line of inquiry concerns classifying dialogue acts in chat. Researchers have attempted dialogue act classification both for 1-on-1 (Kim et al., 2010) and multi-party (Kim et al., 2012; Clark and Popescu-Belis, 2004) online chats. Ang et al. (2005) considered the task of jointly segmenting and classifying utterances comprising multiparty meetings, while Hsueh and Moore (2006) proposed analogous methods for topic segmentation and labeling (other works on topic segmentation include (Galley et al., 2003) and (Eisenstein and Barzilay, 2008)). Incorporating such segmentation methods into the proposed model (rather than relying on inputs to be manually segmented beforehand) would be a natural extension of this work. Additive component models of text have recently gained traction (Eisenstein et al., 2011; Paul,"
D13-1182,W04-3240,0,0.0354424,"lly, (Cretchley et al., 2010) leveraged concept maps to explore conversations between people with schizophrenia and their carers. Briefly, this approach allowed them to (qualitatively) identify two distinct conversational strategies used by care-takers and their patients. Angus et al. (Angus et al., 2012) presented a similar approach in which they used text visualization software to explore patterns of (inferred) topics in consultations. Another thread of research has investigated classifying speech acts in emails into one of a small set of “email speech acts”, e.g., request, propose, commit (Cohen et al., 2004; Goldstein et al., 2006). Cohen et al. (2004) demonstrated that good performance can be achieved for this task via existing text classification technologies. Elsewhere, researchers have explored automatically inferring “speech acts” in various other online social mediums, including message board posts (Qadir and Riloff, 2011), Wikipedia talk pages (Ferschke et al., 2012) and Twitter (Zhang et al., 2012). A separate line of inquiry concerns classifying dialogue acts in chat. Researchers have attempted dialogue act classification both for 1-on-1 (Kim et al., 2010) and multi-party (Kim et al., 2"
D13-1182,D08-1035,0,0.387184,"ffer substantially. The model we develop in this work assumes that transcripts have been manually segmented. While this comes at some cost, segmenting is still much cheaper than annotating transcripts. Manually annotating a single visit with GMIAS codes takes 24 hours and must be performed by someone with substantive domain expertise. By contrast, segmenting transcripts into utterances takes at most 1/4th of the time as annotation and can be done by a less highly-skilled individual. That said, in future work we hope to explore incorporating automatic segmentation methods (Galley et al., 2003; Eisenstein and Barzilay, 2008) into our approach. Each utterance is assigned a single topic code and a single speech act code. Inter-rater agreement has been observed to be relatively high for this task: Kappa between three trained annotators and a reference annotation ranged from 0.89 to 1.0 for topics and 0.81 to 0.95 for speech acts. We next deCount (prevalence) 2939 (0.013) 245 (0.001) 328 (0.001) 4298 (0.018) 1650 (0.007) 111 (0.000) 12796 (0.055) 46 (0.000) 977 (0.004) 15 (0.000) 13753 (0.059) 1049 (0.005) 1005 (0.004) 17611 (0.076) 4617 (0.020) 423 (0.002) 54231 (0.233) 255 (0.001) 4426 (0.019) 119 (0.001) 5517 (0.0"
D13-1182,E12-1079,0,0.018561,"ion software to explore patterns of (inferred) topics in consultations. Another thread of research has investigated classifying speech acts in emails into one of a small set of “email speech acts”, e.g., request, propose, commit (Cohen et al., 2004; Goldstein et al., 2006). Cohen et al. (2004) demonstrated that good performance can be achieved for this task via existing text classification technologies. Elsewhere, researchers have explored automatically inferring “speech acts” in various other online social mediums, including message board posts (Qadir and Riloff, 2011), Wikipedia talk pages (Ferschke et al., 2012) and Twitter (Zhang et al., 2012). A separate line of inquiry concerns classifying dialogue acts in chat. Researchers have attempted dialogue act classification both for 1-on-1 (Kim et al., 2010) and multi-party (Kim et al., 2012; Clark and Popescu-Belis, 2004) online chats. Ang et al. (2005) considered the task of jointly segmenting and classifying utterances comprising multiparty meetings, while Hsueh and Moore (2006) proposed analogous methods for topic segmentation and labeling (other works on topic segmentation include (Galley et al., 2003) and (Eisenstein and Barzilay, 2008)). Incorporat"
D13-1182,P03-1071,0,0.294606,"are not likely to differ substantially. The model we develop in this work assumes that transcripts have been manually segmented. While this comes at some cost, segmenting is still much cheaper than annotating transcripts. Manually annotating a single visit with GMIAS codes takes 24 hours and must be performed by someone with substantive domain expertise. By contrast, segmenting transcripts into utterances takes at most 1/4th of the time as annotation and can be done by a less highly-skilled individual. That said, in future work we hope to explore incorporating automatic segmentation methods (Galley et al., 2003; Eisenstein and Barzilay, 2008) into our approach. Each utterance is assigned a single topic code and a single speech act code. Inter-rater agreement has been observed to be relatively high for this task: Kappa between three trained annotators and a reference annotation ranged from 0.89 to 1.0 for topics and 0.81 to 0.95 for speech acts. We next deCount (prevalence) 2939 (0.013) 245 (0.001) 328 (0.001) 4298 (0.018) 1650 (0.007) 111 (0.000) 12796 (0.055) 46 (0.000) 977 (0.004) 15 (0.000) 13753 (0.059) 1049 (0.005) 1005 (0.004) 17611 (0.076) 4617 (0.020) 423 (0.002) 54231 (0.233) 255 (0.001) 44"
D13-1182,D10-1084,0,0.262144,"onv. Mgmt. Biomedical Ask Q. Biomedical Ask Q. Biomedical Conv. Mgmt. Biomedical Give Info. Table 1: An excerpt from a patient-doctor interaction, annotated with topic and speech act codes. The D and P roles denote doctor and patient, respectively. Conv. Mgmt. abbreviates conversation management; Ask Q. abbreviates ask question. to the same topic but is a question. Both aspects are necessary to understand conversation. Previous computational work on speech acts – which we review in Section 6 – has modeled them in isolation (Perrault and Allen, 1980; Stolcke et al., 1998; Stolcke et al., 2000; Kim et al., 2010), i.e., independent of topical content. But a richer model would account for both speech acts and the contextualizing topic of each utterance. To this end, we develop a novel joint, generative model of topics and speech acts. We focus on physician-patient communication as a motivating domain. This is of interest because 1765 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1765–1775, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics it is widely appreciated that effective communication is an integral part"
D13-1182,Y12-1050,0,0.0144149,"et al., 2004; Goldstein et al., 2006). Cohen et al. (2004) demonstrated that good performance can be achieved for this task via existing text classification technologies. Elsewhere, researchers have explored automatically inferring “speech acts” in various other online social mediums, including message board posts (Qadir and Riloff, 2011), Wikipedia talk pages (Ferschke et al., 2012) and Twitter (Zhang et al., 2012). A separate line of inquiry concerns classifying dialogue acts in chat. Researchers have attempted dialogue act classification both for 1-on-1 (Kim et al., 2010) and multi-party (Kim et al., 2012; Clark and Popescu-Belis, 2004) online chats. Ang et al. (2005) considered the task of jointly segmenting and classifying utterances comprising multiparty meetings, while Hsueh and Moore (2006) proposed analogous methods for topic segmentation and labeling (other works on topic segmentation include (Galley et al., 2003) and (Eisenstein and Barzilay, 2008)). Incorporating such segmentation methods into the proposed model (rather than relying on inputs to be manually segmented beforehand) would be a natural extension of this work. Additive component models of text have recently gained traction"
D13-1182,D12-1009,0,0.0977969,"2004) online chats. Ang et al. (2005) considered the task of jointly segmenting and classifying utterances comprising multiparty meetings, while Hsueh and Moore (2006) proposed analogous methods for topic segmentation and labeling (other works on topic segmentation include (Galley et al., 2003) and (Eisenstein and Barzilay, 2008)). Incorporating such segmentation methods into the proposed model (rather than relying on inputs to be manually segmented beforehand) would be a natural extension of this work. Additive component models of text have recently gained traction (Eisenstein et al., 2011; Paul, 2012; Paul and Dredze, 2012; Paul et al., 2013). To our knowledge, this is the first extension of supervised additive component models to a sequential task.6 7 Conclusions and Future Directions We have proposed a novel Joint, Additive, Sequential (JAS) model of conversational topics and speech acts. In contrast to previous approaches to modeling conversational exchanges, this model factors both the current topic and the current speech act into token emission and state transition probabilities. We demonstrated that this model consistently outperforms a univariate generative baseline that treats spe"
D13-1182,D11-1069,0,0.0225729,"lar approach in which they used text visualization software to explore patterns of (inferred) topics in consultations. Another thread of research has investigated classifying speech acts in emails into one of a small set of “email speech acts”, e.g., request, propose, commit (Cohen et al., 2004; Goldstein et al., 2006). Cohen et al. (2004) demonstrated that good performance can be achieved for this task via existing text classification technologies. Elsewhere, researchers have explored automatically inferring “speech acts” in various other online social mediums, including message board posts (Qadir and Riloff, 2011), Wikipedia talk pages (Ferschke et al., 2012) and Twitter (Zhang et al., 2012). A separate line of inquiry concerns classifying dialogue acts in chat. Researchers have attempted dialogue act classification both for 1-on-1 (Kim et al., 2010) and multi-party (Kim et al., 2012; Clark and Popescu-Belis, 2004) online chats. Ang et al. (2005) considered the task of jointly segmenting and classifying utterances comprising multiparty meetings, while Hsueh and Moore (2006) proposed analogous methods for topic segmentation and labeling (other works on topic segmentation include (Galley et al., 2003) an"
D13-1182,J00-3003,0,0.534962,"Missing"
D13-1182,W12-0603,0,0.0118348,"(inferred) topics in consultations. Another thread of research has investigated classifying speech acts in emails into one of a small set of “email speech acts”, e.g., request, propose, commit (Cohen et al., 2004; Goldstein et al., 2006). Cohen et al. (2004) demonstrated that good performance can be achieved for this task via existing text classification technologies. Elsewhere, researchers have explored automatically inferring “speech acts” in various other online social mediums, including message board posts (Qadir and Riloff, 2011), Wikipedia talk pages (Ferschke et al., 2012) and Twitter (Zhang et al., 2012). A separate line of inquiry concerns classifying dialogue acts in chat. Researchers have attempted dialogue act classification both for 1-on-1 (Kim et al., 2010) and multi-party (Kim et al., 2012; Clark and Popescu-Belis, 2004) online chats. Ang et al. (2005) considered the task of jointly segmenting and classifying utterances comprising multiparty meetings, while Hsueh and Moore (2006) proposed analogous methods for topic segmentation and labeling (other works on topic segmentation include (Galley et al., 2003) and (Eisenstein and Barzilay, 2008)). Incorporating such segmentation methods int"
D13-1182,J80-3003,0,\N,Missing
D16-1076,D14-1181,0,0.464046,"(Section 2) has introduced such methods, but these have relied on linear models such as Support Vector Machines (SVMs) (Joachims, 1998), operating over sparse representations of text. We propose a novel CNN model for text classification that exploits both document labels and associated rationales. Introduction Neural models that exploit word embeddings have recently achieved impressive results on text classification tasks (Goldberg, 2015). Feed-forward Convolutional Neural Networks (CNNs), in particular, have emerged as a relatively simple yet powerful class of models for text classification (Kim, 2014). These neural text classification models have tended to assume a standard supervised learning setting in which instance labels are provided. Here we consider an alternative scenario in which we assume Specific contributions of this work as follows. (1) This is the first work to incorporate rationales into neural models for text classification. (2) Empirically, we show that the proposed model uniformly outperforms relevant baseline approaches across five datasets, including previously proposed models that capitalize on rationales (Zaidan et al., 2007; Marshall et al., 2016) and multiple baseli"
D16-1076,C02-1103,0,0.0540698,"these. And Yang et al. (2016) proposed a hierarchical network with two levels of attention mechanisms for document classification. We discuss this model specifically as well as attention more generally and its relationship to our proposed approach in Section 4.3. 2.2 2 3 Preliminaries: CNNs for text classification Softmax layer Sentence feature vector o 1 max pooling Four feature maps Convolution layer Exploiting rationales In long documents the importance of sentences varies; some are more central than others. Prior work has investigated methods to measure the relative importance sentences (Ko et al., 2002; Murata et al., 2000). In this work we adopt a particular view of sentence importance in the context of document classification. In particular, we assume that documents comprise sentences that directly support their categorization. We call such sentences rationales. The notion of rationales was first introduced by Zaidan et al. (2007). To harness these for classification, they proposed modifying the Support Vector Machine (SVM) objective function to encode a preference for parameter values that result in instances containing manually annotated rationales 1 being more confidently classified th"
D16-1076,P04-1035,0,0.0115114,"icipants and Personnel (BPP): were all trial participants and individuals involved in running the trial blinded as to who was receiving which treatment? (4) Blinding of outcome assessment (BOA): were the parties who measured the outcome(s) of interest blinded to the intervention group assignments? These assessments are somewhat subjective. To increase transparency, researchers performing RoB assessment therefore record rationales (sentences from articles) supporting their assessments. 5.2 Movie Review Dataset We also ran experiments on a movie review (MR) dataset with accompanying rationales. Pang and Lee (2004) developed and published the original version of this dataset, which comprises 1000 positive and 1000 negative movie reviews from the Internet Movie Database (IMDB).5 Zaidan et al. (2007) then 5 RSG AC BPP BOA MR http://www.imdb.com/ 800 N 8399 11512 7997 2706 1800 #sen 300 297 296 309 32.6 #token 9.92 9.87 9.95 9.92 21.2 #rat 0.31 0.15 0.21 0.2 8.0 Table 1: Dataset characteristics. N is the number of instances, #sen is the average sentence count, #token is the average token per-sentence count and #rat is the average number of rationales per document. augmented this dataset by adding rationale"
D16-1076,D11-1136,0,0.242843,"at Austin 2 Department of Primary Care and Public Health Sciences, Kings College London 3 College of Computer and Information Science, Northeastern University yezhang@cs.utexas.edu, iain.marshall@kcl.ac.uk byron@ccs.neu.edu 1 Abstract that we are provided a set of rationales (Zaidan et al., 2007; Zaidan and Eisner, 2008; McDonnell et al., 2016) in addition to instance labels, i.e., sentences or snippets that support the corresponding document categorizations. Providing such rationales during manual classification is a natural interaction for annotators, and requires little additional effort (Settles, 2011; McDonnell et al., 2016). Therefore, when training new classification systems, it is natural to acquire supervision at both the document and sentence level, with the aim of inducing a better predictive model, potentially with less effort. We present a new Convolutional Neural Network (CNN) model for text classification that jointly exploits labels on documents and their constituent sentences. Specifically, we consider scenarios in which annotators explicitly mark sentences (or snippets) that support their overall document categorization, i.e., they provide rationales. Our model exploits such"
D16-1076,D15-1167,0,0.232207,"we describe below and then build upon in this work. Properties of this model were explored empirically in (Zhang and Wallace, 2015). We also note that Zhang et al. (2016) extended this model to jointly accommodate multiple sets of pre-trained word embeddings. Roughly concurrently to Kim, Johnson and Zhang (2014) proposed a similar CNN architecture, although they swapped in one-hot vectors in place of (pre-trained) word embeddings. They later developed a semi-supervised variant of this approach (Johnson and Zhang, 2015). In related recent work on Recurrent Neural Network (RNN) models for text, Tang et al. (2015) proposed using a Long Short Term Memory (LSTM) layer to represent each sentence and then passing another RNN variant over these. And Yang et al. (2016) proposed a hierarchical network with two levels of attention mechanisms for document classification. We discuss this model specifically as well as attention more generally and its relationship to our proposed approach in Section 4.3. 2.2 2 3 Preliminaries: CNNs for text classification Softmax layer Sentence feature vector o 1 max pooling Four feature maps Convolution layer Exploiting rationales In long documents the importance of sentences var"
D16-1076,N16-1174,0,0.787029,"hang et al. (2016) extended this model to jointly accommodate multiple sets of pre-trained word embeddings. Roughly concurrently to Kim, Johnson and Zhang (2014) proposed a similar CNN architecture, although they swapped in one-hot vectors in place of (pre-trained) word embeddings. They later developed a semi-supervised variant of this approach (Johnson and Zhang, 2015). In related recent work on Recurrent Neural Network (RNN) models for text, Tang et al. (2015) proposed using a Long Short Term Memory (LSTM) layer to represent each sentence and then passing another RNN variant over these. And Yang et al. (2016) proposed a hierarchical network with two levels of attention mechanisms for document classification. We discuss this model specifically as well as attention more generally and its relationship to our proposed approach in Section 4.3. 2.2 2 3 Preliminaries: CNNs for text classification Softmax layer Sentence feature vector o 1 max pooling Four feature maps Convolution layer Exploiting rationales In long documents the importance of sentences varies; some are more central than others. Prior work has investigated methods to measure the relative importance sentences (Ko et al., 2002; Murata et al."
D16-1076,P10-2062,0,0.0412439,"Missing"
D16-1076,D08-1004,0,0.145022,"Missing"
D16-1076,N07-1033,0,0.262592,"et powerful class of models for text classification (Kim, 2014). These neural text classification models have tended to assume a standard supervised learning setting in which instance labels are provided. Here we consider an alternative scenario in which we assume Specific contributions of this work as follows. (1) This is the first work to incorporate rationales into neural models for text classification. (2) Empirically, we show that the proposed model uniformly outperforms relevant baseline approaches across five datasets, including previously proposed models that capitalize on rationales (Zaidan et al., 2007; Marshall et al., 2016) and multiple baseline CNN variants, including a CNN equipped with an attention mechanism. We also report state-of-the-art results on the important task of automatically assessing the risks of bias in the studies described in full-text biomedical articles (Marshall et al., 2016). (3) Our model naturally provides explanations for its predic795 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 795–804, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics tions, providing interpretability. We have m"
D16-1076,N16-1178,1,0.894118,"Missing"
D18-1308,W17-2614,0,0.0249626,"BIOASQ2 challenge, in particular, concerns MeSH annotation, and competitive systems have emerged from this in past years (Liu et al., 2014; Tsoumakas et al., 2013); these constitute baseline approaches in the present work. 1 y1 This problem also resembles tagging clinical notes with ICD codes (Mullenbach et al., 2018). 2 http://bioasq.org/ More generally, MeSH annotation is a specific instance of multi-label classification, which has received substantial attention in general (Elisseeff and Weston, 2002; F¨urnkranz et al., 2008; Read et al., 2011; Bhatia et al., 2015; Daum´e III et al., 2017; Chen et al., 2017; Jernite et al., 2016). Our work differs from these prior efforts in that MeSH tagging involves structured multi-label classification: the label space is a tree3 in which nodes represent nested semantic concepts, and the specificity of these increases with depth. Past efforts in multi-label classification have considered hierarchical and tree-based approaches for tagging (Jernite et al., 2016; Beygelzimer et al., 2009; Daum´e III et al., 2017), but these have not assumed a given structured label space; instead, these efforts have attempted to induce trees to improve inference efficiency. By c"
D18-1308,D14-1179,0,0.038065,"Missing"
D18-1308,W14-4012,0,0.0808077,"Missing"
D18-1308,N18-1100,0,0.0502647,"e, thus facilitating improved search and retrieval.1 At present, MeSH annotation is largely performed manually by highly skilled annotators employed by the National Library of Medicine (NLM). Automating this annotation task is thus highly desirable, and there have been considerable efforts to do so. The BIOASQ2 challenge, in particular, concerns MeSH annotation, and competitive systems have emerged from this in past years (Liu et al., 2014; Tsoumakas et al., 2013); these constitute baseline approaches in the present work. 1 y1 This problem also resembles tagging clinical notes with ICD codes (Mullenbach et al., 2018). 2 http://bioasq.org/ More generally, MeSH annotation is a specific instance of multi-label classification, which has received substantial attention in general (Elisseeff and Weston, 2002; F¨urnkranz et al., 2008; Read et al., 2011; Bhatia et al., 2015; Daum´e III et al., 2017; Chen et al., 2017; Jernite et al., 2016). Our work differs from these prior efforts in that MeSH tagging involves structured multi-label classification: the label space is a tree3 in which nodes represent nested semantic concepts, and the specificity of these increases with depth. Past efforts in multi-label classifica"
D18-1497,N10-1122,0,0.0436391,"ce relevance w.r.t. individual aspects, which were derived from terms a priori associated with aspects. This supervision is used to construct a composite loss that captures both classification performance on the source task and a term that enforces invariance between source and target representations. There is also a large body of work that uses probabilistic generative models to recover latent structure in texts. Many of these models derive from Latent Dirichlet Allocation (LDA) (Blei et al., 2003), and some variants have explicitly represented topics and aspects jointly for sentiment tasks (Brody and Elhadad, 2010; Sauper et al., 2010, 2011; Mukherjee and Liu, 2012; Sauper and Barzilay, 2013; Kim et al., 2013). A bit more generally, aspects have also been interpreted as properties spanning entire texts, e.g., a perspective or theme which may then color the discussion of topics (Paul and Girju, 2010). This intuition led to the development of the factorial LDA family of topic models (Paul and Dredze, 2012; Wallace et al., 2014); these model individ4690 Look : deep amber hue , this brew is topped with a finger of off white head . smell of dog unk , green unk , and slightly fruity . taste of belgian yeast"
D18-1497,P16-1036,0,0.0151747,"based judgments have been used in multiple domains, including vision and NLP, to estimate similarity information implicitly. For example, triplet-based similarity embeddings may be learned using ‘crowdkernels’ with applications to multi-view clustering (Amid and Ukkonen, 2015). Models combining similarity with neural networks mainly revolve around Siamese networks (Chopra et al., 2005) which use pairwise distances to learn embeddings (Schroff et al., 2015), a tactic we have followed here. Similarity judgments have also been used to generate document embeddings for IR tasks (Shen et al., 2014; Das et al., 2016). Recently, He et al. (2017) introduced a neural model for aspect extraction that relies on an attention mechanism to identify aspect words. They proposed an autoencoder variant designed to tease apart aspects. In contrast to the method we propose, their approach is unsupervised; discovered aspects may thus not have a clear interpretation. Experiments reported here support this hypothesis, and we provide additional results using their model in the Appendix. Other recent work has focused on text generation from factorized representations (Larsson et al., 2017). And Zhang et al. (2017) proposed"
D18-1497,N15-1184,0,0.0265115,"et using different representations. Baselines perform reasonably well on the domain aspect because reviews from different domains are quite dissimilar. Capturing sentiment information irrespective of domain is more difficult, and most unsupervised models fail in this respect. In Table 11, we observe that cross AUC results are much more pronounced than for the BeerAdvocate data, as the domain and sentiment are uncorrelated (i.e., sentiment is independent of domain). 5 Related Work Work in representation learning for NLP has largely focused on improving word embeddings (Levy and Goldberg, 2014; Faruqui et al., 2015; Huang et al., 2012). But efforts have also been made to embed other textual units, e.g. characters (Kim et al., 2016), and lengthier texts including sentences, paragraphs, and documents (Le and Mikolov, 2014; Kiros et al., 2015). Triplet-based judgments have been used in multiple domains, including vision and NLP, to estimate similarity information implicitly. For example, triplet-based similarity embeddings may be learned using ‘crowdkernels’ with applications to multi-view clustering (Amid and Ukkonen, 2015). Models combining similarity with neural networks mainly revolve around Siamese ne"
D18-1497,P17-1036,0,0.369319,"etween document embeddings of user reviews, we cannot know if this similarity primarily reflects user sentiment, the product discussed, or syntactic patterns. This lack of interpretability makes it difficult to assess whether a learned representations is likely to generalize to a new task or domain, hindering model transferability. Disentangled representations with known semantics could allow more efficient training in settings in which supervision is expensive to obtain (e.g., biomedical NLP). Thus far in NLP, learned distributed representations have, with few exceptions (Ruder et al., 2016; He et al., 2017; Zhang et al., 2017), been entangled: they indiscriminately encode all aspects of texts. Rather than representing text via a monolithic vector, we propose to estimate multiple embeddings that capture complementary aspects of texts, drawing inspiration from the ML in vision community (Whitney, 2016; Veit et al., 2017a). As a motivating example we consider documents that describe clinical trials. Such publications constitute the evidence drawn upon to support evidence-based medicine (EBM), in which one formulates precise clinical questions with respect to the Populations, Interventions, Compara"
D18-1497,P12-1092,0,0.0568656,"resentations. Baselines perform reasonably well on the domain aspect because reviews from different domains are quite dissimilar. Capturing sentiment information irrespective of domain is more difficult, and most unsupervised models fail in this respect. In Table 11, we observe that cross AUC results are much more pronounced than for the BeerAdvocate data, as the domain and sentiment are uncorrelated (i.e., sentiment is independent of domain). 5 Related Work Work in representation learning for NLP has largely focused on improving word embeddings (Levy and Goldberg, 2014; Faruqui et al., 2015; Huang et al., 2012). But efforts have also been made to embed other textual units, e.g. characters (Kim et al., 2016), and lengthier texts including sentences, paragraphs, and documents (Le and Mikolov, 2014; Kiros et al., 2015). Triplet-based judgments have been used in multiple domains, including vision and NLP, to estimate similarity information implicitly. For example, triplet-based similarity embeddings may be learned using ‘crowdkernels’ with applications to multi-view clustering (Amid and Ukkonen, 2015). Models combining similarity with neural networks mainly revolve around Siamese networks (Chopra et al."
D18-1497,P14-2050,0,0.0419266,"each aspect on our test set using different representations. Baselines perform reasonably well on the domain aspect because reviews from different domains are quite dissimilar. Capturing sentiment information irrespective of domain is more difficult, and most unsupervised models fail in this respect. In Table 11, we observe that cross AUC results are much more pronounced than for the BeerAdvocate data, as the domain and sentiment are uncorrelated (i.e., sentiment is independent of domain). 5 Related Work Work in representation learning for NLP has largely focused on improving word embeddings (Levy and Goldberg, 2014; Faruqui et al., 2015; Huang et al., 2012). But efforts have also been made to embed other textual units, e.g. characters (Kim et al., 2016), and lengthier texts including sentences, paragraphs, and documents (Le and Mikolov, 2014; Kiros et al., 2015). Triplet-based judgments have been used in multiple domains, including vision and NLP, to estimate similarity information implicitly. For example, triplet-based similarity embeddings may be learned using ‘crowdkernels’ with applications to multi-view clustering (Amid and Ukkonen, 2015). Models combining similarity with neural networks mainly rev"
D18-1497,P12-1036,0,0.0295205,"erived from terms a priori associated with aspects. This supervision is used to construct a composite loss that captures both classification performance on the source task and a term that enforces invariance between source and target representations. There is also a large body of work that uses probabilistic generative models to recover latent structure in texts. Many of these models derive from Latent Dirichlet Allocation (LDA) (Blei et al., 2003), and some variants have explicitly represented topics and aspects jointly for sentiment tasks (Brody and Elhadad, 2010; Sauper et al., 2010, 2011; Mukherjee and Liu, 2012; Sauper and Barzilay, 2013; Kim et al., 2013). A bit more generally, aspects have also been interpreted as properties spanning entire texts, e.g., a perspective or theme which may then color the discussion of topics (Paul and Girju, 2010). This intuition led to the development of the factorial LDA family of topic models (Paul and Dredze, 2012; Wallace et al., 2014); these model individ4690 Look : deep amber hue , this brew is topped with a finger of off white head . smell of dog unk , green unk , and slightly fruity . taste of belgian yeast , coriander , hard water and bready malt . light bod"
D18-1497,D16-1103,0,0.0182366,"ulate similarities between document embeddings of user reviews, we cannot know if this similarity primarily reflects user sentiment, the product discussed, or syntactic patterns. This lack of interpretability makes it difficult to assess whether a learned representations is likely to generalize to a new task or domain, hindering model transferability. Disentangled representations with known semantics could allow more efficient training in settings in which supervision is expensive to obtain (e.g., biomedical NLP). Thus far in NLP, learned distributed representations have, with few exceptions (Ruder et al., 2016; He et al., 2017; Zhang et al., 2017), been entangled: they indiscriminately encode all aspects of texts. Rather than representing text via a monolithic vector, we propose to estimate multiple embeddings that capture complementary aspects of texts, drawing inspiration from the ML in vision community (Whitney, 2016; Veit et al., 2017a). As a motivating example we consider documents that describe clinical trials. Such publications constitute the evidence drawn upon to support evidence-based medicine (EBM), in which one formulates precise clinical questions with respect to the Populations, Inter"
D18-1497,D10-1037,0,0.0269658,"idual aspects, which were derived from terms a priori associated with aspects. This supervision is used to construct a composite loss that captures both classification performance on the source task and a term that enforces invariance between source and target representations. There is also a large body of work that uses probabilistic generative models to recover latent structure in texts. Many of these models derive from Latent Dirichlet Allocation (LDA) (Blei et al., 2003), and some variants have explicitly represented topics and aspects jointly for sentiment tasks (Brody and Elhadad, 2010; Sauper et al., 2010, 2011; Mukherjee and Liu, 2012; Sauper and Barzilay, 2013; Kim et al., 2013). A bit more generally, aspects have also been interpreted as properties spanning entire texts, e.g., a perspective or theme which may then color the discussion of topics (Paul and Girju, 2010). This intuition led to the development of the factorial LDA family of topic models (Paul and Dredze, 2012; Wallace et al., 2014); these model individ4690 Look : deep amber hue , this brew is topped with a finger of off white head . smell of dog unk , green unk , and slightly fruity . taste of belgian yeast , coriander , hard wa"
D18-1497,P11-1036,0,0.0622386,"Missing"
D18-1497,Q17-1036,0,0.126875,"mbeddings of user reviews, we cannot know if this similarity primarily reflects user sentiment, the product discussed, or syntactic patterns. This lack of interpretability makes it difficult to assess whether a learned representations is likely to generalize to a new task or domain, hindering model transferability. Disentangled representations with known semantics could allow more efficient training in settings in which supervision is expensive to obtain (e.g., biomedical NLP). Thus far in NLP, learned distributed representations have, with few exceptions (Ruder et al., 2016; He et al., 2017; Zhang et al., 2017), been entangled: they indiscriminately encode all aspects of texts. Rather than representing text via a monolithic vector, we propose to estimate multiple embeddings that capture complementary aspects of texts, drawing inspiration from the ML in vision community (Whitney, 2016; Veit et al., 2017a). As a motivating example we consider documents that describe clinical trials. Such publications constitute the evidence drawn upon to support evidence-based medicine (EBM), in which one formulates precise clinical questions with respect to the Populations, Interventions, Comparators and Outcomes (PI"
D19-1003,C18-1161,0,0.0255452,"ve the opportunity to explore and compare alternative AL strategies. Moreover, AL couples the training dataset with the model used to guide its acquisition. We find that subsequently training a successor model with an actively-acquired dataset does not consistently outperform training on i.i.d. sampled data. Our findings raise the question of whether the downsides inherent to AL are worth the modest and inconsistent performance gains it tends to afford. 1 Introduction Although deep learning now achieves state-ofthe-art results on a number of supervised learning tasks (Johnson and Zhang, 2016; Ghaddar and Langlais, 2018), realizing these gains requires large annotated datasets (Shen et al., 2018). This data dependence is problematic because labels are expensive. Several lines of research seek to reduce 1 This may be done either deterministically, by selecting the top-k instances, or stochastically, selecting instances with probabilities proportional to heuristic scores. 21 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 21–30, c Hong Kong, China, November 3–7, 2019. 2019 Association for Comp"
D19-1003,D14-1162,0,0.0825061,"Missing"
D19-1003,D14-1181,0,0.0253407,"Missing"
D19-1003,D18-1318,1,0.785106,"Missing"
D19-1003,C02-1150,0,0.235268,"m movie reviews. The task is to classify sentences as expressing positive or negative sentiment (Pang and Lee, 2005). • Subjectivity: This dataset consists of statements labeled as either objective or subjective (Pang and Lee, 2004). Tasks We now briefly describe the models, datasets, acquisition functions, and implementation details for the experiments we conduct with active learners for text classification (4.1) and NER (4.2). 4.1 • TREC: This task entails categorizing questions into 1 of 6 categories based on the subject of the question (e.g., questions about people, locations, and so on) (Li and Roth, 2002). The TREC dataset defines standard train/test splits, but we generate our own for consistency in train/validation/test proportions across corpora. Text Classification Models We consider three standard models for text classification: Support Vector Machines (SVMs), Convolutional Neural Networks (CNNs) (Kim, 2014; Zhang and Wallace, 2015), and Bidirectional Long Short-Term Memory (BiLSTM) networks (Hochreiter and Schmidhuber, 1997). For SVM, we represent texts via sparse, TF-IDF bag-of-words (BoW) vectors. For neural models (CNN and BiLSTM), we represent each document as a sequence of word embe"
D19-1003,P04-1035,0,0.0476086,"uisition model A? How does this compare to training S on natively acquired data? How does it compare to training S on i.i.d. data? For example, if we use uncertainty sampling under a support vector machine (SVM) to acquire a training set D, and subsequently train a Convolutional Neural Network (CNN) using D, will the CNN perform better than it would have if trained on a dataset acquired via i.i.d. random sampling? And how does it perform compared to using a training corpus actively acquired using the CNN? Figure 1b shows results for a text classification example using the Subjectivity corpus (Pang and Lee, 2004). We consider three models: a Bidirectional Long Short-Term Memory Network (BiLSTM) (Hochreiter and Schmidhuber, 1997), a Convolutional Neural Network (CNN) (Kim, 2014; Zhang and Wallace, 2015), and a Support Vector Machine (SVM) (Joachims, 1998). Training the LSTM with a dataset actively acquired using either of the other models yields predictive performance that is worse than that achieved under i.i.d. sampling. Given that datasets tend to outlast models, these results raise questions regarding the benefits of using AL in practice. We note that in prior work, Tomanek and Morik (2011) also ex"
D19-1003,P05-1015,0,0.0195811,". These learning curves quantify the comparative performance of a particular model achieved using the same amount of supervision, but elicited under different acquisition models. For each model, we compare the learning curves of each acquisition strategy, including active acquisition using a foreign model and subsequent transfer, active acquisition without changing models (i.e., typical AL), and the baseline strategy of i.i.d. sampling. 4 • Movie Reviews: This corpus consists of sentences drawn from movie reviews. The task is to classify sentences as expressing positive or negative sentiment (Pang and Lee, 2005). • Subjectivity: This dataset consists of statements labeled as either objective or subjective (Pang and Lee, 2004). Tasks We now briefly describe the models, datasets, acquisition functions, and implementation details for the experiments we conduct with active learners for text classification (4.1) and NER (4.2). 4.1 • TREC: This task entails categorizing questions into 1 of 6 categories based on the subject of the question (e.g., questions about people, locations, and so on) (Li and Roth, 2002). The TREC dataset defines standard train/test splits, but we generate our own for consistency in"
I17-1026,C10-1039,0,0.0103052,"sentences. (3) SST-2: Derived from SST-1, but pared to only two classes. We again only train and test models on sentences, excluding phrases. (4) Subj: Subjectivity dataset (Pang and Lee, 2005). (5) TREC: Question classification dataset (Li and Roth, 2002). (6) CR: Customer review dataset (Hu and Liu, 2004). (7) MPQA: Opinion polarity dataset (Wiebe et al., 2005). Additionally, we use (8) Opi: Opinosis Dataset, which comprises sentences extracted from user reviews on a given topic, e.g. “sound quality of ipod nano”. There are 51 such topics and each topic contains approximately 100 sentences (Ganesan et al., 2010). (9) Irony (Wallace et al., 2014): this contains 16,006 sentences from reddit labeled as ironic (or not). The dataset is imbalanced (relatively few sentences are ironic). Thus before training, we under-sampled negative instances to make classes sizes equal.1 For this dataset we report the Area Under Curve (AUC), rather than accuracy, because it is imbalanced. Values Google word2vec (3,4,5) 100 ReLU 1-max pooling 0.5 3 Table 1: Baseline configuration. ‘feature maps’ refers to the number of feature maps for each filter region size. ‘ReLU’ refers to rectified linear unit (Maas et al., 2013), a c"
I17-1026,P15-1162,0,0.0265378,"Missing"
I17-1026,D14-1082,0,0.014347,"ge pooling region sizes {3, 10, 20, 30}. We found that average pooling uniformly performed (much) worse than max pooling, at least on the CR and TREC datasets. Our analysis of pooling strategies shows that 1max pooling consistently performs better than alternative strategies for the task of sentence classification. This may be because the location of predictive contexts does not matter, and certain n-grams in the sentence can be more predictive on their own than the entire sentence considered jointly. Sigmoid function (Maas et al., 2013), SoftPlus function (Dugas et al., 2001), Cube function (Chen and Manning, 2014), and tanh cube function (Pei et al., 2015). We use ‘Iden’ to denote the identity function, which means not using any activation function. We show the numerical results of tanh, Softplus, Iden and ReLU in table 6. For 8 out of 9 datasets, the best activation function is one of Iden, ReLU and tanh. The SoftPlus function outperform these on only one dataset (MPQA). Sigmoid, Cube, and tanh cube all consistently performed worse than alternative activation functions. The performance of the tanh function may be due to its zero centering property (compared to Sigmoid). ReLU has the merits of a non-sa"
I17-1026,P14-1062,0,0.811465,"sticated search methods still require knowing which hyperparameters are worth exploring to begin with (and reasonable ranges for each). In this work our aim is to identify empirically the settings that practitioners should expend effort tuning, and those that are either inconsequential with respect to performance or that seem to have a ‘best’ setting independent of the specific dataset, and provide a reasonable range for each hyperpaConvolutional Neural Networks (CNNs) have recently achieved remarkably strong performance on the practically important task of sentence classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2014; Zhang et al., 2016). However, these models require practitioners to specify an exact model architecture and set accompanying hyperparameters, including the filter region size, regularization parameters, and so on. It is currently unknown how sensitive model performance is to changes in these configurations for the task of sentence classification. We thus conduct a sensitivity analysis of one-layer CNNs to explore the effect of architecture components on model performance; our aim is to distinguish between important and comparatively inconsequential design decisions f"
I17-1026,D14-1181,0,0.0957231,"Missing"
I17-1026,C02-1150,0,0.0911926,"ed by Kim (2014). Briefly, these are summarized as follows. (1) MR: sentence polarity dataset from (Pang and Lee, 2005). (2) SST-1: Stanford Sentiment Treebank (Socher et al., 2013). To make input representations consistent across tasks, we only train and test on sentences, in contrast to the use in (Kim, 2014), wherein models were trained on both phrases and sentences. (3) SST-2: Derived from SST-1, but pared to only two classes. We again only train and test models on sentences, excluding phrases. (4) Subj: Subjectivity dataset (Pang and Lee, 2005). (5) TREC: Question classification dataset (Li and Roth, 2002). (6) CR: Customer review dataset (Hu and Liu, 2004). (7) MPQA: Opinion polarity dataset (Wiebe et al., 2005). Additionally, we use (8) Opi: Opinosis Dataset, which comprises sentences extracted from user reviews on a given topic, e.g. “sound quality of ipod nano”. There are 51 such topics and each topic contains approximately 100 sentences (Ganesan et al., 2010). (9) Irony (Wallace et al., 2014): this contains 16,006 sentences from reddit labeled as ironic (or not). The dataset is imbalanced (relatively few sentences are ironic). Thus before training, we under-sampled negative instances to ma"
I17-1026,D15-1251,0,0.019522,"nologies in practice will likely be attracted to simpler variants, which afford fast training and prediction times. Unfortunately, a downside to CNN-based models – even simple ones – is that they require practitioners to specify the exact model architecture to be used and to set the accompanying hyperparameters. In practice, tuning all of these hyperparameters is simply not feasible, especially because parameter estimation is computationally intensive. Emerging research has begun to explore hyperparameter optimization methods, including random search (Bengio, 2012), and Bayesian optimization (Yogatama and Smith, 2015; Bergstra et al., 2013). However, these sophisticated search methods still require knowing which hyperparameters are worth exploring to begin with (and reasonable ranges for each). In this work our aim is to identify empirically the settings that practitioners should expend effort tuning, and those that are either inconsequential with respect to performance or that seem to have a ‘best’ setting independent of the specific dataset, and provide a reasonable range for each hyperpaConvolutional Neural Networks (CNNs) have recently achieved remarkably strong performance on the practically importan"
I17-1026,P17-2024,1,0.858972,"Missing"
I17-1026,P05-1015,0,0.0609971,"e variance due strictly to the parameter estimation procedure. Most prior work, unfortunately, has not reported such variance, despite a highly stochastic learning procedure. This variance is attributable to estimation via SGD, random dropout, and random weight parameter initialization. Description input word vectors filter region size feature maps activation function pooling dropout rate l2 norm constraint Datasets We use nine sentence classification datasets in all; seven of which were also used by Kim (2014). Briefly, these are summarized as follows. (1) MR: sentence polarity dataset from (Pang and Lee, 2005). (2) SST-1: Stanford Sentiment Treebank (Socher et al., 2013). To make input representations consistent across tasks, we only train and test on sentences, in contrast to the use in (Kim, 2014), wherein models were trained on both phrases and sentences. (3) SST-2: Derived from SST-1, but pared to only two classes. We again only train and test models on sentences, excluding phrases. (4) Subj: Subjectivity dataset (Pang and Lee, 2005). (5) TREC: Question classification dataset (Li and Roth, 2002). (6) CR: Customer review dataset (Hu and Liu, 2004). (7) MPQA: Opinion polarity dataset (Wiebe et al"
I17-1026,N16-1178,1,0.892275,"hyperparameters are worth exploring to begin with (and reasonable ranges for each). In this work our aim is to identify empirically the settings that practitioners should expend effort tuning, and those that are either inconsequential with respect to performance or that seem to have a ‘best’ setting independent of the specific dataset, and provide a reasonable range for each hyperpaConvolutional Neural Networks (CNNs) have recently achieved remarkably strong performance on the practically important task of sentence classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2014; Zhang et al., 2016). However, these models require practitioners to specify an exact model architecture and set accompanying hyperparameters, including the filter region size, regularization parameters, and so on. It is currently unknown how sensitive model performance is to changes in these configurations for the task of sentence classification. We thus conduct a sensitivity analysis of one-layer CNNs to explore the effect of architecture components on model performance; our aim is to distinguish between important and comparatively inconsequential design decisions for sentence classification. We focus on one-la"
I17-1026,P15-1031,0,0.0117573,"that average pooling uniformly performed (much) worse than max pooling, at least on the CR and TREC datasets. Our analysis of pooling strategies shows that 1max pooling consistently performs better than alternative strategies for the task of sentence classification. This may be because the location of predictive contexts does not matter, and certain n-grams in the sentence can be more predictive on their own than the entire sentence considered jointly. Sigmoid function (Maas et al., 2013), SoftPlus function (Dugas et al., 2001), Cube function (Chen and Manning, 2014), and tanh cube function (Pei et al., 2015). We use ‘Iden’ to denote the identity function, which means not using any activation function. We show the numerical results of tanh, Softplus, Iden and ReLU in table 6. For 8 out of 9 datasets, the best activation function is one of Iden, ReLU and tanh. The SoftPlus function outperform these on only one dataset (MPQA). Sigmoid, Cube, and tanh cube all consistently performed worse than alternative activation functions. The performance of the tanh function may be due to its zero centering property (compared to Sigmoid). ReLU has the merits of a non-saturating form compared to Sigmoid, and it h"
I17-1026,D14-1162,0,0.0846493,"area reports only mean accuracies calculated via cross-validation. But there is substantial variance in the performance of CNNs, even on the same folds and with model configuration held constant. Therefore, in our experiments we perform replications of cross-validation and report accuracy/Area Under Curve (AUC) score means and ranges over these. 2 2.1 CNN Architecture We begin with a tokenized sentence which we then convert to a sentence matrix, the rows of which are word vector representations of each token. These might be, e.g., outputs from trained word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014) models. We denote the dimensionality of the word vectors by d. If the length of a given sentence is s, then the dimensionality of the sentence matrix is s × d. Suppose that there is a filter matrix w with region size h; w will contain h · d parameters to be estimated. We denote the sentence matrix by A ∈ Rs×d , and use A[i : j] to represent the sub-matrix of A from row i to row j. The output sequence o ∈ Rs−h+1 of the convolution operator is obtained by repeatedly applying the filter on sub-matrices of A: Background and Preliminaries Deep and neural learning methods are now well established i"
I17-1026,D13-1170,0,0.041617,"from row i to row j. The output sequence o ∈ Rs−h+1 of the convolution operator is obtained by repeatedly applying the filter on sub-matrices of A: Background and Preliminaries Deep and neural learning methods are now well established in machine learning (LeCun et al., 2015; Bengio, 2009). They have been especially successful for image and speech processing tasks. More recently, such methods have begun to overtake traditional sparse, linear models for NLP (Goldberg, 2015; Bengio et al., 2003; Mikolov et al., 2013; Collobert and Weston, 2008; Collobert et al., 2011; Kalchbrenner et al., 2014; Socher et al., 2013). Recently, word embeddings have been exploited for sentence classification using CNN architectures. Kalchbrenner (2014) proposed a CNN architecture with multiple convolution layers, positing latent, dense and low-dimensional word vectors (initialized to random values) as inputs. Kim (2014) defined a one-layer CNN architecture that performed comparably. This model uses pre-trained word vectors as inputs, which may be treated as static or non-static. In the former approach, word vectors are treated as fixed inputs, while in the latter they are ‘tuned’ for a specific task. Elsewhere, Johnson and"
I17-1026,P14-2084,1,0.753975,"SST-1, but pared to only two classes. We again only train and test models on sentences, excluding phrases. (4) Subj: Subjectivity dataset (Pang and Lee, 2005). (5) TREC: Question classification dataset (Li and Roth, 2002). (6) CR: Customer review dataset (Hu and Liu, 2004). (7) MPQA: Opinion polarity dataset (Wiebe et al., 2005). Additionally, we use (8) Opi: Opinosis Dataset, which comprises sentences extracted from user reviews on a given topic, e.g. “sound quality of ipod nano”. There are 51 such topics and each topic contains approximately 100 sentences (Ganesan et al., 2010). (9) Irony (Wallace et al., 2014): this contains 16,006 sentences from reddit labeled as ironic (or not). The dataset is imbalanced (relatively few sentences are ironic). Thus before training, we under-sampled negative instances to make classes sizes equal.1 For this dataset we report the Area Under Curve (AUC), rather than accuracy, because it is imbalanced. Values Google word2vec (3,4,5) 100 ReLU 1-max pooling 0.5 3 Table 1: Baseline configuration. ‘feature maps’ refers to the number of feature maps for each filter region size. ‘ReLU’ refers to rectified linear unit (Maas et al., 2013), a commonly used activation function i"
I17-1026,P15-2058,0,0.0130911,"(to the exclusion of more complex models) due to their comparative simplicity and strong empirical performance, which makes it a modern standard baseline method akin to Support Vector Machine (SVMs) and logistic regression. We derive practical advice from our extensive empirical results for those interested in getting the most out of CNNs for sentence classification in real world settings. 1 Introduction Convolutional Neural Networks (CNNs) have recently been shown to achieve impressive results on the practically important task of sentence categorization (Kim, 2014; Kalchbrenner et al., 2014; Wang et al., 2015; Goldberg, 2015; Iyyer et al., 253 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 253–263, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP tors – coupled with observed strong empirical performance makes this a strong contender to supplant existing text classification baselines such as SVM and logistic regression. But in practice one is faced with making several model architecture decisions and setting various hyperparameters. At present, very little empirical data is available to guide such decisions; addressing this gap is our aim"
K16-1017,E14-3007,0,0.0282664,"d sarcasm have used features similar to those used in sentiment analysis. Carvalho et al. (2009) analyzed comments posted by users on a Portuguese online newspaper and found that oral and gestural cues indicate irony. These included: emoticons, onomatopoeic expressions for laughter, heavy punctuation, quotation marks and positive interjections. Others have used text classifiers with features based on word and character n-grams, sentiment lexicons, surface patterns and textual markers (Davidov et al., 2010; Gonz´alez-Ib´anez et al., 2011; Reyes et al., 2013; Lukin and Walker, 2013). Elsewhere, Barbieri and Saggion (2014) derived new wordfrequency based features to detect irony, e.g., combinations of frequent and rare words, ambiguous words, ‘spoken style’ words combined with ‘written style’ words and intensity of adjectives. Riloff et al. (2013) demonstrated that one may exploit the apparent expression of contrasting sentiment in the same utterance as a marker of verbal irony. The aforementioned approaches rely predominantly on features intrinsic to texts, but these will A major downside of these and related approaches, however, is the amount of manual effort required to derive these feature sets. A primary g"
K16-1017,J92-4003,0,0.0821266,"logisticregression based classifier that exploits rich feature sets to achieve strong performance. These are detailed at length in the original paper, but we briefly summarize them here: • response-features, for tweets written in response to another tweet. This set of features captures information relating the two, with BoW features of the original tweet and pairwise cluster indicator features, which the encode Brown clusters observed in both the original and response tweet. • tweet-features, encoding attributes of the target tweet text, including: uni- and bigram bag of words (BoW) features; Brown et al. (1992) word clusters indicators; unlabeled dependency bigrams (both BoW and with Brown cluster representations); part-ofspeech, spelling and abbreviation features; inferred sentiment, at both the tweet and word level; and ‘intensifier’ indicators. We emphasize that implementing this rich set of features took considerable time and effort. This motivates our approach, which aims to effectively induce and exploit contextually-aware representations without manual feature engineering. • author-features, aimed at encoding attributes of the author, including: historically 5 The original study (Bamman and S"
K16-1017,W10-2914,0,0.177865,"n indirect insult (Dews et al., 1995). Most of the previously proposed computational models to detect irony and sarcasm have used features similar to those used in sentiment analysis. Carvalho et al. (2009) analyzed comments posted by users on a Portuguese online newspaper and found that oral and gestural cues indicate irony. These included: emoticons, onomatopoeic expressions for laughter, heavy punctuation, quotation marks and positive interjections. Others have used text classifiers with features based on word and character n-grams, sentiment lexicons, surface patterns and textual markers (Davidov et al., 2010; Gonz´alez-Ib´anez et al., 2011; Reyes et al., 2013; Lukin and Walker, 2013). Elsewhere, Barbieri and Saggion (2014) derived new wordfrequency based features to detect irony, e.g., combinations of frequent and rare words, ambiguous words, ‘spoken style’ words combined with ‘written style’ words and intensity of adjectives. Riloff et al. (2013) demonstrated that one may exploit the apparent expression of contrasting sentiment in the same utterance as a marker of verbal irony. The aforementioned approaches rely predominantly on features intrinsic to texts, but these will A major downside of the"
K16-1017,N13-1039,0,0.0261101,"Missing"
K16-1017,P11-2102,0,0.63636,"Missing"
K16-1017,W15-2905,0,0.205927,"allace et al., 2014). Indeed, the exact same sentence can be interpreted as literal or sarcastic, depending on the speaker. Consider the sarcastic tweet in Figure 1 (ignoring for the moment the attached #sarcasm hashtag). Without knowing the author’s political leanings, it would be difficult to conclude with certainty whether the remark was intended sarcastically or in earnest. Recent work in sarcasm detection on social media has tried to incorporate contextual information by exploiting the preceding messages of a user, to e.g., detect contrasts in sentiments expressed towards named entities (Khattri et al., 2015), infer behavioural traits (Rajadesingan et al., 2015) and capture the relationship between authors and the audience (Bamman and Smith, 2015). However, all of these approaches require the design and implementation of complex features that explicitly encode the content and (relevant) context of messages to be classified. This feature engineering is labor intensive, and depends on external tools and resources. Therefore, deploying such systems in practice is expensive, time-consuming and unwieldy. We propose a novel approach to sarcasm detection on social media that does not require extensive ma"
K16-1017,D14-1181,0,0.00631937,"Missing"
K16-1017,D13-1066,0,0.857192,"Missing"
K16-1017,P14-2084,1,0.731381,"By contrast, we propose to automatically learn and then exploit user embeddings, to be used in concert with lexical signals to recognize sarcasm. Our approach does not require elaborate feature engineering (and concomitant data scraping); fitting user embeddings requires only the text from their previous posts. The experimental results show that the our model outperforms a state-of-the-art approach leveraging an extensive set of carefully crafted features. 1 Figure 1: An illustrative tweet. to discern ironic intent. Appreciating the context of utterances is critical for this; even for humans (Wallace et al., 2014). Indeed, the exact same sentence can be interpreted as literal or sarcastic, depending on the speaker. Consider the sarcastic tweet in Figure 1 (ignoring for the moment the attached #sarcasm hashtag). Without knowing the author’s political leanings, it would be difficult to conclude with certainty whether the remark was intended sarcastically or in earnest. Recent work in sarcasm detection on social media has tried to incorporate contextual information by exploiting the preceding messages of a user, to e.g., detect contrasts in sentiments expressed towards named entities (Khattri et al., 2015"
K16-1017,W13-1104,0,0.0188229,"putational models to detect irony and sarcasm have used features similar to those used in sentiment analysis. Carvalho et al. (2009) analyzed comments posted by users on a Portuguese online newspaper and found that oral and gestural cues indicate irony. These included: emoticons, onomatopoeic expressions for laughter, heavy punctuation, quotation marks and positive interjections. Others have used text classifiers with features based on word and character n-grams, sentiment lexicons, surface patterns and textual markers (Davidov et al., 2010; Gonz´alez-Ib´anez et al., 2011; Reyes et al., 2013; Lukin and Walker, 2013). Elsewhere, Barbieri and Saggion (2014) derived new wordfrequency based features to detect irony, e.g., combinations of frequent and rare words, ambiguous words, ‘spoken style’ words combined with ‘written style’ words and intensity of adjectives. Riloff et al. (2013) demonstrated that one may exploit the apparent expression of contrasting sentiment in the same utterance as a marker of verbal irony. The aforementioned approaches rely predominantly on features intrinsic to texts, but these will A major downside of these and related approaches, however, is the amount of manual effort required t"
K16-1017,P15-1100,1,0.805125,"t towards specific targets (i.e., entities). A tweet is then predicted to be sarcastic if it expresses a sentiment about an entity that contradicts the author’s (estimated) prior sentiment regarding the same. Rajadesingan et al. (2015) built a system based on theories of sarcasm expression from psychology and behavioral sciences. To operationalize such theories, they used several linguistic tools and resources (e.g. lexicons, sentiment classifiers and a PoS tagger), in addition to user profile information and previous posts, to model a range of behavioural aspects (e.g., mood, writing style). Wallace et al. (2015) developed an approach for classifying posts on reddit2 as sarcastic or literal, based in part on the interaction between the specific sub-reddit to which a post was made, the entities mentioned, and the (apparent) sentiment expressed. For example, if a post in the (politically) conservative sub-reddit mentions Obama, it is more likely to have been intended ironically than posts mentioning Obama in the progressive sub-reddit. But this approach is limited because it relies on the unique sub-reddit structure. Bamman and Smith (2015) proposed an approach that relied on an extensive, rich set of f"
K16-1017,P15-1104,1,\N,Missing
N12-1001,P08-1090,0,0.339204,"n, loosely, that these threads constitute their own independent stories, coherent on their own (i.e., 1 No relation. without the broader context of the overarching narrative). I refer to the task of identifying these independent threads and untangling them from one another as multiple narrative disentanglement (MND). The task is of theoretical interest because disentanglement is a necessary pre-requisite to making sense of narrative texts, an interesting direction in NLP that has received an increasing amount of attention (Elson et al., 2010; Elson and McKeown, 2010; Celikyilmaz et al., 2010; Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009). Recognizing the (main) narrative threads comprising a work provides a context for interpreting the text. Disentanglement may thus be viewed as the first step in a literary processing ‘pipeline’. Identifying threads and assigning them to passages may help in automatic plot summarization, social network construction and other literary analysis tasks. Computational approaches to literature look to make narrative sense of unstructured text, i.e., construct models that relate characters and events chronologically: disentanglement is at the heart of this re-constructi"
N12-1001,P09-1068,0,0.403879,"s constitute their own independent stories, coherent on their own (i.e., 1 No relation. without the broader context of the overarching narrative). I refer to the task of identifying these independent threads and untangling them from one another as multiple narrative disentanglement (MND). The task is of theoretical interest because disentanglement is a necessary pre-requisite to making sense of narrative texts, an interesting direction in NLP that has received an increasing amount of attention (Elson et al., 2010; Elson and McKeown, 2010; Celikyilmaz et al., 2010; Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009). Recognizing the (main) narrative threads comprising a work provides a context for interpreting the text. Disentanglement may thus be viewed as the first step in a literary processing ‘pipeline’. Identifying threads and assigning them to passages may help in automatic plot summarization, social network construction and other literary analysis tasks. Computational approaches to literature look to make narrative sense of unstructured text, i.e., construct models that relate characters and events chronologically: disentanglement is at the heart of this re-construction. But MND is also potentiall"
N12-1001,J10-3004,0,0.363348,"xts (Bal, 1997). 1 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1–10, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics entanglement thus has applications outside of computational methods for fiction. In this work, I treat MND as an unsupervised learning task. Given a block of narrative text, the aim is to identify the top k sub-narratives therein, and then to extract the passages comprising them. The proposed task is similar in spirit to the problem of chat disentanglement (Elsner and Charniak, 2010), in which the aim is to assign each utterance in a chat transcription to an associated conversational thread. Indeed, the main objective is the same: disentangle fragments of a monolithic text into chronologically ordered, independently coherent ‘threads’. Despite their similarities, however, narrative disentanglement is a qualitatively different task than chat disentanglement, as I highlight in Section 3. I take inspiration from the literary community, which has studied the theoretical underpinnings of the narrative form at length (Prince, 1982; Prince, 2003; Abbott, 2008). I rely especially"
N12-1001,P11-1118,0,0.0629592,"Missing"
N12-1001,P10-1015,0,0.435105,"erleaved throughout its voluminous (meta-)story. By sub-narrative I mean, loosely, that these threads constitute their own independent stories, coherent on their own (i.e., 1 No relation. without the broader context of the overarching narrative). I refer to the task of identifying these independent threads and untangling them from one another as multiple narrative disentanglement (MND). The task is of theoretical interest because disentanglement is a necessary pre-requisite to making sense of narrative texts, an interesting direction in NLP that has received an increasing amount of attention (Elson et al., 2010; Elson and McKeown, 2010; Celikyilmaz et al., 2010; Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009). Recognizing the (main) narrative threads comprising a work provides a context for interpreting the text. Disentanglement may thus be viewed as the first step in a literary processing ‘pipeline’. Identifying threads and assigning them to passages may help in automatic plot summarization, social network construction and other literary analysis tasks. Computational approaches to literature look to make narrative sense of unstructured text, i.e., construct models that relate characters a"
N12-1001,W03-0430,0,0.0124797,"xtracted in 997 (2). When this topic modbe estimated; only the words in documents are ob7 served. To uncover the topic mixtures latent in docThis is analogous to a multi-label scenario. n d dn 5 eling is performed over the entities, rather than the text, I shall refer to it as narrative modeling. As mentioned above, Step (1) will be taskspecific: what constitutes a passage is inherently subjective. In many cases, however, the text will lend itself to a ‘natural’ segmenting, e.g., at the chapter-level. Standard statistical techniques for named entity recognition (NER) can be used for Step (2) (McCallum and Li, 2003). Algorithm 1 The story of LDA over extracted entities for multiple narrative disentanglement. Draw a mixture of narrative threads θ ∼ Dir(α) for each entity in the passage ei do Draw a narrative thread ti ∼ M ultinomial(θ) Draw ei from p(ei |ti ) end for For the narrative modeling Step (3), I use LDA narrative text (Blei et al., 2003); the generative story for narrative modeling is told segmenter by Algorithm 1.8 This squares with the narrapassages tological view: entities are observed in the text NER with probability proporextractor tional to their likelihood extracted entities of being draw"
N16-1178,P15-1104,0,0.0231138,"Missing"
N16-1178,P12-1015,0,0.0116899,"ach enjoys the following advantages compared to the only existing comparable model (Yin and Sch¨utze, 2015): (i) It can leverage diverse, readily available word embeddings with different dimensions, thus providing flexibility. (ii) It is comparatively simple, and does not, for example, require mutual learning or pre-training. (iii) It is an order of magnitude more efficient in terms of training time. 2 Related Work Prior work has considered combining latent representations of words that capture syntactic and semantic properties (Van de Cruys et al., 2011), and inducing multi-modal embeddings (Bruni et al., 2012) for general NLP tasks. And recently, Luo et al. (2014) proposed a framework that combines multiple word embeddings to measure text similarity, however their focus was not on classification. More similar to our work, Yin and Sch¨utze (2015) proposed MVCNN for sentence classification. This CNN-based architecture accepts multiple word embeddings as inputs. These are then treated as separate ‘channels’, analogous to RGB channels in images. Filters consider all channels simultaneously. MVCNN achieved state-of-the-art performance on multiple sentence classification tasks. However, this model has pr"
N16-1178,D14-1082,0,0.0310234,"rd2vec2 is trained on 100 billion tokens of Google News dataset; (ii) GloVe (Pennington et al., 2014)3 is trained on aggregated global word-word co-occurrence statistics from Common Crawl (840B tokens); and (iii) syntactic word embedding trained on dependency-parsed corpora. These three embedding sets happen to all be 300dimensional, but our model could accommodate arbitrary and variable sizes. We pre-trained our own syntactic embeddings following (Levy and Goldberg, 2014). We parsed the ukWaC corpus (Baroni et al., 2009) using the Stanford Dependency Parser v3.5.2 with Stanford Dependencies (Chen and Manning, 2014) and extracted (word, relation+context) pairs from parse trees. We “collapsed” nodes with prepositions and notated inverse relations separately, e.g., “dog 2 3 https://code.google.com/p/word2vec/ http://nlp.stanford.edu/projects/glove/ 1525 barks” emits two tuples: (barks, nsubj dog) and (dog, nsubj−1 barks). We filter words and contexts that appear fewer than 100 times, resulting in ∼173k words and 1M contexts. We trained 300d vectors using word2vecf4 with default parameters. 4.3 Setup We compared our proposed approaches to a standard CNN that exploits a single set of word embeddings (Kim, 20"
N16-1178,D08-1094,0,0.0234245,"Missing"
N16-1178,P14-1062,0,0.00892531,"Missing"
N16-1178,D14-1181,0,0.323548,"dding sets. This model is much simpler than comparable alternative architectures and requires substantially less training time. Furthermore, it is flexible in that it does not require input word embeddings to be of the same dimensionality. We show that MGNC-CNN consistently outperforms baseline models. 1 Byron C. Wallace2 Introduction Neural models have recently gained popularity for Natural Language Processing (NLP) tasks (Goldberg, 2015; Collobert and Weston, 2008; Cho, 2015). For sentence classification, in particular, Convolution Neural Networks (CNN) have realized impressive performance (Kim, 2014; Zhang and Wallace, 2015). These models operate over word embeddings, i.e., dense, low dimensional vector representations of words that aim to capture salient semantic and syntactic properties (Collobert and Weston, 2008). An important consideration for such models is the Many pre-trained word embeddings are now readily available on the web, induced using different models, corpora, and processing steps. Different embeddings may encode different aspects of language (Pad´o and Lapata, 2007; Erk and Pad´o, 2008; Levy and Goldberg, 2014): those based on bag-ofwords (BoW) statistics tend to captur"
N16-1178,P14-2050,0,0.0651496,"r, Convolution Neural Networks (CNN) have realized impressive performance (Kim, 2014; Zhang and Wallace, 2015). These models operate over word embeddings, i.e., dense, low dimensional vector representations of words that aim to capture salient semantic and syntactic properties (Collobert and Weston, 2008). An important consideration for such models is the Many pre-trained word embeddings are now readily available on the web, induced using different models, corpora, and processing steps. Different embeddings may encode different aspects of language (Pad´o and Lapata, 2007; Erk and Pad´o, 2008; Levy and Goldberg, 2014): those based on bag-ofwords (BoW) statistics tend to capture associations (doctor and hospital), while embeddings based on dependency-parses encode similarity in terms of use (doctor and surgeon). It is natural to consider how these embeddings might be combined to improve NLP models in general and CNNs in particular. Contributions. We propose MGNC-CNN, a novel, simple, scalable CNN architecture that can accommodate multiple off-the-shelf embeddings of variable sizes. Our model treats different word embeddings as distinct groups, and applies CNNs independently to each, thus generating correspo"
N16-1178,C02-1150,0,0.247002,"y independent. MG-CNN applies a max norm constraint to o, while MGNC-CNN applies max norm constraints on o1 and o2 independently (group regularization). Note that one may easily extend the approach to handle more than two embeddings at once. five classes: very negative, negative, neutral, positive, and very positive. (2) SST-2, which has only two classes: negative and positive. For both, we remove phrases of length less than 4 from the training set.1 Subj (Pang and Lee, 2004). The aim here is to classify sentences as either subjective or objective. This comprises 5000 instances of each. TREC (Li and Roth, 2002). A question classification dataset containing six classes: abbreviation, entity, description, human, location and numeric. There are 5500 training and 500 test instances. Irony (Wallace et al., 2014). This dataset contains 16,006 sentences from reddit labeled as ironic (or not). The dataset is imbalanced (relatively few sentences are ironic). Thus before training, we under-sampled negative instances to make classes sizes equal. Note that for this dataset we report the Area Under Curve (AUC), rather than accuracy, because it is imbalanced. 1 As in (Kim, 2014). Model CNN(w2v) CNN(Glv) CNN(Syn)"
N16-1178,D15-1279,0,0.119041,"Missing"
N16-1178,J07-2002,0,0.0655625,"Missing"
N16-1178,P04-1035,0,0.0158137,"movie embedding 1 embedding 2 Figure 1: Illustration of MG-CNN and MGNC-CNN. The filters applied to the respective embeddings are completely independent. MG-CNN applies a max norm constraint to o, while MGNC-CNN applies max norm constraints on o1 and o2 independently (group regularization). Note that one may easily extend the approach to handle more than two embeddings at once. five classes: very negative, negative, neutral, positive, and very positive. (2) SST-2, which has only two classes: negative and positive. For both, we remove phrases of length less than 4 from the training set.1 Subj (Pang and Lee, 2004). The aim here is to classify sentences as either subjective or objective. This comprises 5000 instances of each. TREC (Li and Roth, 2002). A question classification dataset containing six classes: abbreviation, entity, description, human, location and numeric. There are 5500 training and 500 test instances. Irony (Wallace et al., 2014). This dataset contains 16,006 sentences from reddit labeled as ironic (or not). The dataset is imbalanced (relatively few sentences are ironic). Thus before training, we under-sampled negative instances to make classes sizes equal. Note that for this dataset we"
N16-1178,D14-1162,0,0.112325,"-CNN(w2v+Glv) MGNC-CNN(w2v+Syn) MGNC-CNN(w2v+Syn+Glv) Subj 9 3 3 9 3 9 3 9 9 (9,3) (3,3) (81,81,81) SST-1 81 9 81 9 81 9 9 81 1 (81,9) (81,81) (81,81,1) SST-2 81 1 9 3 9 1 3 3 9 (1,1) (81,9) (9,9,9) TREC 9 9 81 3 9 81 81 81 243 (9,81) (81,81) (1,81,81) Irony 243 81 1 1 1 81 9 3 9 (243,243) (81,3) (243,243,3) Table 2: Best λ2 value on the validation set for each method w2v:word2vec. Glv:GloVe. Syn: Syntactic embedding. 4.2 Pre-trained Word Embeddings We consider three sets of word embeddings for our experiments: (i) word2vec2 is trained on 100 billion tokens of Google News dataset; (ii) GloVe (Pennington et al., 2014)3 is trained on aggregated global word-word co-occurrence statistics from Common Crawl (840B tokens); and (iii) syntactic word embedding trained on dependency-parsed corpora. These three embedding sets happen to all be 300dimensional, but our model could accommodate arbitrary and variable sizes. We pre-trained our own syntactic embeddings following (Levy and Goldberg, 2014). We parsed the ukWaC corpus (Baroni et al., 2009) using the Stanford Dependency Parser v3.5.2 with Stanford Dependencies (Chen and Manning, 2014) and extracted (word, relation+context) pairs from parse trees. We “collapsed”"
N16-1178,D13-1170,0,0.00417297,"Missing"
N16-1178,D11-1094,0,0.0403192,"Missing"
N16-1178,P14-2084,1,0.283854,"to handle more than two embeddings at once. five classes: very negative, negative, neutral, positive, and very positive. (2) SST-2, which has only two classes: negative and positive. For both, we remove phrases of length less than 4 from the training set.1 Subj (Pang and Lee, 2004). The aim here is to classify sentences as either subjective or objective. This comprises 5000 instances of each. TREC (Li and Roth, 2002). A question classification dataset containing six classes: abbreviation, entity, description, human, location and numeric. There are 5500 training and 500 test instances. Irony (Wallace et al., 2014). This dataset contains 16,006 sentences from reddit labeled as ironic (or not). The dataset is imbalanced (relatively few sentences are ironic). Thus before training, we under-sampled negative instances to make classes sizes equal. Note that for this dataset we report the Area Under Curve (AUC), rather than accuracy, because it is imbalanced. 1 As in (Kim, 2014). Model CNN(w2v) CNN(Glv) CNN(Syn) MVCNN (Yin and Sch¨utze, 2015) C-CNN(w2v+Glv) C-CNN(w2v+Syn) C-CNN(w2v+Syn+Glv) MG-CNN(w2v+Glv) MG-CNN(w2v+Syn) MG-CNN(w2v+Syn+Glv) MGNC-CNN(w2v+Glv) MGNC-CNN(w2v+Syn) MGNC-CNN(w2v+Syn+Glv) Subj 93.14"
N16-1178,P15-1100,1,0.826219,"NN, a novel, simple, scalable CNN architecture that can accommodate multiple off-the-shelf embeddings of variable sizes. Our model treats different word embeddings as distinct groups, and applies CNNs independently to each, thus generating corresponding feature vectors (one per embedding) which are then 1522 Proceedings of NAACL-HLT 2016, pages 1522–1527, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics concatenated at the classification layer. Inspired by prior work exploiting regularization to encode structure for NLP tasks (Yogatama and Smith, 2014; Wallace et al., 2015), we impose different regularization penalties on weights for features generated from the respective word embedding sets. Our approach enjoys the following advantages compared to the only existing comparable model (Yin and Sch¨utze, 2015): (i) It can leverage diverse, readily available word embeddings with different dimensions, thus providing flexibility. (ii) It is comparatively simple, and does not, for example, require mutual learning or pre-training. (iii) It is an order of magnitude more efficient in terms of training time. 2 Related Work Prior work has considered combining latent represe"
N16-1178,K15-1021,0,0.304421,"Missing"
N18-2060,P14-5010,0,0.00454234,"Missing"
N18-2060,N10-1124,0,0.0218335,"Unfortunately, these aspects are not usually described in a structured way. Abstracts with explicit category headings (Nakayama et al., 2005) partially address this, but these are not standardized nor uniform. Automated solutions are thus emerging to better support medical search, including methods for: identifying sentences containing key pieces of clinical information (Wallace et al., 2016); summarization (Sarker et al., 2016); identifying contradictory claims in medical articles (Alamri and Stevenson, 2016); and information retrieval system prototypes that harness this type of information (Boudin et al., 2010a,b). (I) In Group I, the children were treated with prednisone ... (O) .. reported that Group 2 children underwent fewer isolated bone marrow relapses .. We explore three strategies for exploiting extracted patterns in a state-of-the-art LSTM-CRF sequence tagging model (Lample et al., 2016; Ma and Hovy, 2016): as additional features at the CRF layer; as one-hot indicators concatenated to distributed representations of words; and as individual units embedded in a semantic space shared with words. The second representation improves recall for two extraction tasks, and the third improves precisi"
N18-2060,J07-1005,0,0.138493,"Missing"
N18-2060,N16-1030,0,0.620846,"methods for: identifying sentences containing key pieces of clinical information (Wallace et al., 2016); summarization (Sarker et al., 2016); identifying contradictory claims in medical articles (Alamri and Stevenson, 2016); and information retrieval system prototypes that harness this type of information (Boudin et al., 2010a,b). (I) In Group I, the children were treated with prednisone ... (O) .. reported that Group 2 children underwent fewer isolated bone marrow relapses .. We explore three strategies for exploiting extracted patterns in a state-of-the-art LSTM-CRF sequence tagging model (Lample et al., 2016; Ma and Hovy, 2016): as additional features at the CRF layer; as one-hot indicators concatenated to distributed representations of words; and as individual units embedded in a semantic space shared with words. The second representation improves recall for two extraction tasks, and the third improves precision for all three tasks. We analyze the induced semantic space to show that patterns capture contextual information that is otherwise lost. ∗ * now at Google Inc. 371 Proceedings of NAACL-HLT 2018, pages 371–377 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational L"
N18-2060,P16-1101,0,0.0201047,"Missing"
N19-1150,S16-2018,0,0.0971038,"rovements in model performance. 1 Introduction Assembling training corpora of annotated natural language examples in specialized domains such as biomedicine poses considerable challenges. Experts with the requisite domain knowledge to perform high-quality annotation tend to be expensive, while lay annotators may not have the necessary knowledge to provide high-quality annotations. A practical approach for collecting a sufficiently large corpus would be to use crowdsourcing platforms like Amazon Mechanical Turk (MTurk). However, crowd workers in general are likely to provide noisy annotations (Abad and Moschitti, 2016; Plank et al., 2014; Alonso et al., 2015), an issue exacerbated by the technical nature of specialized content. Some of this noise may reflect worker quality and can be modeled (Abad and Moschitti, 2016; Plank et al., 2014; Cohn and Specia, 2013; Nguyen et al., 2017), but for some instances lay people may simply lack the domain knowledge to provide useful annotation. In this paper we report experiments on the EBM-NLP corpus comprising crowdsourced annotations of medical literature (Nye et al., 2018). We operationalize the concept of annotation difficulty and show how it can be exploited durin"
N19-1150,W15-2711,0,0.0378808,"Missing"
N19-1150,D18-2029,1,0.909448,"nnotations is better than using lay data alone. Does it matter what data is annotated by experts? We demonstrate that a system trained on combined data achieves better predictive performance when experts annotate difficult examples rather than instances selected at i.i.d. random. Our contributions in this work are summarized as follows. We define a task difficulty prediction task and show how this is related to, but distinct from, inter-worker agreement. We introduce a new model for difficulty prediction combining learned representations induced via a pre-trained ‘universal’ sentence encoder (Cer et al., 2018), and a sentence encoder learned from scratch for this task. We show that predicting annotation difficulty can be used to improve the task routing and model performance for a biomedical information extraction task. Our results open up a new direction for ensuring corpus quality. We believe that item difficulty prediction will likely be useful in other, nonspecialized tasks as well, and that the most effective data collection in specialized domains requires research addressing the fundamental questions we examine here. 2 Related Work Crowdsourcing annotation is now a well-studied problem (Snow"
N19-1150,P13-1004,0,0.140217,"uality annotation tend to be expensive, while lay annotators may not have the necessary knowledge to provide high-quality annotations. A practical approach for collecting a sufficiently large corpus would be to use crowdsourcing platforms like Amazon Mechanical Turk (MTurk). However, crowd workers in general are likely to provide noisy annotations (Abad and Moschitti, 2016; Plank et al., 2014; Alonso et al., 2015), an issue exacerbated by the technical nature of specialized content. Some of this noise may reflect worker quality and can be modeled (Abad and Moschitti, 2016; Plank et al., 2014; Cohn and Specia, 2013; Nguyen et al., 2017), but for some instances lay people may simply lack the domain knowledge to provide useful annotation. In this paper we report experiments on the EBM-NLP corpus comprising crowdsourced annotations of medical literature (Nye et al., 2018). We operationalize the concept of annotation difficulty and show how it can be exploited during training to improve information extraction models. We then obtain expert annotations for the abstracts predicted to be most difficult, as well as for a similar number of randomly selected abstracts. The annotation of highly specialized data and"
N19-1150,D14-1181,0,0.00470094,"pare the correlation between inter-annotator agreement and difficulty scores in the training data. Given that the majority of sentences do not contain a PICO span, we only include in these calculations those that contain a reference label. Pearson’s r are 0.34, 0.30 and 0.31 for P, I and O, respectively, confirming that inter-worker agreement and our proposed difficulty score are quite distinct. 6 Predicting Annotation Difficulty We treat difficulty prediction as a regression problem, and propose and evaluate neural model variants for the task. We first train RNN (Chung et al., 2014) and CNN (Kim, 2014) models. We also use the universal sentence encoder (USE) (Cer et al., 2018) to induce sentence representations, and train a model using these as features. Following (Cer et al., 2018), we then experiment with an ensemble model that combines the ‘universal’ and task-specific representations to predict annotation difficulty. We expect these universal embeddings to capture general, high-level semantics, and the task specific representations to capture more granular information. Figure 2 de1474 NGRAM+SVR RNN CNN USE USE+RNN P 0.455 0.521 0.470 0.492 0.550 I 0.311 0.555 0.522 0.518 0.604 O 0.541 0"
N19-1150,P17-1028,1,0.90712,"to be expensive, while lay annotators may not have the necessary knowledge to provide high-quality annotations. A practical approach for collecting a sufficiently large corpus would be to use crowdsourcing platforms like Amazon Mechanical Turk (MTurk). However, crowd workers in general are likely to provide noisy annotations (Abad and Moschitti, 2016; Plank et al., 2014; Alonso et al., 2015), an issue exacerbated by the technical nature of specialized content. Some of this noise may reflect worker quality and can be modeled (Abad and Moschitti, 2016; Plank et al., 2014; Cohn and Specia, 2013; Nguyen et al., 2017), but for some instances lay people may simply lack the domain knowledge to provide useful annotation. In this paper we report experiments on the EBM-NLP corpus comprising crowdsourced annotations of medical literature (Nye et al., 2018). We operationalize the concept of annotation difficulty and show how it can be exploited during training to improve information extraction models. We then obtain expert annotations for the abstracts predicted to be most difficult, as well as for a similar number of randomly selected abstracts. The annotation of highly specialized data and the use of lay and ex"
N19-1150,P18-1019,1,0.84498,"urk (MTurk). However, crowd workers in general are likely to provide noisy annotations (Abad and Moschitti, 2016; Plank et al., 2014; Alonso et al., 2015), an issue exacerbated by the technical nature of specialized content. Some of this noise may reflect worker quality and can be modeled (Abad and Moschitti, 2016; Plank et al., 2014; Cohn and Specia, 2013; Nguyen et al., 2017), but for some instances lay people may simply lack the domain knowledge to provide useful annotation. In this paper we report experiments on the EBM-NLP corpus comprising crowdsourced annotations of medical literature (Nye et al., 2018). We operationalize the concept of annotation difficulty and show how it can be exploited during training to improve information extraction models. We then obtain expert annotations for the abstracts predicted to be most difficult, as well as for a similar number of randomly selected abstracts. The annotation of highly specialized data and the use of lay and expert annotators allow us to examine the following key questions related to lay and expert annotations in specialized domains: Can we predict item difficulty? We define a training instance as difficult if a lay annotator or an automated m"
N19-1150,N18-2060,1,0.834411,"e use Spearmans’ correlation coefficient as a scoring function. Specifically, for each sentence we create two vectors comprising counts of how many times each token was annotated by crowd and expert workers, respectively, and calculate the correlation between these. Sentences with no labels are treated as maximally easy; those with only either crowd worker or expert label(s) are assumed maximally difficult. The training set contains only crowdsourced annotations. To label the training data, we use a 10fold validation like setting. We iteratively retrain the LSTM-CRF-Pattern sequence tagger of Patel et al. (2018) on 9 folds of the training data and use that trained model to predict labels for the 10th. In this way we obtain predictions on the full training set. We then use predicted spans as proxy ‘ground truth’ annotations to calculate the diffi1473 Probability Density of Difficulty Scores 4 Workers crowd workers domain experts Participants Mean: 0.500; Std: 0.296 3 2 1 P 0.52 0.74 I 0.43 0.68 O 0.41 0.57 0 4 Intervention Mean: 0.562; Std: 0.279 Table 2: Average inter-worker agreement. 3 2 1 0 4 spective label types as separate tasks. Outcome Mean: 0.576; Std: 0.279 3 2 5 1 0 0.0 0.2 0.4 Scores 0.6 0"
N19-1150,D08-1027,0,0.284085,"Missing"
N19-1150,E12-2021,0,0.10862,"Missing"
N19-1150,D14-1162,0,0.0823658,"USE USE+RNN P 0.455 0.521 0.470 0.492 0.550 I 0.311 0.555 0.522 0.518 0.604 O 0.541 0.601 0.550 0.580 0.622 Table 3: Pearson correlation coefficients of sentence difficulty predictions. Figure 2: Model architecture. picts the model architecture. Sentences are fed into both the universal sentence encoder and, separately, a task specific neural encoder, yielding two representations. We concatenate these and pass the combined vector to the regression layer. 6.1 Experimental Setup and Results We trained models for each label type separately. Word embeddings were initialized to 300d GloVe vectors (Pennington et al., 2014) trained on common crawl data;2 these are fine-tuned during training. We used the Adam optimizer (Kingma and Ba, 2014) with learning rate and decay set to 0.001 and 0.99, respectively. We used batch sizes of 16. We used the large version of the universal sentence encoder3 with a transformer (Vaswani et al., 2017). We did not update the pretrained sentence encoder parameters during training. All hyperparamaters for all models (including hidden layers, hidden sizes, and dropout) were tuned using Vizier (Golovin et al., 2017) via 10-fold cross validation on the training set maximizing for F1.4 As"
N19-1150,E14-1078,0,0.0766517,"Missing"
N19-1357,D18-1216,0,0.0373664,"ention variants designed explicitly for interpretability; these may provide greater transparency by imposing hard, sparse attention. Such instantiations explicitly select (modest) subsets of inputs to be considered when making a prediction, which are then by construction responsible for model output (Lei et al., 2016; Peters et al., 2018). Structured attention models (Kim et al., 2017) provide a generalized framework for describing and fitting attention variants with explicit probabilistic semantics. Tying attention weights to human-provided rationales is another potentially promising avenue (Bao et al., 2018). We hope our work motivates further development of these methods, resulting in attention variants that both improve predictive performance and provide insights into model predictions. 6 Discussion and Conclusions We have provided evidence that correlation between intuitive feature importance measures (in3550 0.2 0.4 0.6 Max Attention 0.04 0.02 0.00 0.0 0.2 0.4 0.6 Max JS Divergence within 0.10 0.08 0.06 0.04 0.02 0.00 0.0 0.2 0.4 0.6 [0.00, 0.10 0.25) 0.08 [0.25, 0.50) 0.06 [0.50, 0.04 0.75) 0.02 [0.75, 1.00) 0.00 0.00.0 0.20.2 0.40.4 0.60.6 MaxMax JS Divergence JS Divergence within within Ma"
N19-1357,D15-1075,0,0.0118975,".di.unipi.it/~gulli/AG_ corpus_of_news_articles.html matic parsing of news articles from CNN. Each instance comprises a paragraph-question-answer triplet, where the answer is one of the anonymized entities in the paragraph. bAbI (Weston et al., 2015). We consider the three tasks presented in the original bAbI dataset paper, training separate models for each. These entail finding (i) a single supporting fact for a question and (ii) two or (iii) three supporting statements, chained together to compose a coherent line of reasoning. Finally, for Natural Language Inference (NLI): The SNLI dataset (Bowman et al., 2015). 570k human-written English sentence pairs manually labeled for balanced classification with the labels neutral, contradiction, and entailment, supporting the task of natural language inference (NLI). In this work, we generate an attention distribution over premise words conditioned on the hidden representation induced for the hypothesis. We restrict ourselves to comparatively simple instantiations of attention mechanisms, as described in the preceding section. This means we do not consider recently proposed ‘BiAttentive’ architectures that attend to tokens in the respective inputs, condition"
N19-1357,D18-1407,0,0.133241,".0 0.5 (c) Diabetes (BiLSTM) 0.80.3 0.15 0.6 0.2 0.10 0.4 0.1 0.05 0.2 0 0 0.6 0.8 1.0 0.0 0.00 1 0.4 1 1 10.0 0.0 0.2 (f) SNLI (Average) 0.60 0 0.8 0.0 1.0 0.5 1.0 (d) Diabetes (Average) 0.8 0.20 0.6 0.15 0.4 0.10 0.2 0.05 1 0.0 1 0.0 (g) CNN-QA (BiLSTM) 0.5 0.0 1.0 0.00 1.0 1.0 1.0 0.8 0.6 0.4 0.2 0.21 0.4 0.6 0 0.8 1.0 1 0.0 0.0 0.2 (h) BAbI 1 (BiLSTM) Figure 2: Histogram of Kendall τ between attention and gradients. Encoder variants are denoted parenthetically; colors indicate predicted classes. Exhaustive results are available for perusal online. Best viewed in color. ral model behavior (Feng et al., 2018), they do provide measures of individual feature importance with known semantics (Ross et al., 2017). It is thus instructive to ask whether these measures correlate with attention weights. The process we follow to quantify this is described in Algorithm 1. We denote the input resulting from removing the word at position t in x by x−t . Note that we disconnect the computation graph at the attention module so that the gradient does not flow through this layer. Algorithm 1 Feature Importance Computations h ← Enc(x), α ˆ ← softmax(φ(h, Q)) yˆ ← Dec(h, α) P|V | gt ← |w=1 1[xtw = 1] ∂x∂ytw |, ∀t ∈ ["
N19-1357,D18-1537,0,0.0545614,"neural NLP models. These include approaches that measure feature importance based on gradient information (Ross et al., 2017; Sundararajan et al., 2017) (aligned with the gradient-based measures that we have used here), and methods based on representation erasure (Li et al., 2016), in which dimensions are removed and then the resultant change in output is recorded (similar to our experiments with removing tokens from inputs, albeit we do this at the input layer). Comparing such importance measures to attention scores may provide additional insights into the working of attention based models (Ghaeini et al., 2018). Another novel line of work in this direction involves explicitly identifying explanations of black-box predictions via a causal framework (Alvarez-Melis and Jaakkola, 2017). We also note that there has been complementary work demonstrating correlation between human attention and induced attention weights, which was relatively strong when humans agreed on an explanation (Pappas and Popescu-Belis, 2016). It would be interesting to explore if such cases present explicit ‘high precision’ signals in the text (for example, the positive label in diabetes dataset). More specific to attention mechani"
N19-1357,D16-1011,0,0.186945,"Missing"
N19-1357,P11-1015,0,0.108316,".94 0.96 0.79 0.92 0.64 1.0 / 0.65 / 0.64 0.78 Table 1: Dataset characteristics. For train and test size, we list the cardinality for each class, where applicable: 0/1 for binary classification (top), and 0 / 1 / 2 for NLI (bottom). Average length is in tokens. Test metrics are F1 score, accuracy, and micro-F1 for classification, QA, and NLI, respectively; all correspond to performance using a BiLSTM encoder. We note that results using convolutional and average (i.e., non-recurrent) encoders are comparable for classification though markedly worse for QA tasks. IMDB Large Movie Reviews Corpus (Maas et al., 2011). Binary sentiment classification dataset containing 50,000 polarized (positive or negative) movie reviews, split into half for training and testing. Twitter Adverse Drug Reaction dataset (Nikfarjam et al., 2015). A corpus of ∼8000 tweets retrieved from Twitter, annotated by domain experts as mentioning adverse drug reactions. 20 Newsgroups (Hockey vs Baseball). Collection of ∼20,000 newsgroup correspondences, partitioned (nearly) evenly across 20 categories. We extract instances belonging to baseball and hockey, which we designate as 0 and 1, respectively, to derive a binary classification ta"
N19-1357,W16-6213,0,0.0184361,"with removing tokens from inputs, albeit we do this at the input layer). Comparing such importance measures to attention scores may provide additional insights into the working of attention based models (Ghaeini et al., 2018). Another novel line of work in this direction involves explicitly identifying explanations of black-box predictions via a causal framework (Alvarez-Melis and Jaakkola, 2017). We also note that there has been complementary work demonstrating correlation between human attention and induced attention weights, which was relatively strong when humans agreed on an explanation (Pappas and Popescu-Belis, 2016). It would be interesting to explore if such cases present explicit ‘high precision’ signals in the text (for example, the positive label in diabetes dataset). More specific to attention mechanisms, recent promising work has proposed more principled attention variants designed explicitly for interpretability; these may provide greater transparency by imposing hard, sparse attention. Such instantiations explicitly select (modest) subsets of inputs to be considered when making a prediction, which are then by construction responsible for model output (Lei et al., 2016; Peters et al., 2018). Struc"
N19-1357,D16-1244,0,0.0441206,"lish sentence pairs manually labeled for balanced classification with the labels neutral, contradiction, and entailment, supporting the task of natural language inference (NLI). In this work, we generate an attention distribution over premise words conditioned on the hidden representation induced for the hypothesis. We restrict ourselves to comparatively simple instantiations of attention mechanisms, as described in the preceding section. This means we do not consider recently proposed ‘BiAttentive’ architectures that attend to tokens in the respective inputs, conditioned on the other inputs (Parikh et al., 2016; Seo et al., 2016; Xiong et al., 2016). Table 1 provides summary statistics for all datasets, as well as the observed test performances for additional context. 4 Experiments We run a battery of experiments that aim to examine empirical properties of learned attention weights and to interrogate their interpretability and transparency. The key questions are: Do 3545 Dataset SST IMDB ADR Tweets 20News AG News Diabetes Anemia CNN bAbI 1 bAbI 2 bAbI 3 SNLI Class 0 1 0 1 0 1 0 1 0 1 0 1 0 1 Overall Overall Overall Overall 0 1 2 Gradient (BiLSTM) τg Mean ± Std. Sig. Frac. 0.34 ± 0.21 0.48 0.36 ± 0.2"
N19-1357,W18-5450,0,0.0564056,"Missing"
N19-1357,D13-1170,0,0.0166107,"over these: α softmax(φ(h, Q)) ∈ RT . In this work we consider two common similarity functions: Additive φ(h, Q) = vT tanh(W1 h + W2 Q) (Bahdanau et al., 2014) and Scaled Dot-Product φ(h, Q) = hQ √ (Vaswani et al., 2017), where v, W1 , W2 are m model parameters. Finally, a dense layer Dec with parameters θ consumes a weighted instance representation and yields P a prediction yˆ = σ(θ · hα ) ∈ R|Y |, where hα = Tt=1 α ˆ t · ht ; σ is an output activation function; and |Y |denotes the label set size. 3 Datasets and Tasks For binary text classification, we use: Stanford Sentiment Treebank (SST) (Socher et al., 2013). 10,662 sentences tagged with sentiment on a scale from 1 (most negative) to 5 (most positive). We filter out neutral instances and dichotomize the remaining sentences into positive (4, 5) and negative (1, 2). 3 While attention is perhaps most common in seq2seq tasks like translation, our impression is that interpretability is not typically emphasized for such tasks, in general. 4 In the latter case, ht is the embedding of token t after being passed through a linear layer and ReLU activation. 3544 Dataset SST IMDB ADR Tweets 20 Newsgroups AG News Diabetes (MIMIC) Anemia (MIMIC) CNN bAbI (Task"
N19-1357,P17-1088,0,0.0478252,"are now a near-ubiquitous component of neural NLP architectures. Attention weights are often claimed (implicitly or explicitly) to afford insights into the “inner-workings” of models: for a given output one can inspect the inputs to which the model assigned large attention weights. Li et al. (2016) summarized this commonly held view in NLP: “Attention provides an important way to explain the workings of neural models"". Indeed, claims that attention provides interpretability are common in the literature, e.g., (Xu et al., 2015; Choi et al., 2016; Lei et al., 2017; Martins and Astudillo, 2016; Xie et al., 2017).1 Implicit in this is the assumption that the input units (e.g., words) accorded high attention weights are responsible for model outputs. But as far as we are aware, this assumption has not been formally evaluated, and our findings here suggest that it is problematic. More specifically, we empirically investigate the relationship between attention weights, inputs, and outputs. Assuming attention provides an explanation for model predictions, we might expect the following properties to hold. (i) Attention weights should correlate with feature importance measures (e.g., gradient-based measures"
N19-1357,D16-1076,1,0.830386,"attention weights (right). Despite being quite dissimilar, these both yield effectively the same prediction (0.01). Introduction and Motivation Attention mechanisms (Bahdanau et al., 2014) induce conditional distributions over input units to compose a weighted context vector for downstream modules. These are now a near-ubiquitous component of neural NLP architectures. Attention weights are often claimed (implicitly or explicitly) to afford insights into the “inner-workings” of models: for a given output one can inspect the inputs to which the model assigned large attention weights. Li et al. (2016) summarized this commonly held view in NLP: “Attention provides an important way to explain the workings of neural models"". Indeed, claims that attention provides interpretability are common in the literature, e.g., (Xu et al., 2015; Choi et al., 2016; Lei et al., 2017; Martins and Astudillo, 2016; Xie et al., 2017).1 Implicit in this is the assumption that the input units (e.g., words) accorded high attention weights are responsible for model outputs. But as far as we are aware, this assumption has not been formally evaluated, and our findings here suggest that it is problematic. More specifi"
N19-1371,D15-1075,0,0.248725,"el is provided with a prompt that specifies an intervention, a comparator, and an outcome, along with a fulltext article. The model is then to infer the reported findings with respect to this prompt (Figure 1). From a healthcare perspective, this inference task is an essential step for automating extraction of actionable evidence from trial reports. 3705 Proceedings of NAACL-HLT 2019, pages 3705–3717 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics From an NLP standpoint, the proposed task can be seen as an instance of natural language inference (Bowman et al., 2015), viewing the article and prompt as the premise and hypothesis, respectively. However, the problem differs in a few important ways from existing NLP formulations. First, the inputs: prompts are brief (∼13.5 words on average), but articles are long (∼4200 words). Further, only a few snippets of the article will be relevant to the label for a given prompt. Second, prompts in this domain are structured, and include only a few types of key information: interventions, comparators, and outcomes. Methods that exploit this regularity are likely to be more accurate than generic inference algorithms. An"
N19-1371,P17-1020,0,0.0489168,"Missing"
N19-1371,C00-1043,0,0.344392,"Missing"
N19-1371,D15-1050,0,0.0254011,"uced due to shifting objectives. 3712 0.95 0.90 Evidence token AUC 0.85 0.80 0.75 0.70 0.65 0.60 0.55 cond-attn + pretraining cond-attn attn + pretraining attn 40 20 0 Epoch 20 40 Figure 6: Validation evidence token AUCs during training. ‘pretraining’ epochs are depicted as ‘negative’ for the two explicitly supervised attention variants. Note that we use early stopping, so not all models run for the same number of epochs. 7 Related Work The proposed task is situated at the intersection of information extraction (Cardie, 1997), natural language inference (Bowman et al., 2015), evidence mining (Rinott et al., 2015) and question answering (Harabagiu et al., 2000; Hovy et al., 2000). However, our focus on inferring results from lengthy clinical trial reports pertaining to particular prompts constitutes a unique problem, as discussed in the Introduction. Prior systems have attempted to extract information from articles describing RCTs. For example, ExaCT (Kiritchenko et al., 2010) attempts to extract variables describing clinical trials from articles, and ACRES (Summerscales et al., 2011) ingests extracts key variables from abstracts. Blake and Lucic (2015; 2012) considered the problem of automatically ext"
N19-1371,N16-1174,0,0.0513476,"Missing"
N19-1371,N07-1033,0,0.21953,"icle and ICO frame into a vector [a; i; c; o] which is then passed through a feedforward network with a single hidden layer to allow interactions between the prompt and article text.4 As discussed in detail below, we experiment with a variety of attention mechanisms imposed over article tokens. 4.2 Finding the Evidence Exploiting the spans of evidence marked as supporting assessments should improve the predictive performance of models. An additional advantage of modeling this explicitly is that models will then be able to provide rationales for decisions (Lei et al., 2016; Zhang et al., 2016; Zaidan et al., 2007), i.e., snippets of text that support predictions. We therefore experiment with model variants that classify input tokens as being relevant evidence (or not) prior to performing inference. We consider both pipeline and joint instantiations of such models. In the former type, the model first identifies spans in the text and then passes these forward to an independent component that makes predictions on the basis of these. In models of the latter type, evidence span tagging and document-level inference is performed 4 We use a linear hidden layer; experiments adding a nonlinearity (ReLU) did not"
N19-1371,D16-1076,1,0.877833,"nate the encoded article and ICO frame into a vector [a; i; c; o] which is then passed through a feedforward network with a single hidden layer to allow interactions between the prompt and article text.4 As discussed in detail below, we experiment with a variety of attention mechanisms imposed over article tokens. 4.2 Finding the Evidence Exploiting the spans of evidence marked as supporting assessments should improve the predictive performance of models. An additional advantage of modeling this explicitly is that models will then be able to provide rationales for decisions (Lei et al., 2016; Zhang et al., 2016; Zaidan et al., 2007), i.e., snippets of text that support predictions. We therefore experiment with model variants that classify input tokens as being relevant evidence (or not) prior to performing inference. We consider both pipeline and joint instantiations of such models. In the former type, the model first identifies spans in the text and then passes these forward to an independent component that makes predictions on the basis of these. In models of the latter type, evidence span tagging and document-level inference is performed 4 We use a linear hidden layer; experiments adding a nonlin"
N19-1371,D16-1011,1,\N,Missing
N19-1371,D18-1546,0,\N,Missing
N19-1371,W12-4301,0,\N,Missing
P14-2084,W13-1104,0,0.25049,"ve new research on methods for verbal irony detection. 1 https://github.com/bwallace/ ACL-2014-irony 512 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 512–516, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics 2 Previous Work There has recently been a flurry of interesting work on automatic irony detection (Tepperman et al., 2006; Davidov et al., 2010; Carvalho et al., 2009; Burfoot and Baldwin, 2009; Tsur et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011; Filatova, 2012; Reyes et al., 2012; Lukin and Walker, 2013; Riloff et al., 2013). In these works, verbal irony detection has mostly been treated as a standard text classification task, though with some innovative approaches specific to detecting irony. 1 2 4 3 Figure 1: The web-based tool used by our annotators to label reddit comments. Enumerated interface elements are described as follows: 1 the text of the comment to be annotated – sentences marked as ironic are highlighted; 2 buttons to label sentences as ironic or unironic; 3 buttons to request additional context (the embedding discussion thread or associated webpage – see Section 3.2); 4 radio"
P14-2084,D13-1066,0,0.658212,"ds for verbal irony detection. 1 https://github.com/bwallace/ ACL-2014-irony 512 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 512–516, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics 2 Previous Work There has recently been a flurry of interesting work on automatic irony detection (Tepperman et al., 2006; Davidov et al., 2010; Carvalho et al., 2009; Burfoot and Baldwin, 2009; Tsur et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011; Filatova, 2012; Reyes et al., 2012; Lukin and Walker, 2013; Riloff et al., 2013). In these works, verbal irony detection has mostly been treated as a standard text classification task, though with some innovative approaches specific to detecting irony. 1 2 4 3 Figure 1: The web-based tool used by our annotators to label reddit comments. Enumerated interface elements are described as follows: 1 the text of the comment to be annotated – sentences marked as ironic are highlighted; 2 buttons to label sentences as ironic or unironic; 3 buttons to request additional context (the embedding discussion thread or associated webpage – see Section 3.2); 4 radio button to provide conf"
P14-2084,P09-2041,0,0.0486233,"dgements for this task, so too do computers. Our hope is that these observations and this dataset will spur innovative new research on methods for verbal irony detection. 1 https://github.com/bwallace/ ACL-2014-irony 512 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 512–516, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics 2 Previous Work There has recently been a flurry of interesting work on automatic irony detection (Tepperman et al., 2006; Davidov et al., 2010; Carvalho et al., 2009; Burfoot and Baldwin, 2009; Tsur et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011; Filatova, 2012; Reyes et al., 2012; Lukin and Walker, 2013; Riloff et al., 2013). In these works, verbal irony detection has mostly been treated as a standard text classification task, though with some innovative approaches specific to detecting irony. 1 2 4 3 Figure 1: The web-based tool used by our annotators to label reddit comments. Enumerated interface elements are described as follows: 1 the text of the comment to be annotated – sentences marked as ironic are highlighted; 2 buttons to label sentences as ironic or unironic; 3 buttons"
P14-2084,walker-etal-2012-corpus,0,0.0599253,"es marked as ironic are highlighted; 2 buttons to label sentences as ironic or unironic; 3 buttons to request additional context (the embedding discussion thread or associated webpage – see Section 3.2); 4 radio button to provide confidence in comment labels (low, medium or high). The most common data source used to experiment with irony detection systems has been Twitter (Reyes et al., 2012; Gonz´alez-Ib´an˜ ez et al., 2011; Davidov et al., 2010), though Amazon product reviews have been used experimentally as well (Tsur et al., 2010; Davidov et al., 2010; Reyes et al., 2012; Filatova, 2012). Walker et al. (2012) also recently introduced the Internet Argument Corpus (IAC), which includes a sarcasm label (among others). 3 Introducing the reddit Irony Dataset Here we introduce the first version (β 1.0) of our irony corpus. Reddit (http://reddit. com) is a social-news website to which news stories (and other links) are posted, voted on and commented upon. The forum component of reddit is extremely active: popular posts often have well into 1000’s of user comments. Reddit comprises ‘sub-reddits’, which focus on specific topics. For example, http:// reddit.com/r/politics features articles (and hence commen"
P14-2084,filatova-2012-irony,0,\N,Missing
P14-2084,P11-2102,0,\N,Missing
P15-1100,P09-2041,0,0.0373675,"uz does not find general support even in this community. Example comments include: “Stay classy Ted Cruz” and “Great idea on the talkathon Cruz”. The ‘mr’ and ‘king’ terms are almost exclusively references to Obama in the conservative subreddit. In any case, because these are three-way interaction terms, they are all relatively rare: therefore we would caution against over interpretation here. 6 Related Work The task of automated irony detection has recently received a great deal of attention from the NLP and ML communities (Tepperman et al., 2006; Davidov et al., 2010; Carvalho et al., 2009; Burfoot and Baldwin, 2009; Tsur et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011; Filatova, 2012; Reyes et al., 2012; Lukin and Walker, 2013; Riloff et al., 2013). This work has mostly focussed on exploiting token4 ‘Ollie’ is a conservative political commentator. based indicators of verbal irony. For example, it is clear that gratuitous punctuation (e.g. “oh really??!!!”) signals irony (Carvalho et al., 2009). Davidov et al. (2010) proposed a semisupervised approach in which they look for sentence templates indicative of irony. Elsewhere, Riloff et al. (2013) proposed a method that exploits apparently contrasting sentim"
P15-1100,D13-1066,0,0.443222,"terpretation of verbal irony supports this supposition (Grice, 1975; Clark and Gerrig, 1984; Wallace, 2013; Wallace et al., 2014). Individuals will be more likely, in general, to use sarcasm when discussing specific entities. Which entities will depend in part on the community to which the individual belongs. As a proxy for user community, here we leverage the subreddits to which comments were posted. Sentiment may also play an important role. In general, verbal irony is almost always used to convey negative views via ostensibly positive utterances (Sperber and Wilson, 1981). And recent work (Riloff et al., 2013) has exploited features based on sentiment to improve irony detection. To summarize: when assuming an ironic voice we expect that individuals will convey ostensibly positive sentiment about entities, and that these entities will depend on the type of individual in question. We propose capitalizing on such information by introducing features that encode subreddits, sentiment and noun phrases (NNPs), as we describe next. 2.2 Features We leverage the feature sets enumerated in Table 1. Subreddits are observed variables. Noun phrase (NNP) extraction and sentiment inference are performed automatica"
P15-1100,D13-1170,0,0.0025008,"ng an ironic voice we expect that individuals will convey ostensibly positive sentiment about entities, and that these entities will depend on the type of individual in question. We propose capitalizing on such information by introducing features that encode subreddits, sentiment and noun phrases (NNPs), as we describe next. 2.2 Features We leverage the feature sets enumerated in Table 1. Subreddits are observed variables. Noun phrase (NNP) extraction and sentiment inference are performed automatically via state of the art NLP tools. In particular, we use the Stanford Sentiment Analysis tool (Socher et al., 2013) to infer sentiment. To extract NNPs we use the Stanford 1036 Feature Sentiment Subreddit NNP NNP+ Description The inferred sentiment (negative/neutral or positive) for a given comment. the subreddit (e.g., progressive or conservative; atheism or Christianity) to which a comment was posted. Noun phrases (e.g., proper nouns) extracted from comment texts. Noun phrases extracted from comment texts and the thread to which they belong (for example, ‘Obamacare’ from the title in Figure 1). 3 Enforcing sparsity 3.1 Preliminaries In this work we consider linear models with binary outputs (y ∈ {−1, +1}"
P15-1100,N03-1033,0,0.039938,"ample, ‘Obamacare’ from the title in Figure 1). 3 Enforcing sparsity 3.1 Preliminaries In this work we consider linear models with binary outputs (y ∈ {−1, +1}). We will assume we have access to a training dataset comprising n instances, x = {x1 , ..., xn } and associated labels y = {y1 , ..., yn }. We then aim to find a weightvector w that optimizes the following objective. Table 1: Feature types that we exploit. We view the (observed) subreddit as a proxy for user type. We combine this with sentiment and extracted noun phrases (NNPs) to improve classifier performance. Part of Speech tagger (Toutanova et al., 2003). We then introduce ‘bag-of-NNP’ features and features that indicate whether the sentiment inferred for a given sentence was positive or not. Additionally, we introduce ‘interaction’ features that capture combinations of these. For example, a feature that indicates whether a given sentence mentions Obamacare (which will be one of many NNPs automatically extracted) and was posted in the conservative subreddit. This is an example of a two-way interaction. We also experiment with three-way interactions, crossing sentiment with NNPs and subreddits. An example is a feature that indicates if a sente"
P15-1100,P11-1137,0,0.0109233,"utterance to detect irony. While innovative, these approaches still rely on features intrinsic to comments; i.e., they do not attempt to capitalize on contextualizing features external to the comment text. This means that there will necessarily be certain (subtle) ironies that escape detection by such approaches. For example, without any additional information about the speaker, it would be impossible to deduce whether the comment “Obamacare is a great program” is intended sarcastically. Other related recent work has shown the promise of sparse models, both for prediction and interpretation (Eisenstein et al., 2011a; Eisenstein et al., 2011b; Yogatama and Smith, 2014a). Yogatama (2014a; 2014b), e.g., has leveraged the group lasso approach to impose ‘structured’ sparsity on feature weights. Our work here may similarly be viewed as assuming a specific sparsity pattern (specifically that feature weights for ‘interaction features’ will be sparse) and expressing this via regularization. 7 Conclusions and Future Directions We have shown that we can leverage contextualizing information to improve identification of verbal irony in online comments. This is in contrast to previous models, which have relied predom"
P15-1100,filatova-2012-irony,0,0.0232235,"e: “Stay classy Ted Cruz” and “Great idea on the talkathon Cruz”. The ‘mr’ and ‘king’ terms are almost exclusively references to Obama in the conservative subreddit. In any case, because these are three-way interaction terms, they are all relatively rare: therefore we would caution against over interpretation here. 6 Related Work The task of automated irony detection has recently received a great deal of attention from the NLP and ML communities (Tepperman et al., 2006; Davidov et al., 2010; Carvalho et al., 2009; Burfoot and Baldwin, 2009; Tsur et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011; Filatova, 2012; Reyes et al., 2012; Lukin and Walker, 2013; Riloff et al., 2013). This work has mostly focussed on exploiting token4 ‘Ollie’ is a conservative political commentator. based indicators of verbal irony. For example, it is clear that gratuitous punctuation (e.g. “oh really??!!!”) signals irony (Carvalho et al., 2009). Davidov et al. (2010) proposed a semisupervised approach in which they look for sentence templates indicative of irony. Elsewhere, Riloff et al. (2013) proposed a method that exploits apparently contrasting sentiment in the same utterance to detect irony. While innovative, these ap"
P15-1100,P11-2102,0,0.411338,"e comments include: “Stay classy Ted Cruz” and “Great idea on the talkathon Cruz”. The ‘mr’ and ‘king’ terms are almost exclusively references to Obama in the conservative subreddit. In any case, because these are three-way interaction terms, they are all relatively rare: therefore we would caution against over interpretation here. 6 Related Work The task of automated irony detection has recently received a great deal of attention from the NLP and ML communities (Tepperman et al., 2006; Davidov et al., 2010; Carvalho et al., 2009; Burfoot and Baldwin, 2009; Tsur et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011; Filatova, 2012; Reyes et al., 2012; Lukin and Walker, 2013; Riloff et al., 2013). This work has mostly focussed on exploiting token4 ‘Ollie’ is a conservative political commentator. based indicators of verbal irony. For example, it is clear that gratuitous punctuation (e.g. “oh really??!!!”) signals irony (Carvalho et al., 2009). Davidov et al. (2010) proposed a semisupervised approach in which they look for sentence templates indicative of irony. Elsewhere, Riloff et al. (2013) proposed a method that exploits apparently contrasting sentiment in the same utterance to detect irony. While inno"
P15-1100,W13-1104,0,0.195272,"ities (external the comment text itself) provide contextual signals indicating that the shown comment was intended ironically. As we shall see, Obamacare is in general a strong indicator of irony when present in posts to the conservative subreddit, but less so in posts to the progressive subreddit. Introduction and Motivation Automated verbal irony detection is a challenging problem.1 But recognizing when an author has intended a statement ironically is practically important for many text classification tasks (e.g., sentiment detection). Previous models for irony detection (Tsur et al., 2010; Lukin and Walker, 2013; Riloff et al., 1 In this paper we will be a bit cavalier in using the terms ‘verbal irony’ and ‘sarcasm’ interchangeably. We recognize that the latter is a special type of the former, the definition of which is difficult to pin down precisely. 2013) have relied predominantly on features intrinsic to the texts to be classified. By contrast, here we propose exploiting contextualizing information, which is often available for web-based classification tasks. More specifically, we exploit signal gleaned from the conversational threads to which comments belong. Our approach capitalizes on the intu"
P15-1100,P09-1054,0,0.0243295,"feature weights, which is in contrast to the elastic net, which imposes the composite penalty to all feature weights. One can view this as using the regularizer to encourage a sparsity pattern specific to the task at hand. 2 Note that we apply both `1 and `2 penalties to the features in I and T . 3.3 Inference We fit this model via Stochastic Gradient Descent (SGD).3 During each update, we impose both the squared `2 and `1 penalties; the latter is applied only to the contextual/interaction features in I and T . For the `1 penalty, we adopt the cumulative truncated gradient method proposed by Tsuruoka et al. (2009). 4 4.1 Experimental Setup Datasets For our development dataset, we used a subset of the reddit irony corpus (Wallace et al., 2014) comprising annotated comments from the progressive and conservative subreddits. We also report results from experiments performed using a separate, held-out portion of this data, which we did not use during model refinement. Furthermore, we later present results on comments from the atheism and Christianity subreddits (we did not use this data during model development, either). The development dataset includes 1,825 annotated comments (876 and 949 from the progres"
P15-1100,P14-2084,1,0.700577,"(or can be derived) for comments posted to different mediums on the web: for example on Twitter we know who a user follows; and on YouTube we know the channels to which videos belong. 2 2.1 Exploiting context Communities and sentiment As discussed above, a shortcoming with existing models for detecting sarcasm/verbal irony on the web is their failure to capitalize on contextualizing information. But such information is critical to discerning irony. A large body of work on the use and interpretation of verbal irony supports this supposition (Grice, 1975; Clark and Gerrig, 1984; Wallace, 2013; Wallace et al., 2014). Individuals will be more likely, in general, to use sarcasm when discussing specific entities. Which entities will depend in part on the community to which the individual belongs. As a proxy for user community, here we leverage the subreddits to which comments were posted. Sentiment may also play an important role. In general, verbal irony is almost always used to convey negative views via ostensibly positive utterances (Sperber and Wilson, 1981). And recent work (Riloff et al., 2013) has exploited features based on sentiment to improve irony detection. To summarize: when assuming an ironic"
P15-1100,P14-1074,0,0.0119202,"approaches still rely on features intrinsic to comments; i.e., they do not attempt to capitalize on contextualizing features external to the comment text. This means that there will necessarily be certain (subtle) ironies that escape detection by such approaches. For example, without any additional information about the speaker, it would be impossible to deduce whether the comment “Obamacare is a great program” is intended sarcastically. Other related recent work has shown the promise of sparse models, both for prediction and interpretation (Eisenstein et al., 2011a; Eisenstein et al., 2011b; Yogatama and Smith, 2014a). Yogatama (2014a; 2014b), e.g., has leveraged the group lasso approach to impose ‘structured’ sparsity on feature weights. Our work here may similarly be viewed as assuming a specific sparsity pattern (specifically that feature weights for ‘interaction features’ will be sparse) and expressing this via regularization. 7 Conclusions and Future Directions We have shown that we can leverage contextualizing information to improve identification of verbal irony in online comments. This is in contrast to previous models, which have relied predominantly on features that are intrinsic to the texts t"
P17-1028,C02-1025,0,0.0483385,"those from domain experts (Snow et al., 2008). It 1 Soure code and biomedical abstract data: www.github.com/thanhan/seqcrowd-acl17, www.byronwallace.com/EBM_abstracts_data 299 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 299–309 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1028 model from a hidden state to an observation and a transition model from a hidden state to the next hidden state. Later work focused on discriminative models such as Maximum Entropy Models (Chieu and Ng, 2002) and Conditional Random Fields (CRFs) (Lafferty et al., 2001). These were able to achieve strong predictive performance by exploiting arbitrary features, but they may not be the best choice for label aggregation. Also, compared to the simple HMM model, discriminative sequentially structured models require more complex optimization and are generally more difficult to extend. Here we argue for the generative HMMs for our first task of aggregating crowd labels. The generative nature of HMMs is a good fit for existing crowd modeling techniques and also enables very efficient parameter estimation."
P17-1028,D15-1261,0,0.2839,"Missing"
P17-1028,D07-1031,0,0.477994,"sequences in unannotated text given a training set of crowd annotations (Task 2). As part of this work, we propose novel models for working with noisy sequence labels from the crowd. Reported experiments both benchmark existing state-of-the-art approaches (sequential and non-sequential) and show that our proposed models achieve best-in-class performance. As noted in the Abstract, we have also shared our sourcecode and data online for use by the community. In addition to the supervised setting, previous work has studied unsupervised HMMs, e.g., for PoS induction (Goldwater and Griffiths, 2007; Johnson, 2007). These works are similar to our work in trying to infer the hidden states without labeled data. Our graphical model is different in incorporating signal from the crowd labels. For Task 2 (training predictive models), we consider CRFs and LSTMs. CRFs are undirected, conditional models that can exploit arbitrary features. They have achieved strong performance on many sequence labeling tasks (McCallum and Li, 2003), but they depend on hand-crafted features. Recent work has considered end-to-end neural architectures that learn features, e.g., Convolutional Neural Networks (CNNs) (Collobert et al."
P17-1028,N15-1089,0,0.0163622,"al., 2016). Here we modify the LSTM model proposed by Lample et al. (2016) by augmenting the network with ‘crowd worker vectors’. We briefly review two separate threads of relevant prior work: (1) sequence labeling models; and (2) aggregation of crowdsourcing annotations. Crowdsourcing. Acquiring labeled data is critical for training supervised models. Snow et al. (2008) proposed using Amazon Mechanical Turk to collect labels in NLP quickly and at low cost, albeit with some degradation in quality. Subsequent work has developed models for improving aggregate label quality (Raykar et al., 2010; Felt et al., 2015; Kajino et al., 2012; Bi et al., 2014; Liu et al., 2012; Hovy et al., 2013). Sheshadri and Lease (2013) survey and benchmark methods. Sequence labeling. Early work on learning for sequential tasks used HMMs (Bikel et al., 1997). HMMs are a class of generative probabilistic models comprising two components: an emission However, these models are almost all in the binary or multiclass classification setting; only a few have considered sequence labeling. Dredze et al. (2009) proposed a method for learning a CRF 2 Related Work 300 model from multiple labels (although the identities of the annotato"
P17-1028,D14-1181,0,0.00550015,"works are similar to our work in trying to infer the hidden states without labeled data. Our graphical model is different in incorporating signal from the crowd labels. For Task 2 (training predictive models), we consider CRFs and LSTMs. CRFs are undirected, conditional models that can exploit arbitrary features. They have achieved strong performance on many sequence labeling tasks (McCallum and Li, 2003), but they depend on hand-crafted features. Recent work has considered end-to-end neural architectures that learn features, e.g., Convolutional Neural Networks (CNNs) (Collobert et al., 2011; Kim, 2014; Zhang and Wallace, 2015) and LSTMs (Lample et al., 2016). Here we modify the LSTM model proposed by Lample et al. (2016) by augmenting the network with ‘crowd worker vectors’. We briefly review two separate threads of relevant prior work: (1) sequence labeling models; and (2) aggregation of crowdsourcing annotations. Crowdsourcing. Acquiring labeled data is critical for training supervised models. Snow et al. (2008) proposed using Amazon Mechanical Turk to collect labels in NLP quickly and at low cost, albeit with some degradation in quality. Subsequent work has developed models for improvin"
P17-1028,W10-0713,0,0.039515,"Missing"
P17-1028,P07-1094,0,0.0246539,"ask 1) and how to best predict sequences in unannotated text given a training set of crowd annotations (Task 2). As part of this work, we propose novel models for working with noisy sequence labels from the crowd. Reported experiments both benchmark existing state-of-the-art approaches (sequential and non-sequential) and show that our proposed models achieve best-in-class performance. As noted in the Abstract, we have also shared our sourcecode and data online for use by the community. In addition to the supervised setting, previous work has studied unsupervised HMMs, e.g., for PoS induction (Goldwater and Griffiths, 2007; Johnson, 2007). These works are similar to our work in trying to infer the hidden states without labeled data. Our graphical model is different in incorporating signal from the crowd labels. For Task 2 (training predictive models), we consider CRFs and LSTMs. CRFs are undirected, conditional models that can exploit arbitrary features. They have achieved strong performance on many sequence labeling tasks (McCallum and Li, 2003), but they depend on hand-crafted features. Recent work has considered end-to-end neural architectures that learn features, e.g., Convolutional Neural Networks (CNNs) ("
P17-1028,N16-1030,0,0.434422,"beling of unannotated text is typically preferable, as it is more efficient, scalable, and cost-effective. Given a training set of crowd labels, how can we best predict sequences in unannotated text? Should we: (i) consider Task 1 as a pre-processing step and train the model using consensus labels; or (ii) instead directly train the model on all of the individual annotations, as done by Yang et al. (2010)? We investigate both directions in this work. Our approach is to augment existing sequence labeling models such as HMMs (Rabiner and Juang, 1986) and LSTMs (Hochreiter and Schmidhuber, 1997; Lample et al., 2016) by introducing an explicit ”crowd component”. For HMMs, we model this crowd component by including additional parameters for worker label quality and crowd label variables. For the LSTM, we introduce a vector representation for each annotator. In Despite sequences being core to NLP, scant work has considered how to handle noisy sequence labels from multiple annotators for the same text. Given such annotations, we consider two complementary tasks: (1) aggregating sequential crowd labels to infer a best single set of consensus annotations; and (2) using crowd annotations as training data for a"
P17-1028,N13-1132,0,0.76565,"by augmenting the network with ‘crowd worker vectors’. We briefly review two separate threads of relevant prior work: (1) sequence labeling models; and (2) aggregation of crowdsourcing annotations. Crowdsourcing. Acquiring labeled data is critical for training supervised models. Snow et al. (2008) proposed using Amazon Mechanical Turk to collect labels in NLP quickly and at low cost, albeit with some degradation in quality. Subsequent work has developed models for improving aggregate label quality (Raykar et al., 2010; Felt et al., 2015; Kajino et al., 2012; Bi et al., 2014; Liu et al., 2012; Hovy et al., 2013). Sheshadri and Lease (2013) survey and benchmark methods. Sequence labeling. Early work on learning for sequential tasks used HMMs (Bikel et al., 1997). HMMs are a class of generative probabilistic models comprising two components: an emission However, these models are almost all in the binary or multiclass classification setting; only a few have considered sequence labeling. Dredze et al. (2009) proposed a method for learning a CRF 2 Related Work 300 model from multiple labels (although the identities of the annotators or workers were not used). Rodrigues et al. (2014) extended this approach"
P17-1028,P14-2062,0,0.0816103,"ustin, 2 Northeastern University, 3 University of Pennsylvania, atn@cs.utexas.edu, byron@ccs.neu.edu, {ljunyi|nenkova}@seas.upenn.edu, ml@utexas.edu Abstract is therefore essential to model crowdsourced label quality, both to estimate individual annotator reliability and to aggregate individual annotations to induce a single set of “reference standard” consensus labels. While many models have been proposed for aggregating crowd labels for binary or multiclass classification problems (Sheshadri and Lease, 2013), far less work has explored crowdbased annotation of sequences (Finin et al., 2010; Hovy et al., 2014; Rodrigues et al., 2014). In this paper, we investigate two complementary challenges in using sequential crowd labels: how to best aggregate them (Task 1); and how to accurately predict sequences in unannotated text given training data from the crowd (Task 2). For aggregation, one might want to induce a single set of high-quality consensus annotations for various purposes: (i) for direct use at run-time (when a given application requires human-level accuracy in identifying sequences); (ii) for sharing with others; or (iii) for training a predictive model. When human-level accuracy in tagging"
P17-1028,D08-1027,0,0.39838,"Missing"
P17-1028,A97-1029,0,\N,Missing
P17-2024,P14-1074,0,0.0488409,"drawback to learning from scratch in end-to-end neural models is a failure to capitalize on existing knowledge sources. There have been efforts to exploit such resources specifically to induce better word vectors (Yu and Dredze, 2014; Faruqui et al., 2014; Yu et al., 2016; Xu et al., 2014). But these models do not attempt to exploit external resources jointly during training for a particular downstream task (which uses word embeddings as inputs), as we do here. Past work on sparse linear models has shown the potential of exploiting linguistic knowledge in statistical NLP models. For example, Yogatama and Smith (2014) used external resources to inform structured, grouped regularization of loglinear text classification models, yielding improvements over standard regularization approaches. Elsewhere, Doshi-Velez et al. (2015) proposed a variant of LDA that exploits a priori known treeResults We replicate each experiment five times (each is a 10-fold cross validation), and report the mean (min, max) across these replications. Results on the sentiment and biomedical corpora in are presented in Tables 2 and 3, respectively.12 These exploit different external resources to induce the word groupings that in turn i"
P17-2024,P14-2089,0,0.0349429,"s this. 5 Related Work Neural Models for NLP. Recently there has been enormous interest in neural models for NLP generally (Collobert et al., 2011; Goldberg, 2016). Most relevant to this work, simple CNN based models (which we have built on here) have proven extremely effective for text categorization (Kim, 2014; Zhang and Wallace, 2015). Exploiting Linguistic Resources. A potential drawback to learning from scratch in end-to-end neural models is a failure to capitalize on existing knowledge sources. There have been efforts to exploit such resources specifically to induce better word vectors (Yu and Dredze, 2014; Faruqui et al., 2014; Yu et al., 2016; Xu et al., 2014). But these models do not attempt to exploit external resources jointly during training for a particular downstream task (which uses word embeddings as inputs), as we do here. Past work on sparse linear models has shown the potential of exploiting linguistic knowledge in statistical NLP models. For example, Yogatama and Smith (2014) used external resources to inform structured, grouped regularization of loglinear text classification models, yielding improvements over standard regularization approaches. Elsewhere, Doshi-Velez et al. (2015"
P17-2024,D14-1181,0,0.0273714,"Missing"
P17-2024,W16-6106,1,0.837077,"P. Recently there has been enormous interest in neural models for NLP generally (Collobert et al., 2011; Goldberg, 2016). Most relevant to this work, simple CNN based models (which we have built on here) have proven extremely effective for text categorization (Kim, 2014; Zhang and Wallace, 2015). Exploiting Linguistic Resources. A potential drawback to learning from scratch in end-to-end neural models is a failure to capitalize on existing knowledge sources. There have been efforts to exploit such resources specifically to induce better word vectors (Yu and Dredze, 2014; Faruqui et al., 2014; Yu et al., 2016; Xu et al., 2014). But these models do not attempt to exploit external resources jointly during training for a particular downstream task (which uses word embeddings as inputs), as we do here. Past work on sparse linear models has shown the potential of exploiting linguistic knowledge in statistical NLP models. For example, Yogatama and Smith (2014) used external resources to inform structured, grouped regularization of loglinear text classification models, yielding improvements over standard regularization approaches. Elsewhere, Doshi-Velez et al. (2015) proposed a variant of LDA that exploi"
P17-2024,D16-1076,1,0.924791,"hieve this we construct a shared embedding matrix such that words known a priori to be similar are constrained to share some fraction of embedding weights. Concretely, suppose we have N groups of words derived from an external resource. Note that one could derive such groups in several ways; e.g., using the synsets in SentiWordNet. We denote groups by {g1 , g2 , ..., gN }. Each group is associated with an embedding ggi , which we initialize by averaging the pre-trained embeddings of each word in the group. To exploit both grouped and independent word weights, we adopt a two-channel CNN model (Zhang et al., 2016b). The embedding matrix of the first channel is initialized with pre-trained word vectors. We denote this input by Ep ∈ RV ×d (V is the vocabulary size and d the dimension of the word embeddings). The second channel input matrix is initialized with our proposed weightsharing embedding Es ∈ RV ×d . Es is initialized by drawing from both Ep and the external resource following the process we describe below. Given an input text sequence of length l, we construct sequence embedding representations Wp ∈ Rl×d and Ws ∈ Rl×d using the corresponding embedding matrices. We then apply independent sets of"
P17-2024,N16-1178,1,0.937152,"hieve this we construct a shared embedding matrix such that words known a priori to be similar are constrained to share some fraction of embedding weights. Concretely, suppose we have N groups of words derived from an external resource. Note that one could derive such groups in several ways; e.g., using the synsets in SentiWordNet. We denote groups by {g1 , g2 , ..., gN }. Each group is associated with an embedding ggi , which we initialize by averaging the pre-trained embeddings of each word in the group. To exploit both grouped and independent word weights, we adopt a two-channel CNN model (Zhang et al., 2016b). The embedding matrix of the first channel is initialized with pre-trained word vectors. We denote this input by Ep ∈ RV ×d (V is the vocabulary size and d the dimension of the word embeddings). The second channel input matrix is initialized with our proposed weightsharing embedding Es ∈ RV ×d . Es is initialized by drawing from both Ep and the external resource following the process we describe below. Given an input text sequence of length l, we construct sequence embedding representations Wp ∈ Rl×d and Ws ∈ Rl×d using the corresponding embedding matrices. We then apply independent sets of"
P17-2024,P05-1015,0,0.212024,"l parameters aside from the shared weights in the standard way. The number of parameters in our approach scales linearly with the number of channels. But the gradients can actually be back-propagated in a distributed way for each channel, since the convolutional and embedding layers are independent across these. Thus training time scales approximately linearly with the number of parameters in one channel (if the gradient is back-propagated in a distributed way). 3 3.1 Implementation Details and Baselines Experimental Setup Datasets We use three sentiment datasets: a movie review (MR) dataset (Pang and Lee, 2005)3 ; a customer review (CR) dataset (Hu and Liu, 2004)4 ; and an opinion dataset (MPQA) (Wiebe et al., 2005)5 . We also use four biomedical datasets, which concern systematic reviews. The task here is to classify published articles describing clinical trials as relevant or not to a well-specified clinical question. Articles deemed relevant are included in 3 www.cs.cornell.edu/people/pabo/ movie-review-data/ 4 www.cs.uic.edu/˜liub/FBS/ sentiment-analysis.html 5 mpqa.cs.pitt.edu/corpora/mpqa_corpus/ 6 sentiwordnet.isti.cnr.it github.com/percyliang/brown-cluster 8 www.nlm.nih.gov/bsd/disted/ mesht"
P17-2024,baccianella-etal-2010-sentiwordnet,0,\N,Missing
P17-2024,J92-4003,0,\N,Missing
P17-4002,D10-1011,0,0.728793,"ument (bottom). lute accuracy across domains (Zhang et al., 2016). RR incorporates these linear and neural strategies using a simple ensembling strategy. For bias classification, we average the predicted probabilities of RCTs being at low risk of bias from the linear and neural models. To extract corresponding rationales, we induce rankings over all sentences in a given document using both models, and then aggregate these via Borda count (de Borda, 1784). 3.2 3.2.1 Extracting PICO sentences Past work has investigated identifying PICO elements in biomedical texts (Demner-Fushman and Lin, 2007; Boudin et al., 2010). But these efforts have largely considered only article abstracts, limiting their utility: not all clinically salient data is always available in abstracts. One exception to this is a system called ExaCT (Kiritchenko et al., 2010), which does operate on full-texts, although PICO The Population, Interventions/Comparators and Outcomes (PICO) together define the clinical question addressed by a trial. Characterising and representing these is therefore an important aim for automating evidence synthesis. 9 Extraction type General Record number Author Article title Citation Type of Publication Coun"
P17-4002,J07-1005,0,0.0251778,"ion in-place in the source document (bottom). lute accuracy across domains (Zhang et al., 2016). RR incorporates these linear and neural strategies using a simple ensembling strategy. For bias classification, we average the predicted probabilities of RCTs being at low risk of bias from the linear and neural models. To extract corresponding rationales, we induce rankings over all sentences in a given document using both models, and then aggregate these via Borda count (de Borda, 1784). 3.2 3.2.1 Extracting PICO sentences Past work has investigated identifying PICO elements in biomedical texts (Demner-Fushman and Lin, 2007; Boudin et al., 2010). But these efforts have largely considered only article abstracts, limiting their utility: not all clinically salient data is always available in abstracts. One exception to this is a system called ExaCT (Kiritchenko et al., 2010), which does operate on full-texts, although PICO The Population, Interventions/Comparators and Outcomes (PICO) together define the clinical question addressed by a trial. Characterising and representing these is therefore an important aim for automating evidence synthesis. 9 Extraction type General Record number Author Article title Citation Ty"
P17-4002,N07-1033,0,0.0419738,"tics-System Demonstrations, pages 7–12 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-4002 of the data-extraction and synthesis steps of a systematic review using novel NLP models.1 to be assessed, e.g., whether trial participants were adequately blinded. 2 EBM aims to make evidence synthesis transparent. Therefore, it is imperative to provide support for one’s otherwise somewhat subjective appraisals of risks of bias. In practice, this entails extracting quotes from articles supporting judgements, i.e. rationales (Zaidan et al., 2007). An automated system needs to do the same. We have therefore developed models that jointly (1) categorize articles as describing RCTs at ‘low’ or ‘high/unknown’ risk of bias across domains, and, (2) extract rationales supporting these categorizations (Marshall et al., 2014; Marshall et al., 2016; Zhang et al., 2016). Overview of RobotReviewer (RR) RR is a web-based tool which processes journal article PDFs (uploaded by end-users) describing the conduct and results of related RCTs to be synthesised. Using several machine learning (ML) dataextraction models, RR generates a report summarizing ke"
P17-4002,D14-1181,0,0.00982186,"Missing"
P17-4002,D16-1076,1,0.88776,"uately blinded. 2 EBM aims to make evidence synthesis transparent. Therefore, it is imperative to provide support for one’s otherwise somewhat subjective appraisals of risks of bias. In practice, this entails extracting quotes from articles supporting judgements, i.e. rationales (Zaidan et al., 2007). An automated system needs to do the same. We have therefore developed models that jointly (1) categorize articles as describing RCTs at ‘low’ or ‘high/unknown’ risk of bias across domains, and, (2) extract rationales supporting these categorizations (Marshall et al., 2014; Marshall et al., 2016; Zhang et al., 2016). Overview of RobotReviewer (RR) RR is a web-based tool which processes journal article PDFs (uploaded by end-users) describing the conduct and results of related RCTs to be synthesised. Using several machine learning (ML) dataextraction models, RR generates a report summarizing key information from the RCTs, including, e.g., details concerning trial participants, interventions, and reliability. Our ultimate goal is to automate the extraction of the full range of variables necessary to perform evidence synthesis. We list the current functionality of RR and future extraction targets in Table 1."
P17-4002,P09-1113,0,\N,Missing
P18-1019,W15-3817,0,0.0286551,"lability of only small corpora, which have typically provided on the order of a couple hundred annotated abstracts or articles for very complex information extraction tasks. For example, the ExaCT system (Kiritchenko et al., 2010) applies rules to extract 21 aspects of the reported trial. It was developed and validated on a dataset of 182 marked full-text articles. The ACRES system (Summerscales et al., 2011) produces summaries of several trial characteristic, and was trained on 263 annotated abstracts. Hinting at more challenging tasks that can build upon foundational information extraction, Alamri and Stevenson (2015) developed methods for detecting contradictory claims in biomedical papers. Their corpus of annotated claims contains 259 sentences (Alamri and Stevenson, 2016). Larger corpora for EBM tasks have been derived using (noisy) automated annotation approaches. This approach has been used to build, e.g., datasets to facilitate work on Information Retrieval (IR) models for biomedical texts (Scells et al., 2017; Chung, 2009; Boudin et al., 2010). Similar approaches have been used to ‘distantly supervise’ annotation of full-text articles describing clinical trials (Wallace et al., 2016). In contrast to"
P18-1019,N16-1030,0,0.0239175,"g medical literature generally and 204 CRF Participants Interventions Outcomes LSTM-CRF Participants Interventions Outcomes Precision 0.55 0.65 0.83 Precision 0.78 0.61 0.69 Recall 0.51 0.21 0.17 Recall 0.66 0.70 0.58 F-1 0.53 0.32 0.29 F-1 0.71 0.65 0.63 Participants Interventions Outcomes Precision 0.41 0.79 0.24 Precision 0.41 0.59 0.60 Recall 0.20 0.44 0.21 Recall 0.25 0.15 0.51 F-1 0.26 0.57 0.22 F-1 0.31 0.21 0.55 each token index, which is then passed to a CRF layer for prediction. We also exploit characterlevel information by passing a bi-LSTM over the characters comprising each word (Lample et al., 2016); these are appended to the word embedding representations before being passed through the bi-LSTM. 6 Conclusions We have presented EBM-NLP: a new, publicly available corpus comprising 5,000 richly annotated abstracts of articles describing clinical randomized controlled trials. This dataset fills a need for larger scale corpora to facilitate research on NLP methods for processing the biomedical literature, which have the potential to aid the conduct of EBM. The need for such technologies will only become more pressing as the literature continues its torrential growth. The EBM-NLP corpus, acco"
P18-1019,D10-1011,0,0.157081,"ty nye.b@husky.neu.edu jessy@austin.utexas.edu romapatel996@gmail.com Yinfei Yang∗ No affiliation Iain J. Marshall King’s College London Ani Nenkova UPenn yangyin7@gmail.com iain.marshall@kcl.ac.uk nenkova@seas.upenn.edu Byron C. Wallace Northeastern University b.wallace@northeastern.edu Abstract Computational methods could expedite biomedical evidence synthesis (Tsafnat et al., 2013; Wallace et al., 2013) and natural language processing (NLP) in particular can play a key role in the task. Prior work has explored the use of NLP methods to automate biomedical evidence extraction and synthesis (Boudin et al., 2010; Marshall et al., 2017; Ferracane et al., 2016; Verbeke et al., 2012).1 But the area has attracted less attention than it might from the NLP community, due primarily to a dearth of publicly available, annotated corpora with which to train and evaluate models. Here we address this gap by introducing EBMNLP, a new corpus to power NLP models in support of EBM. The corpus, accompanying documentation, baseline model implementations for the proposed tasks, and all code are publicly available.2 EBM-NLP comprises ∼5,000 medical abstracts describing clinical trials, multiply annotated in detail with r"
P18-1019,P16-1101,0,0.100901,"Missing"
P18-1019,P14-5010,0,0.00291024,"of MeSH terms with an instantiation rate above different thresholds for the respective PIO elements are shown in Table 11. 5 5.1 Identifying P, I and O Spans We consider two baseline models: a linear Conditional Random Field (CRF) (Lafferty et al., 2001) and a Long Short-Term Memory (LSTM) neural tagging model, an LSTM-CRF (Lample et al., 2016; Ma and Hovy, 2016). In both models, we treat tokens as being either Inside (I) or Outside (O) of spans. For the CRF, features include: indicators for the current, previous and next words; part of speech tags inferred using the Stanford CoreNLP tagger (Manning et al., 2014); and character information, e.g., whether a token contains digits, uppercase letters, symbols and so on. For the neural model, the model induces features via a bi-directional LSTM that consumes distributed vector representations of input tokens sequentially. The bi-LSTM yields a hidden vector at Tasks & Baselines We outline a few NLP tasks that are central to the aim of processing medical literature generally and 204 CRF Participants Interventions Outcomes LSTM-CRF Participants Interventions Outcomes Precision 0.55 0.65 0.83 Precision 0.78 0.61 0.69 Recall 0.51 0.21 0.17 Recall 0.66 0.70 0.58"
P18-1019,J07-1005,0,0.118292,"d, e.g., datasets to facilitate work on Information Retrieval (IR) models for biomedical texts (Scells et al., 2017; Chung, 2009; Boudin et al., 2010). Similar approaches have been used to ‘distantly supervise’ annotation of full-text articles describing clinical trials (Wallace et al., 2016). In contrast to the corpora discussed above, these automatically derived datasets tend to be relatively large, but they include only shallow annotations. Other work attempts to bypass basic extraction tasks and address more complex biomedical QA and (multi-document) summarization problems to support EBM (Demner-Fushman and Lin, 2007; Moll´a and Santiago-Martinez, 2011; Abacha and 3 Data Collection PubMed provides access to the MEDLINE database3 which indexes titles, abstracts and metadata for articles from selected medical journals dating back to the 1970s. MEDLINE indexes over 24 million abstracts; the majority of these have been manually assigned metadata which we used to retrieved a set of 5,000 articles describing RCTs with an emphasis on cardiovascular diseases, cancer, and autism. These particular topics were selected to cover a range of common conditions. We decomposed the annotation process into two steps, perfor"
P18-1019,W16-6112,1,0.840026,"du romapatel996@gmail.com Yinfei Yang∗ No affiliation Iain J. Marshall King’s College London Ani Nenkova UPenn yangyin7@gmail.com iain.marshall@kcl.ac.uk nenkova@seas.upenn.edu Byron C. Wallace Northeastern University b.wallace@northeastern.edu Abstract Computational methods could expedite biomedical evidence synthesis (Tsafnat et al., 2013; Wallace et al., 2013) and natural language processing (NLP) in particular can play a key role in the task. Prior work has explored the use of NLP methods to automate biomedical evidence extraction and synthesis (Boudin et al., 2010; Marshall et al., 2017; Ferracane et al., 2016; Verbeke et al., 2012).1 But the area has attracted less attention than it might from the NLP community, due primarily to a dearth of publicly available, annotated corpora with which to train and evaluate models. Here we address this gap by introducing EBMNLP, a new corpus to power NLP models in support of EBM. The corpus, accompanying documentation, baseline model implementations for the proposed tasks, and all code are publicly available.2 EBM-NLP comprises ∼5,000 medical abstracts describing clinical trials, multiply annotated in detail with respect to characteristics of the underlying tri"
P18-1019,P17-4002,1,0.894463,"Missing"
P18-1019,U11-1012,0,0.0658617,"Missing"
P18-1019,P14-2062,0,0.0712477,"indicating redundancy in the mentions of PICO elements. In addition, we outline several NLP tasks that would directly support the practice of EBM and that may be explored using the introduced resource. We present baseline models and associated results for these tasks. 2 2.2 Crowdsourcing, which we here define operationally as the use of distributed lay annotators, has shown encouraging results in NLP (Novotney and Callison-Burch, 2010; Sabou et al., 2012). Such annotations are typically imperfect, but methods that aggregate redundant annotations can mitigate this problem (Dalvi et al., 2013; Hovy et al., 2014; Nguyen et al., 2017). Medical articles contain relatively technical content, which intuitively may be difficult for persons without domain expertise to annotate. However, recent promising preliminary work has found that crowdsourced approaches can yield surprisingly high-quality annotations in the domain of EBM specifically (Mortensen et al., 2017; Thomas et al., 2017; Wallace et al., 2017). Related Work We briefly review two lines of research relevant to the current effort: work on NLP to facilitate EBM, and research in crowdsourcing for NLP. 2.1 Crowdsourcing NLP for EBM Prior work on NLP"
P18-1019,P17-1028,1,0.911942,"ncy in the mentions of PICO elements. In addition, we outline several NLP tasks that would directly support the practice of EBM and that may be explored using the introduced resource. We present baseline models and associated results for these tasks. 2 2.2 Crowdsourcing, which we here define operationally as the use of distributed lay annotators, has shown encouraging results in NLP (Novotney and Callison-Burch, 2010; Sabou et al., 2012). Such annotations are typically imperfect, but methods that aggregate redundant annotations can mitigate this problem (Dalvi et al., 2013; Hovy et al., 2014; Nguyen et al., 2017). Medical articles contain relatively technical content, which intuitively may be difficult for persons without domain expertise to annotate. However, recent promising preliminary work has found that crowdsourced approaches can yield surprisingly high-quality annotations in the domain of EBM specifically (Mortensen et al., 2017; Thomas et al., 2017; Wallace et al., 2017). Related Work We briefly review two lines of research relevant to the current effort: work on NLP to facilitate EBM, and research in crowdsourcing for NLP. 2.1 Crowdsourcing NLP for EBM Prior work on NLP for EBM has been limit"
P18-1019,N10-1024,0,0.0830566,"Missing"
P18-1019,E12-2021,0,0.134122,"Missing"
P18-1019,D12-1053,0,0.0489279,"Missing"
W16-6106,N15-1184,0,0.0344535,"Missing"
W16-6106,O97-1002,0,0.061306,"t al. (Rada et al., 1989) , Wu & Palmer (Wu and Palmer, 1994), Leacock & Chodorow (Leacock and Chodorow, 1998), and Nguyen & AlMubaid (Nguyen and Al-Mubaid, 2006), and the Path measure. The second class of techniques uses training corpora and information content (IC) to estimate the semantic similarity between two concepts. IC measures the specificity of a concept in a hierarchy. The IC-based measures account for the probability of the concept occurring in a corpus. A concept with a high IC value is more specific to a topic than one with a low IC value. Resnik (Resnik, 1995), Jiang & Conrath (Jiang and Conrath, 1997) and Lin (Lin, 1998), all have published works on the IC-based similarity measures. Resnik (Resnik, 1995) measures the similarity between two concepts by finding the IC of the LCS of the two concepts. Jiang & Conrath (Jiang and Conrath, 1997) and Lin (Lin, 1998) extended Resnik‘s IC-based measure by incorporating the IC of the individual concepts. Jiang & Conrath measure similarity by finding the IC of each individual concept and of the LCS of them. However, Lin‘s measure is similar to that of Wu & Palmer (Wu and Palmer, 1994), where depth is replaced by information content. Context vector met"
W16-6106,P11-1015,0,0.0048207,"edical School. 3 Method In this section, we provide a brief description of the method used for retrofitting word vector to semantic lexicons, present the design of our work flow, describe the test data and the semantic lexicon we created , and also present the evaluation measures we used. 3.1 Retrofitting Word Vector to Semantic Lexicons Vector space word representations are a critical component of many natural language processing systems. It is common to represent words as discrete indices in a vocabulary, but this fails to capture the rich relational structure of the human semantic lexicon (Maas et al., 2011). Retrofitting is a simple and effective method to improve word vectors using word relation knowledge found in semantic lexicons. It is used as a post-processing step to improve vector quality (Faruqui et al., 2014a). Figure 1 shows a small word graph example with edges connecting semantically related words. The words, cancer, tumor, neoplasm, sarcoma, and swelling, are similar words to each other in a lexical knowledge resource. Grey nodes are observed word vectors built from the corpus, which are independent of each other. White nodes are inferred word vectors, waiting to be retrofitted. The"
W16-6106,W06-2501,0,0.0305683,"snik, 1995) measures the similarity between two concepts by finding the IC of the LCS of the two concepts. Jiang & Conrath (Jiang and Conrath, 1997) and Lin (Lin, 1998) extended Resnik‘s IC-based measure by incorporating the IC of the individual concepts. Jiang & Conrath measure similarity by finding the IC of each individual concept and of the LCS of them. However, Lin‘s measure is similar to that of Wu & Palmer (Wu and Palmer, 1994), where depth is replaced by information content. Context vector metrics based on distributional statistics have also been used to calculate semantic similarity (Patwardhan, 2006; Patwardhan, 2003). By building co-occurrence vectors that represent the contextual profile of concepts, the relatedness between concepts can then be calculated using cosine similarity between vectors corresponding to two given concepts (Pedersen et al., 2007). Though IC-based measures do draw upon distributional information, this is used in a very restricted way to determine the specificity of a concept. Context vector metric-based distributional statistics do not have such limitations on the use of distributional information. However, the taxonomic structure is not taken into account in dis"
W16-6112,P15-1136,0,0.0126208,"ing, 2008). For example, if John Smith is coreferent with Smith, and Smith with Jane Smith, then it should not follow that John Smith and Jane Smith are coreferent. Other work has shown that joint models improve performance. Denis et al. (2007) recognized that anaphoricity (whether an entity is the first mention) and coreference should be treated as a joint task since one informs the other. Durrett and Klein (2014) models coreference together with named entity recognition and linking named entities to Wikipedia entities. Combinations of these models have also yielded improved results, such as Clark and Manning (2015) stacking 89 mention-pair and entity-centric systems (which the current paper uses as its off-the-shelf coreference resolver). Many coreference resolvers exploit deeper linguistic knowledge, beyond the features mentioned above. Chowdhury and Zweigenbaum (2013) eliminated less-informative training instances prior to model training by creating a list of criteria based on semantic and syntactic intuitions such as a mismatch in semantic types. Peng et al. (2015) created predicate schemas to constrain inference, such as two predicates with a semantically shared argument. Yang et al. (2015) used sem"
W16-6112,J07-1005,0,0.0665381,"These observations motivate the coreference features presented in section 4.3. In Table 2, standard care is not a member of any chains. More generally, we can expect salience to help more with intervention arms than control.3 3 Related work 3.1 Automated Identification of Arms Previous work has identified PICO elements either at the word or sentence level. Most research has extracted information from medical abstracts, although some studies have used the full text of the articles (De Bruijn et al., 2008; Zhao et al., 2012; Wallace et al., 2016). One of the seminal studies in PICO extraction (Demner-Fushman and Lin, 2007) collapsed intervention and comparator, where interventions were short noun phrases based largely on recognition of semantic types (mapped to UMLS concepts) and a few manually constructed rules. The intervention/comparator extractor returned a list of all the interventions under study, and the extractor was evaluated at the sentence level. However, it is important to distinguish between experimental and control treatments as the bias for the experimental 3 Cases of joint coreference such as all participants referring to both arms in the example abstract are not addressed in this paper, but pos"
W16-6112,D08-1069,0,0.0648262,"Missing"
W16-6112,N07-1030,0,0.0810657,"Missing"
W16-6112,Q14-1037,0,0.0233569,"hen modeled as latent trees in the graph. Constraints are imposed on these models for improved results, such as enforcing a transitive closure to guarantee you end up with legal assignments (Finkel and Manning, 2008). For example, if John Smith is coreferent with Smith, and Smith with Jane Smith, then it should not follow that John Smith and Jane Smith are coreferent. Other work has shown that joint models improve performance. Denis et al. (2007) recognized that anaphoricity (whether an entity is the first mention) and coreference should be treated as a joint task since one informs the other. Durrett and Klein (2014) models coreference together with named entity recognition and linking named entities to Wikipedia entities. Combinations of these models have also yielded improved results, such as Clark and Manning (2015) stacking 89 mention-pair and entity-centric systems (which the current paper uses as its off-the-shelf coreference resolver). Many coreference resolvers exploit deeper linguistic knowledge, beyond the features mentioned above. Chowdhury and Zweigenbaum (2013) eliminated less-informative training instances prior to model training by creating a list of criteria based on semantic and syntactic"
W16-6112,S15-2051,0,0.0395324,"ly occur in biomedical literature, resolving anaphors that carry a specific semantic type, or sort, such as these drugs. Many of these studies take advantage of linguistic resources such as WordNet5 and FrameNet6 . In the medical area, coreference resolution has been most closely studied for analyzing clinical narrative text such as that found in Electronic Health Records (EHRs), and biomolecular studies. In fact, there have been corpora (i2b2/VA Corpus(Uzuner et al., 2012), GENIA Event Corpus(Kim et al., 2008)) and shared tasks (SemEval-2015 shared task on Analysis of Clinical Text (Task 14)(Elhadad et al., 2015), BioNLP09 shared task(Kim et al., 2009), ShARe/CLEF eHealth 2013 Evaluation Lab Task 1(Pradhan et al., 2013)) created specifically to advance this area. Given that resources such as FrameNet and WordNet are based mostly on news (e.g. British National Corpus, U.S. newswire), a large number of resources have been created to aid in natural language processing of medical texts. By far the largest and most complex is the Unified Medical Language System (UMLS)7 , consisting of three main components: Metathesaurus with terms and codes from many vocabularies (including CPT, ICD-10-CM, MeSH, RxNorm, a"
W16-6112,P08-2012,0,0.0234167,"ng information from partially-completed coreference chains to guide later decisions. Features include whether a mention head word matches any of the head words in the antecedent cluster. • The antecedent tree model (Yu and Joachims, 2009) builds a graph from a document, where the nodes are the mentions and arcs are the links between mention pairs that are coreferent candidates. The coreference chains are then modeled as latent trees in the graph. Constraints are imposed on these models for improved results, such as enforcing a transitive closure to guarantee you end up with legal assignments (Finkel and Manning, 2008). For example, if John Smith is coreferent with Smith, and Smith with Jane Smith, then it should not follow that John Smith and Jane Smith are coreferent. Other work has shown that joint models improve performance. Denis et al. (2007) recognized that anaphoricity (whether an entity is the first mention) and coreference should be treated as a joint task since one informs the other. Durrett and Klein (2014) models coreference together with named entity recognition and linking named entities to Wikipedia entities. Combinations of these models have also yielded improved results, such as Clark and"
W16-6112,C08-1033,0,0.0248128,"for extracting eventargument relations from biomedical texts in the Genia Event Corpus. Jindal and Roth (2013) used very specific domain knowledge to resolve coreference in clinical narratives, such as creating a specific discourse model (i.e. a single patient, several doctors and a few family members) to resolve entities of type ”person”. Despite the active interest in coreference resolution, there has been much less research investigating its application to clinical trial texts. Most of the literature that does exist is applied to the bio-medical field, focusing more on full-text articles (Gasperin and Briscoe, 2008; Huang et al., 2010; Kilicoglu et al., 2016) than on abstracts (Castano et al., 2002; Yang et al., 2004). To the best of the authors’ knowledge, there have been no papers using coreference features to identify arms in clinical trial abstracts. 8 https://metamap.nlm.nih.gov http://www.drugbank.ca 10 http://banner.sourceforge.net 11 http://biotext.berkeley.edu 9 4 Experiment The goal of this experiment is to explore empirically whether incorporating coreference features improves the performance of a classifier for arm identification, as compared to a baseline model without coref features (note"
W16-6112,W09-1401,0,0.0405063,"anaphors that carry a specific semantic type, or sort, such as these drugs. Many of these studies take advantage of linguistic resources such as WordNet5 and FrameNet6 . In the medical area, coreference resolution has been most closely studied for analyzing clinical narrative text such as that found in Electronic Health Records (EHRs), and biomolecular studies. In fact, there have been corpora (i2b2/VA Corpus(Uzuner et al., 2012), GENIA Event Corpus(Kim et al., 2008)) and shared tasks (SemEval-2015 shared task on Analysis of Clinical Text (Task 14)(Elhadad et al., 2015), BioNLP09 shared task(Kim et al., 2009), ShARe/CLEF eHealth 2013 Evaluation Lab Task 1(Pradhan et al., 2013)) created specifically to advance this area. Given that resources such as FrameNet and WordNet are based mostly on news (e.g. British National Corpus, U.S. newswire), a large number of resources have been created to aid in natural language processing of medical texts. By far the largest and most complex is the Unified Medical Language System (UMLS)7 , consisting of three main components: Metathesaurus with terms and codes from many vocabularies (including CPT, ICD-10-CM, MeSH, RxNorm, and SNOMED CT), Semantic Network with sem"
W16-6112,C02-1139,0,0.0628798,"ach token directly, followed by an inference process to constrain the labels to more accurate results. As with previous studies, outcome results were the hardest because they are more variable. A significant limitation of this study is that the abstracts were limited to two-arm trials, and in a specific domain. 3.2 Automated Coreference Resolution Coreference resolution is a long-studied task that remains a challenging problem. Most recent work on coreference resolution builds mainly on one of four models. • The first and most widely-used approach is the mention-pair model (Soon et al., 2001; Ng and Cardie, 2002b). A classifier first identifies all the pairs of mentions which are coreferent. These pairs are then grouped into coreferent chains by clustering techniques such as closest-first (Soon et al., 2001) or best-first (Ng and Cardie, 2002b; Ng and Cardie, 2002a). 4 http://rctbank.ucsf.edu/home/cplus In closest-first, you link to the closest preceding mention, whereas in best-first, you choose the likeliest one. Common features in these models include distance between the two mentions, syntactic features (e.g., POS tags), semantic features (e.g., named entity type), lexical features (e.g., head wo"
W16-6112,P02-1014,0,0.172486,"ach token directly, followed by an inference process to constrain the labels to more accurate results. As with previous studies, outcome results were the hardest because they are more variable. A significant limitation of this study is that the abstracts were limited to two-arm trials, and in a specific domain. 3.2 Automated Coreference Resolution Coreference resolution is a long-studied task that remains a challenging problem. Most recent work on coreference resolution builds mainly on one of four models. • The first and most widely-used approach is the mention-pair model (Soon et al., 2001; Ng and Cardie, 2002b). A classifier first identifies all the pairs of mentions which are coreferent. These pairs are then grouped into coreferent chains by clustering techniques such as closest-first (Soon et al., 2001) or best-first (Ng and Cardie, 2002b; Ng and Cardie, 2002a). 4 http://rctbank.ucsf.edu/home/cplus In closest-first, you link to the closest preceding mention, whereas in best-first, you choose the likeliest one. Common features in these models include distance between the two mentions, syntactic features (e.g., POS tags), semantic features (e.g., named entity type), lexical features (e.g., head wo"
W16-6112,N15-1082,0,0.0173032,"ognition and linking named entities to Wikipedia entities. Combinations of these models have also yielded improved results, such as Clark and Manning (2015) stacking 89 mention-pair and entity-centric systems (which the current paper uses as its off-the-shelf coreference resolver). Many coreference resolvers exploit deeper linguistic knowledge, beyond the features mentioned above. Chowdhury and Zweigenbaum (2013) eliminated less-informative training instances prior to model training by creating a list of criteria based on semantic and syntactic intuitions such as a mismatch in semantic types. Peng et al. (2015) created predicate schemas to constrain inference, such as two predicates with a semantically shared argument. Yang et al. (2015) used semantic role labeling to link the time and locations for event mentions, and for verbal mentions they linked their participants. More recently, Kilicoglu et al. (2016) focused on sortal anaphoras which they found to commonly occur in biomedical literature, resolving anaphors that carry a specific semantic type, or sort, such as these drugs. Many of these studies take advantage of linguistic resources such as WordNet5 and FrameNet6 . In the medical area, corefe"
W16-6112,N12-1091,0,0.017602,"gBank9 , a database of drug names, BANNER10 , a named entity recognizer for biomedical texts, BioText for identifying entities and relations in bioscience texts, and BioFrameNet11 , an extension of FrameNet for molecular biology (and BioWordNet(Poprat et al., 2008) was a failed attempt at extending WordNet also to the biomolecular field). However, when applied to clinical trial texts, these tools prove useful mainly for identifying only medical terms and drug names, and thus more linguistically-motivated resources are still lacking for clinical trial texts. In the area of clinical narratives, Raghavan et al. (2012) took advantage of the temporal features present in these texts to help determine whether two medical concepts corefer with each other. Their 2014 paper (Raghavan et al., 2014) expanded on this idea to identify medical events spanning across narratives, such as admission notes, medical reports, and discharge notes. Yoshikawa et al. (2011) exploited coreference information for extracting eventargument relations from biomedical texts in the Genia Event Corpus. Jindal and Roth (2013) used very specific domain knowledge to resolve coreference in clinical narratives, such as creating a specific dis"
W16-6112,D10-1048,0,0.0195134,"n features in these models include distance between the two mentions, syntactic features (e.g., POS tags), semantic features (e.g., named entity type), lexical features (e.g., head word of the mention), and string matching. • The mention-ranking model (Denis and Baldridge, 2008), reframes the task as a ranking function rather than a classification function, ranking all the candidate antecedents of a mention to determine which candidate antecedent is the most probable. • The entity-centric model makes use of entitylevel information, focusing on features of mention clusters, and not just pairs (Raghunathan et al., 2010). The coreference clusters are built up incrementally, using information from partially-completed coreference chains to guide later decisions. Features include whether a mention head word matches any of the head words in the antecedent cluster. • The antecedent tree model (Yu and Joachims, 2009) builds a graph from a document, where the nodes are the mentions and arcs are the links between mention pairs that are coreferent candidates. The coreference chains are then modeled as latent trees in the graph. Constraints are imposed on these models for improved results, such as enforcing a transitiv"
W16-6112,J01-4004,0,0.373443,"nstead classifies each token directly, followed by an inference process to constrain the labels to more accurate results. As with previous studies, outcome results were the hardest because they are more variable. A significant limitation of this study is that the abstracts were limited to two-arm trials, and in a specific domain. 3.2 Automated Coreference Resolution Coreference resolution is a long-studied task that remains a challenging problem. Most recent work on coreference resolution builds mainly on one of four models. • The first and most widely-used approach is the mention-pair model (Soon et al., 2001; Ng and Cardie, 2002b). A classifier first identifies all the pairs of mentions which are coreferent. These pairs are then grouped into coreferent chains by clustering techniques such as closest-first (Soon et al., 2001) or best-first (Ng and Cardie, 2002b; Ng and Cardie, 2002a). 4 http://rctbank.ucsf.edu/home/cplus In closest-first, you link to the closest preceding mention, whereas in best-first, you choose the likeliest one. Common features in these models include distance between the two mentions, syntactic features (e.g., POS tags), semantic features (e.g., named entity type), lexical fe"
W16-6112,C04-1033,0,0.0499361,"sed very specific domain knowledge to resolve coreference in clinical narratives, such as creating a specific discourse model (i.e. a single patient, several doctors and a few family members) to resolve entities of type ”person”. Despite the active interest in coreference resolution, there has been much less research investigating its application to clinical trial texts. Most of the literature that does exist is applied to the bio-medical field, focusing more on full-text articles (Gasperin and Briscoe, 2008; Huang et al., 2010; Kilicoglu et al., 2016) than on abstracts (Castano et al., 2002; Yang et al., 2004). To the best of the authors’ knowledge, there have been no papers using coreference features to identify arms in clinical trial abstracts. 8 https://metamap.nlm.nih.gov http://www.drugbank.ca 10 http://banner.sourceforge.net 11 http://biotext.berkeley.edu 9 4 Experiment The goal of this experiment is to explore empirically whether incorporating coreference features improves the performance of a classifier for arm identification, as compared to a baseline model without coref features (note that we do not aim to necessarily achieve state-of-the-art results on this task). The task of the classif"
W16-6112,Q15-1037,0,0.116138,"as Clark and Manning (2015) stacking 89 mention-pair and entity-centric systems (which the current paper uses as its off-the-shelf coreference resolver). Many coreference resolvers exploit deeper linguistic knowledge, beyond the features mentioned above. Chowdhury and Zweigenbaum (2013) eliminated less-informative training instances prior to model training by creating a list of criteria based on semantic and syntactic intuitions such as a mismatch in semantic types. Peng et al. (2015) created predicate schemas to constrain inference, such as two predicates with a semantically shared argument. Yang et al. (2015) used semantic role labeling to link the time and locations for event mentions, and for verbal mentions they linked their participants. More recently, Kilicoglu et al. (2016) focused on sortal anaphoras which they found to commonly occur in biomedical literature, resolving anaphors that carry a specific semantic type, or sort, such as these drugs. Many of these studies take advantage of linguistic resources such as WordNet5 and FrameNet6 . In the medical area, coreference resolution has been most closely studied for analyzing clinical narrative text such as that found in Electronic Health Reco"
W19-1902,D18-1407,0,0.140827,"re importance scores, we compute Kendall-τ measure (Table 1) between attention scores and gradients with respect to the tokens comprising documents. Across both corpora and all tasks we observe only a modest correlation between the two for BiLSTM model (the projection based model have higher correspondence, which is expected for such simple architectures). This may be problematic for attention as an explanatory mechanism, given the explicit relationship between gradients and model outputs. (Although we note that gradient based methods themselves pose difficulty with respect to interpretation (Feng et al., 2018)). 4.2 Model Readmission LR + BoW LSTM LSTM + Additive Attention LSTM + Additive Attention (Log Odds at Test) LSTM + Log Odds Attention Mortality LR + BoW LSTM LSTM + Additive Attention LSTM + Additive Attention (Log Odds at Test) LSTM + Log Odds Attention Knee Surgery Complication LR + BoW LSTM LSTM + Additive Attention LSTM + Additive Attention (Log Odds at Test) LSTM + Log Odds Attention Hip Surgery Complication LR + BoW LSTM LSTM + Additive Attention LSTM + Additive Attention (Log Odds at Test) LSTM + Log Odds Attention Phenotyping LR + BoW LSTM LSTM + Additive Attention LSTM + Additive At"
W19-1902,N19-1357,1,0.907835,"n neural network architectures responsible for encoding text (notes) into a fixed-size representation for consumption by downstream layers. Patient histories are often long and may contain information mostly irrelevant to a given target. Encoding this may thus be difficult, and text encoder modules may benefit from attention mechanisms (Bahdanau et al., 2014), which may be imposed to emphasize relevant tokens. In addition to mitigating noise introduced by irrelevant tokens, attention mechanisms are often seen as providing interpretability, or insight into model behavior. However, recent work (Jain and Wallace, 2019) has argued that treating attention as explanation may, at least in some cases, be misguided. Interpretability is especially important for clinical tasks, but incorrect or misleading rationales supporting predictions may be particularly harmful in this domain; this motivates our focused study in this space. To summarize, our contributions are as follows. First, we empirically investigate whether incorporating standard attention mechanisms into RNN-based text encoders improves the performance of predictive models learned over EMR. We find that they do; inclusion of standard additive attention m"
W19-2606,N16-1030,0,0.0138707,"Missing"
W19-2606,P18-1019,1,0.832476,"owed better VAS and CSS patterns than the control group at 1-month follow-up (P < .05). No complications occurred in the study group. In the control group , there were 2 cases of arterial punctures and 3 cases of direct nerve injury with neurological deficit for 2 months. Ultrasonography-guided suprascapular nerve injection is a safe, accurate, and useful procedure compared to the blind technique. Figure 2: Example of a Human RCT abstract with the predicted spans for Participants (red), Intervention (blue) and Outcome (orange) Such granular spans were annotated in the original EBM-NLP corpus (Nye et al., 2018), along with a detailed types of interventions and outcomes. Performance for labeling these details and granular spans however is much lower than that for the original high-level spans that we examine here. An alternative would be to learn chunking rules to identify the condition, individual interventions and individual outcomes in an unsupervised manner, by collocation analysis of the thousands of extracted snippets from the MEDLINE corpus. In sum, progress on IE to aid browsing of the medical literature would require several modifications to track meaningful progress. Evaluation should be on"
