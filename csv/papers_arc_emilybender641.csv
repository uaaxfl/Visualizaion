2021.computel-1.12,Developing a Shared Task for Speech Processing on Endangered Languages,2021,-1,-1,3,0,11446,ginaanne levow,Proceedings of the 4th Workshop on the Use of Computational Methods in the Study of Endangered Languages Volume 1 (Papers),0,None
2020.acl-tutorials.2,Integrating Ethics into the {NLP} Curriculum,2020,-1,-1,1,1,11448,emily bender,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts,0,"To raise awareness among future NLP practitioners and prevent inertia in the field, we need to place ethics in the curriculum for all NLP students{---}not as an elective, but as a core part of their education. Our goal in this tutorial is to empower NLP researchers and practitioners with tools and resources to teach others about how to ethically apply NLP techniques. We will present both high-level strategies for developing an ethics-oriented curriculum, based on experience and best practices, as well as specific sample exercises that can be brought to a classroom. This highly interactive work session will culminate in a shared online resource page that pools lesson plans, assignments, exercise ideas, reading suggestions, and ideas from the attendees. Though the tutorial will focus particularly on examples for university classrooms, we believe these ideas can extend to company-internal workshops or tutorials in a variety of organizations. In this setting, a key lesson is that there is no single approach to ethical NLP: each project requires thoughtful consideration about what steps can be taken to best support people affected by that project. However, we can learn (and teach) what issues to be aware of, what questions to ask, and what strategies are available to mitigate harm."
2020.acl-main.463,"Climbing towards {NLU}: {On} Meaning, Form, and Understanding in the Age of Data",2020,-1,-1,1,1,11448,emily bender,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"The success of the large neural language models on many NLP tasks is exciting. However, we find that these successes sometimes lead to hype in which these models are being described as {``}understanding{''} language or capturing {``}meaning{''}. In this position paper, we argue that a system trained only on form has a priori no way to learn meaning. In keeping with the ACL 2020 theme of {``}Taking Stock of Where We{'}ve Been and Where We{'}re Going{''}, we argue that a clear understanding of the distinction between form and meaning will help guide the field towards better science around natural language understanding."
W19-6005,Handling cross-cutting properties in automatic inference of lexical classes: A case study of Chintang,2019,0,1,3,1,14767,olga zamaraeva,Proceedings of the 3rd Workshop on the Use of Computational Methods in the Study of Endangered Languages Volume 1 (Papers),0,"In the context of the ongoing AGGREGATION project concerned with inferring grammars from interlinear glossed text, we explore the integration of morphological patterns extracted from IGT data with inferred syntactic properties in the context of creating implemented linguistic grammars. We present a case study of Chintang, in which we put emphasis on evaluating the accuracy of these predictions by using them to generate a grammar and parse running text. Our coverage over the corpus is low because the lexicon produced by our system only includes intransitive and transitive verbs and nouns, but it outperforms an expert-built, oracle grammar of similar scope."
W19-0105,Modeling Clausal Complementation for a Grammar Engineering Resource,2019,0,1,3,1,14767,olga zamaraeva,Proceedings of the Society for Computation in Linguistics ({SC}i{L}) 2019,0,None
N19-4022,Visualizing Inferred Morphotactic Systems,2019,0,0,3,0,24412,haley lepp,Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics (Demonstrations),0,"We present a web-based system that facilitates the exploration of complex morphological patterns found in morphologically very rich languages. The need for better understanding of such patterns is urgent for linguistics and important for cross-linguistically applicable natural language processing. In this paper we give an overview of the system architecture and describe a sample case study on Abui [abz], a Trans-New Guinea language spoken in Indonesia."
N19-1235,Neural Text Generation from Rich Semantic Representations,2019,0,3,4,0,26193,valerie hajdik,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"We propose neural models to generate high-quality text from structured representations based on Minimal Recursion Semantics (MRS). MRS is a rich semantic representation that encodes more precise semantic detail than other representations such as Abstract Meaning Representation (AMR). We show that a sequence-to-sequence model that maps a linearization of Dependency MRS, a graph-based representation of MRS, to text can achieve a BLEU score of 66.11 when trained on gold data. The performance of the model can be improved further using a high-precision, broad coverage grammar-based parser to generate a large silver training corpus, achieving a final BLEU score of 77.17 on the full test set, and 83.37 on the subset of test data most closely matching the silver data domain. Our results suggest that MRS-based representations are a good choice for applications that need both structured semantics and the ability to produce natural language text as output."
Q18-1041,Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science,2018,0,26,1,1,11448,emily bender,Transactions of the Association for Computational Linguistics,0,"In this paper, we propose data statements as a design solution and professional practice for natural language processing technologists, in both research and development. Through the adoption and widespread use of data statements, the field can begin to address critical scientific and ethical issues that result from the use of data from certain populations in the development of technology for other populations. We present a form that data statements can take and explore the implications of adopting them as part of regular practice. We argue that data statements will help alleviate issues related to exclusion and bias in language technology, lead to better precision in claims about how natural language processing research can generalize and thus better engineering results, protect companies from public embarrassment, and ultimately lead to language technology that meets its users in their own preferred linguistic style and furthermore does not misrepresent them to others."
P18-5001,100 Things You Always Wanted to Know about Semantics {\\&} Pragmatics But Were Afraid to Ask,2018,0,0,1,1,11448,emily bender,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts,0,"Meaning is a fundamental concept in Natural Language Processing (NLP), given its aim to build systems that mean what they say to you, and understand what you say to them. In order for NLP to scale beyond partial, task-specific solutions, it must be informed by what is known about how humans use language to express and understand communicative intents. The purpose of this tutorial is to present a selection of useful information about semantics and pragmatics, as understood in linguistics, in a way that{'}s accessible to and useful for NLP practitioners with minimal (or even no) prior training in linguistics. The tutorial content is based on a manuscript in progress I am co-authoring with Prof. Alex Lascarides of the University of Edinburgh."
W17-5401,Towards Linguistically Generalizable {NLP} Systems: A Workshop and Shared Task,2017,16,8,4,0,8001,allyson ettinger,Proceedings of the First Workshop on Building Linguistically Generalizable {NLP} Systems,0,"This paper presents a summary of the first Workshop on Building Linguistically Generalizable Natural Language Processing Systems, and the associated Build It Break It, The Language Edition shared task. The goal of this workshop was to bring together researchers in NLP and linguistics with a carefully designed shared task aimed at testing the generalizability of NLP systems beyond the distributions of their training data. We describe the motivation, setup, and participation of the shared task, provide discussion of some highlighted results, and discuss lessons learned."
W17-0106,{STREAMLI}n{ED} Challenges: Aligning Research Interests with Shared Tasks,2017,12,1,2,0,11446,ginaanne levow,Proceedings of the 2nd Workshop on the Use of Computational Methods in the Study of Endangered Languages,0,This paper describes the use of Shared Task Evaluation Campaigns by designing tasks that are compelling to speech and natural language processing researchers while addressing technical challenges in language documentation and exploiting growing archives of endangered language data.
W17-0110,Inferring Case Systems from {IGT}: Enriching the Enrichment,2017,9,0,2,0.952381,23725,kristen howell,Proceedings of the 2nd Workshop on the Use of Computational Methods in the Study of Endangered Languages,0,None
W17-0118,Computational Support for Finding Word Classes: A Case Study of {A}bui,2017,12,0,3,1,14767,olga zamaraeva,Proceedings of the 2nd Workshop on the Use of Computational Methods in the Study of Endangered Languages,0,None
N16-4001,{E}nglish {R}esource {S}emantics,2016,0,1,2,0,26293,dan flickinger,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Tutorial Abstracts,0,None
W15-0128,Layers of Interpretation: On Grammar and Compositionality,2015,39,14,1,1,11448,emily bender,Proceedings of the 11th International Conference on Computational Semantics,0,"With the recent resurgence of interest in semantic annotation of corpora for improved semantic parsing, we observe a tendency which we view as ill-advised, to conflate sentence meaning and speaker meaning into a single mapping, whether done by annotators or by a parser. We argue instead for the more traditional hypothesis that sentence meaning, but not speaker meaning, is compositional, and accordingly that NLP systems would benefit from reusable, automatically derivable, taskindependent semantic representations which target sentence meaning, in order to capture exactly the information in the linguistic signal itself. We further argue that compositional construction of such sentence meaning representations affords better consistency, more comprehensiveness, greater scalability, and less duplication of effort for each new NLP application. For concreteness, we describe one well-tested grammar-based method for producing sentence meaning representations which is efficient for annotators, and which exhibits many of the above benefits. We then report on a small inter-annotator agreement study to quantify the consistency of semantic representations produced via this grammar-based method."
W14-2206,Learning Grammar Specifications from {IGT}: A Case Study of Chintang,2014,31,5,1,1,11448,emily bender,Proceedings of the 2014 Workshop on the Use of Computational Methods in the Study of Endangered Languages,0,"We present a case study of the methodology of using information extracted from interlinear glossed text (IGT) to create of actual working HPSG grammar fragments using the Grammar Matrix focusing on one language: Chintang. Though the results are barely measurable in terms of coverage over running text, they nonetheless provide a proof of concept. Our experience report reflects on the ways in which this task is non-trivial and on mismatches between the assumptions of the methodology and the realities of IGT as produced in a large-scale field project."
P14-1007,Simple Negation Scope Resolution through Deep Parsing: A Semantic Solution to a Semantic Problem,2014,18,16,2,0,34074,woodley packard,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"In this work, we revisit Shared Task 1 from the 2012 *SEM Conference: the automated analysis of negation. Unlike the vast majority of participating systems in 2012, our approach works over explicit and formal representations of propositional semantics, i.e. derives the notion of negation scope assumed in this task from the structure of logical-form meaning representations. We relate the task-specific interpretation of (negation) scope to the concept of (quantifier and operator) scope in mainstream underspecified semantics. With reference to an explicit encoding of semantic predicate-argument structure, we can operationalize the annotation decisions made for the 2012 *SEM task, and demonstrate how a comparatively simple system for negation scope resolution can be built from an off-the-shelf deep parsing system. In a system combination setting, our approach improves over the best published results on this task to date."
xia-etal-2014-enriching,Enriching {ODIN},2014,12,4,5,0,16067,fei xia,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"In this paper, we describe the expansion of the ODIN resource, a database containing many thousands of instances of Interlinear Glossed Text (IGT) for over a thousand languages harvested from scholarly linguistic papers posted to the Web. A database containing a large number of instances of IGT, which are effectively richly annotated and heuristically aligned bitexts, provides a unique resource for bootstrapping NLP tools for resource-poor languages. To make the data in ODIN more readily consumable by tool developers and NLP researchers, we propose a new XML format for IGT, called Xigt. We call the updated release ODIN-II."
flickinger-etal-2014-towards,Towards an Encyclopedia of Compositional Semantics: Documenting the Interface of the {E}nglish {R}esource {G}rammar,2014,16,9,2,0,26293,dan flickinger,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"We motivate and describe the design and development of an emerging encyclopedia of compositional semantics, pursuing three objectives. We first seek to compile a comprehensive catalogue of interoperable semantic analyses, i.e., a precise characterization of meaning representations for a broad range of common semantic phenomena. Second, we operationalize the discovery of semantic phenomena and their definition in terms of what we call their semantic fingerprint, a formal account of the building blocks of meaning representation involved and their configuration. Third, we ground our work in a carefully constructed semantic test suite of minimal exemplars for each phenomenon, along with a `target{'} fingerprint that enables automated regression testing. We work towards these objectives by codifying and documenting the body of knowledge that has been constructed in a long-term collaborative effort, the development of the LinGO English Resource Grammar. Documentation of its semantic interface is a prerequisite to use by non-experts of the grammar and the analyses it produces, but this effort also advances our own understanding of relevant interactions among phenomena, as well as of areas for future work in the grammar."
bender-2014-language,Language {C}o{LLAGE}: Grammatical Description with the {L}in{GO} Grammar Matrix,2014,11,8,1,1,11448,emily bender,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Language CoLLAGE is a collection of grammatical descriptions developed in the context of a grammar engineering graduate course with the LinGO Grammar Matrix. These grammatical descriptions include testsuites in well-formed interlinear glossed text (IGT) format, high-level grammatical characterizations called Âchoices filesÂ, HPSG grammar fragments (capable of parsing and generation), and documentation. As of this writing, Language CoLLAGE includes resources for 52 typologically and areally diverse languages and this number is expected to grow over time. The resources for each language cover a similar range of core grammatical phenomena and are implemented in a uniform framework, compatible with the DELPH-IN suite of processing tools."
J14-1001,{O}bituary: Ivan A. Sag,2014,-1,-1,1,1,11448,emily bender,Computational Linguistics,0,None
W13-2710,Towards Creating Precision Grammars from Interlinear Glossed Text: Inferring Large-Scale Typological Properties,2013,23,15,1,1,11448,emily bender,"Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities",0,We propose to bring together two kinds of linguistic resourcesxe2x80x94interlinear glossed text (IGT) and a language-independent precision grammar resourcexe2x80x94to automatically create precision grammars in the context of language documentation. This paper takes the first steps in that direction by extracting major-constituent word order and case system properties from IGT for a diverse sample of languages.
N12-4001,100 Things You Always Wanted to Know about Linguistics But Were Afraid to Ask*,2012,0,0,1,1,11448,emily bender,Tutorial Abstracts at the Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Many NLP tasks have at their core a subtask of extracting the dependencies---who did what to whom---from natural language sentences. This task can be understood as the inverse of the problem solved in different ways by diverse human languages, namely, how to indicate the relationship between different parts of a sentence. Understanding how languages solve the problem can be extremely useful in both feature design and error analysis in the application of machine learning to NLP. Likewise, understanding cross-linguistic variation can be important for the design of MT systems and other multilingual applications. The purpose of this tutorial is to present in a succinct and accessible fashion information about the structure of human languages that can be useful in creating more linguistically sophisticated, more language independent, and thus more successful NLP systems.n n While many kinds of linguistic structure can be relevant to different NLP tasks, the focus of this tutorial will be on morphosyntax. The tutorial will take an explicitly typological perspective as an understanding of cross-linguistic variation can facilitate the design of more portable (language-independent) NLP systems. In order to help participants retain the information better, the tutorial will be structured interactively. I will ask participants for examples of tasks and data sets they work with, and then as a group we will brainstorm ways in which each of the linguistic properties discussed can related to feature design and/or error analysis for those tasks."
N12-1032,Getting More from Morphology in Multilingual Dependency Parsing,2012,40,7,2,0,42812,matt hohensee,Proceedings of the 2012 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We propose a linguistically motivated set of features to capture morphological agreement and add them to the MSTParser dependency parser. Compared to the built-in morphological feature set, ours is both much smaller and more accurate across a sample of 20 morphologically annotated treebanks. We find increases in accuracy of up to 5.3% absolute. While some of this results from the feature set capturing information unrelated to morphology, there is still significant improvement, up to 4.6% absolute, due to the agreement model."
C12-1016,Deriving a Lexicon for a Precision Grammar from Language Documentation Resources: A Case Study of {C}hintang,2012,28,6,1,1,11448,emily bender,Proceedings of {COLING} 2012,0,"Language documentation projects typically invest a lot of effort in creating digitized lexical resources, which are used in the creation of dictionaries and in the glossing of collected texts. We present and evaluate a methodology for repurposing such a lexical resource developed for Chintang (ISO639-3: ctn), a language of Nepal, for use with a precision implemented grammar developed in the DELPH-IN formalism. The target lexicon, when combined with a set of morphological rules, achieves 57% type-level coverage and 50% token-level coverage of held-out texts, while maintaining a feature-level accuracy F-measure of 70%. As lexicon development is typically one of the most expensive aspects of creating a precision grammar, this represents a significant savings of effort."
Y11-1025,Spring Cleaning and Grammar Compression: Two Techniques for Detection of Redundancy in {HPSG} Grammars,2011,9,1,3,0,2845,antske fokkens,"Proceedings of the 25th Pacific Asia Conference on Language, Information and Computation",0,"This paper presents two approaches that identify which parts of an implemented grammar are used and which parts are computationally inactive. Our results lead to the following insights: even small grammars contain noise due to revised analyses, removing superfluous types from a grammar may help to detect errors in the original grammar and at least half of the types defined in the grammars we investigated do not play a role in the computational process of the grammar."
W11-0707,Annotating Social Acts: Authority Claims and Alignment Moves in {W}ikipedia Talk Pages,2011,-1,-1,1,1,11448,emily bender,Proceedings of the Workshop on Language in Social Media ({LSM} 2011),0,None
D11-1037,Parser Evaluation over Local and Non-Local Deep Dependencies in a Large Corpus,2011,36,25,1,1,11448,emily bender,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,"In order to obtain a fine-grained evaluation of parser accuracy over naturally occurring text, we study 100 examples each of ten reasonably frequent linguistic phenomena, randomly selected from a parsed version of the English Wikipedia. We construct a corresponding set of gold-standard target dependencies for these 1000 sentences, operationalize mappings to these targets from seven state-of-the-art parsers, and evaluate the parsers against this data to measure their level of success in identifying these dependencies."
P10-4001,Grammar Prototyping and Testing with the {L}in{GO} Grammar Matrix Customization System,2010,17,19,1,1,11448,emily bender,Proceedings of the {ACL} 2010 System Demonstrations,0,"This demonstration presents the LinGO Grammar Matrix grammar customization system: a repository of distilled linguistic knowledge and a web-based service which elicits a typological description of a language from the user and yields a customized grammar fragment ready for sustained development into a broad-coverage grammar. We describe the implementation of this repository with an emphasis on how the information is made available to users, including in-browser testing capabilities."
C10-2123,Argument Optionality in the {L}in{GO} Grammar Matrix,2010,25,2,2,0,45638,safiyyah saleem,Coling 2010: Posters,0,"We present a library of implemented HPSG analyses for argument optionality based on typological studies of this phenomenon in the world's languages, developed in the context of a grammar customization system that pairs a cross-linguistic core grammar with extensions for non-universal phenomena on the basis of user input of typological properties. Our analyses are compatible with multiple intersecting phenomena, including person, number, gender, tense, aspect and morphological rule formulation. We achieve 80--100% coverage on test suites from 10 natural languages."
W09-0106,"Linguistically Na{\\\\\i}ve != Language Independent: Why {NLP} Needs Linguistic Typology""",2009,12,38,1,1,11448,emily bender,"Proceedings of the {EACL} 2009 Workshop on the Interaction between Linguistics and Computational Linguistics: Virtuous, Vicious or Vacuous?",0,"In this position paper, I argue that in order to create truly language-independent NLP systems, we need to incorporate linguistic knowledge. The linguistic knowledge in question is not intricate rule systems, but generalizations from linguistic typology about the range of variation in linguistic structures across languages."
W08-2203,Semantic Representations of Syntactically Marked Discourse Status in Crosslinguistic Perspective,2008,16,4,1,1,11448,emily bender,Semantics in Text Processing. {STEP} 2008 Conference Proceedings,0,"This paper presents suggested semantic representations for different types of referring expressions in the format of Minimal Recursion Semantics and sketches syntactic analyses which can create them compositionally. We explore cross-linguistic harmonization of these representations, to promote interoperability and reusability of linguistic analyses. We follow Borthen and Haugereid (2005) in positing COG-ST ('cognitive status') as a feature on the syntax-semantics interface to handle phenomena associated with definiteness. Our proposal helps to unify the treatments of definiteness markers, demonstratives, overt pronouns and null anaphora across languages. In languages with articles, they contribute an existential quantifier and the appropriate value for COG-ST. In other languages, the COG-ST value is determined by an affix. The contribution of demonstrative determiners is decomposed into a COG-ST value, a quantifier, and proximity information, each of which can be contributed by a different kind of grammatical construction in a given language. Along with COG-ST, we posit a feature that distinguishes between pronouns (and null anaphora) that are sensitive to the identity of the referent of their antecedent and those that are sensitive to its type."
W08-0202,"Building a Flexible, Collaborative, Intensive Master{'}s Program in Computational Linguistics",2008,8,1,1,1,11448,emily bender,Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics,0,"We present the design of a professional master's program in Computational Linguistics. This program can be completed in one-year of full-time study, or two-three years of part-time study. Originally designed for CS professionals looking for additional training, the program has evolved in flexibility to accommodate students from more diverse backgrounds and with more diverse goals."
P08-1111,Evaluating a Crosslinguistic Grammar Resource: A Case Study of {W}ambaya,2008,28,20,1,1,11448,emily bender,Proceedings of ACL-08: HLT,1,"This paper evaluates the LinGO Grammar Matrix, a cross-linguistic resource for the development of precision broad coverage grammars, by applying it to the Australian language Wambaya. Despite large typological differences between Wambaya and the languages on which the development of the resource was based, the Grammar Matrix is found to provide a significant jump-start in the creation of the grammar for Wambaya: With less than 5.5 person-weeks of development, the Wambaya grammar was able to assign correct semantic representations to 76% of the sentences in a naturally occurring text. While the work on Wambaya identified some areas of refinement for the Grammar Matrix, 59% of the Matrix-provided types were invoked in the final Wambaya grammar, and only 4% of the Matrix-provided types required modification."
W07-1218,Validation and Regression Testing for a Cross-linguistic Grammar Resource,2007,13,3,1,1,11448,emily bender,{ACL} 2007 Workshop on Deep Linguistic Processing,0,"We present a validation methodology for a cross-linguistic grammar resource which produces output in the form of small grammars based on elicited typological descriptions. Evaluating the resource entails sampling from a very large space of language types, the type and range of which preclude the use of standard test suites development techniques. We produce a database from which gold standard test suites for these grammars can be generated on demand, including well-formed strings paired with all of their valid semantic representations as well as a sample of ill-formed strings. These string-semantics pairs are selected from a set of candidates by a system of regular-expression based filters. The filters amount to an alternative grammar building system, whose generative capacity is limited compared to the actual grammars. We perform error analysis of the discrepancies between the test suites and grammars for a range of language types, and update both systems appropriately. The resulting resource serves as a point of comparison for regression testing in future development."
I05-2035,Rapid Prototyping of Scalable Grammars: Towards Modularity in Extensions to a Language-Independent Core,2005,17,46,1,1,11448,emily bender,Companion Volume to the Proceedings of Conference including Posters/Demos and tutorial abstracts,0,"We present a new way to simplify the construction of precise broad-coverage grammars, employing typologicallymotivated, customizable extensions to a language-independent core grammar. Each xe2x80x98modulexe2x80x99 represents a salient dimension of cross-linguistic variation, and presents the grammar developer with simple choices that result in automatically generated language-specific software. We illustrate the approach for several phenomena and explore the interdependence of the modules."
baldwin-etal-2004-road,Road-testing the {E}nglish {R}esource {G}rammar Over the {B}ritish {N}ational {C}orpus,2004,8,56,2,0,1468,timothy baldwin,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"This paper addresses two questions: (1) when a large deep processing resource developed for relatively closed domains is run over open text, what coverage does it have, and (2) what are the most effective and time-efficient ways of consolidating gaps in the coverage of such as resource?"
W02-1502,The Grammar Matrix: An Open-Source Starter-Kit for the Rapid Development of Cross-linguistically Consistent Broad-Coverage Precision Grammars,2002,18,154,1,1,11448,emily bender,{COLING}-02: Grammar Engineering and Evaluation,0,"The grammar matrix is an open-source starter-kit for the development of broad-coverage HPSGs. By using a type hierarchy to represent cross-linguistic generalizations and providing compatibility with other open-source tools for grammar engineering, evaluation, parsing and generation, it facilitates not only quick start-up but also rapid growth towards the wide coverage necessary for robust natural language processing and the precision parses and semantic representations necessary for natural language understanding."
W02-1508,Parallel Distributed Grammar Engineering for Practical Applications,2002,16,16,2,0,2623,stephan oepen,{COLING}-02: Grammar Engineering and Evaluation,0,"Based on a detailed case study of parallel grammar development distributed across two sites, we review some of the requirements for regression testing in grammar engineering, summarize our approach to systematic competence and performance profiling, and discuss our experience with grammar development for a commercial application. If possible, the workshop presentation will be organized around a software demonstration."
W02-1210,Efficient Deep Processing of {J}apanese,2002,14,93,2,0,6174,melanie siegel,{COLING}-02: The 3rd Workshop on {A}sian Language Resources and International Standardization,0,"We present a broad coverage Japanese grammar written in the HPSG formalism with MRS semantics. The grammar is created for use in real world applications, such that robustness and performance issues play an important role. It is connected to a POS tagging and word segmentation tool. This grammar is being developed in a multilingual context, requiring MRS structures that are easily comparable across languages."
