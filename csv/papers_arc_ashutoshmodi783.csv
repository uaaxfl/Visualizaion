2021.wassa-1.9,An End-to-End Network for Emotion-Cause Pair Extraction,2021,-1,-1,4,0,419,aaditya singh,"Proceedings of the Eleventh Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"The task of Emotion-Cause Pair Extraction (ECPE) aims to extract all potential clause-pairs of emotions and their corresponding causes in a document. Unlike the more well-studied task of Emotion Cause Extraction (ECE), ECPE does not require the emotion clauses to be provided as annotations. Previous works on ECPE have either followed a multi-stage approach where emotion extraction, cause extraction, and pairing are done independently or use complex architectures to resolve its limitations. In this paper, we propose an end-to-end model for the ECPE task. Due to the unavailability of an English language ECPE corpus, we adapt the NTCIR-13 ECE corpus and establish a baseline for the ECPE task on this dataset. On this dataset, the proposed method produces significant performance improvements (â¼ 6.5{\%} increase in F1 score) over the multi-stage approach and achieves comparable performance to the state-of-the-art methods."
2021.semeval-1.19,{R}e{CAM}@{IITK} at {S}em{E}val-2021 Task 4: {BERT} and {ALBERT} based Ensemble for Abstract Word Prediction,2021,-1,-1,2,0,1667,abhishek mittal,Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),0,"This paper describes our system for Task 4 of SemEval-2021: Reading Comprehension of Abstract Meaning (ReCAM). We participated in all subtasks where the main goal was to predict an abstract word missing from a statement. We fine-tuned the pre-trained masked language models namely BERT and ALBERT and used an Ensemble of these as our submitted system on Subtask 1 (ReCAM-Imperceptibility) and Subtask 2 (ReCAM-Nonspecificity). For Subtask 3 (ReCAM-Intersection), we submitted the ALBERT model as it gives the best results. We tried multiple approaches and found that Masked Language Modeling(MLM) based approach works the best."
2021.semeval-1.24,{IITK}@Detox at {S}em{E}val-2021 Task 5: Semi-Supervised Learning and Dice Loss for Toxic Spans Detection,2021,-1,-1,3,0,1689,archit bansal,Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),0,"In this work, we present our approach and findings for SemEval-2021 Task 5 - Toxic Spans Detection. The task{'}s main aim was to identify spans to which a given text{'}s toxicity could be attributed. The task is challenging mainly due to two constraints: the small training dataset and imbalanced class distribution. Our paper investigates two techniques, semi-supervised learning and learning with Self-Adjusting Dice Loss, for tackling these challenges. Our submitted system (ranked ninth on the leader board) consisted of an ensemble of various pre-trained Transformer Language Models trained using either of the above-proposed techniques."
2021.semeval-1.36,Humor@{IITK} at {S}em{E}val-2021 Task 7: Large Language Models for Quantifying Humor and Offensiveness,2021,-1,-1,5,0,1715,aishwarya gupta,Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),0,"Humor and Offense are highly subjective due to multiple word senses, cultural knowledge, and pragmatic competence. Hence, accurately detecting humorous and offensive texts has several compelling use cases in Recommendation Systems and Personalized Content Moderation. However, due to the lack of an extensive labeled dataset, most prior works in this domain haven{'}t explored large neural models for subjective humor understanding. This paper explores whether large neural models and their ensembles can capture the intricacies associated with humor/offense detection and rating. Our experiments on the SemEval-2021 Task 7: HaHackathon show that we can develop reasonable humor and offense detection systems with such models. Our models are ranked 3rd in subtask 1b and consistently ranked around the top 33{\%} of the leaderboard for the remaining subtasks."
2021.semeval-1.40,{B}reaking{BERT}@{IITK} at {S}em{E}val-2021 Task 9: Statement Verification and Evidence Finding with Tables,2021,-1,-1,7,0,1733,aditya jindal,Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),0,"Recently, there has been an interest in the research on factual verification and prediction over structured data like tables and graphs. To circumvent any false news incident, it is necessary to not only model and predict over structured data efficiently but also to explain those predictions. In this paper, as the part of the SemEval-2021 Task 9, we tackle the problem of fact verification and evidence finding over tabular data. There are two subtasks, in which given a table and a statement/fact, the subtask A is to determine whether the statement is inferred from the tabular data and the subtask B is to determine which cells in the table provide evidence for the former subtask. We make a comparison of the baselines and state of the art approaches over the given SemTabFact dataset. We also propose a novel approach CellBERT to solve the task of evidence finding, as a form of Natural Language Inference task. We obtain a 3-way F1 score of 0.69 on subtask A and an F1 score of 0.65 on subtask B."
2021.semeval-1.53,{IITK} at {S}em{E}val-2021 Task 10: Source-Free Unsupervised Domain Adaptation using Class Prototypes,2021,-1,-1,6,0,1779,harshit kumar,Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),0,"Recent progress in deep learning has primarily been fueled by the availability of large amounts of annotated data that is obtained from highly expensive manual annotating pro-cesses. To tackle this issue of availability of annotated data, a lot of research has been done on unsupervised domain adaptation that tries to generate systems for an unlabelled target domain data, given labeled source domain data. However, the availability of annotated or labelled source domain dataset can{'}t always be guaranteed because of data-privacy issues. This is especially the case with medical data, as it may contain sensitive information of the patients. Source-free domain adaptation (SFDA) aims to resolve this issue by us-ing models trained on the source data instead of using the original annotated source data. In this work, we try to build SFDA systems for semantic processing by specifically focusing on the negation detection subtask of the SemEval2021 Task 10. We propose two approaches -ProtoAUGandAdapt-ProtoAUGthat use the idea of self-entropy to choose reliable and high confidence samples, which are then used for data augmentation and subsequent training of the models. Our methods report an improvement of up to 7{\%} in F1 score over the baseline for the Negation Detection subtask."
2021.semeval-1.57,{K}now{G}raph@{IITK} at {S}em{E}val-2021 Task 11: Building Knowledge Graph for {NLP} Research,2021,-1,-1,3,0,1791,shashank shailabh,Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),0,"Research in Natural Language Processing is making rapid advances, resulting in the publication of a large number of research papers. Finding relevant research papers and their contribution to the domain is a challenging problem. In this paper, we address this challenge via the SemEval 2021 Task 11: NLPContributionGraph, by developing a system for a research paper contributions-focused knowledge graph over Natural Language Processing literature. The task is divided into three sub-tasks: extracting contribution sentences that show important contributions in the research article, extracting phrases from the contribution sentences, and predicting the information units in the research article together with triplet formation from the phrases. The proposed system is agnostic to the subject domain and can be applied for building a knowledge graph for any area. We found that transformer-based language models can significantly improve existing techniques and utilized the SciBERT-based model. Our first sub-task uses Bidirectional LSTM (BiLSTM) stacked on top of SciBERT model layers, while the second sub-task uses Conditional Random Field (CRF) on top of SciBERT with BiLSTM. The third sub-task uses a combined SciBERT based neural approach with heuristics for information unit prediction and triplet formation from the phrases. Our system achieved F1 score of 0.38, 0.63 and 0.76 in end-to-end pipeline testing, phrase extraction testing and triplet extraction testing respectively."
2021.semeval-1.62,"{MCL}@{IITK} at {S}em{E}val-2021 Task 2: Multilingual and Cross-lingual Word-in-Context Disambiguation using Augmented Data, Signals, and Transformers",2021,-1,-1,4,0,434,rohan gupta,Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),0,"In this work, we present our approach for solving the SemEval 2021 Task 2: Multilingual and Cross-lingual Word-in-Context Disambiguation (MCL-WiC). The task is a sentence pair classification problem where the goal is to detect whether a given word common to both the sentences evokes the same meaning. We submit systems for both the settings - Multilingual (the pair{'}s sentences belong to the same language) and Cross-Lingual (the pair{'}s sentences belong to different languages). The training data is provided only in English. Consequently, we employ cross-lingual transfer techniques. Our approach employs fine-tuning pre-trained transformer-based language models, like ELECTRA and ALBERT, for the English task and XLM-R for all other tasks. To improve these systems{'} performance, we propose adding a signal to the word to be disambiguated and augmenting our data by sentence pair reversal. We further augment the dataset provided to us with WiC, XL-WiC and SemCor 3.0. Using ensembles, we achieve strong performance in the Multilingual task, placing first in the EN-EN and FR-FR sub-tasks. For the Cross-Lingual setting, we employed translate-test methods and a zero-shot method, using our multilingual models, with the latter performing slightly better."
2021.semeval-1.66,{IITK}@{LCP} at {S}em{E}val-2021 Task 1: Classification for Lexical Complexity Regression Task,2021,-1,-1,5,0,1817,neil shirude,Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),0,"This paper describes our contribution to SemEval 2021 Task 1 (Shardlow et al., 2021): Lexical Complexity Prediction. In our approach, we leverage the ELECTRA model and attempt to mirror the data annotation scheme. Although the task is a regression task, we show that we can treat it as an aggregation of several classification and regression models. This somewhat counter-intuitive approach achieved an MAE score of 0.0654 for Sub-Task 1 and MAE of 0.0811 on Sub-Task 2. Additionally, we used the concept of weak supervision signals from Gloss-BERT in our work, and it significantly improved the MAE score in Sub-Task 1."
2021.semeval-1.175,Counts@{IITK} at {S}em{E}val-2021 Task 8: {S}ci{BERT} Based Entity And Semantic Relation Extraction For Scientific Data,2021,-1,-1,4,0,2079,akash gangwar,Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),0,"This paper presents the system for SemEval 2021 Task 8 (MeasEval). MeasEval is a novel span extraction, classification, and relation extraction task focused on finding quantities, attributes of these quantities, and additional information, including the related measured entities, properties, and measurement contexts. Our submitted system, which placed fifth (team rank) on the leaderboard, consisted of SciBERT with [CLS] token embedding and CRF layer on top. We were also placed first in Quantity (tied) and Unit subtasks, second in MeasuredEntity, Modifier and Qualifies subtasks, and third in Qualifier subtask."
2021.eacl-main.71,Adv-{OLM}: Generating Textual Adversaries via {OLM},2021,-1,-1,3,0,1736,vijit malik,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"Deep learning models are susceptible to adversarial examples that have imperceptible perturbations in the original input, resulting in adversarial attacks against these models. Analysis of these attacks on the state of the art transformers in NLP can help improve the robustness of these models against such adversarial inputs. In this paper, we present Adv-OLM, a black-box attack method that adapts the idea of Occlusion and Language Models (OLM) to the current state of the art attack methods. OLM is used to rank words of a sentence, which are later substituted using word replacement strategies. We experimentally show that our approach outperforms other attack methods for several text classification tasks."
2021.acl-long.313,{ILDC} for {CJPE}: {I}ndian Legal Documents Corpus for Court Judgment Prediction and Explanation,2021,-1,-1,7,0,1736,vijit malik,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"An automated system that could assist a judge in predicting the outcome of a case would help expedite the judicial process. For such a system to be practically useful, predictions by the system should be explainable. To promote research in developing such a system, we introduce ILDC (Indian Legal Documents Corpus). ILDC is a large corpus of 35k Indian Supreme Court cases annotated with original court decisions. A portion of the corpus (a separate test set) is annotated with gold standard explanations by legal experts. Based on ILDC, we propose the task of Court Judgment Prediction and Explanation (CJPE). The task requires an automated system to predict an explainable outcome of a case. We experiment with a battery of baseline models for case predictions and propose a hierarchical occlusion based model for explainability. Our best prediction model has an accuracy of 78{\%} versus 94{\%} for human legal experts, pointing towards the complexity of the prediction task. The analysis of explanations by the proposed algorithm reveals a significant difference in the point of view of the algorithm and legal experts for explaining the judgments, pointing towards scope for future research."
2020.semeval-1.56,{IITK}-{RSA} at {S}em{E}val-2020 Task 5: Detecting Counterfactuals,2020,-1,-1,4,0,15059,anirudh ojha,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"This paper describes our efforts in tackling Task 5 of SemEval-2020. The task involved detecting a class of textual expressions known as counterfactuals and separating them into their constituent elements. Our final submitted approaches were an ensemble of various fine-tuned transformer-based and CNN-based models for the first subtask and a transformer model with dependency tree information for the second subtask. We ranked 4-th and 9-th in the overall leaderboard. We also explored various other approaches that involved classical methods, other neural architectures and incorporation of different linguistic features."
2020.semeval-1.61,{CS}-{NET} at {S}em{E}val-2020 Task 4: {S}iamese {BERT} for {C}om{VE},2020,-1,-1,4,0,15071,soumya dash,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"In this paper, we describe our system for Task 4 of SemEval 2020, which involves differentiating between natural language statements that conform to common sense and those that do not. The organizers propose three subtasks - first, selecting between two sentences, the one which is against common sense. Second, identifying the most crucial reason why a statement does not make sense. Third, generating novel reasons for explaining the against common sense statement. Out of the three subtasks, this paper reports the system description of subtask A and subtask B. This paper proposes a model based on transformer neural network architecture for addressing the subtasks. The novelty in work lies in the architecture design, which handles the logical implication of contradicting statements and simultaneous information extraction from both sentences. We use a parallel instance of transformers, which is responsible for a boost in the performance. We achieved an accuracy of 94.8{\%} in subtask A and 89{\%} in subtask B on the test set."
2020.semeval-1.150,{IITK} at {S}em{E}val-2020 Task 8: Unimodal and Bimodal Sentiment Analysis of {I}nternet Memes,2020,-1,-1,4,0,5933,vishal keswani,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"Social media is abundant in visual and textual information presented together or in isolation. Memes are the most popular form, belonging to the former class. In this paper, we present our approaches for the Memotion Analysis problem as posed in SemEval-2020 Task 8. The goal of this task is to classify memes based on their emotional content and sentiment. We leverage techniques from Natural Language Processing (NLP) and Computer Vision (CV) towards the sentiment classification of internet memes (Subtask A). We consider Bimodal (text and image) as well as Unimodal (text-only) techniques in our study ranging from the Na Ì{\i}ve Bayes classifier to Transformer-based approaches. Our results show that a text-only approach, a simple Feed Forward Neural Network (FFNN) with Word2vec embeddings as input, performs superior to all the others. We stand first in the Sentiment analysis task with a relative improvement of 63{\%} over the baseline macro-F1 score. Our work is relevant to any task concerned with the combination of different modalities."
2020.semeval-1.162,{BAKSA} at {S}em{E}val-2020 Task 9: Bolstering {CNN} with Self-Attention for Sentiment Analysis of Code Mixed Text,2020,-1,-1,4,0,12110,ayush kumar,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"Sentiment Analysis of code-mixed text has diversified applications in opinion mining ranging from tagging user reviews to identifying social or political sentiments of a sub-population. In this paper, we present an ensemble architecture of convolutional neural net (CNN) and self-attention based LSTM for sentiment analysis of code-mixed tweets. While the CNN component helps in the classification of positive and negative tweets, the self-attention based LSTM, helps in the classification of neutral tweets, because of its ability to identify correct sentiment among multiple sentiment bearing units. We achieved F1 scores of 0.707 (ranked 5th) and 0.725 (ranked 13th) on Hindi-English (Hinglish) and Spanish-English (Spanglish) datasets, respectively. The submissions for Hinglish and Spanglish tasks were made under the usernames ayushk and harsh{\_}6 respectively."
2020.semeval-1.217,{IITK} at {S}em{E}val-2020 Task 10: Transformers for Emphasis Selection,2020,-1,-1,4,0,15306,vipul singhal,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,We propose an end-to-end model that takes as input the text and corresponding to each word gives the probability of the word to be emphasized. Our results show that transformer-based models are particularly effective in this task. We achieved an evaluation score of 0.810 and were ranked third on the leaderboard.
2020.semeval-1.231,news{S}weeper at {S}em{E}val-2020 Task 11: Context-Aware Rich Feature Representations for Propaganda Classification,2020,-1,-1,4,0,15325,paramansh singh,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"This paper describes our submissions to SemEval 2020 Task 11: Detection of Propaganda Techniques in News Articles for each of the two subtasks of Span Identification and Technique Classification. We make use of pre-trained BERT language model enhanced with tagging techniques developed for the task of Named Entity Recognition (NER), to develop a system for identifying propaganda spans in the text. For the second subtask, we incorporate contextual features in a pre-trained RoBERTa model for the classification of propaganda techniques. We were ranked 5th in the propaganda technique classification subtask."
2020.semeval-1.282,problem{C}onquero at {S}em{E}val-2020 Task 12: Transformer and Soft Label-based Approaches,2020,-1,-1,4,0,15384,karishma laud,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"In this paper, we present various systems submitted by our team problemConquero for SemEval-2020 Shared Task 12 {``}Multilingual Offensive Language Identification in Social Media{''}. We participated in all the three sub-tasks of OffensEval-2020, and our final submissions during the evaluation phase included transformer-based approaches and a soft label-based approach. BERT based fine-tuned models were submitted for each language of sub-task A (offensive tweet identification). RoBERTa based fine-tuned model for sub-task B (automatic categorization of offense types) was submitted. We submitted two models for sub-task C (offense target identification), one using soft labels and the other using BERT based fine-tuned model. Our ranks for sub-task A were Greek-19 out of 37, Turkish-22 out of 46, Danish-26 out of 39, Arabic-39 out of 53, and English-20 out of 85. We achieved a rank of 28 out of 43 for sub-task B. Our best rank for sub-task C was 20 out of 39 using BERT based fine-tuned model."
2020.finnlp-1.14,{IITK} at the {F}in{S}im Task: Hypernym Detection in Financial Domain via Context-Free and Contextualized Word Embeddings,2020,-1,-1,3,0,5933,vishal keswani,Proceedings of the Second Workshop on Financial Technology and Natural Language Processing,0,None
2020.coling-main.251,Adapting a Language Model for Controlled Affective Text Generation,2020,-1,-1,4,0,21348,tushar goswamy,Proceedings of the 28th International Conference on Computational Linguistics,0,"Human use language not just to convey information but also to express their inner feelings and mental states. In this work, we adapt the state-of-the-art language generation models to generate affective (emotional) text. We posit a model capable of generating affect-driven and topic focused sentences without losing grammatical correctness as the affect intensity increases. We propose to incorporate emotion as prior for the probabilistic state-of-the-art text generation model such as GPT-2. The model gives a user the flexibility to control the category and intensity of emotion as well as the topic of the generated text. Previous attempts at modelling fine-grained emotions fall out on grammatical correctness at extreme intensities, but our model is resilient to this and delivers robust results at all intensities. We conduct automated evaluations and human studies to test the performance of our model, and provide a detailed comparison of the results with other models. In all evaluations, our model outperforms existing affective text generation models."
S19-1032,Generating Animations from Screenplays,2019,23,2,6,0,25234,yeyao zhang,Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*{SEM} 2019),0,"Automatically generating animation from natural language text finds application in a number of areas e.g. movie script writing, instructional videos, and public safety. However, translating natural language text into animation is a challenging task. Existing text-to-animation systems can handle only very simple sentences, which limits their applications. In this paper, we develop a text-to-animation system which is capable of handling complex sentences. We achieve this by introducing a text simplification step into the process. Building on an existing animation generation system for screenwriting, we create a robust NLP pipeline to extract information from screenplays and map them to the system{'}s knowledge base. We develop a set of linguistic transformation rules that simplify complex sentences. Information extracted from the simplified sentences is used to generate a rough storyboard and video depicting the text. Our sentence simplification module outperforms existing systems in terms of BLEU and SARI metrics.We further evaluated our system via a user study: 68{\%} participants believe that our system generates reasonable animation from input screenplays."
N19-1374,Affect-Driven Dialog Generation,2019,0,5,3,0.833333,8676,pierre colombo,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"The majority of current systems for end-to-end dialog generation focus on response quality without an explicit control over the affective content of the responses. In this paper, we present an affect-driven dialog system, which generates emotional responses in a controlled manner using a continuous representation of emotions. The system achieves this by modeling emotions at a word and sequence level using: (1) a vector representation of the desired emotion, (2) an affect regularizer, which penalizes neutral words, and (3) an affect sampling method, which forces the neural network to generate diverse words that are emotionally relevant. During inference, we use a re-ranking procedure that aims to extract the most emotionally relevant responses using a human-in-the-loop optimization process. We study the performance of our system in terms of both quantitative (BLEU score and response diversity), and qualitative (emotional appropriateness) measures."
N19-1376,Topic Spotting using Hierarchical Networks with Self Attention,2019,0,0,2,0,2985,pooja chitkara,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"Success of deep learning techniques have renewed the interest in development of dialogue systems. However, current systems struggle to have consistent long term conversations with the users and fail to build rapport. Topic spotting, the task of automatically inferring the topic of a conversation, has been shown to be helpful in making dialog system more engaging and efficient. We propose a hierarchical model with self attention for topic spotting. Experiments on the Switchboard corpus show the superior performance of our model over previously proposed techniques for topic spotting and deep models for text classification. Additionally, in contrast to offline processing of dialog, we also analyze the performance of our model in a more realistic setting i.e. in an online setting where the topic is identified in real time as the dialog progresses. Results show that our model is able to generalize even with limited information in the online setting."
W18-6236,Disney at {IEST} 2018: Predicting Emotions using an Ensemble,2018,0,1,3,0,26272,wojciech witon,"Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"This paper describes our participating system in the WASSA 2018 shared task on emotion prediction. The task focuses on implicit emotion prediction in a tweet. In this task, keywords corresponding to the six emotion labels used (anger, fear, disgust, joy, sad, and surprise) have been removed from the tweet text, making emotion prediction implicit and the task challenging. We propose a model based on an ensemble of classifiers for prediction. Each classifier uses a sequence of Convolutional Neural Network (CNN) architecture blocks and uses ELMo (Embeddings from Language Model) as an input. Our system achieves a 66.2{\%} F1 score on the test set. The best performing system in the shared task has reported a 71.4{\%} F1 score."
S18-1119,{S}em{E}val-2018 Task 11: Machine Comprehension Using Commonsense Knowledge,2018,0,45,3,0,25223,simon ostermann,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"This report summarizes the results of the SemEval 2018 task on machine comprehension using commonsense knowledge. For this machine comprehension task, we created a new corpus, MCScript. It contains a high number of questions that require commonsense knowledge for finding the correct answer. 11 teams from 4 different countries participated in this shared task, most of them used neural approaches. The best performing system achieves an accuracy of 83.95{\%}, outperforming the baselines by a large margin, but still far from the human upper bound, which was found to be at 98{\%}."
L18-1011,Multi-layer Annotation of the Rigveda,2018,0,0,3,0,14905,oliver hellwig,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1564,{MCS}cript: A Novel Dataset for Assessing Machine Comprehension Using Script Knowledge,2018,21,11,2,0,25223,simon ostermann,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"We introduce a large dataset of narrative texts and questions about these texts, intended to be used in a machine comprehension task that requires reasoning using commonsense knowledge. Our dataset complements similar datasets in that we focus on stories about everyday activities, such as going to the movies or working in the garden, and that the questions require commonsense knowledge, or more specifically, script knowledge, to be answered. We show that our mode of data collection via crowdsourcing results in a substantial amount of such inference questions. The dataset forms the basis of a shared task on commonsense and script knowledge organized at SemEval 2018 and provides challenging test cases for the broader natural language understanding community."
S17-1015,A Mixture Model for Learning Multi-Sense Word Embeddings,2017,40,10,3,0,22809,dai nguyen,Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*{SEM} 2017),0,"Word embeddings are now a standard technique for inducing meaning representations for words. For getting good representations, it is important to take into account different senses of a word. In this paper, we propose a mixture model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks."
Q17-1003,Modeling Semantic Expectation: Using Script Knowledge for Referent Prediction,2017,8,6,1,1,422,ashutosh modi,Transactions of the Association for Computational Linguistics,0,"Recent research in psycholinguistics has provided increasing evidence that humans predict upcoming content. Prediction also affects perception and might be a key to robustness in human language processing. In this paper, we investigate the factors that affect human prediction by building a computational model that can predict upcoming discourse referents based on linguistic knowledge alone vs. linguistic knowledge jointly with common-sense knowledge in the form of scripts. We find that script knowledge significantly improves model estimates of human predictions. In a second study, we test the highly controversial hypothesis that predictability influences referring expression type but do not find evidence for such an effect."
L16-1555,{I}n{S}cript: Narrative texts annotated with script information,2016,0,11,1,1,422,ashutosh modi,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This paper presents the InScript corpus (Narrative Texts Instantiating Script structure). InScript is a corpus of 1,000 stories centered around 10 different scenarios. Verbs and noun phrases are annotated with event and participant types, respectively. Additionally, the text is annotated with coreference information. The corpus shows rich lexical variation and will serve as a unique resource for the study of the role of script knowledge in natural language processing."
K16-1008,Event Embeddings for Semantic Script Modeling,2016,19,15,1,1,422,ashutosh modi,Proceedings of The 20th {SIGNLL} Conference on Computational Natural Language Learning,0,None
S15-1024,Learning to predict script events from domain-specific text,2015,14,1,3,0,8348,rachel rudinger,Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics,0,"The automatic induction of scripts (Schank and Abelson, 1977) has been the focus of many recent works. In this paper, we employ a variety of these methods to learn Schank and Abelsonxe2x80x99s canonical restaurant script, using a novel dataset of restaurant narratives we have compiled from a website called xe2x80x9cDinners from Hell.xe2x80x9d Our models learn narrative chains, script-like structures that we evaluate with the xe2x80x9cnarrative clozexe2x80x9d task (Chambers and Jurafsky, 2008)."
W14-1606,Inducing Neural Models of Script Knowledge,2014,26,34,1,1,422,ashutosh modi,Proceedings of the Eighteenth Conference on Computational Natural Language Learning,0,"Induction of common sense knowledge about prototypical sequence of events has recently received much attention (e.g., Chambers and Jurafsky (2008); Regneri et al. (2010)). Instead of inducing this knowledge in the form of graphs, as in much of the previous work, in our method, distributed representations of event realizations are computed based on distributed representations of predicates and their arguments, and then these representations are used to predict prototypical event orderings. The parameters of the compositional process for computing the event representations and the ranking component of the model are jointly estimated. We show that this approach results in a substantial boost in performance on the event ordering task with respect to the previous approaches, both on natural and crowdsourced texts."
W12-1901,Unsupervised Induction of Frame-Semantic Representations,2012,26,15,1,1,422,ashutosh modi,Proceedings of the {NAACL}-{HLT} Workshop on the Induction of Linguistic Structure,0,"The frame-semantic parsing task is challenging for supervised techniques, even for those few languages where relatively large amounts of labeled data are available. In this preliminary work, we consider unsupervised induction of frame-semantic representations. An existing state-of-the-art Bayesian model for PropBank-style unsupervised semantic role induction (Titov and Klementiev, 2012) is extended to jointly induce semantic frames and their roles. We evaluate the model performance both quantitatively and qualitatively by comparing the induced representation against FrameNet annotations."
