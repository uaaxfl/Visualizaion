2021.semeval-1.117,{HITMI}{\\&}{T} at {S}em{E}val-2021 Task 5: Integrating Transformer and {CRF} for Toxic Spans Detection,2021,-1,-1,3,0,1943,chenyi wang,Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),0,"This paper introduces our system at SemEval-2021 Task 5: Toxic Spans Detection. The task aims to accurately locate toxic spans within a text. Using BIO tagging scheme, we model the task as a token-level sequence labeling task. Our system uses a single model built on the model of multi-layer bidirectional transformer encoder. And we introduce conditional random field (CRF) to make the model learn the constraints between tags. We use ERNIE as pre-trained model, which is more suitable for the task accroding to our experiments. In addition, we use adversarial training with the fast gradient method (FGM) to improve the robustness of the system. Our system obtains 69.85{\%} F1 score, ranking 3rd for the official evaluation."
2021.naacl-main.311,Self-Training for Unsupervised Neural Machine Translation in Unbalanced Training Data Scenarios,2021,-1,-1,6,1,4177,haipeng sun,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Unsupervised neural machine translation (UNMT) that relies solely on massive monolingual corpora has achieved remarkable results in several translation tasks. However, in real-world scenarios, massive monolingual corpora do not exist for some extremely low-resource languages such as Estonian, and UNMT systems usually perform poorly when there is not adequate training corpus for one language. In this paper, we first define and analyze the unbalanced training data scenario for UNMT. Based on this scenario, we propose UNMT self-training mechanisms to train a robust UNMT system and improve its performance in this case. Experimental results on several language pairs show that the proposed methods substantially outperform conventional UNMT systems."
2021.findings-acl.144,Discriminative Reasoning for Document-level Relation Extraction,2021,-1,-1,3,0,7852,wang xu,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.acl-short.99,Issues with Entailment-based Zero-shot Text Classification,2021,-1,-1,4,0,12607,tingting ma,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"The general format of natural language inference (NLI) makes it tempting to be used for zero-shot text classification by casting any target label into a sentence of hypothesis and verifying whether or not it could be entailed by the input, aiming at generic classification applicable on any specified label space. In this opinion piece, we point out a few overlooked issues that are yet to be discussed in this line of work. We observe huge variance across different classification datasets amongst standard BERT-based NLI models and surprisingly find that pre-trained BERT without any fine-tuning can yield competitive performance against BERT fine-tuned for NLI. With the concern that these models heavily rely on spurious lexical patterns for prediction, we also experiment with preliminary approaches for more robust NLI, but the results are in general negative. Our observations reveal implicit but challenging difficulties in entailment-based zero-shot text classification."
2020.semeval-1.145,{CN}-{HIT}-{MI}.{T} at {S}em{E}val-2020 Task 8: Memotion Analysis Based on {BERT},2020,-1,-1,4,0,15195,zhen li,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"Internet memes emotion recognition is focused by many researchers. In this paper, we adopt BERT and ResNet for evaluation of detecting the emotions of Internet memes. We focus on solving the problem of data imbalance and data contains noise. We use RandAugment to enhance the data of the picture, and use Training Signal Annealing (TSA) to solve the impact of the imbalance of the label. At the same time, a new loss function is designed to ensure that the model is not affected by input noise which will improve the robustness of the model. We participated in sub-task a and our model based on BERT obtains 34.58{\%} macro F1 score, ranking 10/32."
2020.emnlp-main.149,Cross Copy Network for Dialogue Generation,2020,-1,-1,7,0,20202,changzhen ji,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"In the past few years, audiences from different fields witness the achievements of sequence-to-sequence models (e.g., LSTM+attention, Pointer Generator Networks and Transformer) to enhance dialogue content generation. While content fluency and accuracy often serve as the major indicators for model training, dialogue logics, carrying critical information for some particular domains, are often ignored. Take customer service and court debate dialogue as examples, compatible logics can be observed across different dialogue instances, and this information can provide vital evidence for utterance generation. In this paper, we propose a novel network architecture - Cross Copy Networks (CCN) to explore the current dialog context and similar dialogue instances{'} logical structure simultaneously. Experiments with two tasks, court debate and customer service content generation, proved that the proposed algorithm is superior to existing state-of-art content generation models."
2020.coling-main.248,Robust Machine Reading Comprehension by Learning Soft labels,2020,-1,-1,5,0,21344,zhenyu zhao,Proceedings of the 28th International Conference on Computational Linguistics,0,"Neural models have achieved great success on the task of machine reading comprehension (MRC), which are typically trained on hard labels. We argue that hard labels limit the model capability on generalization due to the label sparseness problem. In this paper, we propose a robust training method for MRC models to address this problem. Our method consists of three strategies, 1) label smoothing, 2) word overlapping, 3) distribution prediction. All of them help to train models on soft labels. We validate our approach on the representative architecture - ALBERT. Experimental results show that our method can greatly boost the baseline with 1{\%} improvement in average, and achieve state-of-the-art performance on NewsQA and QUOREF."
2020.coling-main.374,Robust Unsupervised Neural Machine Translation with Adversarial Denoising Training,2020,27,1,7,1,4177,haipeng sun,Proceedings of the 28th International Conference on Computational Linguistics,0,"Unsupervised neural machine translation (UNMT) has recently attracted great interest in the machine translation community. The main advantage of the UNMT lies in its easy collection of required large training text sentences while with only a slightly worse performance than supervised neural machine translation which requires expensive annotated translation pairs on some translation tasks. In most studies, the UMNT is trained with clean data without considering its robustness to the noisy data. However, in real-world scenarios, there usually exists noise in the collected input sentences which degrades the performance of the translation system since the UNMT is sensitive to the small perturbations of the input sentences. In this paper, we first time explicitly take the noisy data into consideration to improve the robustness of the UNMT based systems. First of all, we clearly defined two types of noises in training sentences, i.e., word noise and word order noise, and empirically investigate its effect in the UNMT, then we propose adversarial training methods with denoising process in the UNMT. Experimental results on several language pairs show that our proposed methods substantially improved the robustness of the conventional UNMT systems in noisy scenarios."
2020.coling-main.510,Learning to Decouple Relations: Few-Shot Relation Classification with Entity-Guided Attention and Confusion-Aware Training,2020,-1,-1,7,0,20172,yingyao wang,Proceedings of the 28th International Conference on Computational Linguistics,0,"This paper aims to enhance the few-shot relation classification especially for sentences that jointly describe multiple relations. Due to the fact that some relations usually keep high co-occurrence in the same context, previous few-shot relation classifiers struggle to distinguish them with few annotated instances. To alleviate the above relation confusion problem, we propose CTEG, a model equipped with two novel mechanisms to learn to decouple these easily-confused relations. On the one hand, an Entity -Guided Attention (EGA) mechanism, which leverages the syntactic relations and relative positions between each word and the specified entity pair, is introduced to guide the attention to filter out information causing confusion. On the other hand, a Confusion-Aware Training (CAT) method is proposed to explicitly learn to distinguish relations by playing a pushing-away game between classifying a sentence into a true relation and its confusing relation. Extensive experiments are conducted on the FewRel dataset, and the results show that our proposed model achieves comparable and even much better results to strong baselines in terms of accuracy. Furthermore, the ablation test and case study verify the effectiveness of our proposed EGA and CAT, especially in addressing the relation confusion problem."
2020.ccl-1.102,{CAN}-{GRU}: a Hierarchical Model for Emotion Recognition in Dialogue,2020,-1,-1,3,0,22168,ting jiang,Proceedings of the 19th Chinese National Conference on Computational Linguistics,0,"Emotion recognition in dialogue systems has gained attention in the field of natural language processing recent years, because it can be applied in opinion mining from public conversational data on social media. In this paper, we propose a hierarchical model to recognize emotions in the dialogue. In the first layer, in order to extract textual features of utterances, we propose a convolutional self-attention network(CAN). Convolution is used to capture n-gram information and attention mechanism is used to obtain the relevant semantic information among words in the utterance. In the second layer, a GRU-based network helps to capture contextual information in the conversation. Furthermore, we discuss the effects of unidirectional and bidirectional networks. We conduct experiments on Friends dataset and EmotionPush dataset. The results show that our proposed model(CAN-GRU) and its variants achieve better performance than baselines."
2020.autosimtrans-1.2,End-to-End Speech Translation with Adversarial Training,2020,-1,-1,3,0,22311,xuancai li,Proceedings of the First Workshop on Automatic Simultaneous Translation,0,"End-to-End speech translation usually leverages audio-to-text parallel data to train an available speech translation model which has shown impressive results on various speech translation tasks. Due to the artificial cost of collecting audio-to-text parallel data, the speech translation is a natural low-resource translation scenario, which greatly hinders its improvement. In this paper, we proposed a new adversarial training method to leverage target monolingual data to relieve the low-resource shortcoming of speech translation. In our method, the existing speech translation model is considered as a Generator to gain a target language output, and another neural Discriminator is used to guide the distinction between outputs of speech translation model and true target monolingual sentences. Experimental results on the CCMT 2019-BSTC dataset speech translation task demonstrate that the proposed methods can significantly improve the performance of the End-to-End speech translation system."
2020.acl-main.324,Knowledge Distillation for Multilingual Unsupervised Neural Machine Translation,2020,37,1,6,1,4177,haipeng sun,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Unsupervised neural machine translation (UNMT) has recently achieved remarkable results for several language pairs. However, it can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time. That is, research on multilingual UNMT has been limited. In this paper, we empirically introduce a simple method to translate between thirteen languages using a single encoder and a single decoder, making use of multilingual data to improve UNMT for all language pairs. On the basis of the empirical findings, we propose two knowledge distillation methods to further enhance multilingual UNMT performance. Our experiments on a dataset with English translated to and from twelve other languages (including three language families and six language branches) show remarkable results, surpassing strong unsupervised individual baselines while achieving promising performance between non-English language pairs in zero-shot translation scenarios and alleviating poor performance in low-resource language pairs."
2020.acl-main.380,Demographics Should Not Be the Reason of Toxicity: Mitigating Discrimination in Text Classifications with Instance Weighting,2020,32,0,6,0,22872,guanhua zhang,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"With the recent proliferation of the use of text classifications, researchers have found that there are certain unintended biases in text classification datasets. For example, texts containing some demographic identity-terms (e.g., {``}gay{''}, {``}black{''}) are more likely to be abusive in existing abusive language detection datasets. As a result, models trained with these datasets may consider sentences like {``}She makes me happy to be gay{''} as abusive simply because of the word {``}gay.{''} In this paper, we formalize the unintended biases in text classification datasets as a kind of selection bias from the non-discrimination distribution to the discrimination distribution. Based on this formalization, we further propose a model-agnostic debiasing training framework by recovering the non-discrimination distribution using instance weighting, which does not require any extra resources or annotations apart from a pre-defined set of demographic identity-terms. Experiments demonstrate that our method can effectively alleviate the impacts of the unintended biases without significantly hurting models{'} generalization ability."
S19-2101,{CN}-{HIT}-{MI}.{T} at {S}em{E}val-2019 Task 6: Offensive Language Identification Based on {B}i{LSTM} with Double Attention,2019,0,0,3,0,15196,yaojie zhang,Proceedings of the 13th International Workshop on Semantic Evaluation,0,"Offensive language has become pervasive in social media. In Offensive Language Identification tasks, it may be difficult to predict accurately only according to the surface words. So we try to dig deeper semantic information of text. This paper presents use an attention-based two layers bidirectional longshort memory neural network (BiLSTM) for semantic feature extraction. Additionally, a residual connection mechanism is used to synthesize two different deep features, and an emoji attention mechanism is used to extract semantic information of emojis in text. We participated in three sub-tasks of SemEval 2019 Task 6 as CN-HIT-MI.T team. Our macro-averaged F1-score in sub-task A is 0.768, ranking 28/103. We got 0.638 in sub-task B, ranking 30/75. In sub-task C, we got 0.549, ranking 22/65. We also tried some other methods of not submitting results."
P19-1119,Unsupervised Bilingual Word Embedding Agreement for Unsupervised Neural Machine Translation,2019,0,7,6,1,4177,haipeng sun,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Unsupervised bilingual word embedding (UBWE), together with other technologies such as back-translation and denoising, has helped unsupervised neural machine translation (UNMT) achieve remarkable results in several language pairs. In previous methods, UBWE is first trained using non-parallel monolingual corpora and then this pre-trained UBWE is used to initialize the word embedding in the encoder and decoder of UNMT. That is, the training of UBWE and UNMT are separate. In this paper, we first empirically investigate the relationship between UBWE and UNMT. The empirical findings show that the performance of UNMT is significantly affected by the performance of UBWE. Thus, we propose two methods that train UNMT with UBWE agreement. Empirical results on several language pairs show that the proposed methods significantly outperform conventional UNMT."
P19-1296,Sentence-Level Agreement for Neural Machine Translation,2019,0,3,7,0,25711,mingming yang,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"The training objective of neural machine translation (NMT) is to minimize the loss between the words in the translated sentences and those in the references. In NMT, there is a natural correspondence between the source sentence and the target sentence. However, this relationship has only been represented using the entire neural network and the training objective is computed in word-level. In this paper, we propose a sentence-level agreement module to directly minimize the difference between the representation of source and target sentence. The proposed agreement module can be integrated into NMT as an additional training objective function and can also be used to enhance the representation of the source sentences. Empirical results on the NIST Chinese-to-English and WMT English-to-German tasks show the proposed agreement module can significantly improve the NMT performance."
P19-1435,Selection Bias Explorations and Debias Methods for Natural Language Sentence Matching Datasets,2019,39,0,8,0,22872,guanhua zhang,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Natural Language Sentence Matching (NLSM) has gained substantial attention from both academics and the industry, and rich public datasets contribute a lot to this process. However, biased datasets can also hurt the generalization performance of trained models and give untrustworthy evaluation results. For many NLSM datasets, the providers select some pairs of sentences into the datasets, and this sampling procedure can easily bring unintended pattern, i.e., selection bias. One example is the QuoraQP dataset, where some content-independent naive features are unreasonably predictive. Such features are the reflection of the selection bias and termed as the {``}leakage features.{''} In this paper, we investigate the problem of selection bias on six NLSM datasets and find that four out of them are significantly biased. We further propose a training and evaluation framework to alleviate the bias. Experimental results on QuoraQP suggest that the proposed framework can improve the generalization ability of trained models, and give more trustworthy evaluation results for real-world adoptions."
N19-1046,{U}nderstanding and {I}mproving {H}idden {R}epresentations for {N}eural {M}achine {T}ranslation,2019,0,0,5,0,22556,guanlin li,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"Multilayer architectures are currently the gold standard for large-scale neural machine translation. Existing works have explored some methods for understanding the hidden representations, however, they have not sought to improve the translation quality rationally according to their understanding. Towards understanding for performance improvement, we first artificially construct a sequence of nested relative tasks and measure the feature generalization ability of the learned hidden representation over these tasks. Based on our understanding, we then propose to regularize the layer-wise representations with all tree-induced tasks. To overcome the computational bottleneck resulting from the large number of regularization terms, we design efficient approximation methods by selecting a few coarse-to-fine tasks for regularization. Extensive experiments on two widely-used datasets demonstrate the proposed methods only lead to small extra overheads in training but no additional overheads in testing, and achieve consistent improvements (up to +1.3 BLEU) compared to the state-of-the-art translation model."
N19-1205,Improving Neural Machine Translation with Neural Syntactic Distance,2019,0,3,5,1,10519,chunpeng ma,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"The explicit use of syntactic information has been proved useful for neural machine translation (NMT). However, previous methods resort to either tree-structured neural networks or long linearized sequences, both of which are inefficient. Neural syntactic distance (NSD) enables us to represent a constituent tree using a sequence whose length is identical to the number of words in the sentence. NSD has been used for constituent parsing, but not in machine translation. We propose five strategies to improve NMT with NSD. Experiments show that it is not trivial to improve NMT with NSD; however, the proposed strategies are shown to improve translation performance of the baseline model (+2.1 (En{--}Ja), +1.3 (Ja{--}En), +1.2 (En{--}Ch), and +1.0 (Ch{--}En) BLEU)."
D19-1570,Understanding Data Augmentation in Neural Machine Translation: Two Perspectives towards Generalization,2019,0,0,5,0,22556,guanlin li,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Many Data Augmentation (DA) methods have been proposed for neural machine translation. Existing works measure the superiority of DA methods in terms of their performance on a specific test set, but we find that some DA methods do not exhibit consistent improvements across translation tasks. Based on the observation, this paper makes an initial attempt to answer a fundamental question: what benefits, which are consistent across different methods and tasks, does DA in general obtain? Inspired by recent theoretic advances in deep learning, the paper understands DA from two perspectives towards the generalization ability of a model: input sensitivity and prediction margin, which are defined independent of specific test set thereby may lead to findings with relatively low variance. Extensive experiments show that relatively consistent benefits across five DA methods and four translation tasks are achieved regarding both perspectives."
P18-1061,Neural Document Summarization by Jointly Learning to Score and Select Sentences,2018,0,49,6,0,6625,qingyu zhou,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Sentence scoring and sentence selection are two main steps in extractive document summarization systems. However, previous works treat them as two separated subtasks. In this paper, we present a novel end-to-end neural network framework for extractive document summarization by jointly learning to score and select sentences. It first reads the document sentences with a hierarchical encoder to obtain the representation of sentences. Then it builds the output summary by extracting sentences one by one. Different from previous methods, our approach integrates the selection strategy into the scoring model, which directly predicts the relative importance given previously selected sentences. Experiments on the CNN/Daily Mail dataset show that the proposed framework significantly outperforms the state-of-the-art extractive summarization models."
P18-1116,Forest-Based Neural Machine Translation,2018,0,1,4,1,10519,chunpeng ma,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Tree-based neural machine translation (NMT) approaches, although achieved impressive performance, suffer from a major drawback: they only use the 1-best parse tree to direct the translation, which potentially introduces translation mistakes due to parsing errors. For statistical machine translation (SMT), forest-based methods have been proven to be effective for solving this problem, while for NMT this kind of approach has not been attempted. This paper proposes a forest-based NMT method that translates a linearized packed forest under a simple sequence-to-sequence framework (i.e., a forest-to-sequence NMT model). The BLEU score of the proposed method is higher than that of the sequence-to-sequence NMT, tree-based NMT, and forest-based SMT systems."
I17-1002,Context-Aware Smoothing for Neural Machine Translation,2017,12,2,5,0.833333,4178,kehai chen,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),0,"In Neural Machine Translation (NMT), each word is represented as a low-dimension, real-value vector for encoding its syntax and semantic information. This means that even if the word is in a different sentence context, it is represented as the fixed vector to learn source representation. Moreover, a large number of Out-Of-Vocabulary (OOV) words, which have different syntax and semantic information, are represented as the same vector representation of {``}unk{''}. To alleviate this problem, we propose a novel context-aware smoothing method to dynamically learn a sentence-specific vector for each word (including OOV words) depending on its local context words in a sentence. The learned context-aware representation is integrated into the NMT to improve the translation performance. Empirical results on NIST Chinese-to-English translation task show that the proposed approach achieves 1.78 BLEU improvements on average over a strong attentional NMT, and outperforms some existing systems."
D17-1304,Neural Machine Translation with Source Dependency Representation,2017,14,17,7,0.833333,4178,kehai chen,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"Source dependency information has been successfully introduced into statistical machine translation. However, there are only a few preliminary attempts for Neural Machine Translation (NMT), such as concatenating representations of source word and its dependency label together. In this paper, we propose a novel NMT with source dependency representation to improve translation performance of NMT, especially long sentences. Empirical results on NIST Chinese-to-English translation task show that our method achieves 1.6 BLEU improvements on average over a strong NMT system."
L16-1466,Building A Case-based Semantic {E}nglish-{C}hinese Parallel Treebank,2016,0,0,2,0,35192,huaxing shi,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We construct a case-based English-to-Chinese semantic constituent parallel Treebank for a Statistical Machine Translation (SMT) task by labelling each node of the Deep Syntactic Tree (DST) with our refined semantic cases. Since subtree span-crossing is harmful in tree-based SMT, DST is adopted to alleviate this problem. At the same time, we tailor an existing case set to represent bilingual shallow semantic relations more precisely. This Treebank is a part of a semantic corpus building project, which aims to build a semantic bilingual corpus annotated with syntactic, semantic cases and word senses. Data in our Treebank is from the news domain of Datum corpus. 4,000 sentence pairs are selected to cover various lexicons and part-of-speech (POS) n-gram patterns as much as possible. This paper presents the construction of this case Treebank. Also, we have tested the effect of adopting DST structure in alleviating subtree span-crossing. Our preliminary analysis shows that the compatibility between Chinese and English trees can be significantly increased by transforming the parse-tree into the DST. Furthermore, the human agreement rate in annotation is found to be acceptable (90{\%} in English nodes, 75{\%} in Chinese nodes)."
C16-1171,A Distribution-based Model to Learn Bilingual Word Embeddings,2016,25,21,2,1,35773,hailong cao,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"We introduce a distribution based model to learn bilingual word embeddings from monolingual data. It is simple, effective and does not require any parallel data or any seed lexicon. We take advantage of the fact that word embeddings are usually in form of dense real-valued low-dimensional vector and therefore the distribution of them can be accurately estimated. A novel cross-lingual learning objective is proposed which directly matches the distributions of word embeddings in one language with that in the other language. During the joint learning process, we dynamically estimate the distributions of word embeddings in two languages respectively and minimize the dissimilarity between them through standard back propagation algorithm. Our learned bilingual word embeddings allow to group each word and its translations together in the shared vector space. We demonstrate the utility of the learned embeddings on the task of finding word-to-word translations from monolingual corpora. Our model achieved encouraging performance on data in both related languages and substantially different languages."
C16-1236,Constraint-Based Question Answering with Knowledge Graph,2016,22,38,5,1,4598,junwei bao,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"WebQuestions and SimpleQuestions are two benchmark data-sets commonly used in recent knowledge-based question answering (KBQA) work. Most questions in them are {`}simple{'} questions which can be answered based on a single relation in the knowledge base. Such data-sets lack the capability of evaluating KBQA systems on complicated questions. Motivated by this issue, we release a new data-set, namely ComplexQuestions, aiming to measure the quality of KBQA systems on {`}multi-constraint{'} questions which require multiple knowledge base relations to get the answer. Beside, we propose a novel systematic KBQA approach to solve multi-constraint questions. Compared to state-of-the-art methods, our approach not only obtains comparable results on the two existing benchmark data-sets, but also achieves significant improvements on the ComplexQuestions."
P15-1048,Efficient Disfluency Detection with Transition-based Parsing,2015,22,2,4,0.625,7862,shuangzhi wu,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Automatic speech recognition (ASR) outputs often contain various disfluencies. It is necessary to remove these disfluencies before processing downstream tasks. In this paper, an efficient disfluency detection approach based on right-to-left transitionbased parsing is proposed, which can efficiently identify disfluencies and keep ASR outputs grammatical. Our method exploits a global view to capture long-range dependencies for disfluency detection by integrating a rich set of syntactic and disfluency features with linear complexity. The experimental results show that our method outperforms state-of-the-art work and achieves a 85.1% f-score on the commonly used English Switchboard test set. We also apply our method to in-house annotated Chinese data and achieve a significantly higher f-score compared to the baseline of CRF-based approach."
P14-1091,Knowledge-Based Question Answering as Machine Translation,2014,25,70,4,1,4598,junwei bao,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"A typical knowledge-based question answering (KB-QA) system faces two challenges: one is to transform natural language questions into their meaning representations (MRs); the other is to retrieve answers from knowledge bases (KBs) using generated MRs. Unlike previous methods which treat them in a cascaded manner, we present a translation-based approach to solve these two tasks in one unified framework. We translate questions to answers based on CYK parsing. Answers as translations of the span covered by each CYK cell are obtained by a question translation method, which first generates formal triple queries as MRs for the span based on question patterns and relation expressions, and then retrieves answers from a given KB based on triple queries generated. A linear model is defined over derivations, and minimum error rate training is used to tune feature weights based on a set of question-answer pairs. Compared to a KB-QA system using a state-of-the-art semantic parser, our method achieves better results."
D14-1174,Improving Pivot-Based Statistical Machine Translation by Pivoting the Co-occurrence Count of Phrase Pairs,2014,26,11,6,1,40155,xiaoning zhu,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"To overcome the scarceness of bilingual corpora for some language pairs in machine translation, pivot-based SMT uses pivot language as a bridge to generate source-target translation from sourcepivot and pivot-target translation. One of the key issues is to estimate the probabilities for the generated phrase pairs. In this paper, we present a novel approach to calculate the translation probability by pivoting the co-occurrence count of source-pivot and pivot-target phrase pairs. Experimental results on Europarl data and web data show that our method leads to significant improvements over the baseline systems."
C14-1108,A Lexicalized Reordering Model for Hierarchical Phrase-based Translation,2014,22,5,5,1,35773,hailong cao,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"Lexicalized reordering model plays a central role in phrase-based statistical machine translation systems. The reordering model specifies the orientation for each phrase and calculates its probability conditioned on the phrase. In this paper, we describe the necessity and the challenge of introducing such a reordering model for hierarchical phrase-based translation. To deal with the challenge, we propose a novel lexicalized reordering model which is built directly on synchronous rules. For each target phrase contained in a rule, we calculate its orientation probability conditioned on the rule. We test our model on both small and large scale data. On NIST machine translation test sets, our reordering model achieved a 0.6-1.2 BLEU point improvements for Chinese-English translation over a strong baseline hierarchical phrase-based system."
C14-1210,Soft Dependency Matching for Hierarchical Phrase-based Machine Translation,2014,25,1,4,1,35773,hailong cao,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"This paper proposes a soft dependency matching model for hierarchical phrase-based (HPB) machine translation. When a HPB rule is extracted, we enrich it with dependency knowledge automatically learnt from the training data. The dependency knowledge not only encodes the dependency relations between the components inside the rule, but also contains the dependency relations between the rule and its context. When a rule is applied to translate a sentence, the dependency knowledge is used to compute the syntactic structural consistency of the rule against the dependency tree of the sentence. We characterize the structure consistency by three features and integrate them into the standard SMT log-linear model to guide the translation process. Our method is evaluated on multiple Chinese-to-English machine translation test sets. The experimental results show that our soft matching model achieves 0.7-1.4 BLEU points improvements over a strong baseline of an in-house implemented HPB translation system."
P13-2056,Cross-lingual Projections between Languages from Different Families,2013,13,4,2,1,3281,mo yu,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Cross-lingual projection methods can benefit from resource-rich languages to improve performances of NLP tasks in resources-scarce languages. However, these methods confronted the difficulty of syntactic differences between languages especially when the pair of languages varies greatly. To make the projection method well-generalize to diverse languages pairs, we enhance the projection method based on word alignments by introducing target-language word representations as features and proposing a novel noise removing method based on these word representations. Experiments showed that our methods improve the performances greatly on projections between English and Chinese."
P13-2070,A Tightly-coupled Unsupervised Clustering and Bilingual Alignment Model for Transliteration,2013,23,2,2,0,41429,tingting li,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Machine Transliteration is an essential task for many NLP applications. However, names and loan words typically originate from various languages, obey different transliteration rules, and therefore may benefit from being modeled independently. Recently, transliteration models based on Bayesian learning have overcome issues with over-fitting allowing for many-to-many alignment in the training of transliteration models. We propose a novel coupled Dirichlet process mixture model (cDPMM) that simultaneously clusters and bilingually aligns transliteration data within a single unified model. The unified model decomposes into two classes of non-parametric Bayesian component models: a Dirichlet process mixture model for clustering, and a set of multinomial Dirichlet process models that perform bilingual alignment independently for each cluster. The experimental results show that our method considerably outperforms conventional alignment models."
P13-1078,Additive Neural Networks for Statistical Machine Translation,2013,35,31,4,1,3591,lemao liu,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Most statistical machine translation (SMT) systems are modeled using a loglinear framework. Although the log-linear model achieves success in SMT, it still suffers from some limitations: (1) the features are required to be linear with respect to the model itself; (2) features cannot be further interpreted to reach their potential. A neural network is a reasonable method to address these pitfalls. However, modeling SMT with a neural network is not trivial, especially when taking the decoding efficiency into consideration. In this paper, we propose a variant of a neural network, i.e. additive neural networks, for SMT to go beyond the log-linear translation model. In addition, word embedding is employed as the input to the neural network, which encodes each word as a feature vector. Our model outperforms the log-linear translation models with/without embedding features on Chinese-to-English and Japanese-to-English translation tasks."
P13-1079,Hierarchical Phrase Table Combination for Machine Translation,2013,30,2,4,1,20203,conghui zhu,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Typical statistical machine translation systems are batch trained with a given training data and their performances are largely influenced by the amount of data. With the growth of the available data across different domains, it is computationally demanding to perform batch training every time when new data comes. In face of the problem, we propose an efficient phrase table combination method. In particular, we train a Bayesian phrasal inversion transduction grammars for each domain separately. The learned phrase tables are hierarchically combined as if they are drawn from a hierarchical Pitman-Yor process. The performance measured by BLEU is at least as comparable to the traditional batch training method. Furthermore, each phrase table is trained separately in each domain, and while computational overhead is significantly reduced by training them in parallel."
N13-1063,Compound Embedding Features for Semi-supervised Learning,2013,14,22,2,1,3281,mo yu,Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"To solve data sparsity problem, recently there has been a trend in discriminative methods of NLP to use representations of lexical items learned from unlabeled data as features. In this paper, we investigated the usage of word representations learned by neural language models, i.e. word embeddings. The direct usage has disadvantages such as large amount of computation, inadequacy with dealing word ambiguity and rare-words, and the problem of linear non-separability. To overcome these problems, we instead built compound features from continuous word embeddings based on clustering. Experiments showed that the compound features not only improved the performances on several NLP tasks, but also ran faster, suggesting the potential of embeddings."
I13-1032,Tuning {SMT} with a Large Number of Features via Online Feature Grouping,2013,17,2,2,1,3591,lemao liu,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"In this paper, we consider the tuning of statistical machine translation (SMT) models employing a large number of features. We argue that existing tuning methods for these models suffer serious sparsity problems, in which features appearing in the tuning data may not appear in the testing data and thus those features may be over tuned in the tuning data. As a result, we face an over-fitting problem, which limits the generalization abilities of the learned models. Based on our analysis, we propose a novel method based on feature grouping via OSCAR to overcome these pitfalls. Our feature grouping is implemented within an online learning framework and thus it is efficient for a large scale (both for features and examples) of learning in our scenario. Experiment results on IWSLT translation tasks show that the proposed method significantly outperforms the state of the art tuning methods."
I13-1128,Repairing Incorrect Translation with Examples,2013,11,2,4,0,22051,junguo zhu,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"This paper proposes an example driven approach to improve the quality of MT system outputs. Specifically, We extend the system combination method in SMT to combine the examples by two strategies: 1) estimating the confidence of examples by the similarity between source input and the source part of examples; 2) approximating target word posterior probability by the word alignments of the bilingual examples. Experimental results show a significant improvement of 0.64 BLEU score as compared to one online translation service (Google Translate)."
D13-1050,Improving Pivot-Based Statistical Machine Translation Using Random Walk,2013,23,12,6,1,40155,xiaoning zhu,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"This paper proposes a novel approach that utilizes a machine learning method to improve pivot-based statistical machine translation (SMT). For language pairs with few bilingual data, a possible solution in pivot-based SMT using another language as a bridge to generate source-target translation. However, one of the weaknesses is that some useful sourcetarget translations cannot be generated if the corresponding source phrase and target phrase connect to different pivot phrases. To alleviate the problem, we utilize Markov random walks to connect possible translation phrases between source and target language. Experimental results on European Parliament data, spoken language data and web data show that our method leads to significant improvements on all the tasks over the baseline system."
W12-4407,Syllable-based Machine Transliteration with Extra Phrase Features,2012,11,4,3,0,41430,chunyue zhang,Proceedings of the 4th Named Entity Workshop ({NEWS}) 2012,0,"This paper describes our syllable-based phrase transliteration system for the NEWS 2012 shared task on English-Chinese track and its back. Grapheme-based Transliteration maps the character(s) in the source side to the target character(s) directly. However, character-based segmentation on English side will cause ambiguity in alignment step. In this paper we utilize Phrase-based model to solve machine transliteration with the mapping between Chinese characters and English syllables rather than English characters. Two heuristic rule-based syllable segmentation algorithms are applied. This transliteration model also incorporates three phonetic features to enhance discriminative ability for phrase. The primary system achieved 0.330 on Chinese-English and 0.177 on English-Chinese in terms of top-1 accuracy."
D12-1037,Locally Training the Log-Linear Model for {SMT},2012,32,18,4,1,3591,lemao liu,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"In statistical machine translation, minimum error rate training (MERT) is a standard method for tuning a single weight with regard to a given development data. However, due to the diversity and uneven distribution of source sentences, there are two problems suffered by this method. First, its performance is highly dependent on the choice of a development set, which may lead to an unstable performance for testing. Second, translations become inconsistent at the sentence level since tuning is performed globally on a document level. In this paper, we propose a novel local training method to address these two problems. Unlike a global training method, such as MERT, in which a single weight is learned and used for all the input sentences, we perform training and testing in one step by learning a sentence-wise weight for each input sentence. We propose efficient incremental training methods to put the local training into practice. In NIST Chinese-to-English translation tasks, our local training method significantly outperforms MERT with the maximal improvements up to 2.0 BLEU points, meanwhile its efficiency is comparable to that of the global method."
C12-2071,Expected Error Minimization with Ultraconservative Update for {SMT},2012,23,2,2,1,3591,lemao liu,Proceedings of {COLING} 2012: Posters,0,"Minimum error rate training is a popular method for parameter tuning in statistical machine translation (SMT). However, the optimization objective function may change drastically at each optimization step, which may induce MERT instability. We propose an alternative tuning method based on an ultraconservative update, in which the combination of an expected task loss and the distance from the parameters in the previous round are minimized with a variant of gradient descent. Experiments on test datasets of both Chinese-to-English and Spanish-toEnglish translation show that our method can achieve improvements over MERT under the Moses system."
2012.iwslt-evaluation.8,The {HIT}-{LTRC} machine translation system for {IWSLT} 2012,2012,12,1,4,1,40155,xiaoning zhu,Proceedings of the 9th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"In this paper, we describe HIT-LTRC's participation in the IWSLT 2012 evaluation campaign. In this year, we took part in the Olympics Task which required the participants to translate Chinese to English with limited data. Our system is based on Moses[1], which is an open source machine translation system. We mainly used the phrase-based models to carry out our experiments, and factored-based models were also performed in comparison. All the involved tools are freely available. In the evaluation campaign, we focus on data selection, phrase extraction method comparison and phrase table combination."
P11-1016,Target-dependent {T}witter Sentiment Classification,2011,30,464,5,0,44646,long jiang,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"Sentiment analysis on Twitter data has attracted much attention recently. In this paper, we focus on target-dependent Twitter sentiment classification; namely, given a query, we classify the sentiments of the tweets as positive, negative or neutral according to whether they contain positive, negative or neutral sentiments about that query. Here the query serves as the target of the sentiments. The state-of-the-art approaches for solving this problem always adopt the target-independent strategy, which may assign irrelevant sentiments to the given target. Moreover, the state-of-the-art approaches only take the tweet to be classified into consideration when classifying the sentiment; they ignore its context (i.e., related tweets). However, because tweets are usually short and more ambiguous, sometimes it is not enough to consider only the current tweet for sentiment classification. In this paper, we propose to improve target-dependent Twitter sentiment classification by 1) incorporating target-dependent features; and 2) taking related tweets into consideration. According to the experimental results, our approach greatly improves the performance of target-dependent sentiment classification."
2011.mtsummit-papers.28,A Unified and Discriminative Soft Syntactic Constraint Model for Hierarchical Phrase-based Translation,2011,-1,-1,2,1,3591,lemao liu,Proceedings of Machine Translation Summit XIII: Papers,0,None
2011.mtsummit-papers.65,Hypergraph Training and Decoding of System Combination in {SMT},2011,-1,-1,2,0,44910,yupeng liu,Proceedings of Machine Translation Summit XIII: Papers,0,None
W10-4115,Exploring Deep Belief Network for {C}hinese Relation Extraction,2010,18,9,5,0,3161,yu chen,{CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,"Relation extraction is a fundamental task in information extraction that identifies the semantic relationships between two entities in the text. In this paper, a novel model based on Deep Belief Network (DBN) is first presented to detect and classify the relations among Chinese entities. The experiments conducted on the Automatic Content Extraction (ACE) 2004 dataset demonstrate that the proposed approach is effective in handling high dimensional feature space including character N-grams, entity types and the position information. It outperforms the stateof-the-art learning models such as SVM or BP neutral network."
W10-2416,Using Deep Belief Nets for {C}hinese Named Entity Categorization,2010,15,15,5,0,3161,yu chen,Proceedings of the 2010 Named Entities Workshop,0,"Identifying named entities is essential in understanding plain texts. Moreover, the categories of the named entities are indicative of their roles in the texts. In this paper, we propose a novel approach, Deep Belief Nets (DBN), for the Chinese entity mention categorization problem. DBN has very strong representation power and it is able to elaborately self-train for discovering complicated feature combinations. The experiments conducted on the Automatic Context Extraction (ACE) 2004 data set demonstrate the effectiveness of DBN. It outperforms the state-of-the-art learning models such as SVM or BP neural network."
S10-1067,{PKU}{\\_}{HIT}: An Event Detection System Based on Instances Expansion and Rich Syntactic Features,2010,6,0,3,0,27698,shiqi li,Proceedings of the 5th International Workshop on Semantic Evaluation,0,"This paper describes the PKU_HIT system on event detection in the SemEval-2010 Task. We construct three modules for the three sub-tasks of this evaluation. For target verb WSD, we build a Naive Bayesian classifier which uses additional training instances expanded from an untagged Chinese corpus automatically. For sentence SRL and event detection, we use a feature-based machine learning method which makes combined use of both constituent-based and dependency-based features. Experimental results show that the Macro Accuracy of the WSD module reaches 83.81% and F-Score of the SRL module is 55.71%."
S10-1083,{P}eng{Y}uan@{PKU}: Extracting Infrequent Sense Instance with the Same N-Gram Pattern for the {S}em{E}val-2010 Task 15,2010,11,1,4,1,1751,pengyuan liu,Proceedings of the 5th International Workshop on Semantic Evaluation,0,"This paper describes our infrequent sense identification system participating in the SemEval-2010 task 15 on Infrequent Sense Identification for Mandarin Text to Speech Systems. The core system is a supervised system based on the ensembles of Naive Bayesian classifiers. In order to solve the problem of unbalanced sense distribution, we intentionally extract only instances of infrequent sense with the same N-gram pattern as the complement training data from an untagged Chinese corpus -- People's Daily of the year 2001. At the same time, we adjusted the prior probability to adapt to the distribution of the test data and tuned the smoothness coefficient to take the data sparseness into account. Official result shows that, our system ranked the first with the best Macro Accuracy 0.952. We briefly describe this system, its configuration options and the features used for this task and present some discussion of the results."
P10-2002,A Joint Rule Selection Model for Hierarchical Phrase-Based Translation,2010,25,13,5,0,9515,lei cui,Proceedings of the {ACL} 2010 Conference Short Papers,0,"In hierarchical phrase-based SMT systems, statistical models are integrated to guide the hierarchical rule selection for better translation performance. Previous work mainly focused on the selection of either the source side of a hierarchical rule or the target side of a hierarchical rule rather than considering both of them simultaneously. This paper presents a joint model to predict the selection of hierarchical rules. The proposed model is estimated based on four sub-models where the rich context knowledge from both source and target sides is leveraged. Our method can be easily incorporated into the practical SMT systems with the log-linear model framework. The experimental results show that our method can yield significant improvements in performance."
C10-2025,Hybrid Decoding: Decoding with Partial Hypotheses Combination over Multiple {SMT} Systems,2010,22,4,5,0,9515,lei cui,Coling 2010: Posters,0,"In this paper, we present hybrid decoding --- a novel statistical machine translation (SMT) decoding paradigm using multiple SMT systems. In our work, in addition to component SMT systems, system combination method is also employed in generating partial translation hypotheses throughout the decoding process, in which smaller hypotheses generated by each component decoder and hypotheses combination are used in the following decoding steps to generate larger hypotheses. Experimental results on NIST evaluation data sets for Chinese-to-English machine translation (MT) task show that our method can not only achieve significant improvements over individual decoders, but also bring substantial gains compared with a state-of-the-art word-level system combination method."
C10-2076,Combining Constituent and Dependency Syntactic Views for {C}hinese Semantic Role Labeling,2010,30,3,3,0,27698,shiqi li,Coling 2010: Posters,0,"This paper presents a novel feature-based semantic role labeling (SRL) method which uses both constituent and dependency syntactic views. Comparing to the traditional SRL method relying on only one syntactic view, the method has a much richer set of syntactic features. First we select several important constituent-based and dependency-based features from existing studies as basic features. Then, we propose a statistical method to select discriminative combined features which are composed by the basic features. SRL is achieved by using the SVM classifier with both the basic features and the combined features. Experimental results on Chinese Proposition Bank (CPB) show that the method outperforms the traditional constituent-based or dependency-based SRL methods."
C10-2080,Reexamination on Potential for Personalization in Web Search,2010,24,0,5,0,46469,daren li,Coling 2010: Posters,0,"Various strategies have been proposed to enhance web search through utilizing individual user information. However, considering the well acknowledged recurring queries and repetitive clicks among users, it is still an open issue whether using individual user information is a proper direction of efforts in improving the web search. In this paper, we first quantitatively demonstrate that individual user information is more beneficial than common user information. Then we statistically compare the benefit of individual and common user information through Kappa statistic. Finally, we calculate potential for personalization to present an overview of what queries can benefit more from individual user information. All these analyses are conducted on both English AOL log and Chinese Sogou log, and a bilingual perspective statistics consistently confirms our findings."
C10-2086,Head-modifier Relation based Non-lexical Reordering Model for Phrase-Based Translation,2010,30,2,3,0,45617,shui liu,Coling 2010: Posters,0,"Phrase-based statistical MT (SMT) is a milestone in MT. However, the translation model in the phrase based SMT is structure free which greatly limits its reordering capacity. To address this issue, we propose a non-lexical head-modifier based reordering model on word level by utilizing constituent based parse tree in source side. Our experimental results on the NIST Chinese-English benchmarking data show that, with a very small size model, our method significantly outperforms the baseline by 1.48% bleu score."
C10-2138,"Utilizing Variability of Time and Term Content, within and across Users in Session Detection",2010,15,0,5,0,2937,shuqi sun,Coling 2010: Posters,0,"In this paper, we describe a SVM classification framework of session detection task on both Chinese and English query logs. With eight features on the aspects of temporal and content information extracted from pairs of successive queries, the classification models achieve significantly superior performance than the stat-of-the-art method. Additionally, we find through ROC analysis that there exists great discrimination power variability among different features and within the same feature across different users. To fully utilize this variability, we build local models for individual users and combine their predictions with those from the global model. Experiments show that the local models do make significant improvements to the global model, although the amount is small."
C10-2175,All in Strings: a Powerful String-based Automatic {MT} Evaluation Metric with Multiple Granularities,2010,26,9,5,0,22051,junguo zhu,Coling 2010: Posters,0,"String-based metrics of automatic machine translation (MT) evaluation are widely applied in MT research. Meanwhile, some linguistic motivated metrics have been suggested to improve the string-based metrics in sentence-level evaluation. In this work, we attempt to change their original calculation units (granularities) of string-based metrics to generate new features. We then propose a powerful string-based automatic MT evaluation metric, combining all the features with various granularities based on SVM rank and regression models. The experimental results show that i) the new features with various granularities can contribute to the automatic evaluation of translation quality; ii) our proposed string-based metrics with multiple granularities based on SVM regression model can achieve higher correlations with human assessments than the state-of-art automatic metrics."
W09-3106,Train the Machine with What It Can {L}earn{---}{C}orpus Selection for {SMT},2009,14,4,3,1,32292,xiwu han,Proceedings of the 2nd Workshop on Building and Using Comparable Corpora: from Parallel to Non-parallel Corpora ({BUCC}),0,"Statistical machine translation relies heavily on available parallel corpora, but SMT may not have the ability or intelligence to make full use of the training set. Instead of collecting more and more parallel training corpora, this paper aims to improve SMT performance by exploiting the full potential of existing parallel corpora. We first identify literally translated sentence pairs via lexical and grammatical compatibility, and then use these data to train SMT models. One experiment indicates that larger training corpora do not always lead to higher decoding performance when the added data are not literal translations. And another experiment shows that properly enlarging the contribution of literal translation can improve SMT performance significantly."
W09-2305,References Extension for the Automatic Evaluation of {MT} by Syntactic Hybridization,2009,11,0,2,1,7051,bo wang,Proceedings of the Third Workshop on Syntax and Structure in Statistical Translation ({SSST}-3) at {NAACL} {HLT} 2009,0,"Because of the variations of the languages, the coverage of the references is very important to the reference based automatic evaluation of machine translation systems. We propose a method to extend the reference set of the automatic evaluation only based on multiple manual references and their syntactic structures. In our approach, the syntactic equivalents in the reference sentences are identified and hybridized to generate new references. The new method need no external knowledge and can obtain the equivalents of long subsegments of reference sentences. The experimental results show that using the extended reference set the popular automatic evaluation metrics achieve better correlations with the human assessments."
W09-2306,A Study of Translation Rule Classification for Syntax-based Statistical Machine Translation,2009,24,0,4,0,44874,hongfei jiang,Proceedings of the Third Workshop on Syntax and Structure in Statistical Translation ({SSST}-3) at {NAACL} {HLT} 2009,0,"Recently, numerous statistical machine translation models which can utilize various kinds of translation rules are proposed. In these models, not only the conventional syntactic rules but also the non-syntactic rules can be applied. Even the pure phrase rules are includes in some of these models. Although the better performances are reported over the conventional phrase model and syntax model, the mixture of diversified rules still leaves much room for study. In this paper, we present a refined rule classification system. Based on this classification system, the rules are classified according to different standards, such as lexicalization level and generalization. Especially, we refresh the concepts of the structure reordering rules and the discontiguous phrase rules. This novel classification system may supports the SMT research community with some helpful references."
P09-2032,A Statistical Machine Translation Model Based on a Synthetic Synchronous Grammar,2009,9,2,3,0,44874,hongfei jiang,Proceedings of the {ACL}-{IJCNLP} 2009 Conference Short Papers,0,"Recently, various synchronous grammars are proposed for syntax-based machine translation, e.g. synchronous context-free grammar and synchronous tree (sequence) substitution grammar, either purely formal or linguistically motivated. Aiming at combining the strengths of different grammars, we describes a synthetic synchronous grammar (SSG), which tentatively in this paper, integrates a synchronous context-free grammar (SCFG) and a synchronous tree sequence substitution grammar (STSSG) for statistical machine translation. The experimental results on NIST MT05 Chinese-to-English test set show that the SSG based translation system achieves significant improvement over three baseline systems."
P09-2054,{C}hinese Term Extraction Using Different Types of Relevance,2009,13,8,2,1,43936,yuhang yang,Proceedings of the {ACL}-{IJCNLP} 2009 Conference Short Papers,0,"This paper presents a new term extraction approach using relevance between term candidates calculated by a link analysis based method. Different types of relevance are used separately or jointly for term verification. The proposed approach requires no prior domain knowledge and no adaptation for new domains. Consequently, the method can be used in any domain corpus and it is especially useful for resource-limited domains. Evaluations conducted on two different domains for Chinese term extraction show significant improvements over existing techniques and also verify the efficiency and relative domain independent nature of the approach."
yang-etal-2008-chinese-term,{C}hinese Term Extraction Based on Delimiters,2008,10,2,3,1,43936,yuhang yang,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Existing techniques extract term candidates by looking for internal and contextual information associated with domain specific terms. The algorithms always face the dilemma that fewer features are not enough to distinguish terms from non-terms whereas more features lead to more conflicts among selected features. This paper presents a novel approach for term extraction based on delimiters which are much more stable and domain independent. The proposed approach is not as sensitive to term frequency as that of previous works. This approach has no strict limit or hard rules and thus they can deal with all kinds of terms. It also requires no prior domain knowledge and no additional training to adapt to new domains. Consequently, the proposed approach can be applied to different domains easily and it is especially useful for resource-limited domains. Evaluations conducted on two different domains for Chinese term extraction show significant improvements over existing techniques which verifies its efficiency and domain independent nature. Experiments on new term extraction indicate that the proposed approach can also serve as an effective tool for domain lexicon expansion."
C08-1130,{C}hinese Term Extraction Using Minimal Resources,2008,19,9,3,1,43936,yuhang yang,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"This paper presents a new approach for term extraction using minimal resources. A term candidate extraction algorithm is proposed to identify features of the relatively stable and domain independent term delimiters rather than that of the terms. For term verification, a link analysis based method is proposed to calculate the relevance between term candidates and the sentences in the domain specific corpus from which the candidates are extracted. The proposed approach requires no prior domain knowledge, no general corpora, no full segmentation and minimal adaptation for new domains. Consequently, the method can be used in any domain corpus and it is especially useful for resource-limited domains. Evaluations conducted on two different domains for Chinese term extraction show quite significant improvements over existing techniques and also verify the efficiency and relative domain independent nature of the approach. Experiments on new term extraction also indicate that the approach is quite effective for identifying new terms in a domain making it useful for domain knowledge update."
C08-1141,Diagnostic Evaluation of Machine Translation Systems Using Automatically Constructed Linguistic Check-Points,2008,14,23,6,0.202305,4082,ming zhou,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"We present a diagnostic evaluation platform which provides multi-factored evaluation based on automatically constructed check-points. A check-point is a linguistically motivated unit (e.g. an ambiguous word, a noun phrase, a verb~obj collocation, a prepositional phrase etc.), which are pre-defined in a linguistic taxonomy. We present a method that automatically extracts check-points from parallel sentences. By means of checkpoints, our method can monitor a MT system in translating important linguistic phenomena to provide diagnostic evaluation. The effectiveness of our approach for diagnostic evaluation is verified through experiments on various types of MT systems."
W07-2418,The Extraction of Trajectories from Real Texts Based on Linear Classification,2007,5,7,2,0,45610,hanjing li,Proceedings of the 16th Nordic Conference of Computational Linguistics ({NODALIDA} 2007),0,"Text-to-scene conversion systems need to share the spatial descriptions between natural language and the 3D scene. Such applications are the ideal candidates for the extraction of spatial relations from free texts, in which the extraction to trajectories that are focus objects in spatial descriptions is an essential problem. We present an analysis of how the space relations are described in Chinese. Based on this study, we propose a method where the extraction of trajectories is modeled as a binary classification problem and resolved based on a linear classifier with syntactic features. Moreover, experimental results are analyzed in detail to demonstrate the effectiveness of the linear classifier to the extraction problem of the trajectory concept."
W07-0709,Meta-Structure Transformation Model for Statistical Machine Translation,2007,19,3,2,0,49042,jiadong sun,Proceedings of the Second Workshop on Statistical Machine Translation,0,"We propose a novel syntax-based model for statistical machine translation in which meta-structure (MS) and meta-structure sequence (SMS) of a parse tree are defined. In this framework, a parse tree is decomposed into SMS to deal with the structure divergence and the alignment can be reconstructed at different levels of recombination of MS (RM). RM pairs extracted can perform the mapping between the sub-structures across languages. As a result, we have got not only the translation for the target language, but an SMS of its parse tree at the same time. Experiments with BLEU metric show that the model significantly outperforms Pharaoh, a state-art-the-art phrase-based system."
S07-1035,{HIT}-{WSD}: Using Search Engine for Multilingual {C}hinese-{E}nglish Lexical Sample Task,2007,8,2,2,1,1751,pengyuan liu,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"We have participated in the Multilingual Chinese-English Lexical Sample Task of SemEval-2007. Our system disambiguates senses of Chinese words and finds the correct translation in English by using the web as WSD knowledge source. Since all the statistic data is obtained from search engine, the method is considered to be unsupervised and does not require any sense-tagged corpus."
P07-1087,A Unified Tagging Approach to Text Normalization,2007,14,14,5,1,20203,conghui zhu,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"This paper addresses the issue of text normalization, an important yet often overlooked problem in natural language processing. By text normalization, we mean converting xe2x80x98informally inputtedxe2x80x99 text into the canonical form, by eliminating xe2x80x98noisesxe2x80x99 in the text and detecting paragraph and sentence boundaries in the text. Previously, text normalization issues were often undertaken in an ad-hoc fashion or studied separately. This paper first gives a formalization of the entire problem. It then proposes a unified tagging approach to perform the task using Conditional Random Fields (CRF). The paper shows that with the introduction of a small set of tags, most of the text normalization tasks can be performed within the approach. The accuracy of the proposed method is high, because the subtasks of normalization are interdependent and should be performed together. Experimental results on email data cleaning show that the proposed method significantly outperforms the approach of using cascaded models and that of employing independent models."
P06-2043,Improving {E}nglish Subcategorization Acquisition with Diathesis Alternations as Heuristic Information,2006,13,0,2,1,32292,xiwu han,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"Automatically acquired lexicons with subcategorization information have already proved accurate and useful enough for some purposes but their accuracy still shows room for improvement. By means of diathesis alternation, this paper proposes a new filtering method, which improved the performance of Korhonen's acquisition system remarkably, with the precision increased to 91.18% and recall unchanged, making the acquired lexicon much more practical for further manual proofreading and other NLP uses."
O06-3001,Two-Fold Filtering for {C}hinese Subcategorization Acquisition with Diathesis Alternations Used as Heuristic Information,2006,80,1,2,1,32292,xiwu han,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 11, Number 2, June 2006",0,"Automatically acquired lexicons with subcategorization information have been shown to be accurate and useful for some purposes, but their accuracy still shows room for improvement and their usefulness in many applications remains to be investigated. This paper proposes a two-fold filtering method, which in experiments improved the performance of a Chinese acquisition system remarkably, with an increased precision rate of 76.94% and a recall rate of 83.83%, making the acquired lexicon much more practical for further manual proofreading and other NLP uses. And as far as we know, at the present time, these figures represent the best overall performance achieved in Chinese subcategorization acquisition and in similar researches focusing on other languages."
I05-2003,A Hybrid {C}hinese Language Model based on a Combination of Ontology with Statistical Method,2005,15,1,2,0,36315,dequan zheng,Companion Volume to the Proceedings of Conference including Posters/Demos and tutorial abstracts,0,"In this paper, we present a hybrid Chinese language model based on a combination of ontology with statistical method. In this study, we determined the structure of such a Chinese language model. This structure is firstly comprised of an ontology description framework for Chinese words and a representation of Chinese lingual ontology knowledge. Subsequently, a Chinese lingual ontology knowledge bank is automatically acquired by determining, for each word, its cooccurrence with semantic, pragmatics, and syntactic information from the training corpus and the usage of Chinese words will be gotten from lingual ontology knowledge bank for a actual document. To evaluate the performance of this language model, we completed two groups of experiments on texts reordering for Chinese information retrieval and texts similarity computing. Compared with previous works, the proposed method improved the precision of nature language processing."
C04-1104,Subcategorization Acquisition and Evaluation for {C}hinese Verbs,2004,10,10,2,1,32292,xiwu han,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"This paper describes the technology and an experiment of subcategorization acquisition for Chinese verbs. The SCF hypotheses are generated by means of linguistic heuristic information and filtered via statistical methods. Evaluation on the acquisition of 20 multi-pattern verbs shows that our experiment achieved the similar precision and recall with former researches. Besides, simple application of the acquired lexicon to a PCFG parser indicates great potentialities of subcategorization information in the fields of NLP."
W02-1613,Automatic Information Transfer between {E}nglish and {C}hinese,2002,1,0,3,0,12358,jianmin yao,{COLING}-02: Machine Translation in Asia,0,"The translation choice and transfer modules in an English Chinese machine translation system are introduced. The translation choice is realized on basis of a grammar tree and takes the context as a word bag, with the lexicon and POS tag information as context features. The Bayes minimal error probability is taken as the evaluation function of the candidate translation. The rule-based transfer and generation module takes the parsing tree as the input and operates on the information of POS tag, semantics or even the lexicon."
C02-1003,Learning {C}hinese Bracketing Knowledge Based on a Bilingual Language Model,2002,7,12,3,1,37737,yajuan lu,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"This paper proposes a new method for automatic acquisition of Chinese bracketing knowledge from English-Chinese sentence-aligned bilingual corpora. Bilingual sentence pairs are first aligned in syntactic structure by combining English parse trees with a statistical bilingual language model. Chinese bracketing knowledge is then extracted automatically. The preliminary experiments show automatically learned knowledge accords well with manually annotated brackets. The proposed method is particularly useful to acquire bracketing knowledge for a less studied language that lacks tools and resources found in a second language more studied. Although this paper discusses experiments with Chinese and English, the method is also applicable to other language pairs."
C02-1057,An Automatic Evaluation Method for Localization Oriented Lexicalised {EBMT} System,2002,11,4,3,0,12358,jianmin yao,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"To help developing a localization oriented EBMT system, an automatic machine translation evaluation method is implemented which adopts edit distance, cosine correlation and Dice coefficient as criteria. Experiment shows that the evaluation method distinguishes well between good translations and bad ones. To prove that the method is consistent with human evaluation, 6 MT systems are scored and compared. Theoretical analysis is made to validate the experimental results. Correlation coefficient and significance tests at 0.01 level are made to ensure the reliability of the results. Linear regression equations are calculated to map the automatic scoring results to human scorings."
W01-1813,Automatic Detection of Prosody Phrase Boundaries for Text-to-Speech System,2001,6,2,2,0,7872,xin lv,Proceedings of the Seventh International Workshop on Parsing Technologies,0,"Automatic acquisition of the prosodic phrase boundary detecting rules from the text and speech corpora has always been a difficulty for TTS systems. We collected over 5,000 sentences as the corpus, introduced a method based on the transform-based error-driven learning to get the rules for detecting prosodic phrase boundaries, and then used trees to organize the rules in the TTS system. For using the transformation-based error-driven learning, we designed a set of templates especially. Using 1,000 sentences to get rules for the TTS system can reach 92% accuracy in close-test and 73% accuracy in open-test."
O01-2004,Automatic Translation Template Acquisition Based on Bilingual Structure Alignment,2001,16,11,5,1,37737,yajuan lu,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 6, Number 1, {F}ebruary 2001: Special Issue on Natural Language Processing Researches in {MSRA}",0,Knowledge acquisition is a bottleneck in machine translation and many NLP tasks. A method for automatically acquiring translation templates from bilingual corpora is proposed in this paper. Bilingual sentence pairs are first aligned in syntactic structure by combining a language parsing with a statistical bilingual language model. The alignment results are used to extract translation templates which turn out to be very useful in real machine translation.
W00-1211,Statistics Based Hybrid Approach to {C}hinese Base Phrase Identification,2000,3,20,1,1,1945,tiejun zhao,Second {C}hinese Language Processing Workshop,0,"This paper extends the base noun phrase (BNP) identification into a research on Chinese base phrase identification. After briefly introducing some basic concepts on Chinese base phrase, this paper presents a statistics based hybrid model for identifying 7 types of Chinese base phrases in view. Experiments show the efficiency of the proposed method in simplifying sentence structure. Significance of the research lies in it provides a solid foundation for the Chinese parser."
