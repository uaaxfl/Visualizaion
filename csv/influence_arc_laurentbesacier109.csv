2004.iwslt-papers.1,P04-1077,0,0.0797235,"italized and unknown words, polite and direct forms of address (please … / I would like you to…). between the translation produced by the system and a gold translation associated with a set of paraphrases. We can also cite the GMT [24] score which is the harmonic mean (F-measure) of a new proposed precision and recall measures based on a maximum match size between a candidate and a reference translation. Recently, the R O U G E (Recall-Oriented Understudy for Gisting Evaluation) [12] framework, proposed t o automatically determine the quality of summaries, has also been used for MT evaluation [13]. The same authors also proposed O RANGE (Oracle Ranking for Gisting Evaluation), for evaluating evaluation metrics for machine translation [14]. 4.2.2. For the primary condition, there is an inconsistent ranking for system 5 with BLEU (3rd) and NIST (5th). This system outputs are significantly shorter than the output of the other systems. It impact strongly on the results with a brevity penalty for NIST. A fourth parameter came to us when checking the actual strings produced by the each system. We found different use of case (“Tokyo”, vs. “tokyo”), punctuation (“juice, please” vs. “juice plea"
2004.iwslt-papers.1,C04-1072,0,0.133457,"and a gold translation associated with a set of paraphrases. We can also cite the GMT [24] score which is the harmonic mean (F-measure) of a new proposed precision and recall measures based on a maximum match size between a candidate and a reference translation. Recently, the R O U G E (Recall-Oriented Understudy for Gisting Evaluation) [12] framework, proposed t o automatically determine the quality of summaries, has also been used for MT evaluation [13]. The same authors also proposed O RANGE (Oracle Ranking for Gisting Evaluation), for evaluating evaluation metrics for machine translation [14]. 4.2.2. For the primary condition, there is an inconsistent ranking for system 5 with BLEU (3rd) and NIST (5th). This system outputs are significantly shorter than the output of the other systems. It impact strongly on the results with a brevity penalty for NIST. A fourth parameter came to us when checking the actual strings produced by the each system. We found different use of case (“Tokyo”, vs. “tokyo”), punctuation (“juice, please” vs. “juice please”), digits (spelled-out vs. numerals), abbreviations (“OK” vs. “okay”), compound words (“duty-free” vs. “duty free”), sentence boundaries, and"
2004.iwslt-papers.1,N03-2021,0,0.0529929,"Missing"
2004.iwslt-papers.1,niessen-etal-2000-evaluation,0,0.0927176,"Missing"
2004.iwslt-papers.1,C92-2067,0,0.11536,"Missing"
2004.iwslt-papers.1,takezawa-etal-2002-toward,0,0.0345848,"racters. The MT outputs and the references translation were normalized. We observed differences of ±0.15 on the BLEU scores and ±1.8 on the NIST scores. Those differences are quite important. In the future, system outputs and references will be normalized for common evaluation. Under the secondary condition, the ranking is still inconsistent with BLEU and NIST. For both conditions, C-STAR III systems outperformed Systran systems available free of charge on the web. 4.2. A new evaluation framework from C-STAR III Nonetheless, we would like to promote comparative evaluation on the same data set [22] in the C - STAR III framework. This would enable us to tackle some of the questions raised above. 4.2.1. Pilot closed evaluation (2002) The C-STAR III partners ran a pilot evaluation experiment in year 2003 on our common BTEC corpus for two conditions. Development and test data were picked up for BTEC. For development purposes every kind of resources could be used. The test set consisted of 500 English sentences that had their translation into Italian, Japanese, Korean and Chinese within BTEC. Subjective evaluation followed the L inguistic D ata C onsortium evaluation guidelines for the DARPA"
2004.iwslt-papers.1,2003.mtsummit-papers.51,0,0.061178,"lian. This is mainly due to the characteristics of the 98 4/8 Free Systran® MT systems available on the web have been evaluated on the same test set. All systems scores were inferior to those of the C-STAR partners. However, nothing was done to tune them to that particular task, although that is possible on systranet: choice and order of dictionaries, handling of capitalized and unknown words, polite and direct forms of address (please … / I would like you to…). between the translation produced by the system and a gold translation associated with a set of paraphrases. We can also cite the GMT [24] score which is the harmonic mean (F-measure) of a new proposed precision and recall measures based on a maximum match size between a candidate and a reference translation. Recently, the R O U G E (Recall-Oriented Understudy for Gisting Evaluation) [12] framework, proposed t o automatically determine the quality of summaries, has also been used for MT evaluation [13]. The same authors also proposed O RANGE (Oracle Ranking for Gisting Evaluation), for evaluating evaluation metrics for machine translation [14]. 4.2.2. For the primary condition, there is an inconsistent ranking for system 5 with"
2004.iwslt-papers.1,2001.mtsummit-papers.3,0,0.0226989,"Missing"
2004.iwslt-papers.1,2003.mtsummit-papers.9,0,0.013347,"S4 2,59 (5) 2,30 (5) 0,2733 (5) 5,6830 (4) S5 3,21 (3) 3,74 (3) 0,5542 (3) 3,4013 (5) 4.2.3. 4.3. Problems and proposals 4.3.1. NIST [0..∞[ 0.5542 (1) 3.4013 (3) S2 0,3884 (2) 8.1383 (1) S3 0.2733 (3) 5.6830 (2) Table 9: results of the C-STAR III pilot evaluation under the secondary condition 1 Problems 4.3.1.1 Current metrics don&apos;t measure linguistic quality A first problem is that the figures produced with these techniques are not directly interpretable in terms of translation quality2. A lot of work has been done t o correlate objective evaluation results with subjective evaluation results [6, 7, 19], but the results are inconsistent. BLEU is said to correlate well with human judgments of quality, NIST is said to be better than BLEU for theoretical reasons, but BLEU and NIST give contradictory rankings (see above). Hence, if correlation with human judgments is a measure of the quality of a metrics, NIST cannot be better than BLEU… or the correlation is too weak to be meaningful. Another trouble is that, as reported in ACL-03, these metrics often give quite bad scores to high quality human translations. An experiment has been reported, where each of 15 (human) paraphrases had been tested,"
2004.iwslt-papers.1,2003.mtsummit-papers.32,0,0.207577,"Missing"
2004.iwslt-papers.1,W04-1013,0,0.0121128,"m to that particular task, although that is possible on systranet: choice and order of dictionaries, handling of capitalized and unknown words, polite and direct forms of address (please … / I would like you to…). between the translation produced by the system and a gold translation associated with a set of paraphrases. We can also cite the GMT [24] score which is the harmonic mean (F-measure) of a new proposed precision and recall measures based on a maximum match size between a candidate and a reference translation. Recently, the R O U G E (Recall-Oriented Understudy for Gisting Evaluation) [12] framework, proposed t o automatically determine the quality of summaries, has also been used for MT evaluation [13]. The same authors also proposed O RANGE (Oracle Ranking for Gisting Evaluation), for evaluating evaluation metrics for machine translation [14]. 4.2.2. For the primary condition, there is an inconsistent ranking for system 5 with BLEU (3rd) and NIST (5th). This system outputs are significantly shorter than the output of the other systems. It impact strongly on the results with a brevity penalty for NIST. A fourth parameter came to us when checking the actual strings produced by"
2004.iwslt-papers.1,P02-1040,0,\N,Missing
2004.jeptalnrecital-poster.24,J92-4003,0,0.00664423,"ur d’une classe, a la même probabilité d’occurence, dans ce cas une classe peut se réduire à une liste de mots. Lorsqu’on a définit des classes, on peut alors directement remplacer les mots du corpus d’apprentissage par leur classe avant l’apprentissage du modèle de langage. Afin d&apos;introduire des connaissances sémantiques dans le ML, nous proposons de regrouper certains mots dans des classes correspondant a des entités sémantiques de l&apos;IF. Par exemple, les mots « bien », « d&apos;accord » et « okay » seront des éléments de la classe « c:acknowledgment » tel que le spécifie l&apos;IF. Plusieurs articles [5,6] ont montré l’intérêt de l’utilisation de classes dans diverses tâches de Traitement Automatique de Langue Naturelle. La plupart des méthodes pour constituer automatiquement des classes utilisent des critères statistiques permettant, par exemple, de diminuer la perplexité d’un modèle de langage. Dans notre cas, notre critère de choix de classes est guidé par la définition du langage pivot et par les concepts les plus utilisés dans l’IF. Notre approche consiste en deux étapes : (1) la sélection des IFs les plus fréquentes à intégrer comme classes dans le nouveau modèle de langage (2) la calcul"
2004.jeptalnrecital-poster.24,C02-1170,1,0.779843,"nowledge, affirm, negate …). La figure 2 illustre comment cette sélection des IFs les plus fréquentes est réalisée automatiquement à partir d’un corpus textuel brut non annoté manuellement en IF. Dialogues NESPOLE Analyse auto. en IF délissasses 1 croquantes 42 emmènerais 9 emmènerait 26 badgé 19 badge 3439 faillirent 52 pentateuque 309 tabloïde 17 tabloïds 117 attendriraient Selection Ifs les plus freq. {c:affirm} oui, ouais, euh oui, … {c:acknowledge} okay, d’accord, … {c:thank} merci, merci beaucoup, … … Figure 2: Sélection des IFs les plus fréquentes L’analyseur automatique en IF du CLIPS [7] a été utilisé pour analyser un corpus qui comprend 46 transcriptions de dialogues collectés lors du projet NESPOLE [8]. Ce corpus représente des dialogues possibles entre un client et un agent de voyage concernant l’organisation de vacances, la réservation d’hôtels et les activités sportives ou culturelles, dans la région de Trente en Italie. L’analyseur transforme automatiquement tous ces dialogues en une représentation de langage IF. Nous avons par conséquent un Quang VU-MINH Laurent BESACIER Herve BLANCHON Brigitte BIGI corpus aligné français-IF. Bien sûr, ce corpus n’est pas parfait car l"
2004.jeptalnrecital-poster.5,2003.mtsummit-papers.9,0,0.0227624,"Missing"
2004.jeptalnrecital-poster.5,P02-1040,0,0.0726437,"Missing"
2004.jeptalnrecital-poster.5,2001.mtsummit-papers.62,0,0.0399823,"Missing"
2004.jeptalnrecital-poster.5,takezawa-etal-2002-toward,0,0.0659387,"Missing"
2004.jeptalnrecital-poster.5,2004.jeptalnrecital-poster.24,1,0.779833,"Missing"
2009.iwslt-evaluation.9,J97-2003,0,0.0166598,"ented in Figure 3. The ASR lattice (marked as a “word lattice” in the figure, but it is more accurate to say that it is a lattice made up of “unknown” units) is decomposed according to the different sub-word sets (corresponding to different morphological segmentations). Then we create a new starting node S and a new ending node E for the common lattice. We link the node S with starting nodes of all subword lattices (n°1 and n°2) and link ending nodes of all lattices with E. After this step, all lattices are merged into a common lattice. This operation can also be seen as a “union” of lattices [8]. Finally, the obtained lattice is converted into a CN which will keep both ASR ambiguity and Arabic segmentation ambiguity. This latter CN is the input of the translation system which uses, as in section 3, multiple phrase tables corresponding to multiple Arabic segmenters. 5. 5.1. the IWSLT09 organizers and a few publicly available additional data. Multiple Segmentation Experiments Automatic Speech Recognition Word lattice Vocabulary extraction Lattice of Morphemes n°1 Lattice of Morphemes n°2 Generation of the confusion net (CN) Decoding Fig. 3. Multiple segmentation process for spoken lang"
2009.iwslt-evaluation.9,J03-1002,0,0.00225656,"t, made up of 500 sentences, which corresponds to the IWSLT07 evaluation data (we will refer, in the rest of the paper, to tst07 for this data set). The tuning of the MT model parameters (minimum error rate training) was systematically done on the dev06 subset. As additional data, we first used an Arabic / English bilingual dictionary of around 84k entries. This dictionary can be found online 6 . For English LM training, we also used out-of-domain corpora taken from the LDC’s Gigaword corpus7. Our baseline speech translation system was built using tools available in the MT community: - GIZA++ [9] was used for the alignments, -The moses 8 decoder (and the training / testing scripts associated) was used (2008-07-11 release), - SRILM [10] was used to train the LMs and to deal with ASR word graphs, - The Buckwalter morphological segmenter9 and ASVM (a free http://freedict.cvs.sourceforge.net/freedict/eng-ara/ http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC 2003T05 8 Moses open source project: http://www.statmt.org/moses 9 http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC 2002L49 7 Since 2007, the LIG laboratory participates yearly to the IWSLT evaluation camp"
2009.iwslt-evaluation.9,P02-1040,0,0.0835329,"Missing"
2009.iwslt-evaluation.9,W07-0734,0,0.0312049,"Missing"
2009.jeptalnrecital-court.37,nimaan-etal-2006-towards,0,0.0186253,"n’est pas une tâche triviale et introduit des erreurs à cause des ambiguïtés de la langue naturelle et la présence de mots inconnus dans le texte à segmenter. Alors que le manque de données textuelles a un impact sur la performance des modèles de langage, les erreurs introduites par la segmentation automatique peuvent rendre ces données encore moins exploitables. Une alternative possible consiste à calculer les probabilités à partir d’unités sous-lexicales. Parmi les travaux existants qui utilisent des unités sous-lexicales pour la modélisation du langage, nous pouvons citer (Kurimo, 2006), (Abdillahi, 2006) et (Afify, 2006) qui utilisent les morphèmes respectivement pour la modélisation de l&apos;arabe, du finnois, et du somalien. Pour une langue non-segmentée comme le japonais, le caractère (idéogramme) est utilisé dans (Denoual, 2006). Dans un travail précédent sur la reconnaissance automatique de la parole en langue khmère1 (Seng, 2008), nous avons exploité les différentes unités lexicales et sous-lexicales (mot, syllabe et groupe de caractères2) dans la modélisation du langage de cette langue peu dotée. Nous avons proposé des modèles de langage simples basés sur le mot, la syllabe, le groupe de c"
2009.jeptalnrecital-court.37,Y98-1020,0,0.200433,"Missing"
2009.jeptalnrecital-long.9,P91-1022,0,0.511192,"Missing"
2009.jeptalnrecital-long.9,J93-2003,0,0.0622369,"Missing"
2009.jeptalnrecital-long.9,W01-0809,0,0.062659,"Missing"
2009.jeptalnrecital-long.9,J93-1004,0,0.864307,"Missing"
2009.jeptalnrecital-long.9,J03-3001,0,0.32617,"Missing"
2009.jeptalnrecital-long.9,2005.mtsummit-papers.11,0,0.137455,"Missing"
2009.jeptalnrecital-long.9,P07-2045,0,0.00679477,"Missing"
2009.jeptalnrecital-long.9,ma-2006-champollion,0,0.243014,"Missing"
2009.jeptalnrecital-long.9,P06-1011,0,0.0553339,"Missing"
2009.jeptalnrecital-long.9,J03-1002,0,0.0148253,"Missing"
2010.eamt-1.6,J03-3001,0,0.0523936,"ilingual corpora of source and target languages to build a statistical translation model for source/target languages and a statistical language model for target language. The two models and a search module are then used to decode the best translation (Brown et al, 1993; Koehn et al, 2003). Thus, a large parallel bilingual text corpus is a prerequisite. However, such a corpus is not always available, especially for low e-resourced languages. The most common methods to build parallel corpora consist in automatic methods which collect parallel sentence pairs from the Web (Resnik and Smith, 2003; Kilgarriff and Grefenstette, 2003), or alignment methods which extract parallel documents/sentences from two monolingual corpora (Koehn, 2005; Gale and Church, 1993, Patry and Langlais, 2005). There is also the method of extracting parallel sentence pairs from a comparable corpus (Zhao and Vogel, 2002; Fung and Cheung, 2004; Munteanu and Marcu, 2006). Abdul-Rauf and Schwenk (2009) present a semi-supervised extracting method requiring an initial parallel corpus in order to build a first SMT system that will be used during the semisupervised extraction (see more in section 2.1). We assume that in the case of a low e-resourced la"
2010.eamt-1.6,2005.mtsummit-papers.11,0,0.0558981,"ical language model for target language. The two models and a search module are then used to decode the best translation (Brown et al, 1993; Koehn et al, 2003). Thus, a large parallel bilingual text corpus is a prerequisite. However, such a corpus is not always available, especially for low e-resourced languages. The most common methods to build parallel corpora consist in automatic methods which collect parallel sentence pairs from the Web (Resnik and Smith, 2003; Kilgarriff and Grefenstette, 2003), or alignment methods which extract parallel documents/sentences from two monolingual corpora (Koehn, 2005; Gale and Church, 1993, Patry and Langlais, 2005). There is also the method of extracting parallel sentence pairs from a comparable corpus (Zhao and Vogel, 2002; Fung and Cheung, 2004; Munteanu and Marcu, 2006). Abdul-Rauf and Schwenk (2009) present a semi-supervised extracting method requiring an initial parallel corpus in order to build a first SMT system that will be used during the semisupervised extraction (see more in section 2.1). We assume that in the case of a low e-resourced language pair, even a small parallel corpus might not be available to start developing a SMT system. So, does"
2010.eamt-1.6,N03-1017,0,0.00423283,"based) as well as hybrid approaches. However, research on © 2010 European Association for Machine Translation. statistical MT for low e-resourced languages always faces the challenge of getting enough data to support any particular approach. Statistical machine translation (SMT) uses statistical method based on large parallel bilingual corpora of source and target languages to build a statistical translation model for source/target languages and a statistical language model for target language. The two models and a search module are then used to decode the best translation (Brown et al, 1993; Koehn et al, 2003). Thus, a large parallel bilingual text corpus is a prerequisite. However, such a corpus is not always available, especially for low e-resourced languages. The most common methods to build parallel corpora consist in automatic methods which collect parallel sentence pairs from the Web (Resnik and Smith, 2003; Kilgarriff and Grefenstette, 2003), or alignment methods which extract parallel documents/sentences from two monolingual corpora (Koehn, 2005; Gale and Church, 1993, Patry and Langlais, 2005). There is also the method of extracting parallel sentence pairs from a comparable corpus (Zhao an"
2010.eamt-1.6,P07-2045,0,0.00636007,"en small for this preliminary setup. The corpus C1 contains only 50K correct parallel sentence pairs. The corpus C2 contains 25K correct parallel sentence pairs (withdrawn from C1) and 25K wrong sentence pairs. The corpus D, the input data for extracting process, was built from 10K correct parallel sentence pairs and 10K wrong sentence pairs, which were different from sentence pairs of C1 and C2. The correct and the wrong sentence pairs of D were marked to calculate the precision and the recall later. 3.2 System construction Both systems Sys1 and Sys2 were constructed using the Moses toolkit (Koehn et al., 2007). This toolkit contains all of components needed to train the translation model. It also contains tools for tuning these models using minimum error rate training and for evaluating the translation result using the BLEU score. We used the default settings in Moses: - GIZA++ (Och and Ney 2003) was used for word alignments, the “-alignment” option for phrase extraction was “grow-diag-finaland” - 14 features in total were used in the loglinear model: distortion probabilities (6 features), one tri-gram language model probability, bidirectional translation probabilities (2 features) and lexicon weig"
2010.eamt-1.6,2007.tmi-papers.12,0,0.0455757,"ow e-resourced language pair: Vietnamese-French. The last section concludes and gives some perspectives. 2 2.1 Mining parallel data from comparable corpora Extracting methods A comparable corpus contains data which are not parallel but “still closely related by conveying the same information” (Zhao and Vogel, 2002). It may contain “non-aligned sentences that are nevertheless mostly bilingual translations of the same document” (Fung and Cheung, 2004) or contain “various levels of parallelism, such as words, phrases, clauses, sentences, and discourses, depending on the corpora characteristics” (Kumano et al., 2007). Extracting parallel data from comparable corpus has been presented in some previous works. Zhao and Vogel (2002) propose a maximum likelihood criterion which combines sentence length model and a statistical translation lexicon model extracted from an already existing aligned parallel corpus. An iterative process is applied to retrain the translation lexicon model with the extracted data. Munteanu and Marcu (2006) present a method for extracting parallel sub-sentential fragments from a very non-parallel corpus. Each source language document is translated into target language using a bilingual"
2010.eamt-1.6,ma-2006-champollion,0,0.0161235,"nam, there are only four research groups working on MT (Ho, 2005). We focus on mining a bilingual news corpus from the Web and building a Vietnamese-French statistical machine translation (SMT) system. In a former paper (Do et al., 2009), we have presented a mining method (named Method1) based on publication date, special words and sentence alignment result. Firstly, possible parallel document pairs are filtered by using publishing date and special words (numbers, attached symbols, named entities). Secondly, sentences in a possible parallel document pair are aligned using Champollion toolkit (Ma, 2006), which uses lexical information (lexemes, stop words, a bilingual dictionary, etc.). Finally, parallel sentences pairs are extracted based on the sentence alignment information, which combines document length information and lexical information. This method was applied to mine a text corpus from a Vietnamese daily news website, the Vietnam News Agency 1 (VNA) (containing 20,884 French documents and 54,406 Vietnamese documents). This corpus used is a really comparable corpus because it tends to contain parallel sentences or rough translations of sentences on the same topics. 50,322 parallel se"
2010.eamt-1.6,J03-1002,0,0.00480497,"allel sentence pairs and 10K wrong sentence pairs, which were different from sentence pairs of C1 and C2. The correct and the wrong sentence pairs of D were marked to calculate the precision and the recall later. 3.2 System construction Both systems Sys1 and Sys2 were constructed using the Moses toolkit (Koehn et al., 2007). This toolkit contains all of components needed to train the translation model. It also contains tools for tuning these models using minimum error rate training and for evaluating the translation result using the BLEU score. We used the default settings in Moses: - GIZA++ (Och and Ney 2003) was used for word alignments, the “-alignment” option for phrase extraction was “grow-diag-finaland” - 14 features in total were used in the loglinear model: distortion probabilities (6 features), one tri-gram language model probability, bidirectional translation probabilities (2 features) and lexicon weights (2 features), a phrase penalty, a word penalty and a distortion distance penalty. - A 3-gram target language model was built using the SRILM Toolkit (Stolcke, 2002). The target (English) language model was built from the English part of the entire Europarl corpus. The baseline translatio"
2010.eamt-1.6,E09-1003,0,0.225044,"However, such a corpus is not always available, especially for low e-resourced languages. The most common methods to build parallel corpora consist in automatic methods which collect parallel sentence pairs from the Web (Resnik and Smith, 2003; Kilgarriff and Grefenstette, 2003), or alignment methods which extract parallel documents/sentences from two monolingual corpora (Koehn, 2005; Gale and Church, 1993, Patry and Langlais, 2005). There is also the method of extracting parallel sentence pairs from a comparable corpus (Zhao and Vogel, 2002; Fung and Cheung, 2004; Munteanu and Marcu, 2006). Abdul-Rauf and Schwenk (2009) present a semi-supervised extracting method requiring an initial parallel corpus in order to build a first SMT system that will be used during the semisupervised extraction (see more in section 2.1). We assume that in the case of a low e-resourced language pair, even a small parallel corpus might not be available to start developing a SMT system. So, does a fully unsupervised method, starting with a highly noisy parallel corpus, allow to solve the problem of lacking parallel data? Firstly, it is important to note that we consider that “comparable” and “noisy parallel” have equivalent meanings"
2010.eamt-1.6,J93-2003,0,0.0385353,"atistical, example-based) as well as hybrid approaches. However, research on © 2010 European Association for Machine Translation. statistical MT for low e-resourced languages always faces the challenge of getting enough data to support any particular approach. Statistical machine translation (SMT) uses statistical method based on large parallel bilingual corpora of source and target languages to build a statistical translation model for source/target languages and a statistical language model for target language. The two models and a search module are then used to decode the best translation (Brown et al, 1993; Koehn et al, 2003). Thus, a large parallel bilingual text corpus is a prerequisite. However, such a corpus is not always available, especially for low e-resourced languages. The most common methods to build parallel corpora consist in automatic methods which collect parallel sentence pairs from the Web (Resnik and Smith, 2003; Kilgarriff and Grefenstette, 2003), or alignment methods which extract parallel documents/sentences from two monolingual corpora (Koehn, 2005; Gale and Church, 1993, Patry and Langlais, 2005). There is also the method of extracting parallel sentence pairs from a compar"
2010.eamt-1.6,2006.amta-papers.25,0,0.0277475,"hallenges of this work is to see if such a different starting point (noisy comparable corpus, versus truly parallel corpus) can still lead to the design of an extracting system and also improve the quality of the overall translation system. In our research, we focus on mining the parallel sentence pairs. The translation module S0 is a statistical machine translation system, and filtering module bases on evaluation metric estimated for each sentence pair. Several evaluation metrics are used to determine which one is the most suitable: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006) and a modified PER* (see details in section 3.3). A pair is considered as parallel if its evaluation metric is larger (for BLEU, NIST, PER* metrics) or smaller (for TER metric) than a threshold. The extracted sentence pairs are then combined with the system S0 in several ways to create a new translation module. An iterative process is performed which re-translates the source side by this new translation system, re-calculates the evaluation metric and then re-filters the parallel sentence pairs. We hope that each iteration not only increases the number of extracted parallel sentence pairs but"
2010.eamt-1.6,W04-3208,0,0.399472,"parallel bilingual text corpus is a prerequisite. However, such a corpus is not always available, especially for low e-resourced languages. The most common methods to build parallel corpora consist in automatic methods which collect parallel sentence pairs from the Web (Resnik and Smith, 2003; Kilgarriff and Grefenstette, 2003), or alignment methods which extract parallel documents/sentences from two monolingual corpora (Koehn, 2005; Gale and Church, 1993, Patry and Langlais, 2005). There is also the method of extracting parallel sentence pairs from a comparable corpus (Zhao and Vogel, 2002; Fung and Cheung, 2004; Munteanu and Marcu, 2006). Abdul-Rauf and Schwenk (2009) present a semi-supervised extracting method requiring an initial parallel corpus in order to build a first SMT system that will be used during the semisupervised extraction (see more in section 2.1). We assume that in the case of a low e-resourced language pair, even a small parallel corpus might not be available to start developing a SMT system. So, does a fully unsupervised method, starting with a highly noisy parallel corpus, allow to solve the problem of lacking parallel data? Firstly, it is important to note that we consider that"
2010.eamt-1.6,J93-1004,0,0.341292,"model for target language. The two models and a search module are then used to decode the best translation (Brown et al, 1993; Koehn et al, 2003). Thus, a large parallel bilingual text corpus is a prerequisite. However, such a corpus is not always available, especially for low e-resourced languages. The most common methods to build parallel corpora consist in automatic methods which collect parallel sentence pairs from the Web (Resnik and Smith, 2003; Kilgarriff and Grefenstette, 2003), or alignment methods which extract parallel documents/sentences from two monolingual corpora (Koehn, 2005; Gale and Church, 1993, Patry and Langlais, 2005). There is also the method of extracting parallel sentence pairs from a comparable corpus (Zhao and Vogel, 2002; Fung and Cheung, 2004; Munteanu and Marcu, 2006). Abdul-Rauf and Schwenk (2009) present a semi-supervised extracting method requiring an initial parallel corpus in order to build a first SMT system that will be used during the semisupervised extraction (see more in section 2.1). We assume that in the case of a low e-resourced language pair, even a small parallel corpus might not be available to start developing a SMT system. So, does a fully unsupervised m"
2010.eamt-1.6,W09-0430,1,0.737734,"ental setup). 4 Application for Vietnamese - French language pair Vietnamese is the 14th widely-used language in the world; however research on MT for Vietnamese is rare. The earliest MT system for Vietnamese is the system from the Logos Corporation, developed as an English-Vietnamese system for translating aircraft manuals during the 1970s (Hutchins, 2001). Until now, in Vietnam, there are only four research groups working on MT (Ho, 2005). We focus on mining a bilingual news corpus from the Web and building a Vietnamese-French statistical machine translation (SMT) system. In a former paper (Do et al., 2009), we have presented a mining method (named Method1) based on publication date, special words and sentence alignment result. Firstly, possible parallel document pairs are filtered by using publishing date and special words (numbers, attached symbols, named entities). Secondly, sentences in a possible parallel document pair are aligned using Champollion toolkit (Ma, 2006), which uses lexical information (lexemes, stop words, a bilingual dictionary, etc.). Finally, parallel sentences pairs are extracted based on the sentence alignment information, which combines document length information and le"
2010.jeptalnrecital-court.7,W09-3536,1,0.859817,"Missing"
2010.jeptalnrecital-court.7,C08-1068,1,0.872671,"Missing"
2010.jeptalnrecital-long.28,E09-1003,0,0.0467938,"Missing"
2010.jeptalnrecital-long.28,J93-2003,0,0.0407701,"Missing"
2010.jeptalnrecital-long.28,2009.jeptalnrecital-long.9,1,0.691038,"Missing"
2010.jeptalnrecital-long.28,W04-3208,0,0.0601988,"Missing"
2010.jeptalnrecital-long.28,J93-1004,0,0.366916,"Missing"
2010.jeptalnrecital-long.28,J03-3001,0,0.0677042,"Missing"
2010.jeptalnrecital-long.28,2005.mtsummit-papers.11,0,0.0634365,"Missing"
2010.jeptalnrecital-long.28,N03-1017,0,0.0247781,"Missing"
2010.jeptalnrecital-long.28,P07-2045,0,0.0100265,"Missing"
2010.jeptalnrecital-long.28,2007.tmi-papers.12,0,0.108477,"Missing"
2010.jeptalnrecital-long.28,ma-2006-champollion,0,0.0307919,"Missing"
2010.jeptalnrecital-long.28,J03-1002,0,0.00851935,"Missing"
2010.jeptalnrecital-long.28,2006.amta-papers.25,0,0.107176,"Missing"
2011.eamt-1.24,W05-0909,0,0.0260462,"tences using the test2006 dataset selected via oracle filtering mentioned previously. For each of these sentences, we compare the 1-best and oracle-best features and compute the mean value per feature. This is then used to compute two new sets of weights using the R ESCsum and R ESCprod rescoring strategies, described in the previous section. We implemented our rescoring strategies on the devset and then applied the 2 new sets of weights computed on the testset of n-bests. Evaluation is done at a system level for both the development and testsets using BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005). We also evaluate how many sentences contain the oracle candidates in the top position (rank 1). This is shown in Table 3. The last row in each subsection labeled O R ACLE gives the upper bound on each system, i.e. performance if our algorithm was perfect and all the oracles were placed at position 1. We also perform a Top5-BLEU oracle evaluation (shown in Table 4). The difference between the evaluations in Tables 3 and 4 is that the lat173 ter evaluates on a list of top-5 hypotheses for each sentence instead of the usual comparison of a single translation hypothesis with the reference transl"
2011.eamt-1.24,P08-2010,0,0.0375043,"Missing"
2011.eamt-1.24,2009.mtsummit-papers.8,0,0.029929,"instead of the usual comparison of a single translation hypothesis with the reference translation. The sentences used in Table 3 are present in the top 1 position of sentences used in Table 4. This means that when BLEU and METEOR scores are evaluated at system-level, for each sentence, the translation (among 5) with the highest sBLEU score is selected as the translation for that sentence. This is similar to the post-editing scenario where human translators are shown n translations and are asked to either select the best or rank them. Some studies have used as many as 10 translations together (Koehn and Haddow, 2009). We only use 5 in our evaluation. We observe that overall the R ESCsum system shows a modest improvement over the baseline in terms of METEOR scores, but not BLEU scores. This trend is consistent across all the 3 n-best list sizes. We speculate that perhaps the reliance of METEOR on both precision and recall as opposed to precision-based BLEU is a factor for this disagreement between metrics. We also observe that the degree of improvement in the BLEU and METEOR scores of each system from top-1 (Table 3) to top-5 (Table 4) is more obvious in the rescored systems R ESCsum and R ESCprod compared"
2011.eamt-1.24,P06-1096,0,0.10707,"tion metric. We chose BLEU for our experiments, as despite shortcomings such as those pointed out by (Callison-Burch et al., 2006), it remains the most popular metric, and is most often used in MERT for optimizing the feature weights. Our rescoring experiments focus heavily on these weights. Note that BLEU as defined in (Papineni et al., 2002) is a geometric mean of precision n-grams (usually 4), and was not designed to work at the sentencelevel, as is our requirement for the oracle selection. Several sentence-level implementations known as smoothed BLEU have been proposed (Lin and Och, 2004; Liang et al., 2006). We use the one proposed in the latter, as shown in (2). S BLEU = 4 X B LEUi (cand, ref ) 24−i+1 (2) i=1 Figure 1 shows a sample of 10 candidate English translations from an n-best list for a French sentence. The first column gives the respective decoder cost (log-linear score) used to rank an nbest list and the third column displays the sBLEU (sentence-level BLEU score) for each candidate translation. The candidate in the first position in the figure is the 1-best according to the decoder. The 7th-ranked sentence is most similar to the reference translation and hence awarded the highest Deco"
2011.eamt-1.24,C04-1072,0,0.0162498,"an automatic evaluation metric. We chose BLEU for our experiments, as despite shortcomings such as those pointed out by (Callison-Burch et al., 2006), it remains the most popular metric, and is most often used in MERT for optimizing the feature weights. Our rescoring experiments focus heavily on these weights. Note that BLEU as defined in (Papineni et al., 2002) is a geometric mean of precision n-grams (usually 4), and was not designed to work at the sentencelevel, as is our requirement for the oracle selection. Several sentence-level implementations known as smoothed BLEU have been proposed (Lin and Och, 2004; Liang et al., 2006). We use the one proposed in the latter, as shown in (2). S BLEU = 4 X B LEUi (cand, ref ) 24−i+1 (2) i=1 Figure 1 shows a sample of 10 candidate English translations from an n-best list for a French sentence. The first column gives the respective decoder cost (log-linear score) used to rank an nbest list and the third column displays the sBLEU (sentence-level BLEU score) for each candidate translation. The candidate in the first position in the figure is the 1-best according to the decoder. The 7th-ranked sentence is most similar to the reference translation and hence awa"
2011.eamt-1.24,P02-1038,0,0.0631562,"ing two rescoring strategies to push the oracle up the n-best list. We observe modest improvements in METEOR scores over the baseline SMT system trained on French– English Europarl corpora. We present a detailed analysis of the oracle rankings to determine the source of model errors, which in turn has the potential to improve overall system performance. 1 Introduction Phrase-based Statistical Machine Translation (PBSMT) systems typically learn translation, reordering, and target-language features from a large number of parallel sentences. Such features are then combined in a log-linear model (Och and Ney, 2002), the coefficients of which are optimized on an objective function measuring translation quality such as the BLEU metric (Papineni et al., 2002), using Minimum Error Rate Training (MERT) as described in Och (2003). An SMT decoder non-exhaustively explores the exponential search space of translations for each source sentence, scoring each hypothesis using the c 2011 European Association for Machine Translation. formula (Och and Ney, 2002) in (1). P (e|f ) = exp( M X λi hi (e, f )) (1) i=1 The variable h denotes each of the M features (probabilities learned from language models, translation mode"
2011.eamt-1.24,N07-2015,0,0.0346952,"Missing"
2011.eamt-1.24,2009.mtsummit-posters.8,0,0.0251303,"Missing"
2011.eamt-1.24,N03-1017,0,0.0126584,"ystem The set of parallel sentences for all our experiments is extracted from the WMT 20091 Europarl (Koehn, 2005) dataset for the language pair French–English after filtering out sentences longer than 40 words (1,050,398 sentences for training and 2,000 sentences each for development (test2006 dataset) and testing (test2008 dataset)). We train a 5-gram language model using SRILM 2 with Kneser-Ney smoothing (Kneser and Ney , 1995). We train the translation model using GIZA++ 3 for word alignment in both directions followed by phrase-pair extraction using grow-diag-final heuristic described in Koehn et al., (2003). The reordering model is configured with a distance-based reordering and monotone-swapdiscontinuous orientation conditioned on both the source and target languages with respect to previous and next phrases. We use the Moses (Koehn et al., 2007) phrasebased beam-search decoder, setting the stack size to 500 and the distortion limit to 6, and switching on the n-best-list option. Thus, this baseline model uses 15 features, namely 7 distortion features (d1 through d7), 1 language model feature (lm), 5 translation model features (tm1 through tm5), 1 word penalty (w), and 1 unknown word penalty fea"
2011.eamt-1.24,2005.mtsummit-papers.11,0,0.01787,"e here ? is there not here a case of double standards ? sBLEU score. This sentence is the oracle translation for the given French sentence. Note that there may be ties where the oracle is concerned (the 7th and the 10th ranking sentence have the same sBLEU score). Such issues are discussed and dealt with in section 3.4. Oracle-best hypotheses are a good indicator of what could be achieved if our MT models were perfect, i.e. discriminated properly between good and bad hypotheses. 3.2 Baseline System The set of parallel sentences for all our experiments is extracted from the WMT 20091 Europarl (Koehn, 2005) dataset for the language pair French–English after filtering out sentences longer than 40 words (1,050,398 sentences for training and 2,000 sentences each for development (test2006 dataset) and testing (test2008 dataset)). We train a 5-gram language model using SRILM 2 with Kneser-Ney smoothing (Kneser and Ney , 1995). We train the translation model using GIZA++ 3 for word alignment in both directions followed by phrase-pair extraction using grow-diag-final heuristic described in Koehn et al., (2003). The reordering model is configured with a distance-based reordering and monotone-swapdiscont"
2011.eamt-1.24,P07-2045,0,0.00824134,"nd 2,000 sentences each for development (test2006 dataset) and testing (test2008 dataset)). We train a 5-gram language model using SRILM 2 with Kneser-Ney smoothing (Kneser and Ney , 1995). We train the translation model using GIZA++ 3 for word alignment in both directions followed by phrase-pair extraction using grow-diag-final heuristic described in Koehn et al., (2003). The reordering model is configured with a distance-based reordering and monotone-swapdiscontinuous orientation conditioned on both the source and target languages with respect to previous and next phrases. We use the Moses (Koehn et al., 2007) phrasebased beam-search decoder, setting the stack size to 500 and the distortion limit to 6, and switching on the n-best-list option. Thus, this baseline model uses 15 features, namely 7 distortion features (d1 through d7), 1 language model feature (lm), 5 translation model features (tm1 through tm5), 1 word penalty (w), and 1 unknown word penalty feature. Note that the unknown word fea1 http://www.statmt.org/wmt09/ http://www-speech.sri.com/projects/srilm/ 3 http://code.google.com/p/giza-pp/ 2 171 sBLEU 0.0188 0.147 0.0125 0.025 0.025 0.0677 0.563 0.0188 0.0190 0.563 Figure 1: Sample from a"
2011.eamt-1.24,P80-1000,0,0.876409,"Missing"
2011.eamt-1.24,P02-1040,0,0.0897841,"stem trained on French– English Europarl corpora. We present a detailed analysis of the oracle rankings to determine the source of model errors, which in turn has the potential to improve overall system performance. 1 Introduction Phrase-based Statistical Machine Translation (PBSMT) systems typically learn translation, reordering, and target-language features from a large number of parallel sentences. Such features are then combined in a log-linear model (Och and Ney, 2002), the coefficients of which are optimized on an objective function measuring translation quality such as the BLEU metric (Papineni et al., 2002), using Minimum Error Rate Training (MERT) as described in Och (2003). An SMT decoder non-exhaustively explores the exponential search space of translations for each source sentence, scoring each hypothesis using the c 2011 European Association for Machine Translation. formula (Och and Ney, 2002) in (1). P (e|f ) = exp( M X λi hi (e, f )) (1) i=1 The variable h denotes each of the M features (probabilities learned from language models, translation models, etc.) and λ denotes the associated feature weight (coefficient). The candidate translation (in the n-best list) having the highest decoder s"
2011.eamt-1.24,N04-1023,0,0.0284826,"e agree with this statement (cf. figure 2). However, we believe that there is scope for improvement on the baseline features (used in decoding) before extracting more complex features for reranking. 175 Role of oracles in boosting translation accuracy We believe oracle-based training to be a viable method. In future work, we intend to explore more features (especially those used in the reranking literature such as Och et al., (2004)) to help promote oracles. We believe that our oracle-based method can help select better features for reranking. We also plan to use a host of reranking features (Shen et al., 2004) and couple them with our R ESCsum rescoring strategy. We will also generate a feature based on our rescoring formula and use it as an additional feature in discriminative reranking frameworks. We have used here sentence-level BLEU as opposed to system-level BLEU as used in MERT for oracle identification. We plan to use metrics better suited for sentence-level like TER (Snover et al., 2006). 6 Conclusion We analyze the relative position of oracle translations in the n-best list of translation hypotheses to help reranking in a PB-SMT system. We propose two new rescoring strategies. In general,"
2011.eamt-1.24,2006.amta-papers.25,0,0.0377378,"ranking literature such as Och et al., (2004)) to help promote oracles. We believe that our oracle-based method can help select better features for reranking. We also plan to use a host of reranking features (Shen et al., 2004) and couple them with our R ESCsum rescoring strategy. We will also generate a feature based on our rescoring formula and use it as an additional feature in discriminative reranking frameworks. We have used here sentence-level BLEU as opposed to system-level BLEU as used in MERT for oracle identification. We plan to use metrics better suited for sentence-level like TER (Snover et al., 2006). 6 Conclusion We analyze the relative position of oracle translations in the n-best list of translation hypotheses to help reranking in a PB-SMT system. We propose two new rescoring strategies. In general, the improvements provided by reranking the n-best lists is dependent on the size of n and the type of translations produced in the n-best list. We see an improvement in METEOR scores. To conclude, oracles have much to contribute to the ranking of better translations and reducing the model errors. Acknowledgements This work is supported by Science Foundation Ireland (grant number: 07/CE/I114"
2011.eamt-1.24,E06-1032,0,\N,Missing
2011.eamt-1.24,N03-1031,0,\N,Missing
2011.eamt-1.24,N04-1021,0,\N,Missing
2011.eamt-1.24,P03-1021,0,\N,Missing
2011.iwslt-evaluation.8,P07-2045,0,0.00419654,"ng with interpolation. We interpolated a LM trained on the TED training data (47k sentences) with a LM trained on Europarl, News, UN and 68 News-mono (24M sentences in total). After a perplexity test to optimize the interpolation weight (on Dev2010), we chose an interpolation weight equal to 0.5. 2.4. Translation modeling and tuning For the translation model training, the uncased (but punctuated) corpus was word aligned and then, the pairs of source and corresponding target phrases were extracted from the word-aligned bilingual training corpus using the scripts provided with the Moses decoder [3]. The result is a phrasetable containing all the aligned phrases. This phrase-table, produced by the translation modeling, is used to extract several translations models. In the experiments reported here, only 8 features were used in the phrase-based models: 5 translation model scores, 1 distance-based reordering score, 1 LM score and 1 word penalty score. We used the Minimum Error Rate Training (MERT) method to tune the weights. MERT was applied on the TED Dev2010 corpus (934 sentences). Moreover, it is important to note that, during tuning, punctuation was systematically removed from the Nbe"
2011.iwslt-evaluation.8,D07-1103,0,0.0175508,"ut translation task for which decoding (and tuning) is also done without punctuation. +News+UN+Newsmono: 24M sentences in total). The punctuation was restored after translation using this LM and the hidden-ngram command from SRILM toolkit. After repunctuation, we used the SMT-based recaser presented earlier. For the SLT task, the final system submitted by LIG in 2010 was ranked among the best sites that participated to the TALK task last year. 3. Improvements of MT and SLT systems done for 2011 3.1. Iterative improvement of the MT system -apply phrase-table pruning with a technique similar to [4] (retuning with MERT needed after pruning). Table 2 summarizes the iterative improvements done this year over the LIG 2010 system. First, we evaluated the performance of a phrase-table trained on the TED 2011 bilingual data (107268 sentences in total) only with and without tuning (2,3). The target language model was also updated using the TED 2011 mono (111431 sentences) data (4), which slightly increased the performance. The results obtained show a reasonable performance of the PT trained on TED 2011 only, so we experimented multiple phrasetable decoding where translation options are collecte"
2011.iwslt-evaluation.8,P02-1040,0,0.0851348,"Missing"
2011.iwslt-evaluation.8,W11-2154,1,0.875821,"Missing"
2011.iwslt-evaluation.8,2010.iwslt-evaluation.1,0,\N,Missing
2011.jeptalnrecital-long.7,hahn-etal-2008-comparison,0,0.0261435,"Missing"
2011.jeptalnrecital-long.7,P07-2045,0,0.00348906,"Missing"
2011.jeptalnrecital-long.7,P00-1056,0,0.0915877,"Missing"
2011.jeptalnrecital-long.7,N07-1064,0,0.034696,"Missing"
2012.iwslt-evaluation.13,2005.mtsummit-papers.11,0,0.008448,"as well as the LIG ofﬁcial results obtained this year. 2. Resources used in 2012 The following sections describe the resources used to build the translation models as well as the language models. We built three translation models for our machine translation systems (see table 1). • An in-domain translation model trained on TED Talks collection (TED) corpus. • A (bigger) out-of-domain translation model trained on six different (freely available) corpora in which three of them are part of the WMT 2012 shared task training data: – the latest version of the Europarl (version 7) corpus (EUROPARL1 [1]) – the latest version of the News-Commentary (version 7) corpus (NEWS-C) – the United Nations corpus (UN 2 [2]) • We also used the Corpus of Parallel Patent Applications (PCT3 ), the DGT Multilingual Translation Memory of the Acquis Communautaire (DGT-TM [3]), and the EUconst corpus (EU-CONST [4]). These three corpora are all freely available. • An additional out-of-domain translation model was trained on a subset of the French-English Gigaword corpus (GIGA-5M). After cleaning, the whole Gigaword corpus was sorted at sentence level according to the sum of perplexities of the source (English)"
2012.iwslt-evaluation.13,eisele-chen-2010-multiun,0,0.0138842,"be the resources used to build the translation models as well as the language models. We built three translation models for our machine translation systems (see table 1). • An in-domain translation model trained on TED Talks collection (TED) corpus. • A (bigger) out-of-domain translation model trained on six different (freely available) corpora in which three of them are part of the WMT 2012 shared task training data: – the latest version of the Europarl (version 7) corpus (EUROPARL1 [1]) – the latest version of the News-Commentary (version 7) corpus (NEWS-C) – the United Nations corpus (UN 2 [2]) • We also used the Corpus of Parallel Patent Applications (PCT3 ), the DGT Multilingual Translation Memory of the Acquis Communautaire (DGT-TM [3]), and the EUconst corpus (EU-CONST [4]). These three corpora are all freely available. • An additional out-of-domain translation model was trained on a subset of the French-English Gigaword corpus (GIGA-5M). After cleaning, the whole Gigaword corpus was sorted at sentence level according to the sum of perplexities of the source (English) and the target (French) based on two French and English pretrained language models. For this, LMs were trained"
2012.iwslt-evaluation.13,steinberger-etal-2012-dgt,0,0.0128047,"systems (see table 1). • An in-domain translation model trained on TED Talks collection (TED) corpus. • A (bigger) out-of-domain translation model trained on six different (freely available) corpora in which three of them are part of the WMT 2012 shared task training data: – the latest version of the Europarl (version 7) corpus (EUROPARL1 [1]) – the latest version of the News-Commentary (version 7) corpus (NEWS-C) – the United Nations corpus (UN 2 [2]) • We also used the Corpus of Parallel Patent Applications (PCT3 ), the DGT Multilingual Translation Memory of the Acquis Communautaire (DGT-TM [3]), and the EUconst corpus (EU-CONST [4]). These three corpora are all freely available. • An additional out-of-domain translation model was trained on a subset of the French-English Gigaword corpus (GIGA-5M). After cleaning, the whole Gigaword corpus was sorted at sentence level according to the sum of perplexities of the source (English) and the target (French) based on two French and English pretrained language models. For this, LMs were trained separately on all the data listed in table 2 except the Gigaword corpus itself (the News Shufﬂe corpus was also available on the source English side"
2012.iwslt-evaluation.13,P07-1040,0,0.0999912,"Missing"
2012.iwslt-evaluation.13,P09-1066,0,0.0718746,"ature weights are modiﬁed using Minimum Error Rate Training (MERT). Experiments are performed to ﬁnd the optimal size for N-Best list combination. Four systems are used and analysed on combination of two best systems and all the systems. 50-best list was found to be optimal size for both cases. The authors showed that the impact of gradually introducing a new system for combination becomes lower as the number of systems increases. Anyway the best result is obtained when all of the systems are combined. -Co-decoding Recently, the concept of collaborative decoding (codecoding) was introduced by [11] to improve machine translation accuracy by leveraging translation consensus between multiple machine translation decoders. Different from what we described earlier (postprocess the n-best lists or word graphs), this method uses multiple machine translation decoders that collaborate by exchanging partial translation results. Using an iterative decoding approach, n-gram agreement statistics between translations of multiple decoders are employed to re-rank full and partial hypotheses explored in decoding. 3.2. Overview of the Driven Decoding Concept 3.2.1. Driven Decoding Es = arg minE∈Ei Ns  T"
2012.iwslt-evaluation.13,P02-1040,0,0.0936693,"ance between the current hypothesis decoded (called H) and the auxiliary translation available (T) : d(T,H). Let’s say that 2 auxiliary translations are available (from system 1 and system 2) and that 4 distance metrics are available (BLEU, TER, TERp-A and PER); in that case, 8 scores are added to each line of the N-Best list. The distance metrics used in our experiments are described in the next section and then N-Best reordering process is detailed. 3.2.2. Distance Metrics used The distance metrics used are Translation Error Rate (TER), Position independent Error Rate (PER), TERp-A and BLEU [12]. The TER score reﬂects the number of edit operations (insertions, deletions, words substitutions and blocks shifts) needed to transform a hypothesis translation into the reference translation, while the BLEU score is the geometric mean of n-gram precision. Lower TER and higher BLEU score suggest better translation quality. In addition, we use PER score (position independent error rate) which can be seen as a bag-of-words metric potentially interesting in the context of the driven decoding proposed. In addition we use TERp [13] which is an extension of TER eliminating its 105 The 9th Internati"
2012.iwslt-evaluation.13,2009.eamt-1.5,0,0.0170064,"task with and without the use of conﬁdence measure will be shown in Table 3. 5. Experimental Results of LIG Systems 4. Use of Conﬁdence Measures for SMT Besides driven decoding (DD) scores, a sentence conﬁdence score can be added as an additional feature in the N-best list to improve the re-ordering performance. To obtain such a conﬁdence score, a classiﬁer must be constructed. We concatenate two data sets dev2010 + tst2010 to form the training data. Features used to train our model come from the baseline features of the WMT2012 quality estimation shared task (features originally presented in [14]), which can We recall that our systems were systematically tuned on dev2010 corpus. Our baseline system, trained as described in section 2, lead to a BLEU score of 30.28 on tst2010 using 2 translation and re-ordering models (no GIGAword) while it improves to 30.80 using 3 translation and reordering models (using GIGAword). This result has to be compared with 27.58 obtained on tst2010 with our system last year. As far as the driven decoding is concerned, the results show that using the Google 1best hypothesis to guide the 106 The 9th International Workshop on Spoken Language Translation Hong K"
2012.iwslt-evaluation.13,P10-1052,0,0.0330874,"each word in the source corpus. The core element needed for the classiﬁer construction process is the training label for each sentence. The TERp-A metric [13], which we select to perform this task, provides the linguistic and semantic matching between each sentence in training set and its reference (available for dev2010 and tst2010 corpora), then yields the minimum cost for matching normalized by its number of tokens as its score. We then categorize them in a binary set: sentences with score higher than 0.3 is assigned with ”Good” (G) label, otherwise, ”Bad” (B). A CRF-based toolkit, WAPITI [15], is then called to build the classiﬁer. The training phase is conducted using stochastic gradient descent (SGD-L1) algorithm, with values for maximum number of iterations done by the algorithm (-maxiter), stop window size (–stopwin) and stop epsilon (–stopeps) to 200, 6, and 0.00005 respectively. Applying this classiﬁer in both test sets (test2011 + test2012, with WAPITI’s default threshold = 0.5) gives us the result ﬁles detailing hypothesized label along with its probability at the sentence level. Then, the conﬁdence score used is the probability of sentence to be regarded as a “Good” sente"
2012.iwslt-evaluation.13,N07-1029,0,\N,Missing
2012.iwslt-evaluation.13,2008.amta-srw.3,0,\N,Missing
2012.iwslt-papers.19,N07-1064,0,0.163393,"mation-based” APE trained on 12 000 manually post-edited translations, to correct the raw output. There was a signiﬁcant improvement in translation quality with the use of a “transformation-based” APE. The increasing amount of raw MT translation (hypotheses) aligned with their manually post-edited good translations gave rise to the idea of automatic statistical post-edition. A statistical post-edition (SPE) system is developed as a monolingual statistical MT system using the original hypotheses as the source language and the human post-editions as the target language. In 2007, M. Simard & al. [5] were the ﬁrst to propose the use of a phrase-based statistical machine translation (PBMT) system for SPE purpose. In this framework, the PBMT aims to learn “correction rules” between initial MT hypotheses (PBMT source language) and their corrected version (PBMT target language). Such an approach makes SPE easy to learn and tune with new training data. In their work, they successfully showed the efﬁciency of using an SPE system (built with the PBMT Portage) to improve the output of a commercial RBMT system. The experiments were done in a speciﬁc domain (a job offer Web site2 ) and the SPE syst"
2012.iwslt-papers.19,2007.mtsummit-papers.34,0,0.896952,"y showed the efﬁciency of using an SPE system (built with the PBMT Portage) to improve the output of a commercial RBMT system. The experiments were done in a speciﬁc domain (a job offer Web site2 ) and the SPE system was trained using 35,000 manually post-edited sentences. Encouraged by these results, post-editing the outputs 2 http://www.jobbank.gc.ca 284 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 of the PBMT system Portage was also tried but in this setting no improvements were observed. In the same way, the following studies described in [6], [7] and [8] have shown that a RBMT system that was automatically post-edited by a PBMT system performed signiﬁcantly better than each of the individual systems on their own. Quite a lot of studies have focused on pipeline architectures where SPE systems are successfully applied to RBMT systems outputs to improve translation quality. However, only few studies ([9, 8, 10]), have investigated the efﬁciency of SPE systems applied after PBMT systems. The goal of our study is to provide a better understanding of SPE usefulness when pipelined to PBMT systems. We ﬁrst describe our baseline experimen"
2012.iwslt-papers.19,W07-0728,0,0.139022,"wed the efﬁciency of using an SPE system (built with the PBMT Portage) to improve the output of a commercial RBMT system. The experiments were done in a speciﬁc domain (a job offer Web site2 ) and the SPE system was trained using 35,000 manually post-edited sentences. Encouraged by these results, post-editing the outputs 2 http://www.jobbank.gc.ca 284 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 of the PBMT system Portage was also tried but in this setting no improvements were observed. In the same way, the following studies described in [6], [7] and [8] have shown that a RBMT system that was automatically post-edited by a PBMT system performed signiﬁcantly better than each of the individual systems on their own. Quite a lot of studies have focused on pipeline architectures where SPE systems are successfully applied to RBMT systems outputs to improve translation quality. However, only few studies ([9, 8, 10]), have investigated the efﬁciency of SPE systems applied after PBMT systems. The goal of our study is to provide a better understanding of SPE usefulness when pipelined to PBMT systems. We ﬁrst describe our baseline experimental s"
2012.iwslt-papers.19,J03-1002,0,0.00364466,"ation purposes (Section 5)? 2. Experimental setting 2.1. Baseline PBMT Our baseline MT system (described in more detail in [11]) translates news stories (general domain) from French into English. It is a state-of-the-art phrase-based machine translation (PBMT) system presented at the international Workshop of Machine Translation (WMT3 ) evaluation campaign in july 2010. The system was built using free open source toolkits: we used standard Moses [12] system set-up, a 3-gram language model trained with SriLM [13] and Kneser-Ney smoothing, the GIZA++ implementation of IBM word alignment model 4 [14] and the phrase extraction heuristics described in [12]. The system has been trained on two parallel corpora, containing in total 1,638,440 aligned sentences: the fourth version of the Europarl corpus (data derived from transcriptions of European parliament proceedings) and news corpora (data extracted from various Websites). Both corpora were provided in the framework of WMT 2010. The PBMT decoding model is a log-linear combination of fourteen weighted feature functions extracted from the monolingual and bilingual training data: six distortion models; lexicon word-based and phrase-based trans"
2012.iwslt-papers.19,2006.amta-papers.25,0,0.0586335,"e SPE training corpus (in terms of size and domain) are the same in the two cases. 2.4. Evaluation metrics To our knowledge, there is no work that compares both approaches (real vs simulated PE) on the same source language data (post-edited MT hypotheses vs professional translations) to train an SPE. Considering the same source language data, we tried to ﬁnd out if a simulated PE corpus is as effective as a real PE corpus to train an SPE system. This is what we will try to ﬁnd out in the following experiment. Translation output quality has been evaluated using the Translation Error Rate (TER) [18] and the BLEU score [19]. The TER score reﬂects the number of edit operations (insertions, deletions, words substitutions and blocks shifts) needed to transform a hypothesis translation into the reference translation, while the BLEU score is the geometric mean of n-gram precision. Lower TER and higher BLEU scores suggest better translation quality. To ensure that differences between scores are real, we estimated the statistical signiﬁcance of test results in terms of BLEU score, according to the bootstrap resampling method described in [20]. 3. Real vs Simulated post-edited corpus for SPE trai"
2012.iwslt-papers.19,W04-3250,0,0.0532129,"been evaluated using the Translation Error Rate (TER) [18] and the BLEU score [19]. The TER score reﬂects the number of edit operations (insertions, deletions, words substitutions and blocks shifts) needed to transform a hypothesis translation into the reference translation, while the BLEU score is the geometric mean of n-gram precision. Lower TER and higher BLEU scores suggest better translation quality. To ensure that differences between scores are real, we estimated the statistical signiﬁcance of test results in terms of BLEU score, according to the bootstrap resampling method described in [20]. 3. Real vs Simulated post-edited corpus for SPE training 3.1. Previous work In order to build SPE systems, manually post-edited MT hypotheses are usually used as target translations instead of translations produced by professional translators. When preexisting human translations are used, we will speak of “simulated PE” in contrast to “real PE” when target translations are manually post-edited MT hypotheses. It is important to notice that the “real PE” setting corresponds to the workﬂows implemented in real-life situations (when users feedback is re-used to improve a given system) and “simul"
2012.iwslt-papers.19,2011.mtsummit-papers.35,0,0.892487,"International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 of the PBMT system Portage was also tried but in this setting no improvements were observed. In the same way, the following studies described in [6], [7] and [8] have shown that a RBMT system that was automatically post-edited by a PBMT system performed signiﬁcantly better than each of the individual systems on their own. Quite a lot of studies have focused on pipeline architectures where SPE systems are successfully applied to RBMT systems outputs to improve translation quality. However, only few studies ([9, 8, 10]), have investigated the efﬁciency of SPE systems applied after PBMT systems. The goal of our study is to provide a better understanding of SPE usefulness when pipelined to PBMT systems. We ﬁrst describe our baseline experimental settings (Section 2) and then we try to answer the following questions: is there a difference between a real and a simulated corpus for SPE training (Section 3)? Is SPE useful in improving a generic PBMT system and what explains the effectiveness of SPE on specialized domain (Section 4)? And, ﬁnally, is SPE really the simplest and most efﬁcient and effective way for d"
2012.iwslt-papers.19,W07-0732,0,0.0923357,"ly post-edited MT hypotheses are usually used as target translations instead of translations produced by professional translators. When preexisting human translations are used, we will speak of “simulated PE” in contrast to “real PE” when target translations are manually post-edited MT hypotheses. It is important to notice that the “real PE” setting corresponds to the workﬂows implemented in real-life situations (when users feedback is re-used to improve a given system) and “simulated PE” setup will allow access to much more training data (use of pre-translated parallel corpus). Several works [21, 10, 22, 9] have attempted to show that SPE can be successfully trained on pre-existing human translations rather than on system-speciﬁc post-edited translations. Both simulated (MT system hypotheses aligned with 3.2. Experiment In order to build two comparable SPE using real vs simulated target corpus, we used in both cases the same training corpus on the source side (the one described in 2.2) and, for one system we used the PBMT post-edited hypotheses (“real” setting) on the target side and for the other system, we used the translations provided with the parallel corpus (“simulated” setting) as the tar"
2012.iwslt-papers.19,N09-2055,0,0.7581,"International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 of the PBMT system Portage was also tried but in this setting no improvements were observed. In the same way, the following studies described in [6], [7] and [8] have shown that a RBMT system that was automatically post-edited by a PBMT system performed signiﬁcantly better than each of the individual systems on their own. Quite a lot of studies have focused on pipeline architectures where SPE systems are successfully applied to RBMT systems outputs to improve translation quality. However, only few studies ([9, 8, 10]), have investigated the efﬁciency of SPE systems applied after PBMT systems. The goal of our study is to provide a better understanding of SPE usefulness when pipelined to PBMT systems. We ﬁrst describe our baseline experimental settings (Section 2) and then we try to answer the following questions: is there a difference between a real and a simulated corpus for SPE training (Section 3)? Is SPE useful in improving a generic PBMT system and what explains the effectiveness of SPE on specialized domain (Section 4)? And, ﬁnally, is SPE really the simplest and most efﬁcient and effective way for d"
2012.iwslt-papers.19,W10-1723,1,0.835519,"ing of SPE usefulness when pipelined to PBMT systems. We ﬁrst describe our baseline experimental settings (Section 2) and then we try to answer the following questions: is there a difference between a real and a simulated corpus for SPE training (Section 3)? Is SPE useful in improving a generic PBMT system and what explains the effectiveness of SPE on specialized domain (Section 4)? And, ﬁnally, is SPE really the simplest and most efﬁcient and effective way for domainadaptation purposes (Section 5)? 2. Experimental setting 2.1. Baseline PBMT Our baseline MT system (described in more detail in [11]) translates news stories (general domain) from French into English. It is a state-of-the-art phrase-based machine translation (PBMT) system presented at the international Workshop of Machine Translation (WMT3 ) evaluation campaign in july 2010. The system was built using free open source toolkits: we used standard Moses [12] system set-up, a 3-gram language model trained with SriLM [13] and Kneser-Ney smoothing, the GIZA++ implementation of IBM word alignment model 4 [14] and the phrase extraction heuristics described in [12]. The system has been trained on two parallel corpora, containing in"
2012.iwslt-papers.19,P07-2045,0,0.0129007,"he effectiveness of SPE on specialized domain (Section 4)? And, ﬁnally, is SPE really the simplest and most efﬁcient and effective way for domainadaptation purposes (Section 5)? 2. Experimental setting 2.1. Baseline PBMT Our baseline MT system (described in more detail in [11]) translates news stories (general domain) from French into English. It is a state-of-the-art phrase-based machine translation (PBMT) system presented at the international Workshop of Machine Translation (WMT3 ) evaluation campaign in july 2010. The system was built using free open source toolkits: we used standard Moses [12] system set-up, a 3-gram language model trained with SriLM [13] and Kneser-Ney smoothing, the GIZA++ implementation of IBM word alignment model 4 [14] and the phrase extraction heuristics described in [12]. The system has been trained on two parallel corpora, containing in total 1,638,440 aligned sentences: the fourth version of the Europarl corpus (data derived from transcriptions of European parliament proceedings) and news corpora (data extracted from various Websites). Both corpora were provided in the framework of WMT 2010. The PBMT decoding model is a log-linear combination of fourteen w"
2012.iwslt-papers.19,huynh-etal-2008-sectra,1,0.892995,"Missing"
2012.iwslt-papers.19,W09-0419,0,\N,Missing
2012.iwslt-papers.19,P02-1040,0,\N,Missing
2012.iwslt-papers.19,J11-2010,0,\N,Missing
2012.iwslt-papers.19,potet-etal-2012-collection,1,\N,Missing
2012.iwslt-papers.19,P03-1021,0,\N,Missing
2013.mtsummit-wmwumttt.8,2011.freeopmt-1.4,0,0.148883,"Proceedings of the Workshop on Multi-word Units in Machine Translation and Translation Technology, Nice, September 3, 2013. Monti, J., Mitkov, R., Corpas Pastor, G. Seretan V. eds. © 2013 Carlos Ramisch and Laurent Besacier and Alexander Kobzar. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. • What proportion of PVs is translated correctly/acceptably by each SMT paradigm? by Kim and Nakov (2011), who generate monolingual paraphrases of noun compounds to augment the training corpus (e.g. beef import ban → ban on beef import). Probably Monti et al. (2011) present the most similar work to ours. They compile a parallel corpus of sentences containing several types of expressions, including PVs, and compare the outputs of rule-based and SMT systems. While their discussion provides insightful examples, it does not help quantify the extent to which multiword expressions pose problems to MT systems. Moreover, it is not possible to know the exact details of the MT paradigms used in their experiments. Most of the published results to date focus on automatic evaluation measures and only deal with fixed constructions like noun compounds. The present pape"
2013.mtsummit-wmwumttt.8,J08-4004,0,0.0237499,"ively. Fluency and adequacy are annotated separately in two passes. 5.2 could boil this poem down to saying pourriez furoncle ce po`eme jusqu’ a` dire pourriez bouillir ce po`eme descendu a` dire he would think it through and say il pense que c¸a a` travers et dire you couldn ’t figure it out vous ne pouvais pas le comprendre vous ne pouviez pas le d´ecouvrir Then we ’ll test some other ideas out puis nous allons tester certains autres id´ees Inter-annotator agreement In order to validate the evaluation protocol, we calculated inter-annotator agreement,10 following the methodology proposed by Artstein and Poesio (2008). In a first moment, a group of five volunteers annotated a pilot dataset of 156 sentences. 6 8 Problematic source sentences were removed manually, but a small number of such cases accidentally remained in the test data. 9 The guidelines, labels and datasets discussed here are available at http://cameleon.imag.fr/xwiki/bin/ view/Main/Phrasal_verbs_annotation 10 We report values of multi-π (Fleiss’ κ), which estimates chance agreement from the overall category distribution. Results We analyse the results of manual annotation by four human judges on a set of 750 sentences, corresponding to 500 s"
2013.mtsummit-wmwumttt.8,C10-3015,0,0.0301045,"occurrences to be able to learn the constructions. In the remainder of this paper, we make no distinction between systems 1 and 2. 4.2 4.3 Phrasal verb detection PVs were detected in three steps: automatic extraction, filtering heuristics and manual validation. Automatic extraction As described in Section 4.1, we parsed the English corpus using RASP. It performs full syntactic analysis and generates a set of grammatical relations (similar to dependency syntax). The parser has a module for automatic PV detection. However, we are only interested in split PVs. Therefore, we used the mwetoolkit (Ramisch et al., 2010) to extract only sentences that follow the pattern Verb + Object + Particle, where: • Verb is a content verb (POS starts with VV); • Object is a sequence of at least 1 and at most 5 words, excluding verbs; • Particle is a preposition or adverb tagged as II, RR or RP which depends syntactically on the verb with a ncmod_part relation. MT systems We compare SMT systems of two paradigms: a phrase-based system (PBS) and a hierarchical system (HS). The main difference between these two paradigms is the representation of correspondences in the translation model. While the PBS uses word sequences, the"
2013.mtsummit-wmwumttt.8,W09-2907,0,0.122288,"Missing"
2013.mtsummit-wmwumttt.8,bouamor-etal-2012-identifying,0,0.116355,"rd Wordnet entries and experiment with an EnglishArabic system, showing that both strategies result in improvement of translation quality in terms of automatic evaluation measures (BLEU, TER). Other simplistic techniques that have been employed to integrate bilingual lexicons into standard SMT systems include (a) concatenating the lexicon to the training parallel corpus, and (b) artificially appending the lexicon (enriched with artificial probabilities) to the system’s phrase table. This has been applied to Chinese-English terminology (Ren et al., 2009) and English-French nominal expressions (Bouamor et al., 2012). However, results are reported in terms of automatic measures and improvements are not always convincing. For translating noun compounds from and to morphologically rich languages like German, where a compound is in fact a single token formed through concatenation, Stymne (2009) splits the compound into its single word components prior to translation. Then, after translation, postprocessing rules are applied to reorder or merge the components. A different approach was proposed 3 Phrasal verbs Phrasal verbs are recurrent constructions in English. They are composed by a main verb (take) combine"
2013.mtsummit-wmwumttt.8,P06-4020,0,0.0184614,"of the TED conferences, covering a great variety of topics. The colloquial and informal nature of the talks favours the productive use of PVs. Talks are given in English, and are translated by volunteers worldwide. The corpus contains 141,390 EnglishFrench aligned sentences with around 2.5 million tokens in each language. Before feeding the corpus into the MT training pipeline, we performed tokenisation. Tokenisation was performed differently on both languages. Since we wanted to identify PVs in English automatically, we had to parse the English corpus. Therefore, we used the RASP system v2 (Briscoe et al., 2006) to generate the full syntactic analysis of the English sentences. Since the parser contains an embedded tokeniser, we ensured consistency by 1 Available at the Web Inventory of Transcribed and Translated Talks: https://wit3.fbk.eu/ 55 ing the grow-diag-final heuristic. Language models were estimated from the French part of the parallel training corpus using 5-grams with IRSTLM. For the HS, the maximum phrase length was set to 5. The model weights were tuned with MERT, which converged in at most 16 iterations. The training scripts and decoder were configured to print out word alignment informa"
2013.mtsummit-wmwumttt.8,J93-2003,0,0.0254942,"Missing"
2013.mtsummit-wmwumttt.8,E09-3008,0,0.113029,"tful examples, it does not help quantify the extent to which multiword expressions pose problems to MT systems. Moreover, it is not possible to know the exact details of the MT paradigms used in their experiments. Most of the published results to date focus on automatic evaluation measures and only deal with fixed constructions like noun compounds. The present paper presents two original contributions with respect to related work. First, we focus on a more flexible type of construction, phrasal verbs, which are not correctly dealt with by simple integration strategies (Carpuat and Diab, 2010; Stymne, 2009). Secondly, we base our findings on qualitative and quantitative results obtained from a large-scale human evaluation experiment. Moreover, we do not intend to improve a SMT system with multiword unit processing: our goal is rather to evaluate and quantify how hard it is to translate these constructions. We believe that this can help conceiving more linguistically informed models for treating multiword units in MT systems in the future, as opposed to heuristic trial-and-error strategies that can be found in the literature. • Which MT paradigm, phrase-based or hierarchical, can better handle th"
2013.mtsummit-wmwumttt.8,P11-4010,0,0.0589886,"Missing"
2013.mtsummit-wmwumttt.8,N10-1029,0,0.623656,"scussion provides insightful examples, it does not help quantify the extent to which multiword expressions pose problems to MT systems. Moreover, it is not possible to know the exact details of the MT paradigms used in their experiments. Most of the published results to date focus on automatic evaluation measures and only deal with fixed constructions like noun compounds. The present paper presents two original contributions with respect to related work. First, we focus on a more flexible type of construction, phrasal verbs, which are not correctly dealt with by simple integration strategies (Carpuat and Diab, 2010; Stymne, 2009). Secondly, we base our findings on qualitative and quantitative results obtained from a large-scale human evaluation experiment. Moreover, we do not intend to improve a SMT system with multiword unit processing: our goal is rather to evaluate and quantify how hard it is to translate these constructions. We believe that this can help conceiving more linguistically informed models for treating multiword units in MT systems in the future, as opposed to heuristic trial-and-error strategies that can be found in the literature. • Which MT paradigm, phrase-based or hierarchical, can b"
2013.mtsummit-wmwumttt.8,2012.eamt-1.60,0,0.0123761,"emy (e.g. figure out and look up have only 1 sense in Wordnet), others can have multiple uses and senses (e.g. pick up has 16 senses and break up has 19 senses in Wordnet). Many PVs seem to follow a productive pattern of combination of semantically related verbs and a given particle (Fraser, 1976), like verbs used to join material (bolt, cement, nail + down). While some verbs form combinations with almost every 4.1 Parallel corpus and preprocessing For all the experiments carried out in this work — extraction and translation of PVs — the EnglishFrench portion of the TED Talks corpus was used (Cettolo et al., 2012).1 It contains transcriptions of the TED conferences, covering a great variety of topics. The colloquial and informal nature of the talks favours the productive use of PVs. Talks are given in English, and are translated by volunteers worldwide. The corpus contains 141,390 EnglishFrench aligned sentences with around 2.5 million tokens in each language. Before feeding the corpus into the MT training pipeline, we performed tokenisation. Tokenisation was performed differently on both languages. Since we wanted to identify PVs in English automatically, we had to parse the English corpus. Therefore,"
2013.mtsummit-wmwumttt.8,J07-2003,0,0.264238,"Missing"
2013.mtsummit-wmwumttt.8,D11-1060,0,0.0188729,"iosyncrasies of multiword expressions are at the root of translation problems, as exemplified in Table 2. c 2013 European Association for Machine Translation. 53 Proceedings of the Workshop on Multi-word Units in Machine Translation and Translation Technology, Nice, September 3, 2013. Monti, J., Mitkov, R., Corpas Pastor, G. Seretan V. eds. © 2013 Carlos Ramisch and Laurent Besacier and Alexander Kobzar. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. • What proportion of PVs is translated correctly/acceptably by each SMT paradigm? by Kim and Nakov (2011), who generate monolingual paraphrases of noun compounds to augment the training corpus (e.g. beef import ban → ban on beef import). Probably Monti et al. (2011) present the most similar work to ours. They compile a parallel corpus of sentences containing several types of expressions, including PVs, and compare the outputs of rule-based and SMT systems. While their discussion provides insightful examples, it does not help quantify the extent to which multiword expressions pose problems to MT systems. Moreover, it is not possible to know the exact details of the MT paradigms used in their exper"
2013.mtsummit-wmwumttt.8,N03-1017,0,0.0395493,"Missing"
2013.mtsummit-wmwumttt.8,P07-2045,0,0.00654994,"s the representation of correspondences in the translation model. While the PBS uses word sequences, the HS uses synchronous context-free grammars, allowing the use of nonterminal symbols in the phrase table. Intuitively, the HS should be more suitable to translate PVs because it can generalize the intervening words between the verb and the particle. In other words, while the PBS enumerates all possible intervening sequences explicitly (make up, make it up, make the story up, . . . ), the HS can replace them by a single variable (make X up). Both PBS and HS were built using the Moses toolkit (Koehn et al., 2007) and standard training parameters.4 The preprocessed training sets described in Table 1 were used as input for both systems. The corpus was word-aligned using GIZA++ and the phrase tables were extracted usFiltering heuristics The application of this pattern on the parsed corpus generates the PV set (2,071 sentences). Manual inspection allowed us to formulate further heuristics to filter the set. We removed 243 sentences that match one of the following rules around the identified PV: • Verbs go, walk, do, see + locative words;5, 6 • Particles about, well, at; • Locative words followed by the wo"
2014.amta-researchers.23,I13-1033,0,0.0438359,"Missing"
2014.amta-researchers.23,D11-1033,0,0.692562,"scores is equivalent. Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 302 Cross-entropy difference (CED) While PP (or its equivalent CE) were used extensively before, Moore and Lewis (2010) proposed using the difference between the cross entropies scores with respect to the in-domain corpus and with respect to the general corpus. The idea is to prefer sentences that are typical to the in-domain (i.e. low CE) and untypical to the general domain (i.e. high CE). As above, the sentences with the lowest cross entropy difference are selected. Axelrod et al. (2011) have extended over the CED method by computing it bilingually, taking the average of the CED over the source and over the target. They showed that bilingual CED outperforms the monolingual version, as well as other related metrics. In their experiments, best results were achieved when the in-domain and the reduced-general corpora were placed in two different translation tables. This method is still considered the state of the art in this line of research. In our experiments we assess all the above metrics, both monolingually and bilingually, over two different datasets. We validate that bilin"
2014.amta-researchers.23,2012.eamt-1.60,0,0.122441,"Missing"
2014.amta-researchers.23,2012.eamt-1.40,0,0.0613788,"Missing"
2014.amta-researchers.23,eck-etal-2004-language,0,0.0480255,"Missing"
2014.amta-researchers.23,D10-1044,0,0.0359809,"etrics One prominent line of research is using information theory metrics to assess each one of the sentences in the pool, G, and choose the ones that are most similar to the provided in-domain data, I. Perplexity (PP) and cross-entropy (CE) Perplexity (PP) is perhaps the best known metric for DS for SMT. The idea is to compute the perplexity of a language model (LM) built on I, measure the perplexity of this LM over each sentence in G, and select the m sentences with the lowest PP scores. Gao et al. (2002) and Moore and Lewis (2010) employed this metric for language model adaptation, and in (Foster et al., 2010; Yasuda et al., 2008) it was used for translation model adaptation. The same technique is sometimes (e.g. Moore and Lewis (2010)) referred to as cross entropy (CE). Since, by definition, the cross-entropy is simply the exponent in the perplexity score, the base of the exponent is 2 and all scores are positive, a smaller PP means a smaller CE and vice versa. In other words, selecting m sentences with the lowest PP scores or with the lowest CE scores is equivalent. Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 302 Cross-entropy differenc"
2014.amta-researchers.23,E12-1016,0,0.294071,"Missing"
2014.amta-researchers.23,2005.eamt-1.19,0,0.0590488,"Missing"
2014.amta-researchers.23,P07-2045,0,0.00528674,"rom https://wit3.fbk.eu/mt.php?release=2012-02 4 Downloaded Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 304 • T RANSCRIPTS (H4): Radio and television transcripts; standard Arabic. • M ULTI -UN: United Nations official documents.5 • O PEN S UBTITLES Tiedemann (2012): Movie subtitles.6 Dataset N EWS W IT 3 T RANSCRIPTS M ULTI -UN O PEN S UBTITLES Sentences 91K 87K 21K 9.9M 4.4M Tokens-Ar 2.2M 1.9M 561K 222.4M 27.7M Tokens-Fr 2.4M 2.4M 778K 285.5M 32.3M Table 3: Bilingual corpora for training. SMT system and preprocessing We used Moses (Koehn et al., 2007) as our phrase-based SMT system. IRSTLM (Federico et al., 2008) was used to train 5-gram language models over the target side of the (selected) bilingual corpora. Arabic tokenization was done similarly to MADA-TOKAN (Habash et al., 2009), but due to project constraints we used a reimplementation of this tokenizer. More precisely, 300M Arabic words of the M ULTI -UN corpus, segmented using MADA, were used to train a tokenizer using OpenNLP.7 To improve tokenization of punctuations, we applied the Moses tokenizer after the Arabic segmentation was applied. The translation models are trained on lo"
2014.amta-researchers.23,W13-2235,0,0.35071,"01 little available in-domain data. Our work provides insights into the usefulness of the different selection methods to each dataset. In addition, we propose two data selection methods that aim to ease the tension between the similarity and coverage objectives, while the need for compact models that require rather aggressive data filtering, is taken into account. In the first proposed method, an IR model is added to a perplexity-based one, resulting in a model that is ready to use with or without relying on the test set. The second, denoted AVSF, is a domain-adaptation-fitted version of VSF (Lewis and Eetemadi, 2013), an algorithm which was designed to reduce model size without jeopardizing n-gram coverage. Both proposed methods are able to show competitive results in comparison to prior DS techniques. Over one dataset, their performance is better than other assessed methods; over the other – where data selection with size constraints proved more difficult – they outperform methods with comparable training data size, demonstrating a good tradeoff between translation performance and model size. The rest of the paper is organized as follows. Section 2 presents common approaches for data-selection in SMT. Se"
2014.amta-researchers.23,D07-1036,0,0.127695,"Missing"
2014.amta-researchers.23,2013.iwslt-papers.10,1,0.841279,"hat is, we remove all identical bi-sentences from the selected parallel corpus, allowing duplicates to remain in either source or target, but not both. The de-duplicated set, R0 , is then used to construct additional translation and language models. The two types of adaptation are then combined in a single log linear setting, where each makes up one TM or LM. That is, we add an additional TM or LM model based on the IR data rather than train models with the entire selected data. Such separation was shown to be helpful by, e.g., Axelrod et al. (2011), and enables quickly generating the models (Mirkin and Cancedda, 2013). The updated configuration, with 2 TMs and 2 LMs, is then tuned using the development set D, producing a ready-to-use model. If test-set adaptation is possible, we apply the same IR selection over the text for translation, and use the TM and the LM generated with it to substitute those created with the previouslyavailable in-domain data. If we can afford, time-wise, to re-tune the model, that would be the preferred choice. Yet, tuning is a lengthy process, and if the models are of similar properties, tuning may be skipped, as shown in (Mirkin and Cancedda, 2013). The above steps are summarize"
2014.amta-researchers.23,P10-2041,0,0.318888,"will help better translate texts of the same domain as I. 2.1 Information theory metrics One prominent line of research is using information theory metrics to assess each one of the sentences in the pool, G, and choose the ones that are most similar to the provided in-domain data, I. Perplexity (PP) and cross-entropy (CE) Perplexity (PP) is perhaps the best known metric for DS for SMT. The idea is to compute the perplexity of a language model (LM) built on I, measure the perplexity of this LM over each sentence in G, and select the m sentences with the lowest PP scores. Gao et al. (2002) and Moore and Lewis (2010) employed this metric for language model adaptation, and in (Foster et al., 2010; Yasuda et al., 2008) it was used for translation model adaptation. The same technique is sometimes (e.g. Moore and Lewis (2010)) referred to as cross entropy (CE). Since, by definition, the cross-entropy is simply the exponent in the perplexity score, the base of the exponent is 2 and all scores are positive, a smaller PP means a smaller CE and vice versa. In other words, selecting m sentences with the lowest PP scores or with the lowest CE scores is equivalent. Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014"
2014.amta-researchers.23,P02-1040,0,0.0908038,"Missing"
2014.amta-researchers.23,C12-2116,0,0.0191779,"ing of our experiments. Experiments based on information theory metrics are presented in Section 4. Our proposed methods are described in Sections 5 (IR-based adaptation) and 6 (adapted VSF). Section 7 shows a summary of the results and outlines our main conclusions. 2 Data selection for domain adaptation Data selection is a way to adapt the about-to-be-trained model by using only the part of the training data that is more similar to the target domain. DS is a general approach, that has been applied to other tasks other than SMT, including Chinese word segmentation and Part-of-Speech tagging (Song et al., 2012). In SMT, DS is common practice for domain adaptation, where a subset of the bilingual parallel corpus is used for training some or all the models comprising typical phrase-based SMT models (translation, reordering and language models). Apart from better adaptation, data selection has the advantage of making the training set, and in turn the generated models, smaller. This is a factor we consider in this work, avoiding methods or parameters with which the training data is not significantly smaller than the entire set. In this section we review the most common techniques used for selecting trai"
2014.amta-researchers.23,tiedemann-2012-parallel,0,0.0300862,"ce baseline models in our experiments. • N EWS: News commentary. • W IT 3 (Cettolo et al., 2012): Transcribed and translated TED talks.4 2 http://www.trad-campaign.org/ 3 Within the TRAD project, this dataset is referred to as ‘H5’. This is the only dataset that is not publicly available. from https://wit3.fbk.eu/mt.php?release=2012-02 4 Downloaded Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 304 • T RANSCRIPTS (H4): Radio and television transcripts; standard Arabic. • M ULTI -UN: United Nations official documents.5 • O PEN S UBTITLES Tiedemann (2012): Movie subtitles.6 Dataset N EWS W IT 3 T RANSCRIPTS M ULTI -UN O PEN S UBTITLES Sentences 91K 87K 21K 9.9M 4.4M Tokens-Ar 2.2M 1.9M 561K 222.4M 27.7M Tokens-Fr 2.4M 2.4M 778K 285.5M 32.3M Table 3: Bilingual corpora for training. SMT system and preprocessing We used Moses (Koehn et al., 2007) as our phrase-based SMT system. IRSTLM (Federico et al., 2008) was used to train 5-gram language models over the target side of the (selected) bilingual corpora. Arabic tokenization was done similarly to MADA-TOKAN (Habash et al., 2009), but due to project constraints we used a reimplementation of this t"
2014.amta-researchers.23,I08-2088,0,0.0440643,"line of research is using information theory metrics to assess each one of the sentences in the pool, G, and choose the ones that are most similar to the provided in-domain data, I. Perplexity (PP) and cross-entropy (CE) Perplexity (PP) is perhaps the best known metric for DS for SMT. The idea is to compute the perplexity of a language model (LM) built on I, measure the perplexity of this LM over each sentence in G, and select the m sentences with the lowest PP scores. Gao et al. (2002) and Moore and Lewis (2010) employed this metric for language model adaptation, and in (Foster et al., 2010; Yasuda et al., 2008) it was used for translation model adaptation. The same technique is sometimes (e.g. Moore and Lewis (2010)) referred to as cross entropy (CE). Since, by definition, the cross-entropy is simply the exponent in the perplexity score, the base of the exponent is 2 and all scores are positive, a smaller PP means a smaller CE and vice versa. In other words, selecting m sentences with the lowest PP scores or with the lowest CE scores is equivalent. Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 302 Cross-entropy difference (CED) While PP (or i"
2014.eamt-1.23,P07-2045,0,0.00382925,"tences, we applied a Moses-based SMT system to generate their English hypotheses. Next, human translators were invited to correct MT outputs, giving us the post editions. The set of triples (source, hypothesis, post edition) was then divided into the training set (10000 first triples) and test set (881 remaining ones). The WCE model was trained over all 1-best hypotheses of the training set. More details on our WCE system can be found in next section. The N-best list (N = 1000) with involved alignment information is also obtained on the test set (1000 * 881 = 881000 sentences) by using Moses (Koehn et al., 2007) options “-n-best-list” and “-print-alignment-info-in-n-best”. Besides, the SGs are extracted by some parameter settings: “output-search-graph”, “-search-algorithm 1” (using cube pruning) and “-cube-pruning-pop-limit 5000” (adds 5000 hypotheses to each stack). They are compactly encoded under a plain formatted text file that is convenient to transform into userdefined structures for further processing. We then store the SG for each source sentence in a separated file, and the average size is 43.8 MB. 4.2 WCE scores and Oracle Labels We employ the Conditional Random Fields (Lafferty et al., 200"
2014.eamt-1.23,P11-1022,0,0.409726,"Missing"
2014.eamt-1.23,P11-2031,0,0.0326608,"amatically the baseline quality. BL+OR(1a) augments 7.87 points in BLEU, and diminishes 0.0607 (0.0794) point in TER(TERp-A), compared to BL. Meanwhile, with BL+OR(2a), these gains are 7.67, 0.0565 and 0.0514 (in that order). Besides, the contribution of our real WCE system scores seems less prominent, yet positive: the best performing BL+WCE(1a) 123 increases 1.49 BLEU points of BL (0.0029 and 0.0136 gained for TER and TERp-A). More remarkable, tiny p-values (in the range [0.00; 0.02], seen on Table 2) estimated between BLEU of each BL+WCE system and that of BL relying on Approximate Method (Clark et al., 2011) indicate that these performance improvements are significant. Results also reveal that the use of WCE labels are slightly more beneficial than that of confidence probabilities: BL+WCE(1a) and BL+WCE(2a) outperform BL+WCE(1b) and BL+WCE(2b). In both scenarios, we observe that the global update score (Definition 1) performs more fruitfully compared to the local one (Definition 2). For more insightful understanding about WCE scores’ acuteness, we make a comparison with the best achievable hypotheses in the SG (oracles), based on the “LM Oracle” approximation approach presented in (Sokolov et al."
2014.eamt-1.23,P08-2010,0,0.137938,"chievements, the conventional one-pass SMT decoders are still not sufficient yet in yielding human-acceptable translations (Zhang et al., 2006; Venugopal et al., 2007). Therefore, a number of methods to enhance them are proposed, such as: post-editing, re-ranking or re-decoding, etc. Post-editing (Parton et al., c 2014 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 117 2012) is in fact known to be a human-inspired task where the machine post edits translations in a second automatic pass. In re-ranking (Zhang et al., 2006; Duh and Kirchhoff, 2008; Bach et al., 2011), more features are integrated with the existing multiple model scores for re-selecting the best candidate among N-best list. Meanwhile, the redecoding process intervenes directly into the decoder’s search graph (SG), driving it to the optimal path (cheapest hypothesis). The two-pass decoder has been built by several discrepant ways in the past. Kirchhoff and Yang (2005); Zhang et al. (2006) train additional Language Models (LM) and combine LM scores with existing model scores to re-rank the N-best list. Also focusing on the idea of re-ranking, yet Bach et al. (2011); Luong"
2014.eamt-1.23,W05-0821,0,0.0340837,"ence, no derivative works, attribution, CCBY-ND. 117 2012) is in fact known to be a human-inspired task where the machine post edits translations in a second automatic pass. In re-ranking (Zhang et al., 2006; Duh and Kirchhoff, 2008; Bach et al., 2011), more features are integrated with the existing multiple model scores for re-selecting the best candidate among N-best list. Meanwhile, the redecoding process intervenes directly into the decoder’s search graph (SG), driving it to the optimal path (cheapest hypothesis). The two-pass decoder has been built by several discrepant ways in the past. Kirchhoff and Yang (2005); Zhang et al. (2006) train additional Language Models (LM) and combine LM scores with existing model scores to re-rank the N-best list. Also focusing on the idea of re-ranking, yet Bach et al. (2011); Luong et al. (2014) employ sentence and word confidence scores in the second pass. Meanwhile, Venugopal et al. (2007) do a first pass translation without LM, but use it to score the pruned search hyper-graph in the second pass. This work concentrates on a second automatic pass where the costs of all hypotheses in the decoder’s SG containing words of the N-best list will be updated regarding the"
2014.eamt-1.23,W14-0301,1,0.75748,"2008; Bach et al., 2011), more features are integrated with the existing multiple model scores for re-selecting the best candidate among N-best list. Meanwhile, the redecoding process intervenes directly into the decoder’s search graph (SG), driving it to the optimal path (cheapest hypothesis). The two-pass decoder has been built by several discrepant ways in the past. Kirchhoff and Yang (2005); Zhang et al. (2006) train additional Language Models (LM) and combine LM scores with existing model scores to re-rank the N-best list. Also focusing on the idea of re-ranking, yet Bach et al. (2011); Luong et al. (2014) employ sentence and word confidence scores in the second pass. Meanwhile, Venugopal et al. (2007) do a first pass translation without LM, but use it to score the pruned search hyper-graph in the second pass. This work concentrates on a second automatic pass where the costs of all hypotheses in the decoder’s SG containing words of the N-best list will be updated regarding the word quality predicted by Word Confidence Estimation (Ueffing and Ney, 2005) (WCE) system. In single-pass decoding, the decoder searches among complete paths (i.e. those cover all source words) for obtaining the optimalco"
2014.eamt-1.23,2012.eamt-1.34,0,0.270744,"Missing"
2014.eamt-1.23,E12-1013,0,0.0235204,"et al., 2011) indicate that these performance improvements are significant. Results also reveal that the use of WCE labels are slightly more beneficial than that of confidence probabilities: BL+WCE(1a) and BL+WCE(2a) outperform BL+WCE(1b) and BL+WCE(2b). In both scenarios, we observe that the global update score (Definition 1) performs more fruitfully compared to the local one (Definition 2). For more insightful understanding about WCE scores’ acuteness, we make a comparison with the best achievable hypotheses in the SG (oracles), based on the “LM Oracle” approximation approach presented in (Sokolov et al., 2012). This method allows to simplify the oracle decoding to the problem of searching for the cheapest path on a SG where all transition costs are replaced by the n-gram LM scores of the corresponding words. The LM is built for each source sentence using uniquely its target post-edition. We update the SG by assigning all edges with the LM back-off score of the word it contains (instead of using the current transition cost). Finally, we combine the oracles of all sentences yielding BLEU oracle of 66.48. To better understand the benefit of SG redecoding, we compare the obtained performances with thos"
2014.eamt-1.23,H05-1096,0,0.0347679,"(LM) and combine LM scores with existing model scores to re-rank the N-best list. Also focusing on the idea of re-ranking, yet Bach et al. (2011); Luong et al. (2014) employ sentence and word confidence scores in the second pass. Meanwhile, Venugopal et al. (2007) do a first pass translation without LM, but use it to score the pruned search hyper-graph in the second pass. This work concentrates on a second automatic pass where the costs of all hypotheses in the decoder’s SG containing words of the N-best list will be updated regarding the word quality predicted by Word Confidence Estimation (Ueffing and Ney, 2005) (WCE) system. In single-pass decoding, the decoder searches among complete paths (i.e. those cover all source words) for obtaining the optimalcost ones. Essentially, the hypothesis cost is a composite score, synthesized from various SMT models (reordering, translation, LMs etc.). Although the N-bests beat other SG hypotheses in term of model scores, there is no certain clue that they will be the closest to the human references. As the reference closeness is the users’ most pivotal concern on SMT decoder, this work establishes one second pass where model-independent scores related to word conf"
2014.eamt-1.23,2003.mtsummit-papers.52,0,0.296113,"earning method, with WAPITI toolkit (Lavergne et al., 2010), to train the WCE model. A number of knowledge resources are employed for extracting the system-based, lexical, syntactic and semantic characteristics of word, resulting in the total of 25 major feature types as follows: • Target Side: target word; bigram (trigram) backward sequences; number of occurrences • Source Side: source word(s) aligned to the target word 122 • Alignment Context (Bach et al., 2011): the combinations of the target (source) word and all aligned source (target) words in the window ±2 • Word posterior probability (Ueffing et al., 2003) • Pseudo-reference (Google Translate): Does the word appear in the pseudo reference? • Graph topology (Luong et al., 2013): number of alternative paths in the confusion set, maximum and minimum values of posterior probability distribution • Language model (LM) based: length of the longest sequence of the current word and its previous ones in the target (resp. source) LM. For example, with the target word wi : if the sequence wi−2 wi−1 wi appears in the target LM but the sequence wi−3 wi−2 wi−1 wi does not, the n-gram value for wi will be 3. • Lexical Features: word’s Part-Of-Speech (POS); seq"
2014.eamt-1.23,N07-1063,0,0.091305,"to update the cost of SG hypotheses containing it, we hope to “reinforce” or “weaken” them relied on word quality. After the update, new best translations are re-determined using updated costs. In the experiments on our real WCE scores and ideal (oracle) ones, the latter significantly boosts one-pass decoder by 7.87 BLEU points, meanwhile the former yields an improvement of 1.49 points for the same metric. 1 Introduction Beside plenty of commendable achievements, the conventional one-pass SMT decoders are still not sufficient yet in yielding human-acceptable translations (Zhang et al., 2006; Venugopal et al., 2007). Therefore, a number of methods to enhance them are proposed, such as: post-editing, re-ranking or re-decoding, etc. Post-editing (Parton et al., c 2014 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 117 2012) is in fact known to be a human-inspired task where the machine post edits translations in a second automatic pass. In re-ranking (Zhang et al., 2006; Duh and Kirchhoff, 2008; Bach et al., 2011), more features are integrated with the existing multiple model scores for re-selecting the best candidate among N-best list"
2014.eamt-1.23,P10-1062,0,0.269189,"the confusion set, maximum and minimum values of posterior probability distribution • Language model (LM) based: length of the longest sequence of the current word and its previous ones in the target (resp. source) LM. For example, with the target word wi : if the sequence wi−2 wi−1 wi appears in the target LM but the sequence wi−3 wi−2 wi−1 wi does not, the n-gram value for wi will be 3. • Lexical Features: word’s Part-Of-Speech (POS); sequence of POS of all its aligned source words; POS bigram (trigram) backward sequences; punctuation; proper name; numerical • Syntactic Features: null link (Xiong et al., 2010); constituent label; depth in the constituent tree • Semantic Features: number of word senses in WordNet. In the next step, the word’s reference labels (or so-called oracle labels) are initially set by using TERp-A toolkit (Snover et al., 2008) in one of the following classes: “I’ (insertions), “S” (substitutions), “T” (stem matches), “Y” (synonym matches), “P” (phrasal substitutions), “E” (exact matches) and are then regrouped into binary class: “G” (good word) or “B” (bad word). Once having the prediction model, we apply it on the test set (881 x 1000 best = 881000 sentences) and get needed"
2014.eamt-1.23,W06-1626,0,0.398676,"d in the N-best list to update the cost of SG hypotheses containing it, we hope to “reinforce” or “weaken” them relied on word quality. After the update, new best translations are re-determined using updated costs. In the experiments on our real WCE scores and ideal (oracle) ones, the latter significantly boosts one-pass decoder by 7.87 BLEU points, meanwhile the former yields an improvement of 1.49 points for the same metric. 1 Introduction Beside plenty of commendable achievements, the conventional one-pass SMT decoders are still not sufficient yet in yielding human-acceptable translations (Zhang et al., 2006; Venugopal et al., 2007). Therefore, a number of methods to enhance them are proposed, such as: post-editing, re-ranking or re-decoding, etc. Post-editing (Parton et al., c 2014 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 117 2012) is in fact known to be a human-inspired task where the machine post edits translations in a second automatic pass. In re-ranking (Zhang et al., 2006; Duh and Kirchhoff, 2008; Bach et al., 2011), more features are integrated with the existing multiple model scores for re-selecting the best ca"
2015.iwslt-papers.11,N07-1051,0,\N,Missing
2015.iwslt-papers.11,C10-2013,0,\N,Missing
2015.iwslt-papers.11,2003.mtsummit-papers.52,0,\N,Missing
2015.iwslt-papers.11,W10-1723,1,\N,Missing
2015.iwslt-papers.11,W12-3102,0,\N,Missing
2015.iwslt-papers.11,P07-2045,0,\N,Missing
2015.iwslt-papers.11,P11-1022,0,\N,Missing
2015.iwslt-papers.11,W13-2242,0,\N,Missing
2015.iwslt-papers.11,W14-3344,0,\N,Missing
2015.iwslt-papers.11,C04-1046,0,\N,Missing
2015.iwslt-papers.11,P10-1062,0,\N,Missing
2015.iwslt-papers.11,J03-1002,0,\N,Missing
2015.iwslt-papers.11,W12-3110,0,\N,Missing
2015.iwslt-papers.11,W14-3340,0,\N,Missing
2015.iwslt-papers.11,P15-4020,0,\N,Missing
2015.iwslt-papers.11,potet-etal-2012-collection,1,\N,Missing
2015.iwslt-papers.11,W14-3302,0,\N,Missing
2015.iwslt-papers.11,W13-2201,0,\N,Missing
2015.iwslt-papers.11,P10-1052,0,\N,Missing
2015.iwslt-papers.11,W12-3113,0,\N,Missing
2015.iwslt-papers.11,W13-2245,0,\N,Missing
2015.iwslt-papers.11,W14-3342,1,\N,Missing
2015.jeptalnrecital-court.32,C04-1053,0,0.785086,"Missing"
2015.jeptalnrecital-court.32,A00-1031,0,0.599263,"Missing"
2015.jeptalnrecital-court.32,W06-2920,0,0.218701,"Missing"
2015.jeptalnrecital-court.32,P11-1061,0,0.266815,"Missing"
2015.jeptalnrecital-court.32,P13-2112,0,0.68192,"Missing"
2015.jeptalnrecital-court.32,J07-3002,0,0.443875,"Missing"
2015.jeptalnrecital-court.32,2005.mtsummit-papers.11,0,0.0424531,"Missing"
2015.jeptalnrecital-court.32,P00-1056,0,0.757351,"Missing"
2015.jeptalnrecital-court.32,H05-1108,0,0.488272,"Missing"
2015.jeptalnrecital-court.32,P06-1146,0,0.0656778,"Missing"
2015.jeptalnrecital-court.32,petrov-etal-2012-universal,0,0.160827,"Missing"
2015.jeptalnrecital-court.32,Q13-1001,0,0.0284499,"Missing"
2015.jeptalnrecital-court.32,F14-1005,0,0.0350045,"Missing"
2015.jeptalnrecital-court.32,H01-1035,0,0.707797,"Missing"
2015.jeptalnrecital-long.21,2014.iwslt-papers.3,0,0.0848215,"Missing"
2015.jeptalnrecital-long.21,W13-2242,0,0.0550652,"Missing"
2015.jeptalnrecital-long.21,P08-2010,0,0.0603965,"Missing"
2015.jeptalnrecital-long.21,galliano-etal-2006-corpus,0,0.0805766,"Missing"
2015.jeptalnrecital-long.21,W13-2245,0,0.0628553,"Missing"
2015.jeptalnrecital-long.21,P07-2045,0,0.00382776,"Missing"
2015.jeptalnrecital-long.21,P10-1052,0,0.0639667,"Missing"
2015.jeptalnrecital-long.21,2014.eamt-1.23,1,0.818099,"Missing"
2015.jeptalnrecital-long.21,W14-0301,1,0.844331,"Missing"
2015.jeptalnrecital-long.21,W13-2248,1,0.897933,"Missing"
2015.jeptalnrecital-long.21,2012.eamt-1.34,0,0.0553249,"Missing"
2015.jeptalnrecital-long.21,W10-1723,1,0.896895,"Missing"
2015.jeptalnrecital-long.21,potet-etal-2012-collection,1,0.874018,"Missing"
2015.jeptalnrecital-long.21,D08-1065,0,0.0612848,"Missing"
2015.jeptalnrecital-long.21,2003.mtsummit-papers.52,0,0.128287,"Missing"
2015.jeptalnrecital-long.21,N07-1063,0,0.0742563,"Missing"
2015.jeptalnrecital-long.21,P10-1062,0,0.0483527,"Missing"
2015.jeptalnrecital-long.21,W06-3110,0,0.087503,"Missing"
2015.jeptalnrecital-long.21,W06-1626,0,0.0720658,"Missing"
2015.mtsummit-papers.7,W15-1006,0,0.321027,"abase. For English, the database was developed by Snover (2009a). Later, Denkowski and Lavie (2010b), released paraphrase databases for Czech, German, Spanish and French. In 2014, METEOR Universal was released (Denkowski and Lavie 2014) that enabled the construction of the paraphrase database using only the parallel corpora used to develop the MT system (which was not the case in 2010). Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 81 In order to prevent synonyms/paraphrases corresponding to different senses to be treated as semantically equivalent, Apidianaki and Marie (2015) proposed METEOR-WSD. The English references are further disambiguated and annotated using Babelfly (Moro et al., 2014) for several language pairs (French, Hindi, German, Czech and Russian to English). For their experiment, Apidianaki and Marie (2015) got a better segment-level Kendall’s τ correlation than METEOR for 4 language pairs when the paraphrase module was activated. 2.3. Lexical resources 2.3.1. WordNet WordNet is a large lexical database for English, developed by linguists of Princeton University (Fellbaum, 1998). Nowadays, it has become an important and a very useful resource for NL"
2015.mtsummit-papers.7,W05-0909,0,0.924239,"ranslation quality, such as adequacy, fluency, and intelligibility. However, subjective evaluation conducted a posteriori often costs too much (in term of human resources) and, thus, objective evaluation metrics (fast and cheap as long as references are available) are often preferred nowadays. One drawback with automatic evaluation metrics is that they compare the MT hypothesis with few (and sometimes only one) reference translations. This is definitely not enough to capture lexical variation in translation. For this reason, metrics which exploit synonymy or stem similarities, such as METEOR (Banerjee and Lavie, 2005), exhibit better correlation with human judgement. METEOR maps words with the same stem or the same synset using lexico-semantic resources. However, so far, the full potential of METEOR is only exploited when English is the target language (use of WordNet). Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 80 Contribution This paper proposes an extension of METEOR for multiple target languages using a lexical resource called DBnary (Sérasset, 2015). DBnary is an extraction in RDF of the lexical data of multiple editions of Wiktionary. It has several mill"
2015.mtsummit-papers.7,W10-1751,0,0.137501,"produced by a three-leveled mapping approach between the hypothesis and the reference, using additional resources if needed: exact match of the surface forms of the words, exact match of the computed stems of the words, synonymy overlap through shared WordNet “synset” of the words. The second mapping level uses a stemmer and the third mapping level uses Enlgish WordNet. While no free WordNets are available for languages such as French, Spanish or German, current implementation of METEOR for such languages do not support the third mapping level. 2.2. METEOR: the recent extensions METEOR-NEXT (Denkowski and Lavie, 2010a), was introduced to better correlate with human-targeted Translation Edit Rate (HTER) (Snover et al., 2006), a semi-automatic postediting based metric which measures the distance between a MT hypothesis and its post-edited version. The goal was to go beyond a strictly world-level metric with a new aligner supporting phrases (multi-word) matches. Thus, a fourth mapping level was added to implement this new feature using a paraphrase database. For English, the database was developed by Snover (2009a). Later, Denkowski and Lavie (2010b), released paraphrase databases for Czech, German, Spanish"
2015.mtsummit-papers.7,N10-1031,0,0.316805,"produced by a three-leveled mapping approach between the hypothesis and the reference, using additional resources if needed: exact match of the surface forms of the words, exact match of the computed stems of the words, synonymy overlap through shared WordNet “synset” of the words. The second mapping level uses a stemmer and the third mapping level uses Enlgish WordNet. While no free WordNets are available for languages such as French, Spanish or German, current implementation of METEOR for such languages do not support the third mapping level. 2.2. METEOR: the recent extensions METEOR-NEXT (Denkowski and Lavie, 2010a), was introduced to better correlate with human-targeted Translation Edit Rate (HTER) (Snover et al., 2006), a semi-automatic postediting based metric which measures the distance between a MT hypothesis and its post-edited version. The goal was to go beyond a strictly world-level metric with a new aligner supporting phrases (multi-word) matches. Thus, a fourth mapping level was added to implement this new feature using a paraphrase database. For English, the database was developed by Snover (2009a). Later, Denkowski and Lavie (2010b), released paraphrase databases for Czech, German, Spanish"
2015.mtsummit-papers.7,W13-2202,0,0.0350395,"Missing"
2015.mtsummit-papers.7,W14-3336,0,0.110331,"or 2 systems picked up randomy from WMT14 data (French-English MT) In Table 3, METEOR-TTG shows a slight increase in the score, compared to METEORMorphy, because TreeTagger lemmatizes all categories (including Adverbs), whereas Morphy lemmatizes only three categories (Noun, Verb and Adjective). 4. Correlation with human judgements In order to evaluate the correlation of our proposed METEOR-DBnary with human judgements of machine translation outputs, we used the data from the WMT13 Metrics Shared Task (Machacek and Bojar, 2013) for English-to-Spanish MT, and from the WMT14 Metrics Shared Task (Machacek and Bojar, 2014) for French-English, English-French, EnglishGerman and English-Russian MT. Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 84 We present the results in a similar fashion as in the WMT metrics task methodology using the following metrics. More details and formulas can be found in (Machacek and Bojar, 2013) or (Machacek and Bojar, 2014). • System-level using Pearson correlation coefficient between system ranking based on human judgments versus METEOR (we will use our augmented metric and compare it to the baseline METEOR). • Segment-level using Kendall’s"
2015.mtsummit-papers.7,Q14-1019,0,0.0709859,"ses for Czech, German, Spanish and French. In 2014, METEOR Universal was released (Denkowski and Lavie 2014) that enabled the construction of the paraphrase database using only the parallel corpora used to develop the MT system (which was not the case in 2010). Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 81 In order to prevent synonyms/paraphrases corresponding to different senses to be treated as semantically equivalent, Apidianaki and Marie (2015) proposed METEOR-WSD. The English references are further disambiguated and annotated using Babelfly (Moro et al., 2014) for several language pairs (French, Hindi, German, Czech and Russian to English). For their experiment, Apidianaki and Marie (2015) got a better segment-level Kendall’s τ correlation than METEOR for 4 language pairs when the paraphrase module was activated. 2.3. Lexical resources 2.3.1. WordNet WordNet is a large lexical database for English, developed by linguists of Princeton University (Fellbaum, 1998). Nowadays, it has become an important and a very useful resource for NLP applications, such as machine translation, word sense disambiguation, cross-lingual information retrieval etc. WordNe"
2015.mtsummit-papers.7,P02-1040,0,0.0938324,"Missing"
2015.mtsummit-papers.7,2006.amta-papers.25,0,0.0897145,"if needed: exact match of the surface forms of the words, exact match of the computed stems of the words, synonymy overlap through shared WordNet “synset” of the words. The second mapping level uses a stemmer and the third mapping level uses Enlgish WordNet. While no free WordNets are available for languages such as French, Spanish or German, current implementation of METEOR for such languages do not support the third mapping level. 2.2. METEOR: the recent extensions METEOR-NEXT (Denkowski and Lavie, 2010a), was introduced to better correlate with human-targeted Translation Edit Rate (HTER) (Snover et al., 2006), a semi-automatic postediting based metric which measures the distance between a MT hypothesis and its post-edited version. The goal was to go beyond a strictly world-level metric with a new aligner supporting phrases (multi-word) matches. Thus, a fourth mapping level was added to implement this new feature using a paraphrase database. For English, the database was developed by Snover (2009a). Later, Denkowski and Lavie (2010b), released paraphrase databases for Czech, German, Spanish and French. In 2014, METEOR Universal was released (Denkowski and Lavie 2014) that enabled the construction o"
2015.mtsummit-papers.7,W09-0441,0,0.272425,"Missing"
2016.jeptalnrecital-long.21,2012.iwslt-evaluation.13,1,0.869457,"Missing"
2016.jeptalnrecital-long.21,W03-1022,0,0.120238,"Missing"
2016.jeptalnrecital-long.21,J05-1003,0,0.0406407,"Missing"
2016.jeptalnrecital-long.21,W99-0613,0,0.4842,"Missing"
2016.jeptalnrecital-long.21,P11-1061,0,0.0697689,"Missing"
2016.jeptalnrecital-long.21,P13-2112,0,0.0440674,"Missing"
2016.jeptalnrecital-long.21,J07-3002,0,0.0529836,"Missing"
2016.jeptalnrecital-long.21,N15-1157,0,0.022943,"Missing"
2016.jeptalnrecital-long.21,P04-1013,0,0.0197756,"Missing"
2016.jeptalnrecital-long.21,W08-0510,0,0.0166881,"Missing"
2016.jeptalnrecital-long.21,D11-1110,0,0.0525488,"Missing"
2016.jeptalnrecital-long.21,P12-1073,0,0.0332014,"Missing"
2016.jeptalnrecital-long.21,W15-1521,0,0.0202754,"Missing"
2016.jeptalnrecital-long.21,H93-1061,0,0.719631,"Missing"
2016.jeptalnrecital-long.21,P00-1056,0,0.220339,"Missing"
2016.jeptalnrecital-long.21,H05-1108,0,0.103874,"Missing"
2016.jeptalnrecital-long.21,petrov-etal-2012-universal,0,0.091347,"Missing"
2016.jeptalnrecital-long.21,C12-1146,0,0.0298235,"Missing"
2016.jeptalnrecital-long.21,N12-1052,0,0.0530419,"Missing"
2016.jeptalnrecital-long.21,P12-1068,0,0.064653,"Missing"
2016.jeptalnrecital-long.21,D14-1187,0,0.0220272,"Missing"
2016.jeptalnrecital-long.21,K15-1008,0,0.0477618,"Missing"
2016.jeptalnrecital-long.21,H01-1035,0,0.292025,"Missing"
2016.jeptalnrecital-long.21,S07-1051,0,0.0610886,"Missing"
2016.jeptalnrecital-long.21,Y15-1016,1,0.803477,"Missing"
2016.jeptalnrecital-long.21,2015.jeptalnrecital-court.32,1,0.8592,"Missing"
2016.jeptalnrecital-long.23,W05-0909,0,0.356762,"Missing"
2016.jeptalnrecital-long.23,D10-1115,0,0.0873607,"Missing"
2016.jeptalnrecital-long.23,S15-2048,0,0.0407588,"Missing"
2016.jeptalnrecital-long.23,D12-1050,0,0.0648203,"Missing"
2016.jeptalnrecital-long.23,W14-3302,0,0.0614457,"Missing"
2016.jeptalnrecital-long.23,D14-1179,0,0.0134047,"Missing"
2016.jeptalnrecital-long.23,J81-4005,0,0.769578,"Missing"
2016.jeptalnrecital-long.23,N10-1031,0,0.0459707,"Missing"
2016.jeptalnrecital-long.23,W10-1751,0,0.0595858,"Missing"
2016.jeptalnrecital-long.23,W14-3348,0,0.101712,"Missing"
2016.jeptalnrecital-long.23,2015.mtsummit-papers.7,1,0.806899,"Missing"
2016.jeptalnrecital-long.23,W15-3047,0,0.0354392,"Missing"
2016.jeptalnrecital-long.23,P12-1092,0,0.109942,"Missing"
2016.jeptalnrecital-long.23,W15-1521,0,0.0485311,"Missing"
2016.jeptalnrecital-long.23,W14-3336,0,0.0360437,"Missing"
2016.jeptalnrecital-long.23,N13-1090,0,0.098669,"Missing"
2016.jeptalnrecital-long.23,Q14-1019,0,0.0309918,"Missing"
2016.jeptalnrecital-long.23,D15-1222,0,0.045271,"Missing"
2016.jeptalnrecital-long.23,P02-1040,0,0.107255,"Missing"
2016.jeptalnrecital-long.23,2006.amta-papers.25,0,0.162279,"Missing"
2016.jeptalnrecital-long.23,W09-0441,0,0.0556121,"Missing"
2016.jeptalnrecital-long.23,P10-1040,0,0.136217,"Missing"
2016.jeptalnrecital-long.23,W15-3051,0,0.035997,"Missing"
2016.jeptalnrecital-long.23,D13-1141,0,0.0792269,"Missing"
2017.jeptalnrecital-long.5,W16-4015,0,0.0357791,"Missing"
2017.jeptalnrecital-long.5,D10-1115,0,0.0298498,"Missing"
2017.jeptalnrecital-long.5,S15-2048,0,0.0566084,"Missing"
2017.jeptalnrecital-long.5,L16-1662,1,0.871404,"Missing"
2017.jeptalnrecital-long.5,2010.iwslt-evaluation.12,1,0.868413,"Missing"
2017.jeptalnrecital-long.5,D12-1050,0,0.0203048,"Missing"
2017.jeptalnrecital-long.5,D14-1179,0,0.0166687,"Missing"
2017.jeptalnrecital-long.5,P08-1115,0,0.0924009,"Missing"
2017.jeptalnrecital-long.5,P08-2015,0,0.0741144,"Missing"
2017.jeptalnrecital-long.5,P12-1092,0,0.0586573,"Missing"
2017.jeptalnrecital-long.5,P07-2045,0,0.0106168,"Missing"
2017.jeptalnrecital-long.5,D09-1040,0,0.076908,"Missing"
2017.jeptalnrecital-long.5,2005.iwslt-1.19,0,0.0745007,"Missing"
2017.jeptalnrecital-long.5,2014.amta-researchers.23,1,0.862839,"Missing"
2017.jeptalnrecital-long.5,J97-2003,0,0.286516,"Missing"
2017.jeptalnrecital-long.5,P03-1021,0,0.0206422,"Missing"
2017.jeptalnrecital-long.5,P02-1040,0,0.0970493,"Missing"
2017.jeptalnrecital-long.5,P10-1040,0,0.0364167,"Missing"
2019.nsurl-1.9,W14-2201,0,0.0138345,"worked devices that make the data collection task less tedious and less costly. A great example is the LIG-Aikuma application, an extension of Aikuma, “a mobile app that is designed to put the key language documentation tasks of recording, respeaking, and translating in the hands of a speech community… It collects recordings, respeakings, and interpretations, and organizes them for later synchronization with the cloud and archival storage… Recordings are stored alongside a wealth of metadata, including language, GPS coordinates, speaker, and offsets on time-aligned translations and comments” (Bird et al., 2014). This application is open source, freely downloadable on any android based smartphone or mobile device. In fact, “the application LIG-AIKUMA has been successfully tested on different devices (including Samsung Galaxy SIII, Google Nexus 6, HTC Desire 820 smartphones and a Galaxy Tab 4 tablet), and can be downloaded from a dedicated website … Originally intended for language documentation and data collection in the field, the app has also been useful for collecting speech for technological development purposes targeting underresourced languages” (Besacier et al., 2019). In addition to its avail"
2020.coling-main.303,W17-0121,1,0.835002,"uery False Positive query translation hit translation nahne nemekke balanda bininj nemekke mahni mahne namekke balanda-ken bininj-beh yekke mahne it (pronoun) that one white man man that one this (demonstrative) this (demonstrative) that one (other spelling) of the white man (genitive) from the man dry season this (other spelling) Table 2: Top false positive generated by the spoken term detection system in Kunwinjku 3425 (a) screenshot of the app (b) deployment of the app Figure 3: Lexical confirmation app 6 Deployment Building on prior work developing mobile tools for language documentation (Bettinson and Bird, 2017), we have begun to explore methods for deploying the pipeline in a remote community. While the first step of identification of new words is straightforward, the task of lexical confirmation might be much more complex to apply. The members of an Aboriginal community might not be familiar with technologies and they are not necessarily literate (in the narrow western sense). Taking into account these constraints, we built a lexical confirmation app and trialled it on a small lexicon (Fig. 3). The idea would be to load the output of the spoken term detection system into the app. Then a speaker can"
2020.coling-main.303,2020.cl-4.1,1,0.893829,"stems, but the amount of data available in Indigenous language contexts is usually too limited for such methods to be effective. Recent research has shown the efficacy of spoken term detection methods when data are scarce (Menon et al., 2018a; Menon et al., 2018b). Taking advantage of the transcription of a few words would allow us to propagate it through the speech collection and thus assist language workers in their regular transcription work. So-called “sparse transcription” would be also a way to navigate a speech collection and allow us to be selective about what needs to be transcribed (Bird, 2020). Several tools exist for manual transcription, such as Elan and Praat (Wittenburg et al., 2006; Boersma and Weenink, 1996). However such transcriptions are often made in isolation from the speech community (First Languages Australia, 2014), and so we miss out on the opportunity to take advantage of the interests and skills of local people to shape and carry out the transcription work. We present a fieldwork pipeline which combines automatic speech processing and human expertise to support speech transcription in almost-zero resource settings. After giving the background, we detail the workflo"
2020.coling-main.303,L18-1531,1,0.863919,"Missing"
2020.coling-main.303,2020.lrec-1.307,0,0.0356795,"ts and skills of local people to shape and carry out the transcription work. We present a fieldwork pipeline which combines automatic speech processing and human expertise to support speech transcription in almost-zero resource settings. After giving the background, we detail the workflow and propose a pilot experiment on two very low-resource corpora. 2 Background Existing approaches to automatic transcription of endangered languages involve methods that have been developed for automatic speech recognition. While a few hours of transcribed speech can be enough to train single-speaker models (Gupta and Boulianne, 2020b), speaker-independent models require a large amount of training data to produce useful transcriptions (Gupta and Boulianne, 2020a; Foley et al., 2018). Moreover, they draw language workers and speakers into the time-consuming task of exhaustive transcription, forcing them to transcribe densely, including passages that may be difficult or impossible given the early state of our knowledge of the language. A more suitable approach, we believe, involves beginning with stretches of speech where we have the greatest confidence, and only later tackling the more difficult parts. This work is license"
2020.coling-main.303,2020.sltu-1.51,0,0.0345672,"ts and skills of local people to shape and carry out the transcription work. We present a fieldwork pipeline which combines automatic speech processing and human expertise to support speech transcription in almost-zero resource settings. After giving the background, we detail the workflow and propose a pilot experiment on two very low-resource corpora. 2 Background Existing approaches to automatic transcription of endangered languages involve methods that have been developed for automatic speech recognition. While a few hours of transcribed speech can be enough to train single-speaker models (Gupta and Boulianne, 2020b), speaker-independent models require a large amount of training data to produce useful transcriptions (Gupta and Boulianne, 2020a; Foley et al., 2018). Moreover, they draw language workers and speakers into the time-consuming task of exhaustive transcription, forcing them to transcribe densely, including passages that may be difficult or impossible given the early state of our knowledge of the language. A more suitable approach, we believe, involves beginning with stretches of speech where we have the greatest confidence, and only later tackling the more difficult parts. This work is license"
2020.coling-main.303,wittenburg-etal-2006-elan,0,0.0671219,"too limited for such methods to be effective. Recent research has shown the efficacy of spoken term detection methods when data are scarce (Menon et al., 2018a; Menon et al., 2018b). Taking advantage of the transcription of a few words would allow us to propagate it through the speech collection and thus assist language workers in their regular transcription work. So-called “sparse transcription” would be also a way to navigate a speech collection and allow us to be selective about what needs to be transcribed (Bird, 2020). Several tools exist for manual transcription, such as Elan and Praat (Wittenburg et al., 2006; Boersma and Weenink, 1996). However such transcriptions are often made in isolation from the speech community (First Languages Australia, 2014), and so we miss out on the opportunity to take advantage of the interests and skills of local people to shape and carry out the transcription work. We present a fieldwork pipeline which combines automatic speech processing and human expertise to support speech transcription in almost-zero resource settings. After giving the background, we detail the workflow and propose a pilot experiment on two very low-resource corpora. 2 Background Existing approa"
2020.coling-main.443,P19-1126,0,0.0484675,"ence-to-Sequence models are state-of-the-art in a variety of sequence transduction tasks including machine translation (MT). The most widespread models are composed of an encoder that reads the entire source sequence, while a decoder (often equipped with an attention mechanism) iteratively produces the next target token given the full source and the decoded prefix. Aside from the conventional offline use case, recent works adapt sequence-to-sequence models for online (also referred to as simultaneous) decoding with low-latency constraints (Gu et al., 2017; Dalvi et al., 2018; Ma et al., 2019; Arivazhagan et al., 2019). Online decoding is desirable for applications such as real-time speech-to-speech interpretation. In such scenarios, the decoding process starts before the entire input sequence is available, and online prediction generally comes at the cost of reduced translation quality. In this work we focus on online neural machine translation (NMT) with deterministic wait-k decoding policies (Dalvi et al., 2018; Ma et al., 2019). With such a policy, we first read k tokens from the source then alternate between producing a target token and reading another source token (see Figure 1). We consider two seque"
2020.coling-main.443,2020.iwslt-1.27,0,0.0109404,". (2017) rely on Reinforcement Learning to optimize a read/write policy. To combine the end-to-end training of wait-k models with the flexibility of dynamic online decoding, Zheng et al. (2019b) and Zheng et al. (2019a) use Imitation Learning. Recent work on dynamic online translation use monotonic alignments (Raffel et al., 2017) with either a limited or infinite lookback (Chiu and Raffel, 2018; Arivazhagan et al., 2019; Ma et al., 2020). Another adjacent research direction enables revision during online translation to alleviate decoding constraints (Niehues et al., 2016; Zheng et al., 2020; Arivazhagan et al., 2020). In this work, we focus on wait-k and greedy decoding strategies, but unlike other wait-k models (Ma et al., 2019; Zheng et al., 2019b; Zheng et al., 2019a) we opt for uni-directional encoders which are efficient to train in an online setup (Elbayad et al., 2020). 2.2 Error analysis for NMT With the advances in NMT (Bahdanau et al., 2015; Vaswani et al., 2017), the quality of translations has improved substantially leading to claims of human parity in high-resource settings (Wu et al., 2016; Hassan et al., 2018). With such improvements, it becomes more and more difficult for automatic evaluat"
2020.coling-main.443,D16-1025,0,0.235741,"to claims of human parity in high-resource settings (Wu et al., 2016; Hassan et al., 2018). With such improvements, it becomes more and more difficult for automatic evaluation metrics such as BLEU (Papineni et al., 2002) to detect subtle differences. Manual error annotation is a more instructive quality assessment to gain insights into the performance of MT systems, especially in direct comparisons. Human evaluation might lead to conclusions at odd with automatic metrics, as was the case in last year’s WMT English-German evaluation (Barrault et al., 2019). Comparison to SMT and rule-based MT. Bentivogli et al. (2016) studied post-editing of EnglishGerman TED talks and found that NMT makes considerably less word order errors than SMT. They also observed that the performance of NMT degrades faster than SMT with increasing sentence length. Toral and Sánchez-Cartagena (2017) reached similar conclusions on news stories in 9 language directions. Isabelle et al. (2017) tested NMT systems with challenging linguistic material and highlighted the efficiency of NMT systems at handling subject-verb agreement and syntactic and lexico-syntactic divergences and the struggle of NMT with idiomatic phrases. Castilho et al."
2020.coling-main.443,N18-2079,0,0.0741856,"the online setup. 1 Introduction Sequence-to-Sequence models are state-of-the-art in a variety of sequence transduction tasks including machine translation (MT). The most widespread models are composed of an encoder that reads the entire source sequence, while a decoder (often equipped with an attention mechanism) iteratively produces the next target token given the full source and the decoded prefix. Aside from the conventional offline use case, recent works adapt sequence-to-sequence models for online (also referred to as simultaneous) decoding with low-latency constraints (Gu et al., 2017; Dalvi et al., 2018; Ma et al., 2019; Arivazhagan et al., 2019). Online decoding is desirable for applications such as real-time speech-to-speech interpretation. In such scenarios, the decoding process starts before the entire input sequence is available, and online prediction generally comes at the cost of reduced translation quality. In this work we focus on online neural machine translation (NMT) with deterministic wait-k decoding policies (Dalvi et al., 2018; Ma et al., 2019). With such a policy, we first read k tokens from the source then alternate between producing a target token and reading another source"
2020.coling-main.443,N13-1073,0,0.034293,"scribe the factors we use to analyze the results of automatic and human evaluations. Source length. Similar to other evaluation studies of NMT systems (Bentivogli et al., 2016; Toral and Sánchez-Cartagena, 2017; Koehn and Knowles, 2017), we look into the length of the source sequence and its effect on the quality of translation. Lagging difficulty (LD). In the particular context of online translation, source-target alignments are an indicator of how easy it is to translate an input. To measure the lagging difficulty of a pair (x, y), we first estimate source-target alignments with fast-align (Dyer et al., 2013) and then infer a reference decoding path. he reference decoding path, denoted with z align , is non-decreasing and guarantees that at a given decoding position t, zt is larger than or equal to all the source positions aligned with t. The lagging difficulty is finally measured as the Average Lagging (AL) (Ma et al., 2019) of the parsed z align as follows: P align LD(x, y) = τ1 τt=1 zt − |x| τ = arg mint {t |zt = |x|}. (1) |y |(t − 1), AL measures the lag in tokens behind the ideal simultaneous policy wait-0, and so, LD measures the lag of a realistic simultaneous translation that has the align"
2020.coling-main.443,K18-1010,1,0.931939,"ence is available, and online prediction generally comes at the cost of reduced translation quality. In this work we focus on online neural machine translation (NMT) with deterministic wait-k decoding policies (Dalvi et al., 2018; Ma et al., 2019). With such a policy, we first read k tokens from the source then alternate between producing a target token and reading another source token (see Figure 1). We consider two sequence-to-sequence models, a position-based convolutional model and a content-based model with self-attention. We specifically use the recent convolutional Pervasive Attention (Elbayad et al., 2018) and Transformer (Vaswani et al., 2017). We investigate, for both architectures, the impact of online decoding constraints on the translation quality through a carefully designed human evaluation on English )German and German )English language pairs. Our contributions are twofold: (1) our work, to the best of our knowledge, is the first human evaluation of online vs. offline NMT systems. (2) We compare Transformer and Pervasive Attention architectures highlighting the advantages and shortcomings of each when we shift to the online setup. The rest of this paper is organized as follows: we prese"
2020.coling-main.443,1994.amta-1.9,0,0.480581,"ergences and the struggle of NMT with idiomatic phrases. Castilho et al. (2017a), Castilho et al. (2017b) and Van Brussel et al. (2018) observed that NMT outperforms SMT in terms of fluency, but at the same time it is more prone to accuracy errors. Klubiˇcka et al. (2018) made similar observations in an evaluation of English-Croatian, concluding that compared to SMT and rule-based MT, NMT tends to sacrifice completeness of translations in order to increase fluency. Error typologies for MT. Various error typologies with different levels of granularity have been proposed to evaluate MT systems (Flanagan, 1994; Vilar et al., 2006; Stymne and Ahrenberg, 2012; Lommel et al., 2014b). In their evaluation of SMT outputs, Vilar et al. (2006) defined five error categories: missing words, word order, incorrect words, unknown words and punctuation errors. Bentivogli et al. (2016) followed a simpler classification with three types of errors: morphological, lexical, and word order. Their 5048 choice was motivated by the difficulty to disambiguate sub-categories of lexical errors (Popovi´c and Ney, 2011). The evaluation in Klubiˇcka et al. (2018) is based on the Multidimensional Quality Metrics (MQM) framework"
2020.coling-main.443,D14-1140,0,0.0305713,"Missing"
2020.coling-main.443,E17-1099,0,0.0944045,"when we shift to the online setup. 1 Introduction Sequence-to-Sequence models are state-of-the-art in a variety of sequence transduction tasks including machine translation (MT). The most widespread models are composed of an encoder that reads the entire source sequence, while a decoder (often equipped with an attention mechanism) iteratively produces the next target token given the full source and the decoded prefix. Aside from the conventional offline use case, recent works adapt sequence-to-sequence models for online (also referred to as simultaneous) decoding with low-latency constraints (Gu et al., 2017; Dalvi et al., 2018; Ma et al., 2019; Arivazhagan et al., 2019). Online decoding is desirable for applications such as real-time speech-to-speech interpretation. In such scenarios, the decoding process starts before the entire input sequence is available, and online prediction generally comes at the cost of reduced translation quality. In this work we focus on online neural machine translation (NMT) with deterministic wait-k decoding policies (Dalvi et al., 2018; Ma et al., 2019). With such a policy, we first read k tokens from the source then alternate between producing a target token and re"
2020.coling-main.443,E09-1040,0,0.037509,"ub-categories of lexical errors (Popovi´c and Ney, 2011). The evaluation in Klubiˇcka et al. (2018) is based on the Multidimensional Quality Metrics (MQM) framework (Lommel et al., 2014b). In their study, they found that mistranslation is the most frequent accuracy error in NMT translations. Van Brussel et al. (2018) observed that mistranslation and omission errors are particularly challenging for NMT users, because contrary to SMT and rule-based MT, these errors are often not indicated by flawed fluency, which makes them more difficult to identify and post-edit. Error analysis for online MT. Hamon et al. (2009) evaluated a spoken language translation system (ASR+MT) in comparison to a human interpreter, where each segment is judged in terms of adequacy and fluency. Mieno et al. (2015), in search for a unique evaluation metric, examined the usefulness of delay and accuracy in predicting the human judgment of a simultaneous speech translation system. To our knowledge, our work is the first to propose a fine-grained human evaluation of online NMT systems. It focuses on English )German and German )English language pairs, the latter being particularly sensitive to latency constraints. This is in part due"
2020.coling-main.443,D15-1006,0,0.0124546,"line NMT systems. (2) We compare Transformer and Pervasive Attention architectures highlighting the advantages and shortcomings of each when we shift to the online setup. The rest of this paper is organized as follows: we present in §2 related work pertaining to online MT and error analysis of NMT systems. We describe our experimental setup for human evaluation and error analysis in §3. We follow with the evaluation results in §4 and summarize our findings in §5. 2 2.1 Related work Online NMT After pioneering works on online statistical MT (SMT) (Fügen et al., 2007; Yarmohammadi et al., 2013; He et al., 2015; Grissom II et al., 2014; Oda et al., 2015), one of the early works with attention-based This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 5047 Proceedings of the 28th International Conference on Computational Linguistics, pages 5047–5058 Barcelona, Spain (Online), December 8-13, 2020 Source x1 x2 x3 x4 x5 &lt;/s&gt; Source x1 x2 x3 x4 x5 &lt;/s&gt; &lt;s&gt; &lt;s&gt; y1 y1 y2 y2 y3 y3 y4 y4 &lt;/s&gt; &lt;/s&gt; Online (wait-3) Offline (wait-∞) Figure 1: Wait-k decoding as a sequence of reads (horizontal) and writes (vertical) o"
2020.coling-main.443,D17-1263,0,0.0178801,"of MT systems, especially in direct comparisons. Human evaluation might lead to conclusions at odd with automatic metrics, as was the case in last year’s WMT English-German evaluation (Barrault et al., 2019). Comparison to SMT and rule-based MT. Bentivogli et al. (2016) studied post-editing of EnglishGerman TED talks and found that NMT makes considerably less word order errors than SMT. They also observed that the performance of NMT degrades faster than SMT with increasing sentence length. Toral and Sánchez-Cartagena (2017) reached similar conclusions on news stories in 9 language directions. Isabelle et al. (2017) tested NMT systems with challenging linguistic material and highlighted the efficiency of NMT systems at handling subject-verb agreement and syntactic and lexico-syntactic divergences and the struggle of NMT with idiomatic phrases. Castilho et al. (2017a), Castilho et al. (2017b) and Van Brussel et al. (2018) observed that NMT outperforms SMT in terms of fluency, but at the same time it is more prone to accuracy errors. Klubiˇcka et al. (2018) made similar observations in an evaluation of English-Croatian, concluding that compared to SMT and rule-based MT, NMT tends to sacrifice completeness"
2020.coling-main.443,W17-3204,0,0.0205479,"1). TF models have 2M more parameters compared to PA (19M to 17M), they are however faster to train (PA is 8 times slower). In test time, the two models decode in comparable speeds. For a fair comparison, both online and offline models are decoded greedily. We will refer these four models with PA-offline, PA-online, TF-offline and TF-online. 3.1 Analysis factors In this section, we describe the factors we use to analyze the results of automatic and human evaluations. Source length. Similar to other evaluation studies of NMT systems (Bentivogli et al., 2016; Toral and Sánchez-Cartagena, 2017; Koehn and Knowles, 2017), we look into the length of the source sequence and its effect on the quality of translation. Lagging difficulty (LD). In the particular context of online translation, source-target alignments are an indicator of how easy it is to translate an input. To measure the lagging difficulty of a pair (x, y), we first estimate source-target alignments with fast-align (Dyer et al., 2013) and then infer a reference decoding path. he reference decoding path, denoted with z align , is non-decreasing and guarantees that at a given decoding position t, zt is larger than or equal to all the source positions"
2020.coling-main.443,P07-2045,0,0.00632928,"ation. Following Elbayad et al. (2020), we use unidrectional encoders and train the online MT models with ktrain = 7, proven to yield better translations across the latency spectrum. We train our models on IWSLT’14 De )En (German )English) and En )De (English )German) datasets (Cettolo et al., 2014). Sentences longer than 175 words and pairs with length-ratio exceeding 1.5 are removed. The training set consists of 160K pairs with 7283 held out for development and the test set has 6750 pairs from TED dev2010+tst2010-2013. All data is tokenized using the standard scripts from the Moses toolkit (Koehn et al., 2007). Unlike existing work experimenting with this dataset, we did not lowercase the bitexts so that we can correctly assess typography errors in German. We segment sequences using byte pair encoding (Sennrich et al., 2016), BPE for short, on the bi-texts resulting in a shared vocabulary of 32K types. We train Pervasive Attention (PA) with 14 layers and 7-wide filters and Transformer (TF) small for offline and online translation. We evaluated our wait-k models with keval = 3 achieving a low latency of AL ∈ [2.5, 3.5] (see Table 1). TF models have 2M more parameters compared to PA (19M to 17M), the"
2020.coling-main.443,W04-3250,0,0.302179,"s. The training materials are made publicly available for reuse.2 Annotators were remunerated 220 Euros each. 4 Evaluation results 4.1 Automatic evaluation For each translation direction, En )De and De )En, we assess the quality of our systems by measuring BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007), TER (Snover et al., 2006), ROUGEL (Lin, 2004) and BERTScore (Zhang et al., 2020). We use the default weights and parameters for METEOR3 and we report the F1 measure combining BERTScore precision and recall.4 We test for statistical significance with paired bootstrap resampling (Koehn, 2004) using a sample size of 3000 segments. We report the automatic scores evaluated on IWSLT’14 De↔En test set in Table 1 and Figure 3. For bucketed BLEU scores, we bin the test data based on the lagging difficulty of the pair or the source length and measure corpus-level BLEU in each bin. We observe that (1) In offline translation, TF and PA have a comparable performance on De )En with a slight advantage to TF on all metrics except from BLEU. In the En )De direction, TF widens the gap with PA significantly. When binning En )De by lagging difficulty, PA is outperformed by TF in all ranges of diffi"
2020.coling-main.443,W07-0734,0,0.0651693,"priate error types. In addition, annotators practiced the annotation procedure on a calibration set of 30 segments representative of the full test data but not included in the 200 segments to be annotated. Subsequently, annotators were given individual feedback and corrective guidance on their annotations. The training materials are made publicly available for reuse.2 Annotators were remunerated 220 Euros each. 4 Evaluation results 4.1 Automatic evaluation For each translation direction, En )De and De )En, we assess the quality of our systems by measuring BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007), TER (Snover et al., 2006), ROUGEL (Lin, 2004) and BERTScore (Zhang et al., 2020). We use the default weights and parameters for METEOR3 and we report the F1 measure combining BERTScore precision and recall.4 We test for statistical significance with paired bootstrap resampling (Koehn, 2004) using a sample size of 3000 segments. We report the automatic scores evaluated on IWSLT’14 De↔En test set in Table 1 and Figure 3. For bucketed BLEU scores, we bin the test data based on the lagging difficulty of the pair or the source length and measure corpus-level BLEU in each bin. We observe that (1)"
2020.coling-main.443,W04-1013,0,0.0218951,"otation procedure on a calibration set of 30 segments representative of the full test data but not included in the 200 segments to be annotated. Subsequently, annotators were given individual feedback and corrective guidance on their annotations. The training materials are made publicly available for reuse.2 Annotators were remunerated 220 Euros each. 4 Evaluation results 4.1 Automatic evaluation For each translation direction, En )De and De )En, we assess the quality of our systems by measuring BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007), TER (Snover et al., 2006), ROUGEL (Lin, 2004) and BERTScore (Zhang et al., 2020). We use the default weights and parameters for METEOR3 and we report the F1 measure combining BERTScore precision and recall.4 We test for statistical significance with paired bootstrap resampling (Koehn, 2004) using a sample size of 3000 segments. We report the automatic scores evaluated on IWSLT’14 De↔En test set in Table 1 and Figure 3. For bucketed BLEU scores, we bin the test data based on the lagging difficulty of the pair or the source length and measure corpus-level BLEU in each bin. We observe that (1) In offline translation, TF and PA have a compar"
2020.coling-main.443,2014.eamt-1.38,0,0.028924,"Missing"
2020.coling-main.443,P15-1020,0,0.0119093,"er and Pervasive Attention architectures highlighting the advantages and shortcomings of each when we shift to the online setup. The rest of this paper is organized as follows: we present in §2 related work pertaining to online MT and error analysis of NMT systems. We describe our experimental setup for human evaluation and error analysis in §3. We follow with the evaluation results in §4 and summarize our findings in §5. 2 2.1 Related work Online NMT After pioneering works on online statistical MT (SMT) (Fügen et al., 2007; Yarmohammadi et al., 2013; He et al., 2015; Grissom II et al., 2014; Oda et al., 2015), one of the early works with attention-based This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 5047 Proceedings of the 28th International Conference on Computational Linguistics, pages 5047–5058 Barcelona, Spain (Online), December 8-13, 2020 Source x1 x2 x3 x4 x5 &lt;/s&gt; Source x1 x2 x3 x4 x5 &lt;/s&gt; &lt;s&gt; &lt;s&gt; y1 y1 y2 y2 y3 y3 y4 y4 &lt;/s&gt; &lt;/s&gt; Online (wait-3) Offline (wait-∞) Figure 1: Wait-k decoding as a sequence of reads (horizontal) and writes (vertical) over a source-target grid. After first readin"
2020.coling-main.443,P02-1040,0,0.108525,"n wait-k and greedy decoding strategies, but unlike other wait-k models (Ma et al., 2019; Zheng et al., 2019b; Zheng et al., 2019a) we opt for uni-directional encoders which are efficient to train in an online setup (Elbayad et al., 2020). 2.2 Error analysis for NMT With the advances in NMT (Bahdanau et al., 2015; Vaswani et al., 2017), the quality of translations has improved substantially leading to claims of human parity in high-resource settings (Wu et al., 2016; Hassan et al., 2018). With such improvements, it becomes more and more difficult for automatic evaluation metrics such as BLEU (Papineni et al., 2002) to detect subtle differences. Manual error annotation is a more instructive quality assessment to gain insights into the performance of MT systems, especially in direct comparisons. Human evaluation might lead to conclusions at odd with automatic metrics, as was the case in last year’s WMT English-German evaluation (Barrault et al., 2019). Comparison to SMT and rule-based MT. Bentivogli et al. (2016) studied post-editing of EnglishGerman TED talks and found that NMT makes considerably less word order errors than SMT. They also observed that the performance of NMT degrades faster than SMT with"
2020.coling-main.443,J11-4002,0,0.0387817,"Missing"
2020.coling-main.443,P16-1162,0,0.0336999,"e )En (German )English) and En )De (English )German) datasets (Cettolo et al., 2014). Sentences longer than 175 words and pairs with length-ratio exceeding 1.5 are removed. The training set consists of 160K pairs with 7283 held out for development and the test set has 6750 pairs from TED dev2010+tst2010-2013. All data is tokenized using the standard scripts from the Moses toolkit (Koehn et al., 2007). Unlike existing work experimenting with this dataset, we did not lowercase the bitexts so that we can correctly assess typography errors in German. We segment sequences using byte pair encoding (Sennrich et al., 2016), BPE for short, on the bi-texts resulting in a shared vocabulary of 32K types. We train Pervasive Attention (PA) with 14 layers and 7-wide filters and Transformer (TF) small for offline and online translation. We evaluated our wait-k models with keval = 3 achieving a low latency of AL ∈ [2.5, 3.5] (see Table 1). TF models have 2M more parameters compared to PA (19M to 17M), they are however faster to train (PA is 8 times slower). In test time, the two models decode in comparable speeds. For a fair comparison, both online and offline models are decoded greedily. We will refer these four models"
2020.coling-main.443,2006.amta-papers.25,0,0.117323,", annotators practiced the annotation procedure on a calibration set of 30 segments representative of the full test data but not included in the 200 segments to be annotated. Subsequently, annotators were given individual feedback and corrective guidance on their annotations. The training materials are made publicly available for reuse.2 Annotators were remunerated 220 Euros each. 4 Evaluation results 4.1 Automatic evaluation For each translation direction, En )De and De )En, we assess the quality of our systems by measuring BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007), TER (Snover et al., 2006), ROUGEL (Lin, 2004) and BERTScore (Zhang et al., 2020). We use the default weights and parameters for METEOR3 and we report the F1 measure combining BERTScore precision and recall.4 We test for statistical significance with paired bootstrap resampling (Koehn, 2004) using a sample size of 3000 segments. We report the automatic scores evaluated on IWSLT’14 De↔En test set in Table 1 and Figure 3. For bucketed BLEU scores, we bin the test data based on the lagging difficulty of the pair or the source length and measure corpus-level BLEU in each bin. We observe that (1) In offline translation, TF"
2020.coling-main.443,N13-1023,0,0.022221,"ultaneous policy wait-0, and so, LD measures the lag of a realistic simultaneous translation that has the aligned context available when decoding. The higher 5049 Errors Other Fluency (fl) Duplication (du) Grammar (gr) Typography Unintelligible (ty) (un) Word order (wo) Accuracy (ac) Addition (ad) Mistranslation (mt) Non-existing word form (ne) Omission (om) Overly literal (ol) Figure 2: MQM-based error typology used for our manual annotation. LD, the more challenging it is to constrain the latency of the translation. Relying on alignments to assess a pair’s difficulty is, however, not ideal; Sridhar et al. (2013) experienced poor accuracies in streaming speech translation when segmenting the input based on alignments and Grissom II et al. (2014) argued that the translator can accurately predict future words from a partial context and consequently beat the alignment-induced latency. Relative positions. We look into the correlation between the relative positions, source-side and target˜ has a target-side side, with the translation’s quality. An annotated token y˜t of the system’s hypothesis y relative position t/|y|. Similarly, an annotated source token xj has a relative position j/|x|. We argue that wi"
2020.coling-main.443,stymne-ahrenberg-2012-practice,0,0.025748,"h idiomatic phrases. Castilho et al. (2017a), Castilho et al. (2017b) and Van Brussel et al. (2018) observed that NMT outperforms SMT in terms of fluency, but at the same time it is more prone to accuracy errors. Klubiˇcka et al. (2018) made similar observations in an evaluation of English-Croatian, concluding that compared to SMT and rule-based MT, NMT tends to sacrifice completeness of translations in order to increase fluency. Error typologies for MT. Various error typologies with different levels of granularity have been proposed to evaluate MT systems (Flanagan, 1994; Vilar et al., 2006; Stymne and Ahrenberg, 2012; Lommel et al., 2014b). In their evaluation of SMT outputs, Vilar et al. (2006) defined five error categories: missing words, word order, incorrect words, unknown words and punctuation errors. Bentivogli et al. (2016) followed a simpler classification with three types of errors: morphological, lexical, and word order. Their 5048 choice was motivated by the difficulty to disambiguate sub-categories of lexical errors (Popovi´c and Ney, 2011). The evaluation in Klubiˇcka et al. (2018) is based on the Multidimensional Quality Metrics (MQM) framework (Lommel et al., 2014b). In their study, they fo"
2020.coling-main.443,E17-1100,0,0.237692,". Manual error annotation is a more instructive quality assessment to gain insights into the performance of MT systems, especially in direct comparisons. Human evaluation might lead to conclusions at odd with automatic metrics, as was the case in last year’s WMT English-German evaluation (Barrault et al., 2019). Comparison to SMT and rule-based MT. Bentivogli et al. (2016) studied post-editing of EnglishGerman TED talks and found that NMT makes considerably less word order errors than SMT. They also observed that the performance of NMT degrades faster than SMT with increasing sentence length. Toral and Sánchez-Cartagena (2017) reached similar conclusions on news stories in 9 language directions. Isabelle et al. (2017) tested NMT systems with challenging linguistic material and highlighted the efficiency of NMT systems at handling subject-verb agreement and syntactic and lexico-syntactic divergences and the struggle of NMT with idiomatic phrases. Castilho et al. (2017a), Castilho et al. (2017b) and Van Brussel et al. (2018) observed that NMT outperforms SMT in terms of fluency, but at the same time it is more prone to accuracy errors. Klubiˇcka et al. (2018) made similar observations in an evaluation of English-Croa"
2020.coling-main.443,L18-1600,0,0.0386819,"Missing"
2020.coling-main.443,vilar-etal-2006-error,0,0.39166,"Missing"
2020.coling-main.443,D19-1137,0,0.0128818,"en alternates between blocks of l write/read operations. This simple approach outperforms the information based criteria of Cho and Esipova (2016), and allows complete control of the translation delay. Ma et al. (2019) trained Transformer models (Vaswani et al., 2017) with a wait-k decoding policy that first reads k source tokens then alternate single read-writes. For dynamic online decoding, Luo et al. (2017) and Gu et al. (2017) rely on Reinforcement Learning to optimize a read/write policy. To combine the end-to-end training of wait-k models with the flexibility of dynamic online decoding, Zheng et al. (2019b) and Zheng et al. (2019a) use Imitation Learning. Recent work on dynamic online translation use monotonic alignments (Raffel et al., 2017) with either a limited or infinite lookback (Chiu and Raffel, 2018; Arivazhagan et al., 2019; Ma et al., 2020). Another adjacent research direction enables revision during online translation to alleviate decoding constraints (Niehues et al., 2016; Zheng et al., 2020; Arivazhagan et al., 2020). In this work, we focus on wait-k and greedy decoding strategies, but unlike other wait-k models (Ma et al., 2019; Zheng et al., 2019b; Zheng et al., 2019a) we opt fo"
2020.coling-main.443,P19-1582,0,0.0106143,"en alternates between blocks of l write/read operations. This simple approach outperforms the information based criteria of Cho and Esipova (2016), and allows complete control of the translation delay. Ma et al. (2019) trained Transformer models (Vaswani et al., 2017) with a wait-k decoding policy that first reads k source tokens then alternate single read-writes. For dynamic online decoding, Luo et al. (2017) and Gu et al. (2017) rely on Reinforcement Learning to optimize a read/write policy. To combine the end-to-end training of wait-k models with the flexibility of dynamic online decoding, Zheng et al. (2019b) and Zheng et al. (2019a) use Imitation Learning. Recent work on dynamic online translation use monotonic alignments (Raffel et al., 2017) with either a limited or infinite lookback (Chiu and Raffel, 2018; Arivazhagan et al., 2019; Ma et al., 2020). Another adjacent research direction enables revision during online translation to alleviate decoding constraints (Niehues et al., 2016; Zheng et al., 2020; Arivazhagan et al., 2020). In this work, we focus on wait-k and greedy decoding strategies, but unlike other wait-k models (Ma et al., 2019; Zheng et al., 2019b; Zheng et al., 2019a) we opt fo"
2020.coling-main.443,2020.acl-main.42,0,0.010199,"(2017) and Gu et al. (2017) rely on Reinforcement Learning to optimize a read/write policy. To combine the end-to-end training of wait-k models with the flexibility of dynamic online decoding, Zheng et al. (2019b) and Zheng et al. (2019a) use Imitation Learning. Recent work on dynamic online translation use monotonic alignments (Raffel et al., 2017) with either a limited or infinite lookback (Chiu and Raffel, 2018; Arivazhagan et al., 2019; Ma et al., 2020). Another adjacent research direction enables revision during online translation to alleviate decoding constraints (Niehues et al., 2016; Zheng et al., 2020; Arivazhagan et al., 2020). In this work, we focus on wait-k and greedy decoding strategies, but unlike other wait-k models (Ma et al., 2019; Zheng et al., 2019b; Zheng et al., 2019a) we opt for uni-directional encoders which are efficient to train in an online setup (Elbayad et al., 2020). 2.2 Error analysis for NMT With the advances in NMT (Bahdanau et al., 2015; Vaswani et al., 2017), the quality of translations has improved substantially leading to claims of human parity in high-resource settings (Wu et al., 2016; Hassan et al., 2018). With such improvements, it becomes more and more diff"
2020.conll-1.22,D14-1179,0,0.0264958,"Missing"
2020.conll-1.22,P17-1057,0,0.363548,"ype of boundary is the most efficient. We also explore at which level of the network’s architecture such information should be introduced so as to maximise its performances. Finally, we show that using multiple boundary types at once in a hierarchical structure, by which low-level segments are used to recompose high-level segments, is beneficial and yields better results than using lowlevel or high-level segments in isolation. 1 Introduction and Prior Work Visually Grounded Speech (VGS) models whether CNN-based (Harwath and Glass, 2015; Harwath et al., 2016; Kamper et al., 2017) or RNN-based (Chrupała et al., 2017; Merkx et al., 2019) became recently popular as they enable to model complex interaction between two modalities, namely speech and vision, and can thus be used to model child language acquisition, and more specifically lexical acquisition. Indeed, these models are trained to solve a speech-image retrieval task. That is, given a spoken input description, they are trained to retrieve the image that matches the description the best. This task requires the model to identify lexical units that might be relevant in the spoken input, detect which objects are present in the image, and finally see if"
2020.conll-1.22,K19-1032,1,0.717439,"dual connections, followed by an attention mechanism. We use uni-directional recurrent layers and not bi-directional recurrent layers even though it has been shown they lead to better results (Merkx et al., 2019). Indeed, we aim at having a cognitively plausible model: humans process speech in a left-to-right fashion, as speech is being gradually uttered, and not from both ends simultaneously. We use the same loss function as initially used by Chrupała et al. (2017): employed, all seem to segment individual words from the inputted spoken utterance (Harwath et al., 2016; Chrupała et al., 2017; Havard et al., 2019; Havard et al., 2019; Merkx et al., 2019). This result stands also for languages other than English, such as Hindi or Japanese (Harwath et al., 2018; Havard et al., 2019; Azuh et al., 2019; Ohishi et al., 2020). Chrupała et al. (2017) and Merkx et al. (2019), however, observed that not all layers encode wordlike units, suggesting that some layers specialise in lexical processing whereas some other do not encode such information. Contributions Our research question can be framed as follows: what is the segmentation that maximises the performance of an audio-visual network if speech were to be"
2020.emnlp-main.361,N19-1388,0,0.0941177,"of that specific language direction during training. For that, we build upon the recently proposed adapter layers for NMT (Bapna and Firat, 2019), by using monolingual (language-specific) adapter layers, instead of bilingual (language-pair specific) ones. This design difference improves their compositionality, permitting to combine any encoder adapter with other decoder adapters. Monolingual adapter layers perform as good as bilingual adapter layers in ∗ Work done during an internship at NAVER LABS Europe. Zero-shot translation is direct translation in a language pair unseen during training. Aharoni et al. (2019) analyze the zero-shot performance of MNMT models as a function of the number of language pairs. They observe that having more languages results in better zero-shot performance. However, several artifacts arise, as described by Dabre et al. (2020); Zhang et al. (2020); Aharoni et al. (2019); Arivazhagan et al. (2019), like offtarget translation and insufficient modeling capacity of the MNMT models. Zhang et al. (2020) use language-aware layer normalization and linear transformation to improve some drawbacks of MNMT; they also rely massively on backtranslation to improve zero-shot translation."
2020.emnlp-main.361,D19-1165,0,0.37845,", namely to translate in language pair directions which are unseen at training time (zero-shot setting in this paper). Unfortunately, while performance in the low-resource setting indeed increases significantly, their zero-shot performance remains very low (Johnson et al., 2017). In this paper, we propose a neural architecture that allows to translate from any of the source languages towards any of the target languages seen in the training data, regardless of the presence of that specific language direction during training. For that, we build upon the recently proposed adapter layers for NMT (Bapna and Firat, 2019), by using monolingual (language-specific) adapter layers, instead of bilingual (language-pair specific) ones. This design difference improves their compositionality, permitting to combine any encoder adapter with other decoder adapters. Monolingual adapter layers perform as good as bilingual adapter layers in ∗ Work done during an internship at NAVER LABS Europe. Zero-shot translation is direct translation in a language pair unseen during training. Aharoni et al. (2019) analyze the zero-shot performance of MNMT models as a function of the number of language pairs. They observe that having mor"
2020.emnlp-main.361,W19-5361,1,0.841558,"s. Here, we restrict to the top 20 languages,2 resulting in training corpora ranging between 108k and 214k parallel sentences. We use the dataset as a full multiparallel corpus (data aligned in all directions) and simulate an English-centric setting by using only parallel corpora with English as one of the languages. 4.2 Training Architecture We use the Transformer architecture (Vaswani et al., 2017), implemented in fairseq (Ott et al., 2019), which we modify to include monolingual and bilingual adapters. We train a joint BPE model (Sennrich et al., 2016) on all languages, with inline casing (Berard et al., 2019) and 64k merge operations (resulting in a 70k vocabulary size). The Transformer architecture used in this work3 has 4 attention heads, 6 encoder layers, 6 decoder layers, an embedding size of 512 and a feed-forward dimension of 1024. MNMT Training We train a standard MNMT model following similar settings as Johnson et al. (2017). A single many-to-many model is trained on all the data English-centric data, using a sourceside control token to indicate the target language. This model, which we call “parent”, serves as an initialization for our adapter-enabled models. We use Adam (Kingma and Ba, 2"
2020.emnlp-main.361,D19-1167,0,0.0191102,"(sorted by training size in descending order) Figure 5: Ablation study for the zero-shot setting, where we compare the full model using mono-1024 adapter against its degraded versions with (1) only encoder adapters (2) only decoder adapters. (+0.63), compared to +2.77 when both encoder and decoder adapters are enabled. We have seen that in the adaptation case, using the encoder adapter layers alone leads to a severe drop in performance. This might indicate that – at least during the adaptation – important information is captured in the encoder’s adapter layer (in line with previous reports by Kudugunta et al., 2019) or that the decoder adaptation grows dependent on the encoder adapters, to the point where dropping the latter degrades the system. However, further analysis would be needed to confirm either of these hypotheses. 7 Conclusion 0 10 20 30 10 BLEU adapter-enabled models over the MNMT parent model are shown in Figure 3. A median improvement of +1.26 is observed in the mono-64 setting, while the mono-1024 setting brings a median improvement of +2.77. The smallest difference (over the parent model) observed in each case was -0.14 and +0.30 respectively, indicating near-systematic improvement by usi"
2020.emnlp-main.361,D18-1103,0,0.0408774,"ro-shot performance. However, several artifacts arise, as described by Dabre et al. (2020); Zhang et al. (2020); Aharoni et al. (2019); Arivazhagan et al. (2019), like offtarget translation and insufficient modeling capacity of the MNMT models. Zhang et al. (2020) use language-aware layer normalization and linear transformation to improve some drawbacks of MNMT; they also rely massively on backtranslation to improve zero-shot translation. Adaptation to a new language pair may be addressed by training a multilingual model then fine-tuning it with parallel data in the language pair of interest (Neubig and Hu, 2018; Variš and Bojar, 2019; Stickland et al., 2020). Escolano et al. (2020) propose plug-and-play encoders and decoders per language, which take advantage of a single representation in each language but at the cost of larger model sizes. In order to add only a few trainable parameters per task, adapter modules – initially introduced for computer vision (Rebuffi et al., 2017, 2018) – were proposed for language modeling by Houlsby et al. (2019). Bapna and Firat (2019) used them for parameter-efficient adaptation in MNMT. The parameters of the original MNMT network (the parent model) remain fixed, w"
2020.emnlp-main.361,N19-4009,0,0.0357808,"hese parameter counts include the embeddings. over the test set.1 The TED talks dataset is multiparallel, i.e., each English sentence has translations in multiple languages. Here, we restrict to the top 20 languages,2 resulting in training corpora ranging between 108k and 214k parallel sentences. We use the dataset as a full multiparallel corpus (data aligned in all directions) and simulate an English-centric setting by using only parallel corpora with English as one of the languages. 4.2 Training Architecture We use the Transformer architecture (Vaswani et al., 2017), implemented in fairseq (Ott et al., 2019), which we modify to include monolingual and bilingual adapters. We train a joint BPE model (Sennrich et al., 2016) on all languages, with inline casing (Berard et al., 2019) and 64k merge operations (resulting in a 70k vocabulary size). The Transformer architecture used in this work3 has 4 attention heads, 6 encoder layers, 6 decoder layers, an embedding size of 512 and a feed-forward dimension of 1024. MNMT Training We train a standard MNMT model following similar settings as Johnson et al. (2017). A single many-to-many model is trained on all the data English-centric data, using a sourcesid"
2020.emnlp-main.361,W18-6301,0,0.0421228,"eed-forward dimension of 1024. MNMT Training We train a standard MNMT model following similar settings as Johnson et al. (2017). A single many-to-many model is trained on all the data English-centric data, using a sourceside control token to indicate the target language. This model, which we call “parent”, serves as an initialization for our adapter-enabled models. We use Adam (Kingma and Ba, 2015) with an inverse square root schedule, with 4000 warmup updates and a maximum learning rate of 0.0005. We set the maximum batch size per GPU to 4000 tokens, and train on 4 GPUs with mixed-precision (Ott et al., 2018). We apply dropout with a rate of 0.3, and label smoothing with a rate of 0.1. Like Ari1 Obtained by running multi-bleu.perl, or SacreBLEU with the --tok none option, as the TED talks dataset is pretokenized. 2 en, ar, he, ru, ko, it, ja, zh-cn, es, fr, pt-br, nl, tr, ro, pl, bg, vi, de, fa, hu 3 transformer_iwslt_de_en in fairseq vazhagan et al. (2019), we mitigate the training size imbalance between language pairs by following a temperature-based sampling strategy with T = 5. To ensure all languages are represented adequately in the vocabulary, we use the same temperature-based sampling stra"
2020.emnlp-main.361,2020.emnlp-demos.7,0,0.119117,"Missing"
2020.emnlp-main.361,2020.emnlp-main.617,0,0.0734598,"Missing"
2020.emnlp-main.361,N18-2084,0,0.0786606,"ual (language-specific) adapters only require 2n layers. Table 1 summarizes the amount of parameters needed for adaptation with regular finetuning (FT), bilingual adapters (Bapna and Firat, 2019) and our proposed monolingual adapters. In our setting of 20 languages, fine-tuning would multiply the number of parameters by 380 (20 × 19). As the bottleneck dimension determines the increase of parameters, we experiment with both 64 (used in past work) and 1024, which matches the total number of parameters for bilingual adapters (see Table 2). 4 Experimental Setup 4.1 Datasets We use the TED talks (Qi et al., 2018) in all our experiments, and all the numbers are BLEU scores 4466 type bottleneck increase FT mono. biling. mono. – ×380 64 ×1.47 64 ×4.53 1024 ×3.73 Table 2: Increase in parameters over all tasks (380 language directions). FT denotes fine-tuning. The number of parameters of the parent model in this work is 68M. These parameter counts include the embeddings. over the test set.1 The TED talks dataset is multiparallel, i.e., each English sentence has translations in multiple languages. Here, we restrict to the top 20 languages,2 resulting in training corpora ranging between 108k and 214k paralle"
2020.emnlp-main.361,P16-1162,0,0.0733235,", each English sentence has translations in multiple languages. Here, we restrict to the top 20 languages,2 resulting in training corpora ranging between 108k and 214k parallel sentences. We use the dataset as a full multiparallel corpus (data aligned in all directions) and simulate an English-centric setting by using only parallel corpora with English as one of the languages. 4.2 Training Architecture We use the Transformer architecture (Vaswani et al., 2017), implemented in fairseq (Ott et al., 2019), which we modify to include monolingual and bilingual adapters. We train a joint BPE model (Sennrich et al., 2016) on all languages, with inline casing (Berard et al., 2019) and 64k merge operations (resulting in a 70k vocabulary size). The Transformer architecture used in this work3 has 4 attention heads, 6 encoder layers, 6 decoder layers, an embedding size of 512 and a feed-forward dimension of 1024. MNMT Training We train a standard MNMT model following similar settings as Johnson et al. (2017). A single many-to-many model is trained on all the data English-centric data, using a sourceside control token to indicate the target language. This model, which we call “parent”, serves as an initialization fo"
2020.emnlp-main.361,2020.emnlp-main.180,0,0.0636164,"Missing"
2020.emnlp-main.361,P19-2017,0,0.0363243,"However, several artifacts arise, as described by Dabre et al. (2020); Zhang et al. (2020); Aharoni et al. (2019); Arivazhagan et al. (2019), like offtarget translation and insufficient modeling capacity of the MNMT models. Zhang et al. (2020) use language-aware layer normalization and linear transformation to improve some drawbacks of MNMT; they also rely massively on backtranslation to improve zero-shot translation. Adaptation to a new language pair may be addressed by training a multilingual model then fine-tuning it with parallel data in the language pair of interest (Neubig and Hu, 2018; Variš and Bojar, 2019; Stickland et al., 2020). Escolano et al. (2020) propose plug-and-play encoders and decoders per language, which take advantage of a single representation in each language but at the cost of larger model sizes. In order to add only a few trainable parameters per task, adapter modules – initially introduced for computer vision (Rebuffi et al., 2017, 2018) – were proposed for language modeling by Houlsby et al. (2019). Bapna and Firat (2019) used them for parameter-efficient adaptation in MNMT. The parameters of the original MNMT network (the parent model) remain fixed, which permits a high deg"
2020.emnlp-main.361,2020.acl-main.148,0,0.0243245,"ifference improves their compositionality, permitting to combine any encoder adapter with other decoder adapters. Monolingual adapter layers perform as good as bilingual adapter layers in ∗ Work done during an internship at NAVER LABS Europe. Zero-shot translation is direct translation in a language pair unseen during training. Aharoni et al. (2019) analyze the zero-shot performance of MNMT models as a function of the number of language pairs. They observe that having more languages results in better zero-shot performance. However, several artifacts arise, as described by Dabre et al. (2020); Zhang et al. (2020); Aharoni et al. (2019); Arivazhagan et al. (2019), like offtarget translation and insufficient modeling capacity of the MNMT models. Zhang et al. (2020) use language-aware layer normalization and linear transformation to improve some drawbacks of MNMT; they also rely massively on backtranslation to improve zero-shot translation. Adaptation to a new language pair may be addressed by training a multilingual model then fine-tuning it with parallel data in the language pair of interest (Neubig and Hu, 2018; Variš and Bojar, 2019; Stickland et al., 2020). Escolano et al. (2020) propose plug-and-pl"
2020.iwslt-1.2,P19-1126,0,0.076722,"Missing"
2020.iwslt-1.2,2005.mtsummit-papers.11,0,0.172794,".e. reading the full source before writing the target. We evaluate four wait-k systems each trained with a value of ktrain in {5, 7, 9, ∞} and decoded with keval ranging from 2 to 11. We then ensemble the aforementioned wait-k models and evaluate a multipath model that jointly optimizes a large set of wait-k paths. The results demonstrate that multipath is competetive with wait-k without the need to select which path to optimize (some values of k, e.g. 5, underperform in comparison). Ensembling the wait-k models gives a boost of 1.43 BLEU points on average. and WMT’19 data,6 namely, Europarl (Koehn, 2005), News Commentary (Tiedemann, 2012) and Common Crawl (Smith et al., 2013). We remove pairs with a length-ratio exceeding 1.3 from Common Crawl and pairs exceeding a length-ratio of 1.5 from the rest. We develop on MuST-C dev and report results on MuST-C tst-COMMON. For open-vocabulary translation, we use SentencePiece (Kudo and Richardson, 2018) to segment the bitexts with byte pair encoding (Sennrich et al., 2016). This results in a joint vocabulary of 32K types. Details of the training data are provided in Table 5. We train Transformer big architectures and tie the embeddings of the encoder"
2020.iwslt-1.2,D18-2012,0,0.0233125,"nstrate that multipath is competetive with wait-k without the need to select which path to optimize (some values of k, e.g. 5, underperform in comparison). Ensembling the wait-k models gives a boost of 1.43 BLEU points on average. and WMT’19 data,6 namely, Europarl (Koehn, 2005), News Commentary (Tiedemann, 2012) and Common Crawl (Smith et al., 2013). We remove pairs with a length-ratio exceeding 1.3 from Common Crawl and pairs exceeding a length-ratio of 1.5 from the rest. We develop on MuST-C dev and report results on MuST-C tst-COMMON. For open-vocabulary translation, we use SentencePiece (Kudo and Richardson, 2018) to segment the bitexts with byte pair encoding (Sennrich et al., 2016). This results in a joint vocabulary of 32K types. Details of the training data are provided in Table 5. We train Transformer big architectures and tie the embeddings of the encoder with the decoder’s input and output embeddings. We optimize our models with label-smoothed maximum likelihood (Szegedy et al., 2016) with a smoothing rate  = 0.1. The parameters are updated using 6 27 7 https://github.com/pytorch/fairseq/ blob/simulastsharedtask/examples/ simultaneous_translation http://www.statmt.org/wmt19/ 40 TED-LIUM 3 How2"
2020.iwslt-1.2,N18-2079,0,0.0241677,"eous speech translation track, we build on Transformer-based wait-k models for the text-to-text subtask. For speech-to-text simultaneous translation, we attach a wait-k MT system to a hybrid ASR system. We propose an algorithm to control the latency of the ASR+MT cascade and achieve a good latency-quality trade-off on both subtasks. 1 • IWSLT 2020 offline translation track with end-to-end models for the English-German language pair, • IWSLT 2020 simultaneous translation track with a cascade of an ASR system trained using Kaldi (Povey et al., 2011) and an online MT system with wait-k policies (Dalvi et al., 2018; Ma et al., 2019). Introduction While cascaded speech-to-text translation (AST) systems (combining source language speech recognition (ASR) and source-to-target text translation (MT)) remain state-of-the-art, recent works have attempted to build end-to-end AST with very encouraging results (Bérard et al., 2016; Weiss et al., 2017; Bérard et al., 2018; Jia et al., 2019; Sperber et al., 2019). This year, IWSLT 2020 offline translation track attempts to evaluate if endto-end AST will close the gap with cascaded AST for the English-to-German language pair. Another increasingly popular topic is si"
2020.iwslt-1.2,N19-1202,0,0.0478636,"Missing"
2020.iwslt-1.2,N19-4009,0,0.0278244,"multi-path Ensemble 25 23 21 19 1 3 5 7 9 Average Lagging (AL) in detokenized tokens Figure 2: [Text-to-Text] Latency-quality trade-offs evaluated on MuST-C tst-COMMON with greedy decoding. Offline systems have an AL of 18.55 words. The red vertical bars correspond to the AL evaluation thresholds. Adam (Kingma and Ba, 2015) (β1 , β2 = 0.9, 0.98) with a learning rate that follows an inverse squareroot schedule. We train for a total of 50K updates and evaluate with the check-pointed weights corresponding to the lowest (best) loss on the development set. Our models are implemented with Fairseq (Ott et al., 2019). We generate translation hypotheses with greedy decoding and evaluate the latency-quality trade-off by measuring casesensitive detokenized BLEU (Papineni et al., 2002) and word-level Average Lagging (AL) (Ma et al., 2019). Table 5: Parallel training data for the MT systems. Results. We show in Figure 2 the performance of our systems on the test set (MuST tst-COMMON) measured with the provided evaluation server.7 We denote with ktrain =∞ a unidirectional model trained for wait-until-end decoding i.e. reading the full source before writing the target. We evaluate four wait-k systems each traine"
2020.iwslt-1.2,P02-1040,0,0.106509,"OMMON with greedy decoding. Offline systems have an AL of 18.55 words. The red vertical bars correspond to the AL evaluation thresholds. Adam (Kingma and Ba, 2015) (β1 , β2 = 0.9, 0.98) with a learning rate that follows an inverse squareroot schedule. We train for a total of 50K updates and evaluate with the check-pointed weights corresponding to the lowest (best) loss on the development set. Our models are implemented with Fairseq (Ott et al., 2019). We generate translation hypotheses with greedy decoding and evaluate the latency-quality trade-off by measuring casesensitive detokenized BLEU (Papineni et al., 2002) and word-level Average Lagging (AL) (Ma et al., 2019). Table 5: Parallel training data for the MT systems. Results. We show in Figure 2 the performance of our systems on the test set (MuST tst-COMMON) measured with the provided evaluation server.7 We denote with ktrain =∞ a unidirectional model trained for wait-until-end decoding i.e. reading the full source before writing the target. We evaluate four wait-k systems each trained with a value of ktrain in {5, 7, 9, ∞} and decoded with keval ranging from 2 to 11. We then ensemble the aforementioned wait-k models and evaluate a multipath model t"
2020.iwslt-1.2,P16-1162,0,0.0399967,"t which path to optimize (some values of k, e.g. 5, underperform in comparison). Ensembling the wait-k models gives a boost of 1.43 BLEU points on average. and WMT’19 data,6 namely, Europarl (Koehn, 2005), News Commentary (Tiedemann, 2012) and Common Crawl (Smith et al., 2013). We remove pairs with a length-ratio exceeding 1.3 from Common Crawl and pairs exceeding a length-ratio of 1.5 from the rest. We develop on MuST-C dev and report results on MuST-C tst-COMMON. For open-vocabulary translation, we use SentencePiece (Kudo and Richardson, 2018) to segment the bitexts with byte pair encoding (Sennrich et al., 2016). This results in a joint vocabulary of 32K types. Details of the training data are provided in Table 5. We train Transformer big architectures and tie the embeddings of the encoder with the decoder’s input and output embeddings. We optimize our models with label-smoothed maximum likelihood (Szegedy et al., 2016) with a smoothing rate  = 0.1. The parameters are updated using 6 27 7 https://github.com/pytorch/fairseq/ blob/simulastsharedtask/examples/ simultaneous_translation http://www.statmt.org/wmt19/ 40 TED-LIUM 3 How2 Europarl #hours #words #speakers 452 365 94 5.05M 3.31M 0.75M 2,028 13,"
2020.iwslt-1.2,P13-1135,0,0.0524922,"Missing"
2020.iwslt-1.2,Q19-1020,0,0.0316591,"models for the English-German language pair, • IWSLT 2020 simultaneous translation track with a cascade of an ASR system trained using Kaldi (Povey et al., 2011) and an online MT system with wait-k policies (Dalvi et al., 2018; Ma et al., 2019). Introduction While cascaded speech-to-text translation (AST) systems (combining source language speech recognition (ASR) and source-to-target text translation (MT)) remain state-of-the-art, recent works have attempted to build end-to-end AST with very encouraging results (Bérard et al., 2016; Weiss et al., 2017; Bérard et al., 2018; Jia et al., 2019; Sperber et al., 2019). This year, IWSLT 2020 offline translation track attempts to evaluate if endto-end AST will close the gap with cascaded AST for the English-to-German language pair. Another increasingly popular topic is simultaneous (online) machine translation which consists in generating an output hypothesis before the entire ∗ This paper goes as follows: we review the systems built for the offline speech translation track in §2. Then, we present our approaches to the simultaneous track for both text-to-text and speech-to-text subtasks in §3. We ultimately conclude this work in §4. 2 Offline Speech translat"
2020.iwslt-1.2,tiedemann-2012-parallel,0,0.0284139,"ore writing the target. We evaluate four wait-k systems each trained with a value of ktrain in {5, 7, 9, ∞} and decoded with keval ranging from 2 to 11. We then ensemble the aforementioned wait-k models and evaluate a multipath model that jointly optimizes a large set of wait-k paths. The results demonstrate that multipath is competetive with wait-k without the need to select which path to optimize (some values of k, e.g. 5, underperform in comparison). Ensembling the wait-k models gives a boost of 1.43 BLEU points on average. and WMT’19 data,6 namely, Europarl (Koehn, 2005), News Commentary (Tiedemann, 2012) and Common Crawl (Smith et al., 2013). We remove pairs with a length-ratio exceeding 1.3 from Common Crawl and pairs exceeding a length-ratio of 1.5 from the rest. We develop on MuST-C dev and report results on MuST-C tst-COMMON. For open-vocabulary translation, we use SentencePiece (Kudo and Richardson, 2018) to segment the bitexts with byte pair encoding (Sennrich et al., 2016). This results in a joint vocabulary of 32K types. Details of the training data are provided in Table 5. We train Transformer big architectures and tie the embeddings of the encoder with the decoder’s input and output"
2020.jeptalnrecital-eternal.1,W17-1606,0,0.0623342,"Missing"
2020.jeptalnrecital-eternal.1,D18-1334,0,0.0642525,"Missing"
2020.jeptalnrecital-jep.28,couillault-etal-2014-evaluating,0,0.0650173,"Missing"
2020.jeptalnrecital-jep.28,P16-2096,0,0.0571186,"Missing"
2020.jeptalnrecital-jep.28,korvas-etal-2014-free,0,0.0704691,"Missing"
2020.jeptalnrecital-jep.28,D18-1334,0,0.0622398,"Missing"
2020.jeptalnrecital-taln.26,D19-1572,0,0.0323304,"Missing"
2020.jeptalnrecital-taln.26,P18-1031,0,0.0541897,"Missing"
2020.jeptalnrecital-taln.26,P19-1340,0,0.0367789,"Missing"
2020.jeptalnrecital-taln.26,P18-1249,0,0.0395933,"Missing"
2020.jeptalnrecital-taln.26,P07-2045,0,0.0108736,"Missing"
2020.jeptalnrecital-taln.26,P10-1023,0,0.119362,"Missing"
2020.jeptalnrecital-taln.26,D14-1162,0,0.0892081,"Missing"
2020.jeptalnrecital-taln.26,N18-1202,0,0.0990203,"Missing"
2020.jeptalnrecital-taln.26,P10-1114,0,0.0829177,"Missing"
2020.jeptalnrecital-taln.26,D17-1039,0,0.0479498,"Missing"
2020.jeptalnrecital-taln.26,W13-4917,0,0.0975746,"112 500 paires de phrases annotées avec les étiquettes entailment, contradiction ou neutre. FLUE intègre la partie française de ce corpus. Analyse syntaxique et étiquetage morphosyntaxique Nous considérons deux tâches d’analyse syntaxique : analyse en constituants et en dépendances, ainsi que l’étiquetage morphosyntaxique. Pour cela, nous utilisons le French Treebank (Abeillé et al., 2003), une collection de phrases du Monde annotées manuellement en constituants et dépendances syntaxiques. Nous utilisons la version de ce corpus de la campagne d’évaluation SPMRL 2014 décrite par Seddah et al. (2013), qui contient 14759, 1235 et 2541 phrases pour respectivement l’entraînement, le développement et l’évaluation. 271 Désambiguïsation lexicale des verbes et des noms La désambiguïsation lexicale consiste à assigner un sens, parmi un inventaire donné, à des mots d’une phrase. Pour la désambiguïsation lexicale de verbes, nous utilisons les données de FrenchSemEval (Segonne et al., 2019). Il s’agit d’un corpus d’évaluation dont les occurrences de verbes ont été annotées manuellement avec les sens de Wiktionary. 10 Pour la désambiguïsation lexicale des noms, nous utilisons la partie française de l"
2020.jeptalnrecital-taln.26,W19-0422,1,0.888597,"Missing"
2020.jeptalnrecital-taln.26,P16-1162,0,0.146858,"Missing"
2020.jeptalnrecital-taln.26,tiedemann-2012-parallel,0,0.0825572,"Missing"
2020.jeptalnrecital-taln.26,2019.gwc-1.14,1,0.893471,"Missing"
2020.jeptalnrecital-taln.26,W18-5446,0,0.0474061,"Missing"
2020.jeptalnrecital-taln.26,N18-1101,0,0.0610743,"Missing"
2020.jeptalnrecital-taln.26,D19-1382,0,0.0395373,"Missing"
2020.jeptalnrecital-taln.26,N19-1131,0,0.0313441,"Missing"
2020.lrec-1.302,2020.osact-1.2,0,0.0214996,"QuAD (Rajpurkar et al., 2018), surpassing previous methods by a large margin. 2.2. Pre-trained Language Models Beyond English Given the impact of pre-trained language models on NLP downstream tasks in English, several works have recently released pre-trained models for other languages. For instance, ELMo exists for Portuguese, Japanese, German and Basque,5 while BERT and variants were specifically trained for simplified and traditional Chinese8 and German.6 A Portuguese version of MultiFiT is also available.7 Recently, more monolingual BERT-based models have been released, such as for Arabic (Antoun et al., 2020), Dutch (de Vries et al., 2019; Delobelle et al., 2020), Finnish (Virtanen et al., 2019), Italian (Polignano et al., 2019), Portuguese (Souza et al., 2019), Russian (Kuratov and Arkhipov, 2019), Spanish (Ca˜nete et al., 2020), and Vietnamese (Nguyen and Nguyen, 2020). For French, besides pre-trained language models using ULMFiT and MultiFiT configurations,7 CamemBERT (Martin et al., 2019) is a French BERT model concurrent to our work. Another trend considers one model estimated for several languages with a shared vocabulary. The release of multilingual BERT for 104 languages pioneered this app"
2020.lrec-1.302,Q19-1038,0,0.0214599,"19), Portuguese (Souza et al., 2019), Russian (Kuratov and Arkhipov, 2019), Spanish (Ca˜nete et al., 2020), and Vietnamese (Nguyen and Nguyen, 2020). For French, besides pre-trained language models using ULMFiT and MultiFiT configurations,7 CamemBERT (Martin et al., 2019) is a French BERT model concurrent to our work. Another trend considers one model estimated for several languages with a shared vocabulary. The release of multilingual BERT for 104 languages pioneered this approach.8 A recent extension of this work leverages parallel data to build a cross-lingual pre-trained version of LASER (Artetxe and Schwenk, 2019) for 93 languages, XLM (Lample and Conneau, 2019) and XLM-R (Conneau et al., 2019) for 100 languages. 2.3. Evaluation Protocol for French NLP Tasks The existence of a multi-task evaluation benchmark such as GLUE (Wang et al., 2018) for English is highly beneficial to facilitate research in the language of interest. The GLUE benchmark has become a prominent framework to evaluate the performance of NLP models in English. The recent contributions based on pre-trained language models have led to remarkable performance across a wide range of Natural Language Understanding (NLU) tasks. The authors o"
2020.lrec-1.302,W06-1615,0,0.0880109,"2 1 985 XNLI-FR Diverse genres 392 702 2 490 5 010 French Treebank Daily newspaper 14 759 1 235 2 541 FrenchSemEval Diverse genres 55 206 - 3 199 Noun Sense Disambiguation Diverse genres 818 262 - 1 445 Table 2: Descriptions of the datasets included in our FLUE benchmark. 4.1. Text Classification CLS The Cross Lingual Sentiment CLS (Prettenhofer and Stein, 2010) dataset consists of Amazon reviews for three product categories: books, DVD, and music in four languages: English, French, German, and Japanese. Each sample contains a review text and the associated rating from 1 to 5 stars. Following Blitzer et al. (2006) and Prettenhofer and Stein (2010), ratings with 3 stars are removed. Positive reviews have ratings higher than 3 and negative reviews are those rated lower than 3. There is one train and test set for each product category. The train and test sets are balanced, including around 1 000 positive and 1 000 negative reviews for a total of 2 000 reviews in each dataset. We take the French portion to create the binary text classification task in FLUE and report the accuracy on the test set. 4.2. Paraphrasing PAWS-X The Cross-lingual Adversarial Dataset for Paraphrase Identification PAWS-X (Yang et al"
2020.lrec-1.302,P17-1152,0,0.0617991,"Missing"
2020.lrec-1.302,D18-1269,0,0.0239485,"gement. The paraphrasing task is to identify whether the sentences in these pairs are semantically equivalent or not. Similar to previous approaches to create multilingual corpora, Yang et al. (2019a) used machine translation to create the training set for each target language in PAWS-X from the English training set in PAWS. The development and test sets for each language are translated by human translators. We take the related datasets for French to perform the paraphrasing task and report the accuracy on the test set. 4.3. Natural Language Inference XNLI The Cross-lingual NLI (XNLI) corpus (Conneau et al., 2018) extends the development and test sets of the Multi-Genre Natural Language Inference corpus (Williams et al., 2018, MultiNLI) to 15 languages. The development and test sets for each language consist of 7 500 humanannotated examples, making up a total of 112 500 sentence pairs annotated with the labels entailment, contradiction, or neutral. Each sentence pair includes a premise (p) and a hypothesis (h). The Natural Language Inference (NLI) task, also known as recognizing textual entailment (RTE), is to determine whether p entails, contradicts or neither entails nor contradicts h. We take the Fr"
2020.lrec-1.302,P19-4007,0,0.0544476,"Missing"
2020.lrec-1.302,W13-4905,0,0.054327,"shared task organizers. Our word representations are a concatenation of word embeddings and tag embeddings learned together with the model parameters on the French Treebank data itself, and at most one of (fastText, CamemBERT, FlauBERTBASE , FlauBERTBASE , mBERT) word vector. As Dozat and Manning (2016), we use word and tag dropout (d = 0.5) on word and tag embeddings but without dropout on BERT representations. We performed a fairly comprehensive grid search on hyperparameters for each model tested. Results The results are reported in Table 7. The best published results in this shared task (Constant et al., 2013) were involving an ensemble of parsers with additional resources for modelling multi word expressions (MWE), typical of the French treebank annotations. The monolingual French BERT models (CamemBERT, FlauBERT) perform better and set the new state of the art on this dataset with a single parser and without specific modelling for MWEs. One can observe that both FlauBERT models perform marginally better than CamemBERT, while all of them outperform mBERT by a large margin. 2484 Model UAS LAS Best published (Constant et al., 2013) 89.19 85.86 No pre-training fastText pre-training mBERT CamemBERT Fl"
2020.lrec-1.302,2020.findings-emnlp.292,0,0.0289963,"ethods by a large margin. 2.2. Pre-trained Language Models Beyond English Given the impact of pre-trained language models on NLP downstream tasks in English, several works have recently released pre-trained models for other languages. For instance, ELMo exists for Portuguese, Japanese, German and Basque,5 while BERT and variants were specifically trained for simplified and traditional Chinese8 and German.6 A Portuguese version of MultiFiT is also available.7 Recently, more monolingual BERT-based models have been released, such as for Arabic (Antoun et al., 2020), Dutch (de Vries et al., 2019; Delobelle et al., 2020), Finnish (Virtanen et al., 2019), Italian (Polignano et al., 2019), Portuguese (Souza et al., 2019), Russian (Kuratov and Arkhipov, 2019), Spanish (Ca˜nete et al., 2020), and Vietnamese (Nguyen and Nguyen, 2020). For French, besides pre-trained language models using ULMFiT and MultiFiT configurations,7 CamemBERT (Martin et al., 2019) is a French BERT model concurrent to our work. Another trend considers one model estimated for several languages with a shared vocabulary. The release of multilingual BERT for 104 languages pioneered this approach.8 A recent extension of this work leverages paral"
2020.lrec-1.302,N19-1423,0,0.620298,"is-diderot.fr, alexandre.allauzen@espci.fr Abstract Language models have become a key step to achieve state-of-the art results in many different Natural Language Processing (NLP) tasks. Leveraging the huge amount of unlabeled texts nowadays available, they provide an efficient way to pre-train continuous word representations that can be fine-tuned for a downstream task, along with their contextualization at the sentence level. This has been widely demonstrated for English using contextualized representations (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019b). In this paper, we introduce and share FlauBERT, a model learned on a very large and heterogeneous French corpus. Models of different sizes are trained using the new CNRS (French National Centre for Scientific Research) Jean Zay supercomputer. We apply our French language models to diverse NLP tasks (text classification, paraphrasing, natural language inference, parsing, word sense disambiguation) and show that most of the time they outperform other pre-training approaches. Different versions of FlauBERT as well as a unified evaluation protocol for the downstream tasks, c"
2020.lrec-1.302,eisele-chen-2010-multiun,0,0.103871,"Missing"
2020.lrec-1.302,D19-1572,0,0.0171919,"architecture, while ELMo adopts a bidirectional LSTM to build the final embedding for each input token from the concatenation of the left-to-right and rightto-left representations. Another fundamental difference lies in how each model can be tuned to different downstream tasks: ELMo delivers different word vectors that can be interpolated, whereas ULMFiT enables robust fine-tuning of the whole network w.r.t. the downstream tasks. The ability of fine-tuning was shown to significantly boost the performance, and thus this approach has been further developed in the recent works such as MultiFiT (Eisenschlos et al., 2019) or most prominently Transformer-based (Vaswani et al., 2017) architectures: GPT (Radford et al., 2018), BERT (Devlin et al., 2019), XLNet (Yang et al., 2019b), XLM (Lample and Conneau, 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2019), T5 (Raffel et al., 2019). These methods have one after the other established new state-ofthe-art results on various NLP benchmarks, such as GLUE (Wang et al., 2018) or SQuAD (Rajpurkar et al., 2018), surpassing previous methods by a large margin. 2.2. Pre-trained Language Models Beyond English Given the impact of pre-trained language models on NLP do"
2020.lrec-1.302,P18-1031,0,0.289639,"incent.segonne@etu, bcrabbe@linguist}.univ-paris-diderot.fr, alexandre.allauzen@espci.fr Abstract Language models have become a key step to achieve state-of-the art results in many different Natural Language Processing (NLP) tasks. Leveraging the huge amount of unlabeled texts nowadays available, they provide an efficient way to pre-train continuous word representations that can be fine-tuned for a downstream task, along with their contextualization at the sentence level. This has been widely demonstrated for English using contextualized representations (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019b). In this paper, we introduce and share FlauBERT, a model learned on a very large and heterogeneous French corpus. Models of different sizes are trained using the new CNRS (French National Centre for Scientific Research) Jean Zay supercomputer. We apply our French language models to diverse NLP tasks (text classification, paraphrasing, natural language inference, parsing, word sense disambiguation) and show that most of the time they outperform other pre-training approaches. Different versions of FlauBERT as well as a unified eval"
2020.lrec-1.302,P18-1249,0,0.141589,"Table 5. The results confirm the superiority of the French models compared to the multilingual model mBERT on this task. FlauBERTLARGE performs moderately better than CamemBERT. Both of them clearly outperform XLM-RBASE , while cannot surpass XLM-RLARGE . Model XLM-RLARGE † XLM-RBASE † mBERT‡ CamemBERT ‡ FlauBERTBASE FlauBERTLARGE † ‡ Accuracy 85.2 80.1 76.9 81.2 80.6 83.4 Results reported in (Conneau et al., 2019). Results reported in (Martin et al., 2019). Table 5: Results on the French XNLI dataset. 5.4. Constituency Parsing and POS Tagging Model description We use the parser described by Kitaev and Klein (2018) and Kitaev et al. (2019). It is an openly available19 chart parser based on a self-attentive encoder. We compare (i) a model without any pre-trained parameters, (ii) a model that additionally uses and fine-tunes fastText20 pre-trained embeddings, (iii) models based on pre-trained language models: mBERT, CamemBERT, and FlauBERT. We use the default hyperparameters from Kitaev and Klein (2018) for the first two settings and the hyperparameters from Kitaev et al. (2019) when using pretrained language models, except for FlauBERTLARGE . For this last model, we use a different learning rate (0.00001"
2020.lrec-1.302,P19-1340,0,0.0189308,"m the superiority of the French models compared to the multilingual model mBERT on this task. FlauBERTLARGE performs moderately better than CamemBERT. Both of them clearly outperform XLM-RBASE , while cannot surpass XLM-RLARGE . Model XLM-RLARGE † XLM-RBASE † mBERT‡ CamemBERT ‡ FlauBERTBASE FlauBERTLARGE † ‡ Accuracy 85.2 80.1 76.9 81.2 80.6 83.4 Results reported in (Conneau et al., 2019). Results reported in (Martin et al., 2019). Table 5: Results on the French XNLI dataset. 5.4. Constituency Parsing and POS Tagging Model description We use the parser described by Kitaev and Klein (2018) and Kitaev et al. (2019). It is an openly available19 chart parser based on a self-attentive encoder. We compare (i) a model without any pre-trained parameters, (ii) a model that additionally uses and fine-tunes fastText20 pre-trained embeddings, (iii) models based on pre-trained language models: mBERT, CamemBERT, and FlauBERT. We use the default hyperparameters from Kitaev and Klein (2018) for the first two settings and the hyperparameters from Kitaev et al. (2019) when using pretrained language models, except for FlauBERTLARGE . For this last model, we use a different learning rate (0.00001), batch size (8) and ign"
2020.lrec-1.302,P07-2045,0,0.0134554,"to extract the text or download them directly from their websites. The total size of the uncompressed text before preprocessing is 270 GB. More details can be found in Appendix A.1. Data preprocessing For all sub-corpora, we filtered out very short sentences as well as repetitive and nonmeaningful content such as telephone/fax numbers, email addresses, etc. For Common Crawl, which is our largest sub-corpus with 215 GB of raw text, we applied aggressive cleaning to reduce its size to 43.4 GB. All the data were Unicode-normalized in a consistent way before being tokenized using Moses tokenizer (Koehn et al., 2007). The resulting training corpus is 71 GB in size. Our code for downloading and preprocessing data is made publicly available.13 3.2. Models and Training Configurations Model architecture FlauBERT has the same model architecture as BERT (Devlin et al., 2019), which consists of a multi-layer bidirectional Transformer (Vaswani et al., 2017). Following Devlin et al. (2019), we propose two model sizes: • FlauBERTBASE : L = 12, H = 768, A = 12, • FlauBERTLARGE : L = 24, H = 1024, A = 16, where L, H and A respectively denote the number of Transformer blocks, the hidden size, and the number of selfatt"
2020.lrec-1.302,2005.mtsummit-papers.11,0,0.015061,"Missing"
2020.lrec-1.302,W19-5303,0,0.226301,"ding FlauBERT In this section, we describe the training corpus, the text preprocessing pipeline, the model architecture and training configurations to build FlauBERTBASE and FlauBERTLARGE . 3.1. Training Data Data collection Our French text corpus consists of 24 sub-corpora gathered from different sources, covering diverse topics and writing styles, ranging from formal and well-written text (e.g. Wikipedia and books)10 to random text crawled from the Internet (e.g. Common Crawl).11 The data were collected from three main sources: (1) monolingual data for French provided in WMT19 shared tasks (Li et al., 2019, 4 sub-corpora); (2) French text corpora offered in the OPUS collection (Tiedemann, 2012, 8 sub-corpora); and (3) datasets available in the Wikimedia projects (Meta, 2019, 8 sub-corpora). We used the WikiExtractor tool12 to extract the text from Wikipedia. For the other sub-corpora, we either used our 7 https://github.com/piegu/language-models https://github.com/google-research/bert 9 https://github.com/chineseGLUE/chineseGLUE 10 http://www.gutenberg.org 11 http://data.statmt.org/ngrams/deduped2017 12 https://github.com/attardi/wikiextractor 8 4 It should be noted that learning contextual emb"
2020.lrec-1.302,L16-1147,0,0.0701355,"Missing"
2020.lrec-1.302,H93-1061,0,0.0518853,"rt of the Multilingual WSD task of SemEval 2013 (Navigli et al., 2013), which targets nouns only. We adapted the task to use the WordNet 3.0 sense inventory (Miller, 1995) instead of BabelNet (Navigli and Ponzetto, 2010), by converting the sense keys to WordNet 3.0 if a mapping exists in BabelNet, and removing them otherwise. The result of the conversion process is an evaluation corpus composed of 306 sentences and 1 445 French nouns annotated with WordNet sense keys, and manually verified. For the training data, we followed the method proposed by Hadj Salah (2018), and translated the SemCor (Miller et al., 1993) and the WordNet Gloss Corpus16 into French, using the best English-French Machine Translation system of the fairseq toolkit17 (Ott et al., 2019). Finally, we aligned the WordNet sense annotation from the source English words to the the translated French words, using the alignment provided by the MT system. We rely on WordNet sense keys instead of the original BabelNet annotations for the following two reasons. First, WordNet is a resource that is entirely manually verified, and widely used in WSD research (Navigli, 2009). Second, there is already a large quantity of sense annotated data based"
2020.lrec-1.302,P10-1023,0,0.0572014,"(04-20-2018) openly available via Dbnary (S´erasset, 2012). For a given sense of a target key, the sense inventory offers a definition along with one or more examples. For this task, we considered the examples of the sense inventory as training examples and tested our model on the evaluation dataset. Noun Sense Disambiguation We propose a new challenging task for the WSD of French, based on the French part of the Multilingual WSD task of SemEval 2013 (Navigli et al., 2013), which targets nouns only. We adapted the task to use the WordNet 3.0 sense inventory (Miller, 1995) instead of BabelNet (Navigli and Ponzetto, 2010), by converting the sense keys to WordNet 3.0 if a mapping exists in BabelNet, and removing them otherwise. The result of the conversion process is an evaluation corpus composed of 306 sentences and 1 445 French nouns annotated with WordNet sense keys, and manually verified. For the training data, we followed the method proposed by Hadj Salah (2018), and translated the SemCor (Miller et al., 1993) and the WordNet Gloss Corpus16 into French, using the best English-French Machine Translation system of the fairseq toolkit17 (Ott et al., 2019). Finally, we aligned the WordNet sense annotation from"
2020.lrec-1.302,S13-2040,0,0.0327036,"ated version to French provided in XNLI. Following Conneau et al. (2018), we report the test accuracy. 4.4. Parsing and Part-of-Speech Tagging Syntactic parsing consists in assigning a tree structure to a sentence in natural language. We perform parsing on the French Treebank (Abeill´e et al., 2003), a collection of sentences extracted from French daily newspaper Le Monde, and manually annotated with both constituency and dependency syntactic trees and part-of-speech tags. Specifically, we use the version of the corpus instantiated for the SPMRL 2013 shared task and described by Seddah et al. (2013). This version is provided with a standard split representing 14 759 sentences for the training corpus, and respectively 1 235 and 2 541 sentences for the development and evaluation sets. 4.5. Word Sense Disambiguation Tasks Word Sense Disambiguation (WSD) is a classification task which aims to predict the sense of words in a given context according to a specific sense inventory. We used two French WSD tasks: the FrenchSemEval task (Segonne et al., 2019), which targets verbs only, and a modified version of the French part of the Multilingual WSD task of SemEval 2013 (Navigli et al., 2013), whi"
2020.lrec-1.302,2020.findings-emnlp.92,0,0.0305509,"ls for other languages. For instance, ELMo exists for Portuguese, Japanese, German and Basque,5 while BERT and variants were specifically trained for simplified and traditional Chinese8 and German.6 A Portuguese version of MultiFiT is also available.7 Recently, more monolingual BERT-based models have been released, such as for Arabic (Antoun et al., 2020), Dutch (de Vries et al., 2019; Delobelle et al., 2020), Finnish (Virtanen et al., 2019), Italian (Polignano et al., 2019), Portuguese (Souza et al., 2019), Russian (Kuratov and Arkhipov, 2019), Spanish (Ca˜nete et al., 2020), and Vietnamese (Nguyen and Nguyen, 2020). For French, besides pre-trained language models using ULMFiT and MultiFiT configurations,7 CamemBERT (Martin et al., 2019) is a French BERT model concurrent to our work. Another trend considers one model estimated for several languages with a shared vocabulary. The release of multilingual BERT for 104 languages pioneered this approach.8 A recent extension of this work leverages parallel data to build a cross-lingual pre-trained version of LASER (Artetxe and Schwenk, 2019) for 93 languages, XLM (Lample and Conneau, 2019) and XLM-R (Conneau et al., 2019) for 100 languages. 2.3. Evaluation Prot"
2020.lrec-1.302,N19-4009,0,0.0205028,"nventory (Miller, 1995) instead of BabelNet (Navigli and Ponzetto, 2010), by converting the sense keys to WordNet 3.0 if a mapping exists in BabelNet, and removing them otherwise. The result of the conversion process is an evaluation corpus composed of 306 sentences and 1 445 French nouns annotated with WordNet sense keys, and manually verified. For the training data, we followed the method proposed by Hadj Salah (2018), and translated the SemCor (Miller et al., 1993) and the WordNet Gloss Corpus16 into French, using the best English-French Machine Translation system of the fairseq toolkit17 (Ott et al., 2019). Finally, we aligned the WordNet sense annotation from the source English words to the the translated French words, using the alignment provided by the MT system. We rely on WordNet sense keys instead of the original BabelNet annotations for the following two reasons. First, WordNet is a resource that is entirely manually verified, and widely used in WSD research (Navigli, 2009). Second, there is already a large quantity of sense annotated data based on the sense inventory of WordNet (Vial et al., 2018) that we can use for the training of our system. We publicly release18 both our training da"
2020.lrec-1.302,D14-1162,0,0.0923659,"ding Evaluation), are shared to the research community for further reproducible experiments in French NLP. Keywords: FlauBERT, FLUE, BERT, Transformer, French, language model, pre-training, NLP benchmark, text classification, parsing, word sense disambiguation, natural language inference, paraphrase. 1. Introduction A recent game-changing contribution in Natural Language Processing (NLP) was the introduction of deep unsupervised language representations pre-trained using only plain text corpora. Previous word embedding pre-training approaches, such as word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014), learn a single vector for each wordform. By contrast, these new models are trained to produce contextual embeddings: the output representation depends on the entire input sequence (e.g. each token instance has a vector representation that depends on its left and right context). Initially based on recurrent neural networks (Dai and Le, 2015; Ramachandran et al., 2017; Howard and Ruder, 2018; Peters et al., 2018), these models quickly converged towards the use of the Transformer (Vaswani et al., 2017), such as GPT (Radford et al., 2018), BERT (Devlin et al., 2019), XLNet (Yang et al., 2019b),"
2020.lrec-1.302,N18-1202,0,0.806323,"-grenoble-alpes.fr {vincent.segonne@etu, bcrabbe@linguist}.univ-paris-diderot.fr, alexandre.allauzen@espci.fr Abstract Language models have become a key step to achieve state-of-the art results in many different Natural Language Processing (NLP) tasks. Leveraging the huge amount of unlabeled texts nowadays available, they provide an efficient way to pre-train continuous word representations that can be fine-tuned for a downstream task, along with their contextualization at the sentence level. This has been widely demonstrated for English using contextualized representations (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019b). In this paper, we introduce and share FlauBERT, a model learned on a very large and heterogeneous French corpus. Models of different sizes are trained using the new CNRS (French National Centre for Scientific Research) Jean Zay supercomputer. We apply our French language models to diverse NLP tasks (text classification, paraphrasing, natural language inference, parsing, word sense disambiguation) and show that most of the time they outperform other pre-training approaches. Different versions of FlauBERT a"
2020.lrec-1.302,P10-1114,0,0.0398144,"Missing"
2020.lrec-1.302,P18-2124,0,0.0233078,"f fine-tuning was shown to significantly boost the performance, and thus this approach has been further developed in the recent works such as MultiFiT (Eisenschlos et al., 2019) or most prominently Transformer-based (Vaswani et al., 2017) architectures: GPT (Radford et al., 2018), BERT (Devlin et al., 2019), XLNet (Yang et al., 2019b), XLM (Lample and Conneau, 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2019), T5 (Raffel et al., 2019). These methods have one after the other established new state-ofthe-art results on various NLP benchmarks, such as GLUE (Wang et al., 2018) or SQuAD (Rajpurkar et al., 2018), surpassing previous methods by a large margin. 2.2. Pre-trained Language Models Beyond English Given the impact of pre-trained language models on NLP downstream tasks in English, several works have recently released pre-trained models for other languages. For instance, ELMo exists for Portuguese, Japanese, German and Basque,5 while BERT and variants were specifically trained for simplified and traditional Chinese8 and German.6 A Portuguese version of MultiFiT is also available.7 Recently, more monolingual BERT-based models have been released, such as for Arabic (Antoun et al., 2020), Dutch ("
2020.lrec-1.302,D17-1039,0,0.213586,"uage Processing (NLP) was the introduction of deep unsupervised language representations pre-trained using only plain text corpora. Previous word embedding pre-training approaches, such as word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014), learn a single vector for each wordform. By contrast, these new models are trained to produce contextual embeddings: the output representation depends on the entire input sequence (e.g. each token instance has a vector representation that depends on its left and right context). Initially based on recurrent neural networks (Dai and Le, 2015; Ramachandran et al., 2017; Howard and Ruder, 2018; Peters et al., 2018), these models quickly converged towards the use of the Transformer (Vaswani et al., 2017), such as GPT (Radford et al., 2018), BERT (Devlin et al., 2019), XLNet (Yang et al., 2019b), RoBERTa (Liu et al., 2019). Using these pre-trained models in a transfer learning fashion has shown to yield striking improvements across a wide range of NLP tasks. One can easily build state-of-the-art NLP systems thanks to the publicly available pre-trained weights, saving time, energy, and resources. As a consequence, unsupervised language model pre-training has be"
2020.lrec-1.302,W13-4917,0,0.0460081,"Missing"
2020.lrec-1.302,W19-0422,1,0.895578,"Missing"
2020.lrec-1.302,P16-1162,0,0.0200296,"(attention) layers at each training step. Other techniques are also available such as progressive training (Gong et al., 2019), or improving initialization (Zhang et al., 2019a; Xu et al., 2019) and normalization (Nguyen and Salazar, 2019). For training FlauBERTLARGE , we employed pre-norm attention and stochastic depths for their simplicity. We found that these two techniques were sufficient for successful training. We set the rate of layer dropping to 0.2 in all the experiments. Other training details A vocabulary of 50K sub-word units is built using the Byte Pair Encoding (BPE) algorithm (Sennrich et al., 2016). The only difference between our work and RoBERTa is that the training data are preprocessed and tokenized using a basic tokenizer for French (Koehn et al., 2007, Moses), as in XLM (Lample and Conneau, 2019), before the application of BPE. We use fastBPE,15 a very efficient implementation to extract the BPE units and encode the corpora. 15 2481 https://github.com/glample/fastBPE FlauBERTBASE is trained on 32 GPUs Nvidia V100 in 410 hours and FlauBERTLARGE is trained on 128 GPUs in 390 hours, both with the effective batch size of 8192 sequences. Finally, we summarize the differences between Fl"
2020.lrec-1.302,serasset-2012-dbnary,0,0.036519,"Missing"
2020.lrec-1.302,skadins-etal-2014-billions,0,0.0456176,"Missing"
2020.lrec-1.302,tiedemann-2012-parallel,0,0.180846,"peline, the model architecture and training configurations to build FlauBERTBASE and FlauBERTLARGE . 3.1. Training Data Data collection Our French text corpus consists of 24 sub-corpora gathered from different sources, covering diverse topics and writing styles, ranging from formal and well-written text (e.g. Wikipedia and books)10 to random text crawled from the Internet (e.g. Common Crawl).11 The data were collected from three main sources: (1) monolingual data for French provided in WMT19 shared tasks (Li et al., 2019, 4 sub-corpora); (2) French text corpora offered in the OPUS collection (Tiedemann, 2012, 8 sub-corpora); and (3) datasets available in the Wikimedia projects (Meta, 2019, 8 sub-corpora). We used the WikiExtractor tool12 to extract the text from Wikipedia. For the other sub-corpora, we either used our 7 https://github.com/piegu/language-models https://github.com/google-research/bert 9 https://github.com/chineseGLUE/chineseGLUE 10 http://www.gutenberg.org 11 http://data.statmt.org/ngrams/deduped2017 12 https://github.com/attardi/wikiextractor 8 4 It should be noted that learning contextual embeddings was also proposed in (McCann et al., 2017), but in a supervised fashion as they u"
2020.lrec-1.302,W18-1819,0,0.025239,"RTLARGE : warmup steps of 30k, peak learning rate of 3e−4, β1 = 0.9, β2 = 0.98,  = 1e−6 and weight decay of 0.01. Training FlauBERTLARGE Training very deep Transformers is known to be susceptible to instability (Wang et al., 2019b; Nguyen and Salazar, 2019; Xu et al., 2019; Fan et al., 2019). Not surprisingly, we also observed this difficulty when training FlauBERTLARGE using the same configurations as BERTLARGE and RoBERTaLARGE , where divergence happened at an early stage. Several methods have been proposed to tackle this issue. For example, in an updated implementation of the Transformer (Vaswani et al., 2018), layer normalization is applied before each attention layer by default, rather than after each residual block as in the original implementation (Vaswani et al., 2017). These configurations are called pre-norm and post-norm, respectively. It was observed by Vaswani et al. (2018), and again confirmed by later works e.g. (Wang et al., 2019b; Xu et al., 2019; Nguyen and Salazar, 2019), that pre-norm helps stabilize training. Recently, a regularization technique called stochastic depths (Huang et al., 2016) has been demonstrated to be very effective for training deep Transformers, by e.g. Pham et"
2020.lrec-1.302,L18-1166,1,0.830909,"French, using the best English-French Machine Translation system of the fairseq toolkit17 (Ott et al., 2019). Finally, we aligned the WordNet sense annotation from the source English words to the the translated French words, using the alignment provided by the MT system. We rely on WordNet sense keys instead of the original BabelNet annotations for the following two reasons. First, WordNet is a resource that is entirely manually verified, and widely used in WSD research (Navigli, 2009). Second, there is already a large quantity of sense annotated data based on the sense inventory of WordNet (Vial et al., 2018) that we can use for the training of our system. We publicly release18 both our training data and the evaluation data in the UFSAC format (Vial et al., 2018). 5. Experiments and Results In this section, we present FlauBERT fine-tuning results on the FLUE benchmark. We compare the performance of FlauBERT with Multilingual BERT (Devlin et al., 2019, mBERT) and CamemBERT (Martin et al., 2019) on all tasks. In addition, for each task we also include the best non-BERT model for comparison. We made use of the open source libraries (Lample and Conneau, 2019, XLM) and (Wolf et al., 2019, Transformers)"
2020.lrec-1.302,2019.gwc-1.14,1,0.892947,"Missing"
2020.lrec-1.302,W18-5446,0,0.470157,"escribe our methodology to build FlauBERT – French Language Understanding via Bidirectional Encoder Representations from Transformers, a French BERT1 model that outperforms multilingual/cross-lingual models in several downstream NLP tasks, under similar configurations. FlauBERT relies on freely available datasets and is made publicly available in different versions.2 For further reproducible experiments, we also provide the complete processing and training pipeline as well as a general benchmark for evaluating French NLP systems. This evaluation setup is similar to the popular GLUE benchmark (Wang et al., 2018), and is named FLUE (French Language Understanding Evaluation). 2. 2.1. Related Work Pre-trained Language Models Self-supervised3 pre-training on unlabeled text data was first proposed in the task of neural language modeling (Bengio et al., 2003; Collobert and Weston, 2008), where it was shown that a neural network trained to predict next word from prior words can learn useful embedding representations, called word embeddings (each word is represented by a fixed vector). These representations were shown to play an important role in NLP, yielding state-of-the-art performance on multiple tasks ("
2020.lrec-1.302,P19-1176,0,0.104394,"2019) and XLM-R (Conneau et al., 2019) for 100 languages. 2.3. Evaluation Protocol for French NLP Tasks The existence of a multi-task evaluation benchmark such as GLUE (Wang et al., 2018) for English is highly beneficial to facilitate research in the language of interest. The GLUE benchmark has become a prominent framework to evaluate the performance of NLP models in English. The recent contributions based on pre-trained language models have led to remarkable performance across a wide range of Natural Language Understanding (NLU) tasks. The authors of GLUE have therefore introduced SuperGLUE (Wang et al., 2019a): a new benchmark built on the principles of GLUE, including more challenging and diverse set of tasks. A Chinese version of GLUE9 is also developed to evaluate model performance in Chinese NLP tasks. As of now, we have not learned of any such benchmark for French. 3. Building FlauBERT In this section, we describe the training corpus, the text preprocessing pipeline, the model architecture and training configurations to build FlauBERTBASE and FlauBERTLARGE . 3.1. Training Data Data collection Our French text corpus consists of 24 sub-corpora gathered from different sources, covering diverse"
2020.lrec-1.302,N18-1101,0,0.0161936,"ot. Similar to previous approaches to create multilingual corpora, Yang et al. (2019a) used machine translation to create the training set for each target language in PAWS-X from the English training set in PAWS. The development and test sets for each language are translated by human translators. We take the related datasets for French to perform the paraphrasing task and report the accuracy on the test set. 4.3. Natural Language Inference XNLI The Cross-lingual NLI (XNLI) corpus (Conneau et al., 2018) extends the development and test sets of the Multi-Genre Natural Language Inference corpus (Williams et al., 2018, MultiNLI) to 15 languages. The development and test sets for each language consist of 7 500 humanannotated examples, making up a total of 112 500 sentence pairs annotated with the labels entailment, contradiction, or neutral. Each sentence pair includes a premise (p) and a hypothesis (h). The Natural Language Inference (NLI) task, also known as recognizing textual entailment (RTE), is to determine whether p entails, contradicts or neither entails nor contradicts h. We take the French part of the XNLI corpus to form the development and test sets for the NLI task in FLUE. The train set is obta"
2020.lrec-1.302,D19-1382,0,0.0391611,"Missing"
2020.lrec-1.302,N19-1131,0,0.100143,"firmed by later works e.g. (Wang et al., 2019b; Xu et al., 2019; Nguyen and Salazar, 2019), that pre-norm helps stabilize training. Recently, a regularization technique called stochastic depths (Huang et al., 2016) has been demonstrated to be very effective for training deep Transformers, by e.g. Pham et al. (2019) and Fan et al. (2019) who successfully trained architectures of more than 40 layers. The idea is to randomly drop a number of (attention) layers at each training step. Other techniques are also available such as progressive training (Gong et al., 2019), or improving initialization (Zhang et al., 2019a; Xu et al., 2019) and normalization (Nguyen and Salazar, 2019). For training FlauBERTLARGE , we employed pre-norm attention and stochastic depths for their simplicity. We found that these two techniques were sufficient for successful training. We set the rate of layer dropping to 0.2 in all the experiments. Other training details A vocabulary of 50K sub-word units is built using the Byte Pair Encoding (BPE) algorithm (Sennrich et al., 2016). The only difference between our work and RoBERTa is that the training data are preprocessed and tokenized using a basic tokenizer for French (Koehn et a"
2020.lrec-1.799,N19-1009,0,0.0177927,"re also proposed by using weakly supervised data (Jia et al., 2019a), or by adding a second attention mechanism (Sperber et al., 2019). 6486 2.2. Multilingual Approaches Multilingual approaches for speech and language processing are growing ever more popular. They are made possible by the availability of massively parallel language resources covering an increasing number of languages of the world. These resources feed truly multilingual approaches, such as machine translation (Aharoni et al., 2019), syntax parsing (Nivre et al., 2016), automatic speech recognition (Schultz and Schlippe, 2014; Adams et al., 2019), lexical disambiguation (Navigli and Ponzetto, 2010; S´erasset, 2015), and computational dialectology (Christodoulopoulos and Steedman, 2015). 2.3. A Large and Clean Subset of Sentence Aligned Spoken Utterances (MaSS) In this section we present the source material for our multilingual corpus (Section 3.1.), we briefly explain the CMU speech-to-text pipeline (Section 3.2.), and we detail our speech-to-speech pipeline (Section 3.3.). 3.1. 3.2. Corpora for End-to-end Speech Translation To date, few datasets are available for multilingual automatic speech translation (only a few parallel corpora"
2020.lrec-1.799,N19-1388,0,0.025286,"2018), end-to-end speech-to-speech translation (Translatotron) (Jia et al., 2019b). Improvements for end-toend AST were also proposed by using weakly supervised data (Jia et al., 2019a), or by adding a second attention mechanism (Sperber et al., 2019). 6486 2.2. Multilingual Approaches Multilingual approaches for speech and language processing are growing ever more popular. They are made possible by the availability of massively parallel language resources covering an increasing number of languages of the world. These resources feed truly multilingual approaches, such as machine translation (Aharoni et al., 2019), syntax parsing (Nivre et al., 2016), automatic speech recognition (Schultz and Schlippe, 2014; Adams et al., 2019), lexical disambiguation (Navigli and Ponzetto, 2010; S´erasset, 2015), and computational dialectology (Christodoulopoulos and Steedman, 2015). 2.3. A Large and Clean Subset of Sentence Aligned Spoken Utterances (MaSS) In this section we present the source material for our multilingual corpus (Section 3.1.), we briefly explain the CMU speech-to-text pipeline (Section 3.2.), and we detail our speech-to-speech pipeline (Section 3.3.). 3.1. 3.2. Corpora for End-to-end Speech Transla"
2020.lrec-1.799,E17-2076,0,0.020924,"vox.org/cmu_ wilderness/index.html 2 Our definition of a comparable corpus in this work is the following: a non-sentence-aligned corpus, parallel at a broader granularity (e.g. chapter, document). 3 Available at https://github.com/getalp/ mass-dataset We believe the obtained corpus can be useful in several applications, such as speech-to-speech retrieval (Lee et al., 2015), multilingual speech representation learning (Harwath et al., 2018a) and direct speech-to-speech translation (so far, mostly direct speech-to-text translation has been investigated (B´erard et al., 2016; Weiss et al., 2017; Bansal et al., 2017; B´erard et al., 2018)). Moreover, typological and dialectal fields could use such a corpus to solve some of the following novel tasks using parallel speech: word alignment, bilingual lexicon extraction, and semantic retrieval. This paper is organized as follows: after briefly presenting related works in Section 2, we review the dataset source and extraction pipeline in Section 3. Section 4 describes the human verification performed and comments on some of the linguistic features present in the covered languages. Section 5 presents a possible application of the dataset: speechto-speech retrie"
2020.lrec-1.799,N19-1006,0,0.022834,"Section 6 concludes this work. 2. 2.1. Related Work End-to-end Speech Translation Previous Automatic Speech-to-Text Translation (AST) systems operate in two steps: source language Automatic Speech Recognition (ASR) and source-to-target text Machine Translation (MT). However, recent works have attempted to build end-to-end AST without using source language transcription during learning or decoding (B´erard et al., 2016; Weiss et al., 2017), or by using it at training time only (B´erard et al., 2018). Very recently several extensions of these pioneering works were introduced: low-resource AST (Bansal et al., 2019), unsupervised AST (Chung et al., 2018), end-to-end speech-to-speech translation (Translatotron) (Jia et al., 2019b). Improvements for end-toend AST were also proposed by using weakly supervised data (Jia et al., 2019a), or by adding a second attention mechanism (Sperber et al., 2019). 6486 2.2. Multilingual Approaches Multilingual approaches for speech and language processing are growing ever more popular. They are made possible by the availability of massively parallel language resources covering an increasing number of languages of the world. These resources feed truly multilingual approach"
2020.lrec-1.799,P17-1057,0,0.0294386,"amming window size of 25ms and stride of 10ms) extracted from raw speech. The network is trained to minimize the contrastive loss function in Equation 1, which minimizes the cosine distance d between a verse in a given language A, and its corresponding verse in a given language B. It does so by maximizing the distance between mismatching verses pairs (with a given margin α). Thus, verses corresponding to direct translations should lie close in the embedding space. Finally, contrary to Harwath et al. (2018a), in which only one negative example for caption is sampled, we adopted the method from Chrupała et al. (2017), considering every other verse in the batch as a negative example. X X vA ,vB 0 vA L(vA , vB , α) = 0 max[0, α + d(vA , vB ) − d(vA , vB )] ! + X max[0, α + d(vA , vB ) − 0 d(vA , vB )] 0 vB (1) 5. Use Case: Multilingual Speech Retrieval Task Baseline In this section we showcase the usefulness of our corpus on a multilingual setting. We perform speech-tospeech retrieval by adapting a model for visually grounded speech (Harwath et al., 2018b), and we discuss the results for our baseline model. word to indicate its grammatical function (eg. subject, object, etc.) within a clause/sentence. 11 Co"
2020.lrec-1.799,N19-1202,0,0.0855015,"Missing"
2020.lrec-1.799,L18-1001,1,0.830685,"Missing"
2020.lrec-1.799,E17-2002,0,0.0265034,"is baseline, medium ranks are better than chance level (e r = 408) but vary from re = 136 (EN-FR) to re = 219 (EN-FI), which is very poor compared to our baseline model. Interestingly, our best results, obtained for EN-EU (e r = 9) and EN-ES (e r = 12), illustrate that speech-to-speech retrieval task is feasible even for pairs of typologically different languages. Following this experiment, we investigated the correlation between the median rank and two variables: the quality of the alignment (human evaluation) and the syntactic distance between the language pairs (using the lang2vec library (Littell et al., 2017)). Results are provided at Table 5. While there is no correlation between the rank and the syntactic distance, there is a strong negative correlation with respect to the human evaluation (significant for p &lt; 0.1). One possible explanation for this result may be that higher quality alignments (measured by the human evaluation x e) lead to a slightly easier corpus for the speech-speech retrieval task (difficulty being measured by the rank re). If confirmed, this result would suggest that speech-to-speech retrieval scores are a good proxy for rating alignment corpus quality, as performed for text"
2020.lrec-1.799,P10-1023,0,0.0588949,"ata (Jia et al., 2019a), or by adding a second attention mechanism (Sperber et al., 2019). 6486 2.2. Multilingual Approaches Multilingual approaches for speech and language processing are growing ever more popular. They are made possible by the availability of massively parallel language resources covering an increasing number of languages of the world. These resources feed truly multilingual approaches, such as machine translation (Aharoni et al., 2019), syntax parsing (Nivre et al., 2016), automatic speech recognition (Schultz and Schlippe, 2014; Adams et al., 2019), lexical disambiguation (Navigli and Ponzetto, 2010; S´erasset, 2015), and computational dialectology (Christodoulopoulos and Steedman, 2015). 2.3. A Large and Clean Subset of Sentence Aligned Spoken Utterances (MaSS) In this section we present the source material for our multilingual corpus (Section 3.1.), we briefly explain the CMU speech-to-text pipeline (Section 3.2.), and we detail our speech-to-speech pipeline (Section 3.3.). 3.1. 3.2. Corpora for End-to-end Speech Translation To date, few datasets are available for multilingual automatic speech translation (only a few parallel corpora publicly available4 ). For instance, Fisher and Call"
2020.lrec-1.799,L16-1262,0,0.0901372,"Missing"
2020.lrec-1.799,2013.iwslt-papers.14,0,0.0303652,"ational dialectology (Christodoulopoulos and Steedman, 2015). 2.3. A Large and Clean Subset of Sentence Aligned Spoken Utterances (MaSS) In this section we present the source material for our multilingual corpus (Section 3.1.), we briefly explain the CMU speech-to-text pipeline (Section 3.2.), and we detail our speech-to-speech pipeline (Section 3.3.). 3.1. 3.2. Corpora for End-to-end Speech Translation To date, few datasets are available for multilingual automatic speech translation (only a few parallel corpora publicly available4 ). For instance, Fisher and Callhome Spanish-English corpora (Post et al., 2013) provide 38 hours of speech transcriptions of telephonic conversations aligned with their translations. However, these corpora are only medium size and contain low-bandwidth recordings. Microsoft Speech Language Translation (MSLT) corpus (Federmann and Lewis, 2016) also provides speech aligned to translated text, but this corpus is rather small (less than 8 hours per language). A 236 hours extension of Librispeech with French translations was proposed by Kocabiyiko˘glu et al. (2018). They exploited automatic alignment procedures, first at the text level (between transcriptions and translations"
2020.lrec-1.799,schultz-schlippe-2014-globalphone,0,0.0215478,"vements for end-toend AST were also proposed by using weakly supervised data (Jia et al., 2019a), or by adding a second attention mechanism (Sperber et al., 2019). 6486 2.2. Multilingual Approaches Multilingual approaches for speech and language processing are growing ever more popular. They are made possible by the availability of massively parallel language resources covering an increasing number of languages of the world. These resources feed truly multilingual approaches, such as machine translation (Aharoni et al., 2019), syntax parsing (Nivre et al., 2016), automatic speech recognition (Schultz and Schlippe, 2014; Adams et al., 2019), lexical disambiguation (Navigli and Ponzetto, 2010; S´erasset, 2015), and computational dialectology (Christodoulopoulos and Steedman, 2015). 2.3. A Large and Clean Subset of Sentence Aligned Spoken Utterances (MaSS) In this section we present the source material for our multilingual corpus (Section 3.1.), we briefly explain the CMU speech-to-text pipeline (Section 3.2.), and we detail our speech-to-speech pipeline (Section 3.3.). 3.1. 3.2. Corpora for End-to-end Speech Translation To date, few datasets are available for multilingual automatic speech translation (only a"
2020.lrec-1.799,Q19-1020,0,0.0113164,"works have attempted to build end-to-end AST without using source language transcription during learning or decoding (B´erard et al., 2016; Weiss et al., 2017), or by using it at training time only (B´erard et al., 2018). Very recently several extensions of these pioneering works were introduced: low-resource AST (Bansal et al., 2019), unsupervised AST (Chung et al., 2018), end-to-end speech-to-speech translation (Translatotron) (Jia et al., 2019b). Improvements for end-toend AST were also proposed by using weakly supervised data (Jia et al., 2019a), or by adding a second attention mechanism (Sperber et al., 2019). 6486 2.2. Multilingual Approaches Multilingual approaches for speech and language processing are growing ever more popular. They are made possible by the availability of massively parallel language resources covering an increasing number of languages of the world. These resources feed truly multilingual approaches, such as machine translation (Aharoni et al., 2019), syntax parsing (Nivre et al., 2016), automatic speech recognition (Schultz and Schlippe, 2014; Adams et al., 2019), lexical disambiguation (Navigli and Ponzetto, 2010; S´erasset, 2015), and computational dialectology (Christodoul"
2020.lrec-1.813,couillault-etal-2014-evaluating,0,0.0246842,"s from 2010 to 2019. vious work. Wilkinson et al. (2016) developed principles for scientific data management and stewardship, the FAIR Data Principles, based on four foundational data characteristics that are Findability, Accessibility, Interoperability and Reusability (Wilkinson et al., 2016). In our case, findability and accessibility are taken into account by design, resources on OpenSLR being freely accessible. Interoperability and Reusability of data are however not yet achieved. Another attempt to integrate this discussion about data description within the NLP community has been made by Couillault et al. (2014), who proposed an Ethics and Big Data Charter, to help resources creators describe data from a legal and ethical point of view. Hovy and Spruit (2016) highlighted the different social implications of NLP systems, such as exclusion, overgeneralisation and exposure problems. More recently, work by Bender and Friedman (2018) proposed the notion of data statement to ensure data transparency. The common point of all these studies is that information is key. The FAIR Principles are a baseline to guarantee the reproducibility of scientific findings. We need data to be described exhaustively in order"
2020.lrec-1.813,P16-2096,0,0.141327,"based on four foundational data characteristics that are Findability, Accessibility, Interoperability and Reusability (Wilkinson et al., 2016). In our case, findability and accessibility are taken into account by design, resources on OpenSLR being freely accessible. Interoperability and Reusability of data are however not yet achieved. Another attempt to integrate this discussion about data description within the NLP community has been made by Couillault et al. (2014), who proposed an Ethics and Big Data Charter, to help resources creators describe data from a legal and ethical point of view. Hovy and Spruit (2016) highlighted the different social implications of NLP systems, such as exclusion, overgeneralisation and exposure problems. More recently, work by Bender and Friedman (2018) proposed the notion of data statement to ensure data transparency. The common point of all these studies is that information is key. The FAIR Principles are a baseline to guarantee the reproducibility of scientific findings. We need data to be described exhaustively in order to acknowledge demographic bias that may exist within our corpora. As pointed out by Hovy and Spruit (2016), language is always situated and so are la"
2020.lrec-1.813,D18-1334,0,0.32379,"licit requirements about data structures, hence attesting of what metadata resources creators consider important to share when releasing resources for free on the Web. 3. Methodology In order to study gender representation within speech resources, let us start by defining what gender is. In this work, we consider gender as a binary category (male and female speakers). Nevertheless, we are aware that gender as an identity also exists outside of these two categories, but we did not find any mention of non-binary speakers within the corpora surveyed in our study. Following work by Doukhan et al. (2018), we wanted to explore the corpora looking at the number of speakers of each gender category as well as their speech duration, considering both variables as good features to account for gender representation. After the download, we manually extracted information about gender representation in each corpus. 3.1. Speaker Information and Lack of Meta-Data The first difficulty we came across was the general absence of information. As gender in technology is a relatively recent research interest, most of the time gender demographics are not made available by the resources creators. So, on top of the"
2020.lrec-1.813,korvas-etal-2014-free,0,0.076341,"Missing"
2020.sltu-1.11,N16-1109,0,0.0691211,"Missing"
2020.sltu-1.11,P06-1085,0,0.0751927,"can be used for word segmentation and morphological analysis, being known as very robust in low-resource settings (Godard et al., 2016; Goldwater et al., 2009a). In these monolingual models, words are generated by a uni or bigram model over a non-finite inventory, through the use of a Dirichlet process. Although providing reliable segmentation in low-resource settings, these monolingual models are incapable of automatically producing alignments with a foreign language, and therefore the discovered pseudoword segments can be seen as “meaningless”. Godard et al. (2018) also showed that dpseg2 (Goldwater et al., 2006; Goldwater et al., 2009a) behaves poorly on pseudophone units discovered from speech, which limits its application. Here, we investigate its use as an intermediate monolingual-rooted segmentation system, whose discovered boundaries are used as clues by bilingual models. 3. Experimental Settings Multilingual Dataset: For our experiments we use the MaSS dataset (Boito et al., 2020), a fully aligned and multilingual dataset containing 8,130 sentences extracted 2 Available at http://homepages.inf.ed.ac.uk/ sgwater/resources.html 80 Figure 1: An illustration of the hybrid pipeline for the EN&gt;RO la"
2020.sltu-1.11,N09-1036,0,0.0449523,"the dpseg output boundaries. In this augmented input representation, illustrated in Figure 1, a boundary is denoted by a special token which separates the words identified by dpseg. We call this soft-boundary insertion, since the dpseg boundaries inserted into the phoneme sequence can be ignored by the NMT model, and new boundaries can be inserted as well. For instance, in Figure 1 aintrat becomes a intrat (boundary insertion), and urat debine becomes uratdebine (soft-boundary removal). Models for Word Segmentation Monolingual Bayesian Approach Non-parametric Bayesian models (Goldwater, 2007; Johnson and Goldwater, 2009) are statistical approaches that can be used for word segmentation and morphological analysis, being known as very robust in low-resource settings (Godard et al., 2016; Goldwater et al., 2009a). In these monolingual models, words are generated by a uni or bigram model over a non-finite inventory, through the use of a Dirichlet process. Although providing reliable segmentation in low-resource settings, these monolingual models are incapable of automatically producing alignments with a foreign language, and therefore the discovered pseudoword segments can be seen as “meaningless”. Godard et al."
2020.sltu-1.11,W10-2912,0,0.0352998,"Missing"
2020.sltu-1.11,2020.lrec-1.799,1,0.824529,"Missing"
2020.sltu-1.11,P02-1040,0,0.10942,"Missing"
2020.sltu-1.11,strunk-etal-2014-untrained,0,0.0772016,"Missing"
2021.acl-short.103,N19-1006,0,0.0243369,"be used to combine available pre-trained models to perform a multilingual ST task. In particular, we initialize the encoder using a pre-trained ASR encoder (on MuST-C)3 provided by Wang et al. (2020) and the decoder using mBART50, a multilingual denoising auto-encoder pre-trained on 50 languages (Tang et al., 2020). We tune language independent crossattention and language-specific adapters on top of these backbone models (using MuST-C as well). The results presented in Table 4 highlight that fine3 Pre-training on ASR data and then transferring to ST is not new but rather standard. See, e.g., Bansal et al. (2019). tuning cross-attention is crucial to transfer to multilingual ST (rows 3 and 5 show poor results without doing so). Adding adapters to the backbone decoder (row 4) or to both encoder and decoder (row 6) further boosts performance, demonstrating the ability of adapters to connect off-the-shelf models in a modular fashion. The best-performing model in this recipe (row 6) also outperforms bilingual systems (row 1) despite having fewer trainable parameters (190M vs. 248M). It is also important to mention that while we experiment on 8 target languages of MuST-C corpus, the multilingual ST model o"
2021.acl-short.103,D19-1165,0,0.397779,"ff between the aforementioned factors (and thus more generally between versatility and specialization) has often to be made, and depending on the application, one can be favored more than the other. One way to move along the spectrum between multilingual and bilingual models is to use adapter tuning which consists in freezing pre-trained parameters of a multilingual model and injecting lightweight modules between layers resulting in the addition of a small number of language-specific trainable parameters. While adapter tuning was investigated for multilingual neural machine translation (NMT) (Bapna and Firat, 2019), to our knowledge, this paper proposes the first comprehensive analysis of adapters for multilingual speech translation. Our contributions are the following: (1) we show that both versatility and specialization can be achieved by tuning language-specific adapter modules on top of a multilingual system. Bilingual models with higher accuracy than the original multilingual model are obtained, yet keeping a low maintenance complexity; (2) starting from a different initialization point, we show that adapters can also be 817 Proceedings of the 59th Annual Meeting of the Association for Computationa"
2021.acl-short.103,N19-1202,0,0.0418746,"Missing"
2021.acl-short.103,2020.acl-demos.34,0,0.0185407,"etails, together with extensive experimental results, in Section 5 and 6. In the next section, we present our experimental setup. 4 Adapters for Speech Translation In this section, we describe the integration of adapters into a given backbone model for speech translation. As the Transformer (Vaswani et al., 2017) has become increasingly common in speech 1 https://github.com/formiel/fairseq/tree/master/ examples/speech to text/docs/adapters.md 818 4.1 Experimental Setup Dataset MuST-C We evaluate our recipes on MuSTC (Di Gangi et al., 2019), a large-scale one-to-many 2 For speech applications (Inaguma et al., 2020; Wang et al., 2020), the embedding layer of the encoder is often a small convolutional neural network (Fukushima and Miyake, 1982; LeCun et al., 1989). Dict D d Adapter ENC DEC Finetune ENC DEC # params (M) trainable/total de es fr it nl pt ro ru Training data (hours) 408 504 492 465 442 385 432 489 avg mono multi - - - - - 8×31.1/8×31.1 32.1/32.1 22.16 22.37 30.42 30.40 27.92 27.49 22.92 22.79 24.10 24.42 27.19 27.32 21.51 20.78 14.36 14.54 23.82 23.76 3 4 multi multi 64 64 X X X - - 8×0.2/33.7 8×0.6/36.9 22.32 22.75 30.50 31.07 27.55 28.03 22.91 23.04 24.51 24.75 27.36 28.06 21.09 21.20 14."
2021.acl-short.103,2020.aacl-demo.6,1,0.922282,"uppose that our adapter layer is represented by a function g. The new “adapted” output is then given by: used as a glue to connect off-the-shelf systems (an automatic speech recognition (ASR) model and a multilingual denoising auto-encoder mBART (Liu et al., 2020; Tang et al., 2020)) to perform the multilingual ST task. Extensive experiments on the MuST-C dataset (Di Gangi et al., 2019) show that adapter-based fine-tuning can achieve very competitive results to full fine-tuning—while being much more parameter-efficient—in both standard and low-resource settings. Our code based on FAIRSEQ S2T (Wang et al., 2020) is publicly available.1 2 Related Work Adapter layers (or adapters for short) were first proposed in computer vision (Rebuffi et al., 2017), then explored for text classification tasks in NLP (Houlsby et al., 2019). Adapters are generally inserted between the layers of a pre-trained network and finetuned on the adaptation corpus. Bapna and Firat (2019) studied adapters in the context of NMT and evaluated them on two tasks: domain adaptation and massively multilingual NMT. Philip et al. (2020) later introduced monolingual adapters for zero-shot NMT. Other research groups made contributions on"
2021.codi-main.16,2020.autosimtrans-1.5,0,0.0149965,"language may prefer a certain discourse relation over the others that may not be the same as that of the original language. For example, in the case of Korean, which is an agglutinative language, the heavy usage of Korean postpositions (particles) dictates that the causal relation in the above example is revealed explicitly (regardless of the usage of explicit discourse connectives). Such a cross-lingual discourse analysis can provide some insights for improving the quality of (machine) translation (Meyer and Webber, 2013; Meyer and Poláková, 2013; Guzmán et al., 2014; Iruskieta et al., 2015; Chen et al., 2020). This paper presents an interactive system that visualizes the cross-lingual discourse relations using two human-annotated multilingual discourse datasets (Zeyrek et al., 2019; Long et al., 2020) derived from TED talks following the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) framework. Our interactive data dashboard provides users with an overview of relation preservation among all language pairs. We also display a graph network depicting the cross-lingual discourse relations between a pair of languages for a given TED talk, and provide a search function to find sentences with speci"
2021.codi-main.16,P14-1065,0,0.0320662,"language, the grammar and pragmatics of that language may prefer a certain discourse relation over the others that may not be the same as that of the original language. For example, in the case of Korean, which is an agglutinative language, the heavy usage of Korean postpositions (particles) dictates that the causal relation in the above example is revealed explicitly (regardless of the usage of explicit discourse connectives). Such a cross-lingual discourse analysis can provide some insights for improving the quality of (machine) translation (Meyer and Webber, 2013; Meyer and Poláková, 2013; Guzmán et al., 2014; Iruskieta et al., 2015; Chen et al., 2020). This paper presents an interactive system that visualizes the cross-lingual discourse relations using two human-annotated multilingual discourse datasets (Zeyrek et al., 2019; Long et al., 2020) derived from TED talks following the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) framework. Our interactive data dashboard provides users with an overview of relation preservation among all language pairs. We also display a graph network depicting the cross-lingual discourse relations between a pair of languages for a given TED talk, and provide a"
2021.codi-main.16,D19-1410,0,0.0640496,"Missing"
2021.codi-main.16,2020.emnlp-main.365,0,0.0233691,"Missing"
2021.codi-main.16,2020.lrec-1.129,0,0.0109067,"the heavy usage of Korean postpositions (particles) dictates that the causal relation in the above example is revealed explicitly (regardless of the usage of explicit discourse connectives). Such a cross-lingual discourse analysis can provide some insights for improving the quality of (machine) translation (Meyer and Webber, 2013; Meyer and Poláková, 2013; Guzmán et al., 2014; Iruskieta et al., 2015; Chen et al., 2020). This paper presents an interactive system that visualizes the cross-lingual discourse relations using two human-annotated multilingual discourse datasets (Zeyrek et al., 2019; Long et al., 2020) derived from TED talks following the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) framework. Our interactive data dashboard provides users with an overview of relation preservation among all language pairs. We also display a graph network depicting the cross-lingual discourse relations between a pair of languages for a given TED talk, and provide a search function to find sentences with specific query or relation types. We believe that, with this visualization tool, users can easily browse through the multilingual talks and discover which parts of the talks share similar discourse rel"
2021.codi-main.16,W13-3306,0,0.0214045,"se sentences into another language, the grammar and pragmatics of that language may prefer a certain discourse relation over the others that may not be the same as that of the original language. For example, in the case of Korean, which is an agglutinative language, the heavy usage of Korean postpositions (particles) dictates that the causal relation in the above example is revealed explicitly (regardless of the usage of explicit discourse connectives). Such a cross-lingual discourse analysis can provide some insights for improving the quality of (machine) translation (Meyer and Webber, 2013; Meyer and Poláková, 2013; Guzmán et al., 2014; Iruskieta et al., 2015; Chen et al., 2020). This paper presents an interactive system that visualizes the cross-lingual discourse relations using two human-annotated multilingual discourse datasets (Zeyrek et al., 2019; Long et al., 2020) derived from TED talks following the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) framework. Our interactive data dashboard provides users with an overview of relation preservation among all language pairs. We also display a graph network depicting the cross-lingual discourse relations between a pair of languages for a given TED"
2021.codi-main.16,W13-3303,0,0.0174348,"w, when we translate these sentences into another language, the grammar and pragmatics of that language may prefer a certain discourse relation over the others that may not be the same as that of the original language. For example, in the case of Korean, which is an agglutinative language, the heavy usage of Korean postpositions (particles) dictates that the causal relation in the above example is revealed explicitly (regardless of the usage of explicit discourse connectives). Such a cross-lingual discourse analysis can provide some insights for improving the quality of (machine) translation (Meyer and Webber, 2013; Meyer and Poláková, 2013; Guzmán et al., 2014; Iruskieta et al., 2015; Chen et al., 2020). This paper presents an interactive system that visualizes the cross-lingual discourse relations using two human-annotated multilingual discourse datasets (Zeyrek et al., 2019; Long et al., 2020) derived from TED talks following the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) framework. Our interactive data dashboard provides users with an overview of relation preservation among all language pairs. We also display a graph network depicting the cross-lingual discourse relations between a pair of"
2021.codi-main.16,W15-3049,0,0.0204122,"Missing"
2021.codi-main.16,prasad-etal-2008-penn,0,0.0969379,"the above example is revealed explicitly (regardless of the usage of explicit discourse connectives). Such a cross-lingual discourse analysis can provide some insights for improving the quality of (machine) translation (Meyer and Webber, 2013; Meyer and Poláková, 2013; Guzmán et al., 2014; Iruskieta et al., 2015; Chen et al., 2020). This paper presents an interactive system that visualizes the cross-lingual discourse relations using two human-annotated multilingual discourse datasets (Zeyrek et al., 2019; Long et al., 2020) derived from TED talks following the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) framework. Our interactive data dashboard provides users with an overview of relation preservation among all language pairs. We also display a graph network depicting the cross-lingual discourse relations between a pair of languages for a given TED talk, and provide a search function to find sentences with specific query or relation types. We believe that, with this visualization tool, users can easily browse through the multilingual talks and discover which parts of the talks share similar discourse relations, and where they diverge. 2 Preprocessing Datasets We abbreviate the two discourse-a"
2021.computel-1.7,2020.acl-main.740,0,0.0490797,"Missing"
2021.computel-1.7,2020.emnlp-demos.6,0,0.0616605,"Missing"
2021.computel-1.7,2020.sltu-1.43,1,0.81392,"and line toolkit was released called Persephone. To assess the reproducibility of the results on other languages, experiments were extended beyond the Chatino, Na and Tsuut’ina data sets, to a sample of languages from the Pangloss Collection, an online archive of under-resourced languages (Michailovsky et al., 2014). The results confirmed that end-to-end models for automatic phonemic transcription deliver promising performance, and also suggested that preprocessing tasks can to a large extent be automated, thereby increasing the attractiveness of the tool for language documentation workflows (Wisniewski et al., 2020). Another effort in this space is Allosaurus (Li et al., 2020), which leverages multilingual models for phonetic transcription and jointly models language independent phones and language-dependent phonemes. This stands as a 3 3 Bringing ESPnet to Elpis ESPnet is an end-to-end neural network-based speech recognition toolkit. Developed with Pytorch (Paszke et al., 2019) in a research context, the tool satisfies three desiderata for our purposes: (a) it is easy to modify training recipes, which consist of collections of scripts and configuration files that make it easy to perform training and dec"
2021.conll-1.42,W19-5361,1,0.856627,"Missing"
2021.conll-1.42,N19-1423,0,0.013442,"bject, verb, or object. The fourth type, ’contrastive focus’, is when the answer corrects the information in the question, and the word is prosodically emphasized to convey this focused information. Roettger et al. (2019) showed the 4 distinctive F0 pitch forms corresponding to each of 4 focus types in human speech production, and we chose to address contrastive focus, as this category shows the most prominent form regarding the pitch of focused word. 3.2 Defining text prompts Starting from a seed of 50 short sentences, similar to what is presented in Figure 1(d), we expanded them using BERT (Devlin et al., 2019), which stands for Bidirectional Encoder Representations from Transformers. We used Hugging Face (Wolf et al., 2019) library for this task. We simply masked the different subject, verb and object words in our initial utterances and let BERT predict the masked words. More than 10K utterances were generated this way in order to expand our initial sentence set. The full corpus was then manually verified before recording the corresponding speech. We removed the sentences which were not semantically correct. After manual validation of the full corpus, we kept 7320 sentences. 3.3 Recording A profess"
2021.emnlp-main.533,2020.acl-main.703,0,0.0406455,"lly-trained adapter layers to leverage monolingual data for unsupervised machine translation. 2) We introduce a 2-step approach for multilingual UNMT using denoising adapters and multilingual fine-tuning of mBART’s cross-attention with auxiliary parallel data. 3) We conduct experiments on a large set of language pairs showing effectiveness of denoising adapters with and without back-translation. 4) Finally, we provide further analysis to the use of denoising adapters such as extending mBART with completely new languages. 2 2.1 Background mBART fine-tuning for translation in the original BART (Lewis et al., 2020), several noising functions were introduced such as token masking, token deletion, word-span masking, sentence permutation and document rotation; mBART uses only text infilling (which is based on span masking) and sentence permutation. Architecturewise, mBART is a Transformer model (Vaswani et al., 2017) with 12 encoder and 12 decoder layers with hidden dimension of 1024 and 16 attention heads. It has a large multilingual vocabulary of 250k tokens obtained from 100 languages. To finetune mBART to machine translation, the weights of the pretrained model are loaded and all parameters are trained"
2021.emnlp-main.533,2020.findings-emnlp.371,0,0.0210921,"(Sen et al., 2019; fore eliminating the dependence on parallel data. Sun et al., 2020) trained a single shared model for They allow learning and localizing general-purpose multiple language pairs by using a denoising auto- language-specific representations on top of preencoder and back-translation. Sun et al. (2020) also trained models such as mBART. These denoising proposed to use knowledge distillation to enhance adapters can then easily be used for multilingual multilingual unsupervised translation. Another line MT, including unsupervised machine translation of research (Wang et al., 2021; Li et al., 2020; without back-translation. Garcia et al., 2021) has explored the use of auxiliary parallel data in a multilingual UNMT setting. Architecture For our denoising adapters, following Bapna and Firat (2019), we use a simple feedThese studies employ a standard two-stage training forward network with a ReLU activation. Each schema (Conneau and Lample, 2019) that consists adapter module also includes a parametrized norof a first multi-task pretraining step with denoising malization layer that acts on the input of the adapter and translation objectives, and a second fine-tuning step using back-transla"
2021.emnlp-main.533,2020.tacl-1.47,0,0.141188,"et al. (2021). Multilingual UNMT aims at combining these two trends. As depicted in Fig 1, some auxiliary languages have access to parallel data paired with English (en ↔ xx1 ), while unsupervised languages only have monolingual data (zz1 ). The goal of such an approach is to make use of the auxiliary parallel data to learn the translation task and hopefully transfer this task knowledge to the unsupervised languages. The end model should be able to translate to/from English in both the auxiliary and unsupervised languages. This setting has only been addressed very recently (Sun et al., 2020; Liu et al., 2020; Wang et al., 2021; Garcia et al., 2021). However all current approaches rely on back-translation, either offline or online. This is computationally costly and it requires a lot of engineering effort when applied to large-scale setups. In this paper, we propose a 2-step approach based on denoising adapters that enable modular multilingual unsupervised NMT without backtranslation. Our approach combines monolingual denoising adapters with multilingual transfer learn∗ ing on auxiliary parallel data. More precisely our Work done during an internship at NAVER LABS Europe. denoising adapters are li"
2021.emnlp-main.533,2020.emnlp-main.213,0,0.0427468,". data from IITB (Kunchukuttan et al., 2017) and test the resulting model on two unseen languages, Nepali (ne) and Sinhalese (si), from the FLoRes dataset (Guzmán et al., 2019) without any further training on back-translated data. For D ENOISING A DAPTERS, we trained adapters on monolingual data provided by FLoRes for all 4 languages (en, hi, ne, si). Finally for MT transfer, we inserted these language-specific adapters to mBART, and updated cross-attention layers as in the previous experiments. Results are shown in Table 4. We compare results in terms of BLEU,5 chrF (Popovi´c, 2015), C OMET (Rei et al., 2020)6 and BERT Score (Zhang et al., 2020).7 In all three metrics D ENOISING A DAPTERS significantly outperform M BART- FT, showing the effectiveness of denoising adapters for low resource languages, compared to a strong baseline. Note that since we used mBART-50 in our experiments, results for M BARTFT are slightly different from the ones in original paper (mBART-25). 7 Conclusion We have presented denoising adapters, adapter modules trained on monolingual data with a denoising objective, and a 2-step approach to adapt mBART by using these adapters for multilingual unsupervised NMT. Our experiment"
2021.emnlp-main.533,N19-4009,0,0.0129821,"z → en en → zz bg 40.7 hu 27.3 sr 34.2 el 38.7 da 41.1 be 3.12 AVG -6 30.9 bg 35.1 hu 19.2 sr 21.3 el 32.2 da 36.4 be 2.14 AVG -6 B ILINGUAL M BART- FT TASK A. D ENOIS . A. 8.8 11.9 39.8 1.0 1.3 27.5 18.9 24.8 36.9 0.2 0.5 34.6 5.2 8.3 45.5 2.8 4.6 28.4 6.2 8.6 35.5 24.1 11.1 8.6 16.1 25.7 12.1 16.3 24.4 Table 2: Unsupervised translation performance for languages that are new to mBART. on the TED talks bilingual data, with maximum 4k tokens per batch and accumulated gradients over 4 updates. Joint BPE models of size 8k are used for these models. All experiments are performed with the fairseq (Ott et al., 2019) library. results in an extra bilingual model to be trained for each unsupervised language and for all models that are evaluated. 5 Results Table 1 shows translation results for 11 languages that have no parallel data, in zz→en and en→zz directions. The first two blocks in each direction, (1) and (2), give unsupervised translation results without using back-translation. For zz→en, the two baselines M BART- FT and TASK A DAPTERS are quite decent: the ability of mBART to encode the unsupervised source languages and its transfer to NMT using auxiliary parallel data provide good multilingual unsup"
2021.emnlp-main.533,P19-1297,0,0.0223761,"ter architecture that is used in the experiments guages which lacks flexibility for incrementally adding new languages. 3 Denoising Adapters for Multilingual Unsupervised MT We define Multilingual UNMT as the problem of We address the limitations of existing methods menlearning both from parallel data centered in one tioned above by proposing denoising adapters for language (English) and monolingual data for trans- multilingual unsupervised MT. Denoising adapters lating between the centre language and any of the are monolingually-trained language adapters, thereprovided languages. Prior work (Sen et al., 2019; fore eliminating the dependence on parallel data. Sun et al., 2020) trained a single shared model for They allow learning and localizing general-purpose multiple language pairs by using a denoising auto- language-specific representations on top of preencoder and back-translation. Sun et al. (2020) also trained models such as mBART. These denoising proposed to use knowledge distillation to enhance adapters can then easily be used for multilingual multilingual unsupervised translation. Another line MT, including unsupervised machine translation of research (Wang et al., 2021; Li et al., 2020;"
2021.emnlp-main.533,W18-6301,0,0.0206268,"endix A.1 any pretraining and they are trained from scratch. (2) M BART- FT, standard fine-tuning of mBART (Liu et al., 2020) on the multilingual MT task. (3) TASK A DAPTERS, multilingual fine-tuning for language-agnostic MT adapters and cross-attention on top of mBART, similarly to Stickland et al. (2021). The bilingual models and all the mBART variants are fine-tuned on the same English-centric multilingual parallel data. Multilingual MT training details We train mBART-based models by using a maximum batch size of 4k tokens and accumulated gradients over 5 update steps with mixed precision (Ott et al., 2018) for 120k update steps. We apply Adam (Kingma and Ba, 2014) with a polynomial learning rate decay, and a linear warmup of 4 000 steps for a maximum learning rate of 0.0001. Additionally, we use dropout with a rate of 0.3 and label smoothing with a rate of 0.2. For efficient training, we filter Baselines We compare our approach with the folout the unused tokens from the mBART vocabulary lowing baselines: (1) B ILINGUAL, baseline bilinafter tokenization of the training corpora (including gual models trained on TED talks. These are small both TED talks and monolingual datasets) which Transformer"
2021.emnlp-main.533,2020.emnlp-main.617,0,0.0556663,"Missing"
2021.emnlp-main.533,2020.emnlp-main.361,1,0.89388,"coder and an autoregressive decoder (hence Bidirectional and Auto-Regressive Transformer). It is pretrained by reconstructing, 1 i.e. denoising the original text from a noisy version To simplify notation we will refer to mBART-50 as corrupted with a set of noising functions. Although mBART 6651 work instead proposes a fully multilingual transfer learning method for unsupervised MT that requires composition of encoder and decoder adapters. In machine translation, Bapna and Firat (2019) proposed bilingual adapters for improving a pretrained multilingual MT model or for domain adaptation whereas Philip et al. (2020) trained languagespecific adapters in a multilingual MT setup with a focus on zero-shot MT performance. Finally, Stickland et al. (2021) use language-agnostic task adapters for fine-tuning BART and mBART to bilingual and multilingual MT. However, none of these approaches are directly applicable for unsupervised MT task as they train language or task-specific adapters on parallel data. 2.3 Multilingual Unsupervised NMT ADAPTER Feed Forward Self Attention Transformer Encoder (xN) Up Projection ReLU Down Projection Layer Norm Adapter Layer Figure 2: Overview of the adapter architecture that is us"
2021.emnlp-main.533,W15-3049,0,0.0607684,"Missing"
2021.emnlp-main.533,W18-6319,0,0.0123954,"(15.4, 7.2 BLEU respectively) and they could be a good starting point for further improvements. Another direction is to apply denoising adapters to domain adaptation, a use-case where back-translation is a standard solution to leverage monolingual data. We provide supplementary material to facilitate future research.8 Acknowledgements We would like to thank Vassilina Nikoulina, Asa Cooper Stickland, and Naver Lab Europe NLP team for discussions during the project. Furthermore, we thank Arianna Bisazza, Gosse Bouma, Gertjan van Noord and the anonymous reviewers for their feedback. 5 SacreBLEU (Post, 2018) signature: BLEU+c.mixed+#.1+s.exp+tok.13a+v.1.5.0 6 C OMET model: wmt20-comet-da 7 Bert score hash code: roberta-large_L17_no-idf_version=0.3.10 (hug_trans=4.10.0)-rescaled_fast-tokenizer 8 Supplementary material is available at https://europe.naverlabs.com/research/na tural-language-processing/efficient-mult ilingual-machine-translation 6658 References Roee Aharoni, Melvin Johnson, and Orhan Firat. 2019. Massively multilingual neural machine translation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Techno"
2021.emnlp-main.533,N18-2084,0,0.160561,"rs for translation. The second step transfers mBART to multilingual UNMT by plugging in our denoising adapters and then fine-tuning cross-attention with auxiliary parallel data. Our approach also allows extending mBART with new languages which are not included in pretraining as shown in Sect. 6.1. This means that denoising adapters can be trained incrementally after mBART fine-tuning to add any new language to the existing setup. In our experiments, we train denoising adapters for 17 diverse unsupervised languages together with 20 auxiliary languages and evaluate the final model on TED talks (Qi et al., 2018). Our results show that our approach is on par with backtranslation for a majority of languages while being more modular and efficient. Moreover, using denoising adapters jointly with back-translation further improves unsupervised translation performance. Contributions In summary, we make the following contributions: 1) We propose denoising adapters, monolingually-trained adapter layers to leverage monolingual data for unsupervised machine translation. 2) We introduce a 2-step approach for multilingual UNMT using denoising adapters and multilingual fine-tuning of mBART’s cross-attention with a"
2021.emnlp-main.533,2021.naacl-industry.12,0,0.0839622,"ltilingual UNMT aims at combining these two trends. As depicted in Fig 1, some auxiliary languages have access to parallel data paired with English (en ↔ xx1 ), while unsupervised languages only have monolingual data (zz1 ). The goal of such an approach is to make use of the auxiliary parallel data to learn the translation task and hopefully transfer this task knowledge to the unsupervised languages. The end model should be able to translate to/from English in both the auxiliary and unsupervised languages. This setting has only been addressed very recently (Sun et al., 2020; Liu et al., 2020; Wang et al., 2021; Garcia et al., 2021). However all current approaches rely on back-translation, either offline or online. This is computationally costly and it requires a lot of engineering effort when applied to large-scale setups. In this paper, we propose a 2-step approach based on denoising adapters that enable modular multilingual unsupervised NMT without backtranslation. Our approach combines monolingual denoising adapters with multilingual transfer learn∗ ing on auxiliary parallel data. More precisely our Work done during an internship at NAVER LABS Europe. denoising adapters are lightweight adapter m"
2021.findings-acl.250,2021.eacl-main.189,0,0.0950811,"Missing"
2021.findings-acl.250,N19-4009,0,0.0599776,"Missing"
2021.findings-acl.250,P02-1040,0,0.124368,"the idea to the selfattentions in encoder as well. Methodology !2 As our goal was to identify “important” attention heads for different language pairs, we first needed to define a metric or a procedure that can capture the notion of “importance” of an attention head, and selected heads based on this importance. In Section 3.1, we present a set of metrics that quantify certain aspects of attention weights, which to some extent, can be considered as the importance. Section 3.2 illustrates a more direct approach where the importance of a head is defined as the extent of decrease in BLEU scores (Papineni et al., 2002) resulted in pruning the head. 1 3.1 either shared vocabulary or shared special tokens such as SEP , EOS , etc. cov(head) := X X j∈J αi,j i∈I More details on the metrics are provided in Appendix C. 3.2 Sequential Backward Selection of Heads Intuitively, a head can be considered as important if its removal results in a drastic decrease in the BLEU scores. As different combinations of heads can affect the performance differently, we followed the sequential backward selection (SBS) algorithm (Aha and Bankert, 1996), which is a top-down algorithm starting from a feature set of all features (in 2 A"
2021.gebnlp-1.10,2020.lrec-1.813,1,0.873599,"Missing"
2021.gebnlp-1.10,P16-2096,0,0.0301949,"t training by the weight initialization stage. We then evaluate In this paper we question the impact of gender representation in training data on the performance of an end-to-end ASR system. We create an experiment based on the Librispeech corpus and build 3 different training corpora varying only the proportion of data produced by each gender category. We observe that if our system is overall robust to the gender balance or imbalance in training data, it is nonetheless dependant of the adequacy between the individuals present in the training and testing sets. 1 Introduction As pointed out by Hovy and Spruit (2016) in their positional paper on the social impact of NLP, discriminatory performance could be the result of several types of biases. The roots of socio-technical biases in new technology could be situated in its very design, the selection of the data used for training (Garg et al., 2018; Kutuzov et al., 2018), the annotation process (Sap et al., 2019), the intermediary representations such as word embeddings (Bolukbasi et al., 2016; Caliskan et al., 2017) or in the model itself. Gender bias in ASR systems, defined as a systematically and statistically worse recognition for a gender category is s"
2021.gebnlp-1.10,C18-1117,0,0.0123604,"tion of data produced by each gender category. We observe that if our system is overall robust to the gender balance or imbalance in training data, it is nonetheless dependant of the adequacy between the individuals present in the training and testing sets. 1 Introduction As pointed out by Hovy and Spruit (2016) in their positional paper on the social impact of NLP, discriminatory performance could be the result of several types of biases. The roots of socio-technical biases in new technology could be situated in its very design, the selection of the data used for training (Garg et al., 2018; Kutuzov et al., 2018), the annotation process (Sap et al., 2019), the intermediary representations such as word embeddings (Bolukbasi et al., 2016; Caliskan et al., 2017) or in the model itself. Gender bias in ASR systems, defined as a systematically and statistically worse recognition for a gender category is still a working topic (Feng et al., 2021). Pioneer work from (Adda-Decker and Lamel, 2005) found better performance on women’s voices, while a preliminary research on YouTube automatic caption system found better recognition rate of male speech (Tatman, 2017) but no gender-difference in a follow-up study (Ta"
2021.gebnlp-1.10,P19-1163,0,0.0281618,"We observe that if our system is overall robust to the gender balance or imbalance in training data, it is nonetheless dependant of the adequacy between the individuals present in the training and testing sets. 1 Introduction As pointed out by Hovy and Spruit (2016) in their positional paper on the social impact of NLP, discriminatory performance could be the result of several types of biases. The roots of socio-technical biases in new technology could be situated in its very design, the selection of the data used for training (Garg et al., 2018; Kutuzov et al., 2018), the annotation process (Sap et al., 2019), the intermediary representations such as word embeddings (Bolukbasi et al., 2016; Caliskan et al., 2017) or in the model itself. Gender bias in ASR systems, defined as a systematically and statistically worse recognition for a gender category is still a working topic (Feng et al., 2021). Pioneer work from (Adda-Decker and Lamel, 2005) found better performance on women’s voices, while a preliminary research on YouTube automatic caption system found better recognition rate of male speech (Tatman, 2017) but no gender-difference in a follow-up study (Tatman and Kasten, 2017). Recent work on hybr"
2021.gebnlp-1.10,W17-1606,0,0.0183813,"ata used for training (Garg et al., 2018; Kutuzov et al., 2018), the annotation process (Sap et al., 2019), the intermediary representations such as word embeddings (Bolukbasi et al., 2016; Caliskan et al., 2017) or in the model itself. Gender bias in ASR systems, defined as a systematically and statistically worse recognition for a gender category is still a working topic (Feng et al., 2021). Pioneer work from (Adda-Decker and Lamel, 2005) found better performance on women’s voices, while a preliminary research on YouTube automatic caption system found better recognition rate of male speech (Tatman, 2017) but no gender-difference in a follow-up study (Tatman and Kasten, 2017). Recent work on hybrid ASR systems observed that gender imbalance in data could lead to decreased ASR performance on the gender category least represented (Garnerin et al., 2019). This last study was conducted on French broadcast data in which women account for only 1 https://www.newyorker.com/culture/cu ltural-comment/a-century-of-shrill-how-b ias-in-technology-has-hurt-womens-voices 2 see for example, this news report on decreased performance for female speaker in built-in GPS, in which the VP of voice technology stated"
2021.jeptalnrecital-taln.12,P19-1381,0,0.0269732,"Missing"
2021.jeptalnrecital-taln.12,P17-1012,0,0.0626118,"Missing"
2021.jeptalnrecital-taln.12,D17-1215,0,0.0405857,"Missing"
2021.jeptalnrecital-taln.12,W18-5413,0,0.0658679,"Missing"
2021.jeptalnrecital-taln.12,N19-4009,0,0.0372363,"Missing"
2021.jeptalnrecital-taln.12,2020.acl-srw.42,0,0.0312522,"Missing"
C16-1044,W13-3520,0,0.0140378,"anguages. Many automatic word alignment tools are available, such as GIZA++ which implements IBM models (Och and Ney, 2000). However, the noisy (non perfect) outputs of these methods is a serious limitation for the annotation projection based on word alignments (Fraser and Marcu, 2007). To deal with this limitation, recent studies based on cross-lingual representation learning methods have been proposed to avoid using such pre-processed and noisy alignments for label projection. First, these approaches learn language-independent features, across many different languages (Durrett et al., 2012; Al-Rfou et al., 2013; Täckström et al., 2013; Luong et al., 2015; Gouws and Søgaard, 2015; Gouws et al., 2015). Then, the induced representation space is used to train NLP tools by exploiting labeled data from the source language and apply them in the target language. Cross-lingual representation learning approaches have achieved good results in different NLP applications such as cross-language SST and POS tagging (Gouws and Søgaard, 2015), cross-language named entity recognition (Täckström et al., 2012), cross-lingual document classification and lexical translation task (Gouws et al., 2015), cross language depen"
C16-1044,C04-1053,0,0.255705,"of linguistic annotations was pioneered by Yarowsky et al. (2001) who created new monolingual resources by transferring annotations from resource-rich languages onto resource-poor languages through the use of word alignments. The resulting (noisy) annotations are used in conjunction with robust learning algorithms to build cheap unsupervised NLP tools (Padó and Lapata, 2009). This approach has been successfully used to transfer several linguistic annotations between languages (efficient learning of POS taggers (Das and Petrov, 2011; Duong et al., 2013) and accurate projection of word senses (Bentivogli et al., 2004)). Cross-lingual projection requires a parallel corpus and word alignment between source and target languages. Many automatic word alignment tools are available, such as GIZA++ which implements IBM models (Och and Ney, 2000). However, the noisy (non perfect) outputs of these methods is a serious limitation for the annotation projection based on word alignments (Fraser and Marcu, 2007). To deal with this limitation, recent studies based on cross-lingual representation learning methods have been proposed to avoid using such pre-processed and noisy alignments for label projection. First, these ap"
C16-1044,L16-1662,1,0.797549,"using the function f described as f (w) = arg max(PM 12 (t|w)) t 4 (5) Experiments Our models are evaluated on two labeling tasks: Cross-language Part-Of-speech (POS) tagging and Multilingual Super Sense Tagging (SST). 4.1 Multilingual POS Tagging We applied our method to build RNN POS taggers for four target languages - French, German, Greek and Spanish - with English as the source language. In order to determine the effectiveness of our common words representation described in section 3.2.1, we also investigated the use of state-of-the-art bilingual word embeddings (using MultiVec Toolkit (Bérard et al., 2016)) as input to our RNN. 4.1.1 Dataset For French as a target language, we used a training set of 10, 000 parallel sentences, a validation set of 1000 English sentences, and a test set of 1000 French sentences, all extracted from the ARCADE II English-French corpus (Veronis et al., 2008). The test set is tagged with the French TreeTagger (Schmid, 1995) and then manually checked. For German, Greek and Spanish as a target language, we used training and validation data extracted from the Europarl corpus (Koehn, 2005) which are a subset of the training data used in (Das and Petrov, 2011; Duong et al"
C16-1044,2012.iwslt-evaluation.13,1,0.821481,".2.1 Dataset SemCor The SemCor (Miller et al., 1993) is a subset of the Brown Corpus (Kucera and Francis, 1979) labeled with the WordNet (Fellbaum, 1998) senses. MultiSemCor The English-Italian MultiSemcor (MSC-IT-1) corpus is a manual translation of the English SemCor to Italian (Bentivogli et al., 2004). As we already mentioned, we are also interested in measuring the impact of the parallel corpus quality on our method. For this we use two translation systems: (a) Google Translate to translate the English SemCor to Italian (MSC-IT-2) and French (MSCFR-2). (b) LIG machine translation system (Besacier et al., 2012) to translate the English SemCor to French (MSC-FR-1). Training corpus The SemCor was labeled with the WordNet synsets. However, because we train models for SST, we convert SemCor synsets annotations to super senses. We learn our models using the four different versions of MSC (MSC-IT-1,2 - MSC-FR-1,2), with modified Semcor on source side. Test Corpus To evaluate our models, we used the SemEval 2013 Task 12 (Multilingual Word Sense Disambiguation) (Navigli et al., 2013) test corpora, which are available in 5 languages (English, French, German, Spanish and Italian) and labeled with BabelNet (Na"
C16-1044,A00-1031,0,0.737789,"present the simple cross-lingual projection method, considered as our baseline in this work. 3.1 Baseline Cross-lingual Annotation Projection We use direct transfer as a baseline system which is similar to the method described in (Yarowsky et al., 2001). First we tag the source side of the parallel corpus using the available supervised tagger. Next, we align words in the parallel corpus to find out corresponding source and target words. Tags are then projected to the (resource-poor) target language. The target language tagger is trained using any machine learning approach (we use TnT tagger (Brants, 2000) in our experiments). 3.2 Proposed Approach We propose a method for learning multilingual sequence labeling tools based on RNN, as it can be seen in Figure 1. In our approach, a parallel or multi-parallel corpus between a resource-rich language and one or many under-resourced languages is used to extract common (multilingual) and agnostic words representations. These representations, which rely on sentence level alignment only, are used with the source side of the parallel/multi-parallel corpus to learn a neural network tagger in the source language. Since a common representation of source and"
C16-1044,W06-2920,0,0.0237114,"OV 78.9 73.0 70.3 68.8 76.1 76.4 77.5 76.6 77.6 77.8 81.5 77.0 81.9 77.1 82.1 78.7 82.8 — 85.4 — 84.8 — Greek All words OOV 77.5 72.8 71.1 65.4 75.7 70.7 77.2 71.0 77.9 75.3 78.3 74.6 79.2 75.0 79.9 78.5 82.5 — 80.4 — — — Spanish All words OOV 80.0 79.7 73.4 62.4 78.8 72.6 80.5 73.1 80.6 74.7 83.6 81.2 84.4 81.7 84.4 81.9 84.2 — 83.3 — 82.6 — Table 1: Token-level POS tagging accuracy for Simple Projection, SRNN using MultiVec bilingual word embeddings as input, RNN5 , Projection+RNN and methods of Das & Petrov (2011), Duong et al (2013) and Gouws & Søgaard (2015). tasks on dependency parsing (Buchholz and Marsi, 2006)). The evaluation metric (per-token accuracy) and the Petrov et al. (2012) universal tagset are used for evaluation. For training, the English (source) sides of the training corpora (ARCADE II and Europarl) and of the validation corpora are tagged with the English TreeTagger toolkit. Using the matching provided by Petrov et al. (2012), we map the TreeTagger and the CoNLL tagsets to the common Universal Tagset. In order to build our baseline unsupervised tagger (based on a Simple Cross-lingual Projection – see section 3.1), we also tag the target side of the training corpus, with tags projected"
C16-1044,W06-1670,0,0.0440975,"nnotations in the target language from sentencebased alignments only. While most NLP researches on RNN have focused on monolingual tasks1 and sequence labeling (Collobert et al., 2011; Graves, 2012), this paper, however, considers the problem of learning multilingual NLP tools using RNN. Contributions In this paper, we investigate the effectiveness of RNN architectures — Simple RNN (SRNN) and Bidirectional RNN (BRNN) — for multilingual sequence labeling tasks without using any word alignment information. Two NLP tasks are considered: Part-Of-Speech (POS) tagging and Super Sense (SST) tagging (Ciaramita and Altun, 2006). Our RNN architectures demonstrate very competitive results on unsupervised training for new target languages. In addition, we show that the integration of 1 Exceptions are the recent propositions on Neural Machine Translation (Cho et al., 2014; Sutskever et al., 2014) This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/ 450 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 450–460, Osaka, Japan, December 11-17 2016. POS information i"
C16-1044,W99-0613,0,0.0161153,"f-Speech tags) in the RNN to train higher level taggers (for instance, super sense taggers). We demonstrate the validity and genericity of our model by using parallel corpora (obtained by manual or automatic translation). Our experiments are conducted to induce cross-lingual POS and super sense taggers. 1 Introduction In order to minimize the need for annotated resources (produced through manual annotation, or by manual check of automatic annotation), several research works were interested in building Natural Language Processing (NLP) tools based on unsupervised or semi-supervised approaches (Collins and Singer, 1999; Klein, 2005; Goldberg, 2010). For example, NLP tools based on cross-language projection of linguistic annotations achieved good performances in the early 2000s (Yarowsky et al., 2001). The key idea of annotation projection can be summarized as follows: through word alignment in parallel text corpora, the annotations are transferred from the source (resource-rich) language to the target (under-resourced) language, and the resulting annotations are used for supervised training in the target language. However, automatic word alignment errors (Fraser and Marcu, 2007) limit the performance of the"
C16-1044,J81-4005,0,0.659115,"Missing"
C16-1044,P11-1061,0,0.55477,"e finally conclude the paper in Section 5. 2 Related Work Cross-lingual projection of linguistic annotations was pioneered by Yarowsky et al. (2001) who created new monolingual resources by transferring annotations from resource-rich languages onto resource-poor languages through the use of word alignments. The resulting (noisy) annotations are used in conjunction with robust learning algorithms to build cheap unsupervised NLP tools (Padó and Lapata, 2009). This approach has been successfully used to transfer several linguistic annotations between languages (efficient learning of POS taggers (Das and Petrov, 2011; Duong et al., 2013) and accurate projection of word senses (Bentivogli et al., 2004)). Cross-lingual projection requires a parallel corpus and word alignment between source and target languages. Many automatic word alignment tools are available, such as GIZA++ which implements IBM models (Och and Ney, 2000). However, the noisy (non perfect) outputs of these methods is a serious limitation for the annotation projection based on word alignments (Fraser and Marcu, 2007). To deal with this limitation, recent studies based on cross-lingual representation learning methods have been proposed to avo"
C16-1044,P13-2112,0,0.175601,"paper in Section 5. 2 Related Work Cross-lingual projection of linguistic annotations was pioneered by Yarowsky et al. (2001) who created new monolingual resources by transferring annotations from resource-rich languages onto resource-poor languages through the use of word alignments. The resulting (noisy) annotations are used in conjunction with robust learning algorithms to build cheap unsupervised NLP tools (Padó and Lapata, 2009). This approach has been successfully used to transfer several linguistic annotations between languages (efficient learning of POS taggers (Das and Petrov, 2011; Duong et al., 2013) and accurate projection of word senses (Bentivogli et al., 2004)). Cross-lingual projection requires a parallel corpus and word alignment between source and target languages. Many automatic word alignment tools are available, such as GIZA++ which implements IBM models (Och and Ney, 2000). However, the noisy (non perfect) outputs of these methods is a serious limitation for the annotation projection based on word alignments (Fraser and Marcu, 2007). To deal with this limitation, recent studies based on cross-lingual representation learning methods have been proposed to avoid using such pre-pro"
C16-1044,D12-1001,0,0.0607578,"en source and target languages. Many automatic word alignment tools are available, such as GIZA++ which implements IBM models (Och and Ney, 2000). However, the noisy (non perfect) outputs of these methods is a serious limitation for the annotation projection based on word alignments (Fraser and Marcu, 2007). To deal with this limitation, recent studies based on cross-lingual representation learning methods have been proposed to avoid using such pre-processed and noisy alignments for label projection. First, these approaches learn language-independent features, across many different languages (Durrett et al., 2012; Al-Rfou et al., 2013; Täckström et al., 2013; Luong et al., 2015; Gouws and Søgaard, 2015; Gouws et al., 2015). Then, the induced representation space is used to train NLP tools by exploiting labeled data from the source language and apply them in the target language. Cross-lingual representation learning approaches have achieved good results in different NLP applications such as cross-language SST and POS tagging (Gouws and Søgaard, 2015), cross-language named entity recognition (Täckström et al., 2012), cross-lingual document classification and lexical translation task (Gouws et al., 2015)"
C16-1044,J07-3002,0,0.092707,"semi-supervised approaches (Collins and Singer, 1999; Klein, 2005; Goldberg, 2010). For example, NLP tools based on cross-language projection of linguistic annotations achieved good performances in the early 2000s (Yarowsky et al., 2001). The key idea of annotation projection can be summarized as follows: through word alignment in parallel text corpora, the annotations are transferred from the source (resource-rich) language to the target (under-resourced) language, and the resulting annotations are used for supervised training in the target language. However, automatic word alignment errors (Fraser and Marcu, 2007) limit the performance of these approaches. Our work is built upon these previous contributions and observations. We explore the possibility of using Recurrent Neural Networks (RNN) to build multilingual NLP tools for resource-poor languages analysis. The major difference with previous works is that we do not explicitly use word alignment information. Our only assumption is that parallel sentences (source-target) are available and that the source part is annotated. In other words, we try to infer annotations in the target language from sentencebased alignments only. While most NLP researches o"
C16-1044,N15-1157,0,0.0890659,"Missing"
C16-1044,2005.mtsummit-papers.11,0,0.0225251,"use of state-of-the-art bilingual word embeddings (using MultiVec Toolkit (Bérard et al., 2016)) as input to our RNN. 4.1.1 Dataset For French as a target language, we used a training set of 10, 000 parallel sentences, a validation set of 1000 English sentences, and a test set of 1000 French sentences, all extracted from the ARCADE II English-French corpus (Veronis et al., 2008). The test set is tagged with the French TreeTagger (Schmid, 1995) and then manually checked. For German, Greek and Spanish as a target language, we used training and validation data extracted from the Europarl corpus (Koehn, 2005) which are a subset of the training data used in (Das and Petrov, 2011; Duong et al., 2013). This choice allows us to compare our results with those of (Das and Petrov, 2011; Duong et al., 2013; Gouws and Søgaard, 2015). The train data set contains 65, 000 bi-sentences ; a validation set of 10, 000 bi-sentences is also available. For testing, we use the same test corpora as (Das and Petrov, 2011; Duong et al., 2013; Gouws and Søgaard, 2015) (bi-sentences from CoNLL shared 3 words which do not have a known vector representation 455 Lang. Model Simple Projection SRNN MultiVec SRNN BRNN BRNN - OO"
C16-1044,W15-1521,0,0.0320977,"are available, such as GIZA++ which implements IBM models (Och and Ney, 2000). However, the noisy (non perfect) outputs of these methods is a serious limitation for the annotation projection based on word alignments (Fraser and Marcu, 2007). To deal with this limitation, recent studies based on cross-lingual representation learning methods have been proposed to avoid using such pre-processed and noisy alignments for label projection. First, these approaches learn language-independent features, across many different languages (Durrett et al., 2012; Al-Rfou et al., 2013; Täckström et al., 2013; Luong et al., 2015; Gouws and Søgaard, 2015; Gouws et al., 2015). Then, the induced representation space is used to train NLP tools by exploiting labeled data from the source language and apply them in the target language. Cross-lingual representation learning approaches have achieved good results in different NLP applications such as cross-language SST and POS tagging (Gouws and Søgaard, 2015), cross-language named entity recognition (Täckström et al., 2012), cross-lingual document classification and lexical translation task (Gouws et al., 2015), cross language dependency parsing (Durrett et al., 2012; Täckstr"
C16-1044,H93-1061,0,0.126466,"e labeling RNN is proposed. Methodology For training our multilingual RNN models, we just need as input a parallel (or multiparallel) corpus between a resource-rich language and one or many under-resourced languages. Such a parallel corpus can be manually obtained (clean corpus) or automatically obtained (noisy corpus). To show the potential of our approach, we investigate two sequence labeling tasks: cross-language POS tagging and multilingual Super Sense Tagging (SST). For the SST task, we measure the impact of the parallel corpus quality with manual or automatic translations of the SemCor (Miller et al., 1993) translated from English into Italian (manually and automatically) and French (automatically). Outline The remainder of the paper is organized as follows. Section 2 reviews related work. Section 3 describes our cross-language annotation projection approaches based on RNN. Section 4 presents the empirical study and associated results. We finally conclude the paper in Section 5. 2 Related Work Cross-lingual projection of linguistic annotations was pioneered by Yarowsky et al. (2001) who created new monolingual resources by transferring annotations from resource-rich languages onto resource-poor"
C16-1044,S13-2040,0,0.023609,"le Translate to translate the English SemCor to Italian (MSC-IT-2) and French (MSCFR-2). (b) LIG machine translation system (Besacier et al., 2012) to translate the English SemCor to French (MSC-FR-1). Training corpus The SemCor was labeled with the WordNet synsets. However, because we train models for SST, we convert SemCor synsets annotations to super senses. We learn our models using the four different versions of MSC (MSC-IT-1,2 - MSC-FR-1,2), with modified Semcor on source side. Test Corpus To evaluate our models, we used the SemEval 2013 Task 12 (Multilingual Word Sense Disambiguation) (Navigli et al., 2013) test corpora, which are available in 5 languages (English, French, German, Spanish and Italian) and labeled with BabelNet (Navigli and Ponzetto, 2012) senses. We map BabelNet senses to WordNet synsets, then WordNet synsets are mapped to super senses. 4.2.2 SST Systems Evaluated The goals of our SST experiments are twofold: first, to investigate the effectiveness of using POS information to build multilingual super sense tagger, secondly to measure the impact of the parallel corpus quality (manual or automatic translation) on our RNN models (SRNN, BRNN and our proposed variants). To summarize,"
C16-1044,P00-1056,0,0.170144,"resulting (noisy) annotations are used in conjunction with robust learning algorithms to build cheap unsupervised NLP tools (Padó and Lapata, 2009). This approach has been successfully used to transfer several linguistic annotations between languages (efficient learning of POS taggers (Das and Petrov, 2011; Duong et al., 2013) and accurate projection of word senses (Bentivogli et al., 2004)). Cross-lingual projection requires a parallel corpus and word alignment between source and target languages. Many automatic word alignment tools are available, such as GIZA++ which implements IBM models (Och and Ney, 2000). However, the noisy (non perfect) outputs of these methods is a serious limitation for the annotation projection based on word alignments (Fraser and Marcu, 2007). To deal with this limitation, recent studies based on cross-lingual representation learning methods have been proposed to avoid using such pre-processed and noisy alignments for label projection. First, these approaches learn language-independent features, across many different languages (Durrett et al., 2012; Al-Rfou et al., 2013; Täckström et al., 2013; Luong et al., 2015; Gouws and Søgaard, 2015; Gouws et al., 2015). Then, the i"
C16-1044,petrov-etal-2012-universal,0,0.343667,"ree new RNN variants to take into account low level (POS) information in a higher level (SST) annotation task. The question addressed here is: at which layer of the RNN this low level information should be included to improve SST performance? As specified in Figure 3, the POS information can be introduced either at input layer or at forward layer (forward and backward layers for BRNN) or at compression layer. In all these RNN variants, the POS of the current word is also represented with a vector (P OS(t)). Its dimension corresponds to the number of POS tags in the tagset (universal tagset of Petrov et al. (2012) is used). We propose one hot vector representation where only one value is set to 1 and corresponds to the index of current tag (all other values are 0). 3.2.3 Network Training The first step in our approach is to train the neural network, given a parallel corpus (training corpus), and a validation corpus (different from train data) in the source language. In typical applications, the source language is a resource-rich language (which already has an efficient tagger or manually tagged resources). Our RNN models are trained by stochastic gradient descent using usual back-propagation and back-p"
C16-1044,C12-1146,0,0.0410307,"Missing"
C16-1044,N12-1052,0,0.0310849,"rst, these approaches learn language-independent features, across many different languages (Durrett et al., 2012; Al-Rfou et al., 2013; Täckström et al., 2013; Luong et al., 2015; Gouws and Søgaard, 2015; Gouws et al., 2015). Then, the induced representation space is used to train NLP tools by exploiting labeled data from the source language and apply them in the target language. Cross-lingual representation learning approaches have achieved good results in different NLP applications such as cross-language SST and POS tagging (Gouws and Søgaard, 2015), cross-language named entity recognition (Täckström et al., 2012), cross-lingual document classification and lexical translation task (Gouws et al., 2015), cross language dependency parsing (Durrett et al., 2012; Täckström et al., 2013) and cross-language semantic role labeling (Titov and Klementiev, 2012). Our approach described in next section, is inspired by these works since we also try to induce a common language-independent feature space (crosslingual words embeddings). Unlike Durrett et al. (2012) and Gouws and Søgaard (2015), who use bilingual lexicons, and unlike Luong et al. (2015) who use word alignments between the source and target languages2 o"
C16-1044,N13-1126,0,0.033775,"ic word alignment tools are available, such as GIZA++ which implements IBM models (Och and Ney, 2000). However, the noisy (non perfect) outputs of these methods is a serious limitation for the annotation projection based on word alignments (Fraser and Marcu, 2007). To deal with this limitation, recent studies based on cross-lingual representation learning methods have been proposed to avoid using such pre-processed and noisy alignments for label projection. First, these approaches learn language-independent features, across many different languages (Durrett et al., 2012; Al-Rfou et al., 2013; Täckström et al., 2013; Luong et al., 2015; Gouws and Søgaard, 2015; Gouws et al., 2015). Then, the induced representation space is used to train NLP tools by exploiting labeled data from the source language and apply them in the target language. Cross-lingual representation learning approaches have achieved good results in different NLP applications such as cross-language SST and POS tagging (Gouws and Søgaard, 2015), cross-language named entity recognition (Täckström et al., 2012), cross-lingual document classification and lexical translation task (Gouws et al., 2015), cross language dependency parsing (Durrett e"
C16-1044,P12-1068,0,0.125292,"duced representation space is used to train NLP tools by exploiting labeled data from the source language and apply them in the target language. Cross-lingual representation learning approaches have achieved good results in different NLP applications such as cross-language SST and POS tagging (Gouws and Søgaard, 2015), cross-language named entity recognition (Täckström et al., 2012), cross-lingual document classification and lexical translation task (Gouws et al., 2015), cross language dependency parsing (Durrett et al., 2012; Täckström et al., 2013) and cross-language semantic role labeling (Titov and Klementiev, 2012). Our approach described in next section, is inspired by these works since we also try to induce a common language-independent feature space (crosslingual words embeddings). Unlike Durrett et al. (2012) and Gouws and Søgaard (2015), who use bilingual lexicons, and unlike Luong et al. (2015) who use word alignments between the source and target languages2 our common multilingual representation is very agnostic. We use a simple (multilingual) vector representation based on the occurrence of source and target words in a parallel corpus and we let the RNN learn the best internal representations (c"
C16-1044,H01-1035,0,0.477814,"manual or automatic translation). Our experiments are conducted to induce cross-lingual POS and super sense taggers. 1 Introduction In order to minimize the need for annotated resources (produced through manual annotation, or by manual check of automatic annotation), several research works were interested in building Natural Language Processing (NLP) tools based on unsupervised or semi-supervised approaches (Collins and Singer, 1999; Klein, 2005; Goldberg, 2010). For example, NLP tools based on cross-language projection of linguistic annotations achieved good performances in the early 2000s (Yarowsky et al., 2001). The key idea of annotation projection can be summarized as follows: through word alignment in parallel text corpora, the annotations are transferred from the source (resource-rich) language to the target (under-resourced) language, and the resulting annotations are used for supervised training in the target language. However, automatic word alignment errors (Fraser and Marcu, 2007) limit the performance of these approaches. Our work is built upon these previous contributions and observations. We explore the possibility of using Recurrent Neural Networks (RNN) to build multilingual NLP tools"
C16-1044,Y15-1016,1,0.732491,"Missing"
C16-1044,2015.jeptalnrecital-court.32,1,0.76448,"Missing"
C16-1044,W14-4012,0,\N,Missing
C16-1110,W05-0909,0,0.245811,"16, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1159–1168, Osaka, Japan, December 11-17 2016. Contributions: this article attempts to review the contribution of vector representations to measure sentence similarity. We compare them with similarity measures based on lexical resources such as WordNet or DBnary. Machine Translation (MT) evaluation was identified as a particularly interesting application to investigate, since MT evaluation is still an open problem nowadays. More precisely, we propose to augment a well known MT evaluation metric (METEOR (Banerjee and Lavie, 2005)) which allows an approximate matching (through synonymy or morphological similarity) between MT hypothesis and reference. The augmented versions of METEOR proposed (using word embeddings, lexical resources or both) allow us to objectively compare the contribution of each approach to measure sentence similarity. For this, correlations between METEOR and human judgements (of MT outputs) are measured within the framework of WMT 2014 Metrics task. The code of the augmented versions of METEOR is also provided on our Github page3 . Outline: in section 2 (Related Work), we quickly present METEOR, le"
C16-1110,D10-1115,0,0.0445168,"). Table 1 shows the size of the data for languages involved in the experiments later reported in this paper. Additional figures are available on the DBnary public web site5 . Lemmatized forms for DBnary are based on the TreeTagger module (Schmid, 1995), which enables us to find the corresponding synsets. 2.3 Monolingual and bilingual embeddings 2.3.1 Overview Learning word embeddings is an active research area (Bengio et al., 2003; Turian et al., 2010; Collobert et al., 2011; Huang et al., 2012). The main idea is to learn a word representation according to its context: the surrounding words (Baroni and Zamparelli, 2010). The words are projected on a continuous space and those with similar context should be close in this multi-dimensional space. When word vectors are available, a similarity between two words can be measured by a metric such as a cosine similarity. Using word-embeddings for machine translation evaluation is appealing since they can be used to compute similarity between words or phrases in the same language (monolingual embeddings capture intrinsically synonymy or morphological closeness) or in two different languages (bilingual embeddings allow to directly compute a distance between two senten"
C16-1110,L16-1662,1,0.882216,"Missing"
C16-1110,J81-4005,0,0.745114,"Missing"
C16-1110,N10-1031,0,0.0201285,"n judgements by using more than word-to-word alignments between a hypothesis and some references. The alignment is made according to three modules: the first stage uses exact match between word surface forms (Exact module), the second one compares word stems (Stems module) and the third one uses synonyms (Synonym module) from a lexical resource such as WordNet (available for English only in METEOR). One contribution of this paper is to propose an alternative to Stems and Synonym modules: our proposed add-on will be called Vectors module later on. 2.1.2 Recent extensions of METEOR METEOR-NEXT (Denkowski and Lavie, 2010a) was proposed to better correlate with HTER (Humantargeted Translation Edit Rate – HTER (Snover et al., 2006)). HTER is a semi-automatic post-editing based metric, which measures the edit distance between a hypothesis and a reference. METEOR-NEXT proposes to go further than just word-to-word alignment by using phrase-to-phrase alignments. For this, phrase databases were created for several languages like English (Snover et al., 2009), German, French or Czech (Denkowski and Lavie, 2010b). More recently, another version called METEOR Universal used bitexts to extract paraphrases (Denkowski and"
C16-1110,W10-1751,0,0.0244647,"n judgements by using more than word-to-word alignments between a hypothesis and some references. The alignment is made according to three modules: the first stage uses exact match between word surface forms (Exact module), the second one compares word stems (Stems module) and the third one uses synonyms (Synonym module) from a lexical resource such as WordNet (available for English only in METEOR). One contribution of this paper is to propose an alternative to Stems and Synonym modules: our proposed add-on will be called Vectors module later on. 2.1.2 Recent extensions of METEOR METEOR-NEXT (Denkowski and Lavie, 2010a) was proposed to better correlate with HTER (Humantargeted Translation Edit Rate – HTER (Snover et al., 2006)). HTER is a semi-automatic post-editing based metric, which measures the edit distance between a hypothesis and a reference. METEOR-NEXT proposes to go further than just word-to-word alignment by using phrase-to-phrase alignments. For this, phrase databases were created for several languages like English (Snover et al., 2009), German, French or Czech (Denkowski and Lavie, 2010b). More recently, another version called METEOR Universal used bitexts to extract paraphrases (Denkowski and"
C16-1110,W14-3348,0,0.0579533,"d Lavie, 2010a) was proposed to better correlate with HTER (Humantargeted Translation Edit Rate – HTER (Snover et al., 2006)). HTER is a semi-automatic post-editing based metric, which measures the edit distance between a hypothesis and a reference. METEOR-NEXT proposes to go further than just word-to-word alignment by using phrase-to-phrase alignments. For this, phrase databases were created for several languages like English (Snover et al., 2009), German, French or Czech (Denkowski and Lavie, 2010b). More recently, another version called METEOR Universal used bitexts to extract paraphrases (Denkowski and Lavie, 2014). METEOR was also extended by using Word Sense Disambiguation (WSD) techniques (Apidianaki and Marie, 2015). The authors used Babelfly (Moro et al., 2014) for several langage pairs (translation from French, Hindi, German, Czech and Russian to English). A better correlation with human judgement at segment level was observed using WSD in METEOR. Finally, to extend the use of Synonym module to target languages others than English, Elloumi et al. (2015) proposed to replace WordNet by DBnary (Sérasset, 2012). The new target languages equipped with a Synonym module were French, German, Spanish, Russ"
C16-1110,2015.mtsummit-papers.7,1,0.739833,"an, French or Czech (Denkowski and Lavie, 2010b). More recently, another version called METEOR Universal used bitexts to extract paraphrases (Denkowski and Lavie, 2014). METEOR was also extended by using Word Sense Disambiguation (WSD) techniques (Apidianaki and Marie, 2015). The authors used Babelfly (Moro et al., 2014) for several langage pairs (translation from French, Hindi, German, Czech and Russian to English). A better correlation with human judgement at segment level was observed using WSD in METEOR. Finally, to extend the use of Synonym module to target languages others than English, Elloumi et al. (2015) proposed to replace WordNet by DBnary (Sérasset, 2012). The new target languages equipped with a Synonym module were French, German, Spanish, Russian and English. 2.2 Lexical resources 2.2.1 WordNet WordNet is a well known lexical resource for English. Created at the University of Princeton (Fellbaum, 1998), it is used in several NLP tasks such as Machine Translation, Word Sense Disambiguation, 3 https://github.com/cservan/METEOR-E 1160 Cross-lingual Information Retrieval, etc. WordNet links nouns, verbs, adjectives and adverbs to a set of synonyms called “synsets”. Each synset represents a s"
C16-1110,W15-3047,0,0.0410281,"Missing"
C16-1110,P12-1092,0,0.0360721,"erent target languages. Additionally, DBnary contains lexicosemantic relations (syno/anto-nyms, hypo/hypero-nyms, etc.). Table 1 shows the size of the data for languages involved in the experiments later reported in this paper. Additional figures are available on the DBnary public web site5 . Lemmatized forms for DBnary are based on the TreeTagger module (Schmid, 1995), which enables us to find the corresponding synsets. 2.3 Monolingual and bilingual embeddings 2.3.1 Overview Learning word embeddings is an active research area (Bengio et al., 2003; Turian et al., 2010; Collobert et al., 2011; Huang et al., 2012). The main idea is to learn a word representation according to its context: the surrounding words (Baroni and Zamparelli, 2010). The words are projected on a continuous space and those with similar context should be close in this multi-dimensional space. When word vectors are available, a similarity between two words can be measured by a metric such as a cosine similarity. Using word-embeddings for machine translation evaluation is appealing since they can be used to compute similarity between words or phrases in the same language (monolingual embeddings capture intrinsically synonymy or morph"
C16-1110,W15-1521,0,0.0250285,"particular, many contributions were proposed after the work of (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c) on training word embeddings. The main reasons for this strong interest are: the proposal of a simple and efficient neural architecture to learn word vector representations, the availability of an open source tool Word2Vec1 and the rapid structuring of a user community2 . Later on, several contributions have extended the work of Mikolov on word vectors to phrases (sequences of words) (Mikolov et al., 2013b; Le and Mikolov, 2014a) and to bilingual representations (Luong et al., 2015). All these vector representations capture similarities between words, phrases or sentences at different levels (morphological, semantic). However, although these representations can be semantically informative, they do not exactly replace fine-grained information available in lexical-semantic resources such as WordNet (Fellbaum, 1998), BabelNet (Navigli and Ponzetto, 2010), or DBnary (Sérasset, 2012). Such lexical resources are also more easily interpretable by humans as shown in (Panchenko, 2016), but their construction is costly while word embeddings can be trained ad infinitum on any monol"
C16-1110,W14-3336,0,0.118723,"Missing"
C16-1110,N13-1090,0,0.0327015,"n automatic and a reference translation. Our experiments are made in the framework of the Metrics task of WMT 2014. We show that distributed representations are a good alternative to lexico-semantic resources for MT evaluation and they can even bring interesting additional information. The augmented versions of METEOR, using vector representations, are made available on our Github page. 1 Introduction Learning vector representations of words using neural networks has generated a strong enthusiasm in the NLP research community. In particular, many contributions were proposed after the work of (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c) on training word embeddings. The main reasons for this strong interest are: the proposal of a simple and efficient neural architecture to learn word vector representations, the availability of an open source tool Word2Vec1 and the rapid structuring of a user community2 . Later on, several contributions have extended the work of Mikolov on word vectors to phrases (sequences of words) (Mikolov et al., 2013b; Le and Mikolov, 2014a) and to bilingual representations (Luong et al., 2015). All these vector representations capture similarities between w"
C16-1110,Q14-1019,0,0.0221709,"ng based metric, which measures the edit distance between a hypothesis and a reference. METEOR-NEXT proposes to go further than just word-to-word alignment by using phrase-to-phrase alignments. For this, phrase databases were created for several languages like English (Snover et al., 2009), German, French or Czech (Denkowski and Lavie, 2010b). More recently, another version called METEOR Universal used bitexts to extract paraphrases (Denkowski and Lavie, 2014). METEOR was also extended by using Word Sense Disambiguation (WSD) techniques (Apidianaki and Marie, 2015). The authors used Babelfly (Moro et al., 2014) for several langage pairs (translation from French, Hindi, German, Czech and Russian to English). A better correlation with human judgement at segment level was observed using WSD in METEOR. Finally, to extend the use of Synonym module to target languages others than English, Elloumi et al. (2015) proposed to replace WordNet by DBnary (Sérasset, 2012). The new target languages equipped with a Synonym module were French, German, Spanish, Russian and English. 2.2 Lexical resources 2.2.1 WordNet WordNet is a well known lexical resource for English. Created at the University of Princeton (Fellbau"
C16-1110,P10-1023,0,0.030229,"d structuring of a user community2 . Later on, several contributions have extended the work of Mikolov on word vectors to phrases (sequences of words) (Mikolov et al., 2013b; Le and Mikolov, 2014a) and to bilingual representations (Luong et al., 2015). All these vector representations capture similarities between words, phrases or sentences at different levels (morphological, semantic). However, although these representations can be semantically informative, they do not exactly replace fine-grained information available in lexical-semantic resources such as WordNet (Fellbaum, 1998), BabelNet (Navigli and Ponzetto, 2010), or DBnary (Sérasset, 2012). Such lexical resources are also more easily interpretable by humans as shown in (Panchenko, 2016), but their construction is costly while word embeddings can be trained ad infinitum on any monolingual or bilingual corpora. In short, both approaches (lexical resources and word embeddings) have their pros and cons. However, few studies have attempted to compare and combine them. Pioneering work of Faruqui et al. (2014) proposed to refine representations learning using lexical resources. The idea is to force words connected in the lexical network, to have a close rep"
C16-1110,D15-1222,0,0.0790626,"Missing"
C16-1110,L16-1421,0,0.0197845,"es of words) (Mikolov et al., 2013b; Le and Mikolov, 2014a) and to bilingual representations (Luong et al., 2015). All these vector representations capture similarities between words, phrases or sentences at different levels (morphological, semantic). However, although these representations can be semantically informative, they do not exactly replace fine-grained information available in lexical-semantic resources such as WordNet (Fellbaum, 1998), BabelNet (Navigli and Ponzetto, 2010), or DBnary (Sérasset, 2012). Such lexical resources are also more easily interpretable by humans as shown in (Panchenko, 2016), but their construction is costly while word embeddings can be trained ad infinitum on any monolingual or bilingual corpora. In short, both approaches (lexical resources and word embeddings) have their pros and cons. However, few studies have attempted to compare and combine them. Pioneering work of Faruqui et al. (2014) proposed to refine representations learning using lexical resources. The idea is to force words connected in the lexical network, to have a close representation (for example through a synonymy link). The technique proposed is evaluated on several benchmarks (word similarity,"
C16-1110,P02-1040,0,0.100993,"utline: in section 2 (Related Work), we quickly present METEOR, lexical resources and word embeddings. Section 3 presents our propositions to augment METEOR in order to conduct a fair comparison between lexical resources and vector representations respectively. Section 4 presents our experiments made within the framework of WMT 2014, as well as quantitative and qualitative analyses. Finally, section 5 concludes this work and gives some perpectives. 2 Related Work 2.1 An automatic metric for MT evaluation: METEOR 2.1.1 The origins METEOR was proposed to compensate BLEU’s and NIST’s weaknesses (Papineni et al., 2002; Doddington, 2002). In short, METEOR was created to better correlate with human judgements by using more than word-to-word alignments between a hypothesis and some references. The alignment is made according to three modules: the first stage uses exact match between word surface forms (Exact module), the second one compares word stems (Stems module) and the third one uses synonyms (Synonym module) from a lexical resource such as WordNet (available for English only in METEOR). One contribution of this paper is to propose an alternative to Stems and Synonym modules: our proposed add-on will be"
C16-1110,P15-1173,0,0.0250212,"initum on any monolingual or bilingual corpora. In short, both approaches (lexical resources and word embeddings) have their pros and cons. However, few studies have attempted to compare and combine them. Pioneering work of Faruqui et al. (2014) proposed to refine representations learning using lexical resources. The idea is to force words connected in the lexical network, to have a close representation (for example through a synonymy link). The technique proposed is evaluated on several benchmarks (word similarity, sentiment analysis, finding of synonyms). More recently, Panchenko (2016) and Rothe and Schütze (2015) extended word embeddings to sense embeddings and tried to compare them to lexical synsets. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 1 http://word2vec.googlecode.com/svn/trunk/ 2 https://groups.google.com/d/forum/word2vec-toolkit 1159 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1159–1168, Osaka, Japan, December 11-17 2016. Contributions: this article attempts to review the contribution of vector representations to meas"
C16-1110,2006.amta-papers.25,0,0.0961653,"ade according to three modules: the first stage uses exact match between word surface forms (Exact module), the second one compares word stems (Stems module) and the third one uses synonyms (Synonym module) from a lexical resource such as WordNet (available for English only in METEOR). One contribution of this paper is to propose an alternative to Stems and Synonym modules: our proposed add-on will be called Vectors module later on. 2.1.2 Recent extensions of METEOR METEOR-NEXT (Denkowski and Lavie, 2010a) was proposed to better correlate with HTER (Humantargeted Translation Edit Rate – HTER (Snover et al., 2006)). HTER is a semi-automatic post-editing based metric, which measures the edit distance between a hypothesis and a reference. METEOR-NEXT proposes to go further than just word-to-word alignment by using phrase-to-phrase alignments. For this, phrase databases were created for several languages like English (Snover et al., 2009), German, French or Czech (Denkowski and Lavie, 2010b). More recently, another version called METEOR Universal used bitexts to extract paraphrases (Denkowski and Lavie, 2014). METEOR was also extended by using Word Sense Disambiguation (WSD) techniques (Apidianaki and Mar"
C16-1110,W09-0441,0,0.0257369,"ose an alternative to Stems and Synonym modules: our proposed add-on will be called Vectors module later on. 2.1.2 Recent extensions of METEOR METEOR-NEXT (Denkowski and Lavie, 2010a) was proposed to better correlate with HTER (Humantargeted Translation Edit Rate – HTER (Snover et al., 2006)). HTER is a semi-automatic post-editing based metric, which measures the edit distance between a hypothesis and a reference. METEOR-NEXT proposes to go further than just word-to-word alignment by using phrase-to-phrase alignments. For this, phrase databases were created for several languages like English (Snover et al., 2009), German, French or Czech (Denkowski and Lavie, 2010b). More recently, another version called METEOR Universal used bitexts to extract paraphrases (Denkowski and Lavie, 2014). METEOR was also extended by using Word Sense Disambiguation (WSD) techniques (Apidianaki and Marie, 2015). The authors used Babelfly (Moro et al., 2014) for several langage pairs (translation from French, Hindi, German, Czech and Russian to English). A better correlation with human judgement at segment level was observed using WSD in METEOR. Finally, to extend the use of Synonym module to target languages others than Eng"
C16-1110,P10-1040,0,0.0606763,"cted sources languages to more than 1500 different target languages. Additionally, DBnary contains lexicosemantic relations (syno/anto-nyms, hypo/hypero-nyms, etc.). Table 1 shows the size of the data for languages involved in the experiments later reported in this paper. Additional figures are available on the DBnary public web site5 . Lemmatized forms for DBnary are based on the TreeTagger module (Schmid, 1995), which enables us to find the corresponding synsets. 2.3 Monolingual and bilingual embeddings 2.3.1 Overview Learning word embeddings is an active research area (Bengio et al., 2003; Turian et al., 2010; Collobert et al., 2011; Huang et al., 2012). The main idea is to learn a word representation according to its context: the surrounding words (Baroni and Zamparelli, 2010). The words are projected on a continuous space and those with similar context should be close in this multi-dimensional space. When word vectors are available, a similarity between two words can be measured by a metric such as a cosine similarity. Using word-embeddings for machine translation evaluation is appealing since they can be used to compute similarity between words or phrases in the same language (monolingual embed"
C16-1110,W15-3051,0,0.245132,"Missing"
C16-1110,D13-1141,0,0.0832119,"Missing"
C16-1110,W15-1006,0,\N,Missing
D19-5631,D10-1049,0,0.0427602,"still an open challenge (e.g., table records). Existing models lack accuracy, coherence, or adequacy to source material (Wiseman et al., 2017). The two aspects which are mostly addressed in data-to-text generation techniques are identifying the most important information from input data, and verbalizing data as a coherent document: “What to talk about and how?” (Mei et al., 2016). These two challenges have been addressed separately as different modules in pipeline systems (McKeown, 1985; Reiter and Dale, 2000) or in an end-to-end manner with PCFGs or SMTlike approaches (Mooney and Wong, 2007; Angeli et al., 2010; Konstas and Lapata, 2013), or more recently, with neural generation models (Wiseman et al., 2017; Lebret et al., 2016; Mei et al., 2016). In spite of generating fluent text, end-to-end neural generation models perform weakly in terms of best content selection (Wiseman et al., 2017). Recently, Puduppully et al. (2019) trained an endto-end data-to-document generation model on the Rotowire dataset (English summaries of basketball games with structured data).1 They aimed to overcome the shortcomings of end-to-end neural NLG models by explicitly modeling content selection and planning in their ar"
D19-5631,P16-1162,0,0.0240471,"ng sections. 3.1 Machine Translation Track For the MT track, we followed these steps: 274 4 Experiments was not to increase translation quality, but to allow us to use the same model for MT and NLG, which is easier to do at the document level. 3.2 4.1 Data Pre-processing We filter the WMT19-sent parallel corpus with langid.py (Lui and Baldwin, 2012) and remove sentences of more than 175 tokens or with a length ratio greater than 1.5. Then, we apply the official DGT tokenizer (based on NLTK’s word_tokenize) to the non-tokenized text (everything but DGT and Rotowire). We apply BPE segmentation (Sennrich et al., 2016) with a joined SentencePiece-like model (Kudo and Richardson, 2018), with 32k merge operations, obtained on WMT + DGT-train (English + German). The vocabulary threshold is set to 100 and inline casing is applied (Berard et al., 2019). We employ the same joined BPE model and Fairseq dictionary for all models. The metadata is translated into the source language of the MT model used for initialization,3 and segmented into BPE (except for the special tokens) to allow transfer between MT and NLG. Then, we add a corpus tag to each source sequence, which specifies its origin (Rotowire, News-crawl, et"
D19-5631,D18-1045,0,0.0248435,"best checkpoint according to both scores was generally the same. The same observations apply when fine-tuning on NLG or MT+NLG data in the next sections. Like Berard et al. (2019), all our MT models use corpus tags: each source sentence starts with a special token which identifies the corpus it comes from (e.g., Paracrawl, Rotowire, News-crawl). At test time, we use the DGT tag. One thing to note, is that document-level decoding is much slower than its sentence-level counterpart.2 The goal of this document-level fine-tuning 2. Back-translate (BT) the German and English News-crawl by sampling (Edunov et al., 2018). 2 On a single V100, sent-level DGT-valid takes 1 minute to translate, while doc-level DGT-valid takes 6 minutes. 2 Document-Level Generation and Translation Task The goal of the Document-Level Generation and Translation (DGT) task is to generate summaries of basketball games, in two languages (English and German), by using either structured data about the game, a game summary in the other language, or a combination of both. The task features 3 tracks, times 2 target languages (English or German): NLG (Data to Text), MT (Text to Text) and MT+NLG (Text + Data to Text). The data and evaluation"
D19-5631,D17-1239,0,0.01904,"te-of-the-art NMT systems have reported impressive performance on several languages, there are still many challenges in this field especially when context is considered. Currently, the majority of NMT models translate sentences independently, without access to a larger context (e.g., other sentences from the same document or structured information). Additionally, despite improvements in text generation, generating long descriptive summaries conditioned on structured data is still an open challenge (e.g., table records). Existing models lack accuracy, coherence, or adequacy to source material (Wiseman et al., 2017). The two aspects which are mostly addressed in data-to-text generation techniques are identifying the most important information from input data, and verbalizing data as a coherent document: “What to talk about and how?” (Mei et al., 2016). These two challenges have been addressed separately as different modules in pipeline systems (McKeown, 1985; Reiter and Dale, 2000) or in an end-to-end manner with PCFGs or SMTlike approaches (Mooney and Wong, 2007; Angeli et al., 2010; Konstas and Lapata, 2013), or more recently, with neural generation models (Wiseman et al., 2017; Lebret et al., 2016; Me"
D19-5631,W19-5321,0,0.149095,"E Rotowire WMT19-sent WMT19-doc News-crawl EN Split train valid test train valid test EN-DE train EN DE train Docs 242 240 241 3398 727 728 – 68.4k 14.6M 25.1M Sents 3247 3321 3248 45.5k 9.9k 10.0k 28.5M 3.63M 420M 534M 3. Re-train sentence-level MT models on a concatenation of the WMT19 parallel data, DGTtrain and BT. The later was split into 20 parts, one part for each training epoch. This is almost equivalent to oversampling the non-BT data by 20 and doing a single epoch of training. 4. Fine-tune the best sentence-level checkpoint (according to valid perplexity) on documentlevel data. Like Junczys-Dowmunt (2019), we truncated the WMT documents into sequences of maximum 1100 BPE tokens. We also aggregated random sentences from WMT-sent into documents, and upsampled the DGT-train data. Contrary to Junczys-Dowmunt (2019), we do not use any sentence separator or document boundary tags. Table 1: Statistics of the allowed resources. The English sides of DGT-train, valid and test are respectively subsets of Rotowire-train, valid and test. More monolingual data is available, but we only used Rotowire and News-crawl. enough information. We propose a compact way to encode the data available in the original dat"
D19-5631,D18-2012,0,0.0310431,"followed these steps: 274 4 Experiments was not to increase translation quality, but to allow us to use the same model for MT and NLG, which is easier to do at the document level. 3.2 4.1 Data Pre-processing We filter the WMT19-sent parallel corpus with langid.py (Lui and Baldwin, 2012) and remove sentences of more than 175 tokens or with a length ratio greater than 1.5. Then, we apply the official DGT tokenizer (based on NLTK’s word_tokenize) to the non-tokenized text (everything but DGT and Rotowire). We apply BPE segmentation (Sennrich et al., 2016) with a joined SentencePiece-like model (Kudo and Richardson, 2018), with 32k merge operations, obtained on WMT + DGT-train (English + German). The vocabulary threshold is set to 100 and inline casing is applied (Berard et al., 2019). We employ the same joined BPE model and Fairseq dictionary for all models. The metadata is translated into the source language of the MT model used for initialization,3 and segmented into BPE (except for the special tokens) to allow transfer between MT and NLG. Then, we add a corpus tag to each source sequence, which specifies its origin (Rotowire, News-crawl, etc.) Like Junczys-Dowmunt (2019), we split WMT19 documents that are"
D19-5631,D16-1128,0,0.0159904,"(Wiseman et al., 2017). The two aspects which are mostly addressed in data-to-text generation techniques are identifying the most important information from input data, and verbalizing data as a coherent document: “What to talk about and how?” (Mei et al., 2016). These two challenges have been addressed separately as different modules in pipeline systems (McKeown, 1985; Reiter and Dale, 2000) or in an end-to-end manner with PCFGs or SMTlike approaches (Mooney and Wong, 2007; Angeli et al., 2010; Konstas and Lapata, 2013), or more recently, with neural generation models (Wiseman et al., 2017; Lebret et al., 2016; Mei et al., 2016). In spite of generating fluent text, end-to-end neural generation models perform weakly in terms of best content selection (Wiseman et al., 2017). Recently, Puduppully et al. (2019) trained an endto-end data-to-document generation model on the Rotowire dataset (English summaries of basketball games with structured data).1 They aimed to overcome the shortcomings of end-to-end neural NLG models by explicitly modeling content selection and planning in their architecture. We suggest in this paper to leverage the data from both MT and NLG tasks with transfer learning. As both ta"
D19-5631,P12-3005,0,0.0435472,"ocuments, rather than sentence-based outputs. Table 1 describes the allowed parallel and monolingual corpora. 3 Our MT and NLG Approaches All our models (MT, NLG, MT+NLG) are based on Transformer Big (Vaswani et al., 2017). Details for each track are given in the following sections. 3.1 Machine Translation Track For the MT track, we followed these steps: 274 4 Experiments was not to increase translation quality, but to allow us to use the same model for MT and NLG, which is easier to do at the document level. 3.2 4.1 Data Pre-processing We filter the WMT19-sent parallel corpus with langid.py (Lui and Baldwin, 2012) and remove sentences of more than 175 tokens or with a length ratio greater than 1.5. Then, we apply the official DGT tokenizer (based on NLTK’s word_tokenize) to the non-tokenized text (everything but DGT and Rotowire). We apply BPE segmentation (Sennrich et al., 2016) with a joined SentencePiece-like model (Kudo and Richardson, 2018), with 32k merge operations, obtained on WMT + DGT-train (English + German). The vocabulary threshold is set to 100 and inline casing is applied (Berard et al., 2019). We employ the same joined BPE model and Fairseq dictionary for all models. The metadata is tra"
D19-5631,N16-1086,0,0.02136,"ithout access to a larger context (e.g., other sentences from the same document or structured information). Additionally, despite improvements in text generation, generating long descriptive summaries conditioned on structured data is still an open challenge (e.g., table records). Existing models lack accuracy, coherence, or adequacy to source material (Wiseman et al., 2017). The two aspects which are mostly addressed in data-to-text generation techniques are identifying the most important information from input data, and verbalizing data as a coherent document: “What to talk about and how?” (Mei et al., 2016). These two challenges have been addressed separately as different modules in pipeline systems (McKeown, 1985; Reiter and Dale, 2000) or in an end-to-end manner with PCFGs or SMTlike approaches (Mooney and Wong, 2007; Angeli et al., 2010; Konstas and Lapata, 2013), or more recently, with neural generation models (Wiseman et al., 2017; Lebret et al., 2016; Mei et al., 2016). In spite of generating fluent text, end-to-end neural generation models perform weakly in terms of best content selection (Wiseman et al., 2017). Recently, Puduppully et al. (2019) trained an endto-end data-to-document gene"
D19-5631,N07-1022,0,0.0604948,"on structured data is still an open challenge (e.g., table records). Existing models lack accuracy, coherence, or adequacy to source material (Wiseman et al., 2017). The two aspects which are mostly addressed in data-to-text generation techniques are identifying the most important information from input data, and verbalizing data as a coherent document: “What to talk about and how?” (Mei et al., 2016). These two challenges have been addressed separately as different modules in pipeline systems (McKeown, 1985; Reiter and Dale, 2000) or in an end-to-end manner with PCFGs or SMTlike approaches (Mooney and Wong, 2007; Angeli et al., 2010; Konstas and Lapata, 2013), or more recently, with neural generation models (Wiseman et al., 2017; Lebret et al., 2016; Mei et al., 2016). In spite of generating fluent text, end-to-end neural generation models perform weakly in terms of best content selection (Wiseman et al., 2017). Recently, Puduppully et al. (2019) trained an endto-end data-to-document generation model on the Rotowire dataset (English summaries of basketball games with structured data).1 They aimed to overcome the shortcomings of end-to-end neural NLG models by explicitly modeling content selection and"
D19-5631,W19-5333,0,0.0478495,"Missing"
D19-5631,W18-6301,0,0.0340628,"For instance, the home team is always first, but a &lt;WINNER&gt; tag precedes the winning team and its players. We ignore all-zero statistics, but always use the same position for each type of score (e.g., points, then rebounds, then assists) and special tokens to help identify them (e.g., &lt;PTS&gt; 16 and &lt;REB&gt; 8). We try to limit the number of tags to keep the sequences short (e.g., made and attempted free throws and percentage: &lt;FT&gt; 3 5 60). An example of metadata representation is shown in Table 2. 3.3 4.2 Settings All the models are Transformer Big (Vaswani et al., 2017), implemented in Fairseq (Ott et al., 2018). We use the same hyper-parameters as Ott et al. (2018), with Adam and an inverse square root schedule with warmup (maximum LR 0.0005). We apply dropout and label smoothing with a rate of 0.1. The source and target embeddings are shared and tied with the last layer. We train with halfprecision floats on 8 V100 GPUs, with at most 3500 tokens per batch and delayed updates of 10 MT+NLG Track For the MT+NLG track, we concatenate the MT source with the NLG data. We use the same metadata encoding method as in the NLG track and we fine-tune our doc-level MT models (from step 4). We also randomly mask"
E17-2066,L16-1662,1,0.940445,"ulated as follows: The main idea of word embeddings is that their representation is obtained according to the context (the words around it). The words are projected on a continuous space and those with similar context should be close in this multi-dimensional space. A similarity between two word vectors can be measured by cosine similarity. So using wordembeddings for plagiarism detection is appealing since they can be used to calculate similarity between sentences in the same or in two different languages (they capture intrinsically synonymy and morphological closeness). We use the MultiVec (Berard et al., 2016) toolkit for computing and managing the continuous representations of the texts. It includes word2vec (Mikolov et al., 2013), paragraph vector (Le and Mikolov, 2014) and bilingual distributed representations (Luong et al., 2015) features. The corpus used to build the vectors is the News Commentary2 parallel corpus. For training our embeddings, we use CBOW model with a vector size of 100, a window size of 5, a negative sampling parameter of 5, and an alpha of 0.02. 3.1 V = (vector(ui )) (2) i=1 where ui is the ith word of the textual unit and vector is the function which gives the word embeddin"
E17-2066,D14-1082,0,0.0897818,"Missing"
E17-2066,L16-1657,1,0.604763,"Missing"
E17-2066,petrov-etal-2012-universal,0,0.162602,"Missing"
E17-2066,L16-1046,0,0.0676319,"Missing"
E17-2066,P16-1157,0,0.0576904,"Missing"
E17-2066,W15-1521,0,\N,Missing
F12-1080,N10-1024,0,0.059358,"Missing"
F12-1098,devillers-etal-2004-french,0,0.0831867,"Missing"
F12-1098,esteve-etal-2010-epac,1,0.884492,"Missing"
F12-1098,gravier-etal-2004-ester,0,0.0432159,"Missing"
F12-1098,W11-0146,1,0.886156,"Missing"
F12-1098,W11-2039,1,0.883942,"Missing"
F13-1007,J93-2003,0,0.033471,"Missing"
F13-1007,P07-2045,0,0.0217044,"Missing"
F13-1007,N03-1017,0,0.0159377,"Missing"
F13-1007,N03-1019,0,0.0325826,"Missing"
F13-1007,P10-1052,0,0.030939,"Missing"
F13-1007,W11-2168,0,0.0317713,"Missing"
F13-1007,N06-1014,0,0.0304626,"Missing"
F13-1007,J06-4004,0,0.0556549,"Missing"
F13-1007,P03-1021,0,0.019766,"Missing"
F13-1007,P02-1038,0,0.31959,"Missing"
F13-1007,P02-1040,0,0.0847519,"Missing"
F13-1007,N09-3016,0,0.0258671,"Missing"
F13-1007,W95-0107,0,0.0122469,"Missing"
F13-2004,2012.iwslt-evaluation.13,1,0.876458,"Missing"
F13-2004,W08-0509,0,0.0378081,"Missing"
F13-2004,2008.amta-srw.3,0,0.0438155,"Missing"
F13-2004,J10-4005,0,0.0591795,"Missing"
F13-2004,P07-2045,0,0.00959479,"Missing"
F13-2004,P09-1066,0,0.0350692,"Missing"
F13-2004,C04-1072,0,0.0295495,"Missing"
F13-2004,P03-1021,0,0.087008,"Missing"
F13-2004,J03-1002,0,0.0103436,"Missing"
F13-2004,W11-2154,1,0.838748,"Missing"
F13-2004,N07-1029,0,0.0571615,"Missing"
F13-2004,P07-1040,0,0.0504359,"Missing"
F14-2001,P07-2045,0,0.00359908,"Missing"
F14-2001,huynh-etal-2008-sectra,0,0.0421038,"Missing"
F14-2001,C12-1135,0,0.0383631,"Missing"
F14-2001,potet-etal-2012-collection,1,0.881671,"Missing"
F14-2001,specia-etal-2010-dataset,0,0.0604818,"Missing"
K18-1010,D13-1176,0,0.0585156,"se an alternative approach which instead relies on a single 2D convolutional neural network across both sequences. Each layer of our network recodes source tokens on the basis of the output sequence produced so far. Attention-like properties are therefore pervasive throughout the network. Our model yields excellent results, outperforming state-of-the-art encoderdecoder systems, while being conceptually simpler and having fewer parameters. 1 Introduction Deep neural networks have made a profound impact on natural language processing technology in general, and machine translation in particular (Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Jean et al., 2015; LeCun et al., 2015). Machine translation (MT) can be seen as a sequenceto-sequence prediction problem, where the source and target sequences are of different and variable length. Current state-of-the-art approaches are based on encoder-decoder architectures (Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015). The encoder “reads” the variable-length source sequence and maps it into a vector representation. The decoder takes this vector as input and “writes” the target sequence, updating its state each st"
K18-1010,D14-1179,0,0.0408573,"Missing"
K18-1010,D15-1166,0,0.684621,"t or aggregate features of these elements into a single “context” vector that is used by the decoder. Rather than relying on the global representation of the source sequence, the attention mechanism allows the decoder to “look back” into the source sequence and focus on salient positions. Besides this inductive bias, the attention mechanism bypasses the problem of vanishing gradients that most recurrent architectures encounter. However, the current attention mechanisms have limited modeling abilities and are generally a simple weighted sum of the source representations (Bahdanau et al., 2015; Luong et al., 2015), where the weights are the result of a shallow matching between source and target elements. The attention module re-combines the same source token codes and is unable to re-encode or re-interpret the source sequence while decoding. To address these limitations, we propose an alternative neural MT architecture, based on deep 2D convolutional neural networks (CNNs). The product space of the positions in source and target sequences defines the 2D grid over which the network is defined. The convolutional filters are masked to prohibit accessing information derived from future tokens in the target"
K18-1010,N18-1033,0,0.12436,"ut tensor. This reduces the number of parameters and generally improves the performance. Experimental evaluation In this section, we present our experimental setup, followed by quantitative results, qualitative examples of implicit sentence alignments from our model, and a comparison to the state of the art. 100 4.1 Experimental setup Data and pre-processing. We experiment with the IWSLT 2014 bilingual dataset (Cettolo et al., 2014), which contains transcripts of TED talks aligned at sentence level, and translate between German (De) and English (En) in both directions. Following the setup of (Edunov et al., 2018), sentences longer than 175 tokens and pairs with length ratio exceeding 1.5 were removed from the original data. There are 160+7K training sentence pairs, 7K of which are separated and used for validation/development. We report results on a test set of 6,578 pairs obtained by concatenating dev2010 and tst2010-2013. We tokenized and lowercased all data using the standard scripts from the Moses toolkit (Koehn et al., 2007). For open-vocabulary translation, we segment sequences using joint byte pair encoding (Sennrich et al., 2016) with 14K merge operations on the concatenation of source and tar"
K18-1010,P15-1003,0,0.0223038,"word vectors, applying 1D convolutions then aggregating with a max-pooling operator over time (Collobert and Weston, 2008; Kalchbrenner et al., 2014; Kim, 2014). For sequence generation, the works of Ranzato et al. (2016); Bahdanau et al. (2017); Gehring et al. (2017a) mix a convolutional encoder with an RNN decoder. The first entirely convolutional encoder-decoder models where introduced by Kalchbrenner et al. (2016b), but they did not improve over state-of-the-art recurrent architectures. Gehring et al. (2017b) outperformed deep LSTMs for machine translation 1D CNNs with gated linear units (Meng et al., 2015; Oord et al., 2016c; Dauphin et al., 2017) in both the encoder and decoder modules. told Charlie that Bob told Alice &lt;start> Target sequence Alice Source sequence a dit a ` Bob que Charlie Figure 1: Convolutional layers in our model use masked 3×3 filters so that features are only computed from previous output symbols. Illustration of the receptive fields after one (dark blue) and two layers (light blue), together with the masked part of the field of view of a normal 3×3 filter (gray). Such CNN-based models differ from their RNN-based counterparts in that temporal connections are placed betwe"
K18-1010,P17-1012,0,0.176044,"time whilst updating its state. While best known for their use in visual recognition models, (Oord et al., 2016a; Salimans et al., 2017; Reed et al., 2017; Oord et al., 2016c). Recent works also introduced convolutional networks to natural language processing. The first convolutional apporaches to encoding variablelength sequences consist of stacking word vectors, applying 1D convolutions then aggregating with a max-pooling operator over time (Collobert and Weston, 2008; Kalchbrenner et al., 2014; Kim, 2014). For sequence generation, the works of Ranzato et al. (2016); Bahdanau et al. (2017); Gehring et al. (2017a) mix a convolutional encoder with an RNN decoder. The first entirely convolutional encoder-decoder models where introduced by Kalchbrenner et al. (2016b), but they did not improve over state-of-the-art recurrent architectures. Gehring et al. (2017b) outperformed deep LSTMs for machine translation 1D CNNs with gated linear units (Meng et al., 2015; Oord et al., 2016c; Dauphin et al., 2017) in both the encoder and decoder modules. told Charlie that Bob told Alice &lt;start> Target sequence Alice Source sequence a dit a ` Bob que Charlie Figure 1: Convolutional layers in our model use masked 3×3 f"
K18-1010,D18-1549,0,0.032815,"ut and output respectively. Horizontal connections are used for RNNs, diagonal connections for convolutional networks. Vertical connections are used in both cases. Parameters are shared across time-steps (horizontally), but not across layers (vertically). Xij = [yi xj ]. (1) This joint unigram encoding is the input to our convolutional network. tioning on the current decoder state. Vaswani et al. (2017) propose an architecture relying entirely on attention. Positional input coding together with self-attention (Parikh et al., 2016; Lin et al., 2017) replaces recurrent and convolutional layers. Huang et al. (2018) use an attentionlike gating mechanism to alleviate an assumption of monotonic alignment in the phrase-based translation model of Wang et al. (2017). Deng et al. (2018) treat the sentence alignment as a latent variable which they infer using a variational inference network during training to optimize a variational lower-bound on the log-likelihood. Convolutional layers. We use the DenseNet (Huang et al., 2017) convolutional architecture, which is the state of the art for image classification tasks. Layers are densely connected, meaning that each layer takes as input the activations of all the"
K18-1010,P02-1040,0,0.101082,"× 3, as given by the light blue area in Figure 1. To train our models, we use maximum likelihood estimation (MLE) with Adam (β1 = 0.9, β2 = 0.999,  = 1e−8 ) starting with a learning rate of 5e−4 that we scale by a factor of 0.8 if no improvement (δ ≤ 0.01) is noticed on the validation loss after three evaluations, we evaluate every 8K updates. After training all models up to 40 epochs, the best performing model on the validation set is used for decoding the test set. We use a beam-search of width 5 without any length or coverage penalty and measure translation quality using the BLEU metric (Papineni et al., 2002). Baselines. For comparison with state-of-theart architectures, we implemented a bidirectional LSTM encoder-decoder model with dotproduct attention (Bahdanau et al., 2015; Luong et al., 2015) using PyTorch (Paszke et al., 2017), and used Facebook AI Research Sequence-toSequence Toolkit (Gehring et al., 2017b) to train the ConvS2S and Transformer (Vaswani et al., 2017) models on our data. For the Bi-LSTM encoder-decoder, the encoder is a single layer bidirectional LSTM with input embeddings of size 128 and a hidden state of size 4.2 Experimental results Architecture evaluation. In this section"
K18-1010,D16-1244,0,0.0423356,"coder network topology with two hidden layers, nodes at bottom and top represent input and output respectively. Horizontal connections are used for RNNs, diagonal connections for convolutional networks. Vertical connections are used in both cases. Parameters are shared across time-steps (horizontally), but not across layers (vertically). Xij = [yi xj ]. (1) This joint unigram encoding is the input to our convolutional network. tioning on the current decoder state. Vaswani et al. (2017) propose an architecture relying entirely on attention. Positional input coding together with self-attention (Parikh et al., 2016; Lin et al., 2017) replaces recurrent and convolutional layers. Huang et al. (2018) use an attentionlike gating mechanism to alleviate an assumption of monotonic alignment in the phrase-based translation model of Wang et al. (2017). Deng et al. (2018) treat the sentence alignment as a latent variable which they infer using a variational inference network during training to optimize a variational lower-bound on the log-likelihood. Convolutional layers. We use the DenseNet (Huang et al., 2017) convolutional architecture, which is the state of the art for image classification tasks. Layers are d"
K18-1010,P15-1001,0,0.0234475,"le 2D convolutional neural network across both sequences. Each layer of our network recodes source tokens on the basis of the output sequence produced so far. Attention-like properties are therefore pervasive throughout the network. Our model yields excellent results, outperforming state-of-the-art encoderdecoder systems, while being conceptually simpler and having fewer parameters. 1 Introduction Deep neural networks have made a profound impact on natural language processing technology in general, and machine translation in particular (Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Jean et al., 2015; LeCun et al., 2015). Machine translation (MT) can be seen as a sequenceto-sequence prediction problem, where the source and target sequences are of different and variable length. Current state-of-the-art approaches are based on encoder-decoder architectures (Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015). The encoder “reads” the variable-length source sequence and maps it into a vector representation. The decoder takes this vector as input and “writes” the target sequence, updating its state each step with the most recent word that it generated. The basic enc"
K18-1010,J82-2005,0,0.768019,"Missing"
K18-1010,P14-1062,0,0.0458326,"), pages 97–107 c Brussels, Belgium, October 31 - November 1, 2018. 2018 Association for Computational Linguistics ken at a time whilst updating its state. While best known for their use in visual recognition models, (Oord et al., 2016a; Salimans et al., 2017; Reed et al., 2017; Oord et al., 2016c). Recent works also introduced convolutional networks to natural language processing. The first convolutional apporaches to encoding variablelength sequences consist of stacking word vectors, applying 1D convolutions then aggregating with a max-pooling operator over time (Collobert and Weston, 2008; Kalchbrenner et al., 2014; Kim, 2014). For sequence generation, the works of Ranzato et al. (2016); Bahdanau et al. (2017); Gehring et al. (2017a) mix a convolutional encoder with an RNN decoder. The first entirely convolutional encoder-decoder models where introduced by Kalchbrenner et al. (2016b), but they did not improve over state-of-the-art recurrent architectures. Gehring et al. (2017b) outperformed deep LSTMs for machine translation 1D CNNs with gated linear units (Meng et al., 2015; Oord et al., 2016c; Dauphin et al., 2017) in both the encoder and decoder modules. told Charlie that Bob told Alice &lt;start> Targe"
K18-1010,D14-1181,0,0.00342188,"Belgium, October 31 - November 1, 2018. 2018 Association for Computational Linguistics ken at a time whilst updating its state. While best known for their use in visual recognition models, (Oord et al., 2016a; Salimans et al., 2017; Reed et al., 2017; Oord et al., 2016c). Recent works also introduced convolutional networks to natural language processing. The first convolutional apporaches to encoding variablelength sequences consist of stacking word vectors, applying 1D convolutions then aggregating with a max-pooling operator over time (Collobert and Weston, 2008; Kalchbrenner et al., 2014; Kim, 2014). For sequence generation, the works of Ranzato et al. (2016); Bahdanau et al. (2017); Gehring et al. (2017a) mix a convolutional encoder with an RNN decoder. The first entirely convolutional encoder-decoder models where introduced by Kalchbrenner et al. (2016b), but they did not improve over state-of-the-art recurrent architectures. Gehring et al. (2017b) outperformed deep LSTMs for machine translation 1D CNNs with gated linear units (Meng et al., 2015; Oord et al., 2016c; Dauphin et al., 2017) in both the encoder and decoder modules. told Charlie that Bob told Alice &lt;start> Target sequence A"
K18-1010,P07-2045,0,0.00849664,"al., 2014), which contains transcripts of TED talks aligned at sentence level, and translate between German (De) and English (En) in both directions. Following the setup of (Edunov et al., 2018), sentences longer than 175 tokens and pairs with length ratio exceeding 1.5 were removed from the original data. There are 160+7K training sentence pairs, 7K of which are separated and used for validation/development. We report results on a test set of 6,578 pairs obtained by concatenating dev2010 and tst2010-2013. We tokenized and lowercased all data using the standard scripts from the Moses toolkit (Koehn et al., 2007). For open-vocabulary translation, we segment sequences using joint byte pair encoding (Sennrich et al., 2016) with 14K merge operations on the concatenation of source and target languages. This results in a German and English vocabularies of around 12K and 9K types respectively. Model BLEU Flops×105 #params Average Max Attn Max, gated [Max, Attn] 31.57 ± 0.11 33.70 ± 0.06 32.07 ± 0.13 33.66 ± 0.16 33.81 ± 0.03 3.63 3.44 3.61 3.49 3.51 7.18M 7.18M 7.24M 9.64M 7.24M Table 1: Our model (L = 24, g = 32, ds = dt = 128) with different pooling operators and using gated convolutional units. 256 (128"
K18-1010,P16-1162,0,0.0624498,"(De) and English (En) in both directions. Following the setup of (Edunov et al., 2018), sentences longer than 175 tokens and pairs with length ratio exceeding 1.5 were removed from the original data. There are 160+7K training sentence pairs, 7K of which are separated and used for validation/development. We report results on a test set of 6,578 pairs obtained by concatenating dev2010 and tst2010-2013. We tokenized and lowercased all data using the standard scripts from the Moses toolkit (Koehn et al., 2007). For open-vocabulary translation, we segment sequences using joint byte pair encoding (Sennrich et al., 2016) with 14K merge operations on the concatenation of source and target languages. This results in a German and English vocabularies of around 12K and 9K types respectively. Model BLEU Flops×105 #params Average Max Attn Max, gated [Max, Attn] 31.57 ± 0.11 33.70 ± 0.06 32.07 ± 0.13 33.66 ± 0.16 33.81 ± 0.03 3.63 3.44 3.61 3.49 3.51 7.18M 7.18M 7.24M 9.64M 7.24M Table 1: Our model (L = 24, g = 32, ds = dt = 128) with different pooling operators and using gated convolutional units. 256 (128 in each direction). The decoder is a single layer LSTM with similar input size and a hidden size of 256, the t"
K19-1032,K17-1037,0,0.0373554,"Missing"
K19-1032,D14-1179,0,0.0287739,"Missing"
K19-1032,P17-1057,0,0.34838,"y are able to incorporate multiple modalities in a single network and allow the analysis of complex interactions between them. Analysing these models does not only help to understand their technological limitations, but may also yield insight on the cognitive processes at work in humans (Dupoux, 2018) who learn from contextually grounded speech utterances (either visually, haptically, socially, etc.). This is with this idea in mind that one of the first computational model of visually grounded word acquisition was introduced by Roy and Pentland (2002). More recently, Harwath et al. (2016) and Chrupała et al. (2017) were among the first to propose neural models integrating these two modalities. 2 Related Work In this section we explore what is known about word recognition in humans. We then review recent works related to the representation of lan339 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 339–348 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics is another model which builds upon COHORT and TRACE by taking into consideration other features such as word stress.1 To sum up, models of spoken word recognition consider that a se"
K19-1032,J17-4003,0,\N,Missing
L16-1226,piperidis-2012-meta,0,0.0307233,"at were not explicitly labeled. 4. Conclusion The purpose of the CAMOMILE project has been to explore new practices around collaborative annotation and test it on specific use cases with dedicated prototypes. The developed framework can be summarized as a remote repository of annotations which are metadata attached to fragments of the media from a corpus, along with the associated RESTful API. It is thus compatible with other abstraction layers, e.g., annotation graphs (Bird and Liberman, 2001), and the metadata can follow standards 1424 in the domain as proposed in the META-SHARE initiative (Piperidis, 2012)7 . This simple framework was robust enough to support the active learning scenario described in this paper, as long as the organization of a MediaEval task with 20 annotators involved (73426 annotations) (Poignant et al., 2016). Its source code is freely available on GitHub. Further developments would improve the platform, like a direct communication between the clients through WebSockets or a flexible historization of the annotations. 5. Acknowledgements We thank the members of the CAMOMILE international advisory committee for their time and their precious advices and proposals. This work wa"
L16-1226,auer-etal-2010-elan,0,0.0761345,"Missing"
L16-1226,giraudel-etal-2012-repere,0,0.0770122,"Missing"
L16-1611,W14-2201,0,0.0266605,"Missing"
L16-1611,W02-0603,0,0.0257462,"Missing"
L16-1657,C10-1005,0,0.0571791,"Missing"
L16-1657,F13-2001,0,0.0107288,"ch document in the corpus. Then, using this information, parallel English-Spanish pairs are extracted. The process led to nearly 3,000 document pairs. • Conference papers. So far, no corpus includes scientific texts, this is why we collected conference papers that were initially published in one language and then translated by their authors to be published in another language. For practical reasons, we focused exclusively on articles published first in French and then in English. The BibTeX file of French speaking conferences in NLP (the 1997-2014 TALN archives, made available in the works of Boudin (2013)6 and the 2006-2011 RNTI collection made available by the challenge of the EGC 2016 conference7 ) were parsed to extract the names of the authors of each article. Then, names were used as queries in Google Scholar and Google Search Engine. Papers in PDF format corresponding to the most relevant search results were downloaded. We detected the language of each downloaded file according to the Cavnar and Trenkle (1994) classification algorithm and each English candidate file was manually checked to see if a significant part of it was related to one of the French original documents cited in the Bi"
L16-1657,J93-2003,0,0.0530759,"in( |Vx ∩ Vy |− |Vx  Vy |, |Vy ∩ Vx |− |Vy  Vx |) (4) 2.4. Cross-Language Alignment-based Similarity Analysis (CL-ASA) CL-ASA is introduced for the first time by Barr´on-Cede˜no et al. (2008) and developed subsequently by Pinto et al. (2009). The aim of the method is to determinate how a textual unit d written in the language L is potentially the translation of an other textual unit d0 written in a language L0 . CL-ASA involves the creation of a bilingual unigram dictionary which contains the statistical probabilities of translations pairs determined from a parallel corpus. The IBM-1 model (Brown et al., 1993) can be adopted using only the lexical translations. Pinto et al. (2009) proposed a formula that factored the alignment function. Let x and y, two sentences, such as xj is the j th word of the sentence x and yi , the ith word of the sentence y. We want to know the probability p(x, y) that x is the translation of y. p(x, y) = |x| Y p(xj |y) (5) 1 p(xj |yi ) |y |+ 1 (6) j=1 where p(xj |y) = |y| X i=1 Improvements of the method were later proposed. For example, consider for each word x, only the best translations y, above a minimum probability (threshold of 0.4 according to the work of Barr´on-Ce"
L16-1657,2012.eamt-1.60,0,0.0843742,"Missing"
L16-1657,2005.mtsummit-papers.11,0,0.0111405,"w better results on monolingual textual comparisons (Barr´on-Cede˜no et al., 2009). Because machine translation tools can give too multiple translations (all correct but being substantially different) and therefore it is not advisable to make a monolingual alignment with lexical or syntactic methods (Barr´on-Cede˜no et al., 2010). 3. Dataset for the cross-language plagiarism detection task 3.1. Existing corpora There are many multi-language and cross-language dataset listed by OPUS2 website. One example of these most (8) 4164 2 http://opus.lingfil.uu.se/ used corpora is undoubtedly Europarl3 (Koehn, 2005). It is a widely used corpus in cross-language text analysis and machine translation. It is a parallel corpus consisting of the European Parliament exchanges transcriptions, about nearly 10,000 parallel documents in more than 21 languages spoken across the European Union. Similarly, JRC-Acquis4 is also often used in cross-language NLP or translation research. It is a parallel corpus, representing extracts of Acquis Communautaire (applicable laws in the European Union states), available in over 20 languages. As well, Wikipedia is often used as a comparable corpus in multiple languages. These la"
L16-1657,C10-2115,0,0.0905716,"Missing"
L16-1657,P10-1114,0,0.0293547,"00 parallel documents in more than 21 languages spoken across the European Union. Similarly, JRC-Acquis4 is also often used in cross-language NLP or translation research. It is a parallel corpus, representing extracts of Acquis Communautaire (applicable laws in the European Union states), available in over 20 languages. As well, Wikipedia is often used as a comparable corpus in multiple languages. These last two, i.e. JRC-Acquis and Wikipedia, were used by Potthast et al. (2011) for crosslanguage plagiarism detection. Finally, another interesting collection of documents is the one gathered by Prettenhofer and Stein (2010) who collected Amazon Product Reviews (books, DVD and music albums) for a cross-language sentiment analysis task (Google Translate was used to build the parallel corpus). 3.2. books freely available on the Gutenberg Project website5 . The extraction process involves analyzing XML files containing the metadata of each document in the corpus. Then, using this information, parallel English-Spanish pairs are extracted. The process led to nearly 3,000 document pairs. • Conference papers. So far, no corpus includes scientific texts, this is why we collected conference papers that were initially publ"
L16-1662,C12-1089,0,0.0778365,"Missing"
L16-1662,W15-1521,0,0.294005,"evaluated on several NLP tasks: the analogical reasoning task, sentiment analysis, and crosslingual document classification. Keywords: Word embeddings, paragraph vector, bilingual word embeddings, crosslingual document classification 1. Introduction There has been a growing interest in distributed representations for text, largely due to Mikolov et al. [2013a] who propose simple models which can be trained on huge amounts of data. A number of contributions have extended this work to phrases [Mikolov et al., 2013b], text sequences [Le and Mikolov, 2014], bilingual distributed representations [Luong et al., 2015] [Gouws et al., 2015], or bilingual representations for text sequences [Pham et al., 2015]. Although most of these techniques have official or non-official implementations (word2vec, bivec, gensim ˇ uˇrek and Sojka, 2010], etc.), there has been no con[Reh˚ certed effort to regroup all of these techniques in a single toolkit. Contribution This paper presents MultiVec, a toolkit which enables the generation and manipulation of multilingual vector representations at several granularity levels (from word to any sequence of words). MultiVec combines several techniques of the literature: it include"
L16-1662,W15-1512,0,0.0149568,"sslingual document classification. Keywords: Word embeddings, paragraph vector, bilingual word embeddings, crosslingual document classification 1. Introduction There has been a growing interest in distributed representations for text, largely due to Mikolov et al. [2013a] who propose simple models which can be trained on huge amounts of data. A number of contributions have extended this work to phrases [Mikolov et al., 2013b], text sequences [Le and Mikolov, 2014], bilingual distributed representations [Luong et al., 2015] [Gouws et al., 2015], or bilingual representations for text sequences [Pham et al., 2015]. Although most of these techniques have official or non-official implementations (word2vec, bivec, gensim ˇ uˇrek and Sojka, 2010], etc.), there has been no con[Reh˚ certed effort to regroup all of these techniques in a single toolkit. Contribution This paper presents MultiVec, a toolkit which enables the generation and manipulation of multilingual vector representations at several granularity levels (from word to any sequence of words). MultiVec combines several techniques of the literature: it includes most of word2vec’s features [Mikolov et al., 2013a] for learning distributed word repres"
L16-1662,serasset-2012-dbnary,0,0.0318366,"Missing"
L18-1001,W17-0123,0,0.0177293,"ludes this work and gives some perspectives. Attention-based encoder-decoder approaches have been very successful in Machine Translation (Bahdanau et al., 2014), and have shown promising results in Endto-End Speech Translation (B´erard et al., 2016; Weiss et al., 2017) (translation from raw speech, without any intermediate transcription). End-to-End speech translation is also attractive for language documentation, which often uses corpora made of audio recordings aligned with their translation in another language (no transcript in the source language) (Blachon et al., 2016; Adda et al., 2016; Anastasopoulos and Chiang, 2017). However, while large quantities of parallel texts (such as Europarl, OpenSubtitles) are available for training (text) machine translation systems, there are no large (>100h) and open source parallel corpora that include speech in a source language aligned to text in a target language. For End-to-End speech translation, only a few parallel corpora are publicly available. For example, Fisher and Callhome Spanish-English corpora provide 38 hours of speech transcriptions of telephonic conversations aligned with their translations (Post et al., 2013). However, these corpora are only medium size a"
L18-1001,P06-4018,0,0.0275232,"he quality of sentence level alignments, data had to be pre-processed. For English and The main steps of our process are the following: 3.2. Chapters Extraction 5 http://www.gutenberg.org/ http://www.wikisource.org/ 7 http://gallica.bnf.fr/ 8 http://books.google.com 9 http://beq.ebooksgratuits.com 10 http://www.uqac.ca/ 11 One goal of Librispeech was to have as many speakers as possible 12 https://sourceforge.net/projects/ aligner/ 13 https://polyglotte.tuxfamily.org 6 https://www.noslivres.net/ 2 French, our extracted chapters were cleaned with regular expressions. Then, we used Python NLTK (Bird, 2006) sentence split to detect sentence boundaries in the corpora. Furthermore, the bitexts were stemmed (removing suffixes to reduce data sparsity). Finally, parallel sentences found were brought back to their initial form with reverse stemming. This last step was done using Google’s dif f − patch − match library (Fraser, 2012). English Sentence Oh, I beg your pardon! A lane was forthwith opened through the crowd of spectators. No, ”said Catherine,” he is not here; I cannot see him anywhere. sentence pair, we also added En-Fr machine translation output of our English transcripts (Google Translate)"
L18-1001,L16-1657,1,0.818696,"rases polies et avec la langue de la courtoisie. average score of 3.84/5. Some sentences were found uncorrectly aligned but overall, the alignment quality can be considered as correct. The main reason why the average alignment score varies between chapters is reflected by the translations compositionnality. Also, the dictionary that we used for bilingual alignments is inadequate for old texts and results in lower overall confidence scores. We also computed automatic correspondence scores obtained with a cross-language textual similarity detection between transcriptions and their translations (Ferrero et al., 2016). Our idea was to add another automatic score in addition to hunalign score. We computed the correlation between human evaluation scores and hunalign scores and obtained a correlation of 0.41. The same correlation was obtained between human evaluation scores and those obtained automatically with method of (Ferrero et al., 2016). This shows that automatic alignment scores are reasonably correlated with human judgments and could be used to extract a subset of the best alignments by ranking them according to hunalign score for instance. • 3. Partial alignment with compositional translation and ad"
L18-1001,2005.iwslt-1.19,0,0.138797,"Missing"
L18-1001,2013.iwslt-papers.14,0,0.15034,"et al., 2016; Adda et al., 2016; Anastasopoulos and Chiang, 2017). However, while large quantities of parallel texts (such as Europarl, OpenSubtitles) are available for training (text) machine translation systems, there are no large (>100h) and open source parallel corpora that include speech in a source language aligned to text in a target language. For End-to-End speech translation, only a few parallel corpora are publicly available. For example, Fisher and Callhome Spanish-English corpora provide 38 hours of speech transcriptions of telephonic conversations aligned with their translations (Post et al., 2013). However, these corpora are only medium size and contain low-bandwidth recordings. Microsoft Speech Language Translation (MSLT) corpus also provides speech aligned to translated text. Speech is recorded through Skype for English, German and French (Federmann and Lewis, 2016). But this corpus is again rather small (less than 8h per language). Paper contributions. Our objective is to provide a large corpus for direct speech translation evaluation which is an order of magnitude bigger than existing corpora described in the introduction. For this, we propose to enrich an existing (monolingual) co"
L18-1674,L18-1531,1,0.879738,"Missing"
le-etal-2004-spoken,vaufreydaz-etal-2000-new,0,\N,Missing
lefevre-etal-2012-leveraging,esteve-etal-2010-epac,1,\N,Missing
lefevre-etal-2012-leveraging,W11-0146,1,\N,Missing
lefevre-etal-2012-leveraging,W11-2039,1,\N,Missing
lefevre-etal-2012-leveraging,devillers-etal-2004-french,0,\N,Missing
P18-1195,P15-1162,0,0.0277961,"tivations, it is also possible to directly regularize based on the entropy of the output distribution (Pereyra et al., 2017). Data augmentation techniques improve the robustness of the learned models by applying transformations that might be encountered at test time to the training data. In computer vision, this is common practice, and implemented by, e.g., scaling, cropping, and rotating training images (LeCun et al., 1998; Krizhevsky et al., 2012; Paulin et al., 2014). In natural language processing, examples of data augmentation include input noising by randomly dropping some input tokens (Iyyer et al., 2015; Bowman et al., 2015; Kumar et al., 2016), and randomly replacing words with substitutes sampled from the model (Bengio et al., 2015). Xie et al. (2017) introduced data augmentation schemes for RNN language models that leverage n-gram statistics in order to mimic KneserNey smoothing of n-grams models. In the context of machine translation, Fadaee et al. (2017) modify sentences by replacing words with rare ones when this is plausible according to a pretrained language model, and substitutes its equivalent in the target sentence using automatic word alignments. This approach, however, relies on"
P18-1195,D14-1179,0,0.0447186,"Missing"
P18-1195,W14-3348,0,0.0123904,": image captioning and machine translation. 4.1 4.1.1 Image captioning Experimental setup. We use the MS-COCO datatset (Lin et al., 2014), which consists of 82k training images each annotated with five captions. We use the standard splits of Karpathy and Li (2015), with 5k images for validation, and 5k for test. The test set results are generated via beam search (beam size 3) and are evaluated with the MS-COCO captioning evaluation tool. We report CIDE R and BLEU scores on this internal test set. We also report results obtained on the official MS-COCO server that additionally measures METEOR (Denkowski and Lavie, 2014) and ROUGE-L (Lin, 2004). We experiment with both non-attentive LSTMs (Vinyals et al., 2015) and the ResNet baseline of the stateof-the-art top-down attention (Anderson et al., 2017). The MS-COCO vocabulary consists of 9,800 words that occur at least 5 times in the training set. Additional details and hyperparameters can 4.1.2 Results and discussion Restricted vocabulary sampling In this section, we evaluate the impact of the vocabulary subset from which we sample the modified sentences for sequence-level smoothing. We experiment with two rewards: CIDE R , which scores w.r.t. all five availabl"
P18-1195,P17-2090,0,0.0332627,"caling, cropping, and rotating training images (LeCun et al., 1998; Krizhevsky et al., 2012; Paulin et al., 2014). In natural language processing, examples of data augmentation include input noising by randomly dropping some input tokens (Iyyer et al., 2015; Bowman et al., 2015; Kumar et al., 2016), and randomly replacing words with substitutes sampled from the model (Bengio et al., 2015). Xie et al. (2017) introduced data augmentation schemes for RNN language models that leverage n-gram statistics in order to mimic KneserNey smoothing of n-grams models. In the context of machine translation, Fadaee et al. (2017) modify sentences by replacing words with rare ones when this is plausible according to a pretrained language model, and substitutes its equivalent in the target sentence using automatic word alignments. This approach, however, relies on the availability of additional monolingual data for language model training. The de facto standard way to train RNN language models is maximum likelihood estimation (MLE) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015). The sequential factorization of the sequence likelihood generates an additive structure in the loss, with one term correspon"
P18-1195,W04-1013,0,0.0195264,"on. 4.1 4.1.1 Image captioning Experimental setup. We use the MS-COCO datatset (Lin et al., 2014), which consists of 82k training images each annotated with five captions. We use the standard splits of Karpathy and Li (2015), with 5k images for validation, and 5k for test. The test set results are generated via beam search (beam size 3) and are evaluated with the MS-COCO captioning evaluation tool. We report CIDE R and BLEU scores on this internal test set. We also report results obtained on the official MS-COCO server that additionally measures METEOR (Denkowski and Lavie, 2014) and ROUGE-L (Lin, 2004). We experiment with both non-attentive LSTMs (Vinyals et al., 2015) and the ResNet baseline of the stateof-the-art top-down attention (Anderson et al., 2017). The MS-COCO vocabulary consists of 9,800 words that occur at least 5 times in the training set. Additional details and hyperparameters can 4.1.2 Results and discussion Restricted vocabulary sampling In this section, we evaluate the impact of the vocabulary subset from which we sample the modified sentences for sequence-level smoothing. We experiment with two rewards: CIDE R , which scores w.r.t. all five available reference sentences, a"
P18-1195,P02-1040,0,0.101839,"given by θ, and x is a source sentence or an image in the contexts of machine translation and image captioning, respectively. In a recurrent neural network, the sequence y is predicted based on a sequence of states ht , The reward augmented maximum likelihood approach of Norouzi et al. (2016) consists in replacing the sequence-level Dirac δy∗ in Eq. (6) with a distribution pθ (yt |x, y<t ) = pθ (yt |ht ), where r(y, y ∗ ) is a “reward” function that measures the quality of sequence y w.r.t. y ∗ , e.g. metrics used for evaluation of natural language processing tasks can be used, such as BLEU (Papineni et al., 2002) or CIDE R (Vedantam et al., (2) where the RNN state is computed recursively as ( fθ (ht−1 , yt−1 , x) for t ∈ {1, ..T }, ht = (3) gθ (x) for t = 0. r(y|y ∗ ) ∝ exp r(y, y ∗ )/τ, 2096 (8) 2015). The temperature parameter τ controls the concentration of the distribution around y ∗ . When m &gt; 1 ground-truth sequences are paired with the same input x, the reward function can be adapted to fit this setting and be defined as r(y, {y ∗(1) , . . . , y ∗(m) }). The sequence-level smoothed loss function is then given by  `Seq (y ∗ , x) = DKL r(y|y ∗ )||pθ (y|x) = H(r(y|y ∗ )) − Er [ln pθ (y|x)] , (9)"
P18-1195,D14-1162,0,0.0811524,"implicitly increases the support to an exponential number of sequences, unlike the sequence-level smoothing approach. This comes at the price, however, of a naive token-level independence assumption in the smoothing. We define the smoothed token-level distribution, similar as the sequence-level one, as a softmax over a token-level “reward” function, (12) l=1 2097 r(yt |yt∗ ) ∝ exp r(yt , yt∗ )/τ, (13) where τ is again a temperature parameter. As a token-level reward r(yt , yt∗ ) we use the cosine similarity between yt and yt∗ in a semantic wordembedding space. In our experiments we use GloVe (Pennington et al., 2014); preliminary experiments with word2vec (Mikolov et al., 2013) yielded somewhat worse results. Promoting rare tokens. We can further improve the token-level smoothing by promoting rare tokens. To do so, we penalize frequent tokens when smoothing over the vocabulary, by subtracting β freq(yt ) from the reward, where freq(·) denotes the term frequency and β is a non-negative weight. This modification encourages frequent tokens into considering the rare ones. We experimentally found that it is also beneficial for rare tokens to boost frequent ones, as they tend to have mostly rare tokens as neigh"
potet-etal-2012-collection,J11-2010,0,\N,Missing
S17-2012,E17-2066,1,0.839973,"Missing"
S17-2012,S16-1081,0,0.194569,"track). Correlation results of all participants (including ours) on track 4b were much lower and we try to explain why (and question the validity of track 4b) in the last part of this paper. Introduction CompiLIG is a collaboration between Compilatio1 - a company particularly interested in crosslanguage plagiarism detection - and LIG research group on natural language processing (GETALP). Cross-language semantic textual similarity detection is an important step for cross-language plagiarism detection, and evaluation campaigns in this new domain are rare. For the first time, SemEval STS task (Agirre et al., 2016) was extended with a Spanish-English cross-lingual sub-task in 2016. This year, sub-task was renewed under track 4 (divided in two sub-corpora: track 4a and track 4b). Given a sentence in Spanish and a sentence in English, the objective is to compute their semantic textual similarity according to a score from 0 1 www.compilatio.net 109 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 109–114, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics 2 2.1 Cross-Language Textual Similarity Detection Methods available tran"
S17-2012,L16-1662,1,0.863744,"sub-task was renewed under track 4 (divided in two sub-corpora: track 4a and track 4b). Given a sentence in Spanish and a sentence in English, the objective is to compute their semantic textual similarity according to a score from 0 1 www.compilatio.net 109 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 109–114, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics 2 2.1 Cross-Language Textual Similarity Detection Methods available translations from DBNary to build the bag-of-words of a word. We use the MultiVec (Berard et al., 2016) toolkit for computing and managing word embeddings. The corpora used to build the embeddings are Europarl and Wikipedia sub-corpus, part of the dataset of Ferrero et al. (2016)2 . For training our embeddings, we use CBOW model with a vector size of 100, a window size of 5, a negative sampling parameter of 5, and an alpha of 0.02. So, the sets of words Sx0 and Sy0 are the conceptual representations in the same language of Sx and Sy respectively. To calculate the similarity between Sx and Sy , we use a syntactically and frequentially weighted augmentation of the Jaccard distance, defined as: Cr"
S17-2012,D15-1075,0,0.0595905,"Missing"
S17-2012,S16-1089,0,0.0613244,"tail them here. The lines in bold represent the methods that obtain the best mean score in each category of system (best method alone, unsupervised and supervised fusion). The scores for the supervised systems are obtained with 10-folds cross-validation. Translation + Monolingual Word Alignment (T+WA) The last method used is a two-step process. First, we translate the Spanish sentence into English with Google Translate (i.e. we are bringing the two sentences in the same language). Then, we align both utterances. We reuse the monolingual aligner4 of Sultan et al. (2015) with the improvement of Brychcin and Svoboda (2016), who won the cross-lingual sub-task in 2016 (Agirre et al., 2016). Because this improvement has not been released by the initial authors, we propose to share our re-implementation on GitHub5 . If Sx and Sy are two sentences in the same language, then we try to measure their similarity with the following formula: ω(Ax ) + ω(Ay ) ω(Sx ) + ω(Sy ) (7) where idf is the function which gives the inverse document frequency of a word. where wi is the ith word of the sentence S, vector is the function which gives the word embedding vector of a word, ϕ is the same that in formula (4), and . is the scala"
S17-2012,petrov-etal-2012-universal,0,0.0234669,"Missing"
S17-2012,S17-2001,0,0.0332907,"Missing"
S17-2012,L16-1657,1,0.879392,"Missing"
S17-2012,S15-2027,0,0.0983005,"Missing"
tan-besacier-2006-french,vaufreydaz-etal-2000-new,0,\N,Missing
tan-besacier-2006-french,le-etal-2004-spoken,1,\N,Missing
U18-1007,J00-3003,0,0.601454,"Missing"
U18-1007,E17-1041,1,0.918862,"echniques have been proposed to capture the semantic correlation between utterances and DAs. Earlier on, statistical techniques such as Hidden Markov Models (HMMs) were widely used to recognise DAs (Stolcke et al., 2000; Julia et al., 2010). Recently, due to the enormous success of neural networks in sequence labeling/transduction tasks (Sutskever et al., 2014; Bahdanau et al., 2014; Popov, 2016), several recurrent neural network (RNN) based architectures have been proposed to conduct DA classification, resulting in ∗ Equal contribution promising outcomes (Ji et al., 2016; Shen and Lee, 2016; Tran et al., 2017a). Despite the success of previous work in DA classification, there are still several fundamental issues. Firstly, most of the previous works rely on transcriptions (Ji et al., 2016; Shen and Lee, 2016; Tran et al., 2017a). Fewer of these focus on combining speech and textual signals (Julia et al., 2010), and even then, the textual signals in these works utilise the oracle transcriptions. We argue that in the context of a spoken dialog system, oracle transcriptions of utterances are usually not available, i.e. the agent does not have access to the human transcriptions. Speech and textual data"
U18-1007,D17-1229,1,0.625302,"echniques have been proposed to capture the semantic correlation between utterances and DAs. Earlier on, statistical techniques such as Hidden Markov Models (HMMs) were widely used to recognise DAs (Stolcke et al., 2000; Julia et al., 2010). Recently, due to the enormous success of neural networks in sequence labeling/transduction tasks (Sutskever et al., 2014; Bahdanau et al., 2014; Popov, 2016), several recurrent neural network (RNN) based architectures have been proposed to conduct DA classification, resulting in ∗ Equal contribution promising outcomes (Ji et al., 2016; Shen and Lee, 2016; Tran et al., 2017a). Despite the success of previous work in DA classification, there are still several fundamental issues. Firstly, most of the previous works rely on transcriptions (Ji et al., 2016; Shen and Lee, 2016; Tran et al., 2017a). Fewer of these focus on combining speech and textual signals (Julia et al., 2010), and even then, the textual signals in these works utilise the oracle transcriptions. We argue that in the context of a spoken dialog system, oracle transcriptions of utterances are usually not available, i.e. the agent does not have access to the human transcriptions. Speech and textual data"
U18-1007,N16-1037,1,0.904418,"ue (Stolcke et al., 2000), numerous techniques have been proposed to capture the semantic correlation between utterances and DAs. Earlier on, statistical techniques such as Hidden Markov Models (HMMs) were widely used to recognise DAs (Stolcke et al., 2000; Julia et al., 2010). Recently, due to the enormous success of neural networks in sequence labeling/transduction tasks (Sutskever et al., 2014; Bahdanau et al., 2014; Popov, 2016), several recurrent neural network (RNN) based architectures have been proposed to conduct DA classification, resulting in ∗ Equal contribution promising outcomes (Ji et al., 2016; Shen and Lee, 2016; Tran et al., 2017a). Despite the success of previous work in DA classification, there are still several fundamental issues. Firstly, most of the previous works rely on transcriptions (Ji et al., 2016; Shen and Lee, 2016; Tran et al., 2017a). Fewer of these focus on combining speech and textual signals (Julia et al., 2010), and even then, the textual signals in these works utilise the oracle transcriptions. We argue that in the context of a spoken dialog system, oracle transcriptions of utterances are usually not available, i.e. the agent does not have access to the human"
U18-1007,K16-1028,0,0.0661187,"Missing"
W06-3711,1983.tc-1.13,0,0.56224,"ient languages. The IBM MASTOR speech-to-speech translation system has been developed for the DARPA CAST and Transtac programs whose mission is to develop technologies that enable rapid deployment of real-time S2S translation of low-resource languages on portable devices. It originated from the IBM MARS S2S system handling the air travel reservation domain described in [1], which was later significantly improved in all components, including ASR, MT and TTS, and later evolved into the MASTOR multilingual S2S system that covers much broader domains such as medical treatment and force protection [2,3]. More recently, we have further broadened our experience and efforts to very rapidly develop systems for under-studied languages, such as regional dialects of Arabic. The intent of this program is to provide language support to military, medical and humanitarian personnel during operations in foreign territories, by deciphering possibly critical language communications with a two-way real-time speech-to-speech translation system designed for specific tasks such as medical triage and force protection. The initial data collection effort for the project has shown that the domain of force protect"
W09-0430,P91-1022,0,0.334688,"pus. 2.2 Sentence alignment From a PDP D1-D2, the sentence alignment process identifies parallel sentence pairs (PSPs) between two documents D1 and D2. For each D1-D2, we have a set SenAlignmentD1-D2 of PSPs. SenAlignmentD1-D2 = {“sen1-sen2” |sen1 is zero/one/many sentence(s) in document D1, sen2 is zero/one/many sentence(s) in document D2, sen1-sen2 is considered as a PSP}. We call a PSP sen1-sen2 alignment type m:n when sen1 contains m consecutive sentences and sen2 contains n consecutive sentences. Several automatic sentence alignment approaches have been proposed based on sentence length (Brown et al., 1991) and lexical information (Kay and Roscheisen, 1993). A hybrid approach is presented in (Gale and Church, 1993) whose basic hypothesis is that “longer sentences in one language tend to be translated into longer sentences in the other language, and shorter sentences tend to be translated into shorter sentences”. Some toolkits such as Hunalign1 and Vanilla2 implement these approaches. However, they tend to work best when documents D1, D2 contain few sentence deletions and insertions, and mainly contain PSPs of type 1:1. 1 2 166 http://mokk.bme.hu/resources/hunalign http://nl.ijs.si/telri/Vanilla/"
W09-0430,W01-0809,0,0.0335994,"apanese, etc. However, for under-resourced languages, it still remains a big gap. For instance, although Vietnamese is the 14th widely-used language in the world, research on MT for Vietnamese is very rare. The earliest MT system for Vietnamese is the system from the Logos Corporation, developed as an English-Vietnamese system for translating aircraft manuals during the 1970s (Hutchins, 2001). Until now, in Vietnam, there are only four research groups working on MT for VietnameseEnglish (Ho, 2005). However the results are still modest. MT research on Vietnamese-French occurs even more rarely. Doan (2001) proposed a translation module for Vietnamese within ITS3, a multilingual MT system based on the classical analysis-transfer-generation approach. Nguyen (2006) worked on Vietnamese language and Vietnamese-French text alignment. But no complete MT system for this pair of languages has been published so far. There are many approaches for MT: rule-based (direct translation, interlingua-based, transferbased), corpus-based (statistical, example-based) as well as hybrid approaches. We focus on building a Vietnamese-French statistical machine translation (SMT) system. Such an approach requires a para"
W09-0430,J93-1004,0,0.449098,"airs (PSPs) between two documents D1 and D2. For each D1-D2, we have a set SenAlignmentD1-D2 of PSPs. SenAlignmentD1-D2 = {“sen1-sen2” |sen1 is zero/one/many sentence(s) in document D1, sen2 is zero/one/many sentence(s) in document D2, sen1-sen2 is considered as a PSP}. We call a PSP sen1-sen2 alignment type m:n when sen1 contains m consecutive sentences and sen2 contains n consecutive sentences. Several automatic sentence alignment approaches have been proposed based on sentence length (Brown et al., 1991) and lexical information (Kay and Roscheisen, 1993). A hybrid approach is presented in (Gale and Church, 1993) whose basic hypothesis is that “longer sentences in one language tend to be translated into longer sentences in the other language, and shorter sentences tend to be translated into shorter sentences”. Some toolkits such as Hunalign1 and Vanilla2 implement these approaches. However, they tend to work best when documents D1, D2 contain few sentence deletions and insertions, and mainly contain PSPs of type 1:1. 1 2 166 http://mokk.bme.hu/resources/hunalign http://nl.ijs.si/telri/Vanilla/ Ma (2006) provides an open source software called Champollion1 to solve this limitation. Champollion permits"
W09-0430,J03-3001,0,0.0676766,"s the two main steps: document alignment and sentence alignment. We also discuss the proposed document alignment method. 2.1 Document alignment Let S1 be set of documents in language L1; let S2 be set of documents in language L2. Extracting parallel documents or aligning documents from the two sets S1, S2 can be seen as finding the translation document D2 (in the set S2) of a document D1 (in the set S1). We call this pair of documents D1-D2 a parallel document pair (PDP). For collecting bilingual text data for the two sets S1, S2, the Web is an ideal source as it is large, free and available (Kilgarriff and Grefenstette, 2003). For this kind of data, various methods to align documents have been proposed. Documents can be simply aligned based on the anchor link, the clue in URL (Kraaij et al., 2003) or the web page structure (Resnik and Smith, 2003). However, this information is not always available or trustworthy. The titles of documents D1, D2 can also be used (Yang and Li, 2002), but sometimes they are completely different. Another useful source of information is invariant words, such as named entities, dates, and numbers, which are often common in news data. We call these words special words. (Patry and Langlais"
W09-0430,2005.mtsummit-papers.11,0,0.0780072,"different units for Vietnamese (syllables, words, or their combination) is discussed. Section 5 concludes and discusses future work. 2 Mining a comparable text corpus In (Munteanu and Daniel Marcu, 2006), the authors present a method for extracting parallel sub-sentential fragments from comparable bilingual corpora. However this method is in need of an initial parallel bilingual corpus, which is not available for the pair of language VietnameseFrench (in the news domain). The overall process of mining a bilingual text corpus which is used in a SMT system typically takes five following steps (Koehn, 2005): raw data collection, document alignment, sentence splitting, tokenization and sentence alignment. This section presents the two main steps: document alignment and sentence alignment. We also discuss the proposed document alignment method. 2.1 Document alignment Let S1 be set of documents in language L1; let S2 be set of documents in language L2. Extracting parallel documents or aligning documents from the two sets S1, S2 can be seen as finding the translation document D2 (in the set S2) of a document D1 (in the set S1). We call this pair of documents D1-D2 a parallel document pair (PDP). For"
W09-0430,P07-2045,0,0.00478093,"ble Word Dev Tst Fr Vn Fr Vn Fr Vn Fr Vn Fr Vn Fr Vn Nbr of PSPs Training: 47,081 Developing: 198 Testing: 210 Training: 48,864 Developing: 198 Testing: 210 Set Nbr. of vocab (K) Language Trn With the obtained parallel corpus, we attempted to rapidly build a SMT system for VietnameseFrench. The system was built using the Moses toolkit1. The Moses toolkit contains all of the components needed to train both the translation model and the language model. It also contains tools for tuning these models using minimum error rate training and for evaluating the translation result using the BLEU score (Koehn et al., 2007). 4.2 FV System Table 7. The obtained corpus from STRAIN. 4.1 S1FV Vietnamese is segmented into 38.6 21.9 1.8 1.2 1.9 1.3 39.7 33.4 1.8 1.5 1.9 1.6 Nbr. of running words/syllables (K) 1783.6 2190.2 6.3 6.9 6.4 7.1 1893 1629 6.3 4.8 6.3 4.9 Table 8. Our four translation systems. We obtained the performance results for those systems in Table 9. In the case of the systems where Vietnamese was segmented into words, the Vietnamese sentences were changed back to syllable representation before calculating the BLEU scores, so that all the BLEU scores evaluated can be compared to each other. BLEU S1FV"
W09-0430,J03-3003,0,0.0297909,"t S2 be set of documents in language L2. Extracting parallel documents or aligning documents from the two sets S1, S2 can be seen as finding the translation document D2 (in the set S2) of a document D1 (in the set S1). We call this pair of documents D1-D2 a parallel document pair (PDP). For collecting bilingual text data for the two sets S1, S2, the Web is an ideal source as it is large, free and available (Kilgarriff and Grefenstette, 2003). For this kind of data, various methods to align documents have been proposed. Documents can be simply aligned based on the anchor link, the clue in URL (Kraaij et al., 2003) or the web page structure (Resnik and Smith, 2003). However, this information is not always available or trustworthy. The titles of documents D1, D2 can also be used (Yang and Li, 2002), but sometimes they are completely different. Another useful source of information is invariant words, such as named entities, dates, and numbers, which are often common in news data. We call these words special words. (Patry and Langlais, 2005) used numbers, punctuation, and entity names to measure the parallelism between two documents. The order of this information in document is used as an important criteri"
W09-0430,ma-2006-champollion,0,0.167767,"and lexical information (Kay and Roscheisen, 1993). A hybrid approach is presented in (Gale and Church, 1993) whose basic hypothesis is that “longer sentences in one language tend to be translated into longer sentences in the other language, and shorter sentences tend to be translated into shorter sentences”. Some toolkits such as Hunalign1 and Vanilla2 implement these approaches. However, they tend to work best when documents D1, D2 contain few sentence deletions and insertions, and mainly contain PSPs of type 1:1. 1 2 166 http://mokk.bme.hu/resources/hunalign http://nl.ijs.si/telri/Vanilla/ Ma (2006) provides an open source software called Champollion1 to solve this limitation. Champollion permits alignment type m:n (m, n = 0,1,2,3,4), so the length of sentence does not play an important role. Champollion uses also lexical information (lexemes, stop words, bilingual dictionary, etc.) to align sentences. Champollion can easily be adapted to new pairs of languages. Available language pairs in Champollion are English-Arabic and English-Chinese (Ma, 2006). 2.3 Our document alignment method Figure 1 describes our methodology for document alignment. For each document D1 in the set S1, we find t"
W09-0430,J93-2003,0,\N,Missing
W09-0430,J03-3002,0,\N,Missing
W09-0430,P91-1010,0,\N,Missing
W09-0430,J93-1006,0,\N,Missing
W09-0430,P06-1011,0,\N,Missing
W09-0430,N03-1017,0,\N,Missing
W09-3536,W04-1613,0,0.210166,"Missing"
W09-3536,C08-1068,1,0.795735,"Missing"
W09-3536,W98-1005,0,\N,Missing
W09-3536,J98-4003,0,\N,Missing
W10-1723,P02-1040,0,0.0769726,"Missing"
W10-1723,P03-1021,0,0.0650887,"Missing"
W10-1723,W04-3250,0,0.197288,"Missing"
W10-1723,W09-0427,0,0.0116719,"on quality. The final submission is a combination between a standard phrase-based system using the Moses decoder, with appropriate setups and pre-processing, and a lemmatized system to deal with Out-Of-Vocabulary conjugated verbs. 1 Introduction We participated, for the first time, to the shared news translation task of the fifth Workshop on Machine Translation (WMT 2010) for the FrenchEnglish language pair. The submission was performed using a standard phrase-based translation system with appropriate setups and preprocessings in order to deal with system’s unknown words. Indeed, as shown in (Carpuat, 2009), (Habash, 2008) and (Niessen, 2004), handling Ou-of-Vocabulary words with techniques like lemmatization, phrase table extension or morphological pre-processing is a way to improve translation quality. After a short presentation of our baseline system setups we discuss the effect of Out-Of-Vocabulary words in the system and introduce some ideas we chose to implement. In the last part, we evaluate their impact on translation quality using automatic and human evaluations. We used the provided Europarl and News parallel corpora (total 1,638,440 sentences) to train the translation model and the Ne"
W10-1723,J04-2003,0,0.0808342,"Missing"
W10-1723,2009.mtsummit-btm.3,1,0.826755,"Missing"
W10-1723,P07-2045,0,\N,Missing
W10-1723,P08-2015,0,\N,Missing
W10-1723,D08-1076,0,\N,Missing
W10-3601,P03-1051,0,0.0377789,"in the context of under-resourced languages as the resources needed to implement these methods do not exist. For an under-resourced language, we seek segmentation methods that allow better exploitation of the limited resources. In our previous paper (Seng et al., 2009) we have indicated the problems of existing text segmentation approaches and introduced a weighted finite state transducer (WFST) based multiple text segmentation algorithm. Our approach is implemented using the AT & T FSM Toolkit (Mohri et al., 1998). The algorithm is inspired with the work on the segmentation of Arabic words (Lee et al., 2003). The multiple segmentation of a sequence of characters is made using the composition of three controllers. Given a finite list of words we can build a finite state transducer M (or word transducer) that, once composed with an acceptor I of the input string that represent a single character with each arc, generates a lattice of the words that represent all of the possible segmentations. To handle out-of-vocabulary entries, we make a model of any string of characters by a star closure operation over all the possible characters. Thus, the unknown word WFST can parse any sequence of characters an"
W11-2154,W08-0509,0,0.0344349,"of the two seed systems (LIG and LIA), whereas test08 and testcomb08 were used to tune the weights for system combination. test10 was finally put aside to compare internally our methods. 2.2 LIG and LIA system characteristics Both LIG and LIA systems are phrase-based translation models. All the data were first tokenized with the tokenizer provided for the workshop. KneserNey discounted LMs were built from monolingual corpora using the SRILM toolkit (Stolcke, 2002), while bilingual corpora were aligned at the wordlevel using G IZA ++ (Och and Ney, 2003) or its multi-threaded version MG IZA ++ (Gao and Vogel, 2008) for the large corpora UN and giga. Phrase table and lexicalized reordering models were built with M OSES (Koehn et al., 2007). Finally, 14 features were used in the phrase-based models: 1 When not specified otherwise “our” system refers to the LIGA system. 440 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 440–446, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics C ORPORA News Commentary v6 Europarl v6 United Nation corpus 109 corpus D ESIGNATION S IZE ( SENTENCES ) English-French Bilingual training news-c euro UN giga Engl"
W11-2154,2005.eamt-1.19,0,0.0365934,"l when citations included in sentences have to be translated. Two configurations were tested: zone markups inclusion around quotes and wall markups inclusion within zone markups. However, the measured gains were finally too marginal to include the method in the final system. 4.2 Parallel corpus subsampling As the only news parallel corpus provided for the workshop contains 116 k sentence pairs, we must resort to parallel out-of-domain corpora in order to build reliable translation models. Information retrieval (IR) methods have been used in the past to subsample parallel corpora. For example, Hildebrand et al. (2005) used sentences belonging to the development and test corpora as queries to select the k most similar source sentences in an indexed parallel corpus. The retrieved sentence pairs constituted a training corpus for the translation models. The RALI submission for WMT10 proposed a similar approach that builds queries from the monolingual news corpus in order to select sentence pairs stylistically close to the news domain (Huet et al., 2010). This method has the major interest that it does not require to build a new training parallel corpus for each news data set to translate. Following the best co"
W11-2154,W10-1713,1,0.771338,"in order to build reliable translation models. Information retrieval (IR) methods have been used in the past to subsample parallel corpora. For example, Hildebrand et al. (2005) used sentences belonging to the development and test corpora as queries to select the k most similar source sentences in an indexed parallel corpus. The retrieved sentence pairs constituted a training corpus for the translation models. The RALI submission for WMT10 proposed a similar approach that builds queries from the monolingual news corpus in order to select sentence pairs stylistically close to the news domain (Huet et al., 2010). This method has the major interest that it does not require to build a new training parallel corpus for each news data set to translate. Following the best configuration tested in (Huet et al., 2010), we index the three out-of-domain corpora using L EMUR3 , and build queries from English news-s sentences where stop words are removed. The 10 top sentence pairs retrieved per query are selected and added to the new training corpus if they are not redundant with a sentence pair already collected. The process is repeated until the training parallel corpus reaches a threshold over the number of re"
W11-2154,D07-1103,0,0.0637009,"ning on mono-news-c and news-s Training on news-c, euro and UN 5-gram models Training on 10 M sentence pairs selected in news-c, euro, UN and giga Translation model Phrase table filtering Use of -monotone-at-punctuation option Table 2: Distinct features between final configurations retained for the LIG and LIA systems 3.3 Translation model training Translation models were trained from the parallel corpora news-c, euro and UN. Data were aligned at the word-level and then used to build standard phrase-based translation models. We filtered the obtained phrase table using the method described in (Johnson et al., 2007). Since this technique drastically reduces the size of the phrase table, while not degrading (and even slightly improving) the results on the development and test corpora (System 6), we decided to employ filtered phrase tables in the final configuration of the LIG system. 3.4 Tuning For decoding, the system uses a log-linear combination of translation model scores with the LM log-probability. We prevent phrase reordering over punctuation using the M OSES option -monotone-atpunctuation. As the system can be beforehand tuned by adjusting the log-linear combination weights on a development corpus"
W11-2154,P07-2045,0,0.0138313,"10 was finally put aside to compare internally our methods. 2.2 LIG and LIA system characteristics Both LIG and LIA systems are phrase-based translation models. All the data were first tokenized with the tokenizer provided for the workshop. KneserNey discounted LMs were built from monolingual corpora using the SRILM toolkit (Stolcke, 2002), while bilingual corpora were aligned at the wordlevel using G IZA ++ (Och and Ney, 2003) or its multi-threaded version MG IZA ++ (Gao and Vogel, 2008) for the large corpora UN and giga. Phrase table and lexicalized reordering models were built with M OSES (Koehn et al., 2007). Finally, 14 features were used in the phrase-based models: 1 When not specified otherwise “our” system refers to the LIGA system. 440 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 440–446, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics C ORPORA News Commentary v6 Europarl v6 United Nation corpus 109 corpus D ESIGNATION S IZE ( SENTENCES ) English-French Bilingual training news-c euro UN giga English Monolingual training News Commentary v6 mono-news-c Shuffled News Crawl corpus (from 2007 to 2011) news-s Europarl v6 mono"
W11-2154,J03-1002,0,0.00334518,"evoted to model tuning: test09 was used for the development of the two seed systems (LIG and LIA), whereas test08 and testcomb08 were used to tune the weights for system combination. test10 was finally put aside to compare internally our methods. 2.2 LIG and LIA system characteristics Both LIG and LIA systems are phrase-based translation models. All the data were first tokenized with the tokenizer provided for the workshop. KneserNey discounted LMs were built from monolingual corpora using the SRILM toolkit (Stolcke, 2002), while bilingual corpora were aligned at the wordlevel using G IZA ++ (Och and Ney, 2003) or its multi-threaded version MG IZA ++ (Gao and Vogel, 2008) for the large corpora UN and giga. Phrase table and lexicalized reordering models were built with M OSES (Koehn et al., 2007). Finally, 14 features were used in the phrase-based models: 1 When not specified otherwise “our” system refers to the LIGA system. 440 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 440–446, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics C ORPORA News Commentary v6 Europarl v6 United Nation corpus 109 corpus D ESIGNATION S IZE ( SENTENCE"
W11-2154,P03-1021,0,0.0208191,"us (from 2007 to 2011) news-s Europarl v6 mono-euro 116 k 1.8 M 12 M 23 M 181 k 25 M 1.8 M Development newstest2008 newssyscomb2009 newstest2009 test08 testcomb09 test09 2,051 502 2,525 test10 2,489 Test newstest2010 Table 1: Used corpora processed in order to normalize a special French form (named euphonious “t”) as described in (Potet et al., 2010). • 5 translation model scores, • 1 distance-based reordering score, • 6 lexicalized reordering score, • 1 LM score and • 1 word penalty score. The score weights were optimized on the test09 corpus according to the BLEU score with the MERT method (Och, 2003). The experiments led specifically with either LIG or LIA system are respectively described in Sections 3 and 4. Unless otherwise indicated, all the evaluations were performed using case-insensitive BLEU and were computed with the mteval-v13a.pl script provided by NIST. Table 2 summarizes the differences between the final configuration of the systems. 3 The LIG machine translation system LIG participated for the second time to the WMT shared news translation task for the French-English language pair. 3.1 Pre-processing Training data were first lowercased with the P ERL script provided for the"
W12-1305,seng-etal-2008-first,1,0.890468,"Missing"
W13-2248,W12-3110,0,0.032269,"IBM-model based features (Blatz et al., 2004). Ueffing and Ney (2005) propose an approach for phrase-based translation models: a phrase is a sequence of contiguous words and is extracted from word-aligned bilingual training corpus. The confidence value of each word is then computed by summing over all phrase pairs in which the target part contains this word. Xiong et al. (2010) integrate target word’s Part-Of-Speech (POS) and train them by Maximum Entropy Model, allowing significative gains compared to WPP features. Other approaches are based on external features (Soricut and Echihabi, 2010; Felice and Specia, 2012) allowing to deal with various MT systems (e.g. statistical, rule based etc.). In this paper, we propose to use both internal and external features into a conditionnal random fields (CRF) model to predict the label for each word in the MT hypothesis. We organize the article as follows: section 2 explains all the used features. Section 3 presents our experimental settings and the preliminary experiments. Section 4 explores a feature selection refinement and the section 5 presents work using several classifiers associated with a boosting decision. Finally we present Introduction Recently Statist"
W13-2248,P10-1052,0,0.187852,"Missing"
W13-2248,F12-3004,0,0.02544,"Missing"
W13-2248,N07-1051,0,0.0121351,"rce word features: all the source words that align to the target one, represented in BIO1 format. • Source alignment context features: the combinations of the target word and one word before (left source context) or after (right source context) the source word aligned to it. • Target alignment context features: the combinations of the source word and each word in the window ±2 (two before, two after) of the target word. • The word’s constituent label and its depth in the tree (or the distance between it and the tree root) obtained from the constituent tree as an output of the Berkeley parser (Petrov and Klein, 2007) (trained over a Spanish treebank: AnCora3 ). • Target Word’s Posterior Probability (WPP). • Backoff behaviour: a score assigned to the word according to how many times the target Language Model has to back-off in order to assign a probability to the word sequence, as described in (Raybaud et al., 2011). 1 The LIG features • Occurrence in Google Translate hypothesis: we check whether this target word appears in the sentence generated by Google Translate engine for the same source. 2 3 http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/ 387 http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTa"
W13-2248,P10-1063,0,0.0242751,"t al. (2003) associated with IBM-model based features (Blatz et al., 2004). Ueffing and Ney (2005) propose an approach for phrase-based translation models: a phrase is a sequence of contiguous words and is extracted from word-aligned bilingual training corpus. The confidence value of each word is then computed by summing over all phrase pairs in which the target part contains this word. Xiong et al. (2010) integrate target word’s Part-Of-Speech (POS) and train them by Maximum Entropy Model, allowing significative gains compared to WPP features. Other approaches are based on external features (Soricut and Echihabi, 2010; Felice and Specia, 2012) allowing to deal with various MT systems (e.g. statistical, rule based etc.). In this paper, we propose to use both internal and external features into a conditionnal random fields (CRF) model to predict the label for each word in the MT hypothesis. We organize the article as follows: section 2 explains all the used features. Section 3 presents our experimental settings and the preliminary experiments. Section 4 explores a feature selection refinement and the section 5 presents work using several classifiers associated with a boosting decision. Finally we present Int"
W13-2248,H05-1096,0,0.0833261,"the best systems for submission and present the official results obtained. 1 • a binary classification: good (keep) or bad (change) label • a multi-class classification: the label refers to the edit action needed for the token (i.e. keep, delete or substitute). Various approaches have been proposed for WCE: Blatz et al. (2003) combine several features using neural network and naive Bayes learning algorithms. One of the most effective feature combinations is the Word Posterior Probability (WPP) as proposed by Ueffing et al. (2003) associated with IBM-model based features (Blatz et al., 2004). Ueffing and Ney (2005) propose an approach for phrase-based translation models: a phrase is a sequence of contiguous words and is extracted from word-aligned bilingual training corpus. The confidence value of each word is then computed by summing over all phrase pairs in which the target part contains this word. Xiong et al. (2010) integrate target word’s Part-Of-Speech (POS) and train them by Maximum Entropy Model, allowing significative gains compared to WPP features. Other approaches are based on external features (Soricut and Echihabi, 2010; Felice and Specia, 2012) allowing to deal with various MT systems (e.g"
W13-2248,2003.mtsummit-papers.52,0,0.507011,"king advantage of their complementarity is presented and experimented. We then select the best systems for submission and present the official results obtained. 1 • a binary classification: good (keep) or bad (change) label • a multi-class classification: the label refers to the edit action needed for the token (i.e. keep, delete or substitute). Various approaches have been proposed for WCE: Blatz et al. (2003) combine several features using neural network and naive Bayes learning algorithms. One of the most effective feature combinations is the Word Posterior Probability (WPP) as proposed by Ueffing et al. (2003) associated with IBM-model based features (Blatz et al., 2004). Ueffing and Ney (2005) propose an approach for phrase-based translation models: a phrase is a sequence of contiguous words and is extracted from word-aligned bilingual training corpus. The confidence value of each word is then computed by summing over all phrase pairs in which the target part contains this word. Xiong et al. (2010) integrate target word’s Part-Of-Speech (POS) and train them by Maximum Entropy Model, allowing significative gains compared to WPP features. Other approaches are based on external features (Soricut and"
W13-2248,C04-1046,0,\N,Missing
W13-2248,P10-1062,0,\N,Missing
W13-4706,W09-3510,0,0.035093,"Missing"
W13-4706,J07-3002,0,0.0220739,"as Urdu computational linguistics (Zia 1999). Like in Arabic, diacritical marks are sparingly used in written Urdu (Zia 1999). To model this unfortunate situation, we developed another From these two types of Urdu – Hindi parallel data, we developed two types of character alignments using GIZA++ (Och and Ney 2003) in both directions:  Hindi and Urdu with diacritics character alignment,  Hindi and Urdu without diacritics character alignment. 45 3.1.2 # Sentence pair (167) source length 9 target length 7 alignment score : 3.20243e‐05 Cluster alignments Alignment plays a critical role in SMT (Fraser and Marcu 2007; Kumar, Och and Macherey 2007; Huang 2009). The quality of parallel data and the word alignment have a significant impact on learning the translation model and consequently on the quality of the SMT system (Fraser and Marcu 2007; Huang 2009). It is always better do an analysis of the alignment and correct the alignment errors to reduce the Alignment Error Rate (AER). We also analyzed the alignments produced by GIZA++. We found that we can improve our alignments to reduce the AER. The incorrect alignments are highlighted in Table 4 (below) that shows Hindi to Urdu with diacritics alignments of"
W13-4706,P09-1105,0,0.0159907,"n Arabic, diacritical marks are sparingly used in written Urdu (Zia 1999). To model this unfortunate situation, we developed another From these two types of Urdu – Hindi parallel data, we developed two types of character alignments using GIZA++ (Och and Ney 2003) in both directions:  Hindi and Urdu with diacritics character alignment,  Hindi and Urdu without diacritics character alignment. 45 3.1.2 # Sentence pair (167) source length 9 target length 7 alignment score : 3.20243e‐05 Cluster alignments Alignment plays a critical role in SMT (Fraser and Marcu 2007; Kumar, Och and Macherey 2007; Huang 2009). The quality of parallel data and the word alignment have a significant impact on learning the translation model and consequently on the quality of the SMT system (Fraser and Marcu 2007; Huang 2009). It is always better do an analysis of the alignment and correct the alignment errors to reduce the Alignment Error Rate (AER). We also analyzed the alignments produced by GIZA++. We found that we can improve our alignments to reduce the AER. The incorrect alignments are highlighted in Table 4 (below) that shows Hindi to Urdu with diacritics alignments of our sample words of Table 3. The vowel rep"
W13-4706,W04-1613,0,0.0374663,"on learning the translation model and consequently on the quality of the SMT system (Fraser and Marcu 2007; Huang 2009). It is always better do an analysis of the alignment and correct the alignment errors to reduce the Alignment Error Rate (AER). We also analyzed the alignments produced by GIZA++. We found that we can improve our alignments to reduce the AER. The incorrect alignments are highlighted in Table 4 (below) that shows Hindi to Urdu with diacritics alignments of our sample words of Table 3. The vowel representation in Urdu/PersioArabic script is highly complex and contextsensitive (Hussain 2004; Malik, Boitet and Bhattacharyya 2008; Malik et al. 2009). This highly complex and contextual representation leads to wrong character alignments, highlighted in Table 4. In the second row of Table 4, the Hindi vowel इ [ɪ] is aligned with ALEF ( )اand ZER ( ِ◌) is aligned to NULL. The alignment is not completely incorrect, but the vowel इ [ɪ] must be aligned with both ALEF ( )اand ZER (◌ِ ). Similarly, the Hindi vowel उ [ʊ] must be aligned with ALEF ( )اand PESH ( ُ◌) in the third row. In these examples, one character in Hindi must be aligned with a sequence of characters in Urdu. Intere"
W13-4706,P06-1067,0,0.0251764,"2 for Hindi and 4 for Urdu. Another set of 6 target language models are developed by combining the corresponding word and sentence language models. Target Language Models A target language model Ρ is a probabilistic model that scores the well-formedness of different translation solutions produced by the translation model (Koehn, Och and Marcu 2003; Zens and Ney 2003; Och and Ney 2004; AlOnaizan and Papineni 2006). It generates a probability distribution over possible sequences of words and computes the probability of producing a given word given all the words that precede it in the sentence (Al-Onaizan and Papineni 2006). We developed multiple target language models depending on the type of alignments used in the transliteration models and the target language. We broadly categorize them into word language models and sentence language models, discussed below. 3.3.1 Sentence Language Models (SLM) Word Language Models (WLM) A word language model is a 6-gram statistical model that gives a probability distribution over possible sequences of characters and computes the probability of producing a given character or cluster C1, given the 5 characters or clusters that precede it in the word. We developed Hindi – Urdu"
W13-4706,J90-2002,0,0.656839,"Missing"
W13-4706,E09-1050,0,0.0389601,"Missing"
W13-4706,J93-2003,0,0.0897074,"Missing"
W13-4706,W09-3522,0,0.068881,"Missing"
W13-4706,J03-1002,0,0.00591785,"he Urdu words that we have generated from the Roman transcriptions from the DSAL dictionary data contain all required diacritical marks, clearly shown in Table 2. Diacritical marks are the back bone of the Urdu vowel system and they are mandatory for the correct pronunciation of an Urdu word, as well as Urdu computational linguistics (Zia 1999). Like in Arabic, diacritical marks are sparingly used in written Urdu (Zia 1999). To model this unfortunate situation, we developed another From these two types of Urdu – Hindi parallel data, we developed two types of character alignments using GIZA++ (Och and Ney 2003) in both directions:  Hindi and Urdu with diacritics character alignment,  Hindi and Urdu without diacritics character alignment. 45 3.1.2 # Sentence pair (167) source length 9 target length 7 alignment score : 3.20243e‐05 Cluster alignments Alignment plays a critical role in SMT (Fraser and Marcu 2007; Kumar, Och and Macherey 2007; Huang 2009). The quality of parallel data and the word alignment have a significant impact on learning the translation model and consequently on the quality of the SMT system (Fraser and Marcu 2007; Huang 2009). It is always better do an analysis of the alignment"
W13-4706,D07-1005,0,0.0560221,"Missing"
W13-4706,J04-4002,0,0.0575552,"r and then by applying clustering. Finally, we developed character-level and cluster-level Urdu Sentence Language Models using the SRILM toolkit. Similar to word language model, We have developed total 6 sentence language models, 2 for Hindi and 4 for Urdu. Another set of 6 target language models are developed by combining the corresponding word and sentence language models. Target Language Models A target language model Ρ is a probabilistic model that scores the well-formedness of different translation solutions produced by the translation model (Koehn, Och and Marcu 2003; Zens and Ney 2003; Och and Ney 2004; AlOnaizan and Papineni 2006). It generates a probability distribution over possible sequences of words and computes the probability of producing a given word given all the words that precede it in the sentence (Al-Onaizan and Papineni 2006). We developed multiple target language models depending on the type of alignments used in the transliteration models and the target language. We broadly categorize them into word language models and sentence language models, discussed below. 3.3.1 Sentence Language Models (SLM) Word Language Models (WLM) A word language model is a 6-gram statistical model"
W13-4706,W09-3536,1,0.906635,"n the quality of the SMT system (Fraser and Marcu 2007; Huang 2009). It is always better do an analysis of the alignment and correct the alignment errors to reduce the Alignment Error Rate (AER). We also analyzed the alignments produced by GIZA++. We found that we can improve our alignments to reduce the AER. The incorrect alignments are highlighted in Table 4 (below) that shows Hindi to Urdu with diacritics alignments of our sample words of Table 3. The vowel representation in Urdu/PersioArabic script is highly complex and contextsensitive (Hussain 2004; Malik, Boitet and Bhattacharyya 2008; Malik et al. 2009). This highly complex and contextual representation leads to wrong character alignments, highlighted in Table 4. In the second row of Table 4, the Hindi vowel इ [ɪ] is aligned with ALEF ( )اand ZER ( ِ◌) is aligned to NULL. The alignment is not completely incorrect, but the vowel इ [ɪ] must be aligned with both ALEF ( )اand ZER (◌ِ ). Similarly, the Hindi vowel उ [ʊ] must be aligned with ALEF ( )اand PESH ( ُ◌) in the third row. In these examples, one character in Hindi must be aligned with a sequence of characters in Urdu. Interestingly, we have observed that GIZA++ correctly aligns suc"
W13-4706,C08-1068,1,0.933314,"Missing"
W13-4706,P04-1021,0,\N,Missing
W13-4706,W98-1005,0,\N,Missing
W13-4706,W02-0505,0,\N,Missing
W13-4706,P07-2045,0,\N,Missing
W13-4706,P06-2025,0,\N,Missing
W13-4706,P10-1048,0,\N,Missing
W13-4706,N03-1017,0,\N,Missing
W13-4706,P97-1017,0,\N,Missing
W13-4706,P03-1019,0,\N,Missing
W14-0301,P03-1021,0,0.112235,"Missing"
W14-0301,P02-1040,0,0.0929368,"hypotheses. This latter work is the one that is the most related to our paper. However, the major differences are: (1) our proposed sen3 Our Approach Our approach can be expressed in three steps: investigate the potential of using word-level score in N-best list re-ranking, build the WCE system and 2 extract additional features to integrate with the existing log-linear model. 3.1 Investigating the correlation between “word quality” scores and other metrics Firstly, we investigate the correlation between sentence-level scores (obtained from WCE labels) and conventional evaluation scores (BLEU (Papineni et al., 2002), TER and TERp-A (Snover et al., 2008)). For each sentence, a word quality score (WQS) is calculated by: W QS = #00 G00 (good) words #words (1) In other words, we are trying to answer the following question: can the high percentage of “G” (good) words (predicted by WCE system) in a MT output ensure its possibility of having a better BLEU and low TER (TERp-A) value ? This investigation is a strong prerequisite for further experiments in order to check that WCE scores do not bring additional “noise” to the re-ranking process. In this experiment, we compute WQS over our entire French - English da"
W14-0301,2012.eamt-1.34,0,0.257157,"Missing"
W14-0301,potet-etal-2012-collection,1,0.81717,"avergne et al., 2010). Basically, CRF computes the probability of the output sequence Y = (y1 , y2 , ..., yN ) given the input sequence X = (x1 , x2 , ..., xN ) by: Essentially, a WCE system construction consists of two pivotal elements: the features (the SMT system dependent or independent information extracted for each word to represent its characteristics) and the machine learning method (to train the prediction model). Motivated 3 rank in WMT 2013 Quality Estimation Shared Task (Luong et al., 2013b). For building the WCE training and test sets, we use a dataset of 10,881 French sentences (Potet et al., 2012) , and apply a baseline SMT system to generate hypotheses (1000-best list). Our baseline SMT system (presented for WMT 2010 evaluation) keeps the Moses’s default setting (Koehn et al., 2007): log-linear model with 14 weighted feature functions. The translation model is trained on the Europarl and News parallel corpora of WMT102 evaluation campaign (1,638,440 sentences). The target language model is trained by the SRI language modeling toolkit (Stolcke, 2002) on the news monolingual corpus (48,653,884 sentences). Translators were then invited to correct MT outputs, giving us the same amount of"
W14-0301,C04-1046,0,0.0796473,"The interesting uses of WCE include: pointing out the words that need to be corrected by the post-editor, telling readers about the reliability of a specific portion, and selecting the best segments among options from multiple translation systems for combination. Dealing with this problem, various approaches have been proposed: Blatz et al. (2003) combine several features using neural network and naive Bayes learning algorithms. One of the most effective feature combinations is the Word Posterior Probability (WPP) as suggested by Ueffing et al. (2003) associated with IBM-model based features (Blatz et al., 2004). Ueffing and Ney (2005) propose an approach for phrase-based translation models: a phrase is a sequence of contiguous words and is extracted from the word-aligned bilingual training corpus. The confidence value of each word is then computed by summing over all phrase pairs in which the target part contains this word. Xiong et al. (2010) integrate target word’s Part-Of-Speech (POS) and train them by Maximum Entropy Model, allowing significative gains in comparison to WPP features. The novel features from source side, alignment context, and dependency structure (Bach et al., 2011) help to augme"
W14-0301,2012.amta-papers.17,0,0.0901014,"Missing"
W14-0301,P11-2031,0,0.0946635,"Missing"
W14-0301,P10-1063,0,0.0260328,"s extracted from the word-aligned bilingual training corpus. The confidence value of each word is then computed by summing over all phrase pairs in which the target part contains this word. Xiong et al. (2010) integrate target word’s Part-Of-Speech (POS) and train them by Maximum Entropy Model, allowing significative gains in comparison to WPP features. The novel features from source side, alignment context, and dependency structure (Bach et al., 2011) help to augment marginally in F-score as well as the Pearson correlation with human judgment. Other approaches are based on external features (Soricut and Echihabi, 2010; Felice and Specia, 2012) allowing to cope with various MT systems (e.g. statistical, rule based etc.). Among the numerous WCE applications, we consider its contribution in a specific step of SMT pipeline: N-best list reranking. Our WCE system and the proposed reranking features are presented in the next section. 2.1 N-best List Re-ranking Walking through various related work concerning this issue, we observe some prominent ideas. The first attempt focuses on proposing additional Language Models. Kirchhoff and Yang (2005) train one word-based 4-gram model (with modified Kneser-Ney smoothing)"
W14-0301,W12-3110,0,0.0151351,"igned bilingual training corpus. The confidence value of each word is then computed by summing over all phrase pairs in which the target part contains this word. Xiong et al. (2010) integrate target word’s Part-Of-Speech (POS) and train them by Maximum Entropy Model, allowing significative gains in comparison to WPP features. The novel features from source side, alignment context, and dependency structure (Bach et al., 2011) help to augment marginally in F-score as well as the Pearson correlation with human judgment. Other approaches are based on external features (Soricut and Echihabi, 2010; Felice and Specia, 2012) allowing to cope with various MT systems (e.g. statistical, rule based etc.). Among the numerous WCE applications, we consider its contribution in a specific step of SMT pipeline: N-best list reranking. Our WCE system and the proposed reranking features are presented in the next section. 2.1 N-best List Re-ranking Walking through various related work concerning this issue, we observe some prominent ideas. The first attempt focuses on proposing additional Language Models. Kirchhoff and Yang (2005) train one word-based 4-gram model (with modified Kneser-Ney smoothing) and one factored trigram o"
W14-0301,H05-1096,0,0.119399,"of WCE include: pointing out the words that need to be corrected by the post-editor, telling readers about the reliability of a specific portion, and selecting the best segments among options from multiple translation systems for combination. Dealing with this problem, various approaches have been proposed: Blatz et al. (2003) combine several features using neural network and naive Bayes learning algorithms. One of the most effective feature combinations is the Word Posterior Probability (WPP) as suggested by Ueffing et al. (2003) associated with IBM-model based features (Blatz et al., 2004). Ueffing and Ney (2005) propose an approach for phrase-based translation models: a phrase is a sequence of contiguous words and is extracted from the word-aligned bilingual training corpus. The confidence value of each word is then computed by summing over all phrase pairs in which the target part contains this word. Xiong et al. (2010) integrate target word’s Part-Of-Speech (POS) and train them by Maximum Entropy Model, allowing significative gains in comparison to WPP features. The novel features from source side, alignment context, and dependency structure (Bach et al., 2011) help to augment marginally in F-score"
W14-0301,W05-0821,0,0.36147,"Missing"
W14-0301,P07-2045,0,0.00603561,"construction consists of two pivotal elements: the features (the SMT system dependent or independent information extracted for each word to represent its characteristics) and the machine learning method (to train the prediction model). Motivated 3 rank in WMT 2013 Quality Estimation Shared Task (Luong et al., 2013b). For building the WCE training and test sets, we use a dataset of 10,881 French sentences (Potet et al., 2012) , and apply a baseline SMT system to generate hypotheses (1000-best list). Our baseline SMT system (presented for WMT 2010 evaluation) keeps the Moses’s default setting (Koehn et al., 2007): log-linear model with 14 weighted feature functions. The translation model is trained on the Europarl and News parallel corpora of WMT102 evaluation campaign (1,638,440 sentences). The target language model is trained by the SRI language modeling toolkit (Stolcke, 2002) on the news monolingual corpus (48,653,884 sentences). Translators were then invited to correct MT outputs, giving us the same amount of post editions (Potet et al., 2012). The set of triples (source, hypothesis, post edition) is then divided into the training set (10000 first triples) and test set (881 remaining). To train t"
W14-0301,2003.mtsummit-papers.52,0,0.614644,"put. If the error is predicted for each word, this becomes WCE. The interesting uses of WCE include: pointing out the words that need to be corrected by the post-editor, telling readers about the reliability of a specific portion, and selecting the best segments among options from multiple translation systems for combination. Dealing with this problem, various approaches have been proposed: Blatz et al. (2003) combine several features using neural network and naive Bayes learning algorithms. One of the most effective feature combinations is the Word Posterior Probability (WPP) as suggested by Ueffing et al. (2003) associated with IBM-model based features (Blatz et al., 2004). Ueffing and Ney (2005) propose an approach for phrase-based translation models: a phrase is a sequence of contiguous words and is extracted from the word-aligned bilingual training corpus. The confidence value of each word is then computed by summing over all phrase pairs in which the target part contains this word. Xiong et al. (2010) integrate target word’s Part-Of-Speech (POS) and train them by Maximum Entropy Model, allowing significative gains in comparison to WPP features. The novel features from source side, alignment conte"
W14-0301,N07-1063,0,0.541731,"Missing"
W14-0301,P10-1052,0,0.148325,"Missing"
W14-0301,D07-1080,0,0.0823775,"Missing"
W14-0301,W13-2248,1,0.925434,"labeling process, we employ the Conditional Random Fields (CRFs) for our model training, with WAPITI toolkit (Lavergne et al., 2010). Basically, CRF computes the probability of the output sequence Y = (y1 , y2 , ..., yN ) given the input sequence X = (x1 , x2 , ..., xN ) by: Essentially, a WCE system construction consists of two pivotal elements: the features (the SMT system dependent or independent information extracted for each word to represent its characteristics) and the machine learning method (to train the prediction model). Motivated 3 rank in WMT 2013 Quality Estimation Shared Task (Luong et al., 2013b). For building the WCE training and test sets, we use a dataset of 10,881 French sentences (Potet et al., 2012) , and apply a baseline SMT system to generate hypotheses (1000-best list). Our baseline SMT system (presented for WMT 2010 evaluation) keeps the Moses’s default setting (Koehn et al., 2007): log-linear model with 14 weighted feature functions. The translation model is trained on the Europarl and News parallel corpora of WMT102 evaluation campaign (1,638,440 sentences). The target language model is trained by the SRI language modeling toolkit (Stolcke, 2002) on the news monolingual"
W14-0301,C12-1121,0,0.0354352,"Missing"
W14-0301,W06-1626,0,\N,Missing
W14-0301,P11-1022,0,\N,Missing
W14-0301,P10-1062,0,\N,Missing
W14-0301,aziz-etal-2012-pet,0,\N,Missing
W14-0301,2012.eamt-1.31,0,\N,Missing
W14-0301,P08-2010,0,\N,Missing
W14-0301,2012.tc-1.5,0,\N,Missing
W14-3342,N07-1051,0,0.0208934,"re for POS tag. • Language Model (LM) features: the “longest target n-gram length” and “longest source n-gram length”(length of the longest sequence created by the current target (source aligned) word and its previous ones in the target (source) LM). For example, with the target word wi : if the sequence wi−2 wi−1 wi appears in the target LM but the sequence wi−3 wi−2 wi−1 wi does not, the n-gram value for wi will be 3. • The word’s constituent label and its depth in the tree (or the distance between it and the tree root) obtained from the constituent tree as an output of the Berkeley parser (Petrov and Klein, 2007) (trained over a Spanish treebank: AnCora4 ). • Occurrence in Google Translate hypothesis: we check whether this target word appears in • Word Occurrence in multiple translations: one novel point in this year’s shared task is that the targets come from multiple MT 3 5 4 http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/ http://clic.ub.edu/corpus/en/ancora 337 6 http://search.cpan.org/dist/Lingua-Wordnet/Wordnet.pm http://babelnet.org System BL(bin) outputs (from systems or from humans) for the same source sentences. Obviously, one would have a “natural” intuition that: the occurrence"
W14-3342,W13-2245,0,0.0545799,"election strategy on our development set, before dealing with the test set for submission. 1 1.1 • Level 1: the Good class is kept intact, whereas Bad one is further divided into subcategories: Accuracy issue (the word does not accurately reflect the source text) and Fluency issue (the word does not relate to the form or content of the target text). • Multi-class: more detailed judgement, where the translation errors are further decomposed into 16 labels based on MQM1 metric. 1.2 Related work WMT 2013 witnessed several attempts dealing with this evaluation type in its first launch. Han et al. (2013); Luong et al. (2013) employed the Conditional Random Fields (CRF) (Lafferty et al., 2001) model as their Machine Learning method to address the problem as a sequence labeling task. Meanwhile, Bicici (2013) extended the global learning model by dynamic training with adaptive weight updates in the perceptron training algorithm. As far as prediction indicators are concerned, Bicici (2013) proposed seven word feature types and found among them the “common cover links” (the links that point from the leaf node containing this word to other leaf nodes in the same subtree of the syntactic tree) the m"
W14-3342,P07-2045,0,0.00888298,"ts the most outstanding system for submission. The last section summarizes the approach and opens new outlook. 2 • Spanish - English 2013 MT system On the contrary, no specific MT setting is provided (e.g. the code to re-run Moses system like WMT 2013), leading to the unavailability of some crucial resources, such as the N-best list and alignment information. Coping with this, we firstly thought of using the Moses “Constrained Decoding” option as a method to tie our (already available) decoder’s output to the given target translations (this feature is supported by the latest version of Moses (Koehn et al., 2007) in 2013). Our hope was that, by doing so, both N-best list and alignment information would be generated during decoding. But the decoder failed to output all translations (only 1/4 was obtained) when the number of allowed unknown words (-max-unknowns) was set as 0. Switching to non zero value for this option did not help either since, even if more outputs were generated, alignment information was biased in that case due to additional/missing words in the obtained MT output. Ultimately, we decided to employ GIZA++ toolkit (Och and Ney, 2003) to obtain at least the alignment information (and as"
W14-3342,P10-1052,0,0.0614962,"Missing"
W14-3342,W13-2242,0,\N,Missing
W14-3342,J03-1002,0,\N,Missing
W15-0713,W12-3102,0,0.0609502,"Missing"
W15-0713,W14-0311,0,0.018142,"BLEU (Papineni et al., 2002) scores of 36.88 and 37.58 on the IWSLT tst2011 and test2012 corpora, respectively (BLEU evaluated with case and punctuation). The training data used is out-of-domain for the task of literary translation, and as such is clearly not ideal for translating literary texts. In future work, it would be desirable to at least collect literary texts in French to adapt the target language model, and if possible gain access to other works and translations of the same author. Additionally, in future work we may examine the use of real-time translation model adaptation, such as Denkowski et al. (2014). 3.3 Post-editing We use the SECTra w.1 post-editing interface of Huynh et al. (2008). This tool also forms the foundation that gave rise to the interactive Multilingual Access Gateway (iMAG) framework for enabling multilingual website access, with incremental improvement and quality control of the translations. It has been used for many projects (Wang and Boitet, 117 2013), including translation of the EOLLS encyclopedia, as well as multilingual access to dozens of websites (80 demonstrations, 4 industrial contracts, 10 target languages, 820k post-edited segments). Figure 1 shows the post-ed"
W15-0713,2012.iwslt-evaluation.1,0,0.0972504,"Missing"
W15-0713,P07-2045,0,0.01224,"o a book of about 500 pages) in the news domain were collected through crowdsourcing, resulting in one of the largest freely available corpora of postedited machine translations.4 It is, for example, three times larger than that collected by Specia et al. (2010), a well known benchmark in the field. Following Potet et al. (2012), we divide the document to be translated into three equal parts. A translation/post-edition/adaptation loop was applied to the three blocks of text according to the following process: • The first third of the document was translated from English to French using Moses (Hoang et al., 2007), a state-of-the-art phrase-based machine translation system. This machine translation output was then post-edited. • The post-edited data from the third of the document was used to train an updated domainadapted English-French MT system. Given the small amount of post-edited data, adaptation at this point consisted only in adapting the weights of the log-linear SMT model (by using the corrected first third as a development corpus). A similar method is suggested by Pecina et al. (2012) for domain adaptation with a limited quantity of data (we are aware that other more advanced domain adaptatio"
W15-0713,huynh-etal-2008-sectra,0,0.0331025,"orpora, respectively (BLEU evaluated with case and punctuation). The training data used is out-of-domain for the task of literary translation, and as such is clearly not ideal for translating literary texts. In future work, it would be desirable to at least collect literary texts in French to adapt the target language model, and if possible gain access to other works and translations of the same author. Additionally, in future work we may examine the use of real-time translation model adaptation, such as Denkowski et al. (2014). 3.3 Post-editing We use the SECTra w.1 post-editing interface of Huynh et al. (2008). This tool also forms the foundation that gave rise to the interactive Multilingual Access Gateway (iMAG) framework for enabling multilingual website access, with incremental improvement and quality control of the translations. It has been used for many projects (Wang and Boitet, 117 2013), including translation of the EOLLS encyclopedia, as well as multilingual access to dozens of websites (80 demonstrations, 4 industrial contracts, 10 target languages, 820k post-edited segments). Figure 1 shows the post-editing interface in advanced mode. In advanced mode, multiple automatic translations of"
W15-0713,2005.mtsummit-papers.11,0,0.00797042,"which the author, who teaches narrative technique at the university level, never puts aside his poetic ambition, his humour and his fascination for the impact of science and technology on the society. 3.2 MT system used Our machine translation system is a phrase-based system using the Moses toolkit (Hoang et al., 2007). Our system is trained using the data provided in the IWSLT machine translation evaluation campaign (Federico et al., 2012), representing a cumulative total of about 25M sentences: • news-c: version 7 of the News-Commentary corpus, • europarl: version 7 of the Europarl corpus7 (Koehn, 2005), • un: the United-nations corpus,8 • eu-const: corpus which is freely available (Tiedemann, 2009), • dgt-tm: DGT Multilingual Translation Memory of the Acquis Communautaire (Steinberger et al., 2012), • pct: corpus of Parallel Patent Applications9 , • gigaword: 5M sentences extracted from the Gigaword corpus; after cleaning, the whole Gigaword corpus was sorted at sentence level according to the sum of perplexities of the source (English) and the target (French) based on two French and English pretrained language models. Finally, the 5M subset was obtained after filtering out the whole Gigawo"
W15-0713,P02-1040,0,0.10286,"onst+dgt-tm+pct; gigaword5M). The French part of the same corpus is used for language model training, with the addition of the news-shuffle corpus provided as part of the WMT 2012 campaign (Callison-Burch et al., 2012). A 5-gram language model with modified KneserNey smoothing is learned separately for each corpus using the SRILM toolkit (Stolcke, 2002); these models are then interpolated by optimizing perplexity on the IWSLT dev2010 corpus. The weights for the final machine translation system are optimized using the data from the English-French MT task of IWSLT 2012. The system obtains BLEU (Papineni et al., 2002) scores of 36.88 and 37.58 on the IWSLT tst2011 and test2012 corpora, respectively (BLEU evaluated with case and punctuation). The training data used is out-of-domain for the task of literary translation, and as such is clearly not ideal for translating literary texts. In future work, it would be desirable to at least collect literary texts in French to adapt the target language model, and if possible gain access to other works and translations of the same author. Additionally, in future work we may examine the use of real-time translation model adaptation, such as Denkowski et al. (2014). 3.3"
W15-0713,C12-1135,0,0.0236539,"Missing"
W15-0713,potet-etal-2012-collection,1,0.888857,"Missing"
W15-0713,specia-etal-2010-dataset,0,0.0159852,"e at https://github.com/ powersmachinetranslation/DATA 3 https://sites.google.com/site/clfl2014a 115 to create effects using the language and to express meanings (explicit or implied). 3 Methodology For this study, we follow a variant of the post-editing methodology established by Potet et al. (2012). In that work, 12,000 post-edited segments (equivalent to a book of about 500 pages) in the news domain were collected through crowdsourcing, resulting in one of the largest freely available corpora of postedited machine translations.4 It is, for example, three times larger than that collected by Specia et al. (2010), a well known benchmark in the field. Following Potet et al. (2012), we divide the document to be translated into three equal parts. A translation/post-edition/adaptation loop was applied to the three blocks of text according to the following process: • The first third of the document was translated from English to French using Moses (Hoang et al., 2007), a state-of-the-art phrase-based machine translation system. This machine translation output was then post-edited. • The post-edited data from the third of the document was used to train an updated domainadapted English-French MT system. Give"
W15-0713,steinberger-etal-2012-dgt,0,0.0425576,"Missing"
W15-0713,W12-2503,0,0.18657,"machine translation metrics. In §5, we attempt to assess machine translation quality beyond automated metrics, through a human assessment of the final translation; this assessment was performed by a panel of readers and by the official French translator of Richard Powers. 2 Related Work While the idea of post-editing machine translations of scientific and technical works is nearly as old as machine translation (see, for example (Oettinger, 1954)), very little scholarship to date has examined the use of machine translation or post-editing for literary documents. The most closely related work (Voigt and Jurafsky, 2012) that we were able to identify was presented at the ACL workshop on Computational Linguistics for Literature3 ; since 2012, that workshop has examined the use of NLP in the literary field. Voigt and Jurafsky (2012) examine how referential cohesion is expressed in literary and nonliterary texts and how this cohesion affects translation (experiments on Chinese literature and news). The present paper, however, tries to investigate if computer-assisted translation of a complete (and initially un-translated) short story, is feasible or not. For the purposes of this paper, we now define what constit"
W15-0713,2012.amta-wptp.10,0,0.02428,"ality translations (when compared to translating from scratch).1 Autodesk also carried out an experiment to test whether the use of MT would improve the productivity of translators. Results from that experiment (Zhechev, 2012) show that post-editing machine translation output significantly increases productivity when compared to translating a document from scratch. This result held regardless of the language pair, the experience level of the translator, and the translator’s stated preference for post-editing or translating from scratch. These results from academia (Garcia, 2011) and industry (Zhechev, 2012) regarding translation in specialized areas lead us to ask the following questions: Current machine translation (MT) techniques are continuously improving. In specific areas, post-editing (PE) can enable the production of high-quality translations relatively quickly. But is it feasible to translate a literary work (fiction, short story, etc) using such an MT+PE pipeline? This paper offers an initial response to this question. An essay by the American writer Richard Powers, currently not available in French, is automatically translated and post-edited and then revised by non-professional transl"
W17-2502,S16-1081,0,0.0607981,"s in replacing each word of one text by its most likely translations in the language of the other text, leading to a bags-of-words. We use DBNary (S´erasset, 2015) to get the translations. The metric used to compare two texts is a monolingual matching based on strict intersection of bags-of-words. 3 http://nlp.stanford.edu/software/ CRF-NER.shtml 4 https://wit3.fbk.eu/ 5 http://www.statmt.org/wmt13/ translation-task.html#download 8 tables, at chunk and sentence level, the overall F1 score over all sub-corpora of one method in one particular language pair is given. More recently, SemEval-2016 (Agirre et al., 2016) proposed a new subtask on evaluation of cross-lingual semantic textual similarity. Despite the fact that it was the first year that this subtask was attempted, there were 26 submissions from 10 teams. Most of the submissions relied on a machine translation step followed by a monolingual semantic similarity, but 4 teams tried to use learned vector representations (on words or sentences) combined with machine translation confidence (for instance the submission of Lo et al. (2016) or Ataman et al. (2016)). The method that achieved the best performance (Brychcin and Svoboda, 2016) was a supervise"
W17-2502,S16-1086,0,0.0436798,"Missing"
W17-2502,S16-1102,0,0.0623178,"Missing"
W17-2502,J93-2003,0,0.106592,": Number of aligned documents, sentences and noun chunks by sub-corpus. Syntax-Based Models Length Model (Pouliquen et al., 2003), CL-CnG (Potthast et al., 2011), Cognateness Dictionary-Based Models CL-VSM, CL-CTS (Pataki, 2012) Parallel Corpora-Based Models CL-ASA (Pinto et al., 2009), CL-LSI, CL-KCCA Comparable Corpora-Based Models CL-KGA, CL-ESA (Potthast et al., 2008) MT-Based Models Translation + Monolingual Analysis (Muhr et al., 2010) Figure 1: Taxonomy of Potthast et al. (2011), enriched by the study of Danilova (2013), of different approaches for cross-language similarity detection. (Brown et al., 1993) on the concatenation of TED4 (Cettolo et al., 2012) and News5 parallel corpora. We reuse the implementation of Pinto et al. (2009) that proposed a formula that factored the alignment function. document retrieval. Our implementation uses a part of Wikipedia, from which our test data was removed, to build the vector representations of the texts. Cross-Language Explicit Semantic Analysis (CL-ESA) is based on the explicit semantic analysis model introduced for the first time by Gabrilovich and Markovitch (2007), which represents the meaning of a document by a vector based on the vocabulary derive"
W17-2502,S16-1089,0,0.014298,"ecently, SemEval-2016 (Agirre et al., 2016) proposed a new subtask on evaluation of cross-lingual semantic textual similarity. Despite the fact that it was the first year that this subtask was attempted, there were 26 submissions from 10 teams. Most of the submissions relied on a machine translation step followed by a monolingual semantic similarity, but 4 teams tried to use learned vector representations (on words or sentences) combined with machine translation confidence (for instance the submission of Lo et al. (2016) or Ataman et al. (2016)). The method that achieved the best performance (Brychcin and Svoboda, 2016) was a supervised system built on a word alignment-based method proposed by Sultan et al. (2015). This very recent method is, however, not evaluated in this paper. 4 As a preliminary remark, one should note that CL-C3G and CL-ESA lead to the same results for a given language pair (same performance if we reverse source and target languages) due to their symmetrical property. Another remark we can make is that methods are consistent across language pairs: best performing methods are mostly the same, whatever the language pair considered. This is confirmed by the calculation of the Pearson correl"
W17-2502,S17-2001,0,0.0165888,"2 The previous sub-section has shown a consistent behavior of methods across language pairs (strongly consistent) and granularities (less strongly consistent). For this reason, we now propose a detailed analysis for different sub-corpora, for the English-French language pair - at chunk and sentence level - only. Providing these results for all language pairs and granularities would take too much space. Moreover, we also run those state-of-the-art methods on the dataset of the Spanish-English cross-lingual Semantic Textual Similarity task of SemEval-2016 (Agirre et al., 2016) and SemEval-2017 (Cer et al., 2017), and propose a shallower but equally rigorous analysis. However, all those results are also made available as supplementary material on our paper Web page. Table 8 shows the performances of methods on the EN→FR sub-corpora. As mentioned earlier, CL-C3G is in general the most effective method. CL-ESA seems to show better results on comparable corpora, like Wikipedia. In contrast, CL-ASA obtains better results on parallel corpora such as JRC or Europarl collections. CL-CTS and T+MA are pretty efficient and versatile too. It is also interesting to note that the results of the methods are well co"
W17-2502,2012.eamt-1.60,0,0.012308,"chunks by sub-corpus. Syntax-Based Models Length Model (Pouliquen et al., 2003), CL-CnG (Potthast et al., 2011), Cognateness Dictionary-Based Models CL-VSM, CL-CTS (Pataki, 2012) Parallel Corpora-Based Models CL-ASA (Pinto et al., 2009), CL-LSI, CL-KCCA Comparable Corpora-Based Models CL-KGA, CL-ESA (Potthast et al., 2008) MT-Based Models Translation + Monolingual Analysis (Muhr et al., 2010) Figure 1: Taxonomy of Potthast et al. (2011), enriched by the study of Danilova (2013), of different approaches for cross-language similarity detection. (Brown et al., 1993) on the concatenation of TED4 (Cettolo et al., 2012) and News5 parallel corpora. We reuse the implementation of Pinto et al. (2009) that proposed a formula that factored the alignment function. document retrieval. Our implementation uses a part of Wikipedia, from which our test data was removed, to build the vector representations of the texts. Cross-Language Explicit Semantic Analysis (CL-ESA) is based on the explicit semantic analysis model introduced for the first time by Gabrilovich and Markovitch (2007), which represents the meaning of a document by a vector based on the vocabulary derived from Wikipedia, to find a document within a corpus"
W17-2502,R13-2008,0,0.108283,"for Computational Linguistics tion of the methods across language pairs, while section 5.2 presents a detailed analysis on only English-French pair. Finally, section 6 concludes this work and gives a few perspectives. 2 of knowing why texts are similar and thus to assimilate these similarities to plagiarism. At the moment, there are five classes of approaches for cross-language plagiarism detection. The aim of each method is to estimate if two textual units in different languages express the same message or not. Figure 1 presents a taxonomy of Potthast et al. (2011), enriched by the study of Danilova (2013), of the different cross-language plagiarism detection methods grouped by class of approaches. We only describe below the state-of-the-art methods that we evaluate in the paper, one for each class of approaches (those in bold in the Figure 1). Dataset The reference dataset used during our study is the new dataset2 recently introduced by Ferrero et al. (2016). The dataset was specially designed for a rigorous evaluation of cross-language textual similarity detection. The different characteristics of the dataset are synthesized in Table 1, while Table 2 presents the number of aligned units by su"
W17-2502,L16-1657,1,0.892749,"Missing"
W17-2502,S15-2027,0,0.0410248,"Missing"
W17-4608,L16-1611,1,0.666004,"Missing"
W17-4608,E14-2006,0,0.0660452,"Missing"
W17-4608,W15-2522,0,0.0346748,"Missing"
W17-4772,P07-2045,0,0.00424571,"Such translation workflows can result in the production of new training data, that may be re-injected into the system in order to improve it. Common ways to do so are retraining, incremental training, translation memories, or automatic postediting (Chatterjee et al., 2015). In Automatic Post-Editing (APE), the MT system is usually considered as a blackbox: a separate APE system takes as input the outputs of this MT system, and tries to improve them. Statistical PostEditing (SPE) was first proposed by Simard et al. (2007). It consists in training a Statistical Machine Translation (SMT) system (Koehn et al., 2007), to translate from translation hypotheses to a human post-edited version of those. B´echara et al. (2011) then proposed a way to integrate both the transla1.1 Predicting Edit Operations We think that post-editing should be closer to spelling correction than machine translation. Our work is based on Libovick´y et al. (2016), who train a model to predict edit operations instead of words. We predict 4 types of operations: KEEP, DEL, INS(word), and EOS (the end of sentence marker). This results in a vocabulary with three symbols plus as many symbols as there are possible insertions. A benefit of"
W17-4772,W16-2361,0,0.162772,"Missing"
W17-4772,2011.mtsummit-papers.35,0,0.0573549,"Missing"
W17-4772,E17-2056,0,0.0205436,"Missing"
W17-4772,W16-2379,0,0.0599327,"Missing"
W17-4772,2012.iwslt-papers.19,1,0.886063,"Missing"
W17-4772,P15-2026,0,0.0126661,"on two datasets (en-de and de-en) that are made available for the task. 1 Introduction It has become quite common for human translators to use machine translation (MT) as a first step, and then to manually post-edit the translation hypothesis. This can result in a significant gain of time, compared to translating from scratch (Green et al., 2013). Such translation workflows can result in the production of new training data, that may be re-injected into the system in order to improve it. Common ways to do so are retraining, incremental training, translation memories, or automatic postediting (Chatterjee et al., 2015). In Automatic Post-Editing (APE), the MT system is usually considered as a blackbox: a separate APE system takes as input the outputs of this MT system, and tries to improve them. Statistical PostEditing (SPE) was first proposed by Simard et al. (2007). It consists in training a Statistical Machine Translation (SMT) system (Koehn et al., 2007), to translate from translation hypotheses to a human post-edited version of those. B´echara et al. (2011) then proposed a way to integrate both the transla1.1 Predicting Edit Operations We think that post-editing should be closer to spelling correction"
W17-4772,N07-1064,0,0.101202,"Missing"
W17-4772,2006.amta-papers.25,0,0.0582021,"there are strong disadvantages: for one, our SRC and MT sequences have a much poorer vocabulary as those obtained with roundtrip translation (because we only get words that belong to the APE corpus). Yet, we hope that the richer target (PE) may help our models learn a better language model. Experiments This year’s APE task consists in two sub-tasks: a task on English to German post-editing in the IT domain (en-de), and a task on German to English post-editing in the medical domain (de-en). Table 1 gives the size of each of the corpora available. The goal of both tasks is to minimize the HTER (Snover et al., 2006) between our automatic post-editing output, and the human post-editing output. The en-de 23k training set is a concatenation of last year’s 12k dataset, and a newly released 11k dataset. A synthetic corpus was built and used by the winner of last year’s edition (JunczysDowmunt and Grundkiewicz, 2016), and is available this year as additional data (500k and 4M corpora). For the en-de task, we limit our use of external data to the 500k corpus. For the de-en task, we built our own synthetic corpus, using a technique similar to (Junczys-Dowmunt and Grundkiewicz, 2016). 2.1 Synthetic Data TER filte"
W17-4772,W17-4775,0,0.262401,"rce language) sentence. More recent contributions in the same vein are (Chatterjee et al., 2016; Pal et al., 2016). When too little training data is available, one may resort to using synthetic corpora: with simulated PE (Potet et al., 2012), or round-trip translation (Junczys-Dowmunt and Grundkiewicz, 2016). Recently, with the success of Neural Machine Translation (NMT) models (Sutskever et al., 2014; Bahdanau et al., 2015), new kinds of APE methods have been proposed that use encoder-decoder approaches (Junczys-Dowmunt and Grundkiewicz, 2016, 2017; Libovick´y et al., 2016; Pal et al., 2017; Hokamp, 2017), in which a Recurrent Neural Network (RNN) encodes the source sequence into a fixed size representation (encoder), and another RNN uses this representation to output a new sequence. These encoder-decoder models are generally enhanced with an attention mechanism, which learns to look at the entire sequence of encoder states (Bahdanau et al., 2015; Luong et al., 2016). We present novel neural architectures for automatic post-editing. Our models learn to generate sequences of edit operations, and use a taskspecific attention mechanism which gives information about the word being post-edited. Thi"
W17-4772,W16-2378,0,0.327597,"WMT17 Automatic Post-Editing Task Alexandre B´erard Olivier Pietquin Univ. Lille, CNRS, Centrale Lille, Inria, UMR 9189 CRIStAL alexandre.berard@ed.univ-lille1.fr olivier.pietquin@univ-lille1.fr Laurent Besacier LIG, Univ. Grenoble Alpes, CNRS laurent.besacier@univ-grenoble-alpes.fr Abstract tion hypothesis and the original (source language) sentence. More recent contributions in the same vein are (Chatterjee et al., 2016; Pal et al., 2016). When too little training data is available, one may resort to using synthetic corpora: with simulated PE (Potet et al., 2012), or round-trip translation (Junczys-Dowmunt and Grundkiewicz, 2016). Recently, with the success of Neural Machine Translation (NMT) models (Sutskever et al., 2014; Bahdanau et al., 2015), new kinds of APE methods have been proposed that use encoder-decoder approaches (Junczys-Dowmunt and Grundkiewicz, 2016, 2017; Libovick´y et al., 2016; Pal et al., 2017; Hokamp, 2017), in which a Recurrent Neural Network (RNN) encodes the source sequence into a fixed size representation (encoder), and another RNN uses this representation to output a new sequence. These encoder-decoder models are generally enhanced with an attention mechanism, which learns to look at the enti"
W17-4772,N16-1004,0,0.028001,"following sequence of operations: keep ""The"", delete ""cats"", insert ""cat"", keep ""is"", keep ""grey"", insert ""."" The final result is the postedited sequence ""The cat is grey ."" We preprocess the data to extract such edit sequences by following the shortest edit path (similar to a Levenshtein distance, without substitutions, or with a substitution cost of +∞). 1.2 Dev Train 1.3 Chaining Encoders The model we proposed does not make any use of the source side SRC. Making use of this information is not very straightforward in our framework. Indeed, we may consider using a multi-encoder architecture (Zoph and Knight, 2016; JunczysDowmunt and Grundkiewicz, 2017), but it does not make much sense to align an edit operation with the source sequence, and such a model struggles to learn a meaningful alignment. where st is the current state of the decoder, hi is the ith state of the encoder (corresponding to the ith input word). A is the length of the input sequence. W1 , W2 and b2 are learned parameters of the model. This attention vector is used to generate the next output symbol wt and to compute the next state of the decoder st+1 . However, we don’t predict words, but edit operations, which means that we can do s"
W17-4772,I17-1013,0,0.28416,"Missing"
W18-5402,I17-1001,0,0.11056,"Missing"
W18-5402,gravier-etal-2012-etape,1,0.819103,"Missing"
W18-5402,D14-1181,0,0.00777471,"Missing"
W18-5402,D16-1159,0,0.0307087,"Analyzing Learned Representations of a Deep ASR Performance Prediction Model Zied Elloumi1,2 Laurent Besacier2 Olivier Galibert1 Benjamin Lecouteux2 1 2 Laboratoire national de m´etrologie et d’essais (LNE) , France Univ. Grenoble Alpes, CNRS, Grenoble INP, LIG, F-38000 Grenoble, France firstname.name@lne.fr firstname.name@univ-grenoble-alpes.fr Abstract general problem in AI. Recent papers started to address this issue and analyzed hidden representations learned during training of different natural language processing models (Mohamed et al., 2012; Wu and King, 2016; Belinkov and Glass, 2017; Shi et al., 2016; Belinkov et al., 2017; Wang et al., 2017). Contribution. This work is dedicated to the analysis of speech signal embeddings and text embeddings learnt by the CNN during training of our ASR performance prediction model. Our goal is to better understand which information is captured by the deep model and its relation with conditioning factors such as speech style, accent or broadcast program type. For this, we use a data set presented in (Elloumi et al., 2018) which contains a large amount of speech utterances taken from various collections of French broadcast programs. Following a methodology"
W18-5804,U17-1006,0,0.0305163,"world’s languages are expected to go extinct during this century – as much as half of them according to Crystal (2002) and Janson (2003). Such predictions have subsequently fostered a growing interest for a new field, Computational Language Documentation (CLD), as it is now clear that traditional field linguistics alone will not meet the challenge of preserving and documenting all of these languages. CLD attempts to make the most recent research in speech and language technologies available to linguists working on language preservation and documentation (e.g. (Anastasopoulos and Chiang, 2017; Adams et al., 2017)). A remarkable effort in this direction has improved the data collection 1 We indifferently use the terms word discovery and word segmentation to denote the task defined in Section 2.2. 32 Proceedings of the 15th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 32–42 c Brussels, Belgium, October 31, 2018. 2018 The Special Interest Group on Computational Morphology and Phonology https://doi.org/10.18653/v1/P17 capture simple aspects of the syntax and some less trivial aspects of the morphological and phonological structures. As discussed below, both"
W18-5804,C16-1086,0,0.0160834,"nguage, Basaa (A43 (Hamlaoui and Makasso, 2015)). mboshi/myene corresponds to a somewhat crude morphology of Mboshi, also applicable to Myene. Last mboshi/myene_NV refines mboshi/myene with a specification of the morphology of nouns and verbs. Additionally, for basaa, mboshi/myene and mboshi/myene_NV which introduce a notion of prefix, we also test a variant (called respectively basaa+, mboshi/myene+ and mboshi/myene_NV+) containing an explicit list of prefixes in Mboshi. Grammars 4.1 Structuring Grammar Sets Our starting point is the set of grammars used in (Johnson and Goldwater, 2009) and (Eskander et al., 2016) which we progressively specialize through an iterative refinement process involving both field linguists and computer scientists. As we wish to evaluate specific linguistic hypotheses, the initial space of interesting grammars has been generalized in a modular, systematic, and hierarchical way as follows. We distinguish four sections in each grammar: sentence, word, syllable, character. For each section, we test multiple hypotheses, gradually incorporating more linguistic structure. Every hypothesis inside a given section can be combined with every hypothesis of any other section,7 thereby al"
W18-5804,L18-1531,1,0.878737,"Missing"
W18-5804,W17-0123,0,0.0281211,"troduction A large number of the world’s languages are expected to go extinct during this century – as much as half of them according to Crystal (2002) and Janson (2003). Such predictions have subsequently fostered a growing interest for a new field, Computational Language Documentation (CLD), as it is now clear that traditional field linguistics alone will not meet the challenge of preserving and documenting all of these languages. CLD attempts to make the most recent research in speech and language technologies available to linguists working on language preservation and documentation (e.g. (Anastasopoulos and Chiang, 2017; Adams et al., 2017)). A remarkable effort in this direction has improved the data collection 1 We indifferently use the terms word discovery and word segmentation to denote the task defined in Section 2.2. 32 Proceedings of the 15th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 32–42 c Brussels, Belgium, October 31, 2018. 2018 The Special Interest Group on Computational Morphology and Phonology https://doi.org/10.18653/v1/P17 capture simple aspects of the syntax and some less trivial aspects of the morphological and phonological structures. As d"
W18-5804,W14-2201,0,0.0714917,"Missing"
W18-5804,P06-1085,0,0.108277,"Missing"
W18-5804,Q14-1008,0,0.0362652,"Missing"
W18-5804,D13-1034,0,0.0370391,"Missing"
W18-5804,J99-1004,0,0.0168827,"of the form A → β, with A ∈ N and β ∈ (N ∪ W )∗ , and S ∈ N the start symbol. Our grammars will be used to analyze the structure of complete utterances and the start symbol S will always correspond to the sentence top-level. Assuming that S, Words, and Word belong to N , the top level rules will typically look like: S → Words; Words → Word Words; Words → Word, the last two rules abbreviated as Words → Word +. Probabilistic CFGs (PCFGs) (Johnson, 1998) extend this model by associating each rule with a scalar value θA→β , such that for each A ∈ N , P β θA→β = 1. Under some technical conditions (Chi, 1999), PCFGs define probability distributions over the set of parse trees, where the probability of a tree is a product of the probability of the rules it contains. PCFGs can be learned in a supervised way from treebanks or in a unsupervised manner using, for instance, the EM algorithm (Lari and Young, 1990). PCFGs make unrealistic independence assumptions between the different subparts of a tree, an observation that has yielded many subsequent variations and extensions. Adaptor grammars (AGs) (Johnson et al., 2007) define a powerful mechanism to manipulate PCFG distributions to better match the oc"
W18-5804,J98-4004,0,0.0832912,"e useful for word discovery. A CFG is a 4-tuple G = (N, W, R, S) where N and W are respectively the non-terminal and terminal symbols, R a finite set of rules of the form A → β, with A ∈ N and β ∈ (N ∪ W )∗ , and S ∈ N the start symbol. Our grammars will be used to analyze the structure of complete utterances and the start symbol S will always correspond to the sentence top-level. Assuming that S, Words, and Word belong to N , the top level rules will typically look like: S → Words; Words → Word Words; Words → Word, the last two rules abbreviated as Words → Word +. Probabilistic CFGs (PCFGs) (Johnson, 1998) extend this model by associating each rule with a scalar value θA→β , such that for each A ∈ N , P β θA→β = 1. Under some technical conditions (Chi, 1999), PCFGs define probability distributions over the set of parse trees, where the probability of a tree is a product of the probability of the rules it contains. PCFGs can be learned in a supervised way from treebanks or in a unsupervised manner using, for instance, the EM algorithm (Lari and Young, 1990). PCFGs make unrealistic independence assumptions between the different subparts of a tree, an observation that has yielded many subsequent v"
W18-5804,W08-0704,0,0.775201,"Missing"
W18-5804,P08-1046,0,0.0606297,"Missing"
W18-5804,P14-1027,0,0.0181962,"es of studies, AGs are shown to generalize models of unsupervised word segmentations such as the Bayesian nonparametric model of Goldwater (2006), delivering hierarchical (rather than flat) decompositions for words or sentences. While AGs are essentially viewed as an unsupervised grammatical inference tool, several authors have also tried to better inform grammar inference with external knowledge sources. This is the case of Sirts and Goldwater (2013), who study a semi-supervised learning scheme combining annotated data (parse trees) with raw sentences. The linguistic knowledge considered in (Johnson et al., 2014) aims to better model function words in a Conclusion This paper had two main goals: (1) improve upon a strong baseline for the unsupervised discovery of words in two very low-resource Bantu languages; (2) explore the Adaptor Grammar framework as an analysis and prediction tool for linguists studying a new language. Systematic experiments with 162 grammar configurations for each language have shown that using AGs for word segmentation is a way to test linguistic hypotheses during a language documentation process. Conversely, we have also shown that specializing a generic grammar with language s"
W18-5804,N09-1036,0,0.0291861,"phology of a well-studied Bantu language, Basaa (A43 (Hamlaoui and Makasso, 2015)). mboshi/myene corresponds to a somewhat crude morphology of Mboshi, also applicable to Myene. Last mboshi/myene_NV refines mboshi/myene with a specification of the morphology of nouns and verbs. Additionally, for basaa, mboshi/myene and mboshi/myene_NV which introduce a notion of prefix, we also test a variant (called respectively basaa+, mboshi/myene+ and mboshi/myene_NV+) containing an explicit list of prefixes in Mboshi. Grammars 4.1 Structuring Grammar Sets Our starting point is the set of grammars used in (Johnson and Goldwater, 2009) and (Eskander et al., 2016) which we progressively specialize through an iterative refinement process involving both field linguists and computer scientists. As we wish to evaluate specific linguistic hypotheses, the initial space of interesting grammars has been generalized in a modular, systematic, and hierarchical way as follows. We distinguish four sections in each grammar: sentence, word, syllable, character. For each section, we test multiple hypotheses, gradually incorporating more linguistic structure. Every hypothesis inside a given section can be combined with every hypothesis of an"
W18-5804,Q13-1021,0,0.298022,"licit hierarchical model of word internal structure ; an observation that was one of our primary motivations for using AGs in our language documentation work. In this series of studies, AGs are shown to generalize models of unsupervised word segmentations such as the Bayesian nonparametric model of Goldwater (2006), delivering hierarchical (rather than flat) decompositions for words or sentences. While AGs are essentially viewed as an unsupervised grammatical inference tool, several authors have also tried to better inform grammar inference with external knowledge sources. This is the case of Sirts and Goldwater (2013), who study a semi-supervised learning scheme combining annotated data (parse trees) with raw sentences. The linguistic knowledge considered in (Johnson et al., 2014) aims to better model function words in a Conclusion This paper had two main goals: (1) improve upon a strong baseline for the unsupervised discovery of words in two very low-resource Bantu languages; (2) explore the Adaptor Grammar framework as an analysis and prediction tool for linguists studying a new language. Systematic experiments with 162 grammar configurations for each language have shown that using AGs for word segmentat"
Y15-1016,W13-3520,0,0.0434952,"rojected labels with partially supervised monolingual information in order to filter out invalid label sequences. For example, Li et al. (2012), T¨ackstr¨om et al. (2013b) and Wisniewski et al. (2014) have proposed to improve projection performance by using a dictionary of valid tags for each word (coming from Wiktionary 2 ). In another vein, various studies based on crosslingual representation learning methods have proposed to avoid using such pre-processed and noisy alignments for label projection. First, these approaches learn language-independent features, across many different languages (Al-Rfou et al., 2013). Then, the induced representation space is used to train NLP tools by exploiting labeled data from the source language and apply them in the target language. To induce interlingual features, several resources have been used, including bilingual lexicon (Durrett et al., 2012; Gouws and Søgaard, 2015a) and parallel corpora (T¨ackstr¨om et al., 2013a; Gouws et al., 2015b). Cross-lingual representation learning have achieved good results in different NLP applications such as cross-language POS tagging and cross-language super sense (SuS) tagging (Gouws and Søgaard, 2015a), cross-language named en"
Y15-1016,C04-1053,0,0.523267,"daptation data (Section 5). Finally, Section 6 concludes our study and presents our future work. 2 Related Work Several studies have used cross-lingual projection to transfer linguistic annotations from a resourcerich language to a resource-poor language in order to train NLP tools for the target language. The projection approach has been successfully used to transfer several linguistic annotations between languages. Examples include POS (Yarowsky et al., 2001; Das and Petrov, 2011; Duong et al., 2013), named entity (Kim and Lee, 2012), syntactic constituent (Jiang et al., 2011), word senses (Bentivogli et al., 2004; Van der Plas and Apidianaki, 2014), and semantic role labeling (Pad´o , 2007; Annesi and Basili, 2010). In these approaches, the source language is tagged, and tags are projected from the source language to the target language through the use of word alignments in parallel corpora. Then, these partial noisy annotations can be used in conjunction with robust learning algorithms to build unsupervised NLP tools. One limitation of these approaches is due to the poor accuracy of word-alignment algo134 rithms, and also to the weak or incomplete inherent match between the two sides of a bilingual c"
Y15-1016,W06-2920,0,0.050027,"lied our method to build RNN POS taggers for three more target languages — German, Greek and Spanish — with English as the source language, in order to compare our results with those of (Das and Petrov, 2011; Duong et al., 2013; Gouws and Søgaard, 2015a). Our training and validation (English) data extracted from the Europarl corpus (Koehn, 2005) are a subset of the training data of (Das and Petrov, 2011; Duong et al., 2013). The sizes of the data sets are: 65, 000 (train) and 10, 000 (dev) bi-sentences. For testing, we used the same test corpora (from CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006)) as (Das and Petrov, 2011; Duong et al., 2013; Gouws and Søgaard, 2015a). The evaluation metric (per-token accuracy) and the Universal Tagset are the same as before. The source sides of the training corpora (ARCADE II and Europarl) and the validation corpora are tagged with the English TreeTagger Toolkit. Using the matching provided by Petrov et 4 For RNN a single system is used for German, Greek and Spanish 138 al. (2012) we map the TreeTagger and the CoNLL tagsets to a common Universal Tagset. In order to build our unsupervised tagger based on a Simple Cross-lingual Projection (Algorithm 1)"
Y15-1016,A00-1031,0,0.103079,"ct annotations from a source language to a target language, to build unsupervised POS taggers. The algorithm is shortly recalled below. Algorithm 1 : Simple POS Tagger 1: Tag source side of the parallel corpus. 2: Word align the parallel corpus with Giza++ (Och and Ney, 2000) or other word alignment tools. 3: Project tags directly for 1-to-1 alignments. 4: For many-to-one mappings project the tag of the middle word. 5: The unaligned words (target) are tagged with their most frequent associated tag in the corpus. 6: Learn POS tagger on target side of the bi-text with, for instance, TNT tagger (Brants, 2000). 3.2 Unsupervised POS Tagger Based on Recurrent Neural Network There are two major architectures of neural networks: Feedforward (Bengio et al., 2006) and Recurrent Neural Networks (RNN) (Mikolov et al., 2010). Sundermeyer et al. (2013) showed that language models based on recurrent architecture achieve better performance than language models based on feedforward architecture. This is due to the fact that recurrent neural networks do not use a context of limited size. This property led us to use, in our experiments, a simple recurrent architecture (Elman, 1990). PACLIC 29 In this section, we"
Y15-1016,P11-1061,0,0.617034,"h. Secondly, the performance of our approach is evaluated for German in a weakly supervised context, using several amounts of target adaptation data (Section 5). Finally, Section 6 concludes our study and presents our future work. 2 Related Work Several studies have used cross-lingual projection to transfer linguistic annotations from a resourcerich language to a resource-poor language in order to train NLP tools for the target language. The projection approach has been successfully used to transfer several linguistic annotations between languages. Examples include POS (Yarowsky et al., 2001; Das and Petrov, 2011; Duong et al., 2013), named entity (Kim and Lee, 2012), syntactic constituent (Jiang et al., 2011), word senses (Bentivogli et al., 2004; Van der Plas and Apidianaki, 2014), and semantic role labeling (Pad´o , 2007; Annesi and Basili, 2010). In these approaches, the source language is tagged, and tags are projected from the source language to the target language through the use of word alignments in parallel corpora. Then, these partial noisy annotations can be used in conjunction with robust learning algorithms to build unsupervised NLP tools. One limitation of these approaches is due to the"
Y15-1016,P13-2112,0,0.526186,"rmance of our approach is evaluated for German in a weakly supervised context, using several amounts of target adaptation data (Section 5). Finally, Section 6 concludes our study and presents our future work. 2 Related Work Several studies have used cross-lingual projection to transfer linguistic annotations from a resourcerich language to a resource-poor language in order to train NLP tools for the target language. The projection approach has been successfully used to transfer several linguistic annotations between languages. Examples include POS (Yarowsky et al., 2001; Das and Petrov, 2011; Duong et al., 2013), named entity (Kim and Lee, 2012), syntactic constituent (Jiang et al., 2011), word senses (Bentivogli et al., 2004; Van der Plas and Apidianaki, 2014), and semantic role labeling (Pad´o , 2007; Annesi and Basili, 2010). In these approaches, the source language is tagged, and tags are projected from the source language to the target language through the use of word alignments in parallel corpora. Then, these partial noisy annotations can be used in conjunction with robust learning algorithms to build unsupervised NLP tools. One limitation of these approaches is due to the poor accuracy of wor"
Y15-1016,D12-1001,0,0.09708,"lid tags for each word (coming from Wiktionary 2 ). In another vein, various studies based on crosslingual representation learning methods have proposed to avoid using such pre-processed and noisy alignments for label projection. First, these approaches learn language-independent features, across many different languages (Al-Rfou et al., 2013). Then, the induced representation space is used to train NLP tools by exploiting labeled data from the source language and apply them in the target language. To induce interlingual features, several resources have been used, including bilingual lexicon (Durrett et al., 2012; Gouws and Søgaard, 2015a) and parallel corpora (T¨ackstr¨om et al., 2013a; Gouws et al., 2015b). Cross-lingual representation learning have achieved good results in different NLP applications such as cross-language POS tagging and cross-language super sense (SuS) tagging (Gouws and Søgaard, 2015a), cross-language named entity recognition (T¨ackstr¨om et al., 2012), cross-lingual document classification and lexical translation task (Gouws et al., 2015b), cross language dependency parsing (Durrett et al., 2012; T¨ackstr¨om et al., 2013a; Xiao and Guo, 2014) and cross language semantic role lab"
Y15-1016,P04-1013,0,0.0584824,"f the parallel corpus, and to use it for tagging target text. Before describing our bilingual neural network POS tagger, we present the simple crosslingual projection method, considered as our baseline in this work. 3.1 Figure 1: Architecture of the recurrent neural network. labeled text and apply it to tag target language text. We also show that the architecture proposed is well suited for lightly supervised training (adaptation). Finally, several works have investigated how to apply neural networks to NLP applications (Bengio et al., 2006; Collobert and Weston, 2008; Collobert et al., 2011; Henderson, 2004; Mikolov et al., 2010; Federici and Pirrelli, 1993). While Federici and Pirrelli (1993) was one of the earliest attempts to develop a part-of-speech tagger based on a special type of neural network, Bengio et al. (2006) and Mikolov et al. (2010) applied neural networks to build language models. Collobert and Weston (2008) and Collobert et al. (2011) employed a deep learning framework for multi-task learning including part-of-speech tagging, chunking, namedentity recognition, language modelling and semantic role-labeling. Henderson (2004) proposed training methods for learning a statistical pa"
Y15-1016,N15-1157,0,0.430244,"(coming from Wiktionary 2 ). In another vein, various studies based on crosslingual representation learning methods have proposed to avoid using such pre-processed and noisy alignments for label projection. First, these approaches learn language-independent features, across many different languages (Al-Rfou et al., 2013). Then, the induced representation space is used to train NLP tools by exploiting labeled data from the source language and apply them in the target language. To induce interlingual features, several resources have been used, including bilingual lexicon (Durrett et al., 2012; Gouws and Søgaard, 2015a) and parallel corpora (T¨ackstr¨om et al., 2013a; Gouws et al., 2015b). Cross-lingual representation learning have achieved good results in different NLP applications such as cross-language POS tagging and cross-language super sense (SuS) tagging (Gouws and Søgaard, 2015a), cross-language named entity recognition (T¨ackstr¨om et al., 2012), cross-lingual document classification and lexical translation task (Gouws et al., 2015b), cross language dependency parsing (Durrett et al., 2012; T¨ackstr¨om et al., 2013a; Xiao and Guo, 2014) and cross language semantic role labeling ( Titov and Klement"
Y15-1016,D11-1110,0,0.409403,"Missing"
Y15-1016,P12-1073,0,0.172818,"Missing"
Y15-1016,D12-1127,0,0.0598002,"Missing"
Y15-1016,P00-1056,0,0.30196,"results to state-ofthe-art unsupervised POS taggers. 135 Unsupervised POS Tagger Based on a Simple Cross-lingual Projection Our simple POS tagger (described by Algorithm 1) is close to the approach introduced in Yarowsky et al. (2001). These authors were the first to use automatic word alignments (from a bilingual parallel corpus) to project annotations from a source language to a target language, to build unsupervised POS taggers. The algorithm is shortly recalled below. Algorithm 1 : Simple POS Tagger 1: Tag source side of the parallel corpus. 2: Word align the parallel corpus with Giza++ (Och and Ney, 2000) or other word alignment tools. 3: Project tags directly for 1-to-1 alignments. 4: For many-to-one mappings project the tag of the middle word. 5: The unaligned words (target) are tagged with their most frequent associated tag in the corpus. 6: Learn POS tagger on target side of the bi-text with, for instance, TNT tagger (Brants, 2000). 3.2 Unsupervised POS Tagger Based on Recurrent Neural Network There are two major architectures of neural networks: Feedforward (Bengio et al., 2006) and Recurrent Neural Networks (RNN) (Mikolov et al., 2010). Sundermeyer et al. (2013) showed that language mode"
Y15-1016,petrov-etal-2012-universal,0,0.0989177,"can then use the RNN POS tagger, initially trained on source side, to tag the target side (because of our common vector representation). We also use two hidden layers (our preliminary experiments have shown better performance than one hidden layer), with variable sizes (usually 80-1024 neurons) and sigmoid activation function. These hidden layers inherently capture word alignment information. The output layer of our model contains 12 neurons, this number is determined by the POS tagset size. To deal with the potential mismatch in the POS tagsets of source and target languages, we adopted the Petrov et al. (2012) universal tagset (12 tags common for most languages): NOUN (nouns), VERB (verbs), ADJ (adjectives), ADV (adverbs), PRON (pronouns), DET (determiners and articles), ADP (prepositions and postpositions), NUM (numerals), CONJ (conjunctions), PRT (particles), . (punctuation marks) and X (all other categories, e.g., foreign words, abbreviations). 136 Therefore, each output neuron corresponds to one POS tag in the tagset. The softmax activation function is used to normalize the values of output neurons to sum up to 1. Finally, the current word w (in input) is tagged with most probable output tag. 3"
Y15-1016,2005.mtsummit-papers.11,0,0.0783864,"from the train set) tagged with the French TreeTagger Toolkit (Schmid, 1995) and manually checked. Encouraged by the results obtained on the English–French language pair, and in order to confirm our results, we run additional experiments on other languages, we applied our method to build RNN POS taggers for three more target languages — German, Greek and Spanish — with English as the source language, in order to compare our results with those of (Das and Petrov, 2011; Duong et al., 2013; Gouws and Søgaard, 2015a). Our training and validation (English) data extracted from the Europarl corpus (Koehn, 2005) are a subset of the training data of (Das and Petrov, 2011; Duong et al., 2013). The sizes of the data sets are: 65, 000 (train) and 10, 000 (dev) bi-sentences. For testing, we used the same test corpora (from CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006)) as (Das and Petrov, 2011; Duong et al., 2013; Gouws and Søgaard, 2015a). The evaluation metric (per-token accuracy) and the Universal Tagset are the same as before. The source sides of the training corpora (ARCADE II and Europarl) and the validation corpora are tagged with the English TreeTagger Toolkit. Using the matc"
Y15-1016,N12-1052,0,0.241524,"Missing"
Y15-1016,N13-1126,0,0.0891296,"Missing"
Y15-1016,Q13-1001,0,0.0364857,"Missing"
Y15-1016,P12-1068,0,0.140177,"and Søgaard, 2015a) and parallel corpora (T¨ackstr¨om et al., 2013a; Gouws et al., 2015b). Cross-lingual representation learning have achieved good results in different NLP applications such as cross-language POS tagging and cross-language super sense (SuS) tagging (Gouws and Søgaard, 2015a), cross-language named entity recognition (T¨ackstr¨om et al., 2012), cross-lingual document classification and lexical translation task (Gouws et al., 2015b), cross language dependency parsing (Durrett et al., 2012; T¨ackstr¨om et al., 2013a; Xiao and Guo, 2014) and cross language semantic role labeling ( Titov and Klementiev, 2012). Our approach described in next section, is inspired by these works since we also try to learn a common language-independent feature space. Our common (multilingual) representation is based on the occurrence of source and target words in a parallel corpus. Using this representation, we learn a cross-lingual POS tagger (multilingual POS tagger if a multilingual parallel corpus is used) based on a recurrent neural network (RNN) on the source 2 http://www.wiktionary.org/ PACLIC 29 Our approach is the following: we assume that we have a POS tagger in the source language and a parallel corpus. The"
Y15-1016,F14-1005,0,0.0265144,"Missing"
Y15-1016,D14-1187,0,0.124403,"Missing"
Y15-1016,W14-1613,0,0.0137828,"used, including bilingual lexicon (Durrett et al., 2012; Gouws and Søgaard, 2015a) and parallel corpora (T¨ackstr¨om et al., 2013a; Gouws et al., 2015b). Cross-lingual representation learning have achieved good results in different NLP applications such as cross-language POS tagging and cross-language super sense (SuS) tagging (Gouws and Søgaard, 2015a), cross-language named entity recognition (T¨ackstr¨om et al., 2012), cross-lingual document classification and lexical translation task (Gouws et al., 2015b), cross language dependency parsing (Durrett et al., 2012; T¨ackstr¨om et al., 2013a; Xiao and Guo, 2014) and cross language semantic role labeling ( Titov and Klementiev, 2012). Our approach described in next section, is inspired by these works since we also try to learn a common language-independent feature space. Our common (multilingual) representation is based on the occurrence of source and target words in a parallel corpus. Using this representation, we learn a cross-lingual POS tagger (multilingual POS tagger if a multilingual parallel corpus is used) based on a recurrent neural network (RNN) on the source 2 http://www.wiktionary.org/ PACLIC 29 Our approach is the following: we assume tha"
Y15-1016,H01-1035,0,0.848938,"erman, Greek and Spanish. Secondly, the performance of our approach is evaluated for German in a weakly supervised context, using several amounts of target adaptation data (Section 5). Finally, Section 6 concludes our study and presents our future work. 2 Related Work Several studies have used cross-lingual projection to transfer linguistic annotations from a resourcerich language to a resource-poor language in order to train NLP tools for the target language. The projection approach has been successfully used to transfer several linguistic annotations between languages. Examples include POS (Yarowsky et al., 2001; Das and Petrov, 2011; Duong et al., 2013), named entity (Kim and Lee, 2012), syntactic constituent (Jiang et al., 2011), word senses (Bentivogli et al., 2004; Van der Plas and Apidianaki, 2014), and semantic role labeling (Pad´o , 2007; Annesi and Basili, 2010). In these approaches, the source language is tagged, and tags are projected from the source language to the target language through the use of word alignments in parallel corpora. Then, these partial noisy annotations can be used in conjunction with robust learning algorithms to build unsupervised NLP tools. One limitation of these ap"
