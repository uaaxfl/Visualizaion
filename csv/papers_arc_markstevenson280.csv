2021.nlp4if-1.1,Identifying Automatically Generated Headlines using Transformers,2021,-1,-1,3,0,2872,antonis maronikolakis,"Proceedings of the Fourth Workshop on NLP for Internet Freedom: Censorship, Disinformation, and Propaganda",0,"False information spread via the internet and social media influences public opinion and user activity, while generative models enable fake content to be generated faster and more cheaply than had previously been possible. In the not so distant future, identifying fake content generated by deep learning models will play a key role in protecting users from misinformation. To this end, a dataset containing human and computer-generated headlines was created and a user study indicated that humans were only able to identify the fake headlines in 47.8{\%} of the cases. However, the most accurate automatic approach, transformers, achieved an overall accuracy of 85.7{\%}, indicating that content generated from language models can be filtered out accurately."
2021.naacl-main.187,Highly Efficient Knowledge Graph Embedding Learning with {O}rthogonal {P}rocrustes {A}nalysis,2021,-1,-1,4,0,3825,xutan peng,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Knowledge Graph Embeddings (KGEs) have been intensively explored in recent years due to their promise for a wide range of applications. However, existing studies focus on improving the final model performance without acknowledging the computational cost of the proposed approaches, in terms of execution time and environmental impact. This paper proposes a simple yet effective KGE framework which can reduce the training time and carbon footprint by orders of magnitudes compared with state-of-the-art approaches, while producing competitive performance. We highlight three technical innovations: full batch learning via relational matrices, closed-form Orthogonal Procrustes Analysis for KGEs, and non-negative-sampling training. In addition, as the first KGE method whose entity embeddings also store full relation information, our trained models encode rich semantics and are highly interpretable. Comprehensive experiments and ablation studies involving 13 strong baselines and two standard datasets verify the effectiveness and efficiency of our algorithm."
2021.naacl-main.214,Cross-Lingual Word Embedding Refinement by $\\ell_{1}$ Norm Optimisation,2021,-1,-1,3,0,3825,xutan peng,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Cross-Lingual Word Embeddings (CLWEs) encode words from two or more languages in a shared high-dimensional space in which vectors representing words with similar meaning (regardless of language) are closely located. Existing methods for building high-quality CLWEs learn mappings that minimise the â2 norm loss function. However, this optimisation objective has been demonstrated to be sensitive to outliers. Based on the more robust Manhattan norm (aka. â1 norm) goodness-of-fit criterion, this paper proposes a simple post-processing step to improve CLWEs. An advantage of this approach is that it is fully agnostic to the training process of the original CLWEs and can therefore be applied widely. Extensive experiments are performed involving ten diverse languages and embeddings trained on different corpora. Evaluation results based on bilingual lexicon induction and cross-lingual transfer for natural language inference tasks show that the â1 refinement substantially outperforms four state-of-the-art baselines in both supervised and unsupervised settings. It is therefore recommended that this strategy be adopted as a standard for CLWE methods."
2020.lrec-1.465,{P}ara{P}at: The Multi-Million Sentences Parallel Corpus of Patents Abstracts,2020,-1,-1,2,0,13930,felipe soares,Proceedings of the 12th Language Resources and Evaluation Conference,0,"The Google Patents is one of the main important sources of patents information. A striking characteristic is that many of its abstracts are presented in more than one language, thus making it a potential source of parallel corpora. This article presents the development of a parallel corpus from the open access Google Patents dataset in 74 language pairs, comprising more than 68 million sentences and 800 million tokens. Sentences were automatically aligned using the Hunalign algorithm for the largest 22 language pairs, while the others were abstract (i.e. paragraph) aligned. We demonstrate the capabilities of our corpus by training Neural Machine Translation (NMT) models for the main 9 language pairs, with a total of 18 models. Our parallel corpus is freely available in TSV format and with a SQLite database, with complementary information regarding patent metadata."
2020.aacl-main.76,Robustness and Reliability of Gender Bias Assessment in Word Embeddings: The Role of Base Pairs,2020,-1,-1,3,0,23260,haiyang zhang,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing,0,"It has been shown that word embeddings can exhibit gender bias, and various methods have been proposed to quantify this. However, the extent to which the methods are capturing social stereotypes inherited from the data has been debated. Bias is a complex concept and there exist multiple ways to define it. Previous work has leveraged gender word pairs to measure bias and extract biased analogies. We show that the reliance on these gendered pairs has strong limitations: bias measures based off of them are not robust and cannot identify common types of real-world bias, whilst analogies utilising them are unsuitable indicators of bias. In particular, the well-known analogy {``}man is to computer-programmer as woman is to homemaker{''} is due to word similarity rather than bias. This has important implications for work on measuring bias in embeddings and related work debiasing embeddings."
W19-0404,Re-Ranking Words to Improve Interpretability of Automatically Generated Topics,2019,34,0,3,0,24884,areej alokaili,Proceedings of the 13th International Conference on Computational Semantics - Long Papers,0,"Topics models, such as LDA, are widely used in Natural Language Processing. Making their output interpretable is an important area of research with applications to areas such as the enhancement of exploratory search interfaces and the development of interpretable machine learning models. Conventionally, topics are represented by their n most probable words, however, these representations are often difficult for humans to interpret. This paper explores the re-ranking of topic words to generate more interpretable topic representations. A range of approaches are compared and evaluated in two experiments. The first uses crowdworkers to associate topics represented by different word rankings with related documents. The second experiment is an automatic approach based on a document retrieval task applied on multiple domains. Results in both experiments demonstrate that re-ranking words improves topic interpretability and that the most effective re-ranking schemes were those which combine information about the importance of words both within topics and their relative frequency in the entire corpus. In addition, close correlation between the results of the two evaluation approaches suggests that the automatic method proposed here could be used to evaluate re-ranking methods without the need for human judgements."
D19-1351,Modelling Stopping Criteria for Search Results using {P}oisson Processes,2019,0,0,2,0,23261,alison sneyd,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Text retrieval systems often return large sets of documents, particularly when applied to large collections. Stopping criteria can reduce the number of these documents that need to be manually evaluated for relevance by predicting when a suitable level of recall has been achieved. In this work, a novel method for determining a stopping criterion is proposed that models the rate at which relevant documents occur using a Poisson process. This method allows a user to specify both a minimum desired level of recall to achieve and a desired probability of having achieved it. We evaluate our method on a public dataset and compare it with previous techniques for determining stopping criteria."
C18-2008,{H}i{DE}: a Tool for Unrestricted Literature Based Discovery,2018,0,0,2,0.684308,30715,judita preiss,Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations,0,"As the quantity of publications increases daily, researchers are forced to narrow their attention to their own specialism and are therefore less likely to make new connections with other areas. Literature based discovery (LBD) supports the identification of such connections. A number of LBD tools are available, however, they often suffer from limitations such as constraining possible searches or not producing results in real-time. We introduce HiDE (Hidden Discovery Explorer), an online knowledge browsing tool which allows fast access to hidden knowledge generated from all abstracts in Medline. HiDE is fast enough to allow users to explore the full range of hidden connections generated by an LBD system. The tool employs a novel combination of two approaches to LBD: a graph-based approach which allows hidden knowledge to be generated on a large scale and an inference algorithm to identify the most promising (most likely to be non trivial) information. Available at https://skye.shef.ac.uk/kdisc"
C18-1029,Topic or Style? Exploring the Most Useful Features for Authorship Attribution,2018,0,2,2,1,30751,yunita sari,Proceedings of the 27th International Conference on Computational Linguistics,0,"Approaches to authorship attribution, the task of identifying the author of a document, are based on analysis of individuals{'} writing style and/or preferred topics. Although the problem has been widely explored, no previous studies have analysed the relationship between dataset characteristics and effectiveness of different types of features. This study carries out an analysis of four widely used datasets to explore how different types of features affect authorship attribution accuracy under varying conditions. The results of the analysis are applied to authorship attribution models based on both discrete and continuous representations. We apply the conclusions from our analysis to an extension of an existing approach to authorship attribution and outperform the prior state-of-the-art on two out of the four datasets used."
E17-2043,Continuous N-gram Representations for Authorship Attribution,2017,22,15,3,1,30751,yunita sari,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"This paper presents work on using continuous representations for authorship attribution. In contrast to previous work, which uses discrete feature representations, our model learns continuous representations for n-gram features via a neural network jointly with the classification layer. Experimental results demonstrate that the proposed model outperforms the state-of-the-art on two datasets, while producing comparable results on the remaining two."
W16-5606,User profiling with geo-located posts and demographic data,2016,9,2,2,0,33446,adam poulston,Proceedings of the First Workshop on {NLP} and Computational Social Science,0,None
W15-3802,Making the most of limited training data using distant supervision,2015,36,1,2,1,13906,roland roller,Proceedings of {B}io{NLP} 15,0,"Automatic recognition of relationships between key entities in text is an important problem which has many applications. Supervised machine learning techniques have proved to be the most effective approach to this problem. However, they require labelled training data which may not be available in sufficient quantity (or at all) and is expensive to produce. This paper proposes a technique that can be applied when only limited training data is available. The approach uses a form of distant supervision but does not require an external knowledge base. Instead, it uses information from the training set to acquire new labelled data and combines it with manually labelled data. The approach was tested on an adverse drug data set using a limited amount of manually labelled training data and shown to outperform a supervised approach."
W15-3817,Automatic Detection of Answers to Research Questions from {M}edline Abstracts,2015,18,3,2,0,36752,abdulaziz alamri,Proceedings of {B}io{NLP} 15,0,"Given a set of abstracts retrieved from a search engine such as Pubmed, we aim to automatically identify the claim zone in each abstract and then select the best sentence(s) from that zone that can serve as an answer to a given query. The system can provide a fast access mechanism to the most informative sentence(s) in abstracts with respect to the given query."
W15-2612,Held-out versus Gold Standard: Comparison of Evaluation Strategies for Distantly Supervised Relation Extraction from {M}edline abstracts,2015,26,1,2,1,13906,roland roller,Proceedings of the Sixth International Workshop on Health Text Mining and Information Analysis,0,"Distant supervision is a useful technique for creating relation classifiers in the absence of labelled data. The approaches are often evaluated using a held-out portion of the distantly labelled data, thereby avoiding the need for lablelled data entirely. However, held-out evaluation means that systems are tested against noisy data, making it difficult to determine their true accuracy. This paper examines the effectiveness of using held-out data to evaluate relation extraction systems by comparing the results that are produced with those generated using manually labelled versions of the same data. We train classifiers to detect two UMLS Metathesaurus relations (may-treat and may-prevent) in Medline abstracts. A new evaluation data set for these relations is made available. We show that evaluation against a distantly labelled gold standard tends to overestimate performance and that no direct connection can be found between improved performance against distantly and manually labelled gold standards."
S15-1003,A Hybrid Distributional and Knowledge-based Model of Lexical Semantics,2015,40,5,2,1,3288,nikolaos aletras,Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics,0,A range of approaches to the representation of lexical semantics have been explored within Computational Linguistics. Two of the most popular are distributional and knowledgebased models. This paper proposes hybrid models of lexical semantics that combine the advantages of these two approaches. Our models provide robust representations of synonymous words derived from WordNet. We also make use of WordNetxe2x80x99s hierarcy to refine the synset vectors. The models are evaluated on two widely explored tasks involving lexical semantics: lexical similarity and Word Sense Disambiguation. The hybrid models are found to perform better than standard distributional models and have the additional benefit of modelling polysemy.
P15-2045,Improving distant supervision using inference learning,2015,23,3,4,1,13906,roland roller,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Distant supervision is a widely applied approach to automatic training of relation extraction systems and has the advantage that it can generate large amounts of labelled data with minimal effort. However, this data may contain errors and consequently systems trained using distant supervision tend not to perform as well as those based on manually labelled data. This work proposes a novel method for detecting potential false negative training examples using a knowledge inference method. Results show that our approach improves the performance of relation extraction systems trained using distantly supervised data."
W14-1112,Applying {UMLS} for Distantly Supervised Relation Detection,2014,17,7,2,1,13906,roland roller,Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi),0,This paper describes first results using the Unified Medical Language System (UMLS) for distantly supervised relation extraction. UMLS is a large knowledge base which contains information about millions of medical concepts and relations between them. Our approach is evaluated using existing relation extraction data sets that contain relations that are similar to some of those in UMLS.
P14-2103,Labelling Topics using Unsupervised Graph-based Methods,2014,21,20,2,1,3288,nikolaos aletras,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"This paper introduces an unsupervised graph-based method that selects textual labels for automatically generated topics. Our approach uses the topic keywords to query a search engine and generate a graph from the words contained in the results. PageRank is then used to weigh the words in the graph and score the candidate labels. The state-of-the-art method for this task is supervised (Lau et al., 2011). Evaluation on a standard data set shows that the performance of our approach is consistently superior to previously reported methods."
E14-4005,Measuring the Similarity between Automatically Generated Topics,2014,22,14,2,1,3288,nikolaos aletras,"Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics, volume 2: Short Papers",0,"Previous approaches to the problem of measuring similarity between automatically generated topics have been based on comparison of the topicsxe2x80x99 word probability distributions. This paper presents alternative approaches, including ones based on distributional semantics and knowledgebased measures, evaluated by comparison with human judgements. The best performing methods provide reliable estimates of topic similarity comparable with human performance and should be used in preference to the word probability distribution measures used previously."
W13-2701,Generating Paths through Cultural Heritage Collections,2013,28,0,4,1,40905,samuel fernando,"Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities",0,"Cultural heritage collections usually organise sets of items into exhibitions or guided tours. These items are often accompanied by text that describes the theme and topic of the exhibition and provides background context and details of connections with other items. The PATHS project brings the idea of guided tours to digital library collections where a tool to create virtual paths are used to assist with navigation and provide guides on particular subjects and topics. In this paper we characterise and analyse paths of items created by users of our online system. The analysis highlights that most users spend time selecting items relevant to their chosen topic, but few users took time to add background information to the paths. In order to address this, we conducted preliminary investigations to test whether Wikipedia can be used to automatically add background text for sequences of items. In the future we would like to explore the automatic creation of full paths."
W13-2018,Identification of {G}enia Events using Multiple Classifiers,2013,8,3,2,1,13906,roland roller,Proceedings of the {B}io{NLP} Shared Task 2013 Workshop,0,We describe our system to extract genia events that was developed for the BioNLP 2013 Shared Task. Our system uses a supervised information extraction platform based on Support Vector Machines (SVM) and separates the process of event classification into multiple stages. For each event type the SVM parameters are adjusted and feature selection carried out. We find that this optimisation improves the performance of our approach. Overall our system achieved the highest precision score of all systems and was ranked 6th of 10 participating systems on F-measure (strict matching).
W13-0102,Evaluating Topic Coherence Using Distributional Semantics,2013,26,98,2,1,3288,nikolaos aletras,Proceedings of the 10th International Conference on Computational Semantics ({IWCS} 2013) {--} Long Papers,0,This paper introduces distributional semantic similarity methods for automatically measuring the coherence of a set of words generated by a topic model. We construct a semantic space to represent each topic word by making use of Wikipedia as a reference corpus to identify context features and collect frequencies. Relatedness between topic words and context features is measured using variants of Pointwise Mutual Information (PMI). Topic coherence is determined by measuring the distance between these vectors computed using a variety of metrics. Evaluation on three data sets shows that the distributional-based measures outperform the state-of-the-art approach for this task.
S13-1010,Distinguishing Common and Proper Nouns,2013,13,1,2,0.77851,30715,judita preiss,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity",0,"We describe a number of techniques for automatically deriving lists of common and proper nouns, and show that the distinction between the two can be made automatically using a vector space model learning algorithm. We present a direct evaluation on the British National Corpus, and application based evaluations on Twitter messages and on automatic speech recognition (where the system could be employed to restore case)."
S13-1018,{UBC}{\\_}{UOS}-{TYPED}: Regression for typed-similarity,2013,12,4,5,0,8824,eneko agirre,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity",0,We approach the typed-similarity task using a range of heuristics that rely on information from the appropriate metadata fields for each type of similarity. In addition we train a linear regressor for each type of similarity. The results indicate that the linear regression is key for good performance. Our best system was ranked third in the task.
P13-4026,{PATHS}: A System for Accessing Cultural Heritage Collections,2013,22,9,8,0,8824,eneko agirre,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"This paper describes a system for navigating large collections of information about cultural heritage which is applied to Europeana, the European Library. Europeana contains over 20 million artefacts with meta-data in a wide range of European languages. The system currently provides access to Europeana content with meta-data in English and Spanish. The paper describes how Natural Language Processing is used to enrich and organise this meta-data to assist navigation through Europeana and shows how this information is used within the system."
N13-3001,{DALE}: A Word Sense Disambiguation System for Biomedical Documents Trained using Automatically Labeled Examples,2013,10,4,2,0.77851,30715,judita preiss,Proceedings of the 2013 {NAACL} {HLT} Demonstration Session,0,"Automatic interpretation of documents is hampered by the fact that language contains terms which have multiple meanings. These ambiguities can still be found when language is restricted to a particular domain, such as biomedicine. Word Sense Disambiguation (WSD) systems attempt to resolve these ambiguities but are often only able to identify the meanings for a small set of ambiguous terms. DALE (Disambiguation using Automatically Labeled Examples) is a supervised WSD system that can disambiguate a wide range of ambiguities found in biomedical documents. DALE uses the UMLS Metathesaurus as both a sense inventory and as a source of information for automatically generating labeled training examples. DALE is able to disambiguate biomedical documents with the coverage of unsupervised approaches and accuracy of supervised methods."
N13-1016,Representing Topics Using Images,2013,37,22,2,1,3288,nikolaos aletras,Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Topics generated automatically, e.g. using LDA, are now widely used in Computational Linguistics. Topics are normally represented as a set of keywords, often the n terms in a topic with the highest marginal probabilities. We introduce an alternative approach in which topics are represented using images. Candidate images for each topic are retrieved from the web by querying a search engine using the top n terms. The most suitable image is selected from this set using a graph-based algorithm which makes use of textual information from the metadata associated with each image and features extracted from the images themselves. We show that the proposed approach significantly outperforms several baselines and can provide images that are useful to represent a topic."
N13-1079,Unsupervised Domain Tuning to Improve Word Sense Disambiguation,2013,16,5,2,0.77851,30715,judita preiss,Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"The topic of a document can prove to be useful information for Word Sense Disambiguation (WSD) since certain meanings tend to be associated with particular topics. This paper presents an LDA-based approach for WSD, which is trained using any available WSD system to establish a sense per (Latent Dirichlet allocation based) topic. The technique is tested using three unsupervised and one supervised WSD algorithms within the SPORT and FINANCE domains giving a performance increase each time, suggesting that the technique may be useful to improve the performance of any available WSD system."
W12-2429,Scaling up {WSD} with Automatically Generated Examples,2012,26,7,3,0,28732,weiwei cheng,{B}io{NLP}: Proceedings of the 2012 Workshop on Biomedical Natural Language Processing,0,"The most accurate approaches to Word Sense Disambiguation (WSD) for biomedical documents are based on supervised learning. However, these require manually labeled training examples which are expensive to create and consequently supervised WSD systems are normally limited to disambiguating a small set of ambiguous terms. An alternative approach is to create labeled training examples automatically and use them as a substitute for manually labeled ones. This paper describes a large scale WSD system based on automatically labeled examples generated using information from the UMLS Metathesaurus. The labeled examples are generated without any use of labeled training data whatsoever and is therefore completely unsupervised (unlike some previous approaches). The system is evaluated on two widely used data sets and found to outperform a state-of-the-art unsupervised approach which also uses information from the UMLS Metathesaurus."
W12-1012,Computing Similarity between Cultural Heritage Items using Multimodal Features,2012,42,6,2,1,3288,nikolaos aletras,"Proceedings of the 6th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities",0,"A significant amount of information about Cultural Heritage artefacts is now available in digital format and has been made available in digital libraries. Being able to identify items that are similar would be useful for search and navigation through these data sets. Information about items in these repositories is often multimodal, such as pictures of the artefact and an accompanying textual description. This paper explores the use of information from these various media for computing similarity between Cultural Heritage artefacts. Results show that combining information from images and text produces better estimates of similarity than when only a single medium is considered."
W12-1014,Adapting Wikification to Cultural Heritage,2012,8,9,2,1,40905,samuel fernando,"Proceedings of the 6th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities",0,"Large numbers of cultural heritage items are now archived digitally along with accompanying metadata and are available to anyone with internet access. This information could be enriched by adding links to resources that provide background information about the items. Techniques have been developed for automatically adding links to Wikipedia to text but the methods are general and not designed for use with cultural heritage data. This paper explores a range of methods for adapting a system for adding links to Wikipedia to cultural heritage items. The approaches make use of the structure of Wikipedia, including the category hierarchy. It is found that an approach that makes use of Wikipedia's link structure can be used to improve the quality of the Wikipedia links that are added."
S12-1008,Detecting Text Reuse with Modified and Weighted N-grams,2012,16,12,2,0,34870,rao nawab,"*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",0,"Text reuse is common in many scenarios and documents are often based, at least in part, on existing documents. This paper reports an approach to detecting text reuse which identifies not only documents which have been reused verbatim but is also designed to identify cases of reuse when the original has been rewritten. The approach identifies reuse by comparing word n-grams in documents and modifies these (by substituting words with synonyms and deleting words) to identify when text has been altered. The approach is applied to a corpus of newspaper stories and found to outperform a previously reported method."
S12-1097,{U}niversity{\\_}{O}f{\\_}{S}heffield: Two Approaches to Semantic Text Similarity,2012,28,8,5,0,42617,sam biggins,"*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",0,This paper describes the University of Sheffield's submission to SemEval-2012 Task 6: Semantic Text Similarity. Two approaches were developed. The first is an unsupervised technique based on the widely used vector space model and information from WordNet. The second method relies on supervised machine learning and represents each sentence as a set of n-grams. This approach also makes use of information from WordNet. Results from the formal evaluation show that both approaches are useful for determining the similarity in meaning between pairs of sentences with the best performance being obtained by the supervised approach. Incorporating information from WordNet also improves performance for both approaches.
fernando-stevenson-2012-mapping,Mapping {W}ord{N}et synsets to {W}ikipedia articles,2012,12,13,2,1,40905,samuel fernando,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Lexical knowledge bases (LKBs), such as WordNet, have been shown to be useful for a range of language processing tasks. Extending these resources is an expensive and time-consuming process. This paper describes an approach to address this problem by automatically generating a mapping from WordNet synsets to Wikipedia articles. A sample of synsets has been manually annotated with article matches for evaluation purposes. The automatic methods are shown to create mappings with precision of 87.8{\%} and recall of 46.9{\%}. These mappings can then be used as a basis for enriching WordNet with new relations based on Wikipedia links. The manual and automatically created data is available online."
agirre-etal-2012-matching,Matching Cultural Heritage items to {W}ikipedia,2012,10,14,6,0,8824,eneko agirre,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Digitised Cultural Heritage (CH) items usually have short descriptions and lack rich contextual information. Wikipedia articles, on the contrary, include in-depth descriptions and links to related articles, which motivate the enrichment of CH items with information from Wikipedia. In this paper we explore the feasibility of finding matching articles in Wikipedia for a given Cultural Heritage item. We manually annotated a random sample of items from Europeana, and performed a qualitative and quantitative study of the issues and problems that arise, showing that each kind of CH item is different and needs a nuanced definition of what ``matching article'' means. In addition, we test a well-known wikification (aka entity linking) algorithm on the task. Our results indicate that a substantial number of items can be effectively linked to their corresponding Wikipedia article."
C12-1054,Comparing Taxonomies for Organising Collections of Documents,2012,27,9,6,1,40905,samuel fernando,Proceedings of {COLING} 2012,0,"There is a demand for taxonomies to organise large collections of documents into categories for browsing and exploration. This paper examines four existing taxonomies that have been manually created, along with two methods for deriving taxonomies automatically from data items. We use these taxonomies to organise items from a large online cultural heritage collection. We then present two human evaluations of the taxonomies. The first measures the cohesion of the taxonomies to determine how well they group together similar items under the same concept node. The second analyses the concept relations in the taxonomies. The results show that the manual taxonomies have high quality well defined relations. However the novel automatic method is found to generate very high cohesion."
R11-1004,Extracting Relations Within and Across Sentences,2011,24,24,2,0,44497,kumutha swampillai,Proceedings of the International Conference Recent Advances in Natural Language Processing 2011,0,Previous work on relation extraction has focussed on identifying relationships between entities that occur in the same sentence (intra-sentential relations) rather than between entities in different sentences (inter-sentential relations) despite previous research having shown that intersentential relations commonly occur in information extraction corpora. This paper describes a SVM-based approach to relation extraction that is applied to both types. Adapted features and techniques for counter-acting bias in SVM models are used to deal with specific issues that arise in the inter-sentential case. It was found that the structured features used for intrasentential relation extraction can be easily adapted for the inter-sentential case and provides comparable performance.
W10-1907,Improving Summarization of Biomedical Documents Using Word Sense Disambiguation,2010,24,12,2,0,42600,laura plaza,Proceedings of the 2010 Workshop on Biomedical Natural Language Processing,0,"We describe a concept-based summarization system for biomedical documents and show that its performance can be improved using Word Sense Disambiguation. The system represents the documents as graphs formed from concepts and relations from the UMLS. A degree-based clustering algorithm is applied to these graphs to discover different themes or topics within the document. To create the graphs, the MetaMap program is used to map the text onto concepts in the UMLS Metathe-saurus. This paper shows that applying a graph-based Word Sense Disambiguation algorithm to the output of MetaMap improves the quality of the summaries that are generated."
S10-1087,{IIITH}: Domain Specific Word Sense Disambiguation,2010,13,9,4,0,3549,siva reddy,Proceedings of the 5th International Workshop on Semantic Evaluation,0,"We describe two systems that participated in SemEval-2010 task 17 (All-words Word Sense Disambiguation on a Specific Domain) and were ranked in the third and fourth positions in the formal evaluation. Domain adaptation techniques using the background documents released in the task were used to assign ranking scores to the words and their senses. The test data was disambiguated using the Personalized PageRank algorithm which was applied to a graph constructed from the whole of WordNet in which nodes are initialized with ranking scores of words and their senses. In the competition, our systems achieved comparable accuracy of 53.4 and 52.2, which outperforms the most frequent sense baseline (50.5)."
N10-1053,The Effect of Ambiguity on the Automated Acquisition of {WSD} Examples,2010,15,3,1,1,2873,mark stevenson,Human Language Technologies: The 2010 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"Several methods for automatically generating labeled examples that can be used as training data for WSD systems have been proposed, including a semi-supervised approach based on relevance feedback (Stevenson et al., 2008a). This approach was shown to generate examples that improved the performance of a WSD system for a set of ambiguous terms from the biomedical domain. However, we find that this approach does not perform as well on other data sets. The levels of ambiguity in these data sets are analysed and we suggest this is the reason for this negative result."
swampillai-stevenson-2010-inter,Inter-sentential Relations in Information Extraction Corpora,2010,9,13,2,0,44497,kumutha swampillai,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,In natural language relationships between entities can asserted within a single sentence or over many sentences in a document. Many information extraction systems are constrained to extracting binary relations that are asserted within a single sentence (single-sentence relations) and this limits the proportion of relations they can extract since those expressed across multiple sentences (inter-sentential relations) are not considered. The analysis in this paper focuses on finding the distribution of inter-sentential and single-sentence relations in two corpora used for the evaluation of Information Extraction systems: the MUC6 corpus and the ACE corpus from 2003. In order to carry out this analysis we had to manually mark up all the management succession relations described in the MUC6 corpus. It was found that inter-sentential relations constitute 28.5{\%} and 9.4{\%} of the total number of relations in MUC6 and ACE03 respectively. This places upper bounds on the recall of information extraction systems that do not consider relations that are asserted across multiple sentences (71.5{\%} and 90.6{\%} respectively).
W09-1309,Disambiguation of Biomedical Abbreviations,2009,30,36,1,1,2873,mark stevenson,Proceedings of the {B}io{NLP} 2009 Workshop,0,"Abbreviations are common in biomedical documents and many are ambiguous in the sense that they have several potential expansions. Identifying the correct expansion is necessary for language understanding and important for applications such as document retrieval. Identifying the correct expansion can be viewed as a Word Sense Disambiguation (WSD) problem. A WSD system that uses a variety of knowledge sources, including two types of information specific to the biomedical domain, is also described. This system was tested on a corpus of ambiguous abbreviations, created by automatically identifying the correct expansion in Medline abstracts, and found to identify the correct expansion with up to 99% accuracy."
W08-0611,Knowledge Sources for Word Sense Disambiguation of Biomedical Text,2008,20,14,1,1,2873,mark stevenson,Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing,0,"Like text in other domains, biomedical documents contain a range of terms with more than one possible meaning. These ambiguities form a significant obstacle to the automatic processing of biomedical texts. Previous approaches to resolving this problem have made use of a variety of knowledge sources including linguistic information (from the context in which the ambiguous term is used) and domain-specific resources (such as UMLS). In this paper we compare a range of knowledge sources which have been previously used and introduce a novel one: MeSH terms. The best performance is obtained using linguistic features in combination with MeSH terms. Results from our system outperform published results for previously reported systems on a standard test set (the NLM-WSD corpus)."
C08-1102,Acquiring Sense Tagged Examples using Relevance Feedback,2008,24,10,1,1,2873,mark stevenson,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,Supervised approaches to Word Sense Disambiguation (WSD) have been shown to outperform other approaches but are hampered by reliance on labeled training examples (the data acquisition bottleneck). This paper presents a novel approach to the automatic acquisition of labeled examples for WSD which makes use of the Information Retrieval technique of relevance feedback. This semi-supervised method generates additional labeled examples based on existing annotated data. Our approach is applied to a set of ambiguous terms from biomedical journal articles and found to significantly improve the performance of a state-of-the-art WSD system.
W07-1211,A Task-based Comparison of Information Extraction Pattern Models,2007,17,1,2,1,35307,mark greenwood,{ACL} 2007 Workshop on Deep Linguistic Processing,0,Several recent approaches to Information Extraction (IE) have used dependency trees as the basis for an extraction pattern representation. These approaches have used a variety of pattern models (schemes which define the parts of the dependency tree which can be used to form extraction patterns). Previous comparisons of these pattern models are limited by the fact that they have used indirect tasks to evaluate each model. This limitation is addressed here in an experiment which compares four pattern models using an unsupervised learning algorithm and a standard IE scenario. It is found that there is a wide variation between the models' performance and suggests that one model is the most useful for IE.
P07-1006,Learning Expressive Models for Word Sense Disambiguation,2007,22,22,2,0.833333,2509,lucia specia,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"We present a novel approach to the word sense disambiguation problem which makes use of corpus-based evidence combined with background knowledge. Employing an inductive logic programming algorithm, the approach generates expressive disambiguation rules which exploit several knowledge sources and can also model relations between them. The approach is evaluated in two tasks: identification of the correct translation for a set of highly ambiguous verbs in EnglishPortuguese translation and disambiguation of verbs from the Senseval-3 lexical sample task. The average accuracy obtained for the multilingual task outperforms the other machine learning techniques investigated. In the monolingual task, the approach performs as well as the state-of-the-art systems which reported results for the same set of verbs."
W06-2505,Multilingual versus Monolingual {WSD},2006,18,8,3,0.833333,2509,lucia specia,Proceedings of the Workshop on Making Sense of Sense: Bringing Psycholinguistics and Computational Linguistics Together,0,None
W06-0202,Comparing Information Extraction Pattern Models,2006,16,38,1,1,2873,mark stevenson,Proceedings of the Workshop on Information Extraction Beyond The Document,0,"Several recently reported techniques for the automatic acquisition of Information Extraction (IE) systems have used dependency trees as the basis of their extraction pattern representation. These approaches have used a variety of pattern models (schemes for representing IE patterns based on particular parts of the dependency analysis). An appropriate model should be expressive enough to represent the information which is to be extracted from text without being overly complicated. Four previously reported pattern models are evaluated using existing IE evaluation corpora and three dependency parsers. It was found that one model, linked chains, could represent around 95% of the information of interest without generating an unwieldy number of possible patterns."
W06-0204,Improving Semi-supervised Acquisition of Relation Extraction Patterns,2006,-1,-1,2,1,35307,mark greenwood,Proceedings of the Workshop on Information Extraction Beyond The Document,0,None
2006.eamt-1.28,Translation Context Sensitive {WSD},2006,18,3,3,0.833333,2509,lucia specia,Proceedings of the 11th Annual conference of the European Association for Machine Translation,0,None
P05-1047,A Semantic Approach to {IE} Pattern Induction,2005,14,138,1,1,2873,mark stevenson,Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}{'}05),1,This paper presents a novel algorithm for the acquisition of Information Extraction patterns. The approach makes the assumption that useful patterns will have similar meanings to those already identified as relevant. Patterns are compared using a variation of the standard vector space model in which information from an ontology is used to capture semantic similarity. Evaluation shows this algorithm performs well when compared with a previously reported document-centric approach.
stevenson-clough-2004-eurowordnet,{E}uro{W}ord{N}et as a Resource for Cross-language Information Retrieval,2004,9,6,1,1,2873,mark stevenson,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,One of the aims of EuroWordNet (EWN) was to provide a resource for Cross-Language Information Retrieval (CLIR). In this paper we present experiments to test the usefulness of EWN for this purpose via a formal evaluation using the Spanish queries from the TREC6 CLIR test set. All CLIR systems using bilingual dictionaries must find a way of dealing with multiple translations and we employ a word sense disambiguation algorithm for this purpose. Retrieval performance using when the disambiguation algorithm was used was 90% of that recorded using queries which had been disambiguated manually.
C04-1126,Information Extraction from Single and Multiple Sentences,2004,7,3,1,1,2873,mark stevenson,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,Some Information Extraction (IE) systems are limited to extracting events expressed in a single sentence. It is not clear what effect this has on the difficulty of the extraction task. This paper addresses the problem by comparing a corpus which has been annotated using two separate schemes: one which lists all events described in the text and another listing only those expressed within a single sentence. It was found that only 40.6% of the events in the first annotation scheme were fully contained in the second.
rose-etal-2002-reuters,The {R}euters Corpus Volume 1 -from Yesterday{'}s News to Tomorrow{'}s Language Resources,2002,11,226,2,0,53357,tony rose,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"Reuters, the global information, news and technology group, has for the first time made available free of charge, large quantities of archived Reuters news stories for use by research communities around the world. The Reuters Corpus Volume 1 (RCV1) includes over 800,000 news stories typical of the annual English language news output of Reuters. This paper describes the origins of RCV1, the motivations behind its creation, and how it differs from previous corpora. In addition we discuss the system of category coding, whereby each story is annotated for topic, region and industry sector. We also discuss the process by which these codes were applied, and examine the issues involved in maintaining quality and consistency of coding in an operational, commercial environment."
C02-1038,Augmenting Noun Taxonomies by Combining Lexical Similarity Metrics,2002,11,6,1,1,2873,mark stevenson,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,This paper presents a method for augmenting taxonomies with domain information using a simple combination of three existing lexical similarity metrics. The combined approach is evaluated by comparing their results against the annotated SEMCOR corpus. An implementation is described in which WordNet is augmented with thesaural information from the CIDE machine readable dictionary.
J01-3001,The Interaction of Knowledge Sources in Word Sense Disambiguation,2001,54,154,1,1,2873,mark stevenson,Computational Linguistics,0,Word sense disambiguation (WSD) is a computational linguistics task likely to benefit from the tradition of combining different knowledge sources in artificial in telligence research. An important step in the exploration of this hypothesis is to determine which linguistic knowledge sources are most useful and whether their combination leads to improved results. We present a sense tagger which uses several knowledge sources. Tested accuracy exceeds 94% on our evaluation corpus.Our system attempts to disambiguate all content words in running text rather than limiting itself to treating a restricted vocabulary of words. It is argued that this approach is more likely to assist the creation of practical systems.
A00-1012,Experiments on Sentence Boundary Detection,2000,12,48,1,1,2873,mark stevenson,Sixth Applied Natural Language Processing Conference,0,This paper explores the problem of identifying sentence boundaries in the transcriptions produced by automatic speech recognition systems. An experiment which determines the level of human performance for this task is described as well as a memory-based computational approach to the problem.
A00-1040,Using Corpus-derived Name Lists for Named Entity Recognition,2000,7,38,1,1,2873,mark stevenson,Sixth Applied Natural Language Processing Conference,0,"This paper describes experiments to establish the performance of a named entity recognition system which builds categorized lists of names from manually annotated training data. Names in text are then identified using only these lists. This approach does not perform as well as state-of-the-art named entity recognition systems. However, we then show that by using simple filtering techniques for improving the automatically acquired lists, substantial performance benefits can be achieved, with resulting F-measure scores of 87% on a standard test set. These results provide a baseline against which the contribution of more sophisticated supervised learning techniques for NE recognition should be measured."
E99-1050,A Corpus-Based Approach to Deriving Lexical Mappings,1999,4,0,1,1,2873,mark stevenson,Ninth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"This paper proposes a novel, corpusbased, method for producing mappings between lexical resources. Results from a preliminary experiment using part of speech tags suggests this is a promising area for future research."
W98-1208,Implementing a Sense Tagger in a General Architecture for Text Engineering,1998,28,0,2,0,41365,hamish cunningham,New Methods in Language Processing and Computational Natural Language Learning,0,"We describe two systems: GATE (General Architecture for Text Engineering), an architecture to aid in the production and delivery of language engineering systems which significantly reduces development time and ease of reuse in such systems. We also describe a sense tagger which we implemented within the GATE architecture, and which achieves high accuracy (92% of all words in text to a broad semantic level). We used the implementation of the sense tagger as a real-world task on which to evaluate the usefulness of the GATE architecture and identified strengths and weaknesses in the architecture."
P98-2228,Word Sense Disambiguation using Optimised Combinations of Knowledge Sources,1998,21,54,2,0,37311,yorick wilks,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,"Word sense disambiguation algorithms, with few exceptions, have made use of only one lexical knowledge source. We describe a system which performs word sense disambiguation on all content words in free text by combining different knowledge sources: semantic preferences, dictionary definitions and subject/domain codes along with part-of-speech tags, optimised by means of a learning algorithm. We also describe the creation of a new sense tagged corpus by combining existing resources. Tested accuracy of our approach on this corpus exceeds 92%, demonstrating the viability of all-word disambiguation rather than restricting oneself to a small sample."
C98-2223,Word Sense Disambiguation using Optimised Combinations of Knowledge Sources,1998,21,54,2,0,37311,yorick wilks,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,"Word sense disambiguation algorithms, with few exceptions, have made use of only one lexical knowledge source. We describe a system which performs word sense disambiguation on all content words in free text by combining different knowledge sources: semantic preferences, dictionary definitions and subject/domain codes along with part-of-speech tags, optimised by means of a learning algorithm. We also describe the creation of a new sense tagged corpus by combining existing resources. Tested accuracy of our approach on this corpus exceeds 92%, demonstrating the viability of all-word disambiguation rather than restricting oneself to a small sample."
W97-0208,Sense Tagging: Semantic Tagging with a Lexicon,1997,16,41,2,0,37311,yorick wilks,"Tagging Text with Lexical Semantics: Why, What, and How?",0,"Sense tagging, the automatic assignment of the appropriate sense from some lexicon to each of the words in a text, is a specialised instance of the general problem of semantic tagging by category or type. We discuss which recent word sense disambignation algorithms are appropriate for sense tagging. It is our belief that sense tagging can be carried out effectively by combining several simple, independent, methods and we include the design of such a tagger. A prototype of this system has been implemented, correctly tagging 86% of polysemous word tokens in a small test set, providing evidence that our hypothesis is correct."
