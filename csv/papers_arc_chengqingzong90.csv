2021.findings-emnlp.92,Entity-level Cross-modal Learning Improves Multi-modal Machine Translation,2021,-1,-1,3,0,6515,xin huang,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"Multi-modal machine translation (MMT) aims at improving translation performance by incorporating visual information. Most of the studies leverage the visual information through integrating the global image features as auxiliary input or decoding by attending to relevant local regions of the image. However, this kind of usage of visual information makes it difficult to figure out how the visual modality helps and why it works. Inspired by the findings of (CITATION) that entities are most informative in the image, we propose an explicit entity-level cross-modal learning approach that aims to augment the entity representation. Specifically, the approach is framed as a reconstruction task that reconstructs the original textural input from multi-modal input in which entities are replaced with visual features. Then, a multi-task framework is employed to combine the translation task and the reconstruction task to make full use of cross-modal entity representation learning. The extensive experiments demonstrate that our approach can achieve comparable or even better performance than state-of-the-art models. Furthermore, our in-depth analysis shows how visual information improves translation."
2021.emnlp-main.365,{CSDS}: A Fine-Grained {C}hinese Dataset for Customer Service Dialogue Summarization,2021,-1,-1,7,0,9457,haitao lin,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Dialogue summarization has drawn much attention recently. Especially in the customer service domain, agents could use dialogue summaries to help boost their works by quickly knowing customer{'}s issues and service progress. These applications require summaries to contain the perspective of a single speaker and have a clear topic flow structure, while neither are available in existing datasets. Therefore, in this paper, we introduce a novel Chinese dataset for Customer Service Dialogue Summarization (CSDS). CSDS improves the abstractive summaries in two aspects: (1) In addition to the overall summary for the whole dialogue, role-oriented summaries are also provided to acquire different speakers{'} viewpoints. (2) All the summaries sum up each topic separately, thus containing the topic-level structure of the dialogue. We define tasks in CSDS as generating the overall summary and different role-oriented summaries for a given dialogue. Next, we compare various summarization methods on CSDS, and experiment results show that existing methods are prone to generate redundant and incoherent summaries. Besides, the performance becomes much worse when analyzing the performance on role-oriented summaries and topic structures. We hope that this study could benchmark Chinese dialogue summarization and benefit further studies."
2021.acl-long.184,Distributed Representations of Emotion Categories in Emotion Space,2021,-1,-1,2,0,12970,xiangyu wang,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Emotion category is usually divided into different ones by human beings, but it is indeed difficult to clearly distinguish and define the boundaries between different emotion categories. The existing studies working on emotion detection usually focus on how to improve the performance of model prediction, in which emotions are represented with one-hot vectors. However, emotion relations are ignored in one-hot representations. In this article, we first propose a general framework to learn the distributed representations for emotion categories in emotion space from a given emotion classification dataset. Furthermore, based on the soft labels predicted by the pre-trained neural network model, we derive a simple and effective algorithm. Experiments have validated that the proposed representations in emotion space can express emotion relations much better than word vectors in semantic space."
2020.iwslt-1.15,{CASIA}{'}s System for {IWSLT} 2020 Open Domain Translation,2020,-1,-1,9,1,18825,qian wang,Proceedings of the 17th International Conference on Spoken Language Translation,0,This paper describes the CASIA{'}s system for the IWSLT 2020 open domain translation task. This year we participate in both ChineseâJapanese and JapaneseâChinese translation tasks. Our system is neural machine translation system based on Transformer model. We augment the training data with knowledge distillation and back translation to improve the translation performance. Domain data classification and weighted domain model ensemble are introduced to generate the final translation result. We compare and analyze the performance on development data with different model settings and different data processing techniques.
2020.emnlp-main.116,A Knowledge-driven Generative Model for Multi-implication {C}hinese Medical Procedure Entity Normalization,2020,-1,-1,5,0,20158,jinghui yan,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Medical entity normalization, which links medical mentions in the text to entities in knowledge bases, is an important research topic in medical natural language processing. In this paper, we focus on Chinese medical procedure entity normalization. However, nonstandard Chinese expressions and combined procedures present challenges in our problem. The existing strategies relying on the discriminative model are poorly to cope with normalizing combined procedure mentions. We propose a sequence generative framework to directly generate all the corresponding medical procedure entities. we adopt two strategies: category-based constraint decoding and category-based model refining to avoid unrealistic results. The method is capable of linking entities when a mention contains multiple procedure concepts and our comprehensive experiments demonstrate that the proposed model can achieve remarkable improvements over existing baselines, particularly significant in the case of multi-implication Chinese medical procedures."
2020.emnlp-main.175,Dynamic Context Selection for Document-level Neural Machine Translation via Reinforcement Learning,2020,-1,-1,4,0,20228,xiaomian kang,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Document-level neural machine translation has yielded attractive improvements. However, majority of existing methods roughly use all context sentences in a fixed scope. They neglect the fact that different source sentences need different sizes of context. To address this problem, we propose an effective approach to select dynamic context so that the document-level translation model can utilize the more useful selected context sentences to produce better translations. Specifically, we introduce a selection module that is independent of the translation module to score each candidate context sentence. Then, we propose two strategies to explicitly select a variable number of context sentences and feed them into the translation module. We train the two modules end-to-end via reinforcement learning. A novel reward is proposed to encourage the selection and utilization of dynamic context sentences. Experiments demonstrate that our approach can select adaptive context sentences for different source sentences, and significantly improves the performance of document-level translation methods."
2020.coling-main.284,Dual Attention Network for Cross-lingual Entity Alignment,2020,-1,-1,3,0,9014,jian sun,Proceedings of the 28th International Conference on Computational Linguistics,0,"Cross-lingual Entity alignment is an essential part of building a knowledge graph, which can help integrate knowledge among different language knowledge graphs. In the real KGs, there exists an imbalance among the information in the same hierarchy of corresponding entities, which results in the heterogeneity of neighborhood structure, making this task challenging. To tackle this problem, we propose a dual attention network for cross-lingual entity alignment (DAEA). Specifically, our dual attention consists of relation-aware graph attention and hierarchical attention. The relation-aware graph attention aims at selectively aggregating multi-hierarchy neighborhood information to alleviate the difference of heterogeneity among counterpart entities. The hierarchical attention adaptively aggregates the low-hierarchy and the high-hierarchy information, which is beneficial to balance the neighborhood information of counterpart entities and distinguish non-counterpart entities with similar structures. Finally, we treat cross-lingual entity alignment as a process of linking prediction. Experimental results on three real-world cross-lingual entity alignment datasets have shown the effectiveness of DAEA."
2020.coling-main.318,Distill and Replay for Continual Language Learning,2020,-1,-1,4,0,21416,jingyuan sun,Proceedings of the 28th International Conference on Computational Linguistics,0,"Accumulating knowledge to tackle new tasks without necessarily forgetting the old ones is a hallmark of human-like intelligence. But the current dominant paradigm of machine learning is still to train a model that works well on static datasets. When learning tasks in a stream where data distribution may fluctuate, fitting on new tasks often leads to forgetting on the previous ones. We propose a simple yet effective framework that continually learns natural language understanding tasks with one model. Our framework distills knowledge and replays experience from previous tasks when fitting on a new task, thus named DnR (distill and replay). The framework is based on language models and can be smoothly built with different language model architectures. Experimental results demonstrate that DnR outperfoms previous state-of-the-art models in continually learning tasks of the same type but from different domains, as well as tasks of different types. With the distillation method, we further show that it{'}s possible for DnR to incrementally compress the model size while still outperforming most of the baselines. We hope that DnR could promote the empirical application of continual language learning, and contribute to building human-level language intelligence minimally bothered by catastrophic forgetting."
2020.coling-main.397,Knowledge Graph Enhanced Neural Machine Translation via Multi-task Learning on Sub-entity Granularity,2020,-1,-1,6,0.909091,6767,yang zhao,Proceedings of the 28th International Conference on Computational Linguistics,0,"Previous studies combining knowledge graph (KG) with neural machine translation (NMT) have two problems: i) Knowledge under-utilization: they only focus on the entities that appear in both KG and training sentence pairs, making much knowledge in KG unable to be fully utilized. ii) Granularity mismatch: the current KG methods utilize the entity as the basic granularity, while NMT utilizes the sub-word as the granularity, making the KG different to be utilized in NMT. To alleviate above problems, we propose a multi-task learning method on sub-entity granularity. Specifically, we first split the entities in KG and sentence pairs into sub-entity granularity by using joint BPE. Then we utilize the multi-task learning to combine the machine translation task and knowledge reasoning task. The extensive experiments on various translation tasks have demonstrated that our method significantly outperforms the baseline models in both translation quality and handling the entities."
2020.coling-main.496,Multimodal Sentence Summarization via Multimodal Selective Encoding,2020,-1,-1,5,1,3124,haoran li,Proceedings of the 28th International Conference on Computational Linguistics,0,"This paper studies the problem of generating a summary for a given sentence-image pair. Existing multimodal sequence-to-sequence approaches mainly focus on enhancing the decoder by visual signals, while ignoring that the image can improve the ability of the encoder to identify highlights of a news event or a document. Thus, we propose a multimodal selective gate network that considers reciprocal relationships between textual and multi-level visual features, including global image descriptor, activation grids, and object proposals, to select highlights of the event when encoding the source sentence. In addition, we introduce a modality regularization to encourage the summary to capture the highlights embedded in the image more accurately. To verify the generalization of our model, we adopt the multimodal selective gate to the text-based decoder and multimodal-based decoder. Experimental results on a public multimodal sentence summarization dataset demonstrate the advantage of our models over baselines. Further analysis suggests that our proposed multimodal selective gate network can effectively select important information in the input sentence."
2020.autosimtrans-1.4,Improving Autoregressive {NMT} with Non-Autoregressive Model,2020,-1,-1,3,1,7758,long zhou,Proceedings of the First Workshop on Automatic Simultaneous Translation,0,"Autoregressive neural machine translation (NMT) models are often used to teach non-autoregressive models via knowledge distillation. However, there are few studies on improving the quality of autoregressive translation (AT) using non-autoregressive translation (NAT). In this work, we propose a novel Encoder-NAD-AD framework for NMT, aiming at boosting AT with global information produced by NAT model. Specifically, under the semantic guidance of source-side context captured by the encoder, the non-autoregressive decoder (NAD) first learns to generate target-side hidden state sequence in parallel. Then the autoregressive decoder (AD) performs translation from left to right, conditioned on source-side and target-side hidden states. Since AD has global information generated by low-latency NAD, it is more likely to produce a better translation with less time delay. Experiments on WMT14 En-De, WMT16 En-Ro, and IWSLT14 De-En translation tasks demonstrate that our framework achieves significant improvements with only 8{\%} speed degeneration over the autoregressive NMT."
2020.acl-main.121,"Attend, Translate and Summarize: An Efficient Method for Neural Cross-Lingual Summarization",2020,-1,-1,4,1,9459,junnan zhu,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Cross-lingual summarization aims at summarizing a document in one language (e.g., Chinese) into another language (e.g., English). In this paper, we propose a novel method inspired by the translation pattern in the process of obtaining a cross-lingual summary. We first attend to some words in the source text, then translate them into the target language, and summarize to get the final summary. Specifically, we first employ the encoder-decoder attention distribution to attend to the source words. Second, we present three strategies to acquire the translation probability, which helps obtain the translation candidates for each source word. Finally, each summary word is generated either from the neural distribution or from the translation candidates of source words. Experimental results on Chinese-to-English and English-to-Chinese summarization tasks have shown that our proposed method can significantly outperform the baselines, achieving comparable performance with the state-of-the-art."
2020.aacl-main.1,Touch Editing: A Flexible One-Time Interaction Approach for Translation,2020,-1,-1,5,1,18825,qian wang,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing,0,"We propose a touch-based editing method for translation, which is more flexible than traditional keyboard-mouse-based translation postediting. This approach relies on touch actions that users perform to indicate translation errors. We present a dual-encoder model to handle the actions and generate refined translations. To mimic the user feedback, we adopt the TER algorithm comparing between draft translations and references to automatically extract the simulated actions for training data construction. Experiments on translation datasets with simulated editing actions show that our method significantly improves original translation of Transformer (up to 25.31 BLEU) and outperforms existing interactive translation methods (up to 16.64 BLEU). We also conduct experiments on post-editing dataset to further prove the robustness and effectiveness of our method."
Q19-1006,Synchronous Bidirectional Neural Machine Translation,2019,14,12,3,1,7758,long zhou,Transactions of the Association for Computational Linguistics,0,"Existing approaches to neural machine translation (NMT) generate the target language sequence token-by-token from left to right. However, this kind of unidirectional decoding framework cannot make full use of the target-side future contexts which can be produced in a right-to-left decoding direction, and thus suffers from the issue of unbalanced outputs. In this paper, we introduce a synchronous bidirectional{--}neural machine translation (SB-NMT) that predicts its outputs using left-to-right and right-to-left decoding simultaneously and interactively, in order to leverage both of the history and future information at the same time. Specifically, we first propose a new algorithm that enables synchronous bidirectional decoding in a single model. Then, we present an interactive decoding model in which left-to-right (right-to-left) generation does not only depend on its previously generated outputs, but also relies on future contexts predicted by right-to-left (left-to-right) decoding. We extensively evaluate the proposed SB-NMT model on large-scale NIST Chinese{--}English, WMT14 English{--}German, and WMT18 Russian{--}English translation tasks. Experimental results demonstrate that our model achieves significant improvements over the strong Transformer model by 3.92, 1.49, and 1.04 BLEU points, respectively, and obtains the state-of-the-art performance on Chinese{--}English and English{--}German translation tasks."
P19-1117,A Compact and Language-Sensitive Multilingual Translation Method,2019,0,4,6,1,18827,yining wang,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Multilingual neural machine translation (Multi-NMT) with one encoder-decoder model has made remarkable progress due to its simple deployment. However, this multilingual translation paradigm does not make full use of language commonality and parameter sharing between encoder and decoder. Furthermore, this kind of paradigm cannot outperform the individual models trained on bilingual corpus in most cases. In this paper, we propose a compact and language-sensitive method for multilingual translation. To maximize parameter sharing, we first present a universal representor to replace both encoder and decoder models. To make the representor sensitive for specific languages, we further introduce language-sensitive embedding, attention, and discriminator with the ability to enhance model performance. We verify our methods on various translation scenarios, including one-to-many, many-to-many and zero-shot. Extensive experiments demonstrate that our proposed methods remarkably outperform strong standard multilingual translation systems on WMT and IWSLT datasets. Moreover, we find that our model is especially helpful in low-resource and zero-shot translation scenarios."
P19-1361,Incremental Learning from Scratch for Task-Oriented Dialogue Systems,2019,0,0,5,1,1748,weikang wang,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Clarifying user needs is essential for existing task-oriented dialogue systems. However, in real-world applications, developers can never guarantee that all possible user demands are taken into account in the design phase. Consequently, existing systems will break down when encountering unconsidered user needs. To address this problem, we propose a novel incremental learning framework to design task-oriented dialogue systems, or for short Incremental Dialogue System (IDS), without pre-defining the exhaustive list of user needs. Specifically, we introduce an uncertainty estimation module to evaluate the confidence of giving correct responses. If there is high confidence, IDS will provide responses to users. Otherwise, humans will be involved in the dialogue process, and IDS can learn from human intervention through an online learning module. To evaluate our method, we propose a new dataset which simulates unanticipated user needs in the deployment stage. Experiments show that IDS is robust to unconsidered user actions, and can update itself online by smartly selecting only the most effective training data, and hence attains better performance with less annotation cost."
P19-1541,Memory Consolidation for Contextual Spoken Language Understanding with Dialogue Logistic Inference,2019,20,0,4,1,5250,he bai,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Dialogue contexts are proven helpful in the spoken language understanding (SLU) system and they are typically encoded with explicit memory representations. However, most of the previous models learn the context memory with only one objective to maximizing the SLU performance, leaving the context memory under-exploited. In this paper, we propose a new dialogue logistic inference (DLI) task to consolidate the context memory jointly with SLU in the multi-task framework. DLI is defined as sorting a shuffled dialogue session into its original logical order and shares the same memory encoder and retrieval mechanism as the SLU model. Our experimental results show that various popular contextual SLU models can benefit from our approach, and improvements are quite impressive, especially in slot filling."
D19-1185,Are You for Real? Detecting Identity Fraud via Dialogue Interactions,2019,0,0,4,1,1748,weikang wang,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Identity fraud detection is of great importance in many real-world scenarios such as the financial industry. However, few studies addressed this problem before. In this paper, we focus on identity fraud detection in loan applications and propose to solve this problem with a novel interactive dialogue system which consists of two modules. One is the knowledge graph (KG) constructor organizing the personal information for each loan applicant. The other is structured dialogue management that can dynamically generate a series of questions based on the personal KG to ask the applicants and determine their identity states. We also present a heuristic user simulator based on problem analysis to evaluate our method. Experiments have shown that the trainable dialogue system can effectively detect fraudsters, and achieve higher recognition accuracy compared with rule-based systems. Furthermore, our learned dialogue strategies are interpretable and flexible, which can help promote real-world applications."
D19-1297,Attribute-aware Sequence Network for Review Summarization,2019,0,0,4,1,13359,junjie li,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Review summarization aims to generate a condensed summary for a review or multiple reviews. Existing review summarization systems mainly generate summary only based on review content and neglect the authors{'} attributes (e.g., gender, age, and occupation). In fact, when summarizing a review, users with different attributes usually pay attention to specific aspects and have their own word-using habits or writing styles. Therefore, we propose an Attribute-aware Sequence Network (ASN) to take the aforementioned users{'} characteristics into account, which includes three modules: an attribute encoder encodes the attribute preferences over the words; an attribute-aware review encoder adopts an attribute-based selective mechanism to select the important information of a review; and an attribute-aware summary decoder incorporates attribute embedding and attribute-specific word-using habits into word prediction. To validate our model, we collect a new dataset TripAtt, comprising 495,440 attribute-review-summary triplets with three kinds of attribute information: gender, age, and travel status. Extensive experiments show that ASN achieves state-of-the-art performance on review summarization in both auto-metric ROUGE and human evaluation."
D19-1302,{NCLS}: Neural Cross-Lingual Summarization,2019,0,1,7,1,9459,junnan zhu,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Cross-lingual summarization (CLS) is the task to produce a summary in one particular language for a source document in a different language. Existing methods simply divide this task into two steps: summarization and translation, leading to the problem of error propagation. To handle that, we present an end-to-end CLS framework, which we refer to as Neural Cross-Lingual Summarization (NCLS), for the first time. Moreover, we propose to further improve NCLS by incorporating two related tasks, monolingual summarization and machine translation, into the training process of CLS under multi-task learning. Due to the lack of supervised CLS data, we propose a round-trip translation strategy to acquire two high-quality large-scale CLS datasets based on existing monolingual summarization datasets. Experimental results have shown that our NCLS achieves remarkable improvement over traditional pipeline methods on both English-to-Chinese and Chinese-to-English CLS human-corrected test sets. In addition, NCLS with multi-task learning can further significantly improve the quality of generated summaries. We make our dataset and code publicly available here: http://www.nlpr.ia.ac.cn/cip/dataset.htm."
D19-1330,Synchronously Generating Two Languages with Interactive Decoding,2019,0,2,5,1,18827,yining wang,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"In this paper, we introduce a novel interactive approach to translate a source language into two different languages simultaneously and interactively. Specifically, the generation of one language relies on not only previously generated outputs by itself, but also the outputs predicted in the other language. Experimental results on IWSLT and WMT datasets demonstrate that our method can obtain significant improvements over both conventional Neural Machine Translation (NMT) model and multilingual NMT model."
L18-1143,Exploiting Pre-Ordering for Neural Machine Translation,2018,0,2,3,1,6767,yang zhao,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1146,One Sentence One Model for Neural Machine Translation,2018,-1,-1,3,1,29664,xiaoqing li,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
D18-1011,Associative Multichannel Autoencoder for Multimodal Word Representation,2018,0,3,3,1,21417,shaonan wang,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"In this paper we address the problem of learning multimodal word representations by integrating textual, visual and auditory inputs. Inspired by the re-constructive and associative nature of human memory, we propose a novel associative multichannel autoencoder (AMA). Our model first learns the associations between textual and perceptual modalities, so as to predict the missing perceptual information of concepts. Then the textual and predicted perceptual representations are fused through reconstructing their original and associated embeddings. Using a gating mechanism our model assigns different weights to each modality according to the different concepts. Results on six benchmark concept similarity tests show that the proposed method significantly outperforms strong unimodal baselines and state-of-the-art multimodal models."
D18-1036,Addressing Troublesome Words in Neural Machine Translation,2018,0,7,4,1,6767,yang zhao,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"One of the weaknesses of Neural Machine Translation (NMT) is in handling lowfrequency and ambiguous words, which we refer as troublesome words. To address this problem, we propose a novel memoryenhanced NMT method. First, we investigate different strategies to define and detect the troublesome words. Then, a contextual memory is constructed to memorize which target words should be produced in what situations. Finally, we design a hybrid model to dynamically access the contextual memory so as to correctly translate the troublesome words. The extensive experiments on Chinese-to-English and English-to-German translation tasks demonstrate that our method significantly outperforms the strong baseline models in translation quality, especially in handling troublesome words."
D18-1173,"Memory, Show the Way: Memory Based Few Shot Word Representation Learning",2018,0,2,3,0,21416,jingyuan sun,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Distributional semantic models (DSMs) generally require sufficient examples for a word to learn a high quality representation. This is in stark contrast with human who can guess the meaning of a word from one or a few referents only. In this paper, we propose Mem2Vec, a memory based embedding learning method capable of acquiring high quality word representations from fairly limited context. Our method directly adapts the representations produced by a DSM with a longterm memory to guide its guess of a novel word. Based on a pre-trained embedding space, the proposed method delivers impressive performance on two challenging few-shot word similarity tasks. Embeddings learned with our method also lead to considerable improvements over strong baselines on NER and sentiment classification."
D18-1326,Three Strategies to Improve One-to-Many Multilingual Translation,2018,0,7,5,1,18827,yining wang,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Due to the benefits of model compactness, multilingual translation (including many-to-one, many-to-many and one-to-many) based on a universal encoder-decoder architecture attracts more and more attention. However, previous studies show that one-to-many translation based on this framework cannot perform on par with the individually trained models. In this work, we introduce three strategies to improve one-to-many multilingual translation by balancing the shared and unique features. Within the architecture of one decoder for all target languages, we first exploit the use of unique initial states for different target languages. Then, we employ language-dependent positional embeddings. Finally and especially, we propose to divide the hidden cells of the decoder into shared and language-dependent ones. The extensive experiments demonstrate that our proposed methods can obtain remarkable improvements over the strong baselines. Moreover, our strategies can achieve comparable or even better performance than the individually trained translation models."
D18-1415,A Teacher-Student Framework for Maintainable Dialog Manager,2018,0,3,5,1,1748,weikang wang,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Reinforcement learning (RL) is an attractive solution for task-oriented dialog systems. However, extending RL-based systems to handle new intents and slots requires a system redesign. The high maintenance cost makes it difficult to apply RL methods to practical systems on a large scale. To address this issue, we propose a practical teacher-student framework to extend RL-based dialog systems without retraining from scratch. Specifically, the {``}student{''} is an extended dialog manager based on a new ontology, and the {``}teacher{''} is existing resources used for guiding the learning process of the {``}student{''}. By specifying constraints held in the new dialog manager, we transfer knowledge of the {``}teacher{''} to the {``}student{''} without additional resources. Experiments show that the performance of the extended system is comparable to the system trained from scratch. More importantly, the proposed framework makes no assumption about the unsupported intents and slots, which makes it possible to improve RL-based systems incrementally."
D18-1448,{MSMO}: Multimodal Summarization with Multimodal Output,2018,0,7,6,1,9459,junnan zhu,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Multimodal summarization has drawn much attention due to the rapid growth of multimedia data. The output of the current multimodal summarization systems is usually represented in texts. However, we have found through experiments that multimodal output can significantly improve user satisfaction for informativeness of summaries. In this paper, we propose a novel task, multimodal summarization with multimodal output (MSMO). To handle this task, we first collect a large-scale dataset for MSMO research. We then propose a multimodal attention model to jointly generate text and select the most relevant image from the multimodal input. Finally, to evaluate multimodal outputs, we construct a novel multimodal automatic evaluation (MMAE) method which considers both intra-modality salience and inter-modality relevance. The experimental results show the effectiveness of MMAE."
C18-1035,Adopting the Word-Pair-Dependency-Triplets with Individual Comparison for Natural Language Inference,2018,0,1,2,0,30757,qianlong du,Proceedings of the 27th International Conference on Computational Linguistics,0,"This paper proposes to perform natural language inference with Word-Pair-Dependency-Triplets. Most previous DNN-based approaches either ignore syntactic dependency among words, or directly use tree-LSTM to generate sentence representation with irrelevant information. To overcome the problems mentioned above, we adopt Word-Pair-Dependency-Triplets to improve alignment and inference judgment. To be specific, instead of comparing each triplet from one passage with the merged information of another passage, we first propose to perform comparison directly between the triplets of the given passage-pair to make the judgement more interpretable. Experimental results show that the performance of our approach is better than most of the approaches that use tree structures, and is comparable to other state-of-the-art approaches."
C18-1079,"Document-level Multi-aspect Sentiment Classification by Jointly Modeling Users, Aspects, and Overall Ratings",2018,0,3,3,1,13359,junjie li,Proceedings of the 27th International Conference on Computational Linguistics,0,"Document-level multi-aspect sentiment classification aims to predict user{'}s sentiment polarities for different aspects of a product in a review. Existing approaches mainly focus on text information. However, the authors (i.e. users) and overall ratings of reviews are ignored, both of which are proved to be significant on interpreting the sentiments of different aspects in this paper. Therefore, we propose a model called Hierarchical User Aspect Rating Network (HUARN) to consider user preference and overall ratings jointly. Specifically, HUARN adopts a hierarchical architecture to encode word, sentence, and document level information. Then, user attention and aspect attention are introduced into building sentence and document level representation. The document representation is combined with user and overall rating information to predict aspect ratings of a review. Diverse aspects are treated differently and a multi-task framework is adopted. Empirical results on two real-world datasets show that HUARN achieves state-of-the-art performances."
C18-1121,Ensure the Correctness of the Summary: Incorporate Entailment Knowledge into Abstractive Sentence Summarization,2018,0,13,4,1,3124,haoran li,Proceedings of the 27th International Conference on Computational Linguistics,0,"In this paper, we investigate the sentence summarization task that produces a summary from a source sentence. Neural sequence-to-sequence models have gained considerable success for this task, while most existing approaches only focus on improving the informativeness of the summary, which ignore the correctness, i.e., the summary should not contain unrelated information with respect to the source sentence. We argue that correctness is an essential requirement for summarization systems. Considering a correct summary is semantically entailed by the source sentence, we incorporate entailment knowledge into abstractive summarization models. We propose an entailment-aware encoder under multi-task framework (i.e., summarization generation and entailment recognition) and an entailment-aware decoder by entailment Reward Augmented Maximum Likelihood (RAML) training. Experiment results demonstrate that our models significantly outperform baselines from the aspects of informativeness and correctness."
C18-1305,Source Critical Reinforcement Learning for Transferring Spoken Language Understanding to a New Language,2018,0,1,6,1,5250,he bai,Proceedings of the 27th International Conference on Computational Linguistics,0,"To deploy a spoken language understanding (SLU) model to a new language, language transferring is desired to avoid the trouble of acquiring and labeling a new big SLU corpus. An SLU corpus is a monolingual corpus with domain/intent/slot labels. Translating the original SLU corpus into the target language is an attractive strategy. However, SLU corpora consist of plenty of semantic labels (slots), which general-purpose translators cannot handle well, not to mention additional culture differences. This paper focuses on the language transferring task given a small in-domain parallel SLU corpus. The in-domain parallel corpus can be used as the first adaptation on the general translator. But more importantly, we show how to use reinforcement learning (RL) to further adapt the adapted translator, where translated sentences with more proper slot tags receive higher rewards. Our reward is derived from the source input sentence exclusively, unlike reward via actor-critical methods or computing reward with a ground truth target sentence. Hence we can adapt the translator the second time, using the big monolingual SLU corpus from the source language. We evaluate our approach on Chinese to English language transferring for SLU systems. The experimental results show that the generated English SLU corpus via adaptation and reinforcement learning gives us over 97{\%} in the slot F1 score and over 84{\%} accuracy in domain classification. It demonstrates the effectiveness of the proposed language transferring method. Compared with naive translation, our proposed method improves domain classification accuracy by relatively 22{\%}, and the slot filling F1 score by relatively more than 71{\%}."
W17-6005,Learning from Parenthetical Sentences for Term Translation in Machine Translation,2017,12,0,4,0,13067,guoping huang,Proceedings of the 9th {SIGHAN} Workshop on {C}hinese Language Processing,0,"Terms extensively exist in specific domains, and term translation plays a critical role in domain-specific machine translation (MT) tasks. However, it{'}s a challenging task to translate them correctly for the huge number of pre-existing terms and the endless new terms. To achieve better term translation quality, it is necessary to inject external term knowledge into the underlying MT system. Fortunately, there are plenty of term translation knowledge in parenthetical sentences on the Internet. In this paper, we propose a simple, straightforward and effective framework to improve term translation by learning from parenthetical sentences. This framework includes: (1) a focused web crawler; (2) a parenthetical sentence filter, acquiring parenthetical sentences including bilingual term pairs; (3) a term translation knowledge extractor, extracting bilingual term translation candidates; (4) a probability learner, generating the term translation table for MT decoders. The extensive experiments demonstrate that our proposed framework significantly improves the translation quality of terms and sentences."
P17-2060,Neural System Combination for Machine Translation,2017,32,16,4,1,7758,long zhou,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Neural machine translation (NMT) becomes a new approach to machine translation and generates much more fluent results compared to statistical machine translation (SMT). However, SMT is usually better than NMT in translation adequacy. It is therefore a promising direction to combine the advantages of both NMT and SMT. In this paper, we propose a neural system combination framework leveraging multi-source NMT, which takes as input the outputs of NMT and SMT systems and produces the final translation. Extensive experiments on the Chinese-to-English translation task show that our model archives significant improvement by 5.3 BLEU points over the best single system output and 3.4 BLEU points over the state-of-the-art traditional system combination methods."
I17-1039,Towards Neural Machine Translation with Partially Aligned Corpora,2017,30,0,4,1,18827,yining wang,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),0,"While neural machine translation (NMT) has become the new paradigm, the parameter optimization requires large-scale parallel data which is scarce in many domains and language pairs. In this paper, we address a new translation scenario in which there only exists monolingual corpora and phrase pairs. We propose a new method towards translation with partially aligned sentence pairs which are derived from the phrase pairs and monolingual corpora. To make full use of the partially aligned corpora, we adapt the conventional NMT training method in two aspects. On one hand, different generation strategies are designed for aligned and unaligned target words. On the other hand, a different objective function is designed to model the partially aligned parts. The experiments demonstrate that our method can achieve a relatively good result in such a translation scenario, and tiny bitexts can boost translation quality to a large extent."
D17-1029,Exploiting Word Internal Structures for Generic {C}hinese Sentence Representation,2017,16,5,3,1,21417,shaonan wang,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"We introduce a novel mixed characterword architecture to improve Chinese sentence representations, by utilizing rich semantic information of word internal structures. Our architecture uses two key strategies. The first is a mask gate on characters, learning the relation among characters in a word. The second is a maxpooling operation on words, adaptively finding the optimal mixture of the atomic and compositional word representations. Finally, the proposed architecture is applied to various sentence composition models, which achieves substantial performance gains over baseline models on sentence similarity task."
D17-1114,"Multi-modal Summarization for Asynchronous Collection of Text, Image, Audio and Video",2017,15,4,5,1,3124,haoran li,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"The rapid increase of the multimedia data over the Internet necessitates multi-modal summarization from collections of text, image, audio and video. In this work, we propose an extractive Multi-modal Summarization (MMS) method which can automatically generate a textual summary given a set of documents, images, audios and videos related to a specific topic. The key idea is to bridge the semantic gaps between multi-modal contents. For audio information, we design an approach to selectively use its transcription. For vision information, we learn joint representations of texts and images using a neural network. Finally, all the multi-modal aspects are considered to generate the textural summary by maximizing the salience, non-redundancy, readability and coverage through budgeted optimization of submodular functions. We further introduce an MMS corpus in English and Chinese. The experimental results on this dataset demonstrate that our method outperforms other competitive baseline methods."
L16-1159,A Bilingual Discourse Corpus and Its Applications,2016,2,0,3,0,1457,yang liu,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Existing discourse research only focuses on the monolingual languages and the inconsistency between languages limits the power of the discourse theory in multilingual applications such as machine translation. To address this issue, we design and build a bilingual discource corpus in which we are currently defining and annotating the bilingual elementary discourse units (BEDUs). The BEDUs are then organized into hierarchical structures. Using this discourse style, we have annotated nearly 20K LDC sentences. Finally, we design a bilingual discourse based method for machine translation evaluation and show the effectiveness of our bilingual discourse annotations."
K16-2003,An End-to-End {C}hinese Discourse Parser with Adaptation to Explicit and Non-explicit Relation Recognition,2016,18,8,5,0,20228,xiaomian kang,Proceedings of the {C}o{NLL}-16 shared task,0,"This paper describes our end-to-end discourse parser in the CoNLL-2016 Shared Task on Chinese Shallow Discourse Parsing. To adapt to the characteristics of Chinese, we implement a uniform framework for both explicit and non-explicit relation parsing. In this framework, we are the first to utilize a seed-expansion approach for the argument extraction subtask. In the official evaluation, our system achieves an F1 score of 26.90% in overall performance on the blind test set."
D16-1160,Exploiting Source-side Monolingual Data in Neural Machine Translation,2016,30,79,2,1,6594,jiajun zhang,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
C16-1020,An Empirical Exploration of Skip Connections for Sequential Tagging,2016,22,0,3,0,35691,huijia wu,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"In this paper, we empirically explore the effects of various kinds of skip connections in stacked bidirectional LSTMs for sequential tagging. We investigate three kinds of skip connections connecting to LSTM cells: (a) skip connections to the gates, (b) skip connections to the internal states and (c) skip connections to the cell outputs. We present comprehensive experiments showing that skip connections to cell outputs outperform the remaining two. Furthermore, we observe that using gated identity functions as skip mappings works pretty well. Based on this novel skip connections, we successfully train deep stacked bidirectional LSTM models and obtain state-of-the-art results on CCG supertagging and comparable results on POS tagging."
Q15-1020,Domain Adaptation for Syntactic and Semantic Dependency Parsing Using Deep Belief Networks,2015,35,6,3,1,30780,haitong yang,Transactions of the Association for Computational Linguistics,0,"In current systems for syntactic and semantic dependency parsing, people usually define a very high-dimensional feature space to achieve good performance. But these systems often suffer severe performance drops on out-of-domain test data due to the diversity of features of different domains. This paper focuses on how to relieve this domain adaptation problem with the help of unlabeled target domain data. We propose a deep learning method to adapt both syntactic and semantic parsers. With additional unlabeled target domain data, our method can learn a latent feature representation (LFR) that is beneficial to both domains. Experiments on English data in the CoNLL 2009 shared task show that our method largely reduced the performance drop on out-of-domain test data. Moreover, we get a Macro F1 score that is 2.32 points higher than the best system in the CoNLL 2009 shared task in out-of-domain tests."
2015.iwslt-keynotes.1,Improving {SMT} by model filtering and phrase embedding,2015,-1,-1,1,1,6630,chengqing zong,Proceedings of the 12th International Workshop on Spoken Language Translation: Keynotes,0,None
W14-6831,A Study on Personal Attributes Extraction Based on the Combination of Sentences Classifications and Rules,2014,8,0,2,0,38139,nanchang cheng,Proceedings of The Third {CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,"Personal attributes extraction plays a significant role in information mining, event tracing and personal name disambiguation. It mainly involves two problems, attribute recognition and decision making on whether this attribute belongs to the extracted person. Personal attributes generally involve named entities, which are recognized mainly by adjusting word segmentation software. As for those which cannot be recognized by word segmentation, the combination of feature words and rules can be used for their recognition. The combination of sentences classifications and rules is employed for attribute ownership decision. At first, all the sentences in the document are classified into those with attribute words and those without, with the latter omitted. The former are then classified into description sentences with one person and description sentences with more persons, according to the criterion that whether there are more than one person described in the sentence. According to statistics of description sentences with one person, anaphora resolution is not necessary, which reduces recognition errors from anaphora resolution failures. Minimum slicing is used for description sentences with more persons, and attribute ownership decision is made within the minimum language segment with the co-occurrence of both the person and the attribute. This method achieves 0.507388780 and 0.489505010 respectively in the lenient evaluation results and the strict evaluation results of SF_Value in"
P14-2126,{RNN}-based Derivation Structure Prediction for {SMT},2014,19,3,4,1,25622,feifei zhai,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"In this paper, we propose a novel derivation structure prediction (DSP) model for SMT using recursive neural network (RNN). Within the model, two steps are involved: (1) phrase-pair vector representation, to learn vector representations for phrase pairs; (2) derivation structure prediction, to generate a bilingual RNN that aims to distinguish good derivation structures from bad ones. Final experimental results show that our DSP model can significantly improve the translation quality."
P14-1011,Bilingually-constrained Phrase Embeddings for Machine Translation,2014,29,81,5,1,6594,jiajun zhang,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We propose Bilingually-constrained Recursive Auto-encoders (BRAE) to learn semantic phrase embeddings (compact vector representations for phrases), which can distinguish the phrases with different semantic meanings. The BRAE is trained in a way that minimizes the semantic distance of translation equivalents and maximizes the semantic distance of nontranslation pairs simultaneously. After training, the model learns how to embed each phrase semantically in two languages and also learns how to transform semantic embedding space in one language to the other. We evaluate our proposed method on two end-to-end SMT tasks (phrase table pruning and decoding with phrasal semantic similarities) which need to measure semantic similarity between a source phrase and its translation candidates. Extensive experiments show that the BRAE is remarkably effective in these two tasks."
P14-1080,Enhancing Grammatical Cohesion: Generating Transitional Expressions for {SMT},2014,25,10,3,1,39195,mei tu,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Transitional expressions provide glue that holds ideas together in a text and enhance the logical organization, which together help improve readability of a text. However, in most current statistical machine translation (SMT) systems, the outputs of compound-complex sentences still lack proper transitional expressions. As a result, the translations are often hard to read and understand. To address this issue, we propose two novel models to encourage generating such transitional expressions by introducing the source compoundcomplex sentence structure (CSS). Our models include a CSS-based translation model, which generates new CSS-based translation rules, and a generative transfer model, which encourages producing transitional expressions during decoding. The two models are integrated into a hierarchical phrase-based translation system to evaluate their effectiveness. The experimental results show that significant improvements are achieved on various test data meanwhile the translations are more cohesive and smooth."
D14-1041,Multi-Predicate Semantic Role Labeling,2014,32,13,2,1,30780,haitong yang,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"The current approaches to Semantic Role Labeling (SRL) usually perform role classification for each predicate separately and the interaction among individual predicatexe2x80x99s role labeling is ignored if there is more than one predicate in a sentence. In this paper, we prove that different predicates in a sentence could help each other during SRL. In multi-predicate role labeling, there are mainly two key points: argument identification and role labeling of the arguments shared by multiple predicates. To address these issues, in the stage of argument identification, we propose novel predicate-related features which help remove many argument identification errors; in the stage of argument classification, we adopt a discriminative reranking approach to perform role classification of the shared arguments, in which a large set of global features are proposed. We conducted experiments on two standard benchmarks: Chinese PropBank and English PropBank. The experimental results show that our approach can significantly improve SRL performance, especially in Chinese PropBank."
C14-1039,Dynamically Integrating Cross-Domain Translation Memory into Phrase-Based Machine Translation during Decoding,2014,25,4,2,1,7866,kun wang,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"Our previous work focuses on combining translation memory (TM) and statistical machine translation (SMT) when the TM database and the SMT training set are the same. However, the TM database will deviate from the SMT training set in the real task when time goes by. In this work, we concentrate on the task when the TM database and the SMT training set are different and even from different domains. Firstly, we dynamically merge the matched TM phrase-pairs into the SMT phrase table to meet the real application. Secondly, we propose an improved integrated model to distinguish the original and the newly-added phrase-pairs. Thirdly, a simple but effective TM adaptation method is adopted to favor the consistent translations in cross-domain test. Our experiments have shown that merging the TM phrasepairs achieves significant improvements. Furthermore, the proposed approaches are significantly better than the TM, the SMT and previous integration works for both in-domain and cross-domain tests."
Y13-1010,A Study of the Effectiveness of Suffixes for {C}hinese Word Segmentation,2013,30,2,2,1,29664,xiaoqing li,"Proceedings of the 27th Pacific Asia Conference on Language, Information, and Computation ({PACLIC} 27)",0,"We investigate whether suffix related features can significantly improve the performance of character-based approaches for Chinese word segmentation (CWS). Since suffixes are quite productive in forming new words, and OOV is the main error source for CWS, many researchers expect that suffix information can further improve the performance. With this belief, we tried several suffix related features in both generative and discriminative approaches. However, our experiment results have shown that significant improvement can hardly be achieved by incorporating suffix related features into those widely adopted surface features, which is against the commonly believed supposition. Error analysis reveals that the main problem behind this surprising finding is the conflict between the degree of reliability and the coverage rate of suffix related features."
Q13-1020,Unsupervised Tree Induction for Tree-based Translation,2013,44,3,4,1,25622,feifei zhai,Transactions of the Association for Computational Linguistics,0,"In current research, most tree-based translation models are built directly from parse trees. In this study, we go in another direction and build a translation model with an unsupervised tree structure derived from a novel non-parametric Bayesian model. In the model, we utilize synchronous tree substitution grammars (STSG) to capture the bilingual mapping between language pairs. To train the model efficiently, we develop a Gibbs sampler with three novel Gibbs operators. The sampler is capable of exploring the infinite space of tree structures by performing local changes on the tree nodes. Experimental results show that the string-to-tree translation system using our Bayesian tree structures significantly outperforms the strong baseline string-to-tree system using parse trees."
Q13-1024,Large-scale Word Alignment Using Soft Dependency Cohesion Constraints,2013,39,6,2,1,10548,zhiguo wang,Transactions of the Association for Computational Linguistics,0,"Dependency cohesion refers to the observation that phrases dominated by disjoint dependency subtrees in the source language generally do not overlap in the target language. It has been verified to be a useful constraint for word alignment. However, previous work either treats this as a hard constraint or uses it as a feature in discriminative models, which is ineffective for large-scale tasks. In this paper, we take dependency cohesion as a soft constraint, and integrate it into a generative model for large-scale word alignment experiments. We also propose an approximate EM algorithm and a Gibbs sampling algorithm to estimate model parameters in an unsupervised manner. Experiments on large-scale Chinese-English translation tasks demonstrate that our model achieves improvements in both alignment quality and translation quality."
P13-2066,A Novel Translation Framework Based on {R}hetorical {S}tructure {T}heory,2013,15,12,3,1,39195,mei tu,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Rhetorical structure theory (RST) is widely used for discourse understanding, which represents a discourse as a hierarchically semantic structure. In this paper, we propose a novel translation framework with the help of RST. In our framework, the translation process mainly includes three steps: 1) Source RST-tree acquisition: a source sentence is parsed into an RST tree; 2) Rule extraction: translation rules are extracted from the source tree and the target string via bilingual word alignment; 3) RST-based translation: the source RST-tree is translated with translation rules. Experiments on Chinese-to-English show that our RST-based approach achieves improvements of 2.3/0.77/1.43 BLEU points on NIST04/NIST05/CWMT2008 respectively."
P13-2093,Dual Training and Dual Prediction for Polarity Classification,2013,9,13,5,1,8473,rui xia,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Bag-of-words (BOW) is now the most popular way to model text in machine learning based sentiment classification. However, the performance of such approach sometimes remains rather limited due to some fundamental deficiencies of the BOW model. In this paper, we focus on the polarity shift problem, and propose a novel approach, called dual training and dual prediction (DTDP), to address it. The basic idea of DTDP is to first generate artificial samples that are polarity-opposite to the original samples by polarity reversion, and then leverage both the original and opposite samples for (dual) training and (dual) prediction. Experimental results on four datasets demonstrate the effectiveness of the proposed approach for polarity classification."
P13-2110,"A Lattice-based Framework for Joint {C}hinese Word Segmentation, {POS} Tagging and Parsing",2013,17,14,2,1,10548,zhiguo wang,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"For the cascaded task of Chinese word segmentation, POS tagging and parsing, the pipeline approach suffers from error propagation while the joint learning approach suffers from inefficient decoding due to the large combined search space. In this paper, we present a novel lattice-based framework in which a Chinese sentence is first segmented into a word lattice, and then a lattice-based POS tagger and a lattice-based parser are used to process the lattice from two different viewpoints: sequential POS tagging and hierarchical tree building. A strategy is designed to exploit the complementary strengths of the tagger and parser, and encourage them to predict agreed structures. Experimental results on Chinese Treebank show that our lattice-based framework significantly improves the accuracy of the three sub-tasks."
P13-1002,Integrating Translation Memory into Phrase-Based Machine Translation during Decoding,2013,25,15,2,1,7866,kun wang,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Since statistical machine translation (SMT) and translation memory (TM) complement each other in matched and unmatched regions, integrated models are proposed in this paper to incorporate TM information into phrase-based SMT. Unlike previous multi-stage pipeline approaches, which directly merge TM result into the final output, the proposed models refer to the corresponding TM information associated with each phrase at SMT decoding. On a Chinesexe2x80x93English TM database, our experiments show that the proposed integrated Model-III is significantly better than either the SMT or the TM systems when the fuzzy match score is above 0.4. Furthermore, integrated Model-III achieves overall 3.48 BLEU points improvement and 2.62 TER points reduction in comparison with the pure SMT system. Besides, the proposed models also outperform previous approaches significantly."
P13-1111,Handling Ambiguities of Bilingual Predicate-Argument Structures for Statistical Machine Translation,2013,31,6,4,1,25622,feifei zhai,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Predicate-argument structure (PAS) has been demonstrated to be very effective in improving SMT performance. However, since a sourceside PAS might correspond to multiple different target-side PASs, there usually exist many PAS ambiguities during translation. In this paper, we group PAS ambiguities into two types: role ambiguity and gap ambiguity. Then we propose two novel methods to handle the two PAS ambiguities for SMT accordingly: 1) inside context integration; 2) a novel maximum entropy PAS disambiguation (MEPD) model. In this way, we incorporate rich context information of PAS for disambiguation. Then we integrate the two methods into a PASbased translation framework. Experiments show that our approach helps to achieve significant improvements on translation quality."
P13-1140,Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation,2013,34,24,2,1,6594,jiajun zhang,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Currently, almost all of the statistical machine translation (SMT) models are trained with the parallel corpora in some specific domains. However, when it comes to a language pair or a different domain without any bilingual resources, the traditional SMT loses its power. Recently, some research works study the unsupervised SMT for inducing a simple word-based translation model from the monolingual corpora. It successfully bypasses the constraint of bitext for SMT and obtains a relatively promising result. In this paper, we take a step forward and propose a simple but effective method to induce a phrase-based model from the monolingual corpora given an automatically-induced translation lexicon or a manually-edited translation dictionary. We apply our method for the domain adaptation task and the extensive experiments show that our proposed method can substantially improve the translation quality."
J13-2001,A Joint Model to Identify and Align Bilingual Named Entities,2013,66,18,2,1,6531,yufeng chen,Computational Linguistics,0,"In this article, an integrated model is derived that jointly identifies and aligns bilingual named entities NEs between Chinese and English. The model is motivated by the following observations: 1 whether an NE is translated semantically or phonetically depends greatly on its entity type, 2 entities within an aligned pair should share the same type, and 3 the initially detected NEs can act as anchors and provide further information while selecting NE candidates. Based on these observations, this article proposes a translation mode ratio feature defined as the proportion of NE internal tokens that are semantically translated, enforces an entity type consistency constraint, and utilizes additional new NE likelihoods based on the initially detected NE anchors.n n Experiments show that this novel method significantly outperforms the baseline. The type-insensitive F-score of identified NE pairs increases from 78.4% to 88.0% 12.2% relative improvement in our Chinese-English NE alignment task, and the type-sensitive F-score increases from 68.4% to 83.0% 21.3% relative improvement. Furthermore, the proposed model demonstrates its robustness when it is tested across different domains. Finally, when semi-supervised learning is conducted to train the adopted English NE recognition model, the proposed model also significantly boosts the English NE recognition type-sensitive F-score."
C12-1101,Integrating Surface and Abstract Features for Robust Cross-Domain {C}hinese Word Segmentation,2012,28,2,3,1,29664,xiaoqing li,Proceedings of {COLING} 2012,0,"Current character-based approaches are not robust for cross domain Chin ese word segmentation. In this paper, we alleviate this problem by deriving a novel enhanced ch aracter-based generative model with a new abstract aggregate candidate-feature, which indicates if th e given candidate prefers the corresponding position-tag of the longest dictionary matching wo rd. Since the distribution of the proposed feature is invariant across domains, our model thus possesses better generalization ability. Open tests on CIPS-SIGHAN-2010 show that the enhanced generative model achieves robust cross-domain performance for various OOV coverage rates and obtains the best performance on three out of four domains. The enhanced gen erative model is then further integrated with a discriminative model which also utilizes dictionary information . This integrated model is shown to be either superior or comparable to all other models repo rted in the literatur e on every domain of this task."
C12-1185,Machine Translation by Modeling Predicate-Argument Structure Transformation,2012,37,11,4,1,25622,feifei zhai,Proceedings of {COLING} 2012,0,"Machine translation aims to generate a target sentence that is semantically equivalent to the source sentence. However, most of current statistical machine translation models do not model the semantics of sentences. In this paper, we propose a novel translation framework based on predicate-argument struct ure (PAS) for its capacity on grasping the semantics and skeleton structure of sentences. By usin g PAS, the framework effectively models both semantics of languages and global reordering for translation. In the framework, we divide the translation process into 3 steps: (1) PAS acquisition: perform semantic role labeling (SRL) on the input sentences to acquire source-side PASs; (2) Transformation: convert source-side PASs to their target counterparts by predicate-aw are PAS transformation rules; (3) Translation: first translate the predicate and arguments of PAS and then adopt a CKY-style decoding algorithm to translate the entire PAS. Experimental results show that our PAS-based translation framework significantly improves the translation performance."
C12-1186,Tree-based Translation without using Parse Trees,2012,48,11,4,1,25622,feifei zhai,Proceedings of {COLING} 2012,0,"Parse trees are indispensable to the existing tree- based translation models. However, there exist two major challenges in utilizing parse trees: 1) Fo r most language pairs, it is hard to get parse trees due to the lack of syntactic resources for training. 2) Numero us parse trees are not compatible with word alignment which is generally learned by GIZA. Therefore, a number of useful translation rules are often excluded. To overcome these two problems, in this paper we make a great effort to bypass the parse trees and induce effective unsupervised trees for treebased translation models. Our unsupervised trees depend only on the word alignment without utilizing any syntactic resource or linguistic pars er. Hence, they are very beneficial for the translation between resource-poor languages. Our experimental results have shown that the string-to-tree translation system using our unsupervised trees significantly outperforms th e stringto-tree system using parse trees."
2012.iwslt-papers.9,A universal approach to translating numerical and time expressions,2012,11,2,3,1,39195,mei tu,Proceedings of the 9th International Workshop on Spoken Language Translation: Papers,0,"Although statistical machine translation (SMT) has made great progress since it came into being, the translation of numerical and time expressions is still far from satisfactory. Generally speaking, numbers are likely to be out-of-vocabulary (OOV) words due to their non-exhaustive characteristics even when the size of training data is very large, so it is difficult to obtain accurate translation results for the infinite set of numbers only depending on traditional statistical methods. We propose a language-independent framework to recognize and translate numbers more precisely by using a rule-based method. Through designing operators, we succeed to make rules educible and totally separate from codes, thus, we can extend rules to various language-pairs without re-coding, which contributes a lot to the efficient development of an SMT system with good portability. We classify numbers and time expressions into seven types, which are Arabic number, cardinal numbers, ordinal numbers, date, time of day, day of week and figures. A greedy algorithm is developed to deal with rule conflicts. Experiments have shown that our approach can significantly improve the translation performance."
P11-2028,Automatic Evaluation of {C}hinese Translation Output: Word-Level or Character-Level?,2011,17,14,2,1,11675,maoxi li,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,"Word is usually adopted as the smallest unit in most tasks of Chinese language processing. However, for automatic evaluation of the quality of Chinese translation output when translating from other languages, either a word-level approach or a character-level approach is possible. So far, there has been no detailed study to compare the correlations of these two approaches with human assessment. In this paper, we compare word-level metrics with character-level metrics on the submitted output of English-to-Chinese translation systems in the IWSLT'08 CT-EC and NIST'08 EC tasks. Our experimental results reveal that character-level metrics correlate with human assessment better than word-level metrics. Our analysis suggests several key reasons behind this finding."
I11-1016,A Semantic-Specific Model for {C}hinese Named Entity Translation,2011,24,0,2,1,6531,yufeng chen,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"We observe that (1) it is difficult to combine transliteration and meaning translation when transforming named entities (NE); and (2) there are different translation variations in NE translation, due to different semantic information. From this basis, we propose a novel semantic-specific NE translation model, which automatically incorporates the global context from corpus in order to capture substantial semantic information. The presented approach is inspired by example-based translation and realized by log-linear models, integrating monolingual context similarity model, bilingual context similarity model, and mixed language model. The experiments show that the semantic-specific model has substantially and consistently outperformed the baselines and related NE translation systems."
I11-1069,A {POS}-based Ensemble Model for Cross-domain Sentiment Classification,2011,14,31,2,1,8473,rui xia,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"In this paper, we focus on the tasks of cross-domain sentiment classification. We find across different domains, features with some types of part-of-speech (POS) tags are domain-dependent, while some others are domain-free. Based on this finding, we proposed a POS-based ensemble model to efficiently integrate features with different types of POS tags to improve the classification performance. Weights are trained by stochastic gradient descent (SGD) to optimize the perceptron and minimal classification error (MCE) criteria. Experimental results show that the proposed ensemble model is quite effective for the task of cross-domain sentiment classification."
I11-1140,Parse Reranking Based on Higher-Order Lexical Dependencies,2011,10,8,2,1,10548,zhiguo wang,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"Existing work shows that lexical dependencies are helpful for constituent tree parsing. However, only first-order lexical dependencies have been employed and investigated in previous work. In this paper, we propose a method to employing higher-order 1 lexical dependencies for constituent tree evaluation. Our method is based on a parse reranking framework, which provides a constrained search space (via N-best lists or parse forests) and enables our parser to employ relatively complicated dependency features. We evaluate our models on the Penn Chinese Treebank. The highest F1"
D11-1019,Augmenting String-to-Tree Translation Models with Fuzzy Use of Source-side Syntax,2011,33,20,3,1,6594,jiajun zhang,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,"Due to its explicit modeling of the grammaticality of the output via target-side syntax, the string-to-tree model has been shown to be one of the most successful syntax-based translation models. However, a major limitation of this model is that it does not utilize any useful syntactic information on the source side. In this paper, we analyze the difficulties of incorporating source syntax in a string-to-tree model. We then propose a new way to use the source syntax in a fuzzy manner, both in source syntactic annotation and in rule matching. We further explore three algorithms in rule matching: 0-1 matching, likelihood matching, and deep similarity matching. Our method not only guarantees grammatical output with an explicit target tree, but also enables the system to choose the proper translation rules via fuzzy use of the source syntax. Our extensive experiments have shown significant improvements over the state-of-the-art string-to-tree system."
2011.mtsummit-papers.29,Simple but Effective Approaches to Improving Tree-to-tree Model,2011,32,8,4,1,25622,feifei zhai,Proceedings of Machine Translation Summit XIII: Papers,0,None
W10-4133,A Character-Based Joint Model for {CIPS}-{SIGHAN} Word Segmentation Bakeoff 2010,2010,10,3,2,1,7866,kun wang,{CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,"This paper presents a Chinese Word Segmentation system for the closed track of CIPS-SIGHAN Word Segmentation Bakeoff 2010. This system adopts a character-based joint approach, which combines a character-based generative model and a character-based discriminative model. To further improve the crossdomain performance, we use an additional semi-supervised learning procedure to incorporate the unlabeled corpus. The final performance on the closed track for the simplified-character text shows that our system achieves comparable results with other state-of-the-art systems."
W10-4149,Treebank Conversion based Self-training Strategy for Parsing,2010,16,0,2,1,10548,zhiguo wang,{CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,"In this paper, we propose a novel selftraining strategy for parsing which is based on Treebank conversion (SSPTC). In SSPTC, we make full use of the strong points of Treebank conversion and self-training, and offset their weaknesses with each other. To provide good parse selection strategies which are needed in self-training, we score the automatically generated parse trees with parse trees in source Treebank as a reference. To maintain the constituency between source Treebank and conversion Treebank which is needed in Treebank conversion, we get the conversion trees with the help of self-training. In our experiments, SSPTC strategy is utilized to parse Tsinghua Chinese Treebank with the help of Penn Chinese Treebank. The results significantly outperform the baseline parser."
P10-1065,On Jointly Recognizing and Aligning Bilingual Named Entities,2010,12,16,2,1,6531,yufeng chen,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"We observe that (1) how a given named entity (NE) is translated (i.e., either semantically or phonetically) depends greatly on its associated entity type, and (2) entities within an aligned pair should share the same type. Also, (3) those initially detected NEs are anchors, whose information should be used to give certainty scores when selecting candidates. From this basis, an integrated model is thus proposed in this paper to jointly identify and align bilingual named entities between Chinese and English. It adopts a new mapping type ratio feature (which is the proportion of NE internal tokens that are semantically translated), enforces an entity type consistency constraint, and utilizes additional monolingual candidate certainty factors (based on those NE anchors). The experiments show that this novel approach has substantially raised the type-sensitive F-score of identified NE-pairs from 68.4% to 81.7% (42.1% F-score imperfection reduction) in our Chinese-English NE alignment task."
zhou-etal-2010-casia,{CASIA}-{CASSIL}: a {C}hinese Telephone Conversation Corpus in Real Scenarios with Multi-leveled Annotation,2010,5,2,4,0,45960,keyan zhou,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"CASIA-CASSIL is a large-scale corpus base of Chinese human-human naturally-occurring telephone conversations in restricted domains. The first edition consists of 792 90-second conversations belonging to tourism domain, which are selected from 7,639 spontaneous telephone recordings in real scenarios. The corpus is now being annotated with wide range of linguistic and paralinguistic information in multi-levels. The annotations include Turns, Speaker Gender, Orthographic Transcription, Chinese Syllable, Chinese Phonetic Transcription, Prosodic Boundary, Stress of Sentence, Non-Speech Sounds, Voice Quality, Topic, Dialog-act and Adjacency Pairs, Ill-formedness, and Expressive Emotion as well, 13 levels in total. The abundant annotation will be effective especially for studying Chinese spoken language phenomena. This paper describes the whole process to build the conversation corpus, including collecting and selecting the original data, and the follow-up process such as transcribing, annotating, and so on. CASIA-CASSIL is being extended to a large scale corpus base of annotated Chinese dialogs for spoken Chinese study."
D10-1030,Joint Inference for Bilingual Semantic Role Labeling,2010,26,18,2,1,37388,tao zhuang,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"We show that jointly performing semantic role labeling (SRL) on bitext can improve SRL results on both sides. In our approach, we use monolingual SRL systems to produce argument candidates for predicates in bitext at first. Then, we simultaneously generate SRL results for two sides of bitext using our joint inference model. Our model prefers the bilingual SRL result that is not only reasonable on each side of bitext, but also has more consistent argument structures between two sides. To evaluate the consistency between two argument structures, we also formulate a log-linear model to compute the probability of aligning two arguments. We have experimented with our model on Chinese-English parallel Prop-Bank data. Using our joint inference model, F1 scores of SRL results on Chinese and English text achieve 79.53% and 77.87% respectively, which are 1.52 and 1.74 points higher than the results of baseline monolingual SRL combination systems respectively."
C10-2148,Phrase Structure Parsing with Dependency Structure,2010,32,8,2,1,10548,zhiguo wang,Coling 2010: Posters,0,"In this paper we present a novel phrase structure parsing approach with the help of dependency structure. Different with existing phrase parsers, in our approach the inference procedure is guided by dependency structure, which makes the parsing procedure flexibly. The experimental results show our approach is much more accurate. With the help of golden dependency trees, F1 score of our parser achieves 96.08% on Penn English Treebank and 90.61% on Penn Chinese Treebank. With the help of N-best dependency trees generated by modified MSTParser, F1 score achieves 90.54% for English and 83.93% for Chinese."
C10-2153,Exploring the Use of Word Relation Features for Sentiment Classification,2010,15,46,2,1,8473,rui xia,Coling 2010: Posters,0,"Word relation features, which encode relation information between words, are supposed to be effective features for sentiment classification. However, the use of word relation features suffers from two issues. One is the sparse-data problem and the lack of generalization performance; the other is the limitation of using word relations as additional features to unigrams. To address the two issues, we propose a generalized word relation feature extraction method and an ensemble model to efficiently integrate unigrams and different type of word relation features. Furthermore, aimed at reducing the computation complexity, we propose two fast feature selection methods that are specially designed for word relation features. A range of experiments are conducted to evaluate the effectiveness and efficiency of our approaches."
C10-1051,A Novel Reordering Model Based on Multi-layer Phrase for Statistical Machine Translation,2010,26,4,3,1,18840,yanqing he,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"Phrase reordering is of great importance for statistical machine translation. According to the movement of phrase translation, the pattern of phrase reordering can be divided into three classes: monotone, BTG (Bracket Transduction Grammar) and hierarchy. It is a good way to use different styles of reordering models to reorder different phrases according to the characteristics of both the reordering models and phrases itself. In this paper a novel reordering model based on multi-layer phrase (PRML) is proposed, where the source sentence is segmented into different layers of phrases on which different reordering models are applied to get the final translation. This model has some advantages: different styles of phrase reordering models are easily incorporated together; when a complicated reordering model is employed, it can be limited in a smaller scope and replaced with an easier reordering model in larger scope. So this model better trade-offs the translation speed and performance simultaneously."
C10-1132,A Character-Based Joint Model for {C}hinese Word Segmentation,2010,21,34,2,1,7866,kun wang,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"The character-based tagging approach is a dominant technique for Chinese word segmentation, and both discriminative and generative models can be adopted in that framework. However, generative and discriminative character-based approaches are significantly different and complement each other. A simple joint model combining the character-based generative model and the discriminative one is thus proposed in this paper to take advantage of both approaches. Experiments on the Second SIGHAN Bakeoff show that this joint approach achieves 21% relative error reduction over the discriminative model and 14% over the generative one. In addition, closed tests also show that the proposed joint model outperforms all the existing approaches reported in the literature and achieves the best F-score in four out of five corpora."
C10-1153,A Minimum Error Weighting Combination Strategy for {C}hinese Semantic Role Labeling,2010,18,11,2,1,37388,tao zhuang,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"Many Semantic Role Labeling (SRL) combination strategies have been proposed and tested on English SRL task. But little is known about how much Chinese SRL can benefit from system combination. And existing combination strategies trust each individual system's output with the same confidence when merging them into a pool of candidates. In our approach, we assign different weights to different system outputs, and add a weighted merging stage to the conventional SRL combination architecture. We also propose a method to obtain an appropriate weight for each system's output by minimizing some error function on the development set. We have evaluated our strategy on Chinese Proposition Bank data set. With our minimum error weighting strategy, the F1 score of the combined result achieves 80.45%, which is 1.12% higher than baseline combination method's result, and 4.90% higher than the best individual system's result."
Y09-2016,A Framework for Effectively Integrating Hard and Soft Syntactic Rules into Phrase Based Translation,2009,15,6,2,1,6594,jiajun zhang,"Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Volume 2",0,"In adding syntactic knowledge into phrase-based translation, using hard or soft syntactic rules to reorder the source-language aiming to closely approximate the targetlanguage word order has been successful in improving translation quality. However, it suffers from propagating the pre-reordering errors to the later translation step (decoding). In this paper, we propose a novel framework to integrate hard and soft syntactic rules into phrase-based translation more effectively. For a source sentence to be translated, hard or soft syntactic rules are first acquired from the source parse tree prior to translation, and then instead of reordering the source sentence directly, the rules are used as a strong feature integrated into our elaborately designed model to help phrase reordering in the decoding stage. The experiments on NIST Chinese-to-English translation show that our approach, whether incorporating hard or soft rules, significantly outperforms the previous methods."
Y09-2047,"Which is More Suitable for {C}hinese Word Segmentation, the Generative Model or the Discriminative One?",2009,15,13,2,1,7866,kun wang,"Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Volume 2",0,"Since the traditional word-based n-gram model, a generative approach, cannot handle those out-of-vocabulary (OOV) words in the testing-set, the character-based discriminative approach has been widely adopted recently. However, this discriminative model, though is more robust to OOV words, fails to deliver satisfactory performance for those in-vocabulary (IV) words that have been observed before. Having analyzed the wordbased approach, its capability to handle the dependency between adjacent characters within a word, which is believed that the human adopts for doing segmentation, is found to account for its excellent performance for those IV words. To incorporate the intra-word characters dependency, a character-based approach with a generative model is thus proposed in this paper. The experiments conducted on the second SIGHAN Bakeoffs have shown that the proposed model not only achieves a good balance between those IV words and OOV words, but also outperforms the above-mentioned well-known approaches under the similar conditions."
Y09-1025,Layer-Based Dependency Parsing,2009,23,4,2,0,3629,ping jian,"Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Volume 1",0,"In this paper, a layer-based projective dependency parsing approach is presented. This novel approach works layer by layer from the bottom up. Inside the layer the dependency graphs are searched exhaustively while between the layers the parser state transfers deterministically. Taking the dependency layer as the parsing unit, the proposed parser has a lower computational complexity than graph-based models which search for a whole dependency graph and alleviates the error propagation that transition-based models suffer from to some extent. Furthermore, our parser adopts the sequence labeling models to find the optimal sub-graph of the layer which demonstrates that the sequence labeling techniques are also competent for hierarchical structure analysis tasks. Experimental results indicate that the proposed approach offers desirable accuracies and especially a fast parsing speed."
Y09-1035,Approach to Selecting Best Development Set for Phrase-Based Statistical Machine Translation,2009,12,5,3,0,14523,peng liu,"Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Volume 1",0,"In phrase-based statistical machine translation system, the parameters of model are usually obtained from minimum error rate training (MERT) on the development set. So the development set has a great influence on the performance of the translation system. Generally, more development set will achieve more effective and robust parameters but consume much more time on MERT process. In this paper, we propose two methods to select sentences from the large development set, based on the phrase and the sentence structure respectively. The experimental results show that our methods can get better translation performance than the baseline system on the compact development set by using a state-of-the-art SMT system."
P09-1078,A Framework of Feature Selection Methods for Text Categorization,2009,19,87,3,0.769231,9448,shoushan li,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"In text categorization, feature selection (FS) is a strategy that aims at making text classifiers more efficient and accurate. However, when dealing with a new task, it is still difficult to quickly select a suitable one from various FS methods provided by many previous studies. In this paper, we propose a theoretic framework of FS methods based on two basic measurements: frequency measurement and ratio measurement. Then six popular FS methods are in detail discussed under this framework. Moreover, with the guidance of our theoretical analysis, we propose a novel method called weighed frequency and odds (WFO) that combines the two measurements with trained weights. The experimental results on data sets from both topic-based and sentiment classification tasks show that this new method is robust across different tasks and numbers of selected features."
2009.iwslt-evaluation.13,The {CASIA} statistical machine translation system for {IWSLT} 2009,2009,18,8,4,1,11675,maoxi li,Proceedings of the 6th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper reports on the participation of CASIA (Institute of Automation Chinese Academy of Sciences) at the evaluation campaign of the International Workshop on Spoken Language Translation 2009. We participated in the challenge tasks for Chinese-to-English and English-to-Chinese translation respectively and the BTEC task for Chinese-to-English translation only. For all of the tasks, system performance is improved with some special methods as follows: 1) combining different results of Chinese word segmentation, 2) combining different results of word alignments, 3) adding reliable bilingual words with high probabilities to the training data, 4) handling named entities including person names, location names, organization names, temporal and numerical expressions additionally, 5) combining and selecting translations from the outputs of multiple translation engines, 6) replacing Chinese character with Chinese Pinyin to train the translation model for Chinese-to-English ASR challenge task. This is a new approach that has never been introduced before."
P08-2065,Multi-domain Sentiment Classification,2008,4,60,2,0.769231,9448,shoushan li,"Proceedings of ACL-08: HLT, Short Papers",0,"This paper addresses a new task in sentiment classification, called multi-domain sentiment classification, that aims to improve performance through fusing training data from multiple domains. To achieve this, we propose two approaches of fusion, feature-level and classifier-level, to use training data from multiple domains simultaneously. Experimental studies show that multi-domain sentiment classification using the classifier-level approach performs much better than single domain classification (using the training data individually)."
I08-1017,A New Approach to Automatic Document Summarization,2008,18,1,2,0,35090,xiaofeng wu,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,"In this paper we propose a new approach based on Sequence Segmentation Models (SSM) to the extractive document summarization, in which summarizing is regarded as a segment labeling problem. Comparing with the previous work, the difference of our approach is that the employed features are obtained not only from the sentence level, but also from the segment level. In our approach, the semi-Markov CRF model is employed for segment labeling. The preliminary experiments have shown that the approach does outperform all other traditional supervised and unsupervised approaches to document summarization."
C08-1125,Domain Adaptation for Statistical Machine Translation with Domain Dictionary and Monolingual Corpora,2008,16,78,3,0.151515,2935,hua wu,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"Statistical machine translation systems are usually trained on large amounts of bilingual text and monolingual text. In this paper, we propose a method to perform domain adaptation for statistical machine translation, where in-domain bilingual corpora do not exist. This method first uses out-of-domain corpora to train a baseline system and then uses in-domain translation dictionaries and in-domain monolingual corpora to improve the in-domain performance. We propose an algorithm to combine these different resources in a unified framework. Experimental results indicate that our method achieves absolute improvements of 8.16 and 3.36 BLEU scores on Chinese to English translation and English to French translation respectively, as compared with the baselines using only out-of-domain corpora."
C08-1137,Sentence Type Based Reordering Model for Statistical Machine Translation,2008,13,11,2,1,6594,jiajun zhang,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"Many reordering approaches have been proposed for the statistical machine translation (SMT) system. However, the information about the type of source sentence is ignored in the previous works. In this paper, we propose a group of novel reordering models based on the source sentence type for Chinese-to-English translation. In our approach, an SVM-based classifier is employed to classify the given Chinese sentences into three types: special interrogative sentences, other interrogative sentences, and non-question sentences. The different reordering models are developed oriented to the different sentence types. Our experiments show that the novel reordering models have obtained an improvement of more than 2.65% in BLEU for a phrase-based spoken language translation system."
2008.iwslt-evaluation.12,The {CASIA} statistical machine translation system for {IWSLT} 2008,2008,18,3,7,1,18840,yanqing he,Proceedings of the 5th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper describes our statistical machine translation system (CASIA) used in the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2008. In this year's evaluation, we participated in challenge task for Chinese-English and English-Chinese, BTEC task for Chinese-English. Here, we mainly introduce the overview of our system, the primary modules, the key techniques, and the evaluation results."
2008.amta-papers.10,A Generalized Reordering Model for Phrase-Based Statistical Machine Translation,2008,23,2,2,1,18840,yanqing he,Proceedings of the 8th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"Phrase-based translation models are widely studied in statistical machine translation (SMT). However, the existing phrase-based translation models either can not deal with non-contiguous phrases or reorder phrases only by the rules without an effective reordering model. In this paper, we propose a generalized reordering model (GREM) for phrase-based statistical machine translation, which is not only able to capture the knowledge on the local and global reordering of phrases, but also is able to obtain some capabilities of phrasal generalization by using non-contiguous phrases. The experimental results have indicated that our model out- performs MEBTG (enhanced BTG with a maximum entropy-based reordering model) and HPTM (hierarchical phrase-based translation model) by improvement of 1.54{\%} and 0.66{\%} in BLEU."
2008.amta-govandcom.26,Applications of {MT} during Olympic Games 2008,2008,-1,-1,1,1,6630,chengqing zong,Proceedings of the 8th Conference of the Association for Machine Translation in the Americas: Government and Commercial Uses of MT,0,None
2007.iwslt-1.5,{CASIA} phrase-based {SMT} system for {IWSLT}{'}07,2007,-1,-1,3,1,235,yu zhou,Proceedings of the Fourth International Workshop on Spoken Language Translation,0,None
W06-0112,A Hybrid Approach to {C}hinese Base Noun Phrase Chunking,2006,14,6,2,0,45957,fang xu,Proceedings of the Fifth {SIGHAN} Workshop on {C}hinese Language Processing,0,"In this paper, we propose a hybrid approach to chunking Chinese base noun phrases (base NPs), which combines SVM (Support Vector Machine) model and CRF (Conditional Random Field) model. In order to compare the result respectively from two chunkers, we use the discriminative post-processing method, whose measure criterion is the conditional probability generated from the CRF chunker. With respect to the special structures of Chinese base NP and complete analyses of the first two results, we also customize some appropriate grammar rules to avoid ambiguities and prune errors. According to our overall experiments, the method achieves a higher accuracy in the final results."
2006.iwslt-evaluation.13,{NLPR} translation system for {IWSLT} 2006 evaluation campaign,2006,8,3,7,0,50676,chunguang chai,Proceedings of the Third International Workshop on Spoken Language Translation: Evaluation Campaign,0,"In this paper we describe a hybrid approach to Chinese-toEnglish spoken language translation system used for the IWSLT 2006 evaluation campaign. In this system, the phrasebased statistical machine translation (SMT) engine is combined with the template-based machine translation (TBMT) engine and a simple way is proposed to select the best translation from the results generated by the two translation engines. The experiments prove that the combination can improve the performance of translation system. As the input sentences are speech recognition results and have no punctuation information, we restore the punctuation in source sentences in the processing and postprocessing."
I05-2002,A Hierarchical Parsing Approach with Punctuation Processing for Long {C}hinese Sentences,2005,0,17,2,0,25710,xing li,Companion Volume to the Proceedings of Conference including Posters/Demos and tutorial abstracts,0,None
2005.iwslt-1.14,The {CASIA} Phrase-Based Machine Translation System,2005,0,5,6,0,27672,wei pang,Proceedings of the Second International Workshop on Spoken Language Translation,0,None
fafiotte-etal-2004-collecting,Collecting and Sharing Bilingual Spontaneous Speech Corpora: the {C}hin{F}a{D}ial Experiment,2004,3,0,4,0,47565,georges fafiotte,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"We describe here the three main platforms in the ERIM family of Web-based environments for human interpreting, two of them in more details, ERIM-Interp and ERIM-Collect, then ERIM-Aid. Each platform supports an aspect of the collecting or study of spontaneous bilingual dialogues, translated by an interpreter. ERIM-Interp is the core environment, providing mediated communication between speakers and human interpreters over the network. Using ERIM-Collect, French-Chinese interpreting data have been collected within the 3-year ChinFaDial project supported by LIAMA, a French-Chinese laboratory in Beijing. These raw speech data will be made available in the spring of 2004 on an open-access basis, using the DistribDial server, on a CLIPSGETA website. Our goal is to extend such corpora, on a collaborative scheme, to allow other research groups to contribute to the site whatever annotations they may have created, and to share them under the same conditions (GPL). An ERIM-Aid variant is intended to provide focused machine aids to Web-based human interpreters, or to monolingual distant speakers conversing in different languages."
2004.iwslt-evaluation.12,Multi-engine based {C}hinese-to-{E}nglish translation system,2004,8,6,3,0,52491,yuncun zuo,Proceedings of the First International Workshop on Spoken Language Translation: Evaluation Campaign,0,This paper describes a Multi-Engine based Chinese-toEnglish spoken language translation system. The design and implementation of the system is given in detail. Three different translation engines are employed in the system and a very simple way is proposed to select the best translation from all the outputs generated by them. The evaluation results from IWSLT2004 are reported and analyzed in detail. The results prove that the Multi-Engine based system is practical.
W03-1703,Utterance Segmentation Using Combined Approach Based on Bi-directional N-gram and Maximum Entropy,2003,16,8,2,0,41434,ding liu,Proceedings of the Second {SIGHAN} Workshop on {C}hinese Language Processing,0,"This paper proposes a new approach to segmentation of utterances into sentences using a new linguistic model based upon Maximum-entropy-weighted Bi-directional N-grams. The usual N-gram algorithm searches for sentence boundaries in a text from left to right only. Thus a candidate sentence boundary in the text is evaluated mainly with respect to its left context, without fully considering its right context. Using this approach, utterances are often divided into incomplete sentences or fragments. In order to make use of both the right and left contexts of candidate sentence boundaries, we propose a new linguistic modeling approach based on Maximum-entropy-weighted Bi-directional N-grams. Experimental results indicate that the new approach significantly outperforms the usual N-gram algorithm for segmenting both Chinese and English utterances."
W02-0709,Interactive {C}hinese-to-{E}nglish Speech Translation Based on Dialogue Management,2002,14,6,1,1,6630,chengqing zong,Proceedings of the {ACL}-02 Workshop on Speech-to-Speech Translation: Algorithms and Systems,0,"In this paper, we propose a novel paradigm for the Chinese-to-English speech-to-speech (S2S) translation, which is interactive under the guidance of dialogue management. In this approach, the input utterance is first pre-processed and then serially translated by the template-based translator and the inter-lingua based translator. The dialogue management mechanism (DMM) is employed to supervise the interactive analysis for disambiguation of the input. The interaction is led by the system, so the system always acts on its own initiative in the interactive procedure. In this approach, the complicated semantic analysis is not involved."
C02-2028,{C}hinese Syntactic Parsing Based on Extended {GLR} Parsing Algorithm with {PCFG}*,2002,6,0,3,0,10073,yan zhang,{COLING} 2002: The 17th International Conference on Computational Linguistics: Project Notes,0,This paper presents an extended GLR parsing algorithm with grammar PCFG* that is based on Tomita's GLR parsing algorithm and extends it further. We also define a new grammar---PCFG* that is based on PCFG and assigns not only probability but also frequency associated with each rule. So our syntactic parsing system is implemented based on rule-based approach and statistics approach. Furthermore our experiments are executed in two fields: Chinese base noun phrase identification and full syntactic parsing. And the results of these two fields are compared from three ways. The experiments prove that the extended GLR parsing algorithm with PCFG* is an efficient parsing method and a straightforward way to combine statistical property with rules. The experiment results of these two fields are presented in this paper.
C00-2174,{C}hinese Generation in a Spoken Dialogue Translation System,2000,4,3,3,0,2935,hua wu,{COLING} 2000 Volume 2: The 18th International Conference on Computational Linguistics,0,"A Chinese generation module in a speech to speech dialogue translation system is presented here. The input of the generation module is the underspecified semantic representation. Its design is strongly influenced by the underspecification of the inputs and the necessity of real-time and robust processing. We design an efficient generation system comprising a task-oriented microplanner and a general surface realization module for Chinese. The microplanner performs the lexical and syntactic choice and makes inferences from the input and domain knowledge. The output of the microplanner is fully instantiated. This enables the surface realizer to traverse the input in a top-down, depth-first fashion, which in turn speeds the whole generation procedure. The surface realizer also combines the template method and deep generation technology in the same formalism. Preliminary results are also presented in this paper."
