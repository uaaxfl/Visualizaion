2005.sigdial-1.10,N04-1026,1,0.795377,"Why2-Atlas conceptual physics tutoring system (VanLehn et al., 2002).2 Our long term goal is to have this system detect and adapt to student affect, and to investigate whether such an affective version of our system improves learning and other measures of performance. To date we have collected corpora of both human and computer tutoring dialogues, and have demonstrated the feasibility of annotating and recognizing student emotions from lexical, acoustic-prosodic, and dialogue features automatically extractable from these corpora (Litman and Forbes-Riley, 2004a; Litman and Forbes-Riley, 2004b; Forbes-Riley and Litman, 2004). Here, we assume viable emotion recognition and move on to the next step: providing an empirical basis for enhancing our computer tutor to adaptively respond to student affect. We first show how to apply n-gram techniques used in other areas of computational linguistics to mine human-human dialogue corpora for dependent bigrams of student states and tutor responses. We then use our bigram analysis to show: 1) statistically-significant dependencies exist between students’ emotional states and our hu1 We use the terms “affect” and “emotion” loosely to cover emotions and attitudes believed to be"
2005.sigdial-1.10,P03-1031,0,0.0200931,"- HINT neutral - HINT CERT - EXP neutral - EXP uncertain - EXP Obs 1102 402 477 289 199 308 103 97 779 64 383 998 651 109 Exp 1102 247.23 620.08 289 162.62 308 173.31 51.07 779 36.73 438.34 998 561.57 165.49 χ2 169.18 160.96 97.29 20.15 19.77 82.52 69.58 52.82 37.07 25.25 18.98 47.08 40.86 29.00 Table 5: Observed, Expected, and χ2 for Dependent “Certainness” - “State Act” Bigrams (p&lt;.001) 4 Related Work While there have been other approaches to using dialogue n-grams (e.g. (Stolcke et al., 2000; Reithinger et al., 1996)), such n-grams have typically consisted of only dialogue acts, although (Higashinaka et al., 2003) propose computing bigrams of dialogue state and following dialogue act. Moreover, these methods have been used to compute n-gram probabilities for implementing statistical components. We propose a new use of these methods: to mine corpora for only the significant n-grams, for use in designing strategies for adapting to student affect in a computational system. Previous Ngram Statistics Package (NSP) applications have focused on extracting significant word n-grams (Banerjee and Pedersen, 2003), while our “dialogue” bigrams are constructed from multiple turn-level annotations of student certain"
2005.sigdial-1.10,W04-2326,1,0.914664,"lliman, 2004), which is speech-enabled version of the textbased Why2-Atlas conceptual physics tutoring system (VanLehn et al., 2002).2 Our long term goal is to have this system detect and adapt to student affect, and to investigate whether such an affective version of our system improves learning and other measures of performance. To date we have collected corpora of both human and computer tutoring dialogues, and have demonstrated the feasibility of annotating and recognizing student emotions from lexical, acoustic-prosodic, and dialogue features automatically extractable from these corpora (Litman and Forbes-Riley, 2004a; Litman and Forbes-Riley, 2004b; Forbes-Riley and Litman, 2004). Here, we assume viable emotion recognition and move on to the next step: providing an empirical basis for enhancing our computer tutor to adaptively respond to student affect. We first show how to apply n-gram techniques used in other areas of computational linguistics to mine human-human dialogue corpora for dependent bigrams of student states and tutor responses. We then use our bigram analysis to show: 1) statistically-significant dependencies exist between students’ emotional states and our hu1 We use the terms “affect” and"
2005.sigdial-1.10,P04-1045,1,0.749557,"lliman, 2004), which is speech-enabled version of the textbased Why2-Atlas conceptual physics tutoring system (VanLehn et al., 2002).2 Our long term goal is to have this system detect and adapt to student affect, and to investigate whether such an affective version of our system improves learning and other measures of performance. To date we have collected corpora of both human and computer tutoring dialogues, and have demonstrated the feasibility of annotating and recognizing student emotions from lexical, acoustic-prosodic, and dialogue features automatically extractable from these corpora (Litman and Forbes-Riley, 2004a; Litman and Forbes-Riley, 2004b; Forbes-Riley and Litman, 2004). Here, we assume viable emotion recognition and move on to the next step: providing an empirical basis for enhancing our computer tutor to adaptively respond to student affect. We first show how to apply n-gram techniques used in other areas of computational linguistics to mine human-human dialogue corpora for dependent bigrams of student states and tutor responses. We then use our bigram analysis to show: 1) statistically-significant dependencies exist between students’ emotional states and our hu1 We use the terms “affect” and"
2005.sigdial-1.10,N04-3002,1,0.873614,"att et al., 2004; Johnson et al., 2004; Moore et al., 2004). However, while it seems intuitively plausible that human tutors do in fact vary their responses based on the detection of student affect1 , to date this belief has largely been theoretically rather than empirically motivated. We propose using bigram-based techniques as a datadriven method for identifying relationships between student affect and tutor responses in a corpus of human-human spoken tutoring dialogues. To investigate affect and tutorial dialogue systems, we have built ITSPOKE (Intelligent Tutoring SPOKEn dialogue system) (Litman and Silliman, 2004), which is speech-enabled version of the textbased Why2-Atlas conceptual physics tutoring system (VanLehn et al., 2002).2 Our long term goal is to have this system detect and adapt to student affect, and to investigate whether such an affective version of our system improves learning and other measures of performance. To date we have collected corpora of both human and computer tutoring dialogues, and have demonstrated the feasibility of annotating and recognizing student emotions from lexical, acoustic-prosodic, and dialogue features automatically extractable from these corpora (Litman and Fo"
2005.sigdial-1.10,J00-3003,0,0.380767,"Missing"
2007.sigdial-1.23,P96-1009,0,0.0102055,"or collecting dialog corpora using subjects. In Section 3, we introduce the Let’s Go spoken dialog system, which we use to collect both our subject and user corpora. In Section 4, we describe the specific in-lab experiment we conducted with recruited sub125 jects. We then introduce the evaluation measures used for our corpora comparisons in Section 5, followed by a presentation of our results in Section 6. Finally, we further discuss and summarize our results in Section 7. 2 Literature Review In this section we survey a set of spoken dialog papers involving human subject experiments (namely, (Allen et al., 1996), (Batliner et al., 2003), (Bohus and Rudnicky, 2006), (Giorgino et al., 2004), (Gruenstein et al., 2006), (Hof et al., 2006), (Lemon et al., 2006), (Litman and Pan, 2002), (M¨oller et al., 2006), (Rieser et al., 2005), (Roque et al., 2006), (Singh et al., 2000), (Tomko and Rosenfeld, 2006), (Walker et al., 2001), (Walker et al., 2000)), in order to define a “standard” laboratory setting for use in our own experiments with subjects. We survey the literature from four perspectives: subject recruitment, experimental environment, task design, and experimental policies. Subject Recruitment. Recrui"
2007.sigdial-1.23,W07-0305,1,0.107624,"Missing"
2007.sigdial-1.23,W06-1301,0,0.0236486,"ect both our subject and user corpora. In Section 4, we describe the specific in-lab experiment we conducted with recruited sub125 jects. We then introduce the evaluation measures used for our corpora comparisons in Section 5, followed by a presentation of our results in Section 6. Finally, we further discuss and summarize our results in Section 7. 2 Literature Review In this section we survey a set of spoken dialog papers involving human subject experiments (namely, (Allen et al., 1996), (Batliner et al., 2003), (Bohus and Rudnicky, 2006), (Giorgino et al., 2004), (Gruenstein et al., 2006), (Hof et al., 2006), (Lemon et al., 2006), (Litman and Pan, 2002), (M¨oller et al., 2006), (Rieser et al., 2005), (Roque et al., 2006), (Singh et al., 2000), (Tomko and Rosenfeld, 2006), (Walker et al., 2001), (Walker et al., 2000)), in order to define a “standard” laboratory setting for use in our own experiments with subjects. We survey the literature from four perspectives: subject recruitment, experimental environment, task design, and experimental policies. Subject Recruitment. Recruiting subjects involves deciding who to recruit, where to recruit, and how many subjects to recruit. In the studies we surveye"
2007.sigdial-1.23,2005.sigdial-1.11,0,0.0117317,"iment we conducted with recruited sub125 jects. We then introduce the evaluation measures used for our corpora comparisons in Section 5, followed by a presentation of our results in Section 6. Finally, we further discuss and summarize our results in Section 7. 2 Literature Review In this section we survey a set of spoken dialog papers involving human subject experiments (namely, (Allen et al., 1996), (Batliner et al., 2003), (Bohus and Rudnicky, 2006), (Giorgino et al., 2004), (Gruenstein et al., 2006), (Hof et al., 2006), (Lemon et al., 2006), (Litman and Pan, 2002), (M¨oller et al., 2006), (Rieser et al., 2005), (Roque et al., 2006), (Singh et al., 2000), (Tomko and Rosenfeld, 2006), (Walker et al., 2001), (Walker et al., 2000)), in order to define a “standard” laboratory setting for use in our own experiments with subjects. We survey the literature from four perspectives: subject recruitment, experimental environment, task design, and experimental policies. Subject Recruitment. Recruiting subjects involves deciding who to recruit, where to recruit, and how many subjects to recruit. In the studies we surveyed, the number of subjects recruited for each experiment ranged from 10 to 72. Most of the stu"
2007.sigdial-1.23,2005.sigdial-1.6,0,0.238946,"es Institute, Carnegie Mellon University, Pittsburgh PA, 15213, USA 3 Computer Science Department, Carnegie Mellon University, Pittsburgh PA, 15213, USA 4 Dept. of Computer Science & LRDC, University of Pittsburgh, Pittsburgh, PA 15260, USA hua@cs.pitt.edu, {antoine,dbohus,max}@cs.cmu.edu, litman@cs.pitt.edu 1 Abstract interactions with early system prototypes, are often used to better design system functionalities. Once obtained, such corpora are often then used in machine learning approaches to tasks such as dialog strategy optimization (e.g. (Lemon et al., 2006)), or user simulation (e.g. (Schatzmann et al., 2005)). During system evaluation, user satisfaction surveys are often carried out with humans after interacting with a system (Hone and Graham, 2000); given a dialog corpus obtained from such interactions, evaluation frameworks such as PARADISE (Walker et al., 2000) can then be used to predict user satisfaction from measures that can be directly computed from the corpus. Empirical spoken dialog research often involves the collection and analysis of a dialog corpus. However, it is not well understood whether and how a corpus of dialogs collected using recruited subjects differs from a corpus of dial"
2020.acl-main.759,P16-1068,0,0.0861671,"Cs) representing evidence from a source text using the intermediate output of attention layers. We evaluate performance using a feature-based AES requiring TCs. Results show that performance is comparable whether using automatically or manually constructed TCs for 1) representing essays as rubric-based features, 2) grading essays. 1 Introduction Automated essay scoring (AES) systems reliably grade essays at scale, while automated writing evaluation (AWE) systems additionally provide formative feedback to guide revision. Although neural networks currently generate state-of-the-art AES results (Alikaniotis et al., 2016; Taghipour and Ng, 2016; Dong et al., 2017; Farag et al., 2018; Jin et al., 2018; Li et al., 2018; Tay et al., 2018; Zhang and Litman, 2018), non-neural AES create feature representations more easily useable by AWE (Roscoe et al., 2014; Foltz and Rosenstein, 2015; Crossley and McNamara, 2016; Woods et al., 2017; Madnani et al., 2018; Zhang et al., 2019). We believe that neural AES can also provide useful information for creating feature representations, e.g., by exploiting information in the intermediate layers. Our work focuses on a particular source-based essay writing task called the respo"
2020.bea-1.7,D13-1055,0,0.0212864,"oviding formative feedback to guide students’ essay revision. Such systems are only effective if students attend to the feedback provided and revise their essays in ways aligned with such feedback. To date, few AWE systems assess (and are assessed on) the extent to which students’ revisions respond to the feedback provided and thus improve the essay in suggested ways. Moreover, we know little about what students do when they do not revise in expected ways. For example, most natural language processing (NLP) work on writing revision focuses only on annotating and classifying revision purposes (Daxenberger and Gurevych, 2013; Zhang et al., 2017), rather than on assessing the quality of a revision in achieving its purpose. A few 2 Related Work Automated revision detection work has centered on classifying edits on largely non-content level features of writing, such as spelling and morphosyntactic revisions (Max and Wisniewski, 2010), error correction, paraphrase or vandalism detection (Daxenberger and Gurevych, 2013), factual versus fluency edits (Bronner and Monz, 2012), and document- versus word-level revisions (Roscoe et al., 2015). Other research has focused on patterns of revision behavior, for example, the ad"
2020.bea-1.7,P17-1144,1,0.917147,"ide students’ essay revision. Such systems are only effective if students attend to the feedback provided and revise their essays in ways aligned with such feedback. To date, few AWE systems assess (and are assessed on) the extent to which students’ revisions respond to the feedback provided and thus improve the essay in suggested ways. Moreover, we know little about what students do when they do not revise in expected ways. For example, most natural language processing (NLP) work on writing revision focuses only on annotating and classifying revision purposes (Daxenberger and Gurevych, 2013; Zhang et al., 2017), rather than on assessing the quality of a revision in achieving its purpose. A few 2 Related Work Automated revision detection work has centered on classifying edits on largely non-content level features of writing, such as spelling and morphosyntactic revisions (Max and Wisniewski, 2010), error correction, paraphrase or vandalism detection (Daxenberger and Gurevych, 2013), factual versus fluency edits (Bronner and Monz, 2012), and document- versus word-level revisions (Roscoe et al., 2015). Other research has focused on patterns of revision behavior, for example, the addition, deletion, sub"
2020.bea-1.7,W15-0616,1,0.766013,"use 3 Explain the evidence 4 Explain how the evidence connects to the main idea & elaborate information (Zhang, 2020). However, these categories center on general writing features and behaviors. In the context of AWE systems, this could be seen as a limitation because feedback is most useful to students and teachers alike when it is keyed to critical features of a genre – such as claims, reasons, and evidence use in argumentative writing – that are most challenging to teach and learn. Some research has begun to take up the challenge of investigating student revision for argumentative writing (Zhang and Litman, 2015; Zhang et al., 2017). Results show a high level of agreement for human annotation and some relationship to essay improvement, though not at the level of individual argument elements (Zhang and Litman, 2015). Existing schemes also lack in specificity, e.g., they do not distinguish between desirable and undesirable revisions for each argument element in terms of improving essay quality. Prior work on assessing revision quality has evaluated revision in general terms (e.g., strength (Tan and Lee, 2014) or overall improvement (Afrin and Litman, 2018)), but without consideration of the feedback st"
2020.bea-1.7,max-wisniewski-2010-mining,0,0.0230709,"rovided and thus improve the essay in suggested ways. Moreover, we know little about what students do when they do not revise in expected ways. For example, most natural language processing (NLP) work on writing revision focuses only on annotating and classifying revision purposes (Daxenberger and Gurevych, 2013; Zhang et al., 2017), rather than on assessing the quality of a revision in achieving its purpose. A few 2 Related Work Automated revision detection work has centered on classifying edits on largely non-content level features of writing, such as spelling and morphosyntactic revisions (Max and Wisniewski, 2010), error correction, paraphrase or vandalism detection (Daxenberger and Gurevych, 2013), factual versus fluency edits (Bronner and Monz, 2012), and document- versus word-level revisions (Roscoe et al., 2015). Other research has focused on patterns of revision behavior, for example, the addition, deletion, substitution, and reorganization of 75 th Proceedings of the 15 Workshop on Innovative Use of NLP for Building Educational Applications, pages 75–84 c July 10, 2020. 2020 Association for Computational Linguistics No Feedback Message 1 Use more evidence from the article 2 Provide more details f"
2020.bea-1.7,D14-1162,0,0.103803,"Missing"
2020.coling-demos.10,W16-3623,0,0.0606668,"Missing"
2020.coling-demos.10,N19-1423,0,0.00977519,"Missing"
2020.coling-demos.10,P16-2089,0,0.0222769,"entation: argument moves, specificity and collaboration. Discussion Tracker includes visualizations, interactive coded transcripts, collaboration maps, analytics across discussions, and instructional planning. In contrast to teacher dashboards which largely focus on discussion analytics such as amount of student/teacher talk, teacher wait time, and teacher question type (Chen et al., 2014; Gerritsen et al., 2018; Pehmer et al., 2015; Blanchard et al., 2016), DT focuses on students’ collaborative argumentation. In contrast to related NLP algorithms which largely focus on coding student essays (Ghosh et al., 2016; Klebanov et al., 2016; Nguyen and Litman, 2016), asynchronous online discussions (Swanson et al., 2015), and news articles (Li and Nenkova, 2015), DT’s NLP algorithms address the challenges of coding transcripts of synchronous, face-to-face classroom discussions. 2 Description of Discussion Tracker (DT) To use DT, a teacher first uploads a classroom discussion transcript. Next, NLP classifiers code the transcript using a previously developed scheme for representing three important dimensions of collaborative argumentation (Lugini et al., 2018; Olshefski et al., 2020): argument moves (claim,"
2020.coling-demos.10,W16-2808,0,0.0264568,"oves, specificity and collaboration. Discussion Tracker includes visualizations, interactive coded transcripts, collaboration maps, analytics across discussions, and instructional planning. In contrast to teacher dashboards which largely focus on discussion analytics such as amount of student/teacher talk, teacher wait time, and teacher question type (Chen et al., 2014; Gerritsen et al., 2018; Pehmer et al., 2015; Blanchard et al., 2016), DT focuses on students’ collaborative argumentation. In contrast to related NLP algorithms which largely focus on coding student essays (Ghosh et al., 2016; Klebanov et al., 2016; Nguyen and Litman, 2016), asynchronous online discussions (Swanson et al., 2015), and news articles (Li and Nenkova, 2015), DT’s NLP algorithms address the challenges of coding transcripts of synchronous, face-to-face classroom discussions. 2 Description of Discussion Tracker (DT) To use DT, a teacher first uploads a classroom discussion transcript. Next, NLP classifiers code the transcript using a previously developed scheme for representing three important dimensions of collaborative argumentation (Lugini et al., 2018; Olshefski et al., 2020): argument moves (claim, evidence, explanation),"
2020.coling-demos.10,W18-5208,1,0.900481,"Missing"
2020.coling-demos.10,2020.coling-main.128,1,0.772077,"Missing"
2020.coling-demos.10,W18-0511,1,0.840685,"thms which largely focus on coding student essays (Ghosh et al., 2016; Klebanov et al., 2016; Nguyen and Litman, 2016), asynchronous online discussions (Swanson et al., 2015), and news articles (Li and Nenkova, 2015), DT’s NLP algorithms address the challenges of coding transcripts of synchronous, face-to-face classroom discussions. 2 Description of Discussion Tracker (DT) To use DT, a teacher first uploads a classroom discussion transcript. Next, NLP classifiers code the transcript using a previously developed scheme for representing three important dimensions of collaborative argumentation (Lugini et al., 2018; Olshefski et al., 2020): argument moves (claim, evidence, explanation), specificity (low, medium, high), and collaboration (new, agree, extension, challenge/probe). Student turns are the unit of analysis for collaboration. Argumentative Discourse Units (ADUs) — either entire turns, or segments within turns — are the argumentation and specificity units of analysis. Each NLP classifier in DT was developed by training on a previously collected and freely available corpus1 of collaborative argumentation (Olshefski et al., 2020) using transformer-based neural networks. This work is licensed under"
2020.coling-demos.10,P16-1107,1,0.829152,"ollaboration. Discussion Tracker includes visualizations, interactive coded transcripts, collaboration maps, analytics across discussions, and instructional planning. In contrast to teacher dashboards which largely focus on discussion analytics such as amount of student/teacher talk, teacher wait time, and teacher question type (Chen et al., 2014; Gerritsen et al., 2018; Pehmer et al., 2015; Blanchard et al., 2016), DT focuses on students’ collaborative argumentation. In contrast to related NLP algorithms which largely focus on coding student essays (Ghosh et al., 2016; Klebanov et al., 2016; Nguyen and Litman, 2016), asynchronous online discussions (Swanson et al., 2015), and news articles (Li and Nenkova, 2015), DT’s NLP algorithms address the challenges of coding transcripts of synchronous, face-to-face classroom discussions. 2 Description of Discussion Tracker (DT) To use DT, a teacher first uploads a classroom discussion transcript. Next, NLP classifiers code the transcript using a previously developed scheme for representing three important dimensions of collaborative argumentation (Lugini et al., 2018; Olshefski et al., 2020): argument moves (claim, evidence, explanation), specificity (low, medium,"
2020.coling-demos.10,2020.lrec-1.130,1,0.831283,"cus on coding student essays (Ghosh et al., 2016; Klebanov et al., 2016; Nguyen and Litman, 2016), asynchronous online discussions (Swanson et al., 2015), and news articles (Li and Nenkova, 2015), DT’s NLP algorithms address the challenges of coding transcripts of synchronous, face-to-face classroom discussions. 2 Description of Discussion Tracker (DT) To use DT, a teacher first uploads a classroom discussion transcript. Next, NLP classifiers code the transcript using a previously developed scheme for representing three important dimensions of collaborative argumentation (Lugini et al., 2018; Olshefski et al., 2020): argument moves (claim, evidence, explanation), specificity (low, medium, high), and collaboration (new, agree, extension, challenge/probe). Student turns are the unit of analysis for collaboration. Argumentative Discourse Units (ADUs) — either entire turns, or segments within turns — are the argumentation and specificity units of analysis. Each NLP classifier in DT was developed by training on a previously collected and freely available corpus1 of collaborative argumentation (Olshefski et al., 2020) using transformer-based neural networks. This work is licensed under a Creative Commons Attri"
2020.coling-demos.10,W15-4631,0,0.0272994,"nteractive coded transcripts, collaboration maps, analytics across discussions, and instructional planning. In contrast to teacher dashboards which largely focus on discussion analytics such as amount of student/teacher talk, teacher wait time, and teacher question type (Chen et al., 2014; Gerritsen et al., 2018; Pehmer et al., 2015; Blanchard et al., 2016), DT focuses on students’ collaborative argumentation. In contrast to related NLP algorithms which largely focus on coding student essays (Ghosh et al., 2016; Klebanov et al., 2016; Nguyen and Litman, 2016), asynchronous online discussions (Swanson et al., 2015), and news articles (Li and Nenkova, 2015), DT’s NLP algorithms address the challenges of coding transcripts of synchronous, face-to-face classroom discussions. 2 Description of Discussion Tracker (DT) To use DT, a teacher first uploads a classroom discussion transcript. Next, NLP classifiers code the transcript using a previously developed scheme for representing three important dimensions of collaborative argumentation (Lugini et al., 2018; Olshefski et al., 2020): argument moves (claim, evidence, explanation), specificity (low, medium, high), and collaboration (new, agree, extension, challe"
2020.coling-main.128,W17-5112,0,0.0193405,"1 in Table 1 is labeled claim since speaker 7 provides their personal view, while row 2 is labeled evidence because it references facts from a text. While “context” has been used in the argument mining literature to refer to several phenomena, we consider context to be auxiliary textual information outside the span of an ADU. It is generally accepted that context is important in argument mining. Stab and Gurevych (2014) as well as Nguyen and Litman (2016) use context features extracted from the sentence containing an ADU to improve ACC. Persing and Ng (2016), Habernal and Gurevych (2017) and Aker et al. (2017) similarly use contextual features in joint ACI/ACC models. Optiz and Frank (2019) analyze a previous argument mining system and find that, for its predictions, it relies on context more than it does on ADU content. Chakrabarty et al. (2019) indirectly model context in ACC by fine-tuning a BERT model to predict the next sentence (i.e. the context). Eger et al. (2017) analyzed several neural models for jointly performing ACI, ACC and argument relation extraction. All of these works share several limitations: (i) the context is either limited to a single configuration (e.g. one sentence before/a"
2020.coling-main.128,D19-1291,0,0.0753531,"phenomena, we consider context to be auxiliary textual information outside the span of an ADU. It is generally accepted that context is important in argument mining. Stab and Gurevych (2014) as well as Nguyen and Litman (2016) use context features extracted from the sentence containing an ADU to improve ACC. Persing and Ng (2016), Habernal and Gurevych (2017) and Aker et al. (2017) similarly use contextual features in joint ACI/ACC models. Optiz and Frank (2019) analyze a previous argument mining system and find that, for its predictions, it relies on context more than it does on ADU content. Chakrabarty et al. (2019) indirectly model context in ACC by fine-tuning a BERT model to predict the next sentence (i.e. the context). Eger et al. (2017) analyzed several neural models for jointly performing ACI, ACC and argument relation extraction. All of these works share several limitations: (i) the context is either limited to a single configuration (e.g. one sentence before/after the ADU) or optimized along a single dimension (typically size but not position); (ii) only a subset of the features extracted for the target ADU are also extracted for context; (iii) context is typically based on ADU adjacency, althoug"
2020.coling-main.128,N19-1423,0,0.0171878,"Missing"
2020.coling-main.128,P17-1002,0,0.0216058,"important in argument mining. Stab and Gurevych (2014) as well as Nguyen and Litman (2016) use context features extracted from the sentence containing an ADU to improve ACC. Persing and Ng (2016), Habernal and Gurevych (2017) and Aker et al. (2017) similarly use contextual features in joint ACI/ACC models. Optiz and Frank (2019) analyze a previous argument mining system and find that, for its predictions, it relies on context more than it does on ADU content. Chakrabarty et al. (2019) indirectly model context in ACC by fine-tuning a BERT model to predict the next sentence (i.e. the context). Eger et al. (2017) analyzed several neural models for jointly performing ACI, ACC and argument relation extraction. All of these works share several limitations: (i) the context is either limited to a single configuration (e.g. one sentence before/after the ADU) or optimized along a single dimension (typically size but not position); (ii) only a subset of the features extracted for the target ADU are also extracted for context; (iii) context is typically based on ADU adjacency, although other ways of building context (e.g. based on speakers in multi-party dialogues) are possible. This work is licensed under a C"
2020.coling-main.128,J17-1004,0,0.0610705,", evidence, etc. For example, row 1 in Table 1 is labeled claim since speaker 7 provides their personal view, while row 2 is labeled evidence because it references facts from a text. While “context” has been used in the argument mining literature to refer to several phenomena, we consider context to be auxiliary textual information outside the span of an ADU. It is generally accepted that context is important in argument mining. Stab and Gurevych (2014) as well as Nguyen and Litman (2016) use context features extracted from the sentence containing an ADU to improve ACC. Persing and Ng (2016), Habernal and Gurevych (2017) and Aker et al. (2017) similarly use contextual features in joint ACI/ACC models. Optiz and Frank (2019) analyze a previous argument mining system and find that, for its predictions, it relies on context more than it does on ADU content. Chakrabarty et al. (2019) indirectly model context in ACC by fine-tuning a BERT model to predict the next sentence (i.e. the context). Eger et al. (2017) analyzed several neural models for jointly performing ACI, ACC and argument relation extraction. All of these works share several limitations: (i) the context is either limited to a single configuration (e.g"
2020.coling-main.128,W18-5208,1,0.706392,"Missing"
2020.coling-main.128,D15-1166,0,0.0849895,"Missing"
2020.coling-main.128,2020.lrec-1.130,1,0.692831,"Missing"
2020.coling-main.128,W19-4503,0,0.0350719,"Missing"
2020.coling-main.128,N16-1164,0,0.0194511,"ment model, e.g. claims, evidence, etc. For example, row 1 in Table 1 is labeled claim since speaker 7 provides their personal view, while row 2 is labeled evidence because it references facts from a text. While “context” has been used in the argument mining literature to refer to several phenomena, we consider context to be auxiliary textual information outside the span of an ADU. It is generally accepted that context is important in argument mining. Stab and Gurevych (2014) as well as Nguyen and Litman (2016) use context features extracted from the sentence containing an ADU to improve ACC. Persing and Ng (2016), Habernal and Gurevych (2017) and Aker et al. (2017) similarly use contextual features in joint ACI/ACC models. Optiz and Frank (2019) analyze a previous argument mining system and find that, for its predictions, it relies on context more than it does on ADU content. Chakrabarty et al. (2019) indirectly model context in ACC by fine-tuning a BERT model to predict the next sentence (i.e. the context). Eger et al. (2017) analyzed several neural models for jointly performing ACI, ACC and argument relation extraction. All of these works share several limitations: (i) the context is either limited"
2020.coling-main.128,C14-1142,0,0.0307812,"ent identification (ACI). The next task, argument component classification (ACC), consists of assigning a label to each ADU according to an argument model, e.g. claims, evidence, etc. For example, row 1 in Table 1 is labeled claim since speaker 7 provides their personal view, while row 2 is labeled evidence because it references facts from a text. While “context” has been used in the argument mining literature to refer to several phenomena, we consider context to be auxiliary textual information outside the span of an ADU. It is generally accepted that context is important in argument mining. Stab and Gurevych (2014) as well as Nguyen and Litman (2016) use context features extracted from the sentence containing an ADU to improve ACC. Persing and Ng (2016), Habernal and Gurevych (2017) and Aker et al. (2017) similarly use contextual features in joint ACI/ACC models. Optiz and Frank (2019) analyze a previous argument mining system and find that, for its predictions, it relies on context more than it does on ADU content. Chakrabarty et al. (2019) indirectly model context in ACC by fine-tuning a BERT model to predict the next sentence (i.e. the context). Eger et al. (2017) analyzed several neural models for j"
2020.coling-main.128,J09-3003,0,0.206136,"Missing"
2020.lrec-1.130,W14-2109,0,0.0656748,"Missing"
2020.lrec-1.130,P18-1237,0,0.136327,"statistics on the corpus, we provide code and benchmark performance figures for predicting each of the three annotated dimensions, illustrating the challenging nature of this corpus. We then illustrate the benefits the corpus has already afforded NLP algorithms for improving argument mining with multi-task learning and discuss other potential uses of the corpus for further NLP research. 2. Related Work The development of argument mining tasks has been heavily informed by corpora of written texts extracted mostly from the web (e.g., online newspapers, Wikipedia, blog posts, user comments) (Al Khatib et al., 2018; Biran and Rambow, 2011b; Aharoni et al., 2014; Habernal et al., 2014; Park and Cardie, 2018; Swanson et al., 2015; Cabrio ˇ and Villata, 2014; Boltuˇzi´c and Snajder, 2014), from student essays (Stab and Gurevych, 2014; Stab and Gurevych, 2017a; Carlile et al., 2018), from legal documents (parliamentary records and court reports) (Mochales and Moens, 2011; Ashley and Walker, 2013), or from speeches (Mirkin et al., 2018; Lippi and Torroni, 2016). Similarly, NLP developments in specificity have typically drawn on newspaper articles (Louis and Nenkova, 2011; Li and Nenkova, 2015; Li et al., 201"
2020.lrec-1.130,W14-2107,0,0.0832889,"Missing"
2020.lrec-1.130,P18-1058,0,0.267098,"e statistics on the corpus, we provide performance benchmarks and associated code for predicting each dimension separately, illustrate the use of the multiple annotations in the corpus to improve performance via multi-task learning, and finally discuss other ways the corpus might be used to further NLP research. Keywords: Corpus, Discourse, Text mining 1. Introduction Natural Language Processing (NLP) research has advanced considerably in recent years, developing reliable predictors for argumentation (Mirkin et al., 2018; Lippi and Torroni, 2016; Aharoni et al., 2014; Biran and Rambow, 2011a; Carlile et al., 2018; Habernal et al., 2014; Park and Cardie, 2018; Stab and Gurevych, 2014; Stab and Gurevych, 2017a), specificity (Li and Nenkova, 2015; Li et al., 2016; Louis and Nenkova, 2012; Gao et al., 2019), and collaboration (Richey et al., 2016). The majority of corpora informing these developments are made up of written texts (e.g., documents, asynchronous online discussions) extracted from the web and often written by individuals. Additionally, annotations in these corpora typically focus on one linguistic dimension at a time (e.g., specificity or dimensions of argumentation). As such, few published c"
2020.lrec-1.130,D17-1218,0,0.019572,"corpus allows the research community to further analyze inter-dependencies between argumentation, specificity and collaboration, and develop more effective models to take advantage of these dependencies. 3 Recall that while collaboration was annotated at the turn level, the joint model uses the BIO converted (ADU) representation. We thus do not investigate whether the joint ADU model improves the individual turn-level collaboration prediction. 1040 6.3. 9. Other Potential Corpus Uses Going beyond classification, our corpus can also be used in conjunction with other publicly available corpora. Daxenberger et al. (2017) for example performed a qualitative analysis to understand the difference in conceptualization of claims across multiple datasets. None of the datasets analyzed, however, includes transcripts of spoken dialogues. The Discussion Tracker corpus can be used in a similar way, for example, to study the different conceptualization of argument components between spoken multi-party discussions and online multi-party dialogues or written essays. Additionally, the corpus could also support educational research, which has taken interest in classroom discourse since the 1970’s (Howe and Abedin, 2013; Mer"
2020.lrec-1.130,W14-2106,0,0.0823928,"Missing"
2020.lrec-1.130,J17-1004,0,0.13096,"Missing"
2020.lrec-1.130,L16-1620,0,0.0474398,"Missing"
2020.lrec-1.130,louis-nenkova-2012-corpus,0,0.188055,"n the corpus to improve performance via multi-task learning, and finally discuss other ways the corpus might be used to further NLP research. Keywords: Corpus, Discourse, Text mining 1. Introduction Natural Language Processing (NLP) research has advanced considerably in recent years, developing reliable predictors for argumentation (Mirkin et al., 2018; Lippi and Torroni, 2016; Aharoni et al., 2014; Biran and Rambow, 2011a; Carlile et al., 2018; Habernal et al., 2014; Park and Cardie, 2018; Stab and Gurevych, 2014; Stab and Gurevych, 2017a), specificity (Li and Nenkova, 2015; Li et al., 2016; Louis and Nenkova, 2012; Gao et al., 2019), and collaboration (Richey et al., 2016). The majority of corpora informing these developments are made up of written texts (e.g., documents, asynchronous online discussions) extracted from the web and often written by individuals. Additionally, annotations in these corpora typically focus on one linguistic dimension at a time (e.g., specificity or dimensions of argumentation). As such, few published corpora both (1) focus on synchronous multi-party argumentation and (2) include multiple simultaneous annotations. To address the lack of multi-party synchronous argumentation"
2020.lrec-1.130,W17-5006,1,0.849807,"laims than evidence, and almost twice as many explanations as evidence, prompting us to use the weights (claims: 1, evidence: 2, explanations: 4). Table 5 shows the cross-validation results. As we can see from Row 1, the difference between accuracy and f-score indicates that the model performs differently for the three argumentation labels. The f-score for claims, evidence, and explanations are respectively 0.776, 0.565, and 0.164. While specifying class weights at training time helped, this shows that there is ample room for improvement. 6.1.2. Specificity As we showed in our previous study (Lugini and Litman, 2017), using the off-the-shelf Speciteller tool (Li and Nenkova, 2015) for predicting sentence specificity performed poorly when applied to text-based classroom discussions. We were able to significantly improve classification results by proposing features and models explicitly developed for classroom discussions. Like our previous work on argumentation, however, the corpus is not publicly available. To provide a baseline for the Discussion Tracker corpus, we evaluated specificity prediction performance using the same model as described in Section 6.1.1. By using this model we achieved very similar"
2020.lrec-1.130,W18-5208,1,0.866608,"ation simultaneously. It is possible, then, to analyze if and how these dimensions are related. If that is the case, it may be possible to develop more robust and accurate models for automated classification of such dimensions. Carlile et al. (2018) annotated argumentative discourse units for argument component and specificity (among other things) and were able to make use of these two aspects simultaneously to predict argument persuasiveness in written essays. Similarly, we showed in our previous study that specificity can be used to improve the performance of argument component classifiers (Lugini and Litman, 2018). We found that models trained through multi-task learning where the primary task consists of argument component classification and the secondary task consists of specificity classification almost always outperform models that only perform argument component classification. However, the corpus used in our previous study is not publicly available and therefore our previous results are not reproducible by other members of the research community. To provide reproducible performance baselines to facilitate future classifier evaluations using the Discussion Tracker corpus, we present our experiment"
2020.lrec-1.130,W18-0511,1,0.798191,"to an excel document formatted specifically for annotation. 4. Data Annotation The Discussion Tracker corpus includes annotations for three dimensions of student talk that researchers in classroom discourse have associated with positive educational outcomes (Howe et al., 2019; Applebee et al., 2003; Chisholm and Godley, 2011; Soter et al., 2008; Sohmer et al., 2009; Juzwik et al., 2013; Nystrand and Gamoran, 1991) (see Table 3); similar dimensions have also been annotated by NLP researchers for other types of data. Using a classroom discussion annotation scheme optimized for NLP development (Lugini et al., 2018), we annotated student talk for argument moves (claims, evidence and explanations) and specificity (low, medium and high). In addition, we developed an annotation scheme for collaboration that synthesized findings in both classroom discourse research (Engle and Conant, 2002; Keefer et al., 2000) as well as the computer-supported collaborative learning (CSCL) literature (Samei et al., 2014; Zhang et al., 2013). Prior to annotation, speakers were identified using the handwritten notes taken during data collection. Cases in which speaker IDs were difficult to determine were labeled either as ‘St?"
2020.lrec-1.130,D18-1078,0,0.130043,"g., extensions of and disagreements about others’ ideas). In addition to providing descriptive statistics on the corpus, we provide performance benchmarks and associated code for predicting each dimension separately, illustrate the use of the multiple annotations in the corpus to improve performance via multi-task learning, and finally discuss other ways the corpus might be used to further NLP research. Keywords: Corpus, Discourse, Text mining 1. Introduction Natural Language Processing (NLP) research has advanced considerably in recent years, developing reliable predictors for argumentation (Mirkin et al., 2018; Lippi and Torroni, 2016; Aharoni et al., 2014; Biran and Rambow, 2011a; Carlile et al., 2018; Habernal et al., 2014; Park and Cardie, 2018; Stab and Gurevych, 2014; Stab and Gurevych, 2017a), specificity (Li and Nenkova, 2015; Li et al., 2016; Louis and Nenkova, 2012; Gao et al., 2019), and collaboration (Richey et al., 2016). The majority of corpora informing these developments are made up of written texts (e.g., documents, asynchronous online discussions) extracted from the web and often written by individuals. Additionally, annotations in these corpora typically focus on one linguistic di"
2020.lrec-1.130,L18-1257,0,0.317604,"mance benchmarks and associated code for predicting each dimension separately, illustrate the use of the multiple annotations in the corpus to improve performance via multi-task learning, and finally discuss other ways the corpus might be used to further NLP research. Keywords: Corpus, Discourse, Text mining 1. Introduction Natural Language Processing (NLP) research has advanced considerably in recent years, developing reliable predictors for argumentation (Mirkin et al., 2018; Lippi and Torroni, 2016; Aharoni et al., 2014; Biran and Rambow, 2011a; Carlile et al., 2018; Habernal et al., 2014; Park and Cardie, 2018; Stab and Gurevych, 2014; Stab and Gurevych, 2017a), specificity (Li and Nenkova, 2015; Li et al., 2016; Louis and Nenkova, 2012; Gao et al., 2019), and collaboration (Richey et al., 2016). The majority of corpora informing these developments are made up of written texts (e.g., documents, asynchronous online discussions) extracted from the web and often written by individuals. Additionally, annotations in these corpora typically focus on one linguistic dimension at a time (e.g., specificity or dimensions of argumentation). As such, few published corpora both (1) focus on synchronous multi-par"
2020.lrec-1.130,D14-1162,0,0.0806694,"Missing"
2020.lrec-1.130,C14-1142,0,0.670402,"sociated code for predicting each dimension separately, illustrate the use of the multiple annotations in the corpus to improve performance via multi-task learning, and finally discuss other ways the corpus might be used to further NLP research. Keywords: Corpus, Discourse, Text mining 1. Introduction Natural Language Processing (NLP) research has advanced considerably in recent years, developing reliable predictors for argumentation (Mirkin et al., 2018; Lippi and Torroni, 2016; Aharoni et al., 2014; Biran and Rambow, 2011a; Carlile et al., 2018; Habernal et al., 2014; Park and Cardie, 2018; Stab and Gurevych, 2014; Stab and Gurevych, 2017a), specificity (Li and Nenkova, 2015; Li et al., 2016; Louis and Nenkova, 2012; Gao et al., 2019), and collaboration (Richey et al., 2016). The majority of corpora informing these developments are made up of written texts (e.g., documents, asynchronous online discussions) extracted from the web and often written by individuals. Additionally, annotations in these corpora typically focus on one linguistic dimension at a time (e.g., specificity or dimensions of argumentation). As such, few published corpora both (1) focus on synchronous multi-party argumentation and (2)"
2020.lrec-1.130,J17-3005,0,0.111361,"ing each dimension separately, illustrate the use of the multiple annotations in the corpus to improve performance via multi-task learning, and finally discuss other ways the corpus might be used to further NLP research. Keywords: Corpus, Discourse, Text mining 1. Introduction Natural Language Processing (NLP) research has advanced considerably in recent years, developing reliable predictors for argumentation (Mirkin et al., 2018; Lippi and Torroni, 2016; Aharoni et al., 2014; Biran and Rambow, 2011a; Carlile et al., 2018; Habernal et al., 2014; Park and Cardie, 2018; Stab and Gurevych, 2014; Stab and Gurevych, 2017a), specificity (Li and Nenkova, 2015; Li et al., 2016; Louis and Nenkova, 2012; Gao et al., 2019), and collaboration (Richey et al., 2016). The majority of corpora informing these developments are made up of written texts (e.g., documents, asynchronous online discussions) extracted from the web and often written by individuals. Additionally, annotations in these corpora typically focus on one linguistic dimension at a time (e.g., specificity or dimensions of argumentation). As such, few published corpora both (1) focus on synchronous multi-party argumentation and (2) include multiple simultan"
2020.lrec-1.130,W15-4631,0,0.49403,"tated dimensions, illustrating the challenging nature of this corpus. We then illustrate the benefits the corpus has already afforded NLP algorithms for improving argument mining with multi-task learning and discuss other potential uses of the corpus for further NLP research. 2. Related Work The development of argument mining tasks has been heavily informed by corpora of written texts extracted mostly from the web (e.g., online newspapers, Wikipedia, blog posts, user comments) (Al Khatib et al., 2018; Biran and Rambow, 2011b; Aharoni et al., 2014; Habernal et al., 2014; Park and Cardie, 2018; Swanson et al., 2015; Cabrio ˇ and Villata, 2014; Boltuˇzi´c and Snajder, 2014), from student essays (Stab and Gurevych, 2014; Stab and Gurevych, 2017a; Carlile et al., 2018), from legal documents (parliamentary records and court reports) (Mochales and Moens, 2011; Ashley and Walker, 2013), or from speeches (Mirkin et al., 2018; Lippi and Torroni, 2016). Similarly, NLP developments in specificity have typically drawn on newspaper articles (Louis and Nenkova, 2011; Li and Nenkova, 2015; Li et al., 2016) or online content (Gao et al., 2019; Ko et al., 2019). Although work in Computer Supported Collaborative Learnin"
2020.lrec-1.130,P10-1040,0,0.0155968,"gument mining models developed for other types of corpora. It consists of a hybrid model which combines embeddings generated through a neural network with a set of handcrafted features. The handcrafted features consist primarily of two sets: Speciteller feature set, derived from prior work on specificity (Li and Nenkova, 2015), and online dialogue feature set, derived from prior work on argument mining in online dialogues (Swanson et al., 2015). The Speciteller feature set contains the following features, extracted independently for each argument move: average of 100-dimensional word vectors (Turian et al., 2010) for words in the argument move, number of connectives, number of words, number of symbols, number of numbers, number of capital letters, number of stopwords normalized by argument move length, average characters per word, number of subjective and polar words (extracted using the MPQA (Wilson et al., 2009) and the General Inquirer (Stone and Hunt, 1963) dictionaries), average word familiarity (extracted using MRC Psycholinguistic Database (Wilson, 1988)), and inverse document frequency statistics (maximum, minimum). The online dialogue feature sets includes: number of pronouns, number of occur"
2020.lrec-1.130,J09-3003,0,0.0622653,"Li and Nenkova, 2015), and online dialogue feature set, derived from prior work on argument mining in online dialogues (Swanson et al., 2015). The Speciteller feature set contains the following features, extracted independently for each argument move: average of 100-dimensional word vectors (Turian et al., 2010) for words in the argument move, number of connectives, number of words, number of symbols, number of numbers, number of capital letters, number of stopwords normalized by argument move length, average characters per word, number of subjective and polar words (extracted using the MPQA (Wilson et al., 2009) and the General Inquirer (Stone and Hunt, 1963) dictionaries), average word familiarity (extracted using MRC Psycholinguistic Database (Wilson, 1988)), and inverse document frequency statistics (maximum, minimum). The online dialogue feature sets includes: number of pronouns, number of occurrences of words of different lengths, descriptive word-level statistics, term frequency - inverse document frequency of unigrams and bigrams (with frequency greater than 5), and part of speech tag features (unigrams, bigrams and trigrams).1 The neural network model is composed of a series of 3 convolutiona"
2021.argmining-1.14,N19-1054,0,0.0388942,"Missing"
2021.argmining-1.14,2020.acl-main.747,0,0.0373397,"15). We employ the generated pseudo labels from evidenceBERT to initially finetune BERT before finetuning over the manually labeled set. 4.4 Masked language model pretraining In these experiments, we use the top N examples from the automatically labeled data to train on a masked language model objective. We finetune the masked language model using Wolf et al. (2019). 144 Figure 1: Self-training techniques results 4.5 K-nearest neighbors based filtration Following the selection from unlabeled data method in Du et al. (2021), we encode manually labeled and unlabeled datasets using XMLR RoBERTa (Conneau et al., 2020) which achieves state-of-the-art results on the Semantic Textual Similarity benchmark (Cer et al., 2017). Then, for each instance in the manually labeled set, we select the top 5 nearest neighbors from the unlabeled set. We hypothesize that this process will yield more evidence like data from the whole argumentative set. We employ the new retrieved set in the three configurations of self-training we experiment with. 5 Results After presenting the baseline pretrained finetuned language model results, we discuss how adding the different self-training approaches sheds light on the three research"
2021.argmining-1.14,2021.naacl-main.426,0,0.0326493,"tion retrieval (Dehghani et al., 2017) and sentiment analysis (Severyn and Moschitti, 2015). We employ the generated pseudo labels from evidenceBERT to initially finetune BERT before finetuning over the manually labeled set. 4.4 Masked language model pretraining In these experiments, we use the top N examples from the automatically labeled data to train on a masked language model objective. We finetune the masked language model using Wolf et al. (2019). 144 Figure 1: Self-training techniques results 4.5 K-nearest neighbors based filtration Following the selection from unlabeled data method in Du et al. (2021), we encode manually labeled and unlabeled datasets using XMLR RoBERTa (Conneau et al., 2020) which achieves state-of-the-art results on the Semantic Textual Similarity benchmark (Cer et al., 2017). Then, for each instance in the manually labeled set, we select the top 5 nearest neighbors from the unlabeled set. We hypothesize that this process will yield more evidence like data from the whole argumentative set. We employ the new retrieved set in the three configurations of self-training we experiment with. 5 Results After presenting the baseline pretrained finetuned language model results, we"
2021.argmining-1.14,P18-1031,0,0.0489544,"Missing"
2021.argmining-1.14,N19-1423,0,0.0734145,"Missing"
2021.argmining-1.14,2021.eacl-main.65,0,0.0670036,"Missing"
2021.argmining-1.14,W17-5110,0,0.0319797,"practised in a growing number of countries. sentences Debate Topic: &quot;Economy&quot; nonthe price tag was set as being £32.7bilargumentative lion. argumentative &quot;high speed two will help to solve this inequality by increasing connections between north and south .&quot; Table 3: (Non-)Argumentative examples (unlabeled for evidence) in the Webis-Debate-16 dataset 4 Table 1: Labeled Wikipedia examples in the IBM Debater evidence dataset Wikipedia) argumentative corpus called WebisDebate-16, that is unlike Wikipedia, constructed from online debates. Wikipedia unlabeled data. Following the method described in Levy et al. (2017), we use the data retrieved by querying Wikipedia with a query composed of &quot;that&quot; + topic_concept. The work done by (Shnarch et al., 2018) suggests that the previous query can yield argumentative sentences in general, not just claims . The resultant corpus is composed of 29k candidate sentences. Table 2 shows an example unlabeled retrieved sentence out of Wikipedia. example sentences Topic: &quot;Doping in sport&quot; In October, the International Volleyball Federation closed the doping charges after concluding that &quot;there is no evidence of an anti-doping rule violation&quot; [REF]. Table 2: Unlabeled exampl"
2021.argmining-1.14,2021.ccl-1.108,0,0.0765101,"Missing"
2021.argmining-1.14,P15-1053,0,0.0462023,"Missing"
2021.argmining-1.14,P19-1054,0,0.014835,"s. The dataset contains 10846 argumentative phrases and 5556 nonargumentative phrases, therefore, we utilize it as an unlabeled (for evidence) argumentative source. Table 3 shows examples of non-argumentative and argumentative sentences from the Webis-Debate16 corpus. 2 https://webis.de/data/webis-debate-16.html Approach Our primary goal is to investigate different methods of self-training on top of large pretrained language models for evidence extraction. 4.1 Finetune-BERT baseline We start with finetune-BERT as our baseline, which achieved the best results on IBM Debater evidence dataset in Reimers et al. (2019). In our experiments, we refer to this baseline as evidenceBERT. 4.2 Bootstrapped self-training Our first self-training setting is bootstrapped selftraining, where we employ evidenceBERT to annotate unlabeled data. Every epoch, we make predictions over unlabeled data U . For each instance x in U , we extract the probability assigned to the most likely class p(x) = argmaxM (x) where xU and M is our evidenceBERT. The examples are then ranked based on the probabilities and the top N examples selected. In our experiments, we determine N by choosing the percentile of the top examples. Due to limit"
2021.argmining-1.14,P18-1096,0,0.053161,"Missing"
2021.argmining-1.14,P95-1026,0,0.890783,"Missing"
2021.argmining-1.14,S15-2079,0,0.060942,"Missing"
2021.argmining-1.14,P18-2095,0,0.0895213,"cation of Arabic dialects. We instead use self-training on top of pretrained language models for sentence-level evidence classification. Our main incentive is to explore the utility of self-training techniques in argument mining where acquiring manually labeled data is usually hard to get in large quantities. Argument Mining spans various lines of research work. Stab and Gurevych (2014), Habernal and Gurevych (2017), and Persing and Ng (2015) focus on identifying and classifying argument roles in text. Another direction is to mine argument units that are relevant to specific claims or topics (Shnarch et al., 2018; Biran and Rambow, 2011; Levy et al., 2017). Our work directly extends the work in this area by Shnarch et al. (2018). Evidence detection, as viewed in this work, aims at classifying relevant sentences to a certain topic. Shnarch et al. (2018) benefits from large-scale weakly labeled data described in Levy et al. (2017) blended with manually annotated data to train a BiLSTM with GLoVe embeddings and integrate the topic with an attention mechanism. Our work also aims at making use of weakly labeled data. However, instead of using the retrieved data described in Levy et al. (2017) directly, we"
2021.argmining-1.14,D14-1006,0,0.0863504,"Missing"
2021.argmining-1.15,N19-1054,0,0.172206,"tion. However, further analysis shows that the tasks in the MTL configuration should be chosen carefully depending on the focused task. We then try several techniques to increase the innate low precision of the relation classification tasks due to the highly imbalanced data, specifically SMOTE (Chawla et al., 2002) and XGBoost (Chen and Guestrin, 2016). Our results demonstrate that SMOTE is not very helpful but XGBoost, when used with the representations learnt from MTL, can increase the precision and F-scores of the relation identification tasks. 2 Related Work Our work is closely related to Chakrabarty et al. (2019b). Their system, called AMPERSAND, tackles three AM tasks on a dataset created from the Change My View (CMV) subreddit1 (Hidey et al., 2017) and focuses on transfer learning approaches with BERT (Devlin et al., 2019) that take advantage of discourse and dialogue context. Specifically, they define three separate tasks: argument component classification and intra/inter relation identification. For the first task, the requirement is to classify a given sentence into either Claim, Premise or Non-argumentative. For the intra-relation identification task, given a pair of argumentative sentences fro"
2021.argmining-1.15,D19-1291,0,0.0639661,"tion. However, further analysis shows that the tasks in the MTL configuration should be chosen carefully depending on the focused task. We then try several techniques to increase the innate low precision of the relation classification tasks due to the highly imbalanced data, specifically SMOTE (Chawla et al., 2002) and XGBoost (Chen and Guestrin, 2016). Our results demonstrate that SMOTE is not very helpful but XGBoost, when used with the representations learnt from MTL, can increase the precision and F-scores of the relation identification tasks. 2 Related Work Our work is closely related to Chakrabarty et al. (2019b). Their system, called AMPERSAND, tackles three AM tasks on a dataset created from the Change My View (CMV) subreddit1 (Hidey et al., 2017) and focuses on transfer learning approaches with BERT (Devlin et al., 2019) that take advantage of discourse and dialogue context. Specifically, they define three separate tasks: argument component classification and intra/inter relation identification. For the first task, the requirement is to classify a given sentence into either Claim, Premise or Non-argumentative. For the intra-relation identification task, given a pair of argumentative sentences fro"
2021.argmining-1.15,N19-1423,0,0.00585847,"lassification tasks due to the highly imbalanced data, specifically SMOTE (Chawla et al., 2002) and XGBoost (Chen and Guestrin, 2016). Our results demonstrate that SMOTE is not very helpful but XGBoost, when used with the representations learnt from MTL, can increase the precision and F-scores of the relation identification tasks. 2 Related Work Our work is closely related to Chakrabarty et al. (2019b). Their system, called AMPERSAND, tackles three AM tasks on a dataset created from the Change My View (CMV) subreddit1 (Hidey et al., 2017) and focuses on transfer learning approaches with BERT (Devlin et al., 2019) that take advantage of discourse and dialogue context. Specifically, they define three separate tasks: argument component classification and intra/inter relation identification. For the first task, the requirement is to classify a given sentence into either Claim, Premise or Non-argumentative. For the intra-relation identification task, given a pair of argumentative sentences from the same post, we need to answer if an argumentative relation between these two sentences exists. The inter-relation identification task is similar, except that the two sentences are from different posts. However, t"
2021.argmining-1.15,P17-1002,0,0.0174987,"tation. We identify argument components (claim, components, non-argumentative) and two types of relations: intra-turn relations within one post and inter-turn relations across posts. Our results demonstrate that using MTL improves 148 1 https://www.reddit.com/r/changemyview Proceedings of The 8th Workshop on Argument Mining, pages 148–153 Punta Cana, Dominican Republic, November 10–11, 2021. ©2021 Association for Computational Linguistics performance in NLP problems (Søgaard and Goldberg, 2016; Yang et al., 2016; Liu et al., 2019; Peng et al., 2020). Focusing on one single domain and dataset, Eger et al. (2017) treats AM as a sequence tagging problem and uses sub-tasks such as component identification and relation classification as auxiliaries in MTL to improve performances. Schulz et al. (2018) also formalizes argument component identification as a sequence tagging problem but utilizes multiple datasets from different domains in their MTL setup. They observe that the results on a small AM dataset can be improved when other AM datasets are leveraged as auxiliary tasks. These approaches, however, work on monologues where each data instance is from one person and therefore ignore the macro-structure o"
2021.argmining-1.15,W17-5102,0,0.0128427,"y several techniques to increase the innate low precision of the relation classification tasks due to the highly imbalanced data, specifically SMOTE (Chawla et al., 2002) and XGBoost (Chen and Guestrin, 2016). Our results demonstrate that SMOTE is not very helpful but XGBoost, when used with the representations learnt from MTL, can increase the precision and F-scores of the relation identification tasks. 2 Related Work Our work is closely related to Chakrabarty et al. (2019b). Their system, called AMPERSAND, tackles three AM tasks on a dataset created from the Change My View (CMV) subreddit1 (Hidey et al., 2017) and focuses on transfer learning approaches with BERT (Devlin et al., 2019) that take advantage of discourse and dialogue context. Specifically, they define three separate tasks: argument component classification and intra/inter relation identification. For the first task, the requirement is to classify a given sentence into either Claim, Premise or Non-argumentative. For the intra-relation identification task, given a pair of argumentative sentences from the same post, we need to answer if an argumentative relation between these two sentences exists. The inter-relation identification task is"
2021.argmining-1.15,D15-1110,0,0.0283157,"lation prediction separately and had independent BERT models for the tasks. Our approach works on the assumption that the three tasks are related to each other. Many studies have shown that jointly learning several tasks during training usually leads to better Argument mining (AM) focuses on automatically identifying argumentative structures in text, and utilizing these structures in applications. AM tasks include identifying argument components (e.g., “claim”) and relations between them (e.g., “support”). However, most AM studies have focused on monologues or micro-level models of arguments (Peldszus and Stede, 2015; Persing and Ng, 2016; Stab and Gurevych, 2017). AM in dialogues and macro-level models have received less attention (Bentahar et al., 2010; Chakrabarty et al., 2019b). In this study, we extend the work of Chakrabarty et al. (2019b) in AM for persuasive online discussions. Particularly, we take advantage of a multi-task learning (MTL) approach to automatically identify the argument structures in persuasive dialogues that contain both micro-level and macrolevel argumentation. We identify argument components (claim, components, non-argumentative) and two types of relations: intra-turn relations"
2021.argmining-1.15,2020.bionlp-1.22,0,0.0172623,"dialogues that contain both micro-level and macrolevel argumentation. We identify argument components (claim, components, non-argumentative) and two types of relations: intra-turn relations within one post and inter-turn relations across posts. Our results demonstrate that using MTL improves 148 1 https://www.reddit.com/r/changemyview Proceedings of The 8th Workshop on Argument Mining, pages 148–153 Punta Cana, Dominican Republic, November 10–11, 2021. ©2021 Association for Computational Linguistics performance in NLP problems (Søgaard and Goldberg, 2016; Yang et al., 2016; Liu et al., 2019; Peng et al., 2020). Focusing on one single domain and dataset, Eger et al. (2017) treats AM as a sequence tagging problem and uses sub-tasks such as component identification and relation classification as auxiliaries in MTL to improve performances. Schulz et al. (2018) also formalizes argument component identification as a sequence tagging problem but utilizes multiple datasets from different domains in their MTL setup. They observe that the results on a small AM dataset can be improved when other AM datasets are leveraged as auxiliary tasks. These approaches, however, work on monologues where each data instanc"
2021.argmining-1.15,N16-1164,0,0.0218395,"ly and had independent BERT models for the tasks. Our approach works on the assumption that the three tasks are related to each other. Many studies have shown that jointly learning several tasks during training usually leads to better Argument mining (AM) focuses on automatically identifying argumentative structures in text, and utilizing these structures in applications. AM tasks include identifying argument components (e.g., “claim”) and relations between them (e.g., “support”). However, most AM studies have focused on monologues or micro-level models of arguments (Peldszus and Stede, 2015; Persing and Ng, 2016; Stab and Gurevych, 2017). AM in dialogues and macro-level models have received less attention (Bentahar et al., 2010; Chakrabarty et al., 2019b). In this study, we extend the work of Chakrabarty et al. (2019b) in AM for persuasive online discussions. Particularly, we take advantage of a multi-task learning (MTL) approach to automatically identify the argument structures in persuasive dialogues that contain both micro-level and macrolevel argumentation. We identify argument components (claim, components, non-argumentative) and two types of relations: intra-turn relations within one post and i"
2021.argmining-1.15,N18-2006,0,0.0166591,"results demonstrate that using MTL improves 148 1 https://www.reddit.com/r/changemyview Proceedings of The 8th Workshop on Argument Mining, pages 148–153 Punta Cana, Dominican Republic, November 10–11, 2021. ©2021 Association for Computational Linguistics performance in NLP problems (Søgaard and Goldberg, 2016; Yang et al., 2016; Liu et al., 2019; Peng et al., 2020). Focusing on one single domain and dataset, Eger et al. (2017) treats AM as a sequence tagging problem and uses sub-tasks such as component identification and relation classification as auxiliaries in MTL to improve performances. Schulz et al. (2018) also formalizes argument component identification as a sequence tagging problem but utilizes multiple datasets from different domains in their MTL setup. They observe that the results on a small AM dataset can be improved when other AM datasets are leveraged as auxiliary tasks. These approaches, however, work on monologues where each data instance is from one person and therefore ignore the macro-structure of arguments. Our work tackles AM at dialogical level, specifically on online discussion forums. We hypothesize that MTL can help represent both micro and macro structure and use BERT with"
2021.argmining-1.15,P16-2038,0,0.0284206,"h to automatically identify the argument structures in persuasive dialogues that contain both micro-level and macrolevel argumentation. We identify argument components (claim, components, non-argumentative) and two types of relations: intra-turn relations within one post and inter-turn relations across posts. Our results demonstrate that using MTL improves 148 1 https://www.reddit.com/r/changemyview Proceedings of The 8th Workshop on Argument Mining, pages 148–153 Punta Cana, Dominican Republic, November 10–11, 2021. ©2021 Association for Computational Linguistics performance in NLP problems (Søgaard and Goldberg, 2016; Yang et al., 2016; Liu et al., 2019; Peng et al., 2020). Focusing on one single domain and dataset, Eger et al. (2017) treats AM as a sequence tagging problem and uses sub-tasks such as component identification and relation classification as auxiliaries in MTL to improve performances. Schulz et al. (2018) also formalizes argument component identification as a sequence tagging problem but utilizes multiple datasets from different domains in their MTL setup. They observe that the results on a small AM dataset can be improved when other AM datasets are leveraged as auxiliary tasks. These approa"
2021.argmining-1.15,J17-3005,0,0.0274399,"BERT models for the tasks. Our approach works on the assumption that the three tasks are related to each other. Many studies have shown that jointly learning several tasks during training usually leads to better Argument mining (AM) focuses on automatically identifying argumentative structures in text, and utilizing these structures in applications. AM tasks include identifying argument components (e.g., “claim”) and relations between them (e.g., “support”). However, most AM studies have focused on monologues or micro-level models of arguments (Peldszus and Stede, 2015; Persing and Ng, 2016; Stab and Gurevych, 2017). AM in dialogues and macro-level models have received less attention (Bentahar et al., 2010; Chakrabarty et al., 2019b). In this study, we extend the work of Chakrabarty et al. (2019b) in AM for persuasive online discussions. Particularly, we take advantage of a multi-task learning (MTL) approach to automatically identify the argument structures in persuasive dialogues that contain both micro-level and macrolevel argumentation. We identify argument components (claim, components, non-argumentative) and two types of relations: intra-turn relations within one post and inter-turn relations across"
2021.argmining-1.15,P19-1441,0,0.159829,"ures in persuasive dialogues that contain both micro-level and macrolevel argumentation. We identify argument components (claim, components, non-argumentative) and two types of relations: intra-turn relations within one post and inter-turn relations across posts. Our results demonstrate that using MTL improves 148 1 https://www.reddit.com/r/changemyview Proceedings of The 8th Workshop on Argument Mining, pages 148–153 Punta Cana, Dominican Republic, November 10–11, 2021. ©2021 Association for Computational Linguistics performance in NLP problems (Søgaard and Goldberg, 2016; Yang et al., 2016; Liu et al., 2019; Peng et al., 2020). Focusing on one single domain and dataset, Eger et al. (2017) treats AM as a sequence tagging problem and uses sub-tasks such as component identification and relation classification as auxiliaries in MTL to improve performances. Schulz et al. (2018) also formalizes argument component identification as a sequence tagging problem but utilizes multiple datasets from different domains in their MTL setup. They observe that the results on a small AM dataset can be improved when other AM datasets are leveraged as auxiliary tasks. These approaches, however, work on monologues whe"
2021.bea-1.9,K17-1017,0,0.0172485,"edu Abstract Over the more than 50 year history of AES research, the majority of work has used feature-based models (Louis and Higgins, 2010; Persing et al., 2010; Yannakoudakis and Briscoe, 2012; Persing and Ng, 2015; Farra et al., 2015; McNamara et al., 2015; Cummins et al., 2016; Ghosh et al., 2016; Nguyen and Litman, 2018; Amorim et al., 2018; Cozma et al., 2018). However, these models require carefully designed hand-crafted features to represent essays. Recently, neural network models have drawn increasing attention (Alikaniotis et al., 2016; Taghipour and Ng, 2016; Dong and Zhang, 2016; Dong et al., 2017; Farag et al., 2018; Jin et al., 2018; Li et al., 2018; Tay et al., 2018; Nadeem et al., 2019; Mayfield and Black, 2020) due to their powerful ability to generate essay representations automatically and to generate reliable essay score predictions. No matter whether a feature-based or neural network model is used, state-of-the-art AES systems are all supervised, which means that the model needs to be trained on a large number of humangraded essays. Unfortunately, such a humangraded corpus often does not exist, and grading a corpus of essays is a laborious task. To address this problem, we inv"
2021.bea-1.9,P16-1068,0,0.0196547,"ience & LRDC University of Pittsburgh Pittsburgh, PA 15260 dlitman@pitt.edu Abstract Over the more than 50 year history of AES research, the majority of work has used feature-based models (Louis and Higgins, 2010; Persing et al., 2010; Yannakoudakis and Briscoe, 2012; Persing and Ng, 2015; Farra et al., 2015; McNamara et al., 2015; Cummins et al., 2016; Ghosh et al., 2016; Nguyen and Litman, 2018; Amorim et al., 2018; Cozma et al., 2018). However, these models require carefully designed hand-crafted features to represent essays. Recently, neural network models have drawn increasing attention (Alikaniotis et al., 2016; Taghipour and Ng, 2016; Dong and Zhang, 2016; Dong et al., 2017; Farag et al., 2018; Jin et al., 2018; Li et al., 2018; Tay et al., 2018; Nadeem et al., 2019; Mayfield and Black, 2020) due to their powerful ability to generate essay representations automatically and to generate reliable essay score predictions. No matter whether a feature-based or neural network model is used, state-of-the-art AES systems are all supervised, which means that the model needs to be trained on a large number of humangraded essays. Unfortunately, such a humangraded corpus often does not exist, and grading a corp"
2021.bea-1.9,N18-1024,0,0.0167643,"he more than 50 year history of AES research, the majority of work has used feature-based models (Louis and Higgins, 2010; Persing et al., 2010; Yannakoudakis and Briscoe, 2012; Persing and Ng, 2015; Farra et al., 2015; McNamara et al., 2015; Cummins et al., 2016; Ghosh et al., 2016; Nguyen and Litman, 2018; Amorim et al., 2018; Cozma et al., 2018). However, these models require carefully designed hand-crafted features to represent essays. Recently, neural network models have drawn increasing attention (Alikaniotis et al., 2016; Taghipour and Ng, 2016; Dong and Zhang, 2016; Dong et al., 2017; Farag et al., 2018; Jin et al., 2018; Li et al., 2018; Tay et al., 2018; Nadeem et al., 2019; Mayfield and Black, 2020) due to their powerful ability to generate essay representations automatically and to generate reliable essay score predictions. No matter whether a feature-based or neural network model is used, state-of-the-art AES systems are all supervised, which means that the model needs to be trained on a large number of humangraded essays. Unfortunately, such a humangraded corpus often does not exist, and grading a corpus of essays is a laborious task. To address this problem, we investigate using a wea"
2021.bea-1.9,N18-1021,0,0.0304624,"Missing"
2021.bea-1.9,W15-0608,0,0.121513,"Missing"
2021.bea-1.9,P17-1102,0,0.024621,"for extracting Topical Components (TCs). Previously, human expert effort was required to extract TCs. Specifically, experts read through the source article and created lists of topic words and of specific examples that students were expected to use in their essays (Rahimi et al., 2017). In order to eliminate this human effort, three systems were later developed. An LDA-based system (Rahimi and Litman, 2016) used a LDA topic model (Blei et al., 2003) and TurboTopic algorithm (Blei and Lafferty, 2009) for TC extraction. Zhang and Litman (2020) proposed another system based on the PositionRank (Florescu and Caragea, 2017) algorithm. While these two TC extraction systems did not require any human coding, they also did not match prior performance. The state-of-the-art system (Zhang and Litman, 2020) extracted TCs by exploiting the attention weights of a neural AES model. However, human grading effort was needed for model training. In our work, we replace huPreviously, human effort was required to manually create TCs based on expert knowledge of the source text. To eliminate this effort, Zhang and Litman (2020) used the attention layer output of a co-attention neural network to automatically extract TCs. Their ex"
2021.bea-1.9,P16-2089,0,0.0506936,"Missing"
2021.bea-1.9,D13-1180,0,0.0723877,"Missing"
2021.bea-1.9,P18-1100,0,0.0315187,"Missing"
2021.bea-1.9,P13-1113,0,0.0731378,"Missing"
2021.bea-1.9,W16-0507,0,0.0256275,"erage essay word counts for each prompt are 180 and 220, respectively. Table 1: The Evidence score distribution of RTA. man scores with automated essay quality signals for training, while still achieving state-of-the-art TC extraction. We believe that many predictive features used in the traditional feature-based AES systems can be useful signals for our weak supervision approach to TC extraction. For example, lengthbased features (Attali and Burstein, 2006; Chen ¨ and He, 2013; Ostling et al., 2013; Phandi et al., 2015; Zesch et al., 2015b), prompt-relevant features (Louis and Higgins, 2010; Klebanov et al., 2016), or semantic features (Klebanov and Flor, 2013; Persing and Ng, 2013) all weakly relate to the quality of an essay’s content. In this paper, we examine two such signals, word count and topic distribution similarity, and show that with these simple essay quality signals, human-labeled essay scores are unnecessary for TC extraction. 3 4 Prior AES Systems for the RTA Two approaches to AES have been developed for the RTA: 1) a feature-based supervised learning approach (with the features aligned to the Evidence rubric criteria), which we refer to as AESrubric , and 2) a neural network approach, w"
2021.bea-1.9,P18-2080,0,0.0117644,"Source-based Essay Scoring Haoran Zhang Department of Computer Science University of Pittsburgh Pittsburgh, PA 15260 colinzhang@cs.pitt.edu Diane Litman Department of Computer Science & LRDC University of Pittsburgh Pittsburgh, PA 15260 dlitman@pitt.edu Abstract Over the more than 50 year history of AES research, the majority of work has used feature-based models (Louis and Higgins, 2010; Persing et al., 2010; Yannakoudakis and Briscoe, 2012; Persing and Ng, 2015; Farra et al., 2015; McNamara et al., 2015; Cummins et al., 2016; Ghosh et al., 2016; Nguyen and Litman, 2018; Amorim et al., 2018; Cozma et al., 2018). However, these models require carefully designed hand-crafted features to represent essays. Recently, neural network models have drawn increasing attention (Alikaniotis et al., 2016; Taghipour and Ng, 2016; Dong and Zhang, 2016; Dong et al., 2017; Farag et al., 2018; Jin et al., 2018; Li et al., 2018; Tay et al., 2018; Nadeem et al., 2019; Mayfield and Black, 2020) due to their powerful ability to generate essay representations automatically and to generate reliable essay score predictions. No matter whether a feature-based or neural network model is used, state-of-the-art AES systems are al"
2021.bea-1.9,P16-1075,0,0.0254534,"Missing"
2021.bea-1.9,D16-1115,0,0.0210724,"PA 15260 dlitman@pitt.edu Abstract Over the more than 50 year history of AES research, the majority of work has used feature-based models (Louis and Higgins, 2010; Persing et al., 2010; Yannakoudakis and Briscoe, 2012; Persing and Ng, 2015; Farra et al., 2015; McNamara et al., 2015; Cummins et al., 2016; Ghosh et al., 2016; Nguyen and Litman, 2018; Amorim et al., 2018; Cozma et al., 2018). However, these models require carefully designed hand-crafted features to represent essays. Recently, neural network models have drawn increasing attention (Alikaniotis et al., 2016; Taghipour and Ng, 2016; Dong and Zhang, 2016; Dong et al., 2017; Farag et al., 2018; Jin et al., 2018; Li et al., 2018; Tay et al., 2018; Nadeem et al., 2019; Mayfield and Black, 2020) due to their powerful ability to generate essay representations automatically and to generate reliable essay score predictions. No matter whether a feature-based or neural network model is used, state-of-the-art AES systems are all supervised, which means that the model needs to be trained on a large number of humangraded essays. Unfortunately, such a humangraded corpus often does not exist, and grading a corpus of essays is a laborious task. To address t"
2021.bea-1.9,W10-1013,0,0.0469295,"shown in boldface. The average essay word counts for each prompt are 180 and 220, respectively. Table 1: The Evidence score distribution of RTA. man scores with automated essay quality signals for training, while still achieving state-of-the-art TC extraction. We believe that many predictive features used in the traditional feature-based AES systems can be useful signals for our weak supervision approach to TC extraction. For example, lengthbased features (Attali and Burstein, 2006; Chen ¨ and He, 2013; Ostling et al., 2013; Phandi et al., 2015; Zesch et al., 2015b), prompt-relevant features (Louis and Higgins, 2010; Klebanov et al., 2016), or semantic features (Klebanov and Flor, 2013; Persing and Ng, 2013) all weakly relate to the quality of an essay’s content. In this paper, we examine two such signals, word count and topic distribution similarity, and show that with these simple essay quality signals, human-labeled essay scores are unnecessary for TC extraction. 3 4 Prior AES Systems for the RTA Two approaches to AES have been developed for the RTA: 1) a feature-based supervised learning approach (with the features aligned to the Evidence rubric criteria), which we refer to as AESrubric , and 2) a ne"
2021.bea-1.9,2020.bea-1.15,0,0.0120205,"ls (Louis and Higgins, 2010; Persing et al., 2010; Yannakoudakis and Briscoe, 2012; Persing and Ng, 2015; Farra et al., 2015; McNamara et al., 2015; Cummins et al., 2016; Ghosh et al., 2016; Nguyen and Litman, 2018; Amorim et al., 2018; Cozma et al., 2018). However, these models require carefully designed hand-crafted features to represent essays. Recently, neural network models have drawn increasing attention (Alikaniotis et al., 2016; Taghipour and Ng, 2016; Dong and Zhang, 2016; Dong et al., 2017; Farag et al., 2018; Jin et al., 2018; Li et al., 2018; Tay et al., 2018; Nadeem et al., 2019; Mayfield and Black, 2020) due to their powerful ability to generate essay representations automatically and to generate reliable essay score predictions. No matter whether a feature-based or neural network model is used, state-of-the-art AES systems are all supervised, which means that the model needs to be trained on a large number of humangraded essays. Unfortunately, such a humangraded corpus often does not exist, and grading a corpus of essays is a laborious task. To address this problem, we investigate using a weakly supervised AES approach, where automatically available essay quality signals replace the use of h"
2021.bea-1.9,W15-0612,0,0.0181948,"f research in the AES area uses supervised machine learning techniques that require a large number of human-graded essays for training. However, graded essay corpora are usually missing in real classroom scenarios, and annotating a corpus to train an AES model is laborintensive. A prior proposal to address this problem used an unsupervised-learning approach based on a voting algorithm (Chen et al., 2010). The area of short answer scoring has also faced a similar problem. Zesch et al. (2015a) presented a semi-supervised method to reduce the size of the required human-labeled corpus. Meanwhile, Ramachandran et al. (2015) proposed a graph-based lexico-semantic text matching for pattern identification. These works reduce human effort, but do not eliminate them. In contrast, our AES work fully replaces human graded evidence scores with essay quality signals that are easy to extract automatically and to use during training. Although our results show that the signals are not effective for the AES task itself, they are useful for extracting Topical Components (TCs). Previously, human expert effort was required to extract TCs. Specifically, experts read through the source article and created lists of topic words and"
2021.bea-1.9,W19-4450,0,0.0146016,"ed feature-based models (Louis and Higgins, 2010; Persing et al., 2010; Yannakoudakis and Briscoe, 2012; Persing and Ng, 2015; Farra et al., 2015; McNamara et al., 2015; Cummins et al., 2016; Ghosh et al., 2016; Nguyen and Litman, 2018; Amorim et al., 2018; Cozma et al., 2018). However, these models require carefully designed hand-crafted features to represent essays. Recently, neural network models have drawn increasing attention (Alikaniotis et al., 2016; Taghipour and Ng, 2016; Dong and Zhang, 2016; Dong et al., 2017; Farag et al., 2018; Jin et al., 2018; Li et al., 2018; Tay et al., 2018; Nadeem et al., 2019; Mayfield and Black, 2020) due to their powerful ability to generate essay representations automatically and to generate reliable essay score predictions. No matter whether a feature-based or neural network model is used, state-of-the-art AES systems are all supervised, which means that the model needs to be trained on a large number of humangraded essays. Unfortunately, such a humangraded corpus often does not exist, and grading a corpus of essays is a laborious task. To address this problem, we investigate using a weakly supervised AES approach, where automatically available essay quality s"
2021.bea-1.9,D16-1193,0,0.0141576,"Pittsburgh Pittsburgh, PA 15260 dlitman@pitt.edu Abstract Over the more than 50 year history of AES research, the majority of work has used feature-based models (Louis and Higgins, 2010; Persing et al., 2010; Yannakoudakis and Briscoe, 2012; Persing and Ng, 2015; Farra et al., 2015; McNamara et al., 2015; Cummins et al., 2016; Ghosh et al., 2016; Nguyen and Litman, 2018; Amorim et al., 2018; Cozma et al., 2018). However, these models require carefully designed hand-crafted features to represent essays. Recently, neural network models have drawn increasing attention (Alikaniotis et al., 2016; Taghipour and Ng, 2016; Dong and Zhang, 2016; Dong et al., 2017; Farag et al., 2018; Jin et al., 2018; Li et al., 2018; Tay et al., 2018; Nadeem et al., 2019; Mayfield and Black, 2020) due to their powerful ability to generate essay representations automatically and to generate reliable essay score predictions. No matter whether a feature-based or neural network model is used, state-of-the-art AES systems are all supervised, which means that the model needs to be trained on a large number of humangraded essays. Unfortunately, such a humangraded corpus often does not exist, and grading a corpus of essays is a labori"
2021.bea-1.9,W13-1705,0,0.0600118,"Missing"
2021.bea-1.9,W19-4404,0,0.0606419,"Missing"
2021.bea-1.9,D10-1023,0,0.102684,"Missing"
2021.bea-1.9,W12-2004,0,0.074315,"Missing"
2021.bea-1.9,P13-1026,0,0.0233381,"Table 1: The Evidence score distribution of RTA. man scores with automated essay quality signals for training, while still achieving state-of-the-art TC extraction. We believe that many predictive features used in the traditional feature-based AES systems can be useful signals for our weak supervision approach to TC extraction. For example, lengthbased features (Attali and Burstein, 2006; Chen ¨ and He, 2013; Ostling et al., 2013; Phandi et al., 2015; Zesch et al., 2015b), prompt-relevant features (Louis and Higgins, 2010; Klebanov et al., 2016), or semantic features (Klebanov and Flor, 2013; Persing and Ng, 2013) all weakly relate to the quality of an essay’s content. In this paper, we examine two such signals, word count and topic distribution similarity, and show that with these simple essay quality signals, human-labeled essay scores are unnecessary for TC extraction. 3 4 Prior AES Systems for the RTA Two approaches to AES have been developed for the RTA: 1) a feature-based supervised learning approach (with the features aligned to the Evidence rubric criteria), which we refer to as AESrubric , and 2) a neural network approach, which we refer to as AESneural . AESrubric (Rahimi et al., 2017; Zhang"
2021.bea-1.9,W15-0615,0,0.425536,"nstructed TCs, whether for 1) representing essays as rubric-based features, 2) grading essays. 2 Related Work The majority of research in the AES area uses supervised machine learning techniques that require a large number of human-graded essays for training. However, graded essay corpora are usually missing in real classroom scenarios, and annotating a corpus to train an AES model is laborintensive. A prior proposal to address this problem used an unsupervised-learning approach based on a voting algorithm (Chen et al., 2010). The area of short answer scoring has also faced a similar problem. Zesch et al. (2015a) presented a semi-supervised method to reduce the size of the required human-labeled corpus. Meanwhile, Ramachandran et al. (2015) proposed a graph-based lexico-semantic text matching for pattern identification. These works reduce human effort, but do not eliminate them. In contrast, our AES work fully replaces human graded evidence scores with essay quality signals that are easy to extract automatically and to use during training. Although our results show that the signals are not effective for the AES task itself, they are useful for extracting Topical Components (TCs). Previously, human e"
2021.bea-1.9,P15-1053,0,0.0609722,"Missing"
2021.bea-1.9,W15-0626,0,0.367696,"nstructed TCs, whether for 1) representing essays as rubric-based features, 2) grading essays. 2 Related Work The majority of research in the AES area uses supervised machine learning techniques that require a large number of human-graded essays for training. However, graded essay corpora are usually missing in real classroom scenarios, and annotating a corpus to train an AES model is laborintensive. A prior proposal to address this problem used an unsupervised-learning approach based on a voting algorithm (Chen et al., 2010). The area of short answer scoring has also faced a similar problem. Zesch et al. (2015a) presented a semi-supervised method to reduce the size of the required human-labeled corpus. Meanwhile, Ramachandran et al. (2015) proposed a graph-based lexico-semantic text matching for pattern identification. These works reduce human effort, but do not eliminate them. In contrast, our AES work fully replaces human graded evidence scores with essay quality signals that are easy to extract automatically and to use during training. Although our results show that the signals are not effective for the AES task itself, they are useful for extracting Topical Components (TCs). Previously, human e"
2021.bea-1.9,D15-1049,0,0.170423,"Phrases semantically related to the TCs from the source article are shown in boldface. The average essay word counts for each prompt are 180 and 220, respectively. Table 1: The Evidence score distribution of RTA. man scores with automated essay quality signals for training, while still achieving state-of-the-art TC extraction. We believe that many predictive features used in the traditional feature-based AES systems can be useful signals for our weak supervision approach to TC extraction. For example, lengthbased features (Attali and Burstein, 2006; Chen ¨ and He, 2013; Ostling et al., 2013; Phandi et al., 2015; Zesch et al., 2015b), prompt-relevant features (Louis and Higgins, 2010; Klebanov et al., 2016), or semantic features (Klebanov and Flor, 2013; Persing and Ng, 2013) all weakly relate to the quality of an essay’s content. In this paper, we examine two such signals, word count and topic distribution similarity, and show that with these simple essay quality signals, human-labeled essay scores are unnecessary for TC extraction. 3 4 Prior AES Systems for the RTA Two approaches to AES have been developed for the RTA: 1) a feature-based supervised learning approach (with the features aligned to th"
2021.bea-1.9,P17-3013,1,0.844599,"2013) all weakly relate to the quality of an essay’s content. In this paper, we examine two such signals, word count and topic distribution similarity, and show that with these simple essay quality signals, human-labeled essay scores are unnecessary for TC extraction. 3 4 Prior AES Systems for the RTA Two approaches to AES have been developed for the RTA: 1) a feature-based supervised learning approach (with the features aligned to the Evidence rubric criteria), which we refer to as AESrubric , and 2) a neural network approach, which we refer to as AESneural . AESrubric (Rahimi et al., 2017; Zhang and Litman, 2017) used a traditional supervised learning framework with hand-crafted, rubric-based features that require knowledge of TCs to compute. That is, a set of interpretable features were carefully designed to capture the relation between student essays and the two aspects of TCs described above (topic words and specific examples): Number of Pieces of Evidence (NPE): an integer feature based on the list of topic words for each topic that indicates the number of topics (semantically) mentioned in the essay. Concentration (CON): a binary feature that indicates if an essay elaborates on topics, again base"
2021.bea-1.9,W16-0532,1,0.929331,"d evidence scores with essay quality signals that are easy to extract automatically and to use during training. Although our results show that the signals are not effective for the AES task itself, they are useful for extracting Topical Components (TCs). Previously, human expert effort was required to extract TCs. Specifically, experts read through the source article and created lists of topic words and of specific examples that students were expected to use in their essays (Rahimi et al., 2017). In order to eliminate this human effort, three systems were later developed. An LDA-based system (Rahimi and Litman, 2016) used a LDA topic model (Blei et al., 2003) and TurboTopic algorithm (Blei and Lafferty, 2009) for TC extraction. Zhang and Litman (2020) proposed another system based on the PositionRank (Florescu and Caragea, 2017) algorithm. While these two TC extraction systems did not require any human coding, they also did not match prior performance. The state-of-the-art system (Zhang and Litman, 2020) extracted TCs by exploiting the attention weights of a neural AES model. However, human grading effort was needed for model training. In our work, we replace huPreviously, human effort was required to man"
2021.bea-1.9,W18-0549,1,0.844928,"higher attention scores to important sentences, while the self-attention layer on the word (phrase) level assigned higher attention scores to important words (phrases). Therefore, their system extracted important words from important Table 3: The partial list of topic words of RT AM V P . back messages such as “Use more evidence from the article” (based on NPE values) or “Provide more details for each piece of evidence you use” (based on NPE and SPC values). Although AESrubric thus provides useful information for the AWE system, in order to improve the stand-alone AES performance, AESneural (Zhang and Litman, 2018) was later developed. AESneural used a hierarchical neural network model with a self-attention mechanism in the phrase level and a co-attention mechanism in the sentence level. The self-attention layer captures the importance of each individual phrase, while the co-attention layer captures the relationship between the source article and the essay. In terms of the essay score prediction task, AESneural significantly outperforms AESrubric , and no human effort for either TC extraction or feature engineering is required. However, the essay representations created by AESneural cannot be directly u"
2021.bea-1.9,2020.acl-main.759,1,0.819231,"that the signals are not effective for the AES task itself, they are useful for extracting Topical Components (TCs). Previously, human expert effort was required to extract TCs. Specifically, experts read through the source article and created lists of topic words and of specific examples that students were expected to use in their essays (Rahimi et al., 2017). In order to eliminate this human effort, three systems were later developed. An LDA-based system (Rahimi and Litman, 2016) used a LDA topic model (Blei et al., 2003) and TurboTopic algorithm (Blei and Lafferty, 2009) for TC extraction. Zhang and Litman (2020) proposed another system based on the PositionRank (Florescu and Caragea, 2017) algorithm. While these two TC extraction systems did not require any human coding, they also did not match prior performance. The state-of-the-art system (Zhang and Litman, 2020) extracted TCs by exploiting the attention weights of a neural AES model. However, human grading effort was needed for model training. In our work, we replace huPreviously, human effort was required to manually create TCs based on expert knowledge of the source text. To eliminate this effort, Zhang and Litman (2020) used the attention layer"
2021.findings-emnlp.142,P18-1064,0,0.0205042,"h text entailment generation (Pasunuru et al., corporated only one additional task during training 2017), extractive summarization (Chen et al., 2019; (Isonuma et al., 2017; Chen et al., 2019; Pasunuru Hsu et al., 2018), and sentiment classification et al., 2017; Gehrmann et al., 2018). Also, to our (Chan et al., 2020; Ma et al., 2018). While other knowledge, no prior work has tried to tackle multi- research has combined multiple tasks, Lu et al. task summarization in low-resource domains. (2019) integrated only predictive tasks, while Guo Our work attempts to address these gaps by an- et al. (2018) used only generative tasks. Recently, swering the following research questions: Q1) Can Dou and Neubig (2021) proposed using different abstractive summarization performance be boosted tasks as guiding signals. However, the guiding sigvia multitask learning when training from a small nals can only be used one signal at a time with dataset? Q2) Are there some tasks that might be no easy way to combine them. In contrast, our helpful and some that might be harmful for multi- work focuses on both generative and predictive task abstractive summarization? Q3) Will the same tasks, explores task utili"
2021.findings-emnlp.142,P18-1013,0,0.307211,"tasks with minimal fine-tuning (e.g., T5 (Raffel et al., 2019) and BART (Lewis et al., 2020)). How- Multitask learning. Abstractive summarization ever, in multitask learning for text summarization, has been enhanced in multitask learning frameit is still unclear what range of tasks can best sup- works with one additional task, by integrating it port summarization, and most prior work has in- with text entailment generation (Pasunuru et al., corporated only one additional task during training 2017), extractive summarization (Chen et al., 2019; (Isonuma et al., 2017; Chen et al., 2019; Pasunuru Hsu et al., 2018), and sentiment classification et al., 2017; Gehrmann et al., 2018). Also, to our (Chan et al., 2020; Ma et al., 2018). While other knowledge, no prior work has tried to tackle multi- research has combined multiple tasks, Lu et al. task summarization in low-resource domains. (2019) integrated only predictive tasks, while Guo Our work attempts to address these gaps by an- et al. (2018) used only generative tasks. Recently, swering the following research questions: Q1) Can Dou and Neubig (2021) proposed using different abstractive summarization performance be boosted tasks as guiding signals. Ho"
2021.findings-emnlp.142,D17-1223,0,0.0220587,"can be used in numerous downstream 2 Related Work tasks with minimal fine-tuning (e.g., T5 (Raffel et al., 2019) and BART (Lewis et al., 2020)). How- Multitask learning. Abstractive summarization ever, in multitask learning for text summarization, has been enhanced in multitask learning frameit is still unclear what range of tasks can best sup- works with one additional task, by integrating it port summarization, and most prior work has in- with text entailment generation (Pasunuru et al., corporated only one additional task during training 2017), extractive summarization (Chen et al., 2019; (Isonuma et al., 2017; Chen et al., 2019; Pasunuru Hsu et al., 2018), and sentiment classification et al., 2017; Gehrmann et al., 2018). Also, to our (Chan et al., 2020; Ma et al., 2018). While other knowledge, no prior work has tried to tackle multi- research has combined multiple tasks, Lu et al. task summarization in low-resource domains. (2019) integrated only predictive tasks, while Guo Our work attempts to address these gaps by an- et al. (2018) used only generative tasks. Recently, swering the following research questions: Q1) Can Dou and Neubig (2021) proposed using different abstractive summarization perf"
2021.findings-emnlp.142,2020.acl-main.703,0,0.0270292,"esults show that abstractive summarization in low resource domains can be improved via multitask training. We also find that certain auxiliary tasks such as paraphrase detection consistently improve abstractive summarization performance across different models and datasets, while other auxilary tasks like language modeling more often degrade model performance. Recent work has shown that training text encoders using data from multiple tasks helps to produce an encoder that can be used in numerous downstream 2 Related Work tasks with minimal fine-tuning (e.g., T5 (Raffel et al., 2019) and BART (Lewis et al., 2020)). How- Multitask learning. Abstractive summarization ever, in multitask learning for text summarization, has been enhanced in multitask learning frameit is still unclear what range of tasks can best sup- works with one additional task, by integrating it port summarization, and most prior work has in- with text entailment generation (Pasunuru et al., corporated only one additional task during training 2017), extractive summarization (Chen et al., 2019; (Isonuma et al., 2017; Chen et al., 2019; Pasunuru Hsu et al., 2018), and sentiment classification et al., 2017; Gehrmann et al., 2018). Also,"
2021.findings-emnlp.142,W04-1013,0,0.0269237,"Missing"
2021.findings-emnlp.142,D15-1227,1,0.890686,"Missing"
2021.findings-emnlp.142,K16-1028,0,0.0242923,"examined before in the context of multitask summarization, we introduce two new additional auxiliary tasks (paraphrase detection, concept detection)(Section 4.1). Finally, while previous work relied on large training corpora (e.g. CNN/DailyMail (Hermann et al., 2015)), we target low resource domains and try to overcome data scarceness by using the same data to train multiple task modules. Low resource training data. While most abstractive summarization work takes advantage of large corpora such as CNN/DailyMail, New York Times, PubMed, etc. to train models from scratch (Hermann et al., 2015; Nallapati et al., 2016; Cohan et al., 2018), recent work has also targeted low resource domains. Methods proposed to tackle little training data have included data synthesis (Parida and Motlicek, 2019; Magooda and Litman, 2020), few shot learning (Bražinskas et al., 2020), and pretraining (Yu et al., 2021). Our approach is different in that we use the same data multiple times in a multitask setting to boost performance. 3 Summarization Datasets CourseMirror (CM)1 is a student reflection dataset previously used to study both extractive (Luo and Litman, 2015) and abstractive (Magooda and Litman, 2020) summarization."
2021.findings-emnlp.142,W17-4504,0,0.0419742,"Missing"
2021.findings-emnlp.142,2020.findings-emnlp.217,0,0.0112746,"b.io/data.html 1653 https://github.com/abrazinskas/FewSum 4.2 BERT Multitask Integration3 We use a pretrained BERT (Devlin et al., 2019) model as a shared sequence encoder followed by a set of different task-specific modules (Figure 1). In the single task setting, only abstractive summarization is performed. In the multitask setting (integrating one or more auxiliary tasks), encoder weights are also fine-tuned alongside the rest of the model. Figure 1: Proposed BERT-Multitask model. Abstractive summarization (A). While recent work often uses transformers to overcome issues of sequence length (Qi et al., 2020), LSTM based decoders consistently outperform transformer-based ones when trained from scratch on our small CM dataset. Thus, we use LSTMs for our abstractive summarization (primary) task. Extractive summarization (E): The model consists of a linear layer to classify a sentence as part of the summary or not. Document and input sentence are fed to BERT encoder in the format [CLS] DW1 DW2 ...DWn [SEP]SW1 SW2 ....SWm , where DWi is the ith word of the input document, SWi is the ith word of the sentence to classify, and ([CLS], [SEP]) are respectively the starting and separation tokens used by BER"
2021.findings-emnlp.175,2020.emnlp-main.55,0,0.0601964,"Missing"
2021.findings-emnlp.175,2020.acl-main.703,0,0.0214803,"uce a simple approach for data synthesis through paraphrasing. 2) We use data augmentation by sample mixing to move augmentation into the model. 3) We integrate a curriculum into the training process and introduce two new difficulty metrics. 2 Related Work Abstractive summarization for low resource data. Prior proposed methods for tackling do1 Introduction mains with scarce data have included finetuning pre-trained models (Bajaj et al., 2021; Yu et al., Training complex neural models usually requires 2021; Magooda and Litman, 2020) such as BART large amounts of data. However, data annotation (Lewis et al., 2020) or using few-shot learning still poses a challenge for many domains. Thus, much research focuses on data manipulation (e.g., (Bražinskas et al., 2020; Sarkhel et al., 2020). Our work differs in several aspects. First, our work synthesis, augmentation) and additional ways to doesn’t focus on improving a certain summarizahandle data differently during training. Prior work tion model; in contrast, we focus on using data on the synthesis of textual data has focused on efficiently, which can be applied to various models. back translation (Parida and Motlicek, 2019; Wang et al., 2018; Sennrich et a"
2021.findings-emnlp.175,D19-1387,0,0.0278262,"specific pieces of text first during training, then 2045 3 https://github.com/GT-SALT/MixText Curriculum learning: In the curriculum learning experiments, we use a specificity prediction model that consists of a DistilBERT (Sanh et al., 2019) encoder with a logistic regression classification layer (Appendix). We normalize the whole training data values between 1 and N , where N is the number of buckets to split the data. We use N =10. Similarly, we normalize the average ROUGE value to also be between 1 and N . 5.2 Model Training In all of our experiments, we use the BERTSum4 model proposed by Liu and Lapata (2019). We used the same parameters in the original code (Appendix). We conducted experiments on CM and A/Y datasets using proposed methods in a regular training and in a (pretraining→fine-tuning) setting, where we perform pretraining with synthesized data and fine-tuneing using original data. A/Y finetuning R1 R2 No Pretraining Original 27.71 3.83 shuff. 28.34 4.04 shuff.+mask 28.01 4.21 None Cur.(S) 28.69 4.28 Cur.(R) 28.8 4.33 Mix(n=3) 27.85 3.95 With synthetic data pretraining Original 28.27 4.36 Synth.(n=5) Cur.(S) 27.95 4.4 Cur.(R) 28.56 4.25 Original 28.49 4.54 Synth.(n=10) Cur.(S) 28.52 4.5"
2021.findings-emnlp.175,2020.coling-main.606,0,0.0234116,"um into the training process and introduce two new difficulty metrics. 2 Related Work Abstractive summarization for low resource data. Prior proposed methods for tackling do1 Introduction mains with scarce data have included finetuning pre-trained models (Bajaj et al., 2021; Yu et al., Training complex neural models usually requires 2021; Magooda and Litman, 2020) such as BART large amounts of data. However, data annotation (Lewis et al., 2020) or using few-shot learning still poses a challenge for many domains. Thus, much research focuses on data manipulation (e.g., (Bražinskas et al., 2020; Sarkhel et al., 2020). Our work differs in several aspects. First, our work synthesis, augmentation) and additional ways to doesn’t focus on improving a certain summarizahandle data differently during training. Prior work tion model; in contrast, we focus on using data on the synthesis of textual data has focused on efficiently, which can be applied to various models. back translation (Parida and Motlicek, 2019; Wang et al., 2018; Sennrich et al., 2016) and word replace- Second, we focus on techniques that can improve ment (Wang and Yang, 2015; Zhang et al., 2015). the training process without additional data, e.g"
2021.findings-emnlp.175,P16-1009,0,0.0392125,"et al., 2020) or using few-shot learning still poses a challenge for many domains. Thus, much research focuses on data manipulation (e.g., (Bražinskas et al., 2020; Sarkhel et al., 2020). Our work differs in several aspects. First, our work synthesis, augmentation) and additional ways to doesn’t focus on improving a certain summarizahandle data differently during training. Prior work tion model; in contrast, we focus on using data on the synthesis of textual data has focused on efficiently, which can be applied to various models. back translation (Parida and Motlicek, 2019; Wang et al., 2018; Sennrich et al., 2016) and word replace- Second, we focus on techniques that can improve ment (Wang and Yang, 2015; Zhang et al., 2015). the training process without additional data, e.g., synthesis, augmentation, and curriculum learning. We, on the other hand, propose a different approach to data synthesis through paraphrasing. However, Data synthesis and augmentation. Data synsynthesis involves data manipulation on the input thesis for text summarization is underexplored, level, which might expose the model to grammati- with only a few approaches such as back-generation cally or logically incorrect input. Thus, w"
2021.findings-emnlp.175,P19-1486,0,0.0274578,"paraphrasing model. Our work, to our knowledge, is the first to use a strong language model finetuned for paraphrasing to synthesize data for text summarization. Finally, for data augmentation, we base our work on the MixText approach (Chen et al., 2020). While the original MixText model is used for classification-based tasks, we introduce a variation for generative tasks (called MixGen) and use it for abstractive summarization. Curriculum learning. Curriculum learning aims to improve the training procedure with the same amount of data. It has been applied in NLP (Sachan and Xing, 2016, 2018; Tay et al., 2019; Xu et al., 2020; Wang et al., 2020) for machine comprehension, question generation, reading comprehension, NLU and machine translation, respectively. We build on the approach introduced in (Xu et al., 2020); however, the core differences are both the downstream tasks (classification versus abstractive summarization) and the difficulty metrics. In contrast to the only other summarization work that we know of, Kano et al. (2021) focus on large datasets, while we focus on low resource domains. We also introduce two different difficulty metrics (ROUGE and specificity). 3 Summarization Datasets C"
2021.findings-emnlp.175,2020.acl-main.689,0,0.0386785,"r knowledge, is the first to use a strong language model finetuned for paraphrasing to synthesize data for text summarization. Finally, for data augmentation, we base our work on the MixText approach (Chen et al., 2020). While the original MixText model is used for classification-based tasks, we introduce a variation for generative tasks (called MixGen) and use it for abstractive summarization. Curriculum learning. Curriculum learning aims to improve the training procedure with the same amount of data. It has been applied in NLP (Sachan and Xing, 2016, 2018; Tay et al., 2019; Xu et al., 2020; Wang et al., 2020) for machine comprehension, question generation, reading comprehension, NLU and machine translation, respectively. We build on the approach introduced in (Xu et al., 2020); however, the core differences are both the downstream tasks (classification versus abstractive summarization) and the difficulty metrics. In contrast to the only other summarization work that we know of, Kano et al. (2021) focus on large datasets, while we focus on low resource domains. We also introduce two different difficulty metrics (ROUGE and specificity). 3 Summarization Datasets CourseMirror (CM) is a student reflect"
2021.findings-emnlp.175,D15-1306,0,0.0129008,"esearch focuses on data manipulation (e.g., (Bražinskas et al., 2020; Sarkhel et al., 2020). Our work differs in several aspects. First, our work synthesis, augmentation) and additional ways to doesn’t focus on improving a certain summarizahandle data differently during training. Prior work tion model; in contrast, we focus on using data on the synthesis of textual data has focused on efficiently, which can be applied to various models. back translation (Parida and Motlicek, 2019; Wang et al., 2018; Sennrich et al., 2016) and word replace- Second, we focus on techniques that can improve ment (Wang and Yang, 2015; Zhang et al., 2015). the training process without additional data, e.g., synthesis, augmentation, and curriculum learning. We, on the other hand, propose a different approach to data synthesis through paraphrasing. However, Data synthesis and augmentation. Data synsynthesis involves data manipulation on the input thesis for text summarization is underexplored, level, which might expose the model to grammati- with only a few approaches such as back-generation cally or logically incorrect input. Thus, we explore (Parida and Motlicek, 2019) and template-based a second approach to data manipulat"
2021.findings-emnlp.175,D18-1100,0,0.0174936,"annotation (Lewis et al., 2020) or using few-shot learning still poses a challenge for many domains. Thus, much research focuses on data manipulation (e.g., (Bražinskas et al., 2020; Sarkhel et al., 2020). Our work differs in several aspects. First, our work synthesis, augmentation) and additional ways to doesn’t focus on improving a certain summarizahandle data differently during training. Prior work tion model; in contrast, we focus on using data on the synthesis of textual data has focused on efficiently, which can be applied to various models. back translation (Parida and Motlicek, 2019; Wang et al., 2018; Sennrich et al., 2016) and word replace- Second, we focus on techniques that can improve ment (Wang and Yang, 2015; Zhang et al., 2015). the training process without additional data, e.g., synthesis, augmentation, and curriculum learning. We, on the other hand, propose a different approach to data synthesis through paraphrasing. However, Data synthesis and augmentation. Data synsynthesis involves data manipulation on the input thesis for text summarization is underexplored, level, which might expose the model to grammati- with only a few approaches such as back-generation cally or logically"
2021.findings-emnlp.175,P15-1129,0,0.0664819,"Missing"
2021.findings-emnlp.175,P18-1042,0,0.0185265,"McAuley, 2016) and Yelp. The data contains 160 products/businesses split into training, validation and test sets as shown in Table 1. Each of the products/businesses contains a set of 8 reviews. 4 Proposed Model 4.1 Synthesis via paraphrasing with GPT-2 Influenced by work in style transfer (Krishna et al., 2020), we propose synthesizing new human summaries by using paraphrasing to generate other potential summaries that are paraphrases of the original human summary. We use the paraphraser trained by Krishna et al. (2020). They finetuned a large GPT-2 language model with data from PARANMT-50M (Wieting and Gimpel, 2018) to direct the model into generating diverse paraphrases that they later used for style transfer. 4.2 Augmentation with sample mixing MixText is a data augmentation approach based on mixing two input samples by weight summing the features corresponding to the two samples at any level of the model (specific layer of the encoder, after encoder, etc.) using λ. The model is then expected to produce a probability distribution over the available classes, similar to a λ weighted sum of the two samples’ gold predictions. We train the model using KL divergence between the predicted distribution and the"
2021.findings-emnlp.175,2020.acl-main.542,0,0.0236923,". Our work, to our knowledge, is the first to use a strong language model finetuned for paraphrasing to synthesize data for text summarization. Finally, for data augmentation, we base our work on the MixText approach (Chen et al., 2020). While the original MixText model is used for classification-based tasks, we introduce a variation for generative tasks (called MixGen) and use it for abstractive summarization. Curriculum learning. Curriculum learning aims to improve the training procedure with the same amount of data. It has been applied in NLP (Sachan and Xing, 2016, 2018; Tay et al., 2019; Xu et al., 2020; Wang et al., 2020) for machine comprehension, question generation, reading comprehension, NLU and machine translation, respectively. We build on the approach introduced in (Xu et al., 2020); however, the core differences are both the downstream tasks (classification versus abstractive summarization) and the difficulty metrics. In contrast to the only other summarization work that we know of, Kano et al. (2021) focus on large datasets, while we focus on low resource domains. We also introduce two different difficulty metrics (ROUGE and specificity). 3 Summarization Datasets CourseMirror (CM)"
2021.findings-emnlp.175,2021.naacl-main.471,0,0.0834956,"Missing"
A00-2028,P99-1025,1,0.895035,"s, plus a category called other for calls that cannot be automated and must be transferred to a human operator (Gorin et al., 1997). ~ Each category describes a different task, such as person-to-person dialing, or receiving credit for a misdialed number. The system determines which task the caller is requesting on the basis of its understanding of the cMler&apos;s response to the open-ended system greeting A T ~ T, How May I Help You?. Once the task has been determined, the information needed for completing the caller&apos;s request is obtained using dialogue submodules that are specific for each task (Abella and Gorin, 1999). In addition to the TASKSUCCESS dialogues in which HMIHY successfully automates the customer&apos;s call, illustrated in Figure 1, and the calls that are transferred to a human operator, there are three other possible outcomes for a call, all of which are problematic. The first category, which we call HANGUP, results from a customer&apos;s decision to hang up on the system. A sample HANGUP dialogue is in Figure 2. A caller may hang up because s/he is frustrated with the system; our goal is to learn from the corpus which system behaviors led to the caller&apos;s frustration. The second problematic category ("
A00-2028,J99-3003,0,0.0317013,"Missing"
A00-2028,P98-1122,0,0.0690774,"Missing"
A00-2028,P99-1040,1,0.829966,"Missing"
A00-2028,H92-1009,0,0.0372315,"Missing"
A00-2028,P98-2219,1,0.850525,"Missing"
A00-2028,C98-1117,0,\N,Missing
A00-2028,C98-2214,1,\N,Missing
A00-2029,P98-1122,0,0.19038,"e morning&quot;). The current paper looks at prosody as one possible predictor of ASR performance. ASR performance is known to vary based upon speaking style (Weintraub et al., 1996), speaker gender and age, native versus non-native speaker status, and, in general, the deviation of new speech from the training data. Some of this variation is linked to prosody, as prosodic differences have been found to characterize differences in speaking style (Blaauw, 1992) and idiosyncratic differences (Kraayeveld, 1997). Several other studies (Wade et al., 1992; Oviatt et al., 1996; Swerts and Ostendorf, 1997; Levow, 1998; Bell and Gustafson, 1999) report that hyperarticulated speech, characterized by careful enunciation, slowed speaking rate, and increase in pitch and loudness, often occurs when users in human-machine interactions try to correct system errors. Still others have shown that such speech also decreases recognition performance (Soltau and Waibel, 1998). Prosodic features have also been shown to be effective in ranking recognition hypotheses, as a post-processing filter to score ASR hypotheses (Hirschberg, 1991; Veilleux, 1994; Hirose, 1997). In this paper we present results of empirical studies te"
A00-2029,P99-1024,0,0.0840224,"Missing"
A00-2029,P99-1040,1,0.858751,"Missing"
A00-2029,C98-1117,0,\N,Missing
C00-1073,P98-2219,1,0.809491,"Missing"
C00-1073,C98-2214,1,\N,Missing
C14-1187,E06-1039,0,0.625793,"epresentativeness. In comparison, while our approach follows the same extractive summarization paradigm, it is metadata driven, identifying important text units through the guidance of user-provided review helpfulness assessment. When it comes to online reviews, the desired characteristics of a review summary are different from traditional text genres (e.g., news articles), and could vary from one review domain to another. Thus different review summarizers have been proposed to focus on different desired properties of review summaries, primarily based on opinion mining and sentiment analysis (Carenini et al., 2006; Lerman et al., 2009; Lerman and McDonald, 2009; Kim and Zhai, 2009). Here the desired property varies from the coverage of product aspects (Carenini et al., 2006; Lerman et al., 2009) to the degree of agreement on aspect-specific sentiment (Lerman et al., 2009; Lerman and McDonald, 2009; Kim and Zhai, 2009). While there is a large overlap between text summarization and review opinion mining, most work focuses on sentiment-oriented aspect extraction and the output is usually a set of topics words plus their representative text units (Hu and Liu, 2004; Zhuang et al., 2006). However, such a top"
C14-1187,W09-1802,0,0.280427,"of review metadata as supervision. The presented work also extrinsically demonstrates that the helpfulness-related topics learned from the review-level supervision can capture review helpfulness at the sentence-level. 2 Related Work In multi-document extractive summarization, various unsupervised approaches have been proposed to avoid manual annotation. A key task in extractive summarization is to identify important text units. Prior successful extractive summarizers score a sentence based on n-grams within the sentence: by the word frequency (Nenkova and Vanderwende, 2005), bigram coverage (Gillick and Favre, 2009), topic signatures (Lin and Hovy, 2000) or latent topic distribution of the sentence (Haghighi and Vanderwende, 2009), which all aim to capture the “core” content of the text input. Other approaches regard the ngram distribution difference (e.g., Kullback-Lieber (KL) divergence) between the input documents and the summary (Lin et al., 2006), or based on a graph-representation of the document content (Erkan and Radev, 2004; Leskovec13 et al., 2005), with an implicit goal to maximize the output representativeness. In comparison, while our approach follows the same extractive summarization paradi"
C14-1187,N09-1041,0,0.050892,"lated topics learned from the review-level supervision can capture review helpfulness at the sentence-level. 2 Related Work In multi-document extractive summarization, various unsupervised approaches have been proposed to avoid manual annotation. A key task in extractive summarization is to identify important text units. Prior successful extractive summarizers score a sentence based on n-grams within the sentence: by the word frequency (Nenkova and Vanderwende, 2005), bigram coverage (Gillick and Favre, 2009), topic signatures (Lin and Hovy, 2000) or latent topic distribution of the sentence (Haghighi and Vanderwende, 2009), which all aim to capture the “core” content of the text input. Other approaches regard the ngram distribution difference (e.g., Kullback-Lieber (KL) divergence) between the input documents and the summary (Lin et al., 2006), or based on a graph-representation of the document content (Erkan and Radev, 2004; Leskovec13 et al., 2005), with an implicit goal to maximize the output representativeness. In comparison, while our approach follows the same extractive summarization paradigm, it is metadata driven, identifying important text units through the guidance of user-provided review helpfulness"
C14-1187,W06-1650,0,0.551886,"atures based on the inferred hidden topics from sLDA to capture the helpfulness of a review sentence for summarization purposes. We implement our helpfulness-guided review summarizers based on an widely used open-source multi-document extractive summarization framework (MEAD (Radev et al., 2004)). Both human and This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1 2 This is the percentage of readers who found the review to be helpful (Kim et al., 2006). If it is not available, the review helpfulness can be assessed fully automatically (Kim et al., 2006; Liu et al., 2008). 1985 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1985–1995, Dublin, Ireland, August 23-29 2014. automated evaluations show that our helpfulness-guided summarizers outperform a strong baseline that MEAD provides across multiple review domains. Further analysis on the human summaries shows that some effective heuristics proposed for traditional genres might not work well for online reviews, which indirec"
C14-1187,N09-2029,0,0.0805579,"approach follows the same extractive summarization paradigm, it is metadata driven, identifying important text units through the guidance of user-provided review helpfulness assessment. When it comes to online reviews, the desired characteristics of a review summary are different from traditional text genres (e.g., news articles), and could vary from one review domain to another. Thus different review summarizers have been proposed to focus on different desired properties of review summaries, primarily based on opinion mining and sentiment analysis (Carenini et al., 2006; Lerman et al., 2009; Lerman and McDonald, 2009; Kim and Zhai, 2009). Here the desired property varies from the coverage of product aspects (Carenini et al., 2006; Lerman et al., 2009) to the degree of agreement on aspect-specific sentiment (Lerman et al., 2009; Lerman and McDonald, 2009; Kim and Zhai, 2009). While there is a large overlap between text summarization and review opinion mining, most work focuses on sentiment-oriented aspect extraction and the output is usually a set of topics words plus their representative text units (Hu and Liu, 2004; Zhuang et al., 2006). However, such a topic-based summarization framework is beyond the f"
C14-1187,N09-2000,0,0.232745,"Missing"
C14-1187,E09-1059,0,0.781978,"omparison, while our approach follows the same extractive summarization paradigm, it is metadata driven, identifying important text units through the guidance of user-provided review helpfulness assessment. When it comes to online reviews, the desired characteristics of a review summary are different from traditional text genres (e.g., news articles), and could vary from one review domain to another. Thus different review summarizers have been proposed to focus on different desired properties of review summaries, primarily based on opinion mining and sentiment analysis (Carenini et al., 2006; Lerman et al., 2009; Lerman and McDonald, 2009; Kim and Zhai, 2009). Here the desired property varies from the coverage of product aspects (Carenini et al., 2006; Lerman et al., 2009) to the degree of agreement on aspect-specific sentiment (Lerman et al., 2009; Lerman and McDonald, 2009; Kim and Zhai, 2009). While there is a large overlap between text summarization and review opinion mining, most work focuses on sentiment-oriented aspect extraction and the output is usually a set of topics words plus their representative text units (Hu and Liu, 2004; Zhuang et al., 2006). However, such a topic-based summarizatio"
C14-1187,C00-1072,0,0.212434,"ented work also extrinsically demonstrates that the helpfulness-related topics learned from the review-level supervision can capture review helpfulness at the sentence-level. 2 Related Work In multi-document extractive summarization, various unsupervised approaches have been proposed to avoid manual annotation. A key task in extractive summarization is to identify important text units. Prior successful extractive summarizers score a sentence based on n-grams within the sentence: by the word frequency (Nenkova and Vanderwende, 2005), bigram coverage (Gillick and Favre, 2009), topic signatures (Lin and Hovy, 2000) or latent topic distribution of the sentence (Haghighi and Vanderwende, 2009), which all aim to capture the “core” content of the text input. Other approaches regard the ngram distribution difference (e.g., Kullback-Lieber (KL) divergence) between the input documents and the summary (Lin et al., 2006), or based on a graph-representation of the document content (Erkan and Radev, 2004; Leskovec13 et al., 2005), with an implicit goal to maximize the output representativeness. In comparison, while our approach follows the same extractive summarization paradigm, it is metadata driven, identifying"
C14-1187,N06-1059,0,0.35646,"tation. A key task in extractive summarization is to identify important text units. Prior successful extractive summarizers score a sentence based on n-grams within the sentence: by the word frequency (Nenkova and Vanderwende, 2005), bigram coverage (Gillick and Favre, 2009), topic signatures (Lin and Hovy, 2000) or latent topic distribution of the sentence (Haghighi and Vanderwende, 2009), which all aim to capture the “core” content of the text input. Other approaches regard the ngram distribution difference (e.g., Kullback-Lieber (KL) divergence) between the input documents and the summary (Lin et al., 2006), or based on a graph-representation of the document content (Erkan and Radev, 2004; Leskovec13 et al., 2005), with an implicit goal to maximize the output representativeness. In comparison, while our approach follows the same extractive summarization paradigm, it is metadata driven, identifying important text units through the guidance of user-provided review helpfulness assessment. When it comes to online reviews, the desired characteristics of a review summary are different from traditional text genres (e.g., news articles), and could vary from one review domain to another. Thus different r"
C14-1187,W04-1013,0,0.0108339,"Missing"
C14-1187,D07-1035,0,0.457062,"introducing review helpfulness ratings as guidance. In this paper, we utilize review helpfulness via using sLDA. The idea of using sLDA in text summarization is not new. However, the model is previously applied at the sentence level (Li and Li, 2012), which requires human annotation on the sentence importance. In comparison, our use of sLDA is at the document (review) level, using existing metadata of the document (review helpfulness ratings) as the supervision, and thus requiring no annotation at all. With respect to the use of review helpfulness ratings, early work of review summarization (Liu et al., 2007) only consider it as a filtering criteria during input preprocessing. Other researchers use it as the gold-standard for automated review helpfulness prediction, a predictor of product sales (Ghose and Ipeirotis, 2011), a measurement of reviewers’ authority in social network analysis (Lu et al., 2010), etc. 3 Helpfulness features for sentence scoring While the most straightforward way to utilize review helpfulness for content selection is through filtering (Liu et al., 2007) (further discussed in Section 4.3), we also propose to take into account review helpfulness during sentence scoring by le"
C14-1187,W11-1611,0,0.0814029,"Missing"
C14-1187,radev-etal-2004-mead,0,0.110141,"ing sentence-level helpfulness features derived from review-level helpfulness ratings for sentence scoring. As we observe in our pilot study that supervised LDA (sLDA) (Blei and McAuliffe, 2010) trained with review helpfulness ratings has potential in differentiating review helpfulness at the sentence level, we develop features based on the inferred hidden topics from sLDA to capture the helpfulness of a review sentence for summarization purposes. We implement our helpfulness-guided review summarizers based on an widely used open-source multi-document extractive summarization framework (MEAD (Radev et al., 2004)). Both human and This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1 2 This is the percentage of readers who found the review to be helpful (Kim et al., 2006). If it is not available, the review helpfulness can be assessed fully automatically (Kim et al., 2006; Liu et al., 2008). 1985 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1985–1995, Dublin, Ireland, August"
C14-1187,N12-1064,0,0.0573292,"in every review. zl = k means that the topic ID for word at position l in sentence s is k. Given the topic assignments z1:L to words w1:L in a review sentence s, we estimate the contribution of s to the helpfulness of the review it belongs to (Formula 1), as well as the average topic importance in s (Formula 2). While rHelpSum is sensitive to the review length, sHelpSum is sensitive to the sentence length. l=L 1 XX rHelpSum(s) = ηk p(zl = k) N l=1 (1) k l=L sHelpSum(s) = 1 XX ηk p(zl = k) L l=1 (2) k As the topic assignment in each inference iteration might not be the same, Riedl and Biemann (Riedl and Biemann, 2012) proposed the mode method in their application of LDA for text segmentation – use the most frequently assigned topic for each word in all iterations as the final topic assignment – to address the instability issue. Inspired by their idea, we also use the mode method to infer the topic assignment in our task, but only apply the mode method to the last 10 iterations, because the topic distribution might not be well learned at the beginning. 4 Experimental setup To investigate the utility of exploiting user-provided review helpfulness ratings for content selection in extractive summarization, we"
C14-1187,Q13-1008,0,\N,Missing
C16-1006,W16-3605,0,0.0518934,"Missing"
C16-1006,Q13-1032,0,0.179007,"e. We have only demonstrated the highlights of Human Summary 1 to avoid overlaying of two sets of colors on student responses. The superscripts of the phrase highlights are imposed by the authors of this paper to differentiate colors when printed in grayscale (y: yellow , g: green , r: red , b: blue , and m: magenta ). setting. Lastly, a greedy clustering algorithm K-medoids (Kaufman and Rousseeuw, 1987) was previously used to group candidate phrases. It ignores global information and may suffer from a “collapsing” effect, which leads to the generation of a large cluster with unrelated items (Basu et al., 2013). The goal of this work is to explore a phrase-based highlighting scheme, which is new to the summarization task. We aim to improve the phrase summarization framework by exploiting new capabilities that are enabled by the highlighting scheme. In the new scheme, human annotators are instructed to 1) create summary phrases from the student responses, 2) associate a number with each summary phrase which indicates the number of students who raise the issue (henceforth student supporters), and 3) highlight the corresponding phrases in both the human summary and student responses. Table 1 illustrate"
C16-1006,P11-1049,0,0.0163738,"ty detection to group phrases into clusters. • We conduct comprehensive evaluations in terms of both summary text quality, measured by ROUGE (Lin, 2004), and how well phrase summaries capture the most pressing student needs, measured by a new evaluation metric based on color matching. 1 This data set is publicly available at http://www.coursemirror.com/download/dataset2 54 2 Related Work Work on automatic text summarization involves multiple granularities, ranging from keywords, phrases, to sentences. Traditional approaches have largely focused on sentence extraction (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Li et al., 2013) and document abstraction (Liu et al., 2015; Rush et al., 2015; Durrett et al., 2016; Nallapati et al., 2016). In both cases, the produced summary is expected to be cohesive and coherent. We deviate from this path and seek to directly generate a set of bullet points as a summary. Phrases are easy to search and browse like words but more meaningful, and fit better on the small screen of a mobile device compared to sentences (Ueda et al., 2000; Luo et al., 2015). Our task setting differs from those of keyphrase extraction (Wu et al., 2005; Liu et al., 2009; Medelyan et al., 200"
C16-1006,W14-5909,0,0.0249645,"Missing"
C16-1006,stefanescu-etal-2014-latent,0,0.0350288,"Missing"
C16-1006,P16-1188,0,0.0153082,"xt quality, measured by ROUGE (Lin, 2004), and how well phrase summaries capture the most pressing student needs, measured by a new evaluation metric based on color matching. 1 This data set is publicly available at http://www.coursemirror.com/download/dataset2 54 2 Related Work Work on automatic text summarization involves multiple granularities, ranging from keywords, phrases, to sentences. Traditional approaches have largely focused on sentence extraction (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Li et al., 2013) and document abstraction (Liu et al., 2015; Rush et al., 2015; Durrett et al., 2016; Nallapati et al., 2016). In both cases, the produced summary is expected to be cohesive and coherent. We deviate from this path and seek to directly generate a set of bullet points as a summary. Phrases are easy to search and browse like words but more meaningful, and fit better on the small screen of a mobile device compared to sentences (Ueda et al., 2000; Luo et al., 2015). Our task setting differs from those of keyphrase extraction (Wu et al., 2005; Liu et al., 2009; Medelyan et al., 2009; Hasan and Ng, 2014; Kan, 2015). Of key importance is that each summary phrase is associated with a"
C16-1006,P14-1119,0,0.0137364,"i et al., 2013) and document abstraction (Liu et al., 2015; Rush et al., 2015; Durrett et al., 2016; Nallapati et al., 2016). In both cases, the produced summary is expected to be cohesive and coherent. We deviate from this path and seek to directly generate a set of bullet points as a summary. Phrases are easy to search and browse like words but more meaningful, and fit better on the small screen of a mobile device compared to sentences (Ueda et al., 2000; Luo et al., 2015). Our task setting differs from those of keyphrase extraction (Wu et al., 2005; Liu et al., 2009; Medelyan et al., 2009; Hasan and Ng, 2014; Kan, 2015). Of key importance is that each summary phrase is associated with a numerical value, indicating the number of students who raise the issue. This information is critical to course instructors for making informed choices. Intuitively our task setting bears similarity to word/phrase cloud (Yatani et al., 2011; Brooks et al., 2014), where the cloud gives greater prominence to words or phrases that appear frequently in the source text. The downside is that they do not take lexical variety into account or considering semantically-equivalent words/phrases. A summarization system is expec"
C16-1006,W11-1104,0,0.0233764,"Missing"
C16-1006,W15-3601,0,0.0244842,"document abstraction (Liu et al., 2015; Rush et al., 2015; Durrett et al., 2016; Nallapati et al., 2016). In both cases, the produced summary is expected to be cohesive and coherent. We deviate from this path and seek to directly generate a set of bullet points as a summary. Phrases are easy to search and browse like words but more meaningful, and fit better on the small screen of a mobile device compared to sentences (Ueda et al., 2000; Luo et al., 2015). Our task setting differs from those of keyphrase extraction (Wu et al., 2005; Liu et al., 2009; Medelyan et al., 2009; Hasan and Ng, 2014; Kan, 2015). Of key importance is that each summary phrase is associated with a numerical value, indicating the number of students who raise the issue. This information is critical to course instructors for making informed choices. Intuitively our task setting bears similarity to word/phrase cloud (Yatani et al., 2011; Brooks et al., 2014), where the cloud gives greater prominence to words or phrases that appear frequently in the source text. The downside is that they do not take lexical variety into account or considering semantically-equivalent words/phrases. A summarization system is expected to produ"
C16-1006,P10-1052,0,0.0276573,"Missing"
C16-1006,D13-1047,1,0.854627,"nto clusters. • We conduct comprehensive evaluations in terms of both summary text quality, measured by ROUGE (Lin, 2004), and how well phrase summaries capture the most pressing student needs, measured by a new evaluation metric based on color matching. 1 This data set is publicly available at http://www.coursemirror.com/download/dataset2 54 2 Related Work Work on automatic text summarization involves multiple granularities, ranging from keywords, phrases, to sentences. Traditional approaches have largely focused on sentence extraction (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Li et al., 2013) and document abstraction (Liu et al., 2015; Rush et al., 2015; Durrett et al., 2016; Nallapati et al., 2016). In both cases, the produced summary is expected to be cohesive and coherent. We deviate from this path and seek to directly generate a set of bullet points as a summary. Phrases are easy to search and browse like words but more meaningful, and fit better on the small screen of a mobile device compared to sentences (Ueda et al., 2000; Luo et al., 2015). Our task setting differs from those of keyphrase extraction (Wu et al., 2005; Liu et al., 2009; Medelyan et al., 2009; Hasan and Ng, 2"
C16-1006,W04-1013,0,0.190731,"utomatic summarization, a departure from prior work. It highlights the phrases in the human summary and also the semantically similar phrases in student responses. We create a new dataset annotated with this highlighting scheme1 . • We push the boundary of a phrase-based summarization framework by using our highlighting scheme to enable identification of candidate phrases as well as estimation of phrase similarities with supervision, and by using community detection to group phrases into clusters. • We conduct comprehensive evaluations in terms of both summary text quality, measured by ROUGE (Lin, 2004), and how well phrase summaries capture the most pressing student needs, measured by a new evaluation metric based on color matching. 1 This data set is publicly available at http://www.coursemirror.com/download/dataset2 54 2 Related Work Work on automatic text summarization involves multiple granularities, ranging from keywords, phrases, to sentences. Traditional approaches have largely focused on sentence extraction (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Li et al., 2013) and document abstraction (Liu et al., 2015; Rush et al., 2015; Durrett et al., 2016; Nallapati et al., 2"
C16-1006,D09-1027,0,0.0351284,"h, 2009; Berg-Kirkpatrick et al., 2011; Li et al., 2013) and document abstraction (Liu et al., 2015; Rush et al., 2015; Durrett et al., 2016; Nallapati et al., 2016). In both cases, the produced summary is expected to be cohesive and coherent. We deviate from this path and seek to directly generate a set of bullet points as a summary. Phrases are easy to search and browse like words but more meaningful, and fit better on the small screen of a mobile device compared to sentences (Ueda et al., 2000; Luo et al., 2015). Our task setting differs from those of keyphrase extraction (Wu et al., 2005; Liu et al., 2009; Medelyan et al., 2009; Hasan and Ng, 2014; Kan, 2015). Of key importance is that each summary phrase is associated with a numerical value, indicating the number of students who raise the issue. This information is critical to course instructors for making informed choices. Intuitively our task setting bears similarity to word/phrase cloud (Yatani et al., 2011; Brooks et al., 2014), where the cloud gives greater prominence to words or phrases that appear frequently in the source text. The downside is that they do not take lexical variety into account or considering semantically-equivalent wor"
C16-1006,N15-1114,1,0.809757,"aluations in terms of both summary text quality, measured by ROUGE (Lin, 2004), and how well phrase summaries capture the most pressing student needs, measured by a new evaluation metric based on color matching. 1 This data set is publicly available at http://www.coursemirror.com/download/dataset2 54 2 Related Work Work on automatic text summarization involves multiple granularities, ranging from keywords, phrases, to sentences. Traditional approaches have largely focused on sentence extraction (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Li et al., 2013) and document abstraction (Liu et al., 2015; Rush et al., 2015; Durrett et al., 2016; Nallapati et al., 2016). In both cases, the produced summary is expected to be cohesive and coherent. We deviate from this path and seek to directly generate a set of bullet points as a summary. Phrases are easy to search and browse like words but more meaningful, and fit better on the small screen of a mobile device compared to sentences (Ueda et al., 2000; Luo et al., 2015). Our task setting differs from those of keyphrase extraction (Wu et al., 2005; Liu et al., 2009; Medelyan et al., 2009; Hasan and Ng, 2014; Kan, 2015). Of key importance is that"
C16-1006,loza-etal-2014-building,0,0.0247957,"om student responses, however there lacks a comprehensive evaluation of the results, taking the number of student supporters into account. Other related work on student responses includes collecting student responses using a mobile application named CourseMIRROR (Luo et al., 2015; Fan et al., 2015), determining the quality of a student reflective response and providing feedback (Luo and Litman, 2016), and extracting informative sentences from the student feedback (Luo et al., 2016). Traditional approaches to summary annotation have been based on either sentence extracts or document abstracts (Loza et al., 2014; Xiong and Litman, 2014; Wang and Ling, 2016). An effective linkage between the document content and human summary on the micro level have been largely absent. Barker et al.(2016) partially address this challenge by linking a summary back to a group of sentences that support the summary. However, this linkage is weak since it tells only that there is one sentence or more supporting the summary within the group, without explicitly telling which one(s). Approaches such as Pyramid (Nenkova and Passonneau, 2004) have exploited creating Summary Content Units (SCUs) to establish such links and alle"
C16-1006,D15-1227,1,0.744514,"ack submitted by students after each lecture in response to two reflective prompts (Boud et al., 2013): 1) “Describe what you found most interesting in today’s class” and 2) “Describe what was confusing or needed more detail.” Education researchers have demonstrated that asking students to respond to reflection prompts can improve both teaching and learning (Van den Boom et al., 2004; Menekse et al., 2011). However, summarizing these responses for large classes (e.g., introductory STEM, MOOCs) remains costly, time-consuming, and an onerous task for humans (Mosteller, 1989). In our prior work, Luo and Litman (2015) (henceforth L&L) introduced the task of automatic summarization of student responses. The challenges of this task include 1) high lexical variety, because students tend to use different word expressions to communicate the same or similar meanings (e.g., “bike elements” vs. “bicycle parts”), and 2) high length variety, as the student responses range from a single word to multiple sentences. To tackle the challenges, L&L proposed a phrase summarization framework consisting of three stages: phrase extraction, phrase clustering, and phrase ranking. The approach extracts noun phrases from student"
C16-1006,N15-3004,1,0.644177,"o sentences. Traditional approaches have largely focused on sentence extraction (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Li et al., 2013) and document abstraction (Liu et al., 2015; Rush et al., 2015; Durrett et al., 2016; Nallapati et al., 2016). In both cases, the produced summary is expected to be cohesive and coherent. We deviate from this path and seek to directly generate a set of bullet points as a summary. Phrases are easy to search and browse like words but more meaningful, and fit better on the small screen of a mobile device compared to sentences (Ueda et al., 2000; Luo et al., 2015). Our task setting differs from those of keyphrase extraction (Wu et al., 2005; Liu et al., 2009; Medelyan et al., 2009; Hasan and Ng, 2014; Kan, 2015). Of key importance is that each summary phrase is associated with a numerical value, indicating the number of students who raise the issue. This information is critical to course instructors for making informed choices. Intuitively our task setting bears similarity to word/phrase cloud (Yatani et al., 2011; Brooks et al., 2014), where the cloud gives greater prominence to words or phrases that appear frequently in the source text. The downside"
C16-1006,N16-1010,1,0.836379,"d accurate estimates of the number of student supporters for each phrase. Luo and Litman (2015) focus on extracting noun phrases from student responses, however there lacks a comprehensive evaluation of the results, taking the number of student supporters into account. Other related work on student responses includes collecting student responses using a mobile application named CourseMIRROR (Luo et al., 2015; Fan et al., 2015), determining the quality of a student reflective response and providing feedback (Luo and Litman, 2016), and extracting informative sentences from the student feedback (Luo et al., 2016). Traditional approaches to summary annotation have been based on either sentence extracts or document abstracts (Loza et al., 2014; Xiong and Litman, 2014; Wang and Ling, 2016). An effective linkage between the document content and human summary on the micro level have been largely absent. Barker et al.(2016) partially address this challenge by linking a summary back to a group of sentences that support the summary. However, this linkage is weak since it tells only that there is one sentence or more supporting the summary within the group, without explicitly telling which one(s). Approaches s"
C16-1006,W09-1801,0,0.0249462,"ion, and by using community detection to group phrases into clusters. • We conduct comprehensive evaluations in terms of both summary text quality, measured by ROUGE (Lin, 2004), and how well phrase summaries capture the most pressing student needs, measured by a new evaluation metric based on color matching. 1 This data set is publicly available at http://www.coursemirror.com/download/dataset2 54 2 Related Work Work on automatic text summarization involves multiple granularities, ranging from keywords, phrases, to sentences. Traditional approaches have largely focused on sentence extraction (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Li et al., 2013) and document abstraction (Liu et al., 2015; Rush et al., 2015; Durrett et al., 2016; Nallapati et al., 2016). In both cases, the produced summary is expected to be cohesive and coherent. We deviate from this path and seek to directly generate a set of bullet points as a summary. Phrases are easy to search and browse like words but more meaningful, and fit better on the small screen of a mobile device compared to sentences (Ueda et al., 2000; Luo et al., 2015). Our task setting differs from those of keyphrase extraction (Wu et al., 2005; Liu et"
C16-1006,D09-1137,0,0.0269337,"patrick et al., 2011; Li et al., 2013) and document abstraction (Liu et al., 2015; Rush et al., 2015; Durrett et al., 2016; Nallapati et al., 2016). In both cases, the produced summary is expected to be cohesive and coherent. We deviate from this path and seek to directly generate a set of bullet points as a summary. Phrases are easy to search and browse like words but more meaningful, and fit better on the small screen of a mobile device compared to sentences (Ueda et al., 2000; Luo et al., 2015). Our task setting differs from those of keyphrase extraction (Wu et al., 2005; Liu et al., 2009; Medelyan et al., 2009; Hasan and Ng, 2014; Kan, 2015). Of key importance is that each summary phrase is associated with a numerical value, indicating the number of students who raise the issue. This information is critical to course instructors for making informed choices. Intuitively our task setting bears similarity to word/phrase cloud (Yatani et al., 2011; Brooks et al., 2014), where the cloud gives greater prominence to words or phrases that appear frequently in the source text. The downside is that they do not take lexical variety into account or considering semantically-equivalent words/phrases. A summariza"
C16-1006,W13-2117,0,0.0366518,"Missing"
C16-1006,K16-1028,0,0.0402006,"y ROUGE (Lin, 2004), and how well phrase summaries capture the most pressing student needs, measured by a new evaluation metric based on color matching. 1 This data set is publicly available at http://www.coursemirror.com/download/dataset2 54 2 Related Work Work on automatic text summarization involves multiple granularities, ranging from keywords, phrases, to sentences. Traditional approaches have largely focused on sentence extraction (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Li et al., 2013) and document abstraction (Liu et al., 2015; Rush et al., 2015; Durrett et al., 2016; Nallapati et al., 2016). In both cases, the produced summary is expected to be cohesive and coherent. We deviate from this path and seek to directly generate a set of bullet points as a summary. Phrases are easy to search and browse like words but more meaningful, and fit better on the small screen of a mobile device compared to sentences (Ueda et al., 2000; Luo et al., 2015). Our task setting differs from those of keyphrase extraction (Wu et al., 2005; Liu et al., 2009; Medelyan et al., 2009; Hasan and Ng, 2014; Kan, 2015). Of key importance is that each summary phrase is associated with a numerical value, indicati"
C16-1006,N04-1019,0,0.0605178,"proaches to summary annotation have been based on either sentence extracts or document abstracts (Loza et al., 2014; Xiong and Litman, 2014; Wang and Ling, 2016). An effective linkage between the document content and human summary on the micro level have been largely absent. Barker et al.(2016) partially address this challenge by linking a summary back to a group of sentences that support the summary. However, this linkage is weak since it tells only that there is one sentence or more supporting the summary within the group, without explicitly telling which one(s). Approaches such as Pyramid (Nenkova and Passonneau, 2004) have exploited creating Summary Content Units (SCUs) to establish such links and alleviate the challenge. The new highlighting scheme described in this work holds promise for establishing direct links between the phrases in student responses and those in the human summary, allowing us to develop a new evaluation metric based on color matching. 3 New Data and Annotation When reviewing the student feedback, we observe that not all issues are equally important. Some teaching problems are more prominent than others. Summary phrases should naturally reflect the number of students who raise the iss"
C16-1006,P02-1040,0,0.0956881,"(S¸tef˘anescu et al., 2014). One drawback of this approach is that the similarity of phrases that do not appear in a background corpus cannot be captured. In this work we develop an ensemble of similarity metrics by feeding them into a supervised classification framework. We use the phrase highlights as supervision, where phrases of the same color are positive examples and those of different colors are negative examples. We experiment with a range of metrics for measuring lexical similarity, including lexical overlap (Rus et al., 2013), cosine similarity, LIN similarity (Miller, 1995), BLEU (Papineni et al., 2002), SimSum (Lin, 2004), Word Embedding (Goldberg and Levy, 2014), and LSA (Deerwester et al., 1990). LIN similarity is based on WordNet definitions. Lexical overlap, cosine similarity, BLEU, and SimSum are related to how many words the two phrases have in common, while Word Embedding and LSA both capture the phrase similarity in a low dimensional semantic space. Therefore, we use an ensemble of the above similarity metrics by feeding them as features in a SVM classification model, assuming it will be better suited for this task than the LSA alone. Table 5 presents the intrinsic evaluation result"
C16-1006,P11-1110,0,0.0221478,"Missing"
C16-1006,P13-4028,0,0.0141991,"ntial role in phrase-based summarization. Better similarity learning helps produce better phrase clusters, which in turn leads to more accurate estimation of the number of student supporters for each summary phrase. While a human annotator 4 We use the implementation of Wapiti (Lavergne et al., 2010) with default parameters. 57 could distinguish the semantic similarity or dissimilarity of the phrase highlights, it remains unclear if a single similarity metric could fulfill this goal or if we may need an ensemble of different metrics. L&L calculate the pairwise phrase similarity using SEMILAR (Rus et al., 2013) with the latent semantic analysis (LSA) trained on the Touchstone corpus (S¸tef˘anescu et al., 2014). One drawback of this approach is that the similarity of phrases that do not appear in a background corpus cannot be captured. In this work we develop an ensemble of similarity metrics by feeding them into a supervised classification framework. We use the phrase highlights as supervision, where phrases of the same color are positive examples and those of different colors are negative examples. We experiment with a range of metrics for measuring lexical similarity, including lexical overlap (Ru"
C16-1006,D15-1044,0,0.0204712,"of both summary text quality, measured by ROUGE (Lin, 2004), and how well phrase summaries capture the most pressing student needs, measured by a new evaluation metric based on color matching. 1 This data set is publicly available at http://www.coursemirror.com/download/dataset2 54 2 Related Work Work on automatic text summarization involves multiple granularities, ranging from keywords, phrases, to sentences. Traditional approaches have largely focused on sentence extraction (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Li et al., 2013) and document abstraction (Liu et al., 2015; Rush et al., 2015; Durrett et al., 2016; Nallapati et al., 2016). In both cases, the produced summary is expected to be cohesive and coherent. We deviate from this path and seek to directly generate a set of bullet points as a summary. Phrases are easy to search and browse like words but more meaningful, and fit better on the small screen of a mobile device compared to sentences (Ueda et al., 2000; Luo et al., 2015). Our task setting differs from those of keyphrase extraction (Wu et al., 2005; Liu et al., 2009; Medelyan et al., 2009; Hasan and Ng, 2014; Kan, 2015). Of key importance is that each summary phrase"
C16-1006,C00-2127,0,0.0986794,"eywords, phrases, to sentences. Traditional approaches have largely focused on sentence extraction (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Li et al., 2013) and document abstraction (Liu et al., 2015; Rush et al., 2015; Durrett et al., 2016; Nallapati et al., 2016). In both cases, the produced summary is expected to be cohesive and coherent. We deviate from this path and seek to directly generate a set of bullet points as a summary. Phrases are easy to search and browse like words but more meaningful, and fit better on the small screen of a mobile device compared to sentences (Ueda et al., 2000; Luo et al., 2015). Our task setting differs from those of keyphrase extraction (Wu et al., 2005; Liu et al., 2009; Medelyan et al., 2009; Hasan and Ng, 2014; Kan, 2015). Of key importance is that each summary phrase is associated with a numerical value, indicating the number of students who raise the issue. This information is critical to course instructors for making informed choices. Intuitively our task setting bears similarity to word/phrase cloud (Yatani et al., 2011; Brooks et al., 2014), where the cloud gives greater prominence to words or phrases that appear frequently in the source"
C16-1006,N16-1007,0,0.0130269,"comprehensive evaluation of the results, taking the number of student supporters into account. Other related work on student responses includes collecting student responses using a mobile application named CourseMIRROR (Luo et al., 2015; Fan et al., 2015), determining the quality of a student reflective response and providing feedback (Luo and Litman, 2016), and extracting informative sentences from the student feedback (Luo et al., 2016). Traditional approaches to summary annotation have been based on either sentence extracts or document abstracts (Loza et al., 2014; Xiong and Litman, 2014; Wang and Ling, 2016). An effective linkage between the document content and human summary on the micro level have been largely absent. Barker et al.(2016) partially address this challenge by linking a summary back to a group of sentences that support the summary. However, this linkage is weak since it tells only that there is one sentence or more supporting the summary within the group, without explicitly telling which one(s). Approaches such as Pyramid (Nenkova and Passonneau, 2004) have exploited creating Summary Content Units (SCUs) to establish such links and alleviate the challenge. The new highlighting sche"
C16-1006,C14-1187,1,0.831097,"s, however there lacks a comprehensive evaluation of the results, taking the number of student supporters into account. Other related work on student responses includes collecting student responses using a mobile application named CourseMIRROR (Luo et al., 2015; Fan et al., 2015), determining the quality of a student reflective response and providing feedback (Luo and Litman, 2016), and extracting informative sentences from the student feedback (Luo et al., 2016). Traditional approaches to summary annotation have been based on either sentence extracts or document abstracts (Loza et al., 2014; Xiong and Litman, 2014; Wang and Ling, 2016). An effective linkage between the document content and human summary on the micro level have been largely absent. Barker et al.(2016) partially address this challenge by linking a summary back to a group of sentences that support the summary. However, this linkage is weak since it tells only that there is one sentence or more supporting the summary within the group, without explicitly telling which one(s). Approaches such as Pyramid (Nenkova and Passonneau, 2004) have exploited creating Summary Content Units (SCUs) to establish such links and alleviate the challenge. The"
C16-1246,J08-1001,0,0.0370527,"he author introduces the person to be put in the lustful layer. In the second segment (sentences (4) to (8)), the author states why this person belongs there and how he will be treated. PDTB relations are processed from PDTB annotations ignoring the discourse connectives, e.g. (1->2, EntRel) represents the discourse information: (Arg1: Sentence1, Arg2: Sentence2, Relation Type: EntRel). 2 Related Work Previous applications of PDTB-style annotations have typically been based on the extraction of PDTB relation occurrence patterns. Lin et al. (2011) encoded PDTB information into the entity-grid (Barzilay and Lapata, 2008) model for textual coherence evaluation. Mithun and Kosseim (2013) utilized PDTB relations for blog summarization. A limitation of the pattern approach is that since it targets a whole paragraph/essay, it is not straightforward to use for prediction tasks on individual sentences. In our work we propose to infer contextual PDTB discourse relations for each sentence, thus enabling the utilization of less-local PDTB information during single sentence prediction. The construction of our discourse structure looks similar to the building of an RST tree (Duverle and Prendinger, 2009), and there are a"
C16-1246,E12-1036,0,0.0227455,"ist2). In Table 2, distance between sentence 2 and 5 is recorded as (1,1) as we back trace both nodes to their parent node (3->4). As sentence 2 is the real text span in relation node (2->3) and sentence 5 is in the node (5->6), we get distance 1 for sentence 2 as the distances between (2->3) and (3->4) in the tree is 1; similarly, we get distance 1 for sentence 5. 4 Task and Data Description Argumentative revision classification. The task of revision classification aims to detect then categorize an author’s changes to their writing. Revision research has been conducted on Wikipedia articles (Bronner and Monz, 2012; Daxenberger and Gurevych, 2013) and argument-oriented study essays (Zhang and Litman, 2015). The example in Table 1 contains a paragraph from a revised student essay. Comparing to its previous draft, the changes are the addition of sentence 6 and sentence 7. The addition of sentence 6 is labeled as (Null->6, “Add”, “General Content”). The full classification process involves the alignment of sentences/clauses to locate changes (recognizing the alignment Null->6 and the revision operation “Add”) and the classification of change types (identifying the revision type “General Content”). In this"
C16-1246,F12-2042,0,0.0297338,"red to baselines, not only when PDTB annotation comes from humans but also from automatic parsers. 1 Introduction Widely used in discourse research, Penn Discourse Treebank (PDTB)-style annotation (Prasad et al., 2008) adopts a lexically grounded approach by anchoring discourse relations according to discourse connectives. In a typical PDTB annotation process, an annotator first locates discourse connectives (explicit or implicit) then annotates text spans as their arguments. While the process of manual PDTB annotation has been demonstrated to yield reliable results (Alsaif and Markert, 2011; Danlos et al., 2012; Zhou and Xue, 2015; Zeyrek et al., 2013), it yields more shallow annotation when compared to another widely-used discourse scheme, namely Rhetorical Structure Theory (RST) (Mann and Thompson, 1988; Carlson et al., 2002). This is because when using RST a text is represented as a hierarchical discourse tree, while when using PDTB the relations exist only locally (typically between sentences or clauses). The lack of discourse information across larger contexts potentially limits the utility of PDTB-style labels. Feng et al. (2014) found that when applied to the tasks of sentence ordering and es"
C16-1246,D13-1055,0,0.0230548,"nce between sentence 2 and 5 is recorded as (1,1) as we back trace both nodes to their parent node (3->4). As sentence 2 is the real text span in relation node (2->3) and sentence 5 is in the node (5->6), we get distance 1 for sentence 2 as the distances between (2->3) and (3->4) in the tree is 1; similarly, we get distance 1 for sentence 5. 4 Task and Data Description Argumentative revision classification. The task of revision classification aims to detect then categorize an author’s changes to their writing. Revision research has been conducted on Wikipedia articles (Bronner and Monz, 2012; Daxenberger and Gurevych, 2013) and argument-oriented study essays (Zhang and Litman, 2015). The example in Table 1 contains a paragraph from a revised student essay. Comparing to its previous draft, the changes are the addition of sentence 6 and sentence 7. The addition of sentence 6 is labeled as (Null->6, “Add”, “General Content”). The full classification process involves the alignment of sentences/clauses to locate changes (recognizing the alignment Null->6 and the revision operation “Add”) and the classification of change types (identifying the revision type “General Content”). In this work we assume perfect alignment"
C16-1246,P09-1075,0,0.0342339,"to the entity-grid (Barzilay and Lapata, 2008) model for textual coherence evaluation. Mithun and Kosseim (2013) utilized PDTB relations for blog summarization. A limitation of the pattern approach is that since it targets a whole paragraph/essay, it is not straightforward to use for prediction tasks on individual sentences. In our work we propose to infer contextual PDTB discourse relations for each sentence, thus enabling the utilization of less-local PDTB information during single sentence prediction. The construction of our discourse structure looks similar to the building of an RST tree (Duverle and Prendinger, 2009), and there are also prior efforts in combining the benefits of RST and PDTB (e.g., when building a Chinese discourse corpus (Li et al., 2014)). However, the focus of our work is different. The construction of an RST tree aims at creating a discourse representation of the whole sentence/paragraph/essay. We focus on grouping semantically similar text and assigning higher priority to specific local discourse relations. We expect our structure to be able to select the relations that should be propagated to improve performance in downstream applications. 3 Inferring Discourse Information from PDTB"
C16-1246,C14-1089,0,0.0141239,"monstrated to yield reliable results (Alsaif and Markert, 2011; Danlos et al., 2012; Zhou and Xue, 2015; Zeyrek et al., 2013), it yields more shallow annotation when compared to another widely-used discourse scheme, namely Rhetorical Structure Theory (RST) (Mann and Thompson, 1988; Carlson et al., 2002). This is because when using RST a text is represented as a hierarchical discourse tree, while when using PDTB the relations exist only locally (typically between sentences or clauses). The lack of discourse information across larger contexts potentially limits the utility of PDTB-style labels. Feng et al. (2014) found that when applied to the tasks of sentence ordering and essay scoring, an RST-style discourse parser outperformed a PDTB-style parser. Performance on both tasks was also likely impacted by parsing errors. To address both the local nature of PDTB-style annotations as well as the errors introduced by state-of-the-art discourse parsers, we propose to first build paragraph-level discourse structures from annotated PDTB labels, then to infer discourse relations based on these structures. We hypothesize that features extracted from inferred relations will improve performance in downstream app"
C16-1246,W16-3615,1,0.820209,"contains 47 students (94 essays) and 1267 revised sentence pairs, talking about placing contemporaries into Dante’s Inferno. Corpus B contains 63 students (126 essays) and 1044 revised sentence pairs, where students explain the rhetorical strategies used by the speaker/author of a previously read lecture/essay. Distribution of revisions is shown in the first columns of Tables 4 and 5. PDTB annotation. Recently PDTB discourse information was annotated on corpus A by one of the early developers of the D-LTAG environment (which engendered the PDTB framework)5 (Forbes et al., 2003; Webber, 2004; Forbes-Riley et al., 2016). Five relation types were annotated: Explicit, Implicit, EntRel, AltLex and NoRel. Within the Explicit and Implicit types, four level-1 senses were labeled: 4 5 Both corpora are from high school AP English classes. Thus considered as an expert annotator. 2619 1->2, EntRel 1 EntRel 4 Contingency 5 2 Expansion NoRel 3 6 (a) Segment for Draft 1 1->2, EntRel 4 1 Expansion 5 EntRel Contingency 2 NoRel Expansion 3 3->4, Contingency 1 EntRel 6 2->3, Expansion 5->6, NoRel Contingency Contingency 7 8 3->4, Contingency 1 5->8, NoRel 5->6, EntRel 2->3, Expansion 4->5, Expansion 6->7, Contingency 4->5, E"
C16-1246,J97-1003,0,0.400545,"are the same. The relations connecting the semantically dissimilar segments are likely to be more important than the relations within a segment. In Table 1, the Contingency relation between sentences 3 and 4 transits the thesis introduction to the arguments supporting the thesis. The Contingency relation between sentences 6 and 7 is just a transition to smooth the description of how Hue Heffner is going to be treated. 3.2 PDTBSegment Based on intuition 1, the PDTBSegment approach emphasizes the inference of discourse relations. Step1. Linear segmentation. Inspired by the TextTiling algorithm (Hearst, 1997) for linear segmentation, we utilize the “valley” of semantic similarity scores between sentences as the segmentation boundary. The summed word-embedding vector is calculated for each sentence2 and cosine value between vectors is used as the similarity score. Similarity scores indicates a possible segmentation boundary. In the example of Figure 1(a), the similarity between (2,3) and the similarity between (4,5) are larger than the similarity between (3,4), in other words, sentence 3 and 4 has a low similarity score preceded by and followed by high similarity scores, thus the paragraph is first"
C16-1246,D14-1224,0,0.0176509,"tion. A limitation of the pattern approach is that since it targets a whole paragraph/essay, it is not straightforward to use for prediction tasks on individual sentences. In our work we propose to infer contextual PDTB discourse relations for each sentence, thus enabling the utilization of less-local PDTB information during single sentence prediction. The construction of our discourse structure looks similar to the building of an RST tree (Duverle and Prendinger, 2009), and there are also prior efforts in combining the benefits of RST and PDTB (e.g., when building a Chinese discourse corpus (Li et al., 2014)). However, the focus of our work is different. The construction of an RST tree aims at creating a discourse representation of the whole sentence/paragraph/essay. We focus on grouping semantically similar text and assigning higher priority to specific local discourse relations. We expect our structure to be able to select the relations that should be propagated to improve performance in downstream applications. 3 Inferring Discourse Information from PDTB-style Labels 3.1 Intuitions for PDTB relation inference Different from other discourse annotations, the PDTB annotation schema anchors at the"
C16-1246,P11-1100,0,0.0233395,"ents (Section 3.2). In the first segment (sentences (1) to (3)) the author introduces the person to be put in the lustful layer. In the second segment (sentences (4) to (8)), the author states why this person belongs there and how he will be treated. PDTB relations are processed from PDTB annotations ignoring the discourse connectives, e.g. (1->2, EntRel) represents the discourse information: (Arg1: Sentence1, Arg2: Sentence2, Relation Type: EntRel). 2 Related Work Previous applications of PDTB-style annotations have typically been based on the extraction of PDTB relation occurrence patterns. Lin et al. (2011) encoded PDTB information into the entity-grid (Barzilay and Lapata, 2008) model for textual coherence evaluation. Mithun and Kosseim (2013) utilized PDTB relations for blog summarization. A limitation of the pattern approach is that since it targets a whole paragraph/essay, it is not straightforward to use for prediction tasks on individual sentences. In our work we propose to infer contextual PDTB discourse relations for each sentence, thus enabling the utilization of less-local PDTB information during single sentence prediction. The construction of our discourse structure looks similar to t"
C16-1246,I13-1197,0,0.023104,"the second segment (sentences (4) to (8)), the author states why this person belongs there and how he will be treated. PDTB relations are processed from PDTB annotations ignoring the discourse connectives, e.g. (1->2, EntRel) represents the discourse information: (Arg1: Sentence1, Arg2: Sentence2, Relation Type: EntRel). 2 Related Work Previous applications of PDTB-style annotations have typically been based on the extraction of PDTB relation occurrence patterns. Lin et al. (2011) encoded PDTB information into the entity-grid (Barzilay and Lapata, 2008) model for textual coherence evaluation. Mithun and Kosseim (2013) utilized PDTB relations for blog summarization. A limitation of the pattern approach is that since it targets a whole paragraph/essay, it is not straightforward to use for prediction tasks on individual sentences. In our work we propose to infer contextual PDTB discourse relations for each sentence, thus enabling the utilization of less-local PDTB information during single sentence prediction. The construction of our discourse structure looks similar to the building of an RST tree (Duverle and Prendinger, 2009), and there are also prior efforts in combining the benefits of RST and PDTB (e.g.,"
C16-1246,prasad-etal-2008-penn,0,0.121149,"typically ignores larger discourse contexts. In this paper we propose two approaches to infer discourse relations in a paragraph-level context from annotated PDTB labels. We investigate the utility of inferring such discourse information using the task of revision classification. Experimental results demonstrate that the inferred information can significantly improve classification performance compared to baselines, not only when PDTB annotation comes from humans but also from automatic parsers. 1 Introduction Widely used in discourse research, Penn Discourse Treebank (PDTB)-style annotation (Prasad et al., 2008) adopts a lexically grounded approach by anchoring discourse relations according to discourse connectives. In a typical PDTB annotation process, an annotator first locates discourse connectives (explicit or implicit) then annotates text spans as their arguments. While the process of manual PDTB annotation has been demonstrated to yield reliable results (Alsaif and Markert, 2011; Danlos et al., 2012; Zhou and Xue, 2015; Zeyrek et al., 2013), it yields more shallow annotation when compared to another widely-used discourse scheme, namely Rhetorical Structure Theory (RST) (Mann and Thompson, 1988;"
C16-1246,J14-4007,0,0.0607619,"downstream applications. 3 Inferring Discourse Information from PDTB-style Labels 3.1 Intuitions for PDTB relation inference Different from other discourse annotations, the PDTB annotation schema anchors at the labeling of discourse connectives and labels text spans around the connective. The annotator either locates the “Explicit” connectives or manually fills in the “Implicit” connectives between two text spans. The text span where the connective structurally attaches to is called Arg2, while the other text span is called Arg1. The spans are usually used at the level of sentence/phrase. In (Prasad et al., 2014), five relation types are annotated: Explicit, Implicit, AltLex, EntRel and NoRel. Within the Explicit/Implicit relations, the senses of relations are further categorized at multiple levels. In Level-1, the relations are categorized to 4 senses: Comparison, Contingency, Expansion and Temporal. In this paper we focus on the type/sense of Level-1 relations only and ignore the discourse connectives1 . Arg1, Arg2 and the discourse relation type/sense are used as demonstrated in Table 1. For the Explicit/Implicit relations, we use the sense of the relation directly to represent the relation. Below"
C16-1246,W15-0616,1,0.845311,"tasks was also likely impacted by parsing errors. To address both the local nature of PDTB-style annotations as well as the errors introduced by state-of-the-art discourse parsers, we propose to first build paragraph-level discourse structures from annotated PDTB labels, then to infer discourse relations based on these structures. We hypothesize that features extracted from inferred relations will improve performance in downstream applications, compared to features extracted from only original annotations. To verify our hypotheses, we choose the task of argumentative revision classification (Zhang and Litman, 2015; Zhang and Litman, 2016), which aims to identify the purpose of an author’s revisions during argumentative writing. This task first detects the differences between two drafts of a paper at the sentence level, then labels each revision using one of five categories: Claim/Ideas, Warrant/Reasoning/Backing, Evidence, General Content and Surface. While Zhang and Litman (2016) demonstrated decent classification performance without using discourse structure, an error analysis of their results suggests that discourse relations might improve performance (e.g., their current system has trouble differen"
C16-1246,D11-1068,0,\N,Missing
C90-2044,P87-1023,1,0.94398,"ue phrases produced by a single speaker in part of a recorded, transcribed lecture. In Section 2 we review our own and other work on cue phrases, in Section 3 we describe our current empirical studies, in Section 4 we present the results of our analysis, and in Section 5 we discuss theoretical and practical applications of our findings. Cue phrases are linguistic expressions such as &apos;now&apos; and &apos;welg t h a t m a y explicitly mark the structure of a discourse. For example, while the cue phrase &apos;inczdcntally&apos; m a y be used SENTENTIALLY as an adverbial, the DISCOUaSE use initiates a digression. In [8], we noted the ambiguity of cue phrases with respect to discourse and sentential usage and proposed an intonational model for their disambiguation. In this paper, we extend our previous characterization of cue phrases aald generalize its domain of coverage, based on a larger and more comprehensive empirical study: an examination of all cue phrases produced by a single ,~peaker in recorded natural speech. We also associate this prosodic model with orthographic and part-of-speech analyses of cue phrases in text. Such a dual model provides both theoretical justification for current computational"
C90-2044,P84-1085,0,0.0275878,"also been used to reduce the complexity of discourse processing and to increase textual coherence[3, 11, 21]. In Example (1) 1, interpretation of the anaphor &apos;it&apos; as (correctly) co-indexed with THE SYSTEM is facilitated by the presence of the cue phrases &apos;say&apos; and &apos;then&apos;, marking potential antecedents in &apos;... as an E X P E R T D A T A B A S E for AN E X P E R T SYSTEM ...&apos; a s structurally unavailable. 2 Introduction Words and phrases that may directly mark the structure of a discourse have been termed CUE PttR.ASES, C L U E W O R D S , D I S C O U R S E MAI:tKERS~ a r i d DISCOURSE PARTICLES [3, 4, 14, 17, 19]. Some exarnpies are &apos;now&apos;, which marks the introduction of a new subtopic or return to a previous one, &apos;incidentally&apos; and &apos;by the way&apos;, which indicate the beginning of a digression, and &apos;anyway&apos; and &apos;in any case&apos;, which indicate return from a digression. In a previous study[8], we noted that such terms are potentially ambiguous between DISCOURSE and SENTENTIAL uses[18]. So, &apos;now&apos; may be used as a temporal adverbial as well as a discourse marker, &apos;incidentally&apos; may also function as an adverbial, and other cue phrases similarly have one or more senses in addition to their function as markers of"
C90-2044,A88-1019,0,0.0920564,"Missing"
C90-2044,P84-1055,0,0.114207,"scourse and practical application to the generation of synthetic speech. 1 2 Studies The i m p o r t a n t role that cue phrases play in understanding and generating discourse has been well documented in the computational linguistics literature. For example, by indicating the presence of a structural boundary or a relationship between parts of a discourse, cue phrases caa assist in the resolution of anaphora[5, 4, 17] and in the identification of rhetorical relations [10, 12, 17]. Cue phrases have also been used to reduce the complexity of discourse processing and to increase textual coherence[3, 11, 21]. In Example (1) 1, interpretation of the anaphor &apos;it&apos; as (correctly) co-indexed with THE SYSTEM is facilitated by the presence of the cue phrases &apos;say&apos; and &apos;then&apos;, marking potential antecedents in &apos;... as an E X P E R T D A T A B A S E for AN E X P E R T SYSTEM ...&apos; a s structurally unavailable. 2 Introduction Words and phrases that may directly mark the structure of a discourse have been termed CUE PttR.ASES, C L U E W O R D S , D I S C O U R S E MAI:tKERS~ a r i d DISCOURSE PARTICLES [3, 4, 14, 17, 19]. Some exarnpies are &apos;now&apos;, which marks the introduction of a new subtopic or return to a"
C90-2044,J86-3001,0,0.0975938,"te this prosodic model with orthographic and part-of-speech analyses of cue phrases in text. Such a dual model provides both theoretical justification for current computational models of discourse and practical application to the generation of synthetic speech. 1 2 Studies The i m p o r t a n t role that cue phrases play in understanding and generating discourse has been well documented in the computational linguistics literature. For example, by indicating the presence of a structural boundary or a relationship between parts of a discourse, cue phrases caa assist in the resolution of anaphora[5, 4, 17] and in the identification of rhetorical relations [10, 12, 17]. Cue phrases have also been used to reduce the complexity of discourse processing and to increase textual coherence[3, 11, 21]. In Example (1) 1, interpretation of the anaphor &apos;it&apos; as (correctly) co-indexed with THE SYSTEM is facilitated by the presence of the cue phrases &apos;say&apos; and &apos;then&apos;, marking potential antecedents in &apos;... as an E X P E R T D A T A B A S E for AN E X P E R T SYSTEM ...&apos; a s structurally unavailable. 2 Introduction Words and phrases that may directly mark the structure of a discourse have been termed CUE PttR.A"
C90-2044,P89-1015,0,0.027293,"Missing"
C98-2124,P84-1029,0,0.326536,"literal or cooperative), other differences are a function of application task and task/strategy interactions. A PARADISE assessment of the contribution of each evaluation measure to overall performance shows that strategy-dependent dialogue phenomena as well as phenomena associated with speech recognition significantly predict performance. Our results identify the conditions under which TOOT' s cooperative response strategy leads to greater agent performance. 1 2 focus of considerable research in natural language and spoken dialogue systems (Allen and Perrault, 1980; Mays, 1980; Kaplan, 1981; Joshi et al., 1984; McCoy, 1989; Pao and Wilpon, 1992; Moore, 1994; Seneff et al., 1995; Goddeau et al., 1996; Pieraccini et al., 1997). However, despite the existence of many algorithms for generating cooperative responses, there has been little empirical work addressing the evaluation of such algorithms in the context of real-time natural language dialogue systems with human users. Thus it is unclear under what conditions cooperative responses result in more efficient or efficacious dialogues. This paper presents an empirical evaluation TOOT TOOT allows users to access online AMTRAK train schedules via a tele"
C98-2124,J86-2002,0,0.12226,"Missing"
C98-2124,H92-1008,0,0.411086,"fferences are a function of application task and task/strategy interactions. A PARADISE assessment of the contribution of each evaluation measure to overall performance shows that strategy-dependent dialogue phenomena as well as phenomena associated with speech recognition significantly predict performance. Our results identify the conditions under which TOOT' s cooperative response strategy leads to greater agent performance. 1 2 focus of considerable research in natural language and spoken dialogue systems (Allen and Perrault, 1980; Mays, 1980; Kaplan, 1981; Joshi et al., 1984; McCoy, 1989; Pao and Wilpon, 1992; Moore, 1994; Seneff et al., 1995; Goddeau et al., 1996; Pieraccini et al., 1997). However, despite the existence of many algorithms for generating cooperative responses, there has been little empirical work addressing the evaluation of such algorithms in the context of real-time natural language dialogue systems with human users. Thus it is unclear under what conditions cooperative responses result in more efficient or efficacious dialogues. This paper presents an empirical evaluation TOOT TOOT allows users to access online AMTRAK train schedules via a telephone dialogue, as in Figure 1.1 (A"
C98-2124,H92-1005,0,0.0710485,"pute a Mean Recognition score per dialogue. We also listened to the recordings to determine how many times the user interrupted the agent (Barge Ins). To measure dialogue efficiency, the number of System Turns and User Turns were extracted from the dialogue manager log, and the total Elapsed Time was determined from the recording. To measure user satisfaction 4, users responded to the web survey in Figure 4, which assessed their subjective evaluation of the agent's performance. Each question was designed to measure a pattie4Questionnaire-based user satisfaction ratings (Shriberg et al., 1992; Polifroni et al., 1992) have been frequentlyused in the literature as an external indicatorof agent usability. • Was the system easy to understand in this conversation? (TTS Performance) • In this conversation, did the system understand what you said? (ASR Performance) • In this conversation, was it easy to find the schedule you wanted? (Task Ease) • Was the pace of interaction with the system appropriate in this conversation? (Interaction Pace) o In this conversation, did you know what you could say at each point of the dialogue? (User Expertise) • How often was the system sluggish and slow to reply to you in this"
C98-2124,H92-1009,0,0.225674,"ASR rejections, to compute a Mean Recognition score per dialogue. We also listened to the recordings to determine how many times the user interrupted the agent (Barge Ins). To measure dialogue efficiency, the number of System Turns and User Turns were extracted from the dialogue manager log, and the total Elapsed Time was determined from the recording. To measure user satisfaction 4, users responded to the web survey in Figure 4, which assessed their subjective evaluation of the agent's performance. Each question was designed to measure a pattie4Questionnaire-based user satisfaction ratings (Shriberg et al., 1992; Polifroni et al., 1992) have been frequentlyused in the literature as an external indicatorof agent usability. • Was the system easy to understand in this conversation? (TTS Performance) • In this conversation, did the system understand what you said? (ASR Performance) • In this conversation, was it easy to find the schedule you wanted? (Task Ease) • Was the pace of interaction with the system appropriate in this conversation? (Interaction Pace) o In this conversation, did you know what you could say at each point of the dialogue? (User Expertise) • How often was the system sluggish and slow"
C98-2124,P97-1035,1,0.838789,"han literal strategy contributes to greater performance. database queries in TOOT, a spoken dialogue agent Introduction The notion of a cooperative response has been the for accessing online train schedules via a telephone conversation. We conduct an experiment in which 12 users carry out 4 tasks of varying difficulty with one of two versions of TOOT (literal and cooperative TOOT), resulting in a corpus of 48 dialogues. The values for a wide range of evaluation measures are then extracted from this corpus. We analyze our data using both traditional hypothesis testing methods and the PARADISE (Walker et al., 1997; Walker et al., 1998) methodology for estimating a performance function. Hypothesis testing shows that while differences among some evaluation measures depend on the response strategy (literal or cooperative), other differences are a function of application task and task/strategy interactions. A PARADISE assessment of the contribution of each evaluation measure to overall performance shows that strategy-dependent dialogue phenomena as well as phenomena associated with speech recognition significantly predict performance. Our results identify the conditions under which TOOT' s cooperative resp"
D15-1227,Q13-1032,0,0.135418,"Missing"
D15-1227,stefanescu-etal-2014-latent,0,0.131676,"Missing"
D15-1227,N12-1080,0,0.0202109,"he sentence. For example, Ueda et al. (2000) developed an “at-a-glance” summarization method with handcrafted rules. Recently, keyphrase extraction (Hasan and Ng, 2014; Liu et al., 2009; Medelyan et al., 2009; Wu et al., 2005) has received considerable attention, aiming to select important phrases from input documents, which is similar to phrase summarization. In this paper, we propose a general framework to adapt sentence summarization to phrase summarization. Clustering has been used to score sentences and has shown good improvement in text summarization (Yang et al., 2012; Li and Li, 2014; Gung and Kalita, 2012). In this work, we are using a metric clustering with semantic similarity to estimate the student coverage at a phrase level. Similarly, both diversity-based summarization (Carbonell and Goldstein, 1998; Zhang et al., 2005; Zhu et al., 2007) and our proposed method aim to estimate and maximize student coverage by minimizing redundancy in the output phrases. Differently, our method performs the redundancy reduction at a cluster level (a group of phrases) rather than penalize redundancy with a greedy iterative procedure sentence by sentence, and not only the information content is considered, bu"
D15-1227,N09-1041,0,0.127291,"s”). 6 Conclusion In this paper, we presented a novel application to summarize student feedback to reflection prompts by a combination of phrase extraction, phrase clustering and phrase ranking. It makes use of metric clustering to rank the phrases by their student coverage, taking the information source into account. Experimental results demonstrate the good effectiveness of the model. While the proposed method improved the performance against MMR, other summarization methods without an additional MMR component do exist, in1958 cluding SumBasic (Vanderwende et al., 2007), KLSUM and TopicSUM (Haghighi and Vanderwende, 2009). An initial experiment shows they do not yield better performance with default parameters. However, we will revisit it since these methods are meant for full sentences and are not optimized within the phrase framework. In the future, we plan to have additional annotation to evaluate the relative importance using the student coverage numbers. We also deployed CourseMIRROR in a statistics class in Spring 2015 and have created gold-standard summaries, which will allow us to both replicate the intrinsic evaluation of this paper with a new and larger dataset as well conduct an extrinsic evaluation"
D15-1227,P14-1119,0,0.0235882,"ics. results demonstrate the utility of our approach. Although not the focus of this paper, we have also built a mobile application called CourseMIRROR1 that utilizes the proposed summarization algorithm (Luo et al., 2015). Fan et al. (2015) report a preliminary study about the usage of the application. 2 Related Work While summarization systems that extract sentences are dominant, others have published in “summarization” at other levels besides the sentence. For example, Ueda et al. (2000) developed an “at-a-glance” summarization method with handcrafted rules. Recently, keyphrase extraction (Hasan and Ng, 2014; Liu et al., 2009; Medelyan et al., 2009; Wu et al., 2005) has received considerable attention, aiming to select important phrases from input documents, which is similar to phrase summarization. In this paper, we propose a general framework to adapt sentence summarization to phrase summarization. Clustering has been used to score sentences and has shown good improvement in text summarization (Yang et al., 2012; Li and Li, 2014; Gung and Kalita, 2012). In this work, we are using a metric clustering with semantic similarity to estimate the student coverage at a phrase level. Similarly, both div"
D15-1227,C14-1113,0,0.0238189,"levels besides the sentence. For example, Ueda et al. (2000) developed an “at-a-glance” summarization method with handcrafted rules. Recently, keyphrase extraction (Hasan and Ng, 2014; Liu et al., 2009; Medelyan et al., 2009; Wu et al., 2005) has received considerable attention, aiming to select important phrases from input documents, which is similar to phrase summarization. In this paper, we propose a general framework to adapt sentence summarization to phrase summarization. Clustering has been used to score sentences and has shown good improvement in text summarization (Yang et al., 2012; Li and Li, 2014; Gung and Kalita, 2012). In this work, we are using a metric clustering with semantic similarity to estimate the student coverage at a phrase level. Similarly, both diversity-based summarization (Carbonell and Goldstein, 1998; Zhang et al., 2005; Zhu et al., 2007) and our proposed method aim to estimate and maximize student coverage by minimizing redundancy in the output phrases. Differently, our method performs the redundancy reduction at a cluster level (a group of phrases) rather than penalize redundancy with a greedy iterative procedure sentence by sentence, and not only the information c"
D15-1227,W04-1013,0,0.0879532,"omputing relative importance of textual units (working for both sentences and phrases), is used to score the extracted candidate phrases. The top ranked phrase in the cluster is added to the output summary. This process starts from the cluster that has the most estimated student coverage and repeats for the next cluster until the length limit is reached. Note that when the student coverage is the same between two clusters, the score of the top-ranked phrases in the clusters according to LexRank is used to break the tie: the higher, the better. 5 Experiments We use the ROUGE evaluation metric (Lin, 2004) and report R-1 (unigrams), R-2 (bigrams), and RSU4 (bigrams with skip distance up to 4 words), including the recall (R), precision (P) and FMeasure (F). These scores measure the overlap between human-generated summaries and a machine-generated summary. We design and compare a number of other summarization methods to evaluate the proposed phrase summarization approach. Keyphrase extraction. Maui (Medelyan et al., 2009) is selected as the baseline, which is one of the state-of-the-art keyphrase extraction methods. Sentence to phrase summarization. Existing sentence summarization techniques can"
D15-1227,D09-1027,0,0.0776904,"rate the utility of our approach. Although not the focus of this paper, we have also built a mobile application called CourseMIRROR1 that utilizes the proposed summarization algorithm (Luo et al., 2015). Fan et al. (2015) report a preliminary study about the usage of the application. 2 Related Work While summarization systems that extract sentences are dominant, others have published in “summarization” at other levels besides the sentence. For example, Ueda et al. (2000) developed an “at-a-glance” summarization method with handcrafted rules. Recently, keyphrase extraction (Hasan and Ng, 2014; Liu et al., 2009; Medelyan et al., 2009; Wu et al., 2005) has received considerable attention, aiming to select important phrases from input documents, which is similar to phrase summarization. In this paper, we propose a general framework to adapt sentence summarization to phrase summarization. Clustering has been used to score sentences and has shown good improvement in text summarization (Yang et al., 2012; Li and Li, 2014; Gung and Kalita, 2012). In this work, we are using a metric clustering with semantic similarity to estimate the student coverage at a phrase level. Similarly, both diversity-based summa"
D15-1227,N15-3004,1,0.421492,"ntic distance to estimate the student coverage of each phrase in the summary; a semantic metric allows similar phrases to be grouped together even if they are in different textual forms. Experimental 1955 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1955–1960, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. results demonstrate the utility of our approach. Although not the focus of this paper, we have also built a mobile application called CourseMIRROR1 that utilizes the proposed summarization algorithm (Luo et al., 2015). Fan et al. (2015) report a preliminary study about the usage of the application. 2 Related Work While summarization systems that extract sentences are dominant, others have published in “summarization” at other levels besides the sentence. For example, Ueda et al. (2000) developed an “at-a-glance” summarization method with handcrafted rules. Recently, keyphrase extraction (Hasan and Ng, 2014; Liu et al., 2009; Medelyan et al., 2009; Wu et al., 2005) has received considerable attention, aiming to select important phrases from input documents, which is similar to phrase summarization. In this"
D15-1227,D09-1137,0,0.116562,"f our approach. Although not the focus of this paper, we have also built a mobile application called CourseMIRROR1 that utilizes the proposed summarization algorithm (Luo et al., 2015). Fan et al. (2015) report a preliminary study about the usage of the application. 2 Related Work While summarization systems that extract sentences are dominant, others have published in “summarization” at other levels besides the sentence. For example, Ueda et al. (2000) developed an “at-a-glance” summarization method with handcrafted rules. Recently, keyphrase extraction (Hasan and Ng, 2014; Liu et al., 2009; Medelyan et al., 2009; Wu et al., 2005) has received considerable attention, aiming to select important phrases from input documents, which is similar to phrase summarization. In this paper, we propose a general framework to adapt sentence summarization to phrase summarization. Clustering has been used to score sentences and has shown good improvement in text summarization (Yang et al., 2012; Li and Li, 2014; Gung and Kalita, 2012). In this work, we are using a metric clustering with semantic similarity to estimate the student coverage at a phrase level. Similarly, both diversity-based summarization (Carbonell and"
D15-1227,N07-1013,0,0.0548983,"Missing"
D15-1227,P13-4028,0,0.0901512,"Missing"
D15-1227,C00-2127,0,0.488363,"Language Processing, pages 1955–1960, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. results demonstrate the utility of our approach. Although not the focus of this paper, we have also built a mobile application called CourseMIRROR1 that utilizes the proposed summarization algorithm (Luo et al., 2015). Fan et al. (2015) report a preliminary study about the usage of the application. 2 Related Work While summarization systems that extract sentences are dominant, others have published in “summarization” at other levels besides the sentence. For example, Ueda et al. (2000) developed an “at-a-glance” summarization method with handcrafted rules. Recently, keyphrase extraction (Hasan and Ng, 2014; Liu et al., 2009; Medelyan et al., 2009; Wu et al., 2005) has received considerable attention, aiming to select important phrases from input documents, which is similar to phrase summarization. In this paper, we propose a general framework to adapt sentence summarization to phrase summarization. Clustering has been used to score sentences and has shown good improvement in text summarization (Yang et al., 2012; Li and Li, 2014; Gung and Kalita, 2012). In this work, we are"
D16-1149,D15-1109,0,0.0199709,"). In this paper we both investigate group acoustic-prosodic entrainment and examine relationships between group entrainment and a factor from the teamwork literature called participation equality / dominance (Paletz and Schunn, 2011). Also, while freely available speech corpora have supported the study of entrainment in two-party dialogues (e.g., Switchboard, Maptask, the Columbia Games Corpus, Let’s Go), few community resources exist for the study of multi-party entrainment. Some multi-party resources are only text-based (e.g., the online Slashdot forum (Allen et al., 2014), chat dialogues (Afantenos et al., 2015)). Those speech resources that do exist are often less than ideal as they were created for other purposes (e.g., Supreme Court arguments (Beˇnuˇs et al., 2014; DanescuNiculescu-Mizil et al., 2012), the AMI meeting corpus (Carletta et al., 2006)). Although not created to study entrainment, the KTH-Idiap GroupInterviewing corpus (Oertel et al., 2014) is perhaps most relevant as it was explicitly designed to support research on group dynamics. However, the corpus contains only 5 hours of speech, and participants were PhD students so did not differ on variables such as age and social status. The T"
D16-1149,D14-1124,0,0.01872,"s, lexical analysis of transcriptions). In this paper we both investigate group acoustic-prosodic entrainment and examine relationships between group entrainment and a factor from the teamwork literature called participation equality / dominance (Paletz and Schunn, 2011). Also, while freely available speech corpora have supported the study of entrainment in two-party dialogues (e.g., Switchboard, Maptask, the Columbia Games Corpus, Let’s Go), few community resources exist for the study of multi-party entrainment. Some multi-party resources are only text-based (e.g., the online Slashdot forum (Allen et al., 2014), chat dialogues (Afantenos et al., 2015)). Those speech resources that do exist are often less than ideal as they were created for other purposes (e.g., Supreme Court arguments (Beˇnuˇs et al., 2014; DanescuNiculescu-Mizil et al., 2012), the AMI meeting corpus (Carletta et al., 2006)). Although not created to study entrainment, the KTH-Idiap GroupInterviewing corpus (Oertel et al., 2014) is perhaps most relevant as it was explicitly designed to support research on group dynamics. However, the corpus contains only 5 hours of speech, and participants were PhD students so did not differ on varia"
D16-1149,P11-2020,0,0.0170236,"training intervention in which participants were given specific advice based on a needs analysis of the team skills important to the game (Gregory et al., 2013). Such mixed teamwork/taskwork training has been shown to improve team process outcomes (Salas et al., 2008). The other half only had 2 A lab experiment involving a two-player game requiring spoken communication was similarly used to collect the Columbia Games Corpus of 12 spontaneous task-oriented dyadic conversations, which has been used in multiple studies of two-party entrainment (Levitan and Hirschberg, 2011; Levitan et al., 2012; Levitan et al., 2011). Our corpus is approximately 5 times larger, includes speech from teams rather than from dyads, and relatedly includes new types of team-related meta-data. Our corpus also contains both video and audio as our dialogues were face-to-face rather than restricted to voice. 3 As discussed in Section 2, prior research has often found positive relationships between success and entrainment. Figure 1: Dialogue excerpt from a Forbidden IslandT M game. E=Engineer, M=Messenger, and P=Pilot roles in the game. Square brackets indicate overlapping speech. training on the rules of the game, which all teams r"
D16-1149,N12-1002,0,0.624259,"Missing"
D16-1149,W12-1612,0,0.715754,"nt during multiparty cooperative dialogue, we have created a largescale corpus (over 47 hours of recordings) of teams 1 Other terms in the literature include accommodation, adaptation, alignment, convergence, coordination and priming. 2 Background and Related Work The development of methods for automatically quantifying entrainment in text and speech data is an active research area, as entrainment has been shown to correlate with success measures or with social variables for a variety of phenomena, e.g., acoustic-prosodic, lexical, and syntactic (Nenkova et al., 2008; Reitter and Moore, 2007; Mitchell et al., 2012; Levitan et al., 2012; Lee et al., 2011; Stoyanchev and Stent, 2009; Lopes et al., 2013; Lubold and Pon-Barry, 2014; Moon et al., 2014; Sinha and Cassell, 2015). Such research, in turn, requires corpora with certain properties. A highquality spoken language corpus for studying entrain1421 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1421–1431, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics ment would include transcriptions suitable for natural language processing, audio recordings suitable for signal process"
D16-1149,W14-4103,0,0.145661,"Missing"
D16-1149,P08-2043,0,0.232245,"cal of teams. To support the study of entrainment during multiparty cooperative dialogue, we have created a largescale corpus (over 47 hours of recordings) of teams 1 Other terms in the literature include accommodation, adaptation, alignment, convergence, coordination and priming. 2 Background and Related Work The development of methods for automatically quantifying entrainment in text and speech data is an active research area, as entrainment has been shown to correlate with success measures or with social variables for a variety of phenomena, e.g., acoustic-prosodic, lexical, and syntactic (Nenkova et al., 2008; Reitter and Moore, 2007; Mitchell et al., 2012; Levitan et al., 2012; Lee et al., 2011; Stoyanchev and Stent, 2009; Lopes et al., 2013; Lubold and Pon-Barry, 2014; Moon et al., 2014; Sinha and Cassell, 2015). Such research, in turn, requires corpora with certain properties. A highquality spoken language corpus for studying entrain1421 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1421–1431, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics ment would include transcriptions suitable for natural language processi"
D16-1149,N16-1070,0,0.0140189,"over time. With respect to time, team convergence was found for shimmer and jitter independently of temporal interval examined, but for pitch only when comparing the most distant temporal intervals in game 1. 5.2 Participation Equality / Dominance Within psychology, equality of participation has been associated with successful team performance and decision-making (e.g., (Mesmer-Magnus and DeChurch, 2009; Stasser and Titus, 1987)). Within computational linguistics, balance of participation with respect to proposal of ideas was associated with more productive small group (online) conversations (Niculae and Danescu-Niculescu-Mizil, 2016). Extending this literature, we perform a novel inSession Length Team Size Participation Dominance Model R2 Model F B 0.187 108.706 Model 1 SEB 0.067 47.398 β 0.328* 0.269* 0.186 6.761* B 0.197 69.721 −1077.747 Model 2 SEB 0.064 47.980 429.130 0.266 7.015* β 0.344* 0.173 −0.299* Table 4: Summary of hierarchical regression analysis for variables predicting entrainment on pitch-max. * p < .05. n = 62. vestigation of the association between participation equality/dominance and team entrainment, focusing on the time interval showing the most significant convergence results in Section 5.1.4 (entrai"
D16-1149,P07-1102,0,0.227027,"rt the study of entrainment during multiparty cooperative dialogue, we have created a largescale corpus (over 47 hours of recordings) of teams 1 Other terms in the literature include accommodation, adaptation, alignment, convergence, coordination and priming. 2 Background and Related Work The development of methods for automatically quantifying entrainment in text and speech data is an active research area, as entrainment has been shown to correlate with success measures or with social variables for a variety of phenomena, e.g., acoustic-prosodic, lexical, and syntactic (Nenkova et al., 2008; Reitter and Moore, 2007; Mitchell et al., 2012; Levitan et al., 2012; Lee et al., 2011; Stoyanchev and Stent, 2009; Lopes et al., 2013; Lubold and Pon-Barry, 2014; Moon et al., 2014; Sinha and Cassell, 2015). Such research, in turn, requires corpora with certain properties. A highquality spoken language corpus for studying entrain1421 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1421–1431, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics ment would include transcriptions suitable for natural language processing, audio recordings suit"
D16-1149,N09-2048,0,0.356249,"rgescale corpus (over 47 hours of recordings) of teams 1 Other terms in the literature include accommodation, adaptation, alignment, convergence, coordination and priming. 2 Background and Related Work The development of methods for automatically quantifying entrainment in text and speech data is an active research area, as entrainment has been shown to correlate with success measures or with social variables for a variety of phenomena, e.g., acoustic-prosodic, lexical, and syntactic (Nenkova et al., 2008; Reitter and Moore, 2007; Mitchell et al., 2012; Levitan et al., 2012; Lee et al., 2011; Stoyanchev and Stent, 2009; Lopes et al., 2013; Lubold and Pon-Barry, 2014; Moon et al., 2014; Sinha and Cassell, 2015). Such research, in turn, requires corpora with certain properties. A highquality spoken language corpus for studying entrain1421 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1421–1431, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics ment would include transcriptions suitable for natural language processing, audio recordings suitable for signal processing, and meta-data such as task success or speaker demographics. Whi"
E06-1037,2005.sigdial-1.10,1,0.732524,"TS) community. Student Moves refer to the type of answer a student gives. Answers that involve a concept already introduced in the dialogue are called Shallow, answers that involve a novel concept are called Novel, “I don’t know” type answers are called Assertions (As), and Deep answers refer to answers that involve linking two concepts through reasoning. In our study, we merge all non-Shallow moves into a new move “Other.” In addition to Student Moves, we annotated five other features to include in our representation of the student state. Two emotion related features were annotated manually (Forbes-Riley and Litman, 2005): certainty and frustration. Certainty describes how confident a student seemed to be in his answer, while frustration describes how frustrated the student seemed to be in his last response. We include three other features for the Student state that were extracted automatically. Correctness says if the last student answer was correct or incorrect. As noted above, this is what most current tutoring systems use as their state. Percent Correct is the percentage of questions in the current problem the student has answered correctly so far. Finally, if a student performs poorly when it comes to a c"
E06-1037,2005.sigdial-1.5,0,0.0851696,"ving enough data to train on. Our results here indicate that a small training corpus is actually acceptable to use in a MDP framework as long as the state and action features are pruned effectively. The use of features such as context and student moves is nothing new to the ITS community however, such as the BEETLE system (Zinn et al., 2005), but very little work has been done using RL in developing tutoring systems. 7 Related Work RL has been applied to improve dialogue systems in past work but very few approaches have looked at which features are important to include in the dialogue state. (Paek and Chickering, 2005) showed how the state space can be learned from 8 Discussion In this paper we showed that incorporating more information into a representation of the student state has an impact on what actions the tutor should take. We first showed that despite not be295 ing able to test on real users or simulated users just yet, that our generated policies were indeed reliable since they converged in terms of the V-values of each state and the policy for each state. Next, we showed that all five features investigated in this study were indeed important to include when constructing an estimation of the studen"
E06-1037,W03-2111,0,0.0659231,"Missing"
forbes-riley-etal-2008-uncertainty,P01-1066,0,\N,Missing
J06-3004,P98-1122,0,0.360877,"t. In particular, dialogue confirmation strategies may hinder users’ ability to correct system error. For instance, if a system wrongly presents information as being correct, as when it verifies information implicitly, users become confused about how to respond (Krahmer et al. 2001). Other studies have shown that speakers tend to switch to a prosodically “marked” speaking style after communication errors, comparing repetition corrections with the speech being repeated (Wade, Shriberg, and Price 1992; Oviatt et al. 1996; 418 Litman, Hirschberg, and Swerts Corrections in Spoken Dialogue Systems Levow 1998; Bell and Gustafson 1999). Although this speaking style may be effective in problematic human–human communicative settings, there is evidence that suggests it leads to further errors in human–machine interactions (Levow 1998; Soltau and Waibel 2000). That corrections are difficult for ASR systems is generally explained by the fact that they tend to be hyperarticulated—higher, louder, longer—than other turns (Wade, Shriberg, and Price 1992; Oviatt et al. 1996; Levow 1998; Bell and Gustafson 1999; Shimojima, et al. 2001; Soltau and Waibel 1998, 2000; Soltau, Metze, and Waibel 2002), where ASR m"
J06-3004,P99-1040,1,0.609359,"Missing"
J06-3004,A00-2028,1,0.694232,"Corrections in Spoken Dialogue Systems 4.1.4 Dialogue Position and History Features. We also showed that the further a correction is from the original error, the less likely it is to be recognized correctly, and the stronger the correlation with prosodic deviation from the mean values over a speaker’s turns (e.g., more distant corrections are higher in pitch than closer corrections). As a first approximation of this distance feature, we included the feature diadist—distance of the current turn from the beginning of the dialogue. In addition, previous research (Litman, Walker, and Kearns 1999; Walker et al. 2000) has shown that features of the dialogue as a whole and features of more local context can be helpful in predicting “problematic” dialogues. So we looked at a set of features summarizing aspects of the prior dialogue for both the absolute number of times prior turns exhibited certain characteristics (e.g., contained a key word like cancel—priorcancnum) and the percentage of the prior dialogue containing one of these features (e.g., priorcancpct). We also examined means for all our continuous-valued features over the entire dialogue preceding the turn to be predicted (pmn ), such as pmnsyls, th"
J06-3004,C98-1117,0,\N,Missing
J93-3003,A88-1019,0,0.0146172,"Missing"
J93-3003,P84-1055,0,0.797345,"-oriented dialogs, plan-based knowledge could be used to assist in the recognition of discourse structure (Grosz 1977). However, such analysis is often beyond the capabilities of current natural language processing systems. Many domains are also not task-oriented. Additionally, cue phrases are widely used in the identification of rhetorical relations among portions of a text or discourse (Hobbs 1979; Mann and Thompson 1983; Reichman 1985), and have been claimed in general to reduce the complexity of discourse processing and to increase textual coherence in natural language processing systems (Cohen 1984; Litman and Allen 1987; Zuckerman and Pearl 1986). Previous attempts to characterize the set of cue phrases in the linguistic and in the computational literature have typically been extensional, with each cue phrase or set of phrases associated with one or more discourse or conversational functions. In the linguistic literature, cue phrases have been the subject of a number of theoretical and descriptive corpus-based studies that emphasize the diversity of meanings associated with cue phrases as a class, within an overarching framework of function such as discourse c o h e s i v e n e s s or"
J93-3003,J86-3001,0,0.985314,"ture. These include items such as now, which marks the introduction of a new subtopic or return to a previous one; well, which indicates a response to previous material or an explanatory comment; incidentally, by the way, and that reminds me, which indicate the beginning of a digression; and anyway and in any case, which indicate a return from a digression. The recognition and appropriate generation of cue phrases is of particular interest to research in discourse structure. The structural information conveyed by these phrases is crucial to many tasks, such as anaphora resolution (Grosz 1977; Grosz and Sidner 1986; Reichman 1985), the inference of speaker intention and the recognition of speaker plans (Grosz and Sidner 1986; Sidner 1985; Litman and Allen 1987), and the generation of explanations and other text (Zuckerman and Pearl 1986). Despite the crucial role that cue phrases play in theories of discourse and their implementation, however, many questions about how cue phrases are identified and defined remain to be examined. In particular, the question of cue phrase polysemy has yet to receive a satisfactory solution. Each lexical item that has one or more discourse * 600 MountainAvenue,MurrayHill,"
J93-3003,P87-1023,1,0.920857,"n Example 6 is in fact reduced, as Halliday and Hassan (1976) propose, that in Example 10, while interpreted as a discourse use, is nonetheless clearly intonationally prominent. Furthermore, both of the n o w s in Example 5 are also prominent. So it would seem that intonational prominence alone is insufficient to disambiguate between sentential and discourse uses. In this paper we present a more complex model of intonational features and textbased features that can serve to disambiguate between sentential and discourse instances of cue phrases. Our model is based on several empirical studies (Hirschberg and Litman 1987; Litman and Hirschberg 1990): two studies of individual cue phrases in which we develop our model, and a more comprehensive study of cue phrases as a class, in which we confirm and expand our model. Before describing these studies and their results, we must first describe the intonational features examined in our analyses. 3. Phrasing and Accent in English The importance of intonational information to the communication of discourse structure has been recognized in a variety of studies (Butterworth 1975; Schegloff 1979; Brazil, Coulthard, and Johns 1980; Hirschberg and Pierrehumbert 1986; Pier"
J93-3003,P86-1021,1,0.458406,"cal studies (Hirschberg and Litman 1987; Litman and Hirschberg 1990): two studies of individual cue phrases in which we develop our model, and a more comprehensive study of cue phrases as a class, in which we confirm and expand our model. Before describing these studies and their results, we must first describe the intonational features examined in our analyses. 3. Phrasing and Accent in English The importance of intonational information to the communication of discourse structure has been recognized in a variety of studies (Butterworth 1975; Schegloff 1979; Brazil, Coulthard, and Johns 1980; Hirschberg and Pierrehumbert 1986; Pierrehumbert and Hirschberg 1990; Silverman 1987). However, just which intonational features are important and how they communicate discourse information is not well understood. Prerequisite, however, to addressing these issues is the adoption of a framework of intonational description to identify which intonational features will be examined and how they will be characterized. For the studies discussed below, we have adopted Pierrehumbert's (1980) theory of English intonation, which we will describe briefly below. In Pierrehumbert's phonological description of English, intonational contours"
J93-3003,C90-2044,1,0.721327,"ed, as Halliday and Hassan (1976) propose, that in Example 10, while interpreted as a discourse use, is nonetheless clearly intonationally prominent. Furthermore, both of the n o w s in Example 5 are also prominent. So it would seem that intonational prominence alone is insufficient to disambiguate between sentential and discourse uses. In this paper we present a more complex model of intonational features and textbased features that can serve to disambiguate between sentential and discourse instances of cue phrases. Our model is based on several empirical studies (Hirschberg and Litman 1987; Litman and Hirschberg 1990): two studies of individual cue phrases in which we develop our model, and a more comprehensive study of cue phrases as a class, in which we confirm and expand our model. Before describing these studies and their results, we must first describe the intonational features examined in our analyses. 3. Phrasing and Accent in English The importance of intonational information to the communication of discourse structure has been recognized in a variety of studies (Butterworth 1975; Schegloff 1979; Brazil, Coulthard, and Johns 1980; Hirschberg and Pierrehumbert 1986; Pierrehumbert and Hirschberg 1990"
J93-3003,H92-1088,0,0.0134156,"Missing"
J97-1005,P95-1017,0,0.0534294,"Missing"
J97-1005,J96-2004,0,0.106505,"tract significant data for use in defining segmentations for each narrative. The results indicate that the observed distributions are highly significant, i.e., unlikely to have arisen by chance. In Section 3.2.2, we briefly review Cochran's Q (1950), the statistic that we use, and the test of the null hypothesis. We then partition Cochran's Q to determine the lowest value on the x-axis in Figure 3 at which agreements on boundaries become statistically significant. The results indicate significance arises when at least three subjects agree on a boundary. Reliability metrics (Krippendorff 1980; Carletta 1996) are designed to give a robust measure of how well distinct sets of data agree with, or replicate, one another. They are sensitive to the relative proportion of the different data types (e.g., boundaries versus nonboundaries), but insensitive to the statistical likelihood that agreements will occur. We have already discussed how variable the subjects' responses are, both in number and placement of segment boundaries, so we know that our subjects are not replicating the same behavior. However, all 20 narratives show the same pattern of responses as illustrated in Figure 3: certain boundaries ar"
J97-1005,P84-1055,0,0.105596,"and Pollack 1992; Hearst 1994; Walker 1995), and the magnitude of our segmentation task precludes asking subjects to specify hierarchical relations. Finally, we quantify our results using a significance 106 Passonneau and Litman Discourse Segmentation test, a reliability measure, and, for purposes of comparison with other work, percent agreement. 2.2 Correlation of Segmentation with Utterance Features The segmental structure of discourse has been claimed to constrain and be constrained by disparate phenomena, e.g., cue phrases (Hirschberg and Litman 1993; Grosz and Sidner 1986; Reichman 1985; Cohen 1984), plans and intentions (Carberry 1990; Litman and Allen 1990; Grosz and Sidner 1986), prosody (Hirschberg and Pierrehumbert 1986; Butterworth 1980), nominal reference (Webber 1991; Grosz and Sidner 1986; Linde 1979), and tense (Webber 1988; Hwang and Schubert 1992; Song and Cohen 1991). However, just as with the early proposals regarding segmentation, many of these proposals are based on fairly informal studies. It is only recently that attempts have been made to quantitatively evaluate how utterance features correlate with independently justified segmentations. Many of the studies discussed i"
J97-1005,P92-1032,0,0.0304954,"Missing"
J97-1005,J95-2003,0,0.115912,"Missing"
J97-1005,J86-3001,0,0.972787,"assign among segments. The nature of any hypothesized interaction between discourse structure and linguistic devices depends both on the model of discourse that is adopted, and on the types of linguistic devices that are investigated. Here we briefly review previous work on characterizing discourse segments, and on correlating discourse segments with utterance features. We conclude each review by summarizing the differences between our study and previous work. 2.1 Characterizing the Notion of a Segment A number of alternative proposals have been presented, which relate segments to intentions (Grosz and Sidner 1986), Rhetorical Structure Theory (RST) relations (Mann and Thompson 1988) or other semantic relations (Polanyi 1988; Hobbs 1979). The linguistic structure of Grosz and Sidner's (1986) discourse model consists of multiutterance segments and structural relations among them, yielding a discourse tree structure. The hierarchical relations of their linguistic structure are isomorphic with the two other levels of their model, intentional structure and attentional state. Rhetorical relations do not play a role in their model. In Hobbs (1979) and Polanyi (1988), segmental structure is an artifact of cohe"
J97-1005,P94-1002,0,0.888745,"d on which speaker had control. Neither study presented any quantitative analysis of the ability to reliably perform the initial utterance classification. However, in Whittaker and Stenton (1988), a higher level of discourse structure based on topic shifts was agreed upon by at least 4 of 5 judges for 46 of the 56 control shifts. In sum, relatively few quantitative empirical studies have been made of how to annotate discourse corpora with features of discourse structure, and those recent ones that exist use various models such as the Grosz and Sidner model (1986), an informal notion of topic (Hearst 1994; Flammia and Zue 1995), transactions (Isard and Carletta 1995), Relational Discourse Analysis (Moser and Moore 1995), or control (Whittaker and Stenton 1988; Walker and Whittaker 1990). The modalities of the corpora investigated include dialogic or monologic, written, spontaneous or read, and the genres also vary. Quantitative evaluations of subjects' annotations using notions of agreement, interrater reliability, a n d / o r significance show that good results can be difficult to achieve. As discussed in Section 3, our initial aim was to explore basic issues about segmentation, thus we used"
J97-1005,P83-1019,0,0.0772369,"Missing"
J97-1005,J93-3003,1,0.800342,"a tree structure has frequently been questioned (Dale 1992; Moore and Pollack 1992; Hearst 1994; Walker 1995), and the magnitude of our segmentation task precludes asking subjects to specify hierarchical relations. Finally, we quantify our results using a significance 106 Passonneau and Litman Discourse Segmentation test, a reliability measure, and, for purposes of comparison with other work, percent agreement. 2.2 Correlation of Segmentation with Utterance Features The segmental structure of discourse has been claimed to constrain and be constrained by disparate phenomena, e.g., cue phrases (Hirschberg and Litman 1993; Grosz and Sidner 1986; Reichman 1985; Cohen 1984), plans and intentions (Carberry 1990; Litman and Allen 1990; Grosz and Sidner 1986), prosody (Hirschberg and Pierrehumbert 1986; Butterworth 1980), nominal reference (Webber 1991; Grosz and Sidner 1986; Linde 1979), and tense (Webber 1988; Hwang and Schubert 1992; Song and Cohen 1991). However, just as with the early proposals regarding segmentation, many of these proposals are based on fairly informal studies. It is only recently that attempts have been made to quantitatively evaluate how utterance features correlate with independently justi"
J97-1005,P96-1038,0,0.236437,"91; Grosz and Sidner 1986; Linde 1979), and tense (Webber 1988; Hwang and Schubert 1992; Song and Cohen 1991). However, just as with the early proposals regarding segmentation, many of these proposals are based on fairly informal studies. It is only recently that attempts have been made to quantitatively evaluate how utterance features correlate with independently justified segmentations. Many of the studies discussed in the preceding subsection take this approach. The types of linguistic features investigated ind u d e prosody (Grosz and Hirschberg 1992; Nakatani, Hirschberg, and Grosz 1995; Hirschberg and Nakatani 1996; Swerts 1995; Swerts and Ostendorf 1995), term repetition (Hearst 1994), cue words (Moser and Moore 1995; Whittaker and Stenton 1988), and discourse anaphora (Walker and Whittaker 1990). Grosz and Hirschberg (1992) investigate the prosodic structuring of discourse. The correlation of various prosodic features with their independently obtained consensus codings of segmental structure (codings on which all labelers agreed) is analyzed using t-tests; the results support the hypothesis that discourse structure is marked intonationally in read speech. For example, pauses tended to precede phrases"
J97-1005,P86-1021,0,0.00787648,"ubjects to specify hierarchical relations. Finally, we quantify our results using a significance 106 Passonneau and Litman Discourse Segmentation test, a reliability measure, and, for purposes of comparison with other work, percent agreement. 2.2 Correlation of Segmentation with Utterance Features The segmental structure of discourse has been claimed to constrain and be constrained by disparate phenomena, e.g., cue phrases (Hirschberg and Litman 1993; Grosz and Sidner 1986; Reichman 1985; Cohen 1984), plans and intentions (Carberry 1990; Litman and Allen 1990; Grosz and Sidner 1986), prosody (Hirschberg and Pierrehumbert 1986; Butterworth 1980), nominal reference (Webber 1991; Grosz and Sidner 1986; Linde 1979), and tense (Webber 1988; Hwang and Schubert 1992; Song and Cohen 1991). However, just as with the early proposals regarding segmentation, many of these proposals are based on fairly informal studies. It is only recently that attempts have been made to quantitatively evaluate how utterance features correlate with independently justified segmentations. Many of the studies discussed in the preceding subsection take this approach. The types of linguistic features investigated ind u d e prosody (Grosz and Hirsch"
J97-1005,P92-1030,0,0.0127117,"test, a reliability measure, and, for purposes of comparison with other work, percent agreement. 2.2 Correlation of Segmentation with Utterance Features The segmental structure of discourse has been claimed to constrain and be constrained by disparate phenomena, e.g., cue phrases (Hirschberg and Litman 1993; Grosz and Sidner 1986; Reichman 1985; Cohen 1984), plans and intentions (Carberry 1990; Litman and Allen 1990; Grosz and Sidner 1986), prosody (Hirschberg and Pierrehumbert 1986; Butterworth 1980), nominal reference (Webber 1991; Grosz and Sidner 1986; Linde 1979), and tense (Webber 1988; Hwang and Schubert 1992; Song and Cohen 1991). However, just as with the early proposals regarding segmentation, many of these proposals are based on fairly informal studies. It is only recently that attempts have been made to quantitatively evaluate how utterance features correlate with independently justified segmentations. Many of the studies discussed in the preceding subsection take this approach. The types of linguistic features investigated ind u d e prosody (Grosz and Hirschberg 1992; Nakatani, Hirschberg, and Grosz 1995; Hirschberg and Nakatani 1996; Swerts 1995; Swerts and Ostendorf 1995), term repetition"
J97-1005,P93-1041,0,0.44939,"e. Hearst's (1994) TextTiling algorithm structures expository text into sequential segments based on term repetition. Hearst (1994) uses information retrieval metrics (see Section 4.1) to evaluate two versions of TextTiling against independently derived segmentations produced by at least three of seven human judges. Precision was .66 for the best version, compared with .81 for humans; recall was .61 compared with .71 for humans. The use of term repetition (and a related notion of lexical cohesion) is not unique to Hearst's work; related studies include Morris and Hirst (1991), Youmans (1991), Kozima (1993), and Reynar (1994). Unlike Hearst's work, these studies either use segmentations that are not empirically justified, or present only qualitative analyses of the correlation with linguistic devices. After identifying segments, and core and contributor relations within segments, Moser and Moore (1995) investigate whether cue words occur, where they occur, and 107 Computational Linguistics Volume 23, Number 1 what word occurs. In their talk, they presented results showing that the occurrence and placement of a discourse usage of a cue word correlates with relative order of core versus contributo"
J97-1005,P95-1015,1,0.687283,"Missing"
J97-1005,J93-4004,0,0.0169987,"Missing"
J97-1005,J92-4007,0,0.075146,"valuation, cause, and so on. Their coherence relations are similar to those posited in RST (Mann and Thompson 1988), which informs much work in generation. Polanyi (1988) distinguishes among four types of Discourse Constituent Units (DCUs) based on different types of structural relations (e.g., sequence). As in Grosz and Sidner's (1986) model, Polanyi (1988) proposes that DCUs (analogous to segments) are structured as a tree, and in both models, the tree structure of discourse constrains how the discourse evolves, and how referring expressions are processed. Recent work (Moore and Paris 1993; Moore and Pollack 1992) has argued that to account for explanation dialogues, it is necessary to independently model both RST relations and intentions. Researchers have begun to investigate the ability of humans to agree with one another on segmentation, and to propose methodologies for quantifying their findings. The types of discourse units being coded and the relations among them vary. Several studies have used trained coders to locally and globally structure spontaneous or read speech using the model of Grosz and Sidner (1986), including Grosz and Hirschberg 1992; Nakatani, Hirschberg, and Grosz 1995; Stifleman"
J97-1005,J91-1002,0,0.184351,"nteractions, with statistical significance. Hearst's (1994) TextTiling algorithm structures expository text into sequential segments based on term repetition. Hearst (1994) uses information retrieval metrics (see Section 4.1) to evaluate two versions of TextTiling against independently derived segmentations produced by at least three of seven human judges. Precision was .66 for the best version, compared with .81 for humans; recall was .61 compared with .71 for humans. The use of term repetition (and a related notion of lexical cohesion) is not unique to Hearst's work; related studies include Morris and Hirst (1991), Youmans (1991), Kozima (1993), and Reynar (1994). Unlike Hearst's work, these studies either use segmentations that are not empirically justified, or present only qualitative analyses of the correlation with linguistic devices. After identifying segments, and core and contributor relations within segments, Moser and Moore (1995) investigate whether cue words occur, where they occur, and 107 Computational Linguistics Volume 23, Number 1 what word occurs. In their talk, they presented results showing that the occurrence and placement of a discourse usage of a cue word correlates with relative"
J97-1005,P95-1018,0,0.125909,"urse structure, using a spoken corpus of database query interactions. Although the labelers had high levels of agreement, the segmentations were fairly trivial. Isard and Carletta (1995) presented 4 naive subjects and 1 expert coder with transcripts of task-oriented dialogues from the HCRC Map Task Corpus (Anderson et al. 1991). Utterance-like units referred to as moves were identified in the transcripts, and subjects were asked to identify transaction boundaries. Since reliability was lower than the .80 threshold, they concluded that their coding scheme and instructions required improvement. Moser and Moore (1995) investigated the reliability of various features defined in Relational Discourse Analysis (Moser, Moore, and Glendening 1995), based in part on RST. Their corpus consisted of written interactions between tutors and students, using 3 different tutors. Two coders were asked to identify segments, the core utterance of each segment, and certain intentional and informational relations between the core and the other contributor utterances. As reported in their talk (not in the paper), reliability on segment structure and core identification was well over the .80 threshold. Reliability on intentiona"
J97-1005,P94-1050,0,0.164918,"TextTiling algorithm structures expository text into sequential segments based on term repetition. Hearst (1994) uses information retrieval metrics (see Section 4.1) to evaluate two versions of TextTiling against independently derived segmentations produced by at least three of seven human judges. Precision was .66 for the best version, compared with .81 for humans; recall was .61 compared with .71 for humans. The use of term repetition (and a related notion of lexical cohesion) is not unique to Hearst's work; related studies include Morris and Hirst (1991), Youmans (1991), Kozima (1993), and Reynar (1994). Unlike Hearst's work, these studies either use segmentations that are not empirically justified, or present only qualitative analyses of the correlation with linguistic devices. After identifying segments, and core and contributor relations within segments, Moser and Moore (1995) investigate whether cue words occur, where they occur, and 107 Computational Linguistics Volume 23, Number 1 what word occurs. In their talk, they presented results showing that the occurrence and placement of a discourse usage of a cue word correlates with relative order of core versus contributor utterances. For e"
J97-1005,P90-1010,0,0.0125758,"Missing"
J97-1005,J88-2006,0,0.0180438,"Segmentation test, a reliability measure, and, for purposes of comparison with other work, percent agreement. 2.2 Correlation of Segmentation with Utterance Features The segmental structure of discourse has been claimed to constrain and be constrained by disparate phenomena, e.g., cue phrases (Hirschberg and Litman 1993; Grosz and Sidner 1986; Reichman 1985; Cohen 1984), plans and intentions (Carberry 1990; Litman and Allen 1990; Grosz and Sidner 1986), prosody (Hirschberg and Pierrehumbert 1986; Butterworth 1980), nominal reference (Webber 1991; Grosz and Sidner 1986; Linde 1979), and tense (Webber 1988; Hwang and Schubert 1992; Song and Cohen 1991). However, just as with the early proposals regarding segmentation, many of these proposals are based on fairly informal studies. It is only recently that attempts have been made to quantitatively evaluate how utterance features correlate with independently justified segmentations. Many of the studies discussed in the preceding subsection take this approach. The types of linguistic features investigated ind u d e prosody (Grosz and Hirschberg 1992; Nakatani, Hirschberg, and Grosz 1995; Hirschberg and Nakatani 1996; Swerts 1995; Swerts and Ostendor"
J97-1005,P88-1015,0,0.0343641,"written interactions between tutors and students, using 3 different tutors. Two coders were asked to identify segments, the core utterance of each segment, and certain intentional and informational relations between the core and the other contributor utterances. As reported in their talk (not in the paper), reliability on segment structure and core identification was well over the .80 threshold. Reliability on intentional and informational relations was around .75, high enough to support tentative conclusions. Finally, a method for segmenting dialogues based on a notion of control was used in Whittaker and Stenton (1988) and Walker and Whittaker (1990). Utterances were classified into four types, each of which was associated with a rule that assigned a controller; the discourse was then divided into segments, based on which speaker had control. Neither study presented any quantitative analysis of the ability to reliably perform the initial utterance classification. However, in Whittaker and Stenton (1988), a higher level of discourse structure based on topic shifts was agreed upon by at least 4 of 5 judges for 46 of the 56 control shifts. In sum, relatively few quantitative empirical studies have been made of"
J97-1005,P93-1020,1,\N,Missing
J97-1005,J96-2005,0,\N,Missing
N01-1027,A00-2028,1,\N,Missing
N01-1027,A00-2029,1,\N,Missing
N01-1027,P98-1122,0,\N,Missing
N01-1027,C98-1117,0,\N,Missing
N01-1027,P99-1040,1,\N,Missing
N03-2018,P01-1048,1,0.355468,"en, 2002). Building spoken dialogue tutoring systems has great potential benefit, for speech is the most natural and easy to use form of natural language interaction, and it supplies a rich source of prosodic and acoustic information about the speaker’s current mental state, which can be used to monitor the pedagogical effectiveness of student-computer interactions. The success of computer-based tutoring systems could increase if they predicted and adapted to student emotional states, e.g. reinforcing positive states, while rectifying negative states (Evens, 2002). Although (Ang et al., 2002; Litman et al., 2001; Batliner et al., 2000) have hand-labeled naturally-occurring utterances in a variety of corpora for various emotions, then extracted acoustic, prosodic and lexical features and used machine-learning techniques to develop predictive Scott Silliman LRDC, Univ. of Pittsburgh Pittsburgh PA, 15260, USA scotts@pitt.edu models, little work to date has addressed emotion detection in computer-based educational settings. In this paper we describe preliminary annotation of positive, negative, and neutral emotions in a human-human tutoring corpus and discuss the results of pilot machine learning experim"
N04-1026,J96-2004,0,0.0333022,"Missing"
N04-1026,E03-1072,0,0.00510517,"next set of non-acoustic-prosodic features are also automatically derivable from the transcribed dialogue. Turn begin and end times4 are retrieved from turn boundaries, as are the decisions as to whether a turn is a temporal barge-in (i.e., the turn began before the prior tutor turn ended) or a temporal overlap (i.e., the turn began and ended within a tutor turn). These features were motivated by the use of turn position as a feature for emotion prediction in (Ang et al., 2002), and the fact that measures of dialogue interactivity have been shown to correlate with learning gains in tutoring (Core et al., 2003). The number of words and syllables in a turn provide alternative ways to quantify turn duration (Litman et al., 2001). The last set of 6 non-acoustic-prosodic features represent additional syntactic, semantic, and dialogue information that had already been manually annotated in our transcriptions, and thus was available for use as predictors; as future research progresses, this information might one day be computed automatically. Our transcriber labels false starts (e.g., I do-don’t), syntactic questions, and semantic barge-ins. Semantic barge-ins occur when a student turn interrupts a tutor"
N04-1026,W04-2326,1,0.645377,"UTOR : Uh let us talk of one car first. STUDENT : ok. (EMOTION = NEUTRAL)   STUDENT : The engine (EMOTION = POSITIVE) TUTOR : Uh well engine is part of the car, so how can it exert force on itself? STUDENT : um... (EMOTION = NEGATIVE)   Figure 1: Excerpt from Human-Human Spoken Corpus 3 Annotating Student Emotion In our spoken dialogue tutoring corpus, student emotional states can only be identified indirectly – via what is said and/or how it is said. We have developed an annotation scheme for hand labeling the student turns in our corpus with respect to three types of perceived emotions (Litman and Forbes-Riley, 2004): Negative: a strong expression of emotion such as confused, bored, frustrated, uncertain. Because a syntactic question by definition expresses uncertainty, a turn containing only a question is by default labeled negative. An example negative turn is student in Figure 1. Evidence of a negative emotion comes from the lexical item “um”,  1 The human-human corpus corresponds to the humancomputer corpus that will result from ITSPOKE’s evaluation, in that both corpora are collected using the same experimental method, student pool, pre- and post-test, and physics problems. as well as acoustic and p"
N04-1026,N04-3002,1,0.510181,"tains 149 dialogues from 17 students. An average dialogue contains 45.3 student turns (242.2 words) and 44.1 tutor turns (1096.2 words). A corpus example is shown in Figure 1, containing the problem, the student’s original essay, and an annotated (Section 3) excerpt from the subsequent spoken dialogue (some punctuation is added for clarity). 2 TUTOR : If there is a car, what is it that exerts force on the car such that it accelerates forward? The Dialogue System and Corpus We are currently building a spoken dialogue tutorial system called ITSPOKE (Intelligent Tutoring SPOKEn dialogue system) (Litman and Silliman, 2004), with the goal of automatically predicting and adapting to student emotions. ITSPOKE uses as its “back-end” the text-based Why2-Atlas dialogue tutoring system (VanLehn et al., 2002). In ITSPOKE, a student types an essay answering a qualitative physics problem. ITSPOKE then engages the student in spoken dialogue to correct misconceptions and elicit more complete explanations, after which the student revises the essay, thereby ending the tutoring or causing another round of tutoring/essay revision. Student speech is digitized from microphone input and sent to the Sphinx2 recognizer. The most pr"
N04-1026,P01-1048,1,0.910842,"al emotional speech does not necessarily reflect natural speech (Batliner et al., 2003), such as found in tutoring dialogues. When actors are asked to read the same sentence with different emotions, they are restricted to conveying emotion using only acoustic and prosodic features. In natural interactions, however, speakers can convey emotions using other types of features, and can also combine acoustic-prosodic and other feature types. As a result of this mismatch, recent work motivated by spoken dialogue applications has started to use naturally-occurring speech to train emotion predictors (Litman et al., 2001; Lee et al., 2001; Ang et al., 2002; Lee et al., 2002; Batliner et al., 2003; Devillers et al., 2003; Shafran et al., 2003), but often predicts emotions using only acoustic-prosodic features that would be automatically available to a dialogue system in real-time. With noisier data and fewer features, it is not surprising that acoustic-prosodic features alone have been found to be of less predictive utility in these studies, leading spoken dialogue researchers to supplement such features with features based on other sources of information (e.g., lexical, syntactic, discourse). Our methodology"
N04-3002,N04-1026,1,0.488385,"re has been little examination of the ramifications of using a spoken modality for dialogue tutoring. To assess the impact and evaluate the utility of adding spoken language capabilities to dialogue tutoring systems, we have built ITSPOKE (Intelligent Tutoring SPOKEn dialogue system), a spoken dialogue system that uses the Why2-Atlas conceptual physics tutoring system (VanLehn et al., 2002) as its “back-end.” We are using ITSPOKE as a platform for examining whether acoustic-prosodic information can be used to improve the recognition of pedagogically useful information such as student emotion (Forbes-Riley and Litman, 2004; Litman and Forbes-Riley, 2004), and whether speech can improve the performance evaluations of dialogue tutoring systems (e.g., as measured by learning gains, efficiency, usability, etc.) (Ros´e et al., 2003). 2 Application Description ITSPOKE is a speech-enabled version of the Why2Atlas (VanLehn et al., 2002) text-based dialogue tutoring system. As in Why2-Atlas, a student first types a natural language answer to a qualitative physics problem. In ITSPOKE, however, the system engages the student in a spoken dialogue to correct misconceptions and elicit more complete explanations. Consider the"
N04-3002,W01-1609,0,0.017532,"and responding to student emotion. (Aist et al., 2002) have shown that adding emotional processing to a dialogue-based reading tutor increases student persistence. Information in the speech signal such as prosody has been shown to be a rich source of information for predicting emotional states in other types of dialogue interactions (Ang et al., 2002; Lee et al., 2002; Batliner et al., 2003; Devillers et al., 2003; Shafran et al., 2003). With advances in speech technology, several projects have begun to incorporate basic spoken language capabilities into their systems (Mostow and Aist, 2001; Fry et al., 2001; Graesser et al., 2001; Rickel and Johnson, 2000). However, to date there has been little examination of the ramifications of using a spoken modality for dialogue tutoring. To assess the impact and evaluate the utility of adding spoken language capabilities to dialogue tutoring systems, we have built ITSPOKE (Intelligent Tutoring SPOKEn dialogue system), a spoken dialogue system that uses the Why2-Atlas conceptual physics tutoring system (VanLehn et al., 2002) as its “back-end.” We are using ITSPOKE as a platform for examining whether acoustic-prosodic information can be used to improve the r"
N04-3002,W04-2326,1,0.571489,"of the ramifications of using a spoken modality for dialogue tutoring. To assess the impact and evaluate the utility of adding spoken language capabilities to dialogue tutoring systems, we have built ITSPOKE (Intelligent Tutoring SPOKEn dialogue system), a spoken dialogue system that uses the Why2-Atlas conceptual physics tutoring system (VanLehn et al., 2002) as its “back-end.” We are using ITSPOKE as a platform for examining whether acoustic-prosodic information can be used to improve the recognition of pedagogically useful information such as student emotion (Forbes-Riley and Litman, 2004; Litman and Forbes-Riley, 2004), and whether speech can improve the performance evaluations of dialogue tutoring systems (e.g., as measured by learning gains, efficiency, usability, etc.) (Ros´e et al., 2003). 2 Application Description ITSPOKE is a speech-enabled version of the Why2Atlas (VanLehn et al., 2002) text-based dialogue tutoring system. As in Why2-Atlas, a student first types a natural language answer to a qualitative physics problem. In ITSPOKE, however, the system engages the student in a spoken dialogue to correct misconceptions and elicit more complete explanations. Consider the screenshot shown in Figure 1. I"
N04-3002,W03-0205,1,0.79835,"Missing"
N04-3002,A97-2007,0,\N,Missing
N06-1034,bonneau-maynard-etal-2000-predictive,0,0.182224,"is a primary metric for evaluating the performance of these systems; it can be measured, e.g., by comparing student pretests taken prior to system use with posttests taken after system use. In other types of spoken dialogue systems, the user’s subjective judgments about using the system are often considered a primary system performance metric; e.g., user satisfaction has been measured via surveys which ask users to rate systems during use along dimensions such as task ease, speech input/output quality, user expectations and expertise, and user future use (M¨oller, 2005b; Walker et al., 2002; Bonneau-Maynard et al., 2000; Walker et al., 2000; Shriberg et al., 1992). However, it is expensive to run experiments over large numbers of users to obtain reliable system performance measures. The PARADISE model (Walker et al., 1997) proposes instead to predict system performance, using parameters representing interaction costs and benefits between system and user, including task success, dialogue efficiency, and dialogue quality. More formally, a set of interaction parameters are measured in a spoken dialogue system corpus, then used in a multivariate linear regression to predict the target performance variable. The r"
N06-1034,W04-2326,1,0.74347,"rmance. Finally, although student test scores before/after using ITSPOKE will be used as our student learning metric, we hypothesize that these scores may also play a role in predicting user satisfaction. 3.3 User Affect Parameters We hypothesize that user affect plays a role in predicting user satisfaction and student learning. Although affect parameters have not been used in other PARADISE studies (to our knowledge), they are generic; for example, in various spoken dialogue systems, user affect has been annotated and automatically predicted from e.g., acoustic-prosodic and lexical features (Litman and Forbes-Riley, 2004b; Lee et al., 2002; Ang et al., 2002; Batliner et al., 2003). As part of a larger investigation into emotion adaptation, we are manually annotating the student turns in our corpora for affective state. Currently, we are labeling 1 of 4 states of “Certainness”: certain, uncertain, neutral, mixed (certain and uncertain), and we are separately labeling 1 of 2 states of “Frustration/Anger”: frustrated/angry, non-frustrated/angry. These affective states 3 were found in pilot studies to be most prevalent in our tutoring dialogues4 , and are also of interest in other dialogue research, e.g. tutoring"
N06-1034,P04-1045,1,0.823582,"rmance. Finally, although student test scores before/after using ITSPOKE will be used as our student learning metric, we hypothesize that these scores may also play a role in predicting user satisfaction. 3.3 User Affect Parameters We hypothesize that user affect plays a role in predicting user satisfaction and student learning. Although affect parameters have not been used in other PARADISE studies (to our knowledge), they are generic; for example, in various spoken dialogue systems, user affect has been annotated and automatically predicted from e.g., acoustic-prosodic and lexical features (Litman and Forbes-Riley, 2004b; Lee et al., 2002; Ang et al., 2002; Batliner et al., 2003). As part of a larger investigation into emotion adaptation, we are manually annotating the student turns in our corpora for affective state. Currently, we are labeling 1 of 4 states of “Certainness”: certain, uncertain, neutral, mixed (certain and uncertain), and we are separately labeling 1 of 2 states of “Frustration/Anger”: frustrated/angry, non-frustrated/angry. These affective states 3 were found in pilot studies to be most prevalent in our tutoring dialogues4 , and are also of interest in other dialogue research, e.g. tutoring"
N06-1034,2005.sigdial-1.17,0,0.472849,"Missing"
N06-1034,H92-1009,0,0.0645641,"of these systems; it can be measured, e.g., by comparing student pretests taken prior to system use with posttests taken after system use. In other types of spoken dialogue systems, the user’s subjective judgments about using the system are often considered a primary system performance metric; e.g., user satisfaction has been measured via surveys which ask users to rate systems during use along dimensions such as task ease, speech input/output quality, user expectations and expertise, and user future use (M¨oller, 2005b; Walker et al., 2002; Bonneau-Maynard et al., 2000; Walker et al., 2000; Shriberg et al., 1992). However, it is expensive to run experiments over large numbers of users to obtain reliable system performance measures. The PARADISE model (Walker et al., 1997) proposes instead to predict system performance, using parameters representing interaction costs and benefits between system and user, including task success, dialogue efficiency, and dialogue quality. More formally, a set of interaction parameters are measured in a spoken dialogue system corpus, then used in a multivariate linear regression to predict the target performance variable. The resulting model is described by the formula be"
N06-1034,P97-1035,1,0.898043,"Missing"
N06-1035,N04-3002,1,0.878088,"Missing"
N06-1035,2005.sigdial-1.5,0,0.0698775,"Missing"
N06-1035,E06-1037,1,0.904803,"on in prior RL approaches to dialogue systems. In this paper, we use a corpus of dialogues of humans interacting with a spoken dialogue tutoring system to show the comparative utility of adding the three features of concept repetition, frustration level, and student performance. These features are not just unique to the tutoring domain but are important to dialogue systems in general. Our empirical results show that these features all lead to changes in what action the system should take, with concept repetition and frustration having the largest effects. This paper extends our previous work (Tetreault and Litman, 2006) which first presented a methodology for exploring whether adding more complex features to a representation of student state will beneficially alter tutor actions with respect to feedback. Here we present an empirical method of comparing the effects of each feature while also generalizing our findings to a different action choice of what type of follow-up question should a tutor ask the student (as opposed to what type of feedback should the tutor give). In complex domains such as tutoring, testing different policies with real or simulated students can be time consuming and costly so it is imp"
N06-1035,2005.sigdial-1.4,0,0.0642802,"n dialogue tutoring system. In addition, we also look at the effects of these features on what type of a question a tutoring system should ask at any state and compare it with our previous work on using feedback as the system action. 1 Introduction A host of issues confront spoken dialogue system designers, such as choosing the best system action to perform given any user state, and also selecting the right features to best represent the user state. While recent work has focused on using Reinforcement Learning (RL) to address the first issue (such as (Walker, 2000), (Henderson et al., 2005), (Williams et al., 2005a)), there has been very little empirical work on the issue of feature selection in prior RL approaches to dialogue systems. In this paper, we use a corpus of dialogues of humans interacting with a spoken dialogue tutoring system to show the comparative utility of adding the three features of concept repetition, frustration level, and student performance. These features are not just unique to the tutoring domain but are important to dialogue systems in general. Our empirical results show that these features all lead to changes in what action the system should take, with concept repetition and"
N07-1035,2005.sigdial-1.5,0,0.063739,"Missing"
N07-1035,N06-1035,1,0.81042,"ess the reliability of the expected cumulative reward for a given policy, and (2) perform a refined comparison between policies derived from different MDP models. NLP researchers frequently have to deal with issues of data sparsity. Whether the task is machine translation or named-entity recognition, the amount of data one has to train or test with can greatly impact the reliability and robustness of one’s models, results and conclusions. One research area that is particularly sensitive to the data sparsity issue is machine learning, specifiWe apply the proposed approach to our previous work (Tetreault and Litman, 2006) in using RL to improve a spoken dialogue tutoring system. In that work, a dataset of 100 dialogues was used to develop a methodology for selecting which user state features should be included in the MDP state-space. But are 100 dialogues enough to generate reliable policies? In this paper we apply our confidence inAbstract Past approaches for using reinforcement learning to derive dialog control policies have assumed that there was enough collected data to derive a reliable policy. In this paper we present a methodology for numerically constructing confidence intervals for the expected cumula"
N07-2001,2005.sigdial-1.6,0,0.0590345,"ulates realistic user behaviors in a statistical way. 1 Introduction Recently, user simulation has been used in the development of spoken dialog systems. In contrast to experiments with human subjects, which are usually expensive and time consuming, user simulation generates a large corpus of user behaviors in a low-cost and time-efficient manner. For example, user simulation has been used in evaluation of spoken dialog systems (L´opez-C´ozar et al., 2003) and to learn dialog strategies (Scheffler, 2002). However, these studies do not systematically evaluate how helpful a user simulation is. (Schatzmann et al., 2005) propose a set of evaluation measures to assess the realness of the simulated corpora (i.e. how similar are the simulated behaviors and human behaviors). Nevertheless, how realistic a simulated corpus needs to be for different tasks is still an open question. We hypothesize that for tasks like system evaluation, a more realistic simulated corpus is preferable. Since the system strategies are evaluated and adapted based on the analysis of these simulated dialog behaviors, we would expect that these behaviors are what we are going to see in the test phase when the systems interact with human use"
N07-2001,N06-1035,1,0.860394,"Missing"
N07-2011,P04-1045,1,0.785094,"tman@cs.pitt.edu Abstract We use χ2 to investigate the context dependency of student affect in our computer tutoring dialogues, targeting uncertainty in student answers in 3 automatically monitorable contexts. Our results show significant dependencies between uncertain answers and specific contexts. Identification and analysis of these dependencies is our first step in developing an adaptive version of our dialogue system. 1 Introduction Detecting and adapting to user affect is being explored by many researchers to improve dialogue system quality. Detection has received much attention (e.g., (Litman and Forbes-Riley, 2004; Lee and Narayanan, 2005)), but less work has been done on adaptation, due to the difficulty of developing responses and applying them at the right time. Most work on adaptation takes a context-independent approach: use the same type of response after all instances of an affective state. For example, Liu and Picard (2005)’s health assessment system responds with empathy to all instances of user stress. Research suggests, however, that it may be more effective to take a context-dependent approach: develop multiple responses for each affective state, whose use depends on the state’s context. E."
N07-2011,W06-1611,1,0.889642,"Missing"
N12-1010,W09-3933,0,0.466284,"disengagement for spoken dialogue systems, and then evaluating its usefulness as fully as possible prior to its implementation and deployment with real users. Disengaged users are highly undesirable in human-computer interaction because they increase 92 the potential for user dissatisfaction and task failure; thus over the past decade there has already been substantial prior work focused on detecting user disengagement and the closely related states of boredom, motivation and lack of interest (e.g., (Schuller et al., 2010; Wang and Hirschberg, 2011; Jeon et al., 2010; Schuller et al., 2009a; Bohus and Horvitz, 2009; Martalo et al., 2008; Porayska-Pomsta et al., 2008; Kapoor and Picard, 2005; Sidner and Lee, 2003; Forbes-Riley and Litman, 2011b)). Within this work, specific affect definitions vary slightly with the intention of being coherent within the application and domain and being relevant to the specific adaptation goal (Martalo et al., 2008). However, affective systems researchers generally agree that disengaged users show little involvement in the interaction, and often display facial, gestural and linguistic signals such as gaze avoidance, finger tapping, humming, sarcasm, et cetera. The feature"
N12-1010,W11-2036,1,0.826903,"e targeted user uncertainty and disengagement because manual annotation showed them to be the two most common user affective states in our system and both are negatively correlated with task success (Litman and Forbes-Riley, 2009; ForbesRiley and Litman, 2011b). Thus, we hypothesize that providing appropriate responses to these states would reduce their frequency, consequently improving task success. Although we address these user states in the tutoring domain, spoken dialogue researchers across domains and applications have investigated the automatic detection of both user uncertainty (e.g. (Drummond and Litman, 2011; PonBarry and Shieber, 2011; Paek and Ju, 2008; Alwan et al., 2007)) and user disengagement (e.g., (Schuller 91 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 91–102, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics et al., 2010; Wang and Hirschberg, 2011; Schuller et al., 2009a)), to improve system performance. The detection of user disengagement in particular has received substantial attention in recent years, due to growing awareness of its potential for negatively impacti"
N12-1010,W11-2019,0,0.143331,"Missing"
N12-1010,H93-1016,0,0.375853,"Missing"
N12-1010,W09-3940,1,0.923974,"otion is not full-blown (Cowie and Cornelius, 2003). 1 We previously showed that detecting and responding to user uncertainty during spoken dialogue computer tutoring significantly improves task success (Forbes-Riley and Litman, 2011a). We are now taking the next step: incorporating automatic detection and adaptation to user disengagement as well, with the goal of further improving task success. We targeted user uncertainty and disengagement because manual annotation showed them to be the two most common user affective states in our system and both are negatively correlated with task success (Litman and Forbes-Riley, 2009; ForbesRiley and Litman, 2011b). Thus, we hypothesize that providing appropriate responses to these states would reduce their frequency, consequently improving task success. Although we address these user states in the tutoring domain, spoken dialogue researchers across domains and applications have investigated the automatic detection of both user uncertainty (e.g. (Drummond and Litman, 2011; PonBarry and Shieber, 2011; Paek and Ju, 2008; Alwan et al., 2007)) and user disengagement (e.g., (Schuller 91 2012 Conference of the North American Chapter of the Association for Computational Linguist"
N12-1010,W09-3927,1,0.759527,"auto) and our gold standard DISE labels (manual), we first computed the total number of occurrences for each student, and then computed a bivariate Pearson’s correlation between this total and two different metrics of performance: learning gain (LG) and user satisfaction (US). In the tutoring domain, learning is the primary performance metric and as is common in this domain we compute it as normalized learning gain ((posttest score-pretest score)/(112 Spoken dialogue research has shown that redesigning a system in light of such correlational analysis can indeed yield performance improvements (Rotaru and Litman, 2009). Table 3: Correlations between Disengagement and both Satisfaction and Learning in ITSPOKE Corpus (N=72 users) Measure Mean (SD) Total Manual DISE Total Automatic DISE 12.3 (7.3) 12.6 (7.4) pretest score)). In spoken dialogue systems, user satisfaction is the primary performance metric and as is common in this domain we compute it by totaling over the user satisfaction survey scores.13 Table 3 shows first the mean and standard deviation for the DISE label over all students, the Pearson’s Correlation coefficient (R) and its significance (p). As shown, both our manual and automatic DISE labels"
N12-1010,J00-3003,0,0.244111,"Missing"
N12-1010,W11-2018,0,0.144971,"ntly improving task success. Although we address these user states in the tutoring domain, spoken dialogue researchers across domains and applications have investigated the automatic detection of both user uncertainty (e.g. (Drummond and Litman, 2011; PonBarry and Shieber, 2011; Paek and Ju, 2008; Alwan et al., 2007)) and user disengagement (e.g., (Schuller 91 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 91–102, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics et al., 2010; Wang and Hirschberg, 2011; Schuller et al., 2009a)), to improve system performance. The detection of user disengagement in particular has received substantial attention in recent years, due to growing awareness of its potential for negatively impacting commercial applications (Wang and Hirschberg, 2011; Schuller et al., 2009a). In this paper we present a model for automatically detecting user disengagement during spoken dialogue interactions. Intrinsic evaluation of our model yields results on par with those of prior work. However, we argue that while intrinsic evaluations are necessary, they aren’t sufficient when im"
N13-1098,W08-0123,0,0.0653905,"Missing"
N13-1098,W11-0133,0,0.032618,"Missing"
N13-1098,W11-2036,1,0.844208,"Thomason University of Pittsburgh Pittsburgh, PA 15260, USA jdt34@pitt.edu Diane Litman University of Pittsburgh Pittsburgh, PA 15260, USA litman@cs.pitt.edu Abstract system interface can render that system believably automated. An assumption of this WOZ data-gathering strategy is that user behavior will not vary substantially between the WOZ and automated (AUT) experimental setups. However, it was shown in a dialogue system that training with a small set of data from an automated system gave rise to better performance than training with a large set of data from an analogous wizarded system (Drummond and Litman, 2011). There, it was suggested that differences in system automation may be responsible for the performance gap. It is possible that user responses to these dialogue systems differed substantially. This paper aims to investigate this possibility by comparing data between a wizarded and automated version of a tutoring dialogue system. We hypothesize that what users say and how they say it will differ when the only change is whether the system’s speech recognition and correctness evaluation components are wizarded or automated. Wizard-of-Oz experimental setup in a dialogue system is commonly used to"
N13-1098,P07-1034,0,0.0246627,"Missing"
N13-1098,W12-1606,0,0.0313279,"re pronounced in the automated setup as users’ confidence is shaken. Additionally, some technical aspects of our methodology may impact these and future results: using different methods of normalization for user speech values than the one from this paper may affect visibility of observed differences between the setups. Future work may also attempt to address these differences directly. Intentional wizard error could be introduced to frustrate the user into responding as she would to a less accurate system, analogous to intentional errors produced in user simulation in spoken dialogue systems (Lee and Eskenazi, 2012). This strategy would be further informed by studies of the relationship between the system’s evaluation accuracy and student responses’ deviation from wizarded responses. Alternatively, post-hoc domain adaptation could be used to adjust the WOZ data. Generalizable statistical classification domain adaptation (Daum´e and Marcu, 2006) and adaptation demonstrated to work well in NLP-specific domains (Jiang and Zhai, 2007) both have the potential to adjust WOZ data to better match that seen by automated systems. Acknowledgments This work is funded by NSF award 0914615. We thank Scott Silliman for"
N13-1098,W01-1614,0,0.0555258,"automated systems than generally assumed. 1 Introduction In a Wizard-of-Oz (WOZ) experimental setup, some or all of the automated portions of a dialogue system are replaced with a hidden, human evaluator. This setup is often used to gather data from users who believe they are interacting with an automated system (Wolska et al., 2004; Andrews et al., 2008; Becker et al., 2011). This data can inform a downstream, real automated system. A WOZ experimental protocol calls for holding “all other input and output . . . constant so that the only unknown variable is who does the internal processing” (Paek, 2001). Thus, hiding the human wizard’s input by layers of 2 Dialogue System The data for this study is provided by the baseline conditions (one wizarded (WOZ) and one automated (AUT)) of two prior experiments with a spoken tutorial dialogue system. Users of this system were students recruited at our university, and each was a native speaker of American English. Users were novices and were tutored in basic Newtonian physics by the system. Each was engaged by a set of dialogues that illustrated one or more basic physics concepts. Those dialogues included remedial subdialogues that were accessed when"
N13-1098,W01-0902,0,\N,Missing
N13-1098,W04-0911,0,\N,Missing
N13-2002,H05-1073,0,0.0396289,"hich is time-consuming and costly. To obtain a higher quality annotated corpus, it is necessary to spend more time and money on data annotation. For this reason, one often has to accept some tradeoff between data quality and human effort. A significant proportion of corpora are unbalanced, where the distribution of class categories are 8 Joel Chan Department of Psychology University of Pittsburgh Pittsburgh, PA 15260, USA chozen86@gmail.com heavily skewed towards one or a few categories. Unbalanced corpora are common in a number of different tasks, such as emotion detection (Ang et al., 2002; Alm et al., 2005), sentiment classification (Li et al., 2012), polarity of opinion (Carvalho et al., 2011), uncertainty and correctness of student answers in tutoring dialogue systems (Forbes-Riley and Litman, 2011; Dzikovska et al., 2012), text classification (Forman, 2003), information extraction (Hoffmann et al., 2011), and so on1 . In this paper, we present a semi-automated annotation method that can reduce annotation effort for the class of binary unbalanced corpora. Here is our proposed annotation scheme: the first step is to build a high-recall classifier with some initial annotated data with an accepta"
N13-2002,ambati-etal-2010-high,0,0.0273399,"ways. Firstly, we lower the threshold for achieving a high recall classifier. Secondly, with pre-annotation, although people only perform a binary decision of whether the automatic classifier is either right or wrong, they have to go through all the unlabeled data one by one. In contrast, in our scheme, people go through only the positive predictions, which are much less than the whole unlabeled data, due to the unbalanced structure of the data. What’s more, reducing the annotation effort is the goal of this paper but not building a high recall classifier such as Prabhakaran et al. (2012) and Ambati et al. (2010). The approach proposed by Tetreault and Chodorow (2008) is similar to us. However, they assumed they had a high recall classifier but did not explicitly show how to build it. In addition, they did not provide extrinsic evaluation to see whether a corpus generated by pre-annotation is good enough to be used in real applications. 2.2 Uncertainty Prediction Uncertainty is a lack of knowledge about internal state (Pon-Barry and Shieber, 2011). In this paper, we only focus on detection of uncertainty on text. Commonly used features are lexical features such as unigram (Forbes-Riley and Litman, 201"
N13-2002,P11-2099,0,0.0223457,"s necessary to spend more time and money on data annotation. For this reason, one often has to accept some tradeoff between data quality and human effort. A significant proportion of corpora are unbalanced, where the distribution of class categories are 8 Joel Chan Department of Psychology University of Pittsburgh Pittsburgh, PA 15260, USA chozen86@gmail.com heavily skewed towards one or a few categories. Unbalanced corpora are common in a number of different tasks, such as emotion detection (Ang et al., 2002; Alm et al., 2005), sentiment classification (Li et al., 2012), polarity of opinion (Carvalho et al., 2011), uncertainty and correctness of student answers in tutoring dialogue systems (Forbes-Riley and Litman, 2011; Dzikovska et al., 2012), text classification (Forman, 2003), information extraction (Hoffmann et al., 2011), and so on1 . In this paper, we present a semi-automated annotation method that can reduce annotation effort for the class of binary unbalanced corpora. Here is our proposed annotation scheme: the first step is to build a high-recall classifier with some initial annotated data with an acceptable accuracy via a cost-sensitive approach. The second step is to apply this classifier t"
N13-2002,H01-1026,0,0.0307317,"with a small proportion of annotated data, a classifier can achieve comparable performance with all annotated data. However, these approaches still need considerable annotation effort when a large corpus has to be annotated. In that case, all predicted labels have to be rechecked by humans manually. In addition, none of them take advantage of unbalanced data. Another class of effort reduction techniques is pre-annotation, which uses supervised machinelearning systems to automatically assign labels to the whole data and subsequently lets human annotators correct them (Brants and Plaehn, 2000; Chiou et al., 2001; Xue et al., 2002; Ganchev et al., 2007; Chou et al., 2006; Rehbein et al., 2012). Generally speaking, our annotation method belongs to the class of pre-annotation methods. How2 This annotation scheme can also benefit other kinds of tasks. 9 ever, our method improves pre-annotation for unbalanced data in two ways. Firstly, we lower the threshold for achieving a high recall classifier. Secondly, with pre-annotation, although people only perform a binary decision of whether the automatic classifier is either right or wrong, they have to go through all the unlabeled data one by one. In contrast,"
N13-2002,W06-0602,0,0.0734344,"Missing"
N13-2002,E12-1048,0,0.017046,"lity and human effort. A significant proportion of corpora are unbalanced, where the distribution of class categories are 8 Joel Chan Department of Psychology University of Pittsburgh Pittsburgh, PA 15260, USA chozen86@gmail.com heavily skewed towards one or a few categories. Unbalanced corpora are common in a number of different tasks, such as emotion detection (Ang et al., 2002; Alm et al., 2005), sentiment classification (Li et al., 2012), polarity of opinion (Carvalho et al., 2011), uncertainty and correctness of student answers in tutoring dialogue systems (Forbes-Riley and Litman, 2011; Dzikovska et al., 2012), text classification (Forman, 2003), information extraction (Hoffmann et al., 2011), and so on1 . In this paper, we present a semi-automated annotation method that can reduce annotation effort for the class of binary unbalanced corpora. Here is our proposed annotation scheme: the first step is to build a high-recall classifier with some initial annotated data with an acceptable accuracy via a cost-sensitive approach. The second step is to apply this classifier to the rest of the unlabeled data, where the data are then classified with positive or negative labels. The last step is to manually c"
N13-2002,P11-1055,0,0.0149144,"distribution of class categories are 8 Joel Chan Department of Psychology University of Pittsburgh Pittsburgh, PA 15260, USA chozen86@gmail.com heavily skewed towards one or a few categories. Unbalanced corpora are common in a number of different tasks, such as emotion detection (Ang et al., 2002; Alm et al., 2005), sentiment classification (Li et al., 2012), polarity of opinion (Carvalho et al., 2011), uncertainty and correctness of student answers in tutoring dialogue systems (Forbes-Riley and Litman, 2011; Dzikovska et al., 2012), text classification (Forman, 2003), information extraction (Hoffmann et al., 2011), and so on1 . In this paper, we present a semi-automated annotation method that can reduce annotation effort for the class of binary unbalanced corpora. Here is our proposed annotation scheme: the first step is to build a high-recall classifier with some initial annotated data with an acceptable accuracy via a cost-sensitive approach. The second step is to apply this classifier to the rest of the unlabeled data, where the data are then classified with positive or negative labels. The last step is to manually check every positive label and correct it if it is wrong. To apply this method to wor"
N13-2002,C02-1145,0,0.0439588,"tion of annotated data, a classifier can achieve comparable performance with all annotated data. However, these approaches still need considerable annotation effort when a large corpus has to be annotated. In that case, all predicted labels have to be rechecked by humans manually. In addition, none of them take advantage of unbalanced data. Another class of effort reduction techniques is pre-annotation, which uses supervised machinelearning systems to automatically assign labels to the whole data and subsequently lets human annotators correct them (Brants and Plaehn, 2000; Chiou et al., 2001; Xue et al., 2002; Ganchev et al., 2007; Chou et al., 2006; Rehbein et al., 2012). Generally speaking, our annotation method belongs to the class of pre-annotation methods. How2 This annotation scheme can also benefit other kinds of tasks. 9 ever, our method improves pre-annotation for unbalanced data in two ways. Firstly, we lower the threshold for achieving a high recall classifier. Secondly, with pre-annotation, although people only perform a binary decision of whether the automatic classifier is either right or wrong, they have to go through all the unlabeled data one by one. In contrast, in our scheme, pe"
N13-2002,D07-1082,0,0.033312,"xtrinsic evaluation demonstrates the utility of our approach, by showing that results originally achieved using manually-obtained uncertainty annotations can be replicated using semi-automatically obtained uncertainty annotations. Second, a high recall predictor based on a cost-sensitive learner is proposed as a method to semi-automate the annotation of unbalanced classes such as uncertainty. 2 Related Work 2.1 Reducing Annotation Effort A number of semi-supervised learning methods have been proposed in the literature for reducing annotation effort, such as active learning (Cohn et al., 1994; Zhu and Hovy, 2007; Zhu et al., 2010), co-training (Blum and Mitchell, 1998) and self-training (Mihalcea, 2004). Active learning reduces annotation by carefully selecting more useful samples. Co-training relies on several conditional independent classifiers to tag new unlabeled data and self-training takes the advantage of full unlabeled data. These semisupervised learning methods demonstrate that with a small proportion of annotated data, a classifier can achieve comparable performance with all annotated data. However, these approaches still need considerable annotation effort when a large corpus has to be ann"
N13-2002,brants-plaehn-2000-interactive,0,\N,Missing
N13-2002,W12-3807,0,\N,Missing
N13-2002,W04-2405,0,\N,Missing
N13-2002,W08-1205,0,\N,Missing
N13-2002,W07-1509,0,\N,Missing
N13-2002,D12-1013,0,\N,Missing
N15-3004,P13-4028,0,0.164492,"ustering paradigm to estimate the number of students who mention a phrase (Fig. 1.c), which is challenging since different words can be used for the same meaning (i.e. synonym, different word order). We use K-Medoids (Kaufman and Rousseeuw, 1987) for two reasons. First, it works with an arbitrary distance matrix between datapoints. This gives us a chance to try different distance matrices. Since phrases in student responses are sparse (e.g., many appear only once), instead of using frequencybased similarity like cosine, we found it more useful to leverage semantic similarity based on SEMILAR (Rus et al., 2013). Second, it is robust to noise and outliers because it minimizes a sum of pairwise dis18 similarities instead of squared Euclidean distances. Since K-Medoids picks a random set of seeds to initialize as the cluster centers and we prefer phrases in the same cluster are similar to each other, the clustering algorithm runs 100 times and the result with the minimal within-cluster sum of the distances is retained. For setting the number of clusters without tuning, we adapted the method √ used in Wan and Yang (2008), by letting K = V , where K is the number of clusters and V is the number of candid"
N16-1010,P13-1020,0,0.0345312,"Missing"
N16-1010,P11-1049,0,0.676317,"ight of wi , often measured by the number of sentences or documents that contain the concept. The ILP-based summarization approach (Gillick and Favre, 2009) searches for an optimal assignment to the sentence and concept variables so that the selected summary sentences maximize coverage of important concepts. The relationship between concepts and sentences is captured by a co-occurrence matrix A ∈ RN ×M , where Aij = 1 indicates the i-th concept appears in the j-th sentence, and Aij = 0 otherwise. In the literature, bigrams are frequently used as a surrogate for concepts (Gillick et al., 2008; Berg-Kirkpatrick et al., 2011). We follow the convention and use ‘concept’ and ‘bigram’ interchangeably in the paper. max y,z s.t. PN i=1 wi zi PM j=1 Aij yj ≥ zi Aij yj ≤ zi PM j=1 lj yj ≤ L yj ∈ {0, 1}, zi ∈ {0, 1} (1) (2) (3) (4) (5) Two sets of linear constraints are specified to ensure the ILP validity: (1) a concept is selected if and only if at least one sentence carrying it has been selected (Eq. 2), and (2) all concepts in a sentence will 81 be selected if that sentence is selected (Eq. 3). Finally, the selected summary sentences are allowed to contain a total of L words or less (Eq. 4). 3 Our Approach Because of"
N16-1010,C12-1056,0,0.0177446,"ed of aggregating and displaying student responses in a mobile application (Luo et al., 2015; Fan et al., 2015). It adopts a clustering paradigm to address the lexical variety issue. In this work, we leverage matrix imputation to solve this problem and summarize student response at a sentence level. The integer linear programming framework has demonstrated substantial success on summarizing news documents (Gillick et al., 2008; Gillick et al., 2009; Woodsend and Lapata, 2012; Li et al., 2013). Previous studies try to improve this line of work by generating better estimates of concept weights. Galanis et al. (2012) proposed a support vector regression model to estimate bigram frequency in the summary. Berg-Kirkpatrick et al. (2011) explored a supervised approach to learn parameters using a cost-augmentative SVM. Different from the above approaches, we focus on the co-occurrence matrix instead of concept weights, which is another important component of the ILP framework. Most summarization work focuses on summarizing news documents, as driven by the DUC/TAC conferences. Notable systems include maximal marginal relevance (Carbonell and Goldstein, 1998), 84 Conclusion We make the first effort to summarize"
N16-1010,W09-1802,0,0.138358,"student feedback summarization task in terms of both ROUGE scores and human evaluation. 2 ILP Formulation Let D be a set of student responses that consist of M sentences in total. Let yj ∈ {0, 1}, j = {1, · · · , M } indicate if a sentence j is selected (yj = 1) or not (yj = 0) in the summary. Similarly, let N be the number of unique concepts in D. zi ∈ {0, 1}, i = {1, · · · , N } indicate the appearance of concepts in the summary. Each concept i is assigned a weight of wi , often measured by the number of sentences or documents that contain the concept. The ILP-based summarization approach (Gillick and Favre, 2009) searches for an optimal assignment to the sentence and concept variables so that the selected summary sentences maximize coverage of important concepts. The relationship between concepts and sentences is captured by a co-occurrence matrix A ∈ RN ×M , where Aij = 1 indicates the i-th concept appears in the j-th sentence, and Aij = 0 otherwise. In the literature, bigrams are frequently used as a surrogate for concepts (Gillick et al., 2008; Berg-Kirkpatrick et al., 2011). We follow the convention and use ‘concept’ and ‘bigram’ interchangeably in the paper. max y,z s.t. PN i=1 wi zi PM j=1 Aij y"
N16-1010,hong-etal-2014-repository,0,0.153072,"Missing"
N16-1010,P13-1099,0,0.0162714,"tman, 2015) proposes to summarize student responses by extracting phrases rather than sentences in order to meet the need of aggregating and displaying student responses in a mobile application (Luo et al., 2015; Fan et al., 2015). It adopts a clustering paradigm to address the lexical variety issue. In this work, we leverage matrix imputation to solve this problem and summarize student response at a sentence level. The integer linear programming framework has demonstrated substantial success on summarizing news documents (Gillick et al., 2008; Gillick et al., 2009; Woodsend and Lapata, 2012; Li et al., 2013). Previous studies try to improve this line of work by generating better estimates of concept weights. Galanis et al. (2012) proposed a support vector regression model to estimate bigram frequency in the summary. Berg-Kirkpatrick et al. (2011) explored a supervised approach to learn parameters using a cost-augmentative SVM. Different from the above approaches, we focus on the co-occurrence matrix instead of concept weights, which is another important component of the ILP framework. Most summarization work focuses on summarizing news documents, as driven by the DUC/TAC conferences. Notable syst"
N16-1010,N10-1134,0,0.145343,"Missing"
N16-1010,W04-1013,0,0.110635,"Missing"
N16-1010,D15-1227,1,0.365046,"es - importance of cell direction on materials properties System Summary (I LP BASELINE) - drawing and indexing unit cell direction - it was interesting to understand how to find apf and fd from last weeks class - south pole explorers died due to properties of tin System Summary (O UR A PPROACH) - crystal structure directions - surprisingly i found nothing interesting today . - unit cell indexing - vectors in unit cells - unit cell drawing and indexing - the importance of cell direction on material properties 7 Table 4: Example reference and system summaries. 6 Related Work Our previous work (Luo and Litman, 2015) proposes to summarize student responses by extracting phrases rather than sentences in order to meet the need of aggregating and displaying student responses in a mobile application (Luo et al., 2015; Fan et al., 2015). It adopts a clustering paradigm to address the lexical variety issue. In this work, we leverage matrix imputation to solve this problem and summarize student response at a sentence level. The integer linear programming framework has demonstrated substantial success on summarizing news documents (Gillick et al., 2008; Gillick et al., 2009; Woodsend and Lapata, 2012; Li et al.,"
N16-1010,N15-3004,1,0.513825,"class - south pole explorers died due to properties of tin System Summary (O UR A PPROACH) - crystal structure directions - surprisingly i found nothing interesting today . - unit cell indexing - vectors in unit cells - unit cell drawing and indexing - the importance of cell direction on material properties 7 Table 4: Example reference and system summaries. 6 Related Work Our previous work (Luo and Litman, 2015) proposes to summarize student responses by extracting phrases rather than sentences in order to meet the need of aggregating and displaying student responses in a mobile application (Luo et al., 2015; Fan et al., 2015). It adopts a clustering paradigm to address the lexical variety issue. In this work, we leverage matrix imputation to solve this problem and summarize student response at a sentence level. The integer linear programming framework has demonstrated substantial success on summarizing news documents (Gillick et al., 2008; Gillick et al., 2009; Woodsend and Lapata, 2012; Li et al., 2013). Previous studies try to improve this line of work by generating better estimates of concept weights. Galanis et al. (2012) proposed a support vector regression model to estimate bigram frequenc"
N16-1010,D12-1022,0,0.0171666,"r previous work (Luo and Litman, 2015) proposes to summarize student responses by extracting phrases rather than sentences in order to meet the need of aggregating and displaying student responses in a mobile application (Luo et al., 2015; Fan et al., 2015). It adopts a clustering paradigm to address the lexical variety issue. In this work, we leverage matrix imputation to solve this problem and summarize student response at a sentence level. The integer linear programming framework has demonstrated substantial success on summarizing news documents (Gillick et al., 2008; Gillick et al., 2009; Woodsend and Lapata, 2012; Li et al., 2013). Previous studies try to improve this line of work by generating better estimates of concept weights. Galanis et al. (2012) proposed a support vector regression model to estimate bigram frequency in the summary. Berg-Kirkpatrick et al. (2011) explored a supervised approach to learn parameters using a cost-augmentative SVM. Different from the above approaches, we focus on the co-occurrence matrix instead of concept weights, which is another important component of the ILP framework. Most summarization work focuses on summarizing news documents, as driven by the DUC/TAC confere"
N16-1010,C14-1187,1,0.862246,"l marginal relevance (Carbonell and Goldstein, 1998), 84 Conclusion We make the first effort to summarize student feedback using an integer linear programming framework with data imputation. Our approach allows sentences to share co-occurrence statistics and alleviates sparsity issue. Our experiments show that the proposed approach performs competitively against a range of baselines and shows promise for future automation of student feedback analysis. In the future, we may take advantage of the high quality student responses (Luo and Litman, 2016) and explore helpfulness-guided summarization (Xiong and Litman, 2014) to improve the summarization performance. We will also investigate whether the proposed approach benefits other informal text such as product reviews, social media discussions or spontaneous speech conversations, in which we expect the same sparsity issue occurs and the language expression is diverse. Acknowledgments This research is supported by an internal grant from the Learning Research and Development Center at the University of Pittsburgh. We thank Muhsin Menekse for providing the data set. We thank Jingtao Wang, Fan Zhang, Huy Nguyen and Zahra Rahimi for valuable suggestions about the"
N16-1168,E12-1036,0,0.587564,"ns. In this paper, we propose contextbased methods to improve the automatic identification of revision purposes in student argumentative writing. Argumentation plays an important role in analyzing many types of writing such as persuasive essays (Stab et al., 2014), scientific papers (Teufel, 2000) and law documents (Palau and Moens, 2009). In student papers, identifying revision purposes with respect to argument structure has been used to predict the grade improvement in the paper after revision (Zhang and Litman, 2015). Existing works on the analysis of writing revisions (Adler et al., 2011; Bronner and Monz, 2012; Daxenberger and Gurevych, 2013; Zhang and Litman, 2015) typically compare two versions of a text to extract revisions, then classify the purpose of each revision in isolation. That is, while limited contextual features such as revision location have been utilized in prior work, such features are computed from the revision being classified but typically not its neighbors. In addition, ordinary classifiers rather than structured prediction models are typically used. To increase the role of context during prediction, in this paper we 1) introduce new contextual features (e.g., the impact of a r"
N16-1168,D13-1055,0,0.561413,"opose contextbased methods to improve the automatic identification of revision purposes in student argumentative writing. Argumentation plays an important role in analyzing many types of writing such as persuasive essays (Stab et al., 2014), scientific papers (Teufel, 2000) and law documents (Palau and Moens, 2009). In student papers, identifying revision purposes with respect to argument structure has been used to predict the grade improvement in the paper after revision (Zhang and Litman, 2015). Existing works on the analysis of writing revisions (Adler et al., 2011; Bronner and Monz, 2012; Daxenberger and Gurevych, 2013; Zhang and Litman, 2015) typically compare two versions of a text to extract revisions, then classify the purpose of each revision in isolation. That is, while limited contextual features such as revision location have been utilized in prior work, such features are computed from the revision being classified but typically not its neighbors. In addition, ordinary classifiers rather than structured prediction models are typically used. To increase the role of context during prediction, in this paper we 1) introduce new contextual features (e.g., the impact of a revision on local text cohesion),"
N16-1168,P11-1099,0,0.020455,"on is a Claim revision as it modifies the Claim of the paper by removing “rhetorical questions.” This leads to the second Warrant revision, which deletes the Warrant for “rhetorical questions.” ture additional contextual information as text cohesion/coherence changes and revision dependencies. As our task focuses on identifying the argumentative purpose of writing revisions, work in argument mining is also relevant. In fact, many features for predicting argument structure (e.g., location, discourse connectives, punctuation) (Burstein and Marcu, 2003; Moens et al., 2007; Palau and Moens, 2009; Feng and Hirst, 2011) are also used in revision classification. In addition, Lawrence et al. (2014) use changes in topic to detect argumentation, which leads us to hypothesize that different types of argumentative revisions will have different impacts on text cohesion and coherence. Guo et al. (2011) and Park et al. (2015) both utilize Conditional Random Fields (CRFs) for identifying argumentative structures. While we focus on the different task of identifying revisions to argumentation, we similarly hypothesize that dependencies exist between revisions and thus utilize CRFs in our task. While our task is similar"
N16-1168,D11-1025,0,0.0293085,"endencies. As our task focuses on identifying the argumentative purpose of writing revisions, work in argument mining is also relevant. In fact, many features for predicting argument structure (e.g., location, discourse connectives, punctuation) (Burstein and Marcu, 2003; Moens et al., 2007; Palau and Moens, 2009; Feng and Hirst, 2011) are also used in revision classification. In addition, Lawrence et al. (2014) use changes in topic to detect argumentation, which leads us to hypothesize that different types of argumentative revisions will have different impacts on text cohesion and coherence. Guo et al. (2011) and Park et al. (2015) both utilize Conditional Random Fields (CRFs) for identifying argumentative structures. While we focus on the different task of identifying revisions to argumentation, we similarly hypothesize that dependencies exist between revisions and thus utilize CRFs in our task. While our task is similar to argument mining, a key difference is that the revisions do not always appear near each other. For example, a 5-paragraph long essay might have only two or three revisions located at different paragraphs. Thus, the types of previous revisions cannot always be used as the contex"
N16-1168,J97-1003,0,0.425117,"features from not only the aligned sentence pair representing the revision in question, but also for the sentence pairs before and after the revision. The second type (Coh) measures the cohesion and coherence changes in a 2-sentence block around the revision2 . Utilizing the cohesion and coherence difference. Inspired by (Lee et al., 2015; Vaughan and McDonald, 1986), we hypothesize that different revisions can have different impacts on the cohesion and coherence of the essay. We propose to extract features for both impact on cohesion (lexical) and impact on coherence (semantic). Inspired by (Hearst, 1997), sequences of blocks are created for sentences 2 In this paper we consider the most adjacent sentence only. 1426 Figure 2: Example of revision sequence transformation. Each square corresponds to a sentence in the essay, the number of the square represents the index of the sentence in the essay. Dark squares are sentences that are changed. In the example, the 2nd sentence of Draft 1 is modified, the 3rd sentence is deleted and a new sentence is added in Draft 2. in both Draft 1 and Draft 2 as demonstrated in Figure 1. Two types of features are extracted. The first type describes the cohesion a"
N16-1168,W14-2111,0,0.0133843,"torical questions.” This leads to the second Warrant revision, which deletes the Warrant for “rhetorical questions.” ture additional contextual information as text cohesion/coherence changes and revision dependencies. As our task focuses on identifying the argumentative purpose of writing revisions, work in argument mining is also relevant. In fact, many features for predicting argument structure (e.g., location, discourse connectives, punctuation) (Burstein and Marcu, 2003; Moens et al., 2007; Palau and Moens, 2009; Feng and Hirst, 2011) are also used in revision classification. In addition, Lawrence et al. (2014) use changes in topic to detect argumentation, which leads us to hypothesize that different types of argumentative revisions will have different impacts on text cohesion and coherence. Guo et al. (2011) and Park et al. (2015) both utilize Conditional Random Fields (CRFs) for identifying argumentative structures. While we focus on the different task of identifying revisions to argumentation, we similarly hypothesize that dependencies exist between revisions and thus utilize CRFs in our task. While our task is similar to argument mining, a key difference is that the revisions do not always appea"
N16-1168,W15-0506,0,0.0136809,"focuses on identifying the argumentative purpose of writing revisions, work in argument mining is also relevant. In fact, many features for predicting argument structure (e.g., location, discourse connectives, punctuation) (Burstein and Marcu, 2003; Moens et al., 2007; Palau and Moens, 2009; Feng and Hirst, 2011) are also used in revision classification. In addition, Lawrence et al. (2014) use changes in topic to detect argumentation, which leads us to hypothesize that different types of argumentative revisions will have different impacts on text cohesion and coherence. Guo et al. (2011) and Park et al. (2015) both utilize Conditional Random Fields (CRFs) for identifying argumentative structures. While we focus on the different task of identifying revisions to argumentation, we similarly hypothesize that dependencies exist between revisions and thus utilize CRFs in our task. While our task is similar to argument mining, a key difference is that the revisions do not always appear near each other. For example, a 5-paragraph long essay might have only two or three revisions located at different paragraphs. Thus, the types of previous revisions cannot always be used as the contextual information. Moreo"
N16-1168,D14-1006,0,0.0268352,"ross-validation with 300 features selected using learning gain ratio11 . For the SVM approach, we observe that the Coh features yield a significant improvement over the baseline features in Corpus B, and a nonsignificant improvement in Corpus A. This indicates that changes in text cohesion and coherence can in7 Revisions on cross-aligned pairs are marked as Surface. Similar to settings in (Daxenberger and Gurevych, 2013) 9 We compared three models used in discourse analysis and revision classification (C4.5 Decision Tree, SVM and Random Forests) (Burstein et al., 2003; Bronner and Monz, 2012; Stab and Gurevych, 2014) and SVM yielded the best performance. 10 SVM model implemented with Weka (Hall et al., 2009) and CRF model implemented with CRFSuite (Okazaki, 2007) 11 We tested with parameters 100, 200, 300, 500 on a development dataset disjoint from Corpora A and B and chose 300 which yielded the best performance. 8 100 80 60 40 20 0 Distribution of errors SVM CRFs Figure 3: The number of classification errors on Corpus A, “Warrant-General” represents classifying Warrant as General. deed improve the prediction of argumentative revision types. The Ext feature set - which computes features for not only the r"
N16-1168,P86-1015,0,0.384661,"ode part of speech (POS) unigrams and difference in POS tag counts. We implement this feature set as the baseline as our tasks are similar, then propose two new types of contextual features. The first type (Ext) extends prior work by extracting the baseline features from not only the aligned sentence pair representing the revision in question, but also for the sentence pairs before and after the revision. The second type (Coh) measures the cohesion and coherence changes in a 2-sentence block around the revision2 . Utilizing the cohesion and coherence difference. Inspired by (Lee et al., 2015; Vaughan and McDonald, 1986), we hypothesize that different revisions can have different impacts on the cohesion and coherence of the essay. We propose to extract features for both impact on cohesion (lexical) and impact on coherence (semantic). Inspired by (Hearst, 1997), sequences of blocks are created for sentences 2 In this paper we consider the most adjacent sentence only. 1426 Figure 2: Example of revision sequence transformation. Each square corresponds to a sentence in the essay, the number of the square represents the index of the sentence in the essay. Dark squares are sentences that are changed. In the example"
N16-1168,S15-2001,0,0.0459533,"Missing"
N16-1168,W14-1818,1,0.879882,"Missing"
N16-1168,W15-0616,1,0.912903,"h of an author’s revisions allows writing assistance systems to provide better rewriting suggestions. In this paper, we propose contextbased methods to improve the automatic identification of revision purposes in student argumentative writing. Argumentation plays an important role in analyzing many types of writing such as persuasive essays (Stab et al., 2014), scientific papers (Teufel, 2000) and law documents (Palau and Moens, 2009). In student papers, identifying revision purposes with respect to argument structure has been used to predict the grade improvement in the paper after revision (Zhang and Litman, 2015). Existing works on the analysis of writing revisions (Adler et al., 2011; Bronner and Monz, 2012; Daxenberger and Gurevych, 2013; Zhang and Litman, 2015) typically compare two versions of a text to extract revisions, then classify the purpose of each revision in isolation. That is, while limited contextual features such as revision location have been utilized in prior work, such features are computed from the revision being classified but typically not its neighbors. In addition, ordinary classifiers rather than structured prediction models are typically used. To increase the role of context"
N16-1168,N16-3008,1,0.872973,"Missing"
N16-3002,W06-1650,0,0.0261539,"rs with location information in comments. Thus, we extracted words and phrases that signal positional localization in comments of training data. This feature set includes hand-crafted regular expressions of location patterns (e.g., on page 5) (Xiong et al., 2010), location seed words (manually collected, e.g., page, thesis, conclusion), and location bigrams (automatically extracted given the location seeds, e.g., transition paragraph). For each location seed, phrase or regular expression, we count its occurrences or matches in the comment. • Paper content: motivated by topic word features in (Kim et al., 2006), this feature set was designed to model how much of a paper’s content/topic was mentioned in the comment. We first extracted bigrams with TF-IDF above average in the training data, and collected unigrams that make-up these bigrams, e.g., ‘civil’ and ‘war’ in ‘civil war’.5 Domain unigram feature is the number of collected unigrams in the comment. Window size feature is the length of maximal common text span between the comment and the paper (Ernst-Gerlach and Crane, 2008). Similarity feature searches for the highest similarity score between paper sentence to the comment. We extract 5 paper sen"
N16-3002,W14-1812,1,0.841621,"elen et al., 2010). Research in computer science, in turn, has used natural language processing and machine learning to build models for automatically identifying helpful 6 Text review input Instant-feedback SWoRD (Phase #2) 1 YES Submission order > threshold? NO Comment-Level Solution Prediction NO YES 3 Comments revised? Instant Feedback 2 S-ratio > threshold? YES NO Potentially revised text review (output to Phase #3) Figure 1: Architecture of Instant-feedback SWoRD. review properties, including localization and solution (Xiong and Litman, 2010; Nguyen and Litman, 2013; Xiong et al., 2012; Nguyen and Litman, 2014), as well as quality and tone (Ramachandran and Gehringer, 2015). While such prediction models have been evaluated intrinsically (i.e., with respect to predicting gold-standard labels), few have actually been incorporated into working peer review systems and evaluated extrinsically (Ramachandran and Gehringer, 2013; Nguyen et al., 2014). The SWoRD research project1 involves different active research threads for improving the utility of an existing web-based peer review system. Our research in the SWoRD project aims at building instant feedback components for improving the quality of textual pe"
N16-3008,P03-1054,0,0.0237134,"by aligning sentences across drafts. An added sentence or a deleted sentence is treated as aligned to null. The 1 rewriting assistant interface: www.cs.pitt.edu/ ˜zhangfan/argrewrite now supported on chrome and firefox browser only 2 revision correction component: www.cs.pitt.edu/ Tutorial ˜zhangfan/revisionCorrection.jar. to the web and java interface: www.cs.pitt.edu/ ˜zhangfan/argrewrite/tutorial.pdf 38 Figure 1: System structure of our rewriting assistance system. aligned pairs where the sentences in the pair are not identical are extracted as revisions. We first use the Stanford Parser (Klein and Manning, 2003) to break the original text into sentences and then align the sentences using the algorithm in our prior work (Zhang and Litman, 2014) which considers both sentence similarity (calculated using TF*IDF score) and the global context of sentences. Revision classification. Following the argumentative revision definition in our prior work (Zhang and Litman, 2015), revisions are first categorized to Content (Text-based) and Surface3 according to whether the revision changed the meaning of the essay or not. The Text-based revisions include Thesis/Ideas (Claim), Rebuttal, Reasoning (Warrant), Evidence"
N16-3008,W14-1818,1,0.63572,"of these systems, instructors typically can neither correct the errors made by 37 the automatic analysis nor observe/assess the students’ revision efforts. We argue that an intelligent writing assistant ought to be aware of the revision process; it should: 1) identify all significant changes made by a writer between the essay drafts, 2) automatically determine the purposes of these changes, 3) provide the writer the means to compare between drafts in an easy to understand visualization, and 4) support instructor monitoring and corrections in the revision process as well. In our previous work (Zhang and Litman, 2014; Zhang and Litman, 2015), we focused on 1) and 2), the automatic extraction and classification of revisions for argumentative writings. In this work, we extend our framework to integrate the automatic analyzer with a web-based interface to support student argumentative writings. The purpose of each change between revisions is demonstrated to the writer as a kind of feedback. If the author’s revision purpose is not correctly recognized, it indicates that the effect of the writer’s change might have not met the writer’s expectation, which suggests that the writer should revise their revisions."
N16-3008,W15-0616,1,0.84836,"ctors typically can neither correct the errors made by 37 the automatic analysis nor observe/assess the students’ revision efforts. We argue that an intelligent writing assistant ought to be aware of the revision process; it should: 1) identify all significant changes made by a writer between the essay drafts, 2) automatically determine the purposes of these changes, 3) provide the writer the means to compare between drafts in an easy to understand visualization, and 4) support instructor monitoring and corrections in the revision process as well. In our previous work (Zhang and Litman, 2014; Zhang and Litman, 2015), we focused on 1) and 2), the automatic extraction and classification of revisions for argumentative writings. In this work, we extend our framework to integrate the automatic analyzer with a web-based interface to support student argumentative writings. The purpose of each change between revisions is demonstrated to the writer as a kind of feedback. If the author’s revision purpose is not correctly recognized, it indicates that the effect of the writer’s change might have not met the writer’s expectation, which suggests that the writer should revise their revisions. The framework also connec"
P01-1048,A00-2029,1,\N,Missing
P01-1048,N01-1027,1,\N,Missing
P01-1048,P98-1122,0,\N,Missing
P01-1048,C98-1117,0,\N,Missing
P04-1045,N04-1026,1,0.795902,", an example path through the learned “lex” decision tree says predict negative if the utterance contains the word will but does not contain the word decrease). Understanding this result is an area for future research. Within the “+id” sets, we see that “lex” and “asr” perform the same in the NnN and NPN classifications; in EnE “lex+id” significantly outperforms “asr+id”. The utility of the “lex” features compared to “asr” also increases when combined with the “sp” features (with and without identifiers), for both NnN and NPN. Moreover, based on results in (Lee et al., 2002; Ang et al., 2002; Forbes-Riley and Litman, 2004), we hypothesized that combining speech and lexical features would result in better performance than either feature set alone. We instead found that the relative performance of these sets depends both on the emotion classification being predicted and the presence or absence of “id” features. Although consistently with prior research we find that the combined feature sets usually outperform the speech-only feature sets, the combined feature sets frequently perform worse than the lexical-only feature sets. However, we will see in Section 6 that combining knowledge sources does improve prediction"
P04-1045,W04-2326,1,0.569718,"onal, negative vs. nonnegative) can be developed using features typically available to a spoken dialogue system in real-time (e.g, acoustic-prosodic, lexical, dialogue, and/or contextual) (Batliner et al., 2000; Lee et al., 2001; Lee et al., 2002; Ang et al., 2002; Batliner et al., 2003; Shafran et al., 2003). In prior work we built on and generalized such research, by defining a three-way distinction between negative, neutral, and positive student emotional states that could be reliably annotated and accurately predicted in human-human spoken tutoring dialogues (ForbesRiley and Litman, 2004; Litman and Forbes-Riley, 2004). Like the non-tutoring studies, our results showed that combining feature types yielded the highest predictive accuracy. In this paper we investigate the application of our approach to a comparable corpus of computerhuman tutoring dialogues, which displays many different characteristics, such as shorter utterances, little student initiative, and non-overlapping speech. We investigate whether we can annotate and predict student emotions as accurately and whether the relative utility of speech and lexical features as predictors is the same, especially when the output of the speech recognizer is"
P04-1045,N04-3002,1,0.366427,"Missing"
P04-1045,P04-3028,1,0.204105,"ures. While adding identifier features typically also improves performance, combining lexical and speech features does not. Our analyses also suggest that prediction in consensus-labeled turns is harder than in agreed turns, and that prediction in our computerhuman corpus is harder and based on somewhat different features than in our human-human corpus. Our continuing work extends this methodology with the goal of enhancing ITSPOKE to predict and adapt to student emotions. We continue to manually annotate ITSPOKE data, and are exploring partial automation via semi-supervised machine learning (Maeireizo-Tokeshi et al., 2004). Further manual annotation might also improve reliability, as understanding systematic disagreements can lead to coding manual revisions. We are also expanding our feature set to include features suggested in prior dialogue research, tutoring-dependent features (e.g., pedagogical goal), and other features available in our logs (e.g., semantic analysis). Finally, we will explore how the recognized emotions can be used to improve system performance. First, we will label human tutor adaptations to emotional student turns in our human tutoring corpus; this labeling will be used to formulate adapt"
P04-3028,N04-1026,1,0.759015,"only 6 labeled examples in the training set. The Cotraining system added examples from the 140 “pseudo-labeled” examples1 in the Prediction Set. The size of the training set increased in each iteration by adding the 2 best examples (those with the highest confidence scores) labeled by the two learners. The Emotional learner and the NonEmotional learner were set to work with the set of features selected by the wrapper approach to optimize the precision (PPV and NPV) as described in section 4.1. We have applied Weka’s (Witten and Frank, 2000) AdaBoost’s version of j48 decision trees (as used in Forbes-Riley and Litman, 2004) to the 140 unseen examples of the test set for generating the learning curve shown in figure 4. Figure 4 illustrates the learning curve of the accuracy on the test set, taking the union of the set of features selected to label the examples. We used the 3 best features for PPV for the Emotional Learner and the best feature for NPV for the NonEmotional Learner (see Section 4.1). The x-axis shows the number of training examples added; the y-axis shows the accuracy of the classifier on test instances. We compare the learning curve from Co-training with a baseline of majority class and an upper-bo"
P04-3028,W04-2326,1,0.690256,"Missing"
P04-3028,N04-3002,1,0.873332,"Missing"
P06-1025,2005.sigdial-1.10,1,0.781632,"looking at speechbased computer tutoring dialogues instead of more commonly used information retrieval dialogues. Implementing spoken dialogue systems in a new domain has shown that many practices do not port well to the new domain (e.g. confirmation of long prompts (Kearns et al., 2002)). Tutoring, as a new domain for speech applications (Litman and Forbes-Riley, 2004; Pon-Barry et al., 2004), brings forward new factors that can be important for spoken dialogue design. Here we focus on certainty and correctness. Both factors have been shown to play an important role in the tutoring process (Forbes-Riley and Litman, 2005; Liscombe et al., 2005). A common practice in previous work on emotion prediction (Ang et al., 2002; Litman and Forbes-Riley, 2004) is to transform an initial finer level emotion annotation (five or more labels) into a coarser level annotation (2-3 labels). We wanted to understand if this practice can im193 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 193–200, c Sydney, July 2006. 2006 Association for Computational Linguistics pact the dependencies we observe from the data. To test this, we combine our two emotion1 fac"
P06-1025,P04-1044,0,0.172947,"t of the previous work has focused on lower level details of SRP: identifying components responsible for SRP (acoustic model, language model, search algorithm (Chase, 1997)) or prosodic characterization of SRP (Hirschberg et al., 2004). Diane J. Litman University of Pittsburgh Pittsburgh, USA litman@cs.pitt.edu We extend previous work by analyzing the relationship between SRP and higher level dialogue factors. Recent work has shown that dialogue design can benefit from several higher level dialogue factors: dialogue acts (Frampton and Lemon, 2005; Walker et al., 2001), pragmatic plausibility (Gabsdil and Lemon, 2004). Also, it is widely believed that user emotions, as another example of higher level factor, interact with SRP but, currently, there is little hard evidence to support this intuition. We perform our analysis on three high level dialogue factors: frustration/anger, certainty and correctness. Frustration and anger have been observed as the most frequent emotional class in many dialogue systems (Ang et al., 2002) and are associated with a higher word error rate (Bulyko et al., 2005). For this reason, we use the presence of emotions like frustration and anger as our first dialogue factor. Our othe"
P06-1025,W04-2326,1,0.918846,"2002) and are associated with a higher word error rate (Bulyko et al., 2005). For this reason, we use the presence of emotions like frustration and anger as our first dialogue factor. Our other two factors are inspired by another contribution of our study: looking at speechbased computer tutoring dialogues instead of more commonly used information retrieval dialogues. Implementing spoken dialogue systems in a new domain has shown that many practices do not port well to the new domain (e.g. confirmation of long prompts (Kearns et al., 2002)). Tutoring, as a new domain for speech applications (Litman and Forbes-Riley, 2004; Pon-Barry et al., 2004), brings forward new factors that can be important for spoken dialogue design. Here we focus on certainty and correctness. Both factors have been shown to play an important role in the tutoring process (Forbes-Riley and Litman, 2005; Liscombe et al., 2005). A common practice in previous work on emotion prediction (Ang et al., 2002; Litman and Forbes-Riley, 2004) is to transform an initial finer level emotion annotation (five or more labels) into a coarser level annotation (2-3 labels). We wanted to understand if this practice can im193 Proceedings of the 21st Internati"
P06-1025,P01-1066,0,0.0782414,"mplications for building dialogue systems. Most of the previous work has focused on lower level details of SRP: identifying components responsible for SRP (acoustic model, language model, search algorithm (Chase, 1997)) or prosodic characterization of SRP (Hirschberg et al., 2004). Diane J. Litman University of Pittsburgh Pittsburgh, USA litman@cs.pitt.edu We extend previous work by analyzing the relationship between SRP and higher level dialogue factors. Recent work has shown that dialogue design can benefit from several higher level dialogue factors: dialogue acts (Frampton and Lemon, 2005; Walker et al., 2001), pragmatic plausibility (Gabsdil and Lemon, 2004). Also, it is widely believed that user emotions, as another example of higher level factor, interact with SRP but, currently, there is little hard evidence to support this intuition. We perform our analysis on three high level dialogue factors: frustration/anger, certainty and correctness. Frustration and anger have been observed as the most frequent emotional class in many dialogue systems (Ang et al., 2002) and are associated with a higher word error rate (Bulyko et al., 2005). For this reason, we use the presence of emotions like frustratio"
P07-1046,P01-1016,0,0.0443819,"Missing"
P07-1046,J86-3001,0,0.720494,"sk. Thus, interacting with such systems can be characterized by an increased user cognitive load associated with listening to often lengthy system turns and the need to integrate the current information to the discussion overall (Oviatt et al., 2004). We hypothesize that one way to reduce the user’s cognitive load is to make explicit two pieces of information: the purpose of the current system turn, and how the system turn relates to the overall discussion. This information is implicitly encoded in the intentional structure of a discourse as proposed in the Grosz & Sidner theory of discourse (Grosz and Sidner, 1986). Consequently, in this paper we propose using a graphical representation of the discourse structure as a way of improving the performance of complex-domain dialogue systems (note that graphical output is required). We call it the Navigation Map (NM). The NM is a dynamic representation of the discourse segment hierarchy and the discourse segment purpose information enriched with several features (Section 3). To make a parallel with geography, as the system “navigates” with the user through the domain, the NM offers a cartographic view of the discussion. While a somewhat similar graphical repre"
P07-1046,N04-1024,0,0.0257778,"Missing"
P07-1046,P96-1038,0,0.108019,"Missing"
P07-1046,N04-3002,1,0.918165,"er study to investigate if users perceive the NM as helpful in a tutoring spoken dialogue system. From the users’ perspective, our results show that the NM presence allows them to better identify and follow the tutoring plan and to better integrate the instruction. It was also easier for users to concentrate and to learn from the system if the NM was present. Our preliminary analysis on objective metrics further strengthens these findings. 1 Introduction With recent advances in spoken dialogue system technologies, researchers have turned their attention to more complex domains (e.g. tutoring (Litman and Silliman, 2004; Pon-Barry et al., 2006), technical support (Acomb et al., 2007), medication assistance (Allen et al., 2006)). These domains bring forward new challenges and issues that can affect the usability of such systems: increased task complexity, user’s lack of or limited task knowledge, and longer system turns. In typical information access dialogue systems, the task is relatively simple: get the information from the user and return the query results with minimal complexity added by confirmation dialogues. Moreover, in most cases, users have knowledge about the task. However, in complex domains thin"
P07-1046,P93-1020,1,0.572852,"system uses a GUI-based interaction (no speech/text input, no speech output) while we look at a speech-based system. Also, their underlying task (air travel domain) is much simpler than our tutoring task. In addition, the SIH is not always available and users have to activate it manually. Other visual improvements for dialogue-based computer tutors have been explored in the past (e.g. talking heads (Graesser et al., 2003)). However, implementing the NM in a new domain requires little expertise as previous work has shown that naïve users can reliably annotate the information needed for the NM (Passonneau and Litman, 1993). Our NM design choices should also have an equivalent in a new domain (e.g. displaying the recognized user answer can be the equivalent of the correct answers). Other NM usages can also be imagined: e.g. reducing the length of the system turns by removing text information that is implicitly represented in the NM. 7 Conclusions & Future work In this paper we explore the utility of the Navigation Map, a graphical representation of the discourse structure. As our first step towards understanding the benefits of the NM, we ran a user study to investigate if users perceive the NM as useful. From t"
P07-1046,W06-1611,1,0.880069,"Missing"
P07-1046,W07-0304,0,\N,Missing
P08-1071,P04-1009,0,0.0305936,"rediction models of human judgments using previously proposed automatic measures, we find that we cannot reliably predict human ratings using a regression model, but we can predict human rankings by a ranking model. 1 Introduction User simulation has been widely used in different phases in spoken dialog system development. In the system development phase, user simulation is used in training different system components. For example, (Levin et al., 2000) and (Scheffler, 2002) exploit user simulations to generate large corpora for using Reinforcement Learning to develop dialog strategies, while (Chung, 2004) implement user simulation to train the speech recognizer and understanding components. In this study, we also strive to develop a prediction model of the rankings of the simulated users’ performance. However, our approach use human judgments as the gold standard. Although to date there are few studies that use human judges to directly assess the quality of user simulation, we believe that this is a reliable approach to assess the simulated corpora as well as an important step towards developing a comprehensive set of user simulation evaluation measures. First, we can estimate the difficulty o"
P08-1071,P02-1040,0,0.0818777,"eral new automatic measures to form a comprehensive set of statistical evaluation measures. The first group of measures investigates how much information is transmitted in the dialog and how active the dialog participants are. The second group of measures analyzes the style of 623 the dialog and the last group of measures examines the efficiency of the dialogs. While these automatic measures are handy to use, these measures have not been validated by humans. There are well-known practices which validate automatic measures using human judgments. For example, in machine translation, BLEU score (Papineni et al., 2002) is developed to assess the quality of machine translated sentences. Statistical analysis is used to validate this score by showing that BLEU score is highly correlated with the human judgment. In this study, we validate a subset of the automatic measures proposed by (Schatzmann et al., 2005) by correlating the measures with human judgments. We follow the design of (Linguistic Data Consortium, 2005) in obtaining human judgments. We call our study an assessment study. 3 System and User Simulation Models In this section, we describe our dialog system (ITSPOKE) and the user simulation models whic"
P08-1071,2005.sigdial-1.6,0,0.0726375,"performance. Different evaluation approaches are proposed for different tasks. Some studies (e.g., (Walker et al., 1997)) build regression models to predict user satisfaction scores from the system log as well as the user survey. There are also studies that evaluate different systems/system components by ranking the quality of their outputs. For example, (Walker et al., 2001) train a ranking model that ranks the outputs of different language generation strategies based on human judges’ rankings. In this study, we build both a regression model and a ranking model to evaluate user simulation. (Schatzmann et al., 2005) summarize some broadly used automatic evaluation measures for user simulation and integrate several new automatic measures to form a comprehensive set of statistical evaluation measures. The first group of measures investigates how much information is transmitted in the dialog and how active the dialog participants are. The second group of measures analyzes the style of 623 the dialog and the last group of measures examines the efficiency of the dialogs. While these automatic measures are handy to use, these measures have not been validated by humans. There are well-known practices which vali"
P08-1071,P97-1035,1,0.771601,"d automatic measures to predict human judgments, we cannot reliably predict human ratings using a regression model, but we can consistently mimic human judges’ rankings using a ranking model. We suggest that this ranking model can be used to quickly assess the quality of a new simulation model without manual efforts by ranking the new model against the old models. 2 Related Work A lot of research has been done in evaluating different components of Spoken Dialog Systems as well as overall system performance. Different evaluation approaches are proposed for different tasks. Some studies (e.g., (Walker et al., 1997)) build regression models to predict user satisfaction scores from the system log as well as the user survey. There are also studies that evaluate different systems/system components by ranking the quality of their outputs. For example, (Walker et al., 2001) train a ranking model that ranks the outputs of different language generation strategies based on human judges’ rankings. In this study, we build both a regression model and a ranking model to evaluate user simulation. (Schatzmann et al., 2005) summarize some broadly used automatic evaluation measures for user simulation and integrate seve"
P08-1071,N01-1003,0,0.0243419,"he quality of a new simulation model without manual efforts by ranking the new model against the old models. 2 Related Work A lot of research has been done in evaluating different components of Spoken Dialog Systems as well as overall system performance. Different evaluation approaches are proposed for different tasks. Some studies (e.g., (Walker et al., 1997)) build regression models to predict user satisfaction scores from the system log as well as the user survey. There are also studies that evaluate different systems/system components by ranking the quality of their outputs. For example, (Walker et al., 2001) train a ranking model that ranks the outputs of different language generation strategies based on human judges’ rankings. In this study, we build both a regression model and a ranking model to evaluate user simulation. (Schatzmann et al., 2005) summarize some broadly used automatic evaluation measures for user simulation and integrate several new automatic measures to form a comprehensive set of statistical evaluation measures. The first group of measures investigates how much information is transmitted in the dialog and how active the dialog participants are. The second group of measures ana"
P09-1100,2005.sigdial-1.6,0,0.0330806,"ora are often used as training corpora for using MDPs to learn new dialog strategies, in Section 4.2 we estimate how different the learned dialog strategies would be when trained from different simulated corpora. Another way to use user simulation is to test dialog systems. Therefore, in Section 4.3, we compare the user actions predicted by the various simulation models with actual human user actions. 4.1 Measures on Corpus Level Dialog Behaviors We compare the dialog corpora generated by user simulations to our human user corpus using a comprehensive set of corpus level measures proposed by (Schatzmann et al., 2005). Here, we use a subset of the measures which describe high-level dialog features that are applicable to our data. The measures we use include the number of student turns (Sturn), the number of tutor turns (Tturn), the number of words per student turn (Swordrate), the number of words per tutor turn (Twordrate), the ratio of system/user words per dialog (WordRatio), The first author of the paper acts as the domain expert. 891 than the median from the training corpus and -100 otherwise. The action choice of the tutoring system is to give a strong (s) or weak (w) feedback. A strong feedback clear"
P09-1100,N07-2001,1,0.844031,"o give a strong (s) or weak (w) feedback. A strong feedback clearly indicates the correctness of the current student answer while the weak feedback does not. For example, the second system turn in Table 1 contains a weak feedback. If the system says “Your answer is incorrect” at the beginning of this turn, that would be a strong feedback. In order to simulate student certainty, we simply output the student certainty originally associated in each student utterance. Thus, the output of the KC Models here is a student utterance along with the student certainty (cert, ncert). In a previous study (Ai et al., 2007), we investigated the impact of different MDP configurations by comparing the ECRs of the learned dialog strategies. Here, we use one of the best-performing MDP configurations, but vary the simulated corpora that we train the dialog strategies on. Our goal is to see which user simulation performs better in generating a training corpus for dialog strategy learning. and the percentage of correct answers (cRate). 4.2 Measures on Dialog Strategy Learning In this section, we introduce two measures to compare the simulations based on their performance on a dialog strategy learning task. In recent st"
P09-1100,N07-2038,0,0.505384,"((Schatzmann et al., 2007b), (Georgila et al., 2008)). While this approach takes advantage of observed user behaviors in predicting future user behaviors, it suffers from the problem of learning probabilities from one group of users while potentially using them with another group of users. The accuracy of the learned probabilities becomes more questionable when the collected human corpus is small. However, this is a common problem in building new dialog systems, when often no data1 or only a small amount of data is available. An alternative approach is to handcraft user action probabilities ((Schatzmann et al., 2007a), (Janarthanam and Lemon, 2008)). This approach is less dataintensive, but requires nontrivial work by domain experts. What is more, as the number of probabilities increases, it is hard even for the experts to set the probabilities. Since both handcrafting and training user action probabilities have their own pros and cons, it is an interesting research question to investigate which approach is better for a certain task given the amount of data that is available. In this study, we investigate a manual and a trained approach in setting up user action probabilities, applied to building the sam"
P09-1100,2007.sigdial-1.48,0,0.342853,"((Schatzmann et al., 2007b), (Georgila et al., 2008)). While this approach takes advantage of observed user behaviors in predicting future user behaviors, it suffers from the problem of learning probabilities from one group of users while potentially using them with another group of users. The accuracy of the learned probabilities becomes more questionable when the collected human corpus is small. However, this is a common problem in building new dialog systems, when often no data1 or only a small amount of data is available. An alternative approach is to handcraft user action probabilities ((Schatzmann et al., 2007a), (Janarthanam and Lemon, 2008)). This approach is less dataintensive, but requires nontrivial work by domain experts. What is more, as the number of probabilities increases, it is hard even for the experts to set the probabilities. Since both handcrafting and training user action probabilities have their own pros and cons, it is an interesting research question to investigate which approach is better for a certain task given the amount of data that is available. In this study, we investigate a manual and a trained approach in setting up user action probabilities, applied to building the sam"
P09-1100,P08-2013,0,0.187754,"tting Up User Action Probabilities in User Simulations for Dialog System Development Hua Ai University of Pittsburgh Pittsburgh PA, 15260, USA hua@cs.pitt.edu Diane Litman University of Pittsburgh Pittsburgh PA, 15260, USA litman@cs.pitt.edu Abstract most of these current user simulation techniques use probabilistic models to generate user actions, how to set up the probabilities in the simulations is another important problem to solve. One general approach to set up user action probabilities is to learn the probabilities from a collected human user dialog corpus ((Schatzmann et al., 2007b), (Georgila et al., 2008)). While this approach takes advantage of observed user behaviors in predicting future user behaviors, it suffers from the problem of learning probabilities from one group of users while potentially using them with another group of users. The accuracy of the learned probabilities becomes more questionable when the collected human corpus is small. However, this is a common problem in building new dialog systems, when often no data1 or only a small amount of data is available. An alternative approach is to handcraft user action probabilities ((Schatzmann et al., 2007a), (Janarthanam and Lemon, 2"
P09-1100,2007.sigdial-1.11,0,0.02929,"is to estimate these probabilities from human user data. However, when building a new dialog system, usually no data or only a small amount of data is available. In this study, we compare estimating user probabilities from a small user data set versus handcrafting the probabilities. We discuss the pros and cons of both solutions for different dialog system development tasks. 1 Introduction User simulations are widely used in spoken dialog system development. Recent studies use user simulations to generate training corpora to learn dialog strategies automatically ((Williams and Young, 2007), (Lemon and Liu, 2007)), or to evaluate dialog system performance (L´opez-C´ozar et al., 2003). Most studies show that using user simulations significantly improves dialog system performance as well as speeds up system development. Since user simulation is such a useful tool, dialog system researchers have studied how to build user simulations from a variety of perspectives. Some studies look into the impact of training data on user simulations. For example, (Georgila et al., 2008) observe differences between simulated users trained from human users of different age groups. Other studies explore different simulatio"
P09-1100,N04-3002,1,0.787769,"Section 3.2.1. There are 40 probabilities to set up in this model3 . We will explain different approaches to assign these probabilities later in Section 3.2.2. 2 3 System and User Simulations Related Work Most current simulation models are probabilistic models in which the models simulate user actions based on dialog context features (Schatzmann et al., 2006). We represent these models as: P (user action|f eature1 , . . .,f eaturen ) In this section, we describe the dialog system, the human user corpus we collected with the system, and the user simulation we used. 3.1 (1) The ITSPOKE system (Litman and Silliman, 2004) is an Intelligent Tutoring System which teaches Newtonian physics. It is a speechenhanced version of the Why2-Atlas tutoring system (Vanlehn et al., 2002). During the interaction with students, the system initiates a spoken tutoring dialog to correct misconceptions and to The number of probabilities involved in this model is: (# of possible actions-1) ∗ n Y (# of feature values). System and Corpus (2) k=1 Some studies handcraft these probabilities. For example, (Schatzmann et al., 2007a) condition the 3 There are 2 possible actions in our model, 20 possible values for the first feature qClust"
P09-1100,radev-etal-2002-evaluating,0,0.0151284,"obabilistic model, the model will take an action stochastically after the same tutor turn. In other words, we need to take into account the probability for the simulation to predict the right human user action. If the simulation outputs the right action with a small probability, it is less likely that this simulation can correctly predict human user behaviors when generating a large dialog corpus. We consider a simulated action associated with a higher probability to be ranked higher than an action with a lower probability. Then, we use the reciprocal ranking from information retrieval tasks (Radev et al., 2002) to assess the simulation performance10 . Mean Reciprocal Ranking is defined as: M RR = A 1 X 1 A ranki 5 Results We let all user simulations interact with our dialog system, where each simulates 250 low learners and 250 high learners. In this section, we report the results of applying the evaluation measures we discuss in Section 4 on comparing simulated and human user corpora. When we talk about significant results in the statistics tests below, we always mean that the p-value of the test is ≤ 0.05. 5.1 Comparing on Corpus Level Dialog Behavior Figure 1 shows the results of comparisons using"
P11-2088,W06-1650,0,0.228447,"ness of peer reviews Related Work Prior studies of peer review in the Natural Language Processing field have not focused on helpfulness prediction, but instead have been concerned with issues such as highlighting key sentences in papers (Sandor and Vorndran, 2009), detecting important feedback features in reviews (Cho, 2008; Xiong and Litman, 2010), and adapting peer-review assignment (Garcia, 2010). However, given some similarity between peer reviews and other review types, we hypothesize that techniques used to predict review helpfulness in other domains can also be applied to peer reviews. Kim et al. (2006) used regression to predict the helpfulness ranking of product reviews based on various classes of linguistic features. Ghose and Ipeirotis (2010) further examined the socio-economic impact of product reviews using a similar approach and suggested the usefulness of subjectivity analysis. Another study (Liu et al., 2008) of movie reviews showed that helpfulness depends on reviewers’ expertise, their writing style, and the timeliness of the review. Tsur and Rappoport (2009) proposed RevRank to select the most helpful book reviews in an unsupervised fashion based on review lexicons. However, stud"
P11-2088,C00-1072,0,0.196353,"Missing"
P11-2088,P05-1012,0,0.084087,"Missing"
P11-2088,W09-3605,0,0.130812,"to enhance the effectiveness of existing peer-review systems, we propose to automatically predict the helpfulness of peer reviews. In this paper, we examine prior techniques that have been used to successfully rank helpfulness for product reviews, and adapt them to the peer-review domain. In particular, we use an SVM regression algorithm to predict the helpfulness of peer reviews Related Work Prior studies of peer review in the Natural Language Processing field have not focused on helpfulness prediction, but instead have been concerned with issues such as highlighting key sentences in papers (Sandor and Vorndran, 2009), detecting important feedback features in reviews (Cho, 2008; Xiong and Litman, 2010), and adapting peer-review assignment (Garcia, 2010). However, given some similarity between peer reviews and other review types, we hypothesize that techniques used to predict review helpfulness in other domains can also be applied to peer reviews. Kim et al. (2006) used regression to predict the helpfulness ranking of product reviews based on various classes of linguistic features. Ghose and Ipeirotis (2010) further examined the socio-economic impact of product reviews using a similar approach and suggested"
P16-1107,W14-2107,0,0.0987383,"Missing"
P16-1107,P12-2041,0,0.541332,"makes use of contextual features extracted from surrounding sentences of source and target components as well as from topic information of the writings. 1127 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1127–1137, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics Prior argumentative relation mining studies have often used features extracted from argument components to model different aspects of the relations between the components, e.g., relative distance, word pairs, semantic similarity, textual entailment (Cabrio and Villata, 2012; Stab and Gurevych, ˇ 2014b; Boltuˇzi´c and Snajder, 2014; Peldszus and Stede, 2015b). Features extracted from the text surrounding the components have been less explored, e.g., using words and their part-of-speech from adjacent sentences (Peldszus, 2014). The first hypothesis investigated in this paper is that the discourse relations of argument components with adjacent sentences (called context windows in this study, a formal definition is given in §5.3) can help characterize the argumentative relations that connect pairs of argument components. Reconsidering the example in Figure 1, withou"
P16-1107,W01-1605,0,0.175704,"mmon words between the covering sentence with preceding context sentences, and with following context sentences, for source and target components. Discourse relation: for both source and target components, we extract discourse relations between context sentences, and within the covering sentence. We also extract discourse relations between each pair of source context sentence and target context sentence. Each relation defines a boolean feature. We extract both Penn Discourse Treebank (PDTB) relations (Prasad et al., 2008) and Rhetorical Structure Theory Discourse Treebank (RST-DTB) relations (Carlson et al., 2001) using publicly available discourse parsers (Ji and Eisenstein, 2014; Wang and Lan, 2015). Each PDTB relation has sense label defined in a 3-layered (class, type, subtype), e.g., CONTINGENCY.Cause.result. While there are only four semantic class labels at the class-level which may not cover well different aspects of argumentative relation, subtype-level output is not available given the discourse parser we use. Thus, we use relations at type-level as features. For RSTDTB relations, we use only relation labels, but ignore the nucleus and satellite labels of components as they do not provide mor"
P16-1107,W13-2707,0,0.0751546,"that they realize the effects of the negative habit[P remise] ... Introduction By supporting tasks such as automatically identifying argument components1 (e.g., premises, claims) in text, and the argumentative relations (e.g., support, attack) between components, argument (argumentation) mining has been studied for applications in different research fields such as document summarization (Teufel and Moens, ˇ 2002), opinion mining (Boltuˇzi´c and Snajder, 2014), automated essay evaluation (Burstein et al., 2003), legal information systems (Palau and Moens, 2009), and policy modeling platforms (Florou et al., 2013). Given a pair of argument components with one component as the source and the other as the target, argumentative relation mining involves determining whether a relation holds from the source to the target, and classifying the argumentative function of the relation (e.g., support vs. attack). Ar1 There is no consensus yet on an annotation scheme for argument components, or on the minimal textual units to be annotated. We follow Peldszus and Stede (2013) and consider “argument mining as the automatic discovery of an argumentative text portion, and the identification of the relevant components o"
P16-1107,P14-1002,0,0.0252117,"ntences, and with following context sentences, for source and target components. Discourse relation: for both source and target components, we extract discourse relations between context sentences, and within the covering sentence. We also extract discourse relations between each pair of source context sentence and target context sentence. Each relation defines a boolean feature. We extract both Penn Discourse Treebank (PDTB) relations (Prasad et al., 2008) and Rhetorical Structure Theory Discourse Treebank (RST-DTB) relations (Carlson et al., 2001) using publicly available discourse parsers (Ji and Eisenstein, 2014; Wang and Lan, 2015). Each PDTB relation has sense label defined in a 3-layered (class, type, subtype), e.g., CONTINGENCY.Cause.result. While there are only four semantic class labels at the class-level which may not cover well different aspects of argumentative relation, subtype-level output is not available given the discourse parser we use. Thus, we use relations at type-level as features. For RSTDTB relations, we use only relation labels, but ignore the nucleus and satellite labels of components as they do not provide more information given the component order in the pair. Because tempora"
P16-1107,P03-1054,0,0.0409809,"different from the baseline (p < 0.05). Performance on Test Set F1 score of Window-context Features 0.7 0.69 0.68 0.67 0.66 0.65 0.64 0.63 0.62 0 1 2 3 4 5 6 7 8 Window-size Figure 4: Performance of Window-context feature set by window-size. window-size in range [0, 8]8 that yields the best F1 score in 10-fold cross validation. We use the training set as determined in (Stab and Gurevych, 2014b) to train/test9 the models using LibLINEAR algorithm (Fan et al., 2008) without parameter or feature optimization. Cross-validations are conducted using Weka (Hall et al., 2009). We use Stanford parser (Klein and Manning, 2003) to perform text processing. As shown in Figure 4, while increasing the window-size from 2 to 3 improves performance (significantly), using window-sizes greater than 3 does not gain further improvement. We hypothesize that after a certain limit, larger context windows will produce more noise than helpful information for the prediction. Therefore, we set the window-size to 3 in all of our experiments involving Window-context model (all with a separate test set). 8 Windows-size 0 means covering sentence is the only context sentence. We experimented with not using context sentence at all and obta"
P16-1107,C14-1141,0,0.333611,"in Figure 1, without knowing the content “horrendous images are displayed on the cigarette boxes” in sentence 3, one cannot easily tell that “reduction in the number of smokers” in sentence 4 supports the “pictures can influence” claim in sentence 2. We expect that such content relatedness can be revealed from a discourse analysis, e.g., the appearance of a discourse connective “As a result”. While topic information in many writing genres (e.g., scientific publications, Wikipedia articles, student essays) has been used to create features for argument component mining (Teufel and Moens, 2002; Levy et al., 2014; Nguyen and Litman, 2015), topic-based features have been less explored for argumentative relation mining. The second hypothesis investigated in this paper is that features based on topic context also provide useful information for improving argumentative relation mining. In the excerpt below, knowing that ‘online game’ and ‘computer’ are topically related might help a model decide that the claim in sentence 1 supports the claim in sentence 2: (1) People who are addicted to games, especially online games, can eventually bear dangerous consequences[Claim] . (2) Although it is undeniable that c"
P16-1107,N12-1003,0,0.181982,"Missing"
P16-1107,W15-0503,1,0.776205,"ut knowing the content “horrendous images are displayed on the cigarette boxes” in sentence 3, one cannot easily tell that “reduction in the number of smokers” in sentence 4 supports the “pictures can influence” claim in sentence 2. We expect that such content relatedness can be revealed from a discourse analysis, e.g., the appearance of a discourse connective “As a result”. While topic information in many writing genres (e.g., scientific publications, Wikipedia articles, student essays) has been used to create features for argument component mining (Teufel and Moens, 2002; Levy et al., 2014; Nguyen and Litman, 2015), topic-based features have been less explored for argumentative relation mining. The second hypothesis investigated in this paper is that features based on topic context also provide useful information for improving argumentative relation mining. In the excerpt below, knowing that ‘online game’ and ‘computer’ are topically related might help a model decide that the claim in sentence 1 supports the claim in sentence 2: (1) People who are addicted to games, especially online games, can eventually bear dangerous consequences[Claim] . (2) Although it is undeniable that computer is a crucial part"
P16-1107,W14-2105,0,0.0989377,"post-processed LDA (Blei et al., 2003) output to extract a lexicon of argument and domain words from development data. Their semi-supervised approach exploits the topic context through essay titles to guide the extraction. Finally, prior research has explored predicting different argumentative relationship labels between pairs of argument components, e.g., attachment (Peldszus and Stede, 2015a), support vs. non-support (Biran and Rambow, 2011; Cabrio and Villata, 2012; Stab and Gurevych, 2014b), {implicit, explicit}×{support, attack} (Boltuˇzi´c ˇ and Snajder, 2014), verifiability of support (Park and Cardie, 2014). Our experiments use two such argumentative relation classification tasks (Support vs. Non-support, Support vs. Attack) to evaluate the effectiveness of our proposed features. 3 Persuasive Essay Corpus Stab and Gurevych (2014a) compiled the Persuasive Essay Corpus consisting of 90 student argumentative essays and made it publicly available.3 Because the corpus has been utilized for different argument mining tasks (Stab and Gurevych, 2014b; Nguyen and Litman, 2015; Nguyen and Litman, 2016), we use this corpus to demonstrate our context-aware argumentative relation mining approach, and adapt th"
P16-1107,D15-1110,0,0.368495,"target components as well as from topic information of the writings. 1127 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1127–1137, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics Prior argumentative relation mining studies have often used features extracted from argument components to model different aspects of the relations between the components, e.g., relative distance, word pairs, semantic similarity, textual entailment (Cabrio and Villata, 2012; Stab and Gurevych, ˇ 2014b; Boltuˇzi´c and Snajder, 2014; Peldszus and Stede, 2015b). Features extracted from the text surrounding the components have been less explored, e.g., using words and their part-of-speech from adjacent sentences (Peldszus, 2014). The first hypothesis investigated in this paper is that the discourse relations of argument components with adjacent sentences (called context windows in this study, a formal definition is given in §5.3) can help characterize the argumentative relations that connect pairs of argument components. Reconsidering the example in Figure 1, without knowing the content “horrendous images are displayed on the cigarette boxes” in se"
P16-1107,W15-0513,0,0.177214,"target components as well as from topic information of the writings. 1127 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1127–1137, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics Prior argumentative relation mining studies have often used features extracted from argument components to model different aspects of the relations between the components, e.g., relative distance, word pairs, semantic similarity, textual entailment (Cabrio and Villata, 2012; Stab and Gurevych, ˇ 2014b; Boltuˇzi´c and Snajder, 2014; Peldszus and Stede, 2015b). Features extracted from the text surrounding the components have been less explored, e.g., using words and their part-of-speech from adjacent sentences (Peldszus, 2014). The first hypothesis investigated in this paper is that the discourse relations of argument components with adjacent sentences (called context windows in this study, a formal definition is given in §5.3) can help characterize the argumentative relations that connect pairs of argument components. Reconsidering the example in Figure 1, without knowing the content “horrendous images are displayed on the cigarette boxes” in se"
P16-1107,W14-2112,0,0.273635,"c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics Prior argumentative relation mining studies have often used features extracted from argument components to model different aspects of the relations between the components, e.g., relative distance, word pairs, semantic similarity, textual entailment (Cabrio and Villata, 2012; Stab and Gurevych, ˇ 2014b; Boltuˇzi´c and Snajder, 2014; Peldszus and Stede, 2015b). Features extracted from the text surrounding the components have been less explored, e.g., using words and their part-of-speech from adjacent sentences (Peldszus, 2014). The first hypothesis investigated in this paper is that the discourse relations of argument components with adjacent sentences (called context windows in this study, a formal definition is given in §5.3) can help characterize the argumentative relations that connect pairs of argument components. Reconsidering the example in Figure 1, without knowing the content “horrendous images are displayed on the cigarette boxes” in sentence 3, one cannot easily tell that “reduction in the number of smokers” in sentence 4 supports the “pictures can influence” claim in sentence 2. We expect that such cont"
P16-1107,prasad-etal-2008-penn,0,0.182729,"f argumentative relation mining vary from clauses (Stab and Gurevych, 2014b; Peldszus, 2014) to multiple-sentences (Biran and Rambow, 2011; Cabrio and Villata, 2012; Boltuˇzi´c and ˇ Snajder, 2014). Studying claim justification between user comments, Biran and Rambow (2011) proposed that the argumentation in justification of a claim can be characterized with discourse structure in the justification. They however only considered discourse markers but not discourse relations. Cabrio et al. (2013) conducted a corpus analysis and found certain similarity between Penn Discourse TreeBank relations (Prasad et al., 2008) and argumentation schemes (Walton et al., 2008). However they did not discuss how such similarity could be applied to argument mining. Motivated by these findings, we propose to use features extracted from discourse relations be1128 Attack S Pre tween sentences for argumentative relation mining. Moreover, to enable discourse relation features when the textual inputs are only sentences/clauses, we group the inputs with their context sentences. Qazvinian and Radev (2010) used the term “context sentence” to refer to sentences surrounding a citation that contained information about the cited sour"
P16-1107,P10-1057,0,0.0253049,"relations. Cabrio et al. (2013) conducted a corpus analysis and found certain similarity between Penn Discourse TreeBank relations (Prasad et al., 2008) and argumentation schemes (Walton et al., 2008). However they did not discuss how such similarity could be applied to argument mining. Motivated by these findings, we propose to use features extracted from discourse relations be1128 Attack S Pre tween sentences for argumentative relation mining. Moreover, to enable discourse relation features when the textual inputs are only sentences/clauses, we group the inputs with their context sentences. Qazvinian and Radev (2010) used the term “context sentence” to refer to sentences surrounding a citation that contained information about the cited source but did not explicitly cite it. In our study, we only require that the context sentences of an argument component must be in the same paragraph and adjacent to the component. Prior work in argumentative relation mining has used argument component labels to provide constraints during relation identification. For example, when an annotation scheme (e.g., (Peldszus and Stede, 2013; Stab and Gurevych, 2014a)) does not allow relations from claim to premise, no relations a"
P16-1107,C14-1002,0,0.0489169,"Missing"
P16-1107,C14-1142,0,0.0665145,"e relation (e.g., support vs. attack). Ar1 There is no consensus yet on an annotation scheme for argument components, or on the minimal textual units to be annotated. We follow Peldszus and Stede (2013) and consider “argument mining as the automatic discovery of an argumentative text portion, and the identification of the relevant components of the argument presented there.” We also borrow their term “argumentative discourse unit” to refer to the textual units (e.g., text segment, sentence, clause) which are considered as argument components. Figure 1: Excerpt from a student persuasive essay (Stab and Gurevych, 2014a). Sentences are numbered and argument components are tagged. gumentative relation mining - beyond argument component mining - is perceived as an essential step towards more fully identifying the argumentative structure of a text (Peldszus and Stede, 2013; Sergeant, 2013; Stab et al., 2014). Consider the second paragraph shown in Figure 1. Only detecting the argument components (a claim in sentence 2 and two premises in sentences 3 and 4) does not give a complete picture of the argumentation. By looking for relations between these components, one can also see that the two premises together ju"
P16-1107,D14-1006,0,0.57335,"ving Window-context model (all with a separate test set). 8 Windows-size 0 means covering sentence is the only context sentence. We experimented with not using context sentence at all and obtained worse performance. Our data does not have context window with window-size 9 or larger. 9 Note that via cross validation, in each fold some of our training set serves as a development set. We train all models using the training set and report their performances on the test set in Table 2. We also compare our baseline to the reported performance (R EPORT) for Support vs. Non-support classification in (Stab and Gurevych, 2014b). The learning algorithm with parameters are kept the same as in the window-size tuning experiment. Given the skewed class distribution of this data, Accuracy and F1 of Non-support (the major class) are less important than Kappa, F1, and F1 of Support (the minor class). To conduct T-tests for performance significance, we split the test data into subsets by essays’ ID, and record prediction performance for individual essays. We first notice that the performances of our baseline model are better than (or equal to) R E PORTED , except the Macro Recall. We reason that these performance dispariti"
P16-1107,J02-4002,0,0.0687942,"econsidering the example in Figure 1, without knowing the content “horrendous images are displayed on the cigarette boxes” in sentence 3, one cannot easily tell that “reduction in the number of smokers” in sentence 4 supports the “pictures can influence” claim in sentence 2. We expect that such content relatedness can be revealed from a discourse analysis, e.g., the appearance of a discourse connective “As a result”. While topic information in many writing genres (e.g., scientific publications, Wikipedia articles, student essays) has been used to create features for argument component mining (Teufel and Moens, 2002; Levy et al., 2014; Nguyen and Litman, 2015), topic-based features have been less explored for argumentative relation mining. The second hypothesis investigated in this paper is that features based on topic context also provide useful information for improving argumentative relation mining. In the excerpt below, knowing that ‘online game’ and ‘computer’ are topically related might help a model decide that the claim in sentence 1 supports the claim in sentence 2: (1) People who are addicted to games, especially online games, can eventually bear dangerous consequences[Claim] . (2) Although it i"
P16-1107,K15-2002,0,0.0242059,"ng context sentences, for source and target components. Discourse relation: for both source and target components, we extract discourse relations between context sentences, and within the covering sentence. We also extract discourse relations between each pair of source context sentence and target context sentence. Each relation defines a boolean feature. We extract both Penn Discourse Treebank (PDTB) relations (Prasad et al., 2008) and Rhetorical Structure Theory Discourse Treebank (RST-DTB) relations (Carlson et al., 2001) using publicly available discourse parsers (Ji and Eisenstein, 2014; Wang and Lan, 2015). Each PDTB relation has sense label defined in a 3-layered (class, type, subtype), e.g., CONTINGENCY.Cause.result. While there are only four semantic class labels at the class-level which may not cover well different aspects of argumentative relation, subtype-level output is not available given the discourse parser we use. Thus, we use relations at type-level as features. For RSTDTB relations, we use only relation labels, but ignore the nucleus and satellite labels of components as they do not provide more information given the component order in the pair. Because temporal relations were show"
P17-1144,E12-1036,0,0.668116,"involing revision analysis (Zhang and Litman, 2015), paraphrase (Malakasiotis and Androutsopoulos, 2011) and correction detection (Swanson and Yamangil, 2012; Xue and Hwa, 2014). Although there are some corpora resources for NLP research on writing comparisons, most tend to be between individual sentences/phrases for tasks such as paraphrase comparison (Dolan and Brockett, 2005; Tan and Lee, 2014) or grammar error correction (Dahlmeier et al., 2013; Yannakoudakis et al., 2011). In terms of revision analysis, the most relevant work analyzes Wikipedia revisions (Daxenberger and Gurevych, 2013; Bronner and Monz, 2012); however, the domain of Wikipedia is so specialized that the properties of Wikipedia revisions do not correspond well with other kinds of texts. This work presents the ArgRewrite corpus1 to facilitate revision analysis research for argumentative essays. The corpus consists of a collection of three drafts of essays written by university students and employees; the drafts are manually aligned at the sentence level, then the purpose of each revision is manually coded using a revision schema closely related to argument mining/discourse analysis. Within the domain of argumentative essays, the corp"
P17-1144,W99-0411,0,0.224684,"ve essays. Drafts are manually aligned at the sentence level, and the writer’s purpose for each revision is annotated with categories analogous to those used in argument mining and discourse analysis. The corpus should enable advanced research in writing comparison and revision analysis, as demonstrated via our own studies of student revision behavior and of automatic revision purpose prediction. 1 Introduction Most writing-related natural language processing (NLP) research focuses on the analysis of single drafts. Examples include document-level quality assessment (Attali and Burstein, 2006; Burstein and Chodorow, 1999), discourse-level analysis and mining (Burstein et al., 2003; Falakmasir et al., 2014; Persing and Ng, 2016), and fine-grained error detection (Leacock et al., 2010; Grammarly, 2016). Less studied is the analysis of changes between drafts – a comparison of revisions and the properties of the differences. Research on this topic can support applications involing revision analysis (Zhang and Litman, 2015), paraphrase (Malakasiotis and Androutsopoulos, 2011) and correction detection (Swanson and Yamangil, 2012; Xue and Hwa, 2014). Although there are some corpora resources for NLP research on writi"
P17-1144,P01-1014,0,0.115176,"d works differently for first revision attempts and second revision attempts. H6. The revision classification model trained on L2 essays has a different preference from the model trained on native essays. 4.2.2 Methodology We followed the work of (Zhang and Litman, 2015), where unigram features (words) were used as the baseline and the SVM classifier was used. Besides unigrams, three groups of features used in revision analysis, argument mining and discourse analysis research were extracted (Location, Textual and Language) as in Table 6 (Bronner and Monz, 2012; Daxenberger and Gurevych, 2013; Burstein et al., 2001; Falakmasir et al., 2014). For H4, 10-fold (participant) cross-validation is conducted on all the essays in the corpus. Unweighted average F-score for each revision category is reported, using unigram features versus using all features. Zhang and Litman (2015) observed a significant improvement over the unigram baseline using all the features. If H4 is true, we should expect a similar improvement over the unigram baseline using our corpus. For H5, 10-fold cross-validation was conducted for the revisions from Draft1 to Draft2 and revisions from Draft2 to Draft3 separately. We compared the impr"
P17-1144,P16-2051,0,0.0671933,"Missing"
P17-1144,D11-1009,0,0.016758,"guage processing (NLP) research focuses on the analysis of single drafts. Examples include document-level quality assessment (Attali and Burstein, 2006; Burstein and Chodorow, 1999), discourse-level analysis and mining (Burstein et al., 2003; Falakmasir et al., 2014; Persing and Ng, 2016), and fine-grained error detection (Leacock et al., 2010; Grammarly, 2016). Less studied is the analysis of changes between drafts – a comparison of revisions and the properties of the differences. Research on this topic can support applications involing revision analysis (Zhang and Litman, 2015), paraphrase (Malakasiotis and Androutsopoulos, 2011) and correction detection (Swanson and Yamangil, 2012; Xue and Hwa, 2014). Although there are some corpora resources for NLP research on writing comparisons, most tend to be between individual sentences/phrases for tasks such as paraphrase comparison (Dolan and Brockett, 2005; Tan and Lee, 2014) or grammar error correction (Dahlmeier et al., 2013; Yannakoudakis et al., 2011). In terms of revision analysis, the most relevant work analyzes Wikipedia revisions (Daxenberger and Gurevych, 2013; Bronner and Monz, 2012); however, the domain of Wikipedia is so specialized that the properties of Wikipe"
P17-1144,W13-1703,0,0.029161,"Less studied is the analysis of changes between drafts – a comparison of revisions and the properties of the differences. Research on this topic can support applications involing revision analysis (Zhang and Litman, 2015), paraphrase (Malakasiotis and Androutsopoulos, 2011) and correction detection (Swanson and Yamangil, 2012; Xue and Hwa, 2014). Although there are some corpora resources for NLP research on writing comparisons, most tend to be between individual sentences/phrases for tasks such as paraphrase comparison (Dolan and Brockett, 2005; Tan and Lee, 2014) or grammar error correction (Dahlmeier et al., 2013; Yannakoudakis et al., 2011). In terms of revision analysis, the most relevant work analyzes Wikipedia revisions (Daxenberger and Gurevych, 2013; Bronner and Monz, 2012); however, the domain of Wikipedia is so specialized that the properties of Wikipedia revisions do not correspond well with other kinds of texts. This work presents the ArgRewrite corpus1 to facilitate revision analysis research for argumentative essays. The corpus consists of a collection of three drafts of essays written by university students and employees; the drafts are manually aligned at the sentence level, then the pur"
P17-1144,C12-1044,0,0.118232,"the entire sentence pair will be annotated as one sentence revision. Second, we need to define the quality we want to observe about the revision sentence pair. For this first corpus, we focus on recognizing the purpose of the revision, as in the example above. It is a useful property, and it has previously been studied by others in the literature. People have considered both binary purpose categories such as Content vs. Surface (Faigley and Witte, 1981) or Factual vs. Fluency (Bronner and Monz, 2012) as well as more fine-grained categories (Pfeil et al., 2006; Jones, 2008; Liu and Ram, 2009; Daxenberger and Gurevych, 2012; Zhang and Litman, 2015). Our corpus follows the two-tiered schema used by (Zhang and Litman, 2015) (see Section 3.2). Third, we not only have to decide on the annotation format, we also need to decide how to obtain Draft1 Draft2 Draft3 Write Draft1 @home Revise Draft1 @home Revise Draft2 @lab Annotated Revisions I (Rev12) Annotated Revisions II (Rev23) Figure 1: Our collected corpus contains five components: three drafts of an essay and two annotated revisions between drafts. the raw text: argumentative essays with multiple drafts. We decided to sample from a population of predominantly coll"
P17-1144,N16-1164,0,0.027203,"ed with categories analogous to those used in argument mining and discourse analysis. The corpus should enable advanced research in writing comparison and revision analysis, as demonstrated via our own studies of student revision behavior and of automatic revision purpose prediction. 1 Introduction Most writing-related natural language processing (NLP) research focuses on the analysis of single drafts. Examples include document-level quality assessment (Attali and Burstein, 2006; Burstein and Chodorow, 1999), discourse-level analysis and mining (Burstein et al., 2003; Falakmasir et al., 2014; Persing and Ng, 2016), and fine-grained error detection (Leacock et al., 2010; Grammarly, 2016). Less studied is the analysis of changes between drafts – a comparison of revisions and the properties of the differences. Research on this topic can support applications involing revision analysis (Zhang and Litman, 2015), paraphrase (Malakasiotis and Androutsopoulos, 2011) and correction detection (Swanson and Yamangil, 2012; Xue and Hwa, 2014). Although there are some corpora resources for NLP research on writing comparisons, most tend to be between individual sentences/phrases for tasks such as paraphrase comparison"
P17-1144,D13-1055,0,0.522909,"s topic can support applications involing revision analysis (Zhang and Litman, 2015), paraphrase (Malakasiotis and Androutsopoulos, 2011) and correction detection (Swanson and Yamangil, 2012; Xue and Hwa, 2014). Although there are some corpora resources for NLP research on writing comparisons, most tend to be between individual sentences/phrases for tasks such as paraphrase comparison (Dolan and Brockett, 2005; Tan and Lee, 2014) or grammar error correction (Dahlmeier et al., 2013; Yannakoudakis et al., 2011). In terms of revision analysis, the most relevant work analyzes Wikipedia revisions (Daxenberger and Gurevych, 2013; Bronner and Monz, 2012); however, the domain of Wikipedia is so specialized that the properties of Wikipedia revisions do not correspond well with other kinds of texts. This work presents the ArgRewrite corpus1 to facilitate revision analysis research for argumentative essays. The corpus consists of a collection of three drafts of essays written by university students and employees; the drafts are manually aligned at the sentence level, then the purpose of each revision is manually coded using a revision schema closely related to argument mining/discourse analysis. Within the domain of argum"
P17-1144,I05-5002,0,0.0748553,"and fine-grained error detection (Leacock et al., 2010; Grammarly, 2016). Less studied is the analysis of changes between drafts – a comparison of revisions and the properties of the differences. Research on this topic can support applications involing revision analysis (Zhang and Litman, 2015), paraphrase (Malakasiotis and Androutsopoulos, 2011) and correction detection (Swanson and Yamangil, 2012; Xue and Hwa, 2014). Although there are some corpora resources for NLP research on writing comparisons, most tend to be between individual sentences/phrases for tasks such as paraphrase comparison (Dolan and Brockett, 2005; Tan and Lee, 2014) or grammar error correction (Dahlmeier et al., 2013; Yannakoudakis et al., 2011). In terms of revision analysis, the most relevant work analyzes Wikipedia revisions (Daxenberger and Gurevych, 2013; Bronner and Monz, 2012); however, the domain of Wikipedia is so specialized that the properties of Wikipedia revisions do not correspond well with other kinds of texts. This work presents the ArgRewrite corpus1 to facilitate revision analysis research for argumentative essays. The corpus consists of a collection of three drafts of essays written by university students and employ"
P17-1144,C08-2023,0,0.0254765,"The corpus consists of a collection of three drafts of essays written by university students and employees; the drafts are manually aligned at the sentence level, then the purpose of each revision is manually coded using a revision schema closely related to argument mining/discourse analysis. Within the domain of argumentative essays, the corpus will be useful for supporting research in argumentative revision analysis and the application of argument mining techniques. The corpus may also be useful for research on paraphrase comparisons, grammar error correction, and computational stylistics (Popescu and Dinu, 2008; Flekova et al., 2016). In this paper, we present two example uses of our corpus: 1) rewriting behavior data analysis, and 2) automatic revision purpose classification. 2 Corpus Design Decisions Consider this scenario: Alice begins her social science argumentative essay with the sentence “Electronic communication allows people to make connections beyond physical limits.” An analytical system might (rightly) identify the sentence as the thesis of her essay, and an evaluative system might give the essay a low score due to this sentence’s vagueness and a later lack of evidence (though Alice may"
P17-1144,N12-1037,0,0.0202213,"drafts. Examples include document-level quality assessment (Attali and Burstein, 2006; Burstein and Chodorow, 1999), discourse-level analysis and mining (Burstein et al., 2003; Falakmasir et al., 2014; Persing and Ng, 2016), and fine-grained error detection (Leacock et al., 2010; Grammarly, 2016). Less studied is the analysis of changes between drafts – a comparison of revisions and the properties of the differences. Research on this topic can support applications involing revision analysis (Zhang and Litman, 2015), paraphrase (Malakasiotis and Androutsopoulos, 2011) and correction detection (Swanson and Yamangil, 2012; Xue and Hwa, 2014). Although there are some corpora resources for NLP research on writing comparisons, most tend to be between individual sentences/phrases for tasks such as paraphrase comparison (Dolan and Brockett, 2005; Tan and Lee, 2014) or grammar error correction (Dahlmeier et al., 2013; Yannakoudakis et al., 2011). In terms of revision analysis, the most relevant work analyzes Wikipedia revisions (Daxenberger and Gurevych, 2013; Bronner and Monz, 2012); however, the domain of Wikipedia is so specialized that the properties of Wikipedia revisions do not correspond well with other kinds"
P17-1144,P11-1019,0,0.0516712,"ysis of changes between drafts – a comparison of revisions and the properties of the differences. Research on this topic can support applications involing revision analysis (Zhang and Litman, 2015), paraphrase (Malakasiotis and Androutsopoulos, 2011) and correction detection (Swanson and Yamangil, 2012; Xue and Hwa, 2014). Although there are some corpora resources for NLP research on writing comparisons, most tend to be between individual sentences/phrases for tasks such as paraphrase comparison (Dolan and Brockett, 2005; Tan and Lee, 2014) or grammar error correction (Dahlmeier et al., 2013; Yannakoudakis et al., 2011). In terms of revision analysis, the most relevant work analyzes Wikipedia revisions (Daxenberger and Gurevych, 2013; Bronner and Monz, 2012); however, the domain of Wikipedia is so specialized that the properties of Wikipedia revisions do not correspond well with other kinds of texts. This work presents the ArgRewrite corpus1 to facilitate revision analysis research for argumentative essays. The corpus consists of a collection of three drafts of essays written by university students and employees; the drafts are manually aligned at the sentence level, then the purpose of each revision is manu"
P17-1144,W14-1818,1,0.833765,"are shown a computer interface that highlights the differences between their first and second drafts. They are asked to revise and create a third draft to improve the general quality of their essay. We experimented with two variations of revision elicitation. Chosen at random, half of the participants (10 L2 participants and 20 Native participants) are shown Interface A, the interface based on the ArgRewrite system (Zhang et al., 2016), which highlights the annotated differences between the drafts (Figure 2(a)); half of the participants are shown In5 Sentences are first automatically aligned (Zhang and Litman, 2014), then manually corrected by human. 1570 Draft1 This world has no restriction on who one can talk to. Revision Draft2 Purpose Conventions/ This world has no restrictions on Grammar/ whom one can talk to. Spelling Revision Purpose This world has no restrictions on whom one can talk to. Rebuttal/ Reservation The only aspects of communication that this new development improves are internet navigation and faux internet relatability. WordUsage/ Clarity Claims/ Ideas The only aspects of digital communication that this new development improves are internet navigation and faux internet relatability. B"
P17-1144,W15-0616,1,0.429119,"tion Most writing-related natural language processing (NLP) research focuses on the analysis of single drafts. Examples include document-level quality assessment (Attali and Burstein, 2006; Burstein and Chodorow, 1999), discourse-level analysis and mining (Burstein et al., 2003; Falakmasir et al., 2014; Persing and Ng, 2016), and fine-grained error detection (Leacock et al., 2010; Grammarly, 2016). Less studied is the analysis of changes between drafts – a comparison of revisions and the properties of the differences. Research on this topic can support applications involing revision analysis (Zhang and Litman, 2015), paraphrase (Malakasiotis and Androutsopoulos, 2011) and correction detection (Swanson and Yamangil, 2012; Xue and Hwa, 2014). Although there are some corpora resources for NLP research on writing comparisons, most tend to be between individual sentences/phrases for tasks such as paraphrase comparison (Dolan and Brockett, 2005; Tan and Lee, 2014) or grammar error correction (Dahlmeier et al., 2013; Yannakoudakis et al., 2011). In terms of revision analysis, the most relevant work analyzes Wikipedia revisions (Daxenberger and Gurevych, 2013; Bronner and Monz, 2012); however, the domain of Wiki"
P17-1144,P14-2066,0,\N,Missing
P17-1144,P14-2098,1,\N,Missing
P17-1144,N16-3008,1,\N,Missing
P17-3013,P14-1023,0,0.0274774,"Missing"
P17-3013,W16-0533,0,0.0365417,"Missing"
P17-3013,N12-1011,0,0.0784866,"Missing"
P17-3013,D15-1242,0,0.0232199,"Missing"
P17-3013,W10-1013,0,0.404689,"ovide formative feedback to students and teachers at scale, an automated RTA scoring tool is now being developed (Rahimi et al., 2017). This paper focuses on the Evidence dimension of the RTA, which evaluates students’ ability to 2 Related Work Most research studies in automated essay scoring have focused on holistic rubrics (Shermis and Burstein, 2003; Attali and Burstein, 2006). In contrast, our work focuses on evaluating a single dimension to obtain a rubric score for students’ use of evidence from a source text to support their stated position. To evaluate the content of students’ essays, Louis and Higgins (2010) presented a method to detect if an essay is off-topic. Xie et al. (2012) presented a method to evaluate content features by measuring the similarity between essays. Burstein et al. (2001) and Ong et al. (2014) both presented methods to use argumentation mining techniques to evaluate the students’ use of evidence to support claims in persuasive essays. However, those studies are different from 75 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics- Student Research Workshop, pages 75–81 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Comp"
P17-3013,W14-2104,1,0.845912,"’ ability to 2 Related Work Most research studies in automated essay scoring have focused on holistic rubrics (Shermis and Burstein, 2003; Attali and Burstein, 2006). In contrast, our work focuses on evaluating a single dimension to obtain a rubric score for students’ use of evidence from a source text to support their stated position. To evaluate the content of students’ essays, Louis and Higgins (2010) presented a method to detect if an essay is off-topic. Xie et al. (2012) presented a method to evaluate content features by measuring the similarity between essays. Burstein et al. (2001) and Ong et al. (2014) both presented methods to use argumentation mining techniques to evaluate the students’ use of evidence to support claims in persuasive essays. However, those studies are different from 75 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics- Student Research Workshop, pages 75–81 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-3013 this work in that they did not measure how the essay uses material from the source article. Furthermore, young students find it difficult to use sophis"
P84-1063,P84-1063,1,0.0511963,"Missing"
P86-1033,P84-1076,0,0.0258045,"Missing"
P86-1033,P83-1007,0,0.0270275,"Missing"
P86-1033,P84-1063,1,\N,Missing
P87-1023,P82-1005,0,0.0434929,"Missing"
P87-1023,J86-3001,0,0.235995,"Missing"
P87-1023,P84-1085,0,0.0795882,"Missing"
P87-1023,P86-1021,1,\N,Missing
P87-1023,P84-1055,0,\N,Missing
P93-1020,P86-1021,0,0.0946445,"ningful relations among the utterances. As in much of the literature on discourse processing, we assume that certain spans of utterances, referred to here as discourse segments, form coherent units. The segmental structure of discourse has been claimed to constrain and be constrained by disparate phenomena: cue phrases (Hirschberg and Litman, 1993; Gross and Sidner, 1986; Reichman, 1985; Cohen, 1984); lexical cohesion (Morris and Hirst, 1991); plans and intentions (Carberry, 1990; Litman and Allen, 1990; Gross and Sidner, 1986); prosody (Grosz and Hirschberg, 1992; Hirschberg and Gross, 1992; Hirschberg and Pierrehumbert, 1986); reference (Webber, 1991; Gross and Sidner, 1986; Linde, 1979); and tense (Webber, 1988; Hwang and Schubert, 1992; Song and Cohen, 1991). However, there is weak consensus on the nature of segments and the criteria for recognizing or generating them in a natural language processing system. Until recently, little empirical work has been directed at establishing obje'~ively verifiable segment boundaries, even though this is a precondition for avoiding circularity in relating segments to linguistic phenomena. We present the results of a two part study on the reliability of human segmentation, and"
P93-1020,P92-1030,0,0.0216083,"utterances, referred to here as discourse segments, form coherent units. The segmental structure of discourse has been claimed to constrain and be constrained by disparate phenomena: cue phrases (Hirschberg and Litman, 1993; Gross and Sidner, 1986; Reichman, 1985; Cohen, 1984); lexical cohesion (Morris and Hirst, 1991); plans and intentions (Carberry, 1990; Litman and Allen, 1990; Gross and Sidner, 1986); prosody (Grosz and Hirschberg, 1992; Hirschberg and Gross, 1992; Hirschberg and Pierrehumbert, 1986); reference (Webber, 1991; Gross and Sidner, 1986; Linde, 1979); and tense (Webber, 1988; Hwang and Schubert, 1992; Song and Cohen, 1991). However, there is weak consensus on the nature of segments and the criteria for recognizing or generating them in a natural language processing system. Until recently, little empirical work has been directed at establishing obje'~ively verifiable segment boundaries, even though this is a precondition for avoiding circularity in relating segments to linguistic phenomena. We present the results of a two part study on the reliability of human segmentation, and correlation with linguistic cues. We show that human subjects can reliably perform discourse segmentation using s"
P93-1020,W93-0216,1,0.780477,"consists of 20 narrative monologues about the same movie, taken from Chafe (1980) (N~14,000 words). The subjects were introductory psychology students at the University of Connecticut and volunteers solicited from electronic bulletin boards. Each narrative was segmented by 7 subjects. Subjects were instructed to identify each point in a narrative where the speaker had completed one communicative task, and began a new one. They were also instructed to briefly identify the speaker's intention associated with each segment. Intention was explained in common sense terms and by example (details in (Litman and Passonneau, 1993)). To simplify data collection, we did not ask subjects to identify the type of hierarchical relations among segments illustrated in Figure 1. In a pilot study we conducted, subjects found it difficult and time-consuming to identify non-sequential relations. Given that the average length of our narratives is 700 words, this is consistent with previous findings (Rotondo, 1984) that non-linear segmentation is impractical for naive subjects in discourses longer than 200 words. "" Since prosodic phrases were already marked in the transcripts, we restricted subjects to placing boundaries between pro"
P93-1020,J92-4007,0,0.0592915,"the majority of her subjects. Hearst developed a lexical algorithm based on information retrieval measurements to segment text, then qualitatively compared the results with the structures derived from her subjects, as well as with those produced by Morris and Hirst. Iwanska (1993) compares her segmentations of factual reports with segmentations produced using syntactic, semantic, and pragmatic information. We derive segmentations from our empirical data based on the statistiRELIABILITY The correspondence between discourse segments and more abstract units of meaning is poorly understood (see (Moore and Pollack, 1992)). A number of alternative proposals have been presented which directly or indirectly relate segments to intentions (Grosz and Sidner, 1986), RST relations (Mann et al., 1992) or other semantic relations (Polanyi, 1988). We present initial results of an investigation of whether naive subjects can reliably segment discourse using speaker intention as a criterion. Our corpus consists of 20 narrative monologues about the same movie, taken from Chafe (1980) (N~14,000 words). The subjects were introductory psychology students at the University of Connecticut and volunteers solicited from electronic"
P93-1020,J91-1002,0,0.240467,"he ladder, a n d picks s o m e m o r e pears. Figure 1: Discourse Segment Structure INTRODUCTION A discourse consists not simply of a linear sequence of utterances, 1 hut of meaningful relations among the utterances. As in much of the literature on discourse processing, we assume that certain spans of utterances, referred to here as discourse segments, form coherent units. The segmental structure of discourse has been claimed to constrain and be constrained by disparate phenomena: cue phrases (Hirschberg and Litman, 1993; Gross and Sidner, 1986; Reichman, 1985; Cohen, 1984); lexical cohesion (Morris and Hirst, 1991); plans and intentions (Carberry, 1990; Litman and Allen, 1990; Gross and Sidner, 1986); prosody (Grosz and Hirschberg, 1992; Hirschberg and Gross, 1992; Hirschberg and Pierrehumbert, 1986); reference (Webber, 1991; Gross and Sidner, 1986; Linde, 1979); and tense (Webber, 1988; Hwang and Schubert, 1992; Song and Cohen, 1991). However, there is weak consensus on the nature of segments and the criteria for recognizing or generating them in a natural language processing system. Until recently, little empirical work has been directed at establishing obje'~ively verifiable segment boundaries, even"
P93-1020,P84-1055,0,0.102942,"g . A - n d u m [ - ~ goes up the ladder, a n d picks s o m e m o r e pears. Figure 1: Discourse Segment Structure INTRODUCTION A discourse consists not simply of a linear sequence of utterances, 1 hut of meaningful relations among the utterances. As in much of the literature on discourse processing, we assume that certain spans of utterances, referred to here as discourse segments, form coherent units. The segmental structure of discourse has been claimed to constrain and be constrained by disparate phenomena: cue phrases (Hirschberg and Litman, 1993; Gross and Sidner, 1986; Reichman, 1985; Cohen, 1984); lexical cohesion (Morris and Hirst, 1991); plans and intentions (Carberry, 1990; Litman and Allen, 1990; Gross and Sidner, 1986); prosody (Grosz and Hirschberg, 1992; Hirschberg and Gross, 1992; Hirschberg and Pierrehumbert, 1986); reference (Webber, 1991; Gross and Sidner, 1986; Linde, 1979); and tense (Webber, 1988; Hwang and Schubert, 1992; Song and Cohen, 1991). However, there is weak consensus on the nature of segments and the criteria for recognizing or generating them in a natural language processing system. Until recently, little empirical work has been directed at establishing obje'"
P93-1020,P92-1032,0,0.259873,"Missing"
P93-1020,J86-3001,0,0.931042,"Missing"
P93-1020,H92-1089,0,0.137812,"sistent with previous findings (Rotondo, 1984) that non-linear segmentation is impractical for naive subjects in discourses longer than 200 words. "" Since prosodic phrases were already marked in the transcripts, we restricted subjects to placing boundaries between prosodic phrases. In principle, this makes it more likely that subjects will agree on a given boundary than if subjects were completely unrestricted. However, previous studies have shown that the smallest unit subjects use in similar tasks corresponds roughly to a breath group, prosodic phrase, or clause (Chafe, 1980; Rotondo, 1984; Hirschberg and Grosz, 1992). Using smaller units would have artificially lowered the probability for agreement on boundaries. Figure 2 shows the responses of subjects at each potential boundary site for a portion of the excerpt from Figure 1. Prosodic phrases are numbered sequentially, with the first field indicating prosodic phrases with sentence-final contours, and the second 149 3.3 [.35+ [.35] a-nd] he- u-h [.3] p u t s his pears into the basket. l 6 SUBJECTS I NP, PAUSE 4.1 [I.0 [.5] U-hi a number of people are going by, CUE, PAUSE 4.2 [.35+ and [.35]] o n e is [1.15 urn/ / y o u k n o w / I don't know, 4.3 I c a n"
P93-1020,J93-3003,1,0.807764,"the country, m a y b e . i n u-m u-h t h e valley or s o m e t h i n g . A - n d u m [ - ~ goes up the ladder, a n d picks s o m e m o r e pears. Figure 1: Discourse Segment Structure INTRODUCTION A discourse consists not simply of a linear sequence of utterances, 1 hut of meaningful relations among the utterances. As in much of the literature on discourse processing, we assume that certain spans of utterances, referred to here as discourse segments, form coherent units. The segmental structure of discourse has been claimed to constrain and be constrained by disparate phenomena: cue phrases (Hirschberg and Litman, 1993; Gross and Sidner, 1986; Reichman, 1985; Cohen, 1984); lexical cohesion (Morris and Hirst, 1991); plans and intentions (Carberry, 1990; Litman and Allen, 1990; Gross and Sidner, 1986); prosody (Grosz and Hirschberg, 1992; Hirschberg and Gross, 1992; Hirschberg and Pierrehumbert, 1986); reference (Webber, 1991; Gross and Sidner, 1986; Linde, 1979); and tense (Webber, 1988; Hwang and Schubert, 1992; Song and Cohen, 1991). However, there is weak consensus on the nature of segments and the criteria for recognizing or generating them in a natural language processing system. Until recently, little"
P93-1020,J88-2006,0,0.0212679,"rtain spans of utterances, referred to here as discourse segments, form coherent units. The segmental structure of discourse has been claimed to constrain and be constrained by disparate phenomena: cue phrases (Hirschberg and Litman, 1993; Gross and Sidner, 1986; Reichman, 1985; Cohen, 1984); lexical cohesion (Morris and Hirst, 1991); plans and intentions (Carberry, 1990; Litman and Allen, 1990; Gross and Sidner, 1986); prosody (Grosz and Hirschberg, 1992; Hirschberg and Gross, 1992; Hirschberg and Pierrehumbert, 1986); reference (Webber, 1991; Gross and Sidner, 1986; Linde, 1979); and tense (Webber, 1988; Hwang and Schubert, 1992; Song and Cohen, 1991). However, there is weak consensus on the nature of segments and the criteria for recognizing or generating them in a natural language processing system. Until recently, little empirical work has been directed at establishing obje'~ively verifiable segment boundaries, even though this is a precondition for avoiding circularity in relating segments to linguistic phenomena. We present the results of a two part study on the reliability of human segmentation, and correlation with linguistic cues. We show that human subjects can reliably perform disc"
P95-1015,P92-1032,0,0.149119,"Missing"
P95-1015,J86-3001,0,0.976697,"o methods for developing segmentation algorithms from training data: hand tuning and machine learning. When multiple types of features are used, results approach human performance on an independent test set (both methods), and using cross-validation (machine learning). 1 that relies on enriched input features and multiple Introduction Many have argued that discourse has a global structure above the level of individual utterances, and that linguistic phenomena like prosody, cue phrases, and nominal reference are partly conditioned by and reflect this structure (cf. (Grosz and Hirschberg, 1992; Grosz and Sidner, 1986; Hirschberg and Grosz, 1992; Hirschberg and Litman, 1993; Hirschberg and Pierrehumbert, 1986; Hobbs, 1979; Lascarides and Oberlander, 1992; Linde, 1979; Mann and Thompson, 1988; Polanyi, 1988; Reichman, 1985; Webber, 1991)). However, an obstacle to exploiting the relation between global structure and linguistic devices in natural language systems is that there is too little data about how they constrain one another. We have been engaged in a study addressing this gap. In previous work (Passonneau and Litman, 1993), we reported on a method for empirically validating global discourse units, and"
P95-1015,P83-1007,0,0.0123328,"was correlated with discourse signaling uses of cue words in (Hirschberg and Litman, 1993); a potential correlation between discourse signaling uses of cue words and adjacency patterns between cue words was also suggested. Finally, (Litman, 1994) found that treating cue phrases individually rather than as a class enhanced the results of (iiirschberg and Litman, 1993). Passonneau (to appear) examined some of the few claims relating discourse anaphoric noun phrases to global discourse structure in the Pear corpus. Resuits included an absence of correlation of segmental structure with centering (Grosz et al., 1983; Kameyama, 1986), and poor correlation with the contrast between full noun phrases and pronouns. As noted in (Passonneau and Litman, 1993), the NP features largely reflect Passonneau&apos;s hypotheses that adjacent utterances are more likely to contain expressions that corefer, or that are inferentially linked, if they occur within the same segment; and that a definite pronoun is more likely than a full NP to refer to an entity that was mentioned in the current segment, if not in the previous utterance. .63 .64 .72 .68 .06 .07 .12 .11 Table 1: Average human performance. range in length from 51 to"
P95-1015,P94-1002,0,0.413171,"Missing"
P95-1015,H92-1089,0,0.0894321,"segmentation algorithms from training data: hand tuning and machine learning. When multiple types of features are used, results approach human performance on an independent test set (both methods), and using cross-validation (machine learning). 1 that relies on enriched input features and multiple Introduction Many have argued that discourse has a global structure above the level of individual utterances, and that linguistic phenomena like prosody, cue phrases, and nominal reference are partly conditioned by and reflect this structure (cf. (Grosz and Hirschberg, 1992; Grosz and Sidner, 1986; Hirschberg and Grosz, 1992; Hirschberg and Litman, 1993; Hirschberg and Pierrehumbert, 1986; Hobbs, 1979; Lascarides and Oberlander, 1992; Linde, 1979; Mann and Thompson, 1988; Polanyi, 1988; Reichman, 1985; Webber, 1991)). However, an obstacle to exploiting the relation between global structure and linguistic devices in natural language systems is that there is too little data about how they constrain one another. We have been engaged in a study addressing this gap. In previous work (Passonneau and Litman, 1993), we reported on a method for empirically validating global discourse units, and on our evaluation of algori"
P95-1015,J93-3003,1,0.91293,"m training data: hand tuning and machine learning. When multiple types of features are used, results approach human performance on an independent test set (both methods), and using cross-validation (machine learning). 1 that relies on enriched input features and multiple Introduction Many have argued that discourse has a global structure above the level of individual utterances, and that linguistic phenomena like prosody, cue phrases, and nominal reference are partly conditioned by and reflect this structure (cf. (Grosz and Hirschberg, 1992; Grosz and Sidner, 1986; Hirschberg and Grosz, 1992; Hirschberg and Litman, 1993; Hirschberg and Pierrehumbert, 1986; Hobbs, 1979; Lascarides and Oberlander, 1992; Linde, 1979; Mann and Thompson, 1988; Polanyi, 1988; Reichman, 1985; Webber, 1991)). However, an obstacle to exploiting the relation between global structure and linguistic devices in natural language systems is that there is too little data about how they constrain one another. We have been engaged in a study addressing this gap. In previous work (Passonneau and Litman, 1993), we reported on a method for empirically validating global discourse units, and on our evaluation of algorithms to identify these units."
P95-1015,P86-1021,0,0.087275,"and machine learning. When multiple types of features are used, results approach human performance on an independent test set (both methods), and using cross-validation (machine learning). 1 that relies on enriched input features and multiple Introduction Many have argued that discourse has a global structure above the level of individual utterances, and that linguistic phenomena like prosody, cue phrases, and nominal reference are partly conditioned by and reflect this structure (cf. (Grosz and Hirschberg, 1992; Grosz and Sidner, 1986; Hirschberg and Grosz, 1992; Hirschberg and Litman, 1993; Hirschberg and Pierrehumbert, 1986; Hobbs, 1979; Lascarides and Oberlander, 1992; Linde, 1979; Mann and Thompson, 1988; Polanyi, 1988; Reichman, 1985; Webber, 1991)). However, an obstacle to exploiting the relation between global structure and linguistic devices in natural language systems is that there is too little data about how they constrain one another. We have been engaged in a study addressing this gap. In previous work (Passonneau and Litman, 1993), we reported on a method for empirically validating global discourse units, and on our evaluation of algorithms to identify these units. We found significant agreement amon"
P95-1015,P86-1031,0,0.0142102,"discourse signaling uses of cue words in (Hirschberg and Litman, 1993); a potential correlation between discourse signaling uses of cue words and adjacency patterns between cue words was also suggested. Finally, (Litman, 1994) found that treating cue phrases individually rather than as a class enhanced the results of (iiirschberg and Litman, 1993). Passonneau (to appear) examined some of the few claims relating discourse anaphoric noun phrases to global discourse structure in the Pear corpus. Resuits included an absence of correlation of segmental structure with centering (Grosz et al., 1983; Kameyama, 1986), and poor correlation with the contrast between full noun phrases and pronouns. As noted in (Passonneau and Litman, 1993), the NP features largely reflect Passonneau&apos;s hypotheses that adjacent utterances are more likely to contain expressions that corefer, or that are inferentially linked, if they occur within the same segment; and that a definite pronoun is more likely than a full NP to refer to an entity that was mentioned in the current segment, if not in the previous utterance. .63 .64 .72 .68 .06 .07 .12 .11 Table 1: Average human performance. range in length from 51 to 162 phrases (Avg."
P95-1015,P93-1041,0,0.328127,"Missing"
P95-1015,J93-4004,0,0.0377588,"Missing"
P95-1015,J92-4007,0,0.0307282,"Missing"
P95-1015,J91-1002,0,0.0792523,"Missing"
P95-1015,P93-1020,1,0.81315,"re partly conditioned by and reflect this structure (cf. (Grosz and Hirschberg, 1992; Grosz and Sidner, 1986; Hirschberg and Grosz, 1992; Hirschberg and Litman, 1993; Hirschberg and Pierrehumbert, 1986; Hobbs, 1979; Lascarides and Oberlander, 1992; Linde, 1979; Mann and Thompson, 1988; Polanyi, 1988; Reichman, 1985; Webber, 1991)). However, an obstacle to exploiting the relation between global structure and linguistic devices in natural language systems is that there is too little data about how they constrain one another. We have been engaged in a study addressing this gap. In previous work (Passonneau and Litman, 1993), we reported on a method for empirically validating global discourse units, and on our evaluation of algorithms to identify these units. We found significant agreement among naive subjects on a discourse segmentation task, which suggests that global discourse units have some objective reality. However, we also found poor correlation of three untuned algorithms (based on features of referential noun phrases, cue words, and pauses, respectively) with the subjects&apos; segmentations. In this paper, we discuss two methods for developing segmentation algorithms using multiple know*Bellcore did not sup"
P95-1015,P94-1050,0,0.212033,"Missing"
P97-1035,P84-1029,0,0.255803,"Missing"
P97-1035,P95-1019,0,0.0562158,"Missing"
P97-1035,J97-1005,1,0.46079,"Missing"
P97-1035,H92-1005,0,0.189592,"Missing"
P97-1035,P92-1032,0,0.00533955,"Missing"
P97-1035,J86-3001,0,0.175731,"Missing"
P97-1035,H92-1009,0,0.0215109,"Missing"
P97-1035,P96-1038,0,0.00615359,"Missing"
P97-1035,H90-1023,0,0.0327404,"Missing"
P97-1035,J97-1006,0,0.05548,"Missing"
P97-1035,C82-1066,0,\N,Missing
P97-1035,H91-1062,0,\N,Missing
P97-1035,P95-1018,0,\N,Missing
P97-1035,J96-2004,0,\N,Missing
P98-2129,P84-1029,0,0.333526,"n the 2 TOOT TOOT allows users to access online AMTRAK train schedules via a telephone dialogue, as in Figure 1.I (All examples are from the experiment in Section 3.) We have built two versions of TOOT: literal TOOT (LT) and cooperative TOOT (CT). LT and CT have equivalent functionality, but use different response strategies to present tabular results of web queries in a displayless environment) LT and CT incorporate many of the types of database responses in the focus of considerable research in natural language and spoken dialogue systems (Allen and Perrault, 1980; Mays, 1980; Kaplan, 1981; Joshi et al., 1984; McCoy, 1989; Pao and Wilpon, 1992; Moore, 1994; Seneff et al., 1995; Goddeau et al., 1996; Pieraccini et al., 1997). However, despite the existence of many algorithms for generating cooperative responses, there has been little empirical work addressing the evaluation of such algorithms in the context of real-time natural language dialogue systems with human users. Thus it is unclear under what conditions cooperative responses result in more efficient or efficacious dialogues. This paper presents an empirical evaluation &apos;Our domain was chosen to afford future comparison with similar systems,"
P98-2129,J86-2002,0,0.122405,"Missing"
P98-2129,H92-1008,0,0.413772,"access online AMTRAK train schedules via a telephone dialogue, as in Figure 1.I (All examples are from the experiment in Section 3.) We have built two versions of TOOT: literal TOOT (LT) and cooperative TOOT (CT). LT and CT have equivalent functionality, but use different response strategies to present tabular results of web queries in a displayless environment) LT and CT incorporate many of the types of database responses in the focus of considerable research in natural language and spoken dialogue systems (Allen and Perrault, 1980; Mays, 1980; Kaplan, 1981; Joshi et al., 1984; McCoy, 1989; Pao and Wilpon, 1992; Moore, 1994; Seneff et al., 1995; Goddeau et al., 1996; Pieraccini et al., 1997). However, despite the existence of many algorithms for generating cooperative responses, there has been little empirical work addressing the evaluation of such algorithms in the context of real-time natural language dialogue systems with human users. Thus it is unclear under what conditions cooperative responses result in more efficient or efficacious dialogues. This paper presents an empirical evaluation &apos;Our domain was chosen to afford future comparison with similar systems, e.g., (Danieli and Gerbino, 1995)."
P98-2129,H92-1005,0,0.061133,"ute a Mean Recognition score per dialogue. We also listened to the recordings to determine how many times the user interrupted the agent (Barge Ins). To measure dialogue efficiency., the number of System Turns and User Turns were extracted from the dialogue manager log, and the total Elapsed Time was determined from the recording. To measure user satisfaction 4, users responded to the web survey in Figure 4, which assessed their subjective evaluation of the agent&apos;s performance. Each question was designed to measure a partic4Questionnaire-based user satisfaction ratings (Shriberg et al., 1992; Polifroni et al., 1992) have been frequently used in the literature as an external indicator of agent usability. • Was the system easy to understand in this conversation? (TTS Performance) • In this conversation, did the system understand what you said? (ASR Performance) • In this conversation, was it easy to find the schedule you wanted? (Task Ease) • Was the pace of interaction with the system appropriate in this conversation? (Interaction Pace) • In this conversation, did you know what you could say at each point of the dialogue? (User Expertise) • How often was the system sluggish and slow to reply to you in thi"
P98-2129,H92-1009,0,0.212635,"ASR rejections, to compute a Mean Recognition score per dialogue. We also listened to the recordings to determine how many times the user interrupted the agent (Barge Ins). To measure dialogue efficiency., the number of System Turns and User Turns were extracted from the dialogue manager log, and the total Elapsed Time was determined from the recording. To measure user satisfaction 4, users responded to the web survey in Figure 4, which assessed their subjective evaluation of the agent&apos;s performance. Each question was designed to measure a partic4Questionnaire-based user satisfaction ratings (Shriberg et al., 1992; Polifroni et al., 1992) have been frequently used in the literature as an external indicator of agent usability. • Was the system easy to understand in this conversation? (TTS Performance) • In this conversation, did the system understand what you said? (ASR Performance) • In this conversation, was it easy to find the schedule you wanted? (Task Ease) • Was the pace of interaction with the system appropriate in this conversation? (Interaction Pace) • In this conversation, did you know what you could say at each point of the dialogue? (User Expertise) • How often was the system sluggish and sl"
P98-2129,P97-1035,1,0.841035,"aborate the conditions under which TOOT&apos; s cooperative rather than literal strategy contributes to greater performance. database queries in TOOT, a spoken dialogue agent for accessing online train schedules via a telephone conversation. We conduct an experiment in which 12 users carry out 4 tasks of varying difficulty with one of two versions of TOOT (literal and cooperative TOOT), resulting in a corpus of 48 dialogues. The values for a wide range of evaluation measures are then extracted from this corpus. We analyze our data using both traditional hypothesis testing methods and the PARADISE (Walker et al., 1997; Walker et al., 1998) methodology for estimating a performance function. Hypothesis testing shows that while differences among some evaluation measures depend on the response strategy (literal or cooperative), other differences are a function of application task and task/strategy interactions. A PARADISE assessment of the contribution of each evaluation measure to overall performance shows that strategy-dependent dialogue phenomena as well as phenomena associated with speech recognition significantly predict performance. Our results identify the conditions under which TOOT&apos; s cooperative resp"
P99-1040,P98-1122,0,0.283049,"rformance of classifiers constructed from rather different feature sets (such as acoustic and lexical features) suggest that there is some redundancy between these feature sets (at least with respect to the task). Fourth, the fact 315 that the best estimated accuracy was achieved using all of the features suggests that even problems that seem inherently acoustic may best be solved by exploiting higher-level information. This work differs from previous work in focusing on behavior at the (sub)dialogue level, rather than on identifying single misrecognitions at the utterance level (Smith, 1998; Levow, 1998; van Zanten, 1998). The rationale is that a single misrecognition may not warrant a global change in dialogue strategy, whereas a user's repeated problems communicating with the system might warrant such a change. While we are not aware of any other work that has applied machine learning to detecting patterns suggesting that the user is having problems over the course of a dialogue, (Levow, 1998) has applied machine learning to identifying single misrecognitions. We are currently extending our feature set to include acoustic-prosodic features such as those used by Levow, in order to predict m"
P99-1040,P99-1024,0,0.0659675,"ports the use of ""set-valued"" features for representing text, and because if-then rules are often easier for people to understand than decision trees (Quinlan, 1993). Below we describe our corpus of dialogues, the assignment of classes to each dialogue, the extraction of features from each dialogue, and our learning experiments. Corpus: Our corpus consists of a set of 544 dialogues (over 40 hours of speech) between humans and one of three dialogue systems: ANNIE (Kamm et al., 1998), an agent for voice dialing and messaging; ELVIS (Walker et al., 1998b), an agent for accessing email; and TOOT (Litman and Pan, 1999), an agent for accessing online train schedules. Each agent was implemented using a generalpurpose platform for phone-based spoken dialogue systems (Kamm et al., 1997). The dialogues were obtained in controlled experiments designed to evaluate dialogue strategies for each agent. The exper~We also ran experimentsusing the machine learning program BOOSTEXTER(Schapire and Singer,To appear), with results similarto those presentedbelow. iments required users to complete a set of application tasks in conversations with a particular version of the agent. The experiments resulted in both a digitized r"
P99-1040,P98-2129,1,0.868864,"Missing"
P99-1040,H92-1009,0,0.353503,"Missing"
P99-1040,P98-2219,1,0.824553,"been appropriate in dialogue D1 from the Annie system (Kamm et al., 1998), shown in Figure 1. In order to improve performance through such adaptation, a system must first be able to identify, in real time, salient properties of an ongoing dialogue that call for some useful change in system strategy. In other words, adaptive systems should try to automatically identify actionable properties of ongoing dialogues. Previous work has shown that speech recognition performance is an important predictor of user satisfaction, and that changes in dialogue behavior impact speech recognition performance (Walker et al., 1998b; Litman et al., 1998; Kamm et al., 1998). Therefore, in this work, we focus on the task of automatically detecting poor speech recognition performance in several spoken dialogue systems developed at AT&T Labs. Rather than hand-crafting rules that classify speech recognition performance in an ongoing dialogue, we take a machine learning approach. We begin with a collection of system logs from actual dialogues that were labeled by humans as having had ""good"" or ""bad"" speech recognition (the training set). We then apply standard machine learning algorithms to this training set in the hope of di"
P99-1040,C98-1117,0,\N,Missing
P99-1040,C98-2214,1,\N,Missing
P99-1040,C98-2124,1,\N,Missing
W00-0304,C00-1073,1,0.868515,"Missing"
W01-1610,N01-1027,1,0.76379,"Missing"
W01-1610,P98-1122,0,0.0323677,"eir intended actions were carried out correctly or not, in particular when the dialogue system does not give appropriate feedback about its internal representation at the right moment. In addition, users' corrections may miss their goal, because corrections themselves are more dicult for the system to recognize and interpret correctly, which may lead to so-called cyclic (or spiral) errors. That corrections are dicult for ASR systems is generally explained by the fact that they tend to be hyperarticulated |higher, louder, longer . . . than other turns (Wade et al., 1992; Oviatt et al., 1996; Levow, 1998; Bell and Gustafson, 1999; Shimojima et al., 1999), where ASR models are not well adapted to handle this special speaking style. The current paper focuses on user corrections, and looks at places where people rst become aware of a system problem (aware sites""). In other papers (Swerts et al., 2000; Hirschberg et al., 2001; Litman et al., 2001), we have already given some descriptive statistics on corrections and aware sites and we have been looking at methods to automatically predict these two utterance categories. One of our major ndings is that prosody, which had already been shown to be a"
W01-1610,A00-2029,1,0.740134,"a et al., 1999), where ASR models are not well adapted to handle this special speaking style. The current paper focuses on user corrections, and looks at places where people rst become aware of a system problem (aware sites""). In other papers (Swerts et al., 2000; Hirschberg et al., 2001; Litman et al., 2001), we have already given some descriptive statistics on corrections and aware sites and we have been looking at methods to automatically predict these two utterance categories. One of our major ndings is that prosody, which had already been shown to be a good predictor of misrecognitions (Litman et al., 2000; Hirschberg et al., 2000), is also useful to correctly classify corrections and aware sites. In this paper, we will elaborate more on the exact labeling scheme we used, and add further descriptive statistics. More in particular, we address the question whether there is much variance in the way people react to system errors, and if so, to what extent this variance can be explained on the basis of particular properties of the dialogue system. In the following section we rst provide details on the TOOT corpus that we used for our analyses. Then we give information on the labels for corrections a"
W01-1610,P01-1048,1,0.191728,"which may lead to so-called cyclic (or spiral) errors. That corrections are dicult for ASR systems is generally explained by the fact that they tend to be hyperarticulated |higher, louder, longer . . . than other turns (Wade et al., 1992; Oviatt et al., 1996; Levow, 1998; Bell and Gustafson, 1999; Shimojima et al., 1999), where ASR models are not well adapted to handle this special speaking style. The current paper focuses on user corrections, and looks at places where people rst become aware of a system problem (aware sites""). In other papers (Swerts et al., 2000; Hirschberg et al., 2001; Litman et al., 2001), we have already given some descriptive statistics on corrections and aware sites and we have been looking at methods to automatically predict these two utterance categories. One of our major ndings is that prosody, which had already been shown to be a good predictor of misrecognitions (Litman et al., 2000; Hirschberg et al., 2000), is also useful to correctly classify corrections and aware sites. In this paper, we will elaborate more on the exact labeling scheme we used, and add further descriptive statistics. More in particular, we address the question whether there is much variance in the"
W01-1610,P99-1024,0,0.0369287,"ses. Then we give information on the labels for corrections and aware sites, and on the actual labeling procedure. The next section gives the results of some descriptive statistics on properties of corrections and aware sites and on their distributions. We will end the paper with a general discussion of our ndings. 2 The data 2.1 The TOOT corpus Our corpus consists of dialogues between human subjects and TOOT, a spoken dialogue system that allows access to train information from the web via telephone. TOOT was collected to study variations in dialogue strategy and in user-adapted interaction (Litman and Pan, 1999). It is implemented using an IVR (interactive voice response) platform developed at AT&T, combining ASR and textto-speech with a phone interface (Kamm et al., 1997). The system's speech recognizer is a speaker-independent hidden Markov model system with context-dependent phone models for telephone speech and constrained grammars de ning vocabulary at any dialogue state. The platform supports barge-in. Subjects performed four tasks with one of several versions of the system that di ered in terms of locus of initiative (system, user, or mixed), con rmation strategy (explicit, implicit, or none),"
W01-1610,C98-1117,0,\N,Missing
W03-0205,W01-1609,0,0.112852,"ialogue systems are text based, there is reason to believe that speech based tutorial dialogue systems could be more effective. Prior studies have shown considerable benefits of human-human tutoring through spoken interactions (Lemke, 1990; Chi et al., 1994). (Hausmann and Chi, 2002) has shown that spontaneous self-explanation occurs much more frequently in spoken tutoring then in text based tutoring, suggesting that typing requires additional cognitive capacity and thus reduces the cognitive resources available for spontaneous self-explanation. Other research projects (Mostow and Aist, 2001; Fry et al., 2001) have shown that basic spoken natural language capabilities can be implemented quite effectively in computer tutoring systems. Moreover, speech contains prosodic and acoustic information which has been shown to improve the accuracy of predicting emotional states (Ang et al., 2002; Batliner et al., 2000) and user responses to system errors (Litman et al., 2001) that are useful for triggering system adaptation. We are thus currently developing a speech based dialogue system that uses a text based system (VanLehn et al., 2002) as its “back-end”. These systems and their goals will be discussed in"
W03-0205,H93-1016,0,0.0372439,"hysics problem. A computer tutor then engages the student in a natural language dialogue to provide feedback, correct misconceptions, and to elicit more complete explanations. The first version of Why2-Atlas was deployed and evaluated with undergraduate students in the spring of 2002; the system is continuing to be actively developed (Graesser et al., 2002). We are currently developing a speech-enabled version of Why2-ATLAS, called ITSPOKE (Intelligent Tutoring SPOKEn dialogue system), that uses the Why2-Atlas system as its “back-end”. To date we have interfaced the Sphinx2 speech recognizer (Huang et al., 1993) with stochastic language models trained from example user utterances, and the Festival speech synthesizer (Black and Taylor, 1997) for text-to-speech, to the Why2-Atlas backend. The rest of the needed natural language processing components, e.g. the sentence-level syntactic and semantic analysis modules (Ros´e, 2000), discourse and domain level processors (Makatchev et al., 2002), and a finite-state dialogue manager (Ros´e et al., 2001), are provided by a toolkit that is part of the Why2-Atlas backend. The student speech is digitized from microphone input, while the tutor’s synthesized speech"
W03-0205,P01-1048,1,0.792039,"spoken tutoring then in text based tutoring, suggesting that typing requires additional cognitive capacity and thus reduces the cognitive resources available for spontaneous self-explanation. Other research projects (Mostow and Aist, 2001; Fry et al., 2001) have shown that basic spoken natural language capabilities can be implemented quite effectively in computer tutoring systems. Moreover, speech contains prosodic and acoustic information which has been shown to improve the accuracy of predicting emotional states (Ang et al., 2002; Batliner et al., 2000) and user responses to system errors (Litman et al., 2001) that are useful for triggering system adaptation. We are thus currently developing a speech based dialogue system that uses a text based system (VanLehn et al., 2002) as its “back-end”. These systems and their goals will be discussed in Section 2. We expect that the different modalities used by these systems (e.g. text based vs speech based) will display interesting differences with respect to the characteristics of dialogue interaction that may determine their relative merits with respect to increasing student performance. Although human-computer data from the speech based system is not yet"
W03-0205,N03-2018,1,0.797483,"isn’t present in typed dialogue. Connections between learning and emotion have been well documented (Coles, 1999), so it seems likely that the success of computer-based tutoring systems could be greatly increased if they were capable of predicting and adapting to student emotional states, e.g. reinforcing positive states, while rectifying negative states (Evens, 2002). Preliminary machine learning experiments involving emotion annotation and automatic feature extraction from our corpus suggest that ITSPOKE can indeed be enhanced to automatically predict and adapt to student emotional states (Litman et al., 2003). 3 Typed Human-Human Tutoring Corpus The Why2-Atlas Human-Human Typed Tutoring Corpus is a collection of typed tutoring dialogues between (human) tutor and student collected via typed interface, which the tutor plays the same role that Why2-Atlas is designed to perform. The experimental procedure is as follows: 1) students are given a pretest measuring their knowledge of physics, 2) students are asked to read through a small document of background material, 3) students work through a set of up to 10 Why2-Atlas training problems with the human tutor, and 4) students are given a post-test that"
W03-0205,W02-0211,1,0.832815,"eveloping a speech-enabled version of Why2-ATLAS, called ITSPOKE (Intelligent Tutoring SPOKEn dialogue system), that uses the Why2-Atlas system as its “back-end”. To date we have interfaced the Sphinx2 speech recognizer (Huang et al., 1993) with stochastic language models trained from example user utterances, and the Festival speech synthesizer (Black and Taylor, 1997) for text-to-speech, to the Why2-Atlas backend. The rest of the needed natural language processing components, e.g. the sentence-level syntactic and semantic analysis modules (Ros´e, 2000), discourse and domain level processors (Makatchev et al., 2002), and a finite-state dialogue manager (Ros´e et al., 2001), are provided by a toolkit that is part of the Why2-Atlas backend. The student speech is digitized from microphone input, while the tutor’s synthesized speech is played to the student using a speaker and/or headphone. We are now in the process of adapting the knowledge sources needed by the spoken language components to our application domain. For example, we have developed a set of dialogue dependent language models using the experimental human-computer typed corpus (4551 student utterances) obtained during the Why2-Atlas 2002 evaluat"
W03-0409,N01-1027,1,0.837676,"mation, information from the automatic speech recognizer, system conditions and dialog history. Then, each user turn was labeled with respect to every classification task. Even though our classification tasks share the same data, there are clear differences between them. ISCORR and STATUS both deal with user corrections which is quite different from predicting speech recognition errors (handled in WERBIN and CABIN). Moreover, one will expect very little noise or no noise at all when manually annotating WERBIN and CABIN. For more information on our tasks and features, see (Litman et al., 2000; Hirschberg et al., 2001; Litman et al., 2001). There are a number of dimensions where our tasks differ from the tasks from the previous study. First of all our datasets are smaller (2,328 instances compared with at least 23,898). Second, the number of features used is much bigger than the previous study (141 compared with 4-11). Moreover, many features from our datasets are numeric while the previous study had none. These differences will also reflect on our exceptionality measures values. For example, the smallest range for typicality in the previous study was between 0.43 and 10.57 while for our tasks it is betwee"
W03-0409,A00-2029,1,0.834432,"nclude prosodic information, information from the automatic speech recognizer, system conditions and dialog history. Then, each user turn was labeled with respect to every classification task. Even though our classification tasks share the same data, there are clear differences between them. ISCORR and STATUS both deal with user corrections which is quite different from predicting speech recognition errors (handled in WERBIN and CABIN). Moreover, one will expect very little noise or no noise at all when manually annotating WERBIN and CABIN. For more information on our tasks and features, see (Litman et al., 2000; Hirschberg et al., 2001; Litman et al., 2001). There are a number of dimensions where our tasks differ from the tasks from the previous study. First of all our datasets are smaller (2,328 instances compared with at least 23,898). Second, the number of features used is much bigger than the previous study (141 compared with 4-11). Moreover, many features from our datasets are numeric while the previous study had none. These differences will also reflect on our exceptionality measures values. For example, the smallest range for typicality in the previous study was between 0.43 and 10.57 while f"
W03-0409,P01-1048,1,0.799749,"the automatic speech recognizer, system conditions and dialog history. Then, each user turn was labeled with respect to every classification task. Even though our classification tasks share the same data, there are clear differences between them. ISCORR and STATUS both deal with user corrections which is quite different from predicting speech recognition errors (handled in WERBIN and CABIN). Moreover, one will expect very little noise or no noise at all when manually annotating WERBIN and CABIN. For more information on our tasks and features, see (Litman et al., 2000; Hirschberg et al., 2001; Litman et al., 2001). There are a number of dimensions where our tasks differ from the tasks from the previous study. First of all our datasets are smaller (2,328 instances compared with at least 23,898). Second, the number of features used is much bigger than the previous study (141 compared with 4-11). Moreover, many features from our datasets are numeric while the previous study had none. These differences will also reflect on our exceptionality measures values. For example, the smallest range for typicality in the previous study was between 0.43 and 10.57 while for our tasks it is between 0.9 and 1.1. To expl"
W04-2326,H89-2010,0,0.193285,"Missing"
W04-2326,J96-2004,0,\N,Missing
W05-0204,N04-3002,1,0.815462,"a list of individual concepts from these matrices in order of their link strengths, starting with the strongest concept. They show a correlation between this sequence and the order in which subjects name concepts in a freerecall task. In van den Broek’s original implementation, this model was run on short stories. In the current work, the model is extended to cover a corpus of transcripts of physics tutoring dialogs. In the next section we describe this corpus. 3 Corpus of Tutoring Transcripts Our corpus was taken from transcripts collected for the ITSPOKE intelligent tutoring system project (Litman and Silliman, 2004). This project has collected tutoring dialogs with both human and computer tutors. In this paper, we describe results using the human tutor corpus. Students being tutored are first given a pre-test to gauge their physics knowledge. After reading instructional materials about physics, they are given a qualitative physics problem and asked to write an essay describing its solution. The tutor (in our case, a human tutor), examines this essay, identifies points of the argument that are missing or wrong, and engages the student in a dialog to remediate those flaws. When the tutor is satisfied that"
W06-0607,W05-0308,1,\N,Missing
W06-0607,P96-1038,0,\N,Missing
W06-1611,P01-1016,0,0.219038,"ity of Pittsburgh Pittsburgh, USA mrotaru@cs.pitt.edu Diane J. Litman University of Pittsburgh Pittsburgh, USA litman@cs.pitt.edu (Walker et al., 2000). An extensive set of parameters can be found in (Möller, 2005a). In this paper we study the utility of discourse structure as an information source for SDS performance analysis. The discourse structure hierarchy has been shown to be useful for other tasks: understanding specific lexical and prosodic phenomena (Hirschberg and Nakatani, 1996; Levow, 2004), natural language generation (Hovy, 1993), predictive/generative models of postural shifts (Cassell et al., 2001), and essay scoring (Higgins et al., 2004). We perform our analysis on a corpus of speech-based tutoring dialogues. A tutoring SDS (Litman and Silliman, 2004; Pon-Barry et al., 2004) has to discuss concepts, laws and relationships and to engage in complex subdialogues to correct student misconceptions. As a result, dialogues with such systems have a rich discourse structure. We perform three experiments to measure three ways of exploiting the discourse structure. In our first experiment, we test the predictive utility of the discourse structure in itself. For example, we look at whether the nu"
W06-1611,2005.sigdial-1.10,1,0.776783,"(recall the SameGoal–Neutral bigram, Table 4). In our previous work (Rotaru and Litman, 2006), we perp .08 .00 .00 .05 Table 4: Trend and significant transition–certainty bigram correlations In contrast, staying neutral in terms of certainty after a system rejection is positively correlated with learning. These correlations show that based on their position in the discourse structure, neutral student answers will be correlated either negatively or positively with learning. Unlike student state unigram parameters which produce only one significant correlation, 90 learning (Craig et al., 2004; Forbes-Riley and Litman, 2005; Pon-Barry et al., 2006). The performance of our best model is comparable or higher than training performances reported in previous work (Forbes-Riley and Litman, 2006; Möller, 2005b; Walker et al., 2001). Since our training data is relatively small (20 data points) and overfitting might be involved here, in the future we plan to do a more in-depth evaluation by testing if our model generalizes on a larger ITSPOKE corpus we are currently annotating. formed an analysis of the rejected student turns and studied how rejections affect the student state. The results of our analysis suggested a new"
W06-1611,N06-1034,1,0.778923,"imes an Advance transition is followed by an incorrect student answer. Student state Because for our tutoring system student learning is the relevant performance metric, we hypothesize that information about student state in each student turn, in terms of correctness and certainty, will be an important indicator. For example, a student being more correct and certain during her interaction with ITSPOKE might be indicative of a higher learning gain. Also, previous studies have shown that tutoring specific parameters can improve the quality of SDS performance models that model the learning gain (Forbes-Riley and Litman, 2006). In our corpus, each student turn was manually labeled for correctness and certainty (Table 1). While our system assigns a correctness label to each student turn to plan its next move, we choose to use a manual annotation of correctness to eliminate the noise introduced by the automatic speech recognition component and the natural language understanding component. A human annotator used the human transcripts and his physics knowledge to label each student turn for various degrees of correctness: correct, partially correct, incorrect and unable to answer. “Unable to Answer” label was used for"
W06-1611,P04-1044,0,0.0700021,"Missing"
W06-1611,P01-1066,0,0.685068,"istics (Grosz and Sidner, 1986). A critical ingredient of this theory is the intentional structure. According to the theory, each discourse has a discourse purpose/intention. Satisfying the main discourse purpose is achieved by satisfying several smaller purposes/intentions organized in a hierarchical structure. As a result, the discourse is segmented in discourse segments each with an associated discourse segment purpose/intention. This theory has inspired several generic dialogue managers for spoken dialogue systems (Bohus and Rudnicky, 2003). discourse structure hierarchy by flattening it (Walker et al., 2001) (Section 5). As another way to exploit the discourse structure, in our third experiment we look at whether specific trajectories in the discourse structure are indicative of performance. For example, we test if two consecutive pushes in the discourse structure are correlated with higher learning. To measure the predictive utility of our interaction parameters, we focus primarily on correlations with our performance metric (Section 4). There are two reasons for this. First, a significant correlation between an interaction parameter and the performance metric is a good indicator of the paramete"
W06-1611,J86-3001,0,0.65383,"then used in a multivariate linear regression to predict the target performance metric. A critical ingredient in this approach is the relevance of the interaction parameters for the SDS success. A number of parameters that measure the dialogue efficiency (e.g. number of system/user turns, task duration) and the dialogue quality (e.g. recognition accuracy, rejections, helps) have been shown to be successful in 85 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 85–93, c Sydney, July 2006. 2006 Association for Computational Linguistics (Grosz and Sidner, 1986). A critical ingredient of this theory is the intentional structure. According to the theory, each discourse has a discourse purpose/intention. Satisfying the main discourse purpose is achieved by satisfying several smaller purposes/intentions organized in a hierarchical structure. As a result, the discourse is segmented in discourse segments each with an associated discourse segment purpose/intention. This theory has inspired several generic dialogue managers for spoken dialogue systems (Bohus and Rudnicky, 2003). discourse structure hierarchy by flattening it (Walker et al., 2001) (Section 5"
W06-1611,N04-1024,0,0.0583859,"cs.pitt.edu Diane J. Litman University of Pittsburgh Pittsburgh, USA litman@cs.pitt.edu (Walker et al., 2000). An extensive set of parameters can be found in (Möller, 2005a). In this paper we study the utility of discourse structure as an information source for SDS performance analysis. The discourse structure hierarchy has been shown to be useful for other tasks: understanding specific lexical and prosodic phenomena (Hirschberg and Nakatani, 1996; Levow, 2004), natural language generation (Hovy, 1993), predictive/generative models of postural shifts (Cassell et al., 2001), and essay scoring (Higgins et al., 2004). We perform our analysis on a corpus of speech-based tutoring dialogues. A tutoring SDS (Litman and Silliman, 2004; Pon-Barry et al., 2004) has to discuss concepts, laws and relationships and to engage in complex subdialogues to correct student misconceptions. As a result, dialogues with such systems have a rich discourse structure. We perform three experiments to measure three ways of exploiting the discourse structure. In our first experiment, we test the predictive utility of the discourse structure in itself. For example, we look at whether the number of pop-up transitions in the discours"
W06-1611,P96-1038,0,0.0944788,"1: Transition and student state distribution. Please note that each student dialogue has a specific discourse structure based on the dialogue that dynamically emerges based on the correctness of her answers. For this reason, the same system question in terms of content may get a different transition label for different students. For example, in Figure 1, if the student would have answered Tutor2 correctly, the next tutor turn would have had the same content as Tutor5 but the Advance label. Also, while a human annotation of the discourse structure will be more complex but more time consuming (Hirschberg and Nakatani, 1996; Levow, 2004), its advantages are outweighed by the automatic nature of our discourse structure annotation. We would like to highlight that our transition annotation is domain independent and automatic. Our transition labels capture behavior like starting a new dialogue (NewTopLevel), crossing discourse segment boundaries (Push, PopUp, PopUpAdv) and local phenomena inside a discourse segment (Advance, SameGoal). If the discourse structure information is available, the 87 transition information can be automatically computed using the procedure described above. 2.2 ters: unigrams and bigrams. T"
W06-1611,W04-2318,0,0.0165805,"e distribution. Please note that each student dialogue has a specific discourse structure based on the dialogue that dynamically emerges based on the correctness of her answers. For this reason, the same system question in terms of content may get a different transition label for different students. For example, in Figure 1, if the student would have answered Tutor2 correctly, the next tutor turn would have had the same content as Tutor5 but the Advance label. Also, while a human annotation of the discourse structure will be more complex but more time consuming (Hirschberg and Nakatani, 1996; Levow, 2004), its advantages are outweighed by the automatic nature of our discourse structure annotation. We would like to highlight that our transition annotation is domain independent and automatic. Our transition labels capture behavior like starting a new dialogue (NewTopLevel), crossing discourse segment boundaries (Push, PopUp, PopUpAdv) and local phenomena inside a discourse segment (Advance, SameGoal). If the discourse structure information is available, the 87 transition information can be automatically computed using the procedure described above. 2.2 ters: unigrams and bigrams. The difference"
W06-1611,N04-3002,1,0.897908,"An extensive set of parameters can be found in (Möller, 2005a). In this paper we study the utility of discourse structure as an information source for SDS performance analysis. The discourse structure hierarchy has been shown to be useful for other tasks: understanding specific lexical and prosodic phenomena (Hirschberg and Nakatani, 1996; Levow, 2004), natural language generation (Hovy, 1993), predictive/generative models of postural shifts (Cassell et al., 2001), and essay scoring (Higgins et al., 2004). We perform our analysis on a corpus of speech-based tutoring dialogues. A tutoring SDS (Litman and Silliman, 2004; Pon-Barry et al., 2004) has to discuss concepts, laws and relationships and to engage in complex subdialogues to correct student misconceptions. As a result, dialogues with such systems have a rich discourse structure. We perform three experiments to measure three ways of exploiting the discourse structure. In our first experiment, we test the predictive utility of the discourse structure in itself. For example, we look at whether the number of pop-up transitions in the discourse structure hierarchy predicts performance in our system. The second experiment measures the utility of the discour"
W06-1611,2005.sigdial-1.17,0,0.602914,"cts performance in our system. The second experiment measures the utility of the discourse structure as contextual information for two types of student states: correctness and certainty. The intuition behind this experiment is that interaction events should be treated differently based on their position in the discourse structure hierarchy. For example, we test if the number of incorrect answers after a pop-up transition has a higher predictive utility than the total number of incorrect student answers. In contrast, the majority of the previous work either ignores this contextual information (Möller, 2005a; Walker et al., 2000) or makes limited use of the Abstract In this paper we study the utility of discourse structure for spoken dialogue performance modeling. We experiment with various ways of exploiting the discourse structure: in isolation, as context information for other factors (correctness and certainty) and through trajectories in the discourse structure hierarchy. Our correlation and PARADISE results show that, while the discourse structure is not useful in isolation, using the discourse structure as context information for other factors or via trajectories produces highly predictiv"
W06-1611,P06-1025,1,0.784663,"ectly answers a question, entering a remediation subdialogue; she also incorrectly answers the first question in the remediation dialogue entering an even deeper remediation subdialogue. We hypothesize that these situations are indicative of big student knowledge gaps. In our corpus, we find that the more such big knowledge gaps are discovered and addressed by the system the higher the learning gain. The SameGoal–Push bigram captures another type of behavior after system rejections that is positively correlated with learning (recall the SameGoal–Neutral bigram, Table 4). In our previous work (Rotaru and Litman, 2006), we perp .08 .00 .00 .05 Table 4: Trend and significant transition–certainty bigram correlations In contrast, staying neutral in terms of certainty after a system rejection is positively correlated with learning. These correlations show that based on their position in the discourse structure, neutral student answers will be correlated either negatively or positively with learning. Unlike student state unigram parameters which produce only one significant correlation, 90 learning (Craig et al., 2004; Forbes-Riley and Litman, 2005; Pon-Barry et al., 2006). The performance of our best model is c"
W06-1625,P04-1045,1,0.922758,"on analyzing acoustic-prosodic cues (such as pitch, intensity, tempo etc.) in humorous conversations and testing if these cues can help us to automatically distinguish between humorous and nonhumorous (normal) utterances in speech. We hypothesize that not only the lexical content but also the prosody (or how the content is expressed) makes humorous expressions humorous. 1 Introduction As conversational systems are becoming prevalent in our lives, we notice an increasing need for adding social intelligence in computers. There has been a considerable amount of research on incorporating affect (Litman and Forbes-Riley, 2004) (Alm et al., 2005) (D’Mello et al., 2005) (Shroder and Cowie, 2005) (Klein et al., 2002) and personality (Gebhard et al., 2004) in computer interfaces, so that, for instance, user frustrations can be recognized and addressed in a graceful manner. As (Binsted, 1995) correctly pointed out, one way to alleviate user frustrations, and to make humancomputer interaction more natural, personal and interesting for the users, is to model HUMOR. Research in computational humor is still in very early stages, partially because humorous lanThe following sections describe our data collection and pre-proces"
W06-1625,H05-1067,0,0.164056,"analysis reveals significant differences in prosodic characteristics (such as pitch, tempo, energy etc.) of humorous and non-humorous speech, even when accounted for the gender and speaker differences. Humor recognition was carried out using standard supervised learning classifiers, and shows promising results significantly above the baseline. Before we can model humor in real application systems, we must first analyze features that characterize humor. Computational approaches to humor recognition so far primarily rely on lexical and stylistic cues such as alliteration, antonyms, adult slang (Mihalcea and Strapparava, 2005). The focus of our study is, on the other hand, on analyzing acoustic-prosodic cues (such as pitch, intensity, tempo etc.) in humorous conversations and testing if these cues can help us to automatically distinguish between humorous and nonhumorous (normal) utterances in speech. We hypothesize that not only the lexical content but also the prosody (or how the content is expressed) makes humorous expressions humorous. 1 Introduction As conversational systems are becoming prevalent in our lives, we notice an increasing need for adding social intelligence in computers. There has been a considerab"
W06-1625,H05-1073,0,0.0112111,"cues (such as pitch, intensity, tempo etc.) in humorous conversations and testing if these cues can help us to automatically distinguish between humorous and nonhumorous (normal) utterances in speech. We hypothesize that not only the lexical content but also the prosody (or how the content is expressed) makes humorous expressions humorous. 1 Introduction As conversational systems are becoming prevalent in our lives, we notice an increasing need for adding social intelligence in computers. There has been a considerable amount of research on incorporating affect (Litman and Forbes-Riley, 2004) (Alm et al., 2005) (D’Mello et al., 2005) (Shroder and Cowie, 2005) (Klein et al., 2002) and personality (Gebhard et al., 2004) in computer interfaces, so that, for instance, user frustrations can be recognized and addressed in a graceful manner. As (Binsted, 1995) correctly pointed out, one way to alleviate user frustrations, and to make humancomputer interaction more natural, personal and interesting for the users, is to model HUMOR. Research in computational humor is still in very early stages, partially because humorous lanThe following sections describe our data collection and pre-processing, followed by t"
W06-1625,P05-3029,0,0.109494,"Missing"
W09-3927,J86-3001,0,0.735798,"informativeness looks at how much the parameter can help us improve the system. We already know that the parameter is predictive of performance. But this does not tell us if there is a causal link between the two. In fact, the main drive is not to prove a causal link but to show that the interaction parameter will inform a modification of the system and that this modification will improve the system. This paper is part of our broader investigation into the utility of discourse structure for performance analysis. Although each dialogue has an inherent structure called the discourse structure (Grosz and Sidner, 1986), this information has received little attention in performance analysis settings. In our previous work (Rotaru and Litman, 2006), we established the predictiveness of several interaction parameters derived from discourse structure. Here we take a step further and demonstrate the informativeness of these parameters. We show that one of the predictive discourse structure-based parameters (PopUp-Incorrect) informs a promising modification of our system. 1 Although this terminology is not yet established in the SDS community, the investigations behind these properties are a common practice in the"
W09-3927,2005.sigdial-1.17,0,0.206138,"are also important and researchers have experimented with many such strategies as there is no clear winner in all contexts (e.g. (Bohus, 2007; Singh et al., 2002)). However, other factors can only be inferred through empirical analyses. A principled approach to identifying important factors and strategies to handle them comes from performance analysis. This approach was pioneered by the PARADISE framework (Walker et al., 2000). In PARADISE, the SDS behavior is quantified in the form of interaction parameters: e.g. speech recognition performance, number of turns, number of help requests, etc. (Möller, 2005).These parameters are then used in a multivariate linear regression to predict a SDS performance metric (e.g. task completion, user satisfaction: (Singh et al., 2002)). Finally, SDS redesign efforts are informed by the parameters that make it in the regression model. Conceptually, this equates to investigating two properties of interaction parameters: predictiveness and informativeness1. Predictiveness looks at the connection between the parameter and system performance via predictive models (e.g. multivariate linear regression in PARADISE). Once the predictiveness is established, it is import"
W09-3927,W08-0128,0,0.0289537,"information activated by the new PopUp–Incorrect strategy might have a positive effect on users’ correctness for future system questions especially on questions that discuss similar topics. As a result, the system has to correct the user less and, consequently, finish faster. Second, the average total time PI users spend reading the additional information is very small (about 2 minutes) compared to the average dialogue time. 6 Related work Designing robust, efficient and usable spoken dialogue systems (SDS) is a complex process that is still not well understood by the SDS research community (Möller and Ward, 2008). Typically, a number of evaluation/performance 183 metrics are used to compare multiple (versions of) SDS. But what do these metrics and the resulting comparisons tell us about designing SDS? There are several approaches to answering this question, each requiring a different level of supervision. One approach that requires little human supervision is to use reinforcement learning. In this approach, the dialogue is modeled as a (partially observable) Markov Decision Process (Levin et al., 2000; Young et al., 2007). A reward is given at the end of the dialogue (i.e. the evaluation metric) and t"
W09-3927,N04-1006,0,0.0151638,"question, each requiring a different level of supervision. One approach that requires little human supervision is to use reinforcement learning. In this approach, the dialogue is modeled as a (partially observable) Markov Decision Process (Levin et al., 2000; Young et al., 2007). A reward is given at the end of the dialogue (i.e. the evaluation metric) and the reinforcement learning process propagates back the reward to learn what the best strategy to employ at each step is. Other semiautomatic approaches include machine learning and decision theoretic approaches (Levin and Pieraccini, 2006; Paek and Horvitz, 2004). However, these semi-automatic approaches are feasible only in small and limited domains though recent work has shown how more complex domains can be modeled (Young et al., 2007). An approach that works on more complex domains but requires more human effort is through performance analysis: finding and tackling factors that affect the performance (e.g. PARADISE (Walker et al., 2000)). Central to this approach is the quality of the interaction parameters in terms of predicting the performance metric (predictiveness) and informing useful modifications of the system (informativeness). An extensiv"
W09-3927,W08-0101,0,0.0133562,"predict our performance metric. Here, we take a step forward and show that these correlations are not only a surface relationship. We show that redesigning the system in light of an interpretation of a correlation has a positive impact. 1 Introduction The success of a spoken dialogue system (SDS) depends on a large number of factors and the strategies employed to address them. Some of these factors are intuitive. For example, problems with automated speech recognition can derail a dialogue from the normal course: e.g. nonunderstandings, misunderstandings, endpointing, etc. (e.g. (Bohus, 2007; Raux and Eskenazi, 2008)). The strategies used to handle or avoid these situations are also important and researchers have experimented with many such strategies as there is no clear winner in all contexts (e.g. (Bohus, 2007; Singh et al., 2002)). However, other factors can only be inferred through empirical analyses. A principled approach to identifying important factors and strategies to handle them comes from performance analysis. This approach was pioneered by the PARADISE framework (Walker et al., 2000). In PARADISE, the SDS behavior is quantified in the form of interaction parameters: e.g. speech recognition pe"
W09-3927,P07-1046,1,0.922711,"whenever the event occurs. Because the information was presented visually, users can choose which part to read, which meant that we did not have to be on target with our explanations. To return to the spoken dialogue, users pressed a button when done reading the webpage. All webpages included several pieces of information we judged to be helpful. We included the tutor question, the correct answer and a text summary of the instruction so far and of the remediation subdialogue. We also presented a graphical representation of the discourse structure, called the Navigation Map. Our previous work (Rotaru and Litman, 2007) shows that users prefer this feature over not having it on many subjective dimensions related to understanding. Additional information not discussed by the system was also included if applicable: intuitions and examples from real life, the purpose of the question with respect to the current problem and previous problems and/or possible pitfalls. See Appendix 2 for a sample webpage. The information we included in the PopUp– Incorrect webpages has a “reflective” nature. For example, we summarize and discuss the relevant instruction. We also comment on the connection between the current problem"
W09-3927,P01-1066,0,0.0394993,"y of the interaction parameters in terms of predicting the performance metric (predictiveness) and informing useful modifications of the system (informativeness). An extensive set of parameters can be found in (Möller, 2005). Our use of discourse structure for performance analysis extends over previous work in two important aspects. First, we exploit in more detail the hierarchical information in the discourse structure through the domain-independent concept of discourse structure transitions. Most previous work does not use this information (e.g. (Möller, 2005)) or, if used, it is flattened (Walker et al., 2001). Also, to our knowledge, previous work has not employed parameters similar to our transition–phenomena (transition–correctness in this paper) and transition–transition bigram parameters. In addition, several of these parameters are predictive (Rotaru and Litman, 2006). Second, in our work we also look at the informativeness while most of the previous work stops at the predictiveness step. A notable exception is the work by (Litman and Pan, 2002). The factor they look at is user’s having multiple speech recognition problems in the dialogue. This factor is well known in the SDS field and it has"
W09-3927,W06-1611,1,\N,Missing
W09-3940,2005.sigdial-1.17,0,0.0933175,"Missing"
W11-1402,W06-1650,0,0.591832,"review systems, to automatically predict peer-review helpfulness based on features mined from textual reviews using Natural Language Processing (NLP) techniques. Such an intelligent component could enable peer-review systems to 1) control the quality of peer reviews that are sent back to authors, so authors can focus on the helpful ones; and 2) provide feedback to reviewers with respect to their reviewing performance, so students can learn to write better reviews. In our prior work (Xiong and Litman, 2011), we examined whether techniques used for predicting the helpfulness of product reviews (Kim et al., 2006) could be tailored to our peer-review domain, where the definition of helpfulness is largely influenced by the educational context of peer review. While previously we used the average of two expert-provided ratings as our gold standard of peer-review helpfulness1 , there are other types of helpfulness rating (e.g. author perceived helpfulness) that could be the gold standard, and that could potentially impact the features used to build the helpfulness model. In fact, we observe that peer-review helpfulness seems to differ not only between students and experts (example 1), but also between type"
W11-1402,C00-1072,0,0.146562,"Missing"
W11-1402,P05-1012,0,0.125839,"Missing"
W11-1402,W09-3605,0,0.0252211,"ansitions and opinions are most useful in predicting helpfulness as perceived by students, while review length is generally effective in predicting expert helpfulness. While the presence of praise and summary comments are more effective in modeling writingexpert helpfulness, providing solutions is more useful in predicting content-expert helpfulness. 11 2 Related Work To our knowledge, no prior work on peer review from the NLP community has attempted to automatically predict peer-review helpfulness. Instead, the NLP community has focused on issues such as highlighting key sentences in papers (Sandor and Vorndran, 2009), detecting important feedback features in reviews (Cho, 2008; Xiong and Litman, 2010), and adapting peer-review assignment (Garcia, 2010). However, many NLP studies have been done on the helpfulness of other types of reviews, such as product reviews (Kim et al., 2006; Ghose and Ipeirotis, 2010), movie reviews (Liu et al., 2008), book reviews (Tsur and Rappoport, 2009), etc. Kim et al. (2006) used regression to predict the helpfulness ranking of product reviews based on various classes of linguistic features. Ghose and Ipeirotis (2010) further examined the socio-economic impact of product revi"
W11-1402,P11-2088,1,0.827112,"tten in a helpful way. To address this issue, we propose to add a peer-review helpfulness model to current peer-review systems, to automatically predict peer-review helpfulness based on features mined from textual reviews using Natural Language Processing (NLP) techniques. Such an intelligent component could enable peer-review systems to 1) control the quality of peer reviews that are sent back to authors, so authors can focus on the helpful ones; and 2) provide feedback to reviewers with respect to their reviewing performance, so students can learn to write better reviews. In our prior work (Xiong and Litman, 2011), we examined whether techniques used for predicting the helpfulness of product reviews (Kim et al., 2006) could be tailored to our peer-review domain, where the definition of helpfulness is largely influenced by the educational context of peer review. While previously we used the average of two expert-provided ratings as our gold standard of peer-review helpfulness1 , there are other types of helpfulness rating (e.g. author perceived helpfulness) that could be the gold standard, and that could potentially impact the features used to build the helpfulness model. In fact, we observe that peer-r"
W11-1417,N04-3002,1,0.794081,"positively correlated with motivation assessments (DelaRosa and Eskenazi, 2011). Also, in a separate study with the REAP tutor, attempts to manipulate reading motivation by presenting more interesting stories were shown to improve vocabulary learning (Heilman et al., 2010). In addition to influencing learning outcomes, motivational state may also affect which interventions will be effective during tutoring. For example, Ward and Litman (2011) have shown that motivation can significantly affect which students benefit from a reflective reading following interactive tutoring with a the Itspoke (Litman and Silliman, 2004) tutor. Maxine Eskenazi Language Technologies Institute Carnegie Mellon University Pittsburgh, Pa., 15213 max@cmu.edu An accurate way to measure student motivation during tutoring could therefore be valuable to Intelligent Tutoring System (ITS) researchers. Several self-report instruments have been developed which measure various aspects of motivation (e.g. (Pintrich and DeGroot, 1990; McKenna and Kear, 1990)). However, these instruments are too intrusive to be administered during tutoring, for fear of fatally disrupting learning. We would prefer a non-intrusive measure which would allow an IT"
W11-1417,W02-0109,0,0.049875,"g scores for various subsets measures, as well as of questions. for the change-in-motivation score, calculated as post-minus-pre. 138 Finally, the student’s use of “velocity” will be counted as a cohesive tie because of its semantic similarity to “acceleration,” from the preceding turn. The algorithm therefore counts four ties in Table 1. As described more completely in (Ward and Litman, 2008), semantic similarity cohesive ties are counted by measuring two words’ proximity in the WordNet (Miller et al., 1990) hierarchy. We use a simple path distance similarity measure, as implemented in NLTK (Loper and Bird, 2002). This measure counts the number of edges N in the shortest path between two words in WordNet, and calculates similarity as 1 / (1 + N). Our implementation of this semantic similarity measure allows setting a threshold θ, such that only word pairs with stronger-than-threshold similarity are counted. Table 3 shows some semantic similarity pairs counted with a threshold of 0.3. We obtain a normalized cohesion score for each dialog by dividing the tie count by the number of turns in the dialog. We then sum the line normalized counts over all the dialogs for each student, resulting in a perstudent"
W11-1417,H05-1122,0,0.0695707,"f videos. 136 Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 136–141, c Portland, Oregon, 24 June 2011. 2011 Association for Computational Linguistics the tutor and student partners in a tutoring dialog which was shown to be correlated with task success in several corpora of tutorial dialogs. Measures of cohesion have also been used in a variety of NLP tasks such as measuring text readability (e.g. (Pitler and Nenkova, 2008)), measuring stylistic differences in text (Mccarthy et al., 2006), and for topic segmentation in tutorial dialog (Olney and Cai, 2005). Given the previously mentioned results relating motivation to educational task success, these links between task success and cohesion lead us to hypothesize a direct correlation between motivation and cohesion when using the Itspoke tutor. We will first briefly describe the Itspoke tutor, and the corpus of tutoring dialogs used in this study. We will then describe the instrument we used to measure motivation both before and immediately after tutoring, then we will describe the algorithm used to measure cohesion in the tutoring dialogs. Finally, we show results of correlations between the mea"
W11-1417,D08-1020,0,0.0251169,"on, Ward and Litman (2006; 2008) investigated a measure of lexical similarity between 1 In this experiment, the task was to watch a series of videos. 136 Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 136–141, c Portland, Oregon, 24 June 2011. 2011 Association for Computational Linguistics the tutor and student partners in a tutoring dialog which was shown to be correlated with task success in several corpora of tutorial dialogs. Measures of cohesion have also been used in a variety of NLP tasks such as measuring text readability (e.g. (Pitler and Nenkova, 2008)), measuring stylistic differences in text (Mccarthy et al., 2006), and for topic segmentation in tutorial dialog (Olney and Cai, 2005). Given the previously mentioned results relating motivation to educational task success, these links between task success and cohesion lead us to hypothesize a direct correlation between motivation and cohesion when using the Itspoke tutor. We will first briefly describe the Itspoke tutor, and the corpus of tutoring dialogs used in this study. We will then describe the instrument we used to measure motivation both before and immediately after tutoring, then we"
W11-1417,P07-1102,0,0.0197503,"ers are likely to adopt the terms used by a WOZ dialog system, and that this tendency is at least as strong as with human dialog partners. Similarly, Parent and Eskenazi (2010) showed that users of the Let’s Go (Raux et al., 2005) spoken dialog system quickly entrain to its lexical choices. 3 We thank an anonymous reviewer for prompting this discussion. 4 This definition conflates studies of priming, alignment, convergence and accommodation. As with measures of dialog similarity, dialog entrainment has been found to be related to satisfaction and success in task oriented dialogs. For example, Reitter and Moore (2007) found that lexical and syntactic repetition predicted task success in the MapTask corpus. Similarly, Ward and Litman (2007) found that lexical and acoustic-prosodic entrainment are correlated with task success in the Itspoke dialog system. Interestingly, in that work entrainment was more strongly correlated with task success than a measure of dialog cohesion similar to the one used in the current paper. This raises the question of whether such a measure of dialog entrainment might also be a better predictor of motivation than the current measure of cohesion. We hope in future work to further"
W11-2024,A00-2028,1,0.826067,"the likelihood of subsequent higher or lower ranked states, relative to accurate classification. Our tutoring system results illustrate the case where user state misclassification increases the likelihood of negative performance trajectories as compared to accurate classification. 1 Introduction Spoken dialogue systems research has shown that natural language processing errors can negatively impact global system performance. For example, automatic speech recognition errors have been shown to negatively correlate with user satisfaction surveys taken after the system interaction is over (e.g., (Walker et al., 2000a; Pon-Barry et al., 2004)). Automatic user state classification errors have also been shown to negatively impact global performance in spoken dialogue systems (e.g., (PonBarry et al., 2006)). For example, in our prior work with an uncertainty-adaptive spoken dialogue computer tutoring system, we found that recognizing and adapting to the user’s state of uncertainty, over and above his/her state of correctness, significantly improved global learning over all users (as measured by tests taken before and after the system interaction). However, this was only true when the user uncertainty was man"
W11-2036,P07-1056,0,0.0111763,"of automation and the presentation of dialogue content commonly vary between versions. For example, Raux et al (2006) changed dialogue strategies for their Let’s Go bus information system after real-world testing. The second research direction involves proposing methods for domain adaptation. Margolis et al. (2010) observed similar poor performance when porting their dialogue act classifier between three corpora: Switchboard, the Meeting Recorder Dialog Act corpus, and a machine-translated version of the Spanish Callhome corpus. They report promising results through varying their feature set. Blitzer et al. (2007) also observed poor performance and the need for adaptation when porting product review sentiment classifiers. They used four review corpora from Amazon (books, DVDs, electronics, and small appliances), which yielded 12 cross-domain training/testing pairs. Their algorithmic adaptation methods showed promising results. Our work is in the first direction, as we also empirically analyze the impact of differences in training and testing corpora to demonstrate the need for adaptation methods. However, our work differs from domain adaptation, as the corpora in this experiment all come from one intel"
W11-2036,W10-2607,0,0.0132977,"models for analyzing users’ interactions with later versions of the system in new ways, e.g. detecting users’ affect enables the system to respond more appropriately. However, this training data does not always accurately reflect the current version of the system. In particular, differences in the levels of automation and the presentation of dialogue content commonly vary between versions. For example, Raux et al (2006) changed dialogue strategies for their Let’s Go bus information system after real-world testing. The second research direction involves proposing methods for domain adaptation. Margolis et al. (2010) observed similar poor performance when porting their dialogue act classifier between three corpora: Switchboard, the Meeting Recorder Dialog Act corpus, and a machine-translated version of the Spanish Callhome corpus. They report promising results through varying their feature set. Blitzer et al. (2007) also observed poor performance and the need for adaptation when porting product review sentiment classifiers. They used four review corpora from Amazon (books, DVDs, electronics, and small appliances), which yielded 12 cross-domain training/testing pairs. Their algorithmic adaptation methods s"
W11-2036,C08-1123,0,0.0296567,"Litman Department of Computer Science Learning Research & Development Ctr. University of Pittsburgh Pittsburgh, PA 15260 litman@cs.pitt.edu Joanna Drummond Department of Computer Science University of Pittsburgh Pittsburgh, PA 15260 jmd73@cs.pitt.edu Abstract Previous work in dialogue systems with regards to analyzing the impact of using differing training data has primarily been in the domain adaptation field, and has focused on two areas. First, previous work empirically analyzed the need for domain adaptation, i.e. methods for porting existing classifiers to unrelated domains. For example, Webb and Liu (2008) developed a cue-phrase-based dialogue act classifier using the Switchboard corpus, and tested on call center data. While this performed reasonably, training on the call center corpus and testing on Switchboard performed poorly. Many dialogue system developers use data gathered from previous versions of the dialogue system to build models which enable the system to detect and respond to users’ affect. Previous work in the dialogue systems community for domain adaptation has shown that large differences between versions of dialogue systems affect performance of ported models. Thus, we wish to i"
W12-1630,W09-3933,0,0.0974208,"cess and user satisfaction (Section 5), and also yields a reduction both in uncertainty levels (Section 4) and in the likelihood of continued disengagement (Section 6). 2 Related Work User disengagement is highly undesirable because of its potential to increase dissatisfaction and task failure, and there is a growing awareness of its potential to negatively impact commercial applications; thus there has been substantial prior work focused on detecting disengagement (along with the closely related states of boredom and lack of interest) (e.g., (Schuller et al., 2010; Wang and Hirschberg, 2011; Bohus and Horvitz, 2009)). To date, however, only a few disengagement-adaptive systems have been evaluated, and within the tutoring domain these have focused on only one disengagement behavior: gaming. For example, responding to gaming with supplementary material reduced gaming and improved task success for users who most frequently gamed (Baker et al., 2006), while adding 218 progress reports and productive learning tips at the end of problems (i.e., without specifically targeting gaming instances) increased task success, engagement, and user satisfaction (Arroyo et al., 2007). Our research builds on this work but i"
W12-1630,W11-2019,0,0.060834,"Missing"
W12-1630,N12-1010,1,0.845059,"ection and natural language understanding (Forbes-Riley and Litman, 2011b), as well as in a fully automated system version (Forbes-Riley and Litman, 2011a). We are now taking the next step by incorporating adaptation to a second user affective state: user disengagement. We target user disengagement for two reasons: first, our prior manual annotation showed disengagement and uncertainty to be the most frequent user affective states that occur in our system, and second, our prior analyses show that the occurrence of disengagement is negatively correlated with task success and user satisfaction (Forbes-Riley and Litman, 2012).2 Thus, we hypothesized that providing appropriate system responses to both affective states could have multiple benefits: 1) reduce the frequency of one or both states, 2) “break” the nega2 Redesigning a system in light of correlational analyses can improve performance (Rotaru and Litman, 2009). 217 Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 217–226, c Seoul, South Korea, 5-6 July 2012. 2012 Association for Computational Linguistics tive correlations with performance, and 3) yield further improvements in global and local pe"
W12-1630,W09-3927,1,0.838789,"two reasons: first, our prior manual annotation showed disengagement and uncertainty to be the most frequent user affective states that occur in our system, and second, our prior analyses show that the occurrence of disengagement is negatively correlated with task success and user satisfaction (Forbes-Riley and Litman, 2012).2 Thus, we hypothesized that providing appropriate system responses to both affective states could have multiple benefits: 1) reduce the frequency of one or both states, 2) “break” the nega2 Redesigning a system in light of correlational analyses can improve performance (Rotaru and Litman, 2009). 217 Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 217–226, c Seoul, South Korea, 5-6 July 2012. 2012 Association for Computational Linguistics tive correlations with performance, and 3) yield further improvements in global and local performance. In this paper, we test these hypotheses, presenting the results of a controlled experiment evaluating a wizard-of-oz version of our spoken dialogue computer tutor that adapts to both user uncertainty and user disengagement (Section 3). Although we address these states within the tutori"
W12-1630,W11-2018,0,0.285769,"2012 Association for Computational Linguistics tive correlations with performance, and 3) yield further improvements in global and local performance. In this paper, we test these hypotheses, presenting the results of a controlled experiment evaluating a wizard-of-oz version of our spoken dialogue computer tutor that adapts to both user uncertainty and user disengagement (Section 3). Although we address these states within the tutoring domain, speech researchers from other domains and applications are also focusing on detecting and adapting to user disengagement (e.g., (Schuller et al., 2010; Wang and Hirschberg, 2011)) and uncertainty (e.g. (Pon-Barry and Shieber, 2011; Paek and Ju, 2008)) to improve system performance. Our results should be of interest not only to these researchers but also more generally to any researchers working towards comprehensive affect-adaptive spoken dialogue systems. In particular, our results show that iteratively adding new affect adaptations to an existing affect-adaptive system can yield performance improvements. We find no increase (but also no decrease) in task success or user satisfaction, but we do find an increase in motivation for users who most frequently received the"
W12-2020,de-marneffe-etal-2006-generating,0,0.0170037,"Missing"
W12-2020,C00-1072,0,0.0555024,"pared with respect to specific reviewing dimensions using a list of topic words that are automatically computed in real-time. Extracting topic words of peer reviews for comparison purposes is different from most traditional topic-word extraction tasks that are commonly involved in text summarization. In traditional text summarization, the informativeness measurement is designed to extract the common themes, while in our case of comparison, instructors are more concerned with the uniqueness of each target set of peer reviews compared to the others. Thus a topic-signature acquisition algorithm (Lin and Hovy, 2000), which extracts topic words through comparing the vocabulary distribution of a target corpus against that of a generic background corpus using a statistic metric, suits our application better than other approaches, such as probabilistic graphical models (e.g. LDA) and frequency based methods. Therefore, RevExplore considers topic signatures as the topic words for a group of reviews, using all peer 177 reviews as the background corpus.4 Again, to minimize the impact of the domain content of the relevant papers, we apply topic-masking which replaces all domain words5 with “ddd” before computing"
W12-2020,P08-1094,0,0.0195226,"s this weight information to order the topic words as a list, and visualizes the weight as the font size and foreground color of the relevant topic word. These lists are placed in two rows regarding their group membership dimension by dimension. For each dimension, the corresponding lists of both rows are aligned vertically with the same background color to indicate that dimension (e.g. Topic-list detail view of Figure 2). To further facilitate the comparison within a dimension, RevExplore highlights the topic words that are unique to one group with a darker background color. 4 We use TopicS (Nenkova and Louis, 2008) provided by Annie Louis. 5 learned from all student papers against 5000 documents from the English Gigaword Corpus using TopicS. If the user cannot interpret the topic that an extracted word might imply, the user can click on the word to read the relevant original reviews, with that word highlighted in red (e.g. Original reviews pane of Figure 2). 5 Analysis Example Figure 2 shows how RevExplore is used to discover the difference between strong and weak students with respect to their writing performance on “logic” in the history peer-review assignment introduced in Section 4. First we group s"
W12-2020,W09-3605,0,0.0574653,"Missing"
W12-2020,P11-2088,1,0.770728,"er improve student learning, directly or indirectly. Empirical studies of textual review comments based on manual coding have discovered that certain review features (e.g., whether the solution to a problem is explicitly stated in a comment) can predict both whether the problem will be understand and the feedback implemented (Nelson and Schunn, 2009). Our previous studies used machine learning and NLP techniques to automatically identify the presence of such useful features in review comments (Xiong et al., 2010); similar techniques have also been used to determine review comment helpfulness (Xiong and Litman, 2011; Cho, 2008). With respect to paper analysis, S´andor and Vorndran (2009) used NLP to highlight key sentences, in order to focus reviewer attention on important paper aspects. Finally, Giannoukos et al. (2010) focused on peer matching based on students’ profile information to maximize learning outcomes, while Crespo Garcia and Pardo (2010) explored the use of document clustering to adaptively guide the assignment of papers to peers. In contrast to the prior work above, the research presented here is primarily motivated by the needs of instructors, instead of the needs of students. In particula"
W14-1812,E12-2021,0,0.0320317,"back prediction accuracy is not the only reason we advocate for the sentence level. We envision that the sentence level is the necessary lower bound that a peer review system needs to handle new advanced functionalities such as envisioned in Figure 1. Being able to highlight featured text in a peer comment is a useful visualization function that should help peer reviewers learn from live examples, and may also help student authors quickly notice the important point of the comment. Sentence and phrase level annotation is made easy with the availability of many text annotation toolkits; BRAT10 (Stenetorp et al., 2012) is an example. From our work, marking text spans by selecting and clicking requires a minimal additional effort from annotators and does not cause more cognitive workload. Moreover, we hypothesize that through highlighting the text, an annotator has to reason about why she would choose a label, which in turn makes the annotation process more reliable. We plan to test whether annotation performance does indeed improve in future work. 9 back type assessment component for reviews of argument diagrams rather than papers. We have demonstrated that using sentence prediction outputs to label the cor"
W14-1812,H05-1044,0,0.00810839,"Missing"
W14-1812,N09-1030,0,0.0597946,"Missing"
W14-1812,W10-0207,0,0.0309521,"Missing"
W14-1818,E06-1021,0,0.0881093,"Missing"
W14-1818,W10-1004,0,0.0167303,"aft essays from an undergraduate writing class. 1 Introduction Rewriting is considered to be an important process during writing. However, conducting successful rewriting is not an easy task, especially for novice writers. Instructors work hard on providing suggestions for rewriting (Wells et al., 2013), but usually such advice is quite general. We need to understand the changes between revisions better to provide more specific and helpful advice. There has already been work on detecting corrections in sentence revisions (Xue and Hwa, 2014; Swanson and Yamangil, 2012; Heilman and Smith, 2010; Rozovskaya and Roth, 2010). However, these works mainly focus on detecting changes at the level of words or phrases. According to Faigley’s definition of revision change (Faigley and Witte, 1981), these works could help the identification of Surface Changes (changes that do not add or remove information to the original text). However, Text Changes (changes that add or remove information) will be more difficult 2 Related work Hashemi and Schunn (2014) presented a tool to help professors summarize students’ changes across papers before and after peer review. They first split the original documents into sentences and then"
W14-1818,W11-1603,0,0.015854,"although their approach might work for identifying differences within one sentence, it makes mistakes when sentence analysis is the primary concern. Our work avoids the above problem by detecting differences at the sentence level. Sentence alignment is the first step of our method; further inferences about revision changes are then based on the alignments generated. We borrow ideas from the research on sentence alignment for monolingual corpora. Existing research usually focuses on the alignment from the text to its summarization or its simplification (Jing, 2002; Barzilay and Elhadad, 2003; Bott and Saggion, 2011). Barzilay and Elhadad (2003) treat sentence alignment as a classification task. The paragraphs are clustered into groups, and a binary classifier is trained to decide whether two sentences should be aligned or not. Nelken (2006) further improves the performance by using TF*IDF score instead of word overlap and also utilizing global optimization to take sentence order information into consideration. We argue that summarization could be considered as a special form of revision and adapted Nelken’s approach to our approach. Edit sequences are then inferred based on the results of sentence alignm"
W14-1818,E12-1036,0,0.130544,"then be merged. Related work to our method is sentence clustering (Shen et al., 2011; Wang et al., 2009). While sentence clustering is trying to find and cluster sentences similar to each other, our work is to find a cluster of sentences in one document that is similar to one sentence in the other document after merging. 3 3.1 Explanation Adding a word or phrase Omitting a word or phrase exchange words with synonyms rearrange of words or phrases one segment divided into two combine two segments into one Table 1: Code Definition by L.Faigley and S.Witte definition is similar to Bronner’s work (Bronner and Monz, 2012). We choose this definition because these 4 primitives only correspond to one sentence at a time. Add, Delete, Modify indicates that the writer has added/deleted/modified a sentence. Keep means the original sentence is not modified. We believe Permutation, Distribution and Consolidation as defined by Faigley could be described with these four primitives, which could be recognized in the later merge step. 3.2 Data and annotation The corpus we choose consists of paired first and final drafts of short papers written by undergraduates in a course “Social Implications of Computing Technology”. Stud"
W14-1818,N12-1037,0,0.061099,"gh accuracy in an evaluation using first and final draft essays from an undergraduate writing class. 1 Introduction Rewriting is considered to be an important process during writing. However, conducting successful rewriting is not an easy task, especially for novice writers. Instructors work hard on providing suggestions for rewriting (Wells et al., 2013), but usually such advice is quite general. We need to understand the changes between revisions better to provide more specific and helpful advice. There has already been work on detecting corrections in sentence revisions (Xue and Hwa, 2014; Swanson and Yamangil, 2012; Heilman and Smith, 2010; Rozovskaya and Roth, 2010). However, these works mainly focus on detecting changes at the level of words or phrases. According to Faigley’s definition of revision change (Faigley and Witte, 1981), these works could help the identification of Surface Changes (changes that do not add or remove information to the original text). However, Text Changes (changes that add or remove information) will be more difficult 2 Related work Hashemi and Schunn (2014) presented a tool to help professors summarize students’ changes across papers before and after peer review. They first"
W14-1818,P09-2075,0,0.0229855,"is trained to decide whether two sentences should be aligned or not. Nelken (2006) further improves the performance by using TF*IDF score instead of word overlap and also utilizing global optimization to take sentence order information into consideration. We argue that summarization could be considered as a special form of revision and adapted Nelken’s approach to our approach. Edit sequences are then inferred based on the results of sentence alignment. Fragments of edits that come from the same purpose will then be merged. Related work to our method is sentence clustering (Shen et al., 2011; Wang et al., 2009). While sentence clustering is trying to find and cluster sentences similar to each other, our work is to find a cluster of sentences in one document that is similar to one sentence in the other document after merging. 3 3.1 Explanation Adding a word or phrase Omitting a word or phrase exchange words with synonyms rearrange of words or phrases one segment divided into two combine two segments into one Table 1: Code Definition by L.Faigley and S.Witte definition is similar to Bronner’s work (Bronner and Monz, 2012). We choose this definition because these 4 primitives only correspond to one sen"
W14-1818,P14-2098,0,0.111476,"d approach shows high accuracy in an evaluation using first and final draft essays from an undergraduate writing class. 1 Introduction Rewriting is considered to be an important process during writing. However, conducting successful rewriting is not an easy task, especially for novice writers. Instructors work hard on providing suggestions for rewriting (Wells et al., 2013), but usually such advice is quite general. We need to understand the changes between revisions better to provide more specific and helpful advice. There has already been work on detecting corrections in sentence revisions (Xue and Hwa, 2014; Swanson and Yamangil, 2012; Heilman and Smith, 2010; Rozovskaya and Roth, 2010). However, these works mainly focus on detecting changes at the level of words or phrases. According to Faigley’s definition of revision change (Faigley and Witte, 1981), these works could help the identification of Surface Changes (changes that do not add or remove information to the original text). However, Text Changes (changes that add or remove information) will be more difficult 2 Related work Hashemi and Schunn (2014) presented a tool to help professors summarize students’ changes across papers before and a"
W14-1818,N10-1145,0,0.0181629,"using first and final draft essays from an undergraduate writing class. 1 Introduction Rewriting is considered to be an important process during writing. However, conducting successful rewriting is not an easy task, especially for novice writers. Instructors work hard on providing suggestions for rewriting (Wells et al., 2013), but usually such advice is quite general. We need to understand the changes between revisions better to provide more specific and helpful advice. There has already been work on detecting corrections in sentence revisions (Xue and Hwa, 2014; Swanson and Yamangil, 2012; Heilman and Smith, 2010; Rozovskaya and Roth, 2010). However, these works mainly focus on detecting changes at the level of words or phrases. According to Faigley’s definition of revision change (Faigley and Witte, 1981), these works could help the identification of Surface Changes (changes that do not add or remove information to the original text). However, Text Changes (changes that add or remove information) will be more difficult 2 Related work Hashemi and Schunn (2014) presented a tool to help professors summarize students’ changes across papers before and after peer review. They first split the original docum"
W14-2104,P11-1099,0,0.114235,"Missing"
W14-2104,P12-1007,0,0.0200964,"Missing"
W14-2104,P11-1100,0,0.021898,"Missing"
W14-4324,W09-3933,0,0.0124405,"tive States Diane Litman Computer Science Dept. & LRDC University of Pittsburgh Pittsburgh, PA 15260 USA dlitman@pitt.edu Kate Forbes-Riley Learning Research & Development Center University of Pittsburgh Pittsburgh, PA 15260 USA forbesk@pitt.edu Abstract most frequent affective states in prior studies with our system and their presence was negatively correlated with task success2 (Forbes-Riley and Litman, 2011; Forbes-Riley and Litman, 2012). The detection of these and similar states is also of interest to the larger speech and language processing communities, e.g. (Wang and Hirschberg, 2011; Bohus and Horvitz, 2009; Pon-Barry and Shieber, 2011). Our results suggest that while adapting to affect increases task success compared to not adapting at all, the utility of our current methods varies with user gender. Also, we find no difference between adapting to one or two states. We present an evaluation of a spoken dialogue system that detects and adapts to user disengagement and uncertainty in real-time. We compare this version of our system to a version that adapts to only user disengagement, and to a version that ignores user disengagement and uncertainty entirely. We find a significant increase in task s"
W14-4324,W11-0609,0,0.0309167,"Missing"
W14-4324,N12-1010,1,0.852154,"rectness of user answers. This version of the system thus ignored any automatically detected user disengagement or uncertainty. In particular, after each correct answer, ITSPOKE provided positive feedback then moved on to the next topic. After incorrect answers, ITSPOKE instead provided negative hoc correlational (rather than causal) study, using data from an earlier fully-automated version of the uncertainty-adaptive system. Regressions demonstrated that using both automatically labeled disengagement and uncertainty to predict task success significantly outperformed using only disengagement (Forbes-Riley et al., 2012). However, if manual labels were instead used, only disengagement was predictive of learning, and adding uncertainty didn’t help. This suggests that detecting multiple affective states might compensate for the noise that is introduced in a fully-automated system. In this paper we further investigate this hypothesis, by evaluating the utility of adapting to zero, one, or two affective states in a controlled experiment involving fully-automated systems. 2.2 Gender Effects in Dialogue Differences in dialogue structure have been found between male and female students talking to a human tutor (Boye"
W14-4324,E06-1045,0,0.0319222,"nt involving fully-automated systems. 2.2 Gender Effects in Dialogue Differences in dialogue structure have been found between male and female students talking to a human tutor (Boyer et al., 2007). Studies have also shown gender differences in conversational entrainment patterns, for acoustic-prosodic features in human-human dialogues (Levitan et al., 2012) and articles in movie conversations (DanescuNiculescu-Mizil and Lee, 2011). For dialogue systems involving embodied conversational agents, gender effects have been found for facial displays, with females preferring more expressive agents (Foster and Oberlander, 2006). When used for tutoring, females report more positive affect when a learning companion is used, while males are more negative (Woolf et al., 2010). In our own prior work, we compared two uncertainty-adaptive and one non-adaptive versions of a wizarded dialogue system. Our results demonstrated that only one method of adapting to user uncertainty increased task success, and only for female users (Forbes-Riley and Litman, 2009). In this paper we extend this line of research, by adding an affective dialogue system that adapts to two rather than just one user state to our evaluation, and by moving"
W14-4324,N12-1002,0,0.018785,"g multiple affective states might compensate for the noise that is introduced in a fully-automated system. In this paper we further investigate this hypothesis, by evaluating the utility of adapting to zero, one, or two affective states in a controlled experiment involving fully-automated systems. 2.2 Gender Effects in Dialogue Differences in dialogue structure have been found between male and female students talking to a human tutor (Boyer et al., 2007). Studies have also shown gender differences in conversational entrainment patterns, for acoustic-prosodic features in human-human dialogues (Levitan et al., 2012) and articles in movie conversations (DanescuNiculescu-Mizil and Lee, 2011). For dialogue systems involving embodied conversational agents, gender effects have been found for facial displays, with females preferring more expressive agents (Foster and Oberlander, 2006). When used for tutoring, females report more positive affect when a learning companion is used, while males are more negative (Woolf et al., 2010). In our own prior work, we compared two uncertainty-adaptive and one non-adaptive versions of a wizarded dialogue system. Our results demonstrated that only one method of adapting to u"
W14-4324,W11-2018,0,\N,Missing
W14-4324,W12-1630,1,\N,Missing
W15-0503,W14-2107,0,0.175868,"Missing"
W15-0503,N10-1122,0,0.0366959,"argument words learned using a topic model. Role-based word separation in texts have been studied in a wide variety of contexts: opinion and topic word separation in opinion mining (see (Liu, 2012) for a survey), domain and review word separation for review visualization (Xiong and Litman, 2013), domain concept word tagging in tutorial dialogue systems (Litman et al., 2009), and dialog act cues for dialog act tagging (Samuel et al., 1998). Post-processing LDA (Blei et al., 2003) output was studied to identify topics of visual words (Louis and Nenkova, 2013) and representative words of topics (Brody and Elhadad, 2010; Funatsu et al., 2014). Our work is the first of its kind to use topic models to extract argument and domain words from argumentative texts. Our technique has a similarity with (Louis and Nenkova, 2013) in that we use seed words to guide the separation. 6 Conclusions and Future Work We have shown that our novel method for modeling argumentative content and argument topic in academic writings also applies to argument mining in persuasive essays, with our results outperforming a baseline model from a prior study of this genre. Our contributions are 2-fold. First, our proposed features are shown"
W15-0503,P03-1054,0,0.0213329,"bigrams, and numbers of argument and domain words.5 Our proposed model is compact with 956 original features compared to 5132 of the baseline6 . 4 Experimental Results 4.1 Proposed vs. baseline models Our first experiment replicates what was conducted in (Stab and Gurevych, 2014b). We perform 10fold cross validation; in each run we train models using LibLINEAR (Fan et al., 2008) algorithm with top 100 features returned by the InfoGain feature selection algorithm performed in the training folds. We use LightSIDE (lightsidelabs.com) to extract n-grams and production rules, the Stanford parser (Klein and Manning, 2003) to parse the texts, and Weka (Hall et al., 2009) to conduct the machine learning experiments. Table 3 (left) shows the performances of three models. We note that there are notable performance disparities between BaseI (our implementation §3.1), and BaseR (reported performance of the model by 5 A model based on seed words without expansion to argument words yields significantly worse performance than the baseline. This shows the necessity of our proposed topic model. 6 N-grams and production rules of less than 3 occurrences were removed to improve baseline performance. 25 #features Accuracy Ka"
W15-0503,N12-1003,0,0.313072,"uctures compared to production rules. To further make the features topic-independent, we keep only dependency pairs that do not include domain words. 3.2.1 Post-processing a topic model to extract argument and domain words We define argument words as those playing a role of argument indicators and commonly used in different argument topics, e.g. ‘reason’, ‘opinion’, ‘think’. In contrast, domain words are specific terminologies commonly used within the topic, e.g. ‘art’, ‘education’. Our notions of argument and domain languages share a similarity with the idea of shell language and content in (Madnani et al., 2012) in that we aim to model the lexical signals of argumentative content. However while Madnani et al. (2012) emphasized the boundaries between argument shell and content, we do not require such a physical separation between the two aspects of an argument. Instead we emphasize more the lexical signals themselves and allow argument words to occur in the ar3 gument content. For example, the major claim in Figure 1 has two argument words ‘should’ and ‘instead’ which makes the statement controversial. To learn argument and domain words, we run the LDA (Blei et al., 2003) algorithm4 and postprocess th"
W15-0503,W14-2105,0,0.270307,"w that replacing n-grams and syntactic rules with features and constraints using extracted argument and domain words significantly improves argument mining performance for persuasive essays. 1 Introduction Argument mining in text involves automatically identifying argument components as well as argumentative relations between components. Argument mining has been studied in a variety of contexts including essay assessment and feedback (Burstein et al., 2003; Stab and Gurevych, 2014b), visualization and search in legal text (Moens et al., 2007), and opinion mining in online reviews and debates (Park and Cardie, 2014; Boltuži´c and Šnajder, 2014). Problem formulations of argument mining have ranged from argument detection (e.g. does a sentence contain argumentative content?) to argument component (e.g. claims vs. premise) and/or relation (e.g. support vs. attack) classification. Due to the loosely-organized nature of many types of texts, associated argument mining studies have typically used generic linguistic features, e.g. ngrams and syntactic rules, and counted on feature selection to reduce large and sparse feature spaces. For example, in texts such as student essays and product reviews there are opti"
W15-0503,P09-1077,0,0.033413,"in the first/last paragraph, the first/last sentence of a paragraph. Lexical features: all n-grams of length 1-3 extracted from AC’s including preceding text which is not covered by other AC’s in sentence, verbs like ‘believe’, adverbs like ‘also’, and whether the AC has a modal verb. Syntactic features: #sub-clauses and depth of parse tree of the covering sentence, tense of main verb and production rules (VP → VBG NP) from parse tree of the AC. Discourse markers: discourse connectives of 3 relations: comparison, contingency, and expansion but not temporal2 extracted by addDiscourse program (Pitler et al., 2009). 2 Authors of (Stab and Gurevych, 2014b) manually collected 55 PDTB markers after removing those that do not indicate argumentative discourse, e.g. markers of temporal relations. First person pronouns: whether each of I, me, my, mine, and myself is present in the sentence. Contextual features: #tokens, #punctuations, #sub-clauses, and presence of modal verb in preceding and following sentences. 3.2 Proposed model Our proposed model is based on the idea of separating argument and domain words (Nguyen and Litman, submitted) to better model argumentative content and argument topics in text. It i"
W15-0503,P98-2188,0,0.192484,"Moens, 2002), sentiment clue and speech event (Park and Cardie, 2014). However, the major feature sets were still generic n-grams. We 26 propose to replace generic n-grams with argument words learned using a topic model. Role-based word separation in texts have been studied in a wide variety of contexts: opinion and topic word separation in opinion mining (see (Liu, 2012) for a survey), domain and review word separation for review visualization (Xiong and Litman, 2013), domain concept word tagging in tutorial dialogue systems (Litman et al., 2009), and dialog act cues for dialog act tagging (Samuel et al., 1998). Post-processing LDA (Blei et al., 2003) output was studied to identify topics of visual words (Louis and Nenkova, 2013) and representative words of topics (Brody and Elhadad, 2010; Funatsu et al., 2014). Our work is the first of its kind to use topic models to extract argument and domain words from argumentative texts. Our technique has a similarity with (Louis and Nenkova, 2013) in that we use seed words to guide the separation. 6 Conclusions and Future Work We have shown that our novel method for modeling argumentative content and argument topic in academic writings also applies to argumen"
W15-0503,C14-1142,0,0.420718,"aper we investigate the generality of this approach, by applying our methodology to a new corpus of persuasive essays. Our experiments show that replacing n-grams and syntactic rules with features and constraints using extracted argument and domain words significantly improves argument mining performance for persuasive essays. 1 Introduction Argument mining in text involves automatically identifying argument components as well as argumentative relations between components. Argument mining has been studied in a variety of contexts including essay assessment and feedback (Burstein et al., 2003; Stab and Gurevych, 2014b), visualization and search in legal text (Moens et al., 2007), and opinion mining in online reviews and debates (Park and Cardie, 2014; Boltuži´c and Šnajder, 2014). Problem formulations of argument mining have ranged from argument detection (e.g. does a sentence contain argumentative content?) to argument component (e.g. claims vs. premise) and/or relation (e.g. support vs. attack) classification. Due to the loosely-organized nature of many types of texts, associated argument mining studies have typically used generic linguistic features, e.g. ngrams and syntactic rules, and counted on feat"
W15-0503,D14-1006,0,0.387972,"aper we investigate the generality of this approach, by applying our methodology to a new corpus of persuasive essays. Our experiments show that replacing n-grams and syntactic rules with features and constraints using extracted argument and domain words significantly improves argument mining performance for persuasive essays. 1 Introduction Argument mining in text involves automatically identifying argument components as well as argumentative relations between components. Argument mining has been studied in a variety of contexts including essay assessment and feedback (Burstein et al., 2003; Stab and Gurevych, 2014b), visualization and search in legal text (Moens et al., 2007), and opinion mining in online reviews and debates (Park and Cardie, 2014; Boltuži´c and Šnajder, 2014). Problem formulations of argument mining have ranged from argument detection (e.g. does a sentence contain argumentative content?) to argument component (e.g. claims vs. premise) and/or relation (e.g. support vs. attack) classification. Due to the loosely-organized nature of many types of texts, associated argument mining studies have typically used generic linguistic features, e.g. ngrams and syntactic rules, and counted on feat"
W15-0503,J02-4002,0,0.225168,"e.g. claims vs. premise) and/or relation (e.g. support vs. attack) classification. Due to the loosely-organized nature of many types of texts, associated argument mining studies have typically used generic linguistic features, e.g. ngrams and syntactic rules, and counted on feature selection to reduce large and sparse feature spaces. For example, in texts such as student essays and product reviews there are optional titles but typically no section headings, and claims are substantiated by personal experience rather than cited sources. Thus, specialized features as used in scientific articles (Teufel and Moens, 2002) are not available. While this use of generic linguistic features has been effective, we propose a feature reduction method based on the semi-supervised derivation of lexical signals of argumentative and domain content. Our approach was initially developed to identify argument elements, i.e. hypothesis and findings, in academic essays (written following APA guidelines) of college students (Nguyen and Litman, submitted). In particular, we post-processed a topic model to extract argument words (lexical signals of argumentative content) and domain words (terminologies in argument topics) using se"
W15-0503,Q13-1028,0,\N,Missing
W15-0603,P05-1018,0,0.0399046,"sults with the first human rater’s scores. We chose the first human rater because we do not have the scores of the second rater for the entire dataset. We report the performance as Quadratic Weighted Kappa, which is a standard evaluation measure for essay assessment systems. We use corrected paired t-test (Bouckaert and Frank, 2004) to measure the significance of any difference in performance. We use two well-performing baselines from recent methods to evaluate organization and coherence of the essays. The first baseline (EntityGridTT) is based on the entity-grid coherence model introduced by Barzilay and Lapata (2005). This method has been used to measure the coherence of student essays (Burstein et al., 2010). It includes transition probabilities and type/token ratios for each syntactic role as features. We perform a set of experiments using different configurations for the entitygrid baseline, and we find that the best model is an entity-grid model with history=2, salience=1, syntax=on and type/token ratios. We therefore use this best configuration in all experiments. It should be noted that this works to the advantage of the entitygrid baseline since we do not have parameter tuning for the other models."
W15-0603,W98-0303,0,0.62769,"ings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications, 2015, pages 20–30, c Denver, Colorado, June 4, 2015. 2015 Association for Computational Linguistics ence (Burstein et al., 2010) that is shown in Burstein et al. (2013) to be a predictive model on a corpus of essays from grades 6-12. Lexical chaining addresses multiple aspects of coherence such as elaboration, usage of varied vocabulary, and sound organization of thoughts and ideas (Somasundaran et al., 2014). Discourse structure is used to measure the organization of argumentative writing (Cohen, 1987; Burstein et al., 1998; Burstein et al., 2003b). In previous studies, assessments of text coherence have been task-independent, which means that these models are designed to be able to evaluate the coherence of the response to any writing task. Taskindependence is often the goal for automated scoring systems, but it is also important to measure the quality of students’ organization skills when they are responding to a task-dependent prompt. One advantage of task-dependent scores is the ability to provide feedback that is better aligned with the task. One of the types of writing emphasized in the CCSS is writing in"
W15-0603,N10-1099,0,0.180656,"Missing"
W15-0603,J87-1002,0,0.313474,"df 20 Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications, 2015, pages 20–30, c Denver, Colorado, June 4, 2015. 2015 Association for Computational Linguistics ence (Burstein et al., 2010) that is shown in Burstein et al. (2013) to be a predictive model on a corpus of essays from grades 6-12. Lexical chaining addresses multiple aspects of coherence such as elaboration, usage of varied vocabulary, and sound organization of thoughts and ideas (Somasundaran et al., 2014). Discourse structure is used to measure the organization of argumentative writing (Cohen, 1987; Burstein et al., 1998; Burstein et al., 2003b). In previous studies, assessments of text coherence have been task-independent, which means that these models are designed to be able to evaluate the coherence of the response to any writing task. Taskindependence is often the goal for automated scoring systems, but it is also important to measure the quality of students’ organization skills when they are responding to a task-dependent prompt. One advantage of task-dependent scores is the ability to provide feedback that is better aligned with the task. One of the types of writing emphasized in"
W15-0603,J95-2003,0,0.541872,"elements are adherence to the essay topic, elaboration, usage of varied vocabulary, and sound organization of thoughts and ideas. In Crossley and McNamara (2011) the elements are effective lead, clear purpose, clear plan, topic sentences, paragraph transitions, organization, unity, perspective, conviction, grammar, syntax, and mechanics. Many computational methods are used to measure such elements of discourse coherence. Vector-based similarity methods measure lexical relatedness between text segments (Foltz et al., 1998) or between discourse segments (Higgins et al., 2004). Centering theory (Grosz et al., 1995) addresses local coherence (Miltsakaki and Kukich, 2000). Entity-based essay representation along with type/token ratios for each syntactic role is another method to evaluate coher2 See, e.g., Grades 4 and 5 Expanded rubric for analytic and narrative writing retrieved from http://www.parcconline.org/sites/parcc/files/Grade 4-5 ELA Expanded Rubric FOR ANALYTIC AND NARRATIVE WRITING 0.pdf 20 Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications, 2015, pages 20–30, c Denver, Colorado, June 4, 2015. 2015 Association for Computational Linguistics ence (Bur"
W15-0603,N04-1024,0,0.0413222,"Somasundaran et al. (2014) the coherence elements are adherence to the essay topic, elaboration, usage of varied vocabulary, and sound organization of thoughts and ideas. In Crossley and McNamara (2011) the elements are effective lead, clear purpose, clear plan, topic sentences, paragraph transitions, organization, unity, perspective, conviction, grammar, syntax, and mechanics. Many computational methods are used to measure such elements of discourse coherence. Vector-based similarity methods measure lexical relatedness between text segments (Foltz et al., 1998) or between discourse segments (Higgins et al., 2004). Centering theory (Grosz et al., 1995) addresses local coherence (Miltsakaki and Kukich, 2000). Entity-based essay representation along with type/token ratios for each syntactic role is another method to evaluate coher2 See, e.g., Grades 4 and 5 Expanded rubric for analytic and narrative writing retrieved from http://www.parcconline.org/sites/parcc/files/Grade 4-5 ELA Expanded Rubric FOR ANALYTIC AND NARRATIVE WRITING 0.pdf 20 Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications, 2015, pages 20–30, c Denver, Colorado, June 4, 2015. 2015 Association"
W15-0603,J91-1002,0,0.603745,"t al., 2010). It includes transition probabilities and type/token ratios for each syntactic role as features. We perform a set of experiments using different configurations for the entitygrid baseline, and we find that the best model is an entity-grid model with history=2, salience=1, syntax=on and type/token ratios. We therefore use this best configuration in all experiments. It should be noted that this works to the advantage of the entitygrid baseline since we do not have parameter tuning for the other models. The second baseline (LEX1) is a set of features extracted from Lexical Chaining (Morris and Hirst, 1991). We use Galley and McKeown (2003) lexical chaining and extract the first set of features (LEX1) introduced in Somasundaran et al. (2014). We do not implement the second set because we do not have the annotation or the tagger to tag discourse cues. 1 2 3 4 5 6 7 Model EntityGridTT LEX1 EntityGridTT+LEX1 Rubric-based EntityGridTT+Rubric-based LEX1+Rubric-based EntityGridTT+LEX1 +Rubric-based (5–6) 0.42 0.45 0.46 (1) 0.51 (1,2,3) 0.49 (1,2,3) 0.51 (1,2,3) 0.50 (1,2,3) (6–8) 0.49 0.53 (1) 0.54 (1) 0.51 0.53 (1) 0.55 (1) 0.56 (1) Table 5: Performance of our rubric-based model compared to the basel"
W15-0603,P09-2004,0,0.0123364,"the UN-led intervention referenced in the source text): These are two binary features. It measures if all the sentences in the essay are labeled only with “before” or only with “after” topics. A weak essay might, for example, discuss at length the condition of Kenya before the intervention (i.e., address several “before” topics) without referencing the result of the intervention (i.e., “after” topics). Discourse markers: Four features that count the discourse markers from each of the four groups: contingency, expansion, comparison, and temporal, extracted by “AddDiscourse” connective tagger (Pitler and Nenkova, 2009). Eight additional features represent count and percentage of discourse markers from each of the four groups that appear in sentences that are labeled with a topic. Average Chain Size: Average number of nodes in chains. Longer chains indicate more development on each topic. Number and percentage of chains with variety: A chain on a topic has variety if it discusses both aspects (‘before’ and ‘after’) of that topic. (5) Topic ordering and patterns: It is not just the number of topics and the amount of development on each topic that is important. More important is how students organized these to"
W15-0603,C14-1090,0,0.485641,"6TRAITSWRITING.pdf, February 25, 2015 Core State Standards (CCSS), the academic standards adopted widely in 2011 that guide K-12 education, reflect a shift in thinking about the scoring of organization in writing to consider the coherence of ideas in the text2 . The consideration of coherence as a critical aspect of organization of writing is relatively new. Notably, prior studies in natural language processing have examined the concept of discourse coherence, which is highly related to the coherence of topics in an essay, as a measure of the organization of analytic writing. For example, in Somasundaran et al. (2014) the coherence elements are adherence to the essay topic, elaboration, usage of varied vocabulary, and sound organization of thoughts and ideas. In Crossley and McNamara (2011) the elements are effective lead, clear purpose, clear plan, topic sentences, paragraph transitions, organization, unity, perspective, conviction, grammar, syntax, and mechanics. Many computational methods are used to measure such elements of discourse coherence. Vector-based similarity methods measure lexical relatedness between text segments (Foltz et al., 1998) or between discourse segments (Higgins et al., 2004). Cen"
W15-0616,C14-1160,0,0.0140275,"enberger and Gurevych (2013)). Results show that the binary classifications on Wikipedia data achieve a promising result. Classification of finer-grained categories is more difficult and the difficulty varies across different categories. In this paper we explore whether the features used in Wikipedia revision classification can be adapted to the classification of different categories of revisions in our work. We also utilize features from research on argument mining and discourse parsing (Burstein et al., 2003; Burstein and Marcu, 2003; Sporleder and Lascarides, 2008; Falakmasir et al., 2014; Braud and Denis, 2014) and evaluate revision classification both intrinsically and extrinsically. Finally, we explore end-to-end revision processing by combining automatic revision extraction and categorization via automatic classification in a pipelined manner. 3 Sentence-level revision extraction and categorization This section describes our work for sentence-level revision extraction and revision categorization. A corpus study demonstrates the use of the sentencelevel revision annotations for revision analysis. 3.1 Revision extraction As stated in the previous section, our method takes semantic information into"
W15-0616,E12-1036,0,0.188347,"tions made by instructors (Wells et al., 2013), studies modeling revisions for error correction (Xue and Hwa, 2010; Mizumoto et al., 2011) and tools aiming to help students with rewriting (Elireview, 2014; Lightside, 2014). While there is increasing interest in the improvement of writers’ rewriting skills, there is still a lack of study on the details of revisions. First, to find out what has been changed (defined as revision extraction in this paper), a typical approach is to extract and analyze revisions at the word/phrase level based on edits extracted with character-level text comparison (Bronner and Monz, 2012; Daxenberger and Gurevych, 2012). The semantic information of sentences is not considered in the character-level text comparison, which can lead to errors and loss of information in revision extraction. Second, the differentiation of different types of revisions (defined as revision categorization) is typically not fine-grained. A common categorization is a binary classification of revisions according to whether the information of the essay is changed or not (e.g. text-based vs. surface as defined by Faigley and Witte (1981)). This categorization ignores potentially important differences betw"
W15-0616,N13-1055,0,0.0205871,"uous edit segments. Sentence 1 in Draft 1 is wrongly marked as being modified to 1, 2, 3 in Draft 2 because character-level text comparison could not identify the semantic similarity between sentences. the following efforts. First, we propose that it is better to extract revisions at a level higher than the character level, and in particular, explore the sentence-level. This avoids the misalignment errors of character-level text comparisons. Finer-grained studies can still be done on the sentence-level revisions extracted, such as fluency prediction (Chae and Nenkova, 2009), error correction (Cahill et al., 2013; Xue and Hwa, 2014), statement strength identification (Tan and Lee, 2014), etc. Second, we propose a sentence-level revision schema for argumentative writing, a common form of writing in education. In the schema, categories are defined for describing an author’s revision operations and revision purposes. The revision operations can be directly decided according to the results of sentence alignment, while revision purposes can be reliably manually annotated. We also do a corpus study to demonstrate the utility of sentence-level revisions for revision analysis. Finally, we adapt features from"
W15-0616,E09-1017,0,0.0275088,"d as a set of sentences covering the contiguous edit segments. Sentence 1 in Draft 1 is wrongly marked as being modified to 1, 2, 3 in Draft 2 because character-level text comparison could not identify the semantic similarity between sentences. the following efforts. First, we propose that it is better to extract revisions at a level higher than the character level, and in particular, explore the sentence-level. This avoids the misalignment errors of character-level text comparisons. Finer-grained studies can still be done on the sentence-level revisions extracted, such as fluency prediction (Chae and Nenkova, 2009), error correction (Cahill et al., 2013; Xue and Hwa, 2014), statement strength identification (Tan and Lee, 2014), etc. Second, we propose a sentence-level revision schema for argumentative writing, a common form of writing in education. In the schema, categories are defined for describing an author’s revision operations and revision purposes. The revision operations can be directly decided according to the results of sentence alignment, while revision purposes can be reliably manually annotated. We also do a corpus study to demonstrate the utility of sentence-level revisions for revision ana"
W15-0616,C12-1044,0,0.469209,"s (Wells et al., 2013), studies modeling revisions for error correction (Xue and Hwa, 2010; Mizumoto et al., 2011) and tools aiming to help students with rewriting (Elireview, 2014; Lightside, 2014). While there is increasing interest in the improvement of writers’ rewriting skills, there is still a lack of study on the details of revisions. First, to find out what has been changed (defined as revision extraction in this paper), a typical approach is to extract and analyze revisions at the word/phrase level based on edits extracted with character-level text comparison (Bronner and Monz, 2012; Daxenberger and Gurevych, 2012). The semantic information of sentences is not considered in the character-level text comparison, which can lead to errors and loss of information in revision extraction. Second, the differentiation of different types of revisions (defined as revision categorization) is typically not fine-grained. A common categorization is a binary classification of revisions according to whether the information of the essay is changed or not (e.g. text-based vs. surface as defined by Faigley and Witte (1981)). This categorization ignores potentially important differences between revisions under the same high"
W15-0616,D13-1055,0,0.389426,"t differences between revisions under the same high-level category. For example, changing the evidence of a claim and changing the reasoning of a claim are both considered as text-based changes. Usually changing the evidence makes a paper more grounded, while changing the reasoning helps with the paper’s readability. This could indicate different levels of improvement to the original paper. Finally, for the automatic differentiation of revisions (defined as revision classification), while there are works on the classification of Wikipedia revisions (Adler et al., 2011; Bronner and Monz, 2012; Daxenberger and Gurevych, 2013), there is a lack of work on revision classification in other datasets such as student writings. It is not clear whether current features and methods can still be adapted or new features and methods are required. To address the issues above, this paper makes 133 Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications, 2015, pages 133–143, c Denver, Colorado, June 4, 2015. 2015 Association for Computational Linguistics . Figure 1: In the example, words in sentence 1 of Draft 1 are rephrased and reordered to sentence 3 of Draft 2. Sentences 1 and 2 in Dra"
W15-0616,P14-1048,0,0.0138605,"An added sentence or a deleted sentence is treated as aligned to null as in Figure 1. The aligned pairs where the sentences in the pair are not identical are extracted as revisions. For the automatic alignment of sentences, 1 We plan to also explore revision extraction at the clause level in the future. Our approach can be adapted to the clause level by segmenting the clauses first and aligning the segmented clauses after. A potential benefit is that clauses are often the basic units of discourse structures, so extracting clause revisions will allow the direct use of discourse parser outputs (Feng and Hirst, 2014; Lin et al., 2014). However, potential problems are that clauses contain less information for alignment decisions and clause segmentation is noisier. we used the algorithm in our prior work (Zhang and Litman, 2014) which considers both sentence similarity (calculated using TF*IDF score) and the global context of sentences. 3.2 Revision schema definition As shown in Figure 2, two dimensions are considered in the definition of the revision schema: the author’s behavior (revision operation) and the reason for the author’s behavior (revision purpose). Revision operations include three categories:"
W15-0616,P03-1054,0,0.0106343,"xample for a claim; sentences containing “because” are more likely to be a sentence of reasoning; a sentence generated by text-based revisions is possibly more different from the original sentence compared to a sentence generated by surface revisions. These intuitions are operationalized using several feature groups: Named entity features4 (also used in Bronner and Monz (2012)’s Wikipedia revision classification task), Discourse marker features (used by 3 Since Add and Delete operations have only one sentence in the aligned pair, the value of the empty sentence is set to 0. 4 Stanford parser (Klein and Manning, 2003) is used in named entity recognition. 139 Burstein et al. (2003) for discourse structure identification), Sentence difference features and Revision operation (similar features are used by Daxenberger and Gurevych (2013)). Language group. Different types of sentences can have different distributions in POS tags (Daxenberger and Gurevych, 2013). The difference in the number of spelling/grammar mistakes5 is a possible indicator as Conventions/Grammar/Spelling revisions probably decrease the number of mistakes. 4.2 Experiments Experiment 1: Surface vs. text-based As the corpus study in Section 3 s"
W15-0616,P12-2049,0,0.082574,"sentence level (Faigley and Witte, 1981), we propose to extract revisions at the sentence level based on semantic sentence alignment instead. Figure 1 provides an example comparing revisions annotated in our work to revisions extracted in prior work (Bronner and Monz, 2012). Our work identifies the fact that the student added new information to the essay and modified the organization of old sentences. The previous work, however, extracts all the modifications as one unit and cannot distinguish the different kinds of revisions inside the unit. Our method is similar to Lee and Webster’s method (Lee and Webster, 2012), where a sentence-level revision corpus is built from college students’ ESL writings. However, their corpus only includes the comments of the teachers and does not have every revision annotated. Revision categorization In an early educational work from Faigley and Witte (1981), revisions are categorized to text-based change and surface change based on whether they changed the information of the essay or not. A similar categorization (factual vs. fluency) was chosen by Bronner and Monz (2012) for classifying Wikipedia edits. However, many differences could not be captured with such coarse grai"
W15-0616,I11-1017,0,0.0259461,"-based). Extrinsic evaluations demonstrate that our method for automatic revision classification can be used to predict a writer’s improvement. 1 Introduction Rewriting is considered as an important factor of successful writing. Research shows that expert writers revise in ways different from inexperienced writers (Faigley and Witte, 1981). Recognizing the importance of rewriting, more and more efforts are being made to understand and utilize revisions. There are rewriting suggestions made by instructors (Wells et al., 2013), studies modeling revisions for error correction (Xue and Hwa, 2010; Mizumoto et al., 2011) and tools aiming to help students with rewriting (Elireview, 2014; Lightside, 2014). While there is increasing interest in the improvement of writers’ rewriting skills, there is still a lack of study on the details of revisions. First, to find out what has been changed (defined as revision extraction in this paper), a typical approach is to extract and analyze revisions at the word/phrase level based on edits extracted with character-level text comparison (Bronner and Monz, 2012; Daxenberger and Gurevych, 2012). The semantic information of sentences is not considered in the character-level te"
W15-0616,P14-2066,0,0.13558,"d to 1, 2, 3 in Draft 2 because character-level text comparison could not identify the semantic similarity between sentences. the following efforts. First, we propose that it is better to extract revisions at a level higher than the character level, and in particular, explore the sentence-level. This avoids the misalignment errors of character-level text comparisons. Finer-grained studies can still be done on the sentence-level revisions extracted, such as fluency prediction (Chae and Nenkova, 2009), error correction (Cahill et al., 2013; Xue and Hwa, 2014), statement strength identification (Tan and Lee, 2014), etc. Second, we propose a sentence-level revision schema for argumentative writing, a common form of writing in education. In the schema, categories are defined for describing an author’s revision operations and revision purposes. The revision operations can be directly decided according to the results of sentence alignment, while revision purposes can be reliably manually annotated. We also do a corpus study to demonstrate the utility of sentence-level revisions for revision analysis. Finally, we adapt features from Wikipedia revision classification work and explore new features for our cla"
W15-0616,C10-2157,0,0.0307542,"n (surface vs. text-based). Extrinsic evaluations demonstrate that our method for automatic revision classification can be used to predict a writer’s improvement. 1 Introduction Rewriting is considered as an important factor of successful writing. Research shows that expert writers revise in ways different from inexperienced writers (Faigley and Witte, 1981). Recognizing the importance of rewriting, more and more efforts are being made to understand and utilize revisions. There are rewriting suggestions made by instructors (Wells et al., 2013), studies modeling revisions for error correction (Xue and Hwa, 2010; Mizumoto et al., 2011) and tools aiming to help students with rewriting (Elireview, 2014; Lightside, 2014). While there is increasing interest in the improvement of writers’ rewriting skills, there is still a lack of study on the details of revisions. First, to find out what has been changed (defined as revision extraction in this paper), a typical approach is to extract and analyze revisions at the word/phrase level based on edits extracted with character-level text comparison (Bronner and Monz, 2012; Daxenberger and Gurevych, 2012). The semantic information of sentences is not considered i"
W15-0616,P14-2098,0,0.093865,"entence 1 in Draft 1 is wrongly marked as being modified to 1, 2, 3 in Draft 2 because character-level text comparison could not identify the semantic similarity between sentences. the following efforts. First, we propose that it is better to extract revisions at a level higher than the character level, and in particular, explore the sentence-level. This avoids the misalignment errors of character-level text comparisons. Finer-grained studies can still be done on the sentence-level revisions extracted, such as fluency prediction (Chae and Nenkova, 2009), error correction (Cahill et al., 2013; Xue and Hwa, 2014), statement strength identification (Tan and Lee, 2014), etc. Second, we propose a sentence-level revision schema for argumentative writing, a common form of writing in education. In the schema, categories are defined for describing an author’s revision operations and revision purposes. The revision operations can be directly decided according to the results of sentence alignment, while revision purposes can be reliably manually annotated. We also do a corpus study to demonstrate the utility of sentence-level revisions for revision analysis. Finally, we adapt features from Wikipedia revision c"
W15-0616,W14-1818,1,0.614309,"sentences, 1 We plan to also explore revision extraction at the clause level in the future. Our approach can be adapted to the clause level by segmenting the clauses first and aligning the segmented clauses after. A potential benefit is that clauses are often the basic units of discourse structures, so extracting clause revisions will allow the direct use of discourse parser outputs (Feng and Hirst, 2014; Lin et al., 2014). However, potential problems are that clauses contain less information for alignment decisions and clause segmentation is noisier. we used the algorithm in our prior work (Zhang and Litman, 2014) which considers both sentence similarity (calculated using TF*IDF score) and the global context of sentences. 3.2 Revision schema definition As shown in Figure 2, two dimensions are considered in the definition of the revision schema: the author’s behavior (revision operation) and the reason for the author’s behavior (revision purpose). Revision operations include three categories: Add, Delete, Modify. The operations are decided automatically after sentences get aligned. For example, in Figure 1 where Sentence 3 in Draft 2 is aligned to sentence 1 in Draft 1, the revision operation is decided"
W16-0532,P14-2041,0,0.0900817,"ually provided information helps increase the accuracy of a scoring system and its ability to provide meaningful feedback related to the scoring rubric. But involving experts in the scoring process is a drawback for automatically scoring at scale. Research to reduce expert effort has been underway to increase the scalability of scoring systems. A semi-supervised method is used to reduce the amount of required hand-annotated data (Zesch et al., 2015). Text templates or patterns are automatically identified for short answer scoring (Ramachandran et al., 2015). Content importance models (Beigman Klebanov et al., 2014) are used to predict source material that students should select. In this paper, our goal is to use natural language processing to automatically extract from source material a comprehensive list of topics which include: a) important topic words, and b) specific expressions (N-grams with N &gt; 1) that students need to provide in their essays. We call this comprehensive list “topical components”. Automatic extraction of topical components helps to reduce expert effort before the automatic assessment process. We evaluate the usefulness of our method for extracting topical components on the Response"
W16-0532,W05-0206,0,0.0226204,"assessment. Our goal is to reduce the amount of expert effort and improve the scalability of an automatic scoring system. Experimental results show that scoring performance using automatically extracted data-driven topical components is promising. 1 Introduction Automatic essay scoring has increasingly been investigated in recent years. One important aspect of writing assessment, specifically in source-based writing, is evaluation of content. Different methods have been used to assess the content of essays, e.g., bag of words (Mayfield and Rose, 2013), semantic similarity (Foltz et al., 1999; Kakkonen et al., 2005; Lemaire and Dessus, 2001), content vector analysis and cosine similarity (Louis and Higgins, 2010; Higgins et al., 2006; Attali and Burstein, 2006), and Latent Dirichlet Allocation (LDA) topic modeling (Persing and Ng, 2014). These prior studies differ from our research in several ways. Much of the prior work does not target source-based writing and thus does not make use of source materials. Approaches that do make use of source materials are typically designed to detect only if an essay is on-topic. Our source-based assessment, in contrast, is also concerned with localizing in the student"
W16-0532,W10-1013,0,0.316167,"utomatic scoring system. Experimental results show that scoring performance using automatically extracted data-driven topical components is promising. 1 Introduction Automatic essay scoring has increasingly been investigated in recent years. One important aspect of writing assessment, specifically in source-based writing, is evaluation of content. Different methods have been used to assess the content of essays, e.g., bag of words (Mayfield and Rose, 2013), semantic similarity (Foltz et al., 1999; Kakkonen et al., 2005; Lemaire and Dessus, 2001), content vector analysis and cosine similarity (Louis and Higgins, 2010; Higgins et al., 2006; Attali and Burstein, 2006), and Latent Dirichlet Allocation (LDA) topic modeling (Persing and Ng, 2014). These prior studies differ from our research in several ways. Much of the prior work does not target source-based writing and thus does not make use of source materials. Approaches that do make use of source materials are typically designed to detect only if an essay is on-topic. Our source-based assessment, in contrast, is also concerned with localizing in the student essay pieces of evidence that students provided from the source material. This is beVarious kinds o"
W16-0532,P11-1076,0,0.0167196,"uch of the prior work does not target source-based writing and thus does not make use of source materials. Approaches that do make use of source materials are typically designed to detect only if an essay is on-topic. Our source-based assessment, in contrast, is also concerned with localizing in the student essay pieces of evidence that students provided from the source material. This is beVarious kinds of source-based assessments of content (both in essay and short answering scoring) typically require some expert work in advance. Experts have provided reference answers (Nielsen et al., 2009; Mohler et al., 2011) or manually crafted patterns (Sukkarieh et al., 2004; Makatchev and VanLahn, 2007; Nielsen et al., 2009). Using manually provided information helps increase the accuracy of a scoring system and its ability to provide meaningful feedback related to the scoring rubric. But involving experts in the scoring process is a drawback for automatically scoring at scale. Research to reduce expert effort has been underway to increase the scalability of scoring systems. A semi-supervised method is used to reduce the amount of required hand-annotated data (Zesch et al., 2015). Text templates or patterns ar"
W16-0532,P14-1144,0,0.0190413,"ponents is promising. 1 Introduction Automatic essay scoring has increasingly been investigated in recent years. One important aspect of writing assessment, specifically in source-based writing, is evaluation of content. Different methods have been used to assess the content of essays, e.g., bag of words (Mayfield and Rose, 2013), semantic similarity (Foltz et al., 1999; Kakkonen et al., 2005; Lemaire and Dessus, 2001), content vector analysis and cosine similarity (Louis and Higgins, 2010; Higgins et al., 2006; Attali and Burstein, 2006), and Latent Dirichlet Allocation (LDA) topic modeling (Persing and Ng, 2014). These prior studies differ from our research in several ways. Much of the prior work does not target source-based writing and thus does not make use of source materials. Approaches that do make use of source materials are typically designed to detect only if an essay is on-topic. Our source-based assessment, in contrast, is also concerned with localizing in the student essay pieces of evidence that students provided from the source material. This is beVarious kinds of source-based assessments of content (both in essay and short answering scoring) typically require some expert work in advance"
W16-0532,W15-0612,0,0.0551043,"; Makatchev and VanLahn, 2007; Nielsen et al., 2009). Using manually provided information helps increase the accuracy of a scoring system and its ability to provide meaningful feedback related to the scoring rubric. But involving experts in the scoring process is a drawback for automatically scoring at scale. Research to reduce expert effort has been underway to increase the scalability of scoring systems. A semi-supervised method is used to reduce the amount of required hand-annotated data (Zesch et al., 2015). Text templates or patterns are automatically identified for short answer scoring (Ramachandran et al., 2015). Content importance models (Beigman Klebanov et al., 2014) are used to predict source material that students should select. In this paper, our goal is to use natural language processing to automatically extract from source material a comprehensive list of topics which include: a) important topic words, and b) specific expressions (N-grams with N &gt; 1) that students need to provide in their essays. We call this comprehensive list “topical components”. Automatic extraction of topical components helps to reduce expert effort before the automatic assessment process. We evaluate the usefulness of o"
W16-0532,N15-1111,0,0.0456699,"Missing"
W16-0532,W15-0615,0,0.09316,"nswers (Nielsen et al., 2009; Mohler et al., 2011) or manually crafted patterns (Sukkarieh et al., 2004; Makatchev and VanLahn, 2007; Nielsen et al., 2009). Using manually provided information helps increase the accuracy of a scoring system and its ability to provide meaningful feedback related to the scoring rubric. But involving experts in the scoring process is a drawback for automatically scoring at scale. Research to reduce expert effort has been underway to increase the scalability of scoring systems. A semi-supervised method is used to reduce the amount of required hand-annotated data (Zesch et al., 2015). Text templates or patterns are automatically identified for short answer scoring (Ramachandran et al., 2015). Content importance models (Beigman Klebanov et al., 2014) are used to predict source material that students should select. In this paper, our goal is to use natural language processing to automatically extract from source material a comprehensive list of topics which include: a) important topic words, and b) specific expressions (N-grams with N &gt; 1) that students need to provide in their essays. We call this comprehensive list “topical components”. Automatic extraction of topical com"
W16-3615,ghosh-etal-2012-improving,0,0.0540862,"Missing"
W16-3615,W14-4324,1,0.827237,"blems of referential and relational coherency. We hypothesized that these differences would shed light on unclear aspects of the PDTB framework, while also challenging an automatic discourse parser. However, if despite their inherent noise, learning-based datasets could be shown able to be reliably annotated for discourse relations, then they could provide language technology and psycholinguistics research a wealth of new applications. For example, interactions between students’ discourse relation use and their quality and quantity of learning and affective states could be investigated (c.f. (Litman and Forbes-Riley, 2014)), as could the use of discourse relations for improving automated essay graders and writing tutors (c.f. (Zhang et al., 2016)). In this paper we discuss methodological complexities posed by applying the PDTB framework to noisy, learning-based, and argumentative data, including a heightened ambiguity between EntRel, Expansion, and Contingency relations. We present descriptive statistics showing how the relation distributions compare to both the PDTB (Prasad et al., 2014) and BioDRB corpus (Prasad et al., 2011), whose texts possess argumentative structure without being noisy or learning-based."
W16-3615,W03-2608,0,0.135963,"Missing"
W16-3615,D11-1068,0,0.322315,"discussion (IMRAD). The student essays further differ from all prior PDTB applications in that they are noisy, containing not only grammar and spelling errors but also deeper problems of referential and relational coherency. The noise often does not improve between first and second drafts. A.1-A.4 in the appendix provide essay excerpts illustrating noise variations. As shown, not only are spelling and 3.1 Method Prior applications of the PDTB framework have adopted its central tenets and most of its annotation conventions while adapting others to suit language and domain (Prasad et al., 2011; Alsaif and Markert, 2011; Zhou and Xue, 2015; Zeyrek et al., 2013; Sharma et al., 2013; Polkov et al., 2013; Danlos et al., 2012). Prasad et al. (2014) provide a comparative discussion of this prior work. Following this work we too retained PDTB’s central tenets and adhered to most of its annotation conventions but modified some to fit our domain, increase reliability, and reduce cost: a) As in the Hindi DRB (Sharma et al., 2013), our workflow proceeded in one pass through each essay, with each relation annotated for type, argument span, and sense before moving on. b) As in the BioDRB (Prasad et al., 2011), we did no"
W16-3615,C08-2022,0,0.0769492,"Missing"
W16-3615,I13-1011,0,0.0263571,"n that they are noisy, containing not only grammar and spelling errors but also deeper problems of referential and relational coherency. The noise often does not improve between first and second drafts. A.1-A.4 in the appendix provide essay excerpts illustrating noise variations. As shown, not only are spelling and 3.1 Method Prior applications of the PDTB framework have adopted its central tenets and most of its annotation conventions while adapting others to suit language and domain (Prasad et al., 2011; Alsaif and Markert, 2011; Zhou and Xue, 2015; Zeyrek et al., 2013; Sharma et al., 2013; Polkov et al., 2013; Danlos et al., 2012). Prasad et al. (2014) provide a comparative discussion of this prior work. Following this work we too retained PDTB’s central tenets and adhered to most of its annotation conventions but modified some to fit our domain, increase reliability, and reduce cost: a) As in the Hindi DRB (Sharma et al., 2013), our workflow proceeded in one pass through each essay, with each relation annotated for type, argument span, and sense before moving on. b) As in the BioDRB (Prasad et al., 2011), we did not label attribution, as apart from Dante quotes the student was nearly always the s"
W16-3615,prasad-etal-2008-penn,0,0.327937,"Missing"
W16-3615,W15-0616,1,0.659383,"y be implicit, explicit or alternatively lexicalized), its syntactically bound argument (ARG2) is bolded, its non-structural argument (ARG1) is italicized, and its type and sense (where applicable) are in parenthesis. Student Essay Data Most prior PDTB applications have focused on the published news domain, although the Turkish DB (Zeyrek et al., 2013) also used published novels, while the BioDRB (Prasad et al., 2011) used published biomedical research articles. The present study uses first and second drafts of 47 AP English high school student essays (94 essays, 4271 sentences, 75900 words) (Zhang and Litman, 2015). The first drafts were written after students read the first five cantos of Dante’s Inferno, and required explaining why a contemporary should be sent to each of the first six sections of Dante’s hell. The second drafts were revisions by the original writers after they received and generated peer feedback as part of the course. The essays differ markedly from news articles both in possessing an argumentative structure and being learning-based, with the goal that by the second draft they consist of an introduction, intermediate paragraphs developing the reasoning for each contemporary’s placem"
W16-3615,N16-3008,1,0.882436,"Missing"
W16-3615,J14-4007,0,0.409451,"Missing"
W16-3615,K15-2001,0,0.0204129,"hereby the spans may exactly match or overlap. The CoNLL-2015 Shared Task did not report partial match results even though as Lin et al. (2014) note, most disagreements between exact and partial match do not show significant semantic differences (Miltsakaki et al., 2004) and result from small text portions being included or deleted We used the PDTB-trained Lin et al. discourse parser (Lin et al., 2014) to automatically predict our human-annotated relations. As the first endto-end free text PDTB discourse parser, it is typically the parser to which novel technical advances are compared (e.g., (Xue et al., 2015; Ji and Eisenstein, 2014)). In its sequential pipeline architecture, all functional occurrences of a predefined set of discourse connectives are identified, and then their two arguments are identified and assigned a sense. Subsequently within each paragraph all remaining unannotated adjacent sentence pairs are labeled as Non-Explicit, and their argument spans are identified and assigned a sense. EntRel, AltLex and NoRel relations are also predicted during this step. Since our essays are only annotated with Level-1 senses, we used the Lin et al. parser5 in two different ways. First, we used th"
W16-3615,P14-1031,0,0.0527702,"Missing"
W16-3615,W04-2703,0,\N,Missing
W16-3615,D13-1094,0,\N,Missing
W16-3635,N09-1050,0,0.0557296,"Missing"
W16-3635,W15-4617,0,0.0221008,"Missing"
W16-3635,W13-1704,0,0.0299256,"r speech grading to automate the holistic scoring of the conversational speech of non-native speakers of English. 1 Do you work or are you a student I’m a student in university er And what subject are you studying Figure 1: Testing dialogue excerpt between an IELTS human examiner (E) and a candidate (C) (Seedhouse et al., 2014). should be able to build on. Third, there is increasing interest in building automated systems not to replace human examiners during testing, but to help candidates prepare for human testing. Similarly to systems for writing (Burstein et al., 2004; Roscoe et al., 2012; Andersen et al., 2013; Foltz and Rosenstein, 2015), automation could provide unlimited self-assessment and practice opportunities. There is already some educationally-oriented SDS work in computer assisted language learning (Su et al., 2015) and physics tutoring (ForbesRiley and Litman, 2011) to potentially build upon. On the other hand, differences between speaking assessment and traditional SDS applications can also pose research challenges. First, currently available SDS corpora do not focus on including speech from non-native speakers, and when such speech exists it is not scored for English skill. Even if one"
W16-3635,P14-1123,0,0.0335759,"Missing"
W16-3635,P15-2130,1,0.882733,"Missing"
W16-3635,W11-2002,1,0.852474,"Missing"
W16-3635,P15-1105,0,0.0291213,"Missing"
W16-3635,N13-1098,1,0.895007,"Missing"
W16-3635,N13-1101,0,0.0155052,"alogues that are our target for automatic assessment.4 Second, there is a lack of optimal technical infrastructure. Existing SDS components such as speech recognizers will likely need modification to handle nonIntroduction Speaking tests for assessing non-native speakers of English (NNSE) often include tasks involving interactive dialogue between a human examiner and a candidate. An IELTS1 example is shown in Figure 1. In contrast, most automated spoken assessment systems target only the non-interactive portions of existing speaking tests, e.g., the task of responding to a stimulus in TOEFL2 (Wang et al., 2013) or BULATS3 (van Dalen et al., 2015). This gap between the current state of manual and automated testing provides an opportunity for spoken dialogue systems (SDS) research. First, as illustrated by Figure 1, human-human testing dialogues share some features with existing computer-human dialogues, e.g., examiners use standardized topic-based scripts and utterance phrasing. Second, automatic assessment of spontaneous (but non-conversational) speech is an active research area (Chen et al., 2009; Chen and Zechner, 2011; Wang et al., 2013; Bhat et al., 2014; van Dalen et al., 2015; Shashidhar et al"
W16-3635,P11-1073,0,\N,Missing
W17-5006,P05-1045,0,0.00823232,"cond, and third person pronouns; the number of singular and plural pronouns; the number of pronouns for each of the following categories: personal, possessive, reflexive, reciprocal, relative, demonstrative, interrogative, indefinite. Named entities Named entities might give us a sense of characters or places that students discuss, with respect to specificity. For example, saying “I did not like Biff” is more specific than saying “I did not like one of the characters” as it points out which of the characters a student might not like. For this task we used the Stanford Named Entity Recognizer (Finkel et al., 2005) (NER) with the pre-trained 3 class model detecting location, person and organization entities. We extracted the following features: a binary feature indicating the presence/absence of any named entity; a binary feature indicating presence/absence of each of the three named entity classes; the total number of named entities; the total number of named entities per class. We complemented the previous counts by adding a normalized feature, with respect to the Online dialogue features While extracting arguments from online forum dialogues, Swanson at al. (2015) found that Speciteller scores (as a"
W17-5006,L16-1620,0,0.173246,"d summaries of news articles. Li and Nenkova (2015) developed Speciteller, a tool for predicting the specificity score of sentences. Specificity was defined in relation to the amount of details in a sentence. This tool uses a set of shallow features (described in Section 4.2) and two dense word vector representations to train two logistic regression models on Wall Street Journal articles. Additionally, they improved classification accuracy by using a semisupervised co-training method on over thirty thousand sentences from the Associated Press, New York Times, and Wall Street Journal. Finally, Li et al. (2016) improved the annotation scheme used in (Louis and Nenkova, 2011; Li and Nenkova, 2015) by considering contextual information, and by using a scale from 0 to 6 rather than binary specificity annotations. Our annotation scheme is based on prior educational work in coding specificity (Chisholm and Godley, 2011), and our prediction models will incorporate features used by Speciteller. Like other machine learning-based methods, Speciteller is highly dependent on its training data. Since our objective is to analyze classroom discussion, we also draw on work that has used Speciteller to analyze data"
W17-5006,W15-0602,0,0.0295821,"l features that can help explain different asThe neural network is trained by using the hid56 pects of highly specific discussions. Many of the features described above, like N-grams or tf-idf, might have good predictive power but they are not easily interpretable and bear little relation to our codebook. When considering NLP techniques applied to the educational domain, there is an increasing interest in developing models that capture important components of the construct to measure. Rahimi et al. (2017), for example, developed a model for automated essay scoring using rubric-based features; Loukina et al. (2015) evaluated different feature selection methods to obtain interpretable features in an educational setting. In order to create an interpretable feature set we started by manually selected meaningful features from Speciteller (imageability, subjectivity, polarity, and familiarity ratings, number of connectives, fraction of stopwords). At training/test time, this set is combined with features from the Pronoun, Named entities, and Book feature sets. Since all the features from the last 3 sets are interpretable, we only chose a few features from each set, selecting the ones with highest information"
W17-5006,W16-3623,0,0.0309443,"sity of Pittsburgh Pittsburgh, PA 15260 {lucalugini,litman}@cs.pitt.edu Abstract tion by making the annotation of specificity automatic. Specificity is defined by the Oxford Dictionary as “The quality of belonging or relating uniquely to a particular subject” 1 . Natural language processing (NLP) techniques can be used to facilitate the analysis of classroom discussion and of specificity. Chen et al. (2014) developed a tool for teacher self-assessment of classroom discussion through the analysis of the frequency of participation of students in the discussion, and teacherstudent turn patterns. Blanchard et al. (2016) developed a system for detecting teacher questions from classroom discussion recordings. These works, however, do not take into account the actual student discussion content. Speciteller (Li and Nenkova, 2015) is a current state of the art method for predicting sentence specificity. It was developed by analyzing newspaper articles to distinguish between general and specific sentences. Spoken and written language differ in grammatical structure, contextual influence, and cognitive process and skills (Chafe and Tannen, 1987; Biber, 1988). As such we believe that using Speciteller as-is on class"
W17-5006,W15-4631,0,0.294071,"in (Louis and Nenkova, 2011; Li and Nenkova, 2015) by considering contextual information, and by using a scale from 0 to 6 rather than binary specificity annotations. Our annotation scheme is based on prior educational work in coding specificity (Chisholm and Godley, 2011), and our prediction models will incorporate features used by Speciteller. Like other machine learning-based methods, Speciteller is highly dependent on its training data. Since our objective is to analyze classroom discussion, we also draw on work that has used Speciteller to analyze data that is more similar to our corpus. Swanson et al. (2015) analyzed online forum dialogues in the context of argument mining. 1. it involves one character or scene; 2. it gives substantial qualifications or elaboration; 3. it uses content-specific vocabulary; 4. it provides a chain of reasons. If none of the four elements was present, or if the turn at talk refers to all humans or the text in general, the turn at talk is labeled as low specificity. Medium specificity turns at talk contain one of the four elements, while high specificity ones contain at least two of the four elements. Table 1 shows examples of specificity annotation from one of the di"
W17-5006,P10-1040,0,0.0266879,"mber of symbols (including punctuation), average number of characters for the words in the sentence, number of stopwords (normalized by sentence length), number of strongly subjective and polar words (using the MPQA (Wilson et al., 2009) and the General Inquirer (Stone and Hunt, 1963) dictionaries), average word familiarity and imageability (using the MRC Psycholinguistic Database (Wilson, 1988)), average, maximum, minimum inverse document frequency values. Word embeddings features consist of the average of 100-dimensional vectors for each word in the sentence. The embeddings were provided by Turian et al. (2010) and trained on a corpus consisting of news articles. 4.3 gram and bigram in the corpus with frequency of at least 5. Descriptive statistics of lexical features for each turn at talk, namely minimum, maximum, and average, were also used. Syntactic features To mitigate the data sparsity that impacts word n-grams, and to get more generalizable features, we extracted unigrams, bigrams, and trigrams of Parts Of Speech (POS) tags, using the Natural Language Toolkit (Bird et al., 2009). 4.4 Additional feature sets In addition to the previous feature sets, we also extracted the following feature sets"
W17-5006,J09-3003,0,0.0698107,"and models that we propose to predict specificity. 2 Although argument move types are not used in our study, Kappa for the two annotator pairs were 0.75 and 0.89. 3 54 https://www.cis.upenn.edu/ nlp/software/speciteller.html set. Shallow features for each sentence consist of: number of connectives, sentence length (number of words), number of numbers, number of capital letters, number of symbols (including punctuation), average number of characters for the words in the sentence, number of stopwords (normalized by sentence length), number of strongly subjective and polar words (using the MPQA (Wilson et al., 2009) and the General Inquirer (Stone and Hunt, 1963) dictionaries), average word familiarity and imageability (using the MRC Psycholinguistic Database (Wilson, 1988)), average, maximum, minimum inverse document frequency values. Word embeddings features consist of the average of 100-dimensional vectors for each word in the sentence. The embeddings were provided by Turian et al. (2010) and trained on a corpus consisting of news articles. 4.3 gram and bigram in the corpus with frequency of at least 5. Descriptive statistics of lexical features for each turn at talk, namely minimum, maximum, and aver"
W18-0511,W16-2801,0,0.0273671,"nt progression, with evidence given as a direct quote. The knowledge domain of student talk may also matter, that is, whether the talk focuses on disciplinary knowledge or lived experiences. Some research suggests that disciplinary learning opportunities are maximized when students draw on evidence and reasoning that are commonly accepted in the discipline (Resnick and Schantz, 2015), although some studies suggest that evidence or reasoning from lived experiences increases discussion quality (Beach and Myers, 2001). Previous related work in NLP analyzed evidence type for argumentative tweets (Addawood and Bashir, 2016). Although the categories of evidence type are different, their definition of evidence type is in line with our definition of knowledge domain. However, our research is distinct from this research in its application domain (i.e. social media vs. education) and in analyzing knowledge domain for all argumentative components, not only those containing claims. 3.2 Specificity Specificity annotations are based on (Chisholm and Godley, 2011) and have the goal of capturing text-related characteristics expressed in student talk. Specificity labels are directly related to four distinct elements for an"
W18-0511,L16-1620,0,0.056045,"difference consists in the inclusion of the warrant label in our scheme, as it is important to understand how students explicitly use reasoning to connect evidence to claims. 3 Annotation Scheme Educational studies suggest that discussion quality is also influenced by the specificity of student talk (Chisholm and Godley, 2011; Sohmer et al., 2009). Chisholm and Godley found that as specificity increased, the quality of students’ claims and reasoning also increased. Previous NLP research has studied specificity in the context of professionally written newspaper articles (Li and Nenkova, 2015; Li et al., 2016; Louis and Nenkova, 2011, 2012). While the annotation instructions used in these studies work well for general purpose corpora, specificity in text-based discussions also needs to capture particular relations between discussions and texts. Furthermore, since the concept of a sentence is not clearly defined in speech, we annotate argumentative discourse units rather than sentences (see Section 3). 3.1 Our annotation scheme1 uses argument moves as the unit of analysis. We define an argument move as an utterance, or part of an utterance, that contains an argumentative discourse unit (ADU) (Pelds"
W18-0511,louis-nenkova-2012-corpus,0,0.385524,"Missing"
W18-0511,W17-5006,1,0.779227,"e discussion, the annotators might interpret the warrant as new claim. The specificity section shows relatively few low-high label disagreements as com5 Opportunities and Challenges Our annotation scheme introduces opportunities for the educational community to conduct futher research on the relationship between features of student talk, student learning, and discussion quality. Although Chisholm and Godley (2011) and we found relations between our coding constructs and 2 The class distributions for argumentation and specificity labels vary significantly across transcripts, as can be seen in (Lugini and Litman, 2017) and (Godley and Olshefski, 2017). 113 resulting in negative kappa in some cases. Using our scheme to create a corpus of classroom discussion data manually annotated for argumentation, specificity, and knowledge domain will support the development of more robust NLP prediction systems. discussion quality, these were small-scale studies based on manual annotations. Once automated classifiers are developed, such relations between talk and learning can be examined at scale. Also, automatic labeling via a standard coding scheme can support the generalization of findings across studies, and potenti"
W18-0511,P16-2089,0,0.144933,"s, and their warranting or reasoning to support the claims (Reznitskaya et al., 2009; Toulmin, 1958). Many researchers view student 110 Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 110–116 c New Orleans, Louisiana, June 5, 2018. 2018 Association for Computational Linguistics reasoning as of primary importance, particularly when the reasoning is elaborated and highly inferential (Kim, 2014). In Natural Language Processing (NLP), most educationally-oriented argumentation research has focused on corpora of student persuasive essays (Ghosh et al., 2016; Klebanov et al., 2016; Persing and Ng, 2016; Wachsmuth et al., 2016; Stab and Gurevych, 2017; Nguyen and Litman, 2018). We instead focus on multiparty spoken discussion transcripts from classrooms. A second key difference consists in the inclusion of the warrant label in our scheme, as it is important to understand how students explicitly use reasoning to connect evidence to claims. 3 Annotation Scheme Educational studies suggest that discussion quality is also influenced by the specificity of student talk (Chisholm and Godley, 2011; Sohmer et al., 2009). Chisholm and Godley found that as sp"
W18-0511,P17-1091,0,0.203703,"Missing"
W18-0511,D15-1110,0,0.035208,"2012). While the annotation instructions used in these studies work well for general purpose corpora, specificity in text-based discussions also needs to capture particular relations between discussions and texts. Furthermore, since the concept of a sentence is not clearly defined in speech, we annotate argumentative discourse units rather than sentences (see Section 3). 3.1 Our annotation scheme1 uses argument moves as the unit of analysis. We define an argument move as an utterance, or part of an utterance, that contains an argumentative discourse unit (ADU) (Peldszus and Stede, 2013). Like Peldszus and Stede (2015), in this paper we use transcripts already segmented into argument moves and focus on the steps following segmentation, i.e., labeling argumentation, specificity, and knowledge domain. Table 1 shows a section of a transcribed classroom discussion along with labels assigned by a human annotator following segmentation. Argumentation The argumentation scheme is based on (Lee, 2006) and consists of a simplified set of labels derived from Toulmin’s (1958) model: (i) Claim: an arguable statement that presents a particular interpretation of a text or topic. (ii) Evidence: facts, documentation, text r"
W18-0511,W16-2808,0,0.130789,"ing or reasoning to support the claims (Reznitskaya et al., 2009; Toulmin, 1958). Many researchers view student 110 Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 110–116 c New Orleans, Louisiana, June 5, 2018. 2018 Association for Computational Linguistics reasoning as of primary importance, particularly when the reasoning is elaborated and highly inferential (Kim, 2014). In Natural Language Processing (NLP), most educationally-oriented argumentation research has focused on corpora of student persuasive essays (Ghosh et al., 2016; Klebanov et al., 2016; Persing and Ng, 2016; Wachsmuth et al., 2016; Stab and Gurevych, 2017; Nguyen and Litman, 2018). We instead focus on multiparty spoken discussion transcripts from classrooms. A second key difference consists in the inclusion of the warrant label in our scheme, as it is important to understand how students explicitly use reasoning to connect evidence to claims. 3 Annotation Scheme Educational studies suggest that discussion quality is also influenced by the specificity of student talk (Chisholm and Godley, 2011; Sohmer et al., 2009). Chisholm and Godley found that as specificity increased, th"
W18-0511,N16-1164,0,0.0523668,"port the claims (Reznitskaya et al., 2009; Toulmin, 1958). Many researchers view student 110 Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 110–116 c New Orleans, Louisiana, June 5, 2018. 2018 Association for Computational Linguistics reasoning as of primary importance, particularly when the reasoning is elaborated and highly inferential (Kim, 2014). In Natural Language Processing (NLP), most educationally-oriented argumentation research has focused on corpora of student persuasive essays (Ghosh et al., 2016; Klebanov et al., 2016; Persing and Ng, 2016; Wachsmuth et al., 2016; Stab and Gurevych, 2017; Nguyen and Litman, 2018). We instead focus on multiparty spoken discussion transcripts from classrooms. A second key difference consists in the inclusion of the warrant label in our scheme, as it is important to understand how students explicitly use reasoning to connect evidence to claims. 3 Annotation Scheme Educational studies suggest that discussion quality is also influenced by the specificity of student talk (Chisholm and Godley, 2011; Sohmer et al., 2009). Chisholm and Godley found that as specificity increased, the quality of students’"
W18-0511,J17-3005,0,0.0841316,"lmin, 1958). Many researchers view student 110 Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 110–116 c New Orleans, Louisiana, June 5, 2018. 2018 Association for Computational Linguistics reasoning as of primary importance, particularly when the reasoning is elaborated and highly inferential (Kim, 2014). In Natural Language Processing (NLP), most educationally-oriented argumentation research has focused on corpora of student persuasive essays (Ghosh et al., 2016; Klebanov et al., 2016; Persing and Ng, 2016; Wachsmuth et al., 2016; Stab and Gurevych, 2017; Nguyen and Litman, 2018). We instead focus on multiparty spoken discussion transcripts from classrooms. A second key difference consists in the inclusion of the warrant label in our scheme, as it is important to understand how students explicitly use reasoning to connect evidence to claims. 3 Annotation Scheme Educational studies suggest that discussion quality is also influenced by the specificity of student talk (Chisholm and Godley, 2011; Sohmer et al., 2009). Chisholm and Godley found that as specificity increased, the quality of students’ claims and reasoning also increased. Previous NL"
W18-0511,C16-1158,0,0.0572327,"tskaya et al., 2009; Toulmin, 1958). Many researchers view student 110 Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 110–116 c New Orleans, Louisiana, June 5, 2018. 2018 Association for Computational Linguistics reasoning as of primary importance, particularly when the reasoning is elaborated and highly inferential (Kim, 2014). In Natural Language Processing (NLP), most educationally-oriented argumentation research has focused on corpora of student persuasive essays (Ghosh et al., 2016; Klebanov et al., 2016; Persing and Ng, 2016; Wachsmuth et al., 2016; Stab and Gurevych, 2017; Nguyen and Litman, 2018). We instead focus on multiparty spoken discussion transcripts from classrooms. A second key difference consists in the inclusion of the warrant label in our scheme, as it is important to understand how students explicitly use reasoning to connect evidence to claims. 3 Annotation Scheme Educational studies suggest that discussion quality is also influenced by the specificity of student talk (Chisholm and Godley, 2011; Sohmer et al., 2009). Chisholm and Godley found that as specificity increased, the quality of students’ claims and reasoning al"
W18-0528,E12-1036,0,0.448855,"evelopment of a machine-learning model to automatically analyze revision improvement. Specifically, given only two sentences - original and revised, our current goal is to predict if a revised sentence is better than the original. 2 Related Work Prior NLP revision analysis work has developed methods for identifying pairs of original and revised textual units in both Wikipedia articles and student essays, as well as for classifying such pairs with respect to schemas of coarse (e.g., syntactic versus semantic) and fine-grained (e.g., lexical vs. grammatical syntactic changes) revision purposes (Bronner and Monz, 2012; Daxenberger and Gurevych, 2012; Zhang and Litman, 2015; Yang et al., 2017). For example, the ArgRewrite corpus (Zhang et al., 2017) was introduced with the goal to facilitate argumentative revision analysis and automatic revision purpose classification. However, purpose classification does not ad240 Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 240–246 c New Orleans, Louisiana, June 5, 2018. 2018 Association for Computational Linguistics dress revision quality. For example, a spelling change can both fix as well as introduce an e"
W18-0528,N12-1067,0,0.0177312,"with the goal to facilitate argumentative revision analysis and automatic revision purpose classification. However, purpose classification does not ad240 Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 240–246 c New Orleans, Louisiana, June 5, 2018. 2018 Association for Computational Linguistics dress revision quality. For example, a spelling change can both fix as well as introduce an error, while lexical changes can both enhance or reduce fluency. On the other hand, while some work has focused on correction detection in revision (Dahlmeier and Ng, 2012; Xue and Hwa, 2014; Felice et al., 2016), such work has typically been limited to grammatical error detection. The AESW shared task of identifying sentences in need of correction (Daudaravicius et al., 2016) goes beyond just grammatical errors, but the original task does not compare multiple versions of text, and also focuses on scientific writing. In contrast, Tan and Lee (2014) created a dataset of paired revised sentences in academic writing annotated as to whether one sentence was stronger or weaker than the other. Their work directly sheds light on annotating sentence revision quality in"
W18-0528,W16-0506,0,0.189752,"se in the freely available ArgRewrite corpus (Zhang et al., 2017), with improvement annotated using standard rubric criteria for evaluating student argumentative writing. By adapting NLP features used in previous revision classification tasks, we then develop a prediction model that outperforms baselines, even though the size of our non-expert revision corpus is small. Hence, we explore extracting paired revisions from an expert edited dataset to increase training data. The expert revisions are a subset of those in the freely available Automated Evaluation of Scientific Writing (AESW) corpus (Daudaravicius et al., 2016). Our experiments show that with proper sampling, combining expert and non-expert revisions can improve prediction performance, particularly for low-quality revisions. Studies of writing revisions rarely focus on revision quality. To address this issue, we introduce a corpus of between-draft revisions of student argumentative essays, annotated as to whether each revision improves essay quality. We demonstrate a potential usage of our annotations by developing a machine learning model to predict revision improvement. With the goal of expanding training data, we also extract revisions from a dat"
W18-0528,W16-0518,0,0.156375,"rgence4 . We also capture differences using BLEU5 score, motivated by its use in evaluating machinetranslated text quality. Following Zhang and Litman (2015), we calculate the count and difference of spelling and language errors6 , in our case to capture improvement as a result of error corrections. As stated in the annotation guidelines, one way a revised sentence can be better is because it is more precise or specific. Therefore, we introduce the use of the Speciteller (Li and Nenkova, 2015) tool to quantify the specificity of S1 and S2, and take the specificity difference as a new feature. Remse et al. (2016) used parse tree based feaThe Automated Evaluation of Scientific Writing (AESW) (Daudaravicius et al., 2016) shared task was to predict whether a sentence needed editing or not. Professional proof-readers edited sentences to correct issues ranging from grammatical errors to stylistic problems, intuitively yielding ‘Better’ sentences. Therefore, we can use the AESW edit information to create an automatically annotated corpus for revision improvement. In addition, by randomly flipping sentences we can include ‘NotBetter’ labels in the corpus. The AESW dataset was created from different scientifi"
W18-0528,C12-1044,0,0.151896,"learning model to automatically analyze revision improvement. Specifically, given only two sentences - original and revised, our current goal is to predict if a revised sentence is better than the original. 2 Related Work Prior NLP revision analysis work has developed methods for identifying pairs of original and revised textual units in both Wikipedia articles and student essays, as well as for classifying such pairs with respect to schemas of coarse (e.g., syntactic versus semantic) and fine-grained (e.g., lexical vs. grammatical syntactic changes) revision purposes (Bronner and Monz, 2012; Daxenberger and Gurevych, 2012; Zhang and Litman, 2015; Yang et al., 2017). For example, the ArgRewrite corpus (Zhang et al., 2017) was introduced with the goal to facilitate argumentative revision analysis and automatic revision purpose classification. However, purpose classification does not ad240 Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 240–246 c New Orleans, Louisiana, June 5, 2018. 2018 Association for Computational Linguistics dress revision quality. For example, a spelling change can both fix as well as introduce an error, while lexical changes can"
W18-0528,P14-2066,0,0.631648,"example, a spelling change can both fix as well as introduce an error, while lexical changes can both enhance or reduce fluency. On the other hand, while some work has focused on correction detection in revision (Dahlmeier and Ng, 2012; Xue and Hwa, 2014; Felice et al., 2016), such work has typically been limited to grammatical error detection. The AESW shared task of identifying sentences in need of correction (Daudaravicius et al., 2016) goes beyond just grammatical errors, but the original task does not compare multiple versions of text, and also focuses on scientific writing. In contrast, Tan and Lee (2014) created a dataset of paired revised sentences in academic writing annotated as to whether one sentence was stronger or weaker than the other. Their work directly sheds light on annotating sentence revision quality in terms of statement strength. However, their corpus focuses on the abstracts and introductions of ArXiv papers. Building on their annotation methodology, we consider paired sentences as our revision unit, but 1) annotate revision quality in terms of argumentative writing criteria, 2) use a corpus of revisions from non-expert student argumentative essays, and 3) move beyond annotat"
W18-0528,D13-1055,0,0.0200234,"ifferent scientific writing genres (e.g. Mathematics, Astrophysics) with placeholders for anonymization. We use two random samples of 5000 AESW revisions for the experiments in Section 5. “AESW all” samples revisions from all scientific genres, while “AESW plaintext” ignores sentences containing placeholders (e.g. MATH, MATHDISP) to make the data more similar to ArgRewrite. Table 1 shows two example (4 and 5) AESW revisions. 4 Features for Classification We adapt many features from prior studies predicting revision purposes (Adler et al., 2011; Javanmardi et al., 2011; Bronner and Monz, 2012; Daxenberger and Gurevych, 2013; Zhang and Litman, 2015; Remse et al., 2016) as well as introduce new features tailored to predicting improvement. 4 Using scipy.stats.entropy on sentence vectors. Using sentence bleu from nltk.translate.bleu score module, with S1 as reference and S2 as hypothesis. 6 Using python ‘language-check’ tool. 5 242 Experiments Majority baseline AESW all AESW plaintext ArgRewrite ArgRewrite + AESW all ArgRewrite + AESW plaintext Precision 0.417 0.471* 0.511* 0.570* 0.497* 0.574* Recall 0.500 0.470 0.515 0.534 0.501 0.555* F1 0.454 0.468 0.473 0.525* 0.488* 0.551* Table 3: 10-fold cross-validation per"
W18-0528,C16-1079,0,0.0137968,"revision analysis and automatic revision purpose classification. However, purpose classification does not ad240 Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 240–246 c New Orleans, Louisiana, June 5, 2018. 2018 Association for Computational Linguistics dress revision quality. For example, a spelling change can both fix as well as introduce an error, while lexical changes can both enhance or reduce fluency. On the other hand, while some work has focused on correction detection in revision (Dahlmeier and Ng, 2012; Xue and Hwa, 2014; Felice et al., 2016), such work has typically been limited to grammatical error detection. The AESW shared task of identifying sentences in need of correction (Daudaravicius et al., 2016) goes beyond just grammatical errors, but the original task does not compare multiple versions of text, and also focuses on scientific writing. In contrast, Tan and Lee (2014) created a dataset of paired revised sentences in academic writing annotated as to whether one sentence was stronger or weaker than the other. Their work directly sheds light on annotating sentence revision quality in terms of statement strength. However, th"
W18-0528,P14-2098,0,0.0232945,"tate argumentative revision analysis and automatic revision purpose classification. However, purpose classification does not ad240 Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 240–246 c New Orleans, Louisiana, June 5, 2018. 2018 Association for Computational Linguistics dress revision quality. For example, a spelling change can both fix as well as introduce an error, while lexical changes can both enhance or reduce fluency. On the other hand, while some work has focused on correction detection in revision (Dahlmeier and Ng, 2012; Xue and Hwa, 2014; Felice et al., 2016), such work has typically been limited to grammatical error detection. The AESW shared task of identifying sentences in need of correction (Daudaravicius et al., 2016) goes beyond just grammatical errors, but the original task does not compare multiple versions of text, and also focuses on scientific writing. In contrast, Tan and Lee (2014) created a dataset of paired revised sentences in academic writing annotated as to whether one sentence was stronger or weaker than the other. Their work directly sheds light on annotating sentence revision quality in terms of statement"
W18-0528,D17-1213,0,0.0813138,"ment. Specifically, given only two sentences - original and revised, our current goal is to predict if a revised sentence is better than the original. 2 Related Work Prior NLP revision analysis work has developed methods for identifying pairs of original and revised textual units in both Wikipedia articles and student essays, as well as for classifying such pairs with respect to schemas of coarse (e.g., syntactic versus semantic) and fine-grained (e.g., lexical vs. grammatical syntactic changes) revision purposes (Bronner and Monz, 2012; Daxenberger and Gurevych, 2012; Zhang and Litman, 2015; Yang et al., 2017). For example, the ArgRewrite corpus (Zhang et al., 2017) was introduced with the goal to facilitate argumentative revision analysis and automatic revision purpose classification. However, purpose classification does not ad240 Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 240–246 c New Orleans, Louisiana, June 5, 2018. 2018 Association for Computational Linguistics dress revision quality. For example, a spelling change can both fix as well as introduce an error, while lexical changes can both enhance or reduce fluency. On the other"
W18-0528,P17-1144,1,0.847258,"and revised, our current goal is to predict if a revised sentence is better than the original. 2 Related Work Prior NLP revision analysis work has developed methods for identifying pairs of original and revised textual units in both Wikipedia articles and student essays, as well as for classifying such pairs with respect to schemas of coarse (e.g., syntactic versus semantic) and fine-grained (e.g., lexical vs. grammatical syntactic changes) revision purposes (Bronner and Monz, 2012; Daxenberger and Gurevych, 2012; Zhang and Litman, 2015; Yang et al., 2017). For example, the ArgRewrite corpus (Zhang et al., 2017) was introduced with the goal to facilitate argumentative revision analysis and automatic revision purpose classification. However, purpose classification does not ad240 Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 240–246 c New Orleans, Louisiana, June 5, 2018. 2018 Association for Computational Linguistics dress revision quality. For example, a spelling change can both fix as well as introduce an error, while lexical changes can both enhance or reduce fluency. On the other hand, while some work has focused on correction detectio"
W18-0528,W15-0616,1,0.931455,"analyze revision improvement. Specifically, given only two sentences - original and revised, our current goal is to predict if a revised sentence is better than the original. 2 Related Work Prior NLP revision analysis work has developed methods for identifying pairs of original and revised textual units in both Wikipedia articles and student essays, as well as for classifying such pairs with respect to schemas of coarse (e.g., syntactic versus semantic) and fine-grained (e.g., lexical vs. grammatical syntactic changes) revision purposes (Bronner and Monz, 2012; Daxenberger and Gurevych, 2012; Zhang and Litman, 2015; Yang et al., 2017). For example, the ArgRewrite corpus (Zhang et al., 2017) was introduced with the goal to facilitate argumentative revision analysis and automatic revision purpose classification. However, purpose classification does not ad240 Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 240–246 c New Orleans, Louisiana, June 5, 2018. 2018 Association for Computational Linguistics dress revision quality. For example, a spelling change can both fix as well as introduce an error, while lexical changes can both enhance or reduce f"
W18-0549,P16-1068,0,0.139552,"atures to represent linguistic characteristic related to organization and development, lexical complexity, promptspecific vocabulary usage, etc. Similarly to Page (1968), this system used regression equations for assessment of student essays. One limitation of all of the above models is that all need handcrafted features for training the model. In contrast, our model uses a neural network for the AES task and thus does not require feature engineering. Recently, neural network models have been introduced into AES, making the development of handcrafted features unnecessary or at least optional. Alikaniotis et al. (2016) and Taghipour and Ng (2016) presented AES models that used Long Short Term Memory (LSTM) networks. Differently, Dong and Zhang (2016) used a Convolutional Neural Network (CNN) model for essay scoring by applying two CNN layers on both the word level and then sentence level. Later, Dong et al. (2017) presented another work that uses attention pooling to replace the mean over time pooling after the convolutional layer in both word level and sentence levels. However, none of these neural network grading models consider the source article if it exists. In this paper, we introduce a neural network"
W18-0549,P98-1032,0,0.331915,"ferent sentences in the essay. In the following sections, we first present related research. Then we describe our tasks by introducing the ASAP corpus and the RTA corpus. Next, we explain the structure of our co-attention based neural network model. Finally, we discuss the results of our experiments and future plans. 1 https://www.kaggle.com/c/asap-aes 2 Related Work Previous research in AES needed feature engineering. In very early work, Page (1968) developed an AES tool named Project Essay Grade (PEG) by only using linguistic surface features. A more recent well-known AES system is E-Rater (Burstein et al., 1998), which employs many more natural language processing (NLP) technologies. Later, Attali and Burstein (2004) released E-Rater V2, where they created a new set of features to represent linguistic characteristic related to organization and development, lexical complexity, promptspecific vocabulary usage, etc. Similarly to Page (1968), this system used regression equations for assessment of student essays. One limitation of all of the above models is that all need handcrafted features for training the model. In contrast, our model uses a neural network for the AES task and thus does not require fe"
W18-0549,D16-1115,0,0.221544,", etc. Similarly to Page (1968), this system used regression equations for assessment of student essays. One limitation of all of the above models is that all need handcrafted features for training the model. In contrast, our model uses a neural network for the AES task and thus does not require feature engineering. Recently, neural network models have been introduced into AES, making the development of handcrafted features unnecessary or at least optional. Alikaniotis et al. (2016) and Taghipour and Ng (2016) presented AES models that used Long Short Term Memory (LSTM) networks. Differently, Dong and Zhang (2016) used a Convolutional Neural Network (CNN) model for essay scoring by applying two CNN layers on both the word level and then sentence level. Later, Dong et al. (2017) presented another work that uses attention pooling to replace the mean over time pooling after the convolutional layer in both word level and sentence levels. However, none of these neural network grading models consider the source article if it exists. In this paper, we introduce a neural network model that takes the source article into account by using a co-attention mechanism instead of the self-attention mechanism of prior w"
W18-0549,K17-1017,0,0.540604,"afted features for training the model. In contrast, our model uses a neural network for the AES task and thus does not require feature engineering. Recently, neural network models have been introduced into AES, making the development of handcrafted features unnecessary or at least optional. Alikaniotis et al. (2016) and Taghipour and Ng (2016) presented AES models that used Long Short Term Memory (LSTM) networks. Differently, Dong and Zhang (2016) used a Convolutional Neural Network (CNN) model for essay scoring by applying two CNN layers on both the word level and then sentence level. Later, Dong et al. (2017) presented another work that uses attention pooling to replace the mean over time pooling after the convolutional layer in both word level and sentence levels. However, none of these neural network grading models consider the source article if it exists. In this paper, we introduce a neural network model that takes the source article into account by using a co-attention mechanism instead of the self-attention mechanism of prior work. Our work not only focuses on essay assessment using a holistic score, but also evaluates a particular dimension of argument-oriented writing skills, namely use of"
W18-0549,P14-2041,0,0.0270342,"ulary. However, in both of these studies, human effort was still necessary for pre-processing the source article, for example, by having experts manually create a list of important words and phrases in the article which the system would compare with features extracted from the student’s essay. In contrast, our work does not need any human effort to analyze the source article before essay grading. Although Rahimi and Litman (2016) investigated extracting example lists by using LDA (Blei et al., 2003) model, the data-driven model missed an example when there was no essay mentioning the example. Klebanov et al. (2014) predicted which parts of the source material were important and that students needed to use in their essays. The essay score is required to obtain the content importance for their work, but our work does not need to know the essay score while identifying the content importance. 3 Data We use two different essay corpora in our experiments: source-based essays from the ASAP corpus, and source-based RTA essays. While the full ASAP corpus contains essays in response to 8 different prompts, we use only essays in response to the 4 source-dependent prompts. The gold standard ASAP assessment is a hol"
W18-0549,W10-1013,0,0.161503,"another work that uses attention pooling to replace the mean over time pooling after the convolutional layer in both word level and sentence levels. However, none of these neural network grading models consider the source article if it exists. In this paper, we introduce a neural network model that takes the source article into account by using a co-attention mechanism instead of the self-attention mechanism of prior work. Our work not only focuses on essay assessment using a holistic score, but also evaluates a particular dimension of argument-oriented writing skills, namely use of Evidence. Louis and Higgins (2010) analyze only the content of essays by detecting off-topic essays. Ong et al. (2014) used argumentation mining techniques to evaluate if students use enough evidence to support their positions. However, these two prior studies are not suitable for our task because they did not measure the use of content or evidence from a source article. With respect to source-based dimensional essay analysis, Rahimi et al. (2014, 2017) developed a set of rubric-based features that compared a student’s essay and a source article in terms of number of related words or paraphrases. Zhang and Litman (2017) improv"
W18-0549,W14-2104,1,0.84482,"nvolutional layer in both word level and sentence levels. However, none of these neural network grading models consider the source article if it exists. In this paper, we introduce a neural network model that takes the source article into account by using a co-attention mechanism instead of the self-attention mechanism of prior work. Our work not only focuses on essay assessment using a holistic score, but also evaluates a particular dimension of argument-oriented writing skills, namely use of Evidence. Louis and Higgins (2010) analyze only the content of essays by detecting off-topic essays. Ong et al. (2014) used argumentation mining techniques to evaluate if students use enough evidence to support their positions. However, these two prior studies are not suitable for our task because they did not measure the use of content or evidence from a source article. With respect to source-based dimensional essay analysis, Rahimi et al. (2014, 2017) developed a set of rubric-based features that compared a student’s essay and a source article in terms of number of related words or paraphrases. Zhang and Litman (2017) improved their model by introducing word embedding into the feature extraction process to"
W18-0549,D14-1162,0,0.0818856,"res the relationship between the essay and the source article. In particular, a higher attention score will be assigned to sentences that are mentioned in the article but less mentioned in other essays. Our model is a hierarchical neural network and consists of seven layers. Figure 5 shows the structure of our network. The layers in the dashed box were presented by Dong et al. (2017). The sentence level co-attention layer was presented by Seo et al. (2017). 4.1 Word Embedding Layer This layer maps each word in sentences to a high dimension vector. We use the GloVe pre-trained word embeddings (Pennington et al., 2014) to obtain the word embedding vector for each word. It was trained on 6 billion words from Wikipedia 2014 and Gigaword 5. It has 400,000 uncased vocabulary items. The dimensionality of GloVe in our model is 50 dimensions. The outputs of this layer are two matrices, LE ∈ RSe ×We ×dL for the essay and LA ∈ RSa ×Wa ×dL for the article, where Se , Sa , We , Wa , and dL are number of sentences of the essay and the article, length of sentences of the essay and the article, and the embedding size, respectively. Same to Dong et al. (2017), a dropout is applied after the word embedding layer. 4.2 Word"
W18-0549,D15-1049,0,0.119905,"d t-tests are used for significance tests with p < 0.05. Table 3 shows all hyperparameters for training. The code of SELF-ATTN are provided by Dong et al. (2017), they used Keras (Chollet et al., 2015) 1.1.1 and Theano (Theano Development Team, 2016) 0.8.2 as the backend. Because we are using Keras 2.1.3 and TensorFlow (Abadi et al., 2015) 1.4.0 as the backend, we ran all experiments with our frameworks. Therefore, the numbers of SELFATTN have small differences to the numbers reported by the baseline model. For non-neural network baselines, we introduce the SVR and BLRR baselines presented by Phandi et al. (2015) for the ASAP corpus, and SG baseline presented by Zhang and Litman (2017) for the RTA corpus. SVR and BLRR models use Enhanced AI Scoring Engine (EASE)2 to extract four types of features, such as length, part of speech, prompt, and the bag of words. Then they use SVR and BLRR as the classifiers, respectively. We do not perform 2 https://github.com/edx/ease any significance test on both SVR and BLRR because we do not have detailed experiment data. Therefore, we only report the result presented in Phandi et al. (2015). SG model extracts evidence features based on hand-crafted topic and example"
W18-0549,W16-0532,1,0.805553,"Litman (2017) improved their model by introducing word embedding into the feature extraction process to extract relationships previously missed due to lexical errors or use of different vocabulary. However, in both of these studies, human effort was still necessary for pre-processing the source article, for example, by having experts manually create a list of important words and phrases in the article which the system would compare with features extracted from the student’s essay. In contrast, our work does not need any human effort to analyze the source article before essay grading. Although Rahimi and Litman (2016) investigated extracting example lists by using LDA (Blei et al., 2003) model, the data-driven model missed an example when there was no essay mentioning the example. Klebanov et al. (2014) predicted which parts of the source material were important and that students needed to use in their essays. The essay score is required to obtain the content importance for their work, but our work does not need to know the essay score while identifying the content importance. 3 Data We use two different essay corpora in our experiments: source-based essays from the ASAP corpus, and source-based RTA essays"
W18-0549,D16-1193,0,0.483923,"characteristic related to organization and development, lexical complexity, promptspecific vocabulary usage, etc. Similarly to Page (1968), this system used regression equations for assessment of student essays. One limitation of all of the above models is that all need handcrafted features for training the model. In contrast, our model uses a neural network for the AES task and thus does not require feature engineering. Recently, neural network models have been introduced into AES, making the development of handcrafted features unnecessary or at least optional. Alikaniotis et al. (2016) and Taghipour and Ng (2016) presented AES models that used Long Short Term Memory (LSTM) networks. Differently, Dong and Zhang (2016) used a Convolutional Neural Network (CNN) model for essay scoring by applying two CNN layers on both the word level and then sentence level. Later, Dong et al. (2017) presented another work that uses attention pooling to replace the mean over time pooling after the convolutional layer in both word level and sentence levels. However, none of these neural network grading models consider the source article if it exists. In this paper, we introduce a neural network model that takes the source"
W18-0549,P17-3013,1,0.85479,"ence. Louis and Higgins (2010) analyze only the content of essays by detecting off-topic essays. Ong et al. (2014) used argumentation mining techniques to evaluate if students use enough evidence to support their positions. However, these two prior studies are not suitable for our task because they did not measure the use of content or evidence from a source article. With respect to source-based dimensional essay analysis, Rahimi et al. (2014, 2017) developed a set of rubric-based features that compared a student’s essay and a source article in terms of number of related words or paraphrases. Zhang and Litman (2017) improved their model by introducing word embedding into the feature extraction process to extract relationships previously missed due to lexical errors or use of different vocabulary. However, in both of these studies, human effort was still necessary for pre-processing the source article, for example, by having experts manually create a list of important words and phrases in the article which the system would compare with features extracted from the student’s essay. In contrast, our work does not need any human effort to analyze the source article before essay grading. Although Rahimi and Li"
W18-5046,D16-1149,1,0.810878,"Missing"
W18-5046,P16-1050,0,0.0239592,"ell, 2015; Lubold et al., 2015). One of the main measures of entrainment is convergence which is the main focus of this paper. Within a conversation, convergence measures the amount of increase in similarity of speakers over time in terms of linguistic features (Levitan and Hirschberg, 2011). While most research has focused on quantifying the amount of entrainment between speaker pairs (i.e., dyads), recent studies have started to develop measures for quantifying entrainment between larger groups of speakers (Friedberg et al., 2012; Danescu-Niculescu-Mizil et al., 2012; Gonzales et al., 2010; Doyle and Frank, 2016; Litman et al., 2 Convergence for Multi-Party Dialogue The convergence measure that we extend in this paper is adopted from prior work. Originally, convergence between dyads (Levitan and Hirschberg, 2011) was measured by calculating the difference 385 Proceedings of the SIGDIAL 2018 Conference, pages 385–390, c Melbourne, Australia, 12-14 July 2018. 2018 Association for Computational Linguistics between the dissimilarity of speakers in two nonoverlapping time intervals. If the dissimilarity in the second interval was less than in the first, the pair was said to be converging. Extending this w"
W18-5046,W12-1612,0,0.0186471,"measure and also show that in general a proper weighting of the dyadlevel measures performs better than nonweighted averaging in multiple tasks. 1 Introduction Entrainment is the tendency of speakers to begin behaving like one another in conversation. The development of methods for automatically quantifying entrainment in text and speech data is an active research area, as entrainment has been shown to correlate with outcomes such as success measures and social variables for a variety of phenomena, e.g., acoustic-prosodic, lexical, and syntactic (Nenkova et al., 2008; Reitter and Moore, 2007; Mitchell et al., 2012; Levitan et al., 2012; Lee et al., 2011; Stoyanchev and Stent, 2009; Lopes et al., 2013; Lubold and Pon-Barry, 2014; Moon et al., 2014; Sinha and Cassell, 2015; Lubold et al., 2015). One of the main measures of entrainment is convergence which is the main focus of this paper. Within a conversation, convergence measures the amount of increase in similarity of speakers over time in terms of linguistic features (Levitan and Hirschberg, 2011). While most research has focused on quantifying the amount of entrainment between speaker pairs (i.e., dyads), recent studies have started to develop measur"
W18-5046,W14-4103,0,0.0280155,"asks. 1 Introduction Entrainment is the tendency of speakers to begin behaving like one another in conversation. The development of methods for automatically quantifying entrainment in text and speech data is an active research area, as entrainment has been shown to correlate with outcomes such as success measures and social variables for a variety of phenomena, e.g., acoustic-prosodic, lexical, and syntactic (Nenkova et al., 2008; Reitter and Moore, 2007; Mitchell et al., 2012; Levitan et al., 2012; Lee et al., 2011; Stoyanchev and Stent, 2009; Lopes et al., 2013; Lubold and Pon-Barry, 2014; Moon et al., 2014; Sinha and Cassell, 2015; Lubold et al., 2015). One of the main measures of entrainment is convergence which is the main focus of this paper. Within a conversation, convergence measures the amount of increase in similarity of speakers over time in terms of linguistic features (Levitan and Hirschberg, 2011). While most research has focused on quantifying the amount of entrainment between speaker pairs (i.e., dyads), recent studies have started to develop measures for quantifying entrainment between larger groups of speakers (Friedberg et al., 2012; Danescu-Niculescu-Mizil et al., 2012; Gonzale"
W18-5046,E12-1080,0,0.343324,"ior measures, their observations were only based on a few example dialogues and no solutions were proposed. In this paper, we propose a new weighting method to normalize the contribution of speakers based on group dynamics. We explore the effect of our method, participation weighting, and simple averaging when calculating group convergence from dyads. We conclude that our proposed weighted convergence measure performs significantly better on multiple benchmark prediction and regression tasks that have been used to evaluate convergence in prior studies (De Looze et al., 2014; Lee et al., 2011; Jain et al., 2012; Rahimi et al., 2017a; Doyle et al., 2016; Lee et al., 2011). This paper proposes a new weighting method for extending a dyad-level measure of convergence to multi-party dialogues by considering group dynamics instead of simply averaging. Experiments indicate the usefulness of the proposed weighted measure and also show that in general a proper weighting of the dyadlevel measures performs better than nonweighted averaging in multiple tasks. 1 Introduction Entrainment is the tendency of speakers to begin behaving like one another in conversation. The development of methods for automatically qu"
W18-5046,P08-2043,0,0.0268711,"dicate the usefulness of the proposed weighted measure and also show that in general a proper weighting of the dyadlevel measures performs better than nonweighted averaging in multiple tasks. 1 Introduction Entrainment is the tendency of speakers to begin behaving like one another in conversation. The development of methods for automatically quantifying entrainment in text and speech data is an active research area, as entrainment has been shown to correlate with outcomes such as success measures and social variables for a variety of phenomena, e.g., acoustic-prosodic, lexical, and syntactic (Nenkova et al., 2008; Reitter and Moore, 2007; Mitchell et al., 2012; Levitan et al., 2012; Lee et al., 2011; Stoyanchev and Stent, 2009; Lopes et al., 2013; Lubold and Pon-Barry, 2014; Moon et al., 2014; Sinha and Cassell, 2015; Lubold et al., 2015). One of the main measures of entrainment is convergence which is the main focus of this paper. Within a conversation, convergence measures the amount of increase in similarity of speakers over time in terms of linguistic features (Levitan and Hirschberg, 2011). While most research has focused on quantifying the amount of entrainment between speaker pairs (i.e., dyads"
W18-5046,N12-1002,0,0.0183594,"hat in general a proper weighting of the dyadlevel measures performs better than nonweighted averaging in multiple tasks. 1 Introduction Entrainment is the tendency of speakers to begin behaving like one another in conversation. The development of methods for automatically quantifying entrainment in text and speech data is an active research area, as entrainment has been shown to correlate with outcomes such as success measures and social variables for a variety of phenomena, e.g., acoustic-prosodic, lexical, and syntactic (Nenkova et al., 2008; Reitter and Moore, 2007; Mitchell et al., 2012; Levitan et al., 2012; Lee et al., 2011; Stoyanchev and Stent, 2009; Lopes et al., 2013; Lubold and Pon-Barry, 2014; Moon et al., 2014; Sinha and Cassell, 2015; Lubold et al., 2015). One of the main measures of entrainment is convergence which is the main focus of this paper. Within a conversation, convergence measures the amount of increase in similarity of speakers over time in terms of linguistic features (Levitan and Hirschberg, 2011). While most research has focused on quantifying the amount of entrainment between speaker pairs (i.e., dyads), recent studies have started to develop measures for quantifying ent"
W18-5046,P07-1102,0,0.0347257,"of the proposed weighted measure and also show that in general a proper weighting of the dyadlevel measures performs better than nonweighted averaging in multiple tasks. 1 Introduction Entrainment is the tendency of speakers to begin behaving like one another in conversation. The development of methods for automatically quantifying entrainment in text and speech data is an active research area, as entrainment has been shown to correlate with outcomes such as success measures and social variables for a variety of phenomena, e.g., acoustic-prosodic, lexical, and syntactic (Nenkova et al., 2008; Reitter and Moore, 2007; Mitchell et al., 2012; Levitan et al., 2012; Lee et al., 2011; Stoyanchev and Stent, 2009; Lopes et al., 2013; Lubold and Pon-Barry, 2014; Moon et al., 2014; Sinha and Cassell, 2015; Lubold et al., 2015). One of the main measures of entrainment is convergence which is the main focus of this paper. Within a conversation, convergence measures the amount of increase in similarity of speakers over time in terms of linguistic features (Levitan and Hirschberg, 2011). While most research has focused on quantifying the amount of entrainment between speaker pairs (i.e., dyads), recent studies have st"
W18-5046,N09-2048,0,0.0119814,"dyadlevel measures performs better than nonweighted averaging in multiple tasks. 1 Introduction Entrainment is the tendency of speakers to begin behaving like one another in conversation. The development of methods for automatically quantifying entrainment in text and speech data is an active research area, as entrainment has been shown to correlate with outcomes such as success measures and social variables for a variety of phenomena, e.g., acoustic-prosodic, lexical, and syntactic (Nenkova et al., 2008; Reitter and Moore, 2007; Mitchell et al., 2012; Levitan et al., 2012; Lee et al., 2011; Stoyanchev and Stent, 2009; Lopes et al., 2013; Lubold and Pon-Barry, 2014; Moon et al., 2014; Sinha and Cassell, 2015; Lubold et al., 2015). One of the main measures of entrainment is convergence which is the main focus of this paper. Within a conversation, convergence measures the amount of increase in similarity of speakers over time in terms of linguistic features (Levitan and Hirschberg, 2011). While most research has focused on quantifying the amount of entrainment between speaker pairs (i.e., dyads), recent studies have started to develop measures for quantifying entrainment between larger groups of speakers (Fr"
W18-5208,D17-1218,0,0.0454062,"m discussions to detect central propositions and argument facets. Habernal and Gurevych (2017) analyzed user-generated web discourse data from several sources by performing micro-level argumentation mining. While these prior works analyze multiparty discussions, the discussions are neither originally spoken nor in an educational context. Like other areas of natural language processing, argument mining is experiencing an increase in the development of neural network models. Niculae et al. (2017) used a factor graph model which was parametrized by a recurrent neural network. Daxenberger et al. (Daxenberger et al., 2017) investigated the different conceptualizations of claims in several domains by analyzing in-domain and cross-domain performance of recurrent neural networks and convolutional neural networks, in addition to other models. Schulz et al. (Schulz et al., 2018) analyzed the impact of using multitask learning when training on a limited amount of labeled data. In a similar way, we develop several convolutional neural network and recurrent neural student essays are typically written by an individual student, in classroom discussions arguments are formed collaboratively between multiple parties (i.e. m"
W18-5208,W14-2106,0,0.0477436,"et al., 2009; Elizabeth et al., 2012). With the increasing importance of argumentation in classrooms, especially in the context of studentcentered discussions, automatically performing argument component classification is a first step for building tools aimed at helping teachers analyze and better understand student arguments, with the goal of improving students’ learning outcomes. Many current argument mining systems focus on analyzing argumentation in student essays (Stab and Gurevych, 2014, 2017; Nguyen and Litman, 2015, 2018), online dialogues (Swanson et al., 2015; McLaren et al., 2010; Ghosh et al., 2014; Lawrence and Reed, 2017), or in the legal domain (Ashley and Walker, 2013; Palau and Moens, 2009). A key difference between these studies and our work consists in the source of linguistic content: although we analyze written transcriptions of discussions, the original source for our corpora consists of spoken, multi-party, educational discussions, and the difference in cognitive skills and grammatical structure between written and spoken language (Biber, 1988; Chafe and Tannen, 1987) introduces additional complexity. Our work and previous research studies on student essays share the trait of"
W18-5208,J17-1004,0,0.258601,"s to work well on our corpus, although some of the features might still be useful. Also, while some of the previously proposed systems address multiple subproblems simultaneously, e.g. argument component identification and argument component classification, we only focus on argument component classification. Swanson et al. (2015) developed a model for extracting argumentative portions of text from online dialogues, which were later used for summarizing the multiple argument facets. Misra et al. (2015) analyzed dyadic online forum discussions to detect central propositions and argument facets. Habernal and Gurevych (2017) analyzed user-generated web discourse data from several sources by performing micro-level argumentation mining. While these prior works analyze multiparty discussions, the discussions are neither originally spoken nor in an educational context. Like other areas of natural language processing, argument mining is experiencing an increase in the development of neural network models. Niculae et al. (2017) used a factor graph model which was parametrized by a recurrent neural network. Daxenberger et al. (Daxenberger et al., 2017) investigated the different conceptualizations of claims in several d"
W18-5208,D14-1181,0,0.00483723,"Missing"
W18-5208,W17-5114,0,0.0306953,"beth et al., 2012). With the increasing importance of argumentation in classrooms, especially in the context of studentcentered discussions, automatically performing argument component classification is a first step for building tools aimed at helping teachers analyze and better understand student arguments, with the goal of improving students’ learning outcomes. Many current argument mining systems focus on analyzing argumentation in student essays (Stab and Gurevych, 2014, 2017; Nguyen and Litman, 2015, 2018), online dialogues (Swanson et al., 2015; McLaren et al., 2010; Ghosh et al., 2014; Lawrence and Reed, 2017), or in the legal domain (Ashley and Walker, 2013; Palau and Moens, 2009). A key difference between these studies and our work consists in the source of linguistic content: although we analyze written transcriptions of discussions, the original source for our corpora consists of spoken, multi-party, educational discussions, and the difference in cognitive skills and grammatical structure between written and spoken language (Biber, 1988; Chafe and Tannen, 1987) introduces additional complexity. Our work and previous research studies on student essays share the trait of analyzing argumentation i"
W18-5208,W17-5006,1,0.796406,"tion and specificity. For the CNN model, we added a second softmax layer for predicting specificity after the convolutional/pooling layers. Similarly, for the LSTM model we added 61 Figure 1: Neural network models used in this study: neural network only setup (a); model incorporating neural network and handcrafted features (wLDA and online dialogue sets) (b); multi-task setup for neural network only model (c); multi-task setup for model using neural network and handcrafted features (d). we implemented consists in the use of Speciteller (Li and Nenkova, 2015). As observed by Lugini and Litman (Lugini and Litman, 2017), applying Speciteller as-is to domains other than news articles results in a considerable drop in performance. Therefore, instead of including the specificity score obtained by directly applying Specificity to an argument move, we decided to use Speciteller’s features. Figure 2: Argument labels by specificity levels. 5 Experiments and Results This section provides our experimental results. In Section 5.1 we will test our first hypothesis: using an argument mining system trained in a different domain will result in low performance, which can be improved by re-training on classroom discussions"
W18-5208,W18-0511,1,0.61798,"experiment with multitask learning. More detailed comparisons will be given in Section 4. 3 sons explaining how a specific evidence instance supports a specific claim. Chisholm and Godley (2011) observed how specificity has an impact on the quality of the discussion, while Swanson et al. (2015) noted that a relationship exists between specificity and the quality of arguments in online forum dialogues. For the purpose of investigating whether there exists a relationship between specificity and argument components, we additionally annotated data for specificity following the same coding scheme (Lugini et al., 2018). Specificity labels are directly related to four elements for an argument move: (1) it is specific to one (or a few) character or scene; (2) it makes significant qualifications or elaborations; (3) it uses content-specific vocabulary (e.g. quotes from the text); (4) it provides a chain of reasons. The specificity annotation scheme by Lugini et al. includes three labels along a linear scale: (i) Low: statement that does not contain any of these elements. (ii) Medium: statement that accomplishes one of these elements. (iii) High: statement that clearly accomplishes at least two specificity elem"
W18-5208,N15-1046,0,0.0264963,"explicitly understand how students use them to connect evidence to claims. As such, we do not expect prior models to work well on our corpus, although some of the features might still be useful. Also, while some of the previously proposed systems address multiple subproblems simultaneously, e.g. argument component identification and argument component classification, we only focus on argument component classification. Swanson et al. (2015) developed a model for extracting argumentative portions of text from online dialogues, which were later used for summarizing the multiple argument facets. Misra et al. (2015) analyzed dyadic online forum discussions to detect central propositions and argument facets. Habernal and Gurevych (2017) analyzed user-generated web discourse data from several sources by performing micro-level argumentation mining. While these prior works analyze multiparty discussions, the discussions are neither originally spoken nor in an educational context. Like other areas of natural language processing, argument mining is experiencing an increase in the development of neural network models. Niculae et al. (2017) used a factor graph model which was parametrized by a recurrent neural n"
W18-5208,N18-2006,0,0.0690958,", the discussions are neither originally spoken nor in an educational context. Like other areas of natural language processing, argument mining is experiencing an increase in the development of neural network models. Niculae et al. (2017) used a factor graph model which was parametrized by a recurrent neural network. Daxenberger et al. (Daxenberger et al., 2017) investigated the different conceptualizations of claims in several domains by analyzing in-domain and cross-domain performance of recurrent neural networks and convolutional neural networks, in addition to other models. Schulz et al. (Schulz et al., 2018) analyzed the impact of using multitask learning when training on a limited amount of labeled data. In a similar way, we develop several convolutional neural network and recurrent neural student essays are typically written by an individual student, in classroom discussions arguments are formed collaboratively between multiple parties (i.e. multiple students and possibly teachers). While our work shares the multi-party context in which arguments are made with research aimed at argument mining in online dialogues, prior online dialogue studies have not been contextualized in the educational dom"
W18-5208,W15-0503,1,0.935657,"reflected in students’ problem solving and disciplinary skills (Engle and Conant, 2002; Murphy et al., 2009; Elizabeth et al., 2012). With the increasing importance of argumentation in classrooms, especially in the context of studentcentered discussions, automatically performing argument component classification is a first step for building tools aimed at helping teachers analyze and better understand student arguments, with the goal of improving students’ learning outcomes. Many current argument mining systems focus on analyzing argumentation in student essays (Stab and Gurevych, 2014, 2017; Nguyen and Litman, 2015, 2018), online dialogues (Swanson et al., 2015; McLaren et al., 2010; Ghosh et al., 2014; Lawrence and Reed, 2017), or in the legal domain (Ashley and Walker, 2013; Palau and Moens, 2009). A key difference between these studies and our work consists in the source of linguistic content: although we analyze written transcriptions of discussions, the original source for our corpora consists of spoken, multi-party, educational discussions, and the difference in cognitive skills and grammatical structure between written and spoken language (Biber, 1988; Chafe and Tannen, 1987) introduces additiona"
W18-5208,C14-1142,0,0.189881,"Gregory, 2013). This impact is reflected in students’ problem solving and disciplinary skills (Engle and Conant, 2002; Murphy et al., 2009; Elizabeth et al., 2012). With the increasing importance of argumentation in classrooms, especially in the context of studentcentered discussions, automatically performing argument component classification is a first step for building tools aimed at helping teachers analyze and better understand student arguments, with the goal of improving students’ learning outcomes. Many current argument mining systems focus on analyzing argumentation in student essays (Stab and Gurevych, 2014, 2017; Nguyen and Litman, 2015, 2018), online dialogues (Swanson et al., 2015; McLaren et al., 2010; Ghosh et al., 2014; Lawrence and Reed, 2017), or in the legal domain (Ashley and Walker, 2013; Palau and Moens, 2009). A key difference between these studies and our work consists in the source of linguistic content: although we analyze written transcriptions of discussions, the original source for our corpora consists of spoken, multi-party, educational discussions, and the difference in cognitive skills and grammatical structure between written and spoken language (Biber, 1988; Chafe and Tan"
W18-5208,J17-3005,0,0.130899,"Missing"
W18-5208,W15-4631,0,0.519104,"inary skills (Engle and Conant, 2002; Murphy et al., 2009; Elizabeth et al., 2012). With the increasing importance of argumentation in classrooms, especially in the context of studentcentered discussions, automatically performing argument component classification is a first step for building tools aimed at helping teachers analyze and better understand student arguments, with the goal of improving students’ learning outcomes. Many current argument mining systems focus on analyzing argumentation in student essays (Stab and Gurevych, 2014, 2017; Nguyen and Litman, 2015, 2018), online dialogues (Swanson et al., 2015; McLaren et al., 2010; Ghosh et al., 2014; Lawrence and Reed, 2017), or in the legal domain (Ashley and Walker, 2013; Palau and Moens, 2009). A key difference between these studies and our work consists in the source of linguistic content: although we analyze written transcriptions of discussions, the original source for our corpora consists of spoken, multi-party, educational discussions, and the difference in cognitive skills and grammatical structure between written and spoken language (Biber, 1988; Chafe and Tannen, 1987) introduces additional complexity. Our work and previous research st"
W18-5208,P17-1091,0,0.0458325,"ogues, which were later used for summarizing the multiple argument facets. Misra et al. (2015) analyzed dyadic online forum discussions to detect central propositions and argument facets. Habernal and Gurevych (2017) analyzed user-generated web discourse data from several sources by performing micro-level argumentation mining. While these prior works analyze multiparty discussions, the discussions are neither originally spoken nor in an educational context. Like other areas of natural language processing, argument mining is experiencing an increase in the development of neural network models. Niculae et al. (2017) used a factor graph model which was parametrized by a recurrent neural network. Daxenberger et al. (Daxenberger et al., 2017) investigated the different conceptualizations of claims in several domains by analyzing in-domain and cross-domain performance of recurrent neural networks and convolutional neural networks, in addition to other models. Schulz et al. (Schulz et al., 2018) analyzed the impact of using multitask learning when training on a limited amount of labeled data. In a similar way, we develop several convolutional neural network and recurrent neural student essays are typically wr"
W18-5208,D14-1162,0,0.0814683,"Missing"
W18-5208,P15-1053,0,\N,Missing
W93-0216,P92-1032,0,0.156326,"Missing"
W93-0216,J86-3001,0,0.172822,"Missing"
W93-0216,H92-1089,0,0.507239,"Missing"
W93-0216,J93-3003,1,0.892217,"Missing"
W93-0216,P93-1020,1,0.836662,"Missing"
W97-0601,J86-3001,0,0.0138314,"trategies that operate at the level of dialogue subtasks (subdialogues). Consider the longer versions of Dialogues 1 and 2 in Figures 2 and 3. Each utterance in Figures 2 and 3 has been tagged using one or more of the attribute abbreviations in Table 1, according to the subtask(s) the utterance contributes to. As a convention of this type of tagging, utterances that contribute to the success of the whole dialogue, such as greetings, are tagged with all the attributes. Thus the goal of the tagging is to show how the structure of the dialogue reflects the structure of the task (Carbelrry, 1989; Grosz and Sidner, 1986; Litman and Allen, 1990). Tagging by AVM attributes is required to calculate costs over subdialogues, since for any subdialogue, task attributes define the subdialogue. For example, the subdialogue about the attribute arrival-city (SA) consists of utterances A6 and U6, its cost Cl(SA) is 2. Tagging by AVM attributes is also required to calculate the cost of some of the qualitative measures, such as number of repair utterances. (Note that to calculate such costs, each utterance in the corpus of dialogues must also be tagged with respect to the qualitative phenomenon in question, e.g. whether t"
W97-0601,P96-1038,0,0.016709,"For example, let c2 be the number of repair utterances. The repair utterances in Figure 2 are A3 through U6, thus c2(D1) is 10 utterances and c2(SA) is 2 utterances. The repair utterance in Figure 3 is U2, but note that according to the AVM task tagging, U2 simultaneously addresses the information goals for arrival-city and depart-range. In 7This tagging can be hand generated, or system generated and hand corrected. Preliminary studies indicate that reliability for human tagging is higher for AVM attribute tagging than for other types of discourse segment tagging (Passonneau and Litman, 1997; Hirschberg and Nakatani, 1996). 8Previous work has shown that this can be done with high reliability (Hirschman and Pao, 1993). general, if an utterance U contributes to the information goals of N different attributes, each attribute accounts for 1/N of any costs derivable from U. Thus, c2(D2) is .5. Given a set of ci, it is necessary to combine the different cost measures in order to determine their relative contribution to performance. The next section explains how to combine ~ with a set of ci to yield an overall performance measure. 2.4 Estimating a Performance Function Given the definition of success and costs above a"
W97-0601,H93-1004,0,0.0250551,", 1992; Price et al., 1992; Simpson and Fraser, 1993). Another problem is that dialog evaluation is not reducible to transcript evaluation, or to comparison with a wizard's reference answers (Bates and Ayuso, 1993; Polifroni et al., 1992; Price et al., 1992), because the set of potentially acceptable dialogs can be very large. Current proposals for dialog evaluation metrics are both objective and subjective. The objective metrics that have been used to evaluate a dialog as a whole include (Abella, Brown, and Buntschuh, 1996; Ciaremella, 1993; Danieli and Gerbino, 1995; Hirschman et al., 1990; Hirschman et al., 1993; Polifroni et al., 1992; Price et al., 1992; Smith and Hipp, 1994; Smith and Gordon, 1997; Walker, 1996): • percentage of ""non-trivial"" (more than one word) utterances. • mean length of ""non-trivial"" utterances Objective metrics can be calculated without recourse to human judgement, and in many cases, can be calculated automatically by the spoken dialogue system. One possible exception is task-based success measures, such as transaction success, task completion or quality of solution metrics, which can be either an objective or a subjective measure depending on whether the users' goals are we"
W97-0601,H90-1023,0,0.032847,"spect to a set of reference answers • transaction success, task completion, or quality of solution • number of turns or utterances; • dialogue time or task completion time • mean user response time • mean system response time • frequency of diagnostic error messages I Introduction Interactive spoken dialogue systems are based on many component technologies: speech recognition, text-tospeech, natural language understanding, natural language generation, and database query languages. While evaluation metrics for these components are well understand (Sparck-Jones and Galliers, 1996; Walker, 1989; Hirschman et al., 1990), it has been difficult to develop standard metrics for complete systems that integrate all these technologies. One problem is that there are so many potential metrics that can be used to evaluate a dialog system. For example, a dialog system can be evaluated by measuring the system's ability to help users achieve their goals, the system's robustness in detecting and recovering from errors of speech recognition or of understanding, and the overall quality of the system's interactions with users (Danieli and Gerbino, 1995; Hirschman and Pao, 1993; Polifroni et al., 1992; Price et al., 1992; Sim"
W97-0601,J97-1005,1,0.819888,"he utterance is a repair. 8) For example, let c2 be the number of repair utterances. The repair utterances in Figure 2 are A3 through U6, thus c2(D1) is 10 utterances and c2(SA) is 2 utterances. The repair utterance in Figure 3 is U2, but note that according to the AVM task tagging, U2 simultaneously addresses the information goals for arrival-city and depart-range. In 7This tagging can be hand generated, or system generated and hand corrected. Preliminary studies indicate that reliability for human tagging is higher for AVM attribute tagging than for other types of discourse segment tagging (Passonneau and Litman, 1997; Hirschberg and Nakatani, 1996). 8Previous work has shown that this can be done with high reliability (Hirschman and Pao, 1993). general, if an utterance U contributes to the information goals of N different attributes, each attribute accounts for 1/N of any costs derivable from U. Thus, c2(D2) is .5. Given a set of ci, it is necessary to combine the different cost measures in order to determine their relative contribution to performance. The next section explains how to combine ~ with a set of ci to yield an overall performance measure. 2.4 Estimating a Performance Function Given the definit"
W97-0601,H92-1005,0,0.1602,"rs, 1996; Walker, 1989; Hirschman et al., 1990), it has been difficult to develop standard metrics for complete systems that integrate all these technologies. One problem is that there are so many potential metrics that can be used to evaluate a dialog system. For example, a dialog system can be evaluated by measuring the system's ability to help users achieve their goals, the system's robustness in detecting and recovering from errors of speech recognition or of understanding, and the overall quality of the system's interactions with users (Danieli and Gerbino, 1995; Hirschman and Pao, 1993; Polifroni et al., 1992; Price et al., 1992; Simpson and Fraser, 1993). Another problem is that dialog evaluation is not reducible to transcript evaluation, or to comparison with a wizard's reference answers (Bates and Ayuso, 1993; Polifroni et al., 1992; Price et al., 1992), because the set of potentially acceptable dialogs can be very large. Current proposals for dialog evaluation metrics are both objective and subjective. The objective metrics that have been used to evaluate a dialog as a whole include (Abella, Brown, and Buntschuh, 1996; Ciaremella, 1993; Danieli and Gerbino, 1995; Hirschman et al., 1990; Hirsch"
W97-0601,H92-1006,0,0.0185075,"Hirschman et al., 1990), it has been difficult to develop standard metrics for complete systems that integrate all these technologies. One problem is that there are so many potential metrics that can be used to evaluate a dialog system. For example, a dialog system can be evaluated by measuring the system's ability to help users achieve their goals, the system's robustness in detecting and recovering from errors of speech recognition or of understanding, and the overall quality of the system's interactions with users (Danieli and Gerbino, 1995; Hirschman and Pao, 1993; Polifroni et al., 1992; Price et al., 1992; Simpson and Fraser, 1993). Another problem is that dialog evaluation is not reducible to transcript evaluation, or to comparison with a wizard's reference answers (Bates and Ayuso, 1993; Polifroni et al., 1992; Price et al., 1992), because the set of potentially acceptable dialogs can be very large. Current proposals for dialog evaluation metrics are both objective and subjective. The objective metrics that have been used to evaluate a dialog as a whole include (Abella, Brown, and Buntschuh, 1996; Ciaremella, 1993; Danieli and Gerbino, 1995; Hirschman et al., 1990; Hirschman et al., 1993; Po"
W97-0601,H92-1009,0,0.0322943,"le performance evaluation function. The use of decision theory requires a specification of both the objectives of the decision problem and a set of measures (known as attributes in decision theory) for operationalizing the objectives. The PARADISE model is based on the structure of objectives (rectangles) shown in Figure 1. At the top level, this model posits that performance can be correlated with a meaningful external criterion such as usability, and thus that the overall goal of a spoken dialogue agent is to maximize an objective related to usability. User satisfaction ratings (Kamm, 1995; Shriberg, Wade, and Price, 1992; Polifroni et al., 1992) are the most widely used external indicator of the usability of a dialogue agent. The model further posits that two types of factors are potential relevant contributors to user satisfaction, namely task success and dialogue costs. PARADISE uses linear regression to quantify the relative contribution of the success and cost factors to user satisfaction. The task success measure builds on previous measures of transaction success and task completion (Danieli and Gerbino, 1995; Polifroni et al., 1992), but makes use of the Kappa coefficient (Carletta, 1996; Siegel and Cas"
W97-0601,J97-1006,0,0.013058,"uation is not reducible to transcript evaluation, or to comparison with a wizard's reference answers (Bates and Ayuso, 1993; Polifroni et al., 1992; Price et al., 1992), because the set of potentially acceptable dialogs can be very large. Current proposals for dialog evaluation metrics are both objective and subjective. The objective metrics that have been used to evaluate a dialog as a whole include (Abella, Brown, and Buntschuh, 1996; Ciaremella, 1993; Danieli and Gerbino, 1995; Hirschman et al., 1990; Hirschman et al., 1993; Polifroni et al., 1992; Price et al., 1992; Smith and Hipp, 1994; Smith and Gordon, 1997; Walker, 1996): • percentage of ""non-trivial"" (more than one word) utterances. • mean length of ""non-trivial"" utterances Objective metrics can be calculated without recourse to human judgement, and in many cases, can be calculated automatically by the spoken dialogue system. One possible exception is task-based success measures, such as transaction success, task completion or quality of solution metrics, which can be either an objective or a subjective measure depending on whether the users' goals are well-defined at the beginning of the dialogue. This is the case in controlled experiments, b"
W97-0601,P89-1031,1,0.805634,"nswers with respect to a set of reference answers • transaction success, task completion, or quality of solution • number of turns or utterances; • dialogue time or task completion time • mean user response time • mean system response time • frequency of diagnostic error messages I Introduction Interactive spoken dialogue systems are based on many component technologies: speech recognition, text-tospeech, natural language understanding, natural language generation, and database query languages. While evaluation metrics for these components are well understand (Sparck-Jones and Galliers, 1996; Walker, 1989; Hirschman et al., 1990), it has been difficult to develop standard metrics for complete systems that integrate all these technologies. One problem is that there are so many potential metrics that can be used to evaluate a dialog system. For example, a dialog system can be evaluated by measuring the system's ability to help users achieve their goals, the system's robustness in detecting and recovering from errors of speech recognition or of understanding, and the overall quality of the system's interactions with users (Danieli and Gerbino, 1995; Hirschman and Pao, 1993; Polifroni et al., 1992"
W97-0601,P97-1035,1,0.711917,"across systems performing different tasks (Cohen, 1995; Sparck-Jones and Galliers, 1996). It would be useful to know how users' perceptions of performance depend on the strategy used, and on tradeoffs among factors such as efficiency, speed, and accuracy. In addition to agent factors such as the differences in dialogue strategy seen in Dialogues 1 and 2, task factors such as database size and environmental factors such as background noise may also be relevant predictors of performance. In the remainder of this paper, we discuss the PARADISE framework (PARAdigm for Dialogue System Evaluation) (Walker et al., 1997), and that it addresses these limitations, as well as others. We will show that PARADISE provides a useful methodology for evaluating dialog systems that integrates and enhances previous work. 2 Integrating Previous Approaches to Evaluation in the PARADISE Framework MAXIMIZEUSERSATISFACTION MAXIMIZETASK SUCCESS MINIMIZECOSTS ~ MF~SOIIES Figure 1: PARADISE's structure of objectives for spoken dialogue performance The PARADISE framework for spoken dialogue evaluation is based on methods from decision theory (Keeney and Raiffa, 1976; Doyle, 1992), which supports combining the disparate set of per"
W97-0601,H91-1062,0,\N,Missing
