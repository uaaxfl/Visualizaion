2020.acl-tutorials.1,S19-1026,1,0.878132,"Missing"
2020.acl-tutorials.1,P18-2006,0,0.0238567,"u et al., 2014; Strobelt et al., 2018), and saliency measures (Li et al., 2016; Murdoch et al., 2018; Arras et al., 2017), including a walkthrough on how to build a simple attention visualization. Next, we will discuss the construction and use of challenge sets for fine-grained evaluation in the context of different tasks (Conneau and Kiela, 2018; Wang et al., 2018; Isabelle and Kuhn, 2018; Sennrich, 2017, inter alia). Finally, we will review work on generating adversarial examples in NLP, focusing on the challenges brought upon by the discrete nature of textual input (Papernot et al., 2016b; Ebrahimi et al., 2018; Jia and Liang, 2017; Belinkov and Bisk, 2018, inter alia). A detailed outline is provided in Section 3. Throughout the tutorial, we will highlight not only the most commonly applied analysis methods, but also the specific limitations and shortcomings of current approaches. By the end of the tutorial, participants will be better informed where to focus future research efforts. 2 5. Other Methods (a) Generating Explanations (b) Psycholinguistic Methods (c) Testing on Formal Languages 6. Conclusion 4 We would assume acquaintance with core linguistic concepts and basic knowledge of machine learn"
2020.acl-tutorials.1,Q16-1037,0,0.327684,"ndardized. This tutorial aims to fill this gap and introduce the nascent field of interpretability and analysis of neural networks in NLP. The tutorial will cover the main lines of analysis work, mostly drawing on the recent TACL survey by Belinkov and Glass (2019).1 In particular, we will devote a large portion to work aiming to find linguistic information that is captured by neural networks, such as probing classifiers (Hupkes et al., 2018; Adi et al., 2017; Conneau et al., 2018a,b; Tenney et al., 2019b, inter alia), controlled behavior studies on language modelling (Gulordava et al., 2018; Linzen et al., 2016a; Marvin and Linzen, 2018) or inference tasks (Poliak et al., 2018a,b; White et al., 2017; Kim et al., 2019; McCoy et al., 2019; Ross and Pavlick, 2019), psycholinguistic methods (Ettinger et al., 2018; Chrupała and Alishahi, 2019), layerwise analyses (Peters et al., 2018; Tenney et al., 2019a), among other methods (Hewitt and Manning, 2019; Zhang While deep learning has transformed the natural language processing (NLP) field and impacted the larger computational linguistics community, the rise of neural networks is stained by their opaque nature: It is challenging to interpret the inner work"
2020.acl-tutorials.1,C18-1152,0,0.0822829,"ly drawing on the recent TACL survey by Belinkov and Glass (2019).1 In particular, we will devote a large portion to work aiming to find linguistic information that is captured by neural networks, such as probing classifiers (Hupkes et al., 2018; Adi et al., 2017; Conneau et al., 2018a,b; Tenney et al., 2019b, inter alia), controlled behavior studies on language modelling (Gulordava et al., 2018; Linzen et al., 2016a; Marvin and Linzen, 2018) or inference tasks (Poliak et al., 2018a,b; White et al., 2017; Kim et al., 2019; McCoy et al., 2019; Ross and Pavlick, 2019), psycholinguistic methods (Ettinger et al., 2018; Chrupała and Alishahi, 2019), layerwise analyses (Peters et al., 2018; Tenney et al., 2019a), among other methods (Hewitt and Manning, 2019; Zhang While deep learning has transformed the natural language processing (NLP) field and impacted the larger computational linguistics community, the rise of neural networks is stained by their opaque nature: It is challenging to interpret the inner workings of neural network models, and explicate their behavior. Therefore, in the last few years, an increasingly large body of work has been devoted to the analysis and interpretation of neural network mo"
2020.acl-tutorials.1,N18-1108,0,0.0235295,"ds which are not yet standardized. This tutorial aims to fill this gap and introduce the nascent field of interpretability and analysis of neural networks in NLP. The tutorial will cover the main lines of analysis work, mostly drawing on the recent TACL survey by Belinkov and Glass (2019).1 In particular, we will devote a large portion to work aiming to find linguistic information that is captured by neural networks, such as probing classifiers (Hupkes et al., 2018; Adi et al., 2017; Conneau et al., 2018a,b; Tenney et al., 2019b, inter alia), controlled behavior studies on language modelling (Gulordava et al., 2018; Linzen et al., 2016a; Marvin and Linzen, 2018) or inference tasks (Poliak et al., 2018a,b; White et al., 2017; Kim et al., 2019; McCoy et al., 2019; Ross and Pavlick, 2019), psycholinguistic methods (Ettinger et al., 2018; Chrupała and Alishahi, 2019), layerwise analyses (Peters et al., 2018; Tenney et al., 2019a), among other methods (Hewitt and Manning, 2019; Zhang While deep learning has transformed the natural language processing (NLP) field and impacted the larger computational linguistics community, the rise of neural networks is stained by their opaque nature: It is challenging to int"
2020.acl-tutorials.1,D18-1151,0,0.0213835,"al aims to fill this gap and introduce the nascent field of interpretability and analysis of neural networks in NLP. The tutorial will cover the main lines of analysis work, mostly drawing on the recent TACL survey by Belinkov and Glass (2019).1 In particular, we will devote a large portion to work aiming to find linguistic information that is captured by neural networks, such as probing classifiers (Hupkes et al., 2018; Adi et al., 2017; Conneau et al., 2018a,b; Tenney et al., 2019b, inter alia), controlled behavior studies on language modelling (Gulordava et al., 2018; Linzen et al., 2016a; Marvin and Linzen, 2018) or inference tasks (Poliak et al., 2018a,b; White et al., 2017; Kim et al., 2019; McCoy et al., 2019; Ross and Pavlick, 2019), psycholinguistic methods (Ettinger et al., 2018; Chrupała and Alishahi, 2019), layerwise analyses (Peters et al., 2018; Tenney et al., 2019a), among other methods (Hewitt and Manning, 2019; Zhang While deep learning has transformed the natural language processing (NLP) field and impacted the larger computational linguistics community, the rise of neural networks is stained by their opaque nature: It is challenging to interpret the inner workings of neural network mode"
2020.acl-tutorials.1,P19-1334,1,0.821801,"s in NLP. The tutorial will cover the main lines of analysis work, mostly drawing on the recent TACL survey by Belinkov and Glass (2019).1 In particular, we will devote a large portion to work aiming to find linguistic information that is captured by neural networks, such as probing classifiers (Hupkes et al., 2018; Adi et al., 2017; Conneau et al., 2018a,b; Tenney et al., 2019b, inter alia), controlled behavior studies on language modelling (Gulordava et al., 2018; Linzen et al., 2016a; Marvin and Linzen, 2018) or inference tasks (Poliak et al., 2018a,b; White et al., 2017; Kim et al., 2019; McCoy et al., 2019; Ross and Pavlick, 2019), psycholinguistic methods (Ettinger et al., 2018; Chrupała and Alishahi, 2019), layerwise analyses (Peters et al., 2018; Tenney et al., 2019a), among other methods (Hewitt and Manning, 2019; Zhang While deep learning has transformed the natural language processing (NLP) field and impacted the larger computational linguistics community, the rise of neural networks is stained by their opaque nature: It is challenging to interpret the inner workings of neural network models, and explicate their behavior. Therefore, in the last few years, an increasingly large body of wor"
2020.acl-tutorials.1,N19-1419,0,0.0199094,"inguistic information that is captured by neural networks, such as probing classifiers (Hupkes et al., 2018; Adi et al., 2017; Conneau et al., 2018a,b; Tenney et al., 2019b, inter alia), controlled behavior studies on language modelling (Gulordava et al., 2018; Linzen et al., 2016a; Marvin and Linzen, 2018) or inference tasks (Poliak et al., 2018a,b; White et al., 2017; Kim et al., 2019; McCoy et al., 2019; Ross and Pavlick, 2019), psycholinguistic methods (Ettinger et al., 2018; Chrupała and Alishahi, 2019), layerwise analyses (Peters et al., 2018; Tenney et al., 2019a), among other methods (Hewitt and Manning, 2019; Zhang While deep learning has transformed the natural language processing (NLP) field and impacted the larger computational linguistics community, the rise of neural networks is stained by their opaque nature: It is challenging to interpret the inner workings of neural network models, and explicate their behavior. Therefore, in the last few years, an increasingly large body of work has been devoted to the analysis and interpretation of neural network models in NLP. This body of work is so far lacking a common framework and methodology. Moreover, approaching the analysis of modern neural netw"
2020.acl-tutorials.1,D17-1215,0,0.0247434,"t et al., 2018), and saliency measures (Li et al., 2016; Murdoch et al., 2018; Arras et al., 2017), including a walkthrough on how to build a simple attention visualization. Next, we will discuss the construction and use of challenge sets for fine-grained evaluation in the context of different tasks (Conneau and Kiela, 2018; Wang et al., 2018; Isabelle and Kuhn, 2018; Sennrich, 2017, inter alia). Finally, we will review work on generating adversarial examples in NLP, focusing on the challenges brought upon by the discrete nature of textual input (Papernot et al., 2016b; Ebrahimi et al., 2018; Jia and Liang, 2017; Belinkov and Bisk, 2018, inter alia). A detailed outline is provided in Section 3. Throughout the tutorial, we will highlight not only the most commonly applied analysis methods, but also the specific limitations and shortcomings of current approaches. By the end of the tutorial, participants will be better informed where to focus future research efforts. 2 5. Other Methods (a) Generating Explanations (b) Psycholinguistic Methods (c) Testing on Formal Languages 6. Conclusion 4 We would assume acquaintance with core linguistic concepts and basic knowledge of machine learning and neural networ"
2020.acl-tutorials.1,D18-1179,0,0.0175049,"rticular, we will devote a large portion to work aiming to find linguistic information that is captured by neural networks, such as probing classifiers (Hupkes et al., 2018; Adi et al., 2017; Conneau et al., 2018a,b; Tenney et al., 2019b, inter alia), controlled behavior studies on language modelling (Gulordava et al., 2018; Linzen et al., 2016a; Marvin and Linzen, 2018) or inference tasks (Poliak et al., 2018a,b; White et al., 2017; Kim et al., 2019; McCoy et al., 2019; Ross and Pavlick, 2019), psycholinguistic methods (Ettinger et al., 2018; Chrupała and Alishahi, 2019), layerwise analyses (Peters et al., 2018; Tenney et al., 2019a), among other methods (Hewitt and Manning, 2019; Zhang While deep learning has transformed the natural language processing (NLP) field and impacted the larger computational linguistics community, the rise of neural networks is stained by their opaque nature: It is challenging to interpret the inner workings of neural network models, and explicate their behavior. Therefore, in the last few years, an increasingly large body of work has been devoted to the analysis and interpretation of neural network models in NLP. This body of work is so far lacking a common framework and"
2020.acl-tutorials.1,W18-5446,0,0.0656659,"Missing"
2020.acl-tutorials.1,N18-2082,1,0.884697,"Missing"
2020.acl-tutorials.1,I17-1100,0,0.0383709,"Missing"
2020.acl-tutorials.1,W18-5441,1,0.841572,"Missing"
2020.acl-tutorials.1,W18-5448,0,0.0639211,"Missing"
2020.acl-tutorials.1,D19-1228,1,0.833586,"al will cover the main lines of analysis work, mostly drawing on the recent TACL survey by Belinkov and Glass (2019).1 In particular, we will devote a large portion to work aiming to find linguistic information that is captured by neural networks, such as probing classifiers (Hupkes et al., 2018; Adi et al., 2017; Conneau et al., 2018a,b; Tenney et al., 2019b, inter alia), controlled behavior studies on language modelling (Gulordava et al., 2018; Linzen et al., 2016a; Marvin and Linzen, 2018) or inference tasks (Poliak et al., 2018a,b; White et al., 2017; Kim et al., 2019; McCoy et al., 2019; Ross and Pavlick, 2019), psycholinguistic methods (Ettinger et al., 2018; Chrupała and Alishahi, 2019), layerwise analyses (Peters et al., 2018; Tenney et al., 2019a), among other methods (Hewitt and Manning, 2019; Zhang While deep learning has transformed the natural language processing (NLP) field and impacted the larger computational linguistics community, the rise of neural networks is stained by their opaque nature: It is challenging to interpret the inner workings of neural network models, and explicate their behavior. Therefore, in the last few years, an increasingly large body of work has been devoted to the"
2020.acl-tutorials.1,E17-2060,0,0.14015,"ties and Bowman, 2018; Shi et al., 2016). We will also present various interactive visualization methods such as neuron activations (Karpathy et al., 2015; Dalvi et al., 2019), attention mechanisms (Bahdanau et al., 2014; Strobelt et al., 2018), and saliency measures (Li et al., 2016; Murdoch et al., 2018; Arras et al., 2017), including a walkthrough on how to build a simple attention visualization. Next, we will discuss the construction and use of challenge sets for fine-grained evaluation in the context of different tasks (Conneau and Kiela, 2018; Wang et al., 2018; Isabelle and Kuhn, 2018; Sennrich, 2017, inter alia). Finally, we will review work on generating adversarial examples in NLP, focusing on the challenges brought upon by the discrete nature of textual input (Papernot et al., 2016b; Ebrahimi et al., 2018; Jia and Liang, 2017; Belinkov and Bisk, 2018, inter alia). A detailed outline is provided in Section 3. Throughout the tutorial, we will highlight not only the most commonly applied analysis methods, but also the specific limitations and shortcomings of current approaches. By the end of the tutorial, participants will be better informed where to focus future research efforts. 2 5. O"
2020.acl-tutorials.1,D16-1159,0,0.0290059,"rafted features, it is more challenging to interpret the 1 A comprehensive bibliography is found in the accompanying website of the survey: https://boknilev. github.io/nlp-analysis-methods/. 1 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1–5 c July 5, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 (a) How Interaction can help and its limitations (b) Classification and Review of Related Efforts (c) Demo Walk-through: Simple Attention Visualization (d) Broader Perspectives and Opportunities and Bowman, 2018; Shi et al., 2016). We will also present various interactive visualization methods such as neuron activations (Karpathy et al., 2015; Dalvi et al., 2019), attention mechanisms (Bahdanau et al., 2014; Strobelt et al., 2018), and saliency measures (Li et al., 2016; Murdoch et al., 2018; Arras et al., 2017), including a walkthrough on how to build a simple attention visualization. Next, we will discuss the construction and use of challenge sets for fine-grained evaluation in the context of different tasks (Conneau and Kiela, 2018; Wang et al., 2018; Isabelle and Kuhn, 2018; Sennrich, 2017, inter alia). Finally, we"
2020.acl-tutorials.1,P19-1452,1,0.914012,"it requires both a familiarity with recent work in neural NLP and with analysis methods which are not yet standardized. This tutorial aims to fill this gap and introduce the nascent field of interpretability and analysis of neural networks in NLP. The tutorial will cover the main lines of analysis work, mostly drawing on the recent TACL survey by Belinkov and Glass (2019).1 In particular, we will devote a large portion to work aiming to find linguistic information that is captured by neural networks, such as probing classifiers (Hupkes et al., 2018; Adi et al., 2017; Conneau et al., 2018a,b; Tenney et al., 2019b, inter alia), controlled behavior studies on language modelling (Gulordava et al., 2018; Linzen et al., 2016a; Marvin and Linzen, 2018) or inference tasks (Poliak et al., 2018a,b; White et al., 2017; Kim et al., 2019; McCoy et al., 2019; Ross and Pavlick, 2019), psycholinguistic methods (Ettinger et al., 2018; Chrupała and Alishahi, 2019), layerwise analyses (Peters et al., 2018; Tenney et al., 2019a), among other methods (Hewitt and Manning, 2019; Zhang While deep learning has transformed the natural language processing (NLP) field and impacted the larger computational linguistics community"
2020.blackboxnlp-1.4,N18-2017,0,0.049976,"Missing"
2020.blackboxnlp-1.4,W18-5512,0,0.0170279,"Similarity Analysis (RSA; Laakso and Cottrell, 2000; Kriegeskorte et al., 2008) to study the effect of context on encoder representations, while Chrupała and Alishahi (2019) correlated them with syntax. Fine-tuning Comparatively few analyses have focused on understanding the fine-tuning process. Initial studies of fine-tuned encoders have shown state-of-the-art performance on benchmark suites such as GLUE (Wang et al., 2019) and surprising sample efficiency (Peters et al., 2018a). However, behavioral studies with challenge sets (McCoy et al., 2019b; Poliak et al., 2018; Ettinger et al., 2018; Kim et al., 2018) have shown limited ability to generalize to out-of-domain data and across syntactic perturbations. van Aken et al. (2019) focused on question-answering models with taskspecific probes. Peters et al. (2019) analyzed the effects of fine-tuning with respect to the performance of diagnostic classifiers. Gauthier and Levy (2019) studied fine-tuning via RSA, finding a significant divergence between the representations of models fine-tuned on different tasks. Concurrent work by Tamkin et al. (2020) investigated the transferability of pre-trained language models and performed an number of layer ablat"
2020.blackboxnlp-1.4,2020.acl-main.659,0,0.0339969,"Missing"
2020.blackboxnlp-1.4,D19-1445,0,0.0327155,"f pre-trained language models and performed an number of layer ablations. Consistent with our observations in Section 5.2, they find Related Work Base model Many recent papers have focused on understanding sentence encoders such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2019), focusing primarily on the “innate” abilities of the pre-trained (“Base”) models. For example, analyses of attention weights have shown interpretable patterns (Coenen et al., 2019; Vig and Belinkov, 2019; Voita et al., 2019b; Hoover et al., 2019) and found strong correlations to syntax (Clark et al., 2019). Kovaleva et al. (2019) also saw that fine-tuning mainly changes the attention of the last few layers, consistent with our findings in Section 5.1. However, other studies have cast doubt on what conclusions can be drawn from attention patterns (Jain and Wallace, 2019; Serrano and Smith, 2019; Brunner et al., 2019). More generally, supervised probing models and diagnostic classifiers make few assumptions be1 See Belinkov and Glass (2019) and Rogers et al. (2020) for a survey of probing methods. 34 differences in which layers are important for finetuning different tasks. However, none of the prior provides a comprehen"
2020.blackboxnlp-1.4,S10-1006,0,0.0210321,"Missing"
2020.blackboxnlp-1.4,D19-1275,0,0.0314155,"Missing"
2020.blackboxnlp-1.4,N19-1112,0,0.0377366,"ependency parsing involves deeper changes to the encoder. We confirm this by partial-freezing experiments which test how many layers need to change to do well on each task and relate this to an estimate of task difficulty (with respect to the pre-training regime) via layer ablations. Finally, we observe that fine-tuning induces large changes on in-domain examples, yet on out-of-domain sentences, the representations more closely resemble those of the pre-trained model. 2 yond the existence of model activations and can test for the presence of a wide variety of phenomena. Tenney et al. (2019b); Liu et al. (2019); Peters et al. (2018b) introduced task suites that probe for high-level linguistic phenomena such as partof-speech, entity types, and coreference, while Tenney et al. (2019a) showed that these phenomena are represented in a hierarchical order within the layers of BERT. Hewitt and Manning (2019) used a geometrically-motivated probe to explore syntactic structures, and Voita and Titov (2020) and Pimentel et al. (2020) designed informationtheoretic techniques that can measure the model and data complexity.1 While probing models depend on labelled data, parallel work has studied the same encoders"
2020.blackboxnlp-1.4,N19-1357,0,0.0384539,"et al., 2018a) and BERT (Devlin et al., 2019), focusing primarily on the “innate” abilities of the pre-trained (“Base”) models. For example, analyses of attention weights have shown interpretable patterns (Coenen et al., 2019; Vig and Belinkov, 2019; Voita et al., 2019b; Hoover et al., 2019) and found strong correlations to syntax (Clark et al., 2019). Kovaleva et al. (2019) also saw that fine-tuning mainly changes the attention of the last few layers, consistent with our findings in Section 5.1. However, other studies have cast doubt on what conclusions can be drawn from attention patterns (Jain and Wallace, 2019; Serrano and Smith, 2019; Brunner et al., 2019). More generally, supervised probing models and diagnostic classifiers make few assumptions be1 See Belinkov and Glass (2019) and Rogers et al. (2020) for a survey of probing methods. 34 differences in which layers are important for finetuning different tasks. However, none of the prior provides a comprehensive analysis of what happens to the internal representations of the BERT model. In our work, we find that by comparing the Base to the fine-tuned models either via probing, RSA, and layer ablations provides novel insights about this additional"
2020.blackboxnlp-1.4,P19-1334,1,0.897784,"ntations from the pretrained models and have shown that they encode a wide variety of linguistic phenomena (Tenney et al., 2019b; Liu et al., 2019). However, in the standard recipe for models such as BERT (Devlin et al., 2019), after initializing with pre-trained weights, they are then trained for a few epochs on a supervised dataset. Considerably less is understood about what happens during this fine-tuning stage. Current understanding is based largely on the models’ performance. While fine-tuned Transformers achieve state-ofthe-art accuracy, they also can end up learning shallow heuristics (McCoy et al., 2019b; Gururangan et al., 2018; Poliak et al., 2018), suggesting a disconnect between the richness of features learned from pre-training and those used by finetuned models. Thus, in this work, we seek to understand how the internals of the model–the representation space–change when fine-tuned for downstream tasks. We focus on three widelyused NLP tasks: dependency parsing, natural language inference (MNLI), and reading comprehension (SQuAD), and ask: While much recent work has examined how linguistic information is encoded in pretrained sentence representations, comparatively little is understood"
2020.blackboxnlp-1.4,D17-1215,0,0.17055,"on, we pass ordinary sentences (Wikipedia), sentence-pairs (MNLI), or questionanswer pairs (SQuAD) as inputs to the BERT model, and select a random sample (n = 5000) of Overall, our results suggest that linguistic features are still available, and that the fine-tuning process does not lead to catastrophic forgetting. Nonetheless, behavioral analyses have shown that finetuned models can still fail to leverage even simple syntactic knowledge in their predictions (McCoy et al., 2019b,a; Min et al., 2020), and may instead rely on annotation artifacts (Gururangan et al., 2018) or pattern matching (Jia and Liang, 2017). This suggests that the changes from fine-tuning are conservative: rich features are still present even if the model ends up finding a naive, simple solution. 5 Representational Similarity Analysis Where do the representations change? The supervised probes from the previous section are highly targeted: as trained models, they are sensitive to particular linguistic phenomena, but 37 put. To first order, this may be a result of optimization: vanishing gradients result in the most change in the layers closest to the loss. Yet we interestingly do observe significant differences between tasks. For"
2020.blackboxnlp-1.4,2020.acl-main.212,0,0.0218784,"he Pearson correlation between the flattened upper triangulars of the two similarity matrices. In our application, we pass ordinary sentences (Wikipedia), sentence-pairs (MNLI), or questionanswer pairs (SQuAD) as inputs to the BERT model, and select a random sample (n = 5000) of Overall, our results suggest that linguistic features are still available, and that the fine-tuning process does not lead to catastrophic forgetting. Nonetheless, behavioral analyses have shown that finetuned models can still fail to leverage even simple syntactic knowledge in their predictions (McCoy et al., 2019b,a; Min et al., 2020), and may instead rely on annotation artifacts (Gururangan et al., 2018) or pattern matching (Jia and Liang, 2017). This suggests that the changes from fine-tuning are conservative: rich features are still present even if the model ends up finding a naive, simple solution. 5 Representational Similarity Analysis Where do the representations change? The supervised probes from the previous section are highly targeted: as trained models, they are sensitive to particular linguistic phenomena, but 37 put. To first order, this may be a result of optimization: vanishing gradients result in the most ch"
2020.blackboxnlp-1.4,D16-1264,0,0.0569722,"l representations encode various linguistic phenomena, including part-of-speech, entity typing, and coreference. We use the tasks and parameters of Tenney et al. (2019b), which uses a two-layer MLP to predict edge and span labels from frozen encoder representations.4 As we are interested in whether the linguistic knowledge is retained by the model overall, we utilize the mix version of the edge probes, which takes as input a learned scalar mixing of the representations from every layer.5 After training, we report the microaveraged F1 scores on a held-out test set. SQuAD The SQuADv1.1 dataset (Rajpurkar et al., 2016) contains over 100K crowd-sourced question-answer pairs, created from a set of Wikipedia articles. We fine-tune BERT using the architecture and parameters of Devlin et al. (2019), which uses two independent softmax layers to predict the start and end tokens of the answer span. Our average F1 score is 89.2 ± 0.2, slightly higher than the published 88.5. Dependency Parsing We also introduce a BERT model fine-tuned on dependency parsing (Dep). We include this task to present a contrasting perspective from the prior two datasets, since prior research has suggested that much of the information need"
2020.blackboxnlp-1.4,2020.tacl-1.54,0,0.0500988,"Missing"
2020.blackboxnlp-1.4,N18-1202,0,0.418368,"nvolves deeper changes to the encoder. We confirm this by partial-freezing experiments which test how many layers need to change to do well on each task and relate this to an estimate of task difficulty (with respect to the pre-training regime) via layer ablations. Finally, we observe that fine-tuning induces large changes on in-domain examples, yet on out-of-domain sentences, the representations more closely resemble those of the pre-trained model. 2 yond the existence of model activations and can test for the presence of a wide variety of phenomena. Tenney et al. (2019b); Liu et al. (2019); Peters et al. (2018b) introduced task suites that probe for high-level linguistic phenomena such as partof-speech, entity types, and coreference, while Tenney et al. (2019a) showed that these phenomena are represented in a hierarchical order within the layers of BERT. Hewitt and Manning (2019) used a geometrically-motivated probe to explore syntactic structures, and Voita and Titov (2020) and Pimentel et al. (2020) designed informationtheoretic techniques that can measure the model and data complexity.1 While probing models depend on labelled data, parallel work has studied the same encoders using unsupervised t"
2020.blackboxnlp-1.4,N19-1329,0,0.0246077,"these phenomena are represented in a hierarchical order within the layers of BERT. Hewitt and Manning (2019) used a geometrically-motivated probe to explore syntactic structures, and Voita and Titov (2020) and Pimentel et al. (2020) designed informationtheoretic techniques that can measure the model and data complexity.1 While probing models depend on labelled data, parallel work has studied the same encoders using unsupervised techniques. Voita et al. (2019a) used a form of canonical correlation analysis (PWCCA; Morcos et al., 2018) to study the layerwise evolution of representations, while Saphra and Lopez (2019) explored how these representations evolve during training. Abnar et al. (2019) used Representational Similarity Analysis (RSA; Laakso and Cottrell, 2000; Kriegeskorte et al., 2008) to study the effect of context on encoder representations, while Chrupała and Alishahi (2019) correlated them with syntax. Fine-tuning Comparatively few analyses have focused on understanding the fine-tuning process. Initial studies of fine-tuned encoders have shown state-of-the-art performance on benchmark suites such as GLUE (Wang et al., 2019) and surprising sample efficiency (Peters et al., 2018a). However, beh"
2020.blackboxnlp-1.4,P19-1282,0,0.0193324,"(Devlin et al., 2019), focusing primarily on the “innate” abilities of the pre-trained (“Base”) models. For example, analyses of attention weights have shown interpretable patterns (Coenen et al., 2019; Vig and Belinkov, 2019; Voita et al., 2019b; Hoover et al., 2019) and found strong correlations to syntax (Clark et al., 2019). Kovaleva et al. (2019) also saw that fine-tuning mainly changes the attention of the last few layers, consistent with our findings in Section 5.1. However, other studies have cast doubt on what conclusions can be drawn from attention patterns (Jain and Wallace, 2019; Serrano and Smith, 2019; Brunner et al., 2019). More generally, supervised probing models and diagnostic classifiers make few assumptions be1 See Belinkov and Glass (2019) and Rogers et al. (2020) for a survey of probing methods. 34 differences in which layers are important for finetuning different tasks. However, none of the prior provides a comprehensive analysis of what happens to the internal representations of the BERT model. In our work, we find that by comparing the Base to the fine-tuned models either via probing, RSA, and layer ablations provides novel insights about this additional phase of training. 3 wit"
2020.blackboxnlp-1.4,D18-1179,0,0.130957,"nvolves deeper changes to the encoder. We confirm this by partial-freezing experiments which test how many layers need to change to do well on each task and relate this to an estimate of task difficulty (with respect to the pre-training regime) via layer ablations. Finally, we observe that fine-tuning induces large changes on in-domain examples, yet on out-of-domain sentences, the representations more closely resemble those of the pre-trained model. 2 yond the existence of model activations and can test for the presence of a wide variety of phenomena. Tenney et al. (2019b); Liu et al. (2019); Peters et al. (2018b) introduced task suites that probe for high-level linguistic phenomena such as partof-speech, entity types, and coreference, while Tenney et al. (2019a) showed that these phenomena are represented in a hierarchical order within the layers of BERT. Hewitt and Manning (2019) used a geometrically-motivated probe to explore syntactic structures, and Voita and Titov (2020) and Pimentel et al. (2020) designed informationtheoretic techniques that can measure the model and data complexity.1 While probing models depend on labelled data, parallel work has studied the same encoders using unsupervised t"
2020.blackboxnlp-1.4,silveira-etal-2014-gold,0,0.0890095,"Missing"
2020.blackboxnlp-1.4,W19-4302,0,0.0377682,"al., 2019), focusing primarily on the “innate” abilities of the pre-trained (“Base”) models. For example, analyses of attention weights have shown interpretable patterns (Coenen et al., 2019; Vig and Belinkov, 2019; Voita et al., 2019b; Hoover et al., 2019) and found strong correlations to syntax (Clark et al., 2019). Kovaleva et al. (2019) also saw that fine-tuning mainly changes the attention of the last few layers, consistent with our findings in Section 5.1. However, other studies have cast doubt on what conclusions can be drawn from attention patterns (Jain and Wallace, 2019; Serrano and Smith, 2019; Brunner et al., 2019). More generally, supervised probing models and diagnostic classifiers make few assumptions be1 See Belinkov and Glass (2019) and Rogers et al. (2020) for a survey of probing methods. 34 differences in which layers are important for finetuning different tasks. However, none of the prior provides a comprehensive analysis of what happens to the internal representations of the BERT model. In our work, we find that by comparing the Base to the fine-tuned models either via probing, RSA, and layer ablations provides novel insights about this additional phase of training. 3 wit"
2020.blackboxnlp-1.4,2020.findings-emnlp.125,0,0.0204421,"behavioral studies with challenge sets (McCoy et al., 2019b; Poliak et al., 2018; Ettinger et al., 2018; Kim et al., 2018) have shown limited ability to generalize to out-of-domain data and across syntactic perturbations. van Aken et al. (2019) focused on question-answering models with taskspecific probes. Peters et al. (2019) analyzed the effects of fine-tuning with respect to the performance of diagnostic classifiers. Gauthier and Levy (2019) studied fine-tuning via RSA, finding a significant divergence between the representations of models fine-tuned on different tasks. Concurrent work by Tamkin et al. (2020) investigated the transferability of pre-trained language models and performed an number of layer ablations. Consistent with our observations in Section 5.2, they find Related Work Base model Many recent papers have focused on understanding sentence encoders such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2019), focusing primarily on the “innate” abilities of the pre-trained (“Base”) models. For example, analyses of attention weights have shown interpretable patterns (Coenen et al., 2019; Vig and Belinkov, 2019; Voita et al., 2019b; Hoover et al., 2019) and found strong correlatio"
2020.blackboxnlp-1.4,2020.acl-main.420,0,0.021407,"ons more closely resemble those of the pre-trained model. 2 yond the existence of model activations and can test for the presence of a wide variety of phenomena. Tenney et al. (2019b); Liu et al. (2019); Peters et al. (2018b) introduced task suites that probe for high-level linguistic phenomena such as partof-speech, entity types, and coreference, while Tenney et al. (2019a) showed that these phenomena are represented in a hierarchical order within the layers of BERT. Hewitt and Manning (2019) used a geometrically-motivated probe to explore syntactic structures, and Voita and Titov (2020) and Pimentel et al. (2020) designed informationtheoretic techniques that can measure the model and data complexity.1 While probing models depend on labelled data, parallel work has studied the same encoders using unsupervised techniques. Voita et al. (2019a) used a form of canonical correlation analysis (PWCCA; Morcos et al., 2018) to study the layerwise evolution of representations, while Saphra and Lopez (2019) explored how these representations evolve during training. Abnar et al. (2019) used Representational Similarity Analysis (RSA; Laakso and Cottrell, 2000; Kriegeskorte et al., 2008) to study the effect of conte"
2020.blackboxnlp-1.4,D17-3004,0,0.0601326,"Missing"
2020.blackboxnlp-1.4,S18-2023,0,0.05929,"Missing"
2020.blackboxnlp-1.4,P19-1452,1,0.129413,"layers or are there changes throughout the model? (Section 5) • Do these changes generalize or does the new behavior only apply to the specific domain on which fine-tuning occurred? (Section 6) ⇤ Work done as member of the Google AI Residency program https://ai.google/research/ join-us/ai-residency/ 33 Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 33–44 c Online, November 20, 2020. 2020 Association for Computational Linguistics We approach these questions with three complementary analysis techniques. Supervised probing classifiers (Tenney et al., 2019b; Hewitt and Manning, 2019; Voita and Titov, 2020) provide a means of explicitly testing for the presence of pre-specified linguistic phenomena, while Representational Similarity Analysis (RSA; Kriegeskorte et al., 2008) gives a task-agnostic measurement of the change in model activations. Finally, we corroborate our results with two types of layerbased ablations–truncation and partial freezing– and measure their effect on end-task performance. Taken together, we conclude that fine-tuning involves primarily shallow model changes, evidenced by three specific observations. First, linguistic fea"
2020.blackboxnlp-1.4,2020.acl-demos.15,1,0.89448,"Missing"
2020.blackboxnlp-1.4,W19-4808,0,0.0167452,"resentations of models fine-tuned on different tasks. Concurrent work by Tamkin et al. (2020) investigated the transferability of pre-trained language models and performed an number of layer ablations. Consistent with our observations in Section 5.2, they find Related Work Base model Many recent papers have focused on understanding sentence encoders such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2019), focusing primarily on the “innate” abilities of the pre-trained (“Base”) models. For example, analyses of attention weights have shown interpretable patterns (Coenen et al., 2019; Vig and Belinkov, 2019; Voita et al., 2019b; Hoover et al., 2019) and found strong correlations to syntax (Clark et al., 2019). Kovaleva et al. (2019) also saw that fine-tuning mainly changes the attention of the last few layers, consistent with our findings in Section 5.1. However, other studies have cast doubt on what conclusions can be drawn from attention patterns (Jain and Wallace, 2019; Serrano and Smith, 2019; Brunner et al., 2019). More generally, supervised probing models and diagnostic classifiers make few assumptions be1 See Belinkov and Glass (2019) and Rogers et al. (2020) for a survey of probing metho"
2020.blackboxnlp-1.4,D19-1448,0,0.151971,"ed task suites that probe for high-level linguistic phenomena such as partof-speech, entity types, and coreference, while Tenney et al. (2019a) showed that these phenomena are represented in a hierarchical order within the layers of BERT. Hewitt and Manning (2019) used a geometrically-motivated probe to explore syntactic structures, and Voita and Titov (2020) and Pimentel et al. (2020) designed informationtheoretic techniques that can measure the model and data complexity.1 While probing models depend on labelled data, parallel work has studied the same encoders using unsupervised techniques. Voita et al. (2019a) used a form of canonical correlation analysis (PWCCA; Morcos et al., 2018) to study the layerwise evolution of representations, while Saphra and Lopez (2019) explored how these representations evolve during training. Abnar et al. (2019) used Representational Similarity Analysis (RSA; Laakso and Cottrell, 2000; Kriegeskorte et al., 2008) to study the effect of context on encoder representations, while Chrupała and Alishahi (2019) correlated them with syntax. Fine-tuning Comparatively few analyses have focused on understanding the fine-tuning process. Initial studies of fine-tuned encoders ha"
2020.blackboxnlp-1.4,P19-1580,0,0.179514,"ed task suites that probe for high-level linguistic phenomena such as partof-speech, entity types, and coreference, while Tenney et al. (2019a) showed that these phenomena are represented in a hierarchical order within the layers of BERT. Hewitt and Manning (2019) used a geometrically-motivated probe to explore syntactic structures, and Voita and Titov (2020) and Pimentel et al. (2020) designed informationtheoretic techniques that can measure the model and data complexity.1 While probing models depend on labelled data, parallel work has studied the same encoders using unsupervised techniques. Voita et al. (2019a) used a form of canonical correlation analysis (PWCCA; Morcos et al., 2018) to study the layerwise evolution of representations, while Saphra and Lopez (2019) explored how these representations evolve during training. Abnar et al. (2019) used Representational Similarity Analysis (RSA; Laakso and Cottrell, 2000; Kriegeskorte et al., 2008) to study the effect of context on encoder representations, while Chrupała and Alishahi (2019) correlated them with syntax. Fine-tuning Comparatively few analyses have focused on understanding the fine-tuning process. Initial studies of fine-tuned encoders ha"
2020.blackboxnlp-1.4,W18-5448,0,0.0724552,"Missing"
2020.blackboxnlp-1.4,2020.emnlp-main.14,0,0.140673,"l? (Section 5) • Do these changes generalize or does the new behavior only apply to the specific domain on which fine-tuning occurred? (Section 6) ⇤ Work done as member of the Google AI Residency program https://ai.google/research/ join-us/ai-residency/ 33 Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 33–44 c Online, November 20, 2020. 2020 Association for Computational Linguistics We approach these questions with three complementary analysis techniques. Supervised probing classifiers (Tenney et al., 2019b; Hewitt and Manning, 2019; Voita and Titov, 2020) provide a means of explicitly testing for the presence of pre-specified linguistic phenomena, while Representational Similarity Analysis (RSA; Kriegeskorte et al., 2008) gives a task-agnostic measurement of the change in model activations. Finally, we corroborate our results with two types of layerbased ablations–truncation and partial freezing– and measure their effect on end-task performance. Taken together, we conclude that fine-tuning involves primarily shallow model changes, evidenced by three specific observations. First, linguistic features are not lost during fine-tuning but tasks can"
2020.blackboxnlp-1.4,N18-1101,0,0.17032,"e features are forgotten entirely or simply are not always used. We explore this with supervised probing techniques, using edge probing (Tenney et al., 2019b) and structural probes (Hewitt and Manning, 2019) to explore how well linguistic information can be recovered from the fine-tuned model. Experimental Setup BERT We focus on the popular BERT model (Devlin et al., 2019), focusing on the 12-layer base uncased variant.2 We denote the pretrained model as Base and refer to fine-tuned versions by the name of the task. MNLI A common benchmark for natural language understanding, the MNLI dataset (Williams et al., 2018) contains over 433K sentence pairs annotated with textual entailment information. We fine-tune BERT using the architecture and parameters of Devlin et al. (2019), using a softmax layer on [CLS] representation to predict the output label. Across three trials, the evaluation accuracy of our BERT Base model is 83.3 ± 0.1, slightly lower but comparable to the published score of 84.6. Edge Probing Edge probing aims to measure how contextual representations encode various linguistic phenomena, including part-of-speech, entity typing, and coreference. We use the tasks and parameters of Tenney et al."
2020.blackboxnlp-1.4,K17-3001,0,\N,Missing
2020.blackboxnlp-1.4,D18-1151,0,\N,Missing
2020.blackboxnlp-1.4,Q19-1004,0,\N,Missing
2020.blackboxnlp-1.4,P19-1283,0,\N,Missing
2020.blackboxnlp-1.4,N19-1419,0,\N,Missing
2020.blackboxnlp-1.4,P19-1356,0,\N,Missing
2020.blackboxnlp-1.4,N19-1423,0,\N,Missing
2020.blackboxnlp-1.4,D19-1050,0,\N,Missing
2020.blackboxnlp-1.4,D19-1424,0,\N,Missing
2020.blackboxnlp-1.4,2020.tacl-1.3,0,\N,Missing
2020.emnlp-main.335,C12-1118,0,0.0356624,"nd leftleaning queries. Red = right-leaning news sources; blue = left-leaning; gray = nonpartisan or apolitical. α-nDCG @10 @100 Vpretrained Vdeno Vconno 0.907 0.922 0.904 0.915 0.944 0.914 Gini L R P@10 0.215 0.160 0.147 0.207 0.080 0.153 0.78 0.37 0.64 Table 6: Retrieval metrics. For α-nDCG, higher means more diverse; for Gini, lower means more diverse. 7 Related Work Embedding Augmentation. At the lexical level, there is substantial literature that supplements pretrained representations with desired information (Faruqui et al., 2015; Bamman et al., 2014) or improves their interpretability (Murphy et al., 2012; Arora et al., 2018; Lauretig, 2019). However, existing works tend to focus on evaluating the dictionary definitions of words, less so on grounding words to specific real world referents and, to our knowledge, no major attempt yet in interpreting and manipulating the denotation and connotation dimensions of meaning as suggested by the semantic theories discussed in §2. While we do not claim to do full justice to conceptual role semantics either, this paper furnishes a first attempt at implementing a school of semantics introduced by philosophers of language and increasingly popular among cogn"
2020.emnlp-main.335,P19-1273,0,0.0207727,"Missing"
2020.emnlp-main.335,P18-1022,0,0.0187462,"nformation retrieval, and social science research communities. DoriHacohen et al. (2015) discuss the challenges of deploying information retrieval systems in controversial domains, and Puschmann (2019) looks specifically at the effects of search personalization on election-related information. Many approaches have been proposed to improve the diversity of search results, typically by identifying search facets a priori and then training a model to optimize for diversity (Tintarev et al., 2018; Tabrizi and Shakery, 2019; Lunardi, 2019). In terms of linguistic analyses, Rashkin et al. (2017) and Potthast et al. (2018) analyze stylistic patterns that distinguish fake news from real news. Duseja and Jhamtani (2019) study linguistic patterns that distinguish whether individuals are within social media echo chambers. 8 Summary In this paper, we describe the problem of pretrained word embeddings conflating denotation and connotation. We address this issue by introducing an adversarial network that explicitly represents the two properties as two different vector spaces. We confirm that our decomposed spaces encode the desired structure of denotation or connotation by both quantitatively measuring their homogenei"
2020.emnlp-main.335,P17-1068,0,0.0443698,"Missing"
2020.emnlp-main.335,2020.acl-demos.14,0,0.0217762,"Missing"
2020.emnlp-main.335,N19-1088,0,0.026943,"rld referents and, to our knowledge, no major attempt yet in interpreting and manipulating the denotation and connotation dimensions of meaning as suggested by the semantic theories discussed in §2. While we do not claim to do full justice to conceptual role semantics either, this paper furnishes a first attempt at implementing a school of semantics introduced by philosophers of language and increasingly popular among cognitive scientists. Style Transfer. At the sentence level, adversarial setups similar to ours have been previously explored for differentiating style and content. For example, Romanov et al. (2019); Yamshchikov et al. (2019); John et al. (2019) converted informal English to formal English and Yelp reviews from positive to negative sentiment. The motivation for such models is primarily natural language generation and the personalization thereof (Li et al., 2016). Additionally, our framing in terms of Frege’s sense and reference adds clarity to the sometimes illdefined problems explored in style transfer (e.g., treating sentiment as “style”). For example, “she is an undocumented immigrant” and “she is an illegal alien” have the same truth conditions but different connotations, whereas “th"
2020.emnlp-main.335,D13-1010,0,0.0114362,"ns, whereas “the cafe is great” and “the cafe is terrible” have different truth conditions. Modeling Political Language. There is a wealth of work on computational approaches for modeling political language (Glavaˇs et al., 2019). Within NLP, such efforts tend to focus more on describing how language differs between political subgroups, rather than recognizing similarities in denotation across ideological stances, which is the primary goal of our work. For example, Preot¸iuc-Pietro et al. (2017); Han et al. (2019) attempt to predict a person’s political ideology from their social media posts, Sim et al. (2013) detect ideological trends present in political speeches, Fulgoni et al. (2016) predict political leaning of news articles, and Pad´o et al. (2019) focuses on modeling the network structure of policy debates within society. Also highly related is work analyzing linguistic framing in news (Greene and Resnik, 2009; Choi et al., 2012; Baumer et al., 2015). Echo Chambers and Search. The dangers of ideological “echo chambers” have received significant attention across NLP, information retrieval, and social science research communities. DoriHacohen et al. (2015) discuss the challenges of deploying i"
2020.emnlp-main.335,D19-5613,0,0.0163816,"ur knowledge, no major attempt yet in interpreting and manipulating the denotation and connotation dimensions of meaning as suggested by the semantic theories discussed in §2. While we do not claim to do full justice to conceptual role semantics either, this paper furnishes a first attempt at implementing a school of semantics introduced by philosophers of language and increasingly popular among cognitive scientists. Style Transfer. At the sentence level, adversarial setups similar to ours have been previously explored for differentiating style and content. For example, Romanov et al. (2019); Yamshchikov et al. (2019); John et al. (2019) converted informal English to formal English and Yelp reviews from positive to negative sentiment. The motivation for such models is primarily natural language generation and the personalization thereof (Li et al., 2016). Additionally, our framing in terms of Frege’s sense and reference adds clarity to the sometimes illdefined problems explored in style transfer (e.g., treating sentiment as “style”). For example, “she is an undocumented immigrant” and “she is an illegal alien” have the same truth conditions but different connotations, whereas “the cafe is great” and “the c"
2020.starsem-1.16,D19-1065,0,0.0279894,"ebert/nbc 2 Our university namesake, plus paying homage to important Brown corpora in both NLP (Francis and Kucera, 1979) and Child Language Acquisition (Brown, 1973). 143 Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics (*SEM), pages 143–153 Barcelona, Spain (Online), December 12–13, 2020 2.1 els which represent the environment in terms of objects and their physics to models which represent the environment in terms of pixels. We focus on verbs, which have received considerably less attention in work on grounded language learning than have nouns and adjectives (Forbes et al., 2019). More so than nouns, verb learning is believed to rely on subtle combinations of both syntactic and grounded contextual signals (Piccin and Waxman, 2007) and thus progress on verb learning is likely to require new approaches to modeling and supervision. In our experiments, we find that strong baseline models, both featureengineered and neural network models, perform only marginally above chance. However, comparing models reveals intuitive differences in error patterns, and points to directions for future research. 2 2.1.1 Environment Data Collection Environment Construction Our environment is"
2020.starsem-1.16,2020.acl-main.463,0,0.0308776,"asizes the difficulty of learning verbs from naive distributional data. We discuss avenues for future work on cognitively-inspired grounded language learning, and release our corpus with the intent of facilitating research on the topic. 1 Introduction While distributional models of semantics have seen incredible success in recent years (Devlin et al., 2018), most current models lack “grounding”, or a connection between words and their referents in the non-linguistic world. Grounding is an important aspect to representations of meaning and arguably lies at the core of language “understanding” (Bender and Koller, 2020). Work on grounded language learning has tended to make opportunistic use of large available corpora, e.g. by learning from web-scale corpora of image (Bruni et al., 2012) or video captions (Sun et al., 2019), or has been driven by particular downstream applications such as robot navigation (Anderson et al., 2018). In this work, we take an aspirational look at grounded distributional semantics models, based This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http: //creativecommons.org/licenses/by/4.0/. 1 https://github.com/dylanebert/nbc 2 Ou"
2020.starsem-1.16,2020.acl-main.231,0,0.0289323,"have been shown to improve performance on intrinsic semantic similary metrics (Hill et al., 2017; Vuli´c et al., 2017) as well as to be better predictors of human brain activity (Anderson et al., 2015; Bulat et al., 2017). Much prior work has explored the augmentation of standard language modeling objectives with 2D image (Bruni et al., 2011; Kiela et al., 2017; Lazaridou et al., 2015; Silberer and Lapata, 2012; Divvala et al., 2014) and video (Sun et al., 2019) data. Recent work on detecting fine-grained events in videos is particularly relevant (Hendricks et al., 2018; Zhukov et al., 2019; Fried et al., 2020, among others). Especially relevant is the data collected by Gaspers et al. (2014), in which human subjects were asked to play simple games with a physical robot and narrate while doing so. Our data and work differs primarily in that we focus on the ability to ground to symbolic objects and physics rather than only to pixel data. Past work on “situated language learning”, inspired by emergence theories of language acquisition (MacWhinney, 2013), has trained AI agents to learn language from scratch by interacting with humans and/or each other in simulated environments or games (Wang et al., 20"
2020.starsem-1.16,W14-0507,0,0.0153898,"Hill et al., 2017; Vuli´c et al., 2017) as well as to be better predictors of human brain activity (Anderson et al., 2015; Bulat et al., 2017). Much prior work has explored the augmentation of standard language modeling objectives with 2D image (Bruni et al., 2011; Kiela et al., 2017; Lazaridou et al., 2015; Silberer and Lapata, 2012; Divvala et al., 2014) and video (Sun et al., 2019) data. Recent work on detecting fine-grained events in videos is particularly relevant (Hendricks et al., 2018; Zhukov et al., 2019; Fried et al., 2020, among others). Especially relevant is the data collected by Gaspers et al. (2014), in which human subjects were asked to play simple games with a physical robot and narrate while doing so. Our data and work differs primarily in that we focus on the ability to ground to symbolic objects and physics rather than only to pixel data. Past work on “situated language learning”, inspired by emergence theories of language acquisition (MacWhinney, 2013), has trained AI agents to learn language from scratch by interacting with humans and/or each other in simulated environments or games (Wang et al., 2016; Mirowski et al., 2016; Urbanek et al., 2019; Beattie et al., 2016; Hill et al.,"
2020.starsem-1.16,W11-2503,0,0.0549711,"Missing"
2020.starsem-1.16,D18-1168,0,0.0623868,"Missing"
2020.starsem-1.16,D17-1113,0,0.0233363,"open”, participants tended to mime the actions, but the ingame physics data does not faithfully capture the semantics of these verbs (e.g., containers do not actually open). These words highlight limitations of the environment which may be addressed in future work. 4 Related Work We contribute to a large body of research on learning grounded representations of language. Grounded representations have been shown to improve performance on intrinsic semantic similary metrics (Hill et al., 2017; Vuli´c et al., 2017) as well as to be better predictors of human brain activity (Anderson et al., 2015; Bulat et al., 2017). Much prior work has explored the augmentation of standard language modeling objectives with 2D image (Bruni et al., 2011; Kiela et al., 2017; Lazaridou et al., 2015; Silberer and Lapata, 2012; Divvala et al., 2014) and video (Sun et al., 2019) data. Recent work on detecting fine-grained events in videos is particularly relevant (Hendricks et al., 2018; Zhukov et al., 2019; Fried et al., 2020, among others). Especially relevant is the data collected by Gaspers et al. (2014), in which human subjects were asked to play simple games with a physical robot and narrate while doing so. Our data and"
2020.starsem-1.16,N18-1038,0,0.0611499,"Missing"
2020.starsem-1.16,J17-4004,0,0.0605169,"Missing"
2020.starsem-1.16,2020.acl-main.465,0,0.0605219,"Missing"
2020.starsem-1.16,P16-1224,0,0.0273506,"ed et al., 2020, among others). Especially relevant is the data collected by Gaspers et al. (2014), in which human subjects were asked to play simple games with a physical robot and narrate while doing so. Our data and work differs primarily in that we focus on the ability to ground to symbolic objects and physics rather than only to pixel data. Past work on “situated language learning”, inspired by emergence theories of language acquisition (MacWhinney, 2013), has trained AI agents to learn language from scratch by interacting with humans and/or each other in simulated environments or games (Wang et al., 2016; Mirowski et al., 2016; Urbanek et al., 2019; Beattie et al., 2016; Hill et al., 2018; Mirowski et al., 2016), Discussion We compare two types of models for grounded verb learning, one based on 2D visual features and one based on 3D symbolic and spatial features. Our analysis suggests that these approaches favor in different aspects of verb semantics. One open question is how to combine these differing signals, and how to design training objectives that encourage models to chose the right sensory inputs and time scale to which to ground each verb. We evaluated on a small set of verbs that are"
2020.starsem-1.16,D12-1130,0,0.028919,"ighlight limitations of the environment which may be addressed in future work. 4 Related Work We contribute to a large body of research on learning grounded representations of language. Grounded representations have been shown to improve performance on intrinsic semantic similary metrics (Hill et al., 2017; Vuli´c et al., 2017) as well as to be better predictors of human brain activity (Anderson et al., 2015; Bulat et al., 2017). Much prior work has explored the augmentation of standard language modeling objectives with 2D image (Bruni et al., 2011; Kiela et al., 2017; Lazaridou et al., 2015; Silberer and Lapata, 2012; Divvala et al., 2014) and video (Sun et al., 2019) data. Recent work on detecting fine-grained events in videos is particularly relevant (Hendricks et al., 2018; Zhukov et al., 2019; Fried et al., 2020, among others). Especially relevant is the data collected by Gaspers et al. (2014), in which human subjects were asked to play simple games with a physical robot and narrate while doing so. Our data and work differs primarily in that we focus on the ability to ground to symbolic objects and physics rather than only to pixel data. Past work on “situated language learning”, inspired by emergence"
2020.starsem-1.16,D19-1062,0,0.0511877,"Missing"
2021.acl-long.304,2020.acl-main.768,0,0.0534623,"Missing"
2021.acl-long.304,2020.emnlp-main.550,0,0.0222979,"d to the question, prefixed by This question is unanswerable because.... This system is introduced as a control to ensure that length bias is not in play in the main comparison (e.g., users may a priori prefer longer, topicallyrelated answers over short answers). That is, since our system, Presupposition failurebased explanation, yields strictly longer answers than Simple unanswerable, we want to ensure that our system is not preferred merely due to length rather than answer quality. • Open-domain rewrite: A rewrite of the nonoracle output taken from the demo5 of Dense Passage Retrieval (DPR; Karpukhin et al. 2020), a competitive open-domain QA system. This system is introduced to test whether presupposition failure can be easily addressed by expanding the answer source, since a single Wikipedia article was used to determine presupposition failure. If presupposition failure is a problem particular only to closed-book systems, a competitive open-domain system would suffice to address this issue. While the outputs compared are not oracle, this system 5 http://qa.cs.washington.edu:2020/ Question (input) Template Presupposition (output) which philosopher advocated the idea of return to nature when was it di"
2021.acl-long.304,Q19-1026,0,0.0824175,"Missing"
2021.acl-long.304,2020.acl-main.655,0,0.0669021,"Missing"
2021.acl-long.304,P06-1055,0,0.0262157,"model finetuned on verification, zero-shot transfer from fact verification, and a rulebased/NLI hybrid model. Since we used NQ, our models assume a closed-book setup with a single document as the source of verification. 5.1 Step 1: Presupposition Generation Linguistic triggers. Using the linguistic triggers discussed in Section 2, we implemented a rulebased generator to templatically generate presuppositions from questions. See Table 3 for examples, and Appendix C for a full list. Generation. The generator takes as input a constituency parse tree of a question string from the Berkeley Parser (Petrov et al., 2006) and applies trigger-specific transformations to generate the presupposition string (e.g., taking the sentential complement of a factive verb). If there are multiple triggers in a single question, all presuppositions corresponding to the triggers are generated. Thus, a single question may have multiple presuppositions. See Table 3 for examples of input questions and output presuppositions. 7 Code and data will be available at https://github.com/google-research/ google-research/presup-qa 3936 How good is our generation? We analyzed 53 questions and 162 generated presuppositions to estimate the"
2021.acl-long.304,N18-1101,0,0.0558432,"dia articles are often larger than the maximum premise length that NLI models can handle, we split the article into sentences and created n premise-hypothesis pairs for an article with n sentences. Then, we aggregated these predictions and labeled the hypothesis (the presupposition) as verifiable if there are at least k sentences from the document that supported the presupposition. If we had a perfect verifier, k = 1 would suffice to perform verification. We used k = 1 for our experiments, but k could be treated as a hyperparameter. We used ALBERT-xxlarge (Lan et al., 2020) finetuned on MNLI (Williams et al., 2018) and QNLI (Wang et al., 2019) as our NLI model. Finer-tuned NLI. Existing NLI datasets such as QNLI contain a broad distribution of entailment pairs. We adapted the model further to the distribution of entailment pairs that are specific to our generated presuppositions (e.g., Hypothesis: NP is contextually unique) through additional finetuning (i.e., finer-tuning). Through crowdsourcing on an internal platform, we collected entailment labels for 15,929 (presupposition, sentence) pairs, generated from 1000 questions in NQ and 5 sentences sampled randomly from the corresponding Wikipedia pages."
2021.acl-long.304,P18-2124,0,0.13303,"Missing"
2021.acl-long.304,N19-1241,0,0.0993971,"flone et al. (2018) discuss automatically detecting presuppositions, focusing on adverbial triggers (e.g., too, also...), which we excluded due to their infrequency in NQ. Jeretic et al. (2020) investigate whether inferences triggered by presuppositions and implicatures are captured well by NLI models, finding mixed results. Regarding unanswerable questions, their importance in QA (and therefore their inclusion in benchmarks) has been argued by works such as Clark and Gardner (2018) and Zhu et al. (2019). The analysis portion of our work is similar in motivation to unanswerability analyses in Yatskar (2019) and Asai and Choi (2020)—to better understand the causes of unanswerability in QA. Hu et al. (2019); Zhang et al. (2020); Back et al. (2020) consider answerability detection as a core motivation of their modeling approaches and propose components such as independent no-answer losses, answer verification, and answerability scores for answer spans. Our work is most similar to Geva et al. (2021) in proposing to consider implicit assumptions of questions. Furthermore, our work is complementary to QA explanation efforts like Lamm et al. (2020) that only consider answerable questions. Finally, abst"
2021.acl-long.304,P19-1415,0,0.023083,"resuppositions in NLP (but see Clausen and Manning (2009) and Tremper and Frank (2011)). More recently, Cianflone et al. (2018) discuss automatically detecting presuppositions, focusing on adverbial triggers (e.g., too, also...), which we excluded due to their infrequency in NQ. Jeretic et al. (2020) investigate whether inferences triggered by presuppositions and implicatures are captured well by NLI models, finding mixed results. Regarding unanswerable questions, their importance in QA (and therefore their inclusion in benchmarks) has been argued by works such as Clark and Gardner (2018) and Zhu et al. (2019). The analysis portion of our work is similar in motivation to unanswerability analyses in Yatskar (2019) and Asai and Choi (2020)—to better understand the causes of unanswerability in QA. Hu et al. (2019); Zhang et al. (2020); Back et al. (2020) consider answerability detection as a core motivation of their modeling approaches and propose components such as independent no-answer losses, answer verification, and answerability scores for answer spans. Our work is most similar to Geva et al. (2021) in proposing to consider implicit assumptions of questions. Furthermore, our work is complementary"
2021.acl-long.304,N18-1074,0,0.0828177,"Missing"
2021.acl-short.21,2020.acl-main.463,0,0.0372432,"e a distributional language model’s ability to differentiate logical symbols (¬, ∧, ∨). Our findings are largely negative: none of our simulated training corpora result in models which definitively differentiate meaningfully different symbols (e.g., ∧ vs. ∨), suggesting a limitation to the types of semantic signals that current models are able to exploit. 1 Introduction A current open question in natural language processing is to what extent language models (LMs; neural networks trained to predict the likelihood of word forms given textual context) are capable of truly understanding language. Bender and Koller (2020) argue that, since such models are trained exclusively on the form of language, they cannot possibly learn the meaning of language. We argue that the question of whether language models can learn meaning cannot be settled a priori. While language models only have direct access to form, linguistic form often correlates with meaning. The strength of the correlation varies across both different aspects of language and different tests of linguistic competence. While several intuitive tests of un2 2.1 Experimental Design Dataset Generation We consider the form of a sentence to be simply the observe"
2021.acl-short.21,2020.tacl-1.3,0,0.0339776,"Missing"
2021.acl-short.21,2020.acl-main.177,0,0.0314741,"Missing"
2021.acl-short.21,2020.conll-1.45,0,0.0345475,"Missing"
2021.acl-short.21,2020.acl-main.698,0,0.040856,"Missing"
2021.acl-short.21,2020.emnlp-main.731,0,0.093688,"Missing"
2021.acl-short.21,2020.tacl-1.48,0,0.0963208,"Missing"
2021.acl-short.21,D19-1534,0,0.0612452,"Missing"
2021.acl-short.21,W18-5446,0,0.0637251,"Missing"
2021.acl-short.21,D19-1286,0,0.04479,"Missing"
2021.acl-short.21,2020.acl-main.543,0,0.0504199,"Missing"
2021.acl-short.21,D18-1259,0,0.0602206,"Missing"
2021.conll-1.9,2020.conll-1.17,0,0.0310852,"mely, we employ a dataset of monolexemic color terms and color chips represented in CIELAB, a color space with a perceptually meaningful distance metric. Figure 1: Right: Color orientation in 3d CIELAB space. Left: linear mapping from BERT (CC, see §2) color term embeddings to the CIELAB space. features of concrete and abstract concepts, such as objects’ attributes and affordances (Forbes et al., 2019b; Weir et al., 2020). Furthermore, the representational geometry of LMs has been found to naturally reflect human lexical similarity and relatedness judgements, as well as analogy relationships (Chronis and Erk, 2020). However, the extent to which these models reflect the structures that exist in humans’ perceptual world—such as the topology of visual perception (Chen, 1982), the structure of the color spectrum (Ennis and Zaidi, 2019; Provenzi, 2020), or of odour spaces (Rossiter, 1996; Chastrette, 1997)—is not well-understood. If LMs are indeed able to capture such topologies—in some domains, at least—it would mean that these structures are a) somehow reflected 1 Introduction in language and, thereby, encoded in the textual Without grounding or interaction with the world, training data on which models are"
2021.conll-1.9,D19-1109,0,0.0713743,"mal linguistic struc- and architectural inductive biases. To the extent ture (e.g., morphosyntax (Tenney et al., 2019)) they are not, the question becomes whether the inand semantic information (e.g., lexical similarity formation is not there in the data, or whether model (Reif et al., 2019a)). Beyond this, it has been sug- and training objective limitations are to blame. Cergested that text-only training data is enough for tainly, this latter point relates to an ongoing deLMs to also acquire factual and relational informa- bate regarding what exactly language models can tion about the world (Davison et al., 2019; Petroni be expected to learn from ungrounded form alone et al., 2019). This includes, for instance, some (Bender and Koller, 2020; Bisk et al., 2020; Merrill ∗ For correspondence: {abdou,soegaard}@di.ku.dk et al., 2021). While there have been many inter109 Using two methods of evaluating the structural alignment of colors in this space with textderived color term representations, we find significant correspondence. Analyzing the differences in alignment across the color spectrum, we find that warmer colors are, on average, better aligned to the perceptual color space than cooler ones, sugges"
2021.conll-1.9,N19-1423,0,0.0118537,"terms are solicited through a free-naming task, resulting in 122 terms. Perceptual color space Following previous work (Regier et al., 2007; Zaslavsky et al., 2018; Chaabouni et al., 2021), we map colors to their corresponding points in the 3D CIELAB space, where the first dimension L expresses lightness, the second A expresses position between red and green, and the third B expresses the position between blue and yellow. Distances between colors in the space correspond to their perceptual difference. Language models Our analysis is conducted on three widely used language models (LMs): BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019), both of which employ a masked language modelling objective, and ELECTRA (Clark et al., 4 http://www1.icsi.berkeley.edu/wcs/ images/jrus-20100531/wcs-chart-4x.png 110 Figure 2: Our experimental setup. In the center is a Munsell color chart. Each chip in the chart is represented in the CIELAB space (right) and has 51 color term annotations. Color term embeddings are extracted through various methods. In the Representation Similarity Analysis experiments, a corresponding color chip centroid is computed in the CIELAB space. In the Linear Mapping experiments, a colo"
2021.conll-1.9,2020.emnlp-main.395,0,0.0992173,"Missing"
2021.conll-1.9,D19-1065,0,0.135345,"ly and the extent to which models implicitly reflect topological structure that is grounded in world, such as perceptual structure, is unknown. To explore this question, we conduct a thorough case study on color. Namely, we employ a dataset of monolexemic color terms and color chips represented in CIELAB, a color space with a perceptually meaningful distance metric. Figure 1: Right: Color orientation in 3d CIELAB space. Left: linear mapping from BERT (CC, see §2) color term embeddings to the CIELAB space. features of concrete and abstract concepts, such as objects’ attributes and affordances (Forbes et al., 2019b; Weir et al., 2020). Furthermore, the representational geometry of LMs has been found to naturally reflect human lexical similarity and relatedness judgements, as well as analogy relationships (Chronis and Erk, 2020). However, the extent to which these models reflect the structures that exist in humans’ perceptual world—such as the topology of visual perception (Chen, 1982), the structure of the color spectrum (Ennis and Zaidi, 2019; Provenzi, 2020), or of odour spaces (Rossiter, 1996; Chastrette, 1997)—is not well-understood. If LMs are indeed able to capture such topologies—in some domains"
2021.conll-1.9,2021.conll-1.7,0,0.0603838,"Missing"
2021.conll-1.9,D19-1275,0,0.0247702,"on (n = 6) indicates the alignability of the two spaces, given a linear transformation. Centroids corresponding to each Munsell color chip are computed in the color term embedding space via the weighted mean of the embeddings of the 51 terms used to label it. As in the RSA experiments, terms occurring less frequently than the cutoff (f = 100) are excluded. For evaluation, we compute the average (across splits and datapoints) proportion of explained variance as well as the ranking of a predicted color term embedding according to the Pearson distance (1 − r) to gold. Control task As proposed by Hewitt and Liang (2019), we construct a random control task for the linear mapping experiments, wherein we randomly swap each color chip’s CIELAB code for another. This is meant to break the mapping between the color chips and their corresponding terms. Control task results are reported as the mean of 10 different random re-mappings. We report probe selectivity, which is defined as the difference between proportion of explained variance in the standard experimental condition and in the control task (He8 We use the colormath Python package, setting illuminant to C, and assuming 2 degree standard observer. 112 NC Mode"
2021.conll-1.9,J15-4004,0,0.0464436,"tween a chip’s regression score (predicted color 11 chip code ranking) and its surprisal. We find signifLow entropy reflects frequent co-occurrence with a small icant Spearman’s rank correlation between lower subset of the vocabulary and high entropy the converse. 115 Results show that: 6 Related Work Distributional word representations have long been theorized to capture various types of information about the world (Schütze, 1992). Early work in this regard employed semantic similarity and relatedness datasets to measure alignment to human judgements (Agirre et al., 2009; Bruni et al., 2012; Hill et al., 2015). Rubinstein et al. (2015), however, question whether the distributional hypothesis is equally applicable to all types of semantic information, finding that taxonomic properties (such as animacy) are better modelled than attributive ones • deprel-ent and head-ent (but not (color, size, etc.). To a similar end, Lucy and Gaupos-ent) lead to a significantly improved thier (2017) analyze how well distributional repfit compared to the control predictors; we obresentations encode various aspects of grounded serve positive coefficients for both, indicating meaning. They investigate whether language m"
2021.conll-1.9,2020.emnlp-main.254,0,0.0995714,"Missing"
2021.conll-1.9,2021.ccl-1.108,0,0.019753,"Missing"
2021.conll-1.9,W17-2810,0,0.0434436,"Missing"
2021.conll-1.9,P15-2119,0,0.0251507,"ssion score (predicted color 11 chip code ranking) and its surprisal. We find signifLow entropy reflects frequent co-occurrence with a small icant Spearman’s rank correlation between lower subset of the vocabulary and high entropy the converse. 115 Results show that: 6 Related Work Distributional word representations have long been theorized to capture various types of information about the world (Schütze, 1992). Early work in this regard employed semantic similarity and relatedness datasets to measure alignment to human judgements (Agirre et al., 2009; Bruni et al., 2012; Hill et al., 2015). Rubinstein et al. (2015), however, question whether the distributional hypothesis is equally applicable to all types of semantic information, finding that taxonomic properties (such as animacy) are better modelled than attributive ones • deprel-ent and head-ent (but not (color, size, etc.). To a similar end, Lucy and Gaupos-ent) lead to a significantly improved thier (2017) analyze how well distributional repfit compared to the control predictors; we obresentations encode various aspects of grounded serve positive coefficients for both, indicating meaning. They investigate whether language modRSA score is higher for"
2021.conll-1.9,2020.lrec-1.497,0,0.0214251,"Missing"
2021.conll-1.9,D19-1250,0,0.0154348,"hey hold discussions on and modify a more diverse set of syntactic wall-collisions?”, finding that perceptual features heads. This suggests that occurring in a more are poorly modelled compared to encyclopedic and diverse set of contexts might be beneficial for robust representation learning, in correspon- taxonomic ones. More recently, several studies have asked related dence with the idea of sample diversity in the questions in the context of language models. For active learning literature (Brinker, 2003; Yang et al., 2015). pos-ent’s lack of significance, example, Davison et al. (2019) and Petroni et al. (2019) mine LMs for factual and commonsense on the other hand, indicates that the degree of knowledge by converting knowledge base triplets specification offered by the POS tagset might be too coarse to meaningfully differentiate be- into cloze statements that are used to query the models. In a similar vein, Forbes et al. (2019a) tween color terms, e.g. nouns can occur in a variety of DRELs such as subjects, objects, investigate LM representations’ encoding of oboblique modifiers (per the Universal Depende- ject properties (e.g., oranges are round), and affordances (e.g. oranges can be eaten), as we"
2021.conll-1.9,2020.coling-main.605,0,0.0440178,"Missing"
2021.conll-1.9,L16-1680,0,0.0122398,"r the CC configuration, broken down by Munsell color chip; (b) shows suprisal per chip. Circle colors reflect the modal color term assigned to the chips. What factors predict color space alignment? Given that LMs are trained exclusively on text corpora, we hypothesize that alignment between their embeddings and CIELAB is influenced by corpus usage statistics. To determine which factors could predict alignment score, we extract color term log frequency, part-of-speech tag (POS), dependency relation (DREL), and dependency tree head (HEAD) statistics for all color terms from a dependency-parsed (Straka et al., 2016) common crawl corpus. In addition to this, we compute, per color term, the entropy of its normalised PMI distribution (pmi-col, see §2) as a measure of collocation.11 We then fit a Linear Mixed Effects Model (Gałecki and Burzykowski, 2013) to the features listed above, with RSA score (Table 1) as the response variable, and model type as a random effect. We follow a multi-level step-wise model building sequence, where a baseline model is first fit with color term log frequency as a single fixed effect. A model which includes pmi-col as an additional fixed effect is then fit, and these two terms"
2021.conll-1.9,P19-1452,1,0.814956,"odour spaces (Rossiter, 1996; Chastrette, 1997)—is not well-understood. If LMs are indeed able to capture such topologies—in some domains, at least—it would mean that these structures are a) somehow reflected 1 Introduction in language and, thereby, encoded in the textual Without grounding or interaction with the world, training data on which models are trained, and b) language models (LMs) learn representations that learnable using models’ current training objectives encode various aspects of formal linguistic struc- and architectural inductive biases. To the extent ture (e.g., morphosyntax (Tenney et al., 2019)) they are not, the question becomes whether the inand semantic information (e.g., lexical similarity formation is not there in the data, or whether model (Reif et al., 2019a)). Beyond this, it has been sug- and training objective limitations are to blame. Cergested that text-only training data is enough for tainly, this latter point relates to an ongoing deLMs to also acquire factual and relational informa- bate regarding what exactly language models can tion about the world (Davison et al., 2019; Petroni be expected to learn from ungrounded form alone et al., 2019). This includes, for instan"
2021.conll-1.9,2020.emnlp-main.586,0,0.0814761,"Missing"
2021.emnlp-main.72,2021.scil-1.3,0,0.054954,"Missing"
2021.emnlp-main.72,2020.conll-1.17,0,0.0984211,"Missing"
2021.emnlp-main.72,D18-1151,1,0.771295,"Missing"
2021.emnlp-main.72,N19-1423,0,0.038232,"1988). Most speakers of English will agree, for combinations that never occurred in the training example, that if “gorp” is a singular noun, then, set is consistent with a model that learns abstract regardless of the meaning of “gorp”, the utter- representations of lexical items and patterns, i.e., ance “the gorp adds nothing” is grammatical, but abstract features and rules. Second, BERT’s perfor“the gorp add nothing” is not. mance is influenced by absolute frequency effects, The success of contemporary neural language but probing classifiers show that this influence can models such as BERT (Devlin et al., 2019) on lan- be explained by the model’s inability to learn the guage understanding tasks, as well as in more tar- features of a verb form (singular vs. plural) for ingeted linguistic evaluations (Marvin and Linzen, frequent lexical items, rather than a failure to apply ∗ Work done while visiting Google. the rule when the verb form has been classified. Fi932 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 932–948 c November 7–11, 2021. 2021 Association for Computational Linguistics nally, although BERT generally applies rules with high accuracy, it fai"
2021.emnlp-main.72,W16-2524,0,0.0287378,"ural, we use the following procedure. We randomly split our 60 VOI into two groups of 30 verbs each, which we denote as S and P. In each experiment, we set the frequency of the singular verbs in S to Nvary , while holding the frequency of the plural forms of the verbs in S constant at Nconstant . 7.2 Predicting Agreement Feature Likewise, we set the frequency of the plural verbs To test whether the above predicted pattern holds, in P to Nvary , and hold the frequency of the singular we use two probing classifiers (see Veldhoen et al., form of these verbs constant at Nconstant . We run ex2016; Ettinger et al., 2016, on probing) which we periments with Nvary = {1, 10, 30, 100, 300, 1,000, describe below. 3,000, 10,000}, and do this twice for Nconstant set Subject agreement feature probe. Our first to 100 and 1,000. As our evaluation stimuli are 0 balanced such that both v and v occur as the target probe evaluates whether, given a sentence with the in every template for every VOI, we are able to ana- verb masked, the embedding at the masked posilyze the effect of the v:v 0 frequency ratio—holding tion contains information as to whether a singular 0 the absolute frequency of v fixed—for v:v ranging or plur"
2021.emnlp-main.72,2020.acl-main.177,0,0.0611679,"Missing"
2021.emnlp-main.72,N18-1108,1,0.817912,"ic needs providers ... review/reviews a damaging effect. Table 1: Examples of natural and nonce stimuli. Target verbs and their subjects are bolded. The model takes as input the sentence with the verb masked, and is evaluated based on which verb inflection it scores more highly. achieve performances comparable to the public BERT-Base release on GLUE (Wang et al., 2018) (see Appendix A). 4.2 Evaluation Stimuli We evaluate the model’s SVA ability on two classes of stimuli: (1) natural sentences, which are generally both syntactically and semantically coherent, and (2) nonce sentences (following Gulordava et al. 2018) which are grammatically valid but not necessarily semantically coherent (“colorless green ideas sleep furiously”, Chomsky, 1956). Examples of each are shown in Table 1. Evaluating on natural sentences provides a measurement of how well the model can be expected to perform in realistic settings, but these sentences are not ideal for a targeted SVA evaluation since they often contain additional cues relevant to verb inflection, such as other plural verbs or plural determiners, as in “two [SUBJECT] and their dogs [VERB],” making it difficult to discern whether a model has chosen a particular ver"
2021.emnlp-main.790,W12-3810,0,0.0339648,"it pairs of these types of linguistic bias. We show that cues of opinion bias can yield measurable differences in the style and content of generated text. Acknowledgements municative quality (Lipka and Stein, 2010), biased content (Al Khatib et al., 2012). We will build on a large literature on subjectivity that links bias to lexical and grammatical cues, e.g., work identifying common linguistic classes that these biasinducing words might fall into (Wiebe et al., 2004), and work on building predictive models to identify bias-inducing words in natural language sentences (Recasens et al., 2013; Conrad et al., 2012). Different from the above, our work attempts to probe generative language models for these effects. Societal biases in language models Several recent works have looked at bias in language models and the societal effects they may have (Bender et al., 2021; Nadeem et al., 2020). Most relevant is work on identifying “triggers” in text that may lead to toxic degeneration (Wallace et al., 2019), finding that particular nonsensical text inputs led models to produce hate speech. Unlike this work, we focus on measuring LMs’ sensitivity to subtle We would like to acknowledge Dean Carignan, Pooya Morad"
2021.findings-acl.181,P16-1167,0,0.0134141,"odel of learning, and as they often use overly simplistic loss functions. Complementary to our project, Ettinger et al. (2017) encourage robust error analysis of NLP systems through developing challenges that are based on linguistic phenomena, and that have a low barrier to entry. Common sense and probing. NLP has been interested in encoding commonsense relations for a long time (Liu and Singh, 2004). Recent work has shown how pre-trained LMs exhibit common sense knowledge even before fine-tuning (Petroni et al., 2019), and that they can be built and used to mine more commonsense information (Bosselut et al., 2016; Davison et al., 2019). While this signifies how LMs encode some common sense and prototypical properties of nouns (Weir et al., 2020), many researchers are pointing out these models’ insensitivity to context (Ettinger, 2020; Ravichander et al., 2020), which is antithetical to common sense. Challenge Sets Many existing challenge sets have provided concrete frameworks to evaluate models inference ability, through coreference resolution (notably Winograd Schema Challenge WSC (Levesque et al., 2012)) or pronoun resolution (notably in PDP (Morgenstern et al., 2016)). In this work, similar to the"
2021.findings-acl.181,D19-1109,0,0.0286121,"s they often use overly simplistic loss functions. Complementary to our project, Ettinger et al. (2017) encourage robust error analysis of NLP systems through developing challenges that are based on linguistic phenomena, and that have a low barrier to entry. Common sense and probing. NLP has been interested in encoding commonsense relations for a long time (Liu and Singh, 2004). Recent work has shown how pre-trained LMs exhibit common sense knowledge even before fine-tuning (Petroni et al., 2019), and that they can be built and used to mine more commonsense information (Bosselut et al., 2016; Davison et al., 2019). While this signifies how LMs encode some common sense and prototypical properties of nouns (Weir et al., 2020), many researchers are pointing out these models’ insensitivity to context (Ettinger, 2020; Ravichander et al., 2020), which is antithetical to common sense. Challenge Sets Many existing challenge sets have provided concrete frameworks to evaluate models inference ability, through coreference resolution (notably Winograd Schema Challenge WSC (Levesque et al., 2012)) or pronoun resolution (notably in PDP (Morgenstern et al., 2016)). In this work, similar to the Winograd Schemas (Leves"
2021.findings-acl.181,2020.tacl-1.3,0,0.291702,"s performance to human intelligence is, hence, the verification that machines can exhibit a sensitivity to context that would allow them to perform as well on cases that require reasoning about exceptions as on cases that require recalling generic associations. Recent work (Petroni et al., 2019) has shown that large pretrained language models, in particular Masked Language Models (MLMs) such as BERT (Devlin et al., 2018) are competent at associating entities with their common characteristics. For example, BERTLARGE readily recalls apple → edible and charcoal → hot. However, as demonstrated by Ettinger (2020) and Kassner and Sch¨utze (2019), BERT is insensitive to various overt contextual cues, notably negation. For example, given the context “The shower is ,” BERTLARGE predicts the words “cold”, “long”, and “empty”, the same top 3 predictions it makes given the context “The shower is not ”. Such results suggest that while language models like BERT capture many commonsense patterns, such success might be limited to inferences involving common generalizations (appearing in an affirmative context, or using common lexical associations) and not those involving exceptions (appearing in a negative conte"
2021.findings-acl.181,W17-5401,0,0.0624774,"Missing"
2021.findings-acl.181,2020.findings-emnlp.117,0,0.0193553,"motivation, meanwhile, is to adversarially leverage the biases that models associate with entities to “trick” them into choosing incorrect answers. Our work uses adversarially constructed test sets to expose heuristics that models use. This technique has been used widely in probing/analysis work, e.g., (Glockner et al., 2018; Naik et al., 2018; Jia and Liang, 2017; Nie et al., 2019; McCoy et al., 2019). The idea of improving models performance on “exceptions” to “generalizations” also shares much in common with work on bias and fairness in NLP (Rudinger et al., 2018; Zhao et al., 2018, 2019). Gardner et al. (2020) propose the development of contrast sets, which can be developed by manually perturbing existing datasets in small but meaningful ways that would change the gold label. Our work, in contrast, factor in models’ insensitive associations into the construction of challenges in addition to a slight change in context that is leveraged by contrast sets. Kaushik et al. (2019) similarly propose using counterfactually-augmented data to make models more robust against spurious associations. Our work adds to this work by demonstrating that fine-tuning on exception challenges can increase the performance"
2021.findings-acl.181,P18-2103,0,0.0214005,"th language-based biases that would trivialize the task of picking the correct answer. W INO G RANDE aims to minimize the chance of models getting the right answers for the wrong reasons (through leveraging simple lexical associations that are annotation artifacts by human annotators). Our key motivation, meanwhile, is to adversarially leverage the biases that models associate with entities to “trick” them into choosing incorrect answers. Our work uses adversarially constructed test sets to expose heuristics that models use. This technique has been used widely in probing/analysis work, e.g., (Glockner et al., 2018; Naik et al., 2018; Jia and Liang, 2017; Nie et al., 2019; McCoy et al., 2019). The idea of improving models performance on “exceptions” to “generalizations” also shares much in common with work on bias and fairness in NLP (Rudinger et al., 2018; Zhao et al., 2018, 2019). Gardner et al. (2020) propose the development of contrast sets, which can be developed by manually perturbing existing datasets in small but meaningful ways that would change the gold label. Our work, in contrast, factor in models’ insensitive associations into the construction of challenges in addition to a slight change in"
2021.findings-acl.181,2020.sustainlp-1.17,0,0.0295898,"n dimension, 16 attention heads, 336M parameters). We base our procedure off of the insight from Ettinger (2020), which demonstrated BERT’s insensitivity to negation. For example, given the contexts “A robin is (not) a [MASK],” BERT’s top two predictions would share {bird, robin} in common, or given the contexts “A daisy is (not) a [MASK],” BERT’s top three predictions would share {daisy, rose, flower} in common. Our experiments show that this behavior holds consistent for other MLMs such as RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2019), Longformer (Beltagy et al., 2020), SqueezeBERT (Iandola et al., 2020), and MobileBERT (Sun et al., 2020). Thus, to identify context-invariant associations, we feed 8 Generic Associations for BERTLARGE desk → empty, tarantula → poisonous, couch → comfortable, syrup → sweet, honey → sweet, compass → accurate desk → there, tarantula → edible, couch → empty, syrup → bitter, honey → good, compass → true desk → full, tarantula → {female, male}, couch → warm, syrup → edible honey → edible, compass → right desk → {mine, clean}, tarantula → small couch → {clean, big}, syrup → orange, honey → {delicious, there, honey} compass → wrong Table 3: Examples of BERTLARGE generi"
2021.findings-acl.181,D17-1215,0,0.0332935,"lize the task of picking the correct answer. W INO G RANDE aims to minimize the chance of models getting the right answers for the wrong reasons (through leveraging simple lexical associations that are annotation artifacts by human annotators). Our key motivation, meanwhile, is to adversarially leverage the biases that models associate with entities to “trick” them into choosing incorrect answers. Our work uses adversarially constructed test sets to expose heuristics that models use. This technique has been used widely in probing/analysis work, e.g., (Glockner et al., 2018; Naik et al., 2018; Jia and Liang, 2017; Nie et al., 2019; McCoy et al., 2019). The idea of improving models performance on “exceptions” to “generalizations” also shares much in common with work on bias and fairness in NLP (Rudinger et al., 2018; Zhao et al., 2018, 2019). Gardner et al. (2020) propose the development of contrast sets, which can be developed by manually perturbing existing datasets in small but meaningful ways that would change the gold label. Our work, in contrast, factor in models’ insensitive associations into the construction of challenges in addition to a slight change in context that is leveraged by contrast s"
2021.findings-acl.181,2021.ccl-1.108,0,0.094942,"Missing"
2021.findings-acl.181,P19-1334,1,0.805065,"Missing"
2021.findings-acl.181,W13-3819,0,0.0792915,"Missing"
2021.findings-acl.181,D19-1250,0,0.0540959,"Missing"
2021.findings-acl.181,2020.starsem-1.10,0,0.0365284,"that have a low barrier to entry. Common sense and probing. NLP has been interested in encoding commonsense relations for a long time (Liu and Singh, 2004). Recent work has shown how pre-trained LMs exhibit common sense knowledge even before fine-tuning (Petroni et al., 2019), and that they can be built and used to mine more commonsense information (Bosselut et al., 2016; Davison et al., 2019). While this signifies how LMs encode some common sense and prototypical properties of nouns (Weir et al., 2020), many researchers are pointing out these models’ insensitivity to context (Ettinger, 2020; Ravichander et al., 2020), which is antithetical to common sense. Challenge Sets Many existing challenge sets have provided concrete frameworks to evaluate models inference ability, through coreference resolution (notably Winograd Schema Challenge WSC (Levesque et al., 2012)) or pronoun resolution (notably in PDP (Morgenstern et al., 2016)). In this work, similar to the Winograd Schemas (Levesque 2068 Figure 1: Left: Fine-tuning BERTBASE on a dataset that contains both generics and exceptions results in a minimal increase in performance on exceptions. Right: Fine-tuning BERTBASE on a dataset containing only exceptions"
2021.findings-acl.181,N18-2002,0,0.0500503,"Missing"
2021.findings-acl.181,2020.acl-main.195,0,0.0236131,"arameters). We base our procedure off of the insight from Ettinger (2020), which demonstrated BERT’s insensitivity to negation. For example, given the contexts “A robin is (not) a [MASK],” BERT’s top two predictions would share {bird, robin} in common, or given the contexts “A daisy is (not) a [MASK],” BERT’s top three predictions would share {daisy, rose, flower} in common. Our experiments show that this behavior holds consistent for other MLMs such as RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2019), Longformer (Beltagy et al., 2020), SqueezeBERT (Iandola et al., 2020), and MobileBERT (Sun et al., 2020). Thus, to identify context-invariant associations, we feed 8 Generic Associations for BERTLARGE desk → empty, tarantula → poisonous, couch → comfortable, syrup → sweet, honey → sweet, compass → accurate desk → there, tarantula → edible, couch → empty, syrup → bitter, honey → good, compass → true desk → full, tarantula → {female, male}, couch → warm, syrup → edible honey → edible, compass → right desk → {mine, clean}, tarantula → small couch → {clean, big}, syrup → orange, honey → {delicious, there, honey} compass → wrong Table 3: Examples of BERTLARGE generic associations for different values"
2021.findings-acl.181,N18-2003,0,0.0237109,"uman annotators). Our key motivation, meanwhile, is to adversarially leverage the biases that models associate with entities to “trick” them into choosing incorrect answers. Our work uses adversarially constructed test sets to expose heuristics that models use. This technique has been used widely in probing/analysis work, e.g., (Glockner et al., 2018; Naik et al., 2018; Jia and Liang, 2017; Nie et al., 2019; McCoy et al., 2019). The idea of improving models performance on “exceptions” to “generalizations” also shares much in common with work on bias and fairness in NLP (Rudinger et al., 2018; Zhao et al., 2018, 2019). Gardner et al. (2020) propose the development of contrast sets, which can be developed by manually perturbing existing datasets in small but meaningful ways that would change the gold label. Our work, in contrast, factor in models’ insensitive associations into the construction of challenges in addition to a slight change in context that is leveraged by contrast sets. Kaushik et al. (2019) similarly propose using counterfactually-augmented data to make models more robust against spurious associations. Our work adds to this work by demonstrating that fine-tuning on exception challenges"
2021.findings-emnlp.370,2020.acl-main.463,0,0.0708678,"Missing"
2021.findings-emnlp.370,2020.emnlp-main.703,0,0.056967,"Missing"
2021.findings-emnlp.370,2020.acl-main.469,0,0.239009,"onal Linguistics how well text-only models capture commonsense knowledge about the physical world via intrinsic (Ettinger, 2020; Forbes et al., 2019) and extrinsic (Zellers et al., 2018, 2019; Bisk et al., 2020b) measures. Despite the interest in representations of the non-linguistic world, such analyses have not, to our knowledge, been run on multimodal LMs. Vision-and-Language Pretraining. There is a long history of multimodal distributional semantics models (Howell et al., 2005; Lazaridou et al., 2015), to which pretrained transformer-based models are the latest addition (Sun et al., 2019; Li et al., 2020). Evaluations of these recent visionand-language (VL) models has tended to focus on inherently multimodal tasks , e.g., image and video captioning (Sun et al., 2019), visual question answering (Li et al., 2020), or instruction following in robotics (Majumdar et al., 2020). Cao et al. (2020) describes a series of “probing” analyses for multimodal language representations, but focuses on explicit grounding, e.g., to where do models attend in the image when processing “dog”? Little work has analyzed whether the presence of grounded training data impacts the linguistic representations in general."
2021.findings-emnlp.370,Q16-1037,0,0.0294529,"rmance on many benchmark language understanding tasks, 2 Related Work they have been criticized for their lack of grounding, i.e., the ability to connect words to the real- Analyzing Pretrained LMs. There has been world entities, events, and ideas to which they refer. substantial prior work on analyzing pretrained LMs While grounding is obviously necessary for mulit- and the linguistic properties of their representations, modal language understanding tasks (e.g., identify- looking, e.g., at syntactic parse structure (Hewitt ing a dog in an image), it has further been argued and Manning, 2019; Linzen et al., 2016), semantic to be fundamental for learning semantic represen- structure such as semantic roles and coreference tations in general. For example, Bender and Koller (Tenney et al., 2019a), lexical semantics (Chronis (2020) argues that models trained without ground- and Erk, 2020; Vuli´c et al., 2020), and lexical coming will ultimately fail on some text-only tasks such position (Yu and Ettinger, 2020). Particularly releas goal-oriented dialogue, and Merrill et al. (2021) vant to our studies is prior work which has explored 4357 Findings of the Association for Computational Linguistics: EMNLP 2021,"
2021.findings-emnlp.370,P19-1472,0,0.049511,"Missing"
D16-1106,D13-1160,0,0.075922,"Missing"
D16-1106,chang-manning-2012-sutime,0,0.0296917,"us on the those events identified by the system which are relevant to the main fields in the GVDB schema.4 We map the arguments of these events onto the corresponding database fields, e.g. the agent of the event corresponds to the GVDB’s shooter name. Since the system identifies multiple such events per article, we count it as correct as long as one argument correctly matches the corresponding value in the GVDB (e.g. the system is correct as long as one extracted event has an agent which matches the GVDB’s shooter name for that article). In addition, we run the Stanford CoreNLP TimeEx system (Chang and Manning, 2012) over the articles in order to identify the time of the reported incident. We report the system’s performance using both exact match against the gold annotation (“strict”) as well as an approximate match, in which the system is correct if it is either a substring or a superstring of the gold annotation. E.g. if the victim name is Sean Bolton, the approximate metric will count both Bolton and Officer Sean Bolton as correct. While performance is high for certain structured types of information, like dates and times, fields like victim and shooter name are much less reliably identified. Furthermo"
D16-1106,D11-1142,0,0.0254719,"n, coreference resolution, and event detection. We introduce a new and growing dataset, the Gun Violence Database, in order to facilitate the adaptation of current NLP technologies to the domain of gun violence, thus enabling better social science research on this important and under-resourced problem. 1 Although these technological achievements are profound, often times we as researchers apply them to somewhat trivial settings like learning about the latest Hollywood divorces (Wijaya et al., 2015) or learning silly facts about the world, like that hwhite suites, will never go out of, stylei (Fader et al., 2011). In this paper, we call the attention of the NLP community to one particularly good use case of our current technology, which could have profound policy implications: gun violence research. Introduction The field of natural language processing often touts its mission as harnessing the information contained in human language: taking unstructured data in the form of speech and text, and transforming it into information that can be searched, categorized, and reasoned about. This is an ambitious goal, and the current state-of-the-art of language technology has made impressive strides towards unde"
D16-1106,P13-1008,1,0.794198,"mation Table 1: Current contents of the GVDB. Size and level of annotation is continually growing. See Forthcoming Extensions. Current Baselines To establish a baseline level of performance, we run an off-the-shelf information 3 See supplementary material for all extracted information and screenshots. Figure 2: Annotation interface associates structured information (e.g. the time of day when the shooting occurred) with a specific span of text in the article. extraction system on the 7,366 articles and measure precision and recall for identifying key information about the incidents. We use the Li et al. (2013) systems, which identifies a range of entities and events. We focus on the those events identified by the system which are relevant to the main fields in the GVDB schema.4 We map the arguments of these events onto the corresponding database fields, e.g. the agent of the event corresponds to the GVDB’s shooter name. Since the system identifies multiple such events per article, we count it as correct as long as one argument correctly matches the corresponding value in the GVDB (e.g. the system is correct as long as one extracted event has an agent which matches the GVDB’s shooter name for that a"
D16-1106,N10-1021,0,0.103628,"Missing"
D16-1106,W15-1205,0,0.017789,"s. The National Violent Death Registry System, arguably the most organized effort, receives data from only 16 states. Most large-scale epidemiological studies sample information from only 100 Emergency Departments. across the text of thousands of web pages. Replacing expensive, manual data entry with automated processing is exactly the type of problem that NLP is made to solve. In fact, the recent application of NLP tools to social science problems has generated a flurry of exciting and encouraging results. NLP has made novel contributions to the way scientists measure everything from income (Preoctiuc-Pietro et al., 2015b) to mental health (Preoctiuc-Pietro et al., 2015a; Schwartz et al., 2016; Choudhury et al., 2016), disease (Santillana et al., 2015; Ireland et al., 2015; Eichstaedt et al., 2015), and the quality of patient care (Nakhasi et al., 2016; Ranard et al., 2016). Text mining has promise for the study of gun violence, too (Bushman et al., 2016). However, most questions about gun violence are not easily answered using shallow analyses like topic models or word clusters. Epidemiologists want to know, for example, does gun ownership lead to increases in gun violence? Or, is there evidence of contagion"
D16-1106,N13-1008,0,0.0109243,"ssion as harnessing the information contained in human language: taking unstructured data in the form of speech and text, and transforming it into information that can be searched, categorized, and reasoned about. This is an ambitious goal, and the current state-of-the-art of language technology has made impressive strides towards understanding “who did what to whom, when, where, how, and why” (Kao and Poteet, 2007). Advances in NLP have enabled us to read news in real time (Petrovi´c et al., 2010), identify the key players (Ruppenhofer et al., 2009), recognize the relationships between them (Riedel et al., 2013), summarize the new information (Wang et al., 2016), update central databases Gun violence is an undeniable problem in the United States, but its causes are poorly understood, and attempts to reason about solutions are often marred by emotions and political bias. Research into the factors that cause and prevent gun violence is limited by the fact that data collection is expensive, and political agendas have all but eliminated funding on the topic. However, in the form of unstructured natural language published daily by newspapers across the country, data abounds. We argue that this is the exac"
D16-1106,W09-2417,0,0.0216846,"roduction The field of natural language processing often touts its mission as harnessing the information contained in human language: taking unstructured data in the form of speech and text, and transforming it into information that can be searched, categorized, and reasoned about. This is an ambitious goal, and the current state-of-the-art of language technology has made impressive strides towards understanding “who did what to whom, when, where, how, and why” (Kao and Poteet, 2007). Advances in NLP have enabled us to read news in real time (Petrovi´c et al., 2010), identify the key players (Ruppenhofer et al., 2009), recognize the relationships between them (Riedel et al., 2013), summarize the new information (Wang et al., 2016), update central databases Gun violence is an undeniable problem in the United States, but its causes are poorly understood, and attempts to reason about solutions are often marred by emotions and political bias. Research into the factors that cause and prevent gun violence is limited by the fact that data collection is expensive, and political agendas have all but eliminated funding on the topic. However, in the form of unstructured natural language published daily by newspapers"
D16-1106,N16-1008,0,0.0243649,"n language: taking unstructured data in the form of speech and text, and transforming it into information that can be searched, categorized, and reasoned about. This is an ambitious goal, and the current state-of-the-art of language technology has made impressive strides towards understanding “who did what to whom, when, where, how, and why” (Kao and Poteet, 2007). Advances in NLP have enabled us to read news in real time (Petrovi´c et al., 2010), identify the key players (Ruppenhofer et al., 2009), recognize the relationships between them (Riedel et al., 2013), summarize the new information (Wang et al., 2016), update central databases Gun violence is an undeniable problem in the United States, but its causes are poorly understood, and attempts to reason about solutions are often marred by emotions and political bias. Research into the factors that cause and prevent gun violence is limited by the fact that data collection is expensive, and political agendas have all but eliminated funding on the topic. However, in the form of unstructured natural language published daily by newspapers across the country, data abounds. We argue that this is the exact type of information that NLP is designed to organ"
D16-1106,D15-1059,0,0.0129558,"t from news articles across the country. This is an ideal application of NLP technologies, such as relation extraction, coreference resolution, and event detection. We introduce a new and growing dataset, the Gun Violence Database, in order to facilitate the adaptation of current NLP technologies to the domain of gun violence, thus enabling better social science research on this important and under-resourced problem. 1 Although these technological achievements are profound, often times we as researchers apply them to somewhat trivial settings like learning about the latest Hollywood divorces (Wijaya et al., 2015) or learning silly facts about the world, like that hwhite suites, will never go out of, stylei (Fader et al., 2011). In this paper, we call the attention of the NLP community to one particularly good use case of our current technology, which could have profound policy implications: gun violence research. Introduction The field of natural language processing often touts its mission as harnessing the information contained in human language: taking unstructured data in the form of speech and text, and transforming it into information that can be searched, categorized, and reasoned about. This is"
D16-1240,W12-5115,0,0.0465336,"Missing"
D16-1240,chang-manning-2012-sutime,0,0.0665372,"6) I planned to solve the problem tomorrow. We exploit this property to predict implicativeness– whether the truth of a verb’s complement can be inferred– by observing the verb’s usage in practice. 3 Method We hypothesize that, given a large corpus, we should be able to distinguish implicative verbs from nonimplicative verbs by observing how often the main verb tense agrees/disagrees with the tense of the complement clause. Unfortunately, verbs in infinitival complement clauses are not conjugated, and so are not necessarily marked for tense. We therefore use the Stanford Temporal Tagger (TT) (Chang and Manning, 2012) in order to identify time-referring expressions (e.g. tomorrow or last night) and resolve them to either past, present, or future tense. We find all sentences containing VB∗1 to VB2 constructions in the Annotated Gigaword corpus (Napoles et al., 2012). We run the the TT over all of the sentences in order to identify time-referring expressions. We only consider sentences in which a time-referring expression appears and is in a direct dependency relationship with the complement verb (VB2 ). We provide the TT with the document publication dates,3 which are used to resolve each time mention to a"
D16-1240,W07-1409,0,0.0270118,"e. For example, our method finds high tense agreement for choose to and be allowed to, which are often used to communicate, albeit indirectly, that their complements did in fact happen. To convince ourselves that treating such verbs as implicatives makes sense in practice, we manually look through 2228 the RTE3 dataset (Giampiccolo et al., 2007) for examples containing high-scoring verbs according to our method. Table 3 shows some example inferences that hinge precisely on recognizing these types of de facto implicatives. 5 Discussion and Related Work Language understanding tasks such as RTE (Clark et al., 2007; MacCartney, 2009) and bias detection (Recasens et al., 2013) have been shown to require knowledge of implicative verbs, but such knowledge has previously come from manually-built word lists rather than from data. Nairn et al. (2006) and Martin et al. (2009) describe automatic systems to handle implicatives, but require hand-crafted rules for each unique verb that is handled. The tense agreement method we present offers a starting point for acquiring such rules from data, and is well-suited for incorporating into statistical systems. The clear next step is to explore similar data-driven means"
D16-1240,W07-1401,0,0.0915829,"agreement yield more definitive judgments (true/false). Each bar represents aggregated judgements over approx. 20 verbs. Interestingly, tense agreement accurately models verbs that are not implicative by definition, but which nonetheless tend to behave implicatively in practice. For example, our method finds high tense agreement for choose to and be allowed to, which are often used to communicate, albeit indirectly, that their complements did in fact happen. To convince ourselves that treating such verbs as implicatives makes sense in practice, we manually look through 2228 the RTE3 dataset (Giampiccolo et al., 2007) for examples containing high-scoring verbs according to our method. Table 3 shows some example inferences that hinge precisely on recognizing these types of de facto implicatives. 5 Discussion and Related Work Language understanding tasks such as RTE (Clark et al., 2007; MacCartney, 2009) and bias detection (Recasens et al., 2013) have been shown to require knowledge of implicative verbs, but such knowledge has previously come from manually-built word lists rather than from data. Nairn et al. (2006) and Martin et al. (2009) describe automatic systems to handle implicatives, but require hand-c"
D16-1240,S12-1020,0,0.125807,"sts rather than from data. Nairn et al. (2006) and Martin et al. (2009) describe automatic systems to handle implicatives, but require hand-crafted rules for each unique verb that is handled. The tense agreement method we present offers a starting point for acquiring such rules from data, and is well-suited for incorporating into statistical systems. The clear next step is to explore similar data-driven means for learning the specific behaviors of individual implicative verbs, which has been well-studied from a theoretical perspective (Karttunen, 1971; Nairn et al., 2006; Amaral et al., 2012; Karttunen, 2012). Another interesting extension concerns the role of tense in word representations. While currently, tense is rarely built directly into distributional representations of words (Mikolov et al., 2013; Pennington et al., 2014), our results suggest it may offer important insights into the semantics of individual words. We leave this question as a direction for future work. 6 Conclusion Differentiating between implicative and nonimplicative verbs is important for discriminating inferences that can and cannot be made in natural language. We have presented a data-driven method that captures the impl"
D16-1240,W09-3720,0,0.0237304,"nse in practice, we manually look through 2228 the RTE3 dataset (Giampiccolo et al., 2007) for examples containing high-scoring verbs according to our method. Table 3 shows some example inferences that hinge precisely on recognizing these types of de facto implicatives. 5 Discussion and Related Work Language understanding tasks such as RTE (Clark et al., 2007; MacCartney, 2009) and bias detection (Recasens et al., 2013) have been shown to require knowledge of implicative verbs, but such knowledge has previously come from manually-built word lists rather than from data. Nairn et al. (2006) and Martin et al. (2009) describe automatic systems to handle implicatives, but require hand-crafted rules for each unique verb that is handled. The tense agreement method we present offers a starting point for acquiring such rules from data, and is well-suited for incorporating into statistical systems. The clear next step is to explore similar data-driven means for learning the specific behaviors of individual implicative verbs, which has been well-studied from a theoretical perspective (Karttunen, 1971; Nairn et al., 2006; Amaral et al., 2012; Karttunen, 2012). Another interesting extension concerns the role of te"
D16-1240,W06-3907,0,0.47352,"as implicatives makes sense in practice, we manually look through 2228 the RTE3 dataset (Giampiccolo et al., 2007) for examples containing high-scoring verbs according to our method. Table 3 shows some example inferences that hinge precisely on recognizing these types of de facto implicatives. 5 Discussion and Related Work Language understanding tasks such as RTE (Clark et al., 2007; MacCartney, 2009) and bias detection (Recasens et al., 2013) have been shown to require knowledge of implicative verbs, but such knowledge has previously come from manually-built word lists rather than from data. Nairn et al. (2006) and Martin et al. (2009) describe automatic systems to handle implicatives, but require hand-crafted rules for each unique verb that is handled. The tense agreement method we present offers a starting point for acquiring such rules from data, and is well-suited for incorporating into statistical systems. The clear next step is to explore similar data-driven means for learning the specific behaviors of individual implicative verbs, which has been well-studied from a theoretical perspective (Karttunen, 1971; Nairn et al., 2006; Amaral et al., 2012; Karttunen, 2012). Another interesting extensio"
D16-1240,W12-3018,0,0.0761681,"Missing"
D16-1240,D14-1162,0,0.0786456,"Missing"
D16-1240,P13-1162,0,0.154291,"ve verbs, like want, which do not permit any inference regarding their complements, and for which the truth of the complement is unaffected by negation in the main clause (Table 1). The method described in this paper aims to separate implicatives from non-implicatives (manage vs. want), rather than to differentiate between types implicatives (manage vs. fail). Making this implicative/non-implicative distinction is a necessary first step toward handling inferences involving embedded clauses, and one that, to date, has only been performed using manually-constructed word lists (MacCartney, 2009; Recasens et al., 2013). 2.1 Tense Constraints on Complement Clauses Karttunen (1971) observed that, in sentences involving implicatives, the tense of the main verb must necessarily match the tense of the complement clause. For example, (3), in which the main clause and the complement are both in the past tense, is acceptable but (4), in which the complement is in the future, is clearly not. For non-implicatives, however, tives, factives presuppose, rather than entail, their complements. E.g. both I was/was not glad to solve the problem entail I solved the problem. We do not address factives here, as factives rarely"
D18-1007,P17-1080,0,0.0161305,"ast test set, the decrease was not significant. We leave this investigating for a future thorough study. 5 Related Work Exploring what linguistic phenomena neural models learn Many tests have been used to probe how well neural models learn different linguistic phenomena. Linzen et al. (2016) use “number agreement in English subject-verb dependencies” to show that LSTMs learn about syntaxsensitive dependencies. In addition to syntax (Shi et al., 2016), researchers have used other labeling tasks to investigate whether neural machine translation (NMT) models learn different linguistic phenomena (Belinkov et al., 2017a,b; Dalvi et al., 2017; Marvin and Koehn, 2018). Recently, Poliak et al. (2018a) used recast NLI datasets to investigate semantics captured by NMT encoders. Targeted Tests for Natural Language Understanding We follow a long line of work focused on building datasets to test how well NLU systems perform distinct types of semantic reasoning. FraCaS uses a limited number of sentencepairs to test whether systems understand semantic phenomena, e.g. generalized quantifiers, temporal references, and (nominal) anaphora (Cooper et al., 1996). FraCas cannot be used to train neural models – it includes j"
D18-1007,I17-1001,0,0.0114653,"ast test set, the decrease was not significant. We leave this investigating for a future thorough study. 5 Related Work Exploring what linguistic phenomena neural models learn Many tests have been used to probe how well neural models learn different linguistic phenomena. Linzen et al. (2016) use “number agreement in English subject-verb dependencies” to show that LSTMs learn about syntaxsensitive dependencies. In addition to syntax (Shi et al., 2016), researchers have used other labeling tasks to investigate whether neural machine translation (NMT) models learn different linguistic phenomena (Belinkov et al., 2017a,b; Dalvi et al., 2017; Marvin and Koehn, 2018). Recently, Poliak et al. (2018a) used recast NLI datasets to investigate semantics captured by NMT encoders. Targeted Tests for Natural Language Understanding We follow a long line of work focused on building datasets to test how well NLU systems perform distinct types of semantic reasoning. FraCaS uses a limited number of sentencepairs to test whether systems understand semantic phenomena, e.g. generalized quantifiers, temporal references, and (nominal) anaphora (Cooper et al., 1996). FraCas cannot be used to train neural models – it includes j"
D18-1007,I17-1015,0,0.011537,"was not significant. We leave this investigating for a future thorough study. 5 Related Work Exploring what linguistic phenomena neural models learn Many tests have been used to probe how well neural models learn different linguistic phenomena. Linzen et al. (2016) use “number agreement in English subject-verb dependencies” to show that LSTMs learn about syntaxsensitive dependencies. In addition to syntax (Shi et al., 2016), researchers have used other labeling tasks to investigate whether neural machine translation (NMT) models learn different linguistic phenomena (Belinkov et al., 2017a,b; Dalvi et al., 2017; Marvin and Koehn, 2018). Recently, Poliak et al. (2018a) used recast NLI datasets to investigate semantics captured by NMT encoders. Targeted Tests for Natural Language Understanding We follow a long line of work focused on building datasets to test how well NLU systems perform distinct types of semantic reasoning. FraCaS uses a limited number of sentencepairs to test whether systems understand semantic phenomena, e.g. generalized quantifiers, temporal references, and (nominal) anaphora (Cooper et al., 1996). FraCas cannot be used to train neural models – it includes just roughly 300 highqua"
D18-1007,D15-1075,0,0.647225,"I Stefan had visited his son in Bulgaria Stefan was born in Bulgaria Puns I Kim heard masks have no face value Kim heard a pun I Tod heard that thrift is better than annuity Tod heard a pun 3 7 3 7 3 7 Table 1: Example sentence pairs for different semantic phenomena. I indicates the line is a context and the following line is its corresponding hypothesis. 3 and 7 respectively indicate that the context entails, or does not entail the hypothesis. Appendix A includes more recast examples. Introduction A plethora of new natural language inference (NLI)1 datasets has been created in recent years (Bowman et al., 2015; Williams et al., 2017; Lai et al., 2017; Khot et al., 2018). However, these datasets do not provide clear insight into what type of reasoning or inference a model may be performing. For example, these datasets cannot be used to evaluate whether competitive NLI models can determine if an event occurred, correctly differentiate between figurative and literal language, or accurately identify and categorize named entities. Consequently, these datasets cannot answer how well sentence representation learning models capture distinct semantic phenomena necessary for general natural language understa"
D18-1007,D11-1142,0,0.0351374,"icates were extracted directly from Wikipedia infoboxes and are not cleaned. As a result, many relations are redundant with one another (birthPlace, hometown) and some relations do not correspond to obvious natural language glosses based on the name alone (demographics1Info). Thus, we construct a template for each predicate p by manually inspecting 1) a sample of entities which are related by p 2) a sample of sentences in which those entities co-occur and 3) the most frequent natural language strings which join entities related by p according to a OpenIE triple database (Schmitz et al., 2012; Fader et al., 2011) extracted from a large text corpus. We then manually write a simple template (e.g. Mention1 was born in Mention2) for p, ignoring any unclear relations. In total, we end up with 574 unique relations, expressed by 354 unique templates. For each such hypothesis generated, we create a number of contexts. We begin with the FACC1 corpus (Gabrilovich et al., 2013) which contains natural language sentences from ClueWeb in which entities have been automatically linked to disambiguated Freebase entities, when possible. Then, given a tuple hentity1, relation, entity2i, we find every sentence which cont"
D18-1007,P15-1070,0,0.0283141,"Missing"
D18-1007,S17-2005,0,0.0126155,"onstrates natural language’s expressiveness and wide variations. Understanding and recognizing figurative language “entail[s] cognitive capabilities to abstract and meta-represent meanings beyond physical words” (Reyes et al., 2012). Puns are prime examples of figurative language that may perplex general NLU systems as they are one of the more regular uses of linguistic ambiguity (Binsted, 1996) and rely on a wide-range of phonetic, morphological, syntactic, and semantic ambiguity (Pepicello and Green, 1984; Binsted, 1996; Bekinschtein et al., 2011). We recast puns from Yang et al. (2015) and Miller et al. (2017) using templates to generate contexts (6a) and hypotheses (6b), (6c). We replace Name with names sampled from a distribution based on US census data,11 and Pun with the original sentence. If the original sentence was labeled as containing a pun, the (6a)-(6b) pair is labeled as ENTAILED and (6a)-(6c) is labeled as NOT- ENTAILED , otherwise we swap the labels. Michael swatted the fly cause(E, Agent) Agent caused the E Michael caused the swatting We use the Berkeley Parser (Petrov et al., 2006) to match tokens in an example sentence with the thematic roles and then fill in the templates with the"
D18-1007,I17-1011,0,0.0716552,"efan was born in Bulgaria Puns I Kim heard masks have no face value Kim heard a pun I Tod heard that thrift is better than annuity Tod heard a pun 3 7 3 7 3 7 Table 1: Example sentence pairs for different semantic phenomena. I indicates the line is a context and the following line is its corresponding hypothesis. 3 and 7 respectively indicate that the context entails, or does not entail the hypothesis. Appendix A includes more recast examples. Introduction A plethora of new natural language inference (NLI)1 datasets has been created in recent years (Bowman et al., 2015; Williams et al., 2017; Lai et al., 2017; Khot et al., 2018). However, these datasets do not provide clear insight into what type of reasoning or inference a model may be performing. For example, these datasets cannot be used to evaluate whether competitive NLI models can determine if an event occurred, correctly differentiate between figurative and literal language, or accurately identify and categorize named entities. Consequently, these datasets cannot answer how well sentence representation learning models capture distinct semantic phenomena necessary for general natural language understanding (NLU). To answer these questions, w"
D18-1007,P06-1055,0,0.00824722,"and Green, 1984; Binsted, 1996; Bekinschtein et al., 2011). We recast puns from Yang et al. (2015) and Miller et al. (2017) using templates to generate contexts (6a) and hypotheses (6b), (6c). We replace Name with names sampled from a distribution based on US census data,11 and Pun with the original sentence. If the original sentence was labeled as containing a pun, the (6a)-(6b) pair is labeled as ENTAILED and (6a)-(6c) is labeled as NOT- ENTAILED , otherwise we swap the labels. Michael swatted the fly cause(E, Agent) Agent caused the E Michael caused the swatting We use the Berkeley Parser (Petrov et al., 2006) to match tokens in an example sentence with the thematic roles and then fill in the templates with the matched tokens (5d). We also decompose multi-argument predicates into unary predicates to increase the number of hypotheses we generate. On average, each context is paired with 4.5 hypotheses. We generate NOT- ENTAILED hypotheses by filling in templates with incorrect thematic roles. 9 We partition the recast NLI examples into train/development/test splits such that all example sentences from a VerbNet class (which we use a NLI hypothesis) appear in only one partition of our dataset. In turn"
D18-1007,L16-1699,0,0.0478279,"Missing"
D18-1007,P09-1113,0,0.0119408,". http://www.ssa.gov/oact/babynames/ names.zip This is similar to Aharon et al. (2010)’s template matching to generate entailment rules from FrameNet (Baker et al., 1998). 11 71 (7) a. b. c. d. pairs labeled as 1 − 4 and 5 to ENTAILED and NOT- ENTAILED respectively.12 We apply pruning methods (described in Appendix B.4) to combat issues related to noisy, ungrammatical hypotheses and disagreement between multiple annotators. Name was born in Place Name is from Place Name, a Place native, . . . Name visited Place Natural language surface forms are often used in RE in a weak-supervision setting (Mintz et al., 2009; Hoffmann et al., 2011; Riedel et al., 2013). That is, if entity1 and entity2 are known to be related by relation, it is assumed that every sentence observed which mentions both entity1 and entity2 is assumed to be a realization of relation: i.e. (7d) would (falsely) be taken as evidence of the birthPlace relation. Here we first generate hypotheses and then corresponding contexts. To generate hypotheses, we begin with entity-relation triples extracted from DBPedia infoboxes: e.g. hBarack Obama, birthPlace, Hawaiii. These relation predicates were extracted directly from Wikipedia infoboxes and"
D18-1007,N18-2082,1,0.897386,"Missing"
D18-1007,C18-1198,0,0.0391277,"systems on nearly the full complexity 16 By 32.81, 31.00, and 30.83 points respectively. 74 Glockner et al. (2018) introduce a modified version of SNLI to test how well NLI models perform when requiring lexical and world knowledge. Wang et al. (2018)’s GLUE dataset is intended to evaluate and potentially train a sentence representation to perform well across different NLP tasks. This continues an aspect of the initial RTE collection, designed to be representative of downstream tasks like QA, MT, and IR (Dagan et al., 2010). While GLUE is therefore concerned with applied tasks, DNC, as well as Naik et al. (2018)’s NLI stress tests, is concerned with probing the capabilities of NLU models to capture explicitly distinguished aspects of meaning. While one may conjecture that the latter is needed to be “solved” to eventually “solve” the former, it may be that these goals only partially overlap. Some NLP researchers might focus on probing for semantic phenomena in sentence representations while others may be more interested in developing single sentence representations that can help models perform well on a wide array of downstream tasks. datasets’ training set to investigate what happens if we train our"
D18-1007,S18-2023,1,0.894734,"Missing"
D18-1007,P17-1117,0,0.0245338,"e ability to perform pronoun resolution is essential to language understanding, in many cases requiring common-sense reasoning about the world (Levesque et al., 2012). White et al. (2017) show that this task can be directly recast as an NLI problem by transforming Winograd schemas into NLI sentence pairs. Using a similar formula Rudinger et al. (2018a) introduce Winogender schemas, minimal sentence pairs that differ only by pronoun gender. With this 3 This changed as large NLI datasets have recently been used to train, or pre-train, models to perform NLI, or other tasks (Conneau et al., 2017; Pasunuru and Bansal, 2017). 4 Appendix B.1 provides an example. 5 We replace Event with the event described in the context. 6 We ensure grammatical hypotheses by appropriately conjugating “is a” when needed. 69 Sem. Phenomena Dataset # pairs Automated Event Factuality Decomp (Rudinger et al., 2018b) UW (Lee et al., 2015) MeanTime (Minard et al., 2016) 42K (41,888) 5K (5,094) .7K (738) 3 3 3 Named Entity Recognition Groningen (Bos et al., 2017) CoNLL (Tjong Kim Sang and De Meulder, 2003) 260K (261,406) 60K (59,970) 3 3 Gendered Anaphora Winogender (Rudinger et al., 2018a) .4K (464) 7 Lexicosyntactic Inference VerbCorner"
D18-1007,N13-1008,0,0.0270329,"zip This is similar to Aharon et al. (2010)’s template matching to generate entailment rules from FrameNet (Baker et al., 1998). 11 71 (7) a. b. c. d. pairs labeled as 1 − 4 and 5 to ENTAILED and NOT- ENTAILED respectively.12 We apply pruning methods (described in Appendix B.4) to combat issues related to noisy, ungrammatical hypotheses and disagreement between multiple annotators. Name was born in Place Name is from Place Name, a Place native, . . . Name visited Place Natural language surface forms are often used in RE in a weak-supervision setting (Mintz et al., 2009; Hoffmann et al., 2011; Riedel et al., 2013). That is, if entity1 and entity2 are known to be related by relation, it is assumed that every sentence observed which mentions both entity1 and entity2 is assumed to be a realization of relation: i.e. (7d) would (falsely) be taken as evidence of the birthPlace relation. Here we first generate hypotheses and then corresponding contexts. To generate hypotheses, we begin with entity-relation triples extracted from DBPedia infoboxes: e.g. hBarack Obama, birthPlace, Hawaiii. These relation predicates were extracted directly from Wikipedia infoboxes and are not cleaned. As a result, many relations"
D18-1007,P16-1204,1,0.926176,"ch that is entailed, neutral, and contradicted by a caption extracted from the Flickr30k corpus (Young et al., 2014). Although these datasets are widely used to train and evaluate sentence representations, a high accuracy is not indicative of what types of reasoning NLI models perform. Workers were free to create any type of hypothesis for each context and label. Such datasets cannot be used to determine how well an NLI model captures many desired capabilities of language understanding systems, e.g. paraphrastic inference, complex anaphora resolution (White et al., 2017), or compositionality (Pavlick and Callison-Burch, 2016; Dasgupta et al., 2018). By converting prior annotation of a specific phenomenon into NLI examples, recasting allows us to create a diverse NLI benchmark that tests a model’s ability to perform distinct types of reasoning. Why These Semantic Phenomena? A long term goal is to develop NLU systems that can achieve human levels of understanding and reasoning. Investigating how different architectures and training corpora can help a system perform human-level general NLU is an important step in this direction. DNC contains recast NLI pairs that are easily understandable by humans and can be used t"
D18-1007,P15-2067,1,0.897725,"Missing"
D18-1007,W17-1609,1,0.882467,"Missing"
D18-1007,D14-1162,0,0.0790388,"Missing"
D18-1007,N18-2002,1,0.883369,"Missing"
D18-1007,N18-1067,1,0.880757,"Missing"
D18-1007,W18-5446,0,0.0711151,"Missing"
D18-1007,I17-1100,1,0.851337,"Missing"
D18-1007,D12-1048,0,0.0309356,"i. These relation predicates were extracted directly from Wikipedia infoboxes and are not cleaned. As a result, many relations are redundant with one another (birthPlace, hometown) and some relations do not correspond to obvious natural language glosses based on the name alone (demographics1Info). Thus, we construct a template for each predicate p by manually inspecting 1) a sample of entities which are related by p 2) a sample of sentences in which those entities co-occur and 3) the most frequent natural language strings which join entities related by p according to a OpenIE triple database (Schmitz et al., 2012; Fader et al., 2011) extracted from a large text corpus. We then manually write a simple template (e.g. Mention1 was born in Mention2) for p, ignoring any unclear relations. In total, we end up with 574 unique relations, expressed by 354 unique templates. For each such hypothesis generated, we create a number of contexts. We begin with the FACC1 corpus (Gabrilovich et al., 2013) which contains natural language sentences from ClueWeb in which entities have been automatically linked to disambiguated Freebase entities, when possible. Then, given a tuple hentity1, relation, entity2i, we find ever"
D18-1007,D16-1159,0,0.0133472,"nvestigate what happens if we train our models on a subsample of each training set instead of the entire DNC. Although we noticed a slight decrease across each recast test set, the decrease was not significant. We leave this investigating for a future thorough study. 5 Related Work Exploring what linguistic phenomena neural models learn Many tests have been used to probe how well neural models learn different linguistic phenomena. Linzen et al. (2016) use “number agreement in English subject-verb dependencies” to show that LSTMs learn about syntaxsensitive dependencies. In addition to syntax (Shi et al., 2016), researchers have used other labeling tasks to investigate whether neural machine translation (NMT) models learn different linguistic phenomena (Belinkov et al., 2017a,b; Dalvi et al., 2017; Marvin and Koehn, 2018). Recently, Poliak et al. (2018a) used recast NLI datasets to investigate semantics captured by NMT encoders. Targeted Tests for Natural Language Understanding We follow a long line of work focused on building datasets to test how well NLU systems perform distinct types of semantic reasoning. FraCaS uses a limited number of sentencepairs to test whether systems understand semantic p"
D18-1007,W03-0419,0,0.503394,"Missing"
D18-1007,D15-1284,0,0.0188814,"Figurative language demonstrates natural language’s expressiveness and wide variations. Understanding and recognizing figurative language “entail[s] cognitive capabilities to abstract and meta-represent meanings beyond physical words” (Reyes et al., 2012). Puns are prime examples of figurative language that may perplex general NLU systems as they are one of the more regular uses of linguistic ambiguity (Binsted, 1996) and rely on a wide-range of phonetic, morphological, syntactic, and semantic ambiguity (Pepicello and Green, 1984; Binsted, 1996; Bekinschtein et al., 2011). We recast puns from Yang et al. (2015) and Miller et al. (2017) using templates to generate contexts (6a) and hypotheses (6b), (6c). We replace Name with names sampled from a distribution based on US census data,11 and Pun with the original sentence. If the original sentence was labeled as containing a pun, the (6a)-(6b) pair is labeled as ENTAILED and (6a)-(6c) is labeled as NOT- ENTAILED , otherwise we swap the labels. Michael swatted the fly cause(E, Agent) Agent caused the E Michael caused the swatting We use the Berkeley Parser (Petrov et al., 2006) to match tokens in an example sentence with the thematic roles and then fill"
D18-1007,L18-1239,0,0.0700741,"leveraging existing datasets to create NLI examples (Glickman, 2006; White et al., 2017). We recast annotations from a total of 13 datasets across 7 NLP tasks into labeled NLI examples. The tasks include event factuality, named entity recognition, gendered anaphora resolution, sentiment analysis, relationship extraction, pun detection, and lexicosyntactic inference. Currently, the DNC contains over half a million labeled examples. Table 1 includes NLI pairs that test specific types of reasoning. Using a hypothesis-only NLI model, with access to just hypothesis sentences, as a strong baseline (Tsuchiya, 2018; Gururangan et al., 2018; Poliak et al., 2018b), our experiments demonstrate how DNC can be used to probe a model’s ability to capture different types of semantic reasoning 1 The task of determining if a hypothesis would likely be inferred from a context, or premise; also known as Recognizing Textual Entailment (RTE) (Dagan et al., 2006, 2013). 67 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 67–81 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics We limit some biases by not relying on humans to g"
D18-1007,Q14-1006,0,0.0500487,"of semantic NLP tasks are freely available. By leveraging existing semantic annotations already invested in by the community we can generate and label NLI pairs at little cost and create large NLI datasets to train data hungry models. NLU Insights Popular NLI datasets, e.g. Stanford Natural Language Inference (SNLI) (Bowman et al., 2015) and its successor MultiNLI (Williams et al., 2017), were created by eliciting hypotheses from humans. Crowd-source workers were tasked with writing one sentence each that is entailed, neutral, and contradicted by a caption extracted from the Flickr30k corpus (Young et al., 2014). Although these datasets are widely used to train and evaluate sentence representations, a high accuracy is not indicative of what types of reasoning NLI models perform. Workers were free to create any type of hypothesis for each context and label. Such datasets cannot be used to determine how well an NLI model captures many desired capabilities of language understanding systems, e.g. paraphrastic inference, complex anaphora resolution (White et al., 2017), or compositionality (Pavlick and Callison-Burch, 2016; Dasgupta et al., 2018). By converting prior annotation of a specific phenomenon in"
D18-1007,Q17-1027,1,0.888526,"Missing"
D18-1028,P16-1231,0,0.0141616,"ordpiece model (Schuster and Nakajima, 2012) with a vocabulary of 16,000. Experimental Design. We train one version of this model on the same set of 23M English examples as the discriminative insertion model from §6.1; we refer to the model trained on this data as Edits. For comparison, we train an identical model on a set of simulated insertions which we create by sampling sentences from Wikipedia and removing contiguous spans of tokens, which we then treat as the insertion phrases. To ensure that this data is reasonably comparable to the WikiAtomicEdits data, we parse the sampled sentences (Andor et al., 2016) and only remove a span if it represents a full subtree of the dependency parse and is not the subject of the sentence.11 We generate 23M such “psuedo-edits” for training, the same 7 Related Work Wikipedia Edits. Wikipedia edit history has been used as a source of supervision for a variety of NLP tasks, including sentence compression and simplification (Yamangil and Nelken, 2008; Yatskar et al., 2010), paraphrasing (Max and Wisniewski, 2010), entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), and writing assistance (Zesch, 2012; Cahill et al., 2013; Grundkiewicz and Junczys-Do"
D18-1028,W12-4006,0,0.388033,"a is reasonably comparable to the WikiAtomicEdits data, we parse the sampled sentences (Andor et al., 2016) and only remove a span if it represents a full subtree of the dependency parse and is not the subject of the sentence.11 We generate 23M such “psuedo-edits” for training, the same 7 Related Work Wikipedia Edits. Wikipedia edit history has been used as a source of supervision for a variety of NLP tasks, including sentence compression and simplification (Yamangil and Nelken, 2008; Yatskar et al., 2010), paraphrasing (Max and Wisniewski, 2010), entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), and writing assistance (Zesch, 2012; Cahill et al., 2013; Grundkiewicz and Junczys-Dowmunt, 2014). User edits from Wikipedia and elsewhere have also been analyzed extensively for insight into the editing process and the types of edits made (Daxenberger and Gurevych, 2012, 2013; Yang et al., 2017) 11 Not all of the inserted phrases in WikiAtomicEdits are well-formed constituents. However, generating psuedo-edits using this heuristic provided a cleaner, more realistic comparison than using fully-random spans. 312 She is cited as the first female superstar of Hindi Cinema and India ’s Meryl Str"
D18-1028,N13-1055,0,0.0610098,"parse the sampled sentences (Andor et al., 2016) and only remove a span if it represents a full subtree of the dependency parse and is not the subject of the sentence.11 We generate 23M such “psuedo-edits” for training, the same 7 Related Work Wikipedia Edits. Wikipedia edit history has been used as a source of supervision for a variety of NLP tasks, including sentence compression and simplification (Yamangil and Nelken, 2008; Yatskar et al., 2010), paraphrasing (Max and Wisniewski, 2010), entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), and writing assistance (Zesch, 2012; Cahill et al., 2013; Grundkiewicz and Junczys-Dowmunt, 2014). User edits from Wikipedia and elsewhere have also been analyzed extensively for insight into the editing process and the types of edits made (Daxenberger and Gurevych, 2012, 2013; Yang et al., 2017) 11 Not all of the inserted phrases in WikiAtomicEdits are well-formed constituents. However, generating psuedo-edits using this heuristic provided a cleaner, more realistic comparison than using fully-random spans. 312 She is cited as the first female superstar of Hindi Cinema and India ’s Meryl Streep Edits General and is the best actress of the film in j"
D18-1028,max-wisniewski-2010-mining,0,0.0968145,"ns, which we then treat as the insertion phrases. To ensure that this data is reasonably comparable to the WikiAtomicEdits data, we parse the sampled sentences (Andor et al., 2016) and only remove a span if it represents a full subtree of the dependency parse and is not the subject of the sentence.11 We generate 23M such “psuedo-edits” for training, the same 7 Related Work Wikipedia Edits. Wikipedia edit history has been used as a source of supervision for a variety of NLP tasks, including sentence compression and simplification (Yamangil and Nelken, 2008; Yatskar et al., 2010), paraphrasing (Max and Wisniewski, 2010), entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), and writing assistance (Zesch, 2012; Cahill et al., 2013; Grundkiewicz and Junczys-Dowmunt, 2014). User edits from Wikipedia and elsewhere have also been analyzed extensively for insight into the editing process and the types of edits made (Daxenberger and Gurevych, 2012, 2013; Yang et al., 2017) 11 Not all of the inserted phrases in WikiAtomicEdits are well-formed constituents. However, generating psuedo-edits using this heuristic provided a cleaner, more realistic comparison than using fully-random spans. 312 She is cited"
D18-1028,L18-1008,0,0.0227062,"13). We train this language model for each language on an average of ∼ 500 million tokens from Wikipedia. Second, we evaluate a discriminative model specifically trained on the insertion data (Discriminative Model). This model represents the base sentence using a sentence encoder that produces a contextdependent representation of every word index in the sentence, and then at test time, compares the learned representation of each index with the representation of the phrase p to be inserted. We use a 256-dimensional 2-layer biLSTM encoder, initialized with FastText 300-dimensional word vectors (Mikolov et al., 2018; Grave et al., 2018).7 We hold out 50K and 10K insertion edits for each language as development and test sets, and use the remaining edits (insertions and deletions) as training data. This provides us with at least 1 million examples for training in each language (cf. Table 2). See Supplementary Material for additional details. General LM 68.1 58.7 67.0 69.9 69.0 73.0 72.9 65.5 68.0 Discr. Model 72.9 68.4 70.1 73.4 72.9 74.2 74.3 68.9 71.8 Table 8: Insertion accuracy on the test set. make. For each model, we take a random sample of 50 examples on which the model made a correct prediction and"
D18-1028,D17-1070,0,0.0589149,"Missing"
D18-1028,C12-1044,0,0.175753,"” for training, the same 7 Related Work Wikipedia Edits. Wikipedia edit history has been used as a source of supervision for a variety of NLP tasks, including sentence compression and simplification (Yamangil and Nelken, 2008; Yatskar et al., 2010), paraphrasing (Max and Wisniewski, 2010), entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), and writing assistance (Zesch, 2012; Cahill et al., 2013; Grundkiewicz and Junczys-Dowmunt, 2014). User edits from Wikipedia and elsewhere have also been analyzed extensively for insight into the editing process and the types of edits made (Daxenberger and Gurevych, 2012, 2013; Yang et al., 2017) 11 Not all of the inserted phrases in WikiAtomicEdits are well-formed constituents. However, generating psuedo-edits using this heuristic provided a cleaner, more realistic comparison than using fully-random spans. 312 She is cited as the first female superstar of Hindi Cinema and India ’s Meryl Streep Edits General and is the best actress of the film in japan and is the best actress of the Indian cinema in june in 2011 and is the best actress of the film industry He is married to Aida Leanca and has two children Edits General and has a daughter in january , and has"
D18-1028,D17-1064,0,0.0514149,"Missing"
D18-1028,D13-1055,0,0.14171,"Missing"
D18-1028,N09-2061,0,0.0135701,"1.1 42.9 Table 2: The number of instances (in millions) of atomic insertions/deletions for each language. 3 3.1 WikiAtomicEdits: Corpus Creation Extracting Edits Wikipedia edits can be accessed through Wikipedia dumps. The edits are stored as diffs on the entire Wikipedia page, meaning some processing is required to reconstruct the changes that were made at the sentence level. We use historical snapshots of each Wikipedia document and compare against subsequent snapshots to extract sentence-level edits. We strip the HTML tags and Wikipedia markup of the page and then run a sentence splitter (Gillick, 2009) to obtain a list of sentences for each snapshot. Rather than run a full, quadratic-time (Myers, 1986) sequence alignment to compare the two lists of sentences, which is infeasible for long articles, we propose an efficient precision-oriented approximation. Given n sentences in one snapshot (“base”) and m sentences in a subsequent one (“edited”), we assume that most edits are local and restrict our attention to a fixed-size window. For each sentence si in the base snapshot, we compute pairwise BLEU scores (Papineni et al., 2002) between si i+k and the sentences {tj }j=i−k (k = 5) in the edited"
D18-1028,L18-1550,0,0.0242656,"guage model for each language on an average of ∼ 500 million tokens from Wikipedia. Second, we evaluate a discriminative model specifically trained on the insertion data (Discriminative Model). This model represents the base sentence using a sentence encoder that produces a contextdependent representation of every word index in the sentence, and then at test time, compares the learned representation of each index with the representation of the phrase p to be inserted. We use a 256-dimensional 2-layer biLSTM encoder, initialized with FastText 300-dimensional word vectors (Mikolov et al., 2018; Grave et al., 2018).7 We hold out 50K and 10K insertion edits for each language as development and test sets, and use the remaining edits (insertions and deletions) as training data. This provides us with at least 1 million examples for training in each language (cf. Table 2). See Supplementary Material for additional details. General LM 68.1 58.7 67.0 69.9 69.0 73.0 72.9 65.5 68.0 Discr. Model 72.9 68.4 70.1 73.4 72.9 74.2 74.3 68.9 71.8 Table 8: Insertion accuracy on the test set. make. For each model, we take a random sample of 50 examples on which the model made a correct prediction and 50 examples on which"
D18-1028,P02-1040,0,0.105117,"L tags and Wikipedia markup of the page and then run a sentence splitter (Gillick, 2009) to obtain a list of sentences for each snapshot. Rather than run a full, quadratic-time (Myers, 1986) sequence alignment to compare the two lists of sentences, which is infeasible for long articles, we propose an efficient precision-oriented approximation. Given n sentences in one snapshot (“base”) and m sentences in a subsequent one (“edited”), we assume that most edits are local and restrict our attention to a fixed-size window. For each sentence si in the base snapshot, we compute pairwise BLEU scores (Papineni et al., 2002) between si i+k and the sentences {tj }j=i−k (k = 5) in the edited snapshot. We consider the sentence with the highest BLEU score in this window as a candidate. If the sentences are not identical and the difference consists of an insertion or deletion of a single contiguous phrase2 , we add this example to the corpus. For each article, we run this algorithm over the most recent 100,000 snapshots as of February 2018. We extract edits for 8 languages. Statistics are shown in Table 2. 1. The original sentence s does not effectively communicate some piece of information. 2. A reasonable reader of"
D18-1028,D14-1162,0,0.0809643,"n to reporting standard LM perplexity, we compute two measures of performance, which are intended to provide an intuitive picture of how well each model captures the nature of the information that is introduced by the human editors. Specifically, we compute Exact Match as the proportion of sentences for which the model produced the gold phrase (i.e. the phrase inserted by the human editor) somewhere among the top 10 phrases. We also compute Similarity@1 as the mean cosine similarity of each top-ranked phrase and respective gold phrase over the test set. We use the sum of the Glove embeddings (Pennington et al., 2014) of each word in the phrase as a simple approximation of the phrase vector. Table 11 shows the results. We see that, compared to the model trained on General Wikipedia, the model trained on WikiAtomicEdits generates edits which are more similar to the human insertions, according to all of our metrics. Table 10 provides a few qualitative examples of how the phrases generated by the Edits model differ from those generated by the General model. Specifically, we see that the Edits model proposes phrases which better capture the discourse function of the human edit: e.g. providing context for/elabo"
D18-1028,N18-1202,0,0.0802425,"Missing"
D18-1028,Q18-1031,0,0.0624248,"Missing"
D18-1028,P14-2066,0,0.0255462,"e(s)) have been POS-tagged and dependency parsed (Andor et al., 2016) as well as scored using a SOTA LM (Jozefowicz et al., 2016). We also release the 5K 5-way human insertion annotations for English, and 1K 3-way annotations each for Spanish and German, as described in §4. Table 11: Comparison of how closely each model’s generated phrases match the phrase inserted by the human editor. “Edits” was trained on WikiAtomicEdits and “General” was trained on comparable data not derived from human edits. We consider the top 10 phrases generated by each model. and to better understand argumentation (Tan and Lee, 2014). Particular attention has been given to spam edits (Adler et al., 2011) and editor quality (Leskovec et al., 2010). Our work differs in that WikiAtomicEdits is much larger than currently available corpora, both by number of languages and by size of individual languages. In addition, our focus on atomic edits should facilitate more controlled studies of semantics and discourse. 8 Conclusion We have introduced the WikiAtomicEdits corpus, derived from Wikipedia’s edit history, which contains 43M examples of atomic insertions and deletions in 8 languages. We have shown that the language in this c"
D18-1028,D15-1059,0,0.0126598,"index i, generate candidate phrases that would be appropriate to insert into s at i (§6.2). 6.1 Predicting Insertion Locations Task. This task–given a phrase p and a sentence s, choose the best index i in s at which to insert p–is identical to the task we asked humans to perform in §4. We consider two simple models for performing this task: a basic language model and a discriminative model trained on the insertion data. We report performance as overall accuracy. We analyze whether a model which is trained to 6 We note that the addition of “former” is likely tied to changes in the real world (Wijaya et al., 2015). 310 model insertions directly captures something different than a general language model in terms of the types of errors each model makes. German English Spanish French Italian Japanese Russian Chinese Average Models. We evaluate two models. First, we evaluate a standard language modeling baseline (General LM), in which we simply insert the phrase p at every possible point in s and chose the index which yields the lowest perplexity. We use the LSTM language model from Jozefowicz et al. (2016), which obtained SOTA results on language modeling on the one billion words benchmark for English (Ch"
D18-1028,P08-2035,0,0.0313163,"g sentences from Wikipedia and removing contiguous spans of tokens, which we then treat as the insertion phrases. To ensure that this data is reasonably comparable to the WikiAtomicEdits data, we parse the sampled sentences (Andor et al., 2016) and only remove a span if it represents a full subtree of the dependency parse and is not the subject of the sentence.11 We generate 23M such “psuedo-edits” for training, the same 7 Related Work Wikipedia Edits. Wikipedia edit history has been used as a source of supervision for a variety of NLP tasks, including sentence compression and simplification (Yamangil and Nelken, 2008; Yatskar et al., 2010), paraphrasing (Max and Wisniewski, 2010), entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), and writing assistance (Zesch, 2012; Cahill et al., 2013; Grundkiewicz and Junczys-Dowmunt, 2014). User edits from Wikipedia and elsewhere have also been analyzed extensively for insight into the editing process and the types of edits made (Daxenberger and Gurevych, 2012, 2013; Yang et al., 2017) 11 Not all of the inserted phrases in WikiAtomicEdits are well-formed constituents. However, generating psuedo-edits using this heuristic provided a cleaner, more reali"
D18-1028,D17-1213,0,0.555849,"n at both the phrase and the sentence level. We mine Wikipedia edit history to create a corpus of 43 million atomic insertion and deletion edits covering 8 languages. We argue that the corpus contains distinct semantic signals not present in raw text. We thus focus our experiments on answering the following questions: Introduction Written language often undergoes several rounds of revision as human authors determine exactly what information they want their words to convey. On Wikipedia, this process is carried out collectively by a large community at a rate of nearly two revisions per second (Yang et al., 2017). While Wikipedia’s revision history contains arbitrarily complex edits, our corpus and analysis focuses on atomic insertion edits: instances in which an editor has inserted a single, contiguous span of text into an existing complete sentence (Table 1). This restriction allows us to make several assumptions which we believe make the data an especially powerful source of signal. Namely, we can assume that 1) some information was not communicated by the original sentence, 2) that information should have been communicated (according to a human editor), and 3) that information is communicated by t"
D18-1028,N10-1056,0,0.06066,"and removing contiguous spans of tokens, which we then treat as the insertion phrases. To ensure that this data is reasonably comparable to the WikiAtomicEdits data, we parse the sampled sentences (Andor et al., 2016) and only remove a span if it represents a full subtree of the dependency parse and is not the subject of the sentence.11 We generate 23M such “psuedo-edits” for training, the same 7 Related Work Wikipedia Edits. Wikipedia edit history has been used as a source of supervision for a variety of NLP tasks, including sentence compression and simplification (Yamangil and Nelken, 2008; Yatskar et al., 2010), paraphrasing (Max and Wisniewski, 2010), entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), and writing assistance (Zesch, 2012; Cahill et al., 2013; Grundkiewicz and Junczys-Dowmunt, 2014). User edits from Wikipedia and elsewhere have also been analyzed extensively for insight into the editing process and the types of edits made (Daxenberger and Gurevych, 2012, 2013; Yang et al., 2017) 11 Not all of the inserted phrases in WikiAtomicEdits are well-formed constituents. However, generating psuedo-edits using this heuristic provided a cleaner, more realistic comparison than us"
D18-1028,E12-1054,0,0.080273,"its data, we parse the sampled sentences (Andor et al., 2016) and only remove a span if it represents a full subtree of the dependency parse and is not the subject of the sentence.11 We generate 23M such “psuedo-edits” for training, the same 7 Related Work Wikipedia Edits. Wikipedia edit history has been used as a source of supervision for a variety of NLP tasks, including sentence compression and simplification (Yamangil and Nelken, 2008; Yatskar et al., 2010), paraphrasing (Max and Wisniewski, 2010), entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), and writing assistance (Zesch, 2012; Cahill et al., 2013; Grundkiewicz and Junczys-Dowmunt, 2014). User edits from Wikipedia and elsewhere have also been analyzed extensively for insight into the editing process and the types of edits made (Daxenberger and Gurevych, 2012, 2013; Yang et al., 2017) 11 Not all of the inserted phrases in WikiAtomicEdits are well-formed constituents. However, generating psuedo-edits using this heuristic provided a cleaner, more realistic comparison than using fully-random spans. 312 She is cited as the first female superstar of Hindi Cinema and India ’s Meryl Streep Edits General and is the best act"
D18-1028,W10-3504,0,\N,Missing
D18-1202,J84-3009,0,0.153249,"Missing"
D18-1202,P18-1128,0,0.0192846,"* {quick} < {fast} < {speedy}* †† : p ≤ .01 †: p ≤ .05 Table 2: Pairwise relation prediction and global ranking results for each score type in isolation, and for the bestscoring combinations of 2 and 3 score types on each dataset. For the global ranking accuracy and average τb results, we denote with the † symbol scores for metrics incorporating paraphrase-based evidence that significantly out-perform both scorepat and scoresocal under the paired Student’s t-test, using the Anderson-Darling test to confirm that scores conform to a normal distribution (Fisher, 1935; Anderson and Darling, 1954; Dror et al., 2018). Example output is also given, with correct rankings starred. or positive correlation, and a value of 0 indicating no correlation between predicted and gold rankings. We report τb as a weighted average over scales in each dataset, where weights correspond to the number of adjective pairs in each scale. Spearman’s rho (ρ). We report the Spearman’s ρ rank correlation coefficient between predicted (rP (J)) and gold-standard (rG (J)) ranking permutations. For each dataset, we calculate this metric just once by treating each adjective in a particular scale as a single data point, and calculating a"
D18-1202,N13-1092,1,0.86228,"Missing"
D18-1202,P93-1023,0,0.785283,"xtracting sets of same-attribute adjectives from WordNet ‘dumbbells’ – consisting of two direct antonyms at the poles and satellites of synonymous/related adjectives incident to each antonym (Gross and Miller, 1990) – and ordering them by intensity. The annotations, however, are not in WordNet as of its latest version (3.1). Work on adjective intensity generally focuses on two related tasks: clustering adjectives based on the attributes they modify, and ranking sameattribute adjectives by intensity. With respect to the former, common approaches involve clustering adjectives by their contexts (Hatzivassiloglou and McKeown, 1993; Shivade et al., 2015). We do not focus on the clustering task in this paper, but concentrate on the ranking task. Approaches to the task of ranking scalar adjectives by their intensity mostly fall under the paradigms of pattern-based or lexicon-based approaches. Pattern-based approaches work by extracting lexical (Sheinman and Tokunaga, 2009; de Melo and Bansal, 2013; Sheinman et al., 2013) or syntactic (Shivade et al., 2015) patterns indicative of an intensity relationship from large corpora. For example, the patterns “X, but not Y” and “not just X but Y” provide evidence that X is an adjec"
D18-1202,C92-2082,0,0.163772,"approaches rely particularly pleased quite limited rather odd so silly completely mad ↔ ↔ ↔ ↔ ↔ ecstatic restricted crazy dumb crazy Figure 1: Examples of paraphrases from PPDB of the form RB JJu ↔ JJv which can be used to infer pairwise intensity relationships (JJu < JJv ). on pattern-based or lexicon-based methods to predict the intensity ranking of adjectives. Patternbased approaches search large corpora for lexical patterns that indicate an intensity relationship – for example, “not just X, but Y” implies X < Y. As with pattern-based approaches for other tasks (such as hypernym discovery (Hearst, 1992)), they are precise but have relatively sparse coverage of comparable adjectives, even when using webscale corpora (de Melo and Bansal, 2013; Ruppenhofer et al., 2014). Lexicon-based approaches employ resources that map an adjective to a realvalued number that encodes both intensity and polarity (e.g. good might map to 1 and phenomenal to 5, while bad maps to -1 and awful to -3). They can also be precise, but may not cover all adjectives of interest. We propose paraphrases as a new source of evidence for the relative intensity of scalar adjectives. A paraphrase is a pair of words or phrases wi"
D18-1202,D13-1169,0,0.230759,"Missing"
D18-1202,P10-1018,0,0.34145,"Missing"
D18-1202,Q13-1023,0,0.647048,"Missing"
D18-1202,P15-2070,1,0.904063,"Missing"
D18-1202,R15-1071,0,0.0652019,"adjectives ju and jv in JJG RAPH provides evidence about the relative intensity relationship between them. However, it has just been noted that JJG RAPH is noisy, containing both contradictory/cyclic edges and adverbs that are not uniformly intensifying. Rather than try to eliminate cycles, or manually annotate each adverb with a weight corresponding to its intensity and polarity 4 Other Intensity Evidence Our experiments compare the proposed paraphrase approach with existing pattern- and lexicon-based approaches. 4.1 Figure 3: A subgraph of JJG RAPH, depicting its directed graph structure. (Ruppenhofer et al., 2015; Taboada et al., 2011), we aim to learn these weights automatically in the process of predicting pairwise intensity. Given adjective pair (ju , jv ), we build a classifier that outputs a score from 0 to 1 indicating the predicted likelihood that ju < jv . Its binary features correspond to adverb edges from ju to jv and from jv to ju in JJG RAPH. The feature space includes only adverbs from R that appear at least 10 times in JJG RAPH, resulting in features for m = 259 unique adverbs in each direction (i.e. from ju to jv and vice versa) for 2m = 518 binary features total. Note that while all ad"
D18-1202,E14-4023,0,0.0822145,"paraphrases from PPDB of the form RB JJu ↔ JJv which can be used to infer pairwise intensity relationships (JJu < JJv ). on pattern-based or lexicon-based methods to predict the intensity ranking of adjectives. Patternbased approaches search large corpora for lexical patterns that indicate an intensity relationship – for example, “not just X, but Y” implies X < Y. As with pattern-based approaches for other tasks (such as hypernym discovery (Hearst, 1992)), they are precise but have relatively sparse coverage of comparable adjectives, even when using webscale corpora (de Melo and Bansal, 2013; Ruppenhofer et al., 2014). Lexicon-based approaches employ resources that map an adjective to a realvalued number that encodes both intensity and polarity (e.g. good might map to 1 and phenomenal to 5, while bad maps to -1 and awful to -3). They can also be precise, but may not cover all adjectives of interest. We propose paraphrases as a new source of evidence for the relative intensity of scalar adjectives. A paraphrase is a pair of words or phrases with approximately similar meaning, such as really great ↔ phenomenal. Adjectival paraphrases can be exploited to uncover intensity relationships. A paraphrase pair of t"
D18-1202,D15-1300,0,0.0141514,"patterns “X, but not Y” and “not just X but Y” provide evidence that X is an adjective less intense than Y. Lexicon-based approaches are derived from the premise that adjectives can provide information about the sentiment of a text (Hatzivassiloglou and McKeown, 1993). These methods draw upon a 1 www.paraphrase.org lexicon that maps adjectives to real-valued scores encoding both sentiment polarity and intensity. The lexicon might be compiled automatically – for example, from analyzing adjectives’ appearance in star-valued product or movie reviews (de Marneffe et al., 2010; Rill et al., 2012; Sharma et al., 2015; Ruppenhofer et al., 2014) – or manually. In our experiments we utilize the manually-compiled SO-CAL lexicon (Taboada et al., 2011). Our paraphrase-based approach to inferring relative adjective intensity is based on paraphrases that combine adjectives with adverbial modifiers. A tangentially related approach is Collex (Ruppenhofer et al., 2014), which is motivated by the intuition that adjectives with extreme intensities are modified by different adverbs from adjectives with more moderate intensities: extreme adverbs like absolutely are more likely to modify extreme adjectives like brilliant"
D18-1202,N15-1051,0,0.413677,"Missing"
D18-1202,J11-2001,0,0.101837,"hes are derived from the premise that adjectives can provide information about the sentiment of a text (Hatzivassiloglou and McKeown, 1993). These methods draw upon a 1 www.paraphrase.org lexicon that maps adjectives to real-valued scores encoding both sentiment polarity and intensity. The lexicon might be compiled automatically – for example, from analyzing adjectives’ appearance in star-valued product or movie reviews (de Marneffe et al., 2010; Rill et al., 2012; Sharma et al., 2015; Ruppenhofer et al., 2014) – or manually. In our experiments we utilize the manually-compiled SO-CAL lexicon (Taboada et al., 2011). Our paraphrase-based approach to inferring relative adjective intensity is based on paraphrases that combine adjectives with adverbial modifiers. A tangentially related approach is Collex (Ruppenhofer et al., 2014), which is motivated by the intuition that adjectives with extreme intensities are modified by different adverbs from adjectives with more moderate intensities: extreme adverbs like absolutely are more likely to modify extreme adjectives like brilliant than are moderate adverbs like very. Unlike Collex, which requires predetermined sets of ‘end-of-scale’ and ‘normal’ adverbial modi"
D18-1202,L16-1424,0,0.507498,"ng a full scale (e.g. freezing to sweltering), or a half scale (warm to sweltering); all three test sets group adjectives into half scales. The three datasets are described here, and their characteristics are given in Table 1. deMelo (de Melo and Bansal, 2013)4 . 87 adjective 4 http://demelo.org/gdm/intensity/ sets are extracted from WordNet ‘dumbbell’ structures (Gross and Miller, 1990), and partitioned into half-scale sets based on their pattern-based evidence in the Google N-Grams corpus (Brants and Franz, 2009). Sets are manually annotated for intensity relations (<, >, and =). Wilkinson (Wilkinson and Oates, 2016). Twelve adjective sets are generated by presenting crowd workers with small seed sets (e.g. huge, small, microscopic), and eliciting similar adjectives. Sets are automatically cleaned for consistency, and then annotated for intensity by crowd workers. While the original dataset contains full scales, we manually sub-divide these into 21 half-scales for use in this study. Details on the modification from full- to half-scales are in the Supplemental Material. Crowd. We also crowdsourced a new set of adjective scales with high coverage of the PPDB vocabulary. In a three-step process, we first ask"
D19-1228,D15-1075,0,0.0445507,"oach: we take the position that models which consistently mirror human inferences about veridicality in context can be said to understand veridicality in general. We acknowledge that the question of what is the “right” approach to NLI has existed since the original definition of the recognizing textual entailment (RTE) task (Dagan et al., 2006) and remains open. However, there has been a de facto endorsement of the speakermeaning definition, evidenced by the widespread adoption of NLI datasets which favor informal, “natural” inferences over prescriptivist annotation guidelines (Manning, 2006; Bowman et al., 2015; Williams et al., 2018). (Note, recently, there have been explicit endorsements as well; see Westera 2231 Factive [+/+] He realized that he had to leave this house. He did not realize that he had to leave this house. → → He had to leave this house. He had to leave this house. Implic. [+/−] At that moment, I happened to look up. At that moment, I did not happen to look up. → →¬ At that moment, I looked up. At that moment, I looked up. Implic. [−/◦] He refused to do the same. He did not refuse to do the same. →¬ 6→ He did the same. He did the same. NA [◦/◦] Many felt that its inclusion was a mi"
D19-1228,N19-1365,0,0.0622757,"ciated logical inference rules. MacCartney and Manning (2009); Angeli and Manning (2014); and others incorporated knowledge of verb signatures within a natural logic framework (MacCartney, 2009; S´anchez Valencia, 1991) in order to perform natural language inference. Richardson and Kuhn (2012) incorporated signatures into a semantic parsing system. Several recent models of event factuality similarly make use of veridicality lexicons as input to larger machine-learned systems for event factuality (Saur´ı and Pustejovsky, 2012; Lotan et al., 2013; Stanovsky et al., 2017; Rudinger et al., 2018). Cases et al. (2019) used nested veridicality inferences as a test case for a meta-learning model, again assuming verb signatures as “meta information” known a priori. Pragmatics (Speaker Meaning). Geis and Zwicky (1971) observed that implicative verbs often give rise to “invited inferences”, beyond what is explainable by the lexical semantic type of the verb. For example, on hearing “He did not refuse to speak”, one naturally concludes that “He spoke” unless additional qualifications are made (e.g. “...he just didn’t have anything to say”). de Marneffe et al. (2012) explored this idea in depth and presented evid"
D19-1228,W17-6807,0,0.0155359,"6). Work on veridicality which aligns with the sentencemeaning perspective tends to focus on characterizing verbs according to their lexical semantic classes (or “signatures”), while work which aligns with the speaker-meaning approach focuses on representing “world knowledge” and evaluating inferences in naturalistic contexts. Lexical Semantics (Sentence Meaning). Most prior work treats veridicality as a lexical semantic phenomenon. Such work is largely based on lexicons of verb signatures which specify the types of inferences licensed by individual verbs (Karttunen, 2012; Nairn et al., 2006; Falk and Martin, 2017). White and Rawlins (2018); White et al. (2018) evaluated neural models’ ability to carry out inferences in line with these signatures, making use of templatized “semantically bleached” stimuli (e.g. “someone knew something”) in order to avoid confounds introduced by world knowledge and pragmatic inference. McCoy et al. (2019) perform a similar study, though without specific focus on veridicality lexicons. Most applied work related to veridicality also falls under the lexical semantic approach. In nearly all cases, relevant system development involves explicit incorporation of verb lexicons an"
D19-1228,S12-1020,0,0.161072,"which it was generated (Manning, 2006). Work on veridicality which aligns with the sentencemeaning perspective tends to focus on characterizing verbs according to their lexical semantic classes (or “signatures”), while work which aligns with the speaker-meaning approach focuses on representing “world knowledge” and evaluating inferences in naturalistic contexts. Lexical Semantics (Sentence Meaning). Most prior work treats veridicality as a lexical semantic phenomenon. Such work is largely based on lexicons of verb signatures which specify the types of inferences licensed by individual verbs (Karttunen, 2012; Nairn et al., 2006; Falk and Martin, 2017). White and Rawlins (2018); White et al. (2018) evaluated neural models’ ability to carry out inferences in line with these signatures, making use of templatized “semantically bleached” stimuli (e.g. “someone knew something”) in order to avoid confounds introduced by world knowledge and pragmatic inference. McCoy et al. (2019) perform a similar study, though without specific focus on veridicality lexicons. Most applied work related to veridicality also falls under the lexical semantic approach. In nearly all cases, relevant system development involve"
D19-1228,J12-2003,0,0.0657384,"Missing"
D19-1228,P19-1334,1,0.862895,"Missing"
D19-1228,W06-3907,0,0.0684411,"erated (Manning, 2006). Work on veridicality which aligns with the sentencemeaning perspective tends to focus on characterizing verbs according to their lexical semantic classes (or “signatures”), while work which aligns with the speaker-meaning approach focuses on representing “world knowledge” and evaluating inferences in naturalistic contexts. Lexical Semantics (Sentence Meaning). Most prior work treats veridicality as a lexical semantic phenomenon. Such work is largely based on lexicons of verb signatures which specify the types of inferences licensed by individual verbs (Karttunen, 2012; Nairn et al., 2006; Falk and Martin, 2017). White and Rawlins (2018); White et al. (2018) evaluated neural models’ ability to carry out inferences in line with these signatures, making use of templatized “semantically bleached” stimuli (e.g. “someone knew something”) in order to avoid confounds introduced by world knowledge and pragmatic inference. McCoy et al. (2019) perform a similar study, though without specific focus on veridicality lexicons. Most applied work related to veridicality also falls under the lexical semantic approach. In nearly all cases, relevant system development involves explicit incorpora"
D19-1228,D16-1240,1,0.845599,"o “invited inferences”, beyond what is explainable by the lexical semantic type of the verb. For example, on hearing “He did not refuse to speak”, one naturally concludes that “He spoke” unless additional qualifications are made (e.g. “...he just didn’t have anything to say”). de Marneffe et al. (2012) explored this idea in depth and presented evidence that such pragmatic inferences are both pervasive and annotatordependent, but nonetheless systematic enough to be relevant for NLP models. Karttunen et al. (2014) makes similar observations specifically in the case of evaluative adjectives, and Pavlick and Callison-Burch (2016) specifically in the case of simple implicative verbs. In non-computational linguistics, Simons et al. (2017, 2010); Tonhauser et al. (2018) take a strong stance and argue that veridicality judgements are entirely pragmatic, dependent solely on the question under discussion (QUD) within the given discourse. This Work. This paper assumes the speakermeaning approach: we take the position that models which consistently mirror human inferences about veridicality in context can be said to understand veridicality in general. We acknowledge that the question of what is the “right” approach to NLI has"
D19-1228,C12-2098,0,0.155572,"knowledge and pragmatic inference. McCoy et al. (2019) perform a similar study, though without specific focus on veridicality lexicons. Most applied work related to veridicality also falls under the lexical semantic approach. In nearly all cases, relevant system development involves explicit incorporation of verb lexicons and associated logical inference rules. MacCartney and Manning (2009); Angeli and Manning (2014); and others incorporated knowledge of verb signatures within a natural logic framework (MacCartney, 2009; S´anchez Valencia, 1991) in order to perform natural language inference. Richardson and Kuhn (2012) incorporated signatures into a semantic parsing system. Several recent models of event factuality similarly make use of veridicality lexicons as input to larger machine-learned systems for event factuality (Saur´ı and Pustejovsky, 2012; Lotan et al., 2013; Stanovsky et al., 2017; Rudinger et al., 2018). Cases et al. (2019) used nested veridicality inferences as a test case for a meta-learning model, again assuming verb signatures as “meta information” known a priori. Pragmatics (Speaker Meaning). Geis and Zwicky (1971) observed that implicative verbs often give rise to “invited inferences”, b"
D19-1228,N18-1067,0,0.0334335,"Missing"
D19-1228,J12-2002,0,0.0776828,"Missing"
D19-1228,P17-2056,0,0.0865296,"explicit incorporation of verb lexicons and associated logical inference rules. MacCartney and Manning (2009); Angeli and Manning (2014); and others incorporated knowledge of verb signatures within a natural logic framework (MacCartney, 2009; S´anchez Valencia, 1991) in order to perform natural language inference. Richardson and Kuhn (2012) incorporated signatures into a semantic parsing system. Several recent models of event factuality similarly make use of veridicality lexicons as input to larger machine-learned systems for event factuality (Saur´ı and Pustejovsky, 2012; Lotan et al., 2013; Stanovsky et al., 2017; Rudinger et al., 2018). Cases et al. (2019) used nested veridicality inferences as a test case for a meta-learning model, again assuming verb signatures as “meta information” known a priori. Pragmatics (Speaker Meaning). Geis and Zwicky (1971) observed that implicative verbs often give rise to “invited inferences”, beyond what is explainable by the lexical semantic type of the verb. For example, on hearing “He did not refuse to speak”, one naturally concludes that “He spoke” unless additional qualifications are made (e.g. “...he just didn’t have anything to say”). de Marneffe et al. (2012) e"
D19-1228,W19-0410,0,0.0574315,"Missing"
D19-1228,D18-1501,0,0.0356347,"Missing"
D19-1228,N18-1101,0,0.377203,"ition that models which consistently mirror human inferences about veridicality in context can be said to understand veridicality in general. We acknowledge that the question of what is the “right” approach to NLI has existed since the original definition of the recognizing textual entailment (RTE) task (Dagan et al., 2006) and remains open. However, there has been a de facto endorsement of the speakermeaning definition, evidenced by the widespread adoption of NLI datasets which favor informal, “natural” inferences over prescriptivist annotation guidelines (Manning, 2006; Bowman et al., 2015; Williams et al., 2018). (Note, recently, there have been explicit endorsements as well; see Westera 2231 Factive [+/+] He realized that he had to leave this house. He did not realize that he had to leave this house. → → He had to leave this house. He had to leave this house. Implic. [+/−] At that moment, I happened to look up. At that moment, I did not happen to look up. → →¬ At that moment, I looked up. At that moment, I looked up. Implic. [−/◦] He refused to do the same. He did not refuse to do the same. →¬ 6→ He did the same. He did the same. NA [◦/◦] Many felt that its inclusion was a mistake. Many did not feel"
D19-1228,W05-1206,0,0.197666,"nd Related Work There is significant work, both in linguistics and NLP, on veridicality and closely-related topics (factuality, entailment, etc). We view past work on veridicality within NLP as largely divisible into two groups, which align with two differing perspectives on the role of the NLI task: the sentencemeaning perspective and the speaker-meaning perspective. Briefly, the sentence meaning approach to NLI takes the position that NLP systems should strive to model the aspects of a sentence’s semantics which are closely derivable from the lexicon and which hold independently of context (Zaenen et al., 2005). In contrast, the speaker meaning approach to NLI takes the position that NLP systems should prioritize representation of the goaldirected meaning of a sentence within the context in which it was generated (Manning, 2006). Work on veridicality which aligns with the sentencemeaning perspective tends to focus on characterizing verbs according to their lexical semantic classes (or “signatures”), while work which aligns with the speaker-meaning approach focuses on representing “world knowledge” and evaluating inferences in naturalistic contexts. Lexical Semantics (Sentence Meaning). Most prior wo"
N15-1023,I13-1010,0,0.0410107,"on et al., 2005; Feng et al., 2013; Ji and Lin, 2009; Resnik, 1995) have become key in overcoming issues with lexical sparseness and in providing practical semantic information for natural language processing systems (Miller et al., 2004; Rutherford and Xue, 2014; Velikovich et al., 2010; Dodge et al., 2012). Most work on stylistic variation, however, has focused on larger units of text (Louis and Nenkova, 2013; Danescu-Niculescu-Mizil et al., 2012; Greene and Resnik, 2009; Xu et al., 2012) and studies of style at the lexical level have been scant. The few recent efforts (Brooke et al., 2010; Brooke and Hirst, 2013b; Formal/Casual jesus/my gosh 18 years/eighteen respiratory/breathing yes/yeah decade/ten years 1970s/the seventies foremost/first of all megan/you there somewhere/some place this film/that movie full/a whole bunch otherwise/another thing father/my dad recreation/hobby Complex/Simple great/a lot cinema/a movie a large/a big music/the band much/many things exposure/the show relative/his family matters/the things april/apr journal/diary the world/everybody burial/funeral rail/the train physicians/a doctor Table 1: Paraphrases with large style differences. Our method learns these distinctions au"
N15-1023,N13-1078,0,0.0418022,"on et al., 2005; Feng et al., 2013; Ji and Lin, 2009; Resnik, 1995) have become key in overcoming issues with lexical sparseness and in providing practical semantic information for natural language processing systems (Miller et al., 2004; Rutherford and Xue, 2014; Velikovich et al., 2010; Dodge et al., 2012). Most work on stylistic variation, however, has focused on larger units of text (Louis and Nenkova, 2013; Danescu-Niculescu-Mizil et al., 2012; Greene and Resnik, 2009; Xu et al., 2012) and studies of style at the lexical level have been scant. The few recent efforts (Brooke et al., 2010; Brooke and Hirst, 2013b; Formal/Casual jesus/my gosh 18 years/eighteen respiratory/breathing yes/yeah decade/ten years 1970s/the seventies foremost/first of all megan/you there somewhere/some place this film/that movie full/a whole bunch otherwise/another thing father/my dad recreation/hobby Complex/Simple great/a lot cinema/a movie a large/a big music/the band much/many things exposure/the show relative/his family matters/the things april/apr journal/diary the world/everybody burial/funeral rail/the train physicians/a doctor Table 1: Paraphrases with large style differences. Our method learns these distinctions au"
N15-1023,C10-2011,0,0.200595,"wn et al., 1992; Wilson et al., 2005; Feng et al., 2013; Ji and Lin, 2009; Resnik, 1995) have become key in overcoming issues with lexical sparseness and in providing practical semantic information for natural language processing systems (Miller et al., 2004; Rutherford and Xue, 2014; Velikovich et al., 2010; Dodge et al., 2012). Most work on stylistic variation, however, has focused on larger units of text (Louis and Nenkova, 2013; Danescu-Niculescu-Mizil et al., 2012; Greene and Resnik, 2009; Xu et al., 2012) and studies of style at the lexical level have been scant. The few recent efforts (Brooke et al., 2010; Brooke and Hirst, 2013b; Formal/Casual jesus/my gosh 18 years/eighteen respiratory/breathing yes/yeah decade/ten years 1970s/the seventies foremost/first of all megan/you there somewhere/some place this film/that movie full/a whole bunch otherwise/another thing father/my dad recreation/hobby Complex/Simple great/a lot cinema/a movie a large/a big music/the band much/many things exposure/the show relative/his family matters/the things april/apr journal/diary the world/everybody burial/funeral rail/the train physicians/a doctor Table 1: Paraphrases with large style differences. Our method lear"
N15-1023,J92-4003,0,0.0950185,"ns to genre analysis and paraphrasing. 1 Introduction True language understanding requires comprehending not just what is said, but how it is said, yet only recently have computational approaches been applied to the subtleties of tone and style. As the expectations on language technologies grow to include tailored search, context-aware inference, and analysis of author belief, an understanding of style becomes crucial. Lexical features have proven indispensable for the good performance of most applications dealing with language. Particularly, more generalized characterizations of the lexicon (Brown et al., 1992; Wilson et al., 2005; Feng et al., 2013; Ji and Lin, 2009; Resnik, 1995) have become key in overcoming issues with lexical sparseness and in providing practical semantic information for natural language processing systems (Miller et al., 2004; Rutherford and Xue, 2014; Velikovich et al., 2010; Dodge et al., 2012). Most work on stylistic variation, however, has focused on larger units of text (Louis and Nenkova, 2013; Danescu-Niculescu-Mizil et al., 2012; Greene and Resnik, 2009; Xu et al., 2012) and studies of style at the lexical level have been scant. The few recent efforts (Brooke et al.,"
N15-1023,N04-1025,0,0.0231148,"Missing"
N15-1023,P11-2117,0,0.0203954,"way consistent with human judgements. 2 Method We focus on two style dimensions: formality and complexity. We define formal language as the way one talks to a superior, whereas casual language is used with friends. We define simple language to be that used to talk to children or non-native English speakers, whereas more complex language is used by academics or domain experts. We use the Europarl corpus of parliamentary proceedings as an example of formal text and the Switchboard corpus of informal telephone conversations as casual text. We use articles from Wikipedia and simplified Wikipedia (Coster and Kauchak, 2011) as examples of complex and simple language respectively. For each style dimension, we subsample sentences from the larger corpus so that the two ends of the spectrum are roughly balanced. We end up with roughly 300K sentences each for formal/casual text and about 500K sentences each for simple/complex text.1 Given examples of language at each end of a style dimension, we score a phrase by the log ratio of the probability of observing the word in the reference corpus (REF) to observing it in the combined corpora (ALL). For formality the reference corpus is Europarl and the combined data is Eur"
N15-1023,P12-1094,0,0.0134308,"have proven indispensable for the good performance of most applications dealing with language. Particularly, more generalized characterizations of the lexicon (Brown et al., 1992; Wilson et al., 2005; Feng et al., 2013; Ji and Lin, 2009; Resnik, 1995) have become key in overcoming issues with lexical sparseness and in providing practical semantic information for natural language processing systems (Miller et al., 2004; Rutherford and Xue, 2014; Velikovich et al., 2010; Dodge et al., 2012). Most work on stylistic variation, however, has focused on larger units of text (Louis and Nenkova, 2013; Danescu-Niculescu-Mizil et al., 2012; Greene and Resnik, 2009; Xu et al., 2012) and studies of style at the lexical level have been scant. The few recent efforts (Brooke et al., 2010; Brooke and Hirst, 2013b; Formal/Casual jesus/my gosh 18 years/eighteen respiratory/breathing yes/yeah decade/ten years 1970s/the seventies foremost/first of all megan/you there somewhere/some place this film/that movie full/a whole bunch otherwise/another thing father/my dad recreation/hobby Complex/Simple great/a lot cinema/a movie a large/a big music/the band much/many things exposure/the show relative/his family matters/the things april/apr jour"
N15-1023,N12-1094,0,0.00641177,"d search, context-aware inference, and analysis of author belief, an understanding of style becomes crucial. Lexical features have proven indispensable for the good performance of most applications dealing with language. Particularly, more generalized characterizations of the lexicon (Brown et al., 1992; Wilson et al., 2005; Feng et al., 2013; Ji and Lin, 2009; Resnik, 1995) have become key in overcoming issues with lexical sparseness and in providing practical semantic information for natural language processing systems (Miller et al., 2004; Rutherford and Xue, 2014; Velikovich et al., 2010; Dodge et al., 2012). Most work on stylistic variation, however, has focused on larger units of text (Louis and Nenkova, 2013; Danescu-Niculescu-Mizil et al., 2012; Greene and Resnik, 2009; Xu et al., 2012) and studies of style at the lexical level have been scant. The few recent efforts (Brooke et al., 2010; Brooke and Hirst, 2013b; Formal/Casual jesus/my gosh 18 years/eighteen respiratory/breathing yes/yeah decade/ten years 1970s/the seventies foremost/first of all megan/you there somewhere/some place this film/that movie full/a whole bunch otherwise/another thing father/my dad recreation/hobby Complex/Simple g"
N15-1023,P13-1174,0,0.0163253,"Introduction True language understanding requires comprehending not just what is said, but how it is said, yet only recently have computational approaches been applied to the subtleties of tone and style. As the expectations on language technologies grow to include tailored search, context-aware inference, and analysis of author belief, an understanding of style becomes crucial. Lexical features have proven indispensable for the good performance of most applications dealing with language. Particularly, more generalized characterizations of the lexicon (Brown et al., 1992; Wilson et al., 2005; Feng et al., 2013; Ji and Lin, 2009; Resnik, 1995) have become key in overcoming issues with lexical sparseness and in providing practical semantic information for natural language processing systems (Miller et al., 2004; Rutherford and Xue, 2014; Velikovich et al., 2010; Dodge et al., 2012). Most work on stylistic variation, however, has focused on larger units of text (Louis and Nenkova, 2013; Danescu-Niculescu-Mizil et al., 2012; Greene and Resnik, 2009; Xu et al., 2012) and studies of style at the lexical level have been scant. The few recent efforts (Brooke et al., 2010; Brooke and Hirst, 2013b; Formal/Ca"
N15-1023,N13-1092,0,0.118147,"Missing"
N15-1023,N09-1057,0,0.0136519,"performance of most applications dealing with language. Particularly, more generalized characterizations of the lexicon (Brown et al., 1992; Wilson et al., 2005; Feng et al., 2013; Ji and Lin, 2009; Resnik, 1995) have become key in overcoming issues with lexical sparseness and in providing practical semantic information for natural language processing systems (Miller et al., 2004; Rutherford and Xue, 2014; Velikovich et al., 2010; Dodge et al., 2012). Most work on stylistic variation, however, has focused on larger units of text (Louis and Nenkova, 2013; Danescu-Niculescu-Mizil et al., 2012; Greene and Resnik, 2009; Xu et al., 2012) and studies of style at the lexical level have been scant. The few recent efforts (Brooke et al., 2010; Brooke and Hirst, 2013b; Formal/Casual jesus/my gosh 18 years/eighteen respiratory/breathing yes/yeah decade/ten years 1970s/the seventies foremost/first of all megan/you there somewhere/some place this film/that movie full/a whole bunch otherwise/another thing father/my dad recreation/hobby Complex/Simple great/a lot cinema/a movie a large/a big music/the band much/many things exposure/the show relative/his family matters/the things april/apr journal/diary the world/every"
N15-1023,P10-2013,0,0.0119118,"largest difference in log-ratio style score. Random guessing achieves an accuracy of 0.5. 4.2 annotations, we see some sentences for which the judgement seems unanimous among annotators and some sentences for which there is very little consensus (Table 7). We discuss this variation further in Section 5. Genre characterization Now we explore if the dimensions we learned at the sub-sentential level can be used to capture stylistic variation at the sentence and genre level. Sentence-level human judgements We gather human ratings of formality and complexity for 900 sentences from the MASC corpus (Ide et al., 2010): 20 sentences from each of 18 genres.2 Recently data from this corpus has been used to study genre difference in terms of pronoun, named entity, punctuation and part of speech usage (Passonneau et al., 2014). We use the data to test a specific hypothesis that automatically induced scores for lexical style are predictive of perceptions of sentence- and genre-level style. We average 7 independent human scores to get sentence-level style scores. To get genre-level style scores, we use the the average of the 20 sentencelevel scores for the sentences belonging to that genre. In human perception, t"
N15-1023,Y09-1024,0,0.0297932,"anguage understanding requires comprehending not just what is said, but how it is said, yet only recently have computational approaches been applied to the subtleties of tone and style. As the expectations on language technologies grow to include tailored search, context-aware inference, and analysis of author belief, an understanding of style becomes crucial. Lexical features have proven indispensable for the good performance of most applications dealing with language. Particularly, more generalized characterizations of the lexicon (Brown et al., 1992; Wilson et al., 2005; Feng et al., 2013; Ji and Lin, 2009; Resnik, 1995) have become key in overcoming issues with lexical sparseness and in providing practical semantic information for natural language processing systems (Miller et al., 2004; Rutherford and Xue, 2014; Velikovich et al., 2010; Dodge et al., 2012). Most work on stylistic variation, however, has focused on larger units of text (Louis and Nenkova, 2013; Danescu-Niculescu-Mizil et al., 2012; Greene and Resnik, 2009; Xu et al., 2012) and studies of style at the lexical level have been scant. The few recent efforts (Brooke et al., 2010; Brooke and Hirst, 2013b; Formal/Casual jesus/my gosh"
N15-1023,Q13-1028,1,0.817237,"rucial. Lexical features have proven indispensable for the good performance of most applications dealing with language. Particularly, more generalized characterizations of the lexicon (Brown et al., 1992; Wilson et al., 2005; Feng et al., 2013; Ji and Lin, 2009; Resnik, 1995) have become key in overcoming issues with lexical sparseness and in providing practical semantic information for natural language processing systems (Miller et al., 2004; Rutherford and Xue, 2014; Velikovich et al., 2010; Dodge et al., 2012). Most work on stylistic variation, however, has focused on larger units of text (Louis and Nenkova, 2013; Danescu-Niculescu-Mizil et al., 2012; Greene and Resnik, 2009; Xu et al., 2012) and studies of style at the lexical level have been scant. The few recent efforts (Brooke et al., 2010; Brooke and Hirst, 2013b; Formal/Casual jesus/my gosh 18 years/eighteen respiratory/breathing yes/yeah decade/ten years 1970s/the seventies foremost/first of all megan/you there somewhere/some place this film/that movie full/a whole bunch otherwise/another thing father/my dad recreation/hobby Complex/Simple great/a lot cinema/a movie a large/a big music/the band much/many things exposure/the show relative/his fa"
N15-1023,N04-1043,0,0.00695759,"le. As the expectations on language technologies grow to include tailored search, context-aware inference, and analysis of author belief, an understanding of style becomes crucial. Lexical features have proven indispensable for the good performance of most applications dealing with language. Particularly, more generalized characterizations of the lexicon (Brown et al., 1992; Wilson et al., 2005; Feng et al., 2013; Ji and Lin, 2009; Resnik, 1995) have become key in overcoming issues with lexical sparseness and in providing practical semantic information for natural language processing systems (Miller et al., 2004; Rutherford and Xue, 2014; Velikovich et al., 2010; Dodge et al., 2012). Most work on stylistic variation, however, has focused on larger units of text (Louis and Nenkova, 2013; Danescu-Niculescu-Mizil et al., 2012; Greene and Resnik, 2009; Xu et al., 2012) and studies of style at the lexical level have been scant. The few recent efforts (Brooke et al., 2010; Brooke and Hirst, 2013b; Formal/Casual jesus/my gosh 18 years/eighteen respiratory/breathing yes/yeah decade/ten years 1970s/the seventies foremost/first of all megan/you there somewhere/some place this film/that movie full/a whole bunch"
N15-1023,C14-1054,0,0.0140907,"ces for which there is very little consensus (Table 7). We discuss this variation further in Section 5. Genre characterization Now we explore if the dimensions we learned at the sub-sentential level can be used to capture stylistic variation at the sentence and genre level. Sentence-level human judgements We gather human ratings of formality and complexity for 900 sentences from the MASC corpus (Ide et al., 2010): 20 sentences from each of 18 genres.2 Recently data from this corpus has been used to study genre difference in terms of pronoun, named entity, punctuation and part of speech usage (Passonneau et al., 2014). We use the data to test a specific hypothesis that automatically induced scores for lexical style are predictive of perceptions of sentence- and genre-level style. We average 7 independent human scores to get sentence-level style scores. To get genre-level style scores, we use the the average of the 20 sentencelevel scores for the sentences belonging to that genre. In human perception, the formality and complexity dimensions are highly correlated (Spearman ρ = 0.7). However, we see many interesting examples of sentences which break this trend (Table 6). Overall, inter-annotator correlations"
N15-1023,P13-1162,0,0.0210492,"ncounters words for which there exist a range of paraphrases. Her lexical choice in these cases signals the style independent of the topic. Table 8 shows how well our two scoring methods correlate with the human judgements of sentences’ styles. The “all words” method performs very well, correlating with humans nearly as well as humans correlate with each other. Interestingly, when using paraphrases only we maintain significant correlations. This ability to differentiate stylistic variation without relying on cues from topic words could be especially important for tasks such as bias detection (Recasens et al., 2013) and readability (Callan, 2004; Kanungo and Orr, 2009). Inter-anno. All words PP only Formality Sent. Genre 0.47 – 0.44 0.77 0.18 0.63 Complexity Sent. Genre 0.48 – 0.43 0.80 0.23 0.45 6 Table 8: Spearman ρ of automatic rankings with human rankings. Genres are the concatenation of sentences from that genre. In “all words,” a text’s score is the average log-ratio style score of its words. In “PP only,” a text’s score is the proportion of times a formal term was chosen when more casual paraphrases existed, effectively capturing style independent of topic. 5 tasks, capturing style information in"
N15-1023,E14-1068,0,0.0109244,"ns on language technologies grow to include tailored search, context-aware inference, and analysis of author belief, an understanding of style becomes crucial. Lexical features have proven indispensable for the good performance of most applications dealing with language. Particularly, more generalized characterizations of the lexicon (Brown et al., 1992; Wilson et al., 2005; Feng et al., 2013; Ji and Lin, 2009; Resnik, 1995) have become key in overcoming issues with lexical sparseness and in providing practical semantic information for natural language processing systems (Miller et al., 2004; Rutherford and Xue, 2014; Velikovich et al., 2010; Dodge et al., 2012). Most work on stylistic variation, however, has focused on larger units of text (Louis and Nenkova, 2013; Danescu-Niculescu-Mizil et al., 2012; Greene and Resnik, 2009; Xu et al., 2012) and studies of style at the lexical level have been scant. The few recent efforts (Brooke et al., 2010; Brooke and Hirst, 2013b; Formal/Casual jesus/my gosh 18 years/eighteen respiratory/breathing yes/yeah decade/ten years 1970s/the seventies foremost/first of all megan/you there somewhere/some place this film/that movie full/a whole bunch otherwise/another thing f"
N15-1023,N10-1119,0,0.0285426,"s grow to include tailored search, context-aware inference, and analysis of author belief, an understanding of style becomes crucial. Lexical features have proven indispensable for the good performance of most applications dealing with language. Particularly, more generalized characterizations of the lexicon (Brown et al., 1992; Wilson et al., 2005; Feng et al., 2013; Ji and Lin, 2009; Resnik, 1995) have become key in overcoming issues with lexical sparseness and in providing practical semantic information for natural language processing systems (Miller et al., 2004; Rutherford and Xue, 2014; Velikovich et al., 2010; Dodge et al., 2012). Most work on stylistic variation, however, has focused on larger units of text (Louis and Nenkova, 2013; Danescu-Niculescu-Mizil et al., 2012; Greene and Resnik, 2009; Xu et al., 2012) and studies of style at the lexical level have been scant. The few recent efforts (Brooke et al., 2010; Brooke and Hirst, 2013b; Formal/Casual jesus/my gosh 18 years/eighteen respiratory/breathing yes/yeah decade/ten years 1970s/the seventies foremost/first of all megan/you there somewhere/some place this film/that movie full/a whole bunch otherwise/another thing father/my dad recreation/h"
N15-1023,H05-1044,0,0.00270404,"and paraphrasing. 1 Introduction True language understanding requires comprehending not just what is said, but how it is said, yet only recently have computational approaches been applied to the subtleties of tone and style. As the expectations on language technologies grow to include tailored search, context-aware inference, and analysis of author belief, an understanding of style becomes crucial. Lexical features have proven indispensable for the good performance of most applications dealing with language. Particularly, more generalized characterizations of the lexicon (Brown et al., 1992; Wilson et al., 2005; Feng et al., 2013; Ji and Lin, 2009; Resnik, 1995) have become key in overcoming issues with lexical sparseness and in providing practical semantic information for natural language processing systems (Miller et al., 2004; Rutherford and Xue, 2014; Velikovich et al., 2010; Dodge et al., 2012). Most work on stylistic variation, however, has focused on larger units of text (Louis and Nenkova, 2013; Danescu-Niculescu-Mizil et al., 2012; Greene and Resnik, 2009; Xu et al., 2012) and studies of style at the lexical level have been scant. The few recent efforts (Brooke et al., 2010; Brooke and Hirs"
N15-1023,C12-1177,0,0.0250819,"ications dealing with language. Particularly, more generalized characterizations of the lexicon (Brown et al., 1992; Wilson et al., 2005; Feng et al., 2013; Ji and Lin, 2009; Resnik, 1995) have become key in overcoming issues with lexical sparseness and in providing practical semantic information for natural language processing systems (Miller et al., 2004; Rutherford and Xue, 2014; Velikovich et al., 2010; Dodge et al., 2012). Most work on stylistic variation, however, has focused on larger units of text (Louis and Nenkova, 2013; Danescu-Niculescu-Mizil et al., 2012; Greene and Resnik, 2009; Xu et al., 2012) and studies of style at the lexical level have been scant. The few recent efforts (Brooke et al., 2010; Brooke and Hirst, 2013b; Formal/Casual jesus/my gosh 18 years/eighteen respiratory/breathing yes/yeah decade/ten years 1970s/the seventies foremost/first of all megan/you there somewhere/some place this film/that movie full/a whole bunch otherwise/another thing father/my dad recreation/hobby Complex/Simple great/a lot cinema/a movie a large/a big music/the band much/many things exposure/the show relative/his family matters/the things april/apr journal/diary the world/everybody burial/funera"
N15-1023,J02-2001,0,\N,Missing
P14-1107,E09-1003,0,0.0183789,"T gets stuck in a severe bottleneck for many minority or ‘low resource’ languages with insufficient data. This drastically limits which languages SMT can be successfully applied to. Because of this, collecting parallel corpora for minor languages has become an interesting research challenge. There are various options for creating training data for new language pairs. Past approaches have examined harvesting translated documents from the web (Resnik and Smith, 2003; Uszkoreit et al., 2010; Smith et al., 2013), or discovering parallel fragments from comparable corpora (Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010). Until relatively recently, little consideration has been given to creating parallel data from scratch. This is because the cost of hiring professional translators is prohibitively high. For instance, Germann (2001) hoped to hire professional translators to create a modest sized 100,000 word Tamil-English parallel corpus, but were stymied by the costs and the difficulty of finding good translators for a short-term commitment. Recently, crowdsourcing has opened the possibility of translating large amounts of text at low cost using non-professional translators. Facebook loc"
P14-1107,W10-0710,0,0.0144795,"expert labelings for the same input are aggregated, through simple voting or through weighting votes based on how closely non-experts matched experts on a small amount of calibration data. MTurk has subsequently been widely adopted by the NLP community and used for an extensive range of speech and language applications (Callison-Burch and Dredze, 2010). Although hiring professional translators to create bilingual training data for machine translation systems has been deemed infeasible, Mechanical Turk has provided a low cost way of creating large volumes of translations (Callison-Burch, 2009; Ambati and Vogel, 2010). For instance, Zbib et al. (2012; Zbib et al. (2013) translated 1.5 million words of Levine Arabic and Egyptian Arabic, and showed that a statistical translation system trained on the dialect data outperformed a system trained on 100 times more MSA data. Post et al. (2012) used MTurk to create parallel corpora for six Indian languages for less than $0.01 per word. MTurk workers translated more than half a million words worth of Malayalam in less than a week. Several researchers have examined the use of active learning to further reduce the cost of translation (Ambati et al., 2010; Ambati, 201"
P14-1107,ambati-etal-2010-active,0,0.0223859,"h, 2009; Ambati and Vogel, 2010). For instance, Zbib et al. (2012; Zbib et al. (2013) translated 1.5 million words of Levine Arabic and Egyptian Arabic, and showed that a statistical translation system trained on the dialect data outperformed a system trained on 100 times more MSA data. Post et al. (2012) used MTurk to create parallel corpora for six Indian languages for less than $0.01 per word. MTurk workers translated more than half a million words worth of Malayalam in less than a week. Several researchers have examined the use of active learning to further reduce the cost of translation (Ambati et al., 2010; Ambati, 2012; Bloodgood and Callison-Burch, 2010). Crowdsourcing allowed real studies to be conducted whereas most past active learning were simulated. Pavlick et al. (2014) conducted a large-scale demographic study of the languages spoken by workers on MTurk by translating 10,000 words in each of 100 languages. Chen and Dolan (2012) examined the steps necessary to build a persistent multilingual workforce on MTurk. This paper is most closely related to previous work by Zaidan and Callison-Burch (2011), who showed that non-professional translators could approach the level of professional tra"
P14-1107,P10-1088,1,0.845564,"or instance, Zbib et al. (2012; Zbib et al. (2013) translated 1.5 million words of Levine Arabic and Egyptian Arabic, and showed that a statistical translation system trained on the dialect data outperformed a system trained on 100 times more MSA data. Post et al. (2012) used MTurk to create parallel corpora for six Indian languages for less than $0.01 per word. MTurk workers translated more than half a million words worth of Malayalam in less than a week. Several researchers have examined the use of active learning to further reduce the cost of translation (Ambati et al., 2010; Ambati, 2012; Bloodgood and Callison-Burch, 2010). Crowdsourcing allowed real studies to be conducted whereas most past active learning were simulated. Pavlick et al. (2014) conducted a large-scale demographic study of the languages spoken by workers on MTurk by translating 10,000 words in each of 100 languages. Chen and Dolan (2012) examined the steps necessary to build a persistent multilingual workforce on MTurk. This paper is most closely related to previous work by Zaidan and Callison-Burch (2011), who showed that non-professional translators could approach the level of professional translators. They solicited multiple redundant transla"
P14-1107,W10-0701,1,0.794839,"k, following pioneering work by Snow et al. (2008) who showed that the platform was a viable way of collecting data for a wide variety of NLP tasks at low cost and in large volumes. They further showed that non-expert annotations are similar to expert annotations when many non-expert labelings for the same input are aggregated, through simple voting or through weighting votes based on how closely non-experts matched experts on a small amount of calibration data. MTurk has subsequently been widely adopted by the NLP community and used for an extensive range of speech and language applications (Callison-Burch and Dredze, 2010). Although hiring professional translators to create bilingual training data for machine translation systems has been deemed infeasible, Mechanical Turk has provided a low cost way of creating large volumes of translations (Callison-Burch, 2009; Ambati and Vogel, 2010). For instance, Zbib et al. (2012; Zbib et al. (2013) translated 1.5 million words of Levine Arabic and Egyptian Arabic, and showed that a statistical translation system trained on the dialect data outperformed a system trained on 100 times more MSA data. Post et al. (2012) used MTurk to create parallel corpora for six Indian lan"
P14-1107,D09-1030,1,0.817253,"tations when many non-expert labelings for the same input are aggregated, through simple voting or through weighting votes based on how closely non-experts matched experts on a small amount of calibration data. MTurk has subsequently been widely adopted by the NLP community and used for an extensive range of speech and language applications (Callison-Burch and Dredze, 2010). Although hiring professional translators to create bilingual training data for machine translation systems has been deemed infeasible, Mechanical Turk has provided a low cost way of creating large volumes of translations (Callison-Burch, 2009; Ambati and Vogel, 2010). For instance, Zbib et al. (2012; Zbib et al. (2013) translated 1.5 million words of Levine Arabic and Egyptian Arabic, and showed that a statistical translation system trained on the dialect data outperformed a system trained on 100 times more MSA data. Post et al. (2012) used MTurk to create parallel corpora for six Indian languages for less than $0.01 per word. MTurk workers translated more than half a million words worth of Malayalam in less than a week. Several researchers have examined the use of active learning to further reduce the cost of translation (Ambati"
P14-1107,W01-1409,0,0.0602304,"become an interesting research challenge. There are various options for creating training data for new language pairs. Past approaches have examined harvesting translated documents from the web (Resnik and Smith, 2003; Uszkoreit et al., 2010; Smith et al., 2013), or discovering parallel fragments from comparable corpora (Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010). Until relatively recently, little consideration has been given to creating parallel data from scratch. This is because the cost of hiring professional translators is prohibitively high. For instance, Germann (2001) hoped to hire professional translators to create a modest sized 100,000 word Tamil-English parallel corpus, but were stymied by the costs and the difficulty of finding good translators for a short-term commitment. Recently, crowdsourcing has opened the possibility of translating large amounts of text at low cost using non-professional translators. Facebook localized its web site into different languages using volunteers (TechCrunch, 2008). DuoLingo turns translation into an educational game, and translates web content using its language learners (von Ahn, 2013). Rather than relying on volunte"
P14-1107,N10-1078,0,0.0308235,"lems with hiring editors via MTurk for a word processing application. Workers were either lazy (meaning they made only minimal edits) or overly zealous (meaning they made many unnecessary edits). Bernstein et al. (2010) addressed this problem with a three step find-fix-verify process. In the first step, workers click on one word or phrase that needed to be corrected. In the next step, a separate group of workers proposed correc1 A variety of HCI and NLP studies have confirmed the efficacy of monolingual or bilingual individuals post-editing of machine translation output (Callison-Burch, 2005; Koehn, 2010; Green et al., 2013). Past NLP work has also examined automatic post-editing(Knight and Chander, 1994). tions to problematic regions that had been identified by multiple workers in the first pass. In the final step, other workers would validate whether the proposed corrections were good. Most NLP research into crowdsourcing has focused on Mechanical Turk, following pioneering work by Snow et al. (2008) who showed that the platform was a viable way of collecting data for a wide variety of NLP tasks at low cost and in large volumes. They further showed that non-expert annotations are similar to"
P14-1107,W10-1718,1,0.892503,"Missing"
P14-1107,Q13-1028,0,0.0230426,"Missing"
P14-1107,J05-4003,0,0.051185,"e, French-English, etc. SMT gets stuck in a severe bottleneck for many minority or ‘low resource’ languages with insufficient data. This drastically limits which languages SMT can be successfully applied to. Because of this, collecting parallel corpora for minor languages has become an interesting research challenge. There are various options for creating training data for new language pairs. Past approaches have examined harvesting translated documents from the web (Resnik and Smith, 2003; Uszkoreit et al., 2010; Smith et al., 2013), or discovering parallel fragments from comparable corpora (Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010). Until relatively recently, little consideration has been given to creating parallel data from scratch. This is because the cost of hiring professional translators is prohibitively high. For instance, Germann (2001) hoped to hire professional translators to create a modest sized 100,000 word Tamil-English parallel corpus, but were stymied by the costs and the difficulty of finding good translators for a short-term commitment. Recently, crowdsourcing has opened the possibility of translating large amounts of text at low cost using non-professi"
P14-1107,P02-1040,0,0.100739,"n ti and tj . ( #col (eij ∈ ET ) I(ti , tj ) = , (9) 0 otherwise Then the adjacency matrix N is then defined as (10) 5 (12) Evaluation We are interested in testing our random walk method, which incorporates information from both the candidate translations and from the Turkers. We want to test two versions of our proposed collaborative co-ranking method: 1) based on the unedited translations only and 2) based on the edited sentences after translator/editor collaborations. Metric Since we have four professional translation sets, we can calculate the Bilingual Evaluation Understudy (BLEU) score (Papineni et al., 2002) for one professional translator (P1) using the other three (P2,3,4) as a reference set. We repeat the process four times, scoring each professional translator against the others, to calculate the expected range of professional quality translation. In the following sections, we evaluate each of our methods by calculating BLEU scores against the same four sets of three reference translations. Therefore, each number reported in our experimental results is an average of four numbers, corresponding to the four possible ways of choosing 3 of the 4 reference sets. This allows us to compare the BLEU"
P14-1107,W12-3152,1,0.885996,"range of speech and language applications (Callison-Burch and Dredze, 2010). Although hiring professional translators to create bilingual training data for machine translation systems has been deemed infeasible, Mechanical Turk has provided a low cost way of creating large volumes of translations (Callison-Burch, 2009; Ambati and Vogel, 2010). For instance, Zbib et al. (2012; Zbib et al. (2013) translated 1.5 million words of Levine Arabic and Egyptian Arabic, and showed that a statistical translation system trained on the dialect data outperformed a system trained on 100 times more MSA data. Post et al. (2012) used MTurk to create parallel corpora for six Indian languages for less than $0.01 per word. MTurk workers translated more than half a million words worth of Malayalam in less than a week. Several researchers have examined the use of active learning to further reduce the cost of translation (Ambati et al., 2010; Ambati, 2012; Bloodgood and Callison-Burch, 2010). Crowdsourcing allowed real studies to be conducted whereas most past active learning were simulated. Pavlick et al. (2014) conducted a large-scale demographic study of the languages spoken by workers on MTurk by translating 10,000 wor"
P14-1107,J03-3002,0,0.0131709,"actice it produces the state-of-art results only for language pairs with ample training data, like English-Arabic, EnglishChinese, French-English, etc. SMT gets stuck in a severe bottleneck for many minority or ‘low resource’ languages with insufficient data. This drastically limits which languages SMT can be successfully applied to. Because of this, collecting parallel corpora for minor languages has become an interesting research challenge. There are various options for creating training data for new language pairs. Past approaches have examined harvesting translated documents from the web (Resnik and Smith, 2003; Uszkoreit et al., 2010; Smith et al., 2013), or discovering parallel fragments from comparable corpora (Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010). Until relatively recently, little consideration has been given to creating parallel data from scratch. This is because the cost of hiring professional translators is prohibitively high. For instance, Germann (2001) hoped to hire professional translators to create a modest sized 100,000 word Tamil-English parallel corpus, but were stymied by the costs and the difficulty of finding good translators for a short-term"
P14-1107,N10-1063,0,0.0140024,"eneck for many minority or ‘low resource’ languages with insufficient data. This drastically limits which languages SMT can be successfully applied to. Because of this, collecting parallel corpora for minor languages has become an interesting research challenge. There are various options for creating training data for new language pairs. Past approaches have examined harvesting translated documents from the web (Resnik and Smith, 2003; Uszkoreit et al., 2010; Smith et al., 2013), or discovering parallel fragments from comparable corpora (Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010). Until relatively recently, little consideration has been given to creating parallel data from scratch. This is because the cost of hiring professional translators is prohibitively high. For instance, Germann (2001) hoped to hire professional translators to create a modest sized 100,000 word Tamil-English parallel corpus, but were stymied by the costs and the difficulty of finding good translators for a short-term commitment. Recently, crowdsourcing has opened the possibility of translating large amounts of text at low cost using non-professional translators. Facebook localized its web site i"
P14-1107,P13-1135,1,0.835643,"for language pairs with ample training data, like English-Arabic, EnglishChinese, French-English, etc. SMT gets stuck in a severe bottleneck for many minority or ‘low resource’ languages with insufficient data. This drastically limits which languages SMT can be successfully applied to. Because of this, collecting parallel corpora for minor languages has become an interesting research challenge. There are various options for creating training data for new language pairs. Past approaches have examined harvesting translated documents from the web (Resnik and Smith, 2003; Uszkoreit et al., 2010; Smith et al., 2013), or discovering parallel fragments from comparable corpora (Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010). Until relatively recently, little consideration has been given to creating parallel data from scratch. This is because the cost of hiring professional translators is prohibitively high. For instance, Germann (2001) hoped to hire professional translators to create a modest sized 100,000 word Tamil-English parallel corpus, but were stymied by the costs and the difficulty of finding good translators for a short-term commitment. Recently, crowdsourcing has opene"
P14-1107,2006.amta-papers.25,0,0.0296192,"om the candidates produced by the collaboration of translator/post-editor pairs. The third oracle operates at the worker level: for each source segment, we choose from the translations the one provided by the worker whose translations (over all sentences) score the highest on average. The fourth oracle also operates at the worker level, but selects from sentences produced by translator/post-editor collaborations. These oracle methods represent ideal solutions under our scenario. We also examine two voting-inspired methods. The first method selects the translation with the minimum average TER (Snover et al., 2006) against the other translations; intuitively, this would represent the “consensus” translation. The second method selects the translation generated by the Turker who, on average, provides translations with the minimum average TER. Results A summary of our results in given in Table 2. As expected, random selection yields bad performance, with a BLEU score of 30.52. The oracles indicate that there is usually an acceptable translation from the Turkers for any given sentence. Since the oracles select from a small group of only 4 translations per source segment, they are not overly optimistic, and"
P14-1107,D08-1027,0,0.0720958,"Missing"
P14-1107,C10-1124,0,0.0139175,"tate-of-art results only for language pairs with ample training data, like English-Arabic, EnglishChinese, French-English, etc. SMT gets stuck in a severe bottleneck for many minority or ‘low resource’ languages with insufficient data. This drastically limits which languages SMT can be successfully applied to. Because of this, collecting parallel corpora for minor languages has become an interesting research challenge. There are various options for creating training data for new language pairs. Past approaches have examined harvesting translated documents from the web (Resnik and Smith, 2003; Uszkoreit et al., 2010; Smith et al., 2013), or discovering parallel fragments from comparable corpora (Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010). Until relatively recently, little consideration has been given to creating parallel data from scratch. This is because the cost of hiring professional translators is prohibitively high. For instance, Germann (2001) hoped to hire professional translators to create a modest sized 100,000 word Tamil-English parallel corpus, but were stymied by the costs and the difficulty of finding good translators for a short-term commitment. Recently, cr"
P14-1107,D11-1040,1,0.792595,"n, with the lowest TERgold on the top. Some translators receive low TERgold scores due to superficial errors, which can be easily improved through editing. In the above example, the middle-ranked translation (green) becomes the best translation after being revised by a good editor. post-edited sentences (henceforth “candidates”) as nodes. These two graphs, GT and GC are combined as subgraphs of a third graph (GT C ). Edges in GT C connect author pairs (nodes in GT ) to the candidate that they produced (nodes in GC ). Together, GT , GC , and GT C define a co-ranking problem (Yan et al., 2012a; Yan et al., 2011b; Yan et al., 2012b) with linkage establishment (Yan et al., 2011a; Yan et al., 2012c), which we define formally as follows. Let G denote the heterogeneous graph with nodes V and edges E. Let G = (V ,E) = (VT , VC , ET , EC , ET C ). G is divided into three subgraphs, GT , GC , and GT C . GC = (VC , EC ) is a weighted undirected graph representing the candidates and their lexical relationships to one another. Let VC denote a collection of translated and edited candidates, and EC the lexical similarity between the candidates (see Section 4.3 for details). GT = (VT , ET ) is a weighted undirect"
P14-1107,P12-1054,1,0.828,"of each translation, with the lowest TERgold on the top. Some translators receive low TERgold scores due to superficial errors, which can be easily improved through editing. In the above example, the middle-ranked translation (green) becomes the best translation after being revised by a good editor. post-edited sentences (henceforth “candidates”) as nodes. These two graphs, GT and GC are combined as subgraphs of a third graph (GT C ). Edges in GT C connect author pairs (nodes in GT ) to the candidate that they produced (nodes in GC ). Together, GT , GC , and GT C define a co-ranking problem (Yan et al., 2012a; Yan et al., 2011b; Yan et al., 2012b) with linkage establishment (Yan et al., 2011a; Yan et al., 2012c), which we define formally as follows. Let G denote the heterogeneous graph with nodes V and edges E. Let G = (V ,E) = (VT , VC , ET , EC , ET C ). G is divided into three subgraphs, GT , GC , and GT C . GC = (VC , EC ) is a weighted undirected graph representing the candidates and their lexical relationships to one another. Let VC denote a collection of translated and edited candidates, and EC the lexical similarity between the candidates (see Section 4.3 for details). GT = (VT , ET ) is"
P14-1107,P11-1122,1,0.917573,"Missing"
P14-1107,N12-1006,1,0.850492,"aggregated, through simple voting or through weighting votes based on how closely non-experts matched experts on a small amount of calibration data. MTurk has subsequently been widely adopted by the NLP community and used for an extensive range of speech and language applications (Callison-Burch and Dredze, 2010). Although hiring professional translators to create bilingual training data for machine translation systems has been deemed infeasible, Mechanical Turk has provided a low cost way of creating large volumes of translations (Callison-Burch, 2009; Ambati and Vogel, 2010). For instance, Zbib et al. (2012; Zbib et al. (2013) translated 1.5 million words of Levine Arabic and Egyptian Arabic, and showed that a statistical translation system trained on the dialect data outperformed a system trained on 100 times more MSA data. Post et al. (2012) used MTurk to create parallel corpora for six Indian languages for less than $0.01 per word. MTurk workers translated more than half a million words worth of Malayalam in less than a week. Several researchers have examined the use of active learning to further reduce the cost of translation (Ambati et al., 2010; Ambati, 2012; Bloodgood and Callison-Burch,"
P14-1107,N13-1069,0,0.0366214,"Missing"
P14-1107,Q14-1007,1,\N,Missing
P15-1146,P05-1074,1,0.5637,"DB, annotated on MTurk as described in Section 5 1514 Lexical Distributional Paraphrase Translation Path WordNet We use the lemmas, POS tags, and phrase lengths of p1 and p2 , the substrings shared by p1 and p2 , and the Levenstein, Jaccard, and Hamming distances between p1 and p2 . Given a dependency context vectors for p1 and p2 , we compute the number of shared contexts, and the Jaccard, Cosine, Lin1998, Weeds2004, Clarke2009, and Szpektor2008 similarities between the vectors. We include 33 paraphrase features distributed with PPDB, which include the paraphrase probabilities as computed in Bannard and Callison-Burch (2005). We refer the reader to Ganitkevitch and CallisonBurch (2014) for a complete description of all of the features included with PPDB. We include the number of foreign language “pivots” (translations) shared by p1 and p2 for each of 24 languages used in the construction of PPDB, as a fraction of the total number of translations observed for each of p1 and p2 . We include a sparse vector of all lexico-syntactic patterns (paths through a dependency parse) which are observed between p1 and p2 in the Annotated Gigaword corpus (Napoles et al., 2012). We include binary features indicating whether Word"
P15-1146,D07-1017,0,0.0891599,"country/patriotic drive/vehicle family/home basketball/court playing/toy islamic/jihad delay/time Unrelated girl/play found/party profit/year man/talk car/family holiday/series green/tennis sunday/tour city/south back/view Table 1: Examples of different types of entailment relations appearing in PPDB. Burch, 2007), and unrelated pairs, e.g. due to misalignments or polysemy in the foreign language. The unclear semantics severely limits the applicability of paraphrase resources to natural language understanding (NLU) tasks. Some efforts have been made to identify directionality of paraphrases (Bhagat et al., 2007; Kotlerman et al., 2010), but tasks like RTE require even richer semantic information. For example, in the T/H pair shown in Figure 1, a system needs information not only about equivalent words (12/twelve) and asymmetric entailments (riots/unrest), but also semantic exclusion (Denmark/Jordan). Such lexical entailment relations are captured by natural logic, a formalism which views natural language itself as a meaning representation, eschewing external representations such as First Order Logic (FOL). This is a great fit for automatically extracted paraphrases, since the phrase pairs themselves"
P15-1146,S14-2114,1,0.90105,"Missing"
P15-1146,W08-2222,1,0.588578,"Missing"
P15-1146,S14-2141,0,0.0747714,"dependent), compared to only 700K in PPDB-S (where the majority type is equivalent). using data motivated by the applications to information retrieval, information extraction, summarization, machine translation evaluation, and more recently, question answering (Giampiccolo et al., 2007) and essay grading (Clark et al., 2013). RTE systems vary considerably in their choice of representation and inference procedure. In the most recent shared task on RTE, some systems used deep logical representations of text, allowing them to invoke theorem provers (Bjerva et al., 2014) or Markov Logic Networks (Beltagy et al., 2014) to perform the inference, while others used shallower representations, relying on machine learning to perform inference (Lai and Hockenmaier, 2014; Zhao et al., 2014). Systems based on natural logic (MacCartney and Manning, 2007) use natural language as a representation, but still perform inference using a structured algebra rather than a statistical model. Regardless of the inference procedure, improvements to external lexical resources can improve RTE systems across the board (Clark et al., 2007). 3 The Paraphrase Database (PPDB) PPDB is currently the largest available collection of paraphr"
P15-1146,W04-3205,0,0.0861134,"tempted to build or augment lexical ontologies automatically, to discover other types of lexical relations like hypernyms. Most of these approaches rely on lexico-syntactic patterns. Hearst (1992) searched for hand-written patterns (e.g. “an X is a Y”) in a large corpus in order to learn taxonomic relations between nouns. Snow et al. (2006) used dependency parses to automatically learn such patterns, which they used to augment WordNet with new hypernym relations. Similar monolingual signals have been used to learn fine-grained relationships between verbs, such as enablement and happensbefore (Chklovski and Pantel, 2004; Hashimoto et al., 2009). Recognizing Textual Entailment The shared RTE tasks (Dagan et al., 2006) have been a springboard for research in natural language inference, 1513 Figure 2: Distribution of entailment relations in different sizes of PPDB. Distributions are estimated from our manual annotations of randomly sampled pairs. PPDB-XXXL contains over 77MM paraphrase pairs (where the majority type is independent), compared to only 700K in PPDB-S (where the majority type is equivalent). using data motivated by the applications to information retrieval, information extraction, summarization, ma"
P15-1146,P11-1062,0,0.0709412,"Missing"
P15-1146,S13-2045,0,0.00626621,"ringboard for research in natural language inference, 1513 Figure 2: Distribution of entailment relations in different sizes of PPDB. Distributions are estimated from our manual annotations of randomly sampled pairs. PPDB-XXXL contains over 77MM paraphrase pairs (where the majority type is independent), compared to only 700K in PPDB-S (where the majority type is equivalent). using data motivated by the applications to information retrieval, information extraction, summarization, machine translation evaluation, and more recently, question answering (Giampiccolo et al., 2007) and essay grading (Clark et al., 2013). RTE systems vary considerably in their choice of representation and inference procedure. In the most recent shared task on RTE, some systems used deep logical representations of text, allowing them to invoke theorem provers (Bjerva et al., 2014) or Markov Logic Networks (Beltagy et al., 2014) to perform the inference, while others used shallower representations, relying on machine learning to perform inference (Lai and Hockenmaier, 2014; Zhao et al., 2014). Systems based on natural logic (MacCartney and Manning, 2007) use natural language as a representation, but still perform inference usin"
P15-1146,W09-0215,0,0.0129149,"ve of the ¬ relation. Distributional features Lin and Pantel (2001) attempted to mine inference rules from text by finding paths in a dependency tree which connect the same nouns. The intuition is that good paraphrases should tend to modify and be modified by the same words. Given context vectors, Lin and Pantel (2001) used a symmetric similarity metric (Lin, 1998) to find candidate paraphrases. We build dependency context vectors for each word in our data and compute both symmetric as well as more recently proposed asymmetric similarity measures (Weeds et al., 2004; Szpektor and Dagan, 2008; Clarke, 2009), which are potentially better suited for identifying A paraphrases. Table 3 gives a comparison of the pairs which are considered “most similar” according to several of these metrics. 6.2 Bilingual features We explore a variety of bilingual features, which we expect to provide complimentary signal to the monolingual features. Each pair in PPDB is associated with several paraphrase probabilities, which are based on the probabilities of aligning each word to the foreign “pivot” phrase (a foreign translation shared by the two phrases), computed as described in Bannard and Callison-Burch (2005). W"
P15-1146,C04-1051,0,0.466255,"sk of recognizing textual entailment (RTE). In RTE, a system is given two pieces of text, often called the text (T) and the hypothesis (H), and asked to determine whether T entails H, T contradicts H, or T and H are unrelatable (Figure 1). In contrast, data-driving paraphrasing typically sidesteps developing a clear definition of “meaning the same thing” and instead “assume[s] paraphrasing is a coherent notion and concentrate[s] on devices that can produce paraphrases” (Barzilay, 2003). Recent work on paraphrase extraction has resulted in enormous paraphrase collections (Lin and Pantel, 2001; Dolan et al., 2004; Ganitkevitch et al., 2013), but the usefulness of these collections is limited by the fast-and-loose treatment of the meaning of paraphrases. One concrete definition that is sometimes used for paraphrases requires that they be bidirectionally entailing (Androutsopoulos and Malakasiotis, 2010). That is, in terms of RTE, it is assumed that if P is a paraphrase of Q, then P entails Q and Q entails P. In reality, paraphrases are often more nuanced (Bhagat and Hovy, 2013), and the entries in most paraphrase resources certainly do not match this definition. For instance, Lin and Pantel (2001) extr"
P15-1146,ganitkevitch-callison-burch-2014-multilingual,1,0.922495,"Missing"
P15-1146,S14-2055,0,0.0262925,"trieval, information extraction, summarization, machine translation evaluation, and more recently, question answering (Giampiccolo et al., 2007) and essay grading (Clark et al., 2013). RTE systems vary considerably in their choice of representation and inference procedure. In the most recent shared task on RTE, some systems used deep logical representations of text, allowing them to invoke theorem provers (Bjerva et al., 2014) or Markov Logic Networks (Beltagy et al., 2014) to perform the inference, while others used shallower representations, relying on machine learning to perform inference (Lai and Hockenmaier, 2014; Zhao et al., 2014). Systems based on natural logic (MacCartney and Manning, 2007) use natural language as a representation, but still perform inference using a structured algebra rather than a statistical model. Regardless of the inference procedure, improvements to external lexical resources can improve RTE systems across the board (Clark et al., 2007). 3 The Paraphrase Database (PPDB) PPDB is currently the largest available collection of paraphrases. Compared to other paraphrase resources such as the DIRT database (12 million rules) (Lin and Pantel, 2001) and the MSR paraphrase phrase tabl"
P15-1146,P98-2127,0,0.382648,"⇠ Cosine Similarity shades/the shade yard/backyard each other/man picture/drawing practice/target Monolingual (symmetric) ¬ large/small ⌘ few/several ¬ different/same ¬ other/same ¬ put/take Monolingual (asymmetric) A boy/little boy A man/two men A child/three children ⌘ is playing/play A side/both sides ⌘ A ⌘ ⌘ ⌘ Bilingual dad/father some kid/child a lot of/many female/woman male/man Table 3: Top scoring pairs (x/y) according to various similarity measures, along with their manually classified entailment labels. Column 1 is cosine similarity based on dependency contexts. Column 2 is based on Lin (1998), column 3 on Weeds (2004), and column 4 is a novel feature. Precise definitions of each metric are given in the supplementary material. in X and in Y separate X from Y to X and/or to Y from X to Y more/less X than Y ate levels of agreement (Fleiss’s  = 0.56) (Landis and Koch, 1977). For a fuller discussion of the annotation, refer to the supplementary material. 6 Automatic Classification Table 4: Top paths associated with the ¬ class. We aim to build a classifier to automatically assign entailment types to entries in the PPDB, and to demonstrate that it performs well both intrinsically and e"
P15-1146,W07-1431,0,0.153459,"wing external representations such as First Order Logic (FOL). This is a great fit for automatically extracted paraphrases, since the phrase pairs themselves can be used as the semantic representation with minimal additional annotation. But as is, paraphrase resources lack such annotation. As a result, NLU systems rely on manually built resources like WordNet, which are limited in coverage and often lead to incorrect inferences (Kaplan and Schubert, 2001). In fact, in the most recent RTE challenge, over half of the submitted systems used WordNet (Pontiki et al., 2014). Even the NatLog system (MacCartney and Manning, 2007), which popularized natural logic for RTE, relied on WordNet and did not solve the problem of assigning natural logic relations at scale. The main contributions of this paper are: • We add a concrete, interpretable semantics to the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013), the largest paraphrase resource currently available. We give each entry in the database a label describing the entailment relationship between the phrases. • We develop a statistical model to predict these relations. The enormous size of PPDB– over 77 million phrase pairs!– makes it impossible to perform this t"
P15-1146,J10-3003,0,0.048805,"Missing"
P15-1146,N13-1092,1,0.191264,"Missing"
P15-1146,S14-2001,0,0.0556365,"Missing"
P15-1146,W07-1401,0,0.0157195,"RTE tasks (Dagan et al., 2006) have been a springboard for research in natural language inference, 1513 Figure 2: Distribution of entailment relations in different sizes of PPDB. Distributions are estimated from our manual annotations of randomly sampled pairs. PPDB-XXXL contains over 77MM paraphrase pairs (where the majority type is independent), compared to only 700K in PPDB-S (where the majority type is equivalent). using data motivated by the applications to information retrieval, information extraction, summarization, machine translation evaluation, and more recently, question answering (Giampiccolo et al., 2007) and essay grading (Clark et al., 2013). RTE systems vary considerably in their choice of representation and inference procedure. In the most recent shared task on RTE, some systems used deep logical representations of text, allowing them to invoke theorem provers (Bjerva et al., 2014) or Markov Logic Networks (Beltagy et al., 2014) to perform the inference, while others used shallower representations, relying on machine learning to perform inference (Lai and Hockenmaier, 2014; Zhao et al., 2014). Systems based on natural logic (MacCartney and Manning, 2007) use natural language as a represent"
P15-1146,W12-3018,1,0.892473,"Missing"
P15-1146,D09-1122,0,0.082205,"Missing"
P15-1146,C92-2082,0,0.478737,"ources Approaches to paraphrase identification have exploited signal from distributional contexts (Lin and Pantel, 2001; Szpektor et al., 2004), comparable corpora (Dolan et al., 2004; Xu et al., 2014), and graph structures (Berant et al., 2011; Brockett et al., 2013). These approaches are scalable, but they often assume that all relations are equivalence relations (Madnani and Dorr, 2010). Several efforts have attempted to build or augment lexical ontologies automatically, to discover other types of lexical relations like hypernyms. Most of these approaches rely on lexico-syntactic patterns. Hearst (1992) searched for hand-written patterns (e.g. “an X is a Y”) in a large corpus in order to learn taxonomic relations between nouns. Snow et al. (2006) used dependency parses to automatically learn such patterns, which they used to augment WordNet with new hypernym relations. Similar monolingual signals have been used to learn fine-grained relationships between verbs, such as enablement and happensbefore (Chklovski and Pantel, 2004; Hashimoto et al., 2009). Recognizing Textual Entailment The shared RTE tasks (Dagan et al., 2006) have been a springboard for research in natural language inference, 15"
P15-1146,P06-1101,0,0.0656433,"4), comparable corpora (Dolan et al., 2004; Xu et al., 2014), and graph structures (Berant et al., 2011; Brockett et al., 2013). These approaches are scalable, but they often assume that all relations are equivalence relations (Madnani and Dorr, 2010). Several efforts have attempted to build or augment lexical ontologies automatically, to discover other types of lexical relations like hypernyms. Most of these approaches rely on lexico-syntactic patterns. Hearst (1992) searched for hand-written patterns (e.g. “an X is a Y”) in a large corpus in order to learn taxonomic relations between nouns. Snow et al. (2006) used dependency parses to automatically learn such patterns, which they used to augment WordNet with new hypernym relations. Similar monolingual signals have been used to learn fine-grained relationships between verbs, such as enablement and happensbefore (Chklovski and Pantel, 2004; Hashimoto et al., 2009). Recognizing Textual Entailment The shared RTE tasks (Dagan et al., 2006) have been a springboard for research in natural language inference, 1513 Figure 2: Distribution of entailment relations in different sizes of PPDB. Distributions are estimated from our manual annotations of randomly"
P15-1146,C08-1107,0,0.0134612,"of the paths most indicative of the ¬ relation. Distributional features Lin and Pantel (2001) attempted to mine inference rules from text by finding paths in a dependency tree which connect the same nouns. The intuition is that good paraphrases should tend to modify and be modified by the same words. Given context vectors, Lin and Pantel (2001) used a symmetric similarity metric (Lin, 1998) to find candidate paraphrases. We build dependency context vectors for each word in our data and compute both symmetric as well as more recently proposed asymmetric similarity measures (Weeds et al., 2004; Szpektor and Dagan, 2008; Clarke, 2009), which are potentially better suited for identifying A paraphrases. Table 3 gives a comparison of the pairs which are considered “most similar” according to several of these metrics. 6.2 Bilingual features We explore a variety of bilingual features, which we expect to provide complimentary signal to the monolingual features. Each pair in PPDB is associated with several paraphrase probabilities, which are based on the probabilities of aligning each word to the foreign “pivot” phrase (a foreign translation shared by the two phrases), computed as described in Bannard and Callison-"
P15-1146,W04-3206,0,0.0162431,"Missing"
P15-1146,C04-1146,0,0.0100585,"es examples of some of the paths most indicative of the ¬ relation. Distributional features Lin and Pantel (2001) attempted to mine inference rules from text by finding paths in a dependency tree which connect the same nouns. The intuition is that good paraphrases should tend to modify and be modified by the same words. Given context vectors, Lin and Pantel (2001) used a symmetric similarity metric (Lin, 1998) to find candidate paraphrases. We build dependency context vectors for each word in our data and compute both symmetric as well as more recently proposed asymmetric similarity measures (Weeds et al., 2004; Szpektor and Dagan, 2008; Clarke, 2009), which are potentially better suited for identifying A paraphrases. Table 3 gives a comparison of the pairs which are considered “most similar” according to several of these metrics. 6.2 Bilingual features We explore a variety of bilingual features, which we expect to provide complimentary signal to the monolingual features. Each pair in PPDB is associated with several paraphrase probabilities, which are based on the probabilities of aligning each word to the foreign “pivot” phrase (a foreign translation shared by the two phrases), computed as describe"
P15-1146,Q14-1034,1,0.68632,"Missing"
P15-1146,S14-2044,0,0.0221701,"tion, summarization, machine translation evaluation, and more recently, question answering (Giampiccolo et al., 2007) and essay grading (Clark et al., 2013). RTE systems vary considerably in their choice of representation and inference procedure. In the most recent shared task on RTE, some systems used deep logical representations of text, allowing them to invoke theorem provers (Bjerva et al., 2014) or Markov Logic Networks (Beltagy et al., 2014) to perform the inference, while others used shallower representations, relying on machine learning to perform inference (Lai and Hockenmaier, 2014; Zhao et al., 2014). Systems based on natural logic (MacCartney and Manning, 2007) use natural language as a representation, but still perform inference using a structured algebra rather than a statistical model. Regardless of the inference procedure, improvements to external lexical resources can improve RTE systems across the board (Clark et al., 2007). 3 The Paraphrase Database (PPDB) PPDB is currently the largest available collection of paraphrases. Compared to other paraphrase resources such as the DIRT database (12 million rules) (Lin and Pantel, 2001) and the MSR paraphrase phrase table (13 million) (Dola"
P15-1146,W07-1409,0,\N,Missing
P15-1146,C98-2122,0,\N,Missing
P15-1146,S14-2004,0,\N,Missing
P15-1146,J13-3001,0,\N,Missing
P15-2010,D08-1021,1,0.774015,"phrases. We address the problem of customizing paraphrase models to specific target domains. We explore the following ideas: Introduction Many data-driven paraphrase extraction algorithms have been developed in recent years (Madnani and Dorr, 2010; Androutsopoulos and Malakasiotis, 2010). These algorithms attempt to learn paraphrase rules, where one phrase can be replaced with another phrase which has equivalent meaning in at least some context. Determining whether a paraphrase is appropriate for a specific context is a difficult problem (Bhagat and Hovy, 2013), encompassing issues of syntax (Callison-Burch, 2008), word sense (Apidianaki et al., 2014), and style (Xu et al., 2012; Pavlick and Nenkova, 2015). To date, the question of how domain effects paraphrase has been left unexplored. Although most paraphrase extraction algorithms attempt to estimate a confidence with which a paraphrase rule might apply, these scores are not differentiated by domain, and instead correspond to the general domain represented by the model’s training data. As illustrated by Table 1, paraphrases that are highly probable in the general domain (e.g. hot = sexy) can be extremely improbable in more specialized domains like bi"
P15-2010,P13-2121,0,0.0208555,"m the entire bitext. 4 Experimental Conditions Domain data We evaluate our domain-specific paraphrasing model in the target domain of biology. Our monolingual in-domain data is a combination of text from the GENIA database (Kim et al., 2003) and text from an introductory biology textbook. Our bilingual general-domain data is the 109 word parallel corpus (Callison-Burch et al., 58 2009), a collection of French-English parallel data covering a mix of genres from legal text (Steinberger et al., 2006) to movie subtitles (Tiedemann, 2012). We use 5-gram language models with Kneser-Ney discounting (Heafield et al., 2013). Evaluation We measure the precision and recall of paraphrase pairs produced by each of our models by collecting human judgments of what paraphrases are acceptable in sentences drawn from the target domain and in sentences drawn from the general domain. We sample 15K sentences from our biology data, and 10K general-domain sentences from Wikipedia. We select a phrase from each sentence, and show the list of candidate paraphrases1 to 5 human judges. Judges make a binary decision about whether each paraphrase is appropriate given the domain-specific context. We consider a paraphrase rule to be g"
P15-2010,J10-3003,0,0.0554926,"paraphrase extraction techniques learn paraphrases for a mix of senses that work well in general. But in specific domains, paraphrasing should be sensitive to specialized language use. domain: the verb treat is used in expressions like treat you to dinner in conversational domains versus treat an infection in biology. This domain shift changes the acceptability of its paraphrases. We address the problem of customizing paraphrase models to specific target domains. We explore the following ideas: Introduction Many data-driven paraphrase extraction algorithms have been developed in recent years (Madnani and Dorr, 2010; Androutsopoulos and Malakasiotis, 2010). These algorithms attempt to learn paraphrase rules, where one phrase can be replaced with another phrase which has equivalent meaning in at least some context. Determining whether a paraphrase is appropriate for a specific context is a difficult problem (Bhagat and Hovy, 2013), encompassing issues of syntax (Callison-Burch, 2008), word sense (Apidianaki et al., 2014), and style (Xu et al., 2012; Pavlick and Nenkova, 2015). To date, the question of how domain effects paraphrase has been left unexplored. Although most paraphrase extraction algorithms at"
P15-2010,W07-0716,0,0.0611515,"Missing"
P15-2010,D09-1074,0,0.0542275,"Missing"
P15-2010,apidianaki-etal-2014-semantic,0,0.075967,"customizing paraphrase models to specific target domains. We explore the following ideas: Introduction Many data-driven paraphrase extraction algorithms have been developed in recent years (Madnani and Dorr, 2010; Androutsopoulos and Malakasiotis, 2010). These algorithms attempt to learn paraphrase rules, where one phrase can be replaced with another phrase which has equivalent meaning in at least some context. Determining whether a paraphrase is appropriate for a specific context is a difficult problem (Bhagat and Hovy, 2013), encompassing issues of syntax (Callison-Burch, 2008), word sense (Apidianaki et al., 2014), and style (Xu et al., 2012; Pavlick and Nenkova, 2015). To date, the question of how domain effects paraphrase has been left unexplored. Although most paraphrase extraction algorithms attempt to estimate a confidence with which a paraphrase rule might apply, these scores are not differentiated by domain, and instead correspond to the general domain represented by the model’s training data. As illustrated by Table 1, paraphrases that are highly probable in the general domain (e.g. hot = sexy) can be extremely improbable in more specialized domains like biology. Dominant word senses change dep"
P15-2010,P10-2041,0,0.527199,"data. 2. We improve our domain-specific paraphrases by weighting each training example based on its domain score, instead of treating each example equally. 3. We dramatically improve recall while maintaining precision by combining the subsampled in-domain paraphrase scores with the general-domain paraphrase scores. 2 Background The paraphrase extraction algorithm that we customize is the bilingual pivoting method (Bannard and Callison-Burch, 2005) that was used to create PPDB, the paraphrase database (Ganitkevitch et al., 2013). To perform the subsampling, we adapt and improve the method that Moore and Lewis (2010) originally developed for domain-specific language models in machine translation. ∗ Incubated by the Allen Institute for Artificial Intelligence. 57 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 57–62, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2.1 according to each language model (LM). That is, for a sentence si , we compute Paraphrase extraction Paraphrases can be extracted via bilingual pivoting. Intuitively, if two"
P15-2010,D11-1033,0,0.0380065,"2 provides examples of paraphrases extracted using our domain-specific AUC 39.5 40.8 40.8 41.2 41.9 42.3 43.7 ∆absolute – +1.3 +1.3 +1.7 +2.4 +2.8 +4.2 ∆relative – +3.3 +3.3 +4.3 +6.1 +7.1 +10.6 Table 3: AUC (× 100) for each model in the biology domain from Figure 2(a). model for biology versus the baseline model. 6 Related Work Domain-specific paraphrasing has not received previous attention, but there is relevant prior work on domain-specific machine translation (MT). We build on the Moore-Lewis method, which has been used for language models (Moore and Lewis, 2010) and translation models (Axelrod et al., 2011). Similar methods use LM perplexity to rank sentences (Gao et al., 2002; Yasuda et al., 2008), rather than the difference in cross-entropy. Within MT, Foster and Kuhn (2007) used loglinear weightings of translation probabilities to combine models trained in different domains, as we do here. Relevant to our proposed method of 60 fractional counting, (Madnani et al., 2007) used introduced a count-centric approach to paraphrase probability estimation. Matsoukas et al. (2009) and Foster et al. (2010) explored weighted training sentences for MT, but set weights discriminatively based on sentence-le"
P15-2010,N15-1023,1,0.825856,"ns. We explore the following ideas: Introduction Many data-driven paraphrase extraction algorithms have been developed in recent years (Madnani and Dorr, 2010; Androutsopoulos and Malakasiotis, 2010). These algorithms attempt to learn paraphrase rules, where one phrase can be replaced with another phrase which has equivalent meaning in at least some context. Determining whether a paraphrase is appropriate for a specific context is a difficult problem (Bhagat and Hovy, 2013), encompassing issues of syntax (Callison-Burch, 2008), word sense (Apidianaki et al., 2014), and style (Xu et al., 2012; Pavlick and Nenkova, 2015). To date, the question of how domain effects paraphrase has been left unexplored. Although most paraphrase extraction algorithms attempt to estimate a confidence with which a paraphrase rule might apply, these scores are not differentiated by domain, and instead correspond to the general domain represented by the model’s training data. As illustrated by Table 1, paraphrases that are highly probable in the general domain (e.g. hot = sexy) can be extremely improbable in more specialized domains like biology. Dominant word senses change depending on 1. We sort sentences in the training corpus ba"
P15-2010,P05-1074,1,0.900281,"ge depending on 1. We sort sentences in the training corpus based on how well they represent the target domain, and then extract paraphrases from a subsample of the most domain-like data. 2. We improve our domain-specific paraphrases by weighting each training example based on its domain score, instead of treating each example equally. 3. We dramatically improve recall while maintaining precision by combining the subsampled in-domain paraphrase scores with the general-domain paraphrase scores. 2 Background The paraphrase extraction algorithm that we customize is the bilingual pivoting method (Bannard and Callison-Burch, 2005) that was used to create PPDB, the paraphrase database (Ganitkevitch et al., 2013). To perform the subsampling, we adapt and improve the method that Moore and Lewis (2010) originally developed for domain-specific language models in machine translation. ∗ Incubated by the Allen Institute for Artificial Intelligence. 57 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 57–62, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2.1 ac"
P15-2010,tiedemann-2012-parallel,0,0.0169721,"advantage of producing the full set of paraphrases that can be extracted from the entire bitext. 4 Experimental Conditions Domain data We evaluate our domain-specific paraphrasing model in the target domain of biology. Our monolingual in-domain data is a combination of text from the GENIA database (Kim et al., 2003) and text from an introductory biology textbook. Our bilingual general-domain data is the 109 word parallel corpus (Callison-Burch et al., 58 2009), a collection of French-English parallel data covering a mix of genres from legal text (Steinberger et al., 2006) to movie subtitles (Tiedemann, 2012). We use 5-gram language models with Kneser-Ney discounting (Heafield et al., 2013). Evaluation We measure the precision and recall of paraphrase pairs produced by each of our models by collecting human judgments of what paraphrases are acceptable in sentences drawn from the target domain and in sentences drawn from the general domain. We sample 15K sentences from our biology data, and 10K general-domain sentences from Wikipedia. We select a phrase from each sentence, and show the list of candidate paraphrases1 to 5 human judges. Judges make a binary decision about whether each paraphrase is a"
P15-2010,C12-1177,0,0.0191268,"ific target domains. We explore the following ideas: Introduction Many data-driven paraphrase extraction algorithms have been developed in recent years (Madnani and Dorr, 2010; Androutsopoulos and Malakasiotis, 2010). These algorithms attempt to learn paraphrase rules, where one phrase can be replaced with another phrase which has equivalent meaning in at least some context. Determining whether a paraphrase is appropriate for a specific context is a difficult problem (Bhagat and Hovy, 2013), encompassing issues of syntax (Callison-Burch, 2008), word sense (Apidianaki et al., 2014), and style (Xu et al., 2012; Pavlick and Nenkova, 2015). To date, the question of how domain effects paraphrase has been left unexplored. Although most paraphrase extraction algorithms attempt to estimate a confidence with which a paraphrase rule might apply, these scores are not differentiated by domain, and instead correspond to the general domain represented by the model’s training data. As illustrated by Table 1, paraphrases that are highly probable in the general domain (e.g. hot = sexy) can be extremely improbable in more specialized domains like biology. Dominant word senses change depending on 1. We sort sentenc"
P15-2010,I08-2088,0,0.0804447,"Missing"
P15-2010,steinberger-etal-2006-jrc,0,\N,Missing
P15-2010,D10-1044,0,\N,Missing
P15-2010,W09-0401,1,\N,Missing
P15-2010,N13-1092,1,\N,Missing
P15-2010,J13-3001,0,\N,Missing
P15-2067,W07-1424,0,0.157149,"Missing"
P15-2067,ferrandez-etal-2010-aligning,0,0.118792,"Missing"
P15-2067,P13-2130,0,0.15027,"Missing"
P15-2067,N13-1092,1,0.691494,"Missing"
P15-2067,P10-2045,0,0.033468,"Missing"
P15-2067,J02-3001,0,0.237857,"Missing"
P15-2067,P98-1013,0,0.13149,"Missing"
P15-2067,W10-0735,0,0.029492,"Missing"
P15-2067,P13-2121,0,0.0542521,"Missing"
P15-2067,P14-1136,0,0.035095,"Missing"
P15-2067,D08-1021,1,0.826454,"Missing"
P15-2067,W10-0907,0,0.0542799,"Missing"
P15-2067,N03-2022,0,0.0541534,"Missing"
P15-2067,P11-1144,0,0.0227914,"Missing"
P15-2067,C10-2107,0,0.0853446,"Missing"
P15-2067,N12-1086,0,0.098033,"Missing"
P15-2067,D08-1048,0,0.205318,"Missing"
P15-2067,W14-2901,1,0.897335,"Missing"
P15-2067,D07-1002,0,0.049493,"Missing"
P15-2067,C98-1013,0,\N,Missing
P15-2070,D11-1108,1,0.522536,"Missing"
P15-2070,N13-1092,1,0.806581,"Missing"
P15-2070,S13-1005,0,0.0116965,"udes finegrained entailment relations, word embedding similarities, and style annotations. 1 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. Introduction The Paraphrase Database (PPDB) is a collection of over 100 million paraphrases that was automatically constructed by Ganitkevitch et al. (2013). Although it is relatively new, it has been adopted by a large number of researchers, who have demonstrated that it is useful for a variety of natural language processing tasks. It has been used for recognizing textual entailment (Beltagy et al., 2014; Bjerva et al., 2014), measuring the semantic similarity of texts (Han et al., 2013; Ji and Eisenstein, 2013; Sultan et al., 2014b), monolingual alignment (Yao et al., 2013; Sultan et al., 2014a), natural language generation (Ganitkevitch et al., 2011), and improved lexical embeddings (Yu and Dredze, 2014; Rastogi et al., 2015; Faruqui et al., 2015). For any given input phrase to PPDB, there are often dozens or hundreds of possible paraphrases. There are several interesting research questions that arise because of the number and variety of paraphrases in PPDB. How can we distinguish between correct and incorrect paraphrases? Within the paraphrase sets, are all of the paraphr"
P15-2070,D13-1090,0,0.0129039,"ntailment relations, word embedding similarities, and style annotations. 1 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. Introduction The Paraphrase Database (PPDB) is a collection of over 100 million paraphrases that was automatically constructed by Ganitkevitch et al. (2013). Although it is relatively new, it has been adopted by a large number of researchers, who have demonstrated that it is useful for a variety of natural language processing tasks. It has been used for recognizing textual entailment (Beltagy et al., 2014; Bjerva et al., 2014), measuring the semantic similarity of texts (Han et al., 2013; Ji and Eisenstein, 2013; Sultan et al., 2014b), monolingual alignment (Yao et al., 2013; Sultan et al., 2014a), natural language generation (Ganitkevitch et al., 2011), and improved lexical embeddings (Yu and Dredze, 2014; Rastogi et al., 2015; Faruqui et al., 2015). For any given input phrase to PPDB, there are often dozens or hundreds of possible paraphrases. There are several interesting research questions that arise because of the number and variety of paraphrases in PPDB. How can we distinguish between correct and incorrect paraphrases? Within the paraphrase sets, are all of the paraphrases truly substitutable"
P15-2070,P05-1074,1,0.656316,"zontal axis) for four ways of automatically ranking the paraphrases: p(e2 |e1 ) (far left), PPDB 1.0’s heuristic ranking method (middle left), word2vec similarity (middle right), and our supervised model for PPDB 2.0 (far right). Our rankings achieve the highest correlation with human judgements with a Spearman’s ρ of 0.71. Upon publication of this paper, we will release PPDB 2.0 along with a set of 26K phrase pairs annotated with human similarity judgments. 2 + + + + + + Improved rankings of paraphrases The notion of ranking paraphrases goes back to the original method that PPDB is based on. Bannard and Callison-Burch (2005) introduced the bilingual pivoting method, which extracts incarcerated as a potential paraphrase of put in prison since they are both aligned to festgenommen in different sentence pairs in an English-German bitext. Since incarcerated aligns to many foreign words (in many languages) the list of potential paraphrases is long. Paraphrases vary in quality since the alignments are automatically produced and noisy. In order to rank the paraphrases, Bannard and CallisonBurch (2005) define a paraphrase probability in terms of the translation model probabilities p(f |e) and p(e|f ): p(e2 |e1 ) ≈ X p(e2"
P15-2070,D11-1009,0,0.0153625,"straining to types which appear in PPDB), and collected human judgments for their full list of paraphrases. 2 https://code.google.com/p/word2vec/ For phrases, we use the vector of the rarest word as an approximation of the vector for the phrase. 3 427 1. 2. 3. 4. 5. achieves a 9-12 point improvement in MRR over the PPDB 1.0 rankings. Similarly, it improves AP by 7-9 points. 3 Other Additions Entailment relations 4 the final analysis the last the finish the final part the last part Related Work The most closely related work to our supervised re-ranking of PPDB is work by Zhao et al. (2008) and Malakasiotis and Androutsopoulos (2011). Zhao et al. (2008) improved Bannard and Callison-Burch (2005)’s paraphrase probability by converting it into log-linear model inspired by machine translation, allowing them to incorporate a variety of features. Malakasiotis and Androutsopoulos (2011) developed a similar model trained on human judgements. Both efforts apply their model to natural language generation by paraphrasing full sentences. We apply our model to the sub-sentential paraphrases directly, in order to improve the quality of the Paraphrase Database. Also related is work by Chan et al. (2011) which reranked bilingually-extra"
P15-2070,S14-2141,0,0.0183407,"s than PPDB 1.0’s heuristic rankings. Each paraphrase pair in the database now also includes finegrained entailment relations, word embedding similarities, and style annotations. 1 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. Introduction The Paraphrase Database (PPDB) is a collection of over 100 million paraphrases that was automatically constructed by Ganitkevitch et al. (2013). Although it is relatively new, it has been adopted by a large number of researchers, who have demonstrated that it is useful for a variety of natural language processing tasks. It has been used for recognizing textual entailment (Beltagy et al., 2014; Bjerva et al., 2014), measuring the semantic similarity of texts (Han et al., 2013; Ji and Eisenstein, 2013; Sultan et al., 2014b), monolingual alignment (Yao et al., 2013; Sultan et al., 2014a), natural language generation (Ganitkevitch et al., 2011), and improved lexical embeddings (Yu and Dredze, 2014; Rastogi et al., 2015; Faruqui et al., 2015). For any given input phrase to PPDB, there are often dozens or hundreds of possible paraphrases. There are several interesting research questions that arise because of the number and variety of paraphrases in PPDB. How can we distinguish between c"
P15-2070,N15-1023,1,0.665525,"2015). However, the Rastogi et al. (2015) embeddings included here were shown to be state-of-the art in Style scores Some of the variation within paraphrase sets can be attributed to stylistic variations of language. We automatically induce style information on each rule in PPDB for two dimensions– complexity and formality. Table 2 shows some paraphrases of the end, sorted from most complex to most simple using these scores. These classifications could be useful for natural language generation tasks like text simplification (Xu et al., 2015). A complete evaluation of these scores is given in Pavlick and Nenkova (2015). 3.3 11. 12. 13. 14. 15. used to measure word and phrase similarity, possibly to improve paraphrasing. Multiview Latent Semantic Analysis (MVLSA) is a state-of-the-art method for modeling word similarities. MVLSA can incorporate an arbitrary number of data views, such as monolingual signals, bilingual signals, and even signals from other embeddings. PPDB 2.0 contains new similarity features based on MVLSA embeddings for all phrases. A complete discussion is given in Rastogi et al. (2015). Although we typically think of paraphrases as equivalent or as bidirectionally entailing, a substantial f"
P15-2070,P11-1062,0,0.0101222,"milar model trained on human judgements. Both efforts apply their model to natural language generation by paraphrasing full sentences. We apply our model to the sub-sentential paraphrases directly, in order to improve the quality of the Paraphrase Database. Also related is work by Chan et al. (2011) which reranked bilingually-extracted paraphrases using monolingual distributional similarities, but did not use a supervised model. Work that is relevant to our classification of semantic entailment types to each paraphrase, includes learning directionality of inference rules (Bhagat et al., 2007; Berant et al., 2011) and learning hypernyms rather than paraphrases (Snow et al., 2004). Our style annotations are related to Xu et al. (2012)’s efforts at learning stylistic paraphrases. Our word embeddings additions to the paraphrase database are related to many current projects on that topic, including projects that attempt to customize embeddings to lexical resources (Faruqui et al., 2015). However, the Rastogi et al. (2015) embeddings included here were shown to be state-of-the art in Style scores Some of the variation within paraphrase sets can be attributed to stylistic variations of language. We automatic"
P15-2070,P15-1146,1,0.863748,"Missing"
P15-2070,D07-1017,0,0.0315597,"(2011) developed a similar model trained on human judgements. Both efforts apply their model to natural language generation by paraphrasing full sentences. We apply our model to the sub-sentential paraphrases directly, in order to improve the quality of the Paraphrase Database. Also related is work by Chan et al. (2011) which reranked bilingually-extracted paraphrases using monolingual distributional similarities, but did not use a supervised model. Work that is relevant to our classification of semantic entailment types to each paraphrase, includes learning directionality of inference rules (Bhagat et al., 2007; Berant et al., 2011) and learning hypernyms rather than paraphrases (Snow et al., 2004). Our style annotations are related to Xu et al. (2012)’s efforts at learning stylistic paraphrases. Our word embeddings additions to the paraphrase database are related to many current projects on that topic, including projects that attempt to customize embeddings to lexical resources (Faruqui et al., 2015). However, the Rastogi et al. (2015) embeddings included here were shown to be state-of-the art in Style scores Some of the variation within paraphrase sets can be attributed to stylistic variations of"
P15-2070,D14-1162,0,0.112672,"Missing"
P15-2070,S14-2114,0,0.0184637,"Missing"
P15-2070,N15-1058,1,0.0846646,"Missing"
P15-2070,Q14-1018,0,0.0256743,"embedding similarities, and style annotations. 1 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. Introduction The Paraphrase Database (PPDB) is a collection of over 100 million paraphrases that was automatically constructed by Ganitkevitch et al. (2013). Although it is relatively new, it has been adopted by a large number of researchers, who have demonstrated that it is useful for a variety of natural language processing tasks. It has been used for recognizing textual entailment (Beltagy et al., 2014; Bjerva et al., 2014), measuring the semantic similarity of texts (Han et al., 2013; Ji and Eisenstein, 2013; Sultan et al., 2014b), monolingual alignment (Yao et al., 2013; Sultan et al., 2014a), natural language generation (Ganitkevitch et al., 2011), and improved lexical embeddings (Yu and Dredze, 2014; Rastogi et al., 2015; Faruqui et al., 2015). For any given input phrase to PPDB, there are often dozens or hundreds of possible paraphrases. There are several interesting research questions that arise because of the number and variety of paraphrases in PPDB. How can we distinguish between correct and incorrect paraphrases? Within the paraphrase sets, are all of the paraphrases truly substitutable or do they sometimes"
P15-2070,S14-2039,0,0.013621,"embedding similarities, and style annotations. 1 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. Introduction The Paraphrase Database (PPDB) is a collection of over 100 million paraphrases that was automatically constructed by Ganitkevitch et al. (2013). Although it is relatively new, it has been adopted by a large number of researchers, who have demonstrated that it is useful for a variety of natural language processing tasks. It has been used for recognizing textual entailment (Beltagy et al., 2014; Bjerva et al., 2014), measuring the semantic similarity of texts (Han et al., 2013; Ji and Eisenstein, 2013; Sultan et al., 2014b), monolingual alignment (Yao et al., 2013; Sultan et al., 2014a), natural language generation (Ganitkevitch et al., 2011), and improved lexical embeddings (Yu and Dredze, 2014; Rastogi et al., 2015; Faruqui et al., 2015). For any given input phrase to PPDB, there are often dozens or hundreds of possible paraphrases. There are several interesting research questions that arise because of the number and variety of paraphrases in PPDB. How can we distinguish between correct and incorrect paraphrases? Within the paraphrase sets, are all of the paraphrases truly substitutable or do they sometimes"
P15-2070,C12-1177,0,0.00936657,"ntences. We apply our model to the sub-sentential paraphrases directly, in order to improve the quality of the Paraphrase Database. Also related is work by Chan et al. (2011) which reranked bilingually-extracted paraphrases using monolingual distributional similarities, but did not use a supervised model. Work that is relevant to our classification of semantic entailment types to each paraphrase, includes learning directionality of inference rules (Bhagat et al., 2007; Berant et al., 2011) and learning hypernyms rather than paraphrases (Snow et al., 2004). Our style annotations are related to Xu et al. (2012)’s efforts at learning stylistic paraphrases. Our word embeddings additions to the paraphrase database are related to many current projects on that topic, including projects that attempt to customize embeddings to lexical resources (Faruqui et al., 2015). However, the Rastogi et al. (2015) embeddings included here were shown to be state-of-the art in Style scores Some of the variation within paraphrase sets can be attributed to stylistic variations of language. We automatically induce style information on each rule in PPDB for two dimensions– complexity and formality. Table 2 shows some paraph"
P15-2070,Q15-1021,1,0.0693675,"ttempt to customize embeddings to lexical resources (Faruqui et al., 2015). However, the Rastogi et al. (2015) embeddings included here were shown to be state-of-the art in Style scores Some of the variation within paraphrase sets can be attributed to stylistic variations of language. We automatically induce style information on each rule in PPDB for two dimensions– complexity and formality. Table 2 shows some paraphrases of the end, sorted from most complex to most simple using these scores. These classifications could be useful for natural language generation tasks like text simplification (Xu et al., 2015). A complete evaluation of these scores is given in Pavlick and Nenkova (2015). 3.3 11. 12. 13. 14. 15. used to measure word and phrase similarity, possibly to improve paraphrasing. Multiview Latent Semantic Analysis (MVLSA) is a state-of-the-art method for modeling word similarities. MVLSA can incorporate an arbitrary number of data views, such as monolingual signals, bilingual signals, and even signals from other embeddings. PPDB 2.0 contains new similarity features based on MVLSA embeddings for all phrases. A complete discussion is given in Rastogi et al. (2015). Although we typically think"
P15-2070,D13-1056,1,0.547488,"Missing"
P15-2070,P14-2089,0,0.046308,"was automatically constructed by Ganitkevitch et al. (2013). Although it is relatively new, it has been adopted by a large number of researchers, who have demonstrated that it is useful for a variety of natural language processing tasks. It has been used for recognizing textual entailment (Beltagy et al., 2014; Bjerva et al., 2014), measuring the semantic similarity of texts (Han et al., 2013; Ji and Eisenstein, 2013; Sultan et al., 2014b), monolingual alignment (Yao et al., 2013; Sultan et al., 2014a), natural language generation (Ganitkevitch et al., 2011), and improved lexical embeddings (Yu and Dredze, 2014; Rastogi et al., 2015; Faruqui et al., 2015). For any given input phrase to PPDB, there are often dozens or hundreds of possible paraphrases. There are several interesting research questions that arise because of the number and variety of paraphrases in PPDB. How can we distinguish between correct and incorrect paraphrases? Within the paraphrase sets, are all of the paraphrases truly substitutable or do they sometimes exhibit other types of relationships (like directional entailment)? When the paraphrases share the same meaning, are there stylistic reasons why we should choose one versus anot"
P15-2070,P08-1116,0,0.0365443,"pes from Wikipedia (constraining to types which appear in PPDB), and collected human judgments for their full list of paraphrases. 2 https://code.google.com/p/word2vec/ For phrases, we use the vector of the rarest word as an approximation of the vector for the phrase. 3 427 1. 2. 3. 4. 5. achieves a 9-12 point improvement in MRR over the PPDB 1.0 rankings. Similarly, it improves AP by 7-9 points. 3 Other Additions Entailment relations 4 the final analysis the last the finish the final part the last part Related Work The most closely related work to our supervised re-ranking of PPDB is work by Zhao et al. (2008) and Malakasiotis and Androutsopoulos (2011). Zhao et al. (2008) improved Bannard and Callison-Burch (2005)’s paraphrase probability by converting it into log-linear model inspired by machine translation, allowing them to incorporate a variety of features. Malakasiotis and Androutsopoulos (2011) developed a similar model trained on human judgements. Both efforts apply their model to natural language generation by paraphrasing full sentences. We apply our model to the sub-sentential paraphrases directly, in order to improve the quality of the Paraphrase Database. Also related is work by Chan et"
P15-2070,N15-1184,0,\N,Missing
P15-2070,W11-2504,1,\N,Missing
P15-2070,D08-1021,1,\N,Missing
P16-1204,W06-1805,0,0.175972,"ther it is modifying baby or control. Pustejovsky (2013) offer a preliminary analysis of the contextual complexities surrounding adjective inference, which reinforces many of the observations we have made here. Hartung and Frank (2011) analyze adjectives in terms of the properties they modify but don’t address them from an entailment perspective. Tien Nguyen et al. (2014) look at the 2170 adjectives in the restricted domain of computer vision. Other past work has employed first-order logic and other formal representations of adjectives in order to provide compositional entailment predictions (Amoia and Gardent, 2006; Amoia and Gardent, 2007; McCrae et al., 2014). Although theoretically appealing, such rigid logics are unlikely to provide the flexibility needed to handle the type of common-sense inferences we have discussed here. Distributional representations provide much greater flexibility in terms of representation (Baroni and Zamparelli, 2010; Guevara, 2010; Boleda et al., 2013). However, work on distributional AN composition has so far remained out-of-context, and has mostly been evaluated in terms of overall “similarity” rather than directly addressing the entailment properties associated with comp"
P16-1204,W07-1430,0,0.196159,"or control. Pustejovsky (2013) offer a preliminary analysis of the contextual complexities surrounding adjective inference, which reinforces many of the observations we have made here. Hartung and Frank (2011) analyze adjectives in terms of the properties they modify but don’t address them from an entailment perspective. Tien Nguyen et al. (2014) look at the 2170 adjectives in the restricted domain of computer vision. Other past work has employed first-order logic and other formal representations of adjectives in order to provide compositional entailment predictions (Amoia and Gardent, 2006; Amoia and Gardent, 2007; McCrae et al., 2014). Although theoretically appealing, such rigid logics are unlikely to provide the flexibility needed to handle the type of common-sense inferences we have discussed here. Distributional representations provide much greater flexibility in terms of representation (Baroni and Zamparelli, 2010; Guevara, 2010; Boleda et al., 2013). However, work on distributional AN composition has so far remained out-of-context, and has mostly been evaluated in terms of overall “similarity” rather than directly addressing the entailment properties associated with composition. 9 Conclusion We"
P16-1204,D14-1059,0,0.0246266,"DEL(red, 5) a1 girl2 in3 a4 dress5 We say that the entailment relation that holds between x and e(x) is generated by the edit e. In the above example, we would say that e generates a forward entailment (@) since a girl in a red dress entails a girl in a dress. 3 Natural Logic Entailment Relations Natural logic (MacCartney, 2009) is a formalism that describes entailment relationships between natural language strings, rather than operating over mathematical formulae. Natural logic enables both light-weight representation and robust inference, and is an increasingly popular choice for NLU tasks (Angeli and Manning, 2014; Bowman et al., 2015b; Pavlick et al., 2015). There are seven “basic entailment relations” described by natural logic, five of which we explore here.1 These five relations, as they might hold between an AN and the head N, are summarized in Figure 1. The forward entailment relation is the restrictive case, in which the AN (brown dog) is a subset of (and thus entails) the N (dog) but the N does not entail the AN (dog does not entail brown dog). The symmetric reverse entailment can also occur, in which the N is a subset of the set denoted by the AN. An example of this is the AN possible solution"
P16-1204,P15-1034,0,0.014064,"Missing"
P16-1204,D10-1115,0,0.0125819,"nt perspective. Tien Nguyen et al. (2014) look at the 2170 adjectives in the restricted domain of computer vision. Other past work has employed first-order logic and other formal representations of adjectives in order to provide compositional entailment predictions (Amoia and Gardent, 2006; Amoia and Gardent, 2007; McCrae et al., 2014). Although theoretically appealing, such rigid logics are unlikely to provide the flexibility needed to handle the type of common-sense inferences we have discussed here. Distributional representations provide much greater flexibility in terms of representation (Baroni and Zamparelli, 2010; Guevara, 2010; Boleda et al., 2013). However, work on distributional AN composition has so far remained out-of-context, and has mostly been evaluated in terms of overall “similarity” rather than directly addressing the entailment properties associated with composition. 9 Conclusion We have investigated the problem of adjective-noun composition, specifically in relation to the task of RTE. AN composition is capable of producing a range of natural logic entailment relationship, at odds with commonly-used heuristics which treat all adjectives a restrictive. We have shown that predicting these e"
P16-1204,E12-1004,0,0.0148644,"bine. Adjectival modification is one of the most basic types of composition in natural language. Most existing work in NLU makes a simplifying assumption that adjectives tend to be restrictive– i.e. adding an adjective modifier limits the set of things to which the noun phrase can refer. For example, the set of little dogs is a subset of the set of dogs, and we cannot in general say that dog entails little dog. This assumption has been exploited by high-performing RTE systems (MacCartney and Manning, 2008; Stern and Dagan, 2012), as well as used as the basis for learning new entailment rules (Baroni et al., 2012; Young et al., 2014). However, this simplified view of adjectival modification often breaks down in practice. Consider the question of whether laugh entails bitter laugh in the follow• We benchmark several state-of-the-art RTE systems on this task. 2 Recognizing Textual Entailment The task of recognizing textual entailment (RTE) (Dagan et al., 2006) is commonly used to evaluate the state-of-the-art of automatic NLU. The RTE task is: given two utterances, a premise (p) and a hypothesis (h), would a human reading p typically infer that h is most likely true? Systems are expected to produce eith"
P16-1204,S14-2114,0,0.0577574,"Missing"
P16-1204,W13-0104,0,0.184772,"Missing"
P16-1204,D15-1075,0,0.625266,"uistic phenomena like implicature (3) have yet to be explicitly included in RTE tasks, commonsense inferences like those in (4) (from the SICK dataset) have become a common part of NLU tasks like RTE, question answering, and image labeling. chased by a bear and are running for their lives! Example (4) is just one of many RTE problems which rely on intuition rather than strict logical inference. Transformation-based RTE. There have been an enormous range of approaches to automatic RTE– from those based on theorem proving (Bjerva et al., 2014) to those based on vector space models of semantics (Bowman et al., 2015a). Transformation-based RTE systems attempt to solve the RTE problem by identifying a sequence of atomic edits (MacCartney, 2009) which can be applied, one by one, in order to transform p into h. Each edit can be associated with some entailment relation. Then, the entailment relation that holds between p and h overall is a function of the entailment relations associated with each atomic edit. This approach is appealing in that it breaks potentially complex p/h pairs into a series of bite-sized pieces. Transformation-based RTE is widely used, not only in rule-based approaches (MacCartney and M"
P16-1204,W15-0705,0,0.0151887,"ly artificial contexts, as we believe this will result in a greater variety of entailment relations and will avoid systematically biasing our judgements toward entailments. Second, we use the most frequent AN pairs, as these will better represent the types of ANs that NLU systems are likely to encounter in practice. We look at four different corpora capturing four different genres: Annotated Gigaword (Napoles et al., 2012) (News), image captions (Young et al., 2014) (Image Captions), the Internet Argument Corpus (Walker et al., 2012) (Forums), and the prose fiction subset of GutenTag dataset (Brooke et al., 2015) (Literature). From each corpus, we select the 100 nouns which occur with the largest number of unique adjectives. Then, for each noun, we take the 10 adjectives with which the noun occurs most often. For each AN, we choose 3 contexts2 in which the N appears unmodified, and generate p/h pairs by inserting the A into each. We collect 3 judgements for each p/h pair. Since this task is subjective, and we want to focus our analysis on clean instances on which human agreement is high, we remove pairs for which one or more of the annotators chose the “does not make sense” option and pairs for which"
P16-1204,W04-3205,0,0.0490343,", 2014) includes a suite of RTE systems, including baseline systems as well as featurerich supervised systems which provide state-of-the-art performance on the RTE3 datasets (Giampiccolo et al., 2007). We test two systems from Excitement: the simple Maximum Entropy (MaxEnt) model which uses a suite of dense, similarity-based features (e.g. word overlap, cosine similarity), and the more sophisticated Maximum Entropy model (MaxEnt+LR) which uses the same similarity-based features but additionally incorporates features from external lexical resources such as WordNet (Miller, 1995) and VerbOcean (Chklovski and Pantel, 2004). We also train a standard unigram model (BOW). Transformation-based. The Excitement platform also includes a transformation-based RTE system called BIUTEE (Stern and Dagan, 2012). The BIUTEE system derives a sequence of edits that can be used to transform the premise into the hypothesis. These edits are represented using feature vectors, and the system searches over edit sequences for the lowest cost “proof” of either entailment or non-entailment. The feature weights are set by logistic regression during training. Deep learning. Bowman et al. (2015a) recently reported very promising results u"
P16-1204,W07-1401,0,0.0279932,"2 Recognizing Textual Entailment The task of recognizing textual entailment (RTE) (Dagan et al., 2006) is commonly used to evaluate the state-of-the-art of automatic NLU. The RTE task is: given two utterances, a premise (p) and a hypothesis (h), would a human reading p typically infer that h is most likely true? Systems are expected to produce either a binary (YES/NO) or trinary (ENTAILMENT/CONTRADICTION/UNKNOWN) output. The type of knowledge tested in the RTE task has shifted in recent years. While older datasets mostly captured logical reasoning (Cooper et al., 1996) and lexical knowledge (Giampiccolo et al., 2007) (see Examples (1) and (2) in Table 1), the recent datasets have become increasingly reliant on common-sense knowledge of scenes and events (Marelli et al., 2014). In Example (4) in Table 1, for which the gold label is ENTAILMENT , it is perfectly reasonable to assume the dogs are playing. However, this is not necessarily true that running entails playing– maybe the dogs are being 2164 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2164–2173, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics (1) FraCas (2) RTE2"
P16-1204,W10-2805,0,0.0149709,"t al. (2014) look at the 2170 adjectives in the restricted domain of computer vision. Other past work has employed first-order logic and other formal representations of adjectives in order to provide compositional entailment predictions (Amoia and Gardent, 2006; Amoia and Gardent, 2007; McCrae et al., 2014). Although theoretically appealing, such rigid logics are unlikely to provide the flexibility needed to handle the type of common-sense inferences we have discussed here. Distributional representations provide much greater flexibility in terms of representation (Baroni and Zamparelli, 2010; Guevara, 2010; Boleda et al., 2013). However, work on distributional AN composition has so far remained out-of-context, and has mostly been evaluated in terms of overall “similarity” rather than directly addressing the entailment properties associated with composition. 9 Conclusion We have investigated the problem of adjective-noun composition, specifically in relation to the task of RTE. AN composition is capable of producing a range of natural logic entailment relationship, at odds with commonly-used heuristics which treat all adjectives a restrictive. We have shown that predicting these entailment relat"
P16-1204,D11-1050,0,0.0139231,"n NLP, has explored different classes of adjectives (e.g. privative, intensional) as they relate to entailment (Kamp and Partee, 1995; Partee, 2007; Boleda et al., 2013; Nayak et al., 2014). In general, prior studies have focused on modeling properties of the adjectives alone, ignoring the context-dependent nature of AN/N entailments– i.e. in prior work little is always restrictive, whether it is modifying baby or control. Pustejovsky (2013) offer a preliminary analysis of the contextual complexities surrounding adjective inference, which reinforces many of the observations we have made here. Hartung and Frank (2011) analyze adjectives in terms of the properties they modify but don’t address them from an entailment perspective. Tien Nguyen et al. (2014) look at the 2170 adjectives in the restricted domain of computer vision. Other past work has employed first-order logic and other formal representations of adjectives in order to provide compositional entailment predictions (Amoia and Gardent, 2006; Amoia and Gardent, 2007; McCrae et al., 2014). Although theoretically appealing, such rigid logics are unlikely to provide the flexibility needed to handle the type of common-sense inferences we have discussed"
P16-1204,P14-5008,0,0.0201818,"Missing"
P16-1204,marelli-etal-2014-sick,0,0.0138331,"NLU. The RTE task is: given two utterances, a premise (p) and a hypothesis (h), would a human reading p typically infer that h is most likely true? Systems are expected to produce either a binary (YES/NO) or trinary (ENTAILMENT/CONTRADICTION/UNKNOWN) output. The type of knowledge tested in the RTE task has shifted in recent years. While older datasets mostly captured logical reasoning (Cooper et al., 1996) and lexical knowledge (Giampiccolo et al., 2007) (see Examples (1) and (2) in Table 1), the recent datasets have become increasingly reliant on common-sense knowledge of scenes and events (Marelli et al., 2014). In Example (4) in Table 1, for which the gold label is ENTAILMENT , it is perfectly reasonable to assume the dogs are playing. However, this is not necessarily true that running entails playing– maybe the dogs are being 2164 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2164–2173, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics (1) FraCas (2) RTE2 (3) NA (4) SICK p h p h p h p h No delegate finished the report on time. Some Scandinavian delegate finished the report on time. Trade between China and India is"
P16-1204,W14-4724,0,0.0548291,"(2013) offer a preliminary analysis of the contextual complexities surrounding adjective inference, which reinforces many of the observations we have made here. Hartung and Frank (2011) analyze adjectives in terms of the properties they modify but don’t address them from an entailment perspective. Tien Nguyen et al. (2014) look at the 2170 adjectives in the restricted domain of computer vision. Other past work has employed first-order logic and other formal representations of adjectives in order to provide compositional entailment predictions (Amoia and Gardent, 2006; Amoia and Gardent, 2007; McCrae et al., 2014). Although theoretically appealing, such rigid logics are unlikely to provide the flexibility needed to handle the type of common-sense inferences we have discussed here. Distributional representations provide much greater flexibility in terms of representation (Baroni and Zamparelli, 2010; Guevara, 2010; Boleda et al., 2013). However, work on distributional AN composition has so far remained out-of-context, and has mostly been evaluated in terms of overall “similarity” rather than directly addressing the entailment properties associated with composition. 9 Conclusion We have investigated the"
P16-1204,W12-3018,0,0.0156506,"Missing"
P16-1204,P15-1146,1,0.572693,"Missing"
P16-1204,C08-1066,0,0.0382672,"ce requires understanding not only the meanings of the individual words, but also understanding how those meanings combine. Adjectival modification is one of the most basic types of composition in natural language. Most existing work in NLU makes a simplifying assumption that adjectives tend to be restrictive– i.e. adding an adjective modifier limits the set of things to which the noun phrase can refer. For example, the set of little dogs is a subset of the set of dogs, and we cannot in general say that dog entails little dog. This assumption has been exploited by high-performing RTE systems (MacCartney and Manning, 2008; Stern and Dagan, 2012), as well as used as the basis for learning new entailment rules (Baroni et al., 2012; Young et al., 2014). However, this simplified view of adjectival modification often breaks down in practice. Consider the question of whether laugh entails bitter laugh in the follow• We benchmark several state-of-the-art RTE systems on this task. 2 Recognizing Textual Entailment The task of recognizing textual entailment (RTE) (Dagan et al., 2006) is commonly used to evaluate the state-of-the-art of automatic NLU. The RTE task is: given two utterances, a premise (p) and a hypothesis"
P16-1204,W13-0509,0,0.0950159,"to human levels, indicating that the systems fail even to memorize the most-likely class for each adjective in training. 8 Related Work Past work, both in linguistics and in NLP, has explored different classes of adjectives (e.g. privative, intensional) as they relate to entailment (Kamp and Partee, 1995; Partee, 2007; Boleda et al., 2013; Nayak et al., 2014). In general, prior studies have focused on modeling properties of the adjectives alone, ignoring the context-dependent nature of AN/N entailments– i.e. in prior work little is always restrictive, whether it is modifying baby or control. Pustejovsky (2013) offer a preliminary analysis of the contextual complexities surrounding adjective inference, which reinforces many of the observations we have made here. Hartung and Frank (2011) analyze adjectives in terms of the properties they modify but don’t address them from an entailment perspective. Tien Nguyen et al. (2014) look at the 2170 adjectives in the restricted domain of computer vision. Other past work has employed first-order logic and other formal representations of adjectives in order to provide compositional entailment predictions (Amoia and Gardent, 2006; Amoia and Gardent, 2007; McCrae"
P16-1204,P12-3013,0,0.374545,"only the meanings of the individual words, but also understanding how those meanings combine. Adjectival modification is one of the most basic types of composition in natural language. Most existing work in NLU makes a simplifying assumption that adjectives tend to be restrictive– i.e. adding an adjective modifier limits the set of things to which the noun phrase can refer. For example, the set of little dogs is a subset of the set of dogs, and we cannot in general say that dog entails little dog. This assumption has been exploited by high-performing RTE systems (MacCartney and Manning, 2008; Stern and Dagan, 2012), as well as used as the basis for learning new entailment rules (Baroni et al., 2012; Young et al., 2014). However, this simplified view of adjectival modification often breaks down in practice. Consider the question of whether laugh entails bitter laugh in the follow• We benchmark several state-of-the-art RTE systems on this task. 2 Recognizing Textual Entailment The task of recognizing textual entailment (RTE) (Dagan et al., 2006) is commonly used to evaluate the state-of-the-art of automatic NLU. The RTE task is: given two utterances, a premise (p) and a hypothesis (h), would a human readi"
P16-1204,W14-5418,0,0.0225433,"2007; Boleda et al., 2013; Nayak et al., 2014). In general, prior studies have focused on modeling properties of the adjectives alone, ignoring the context-dependent nature of AN/N entailments– i.e. in prior work little is always restrictive, whether it is modifying baby or control. Pustejovsky (2013) offer a preliminary analysis of the contextual complexities surrounding adjective inference, which reinforces many of the observations we have made here. Hartung and Frank (2011) analyze adjectives in terms of the properties they modify but don’t address them from an entailment perspective. Tien Nguyen et al. (2014) look at the 2170 adjectives in the restricted domain of computer vision. Other past work has employed first-order logic and other formal representations of adjectives in order to provide compositional entailment predictions (Amoia and Gardent, 2006; Amoia and Gardent, 2007; McCrae et al., 2014). Although theoretically appealing, such rigid logics are unlikely to provide the flexibility needed to handle the type of common-sense inferences we have discussed here. Distributional representations provide much greater flexibility in terms of representation (Baroni and Zamparelli, 2010; Guevara, 201"
P16-1204,walker-etal-2012-corpus,0,0.0180227,"e above pilot experiments, we proceed with our study as follows. First, we use only artificial contexts, as we believe this will result in a greater variety of entailment relations and will avoid systematically biasing our judgements toward entailments. Second, we use the most frequent AN pairs, as these will better represent the types of ANs that NLU systems are likely to encounter in practice. We look at four different corpora capturing four different genres: Annotated Gigaword (Napoles et al., 2012) (News), image captions (Young et al., 2014) (Image Captions), the Internet Argument Corpus (Walker et al., 2012) (Forums), and the prose fiction subset of GutenTag dataset (Brooke et al., 2015) (Literature). From each corpus, we select the 100 nouns which occur with the largest number of unique adjectives. Then, for each noun, we take the 10 adjectives with which the noun occurs most often. For each AN, we choose 3 contexts2 in which the N appears unmodified, and generate p/h pairs by inserting the A into each. We collect 3 judgements for each p/h pair. Since this task is subjective, and we want to focus our analysis on clean instances on which human agreement is high, we remove pairs for which one or m"
P16-1204,Q14-1006,0,0.583785,"fication is one of the most basic types of composition in natural language. Most existing work in NLU makes a simplifying assumption that adjectives tend to be restrictive– i.e. adding an adjective modifier limits the set of things to which the noun phrase can refer. For example, the set of little dogs is a subset of the set of dogs, and we cannot in general say that dog entails little dog. This assumption has been exploited by high-performing RTE systems (MacCartney and Manning, 2008; Stern and Dagan, 2012), as well as used as the basis for learning new entailment rules (Baroni et al., 2012; Young et al., 2014). However, this simplified view of adjectival modification often breaks down in practice. Consider the question of whether laugh entails bitter laugh in the follow• We benchmark several state-of-the-art RTE systems on this task. 2 Recognizing Textual Entailment The task of recognizing textual entailment (RTE) (Dagan et al., 2006) is commonly used to evaluate the state-of-the-art of automatic NLU. The RTE task is: given two utterances, a premise (p) and a hypothesis (h), would a human reading p typically infer that h is most likely true? Systems are expected to produce either a binary (YES/NO)"
P16-2024,E99-1042,0,0.561558,"Missing"
P16-2024,W11-1601,0,0.0591354,"and which match the syntactic category of the target. On average, Simple PPDB proposes 8.8 such candidate simplifications per target. Comparison to existing methods. Our baselines include three existing methods for generating lists of candidates that were proposed in prior work. The methods we test for generating lists of candidate paraphrases for a given target are: the WordNetGenerator, which pulls synonyms from WordNet (Devlin and Tait, 1998; Carroll et al., 1999), the KauchakGenerator, which generates candidates based on automatic alignments between Simple Wikipedia and normal Wikipedia (Coster and Kauchak, 2011a), and the GlavasGenerator, which generates candidates from nearby phrases in vector space (Glavaˇs and ˇ Stajner, 2015) (we use the pre-trained Word2Vec VSM (Mikolov et al., 2013)). For each generated list, we follow Horn et al. (2014)’s supervised SVM Rank approach to rank the candidates for simplicity. We reimplement the main features of their model: namely, word frequencies according to the Google NGrams corpus (Brants and Franz, 2006) and the Simple Wikipedia corpus, and the alignment probabilities according to automatic word alignments between Wikipedia and Simple Wikipedia sentences (C"
P16-2024,P11-2117,0,0.0242177,"and which match the syntactic category of the target. On average, Simple PPDB proposes 8.8 such candidate simplifications per target. Comparison to existing methods. Our baselines include three existing methods for generating lists of candidates that were proposed in prior work. The methods we test for generating lists of candidate paraphrases for a given target are: the WordNetGenerator, which pulls synonyms from WordNet (Devlin and Tait, 1998; Carroll et al., 1999), the KauchakGenerator, which generates candidates based on automatic alignments between Simple Wikipedia and normal Wikipedia (Coster and Kauchak, 2011a), and the GlavasGenerator, which generates candidates from nearby phrases in vector space (Glavaˇs and ˇ Stajner, 2015) (we use the pre-trained Word2Vec VSM (Mikolov et al., 2013)). For each generated list, we follow Horn et al. (2014)’s supervised SVM Rank approach to rank the candidates for simplicity. We reimplement the main features of their model: namely, word frequencies according to the Google NGrams corpus (Brants and Franz, 2006) and the Simple Wikipedia corpus, and the alignment probabilities according to automatic word alignments between Wikipedia and Simple Wikipedia sentences (C"
P16-2024,N13-1092,1,0.427348,"Missing"
P16-2024,P15-2011,0,0.139424,"Missing"
P16-2024,C04-1129,0,0.0311292,"e Paraphrase Database containing 4.5 million simplifying paraphrase rules. The large scale of Simple PPDB will support research into increasingly advanced methods for text simplification. Motivation Language is complex, and the process of reading and understanding language is difficult for many groups of people. The goal of text simplification is to rewrite text in order to make it easier to understand, for example, by children (De Belder and Moens, 2010), language learners (Petersen and Ostendorf, 2007), people with disabilities (Rello et al., 2013; Evans et al., 2014), and even by machines (Siddharthan et al., 2004). Automatic text simplification (Napoles and Dredze, 2010; Wubben et al., 2012; Xu et al., 2016) has the potential to dramatically increase access to information by making written documents available at all reading levels. Full text simplification involves many steps, including grammatical restructuring and summarization (Feng, 2008). One of the most basic subtasks is lexical simplification (Specia et al., 2012)– replacing complicated words and phrases with simpler paraphrases. While there is active research in the area of lexical simplification (Coster ˇ and Kauchak, 2011a; Glavaˇs and Stajne"
P16-2024,P14-2075,0,0.101407,"tes that were proposed in prior work. The methods we test for generating lists of candidate paraphrases for a given target are: the WordNetGenerator, which pulls synonyms from WordNet (Devlin and Tait, 1998; Carroll et al., 1999), the KauchakGenerator, which generates candidates based on automatic alignments between Simple Wikipedia and normal Wikipedia (Coster and Kauchak, 2011a), and the GlavasGenerator, which generates candidates from nearby phrases in vector space (Glavaˇs and ˇ Stajner, 2015) (we use the pre-trained Word2Vec VSM (Mikolov et al., 2013)). For each generated list, we follow Horn et al. (2014)’s supervised SVM Rank approach to rank the candidates for simplicity. We reimplement the main features of their model: namely, word frequencies according to the Google NGrams corpus (Brants and Franz, 2006) and the Simple Wikipedia corpus, and the alignment probabilities according to automatic word alignments between Wikipedia and Simple Wikipedia sentences (Coster and Kauchak, 2011b). We omit the language modeling features since our evaluation does not consider the context in which the substitution is to be applied. All of these methods (the three generation methods and the ranker) are imple"
P16-2024,S12-1046,0,0.0133657,"by children (De Belder and Moens, 2010), language learners (Petersen and Ostendorf, 2007), people with disabilities (Rello et al., 2013; Evans et al., 2014), and even by machines (Siddharthan et al., 2004). Automatic text simplification (Napoles and Dredze, 2010; Wubben et al., 2012; Xu et al., 2016) has the potential to dramatically increase access to information by making written documents available at all reading levels. Full text simplification involves many steps, including grammatical restructuring and summarization (Feng, 2008). One of the most basic subtasks is lexical simplification (Specia et al., 2012)– replacing complicated words and phrases with simpler paraphrases. While there is active research in the area of lexical simplification (Coster ˇ and Kauchak, 2011a; Glavaˇs and Stajner, 2015; Paetzold, 2015), existing models have been byand-large limited to single words. Often, how1 http://www.seas.upenn.edu/˜nlp/ resources/simple-ppdb.tgz 143 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 143–148, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics 2 Identifying Simplification Rules 2.1 scored, the average huma"
P16-2024,W10-0406,0,0.0253281,"araphrase rules. The large scale of Simple PPDB will support research into increasingly advanced methods for text simplification. Motivation Language is complex, and the process of reading and understanding language is difficult for many groups of people. The goal of text simplification is to rewrite text in order to make it easier to understand, for example, by children (De Belder and Moens, 2010), language learners (Petersen and Ostendorf, 2007), people with disabilities (Rello et al., 2013; Evans et al., 2014), and even by machines (Siddharthan et al., 2004). Automatic text simplification (Napoles and Dredze, 2010; Wubben et al., 2012; Xu et al., 2016) has the potential to dramatically increase access to information by making written documents available at all reading levels. Full text simplification involves many steps, including grammatical restructuring and summarization (Feng, 2008). One of the most basic subtasks is lexical simplification (Specia et al., 2012)– replacing complicated words and phrases with simpler paraphrases. While there is active research in the area of lexical simplification (Coster ˇ and Kauchak, 2011a; Glavaˇs and Stajner, 2015; Paetzold, 2015), existing models have been byand"
P16-2024,P12-1107,0,0.0348609,"Missing"
P16-2024,P15-4015,0,0.0614715,"e candidates for simplicity. We reimplement the main features of their model: namely, word frequencies according to the Google NGrams corpus (Brants and Franz, 2006) and the Simple Wikipedia corpus, and the alignment probabilities according to automatic word alignments between Wikipedia and Simple Wikipedia sentences (Coster and Kauchak, 2011b). We omit the language modeling features since our evaluation does not consider the context in which the substitution is to be applied. All of these methods (the three generation methods and the ranker) are implemented as part of the LEXenstein toolkit (Paetzold and Specia, 2015). We use the LEXenstein implementations for the results reported here, using off-the-shelf configurations and treating each method as a black box. Table 2: Accuracy on 10-fold cross-validation, and precision for identifying simplifying rules. Folds are constructed so that train and test vocabularies are disjoint. 5 points higher than the strongest baseline, a supervised model which uses only word embeddings as features. 2.3 Simple PPDB We run the trained model described above over all 7.5 million paraphrase rules. From the predictions, we construct Simple PPDB: a list of 4.5 million simplifyin"
P16-2024,Q15-1021,1,0.526854,"i, for each feature f , we include f (e1 ), f (e2 ) and f (e1 ) − f (e2 ).3 We also include the cosine similarity of the averaged word embeddings and the PPDB paraphrase quality score as features. We train a multi-class logistic regression model4 to predict if the application of a paraphrase rule will result in 1) simpler output, 2) more complex output, or 3) non-sense output. Data. We collect our training data in two phases. First, we sample 1,000 phrases from the vocabulary of the PPDB. We limit ourselves to words which also appear at least once in the Newsela corpus for text simplifcation (Xu et al., 2015), in order to ensure that we focus our model on the types of words for which the final resource is most likely to be applied. For each of these 1,000 words/phrases, we sample up to 10 candidate paraphrases from PPDB, stratified evenly across paraphrase quality scores. We ask workers on Amazon Mechanical Turk to rate each of the chosen paraphrase rules on a scale from 1 to 5 to indicate how well the paraphrase preserves the meaning of the original phrase. We use the same annotation design used in Pavlick et al. (2015). We have 5 workers judge each pair, omitting workers who do not provide corre"
P16-2024,N15-2002,0,0.0616377,"Missing"
P16-2024,Q16-1029,1,0.554725,"will support research into increasingly advanced methods for text simplification. Motivation Language is complex, and the process of reading and understanding language is difficult for many groups of people. The goal of text simplification is to rewrite text in order to make it easier to understand, for example, by children (De Belder and Moens, 2010), language learners (Petersen and Ostendorf, 2007), people with disabilities (Rello et al., 2013; Evans et al., 2014), and even by machines (Siddharthan et al., 2004). Automatic text simplification (Napoles and Dredze, 2010; Wubben et al., 2012; Xu et al., 2016) has the potential to dramatically increase access to information by making written documents available at all reading levels. Full text simplification involves many steps, including grammatical restructuring and summarization (Feng, 2008). One of the most basic subtasks is lexical simplification (Specia et al., 2012)– replacing complicated words and phrases with simpler paraphrases. While there is active research in the area of lexical simplification (Coster ˇ and Kauchak, 2011a; Glavaˇs and Stajner, 2015; Paetzold, 2015), existing models have been byand-large limited to single words. Often,"
P16-2024,N15-1023,1,0.593985,"Missing"
P16-2024,P15-2070,1,0.867766,"Missing"
P16-2024,W14-1215,0,\N,Missing
P17-1192,P14-1098,0,0.0347051,"Missing"
P17-1192,D10-1115,0,0.0493839,"orated into computational models previously, prior work focuses on either assigning an intrinsic meaning to M or on operationalizing M in a truth-theoretic sense, but not on doing both simultaneously. For example, Young et al. (2014) focuses exclusively on the subset selection aspect of modification. That is, given a set of instances H and a modifier M , their method could return the subset M H. However, their method does not model the meaning of the modifier itself, so that, e.g., if there were no red cars in their model of the world, the phrase “red cars” would have no meaning. In contrast, Baroni and Zamparelli (2010) models the meaning of modifiers explicitly as functions that map between vector-space representations of nouns. However, their model focuses on similarity between class labels–e.g., to say that “important routes” is similar to “major roads”–and it is not obvious how the method could be operationalized in order to identify instances of those classes. A contribution of our work is to model the semantics of M intrinsically, but in a way that permits application in the model theoretic setting. We learn an explicit model of the “meaning” of a modifier M relative to a head H, represented as a distr"
P17-1192,P15-1127,0,0.0155885,"al semantics framework to address two aspects of semantics that are often kept separate in NLP: assigning intrinsic “meaning” to a phrase, and reasoning about that phrase in a truth-theoretic context. 2 Related Work Noun Phrase Interpretation. Compound noun phrases (“jazz musician”) communicate implicit semantic relations between modifiers and the head. Many efforts to provide semantic interpretations of such phrases rely on matching the compound to pre-defined patterns or semantic ontologies (Fares et al., 2015; ´ S´eaghdha and Copestake, 2007; Tratz and O Hovy, 2010; Surtani and Paul, 2015; Choi et al., 2015). Recently, interpretations may take the form of arbitrary natural language predicates (Hendrickx et al., 2013). Most approaches are supervised, comparing unseen noun compounds to the most similar phrase seen in training (Wijaya and Gianfortoni, 2011; Nulty and Costello, 2013; Van de Cruys et al., 2013). Other unsupervised approaches apply information extraction techniques to paraphrase noun compounds (Kim and Nakov, 2011; Xavier and Strube de Lima, 2014; Pa¸sca, 2015). They focus exclusively on providing good paraphrases for an input noun compound. To our knowledge, ours is the first attempt"
P17-1192,D11-1142,0,0.0439309,"nce that p expresses a true relation between s and o. Both O and D are extracted from a sample of around 1 billion Web documents in English. The supplementary material gives additional details. We instantiate O with an IsA repository constructed by applying Hearst patterns to the Web documents. Instances are represented as automatically-disambiguated entity mentions4 which, when possible, are resolved to Wikipedia pages. Classes are represented as (non-disambiguated) natural language strings. We instantiate D with a large repository of facts extracted using in-house implementations of ReVerb (Fader et al., 2011) and OLLIE (Mausam et al., 2012). The predicates are extracted as natural language strings. Subjects and objects may be either disambiguated entity references or natural language strings. Every tuple is included in both the forward and the reverse direction. E.g. hjazz, perform at, venuei also appears as hvenue, ←perform at, jazzi, where ← is a spe4 “Entity mentions” may be individuals, like “Barack Obama”, but may also be concepts like “jazz”. cial character signifying inverted predicates. These inverted predicates simplify the following definitions. In total, O contains 1.1M tuples and D con"
P17-1192,P14-1089,0,0.012964,"hese approaches all share the limitation that, in practice, in order for a class to be populated with instances, the entire class label has to have been observed verbatim in text. This requirement limits the ability to handle arbitrarily fine-grained classes. Our work addresses this limitation by modeling fine-grained class labels compositionally. Thus the proposed method can combine evidence from multiple sentences, and can perform IsA extraction without requiring any example instances of a given class.1 Taxonomy Construction. Previous work on the construction of a taxonomy of IsA relations (Flati et al., 2014; de Melo and Weikum, 2010; Kozareva and Hovy, 2010; Ponzetto and Strube, 2007; Ponzetto and Navigli, 2009) considers that task to be different than extracting a flat set of IsA relations from text in practice. Challenges specific to taxonomy construction include overall concept positioning and how to discover whether concepts are unrelated, subordinated or parallel to each other (Kozareva and Hovy, 2010); the need to refine and enrich the taxonomy (Flati et al., 2014); the difficulty in adding relevant IsA relations towards the top of the taxonomy (Ponzetto and Navigli, 2009); eliminating cyc"
P17-1192,C92-2082,0,0.452485,"d Hearst patterns. 1 Figure 1: We extract instances of fine-grained classes by considering each of the modifiers in the class label individually. This allows us to extract instances even when the full class label never appears in text. Introduction The majority of approaches (Snow et al., 2006; Shwartz et al., 2016) for extracting IsA relations from text rely on lexical patterns as the primary signal of whether an instance belongs to a class. For example, observing a pattern like “X such as Y” is a strong indication that Y (e.g., “Charles Mingus”) is an instance of class X (e.g., “musician”) (Hearst, 1992). Methods based on these “Hearst patterns” assume that class labels can be treated as atomic lexicalized units. This assumption has several significant weakness. First, in order to recognize an instance of a class, these patternbased methods require that the entire class label be observed verbatim in text. The requirement is reasonable for class labels containing a single word, but in practice, there are many possible fine-grained classes: not only “musicians” but also “1950s American jazz musicians”. The probability that a given label will appear in its entirety within one of the expected pat"
P17-1192,S13-2025,0,0.168502,"Missing"
P17-1192,D11-1060,0,0.202381,"y on matching the compound to pre-defined patterns or semantic ontologies (Fares et al., 2015; ´ S´eaghdha and Copestake, 2007; Tratz and O Hovy, 2010; Surtani and Paul, 2015; Choi et al., 2015). Recently, interpretations may take the form of arbitrary natural language predicates (Hendrickx et al., 2013). Most approaches are supervised, comparing unseen noun compounds to the most similar phrase seen in training (Wijaya and Gianfortoni, 2011; Nulty and Costello, 2013; Van de Cruys et al., 2013). Other unsupervised approaches apply information extraction techniques to paraphrase noun compounds (Kim and Nakov, 2011; Xavier and Strube de Lima, 2014; Pa¸sca, 2015). They focus exclusively on providing good paraphrases for an input noun compound. To our knowledge, ours is the first attempt to use these interpretations for the downstream task of IsA relation extraction. IsA Relation Extraction. Most efforts to acquire taxonomic relations from text build on the seminal work of Hearst (1992), which observes that certain textual patterns–e.g., “X and other Y”–are high-precision indicators of whether X is a member of class Y. Recent work focuses on learning such patterns automatically from corpora (Snow et al.,"
P17-1192,P16-4011,0,0.0260884,"terpretations for the downstream task of IsA relation extraction. IsA Relation Extraction. Most efforts to acquire taxonomic relations from text build on the seminal work of Hearst (1992), which observes that certain textual patterns–e.g., “X and other Y”–are high-precision indicators of whether X is a member of class Y. Recent work focuses on learning such patterns automatically from corpora (Snow et al., 2006; Shwartz et al., 2016). These IsA extraction techniques provide a key step for the more general task of knowledge base population. The “universal schema” approach (Riedel et al., 2013; Kirschnick et al., 2016; Verga et al., 2017), which infers relations using matrix factorization, often includes Hearst patterns as input features. Graphical (Bansal et al., 2014) and joint inference models (Movshovitz-Attias and Cohen, 2015) typically require Hearst patterns to define an inventory of possible classes. A separate line of work avoids Hearst patterns by instead exploiting semi-structured data from HTML markup (Wang and Cohen, 2009; Dalvi et al., 2012; Pasupat and Liang, 2014). These approaches all share the limitation that, in practice, in order for a class to be populated with instances, the entire cl"
P17-1192,D10-1108,0,0.0270828,"in practice, in order for a class to be populated with instances, the entire class label has to have been observed verbatim in text. This requirement limits the ability to handle arbitrarily fine-grained classes. Our work addresses this limitation by modeling fine-grained class labels compositionally. Thus the proposed method can combine evidence from multiple sentences, and can perform IsA extraction without requiring any example instances of a given class.1 Taxonomy Construction. Previous work on the construction of a taxonomy of IsA relations (Flati et al., 2014; de Melo and Weikum, 2010; Kozareva and Hovy, 2010; Ponzetto and Strube, 2007; Ponzetto and Navigli, 2009) considers that task to be different than extracting a flat set of IsA relations from text in practice. Challenges specific to taxonomy construction include overall concept positioning and how to discover whether concepts are unrelated, subordinated or parallel to each other (Kozareva and Hovy, 2010); the need to refine and enrich the taxonomy (Flati et al., 2014); the difficulty in adding relevant IsA relations towards the top of the taxonomy (Ponzetto and Navigli, 2009); eliminating cycles and inconsistencies (Ponzetto and Navigli, 2009"
P17-1192,P09-1116,0,0.0231342,"learned by observing predicates that relate instances of class H to modifier M (I2 ). Results are similar when using the class label H directly (I1 ). We spell out inverted predicates (Section 4.2) so wildcards (*) may appear as subjects or objects. Class Label American company American composer American novel jazz album jazz composer jazz venue Thus, I1 is computed as below: I1 (M H) = {hhp, M i, w × sim(M, N )i |hH, p, N, wi ∈ D} (6) where sim(M, N ) is the cosine similarity between M and N . I2 is computed analogously. We compute sim using a vector space built from Web documents following Lin and Wu (2009); Pantel et al. (2009). We retain the 100 most similar phrases for each of ∼10M phrases, and consider all other similarities to be 0. 4.4 Analysis of Property Profiles Table 1 provides examples of good and bad property profiles for several M Hs. In general, frequent relations between M and H capture relevant properties of M H, but it is not always the case. To illustrate, the most frequently discussed relation between “child” and “actor” is that actors have children, but this property is not indicative of the meaning of “child actor”. Qualitatively, the top-ranked interpretations learned by us"
P17-1192,D12-1048,0,0.0288315,"ation between s and o. Both O and D are extracted from a sample of around 1 billion Web documents in English. The supplementary material gives additional details. We instantiate O with an IsA repository constructed by applying Hearst patterns to the Web documents. Instances are represented as automatically-disambiguated entity mentions4 which, when possible, are resolved to Wikipedia pages. Classes are represented as (non-disambiguated) natural language strings. We instantiate D with a large repository of facts extracted using in-house implementations of ReVerb (Fader et al., 2011) and OLLIE (Mausam et al., 2012). The predicates are extracted as natural language strings. Subjects and objects may be either disambiguated entity references or natural language strings. Every tuple is included in both the forward and the reverse direction. E.g. hjazz, perform at, venuei also appears as hvenue, ←perform at, jazzi, where ← is a spe4 “Entity mentions” may be individuals, like “Barack Obama”, but may also be concepts like “jazz”. cial character signifying inverted predicates. These inverted predicates simplify the following definitions. In total, O contains 1.1M tuples and D contains 30M tuples. 4.3 Building P"
P17-1192,P15-1140,0,0.0132569,"t certain textual patterns–e.g., “X and other Y”–are high-precision indicators of whether X is a member of class Y. Recent work focuses on learning such patterns automatically from corpora (Snow et al., 2006; Shwartz et al., 2016). These IsA extraction techniques provide a key step for the more general task of knowledge base population. The “universal schema” approach (Riedel et al., 2013; Kirschnick et al., 2016; Verga et al., 2017), which infers relations using matrix factorization, often includes Hearst patterns as input features. Graphical (Bansal et al., 2014) and joint inference models (Movshovitz-Attias and Cohen, 2015) typically require Hearst patterns to define an inventory of possible classes. A separate line of work avoids Hearst patterns by instead exploiting semi-structured data from HTML markup (Wang and Cohen, 2009; Dalvi et al., 2012; Pasupat and Liang, 2014). These approaches all share the limitation that, in practice, in order for a class to be populated with instances, the entire class label has to have been observed verbatim in text. This requirement limits the ability to handle arbitrarily fine-grained classes. Our work addresses this limitation by modeling fine-grained class labels composition"
P17-1192,W07-1108,0,0.0322769,"Missing"
P17-1192,N15-1037,1,0.902362,"Missing"
P17-1192,D09-1098,0,0.0389397,"ng predicates that relate instances of class H to modifier M (I2 ). Results are similar when using the class label H directly (I1 ). We spell out inverted predicates (Section 4.2) so wildcards (*) may appear as subjects or objects. Class Label American company American composer American novel jazz album jazz composer jazz venue Thus, I1 is computed as below: I1 (M H) = {hhp, M i, w × sim(M, N )i |hH, p, N, wi ∈ D} (6) where sim(M, N ) is the cosine similarity between M and N . I2 is computed analogously. We compute sim using a vector space built from Web documents following Lin and Wu (2009); Pantel et al. (2009). We retain the 100 most similar phrases for each of ∼10M phrases, and consider all other similarities to be 0. 4.4 Analysis of Property Profiles Table 1 provides examples of good and bad property profiles for several M Hs. In general, frequent relations between M and H capture relevant properties of M H, but it is not always the case. To illustrate, the most frequently discussed relation between “child” and “actor” is that actors have children, but this property is not indicative of the meaning of “child actor”. Qualitatively, the top-ranked interpretations learned by using the head noun dire"
P17-1192,P14-1037,0,0.0215137,"provide a key step for the more general task of knowledge base population. The “universal schema” approach (Riedel et al., 2013; Kirschnick et al., 2016; Verga et al., 2017), which infers relations using matrix factorization, often includes Hearst patterns as input features. Graphical (Bansal et al., 2014) and joint inference models (Movshovitz-Attias and Cohen, 2015) typically require Hearst patterns to define an inventory of possible classes. A separate line of work avoids Hearst patterns by instead exploiting semi-structured data from HTML markup (Wang and Cohen, 2009; Dalvi et al., 2012; Pasupat and Liang, 2014). These approaches all share the limitation that, in practice, in order for a class to be populated with instances, the entire class label has to have been observed verbatim in text. This requirement limits the ability to handle arbitrarily fine-grained classes. Our work addresses this limitation by modeling fine-grained class labels compositionally. Thus the proposed method can combine evidence from multiple sentences, and can perform IsA extraction without requiring any example instances of a given class.1 Taxonomy Construction. Previous work on the construction of a taxonomy of IsA relation"
P17-1192,N13-1008,0,0.032996,"tempt to use these interpretations for the downstream task of IsA relation extraction. IsA Relation Extraction. Most efforts to acquire taxonomic relations from text build on the seminal work of Hearst (1992), which observes that certain textual patterns–e.g., “X and other Y”–are high-precision indicators of whether X is a member of class Y. Recent work focuses on learning such patterns automatically from corpora (Snow et al., 2006; Shwartz et al., 2016). These IsA extraction techniques provide a key step for the more general task of knowledge base population. The “universal schema” approach (Riedel et al., 2013; Kirschnick et al., 2016; Verga et al., 2017), which infers relations using matrix factorization, often includes Hearst patterns as input features. Graphical (Bansal et al., 2014) and joint inference models (Movshovitz-Attias and Cohen, 2015) typically require Hearst patterns to define an inventory of possible classes. A separate line of work avoids Hearst patterns by instead exploiting semi-structured data from HTML markup (Wang and Cohen, 2009; Dalvi et al., 2012; Pasupat and Liang, 2014). These approaches all share the limitation that, in practice, in order for a class to be populated with"
P17-1192,P16-1226,0,0.137831,"treat class labels as single lexical units, the proposed method considers each of the individual modifiers in the class label relative to the head. An evaluation on the task of reconstructing Wikipedia category pages demonstrates a &gt;10 point increase in AUC, over a strong baseline relying on widely-used Hearst patterns. 1 Figure 1: We extract instances of fine-grained classes by considering each of the modifiers in the class label individually. This allows us to extract instances even when the full class label never appears in text. Introduction The majority of approaches (Snow et al., 2006; Shwartz et al., 2016) for extracting IsA relations from text rely on lexical patterns as the primary signal of whether an instance belongs to a class. For example, observing a pattern like “X such as Y” is a strong indication that Y (e.g., “Charles Mingus”) is an instance of class X (e.g., “musician”) (Hearst, 1992). Methods based on these “Hearst patterns” assume that class labels can be treated as atomic lexicalized units. This assumption has several significant weakness. First, in order to recognize an instance of a class, these patternbased methods require that the entire class label be observed verbatim in te"
P17-1192,P06-1101,0,0.169539,"art methods tend to treat class labels as single lexical units, the proposed method considers each of the individual modifiers in the class label relative to the head. An evaluation on the task of reconstructing Wikipedia category pages demonstrates a &gt;10 point increase in AUC, over a strong baseline relying on widely-used Hearst patterns. 1 Figure 1: We extract instances of fine-grained classes by considering each of the modifiers in the class label individually. This allows us to extract instances even when the full class label never appears in text. Introduction The majority of approaches (Snow et al., 2006; Shwartz et al., 2016) for extracting IsA relations from text rely on lexical patterns as the primary signal of whether an instance belongs to a class. For example, observing a pattern like “X such as Y” is a strong indication that Y (e.g., “Charles Mingus”) is an instance of class X (e.g., “musician”) (Hearst, 1992). Methods based on these “Hearst patterns” assume that class labels can be treated as atomic lexicalized units. This assumption has several significant weakness. First, in order to recognize an instance of a class, these patternbased methods require that the entire class label be"
P17-1192,R15-1082,0,0.0130565,"ationalization of a formal semantics framework to address two aspects of semantics that are often kept separate in NLP: assigning intrinsic “meaning” to a phrase, and reasoning about that phrase in a truth-theoretic context. 2 Related Work Noun Phrase Interpretation. Compound noun phrases (“jazz musician”) communicate implicit semantic relations between modifiers and the head. Many efforts to provide semantic interpretations of such phrases rely on matching the compound to pre-defined patterns or semantic ontologies (Fares et al., 2015; ´ S´eaghdha and Copestake, 2007; Tratz and O Hovy, 2010; Surtani and Paul, 2015; Choi et al., 2015). Recently, interpretations may take the form of arbitrary natural language predicates (Hendrickx et al., 2013). Most approaches are supervised, comparing unseen noun compounds to the most similar phrase seen in training (Wijaya and Gianfortoni, 2011; Nulty and Costello, 2013; Van de Cruys et al., 2013). Other unsupervised approaches apply information extraction techniques to paraphrase noun compounds (Kim and Nakov, 2011; Xavier and Strube de Lima, 2014; Pa¸sca, 2015). They focus exclusively on providing good paraphrases for an input noun compound. To our knowledge, ours i"
P17-1192,P10-1070,0,0.113842,"Missing"
P17-1192,S13-2026,0,0.402412,"Missing"
P17-1192,E17-1058,0,0.0241787,"nstream task of IsA relation extraction. IsA Relation Extraction. Most efforts to acquire taxonomic relations from text build on the seminal work of Hearst (1992), which observes that certain textual patterns–e.g., “X and other Y”–are high-precision indicators of whether X is a member of class Y. Recent work focuses on learning such patterns automatically from corpora (Snow et al., 2006; Shwartz et al., 2016). These IsA extraction techniques provide a key step for the more general task of knowledge base population. The “universal schema” approach (Riedel et al., 2013; Kirschnick et al., 2016; Verga et al., 2017), which infers relations using matrix factorization, often includes Hearst patterns as input features. Graphical (Bansal et al., 2014) and joint inference models (Movshovitz-Attias and Cohen, 2015) typically require Hearst patterns to define an inventory of possible classes. A separate line of work avoids Hearst patterns by instead exploiting semi-structured data from HTML markup (Wang and Cohen, 2009; Dalvi et al., 2012; Pasupat and Liang, 2014). These approaches all share the limitation that, in practice, in order for a class to be populated with instances, the entire class label has to have"
P17-1192,P09-1050,0,0.021542,"l., 2016). These IsA extraction techniques provide a key step for the more general task of knowledge base population. The “universal schema” approach (Riedel et al., 2013; Kirschnick et al., 2016; Verga et al., 2017), which infers relations using matrix factorization, often includes Hearst patterns as input features. Graphical (Bansal et al., 2014) and joint inference models (Movshovitz-Attias and Cohen, 2015) typically require Hearst patterns to define an inventory of possible classes. A separate line of work avoids Hearst patterns by instead exploiting semi-structured data from HTML markup (Wang and Cohen, 2009; Dalvi et al., 2012; Pasupat and Liang, 2014). These approaches all share the limitation that, in practice, in order for a class to be populated with instances, the entire class label has to have been observed verbatim in text. This requirement limits the ability to handle arbitrarily fine-grained classes. Our work addresses this limitation by modeling fine-grained class labels compositionally. Thus the proposed method can combine evidence from multiple sentences, and can perform IsA extraction without requiring any example instances of a given class.1 Taxonomy Construction. Previous work on"
P17-1192,xavier-lima-2014-boosting,0,0.309348,"Missing"
P17-1192,Q14-1006,0,0.0290113,"at modifiers are subsective, acknowledging the limitations (Kamp and Partee, 1995). 1950s musician have been observed. Second, the modifier is a function that can be applied in a truth-theoretic setting. That is, applying “1950s” to the set of “musicians” returns exactly the set of “1950s musicians”. Computational Approaches. While the notion of modifiers as functions has been incorporated into computational models previously, prior work focuses on either assigning an intrinsic meaning to M or on operationalizing M in a truth-theoretic sense, but not on doing both simultaneously. For example, Young et al. (2014) focuses exclusively on the subset selection aspect of modification. That is, given a set of instances H and a modifier M , their method could return the subset M H. However, their method does not model the meaning of the modifier itself, so that, e.g., if there were no red cars in their model of the world, the phrase “red cars” would have no meaning. In contrast, Baroni and Zamparelli (2010) models the meaning of modifiers explicitly as functions that map between vector-space representations of nouns. However, their model focuses on similarity between class labels–e.g., to say that “important"
P19-1334,D16-1203,0,0.0429074,"e for the model to learn to generalize to more challenging cases as a human performing the task would. This issue has been documented across domains in artificial intelligence. In computer vision, for example, neural networks trained to recognize objects are misled by contextual heuristics: a network that is able to recognize monkeys in a typical context with high accuracy may nevertheless label a monkey holding a guitar as a human, since in the training set guitars tend to co-occur with humans but not monkeys (Wang et al., 2018). Similar heuristics arise in visual question answering systems (Agrawal et al., 2016). The current paper addresses this issue in the domain of natural language inference (NLI), the task of determining whether a premise sentence entails (i.e., implies the truth of) a hypothesis sentence (Condoravdi et al., 2003; Dagan et al., 2006; Bowman et al., 2015). As in other domains, neural NLI models have been shown to learn shallow heuristics, in this case based on the presence of specific words (Naik et al., 2018; Sanchez et al., 2018). For example, a model might assign a label of contradiction to any input containing the word not, since not often appears in the examples of contradict"
P19-1334,D15-1075,0,0.753994,"contextual heuristics: a network that is able to recognize monkeys in a typical context with high accuracy may nevertheless label a monkey holding a guitar as a human, since in the training set guitars tend to co-occur with humans but not monkeys (Wang et al., 2018). Similar heuristics arise in visual question answering systems (Agrawal et al., 2016). The current paper addresses this issue in the domain of natural language inference (NLI), the task of determining whether a premise sentence entails (i.e., implies the truth of) a hypothesis sentence (Condoravdi et al., 2003; Dagan et al., 2006; Bowman et al., 2015). As in other domains, neural NLI models have been shown to learn shallow heuristics, in this case based on the presence of specific words (Naik et al., 2018; Sanchez et al., 2018). For example, a model might assign a label of contradiction to any input containing the word not, since not often appears in the examples of contradiction in standard NLI training sets. The focus of our work is on heuristics that are based on superficial syntactic properties. Consider the following sentence pair, which has the target label entailment: (1) Premise: The judge was paid by the actor. Hypothesis: The act"
P19-1334,P16-1139,0,0.124797,"euristics is that their input representations may make them susceptible to these heuristics. The lexical overlap heuristic disregards the order of the words in the sentence and considers only their identity, so it is likely to be adopted by bag-of-words NLI models (e.g., Parikh et al. 2016). The subsequence heuristic considers linearly adjacent chunks of words, so one might expect it to be adopted by standard RNNs, which process sentences in linear order. Finally, the constituent heuristic appeals to components of the parse tree, so one might expect to see it adopted by tree-based NLI models (Bowman et al., 2016). 3 Dataset Construction For each heuristic, we generated five templates for examples that support the heuristic and five templates for examples that contradict it. Below is one template for the subsequence heuristic; see Appendix B for a full list of templates. (4) The N1 P the N2 V. 9 The N2 V. The lawyer by the actor ran. 9 The actor ran. We generated 1,000 examples from each template, for a total of 10,000 examples per heuristic. Some heuristics are special cases of others, but we made sure that the examples for one heuristic did not also fall under a more narrowly defined heuristic. That"
P19-1334,N19-1423,0,0.254969,"Missing"
P19-1334,C18-1152,0,0.182781,"al overlap cases, the hypothesis was not a subsequence or constituent of the premise; for subsequence cases, the hypothesis was not a constituent of the premise. 3.1 Dataset Controls Plausibility: One advantage of generating examples from templates—instead of, e.g., modifying naturally-occurring examples—is that we can ensure the plausibility of all generated sentences. For example, we do not generate cases such as The student read the book 9 The book read the student, which could ostensibly be solved using a hypothesis-plausibility heuristic. To achieve this, we drew our core vocabulary from Ettinger et al. (2018), where every noun was a plausible subject of every verb or a plausible object of every transitive verb. Some templates required expanding this core vocabulary; in those cases, we manually curated the additions to ensure plausibility. 3430 Selectional criteria: Some of our example types depend on the availability of lexically-specific verb frames. For example, (5) requires awareness of the fact that believed can take a clause (the lawyer saw the officer) as its complement: (5) The doctor believed the lawyer saw the officer. 9 The doctor believed the lawyer. It is arguably unfair to expect a mo"
P19-1334,P17-1152,0,0.131556,"Missing"
P19-1334,N18-1108,1,0.895833,"Missing"
P19-1334,W18-5441,1,0.897302,"Missing"
P19-1334,S18-2023,0,0.205224,"Missing"
P19-1334,S10-1060,0,0.0364375,"er than chance accuracy on those datasets by only looking at the hypothesis. Other recent works address possible ways in which NLI models might use fallible heuristics, focusing on semantic phenomena, such as lexical inferences (Glockner et al., 2018) or quantifiers (Geiger et al., 2018), or biases based on specific words (Sanchez et al., 2018). Our work focuses instead on structural phenomena, following the proof-of-concept work done by Dasgupta et al. (2018). Our focus on using NLI to address how models capture structure follows some older work about using NLI for the evaluation of parsers (Rimell and Clark, 2010; Mehdad et al., 2010). NLI has been used to investigate many other types of linguistic information besides syntactic structure (Poliak et al., 2018a; White et al., 2017). Outside NLI, multiple projects have used classification tasks to understand what linguistic and/or structural information is present in vector encodings of sentences (e.g., Adi et al., 2017; Ettinger et al., 2018; Conneau et al., 2018). We instead choose the behavioral approach of using task performance on critical cases. Unlike the classification approach, this approach is agnostic to model structure; our dataset could be u"
P19-1334,N18-1067,0,0.0457247,"Missing"
P19-1334,N18-1179,0,0.197271,"e training set guitars tend to co-occur with humans but not monkeys (Wang et al., 2018). Similar heuristics arise in visual question answering systems (Agrawal et al., 2016). The current paper addresses this issue in the domain of natural language inference (NLI), the task of determining whether a premise sentence entails (i.e., implies the truth of) a hypothesis sentence (Condoravdi et al., 2003; Dagan et al., 2006; Bowman et al., 2015). As in other domains, neural NLI models have been shown to learn shallow heuristics, in this case based on the presence of specific words (Naik et al., 2018; Sanchez et al., 2018). For example, a model might assign a label of contradiction to any input containing the word not, since not often appears in the examples of contradiction in standard NLI training sets. The focus of our work is on heuristics that are based on superficial syntactic properties. Consider the following sentence pair, which has the target label entailment: (1) Premise: The judge was paid by the actor. Hypothesis: The actor paid the judge. An NLI system that labels this example correctly might do so not by reasoning about the meanings of these sentences, but rather by assuming that the premise enta"
P19-1334,W03-0906,0,\N,Missing
P19-1334,P03-1054,0,\N,Missing
P19-1334,N10-1146,0,\N,Missing
P19-1334,D16-1244,0,\N,Missing
P19-1334,Q16-1037,1,\N,Missing
P19-1334,D16-1240,1,\N,Missing
P19-1334,D17-1070,0,\N,Missing
P19-1334,I17-1100,0,\N,Missing
P19-1334,Q18-1019,0,\N,Missing
P19-1334,W18-2501,0,\N,Missing
P19-1334,P18-2103,0,\N,Missing
P19-1334,D18-1501,0,\N,Missing
P19-1334,D18-1151,1,\N,Missing
P19-1334,D18-1007,1,\N,Missing
P19-1334,P19-1449,0,\N,Missing
P19-1334,N18-1101,0,\N,Missing
P19-1439,E17-2026,0,0.0800118,"ng and fine-tuning tasks, rather than architectures or specific linguistic capabilities. Some of our experiments resemble those of Yogatama et al. (2019), who also empirically investigate transfer performance with limited amounts of data and find similar evidence of catastrophic forgetting. 4466 Multitask representation learning in NLP is well studied, and again can be traced back at least as far as Collobert et al. (2011). Luong et al. (2016) show promising results combining translation and parsing; Subramanian et al. (2018) benefit from multitask learning in sentence-to-vector encoding; and Bingel and Søgaard (2017) and Changpinyo et al. (2018) offer studies of when multitask learning is helpful for lower-level NLP tasks. 3 Task |Train| Task Type GLUE Tasks CoLA SST MRPC QQP STS MNLI QNLI RTE WNLI 8.5K 67K 3.7K 364K 7K 393K 105K 2.5K 634 acceptability sentiment paraphrase detection paraphrase detection sentence similarity NLI QA (NLI) NLI coreference resolution (NLI) Outside Tasks Transfer Paradigms DisSent WT LM WT LM BWB MT En-De MT En-Ru Reddit SkipThought We consider two recent paradigms for transfer learning: pretraining and intermediate training. See Figure 1 for a graphical depiction. Pretraining"
P19-1439,S17-2001,0,0.0228104,"line is especially strong because our ELMo-style models use a skip connection from the input of the encoder to the output, allowing the task-specific component to see the input representations, yielding a model similar to Iyyer et al. (2015). GLUE Tasks We use the nine tasks included with GLUE as pretraining and intermediate tasks: acceptability classification with CoLA (Warstadt et al., 2018); binary sentiment classification with SST (Socher et al., 2013); semantic similarity with the MSR Paraphrase Corpus (MRPC; Dolan and Brockett, 2005), Quora Question Pairs1 (QQP), and STS-Benchmark (STS; Cer et al., 2017); and textual entailment with the Multi-Genre NLI Corpus (MNLI Williams et al., 2018), RTE 1, 2, 3, and 5 (RTE; Dagan et al., 2006, et seq.), and data from SQuAD (QNLI;2 Rajpurkar et al., 2016) and the Winograd Schema Challenge (WNLI; Levesque et al., 2011) recast as entailment in the style of White et al. (2017). MNLI and QQP have previously been shown to be effective for pretraining in other settings (Conneau et al., 2017; Subramanian et al., 2018; Phang et al., 2018). Other tasks are included to represent a broad sample of labeling schemes commonly used in NLP. Outside Tasks We train langua"
P19-1439,C18-1251,0,0.0459627,"Missing"
P19-1439,D14-1082,0,0.0210975,"Missing"
P19-1439,D17-1070,0,0.20309,"bservations suggest that while scaling up LM pretraining (as in Radford et al., 2019) is likely the most straightforward path to further gains, our current methods for multitask and transfer learning may be substantially limiting our results. 2 Related Work Work on reusable sentence encoders can be traced back at least as far as the multitask model of Collobert et al. (2011). Several works focused on learning reusable sentence-to-vector encodings, where the pretrained encoder produces a fixed-size representation for each input sentence (Dai and Le, 2015; Kiros et al., 2015; Hill et al., 2016; Conneau et al., 2017). More recent reusable sentence encoders such as CoVe (McCann et al., 2017) and GPT (Radford et al., 2018) instead represent sentences as sequences of vectors. These methods work well, but most use distinct pretraining objectives, and none offers a substantial investigation of the choice of objective like we conduct here. We build on two methods for pretraining sentence encoders on language modeling: ELMo and BERT. ELMo consists of a forward and backward LSTM (Hochreiter and Schmidhuber, 1997), the hidden states of which are used to produce a contextual vector representation for each token in"
P19-1439,N19-1423,0,0.616052,"ond Language Modeling Alex Wang,∗1 Jan Hula,2 Patrick Xia,2 Raghavendra Pappagari,2 R. Thomas McCoy,2 , Roma Patel,3 Najoung Kim,2 Ian Tenney,4 Yinghui Huang,6 Katherin Yu,5 Shuning Jin,7 Berlin Chen8 Benjamin Van Durme,2 Edouard Grave,5 Ellie Pavlick,3,4 and Samuel R. Bowman1 1 New York University, 2 Johns Hopkins University, 3 Brown University, 4 Google AI Language, 5 Facebook, 6 IBM, 7 University of Minnesota Duluth, 8 Swarthmore College Abstract Natural language understanding has recently seen a surge of progress with the use of sentence encoders like ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2019) which are pretrained on variants of language modeling. We conduct the first large-scale systematic study of candidate pretraining tasks, comparing 19 different tasks both as alternatives and complements to language modeling. Our primary results support the use language modeling, especially when combined with pretraining on additional labeled-data tasks. However, our results are mixed across pretraining tasks and show some concerning trends: In ELMo’s pretrain-then-freeze paradigm, random baselines are worryingly strong and results vary strikingly across target tasks. In addition, fine-tuning"
P19-1439,I05-5002,0,0.0294281,"a baseline of a randomly initialized BiLSTM with no further training. This baseline is especially strong because our ELMo-style models use a skip connection from the input of the encoder to the output, allowing the task-specific component to see the input representations, yielding a model similar to Iyyer et al. (2015). GLUE Tasks We use the nine tasks included with GLUE as pretraining and intermediate tasks: acceptability classification with CoLA (Warstadt et al., 2018); binary sentiment classification with SST (Socher et al., 2013); semantic similarity with the MSR Paraphrase Corpus (MRPC; Dolan and Brockett, 2005), Quora Question Pairs1 (QQP), and STS-Benchmark (STS; Cer et al., 2017); and textual entailment with the Multi-Genre NLI Corpus (MNLI Williams et al., 2018), RTE 1, 2, 3, and 5 (RTE; Dagan et al., 2006, et seq.), and data from SQuAD (QNLI;2 Rajpurkar et al., 2016) and the Winograd Schema Challenge (WNLI; Levesque et al., 2011) recast as entailment in the style of White et al. (2017). MNLI and QQP have previously been shown to be effective for pretraining in other settings (Conneau et al., 2017; Subramanian et al., 2018; Phang et al., 2018). Other tasks are included to represent a broad sample"
P19-1439,N16-1162,0,0.0469202,"Missing"
P19-1439,J07-3004,0,0.0375747,"Missing"
P19-1439,P18-1031,0,0.0247575,"(left), and learn a target task model on top of the representations it produces (right). Middle (intermediate ELMo training): We train a BiLSTM on top of ELMo for an intermediate task (left). We then train a target task model on top of the intermediate task BiLSTM and ELMo (right). Bottom (intermediate BERT training): We fine-tune BERT on an intermediate task (left), and then fine-tune the resulting model again on a target task (right). limitation has prompted interest in pretraining for these encoders: The encoders are first trained on outside data, and then plugged into a target task model. Howard and Ruder (2018), Peters et al. (2018a), Radford et al. (2018), and Devlin et al. (2019) establish that encoders pretrained on variants of the language modeling task can be reused to yield strong performance on downstream NLP tasks. Subsequent work has homed in on language modeling (LM) pretraining, finding that such mod4465 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4465–4476 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics els can be productively fine-tuned on intermediate tasks like natural language inference bef"
P19-1439,P15-1162,0,0.0816432,"Missing"
P19-1439,N18-1038,0,0.0431129,"Missing"
P19-1439,P19-1441,0,0.176928,"sed to produce a contextual vector representation for each token in the inputted sequence. ELMo is adapted to target tasks by freezing the model weights and only learning a set of task-specific scalar weights that are used to compute a linear combination of the LSTM layers. BERT consists of a pretrained Transformer (Vaswani et al., 2017), and is adapted to downstream tasks by fine-tuning the entire model. Follow-up work has explored parameterefficient fine-tuning (Stickland and Murray, 2019; Houlsby et al., 2019) and better target task adaptation via multitask fine-tuning (Phang et al., 2018; Liu et al., 2019), but work in this area is nascent. The successes of sentence encoder pretraining have sparked a line of work analyzing these models (Zhang and Bowman, 2018; Peters et al., 2018b; Tenney et al., 2019b; Peters et al., 2019; Tenney et al., 2019a; Liu et al., 2019, i.a.). Our work also attempts to better understand what is learned by pretrained encoders, but we study this question entirely through the lens of pretraining and fine-tuning tasks, rather than architectures or specific linguistic capabilities. Some of our experiments resemble those of Yogatama et al. (2019), who also empirically inves"
P19-1439,J93-2004,0,0.0711626,"Missing"
P19-1439,P19-1442,0,0.0239473,"n WMT14 English-German (Bojar et al., 2014) and WMT17 English-Russian (Bojar et al., 2017). We train SkipThought-style sequence-to-sequence (seq2seq) models to read a 1 data.quora.com/First-Quora-DatasetRelease-Question-Pairs 2 QNLI has been re-released with updated splits since the original release. We use the original splits. sentence from WT and predict the following sentence (Kiros et al., 2015; Tang et al., 2017). We train DisSent models to read two clauses from WT that are connected by a discourse marker such as and, but, or so and predict the the discourse marker (Jernite et al., 2017; Nie et al., 2019). Finally, we train seq2seq models to predict the response to a given comment from Reddit, using a previously existing dataset obtained by a third party (available on pushshift.io), comprised of 18M comment– response pairs from 2008-2011. This dataset was used by Yang et al. (2018) to train sentence encoders. Multitask Learning We consider three sets of these tasks for multitask pretraining and intermediate training: all GLUE tasks, all non-GLUE (outside) tasks, and all tasks. 5 Models and Experimental Details We implement our models using the jiant toolkit,3 which is in turn built on AllenNLP"
P19-1439,N18-1202,0,0.787227,"? Sentence-Level Pretraining Beyond Language Modeling Alex Wang,∗1 Jan Hula,2 Patrick Xia,2 Raghavendra Pappagari,2 R. Thomas McCoy,2 , Roma Patel,3 Najoung Kim,2 Ian Tenney,4 Yinghui Huang,6 Katherin Yu,5 Shuning Jin,7 Berlin Chen8 Benjamin Van Durme,2 Edouard Grave,5 Ellie Pavlick,3,4 and Samuel R. Bowman1 1 New York University, 2 Johns Hopkins University, 3 Brown University, 4 Google AI Language, 5 Facebook, 6 IBM, 7 University of Minnesota Duluth, 8 Swarthmore College Abstract Natural language understanding has recently seen a surge of progress with the use of sentence encoders like ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2019) which are pretrained on variants of language modeling. We conduct the first large-scale systematic study of candidate pretraining tasks, comparing 19 different tasks both as alternatives and complements to language modeling. Our primary results support the use language modeling, especially when combined with pretraining on additional labeled-data tasks. However, our results are mixed across pretraining tasks and show some concerning trends: In ELMo’s pretrain-then-freeze paradigm, random baselines are worryingly strong and results vary strikingly across target"
P19-1439,W19-4302,0,0.0321696,"sed to compute a linear combination of the LSTM layers. BERT consists of a pretrained Transformer (Vaswani et al., 2017), and is adapted to downstream tasks by fine-tuning the entire model. Follow-up work has explored parameterefficient fine-tuning (Stickland and Murray, 2019; Houlsby et al., 2019) and better target task adaptation via multitask fine-tuning (Phang et al., 2018; Liu et al., 2019), but work in this area is nascent. The successes of sentence encoder pretraining have sparked a line of work analyzing these models (Zhang and Bowman, 2018; Peters et al., 2018b; Tenney et al., 2019b; Peters et al., 2019; Tenney et al., 2019a; Liu et al., 2019, i.a.). Our work also attempts to better understand what is learned by pretrained encoders, but we study this question entirely through the lens of pretraining and fine-tuning tasks, rather than architectures or specific linguistic capabilities. Some of our experiments resemble those of Yogatama et al. (2019), who also empirically investigate transfer performance with limited amounts of data and find similar evidence of catastrophic forgetting. 4466 Multitask representation learning in NLP is well studied, and again can be traced back at least as far as"
P19-1439,D18-1179,0,0.247572,"? Sentence-Level Pretraining Beyond Language Modeling Alex Wang,∗1 Jan Hula,2 Patrick Xia,2 Raghavendra Pappagari,2 R. Thomas McCoy,2 , Roma Patel,3 Najoung Kim,2 Ian Tenney,4 Yinghui Huang,6 Katherin Yu,5 Shuning Jin,7 Berlin Chen8 Benjamin Van Durme,2 Edouard Grave,5 Ellie Pavlick,3,4 and Samuel R. Bowman1 1 New York University, 2 Johns Hopkins University, 3 Brown University, 4 Google AI Language, 5 Facebook, 6 IBM, 7 University of Minnesota Duluth, 8 Swarthmore College Abstract Natural language understanding has recently seen a surge of progress with the use of sentence encoders like ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2019) which are pretrained on variants of language modeling. We conduct the first large-scale systematic study of candidate pretraining tasks, comparing 19 different tasks both as alternatives and complements to language modeling. Our primary results support the use language modeling, especially when combined with pretraining on additional labeled-data tasks. However, our results are mixed across pretraining tasks and show some concerning trends: In ELMo’s pretrain-then-freeze paradigm, random baselines are worryingly strong and results vary strikingly across target"
P19-1439,P18-2124,0,0.0384363,"Task Model BiLSTM Pretrained BiLSTM Input Text Input Text Intermediate Task Output Target Task Output Intermediate Task Model Target Task Model BiLSTM Intermediate Task-Trained BiLSTM ELMo Introduction State-of-the-art models in natural language processing (NLP) often incorporate encoder functions which generate a sequence of vectors intended to represent the in-context meaning of each word in an input text. These encoders have typically been trained directly on the target task at hand, which can be effective for data-rich tasks and yields human performance on some narrowlydefined benchmarks (Rajpurkar et al., 2018; Hassan et al., 2018), but is tenable only for the few tasks with millions of training data examples. This ∗ This paper supercedes “Looking for ELMo’s Friends: Sentence-Level Pretraining Beyond Language Modeling”, an earlier version of this work by the same authors. Correspondence to: alexwang@nyu.edu ❄ ❄ ❄ ELMo ❄ Input Text Input Text Intermediate Task Output Target Task Output Intermediate Task Model Target Task Model BERT Intermediate Task-Trained BERT Input Text Input Text Figure 1: Learning settings that we consider. Model components with frozen parameters are shown in gray and decorated"
P19-1439,P16-2022,0,0.0140103,"sion classifier. For seq2seq tasks (MT, SkipThought, pushshift.io Reddit dataset) we replace the classifier with a single-layer LSTM word-level decoder and initialize the hidden state with the [CLS] representation. For ELMo-style models, we use several model types: • Single-sentence classification tasks: We train a linear projection over the output states of the encoder, max-pool those projected states, and feed the result to an MLP. 4469 • Sentence-pair tasks: We perform the same steps on both sentences and use the heuristic feature vector [h1 ; h2 ; h1 · h2 ; h1 − h2 ] in the MLP, following Mou et al. (2016). When training target-task models on QQP, STS, MNLI, and QNLI, we use a cross-sentence attention mechanism similar to BiDAF (Seo et al., 2017). We do not use this mechanism in other cases as early results indicated it hurt transfer performance. • Seq2seq tasks (MT, SkipThought, pushshift.io Reddit dataset): We use a single-layer LSTM decoder where the hidden state is initialized with the pooled input representation. • Language modeling: We follow ELMo by concatenating forward and backward models and learning layer mixing weights. To use GLUE tasks for pretraining or intermediate training in a"
P19-1439,D13-1170,0,0.00874927,", 2019b). Accordingly, our pretraining and intermediate ELMo experiments include a baseline of a randomly initialized BiLSTM with no further training. This baseline is especially strong because our ELMo-style models use a skip connection from the input of the encoder to the output, allowing the task-specific component to see the input representations, yielding a model similar to Iyyer et al. (2015). GLUE Tasks We use the nine tasks included with GLUE as pretraining and intermediate tasks: acceptability classification with CoLA (Warstadt et al., 2018); binary sentiment classification with SST (Socher et al., 2013); semantic similarity with the MSR Paraphrase Corpus (MRPC; Dolan and Brockett, 2005), Quora Question Pairs1 (QQP), and STS-Benchmark (STS; Cer et al., 2017); and textual entailment with the Multi-Genre NLI Corpus (MNLI Williams et al., 2018), RTE 1, 2, 3, and 5 (RTE; Dagan et al., 2006, et seq.), and data from SQuAD (QNLI;2 Rajpurkar et al., 2016) and the Winograd Schema Challenge (WNLI; Levesque et al., 2011) recast as entailment in the style of White et al. (2017). MNLI and QQP have previously been shown to be effective for pretraining in other settings (Conneau et al., 2017; Subramanian et"
P19-1439,W17-2625,0,0.0590097,"Missing"
P19-1439,P19-1452,1,0.863686,"lar weights that are used to compute a linear combination of the LSTM layers. BERT consists of a pretrained Transformer (Vaswani et al., 2017), and is adapted to downstream tasks by fine-tuning the entire model. Follow-up work has explored parameterefficient fine-tuning (Stickland and Murray, 2019; Houlsby et al., 2019) and better target task adaptation via multitask fine-tuning (Phang et al., 2018; Liu et al., 2019), but work in this area is nascent. The successes of sentence encoder pretraining have sparked a line of work analyzing these models (Zhang and Bowman, 2018; Peters et al., 2018b; Tenney et al., 2019b; Peters et al., 2019; Tenney et al., 2019a; Liu et al., 2019, i.a.). Our work also attempts to better understand what is learned by pretrained encoders, but we study this question entirely through the lens of pretraining and fine-tuning tasks, rather than architectures or specific linguistic capabilities. Some of our experiments resemble those of Yogatama et al. (2019), who also empirically investigate transfer performance with limited amounts of data and find similar evidence of catastrophic forgetting. 4466 Multitask representation learning in NLP is well studied, and again can be traced b"
P19-1439,I17-1100,1,0.867491,"Missing"
P19-1439,N18-1101,1,0.826868,"om the input of the encoder to the output, allowing the task-specific component to see the input representations, yielding a model similar to Iyyer et al. (2015). GLUE Tasks We use the nine tasks included with GLUE as pretraining and intermediate tasks: acceptability classification with CoLA (Warstadt et al., 2018); binary sentiment classification with SST (Socher et al., 2013); semantic similarity with the MSR Paraphrase Corpus (MRPC; Dolan and Brockett, 2005), Quora Question Pairs1 (QQP), and STS-Benchmark (STS; Cer et al., 2017); and textual entailment with the Multi-Genre NLI Corpus (MNLI Williams et al., 2018), RTE 1, 2, 3, and 5 (RTE; Dagan et al., 2006, et seq.), and data from SQuAD (QNLI;2 Rajpurkar et al., 2016) and the Winograd Schema Challenge (WNLI; Levesque et al., 2011) recast as entailment in the style of White et al. (2017). MNLI and QQP have previously been shown to be effective for pretraining in other settings (Conneau et al., 2017; Subramanian et al., 2018; Phang et al., 2018). Other tasks are included to represent a broad sample of labeling schemes commonly used in NLP. Outside Tasks We train language models on two datasets: WikiText-103 (WT; Merity et al., 2017) and Billion Word La"
P19-1439,1983.tc-1.13,0,0.144827,"Missing"
P19-1439,W18-3022,0,0.0192405,"he original release. We use the original splits. sentence from WT and predict the following sentence (Kiros et al., 2015; Tang et al., 2017). We train DisSent models to read two clauses from WT that are connected by a discourse marker such as and, but, or so and predict the the discourse marker (Jernite et al., 2017; Nie et al., 2019). Finally, we train seq2seq models to predict the response to a given comment from Reddit, using a previously existing dataset obtained by a third party (available on pushshift.io), comprised of 18M comment– response pairs from 2008-2011. This dataset was used by Yang et al. (2018) to train sentence encoders. Multitask Learning We consider three sets of these tasks for multitask pretraining and intermediate training: all GLUE tasks, all non-GLUE (outside) tasks, and all tasks. 5 Models and Experimental Details We implement our models using the jiant toolkit,3 which is in turn built on AllenNLP (Gardner et al., 2017) and on a public PyTorch implementation of BERT.4 Appendix A presents additional details. Encoder Architecture For both the pretraining and intermediate ELMo experiments, we process words using a pretrained character-level convolutional neural network (CNN) f"
P19-1439,W18-5448,1,0.914222,"s and only learning a set of task-specific scalar weights that are used to compute a linear combination of the LSTM layers. BERT consists of a pretrained Transformer (Vaswani et al., 2017), and is adapted to downstream tasks by fine-tuning the entire model. Follow-up work has explored parameterefficient fine-tuning (Stickland and Murray, 2019; Houlsby et al., 2019) and better target task adaptation via multitask fine-tuning (Phang et al., 2018; Liu et al., 2019), but work in this area is nascent. The successes of sentence encoder pretraining have sparked a line of work analyzing these models (Zhang and Bowman, 2018; Peters et al., 2018b; Tenney et al., 2019b; Peters et al., 2019; Tenney et al., 2019a; Liu et al., 2019, i.a.). Our work also attempts to better understand what is learned by pretrained encoders, but we study this question entirely through the lens of pretraining and fine-tuning tasks, rather than architectures or specific linguistic capabilities. Some of our experiments resemble those of Yogatama et al. (2019), who also empirically investigate transfer performance with limited amounts of data and find similar evidence of catastrophic forgetting. 4466 Multitask representation learning in NLP"
P19-1439,Q17-1027,1,0.847544,"Missing"
P19-1452,P19-1441,0,0.0293115,": part-of-speech (POS), constituents (Consts.), dependencies (Deps.), entities, semantic role labeling (SRL), coreference (Coref.), semantic proto-roles (SPR; Reisinger et al., 2015), and relation classification (SemEval). These tasks are derived from standard benchmark datasets, and evaluated with a common metric–micro-averaged F1–to facilitate comparison across tasks. 2 BERT. The BERT model (Devlin et al., 2019) has shown state-of-the-art performance on many tasks, and its deep Transformer architecture (Vaswani et al., 2017) is typical of many recent models (e.g. Radford et al., 2018, 2019; Liu et al., 2019). We focus on the stock BERT models (base and large, uncased), which are trained with a multi-task objective (masked language modeling and next-sentence prediction) over a 3.3B word English corpus. Since we want to understand how the network represents language as a result of pretraining, we follow Tenney et al. (2019) (departing from standard BERT usage) and freeze the encoder weights. This prevents the encoder from rearranging its internal representations to better suit the probing task. Given input tokens T = [t0 , t1 , . . . , tn ], a deep encoder produces a set of layer activations H (0)"
P19-1452,P14-5010,0,0.00350132,"sponsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lowerlevel decisions on the basis of disambiguating information from higher-level representations. 1 Introduction Pre-trained sentence encoders such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2019) have rapidly improved the state of the art on many NLP tasks, and seem poised to displace both static word embeddings (Mikolov et al., 2013) and discrete pipelines (Manning et al., 2014) as the basis for natural language processing systems. While this has been a boon for performance, it has come at the cost of interpretability, and it remains unclear whether such models are in fact learning the kind of abstractions that we intuitively believe are important for representing natural language, or are simply modeling complex co-occurrence statistics. A wave of recent work has begun to “probe” state-of-the-art models to understand whether they are representing language in a satisfying way. Much of this work is behavior-based, designing controlled test sets and analyzing errors in"
P19-1452,D18-1151,0,0.0688029,"t of interpretability, and it remains unclear whether such models are in fact learning the kind of abstractions that we intuitively believe are important for representing natural language, or are simply modeling complex co-occurrence statistics. A wave of recent work has begun to “probe” state-of-the-art models to understand whether they are representing language in a satisfying way. Much of this work is behavior-based, designing controlled test sets and analyzing errors in order to reverse-engineer the types of abstractions the model may or may not be representing (e.g. Conneau et al., 2018; Marvin and Linzen, 2018; Poliak et al., 2018). Parallel efforts inspect the structure of the network directly, to assess whether there exist localizable regions associated with distinct types of linguistic decisions. Such work has produced evidence that deep language models can encode a range of syntactic and semantic information (e.g. Shi et al., 2016; Belinkov, 2018; Tenney et al., 2019), and that more complex structures are represented hierarchically in the higher layers of the model (Peters et al., 2018b; Blevins et al., 2018). We build on this latter line of work, focusing on the BERT model (Devlin et al., 2019"
P19-1452,N18-1202,0,0.640756,"aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lowerlevel decisions on the basis of disambiguating information from higher-level representations. 1 Introduction Pre-trained sentence encoders such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2019) have rapidly improved the state of the art on many NLP tasks, and seem poised to displace both static word embeddings (Mikolov et al., 2013) and discrete pipelines (Manning et al., 2014) as the basis for natural language processing systems. While this has been a boon for performance, it has come at the cost of interpretability, and it remains unclear whether such models are in fact learning the kind of abstractions that we intuitively believe are important for representing natural language, or are simply modeling complex co-occurrence statistics. A wave of rec"
P19-1452,D18-1179,0,0.306082,"aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lowerlevel decisions on the basis of disambiguating information from higher-level representations. 1 Introduction Pre-trained sentence encoders such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2019) have rapidly improved the state of the art on many NLP tasks, and seem poised to displace both static word embeddings (Mikolov et al., 2013) and discrete pipelines (Manning et al., 2014) as the basis for natural language processing systems. While this has been a boon for performance, it has come at the cost of interpretability, and it remains unclear whether such models are in fact learning the kind of abstractions that we intuitively believe are important for representing natural language, or are simply modeling complex co-occurrence statistics. A wave of rec"
P19-1452,W18-5441,1,0.742275,"Missing"
P19-1452,Q15-1034,0,0.0798062,"Missing"
P19-1452,D16-1159,0,0.0182544,"y are representing language in a satisfying way. Much of this work is behavior-based, designing controlled test sets and analyzing errors in order to reverse-engineer the types of abstractions the model may or may not be representing (e.g. Conneau et al., 2018; Marvin and Linzen, 2018; Poliak et al., 2018). Parallel efforts inspect the structure of the network directly, to assess whether there exist localizable regions associated with distinct types of linguistic decisions. Such work has produced evidence that deep language models can encode a range of syntactic and semantic information (e.g. Shi et al., 2016; Belinkov, 2018; Tenney et al., 2019), and that more complex structures are represented hierarchically in the higher layers of the model (Peters et al., 2018b; Blevins et al., 2018). We build on this latter line of work, focusing on the BERT model (Devlin et al., 2019), and use a suite of probing tasks (Tenney et al., 2019) derived from the traditional NLP pipeline to quantify where specific types of linguistic information are encoded. Building on observations (Peters et al., 2018b) that lower layers of a language model encode more local syntax while higher layers capture more complex semanti"
P19-1452,silveira-etal-2014-gold,0,0.0635284,"Missing"
P19-1452,D17-3004,0,0.0648136,"Missing"
Q14-1007,W10-0710,0,0.0986571,"cedonian Bulgarian Catalan Punjabi 253 131 85 72 61 48 41 37 31 25 22 21 Malayalam Telugu Portuguese German Urdu Russian Gujarati Turkish Cebuano Swahili Thai Others 219 87 82 66 56 44 39 35 29 23 22 ≤ 20 Table 1: Self-reported native language of 3,216 bilingual Turkers. Not shown are 49 languages with ≤20 speakers. We omit 1,801 Turkers who did not report their native language, 243 who reported 2 native languages, and 83 with ≥3 native languages. Several researchers have examined cost optimization using active learning techniques to select the most useful sentences or fragments to translate (Ambati and Vogel, 2010; Bloodgood and CallisonBurch, 2010; Ambati, 2012). To contrast our research with previous work, the main contributions of this paper are: (1) a robust methodology for assessing the bilingual skills of anonymous workers, (2) the largest-scale census to date of language skills of workers on MTurk, and (3) a detailed analysis of the data gathered in our study. 3 Experimental Design The central task in this study was to investigate Mechanical Turk’s bilingual population. We accomplished this through self-reported surveys combined with a HIT to translate individual words for 100 languages. We eval"
Q14-1007,ambati-etal-2010-active,0,0.134405,"ual texts. They examined translations of 100 words in 42 low-resource languages, and reported geolocated countries for their workers (India, the US, Romania, Pakistan, Macedonia, Latvia, Bangladesh and the Philippines). Irvine and Klementiev discussed the difficulty of quality control and assessing the plausibility of workers’ language skills for rare languages, which we address in this paper. Several researchers have investigated using MTurk to build bilingual parallel corpora for machine translation, a task which stands to benefit low cost, high volume translation on demand (Germann, 2001). Ambati et al. (2010) conducted a pilot study by posting 25 sentences to MTurk for Spanish, Chinese, Hindi, Telugu, Urdu, and Haitian Creole. In a study of 2000 Urdu sentences, Zaidan and Callison-Burch (2011) presented methods for achieving professional-level translation quality from Turkers by soliciting multiple English translations of each foreign sentence. Zbib et al. (2012) used crowdsourcing to construct a 1.5 million word parallel corpus of dialect Arabic and English, training a statistical machine translation system that produced higher quality translations of dialect Arabic than a system a trained on 100"
Q14-1007,P10-1088,1,0.849791,"Missing"
Q14-1007,W10-0701,1,0.73723,"Missing"
Q14-1007,W01-1409,0,0.50491,"ed from monolingual texts. They examined translations of 100 words in 42 low-resource languages, and reported geolocated countries for their workers (India, the US, Romania, Pakistan, Macedonia, Latvia, Bangladesh and the Philippines). Irvine and Klementiev discussed the difficulty of quality control and assessing the plausibility of workers’ language skills for rare languages, which we address in this paper. Several researchers have investigated using MTurk to build bilingual parallel corpora for machine translation, a task which stands to benefit low cost, high volume translation on demand (Germann, 2001). Ambati et al. (2010) conducted a pilot study by posting 25 sentences to MTurk for Spanish, Chinese, Hindi, Telugu, Urdu, and Haitian Creole. In a study of 2000 Urdu sentences, Zaidan and Callison-Burch (2011) presented methods for achieving professional-level translation quality from Turkers by soliciting multiple English translations of each foreign sentence. Zbib et al. (2012) used crowdsourcing to construct a 1.5 million word parallel corpus of dialect Arabic and English, training a statistical machine translation system that produced higher quality translations of dialect Arabic than a s"
Q14-1007,W11-2148,0,0.0270578,"ting multiple English translations of each foreign sentence. Zbib et al. (2012) used crowdsourcing to construct a 1.5 million word parallel corpus of dialect Arabic and English, training a statistical machine translation system that produced higher quality translations of dialect Arabic than a system a trained on 100 times more Modern Standard Arabic-English parallel data. Zbib et al. (2013) conducted a systematic study that showed that training an MT system on crowdsourced translations resulted in the same performance as training on professional translations, at 15 the cost. Hu et al. (2010; Hu et al. (2011) performed crowdsourced translation by having monolingual speakers collaborate and iteratively improve MT output. 81 English Hindi Chinese Arabic French Tagalog Italian Hebrew Vietnamese Swedish Hungarian Lithuanian 689 149 86 74 63 54 43 38 34 26 23 21 Tamil Spanish Romanian Kannada Polish Marathi Bengali Dutch Macedonian Bulgarian Catalan Punjabi 253 131 85 72 61 48 41 37 31 25 22 21 Malayalam Telugu Portuguese German Urdu Russian Gujarati Turkish Cebuano Swahili Thai Others 219 87 82 66 56 44 39 35 29 23 22 ≤ 20 Table 1: Self-reported native language of 3,216 bilingual Turkers. Not shown ar"
Q14-1007,W10-0717,1,0.775416,"ographic study by Ipeirotis (2010b) focused on age, gender, martial status, income levels, motivation for working on MTurk, and whether workers used it as a primary or supplemental form of income. The study contrasted Indian and US workers. Ross et al. (2010) completed a longitudinal follow-on study. A number of other studies have informally investigated Turkers’ language abilities. Munro and Tily (2011) compiled survey responses of 2,000 Turkers, revealing that four of the six most represented languages come from India (the top six being Hindi, Malayalam, Tamil, Spanish, French, and Telugu). Irvine and Klementiev (2010) had Turkers evaluate the accuracy of translations that had been automatically inducted from monolingual texts. They examined translations of 100 words in 42 low-resource languages, and reported geolocated countries for their workers (India, the US, Romania, Pakistan, Macedonia, Latvia, Bangladesh and the Philippines). Irvine and Klementiev discussed the difficulty of quality control and assessing the plausibility of workers’ language skills for rare languages, which we address in this paper. Several researchers have investigated using MTurk to build bilingual parallel corpora for machine tran"
Q14-1007,W10-0729,0,0.0260829,"Little et it first offered payment only in Amazon credits, and al., 2009; Quinn and Bederson, 2011). It has appli- later offered direct payment in US dollars. More recation to research areas like human-computer inter- cently, it has expanded to include one foreign curaction (Bigham et al., 2010; Bernstein et al., 2010), rency, the Indian rupee. Despite its payments becomputer vision (Sorokin and Forsyth, 2008; Deng ing limited to two currencies or Amazon credits, et al., 2010; Rashtchian et al., 2010), speech pro- MTurk claims over half a million workers from 190 cessing (Marge et al., 2010; Lane et al., 2010; Parent countries (Amazon, 2013). This suggests that its file:///Users/ellie/Documents/Research/turker-demographics/code/src/20130905/paper-rewrite/turkermap.html and Eskenazi, 2011; Eskenazi et al., 2013), and natu- worker population should represent a diverse set of ral language processing (Snow et al., 2008; Callison- languages. 80 1/1 A demographic study by Ipeirotis (2010b) focused on age, gender, martial status, income levels, motivation for working on MTurk, and whether workers used it as a primary or supplemental form of income. The study contrasted Indian and US workers. Ross et al."
Q14-1007,D11-1143,0,0.0205911,"Missing"
Q14-1007,W10-0716,0,0.012553,"call (von Ahn, 2005; Little et it first offered payment only in Amazon credits, and al., 2009; Quinn and Bederson, 2011). It has appli- later offered direct payment in US dollars. More recation to research areas like human-computer inter- cently, it has expanded to include one foreign curaction (Bigham et al., 2010; Bernstein et al., 2010), rency, the Indian rupee. Despite its payments becomputer vision (Sorokin and Forsyth, 2008; Deng ing limited to two currencies or Amazon credits, et al., 2010; Rashtchian et al., 2010), speech pro- MTurk claims over half a million workers from 190 cessing (Marge et al., 2010; Lane et al., 2010; Parent countries (Amazon, 2013). This suggests that its file:///Users/ellie/Documents/Research/turker-demographics/code/src/20130905/paper-rewrite/turkermap.html and Eskenazi, 2011; Eskenazi et al., 2013), and natu- worker population should represent a diverse set of ral language processing (Snow et al., 2008; Callison- languages. 80 1/1 A demographic study by Ipeirotis (2010b) focused on age, gender, martial status, income levels, motivation for working on MTurk, and whether workers used it as a primary or supplemental form of income. The study contrasted Indian and US wo"
Q14-1007,N10-1024,1,0.220696,"Missing"
Q14-1007,W12-3152,1,0.790157,"Some languages follow the pattern of having a smattering of assignments completed early, with the rate picking up later. Figure 6 gives the throughput of the full-sentence translation task for the six Indian languages. The fastest language was Malayalam, for which we collected half a million words of translations in just under a week. Table 4 gives the size of the data set that we created for each of these languages. Training SMT systems We trained statistical translation models from the parallel corpora that we created for the six Indian languages using the Joshua machine translation system (Post et al., 2012). Table 5 shows the translation performance when trained on the bitexts alone, and when incorporating the bilingual dictionaries created in our earlier HIT. The scores reflect the performance when tested on held out sentences from the training data. Adding the dic87 trained on bitexts alone 12.03 16.19 6.65 8.08 11.94 19.22 bitext + dictionaries 17.29 18.10 9.72 9.66 13.70 21.98 BLEU ∆ 5.26 1.91 3.07 1.58 1.76 2.76 Table 5: BLEU scores for translating into English using bilingual parallel corpora by themselves, and with the addition of single-word dictionaries. Scores are calculated using four"
Q14-1007,W10-0721,0,0.0153205,"tation, where people can Ernkvist, 2011). When Amazon introduced MTurk, be treated as a function call (von Ahn, 2005; Little et it first offered payment only in Amazon credits, and al., 2009; Quinn and Bederson, 2011). It has appli- later offered direct payment in US dollars. More recation to research areas like human-computer inter- cently, it has expanded to include one foreign curaction (Bigham et al., 2010; Bernstein et al., 2010), rency, the Indian rupee. Despite its payments becomputer vision (Sorokin and Forsyth, 2008; Deng ing limited to two currencies or Amazon credits, et al., 2010; Rashtchian et al., 2010), speech pro- MTurk claims over half a million workers from 190 cessing (Marge et al., 2010; Lane et al., 2010; Parent countries (Amazon, 2013). This suggests that its file:///Users/ellie/Documents/Research/turker-demographics/code/src/20130905/paper-rewrite/turkermap.html and Eskenazi, 2011; Eskenazi et al., 2013), and natu- worker population should represent a diverse set of ral language processing (Snow et al., 2008; Callison- languages. 80 1/1 A demographic study by Ipeirotis (2010b) focused on age, gender, martial status, income levels, motivation for working on MTurk, and whether workers"
Q14-1007,D08-1027,0,0.151761,"Missing"
Q14-1007,P11-1122,1,0.467909,"a, Latvia, Bangladesh and the Philippines). Irvine and Klementiev discussed the difficulty of quality control and assessing the plausibility of workers’ language skills for rare languages, which we address in this paper. Several researchers have investigated using MTurk to build bilingual parallel corpora for machine translation, a task which stands to benefit low cost, high volume translation on demand (Germann, 2001). Ambati et al. (2010) conducted a pilot study by posting 25 sentences to MTurk for Spanish, Chinese, Hindi, Telugu, Urdu, and Haitian Creole. In a study of 2000 Urdu sentences, Zaidan and Callison-Burch (2011) presented methods for achieving professional-level translation quality from Turkers by soliciting multiple English translations of each foreign sentence. Zbib et al. (2012) used crowdsourcing to construct a 1.5 million word parallel corpus of dialect Arabic and English, training a statistical machine translation system that produced higher quality translations of dialect Arabic than a system a trained on 100 times more Modern Standard Arabic-English parallel data. Zbib et al. (2013) conducted a systematic study that showed that training an MT system on crowdsourced translations resulted in th"
Q14-1007,N12-1006,1,0.854548,", which we address in this paper. Several researchers have investigated using MTurk to build bilingual parallel corpora for machine translation, a task which stands to benefit low cost, high volume translation on demand (Germann, 2001). Ambati et al. (2010) conducted a pilot study by posting 25 sentences to MTurk for Spanish, Chinese, Hindi, Telugu, Urdu, and Haitian Creole. In a study of 2000 Urdu sentences, Zaidan and Callison-Burch (2011) presented methods for achieving professional-level translation quality from Turkers by soliciting multiple English translations of each foreign sentence. Zbib et al. (2012) used crowdsourcing to construct a 1.5 million word parallel corpus of dialect Arabic and English, training a statistical machine translation system that produced higher quality translations of dialect Arabic than a system a trained on 100 times more Modern Standard Arabic-English parallel data. Zbib et al. (2013) conducted a systematic study that showed that training an MT system on crowdsourced translations resulted in the same performance as training on professional translations, at 15 the cost. Hu et al. (2010; Hu et al. (2011) performed crowdsourced translation by having monolingual speak"
Q14-1007,N13-1069,0,0.0609685,"for Spanish, Chinese, Hindi, Telugu, Urdu, and Haitian Creole. In a study of 2000 Urdu sentences, Zaidan and Callison-Burch (2011) presented methods for achieving professional-level translation quality from Turkers by soliciting multiple English translations of each foreign sentence. Zbib et al. (2012) used crowdsourcing to construct a 1.5 million word parallel corpus of dialect Arabic and English, training a statistical machine translation system that produced higher quality translations of dialect Arabic than a system a trained on 100 times more Modern Standard Arabic-English parallel data. Zbib et al. (2013) conducted a systematic study that showed that training an MT system on crowdsourced translations resulted in the same performance as training on professional translations, at 15 the cost. Hu et al. (2010; Hu et al. (2011) performed crowdsourced translation by having monolingual speakers collaborate and iteratively improve MT output. 81 English Hindi Chinese Arabic French Tagalog Italian Hebrew Vietnamese Swedish Hungarian Lithuanian 689 149 86 74 63 54 43 38 34 26 23 21 Tamil Spanish Romanian Kannada Polish Marathi Bengali Dutch Macedonian Bulgarian Catalan Punjabi 253 131 85 72 61 48 41 37 3"
Q14-1007,W10-0707,0,\N,Missing
Q16-1005,W11-2826,0,0.236687,"1987; Lahiri et al., 2011). Other recent work adopts a less abstract definition which is similar to the notion of “noisy text”– e.g. use of slang and poor grammar (Mosquera and Moreda, 2012a; Peterson et al., 2011). As a result, many rules have been explored for recognizing and generating informal language. Some of these rules are abstract, such as the level of implicature (Heylighen and Dewaele, 1999; Lahiri, 2015) or the degree of subjectivity (Mosquera and Moreda, 2012a), while others are much more concrete, such as the number of adjectives (Fang and Cao, 2009) or use of contractions (Abu Sheikha and Inkpen, 2011). Much prior work on detecting formality has focused on the lexical level (Brooke et al., 2010; Brooke and Hirst, 2014; Pavlick and Nenkova, 2015). For larger units of text, perhaps the best-known method for measuring formality is the F-score1 (Heylighen and Dewaele, 1999), which is based on relative part-of-speech frequencies. F-score and its more recent variants (Li et al., 2013) provide a coarse measure of formality, but are designed to work at the genrelevel, making them less reliable for shorter units of text such as sentences (Lahiri, 2015). Exist1 We use special font to denote Heylighen"
Q16-1005,C14-1205,0,0.490597,"text”– e.g. use of slang and poor grammar (Mosquera and Moreda, 2012a; Peterson et al., 2011). As a result, many rules have been explored for recognizing and generating informal language. Some of these rules are abstract, such as the level of implicature (Heylighen and Dewaele, 1999; Lahiri, 2015) or the degree of subjectivity (Mosquera and Moreda, 2012a), while others are much more concrete, such as the number of adjectives (Fang and Cao, 2009) or use of contractions (Abu Sheikha and Inkpen, 2011). Much prior work on detecting formality has focused on the lexical level (Brooke et al., 2010; Brooke and Hirst, 2014; Pavlick and Nenkova, 2015). For larger units of text, perhaps the best-known method for measuring formality is the F-score1 (Heylighen and Dewaele, 1999), which is based on relative part-of-speech frequencies. F-score and its more recent variants (Li et al., 2013) provide a coarse measure of formality, but are designed to work at the genrelevel, making them less reliable for shorter units of text such as sentences (Lahiri, 2015). Exist1 We use special font to denote Heylighen and Dewaele’s F-score to avoid confusion with F1 measure. 62 ing statistical approaches to detecting formality (Abu S"
Q16-1005,C10-2011,0,0.109136,"ute this score in the same way as Sidhaye and Cheung (2015), who used it to measure the formality of tweets. • Ngram classifier: As our final baseline, we train a ridge regression model which uses only ngrams (unigrams, bigrams, and trigrams) as features. Comparison against previously published models. Note that we are not able to make a meaningful comparison against against any of the previously published statistical models for formality detection. To our knowledge, there are three relevant previous publications that produced statistical models for detecting formality: Abu Sheikha and Inkpen (2010), Peterson et al. (2011), and Mosquera and Moreda (2012b). All three of these models performed a binary classification (as opposed to regression) and operated at the document (as opposed to sentence level). We were able to closely reimplement the model of Peterson et al. (2011), but we choose not to include the results here since their model was designed for binary email-level classification and thus relies on domain-specific features (e.g. casing in the subject line), that are not available in our real-valued, sentence-level datasets. The other models and the data/lexicons on which they relie"
Q16-1005,P13-1025,0,0.688285,"nits of text such as sentences (Lahiri, 2015). Exist1 We use special font to denote Heylighen and Dewaele’s F-score to avoid confusion with F1 measure. 62 ing statistical approaches to detecting formality (Abu Sheikha and Inkpen, 2010; Peterson et al., 2011; Mosquera and Moreda, 2012b) have treated the problem as a binary classification task and relied heavily on word lists to differentiate the two classes. Linguistics literature supports treating formality as a continuum (Irvine, 1979; Heylighen and Dewaele, 1999), as has been done in studies of other pragmatic dimensions such as politeness (Danescu-Niculescu-Mizil et al., 2013) and emotiveness (Walker et al., 2012). Lahiri et al. (2011) provided a preliminary investigation of annotating formality on an ordinal scale and released a dataset of sentence-level formality annotations (Lahiri, 2015), but did not use their data in any computational tasks. This paper extends prior work by (i) introducing a statistical regression model of formality which is based on an empirical analysis of human perceptions rather than on heuristics and (ii) by applying that model to a linguistic analysis of online discussions. 3 Human perceptions of formality Before we can automatically rec"
Q16-1005,Y09-1015,0,0.0264926,"stance and shared knowledge (Sigley, 1997; Hovy, 1987; Lahiri et al., 2011). Other recent work adopts a less abstract definition which is similar to the notion of “noisy text”– e.g. use of slang and poor grammar (Mosquera and Moreda, 2012a; Peterson et al., 2011). As a result, many rules have been explored for recognizing and generating informal language. Some of these rules are abstract, such as the level of implicature (Heylighen and Dewaele, 1999; Lahiri, 2015) or the degree of subjectivity (Mosquera and Moreda, 2012a), while others are much more concrete, such as the number of adjectives (Fang and Cao, 2009) or use of contractions (Abu Sheikha and Inkpen, 2011). Much prior work on detecting formality has focused on the lexical level (Brooke et al., 2010; Brooke and Hirst, 2014; Pavlick and Nenkova, 2015). For larger units of text, perhaps the best-known method for measuring formality is the F-score1 (Heylighen and Dewaele, 1999), which is based on relative part-of-speech frequencies. F-score and its more recent variants (Li et al., 2013) provide a coarse measure of formality, but are designed to work at the genrelevel, making them less reliable for shorter units of text such as sentences (Lahiri,"
Q16-1005,N15-1185,0,0.0196514,"1979; Brown and Fraser, 1979). The formal-informal dimension has even been called the “most important dimension of variation between styles” (Heylighen and Dewaele, 1999). A speaker’s level of formality can reveal information about their familiarity with a person, opinions of a topic, and goals for an interaction (Hovy, 1987; Endrass et al., 2011). As a result, the ability to recognize formality is an integral part of dialogue systems (Mairesse, 2008; Mairesse and Walker, 2011; Battaglino and Bickmore, 2015), sociolinguistic analyses (Danescu-Niculescu-Mizil et al., 2012; Justo et al., 2014; Krishnan and Eisenstein, 2015), human-computer interaction (Johnson et al., 2005; Khosmood and Walker, 2010), summarization (Sidhaye and Cheung, 2015), and automatic writing assessment (Felice and Deane, 2012). Formality can also indicate contextindependent, universal statements (Heylighen and Dewaele, 1999), making formality detection relevant for tasks such as knowledge base population (Suh et al., 2006; Reiter and Frank, 2010) and textual entailment (Dagan et al., 2006). This paper investigates formality in online written communication. The contributions are as follows: 1) We provide an analysis of humans’ subjective pe"
Q16-1005,J11-3002,0,0.0303536,"have observed that it subsumes a range of dimensions of style including serious-trivial, polite-casual, and level of shared knowledge (Irvine, 1979; Brown and Fraser, 1979). The formal-informal dimension has even been called the “most important dimension of variation between styles” (Heylighen and Dewaele, 1999). A speaker’s level of formality can reveal information about their familiarity with a person, opinions of a topic, and goals for an interaction (Hovy, 1987; Endrass et al., 2011). As a result, the ability to recognize formality is an integral part of dialogue systems (Mairesse, 2008; Mairesse and Walker, 2011; Battaglino and Bickmore, 2015), sociolinguistic analyses (Danescu-Niculescu-Mizil et al., 2012; Justo et al., 2014; Krishnan and Eisenstein, 2015), human-computer interaction (Johnson et al., 2005; Khosmood and Walker, 2010), summarization (Sidhaye and Cheung, 2015), and automatic writing assessment (Felice and Deane, 2012). Formality can also indicate contextindependent, universal statements (Heylighen and Dewaele, 1999), making formality detection relevant for tasks such as knowledge base population (Suh et al., 2006; Reiter and Frank, 2010) and textual entailment (Dagan et al., 2006). Thi"
Q16-1005,N15-1023,1,0.847193,"g and poor grammar (Mosquera and Moreda, 2012a; Peterson et al., 2011). As a result, many rules have been explored for recognizing and generating informal language. Some of these rules are abstract, such as the level of implicature (Heylighen and Dewaele, 1999; Lahiri, 2015) or the degree of subjectivity (Mosquera and Moreda, 2012a), while others are much more concrete, such as the number of adjectives (Fang and Cao, 2009) or use of contractions (Abu Sheikha and Inkpen, 2011). Much prior work on detecting formality has focused on the lexical level (Brooke et al., 2010; Brooke and Hirst, 2014; Pavlick and Nenkova, 2015). For larger units of text, perhaps the best-known method for measuring formality is the F-score1 (Heylighen and Dewaele, 1999), which is based on relative part-of-speech frequencies. F-score and its more recent variants (Li et al., 2013) provide a coarse measure of formality, but are designed to work at the genrelevel, making them less reliable for shorter units of text such as sentences (Lahiri, 2015). Exist1 We use special font to denote Heylighen and Dewaele’s F-score to avoid confusion with F1 measure. 62 ing statistical approaches to detecting formality (Abu Sheikha and Inkpen, 2010; Pet"
Q16-1005,W11-0711,0,0.242466,"Missing"
Q16-1005,P10-1005,0,0.0369924,"gral part of dialogue systems (Mairesse, 2008; Mairesse and Walker, 2011; Battaglino and Bickmore, 2015), sociolinguistic analyses (Danescu-Niculescu-Mizil et al., 2012; Justo et al., 2014; Krishnan and Eisenstein, 2015), human-computer interaction (Johnson et al., 2005; Khosmood and Walker, 2010), summarization (Sidhaye and Cheung, 2015), and automatic writing assessment (Felice and Deane, 2012). Formality can also indicate contextindependent, universal statements (Heylighen and Dewaele, 1999), making formality detection relevant for tasks such as knowledge base population (Suh et al., 2006; Reiter and Frank, 2010) and textual entailment (Dagan et al., 2006). This paper investigates formality in online written communication. The contributions are as follows: 1) We provide an analysis of humans’ subjective perceptions of formality in four different genres. We highlight areas of high and low agreement and extract patterns that consis61 Transactions of the Association for Computational Linguistics, vol. 4, pp. 61–74, 2016. Action Editor: Janyce Wiebe and Kristina Toutanova. Submission batch: 10/2015; Revision batch: 12/2015; Published 3/2016. c 2016 Association for Computational Linguistics. Distributed un"
Q16-1005,D15-1014,0,0.0562636,"between styles” (Heylighen and Dewaele, 1999). A speaker’s level of formality can reveal information about their familiarity with a person, opinions of a topic, and goals for an interaction (Hovy, 1987; Endrass et al., 2011). As a result, the ability to recognize formality is an integral part of dialogue systems (Mairesse, 2008; Mairesse and Walker, 2011; Battaglino and Bickmore, 2015), sociolinguistic analyses (Danescu-Niculescu-Mizil et al., 2012; Justo et al., 2014; Krishnan and Eisenstein, 2015), human-computer interaction (Johnson et al., 2005; Khosmood and Walker, 2010), summarization (Sidhaye and Cheung, 2015), and automatic writing assessment (Felice and Deane, 2012). Formality can also indicate contextindependent, universal statements (Heylighen and Dewaele, 1999), making formality detection relevant for tasks such as knowledge base population (Suh et al., 2006; Reiter and Frank, 2010) and textual entailment (Dagan et al., 2006). This paper investigates formality in online written communication. The contributions are as follows: 1) We provide an analysis of humans’ subjective perceptions of formality in four different genres. We highlight areas of high and low agreement and extract patterns that"
Q16-1005,walker-etal-2012-corpus,0,0.155054,"e use special font to denote Heylighen and Dewaele’s F-score to avoid confusion with F1 measure. 62 ing statistical approaches to detecting formality (Abu Sheikha and Inkpen, 2010; Peterson et al., 2011; Mosquera and Moreda, 2012b) have treated the problem as a binary classification task and relied heavily on word lists to differentiate the two classes. Linguistics literature supports treating formality as a continuum (Irvine, 1979; Heylighen and Dewaele, 1999), as has been done in studies of other pragmatic dimensions such as politeness (Danescu-Niculescu-Mizil et al., 2013) and emotiveness (Walker et al., 2012). Lahiri et al. (2011) provided a preliminary investigation of annotating formality on an ordinal scale and released a dataset of sentence-level formality annotations (Lahiri, 2015), but did not use their data in any computational tasks. This paper extends prior work by (i) introducing a statistical regression model of formality which is based on an empirical analysis of human perceptions rather than on heuristics and (ii) by applying that model to a linguistic analysis of online discussions. 3 Human perceptions of formality Before we can automatically recognize formality, we need an understan"
Q16-1005,W15-0515,0,\N,Missing
Q16-1029,W14-1214,0,0.104671,"Missing"
Q16-1029,C14-1188,0,0.155152,"s dyslexia (Rello et al., 2013), autism (Evans et al., 2014), and aphasia (Carroll et al., 1999). It is widely accepted that sentence simplification can be implemented by three major types of operations: splitting, deletion and paraphrasing (Feng, 2008). The splitting operation decomposes a long sentence into a sequence of shorter sentences. Deletion removes less important parts of a sentence. The paraphrasing operation includes reordering, lexical substitutions and syntactic transformations. While sentence splitting (Siddharthan, 2006; Petersen and Ostendorf, 2007; Narayan and Gardent, 2014; Angrosh et al., 2014) and deletion (Knight and Marcu 2002; Clarke and Lapata 2006; Filippova and Strube 2008; Filippova et al. 2015; Rush et al. 2015; and others) have been intensively studied, there has been considerably less research on developing new paraphrasing models for text simplification — most previous work has used off-the-shelf statistical machine translation (SMT) technology and achieved reasonable results (Coster and Kauchak, 2011a,b; Wubben ˇ et al., 2012; Stajner et al., 2015). However, they have either treated the judgment technology as a black (Coster and Kauchak, 2011a,b; Narayan and Garˇ dent,"
Q16-1029,P05-1074,1,0.147414,"ber of transformations that simplify. However, we argue that because the size of the Normal-Simple Wikipedia parallel corpus is quite small (108k sentence pairs with 2 million words), the diversity and coverage of patterns that can be learned is actually quite limited. In this paper we will leverage the large-scale Paraphrase Database (PPDB)4 (Ganitkevitch et al., 2013; Pavlick et al., 2015) as a rich source of lexical, phrasal and syntactic simplification operations. It is created by extracting English paraphrases from bilingual parallel corpora using a technique called “bilingual pivoting” (Bannard and Callison-Burch, 2005). The PPDB is represented as a synchronous context-free grammar (SCFG), which is commonly used as the formalism for syntax-based machine translation (Zollmann and Venugopal, 2006; Chiang, 2007; Weese et al., 2011). Table 1 shows some example paraphrase rules in the PPDB. PPDB employs 1000 times more data (106 million sentence pairs with 2 billion words) than the Normal-Simple Wikipedia parallel corpus. The English portion of PPDB contains over 220 million paraphrase rules, consisting of 8 million lexical, 73 million phrasal and 140 million syntactic para4 http://paraphrase.org phrase patterns."
Q16-1029,P11-2087,0,0.0097729,"r knowledge, no tunable metric has been attempted for simplification, except for BLEU. Nor do any evaluation metrics exist for simplification, although there are several designed for other text-to-text generation tasks: grammatical error correction (Napoles et al., 2015; Felice and Briscoe, 2015; Dahlmeier and Ng, 2012), paraphrase generation (Chen and Dolan, 2011; Xu et al., 2012; Sun and Zhou, 2012), and conversation generation (Galley et al., 2015). Another line of related work is lexical simplification that focuses on finding simpler synonyms of a given complex word (Yatskar et al., 2010; Biran et al., 2011; Specia et al., 2012; Horn et al., 2014). 3 Adapting Machine Translation for Simplification We adapt the machinery of statistical machine translation to the task of text simplification by making changes in the following four key components: 3.1 Simplification-specific Objective Functions In the statistical machine translation framework, one crucial element is to design automatic evaluation metrics to be used as training objectives. Training algorithms, such as MERT (Och, 2003) or PRO (Hopkins and May, 2011), then directly optimize the model parameters such that the end-to-end simplification q"
Q16-1029,E99-1042,0,0.71526,"development for this task. 1 Introduction The goal of text simplification is to rewrite an input text so that the output is more readable. Text simplification has applications for reducing input complexity for natural language processing (Siddharthan et al., 2004; Miwa et al., 2010; Chen et al., 2012b) and providing reading aids for people with limited language skills (Petersen and Ostendorf, 2007; Watanabe et al., 2009; Allen, 2009; De Belder and Moens, 2010; Siddharthan and Katsos, 2010) or language impairments such as dyslexia (Rello et al., 2013), autism (Evans et al., 2014), and aphasia (Carroll et al., 1999). It is widely accepted that sentence simplification can be implemented by three major types of operations: splitting, deletion and paraphrasing (Feng, 2008). The splitting operation decomposes a long sentence into a sequence of shorter sentences. Deletion removes less important parts of a sentence. The paraphrasing operation includes reordering, lexical substitutions and syntactic transformations. While sentence splitting (Siddharthan, 2006; Petersen and Ostendorf, 2007; Narayan and Gardent, 2014; Angrosh et al., 2014) and deletion (Knight and Marcu 2002; Clarke and Lapata 2006; Filippova and"
Q16-1029,W11-2504,1,0.904086,"Missing"
Q16-1029,C96-2183,0,0.904046,"ions Poperation and recalls Roperation : SARI = d1 Fadd + d2 Fkeep + d3 Pdel (7) where d1 = d2 = d3 = 1/3 and Poperation = Roperation 1 k 1 = k Foperation = X poperation (n) n=[1,...,k] X roperation (n) n=[1,...,k] 2 × Poperation × Roperation Poperation + Roperation operation ∈ [del, keep, add] where k is the highest n-gram order and set to 4 in our experiments. 3.2 Incorporating Large-Scale Paraphrase Rules Another challenge for text simplification is generating an ample set of rewrite rules that potentially simplify an input sentence. Most early work has relied on either hand-crafted rules (Chandrasekar et al., 1996; Carroll et al., 1999; Siddharthan, 2006; Vickrey and Koller, 2008) or dictionaries like WordNet (Devlin et al., 1999; Kaji et al., 2002; Inui et al., 2003). Other more recent studies have relied on 405 the parallel Normal-Simple Wikipedia Corpus to automatically extract rewrite rules (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011b; Wubben et al., 2012; Narayan and Gardent, 2014; Siddharthan and Angrosh, 2014; Angrosh et al., 2014). This technique does manage to learn a small number of transformations that simplify. However, we argue that because the size of the Normal"
Q16-1029,P12-1098,0,0.134153,"text simplification, taking advantage of large-scale paraphrases learned from bilingual texts and a small amount of manual simplifications with multiple references. Our work is the first to design automatic metrics that are effective for tuning and evaluating simplification systems, which will facilitate iterative development for this task. 1 Introduction The goal of text simplification is to rewrite an input text so that the output is more readable. Text simplification has applications for reducing input complexity for natural language processing (Siddharthan et al., 2004; Miwa et al., 2010; Chen et al., 2012b) and providing reading aids for people with limited language skills (Petersen and Ostendorf, 2007; Watanabe et al., 2009; Allen, 2009; De Belder and Moens, 2010; Siddharthan and Katsos, 2010) or language impairments such as dyslexia (Rello et al., 2013), autism (Evans et al., 2014), and aphasia (Carroll et al., 1999). It is widely accepted that sentence simplification can be implemented by three major types of operations: splitting, deletion and paraphrasing (Feng, 2008). The splitting operation decomposes a long sentence into a sequence of shorter sentences. Deletion removes less important"
Q16-1029,P11-1020,0,0.0443074,"y word choice and grammar. Our work is also related to other tunable metrics designed to be very simple and light-weight to ensure fast repeated computation for tuning bilingual translation models (Liu et al., 2010; Chen et al., 2012a). To the best of our knowledge, no tunable metric has been attempted for simplification, except for BLEU. Nor do any evaluation metrics exist for simplification, although there are several designed for other text-to-text generation tasks: grammatical error correction (Napoles et al., 2015; Felice and Briscoe, 2015; Dahlmeier and Ng, 2012), paraphrase generation (Chen and Dolan, 2011; Xu et al., 2012; Sun and Zhou, 2012), and conversation generation (Galley et al., 2015). Another line of related work is lexical simplification that focuses on finding simpler synonyms of a given complex word (Yatskar et al., 2010; Biran et al., 2011; Specia et al., 2012; Horn et al., 2014). 3 Adapting Machine Translation for Simplification We adapt the machinery of statistical machine translation to the task of text simplification by making changes in the following four key components: 3.1 Simplification-specific Objective Functions In the statistical machine translation framework, one cruc"
Q16-1029,C12-1034,0,0.303799,"text simplification, taking advantage of large-scale paraphrases learned from bilingual texts and a small amount of manual simplifications with multiple references. Our work is the first to design automatic metrics that are effective for tuning and evaluating simplification systems, which will facilitate iterative development for this task. 1 Introduction The goal of text simplification is to rewrite an input text so that the output is more readable. Text simplification has applications for reducing input complexity for natural language processing (Siddharthan et al., 2004; Miwa et al., 2010; Chen et al., 2012b) and providing reading aids for people with limited language skills (Petersen and Ostendorf, 2007; Watanabe et al., 2009; Allen, 2009; De Belder and Moens, 2010; Siddharthan and Katsos, 2010) or language impairments such as dyslexia (Rello et al., 2013), autism (Evans et al., 2014), and aphasia (Carroll et al., 1999). It is widely accepted that sentence simplification can be implemented by three major types of operations: splitting, deletion and paraphrasing (Feng, 2008). The splitting operation decomposes a long sentence into a sequence of shorter sentences. Deletion removes less important"
Q16-1029,J07-2003,0,0.0340451,"patterns that can be learned is actually quite limited. In this paper we will leverage the large-scale Paraphrase Database (PPDB)4 (Ganitkevitch et al., 2013; Pavlick et al., 2015) as a rich source of lexical, phrasal and syntactic simplification operations. It is created by extracting English paraphrases from bilingual parallel corpora using a technique called “bilingual pivoting” (Bannard and Callison-Burch, 2005). The PPDB is represented as a synchronous context-free grammar (SCFG), which is commonly used as the formalism for syntax-based machine translation (Zollmann and Venugopal, 2006; Chiang, 2007; Weese et al., 2011). Table 1 shows some example paraphrase rules in the PPDB. PPDB employs 1000 times more data (106 million sentence pairs with 2 billion words) than the Normal-Simple Wikipedia parallel corpus. The English portion of PPDB contains over 220 million paraphrase rules, consisting of 8 million lexical, 73 million phrasal and 140 million syntactic para4 http://paraphrase.org phrase patterns. The key differences between the paraphrase rules from PPDB and the transformations learned by the naive application of SMT to the Normal-Simple Wikipedia parallel corpus, are that the PPDB pa"
Q16-1029,P06-1048,0,0.042514,"), and aphasia (Carroll et al., 1999). It is widely accepted that sentence simplification can be implemented by three major types of operations: splitting, deletion and paraphrasing (Feng, 2008). The splitting operation decomposes a long sentence into a sequence of shorter sentences. Deletion removes less important parts of a sentence. The paraphrasing operation includes reordering, lexical substitutions and syntactic transformations. While sentence splitting (Siddharthan, 2006; Petersen and Ostendorf, 2007; Narayan and Gardent, 2014; Angrosh et al., 2014) and deletion (Knight and Marcu 2002; Clarke and Lapata 2006; Filippova and Strube 2008; Filippova et al. 2015; Rush et al. 2015; and others) have been intensively studied, there has been considerably less research on developing new paraphrasing models for text simplification — most previous work has used off-the-shelf statistical machine translation (SMT) technology and achieved reasonable results (Coster and Kauchak, 2011a,b; Wubben ˇ et al., 2012; Stajner et al., 2015). However, they have either treated the judgment technology as a black (Coster and Kauchak, 2011a,b; Narayan and Garˇ dent, 2014; Angrosh et al., 2014; Stajner et al., 2015) or they ha"
Q16-1029,W11-1601,0,0.212736,"includes reordering, lexical substitutions and syntactic transformations. While sentence splitting (Siddharthan, 2006; Petersen and Ostendorf, 2007; Narayan and Gardent, 2014; Angrosh et al., 2014) and deletion (Knight and Marcu 2002; Clarke and Lapata 2006; Filippova and Strube 2008; Filippova et al. 2015; Rush et al. 2015; and others) have been intensively studied, there has been considerably less research on developing new paraphrasing models for text simplification — most previous work has used off-the-shelf statistical machine translation (SMT) technology and achieved reasonable results (Coster and Kauchak, 2011a,b; Wubben ˇ et al., 2012; Stajner et al., 2015). However, they have either treated the judgment technology as a black (Coster and Kauchak, 2011a,b; Narayan and Garˇ dent, 2014; Angrosh et al., 2014; Stajner et al., 2015) or they have been limited to modifying only one aspect of it, such as the translation model (Zhu et al., 2010; Woodsend and Lapata, 2011) or the reranking component (Wubben et al., 2012). In this paper, we present a complete adaptation of a syntax-based machine translation framework to perform simplification. Our methodology poses text simplification as a paraphrasing proble"
Q16-1029,P11-2117,0,0.0757764,"includes reordering, lexical substitutions and syntactic transformations. While sentence splitting (Siddharthan, 2006; Petersen and Ostendorf, 2007; Narayan and Gardent, 2014; Angrosh et al., 2014) and deletion (Knight and Marcu 2002; Clarke and Lapata 2006; Filippova and Strube 2008; Filippova et al. 2015; Rush et al. 2015; and others) have been intensively studied, there has been considerably less research on developing new paraphrasing models for text simplification — most previous work has used off-the-shelf statistical machine translation (SMT) technology and achieved reasonable results (Coster and Kauchak, 2011a,b; Wubben ˇ et al., 2012; Stajner et al., 2015). However, they have either treated the judgment technology as a black (Coster and Kauchak, 2011a,b; Narayan and Garˇ dent, 2014; Angrosh et al., 2014; Stajner et al., 2015) or they have been limited to modifying only one aspect of it, such as the translation model (Zhu et al., 2010; Woodsend and Lapata, 2011) or the reranking component (Wubben et al., 2012). In this paper, we present a complete adaptation of a syntax-based machine translation framework to perform simplification. Our methodology poses text simplification as a paraphrasing proble"
Q16-1029,N12-1067,0,0.0258186,"ain and the solution space is more constrained by word choice and grammar. Our work is also related to other tunable metrics designed to be very simple and light-weight to ensure fast repeated computation for tuning bilingual translation models (Liu et al., 2010; Chen et al., 2012a). To the best of our knowledge, no tunable metric has been attempted for simplification, except for BLEU. Nor do any evaluation metrics exist for simplification, although there are several designed for other text-to-text generation tasks: grammatical error correction (Napoles et al., 2015; Felice and Briscoe, 2015; Dahlmeier and Ng, 2012), paraphrase generation (Chen and Dolan, 2011; Xu et al., 2012; Sun and Zhou, 2012), and conversation generation (Galley et al., 2015). Another line of related work is lexical simplification that focuses on finding simpler synonyms of a given complex word (Yatskar et al., 2010; Biran et al., 2011; Specia et al., 2012; Horn et al., 2014). 3 Adapting Machine Translation for Simplification We adapt the machinery of statistical machine translation to the task of text simplification by making changes in the following four key components: 3.1 Simplification-specific Objective Functions In the statis"
Q16-1029,W14-1215,0,0.106191,", which will facilitate iterative development for this task. 1 Introduction The goal of text simplification is to rewrite an input text so that the output is more readable. Text simplification has applications for reducing input complexity for natural language processing (Siddharthan et al., 2004; Miwa et al., 2010; Chen et al., 2012b) and providing reading aids for people with limited language skills (Petersen and Ostendorf, 2007; Watanabe et al., 2009; Allen, 2009; De Belder and Moens, 2010; Siddharthan and Katsos, 2010) or language impairments such as dyslexia (Rello et al., 2013), autism (Evans et al., 2014), and aphasia (Carroll et al., 1999). It is widely accepted that sentence simplification can be implemented by three major types of operations: splitting, deletion and paraphrasing (Feng, 2008). The splitting operation decomposes a long sentence into a sequence of shorter sentences. Deletion removes less important parts of a sentence. The paraphrasing operation includes reordering, lexical substitutions and syntactic transformations. While sentence splitting (Siddharthan, 2006; Petersen and Ostendorf, 2007; Narayan and Gardent, 2014; Angrosh et al., 2014) and deletion (Knight and Marcu 2002; C"
Q16-1029,N15-1060,0,0.0389548,"data is much harder to obtain and the solution space is more constrained by word choice and grammar. Our work is also related to other tunable metrics designed to be very simple and light-weight to ensure fast repeated computation for tuning bilingual translation models (Liu et al., 2010; Chen et al., 2012a). To the best of our knowledge, no tunable metric has been attempted for simplification, except for BLEU. Nor do any evaluation metrics exist for simplification, although there are several designed for other text-to-text generation tasks: grammatical error correction (Napoles et al., 2015; Felice and Briscoe, 2015; Dahlmeier and Ng, 2012), paraphrase generation (Chen and Dolan, 2011; Xu et al., 2012; Sun and Zhou, 2012), and conversation generation (Galley et al., 2015). Another line of related work is lexical simplification that focuses on finding simpler synonyms of a given complex word (Yatskar et al., 2010; Biran et al., 2011; Specia et al., 2012; Horn et al., 2014). 3 Adapting Machine Translation for Simplification We adapt the machinery of statistical machine translation to the task of text simplification by making changes in the following four key components: 3.1 Simplification-specific Objectiv"
Q16-1029,D15-1042,0,0.0371305,"accepted that sentence simplification can be implemented by three major types of operations: splitting, deletion and paraphrasing (Feng, 2008). The splitting operation decomposes a long sentence into a sequence of shorter sentences. Deletion removes less important parts of a sentence. The paraphrasing operation includes reordering, lexical substitutions and syntactic transformations. While sentence splitting (Siddharthan, 2006; Petersen and Ostendorf, 2007; Narayan and Gardent, 2014; Angrosh et al., 2014) and deletion (Knight and Marcu 2002; Clarke and Lapata 2006; Filippova and Strube 2008; Filippova et al. 2015; Rush et al. 2015; and others) have been intensively studied, there has been considerably less research on developing new paraphrasing models for text simplification — most previous work has used off-the-shelf statistical machine translation (SMT) technology and achieved reasonable results (Coster and Kauchak, 2011a,b; Wubben ˇ et al., 2012; Stajner et al., 2015). However, they have either treated the judgment technology as a black (Coster and Kauchak, 2011a,b; Narayan and Garˇ dent, 2014; Angrosh et al., 2014; Stajner et al., 2015) or they have been limited to modifying only one aspect of it"
Q16-1029,W08-1105,0,0.0147252,"et al., 1999). It is widely accepted that sentence simplification can be implemented by three major types of operations: splitting, deletion and paraphrasing (Feng, 2008). The splitting operation decomposes a long sentence into a sequence of shorter sentences. Deletion removes less important parts of a sentence. The paraphrasing operation includes reordering, lexical substitutions and syntactic transformations. While sentence splitting (Siddharthan, 2006; Petersen and Ostendorf, 2007; Narayan and Gardent, 2014; Angrosh et al., 2014) and deletion (Knight and Marcu 2002; Clarke and Lapata 2006; Filippova and Strube 2008; Filippova et al. 2015; Rush et al. 2015; and others) have been intensively studied, there has been considerably less research on developing new paraphrasing models for text simplification — most previous work has used off-the-shelf statistical machine translation (SMT) technology and achieved reasonable results (Coster and Kauchak, 2011a,b; Wubben ˇ et al., 2012; Stajner et al., 2015). However, they have either treated the judgment technology as a black (Coster and Kauchak, 2011a,b; Narayan and Garˇ dent, 2014; Angrosh et al., 2014; Stajner et al., 2015) or they have been limited to modifyin"
Q16-1029,P15-2073,0,0.0139864,"be very simple and light-weight to ensure fast repeated computation for tuning bilingual translation models (Liu et al., 2010; Chen et al., 2012a). To the best of our knowledge, no tunable metric has been attempted for simplification, except for BLEU. Nor do any evaluation metrics exist for simplification, although there are several designed for other text-to-text generation tasks: grammatical error correction (Napoles et al., 2015; Felice and Briscoe, 2015; Dahlmeier and Ng, 2012), paraphrase generation (Chen and Dolan, 2011; Xu et al., 2012; Sun and Zhou, 2012), and conversation generation (Galley et al., 2015). Another line of related work is lexical simplification that focuses on finding simpler synonyms of a given complex word (Yatskar et al., 2010; Biran et al., 2011; Specia et al., 2012; Horn et al., 2014). 3 Adapting Machine Translation for Simplification We adapt the machinery of statistical machine translation to the task of text simplification by making changes in the following four key components: 3.1 Simplification-specific Objective Functions In the statistical machine translation framework, one crucial element is to design automatic evaluation metrics to be used as training objectives."
Q16-1029,W12-3134,1,0.938317,"ctions are phrase translation probabilities, word-for-word lexical translation probabilities, a rule application penalty (which governs whether the system prefers fewer longer phrases or a greater number of shorter phrases), and a language model probability. Together these features are what the model uses to distinguish between good and bad translations. For monolingual translation tasks, previous research suggests that features like paraphrase probability and distributional similarity are potentially helpful in picking out good paraphrases (Chan et al., 2011) and for text-to-text generation (Ganitkevitch et al., 2012b). While these two features quantify how good a paraphrase rule is in general, they do not indicate how good the rule is for a specific task, like simplification. For each paraphrase rule, we use all the 33 features that were distributed with PPDB 1.0 and add 406 9 new features for simplification purposes:5 length in characters, length in words, number of syllables, language model scores, and fraction of common English words in each rule. These features are computed for both sides of a paraphrase pattern, the word with the maximum number of syllables on each side and the difference between th"
Q16-1029,S12-1034,1,0.867442,"Missing"
Q16-1029,N13-1092,1,0.395478,"Missing"
Q16-1029,D11-1125,0,0.0478686,"on that focuses on finding simpler synonyms of a given complex word (Yatskar et al., 2010; Biran et al., 2011; Specia et al., 2012; Horn et al., 2014). 3 Adapting Machine Translation for Simplification We adapt the machinery of statistical machine translation to the task of text simplification by making changes in the following four key components: 3.1 Simplification-specific Objective Functions In the statistical machine translation framework, one crucial element is to design automatic evaluation metrics to be used as training objectives. Training algorithms, such as MERT (Och, 2003) or PRO (Hopkins and May, 2011), then directly optimize the model parameters such that the end-to-end simplification quality is optimal. Unfortunately, previous work on text simplification has only used BLEU for tuning, which is insufficient as we show empirically in Section 4. We propose two new light-weight metrics instead: FKBLEU that explicitly measures readability and SARI that implicitly measures it by comparing against the input and references. Unlike machine translation metrics which do not compare against the (foreign) input sentence, it is necessary to compare simplification system outputs against the inputs to as"
Q16-1029,P14-2075,0,0.0134708,"ttempted for simplification, except for BLEU. Nor do any evaluation metrics exist for simplification, although there are several designed for other text-to-text generation tasks: grammatical error correction (Napoles et al., 2015; Felice and Briscoe, 2015; Dahlmeier and Ng, 2012), paraphrase generation (Chen and Dolan, 2011; Xu et al., 2012; Sun and Zhou, 2012), and conversation generation (Galley et al., 2015). Another line of related work is lexical simplification that focuses on finding simpler synonyms of a given complex word (Yatskar et al., 2010; Biran et al., 2011; Specia et al., 2012; Horn et al., 2014). 3 Adapting Machine Translation for Simplification We adapt the machinery of statistical machine translation to the task of text simplification by making changes in the following four key components: 3.1 Simplification-specific Objective Functions In the statistical machine translation framework, one crucial element is to design automatic evaluation metrics to be used as training objectives. Training algorithms, such as MERT (Och, 2003) or PRO (Hopkins and May, 2011), then directly optimize the model parameters such that the end-to-end simplification quality is optimal. Unfortunately, previou"
Q16-1029,N15-1022,0,0.0688657,"n or sentence splitting could be applied as pre- or post-processing steps. 2 Background Xu et al. (2015) laid out a series of problems that are present in current text simplification research, and argued that we should deviate from the previous state-of-the-art benchmarking setup. First, the Simple English Wikipedia data has dominated simplification research since 2010 (Zhu et al., 2010; Siddharthan, 2014), and is used together with Standard English Wikipedia to create parallel text to train MT-based simplification systems. However, recent studies (Xu et al., 2015; Amancio and ˇ Specia, 2014; Hwang et al., 2015; Stajner et al., 2015) showed that the parallel Wikipedia simplification corpus contains a large proportion of inadequate (not much simpler) or inaccurate (not aligned or only partially aligned) simplifications. It is one of the leading reasons that existing simplification systems struggle to generate simplifying paraphrases and leave the input sentences unchanged (Wubben 1 Our code and data are made available at: https:// github.com/cocoxu/simplification/ 402 et al., 2012). Previously researchers attempted some quick fixes by adding phrasal deletion rules (Coster and Kauchak, 2011a) or reran"
Q16-1029,W03-1602,0,0.0887002,"ation (n) n=[1,...,k] X roperation (n) n=[1,...,k] 2 × Poperation × Roperation Poperation + Roperation operation ∈ [del, keep, add] where k is the highest n-gram order and set to 4 in our experiments. 3.2 Incorporating Large-Scale Paraphrase Rules Another challenge for text simplification is generating an ample set of rewrite rules that potentially simplify an input sentence. Most early work has relied on either hand-crafted rules (Chandrasekar et al., 1996; Carroll et al., 1999; Siddharthan, 2006; Vickrey and Koller, 2008) or dictionaries like WordNet (Devlin et al., 1999; Kaji et al., 2002; Inui et al., 2003). Other more recent studies have relied on 405 the parallel Normal-Simple Wikipedia Corpus to automatically extract rewrite rules (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011b; Wubben et al., 2012; Narayan and Gardent, 2014; Siddharthan and Angrosh, 2014; Angrosh et al., 2014). This technique does manage to learn a small number of transformations that simplify. However, we argue that because the size of the Normal-Simple Wikipedia parallel corpus is quite small (108k sentence pairs with 2 million words), the diversity and coverage of patterns that can be learned is ac"
Q16-1029,P02-1028,0,0.0887695,"operation = X poperation (n) n=[1,...,k] X roperation (n) n=[1,...,k] 2 × Poperation × Roperation Poperation + Roperation operation ∈ [del, keep, add] where k is the highest n-gram order and set to 4 in our experiments. 3.2 Incorporating Large-Scale Paraphrase Rules Another challenge for text simplification is generating an ample set of rewrite rules that potentially simplify an input sentence. Most early work has relied on either hand-crafted rules (Chandrasekar et al., 1996; Carroll et al., 1999; Siddharthan, 2006; Vickrey and Koller, 2008) or dictionaries like WordNet (Devlin et al., 1999; Kaji et al., 2002; Inui et al., 2003). Other more recent studies have relied on 405 the parallel Normal-Simple Wikipedia Corpus to automatically extract rewrite rules (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011b; Wubben et al., 2012; Narayan and Gardent, 2014; Siddharthan and Angrosh, 2014; Angrosh et al., 2014). This technique does manage to learn a small number of transformations that simplify. However, we argue that because the size of the Normal-Simple Wikipedia parallel corpus is quite small (108k sentence pairs with 2 million words), the diversity and coverage of patterns that"
Q16-1029,P13-1151,0,0.30462,"c task, like simplification. For each paraphrase rule, we use all the 33 features that were distributed with PPDB 1.0 and add 406 9 new features for simplification purposes:5 length in characters, length in words, number of syllables, language model scores, and fraction of common English words in each rule. These features are computed for both sides of a paraphrase pattern, the word with the maximum number of syllables on each side and the difference between the two sides, when it is applicable. We use language models built from the Gigaword corpus and the Simple Wikipedia corpus collected by Kauchak (2013). We also use a list of 3000 most common US English words compiled by Paul and Bernice Noll.6 3.4 Creating Multiple References Like with machine translation, where there are many equally good translations, in simplification there may be several ways of simplifying a sentence. Most previous work on text simplification only uses a single reference simplification, often from the Simple Wikipedia. This is undesirable since the Simple Wikipedia contains a large proportion of inadequate or inaccurate simplifications (Xu et al., 2015) . In this study, we collect multiple human reference simplificatio"
Q16-1029,W10-1754,0,0.0250509,"ression, in which compression of word and sentence lengths can be more straightforwardly implemented in features and the objective function in the SMT framework. We want to stress that sentence simplification is not a simple extension of sentence compression, but is a much more complicated task, primarily because high-quality data is much harder to obtain and the solution space is more constrained by word choice and grammar. Our work is also related to other tunable metrics designed to be very simple and light-weight to ensure fast repeated computation for tuning bilingual translation models (Liu et al., 2010; Chen et al., 2012a). To the best of our knowledge, no tunable metric has been attempted for simplification, except for BLEU. Nor do any evaluation metrics exist for simplification, although there are several designed for other text-to-text generation tasks: grammatical error correction (Napoles et al., 2015; Felice and Briscoe, 2015; Dahlmeier and Ng, 2012), paraphrase generation (Chen and Dolan, 2011; Xu et al., 2012; Sun and Zhou, 2012), and conversation generation (Galley et al., 2015). Another line of related work is lexical simplification that focuses on finding simpler synonyms of a gi"
Q16-1029,C10-1089,0,0.0709952,"slation to perform text simplification, taking advantage of large-scale paraphrases learned from bilingual texts and a small amount of manual simplifications with multiple references. Our work is the first to design automatic metrics that are effective for tuning and evaluating simplification systems, which will facilitate iterative development for this task. 1 Introduction The goal of text simplification is to rewrite an input text so that the output is more readable. Text simplification has applications for reducing input complexity for natural language processing (Siddharthan et al., 2004; Miwa et al., 2010; Chen et al., 2012b) and providing reading aids for people with limited language skills (Petersen and Ostendorf, 2007; Watanabe et al., 2009; Allen, 2009; De Belder and Moens, 2010; Siddharthan and Katsos, 2010) or language impairments such as dyslexia (Rello et al., 2013), autism (Evans et al., 2014), and aphasia (Carroll et al., 1999). It is widely accepted that sentence simplification can be implemented by three major types of operations: splitting, deletion and paraphrasing (Feng, 2008). The splitting operation decomposes a long sentence into a sequence of shorter sentences. Deletion remo"
Q16-1029,P15-2097,1,0.8131,"because high-quality data is much harder to obtain and the solution space is more constrained by word choice and grammar. Our work is also related to other tunable metrics designed to be very simple and light-weight to ensure fast repeated computation for tuning bilingual translation models (Liu et al., 2010; Chen et al., 2012a). To the best of our knowledge, no tunable metric has been attempted for simplification, except for BLEU. Nor do any evaluation metrics exist for simplification, although there are several designed for other text-to-text generation tasks: grammatical error correction (Napoles et al., 2015; Felice and Briscoe, 2015; Dahlmeier and Ng, 2012), paraphrase generation (Chen and Dolan, 2011; Xu et al., 2012; Sun and Zhou, 2012), and conversation generation (Galley et al., 2015). Another line of related work is lexical simplification that focuses on finding simpler synonyms of a given complex word (Yatskar et al., 2010; Biran et al., 2011; Specia et al., 2012; Horn et al., 2014). 3 Adapting Machine Translation for Simplification We adapt the machinery of statistical machine translation to the task of text simplification by making changes in the following four key components: 3.1 Simpli"
Q16-1029,P14-1041,0,0.733939,"language impairments such as dyslexia (Rello et al., 2013), autism (Evans et al., 2014), and aphasia (Carroll et al., 1999). It is widely accepted that sentence simplification can be implemented by three major types of operations: splitting, deletion and paraphrasing (Feng, 2008). The splitting operation decomposes a long sentence into a sequence of shorter sentences. Deletion removes less important parts of a sentence. The paraphrasing operation includes reordering, lexical substitutions and syntactic transformations. While sentence splitting (Siddharthan, 2006; Petersen and Ostendorf, 2007; Narayan and Gardent, 2014; Angrosh et al., 2014) and deletion (Knight and Marcu 2002; Clarke and Lapata 2006; Filippova and Strube 2008; Filippova et al. 2015; Rush et al. 2015; and others) have been intensively studied, there has been considerably less research on developing new paraphrasing models for text simplification — most previous work has used off-the-shelf statistical machine translation (SMT) technology and achieved reasonable results (Coster and Kauchak, 2011a,b; Wubben ˇ et al., 2012; Stajner et al., 2015). However, they have either treated the judgment technology as a black (Coster and Kauchak, 2011a,b;"
Q16-1029,P03-1021,0,0.0352014,"exical simplification that focuses on finding simpler synonyms of a given complex word (Yatskar et al., 2010; Biran et al., 2011; Specia et al., 2012; Horn et al., 2014). 3 Adapting Machine Translation for Simplification We adapt the machinery of statistical machine translation to the task of text simplification by making changes in the following four key components: 3.1 Simplification-specific Objective Functions In the statistical machine translation framework, one crucial element is to design automatic evaluation metrics to be used as training objectives. Training algorithms, such as MERT (Och, 2003) or PRO (Hopkins and May, 2011), then directly optimize the model parameters such that the end-to-end simplification quality is optimal. Unfortunately, previous work on text simplification has only used BLEU for tuning, which is insufficient as we show empirically in Section 4. We propose two new light-weight metrics instead: FKBLEU that explicitly measures readability and SARI that implicitly measures it by comparing against the input and references. Unlike machine translation metrics which do not compare against the (foreign) input sentence, it is necessary to compare simplification system o"
Q16-1029,P15-1146,1,0.25619,"ules (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011b; Wubben et al., 2012; Narayan and Gardent, 2014; Siddharthan and Angrosh, 2014; Angrosh et al., 2014). This technique does manage to learn a small number of transformations that simplify. However, we argue that because the size of the Normal-Simple Wikipedia parallel corpus is quite small (108k sentence pairs with 2 million words), the diversity and coverage of patterns that can be learned is actually quite limited. In this paper we will leverage the large-scale Paraphrase Database (PPDB)4 (Ganitkevitch et al., 2013; Pavlick et al., 2015) as a rich source of lexical, phrasal and syntactic simplification operations. It is created by extracting English paraphrases from bilingual parallel corpora using a technique called “bilingual pivoting” (Bannard and Callison-Burch, 2005). The PPDB is represented as a synchronous context-free grammar (SCFG), which is commonly used as the formalism for syntax-based machine translation (Zollmann and Venugopal, 2006; Chiang, 2007; Weese et al., 2011). Table 1 shows some example paraphrase rules in the PPDB. PPDB employs 1000 times more data (106 million sentence pairs with 2 billion words) than"
Q16-1029,P16-2024,1,0.58114,"stral, old, age-old, archeological, former, antiquated, longstanding, archaic, centuries-old, and so on. However, there is nothing inherent in the rule extraction process to say which of the PPDB paraphrases are simplifications. In this paper, we model the task by incorporating rich features into each rule and let SMT advances in decoding and optimization determine how well a rule simplifies an input phrase. An alternative way of using PPDB for simplification would be to simply discard any of its rules which did not result in a simplified output, possibly using a simple supervised classifier (Pavlick and Callison-Burch, 2016). 3.3 Simplification-specific Features for Paraphrase Rules Designing good features is an essential aspect of modeling. For each input sentence i and its candidate output sentence j, a vector of feature functions ϕ ~ = {ϕ1 ...ϕN } are combined with a weight vector w ~ in a linear model to obtain a single score hw~ : hw~ (i, j) = w ~ ·ϕ ~ (i, j) (8) In SMT, typical feature functions are phrase translation probabilities, word-for-word lexical translation probabilities, a rule application penalty (which governs whether the system prefers fewer longer phrases or a greater number of shorter phrases"
Q16-1029,W14-1210,0,0.0624395,"sentences, and randomly split them into 2000 sentences for tuning, 350 for evaluation. Many crowdsourcing workers were able to provide simplifications of good quality and diversity (see Table 2 5 We release the data with details for each feature. http://www.manythings.org/vocabulary/ lists/l/noll-about.php 6 for an example and Table 4 for the manual quality evaluation). Having multiple references allows us to develop automatic metrics similar to BLEU to take advantage of the variation across many people’s simplifications. We leave more in-depth investigations on crowdsourcing simplification (Pellow and Eskenazi, 2014a,b) for future work. 3.5 Tuning Parameters Like in statistical machine translation, we set the weights of the linear model w ~ in the Equation (8) so that the system’s output is optimized with respect to the automatic evaluation metric on the 2000 sentence development set. We use the pairwise ranking optimization (PRO) algorithm (Hopkins and May, 2011) implemented in the open-source Joshua toolkit (Ganitkevitch et al., 2012a; Post et al., 2013) for tuning. Specifically, we train the system to distinguish a good candidate output j from a bad candidate j 0 , measured by an objective function o"
Q16-1029,W13-2226,1,0.83823,"to take advantage of the variation across many people’s simplifications. We leave more in-depth investigations on crowdsourcing simplification (Pellow and Eskenazi, 2014a,b) for future work. 3.5 Tuning Parameters Like in statistical machine translation, we set the weights of the linear model w ~ in the Equation (8) so that the system’s output is optimized with respect to the automatic evaluation metric on the 2000 sentence development set. We use the pairwise ranking optimization (PRO) algorithm (Hopkins and May, 2011) implemented in the open-source Joshua toolkit (Ganitkevitch et al., 2012a; Post et al., 2013) for tuning. Specifically, we train the system to distinguish a good candidate output j from a bad candidate j 0 , measured by an objective function o (Section 3.1), for an input sentence i: o(i, j) >o(i, j 0 ) ⇐⇒ hw~ (i, j) > hw~ (i, j 0 ) ⇐⇒ hw~ (i, j) − hw~ (i, j 0 ) > 0 ⇐⇒ w ~ ·ϕ ~ (i, j) − w ~ ·ϕ ~ (i, j 0 ) > 0 (9) ⇐⇒ w ~ · (~ ϕ(i, j) − ϕ ~ (i, j 0 )) > 0 Thus, the optimization reduces to a binary classification problem. Each training instance is the difference vector ϕ ~ (i, j) − ϕ ~ (i, j 0 )) of a pair of candidates, and its training label is positive or negative depending on whether"
Q16-1029,D15-1044,0,0.0729444,"Missing"
Q16-1029,E14-1076,0,0.0314472,"text simplification is generating an ample set of rewrite rules that potentially simplify an input sentence. Most early work has relied on either hand-crafted rules (Chandrasekar et al., 1996; Carroll et al., 1999; Siddharthan, 2006; Vickrey and Koller, 2008) or dictionaries like WordNet (Devlin et al., 1999; Kaji et al., 2002; Inui et al., 2003). Other more recent studies have relied on 405 the parallel Normal-Simple Wikipedia Corpus to automatically extract rewrite rules (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011b; Wubben et al., 2012; Narayan and Gardent, 2014; Siddharthan and Angrosh, 2014; Angrosh et al., 2014). This technique does manage to learn a small number of transformations that simplify. However, we argue that because the size of the Normal-Simple Wikipedia parallel corpus is quite small (108k sentence pairs with 2 million words), the diversity and coverage of patterns that can be learned is actually quite limited. In this paper we will leverage the large-scale Paraphrase Database (PPDB)4 (Ganitkevitch et al., 2013; Pavlick et al., 2015) as a rich source of lexical, phrasal and syntactic simplification operations. It is created by extracting English paraphrases from bi"
Q16-1029,N10-1144,0,0.0389537,"first to design automatic metrics that are effective for tuning and evaluating simplification systems, which will facilitate iterative development for this task. 1 Introduction The goal of text simplification is to rewrite an input text so that the output is more readable. Text simplification has applications for reducing input complexity for natural language processing (Siddharthan et al., 2004; Miwa et al., 2010; Chen et al., 2012b) and providing reading aids for people with limited language skills (Petersen and Ostendorf, 2007; Watanabe et al., 2009; Allen, 2009; De Belder and Moens, 2010; Siddharthan and Katsos, 2010) or language impairments such as dyslexia (Rello et al., 2013), autism (Evans et al., 2014), and aphasia (Carroll et al., 1999). It is widely accepted that sentence simplification can be implemented by three major types of operations: splitting, deletion and paraphrasing (Feng, 2008). The splitting operation decomposes a long sentence into a sequence of shorter sentences. Deletion removes less important parts of a sentence. The paraphrasing operation includes reordering, lexical substitutions and syntactic transformations. While sentence splitting (Siddharthan, 2006; Petersen and Ostendorf, 20"
Q16-1029,C04-1129,0,0.0352311,"f statistical machine translation to perform text simplification, taking advantage of large-scale paraphrases learned from bilingual texts and a small amount of manual simplifications with multiple references. Our work is the first to design automatic metrics that are effective for tuning and evaluating simplification systems, which will facilitate iterative development for this task. 1 Introduction The goal of text simplification is to rewrite an input text so that the output is more readable. Text simplification has applications for reducing input complexity for natural language processing (Siddharthan et al., 2004; Miwa et al., 2010; Chen et al., 2012b) and providing reading aids for people with limited language skills (Petersen and Ostendorf, 2007; Watanabe et al., 2009; Allen, 2009; De Belder and Moens, 2010; Siddharthan and Katsos, 2010) or language impairments such as dyslexia (Rello et al., 2013), autism (Evans et al., 2014), and aphasia (Carroll et al., 1999). It is widely accepted that sentence simplification can be implemented by three major types of operations: splitting, deletion and paraphrasing (Feng, 2008). The splitting operation decomposes a long sentence into a sequence of shorter sente"
Q16-1029,S12-1046,0,0.0151082,"Missing"
Q16-1029,P15-2135,0,0.164802,"Missing"
Q16-1029,W14-1201,0,0.576387,"rules, which is important given the fact that Simple Wikipedia and the newly released Newsela simplification corpus (Xu et al., 2015) are only available for English. Second, previous evaluation used in the simplification literature is uninformative and not comparable across models due to the complications between the three different operations of paraphrasing, deletion, and splitting. This, combined with the unreliable quality of Simple Wikipedia as a gold reference for evaluation, has been the bottleneck for developing automatic metrics. There exist only a few studˇ ies (Wubben et al., 2012; Stajner et al., 2014) on automatic simplification evaluation using existing MT metrics which show limited correlation with human assessments. In this paper, we restrict ourselves to lexical simplification, where we believe MT-derived evaluation metrics can best be deployed. Our newly proposed metric is the first automatic metric that shows reasonable correlation with human evaluation on the text simplification task. We also introduce multiple references to make automatic evaluation feasible. The most related work to ours is that of Ganitkevitch et al. (2013) on sentence compression, in which compression of word an"
Q16-1029,N10-1056,0,0.0281531,"Missing"
Q16-1029,P12-2008,0,0.533776,"also related to other tunable metrics designed to be very simple and light-weight to ensure fast repeated computation for tuning bilingual translation models (Liu et al., 2010; Chen et al., 2012a). To the best of our knowledge, no tunable metric has been attempted for simplification, except for BLEU. Nor do any evaluation metrics exist for simplification, although there are several designed for other text-to-text generation tasks: grammatical error correction (Napoles et al., 2015; Felice and Briscoe, 2015; Dahlmeier and Ng, 2012), paraphrase generation (Chen and Dolan, 2011; Xu et al., 2012; Sun and Zhou, 2012), and conversation generation (Galley et al., 2015). Another line of related work is lexical simplification that focuses on finding simpler synonyms of a given complex word (Yatskar et al., 2010; Biran et al., 2011; Specia et al., 2012; Horn et al., 2014). 3 Adapting Machine Translation for Simplification We adapt the machinery of statistical machine translation to the task of text simplification by making changes in the following four key components: 3.1 Simplification-specific Objective Functions In the statistical machine translation framework, one crucial element is to design automatic eva"
Q16-1029,C10-1152,0,0.796098,"ave been intensively studied, there has been considerably less research on developing new paraphrasing models for text simplification — most previous work has used off-the-shelf statistical machine translation (SMT) technology and achieved reasonable results (Coster and Kauchak, 2011a,b; Wubben ˇ et al., 2012; Stajner et al., 2015). However, they have either treated the judgment technology as a black (Coster and Kauchak, 2011a,b; Narayan and Garˇ dent, 2014; Angrosh et al., 2014; Stajner et al., 2015) or they have been limited to modifying only one aspect of it, such as the translation model (Zhu et al., 2010; Woodsend and Lapata, 2011) or the reranking component (Wubben et al., 2012). In this paper, we present a complete adaptation of a syntax-based machine translation framework to perform simplification. Our methodology poses text simplification as a paraphrasing problem: given an input text, rewrite it subject to the constraints that the output should be simpler than the input, while preserving as much meaning of the input as possible, and maintaining the well-formedness of the text. Going beyond previous work, we make di401 Transactions of the Association for Computational Linguistics, vol. 4,"
Q16-1029,P08-1040,0,0.495345,"d3 Pdel (7) where d1 = d2 = d3 = 1/3 and Poperation = Roperation 1 k 1 = k Foperation = X poperation (n) n=[1,...,k] X roperation (n) n=[1,...,k] 2 × Poperation × Roperation Poperation + Roperation operation ∈ [del, keep, add] where k is the highest n-gram order and set to 4 in our experiments. 3.2 Incorporating Large-Scale Paraphrase Rules Another challenge for text simplification is generating an ample set of rewrite rules that potentially simplify an input sentence. Most early work has relied on either hand-crafted rules (Chandrasekar et al., 1996; Carroll et al., 1999; Siddharthan, 2006; Vickrey and Koller, 2008) or dictionaries like WordNet (Devlin et al., 1999; Kaji et al., 2002; Inui et al., 2003). Other more recent studies have relied on 405 the parallel Normal-Simple Wikipedia Corpus to automatically extract rewrite rules (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011b; Wubben et al., 2012; Narayan and Gardent, 2014; Siddharthan and Angrosh, 2014; Angrosh et al., 2014). This technique does manage to learn a small number of transformations that simplify. However, we argue that because the size of the Normal-Simple Wikipedia parallel corpus is quite small (108k sentence pair"
Q16-1029,W11-2160,1,0.839407,"can be learned is actually quite limited. In this paper we will leverage the large-scale Paraphrase Database (PPDB)4 (Ganitkevitch et al., 2013; Pavlick et al., 2015) as a rich source of lexical, phrasal and syntactic simplification operations. It is created by extracting English paraphrases from bilingual parallel corpora using a technique called “bilingual pivoting” (Bannard and Callison-Burch, 2005). The PPDB is represented as a synchronous context-free grammar (SCFG), which is commonly used as the formalism for syntax-based machine translation (Zollmann and Venugopal, 2006; Chiang, 2007; Weese et al., 2011). Table 1 shows some example paraphrase rules in the PPDB. PPDB employs 1000 times more data (106 million sentence pairs with 2 billion words) than the Normal-Simple Wikipedia parallel corpus. The English portion of PPDB contains over 220 million paraphrase rules, consisting of 8 million lexical, 73 million phrasal and 140 million syntactic para4 http://paraphrase.org phrase patterns. The key differences between the paraphrase rules from PPDB and the transformations learned by the naive application of SMT to the Normal-Simple Wikipedia parallel corpus, are that the PPDB paraphrases are much mo"
Q16-1029,D11-1038,0,0.156758,"ly studied, there has been considerably less research on developing new paraphrasing models for text simplification — most previous work has used off-the-shelf statistical machine translation (SMT) technology and achieved reasonable results (Coster and Kauchak, 2011a,b; Wubben ˇ et al., 2012; Stajner et al., 2015). However, they have either treated the judgment technology as a black (Coster and Kauchak, 2011a,b; Narayan and Garˇ dent, 2014; Angrosh et al., 2014; Stajner et al., 2015) or they have been limited to modifying only one aspect of it, such as the translation model (Zhu et al., 2010; Woodsend and Lapata, 2011) or the reranking component (Wubben et al., 2012). In this paper, we present a complete adaptation of a syntax-based machine translation framework to perform simplification. Our methodology poses text simplification as a paraphrasing problem: given an input text, rewrite it subject to the constraints that the output should be simpler than the input, while preserving as much meaning of the input as possible, and maintaining the well-formedness of the text. Going beyond previous work, we make di401 Transactions of the Association for Computational Linguistics, vol. 4, pp. 401–415, 2016. Action E"
Q16-1029,P12-1107,0,0.19806,"Missing"
Q16-1029,Q15-1021,1,0.646143,"ct modifications to four key components in the SMT pipeline:1 1) two novel simplification-specific tunable metrics; 2) large-scale paraphrase rules automatically derived from bilingual parallel corpora, which are more naturally and abundantly available than manually simplified texts; 3) rich rule-level simplification features; and 4) multiple reference simplifications collected via crowdsourcing for tuning and evaluation. In particular, we report the first study that shows promising correlations of automatic metrics with human evaluation. Our work answers the call made in a recent TACL paper (Xu et al., 2015) to address problems in current simplification research — we amend human evaluation criteria, develop automatic metrics, and generate an improved multiple reference dataset. Our work is primarily focused on lexical simplification (rewriting words or phrases with simpler versions), and to a lesser extent on syntactic rewrite rules that simplify the input. It largely ignores the important subtasks of sentence splitting and deletion. Our focus on lexical simplification does not affect the generality of the presented work, since deletion or sentence splitting could be applied as pre- or post-proce"
Q16-1029,C12-1177,1,0.164975,"mar. Our work is also related to other tunable metrics designed to be very simple and light-weight to ensure fast repeated computation for tuning bilingual translation models (Liu et al., 2010; Chen et al., 2012a). To the best of our knowledge, no tunable metric has been attempted for simplification, except for BLEU. Nor do any evaluation metrics exist for simplification, although there are several designed for other text-to-text generation tasks: grammatical error correction (Napoles et al., 2015; Felice and Briscoe, 2015; Dahlmeier and Ng, 2012), paraphrase generation (Chen and Dolan, 2011; Xu et al., 2012; Sun and Zhou, 2012), and conversation generation (Galley et al., 2015). Another line of related work is lexical simplification that focuses on finding simpler synonyms of a given complex word (Yatskar et al., 2010; Biran et al., 2011; Specia et al., 2012; Horn et al., 2014). 3 Adapting Machine Translation for Simplification We adapt the machinery of statistical machine translation to the task of text simplification by making changes in the following four key components: 3.1 Simplification-specific Objective Functions In the statistical machine translation framework, one crucial element is to"
Q16-1029,W06-3119,0,\N,Missing
Q16-1029,N15-1072,1,\N,Missing
Q19-1043,P14-1114,0,0.0265283,"xtual inference, the prevailing opinion is in favor of the latter (i.e., allowing semantic judgments to be probabilistic) with few (if any) advocating that we should build systems that only support discrete true/false decisions. Still, significant theoretical and algorithmic work has gone into making probabilistic logics work in practice. Such work includes (controversial) formalisms such as fuzzy set theory (Zadeh, 1994, 1996), as well as more generally accepted formalisms which assume access to boolean groundings, such as probabilistic soft logic (Friedman et al., 1999; Kimmig et al., 2012; Beltagy et al., 2014) and Markov logic networks (Richardson and Domingos, 2006). Also related is work on collecting and analyzing graded entailment judgments (de Marneffe et al., 2012). We note that the question of strict vs. graded entailment judgments pertains to modeling of uncertainty within an individual rater’s judgments. This is independent of the question of if/how to model disagreements between raters, which is the our focus in this work. Embracing Rater Disagreement. Significant past work has looked an annotator disagreement in linguistic annotations, and has advocated that this 689 disagreement should b"
Q19-1043,D15-1075,0,0.71781,"f the most fundamental of language understanding tasks, with Montague himself calling entailment ‘‘the basic aim of semantics’’ (Montague, 1970). Computational work on recognizing textual entailment (RTE) (also called natural language inference, or NLI) has a long history, ranging from early efforts to model logical phenomena (Cooper et al., 1996), to later statistical methods for modeling practical inferences needed for applications like information retrieval and extraction (Dagan et al., 2006), to current work on learning common sense human inferences from hundreds of thousands of examples (Bowman et al., 2015; Williams et al., 2018). Broadly speaking, the goal of the NLI task is to train models to make the inferences that a human would make. Currently, ‘‘the inferences that a human would make’’ are determined by asking multiple human raters to label pairs of sentences, • We perform a large-scale study of humans’ sentence-level inferences and measure the degree to which observed disagreements persist across samples of annotators. • We show that current state-of-the-art NLI systems do not capture this disagreement by default (by virtue of treating NLI as probabilistic) and argue that NLI evaluation"
Q19-1043,D08-1070,0,0.035536,"cknowledging that truly natural annotation is difficult) with the hope that models trained to perform such tasks will be the best match for the ‘‘real world’’ settings in which we hope to deploy them. That is, we generally prefer to punt on precise definitions, and instead train our models to ‘‘do what humans do’’. In this paper, we have shown that defining ‘‘what humans do’’ is not straightforward, as humans do not necessarily handle ambiguity or communicate uncertainty in the same way as one another. Thus, as was the case for pipelined systems (Zadrozny and Elkan, 2002; Finkel et al., 2006; Bunescu, 2008) and related discussions of model calibration (Kuleshov and Liang, 2015), we argue that the best approach is to propagate uncertainty downstream, so that end tasks can decide if and how to handle inferences on which humans are likely to disagree. From the point of view of current neural NLI models—and the sentence encoders on top of which they are built—this means that a representation should be evaluated in terms of its ability to predict the full distribution of human inferences (e.g., by reporting crossentropy against a distribution of human ratings), rather than to predict a single aggrega"
Q19-1043,W10-0701,0,0.0198229,"Missing"
Q19-1043,W17-7203,0,0.0127637,"ithub.io 688 Garoufi (2007) provides an overview of attempts that have been made to circumscribe the annotation process by providing finer-grained annotation options, in order to bring it more in line with the sentence-meaning task definition. Westera and Boleda (2019), in the context of advocating for distributional models of semantics in general, makes a case in favor of the speaker-meaning approach, arguing that issues like entailment, reference, and truth conditions should not fall within the purview of sentence meaning at all, despite being quintessential topics of formal semantic study. Chatzikyriakidis et al. (2017) overview NLI datasets, observing that datasets tend to be designed with one of these perspectives in mind, and thus all datasets ‘‘fail to capture the wealth of inferential mechanisms present in NLI and seem to be driven by the dominant discourse in the field at the time of their creation.’’ An orthogonal line of discussion about the definition of entailment focuses on the question of whether truth-conditional semantics should be strictly binary (propositions are either true or false) or rather treated as continuous/probabilistic values. Currently, at least within computationally minded work"
Q19-1043,W04-3205,0,0.304286,"Missing"
Q19-1043,C18-1152,0,0.0310109,"or work in computational semantics and language technology in general primarily because NLI has, historically (Cooper et al., 1996; Dagan et al., 2006) as well as presently (White et al., 2017), been proposed as a means for evaluating a model’s ‘‘intrinsic’’ understanding of language: As originally framed by Dagan et al. (2006), NLI was proposed as an intermediate task for evaluating whether a model will be useful in applications, and currently, NLI is increasingly used as a means for ‘‘probing’’ neural models to assess their knowledge of arbitrary linguistic phenomena (Dasgupta et al., 2018; Ettinger et al., 2018; Poliak et al., 2018b; White et al., 2017; Poliak et al., 2018a; McCoy et al., 2019). In other words, NLI has largely become an evaluation lingua franca through which we diagnose what a semantic representation knows. With the increased interest in ‘‘general-purpose’’, ‘‘task independent’’ semantic representations,13, 14 it is particularly important that intrinsic evaluations are reliable, if comparison of such representations are to be meaningful. As discussed, the preference among many in NLP (the authors included) is to avoid tasks which take a prescriptivist approach to language and meanin"
Q19-1043,W06-1673,0,0.0649022,"etting as possible (acknowledging that truly natural annotation is difficult) with the hope that models trained to perform such tasks will be the best match for the ‘‘real world’’ settings in which we hope to deploy them. That is, we generally prefer to punt on precise definitions, and instead train our models to ‘‘do what humans do’’. In this paper, we have shown that defining ‘‘what humans do’’ is not straightforward, as humans do not necessarily handle ambiguity or communicate uncertainty in the same way as one another. Thus, as was the case for pipelined systems (Zadrozny and Elkan, 2002; Finkel et al., 2006; Bunescu, 2008) and related discussions of model calibration (Kuleshov and Liang, 2015), we argue that the best approach is to propagate uncertainty downstream, so that end tasks can decide if and how to handle inferences on which humans are likely to disagree. From the point of view of current neural NLI models—and the sentence encoders on top of which they are built—this means that a representation should be evaluated in terms of its ability to predict the full distribution of human inferences (e.g., by reporting crossentropy against a distribution of human ratings), rather than to predict"
Q19-1043,N19-1423,0,0.0252416,"riginal labels (i.e., those in the published version of the data) and our discretized newly collected labels are given in the first column of Table 3. We note that measuring agreement and model accuracy in terms of these discrete distributions is not ideal, and it would be preferable to train the model to directly predict the full distributions, but because we do not have sufficient training data to do this (we only collected full distributions for 100 p/h pairs per dataset) we must work in terms of the discrete labels provided by the existing training datasets. Model. We use pretrained BERT (Devlin et al., 2019),12 fine-tuned on the training splits of the datasets from which our test data was drawn. That is, we fine-tune BERT five times, once on each dataset, and then test each model on the subset of our re-annotated p/h pairs that were drawn from 11 We also try removing JOCI from our analysis entirely, since it is the noisiest dataset, and still reach the same conclusions from our subsequent analysis. 12 https://github.com/google-research/bert Metrics. We want to quantify how well the model’s predicted softmax distribution captures the distribution over possible labels we see when we solicit judgmen"
Q19-1043,N19-1224,0,0.0561401,"ction for a POS tagger improves downstream performance. Similar approaches have been applied in parsing (Mart´ınez Alonso et al., 2015) and supersense tagging (Mart´ınez Alonso et al., 2016). Specifically relevant to this work is past discussion of disagreement on semantic annotation tasks, including anaphora resolution (Poesio and Artstein, 2005), coreference (Versley, 2008; Recasens et al., 2011), word sense disambiguation (Erk and McCarthy, 2009; Passonneau et al., 2012; Jurgens, 2013), veridicality (Geis and Zwicky, 1971; Karttunen et al., 2014; de Marneffe et al., 2012), semantic frames (Dumitrache et al., 2019), and grounding (Reidsma and op den Akker, 2008). Most of this work focuses on the uncertainty of individual raters, oftentimes concluding that such uncertainty can be addressed by shifting to a graded rather than discrete labeling schema and/or that uncertainty can be leveraged as a means for detecting inherently ambiguous items. In contrast, we do not look at measures of uncertainty/ambiguity from the point of view of an individual (though this is a very interesting question); rather, we focus on disagreements that exist between raters. We agree strongly that semantic judgments should be tre"
Q19-1043,C92-2082,0,0.171266,"p with a substitute w2 , where w2 has a known lexical semantic relationship to w1 . Specifically, we use as set of 300 word pairs: 100 hypernym/hyponym pairs, 100 antonym pairs, and 100 co-hyponym pairs. We chose these categories in order to ensure that our analysis consists of meaningful substitutions and that it covers a variety of types of inference judgments. Our hypernyms and antonyms are 683 taken from WordNet (Fellbaum, 1998), with hypernyms limited to first-sense immediate hypernyms. Our co-hyponyms are taken from an internal database, which we constructed by running Hearst patterns (Hearst, 1992) over a large text corpus. The 300 word pairs we used are available for inspection at https://github.com/epavlick/NLIvariation-data. After making the substitution, we score each candidate p and h with a language model (J´ozefowicz et al., 2016) and disregard pairs for which the perplexity of h is more than 5 points above that of p. This threshold was chosen based on manual inspection of a sample of the output, and is effective at removing sentences in which the substitution yielded a meaningless hypothesis—for example, by replacing a w1 that was part of a multiword expression. For each resulti"
Q19-1043,D09-1046,0,0.036836,"should be taken as signal rather than noise (Aroyo et al., 2018; Palomaki et al., 2018). Plank et al. (2014) showed that incorporating rater uncertainty into the loss function for a POS tagger improves downstream performance. Similar approaches have been applied in parsing (Mart´ınez Alonso et al., 2015) and supersense tagging (Mart´ınez Alonso et al., 2016). Specifically relevant to this work is past discussion of disagreement on semantic annotation tasks, including anaphora resolution (Poesio and Artstein, 2005), coreference (Versley, 2008; Recasens et al., 2011), word sense disambiguation (Erk and McCarthy, 2009; Passonneau et al., 2012; Jurgens, 2013), veridicality (Geis and Zwicky, 1971; Karttunen et al., 2014; de Marneffe et al., 2012), semantic frames (Dumitrache et al., 2019), and grounding (Reidsma and op den Akker, 2008). Most of this work focuses on the uncertainty of individual raters, oftentimes concluding that such uncertainty can be addressed by shifting to a graded rather than discrete labeling schema and/or that uncertainty can be leveraged as a means for detecting inherently ambiguous items. In contrast, we do not look at measures of uncertainty/ambiguity from the point of view of an i"
Q19-1043,J12-2003,0,0.0605579,"Missing"
Q19-1043,P16-1204,1,0.895677,"Missing"
Q19-1043,S16-2014,1,0.839677,"said, there has been a clear gravitation toward the latter, apparent in the widespread adoption of inference datasets that explicitly prioritize natural inferences over rigorous annotation guidelines (Bowman et al., 2015; Williams et al., 2018), and in the overall shift to the word ‘‘inference’’ over ‘‘entailment.’’ There has also been significant empirical evidence supporting the argument that humans’ semantic inferences are uncertain and context-sensitive (Poesio and Artstein, 2005; Versley, 2008; Simons et al., 2010; Recasens et al., 2011; de Marneffe et al., 2012; Passonneau et al., 2012; Pavlick and Callison-Burch, 2016a,b; Tonhauser et al., 2018, among others) suggesting computational models would benefit from focusing on ‘‘speaker meaning’’ over ‘‘sentence meaning’’ when it comes to NLI (Manning, 2006; Westera and Boleda, 2019). Thus, in this paper, we assume that NLP will maintain this hands-off approach to NLI, avoiding definitions of what inferences humans should make or which types of knowledge they should invoke. We take the position that, ultimately, our Figure 1: Example p/h pair on which humans exhibit strong disagreements about whether h can be inferred from p. Here, the disagreement appears to st"
Q19-1043,W16-1706,0,0.0892585,"the question of if/how to model disagreements between raters, which is the our focus in this work. Embracing Rater Disagreement. Significant past work has looked an annotator disagreement in linguistic annotations, and has advocated that this 689 disagreement should be taken as signal rather than noise (Aroyo et al., 2018; Palomaki et al., 2018). Plank et al. (2014) showed that incorporating rater uncertainty into the loss function for a POS tagger improves downstream performance. Similar approaches have been applied in parsing (Mart´ınez Alonso et al., 2015) and supersense tagging (Mart´ınez Alonso et al., 2016). Specifically relevant to this work is past discussion of disagreement on semantic annotation tasks, including anaphora resolution (Poesio and Artstein, 2005), coreference (Versley, 2008; Recasens et al., 2011), word sense disambiguation (Erk and McCarthy, 2009; Passonneau et al., 2012; Jurgens, 2013), veridicality (Geis and Zwicky, 1971; Karttunen et al., 2014; de Marneffe et al., 2012), semantic frames (Dumitrache et al., 2019), and grounding (Reidsma and op den Akker, 2008). Most of this work focuses on the uncertainty of individual raters, oftentimes concluding that such uncertainty can b"
Q19-1043,E14-1078,0,0.253805,"on collecting and analyzing graded entailment judgments (de Marneffe et al., 2012). We note that the question of strict vs. graded entailment judgments pertains to modeling of uncertainty within an individual rater’s judgments. This is independent of the question of if/how to model disagreements between raters, which is the our focus in this work. Embracing Rater Disagreement. Significant past work has looked an annotator disagreement in linguistic annotations, and has advocated that this 689 disagreement should be taken as signal rather than noise (Aroyo et al., 2018; Palomaki et al., 2018). Plank et al. (2014) showed that incorporating rater uncertainty into the loss function for a POS tagger improves downstream performance. Similar approaches have been applied in parsing (Mart´ınez Alonso et al., 2015) and supersense tagging (Mart´ınez Alonso et al., 2016). Specifically relevant to this work is past discussion of disagreement on semantic annotation tasks, including anaphora resolution (Poesio and Artstein, 2005), coreference (Versley, 2008; Recasens et al., 2011), word sense disambiguation (Erk and McCarthy, 2009; Passonneau et al., 2012; Jurgens, 2013), veridicality (Geis and Zwicky, 1971; Karttu"
Q19-1043,N15-1152,0,0.0574694,"n individual rater’s judgments. This is independent of the question of if/how to model disagreements between raters, which is the our focus in this work. Embracing Rater Disagreement. Significant past work has looked an annotator disagreement in linguistic annotations, and has advocated that this 689 disagreement should be taken as signal rather than noise (Aroyo et al., 2018; Palomaki et al., 2018). Plank et al. (2014) showed that incorporating rater uncertainty into the loss function for a POS tagger improves downstream performance. Similar approaches have been applied in parsing (Mart´ınez Alonso et al., 2015) and supersense tagging (Mart´ınez Alonso et al., 2016). Specifically relevant to this work is past discussion of disagreement on semantic annotation tasks, including anaphora resolution (Poesio and Artstein, 2005), coreference (Versley, 2008; Recasens et al., 2011), word sense disambiguation (Erk and McCarthy, 2009; Passonneau et al., 2012; Jurgens, 2013), veridicality (Geis and Zwicky, 1971; Karttunen et al., 2014; de Marneffe et al., 2012), semantic frames (Dumitrache et al., 2019), and grounding (Reidsma and op den Akker, 2008). Most of this work focuses on the uncertainty of individual ra"
Q19-1043,P19-1334,1,0.854204,"NLI has, historically (Cooper et al., 1996; Dagan et al., 2006) as well as presently (White et al., 2017), been proposed as a means for evaluating a model’s ‘‘intrinsic’’ understanding of language: As originally framed by Dagan et al. (2006), NLI was proposed as an intermediate task for evaluating whether a model will be useful in applications, and currently, NLI is increasingly used as a means for ‘‘probing’’ neural models to assess their knowledge of arbitrary linguistic phenomena (Dasgupta et al., 2018; Ettinger et al., 2018; Poliak et al., 2018b; White et al., 2017; Poliak et al., 2018a; McCoy et al., 2019). In other words, NLI has largely become an evaluation lingua franca through which we diagnose what a semantic representation knows. With the increased interest in ‘‘general-purpose’’, ‘‘task independent’’ semantic representations,13, 14 it is particularly important that intrinsic evaluations are reliable, if comparison of such representations are to be meaningful. As discussed, the preference among many in NLP (the authors included) is to avoid tasks which take a prescriptivist approach to language and meaning. Instead, we attempt to design tasks which capture humans’ linguistic behavior in a"
Q19-1043,W05-0311,0,0.840461,"roaches offers the better cost–benefit tradeoff: precise (at risk of being impractical), or organic (at risk of being ill-defined). That said, there has been a clear gravitation toward the latter, apparent in the widespread adoption of inference datasets that explicitly prioritize natural inferences over rigorous annotation guidelines (Bowman et al., 2015; Williams et al., 2018), and in the overall shift to the word ‘‘inference’’ over ‘‘entailment.’’ There has also been significant empirical evidence supporting the argument that humans’ semantic inferences are uncertain and context-sensitive (Poesio and Artstein, 2005; Versley, 2008; Simons et al., 2010; Recasens et al., 2011; de Marneffe et al., 2012; Passonneau et al., 2012; Pavlick and Callison-Burch, 2016a,b; Tonhauser et al., 2018, among others) suggesting computational models would benefit from focusing on ‘‘speaker meaning’’ over ‘‘sentence meaning’’ when it comes to NLI (Manning, 2006; Westera and Boleda, 2019). Thus, in this paper, we assume that NLP will maintain this hands-off approach to NLI, avoiding definitions of what inferences humans should make or which types of knowledge they should invoke. We take the position that, ultimately, our Figu"
Q19-1043,N19-1176,0,0.0591386,"this paper could serve as a start towards this end, a larger effort to augment or replace existing evaluation sets with full distributions of judgments would be necessary in order to yield a meaningful redefinition of the NLI task. Second, changes would be required to enable models to learn to predict these distributions. One approach could be to annotate training data, not just evaluation data, with full distributions, and optimize for the objective directly. This would clearly incur additional costs, but could be overcome with more creative crowdsourcing techniques (Dumitrache et al., 2013; Poesio et al., 2019). However, requiring direct supervision of full distributions is arguably an unsatisfying solution: Rarely if ever do humans witness multiple people responding to identical stimuli. Rather, more plausibly, we form generalizations about the linguistic phenomena that give rise to uncertainty on the basis of a large number of singly labeled examples. Thus, ideally, progress can be made by developing new architectures and/or training objectives that enable models to learn a notion of uncertainty that is consistent with the full range of possible human inferences, despite observing labels from only"
Q19-1043,N18-2082,0,0.0626201,"Missing"
Q19-1043,W18-5441,1,0.902888,"Missing"
Q19-1043,D08-1027,0,0.419445,"Missing"
Q19-1043,W19-0410,0,0.142397,"Williams et al., 2018), and in the overall shift to the word ‘‘inference’’ over ‘‘entailment.’’ There has also been significant empirical evidence supporting the argument that humans’ semantic inferences are uncertain and context-sensitive (Poesio and Artstein, 2005; Versley, 2008; Simons et al., 2010; Recasens et al., 2011; de Marneffe et al., 2012; Passonneau et al., 2012; Pavlick and Callison-Burch, 2016a,b; Tonhauser et al., 2018, among others) suggesting computational models would benefit from focusing on ‘‘speaker meaning’’ over ‘‘sentence meaning’’ when it comes to NLI (Manning, 2006; Westera and Boleda, 2019). Thus, in this paper, we assume that NLP will maintain this hands-off approach to NLI, avoiding definitions of what inferences humans should make or which types of knowledge they should invoke. We take the position that, ultimately, our Figure 1: Example p/h pair on which humans exhibit strong disagreements about whether h can be inferred from p. Here, the disagreement appears to stem from the implicature, but we observe similar disagreements on a variety of linguistic phenomena. 2 The RTE/NLI Task The task of RTE/NLI is fundamentally concerned with drawing conclusions about the world on the"
Q19-1043,W08-1203,0,0.181305,"Missing"
Q19-1043,I17-1100,0,0.0754596,"Missing"
Q19-1043,N18-1101,0,0.586442,"l of language understanding tasks, with Montague himself calling entailment ‘‘the basic aim of semantics’’ (Montague, 1970). Computational work on recognizing textual entailment (RTE) (also called natural language inference, or NLI) has a long history, ranging from early efforts to model logical phenomena (Cooper et al., 1996), to later statistical methods for modeling practical inferences needed for applications like information retrieval and extraction (Dagan et al., 2006), to current work on learning common sense human inferences from hundreds of thousands of examples (Bowman et al., 2015; Williams et al., 2018). Broadly speaking, the goal of the NLI task is to train models to make the inferences that a human would make. Currently, ‘‘the inferences that a human would make’’ are determined by asking multiple human raters to label pairs of sentences, • We perform a large-scale study of humans’ sentence-level inferences and measure the degree to which observed disagreements persist across samples of annotators. • We show that current state-of-the-art NLI systems do not capture this disagreement by default (by virtue of treating NLI as probabilistic) and argue that NLI evaluation should explicitly incent"
Q19-1043,W05-1206,0,0.430809,"dict distributions over human judgments. • We discuss our results with respect to the definition of the NLI task, and its increased usage as a diagnostic task for evaluating ‘‘general purpose’’ representations of natural language. 677 Transactions of the Association for Computational Linguistics, vol. 7, pp. 677–694, 2019. https://doi.org/10.1162/tacl a 00293 Action Editor: Christopher Potts. Submission batch: 5/2019; Published: 11/2019. c 2019 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.  surrounded the original definition of the RTE task. In particular, Zaenen et al. (2005) argued that the definition needed to be made more precise, so as to circumscribe the extent to which ‘‘world knowledge’’ should be allowed to factor into inferences, and to explicitly differentiate between distinct forms of textual inference (e.g., entailment vs. conventional implicature vs. conversational implicature). Manning (2006) made a counterargument, pushing back against a prescriptivist definition of what types of inferences are or are not licensed in a specific context, instead advocating that annotation tasks should be ‘‘natural’’ for untrained annotators, and that the role of NLP"
Q19-1043,W07-1401,0,\N,Missing
Q19-1043,Q17-1027,0,\N,Missing
Q19-1043,D18-1007,1,\N,Missing
S16-2014,W06-1805,0,0.822311,"as led to the generalization that the deletion of non-subsective adjectives tends to result in contradictory utterances: Moussaoui is a would-be hijacker entails that it is not the case that Moussaoui is a hijacker. This generalization has prompted normative rules for the treatment of such adjectives in various NLP tasks. In information extraction, it is assumed that systems cannot extract useful rules from sentences containing non-subsective modifiers (Angeli et al., 2015), and in RTE, it is assumed that systems should uniformly penalize insertions and deletions of non-subsective adjectives (Amoia and Gardent, 2006). 1. (a) U.S. District Judge Leonie Brinkema accepted would-be hijacker Zacarias Moussaoui’s guilty pleas . . . (b) Moussaoui participated in the Sept. 11 attacks. 114 Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics (*SEM 2016), pages 114–119, Berlin, Germany, August 11-12, 2016. Privative (e.g. fake) N AN Plain Non-subsective (e.g. alleged) N AN Subsective (e.g. red) AN Figure 1: Three main classes of adjectives. If their entailment behavior is consistent with their theoretical definitions, we would expect our annotations (Section 3) to produce the insertion ("
S16-2014,W07-1430,0,0.70578,"d and Related Work Classes of Adjectives. Adjectives are commonly classified taxonomically as either subsective or non-subsective (Kamp and Partee, 1995). Subsective adjectives are adjectives which pick out a subset of the set denoted by the unmodified noun; that is, AN ⊂ N1 . For non-subsective adjectives, in contrast, the AN cannot be guaranteed to be a subset of N. For example, clever is subsective, and so a clever thief is always a thief. However, While the hierarchical classification of adjectives described above is widely accepted and often applied in NLP tasks (Amoia and Gardent, 2006; Amoia and Gardent, 2007; Boleda et al., 2012; McCrae et al., 2014), it is not undisputed. Some linguists take the position that in fact privative ad1 We use the notation N and AN to refer both the the natural language expression itself (e.g. red car) as well as its denotation, e.g. {x|x is a red car}. 115 jectives are simply another type of subsective adjective (Partee, 2003; McNally and Boleda, 2004; Abdullah and Frost, 2005; Partee, 2007). Advocates of this theory argue that the denotation of the noun should be expanded to include both the properties captured by the privative adjectives as well as those captured b"
S16-2014,P15-1034,0,0.130864,"er is not a hijacker. The observation that adjective-nouns (ANs) involving non-subsective adjectives do not entail the underlying nouns (Ns) has led to the generalization that the deletion of non-subsective adjectives tends to result in contradictory utterances: Moussaoui is a would-be hijacker entails that it is not the case that Moussaoui is a hijacker. This generalization has prompted normative rules for the treatment of such adjectives in various NLP tasks. In information extraction, it is assumed that systems cannot extract useful rules from sentences containing non-subsective modifiers (Angeli et al., 2015), and in RTE, it is assumed that systems should uniformly penalize insertions and deletions of non-subsective adjectives (Amoia and Gardent, 2006). 1. (a) U.S. District Judge Leonie Brinkema accepted would-be hijacker Zacarias Moussaoui’s guilty pleas . . . (b) Moussaoui participated in the Sept. 11 attacks. 114 Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics (*SEM 2016), pages 114–119, Berlin, Germany, August 11-12, 2016. Privative (e.g. fake) N AN Plain Non-subsective (e.g. alleged) N AN Subsective (e.g. red) AN Figure 1: Three main classes of adjectives. If"
S16-2014,W12-3018,0,0.0278809,"Missing"
S16-2014,W15-0103,0,0.0313077,"meaning (e.g. the functional features of the noun) without modifying the extension of the noun (Del Pinal, 2015). Such an analysis can explain how we can understand a fake gun as having many, but not all, of the properties of a gun. Several other studies abandon the attempt to organize adjectives taxonomically, and instead focus on the properties of the modified noun. Nayak et al. (2014) categorize non-subsective adjectives in terms of the proportion of properties that are shared between the N and the AN and Pustejovsky (2013) focus on syntactic cues about exactly which properties are shared. Bakhshandh and Allen (2015) analyze adjectives by observing that, e.g., red modifies color while tall modifies size. In Section 5, we discuss the potential benefits of pursuing these property-based analyses in relation to our experimental findings. through the Annotated Gigaword corpus (Napoles et al., 2012) for occurrences of each adjective in the list, restricting to cases in which the adjective appears as an adjective modifier of (is in an amod dependency relation with) a common noun (NN). For each adjective, we choose 10 sentences such that the adjective modifies a different noun in each. As a control, we take a sma"
S16-2014,D12-1112,0,0.0303058,"s of Adjectives. Adjectives are commonly classified taxonomically as either subsective or non-subsective (Kamp and Partee, 1995). Subsective adjectives are adjectives which pick out a subset of the set denoted by the unmodified noun; that is, AN ⊂ N1 . For non-subsective adjectives, in contrast, the AN cannot be guaranteed to be a subset of N. For example, clever is subsective, and so a clever thief is always a thief. However, While the hierarchical classification of adjectives described above is widely accepted and often applied in NLP tasks (Amoia and Gardent, 2006; Amoia and Gardent, 2007; Boleda et al., 2012; McCrae et al., 2014), it is not undisputed. Some linguists take the position that in fact privative ad1 We use the notation N and AN to refer both the the natural language expression itself (e.g. red car) as well as its denotation, e.g. {x|x is a red car}. 115 jectives are simply another type of subsective adjective (Partee, 2003; McNally and Boleda, 2004; Abdullah and Frost, 2005; Partee, 2007). Advocates of this theory argue that the denotation of the noun should be expanded to include both the properties captured by the privative adjectives as well as those captured by the subsective adje"
S16-2014,P16-1204,1,0.889631,"Missing"
S16-2014,W13-0509,0,0.0169333,"ve a “dual semantic structure” and that non-subsective adjectives modify part of this meaning (e.g. the functional features of the noun) without modifying the extension of the noun (Del Pinal, 2015). Such an analysis can explain how we can understand a fake gun as having many, but not all, of the properties of a gun. Several other studies abandon the attempt to organize adjectives taxonomically, and instead focus on the properties of the modified noun. Nayak et al. (2014) categorize non-subsective adjectives in terms of the proportion of properties that are shared between the N and the AN and Pustejovsky (2013) focus on syntactic cues about exactly which properties are shared. Bakhshandh and Allen (2015) analyze adjectives by observing that, e.g., red modifies color while tall modifies size. In Section 5, we discuss the potential benefits of pursuing these property-based analyses in relation to our experimental findings. through the Annotated Gigaword corpus (Napoles et al., 2012) for occurrences of each adjective in the list, restricting to cases in which the adjective appears as an adjective modifier of (is in an amod dependency relation with) a common noun (NN). For each adjective, we choose 10 s"
S16-2014,W07-1401,0,0.119521,"an instance of an adjective-noun phrase is an instance of the noun: a red car is a car and a successful senator is a senator. In contrast, adjective-noun phrases involving non-subsective adjectives, such as imaginary and former (Table 1), denote a set that is disjoint from the denotation of the nouns they modify: an imaginary car is not a car and a former senator is not a senator. Understanding whether or not adjectives are subsective is critical in any task involving natural language inference. For example, consider the below sentence pair from the Recognizing Textual Entailment (RTE) task (Giampiccolo et al., 2007): In this example, recognizing that 1(a) does not entail 1(b) hinges on understanding that a would-be hijacker is not a hijacker. The observation that adjective-nouns (ANs) involving non-subsective adjectives do not entail the underlying nouns (Ns) has led to the generalization that the deletion of non-subsective adjectives tends to result in contradictory utterances: Moussaoui is a would-be hijacker entails that it is not the case that Moussaoui is a hijacker. This generalization has prompted normative rules for the treatment of such adjectives in various NLP tasks. In information extraction,"
S16-2014,W14-4724,0,0.116189,"ctives are commonly classified taxonomically as either subsective or non-subsective (Kamp and Partee, 1995). Subsective adjectives are adjectives which pick out a subset of the set denoted by the unmodified noun; that is, AN ⊂ N1 . For non-subsective adjectives, in contrast, the AN cannot be guaranteed to be a subset of N. For example, clever is subsective, and so a clever thief is always a thief. However, While the hierarchical classification of adjectives described above is widely accepted and often applied in NLP tasks (Amoia and Gardent, 2006; Amoia and Gardent, 2007; Boleda et al., 2012; McCrae et al., 2014), it is not undisputed. Some linguists take the position that in fact privative ad1 We use the notation N and AN to refer both the the natural language expression itself (e.g. red car) as well as its denotation, e.g. {x|x is a red car}. 115 jectives are simply another type of subsective adjective (Partee, 2003; McNally and Boleda, 2004; Abdullah and Frost, 2005; Partee, 2007). Advocates of this theory argue that the denotation of the noun should be expanded to include both the properties captured by the privative adjectives as well as those captured by the subsective adjectives. This expanded"
S19-1026,J99-2004,0,0.0608644,"Missing"
S19-1026,W15-2712,0,0.0230209,"cture constant. We ask whether the linguistic properties implicitly captured by pretraining objectives measurably affect the types of linguistic information encoded in the learned representations. To this end, we explore whether qualitatively different objectives lead to demonstrably different sentence representations. We focus our analysis on function words because they play a key role in compositional meaning—e.g., introducing and identifying discourse referents or representing relationships between entities or ideas—and are not yet considered to be well-modeled by distributional semantics (Bernardi et al., 2015). Our results suggest that different pretraining objectives give rise to differences in function word comprehension; for instance, we see that natural language inference helps understanding negation, and CCG supertagging helps recognizing meaningful sentence boundaries. However, overall, we find that the observed differences are not always straightforwardly interpretable, and further investigation is needed to determine which specific aspects of pretraining tasks yield good representations of function words. The analyses we present contribute new results in an ongoing line of research aimed at"
S19-1026,N19-1423,0,0.201655,"n of negation. 1 Introduction Many recent advances in NLP have been driven by new approaches to representation learning— i.e., the design of models whose primary aim is to yield representations of words or sentences that are useful for a range of downstream applications (Bowman et al., 2017). Approaches to representation learning typically differ in the architecture of the model used to learn the representations, the objective used to train that network, or both. Varying these factors can significantly impact performance on a broad range of NLP tasks (McCann et al., 2017; Peters et al., 2018; Devlin et al., 2019). This paper investigates the role of pretraining objectives of sentence encoders, with respect to ∗ Corresponding authors: Najoung Kim (n.kim@jhu.edu), Ellie Pavlick (ellie pavlick@brown.edu) their capacity to understand function words (e.g., prepositions, conjunctions). Although the importance of finding an effective pretraining objective for learning better (or more generalizable) representations is well acknowledged, relatively few studies offer a controlled comparison of diverse pretraining objectives, holding model architecture constant. We ask whether the linguistic properties implicitl"
S19-1026,W16-2524,0,0.0505528,"aining and probing datasets. A regression analysis shows that vocabulary overlap overall is not a significant predictor of performance on the probing set (p = 0.39). No single probing set performance was significantly affected by vocabulary overlap either (all p > .05 after Bonferroni correction for multiple comparisons). Figure 2: Prediction overlap on the probing tasks for models trained on different pretraining tasks (i.e., how often models make identical predictions on a particular probing set). 5 Related Work An active line of work focuses on “probing” neural representations of language. Ettinger et al. (2016, 2017); Zhu et al. (2018), i.a., use a task-based approach similar to ours, where tasks that require a specific subset of linguistic knowledge are used to perform qualitative evaluation. Gulordava et al. (2018), Giulianelli et al. (2018), Rønning et al. (2018), and Jumelet and Hupkes (2018) make a focused contribution towards a particular linguistic phenomenon (agreement, ellipsis, negative polarity). Using recast NLI, Poliak et al. (2018a) probe for semantic phenomena in neural machine translation encoders. Stali¯unaite and Bonfil (2017); Mahler et al. (2017); Ribeiro et al. (2018) use simil"
S19-1026,W17-5401,0,0.0333637,"Missing"
S19-1026,W18-5426,0,0.050884,"bulary overlap either (all p > .05 after Bonferroni correction for multiple comparisons). Figure 2: Prediction overlap on the probing tasks for models trained on different pretraining tasks (i.e., how often models make identical predictions on a particular probing set). 5 Related Work An active line of work focuses on “probing” neural representations of language. Ettinger et al. (2016, 2017); Zhu et al. (2018), i.a., use a task-based approach similar to ours, where tasks that require a specific subset of linguistic knowledge are used to perform qualitative evaluation. Gulordava et al. (2018), Giulianelli et al. (2018), Rønning et al. (2018), and Jumelet and Hupkes (2018) make a focused contribution towards a particular linguistic phenomenon (agreement, ellipsis, negative polarity). Using recast NLI, Poliak et al. (2018a) probe for semantic phenomena in neural machine translation encoders. Stali¯unaite and Bonfil (2017); Mahler et al. (2017); Ribeiro et al. (2018) use similar strategies to our structural mutation method, although their primary goal was to break existing systems by adversarial modifications rather than to compare different models. Ribeiro et al. (2018) and our work both test for proper compr"
S19-1026,N18-1038,0,0.0609676,"Missing"
S19-1026,W17-5405,0,0.026707,"Missing"
S19-1026,C18-1198,0,0.038925,"probe for semantic phenomena in neural machine translation encoders. Stali¯unaite and Bonfil (2017); Mahler et al. (2017); Ribeiro et al. (2018) use similar strategies to our structural mutation method, although their primary goal was to break existing systems by adversarial modifications rather than to compare different models. Ribeiro et al. (2018) and our work both test for proper comprehension of the modified expressions, but our modifications are designed to induce semantic changes whereas their modifications are intended to preserve the original meaning. Our strategy is close to that of Naik et al. (2018), but our modifications are more constrained and lexically targeted. The design of our NLI-style probing tasks follows the recent line of work which advocates for NLI as a general-purpose format for diagnostic tasks (White et al., 2017; Poliak et al., 2018b). This idea is similar in spirit to McCann et al. (2018), which advocates for question answering as a general-purpose format, to edge probing (Tenney et al., 2019) which probes for syntactic and semantic structures via a common labeling format, and to GLUE (Wang et al., 2018) which aggregates a variety of tasks that share a common sentencec"
S19-1026,N18-1202,0,0.406307,"elps the comprehension of negation. 1 Introduction Many recent advances in NLP have been driven by new approaches to representation learning— i.e., the design of models whose primary aim is to yield representations of words or sentences that are useful for a range of downstream applications (Bowman et al., 2017). Approaches to representation learning typically differ in the architecture of the model used to learn the representations, the objective used to train that network, or both. Varying these factors can significantly impact performance on a broad range of NLP tasks (McCann et al., 2017; Peters et al., 2018; Devlin et al., 2019). This paper investigates the role of pretraining objectives of sentence encoders, with respect to ∗ Corresponding authors: Najoung Kim (n.kim@jhu.edu), Ellie Pavlick (ellie pavlick@brown.edu) their capacity to understand function words (e.g., prepositions, conjunctions). Although the importance of finding an effective pretraining objective for learning better (or more generalizable) representations is well acknowledged, relatively few studies offer a controlled comparison of diverse pretraining objectives, holding model architecture constant. We ask whether the linguisti"
S19-1026,N18-2082,1,0.879934,"Missing"
S19-1026,W18-5441,1,0.897134,"Missing"
S19-1026,P18-1079,0,0.0486921,"language. Ettinger et al. (2016, 2017); Zhu et al. (2018), i.a., use a task-based approach similar to ours, where tasks that require a specific subset of linguistic knowledge are used to perform qualitative evaluation. Gulordava et al. (2018), Giulianelli et al. (2018), Rønning et al. (2018), and Jumelet and Hupkes (2018) make a focused contribution towards a particular linguistic phenomenon (agreement, ellipsis, negative polarity). Using recast NLI, Poliak et al. (2018a) probe for semantic phenomena in neural machine translation encoders. Stali¯unaite and Bonfil (2017); Mahler et al. (2017); Ribeiro et al. (2018) use similar strategies to our structural mutation method, although their primary goal was to break existing systems by adversarial modifications rather than to compare different models. Ribeiro et al. (2018) and our work both test for proper comprehension of the modified expressions, but our modifications are designed to induce semantic changes whereas their modifications are intended to preserve the original meaning. Our strategy is close to that of Naik et al. (2018), but our modifications are more constrained and lexically targeted. The design of our NLI-style probing tasks follows the rec"
S19-1026,W18-5409,0,0.0502653,"Missing"
S19-1026,P18-1018,0,0.0235763,"s none of them correctly (0/6). Here is an example of a numeric usage of below that the NLI model answered correctly but the image model answered incorrectly: P: Only those whose incomes do not exceed 125 percent of the federal poverty level qualify . . . H: Those whose incomes are below 125 percent qualify . . . (P→H) The image model’s bias towards the spatial usage is intuitive, since the numeric usage of below (i.e., as a counterpart to exceed) is difficult to learn from visual clues only. This concrete-abstract duality, which is not specific to below but common to most other prepositions (Schneider et al., 2018), may partially explain why the image-caption model behaves so differently from all other models, which are not trained on a multimodal objective. 4.3 Data Size and Genre Effects As can be seen from the varying sizes of the pretraining dataset reported in Figure 1, seeing more data during pretraining does not imply better performance on probing tasks. Also, as noted before, the fact that pretraining can hurt performance suggests that if the task is not the “right” task, adding more datapoints during pretraining can lead models to learn counterproductive representations. Another potential confo"
S19-1026,W17-5410,0,0.0296726,"Missing"
S19-1026,W17-2625,0,0.063243,"Missing"
S19-1026,W18-5446,1,0.80362,"o preserve the original meaning. Our strategy is close to that of Naik et al. (2018), but our modifications are more constrained and lexically targeted. The design of our NLI-style probing tasks follows the recent line of work which advocates for NLI as a general-purpose format for diagnostic tasks (White et al., 2017; Poliak et al., 2018b). This idea is similar in spirit to McCann et al. (2018), which advocates for question answering as a general-purpose format, to edge probing (Tenney et al., 2019) which probes for syntactic and semantic structures via a common labeling format, and to GLUE (Wang et al., 2018) which aggregates a variety of tasks that share a common sentenceclassification format. The primary difference in our work is that we focus specifically on the understanding of function words in context. We also present a suite of several tasks, but each one focuses on a particular structure, whereas tasks proposed in the works above generally aggregate multiple phenomena. Each of our tasks isolates each function word type and employ a targeted modification strategy that gives us a more narrowlyfocused, informative scope of analysis. 6 Conclusion We propose a new challenge set of nine tasks th"
S19-1026,I17-1100,1,0.878632,"Missing"
S19-1026,N18-1101,1,0.788304,"dex (σ = 2) and rounding to the nearest integer. 2.3 NLI-Based Tasks Our NLI-based probing tasks ask whether the choice of function word affects the inferences licensed by a sentence. These tasks consist of a pair of sentences—a premise p and a hypothesis h— and ask whether or not p entails h. We exploit the label changes induced by a targeted mutation of 3 We use WikiText instead of BWB because adjacent sentences in BWB are not logically contiguous and therefore may not be from the same discourse context. the sentence pairs taken from the Multi-genre Natural Language Inference dataset (MNLI, Williams et al., 2018). The rationale is that, if a change to a single function word in the premise changes the entailment label, that function word must play a significant role in the semantics of the sentence. Prepositions We manually curate a list of prepositions (see Appendix D) that are likely to be swapped with each other without affecting the grammaticality of the sentence. We generate mutated NLI pairs by finding occurrences of the prepositions in our list and randomly replacing them with other prepositions in the list. Our list consists of a set of locatives4 and several other manually-selected preposition"
S19-1026,Q17-1027,1,0.86489,"Missing"
S19-1026,P18-2100,0,0.0613273,"Missing"
W15-2614,P14-5010,0,0.0132468,"Missing"
W15-2614,D13-1170,0,0.00559072,"Missing"
W15-2614,W14-4907,0,0.0607018,"Missing"
W15-2614,D08-1027,0,0.167875,"Missing"
W15-2614,W09-1904,0,0.0255801,"collect crowdsourced annotations at scale (Khare et al., 2015; Wang et al., 2013). In some NLP problems, the annotation task requires some degree of common linguistic knowledge that most non-experts are assumed to have. By examining the accuracy of crowdsourced data and its usefulness in training models to perform common NLP tasks, previous research has shown that deficiencies in individual crowd worker accuracy can be overcome by taking consensus votes over multiple annotators or weighting the votes of annotators based on their overall performance (MacLean and Heer, 2013; Zhai et al., 2013; Hsueh et al., 2009; Snow et al., 2008). • The mastoid air cells are well-pneumatized. (mastoid) • Bilateral dysplastic vestibules and lateral semicircular canals. (semicircular canal) • The external auditory canal is patent. (EAC) Labeling some of these sentences might require a non-expert to do additional research. (e.g. Should a mastoid air cell be pneumatized? Does lateral describe the condition of the semicircular canal, or is lateral semicircular canal a compound noun?) In this work, we extend the study of crowdsourcing annotations to text-labeling tasks that require domain knowledge. Specifically, we exam"
W15-2614,H89-2078,0,0.612212,"Missing"
W18-5441,D15-1075,0,0.450763,"lation Ward was born in Perth Extraction I Stefan had visited his son in Bulgaria Stefan was born in Bulgaria Puns I Kim heard masks have no face value Kim heard a pun I Tod heard that thrift is better than annuity Tod heard a pun 3 7 3 7 3 7 Table 1: Example sentence pairs for different semantic phe1 nomena. I indicates the line is a context and the following line is its corresponding hypothesis. 3 and 7 respectively indicate that the context entails, or does not entail the hypothesis. Introduction A plethora of new natural language inference (NLI)1 datasets has been created in recent years (Bowman et al., 2015; Williams et al., 2017; Lai et al., 2017; Khot et al., 2018). However, these datasets do not provide clear insight into what type of reasoning or inference a model may be performing. For example, these datasets cannot be used to evaluate whether competitive NLI models can determine if an event occurred, correctly differentiate between figurative and literal language, or accurately identify and categorize named entities. Consequently, these datasets cannot answer how well sentence representation learning models capture distinct semantic phenomena necessary for general natural language understa"
W18-5441,W17-7203,0,0.0813304,"7). We recast annotations from a total of 13 datasets across 7 NLP tasks into labeled NLI examples. The tasks include event factuality, named entity recognition, gendered anaphora resolution, sentiment analysis, relationship extraction, pun detection, and lexicosyntactic inference (Table 2). Currently, DNC contains over half a million labeled examples that can be used to probe a model’s ability to capture different types of semantic reasoning necessary for general NLU. In short, this work answers a recent plea to the community to test “more kinds of inference” than in previous challenge sets (Chatzikyriakidis et al., 2017). 1 The task of determining if a hypothesis would likely be inferred from a context, or premise; also known as Recognizing Textual Entailment (RTE) (Dagan et al., 2006, 2013). 337 Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 337–340 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics 2 Motivation & Background Compared to eliciting NLI datasets directly, i.e. asking humans to author contexts and/or hypothesis sentences, recasting can 1) help determine whether an NLU model performs distinct types o"
W18-5441,P17-1117,0,0.0482847,"Missing"
W18-5441,P16-1204,1,0.834533,"ch that is entailed, neutral, and contradicted by a caption extracted from the Flickr30k corpus (Young et al., 2014). Although these datasets are widely used to train and evaluate sentence representations, a high accuracy is not indicative of what types of reasoning NLI models perform. Workers were free to create any type of hypothesis for each context and label. Such datasets cannot be used to determine how well an NLI model captures many desired capabilities of language understanding systems, e.g. paraphrastic inference, complex anaphora resolution (White et al., 2017), or compositionality (Pavlick and Callison-Burch, 2016; Dasgupta et al., 2018). By converting prior annotation of a specific phenomenon into NLI examples, recasting allows us to create a diverse NLI benchmark that tests a model’s ability to perform distinct types of reasoning. Phenomena Dataset Event Factuality Decomp (Rudinger et al., 2018b) UW (Lee et al., 2015) MeanTime (Minard et al., 2016) Named Entity Recognition Groningen (Bos et al., 2017) CoNLL (Tjong Kim Sang and De Meulder, 2003) Gendered Anaphora Winogender (Rudinger et al., 2018a) Lexicosyntactic Inference VerbCorner (Hartshorne et al., 2013) MegaVeridicality (White and Rawlins, 2018"
W18-5441,S18-2023,1,0.895623,"Missing"
W18-5441,W17-1609,1,0.891312,"Missing"
W18-5441,D15-1284,0,0.0485985,"Missing"
W18-5441,N18-2002,1,0.887236,"Missing"
W18-5441,Q14-1006,0,0.0417421,"or hypothesis sentences, recasting can 1) help determine whether an NLU model performs distinct types of reasoning; 2) limit types of biases observed in previous NLI data; and 3) generate examples cheaply, potentially at large scales. NLU Insights Popular NLI datasets, e.g. Stanford Natural Language Inference (SNLI) (Bowman et al., 2015) and its successor MultiNLI (Williams et al., 2017), were created by eliciting hypotheses from humans. Crowd-source workers were tasked with writing one sentence each that is entailed, neutral, and contradicted by a caption extracted from the Flickr30k corpus (Young et al., 2014). Although these datasets are widely used to train and evaluate sentence representations, a high accuracy is not indicative of what types of reasoning NLI models perform. Workers were free to create any type of hypothesis for each context and label. Such datasets cannot be used to determine how well an NLI model captures many desired capabilities of language understanding systems, e.g. paraphrastic inference, complex anaphora resolution (White et al., 2017), or compositionality (Pavlick and Callison-Burch, 2016; Dasgupta et al., 2018). By converting prior annotation of a specific phenomenon in"
W18-5441,N18-1067,1,0.871771,"Missing"
W18-5441,Q17-1027,1,0.883049,"Missing"
W18-5441,W03-0419,0,0.563777,"Missing"
W18-5441,L18-1239,0,0.0690592,"CoNLL (Tjong Kim Sang and De Meulder, 2003) Gendered Anaphora Winogender (Rudinger et al., 2018a) Lexicosyntactic Inference VerbCorner (Hartshorne et al., 2013) MegaVeridicality (White and Rawlins, 2018) VerbNet (Schuler, 2005) Puns (Yang et al., 2015) SemEval 2017 Task 7 (Miller et al., 2017) Relationship Extraction FACC1 (Gabrilovich et al., 2013) Sentiment Analysis (Kotzias et al., 2015) Table 2: List of each type of semantic phenomena paired with its corresponding dataset(s) we recast. cast.2 Experimental results using hypothesis-only models (Poliak et al., 2018; Gururangan et al., 2018; Tsuchiya, 2018) can indicate to what degree the recast datasets retain some biases that may be present in the original semantic datasets. NLI Examples at Large-scale Generating NLI datasets from scratch is costly. Humans must be paid to generate or label natural language text. This linearly scales costs as the amount of generated NLI-pairs increases. Existing annotations for a wide array of semantic NLP tasks are freely available. By leveraging existing semantic annotations already invested in by the community we can generate and label NLI pairs at little cost and create large NLI datasets to train data hung"
W18-5441,I17-1100,1,0.675938,"Missing"
W19-2918,C18-1197,0,0.0183655,"ical entailment detection, and relatively well on graded entailment judgements. While the higherdimensional Prototype-based approach performed well on categorization, in general our Prototypebased approach performs subpar. The extent to which this is theory vs. approach can’t be determined by this research - the vagueness of the distance function d proposed by Prototype Theory gives way to a vast world of unexplored cognitively plausible instantiations that we look forward to exploring. In general, the present study relates to the ample prior work on visually-grounded meaning representations. Beinborn et al. (2018) gives an indepth survey of work in this area, from both a computational and a cognitive perspective. Of particular relevance to our work is prior work on multimodal lexical semantics, e.g. work which extends skipgram-like training procedures to include both visual and text information Lazaridou et al. (2015); Silberer and Lapata (2012); Silberer et al. (2017); Collell et al. (2017); Kiela et al. (2016); Kiros et al. (2018). Such representations not only perform better in practice, but have been shown to be more cognitively-plausible in terms of their ability to predict human brain activity (B"
W19-2918,D17-1113,0,0.0152126,") gives an indepth survey of work in this area, from both a computational and a cognitive perspective. Of particular relevance to our work is prior work on multimodal lexical semantics, e.g. work which extends skipgram-like training procedures to include both visual and text information Lazaridou et al. (2015); Silberer and Lapata (2012); Silberer et al. (2017); Collell et al. (2017); Kiela et al. (2016); Kiros et al. (2018). Such representations not only perform better in practice, but have been shown to be more cognitively-plausible in terms of their ability to predict human brain activity (Bulat et al., 2017). Beyond lexical representations, multimodal representations have been incorporated representations of more complex concepts such as frames (Shutova et al., 2017) and full sentences (Han et al., 2017). Again, our work differs in that we are not focused on harnessing visual data per se; rather, our focus is on how, given a representation of the world to which we can “ground” meaning, different theories can be operationalized, and how the assumptions of these theories affect performance on basic tasks. That Acknowledgements We would like to thank Roman Feiman, Eugene Charniak, and members of the"
W19-2918,D17-1305,0,0.0487274,"Missing"
W19-2918,P15-2020,0,0.0605159,"Missing"
W19-2918,P15-1146,1,0.885028,"Missing"
W19-2918,P17-1192,1,0.83065,", which sought to instantiate the formal semantics notion of set-theoretic entailment using images to represent the “worlds” to which natural language refers. Their work focused on representations motivated by Classical Theory, and dealt with literal sets of discrete im166 is, we view our work as complementary to, rather than competing with, existing ongoing work on multimodal and grounded representations. Finally, there is an enormous body of work aimed at modelling lexical entailment using textonly training data, recently (Shwartz et al., 2016; Chang et al., 2017; Vuli´c and Mrkˇsi´c, 2017; Pavlick and Pasca, 2017; Pavlick et al., 2015). Such work often treats lexical entailment as a supervised learning problem, or at least as a task to which we should tune directly. We view such approaches as fundamentally different from what we present here. That is, our work focuses on how to form concepts which relate language to the world, with the assumption that inferences about entailment should come from reasoning directly about the extensions of these concepts, rather than indirectly by relating the surface forms which refer to those denotations. ages, meaning it could not generalize to referents outside the"
W19-2918,D16-1043,0,0.0180742,"unexplored cognitively plausible instantiations that we look forward to exploring. In general, the present study relates to the ample prior work on visually-grounded meaning representations. Beinborn et al. (2018) gives an indepth survey of work in this area, from both a computational and a cognitive perspective. Of particular relevance to our work is prior work on multimodal lexical semantics, e.g. work which extends skipgram-like training procedures to include both visual and text information Lazaridou et al. (2015); Silberer and Lapata (2012); Silberer et al. (2017); Collell et al. (2017); Kiela et al. (2016); Kiros et al. (2018). Such representations not only perform better in practice, but have been shown to be more cognitively-plausible in terms of their ability to predict human brain activity (Bulat et al., 2017). Beyond lexical representations, multimodal representations have been incorporated representations of more complex concepts such as frames (Shutova et al., 2017) and full sentences (Han et al., 2017). Again, our work differs in that we are not focused on harnessing visual data per se; rather, our focus is on how, given a representation of the world to which we can “ground” meaning, di"
W19-2918,S17-1018,0,0.0393022,"Missing"
W19-2918,D12-1130,0,0.0259288,"tance function d proposed by Prototype Theory gives way to a vast world of unexplored cognitively plausible instantiations that we look forward to exploring. In general, the present study relates to the ample prior work on visually-grounded meaning representations. Beinborn et al. (2018) gives an indepth survey of work in this area, from both a computational and a cognitive perspective. Of particular relevance to our work is prior work on multimodal lexical semantics, e.g. work which extends skipgram-like training procedures to include both visual and text information Lazaridou et al. (2015); Silberer and Lapata (2012); Silberer et al. (2017); Collell et al. (2017); Kiela et al. (2016); Kiros et al. (2018). Such representations not only perform better in practice, but have been shown to be more cognitively-plausible in terms of their ability to predict human brain activity (Bulat et al., 2017). Beyond lexical representations, multimodal representations have been incorporated representations of more complex concepts such as frames (Shutova et al., 2017) and full sentences (Han et al., 2017). Again, our work differs in that we are not focused on harnessing visual data per se; rather, our focus is on how, give"
W19-2918,J17-4004,0,0.0530454,"Missing"
W19-2918,C14-1212,0,0.0251539,"eights with the best validation loss, stopping training after 5 epochs without improvement. Training takes only a few minutes on a desktop with an Nvidia GTX 1070 GPU. 3.3 4.1 Graded WBLESS HYPERLEX stove→object scarf→garment pistol→ weapon X X X kangaroo→animal mammal→animal grape→food 6.0 6.0 5.9 grain→corn telephone→stove jacket→raincoat X X X animal→mammal horn→car plate→spoon 0.8 0.9 0.2 Table 1: Positive and negative examples from each of our lexical entailment (LE) evaluation sets. WBLESS . For the standard (binary) lexical entailment task, we use the WBLESS lexical entailment dataset (Weeds et al., 2014), which consists of 1,168 word pairs, containing an equal number of positive and negative lexical entailment examples. Positive examples are hyponymhypernym pairs, where negative examples include reversed entailment pairs, co-hyponyms, holonym-meronym pairs, and random word pairs. Dimensionality Reduction Due to the exponential complexity of algorithms used to compute convex hulls (specifically QuickHull (Barber et al., 1996)) we are unable to compute Classical-based representations for values of d &gt; 4. For now, we address this by training the VAE with higher dimensional encodings, then projec"
W19-2918,Q14-1006,0,0.496734,"ap between these sets: P fC1 (x) × fC2 (x) P entail(C1 , C2 ) = x∈X (3) x∈X fC1 (x) Definitions Notation We will use C to represent a concept and x to represent a potential “instance” of the concept. Intuitively, we can think of x as an entity when C is a That is, the probability that C1 entails C2 is exactly the probability that a given instances of C1 is also an instance of C2. 161 2.3 to represent “instances”. That is, our X is the space of all images and our C maps one-to-one onto English nouns. A similar approach, using images as a representation of “the world”, has been used previously (Young et al., 2014). We adopt this approach as it enables a fairly direct way to instantiate abstract formal theories using representations (pixels) which can be handled straightforwardly by current computational models. We do not make the claim that visual attributes are the only relevant attributes which factor into representations of concepts. Rather, our focus is on testing in general how the choice of representation affects the predictions made by models, assuming that some representation of “the world” is given a priori. In other words, our choice to use only visual attributes is a methodologically-motivat"
