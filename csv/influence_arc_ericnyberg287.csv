1988.tmi-1.4,P84-1107,0,0.0223256,"ortant when it is difficult to constrain the types of output in generation, and, consequently, when the lexicon becomes large. Machine translation and automatic text summarization are among applications that by nature require a wide range of outputs and have to use a sizable lexicon. Note that of these two, the former does not involve utterance planning and concentrates on lexical and syntactic realization. In the natural language generation community, attention to the task of lexical selection attention has recently grown. Of course, it has always been recognized as an important problem (cf. Danlos, 1984; Jacobs, 1985; Bienkowski, 1986; and the survey Cumming, 1986) and was addressed in a well-known early generation project (Goldman, 1975). However, most major NLG efforts of the early 1980s (e.g., Mann and Matthiessen, 1985; McDonald, 1983; McKeown, 1985, Appelt, 1985) concentrated more on syntactic and stylistic realization as well as to text planning than to lexical selection. Among those who dealt with lexical selection was the SEMSYN project (Rösner, 1986). Currently, work on lexical selection is going on at the University of Pennsylvania (e.g., Marcus, 1987) and at University of Californ"
1988.tmi-1.4,C86-1131,0,0.012376,"namely, the selection of open-class lexical items to realize the meanings of object, event and property tokens in the input. Thus, the output of the generation module described here is a lexical unit or a pronoun in the target language. Our approach (and especially the expected input) to text generation is similar to that of the SEMSYN project (e.g., Rösner, 1986). Lexical selection is not, however, an immediate concern of and is not discussed at any length in SEMSYN descriptions (see Laubsch et al., 1984, p. 492), and a published analysis of practical difficulties encountered by the project (Hanakata et al., 1986) does not address this issue at all. Furthermore, since until very recently that project had to generate sentence-length texts (article titles), the problem of definite descriptions, pronominalization and ellipsis did not become acutely important. 3 Why is it a difficult task? Lexical choice is not a straightforward task. Suppose we have to express in English the meaning 'a person whose sex is male and whose age is between 13 and 15 years.' What knowledge do people use in order to come up with an appropriate choice out of such candidate realizations as those listed in (1). (1) boy, kid, teenag"
1988.tmi-1.4,P84-1105,0,0.0215193,"agmatic content (see Section 4 for more details). In this paper we deal with a subset of the generation task, namely, the selection of open-class lexical items to realize the meanings of object, event and property tokens in the input. Thus, the output of the generation module described here is a lexical unit or a pronoun in the target language. Our approach (and especially the expected input) to text generation is similar to that of the SEMSYN project (e.g., Rösner, 1986). Lexical selection is not, however, an immediate concern of and is not discussed at any length in SEMSYN descriptions (see Laubsch et al., 1984, p. 492), and a published analysis of practical difficulties encountered by the project (Hanakata et al., 1986) does not address this issue at all. Furthermore, since until very recently that project had to generate sentence-length texts (article titles), the problem of definite descriptions, pronominalization and ellipsis did not become acutely important. 3 Why is it a difficult task? Lexical choice is not a straightforward task. Suppose we have to express in English the meaning 'a person whose sex is male and whose age is between 13 and 15 years.' What knowledge do people use in order to co"
1988.tmi-1.4,C86-1031,0,0.0204453,"onstrain without any filtering actually achieved. (If there is no such provision there is a danger of a deadlock when all the processes will be in the waiting mode.) This eventuality is taken care of by rigging the probabilities of the two outcomes in toss-a-coin2. Heuristically, the choice of the main verb (proposition head) is the most independent, followed by the choice of noun phrase heads. Collocationally-constrain uses the collocation information in the lexicon entries to match 2 This mechanism is similar to the one used in the Friendly-Neighbors algorithm of the PARPAR parallel parser (Lozinskii and Nirenburg, 1986). and filter the candidate sets. If the residual set has cardinality one, the result is posted on the LSBB. If there exist more than one candidate, the function select-best (i.e., the Matcher) is called to perform context-independent lexical selection based on the quality of the match between the meaning pattern of the ILT frame on the one hand and the weighted meaning patterns of the GL entries in the candidate set on the other. The predicate modifier-realization-indicated returns t if there exist properties in the ILT frame that were not accounted for in the head realization of the lexical i"
1988.tmi-1.4,T87-1046,0,0.0264698,"d as an important problem (cf. Danlos, 1984; Jacobs, 1985; Bienkowski, 1986; and the survey Cumming, 1986) and was addressed in a well-known early generation project (Goldman, 1975). However, most major NLG efforts of the early 1980s (e.g., Mann and Matthiessen, 1985; McDonald, 1983; McKeown, 1985, Appelt, 1985) concentrated more on syntactic and stylistic realization as well as to text planning than to lexical selection. Among those who dealt with lexical selection was the SEMSYN project (Rösner, 1986). Currently, work on lexical selection is going on at the University of Pennsylvania (e.g., Marcus, 1987) and at University of California at Berkeley (Ward, 1988). Still, the set of problems facing this field is significant. One motivation for our research was that we agree with Marcus (1987, p. 211) that at present 'most generation systems don't use words at all,' and we believe that the quality of generation output will improve significantly once an adequate lexical selection component becomes a standard part of a NLG system. 2 The Task Research reported in this paper was performed within the DIOGENES project (Nirenburg, 1987), whose objective is to provide a high-quality generator for a knowle"
1988.tmi-1.4,C86-1148,1,0.816188,"be generated (the first application of DIOGENES is, for example, in the domain of computer hardware manuals) • a generation lexicon that links (sub)world concepts (or, more accurately, their instances) with particular lexical units of the target language. The above description is necessarily incomplete. See Nirenburg, 1987 for an extensive specification of all the facets of DIOGENES. The input to DIOGENES, known as interlingua text (ILT), is a set of FrameKit frames representing the propositional and non-propositional meanings of the source language text input into the translation system (see Nirenburg et al., 1986, 1987b; Nirenburg and Carbonell, 1987 for a detailed description). The ILT is produced during the analysis stage of the MT process by the parser, the semantic interpreter and, if needed, with human help. This latter stage is needed because our requirements for the input to generation are such that no current analysis system can produce them in a completely automatic fashion. The following is a sample InterLingua Text. (make-frame clausel (clauseid (value clausel)) (propositionid (value propositionl)) (speechactid (value speech-actl))) (make-frame propositionl (propositionid (value proposition"
1988.tmi-1.4,C88-2149,0,0.013119,"Bienkowski, 1986; and the survey Cumming, 1986) and was addressed in a well-known early generation project (Goldman, 1975). However, most major NLG efforts of the early 1980s (e.g., Mann and Matthiessen, 1985; McDonald, 1983; McKeown, 1985, Appelt, 1985) concentrated more on syntactic and stylistic realization as well as to text planning than to lexical selection. Among those who dealt with lexical selection was the SEMSYN project (Rösner, 1986). Currently, work on lexical selection is going on at the University of Pennsylvania (e.g., Marcus, 1987) and at University of California at Berkeley (Ward, 1988). Still, the set of problems facing this field is significant. One motivation for our research was that we agree with Marcus (1987, p. 211) that at present 'most generation systems don't use words at all,' and we believe that the quality of generation output will improve significantly once an adequate lexical selection component becomes a standard part of a NLG system. 2 The Task Research reported in this paper was performed within the DIOGENES project (Nirenburg, 1987), whose objective is to provide a high-quality generator for a knowledge-based interlingual machine translation system. The in"
1997.mtsummit-papers.2,1994.amta-1.10,0,0.108479,"Missing"
1999.tmi-1.22,1991.mtsummit-papers.9,1,0.837321,"is issue and demonstrate a method which reduces the time and effort to build high-quality KBMT systems. A semantic model developed for a particular domain may not cover all of the structural attachments in sentences which the system will eventually encounter. Therefore, a system which relies only on a semantic model for accurate attachment will require constant update. Furthermore, it is often necessary to process new documents for new product lines not covered by the existing domain model, resulting in an ongoing need to update the domain model over time. The KANT machine translation system (Mitamura et al. 1991) queries the author to disambiguate interactively if the domain model cannot disambiguate a structural attachment automatically. This solution is not always satisfactory - interactive disambiguation is not always accurate, and it is always a time-consuming task, and hence costly in terms of overall system productivity. 218 In this paper, we present the results of an experiment which combines domainindependent heuristics with a semantic knowledge base. We explore a multiple-strategy approach which preserves a high degree of translation quality, while reducing both the need for interactive disam"
1999.tmi-1.22,C92-3168,1,0.861449,"Missing"
1999.tmi-1.22,P90-1004,0,0.0160681,"Missing"
2001.mtsummit-papers.43,C88-1021,0,0.58799,"Missing"
2001.mtsummit-papers.43,C90-3063,0,0.0994061,"Missing"
2001.mtsummit-papers.43,P98-1064,0,0.034423,"Missing"
2001.mtsummit-papers.43,C96-1021,0,0.0604466,"Missing"
2001.mtsummit-papers.43,J94-4002,0,0.159566,"Missing"
2001.mtsummit-papers.43,J90-4001,0,0.734918,"Missing"
2001.mtsummit-papers.43,P98-2143,0,0.0437377,"Missing"
2001.mtsummit-papers.43,1991.mtsummit-papers.9,1,0.724613,"-poor systems, it can be inferred that adding more linguistic knowledge reduces the need for scoring procedures to prune incorrect antecedents. If necessary, semantic knowledge can be used once syntactic rules have been exhausted. In the next sections, we explain the details of our resolution algorithm, present the results of an evaluation on texts drawn from technical manuals, and discuss some implications for current and future work. KANTOO Anaphora Resolution KANTOO is a knowledge-based, interlingual machine translation system, the most recent implementation of the original KANT MT system (Mitamura et al. 1991). KANTOO accepts Controlled English as input (Mitamura and Nyberg, 1995); the current input specification is referred to as KANT Controlled English (KCE). KCE places some restrictions on vocabulary and grammar; although some of the sentences in this study were rewritten to conform to KCE, we did not edit pronominal anaphors or any other constituents relevant to the anaphor resolution process. Identification of Possible Antecedents Possible antecedents for a given pronoun are identified according to a set of pre-defined constraints: 1. The candidate antecedent must be a noun, unit, tag, or conj"
2001.mtsummit-papers.43,C94-2189,0,\N,Missing
2001.mtsummit-papers.43,C98-2138,0,\N,Missing
2002.tmi-papers.13,C88-1021,0,0.191998,"Missing"
2002.tmi-papers.13,C90-3063,0,0.0606638,"Missing"
2002.tmi-papers.13,P98-1064,0,0.0292616,"Missing"
2002.tmi-papers.13,C96-1021,0,0.068191,"Missing"
2002.tmi-papers.13,J94-4002,0,0.0744051,"Missing"
2002.tmi-papers.13,J90-4001,0,0.105424,"Missing"
2002.tmi-papers.13,C94-2189,0,0.0488992,"Missing"
2002.tmi-papers.13,P98-2143,0,0.059915,"Missing"
2002.tmi-papers.13,1991.mtsummit-papers.9,1,0.480272,"on indicates performance comparable to that of score-based, knowledge-poor systems, it can be inferred that adding more linguistic knowledge reduces the need for scoring procedures to prune incorrect antecedents. If necessary, semantic knowledge can be used once syntactic rules have been exhausted. In the next sections, we discuss the details of our resolution algorithm, and the results of an evaluation on technical texts which were translated to Spanish and German. We conclude with a discussion of some implications for current and future work. 2 KANTOO Multilingual MT System The KANT System (Mitamura et al. 1991) is a knowledge-based, interlingual machine translation system, developed for multilingual translations of technical documents in various domains. Application domains include heavy equipment documentation, computer manuals, automotive documentation, and medical records written in controlled language (Mitamura and Nyberg 1995). KANTOO is the reimplementation of the original KANT MT system, and also accepts Controlled English as input. The current input specification is referred to as KANT Controlled English (KCE). KCE places some restrictions on vocabulary and grammar. Although some of the sent"
2003.mtsummit-systems.13,1991.mtsummit-papers.9,1,0.732638,"Missing"
2020.nlposs-1.6,2020.emnlp-main.550,0,0.0806856,"presentations (e.g., FAISS and Annoy), or only perform mixing at the re-ranking stage. 1 The first retrieval stage is commonly referred to as the candidate generation (i.e., we generate candidates for re-scoring). Until about 2019, the candidate generation would exclusively rely on a traditional search engine such as Lucene,1 which indexes occurrences of individual terms, their lemmas or stems (Manning et al., 2010). In that, there are several recent papers where promising results were achieved by generating dense embeddings and using a k -NN search library to retrieve them (Lee et al., 2019; Karpukhin et al., 2020; Xiong et al., 2020). However, these studies typically have at least one of the following flaws: (1) they compare against a weak baseline such as untuned BM25 or (2) they rely on exact k -NN search, thus, totally ignoring practical efficiencyeffectiveness and scalability trade-offs related to using k -NN search, see, e.g., §3.3 in Boytsov (2018). FlexNeuART implements some of the most effective non-neural ranking signals: It produced best non-neural runs in the TREC 2019 deep learning challenge (Craswell et al., 2020) and would be a good tool to verify these results. Introduction Although the"
2020.nlposs-1.6,P19-1612,0,0.0270389,"), purely dense representations (e.g., FAISS and Annoy), or only perform mixing at the re-ranking stage. 1 The first retrieval stage is commonly referred to as the candidate generation (i.e., we generate candidates for re-scoring). Until about 2019, the candidate generation would exclusively rely on a traditional search engine such as Lucene,1 which indexes occurrences of individual terms, their lemmas or stems (Manning et al., 2010). In that, there are several recent papers where promising results were achieved by generating dense embeddings and using a k -NN search library to retrieve them (Lee et al., 2019; Karpukhin et al., 2020; Xiong et al., 2020). However, these studies typically have at least one of the following flaws: (1) they compare against a weak baseline such as untuned BM25 or (2) they rely on exact k -NN search, thus, totally ignoring practical efficiencyeffectiveness and scalability trade-offs related to using k -NN search, see, e.g., §3.3 in Boytsov (2018). FlexNeuART implements some of the most effective non-neural ranking signals: It produced best non-neural runs in the TREC 2019 deep learning challenge (Craswell et al., 2020) and would be a good tool to verify these results. I"
2020.nlposs-1.6,J03-1002,0,0.0213927,"nal tokenizers, we also use the BERT tokenizer from the HuggingFace Transformers library (Wolf et al., 2019). This tokenizer can split a single word into several sub-word pieces (Wu et al., 2016). The stopword list is not applied to BERT tokens. Training Model 1, which is a translation model, requires a parallel corpus where queries are paired with respective relevant documents. The parallel corpus is also known as a bitext. In the case of MS MARCO collections documents are much longer than queries, which makes it impossible to compute translation probabilities using standard alignment tools (Och and Ney, 2003).18 Hence, for each pair of query q and its relevant document d, we first split d into multiple short chunks d1 , d2 , . . . dn . Then, we replace the pair (q, d) with a set of pairs {(q, di )}. We evaluate performance of several models and their combinations. Each model name is abbreviated as X (Y), where X is a type of the model (see §3.3 for details) and Y is a type of the text field. Specifically, we index original tokens, lemmas, as well as BERT tokens extracted from the main document text. For MS MARCO documents, which come in HTML format, we also extract tokens and lemmas from the title"
2020.nlposs-1.6,P07-1059,0,0.139818,"Missing"
2020.nlposs-1.6,P19-1436,0,0.0328082,"Missing"
2020.nlposs-1.6,J11-2003,0,0.0366144,"et al., 2000). 4 • A proxy query- and document embedder, that produces fixed-size dense vectors for queries and documents. The similarity is the inner product between query and document embeddings. This scorer operates as an Apache Thrift server. Experiments We carry out experiments with two objectives: (1) measuring effectiveness of implemented ranking models; (2) demonstrating the value of a well-tuned traditional IR system. We use two recently released MS MARCO collections (Craswell et al., 2020; Nguyen et al., 2016) and a community question answering (CQA) collection Yahoo Answers Manner (Surdeanu et al., 2011). Collection statistics is summarized in Table 1. • A BM25-based pseudo-relevance feedback model RM3. Unlike a common approach where RM3 is used for queryexpansion, we use it in re-ranking mode (Diaz, 2015). MS MARCO has a document and a passage re-ranking task where all queries can be answered using a short text snippet. There are three sets of queries in each task. In addition to one large query set with sparse judgments, there are two small evaluation Although FlexNeuART supports complex scoring models, these can be computationally too expensive to be used directly for retrieval (Boytsov et"
2020.nlposs-1.6,1983.tc-1.13,0,0.175604,"Missing"
2021.dialdoc-1.14,2020.emnlp-main.652,0,0.639758,"Missing"
2021.dialdoc-1.14,2020.acl-main.703,0,0.022823,"xtraction dataset, and CoQA, because of its similar task structure, where models must answer questions based on both dialogue history and document-based context. 3.3.1 3.2 Baselines For Subtask1, the baseline model is the BERTlarge-uncased-whole-word-masking model (Devlin et al., 2019). A span-extraction head is added on top of BERT, and the model is fine-tuned on the Doc2Dial knowledge identification dataset. For each example, an entire document is used as the context and the reverse concatenated dialogue history is used as the question. For Subtask2, the baseline model is the BARTlarge-CNN (Lewis et al., 2020) model: a pre-trained BART model is first fine-tuned on the CNN summarization task, then fine-tuned on Doc2Dial response generation dataset. The entire document and full dialogue history are used as the context and the model is trained to generate the next dialogue response. 3.3 Approaches: Knowledge Identification Based on error analysis of baseline results, we found that the model is making a lot of empty predictions. This is mainly because the documents in Doc2Dial are very long, necessitating a slidingwindow approach. Consequently, if a text chunk does not contain any relevant information"
2021.dialdoc-1.14,D18-1241,0,0.0164921,"ument span, for the next-agent conversational turn. For response generation (Subtask2), the main objective is to generate the next-agent response, in natural language. We experiment with ∗ * Equal contribution; alphabetized by surname https://doc2dial.github.io/ workshop2021/shared.html 1 2 Related Work There are many previous works that study the problem of dialogue-based question answering. Some of them only focused on answering the questions based on dialogue history alone (Ma et al., 2018; Li et al., 2020), while, for others, the dialogue and question-answer pairs are based on a document (Choi et al., 2018). Most of these tasks are extractive in nature, meaning that the exact answer can be located in the document or dialogue. Among them, CoQA (Reddy et al., 2019) is the most similar task to Doc2Dial dataset. The main objective of the CoQA challenge is to measure machine learning models’ ability to comprehend text and answer related questions that appear in a conversation; also, because some answers may not appear explicitly in the document, the model may be required to synthesize the answer based on evidence. The two sub-tasks we study in this paper differ from those described above—mainly in te"
2021.dialdoc-1.14,N19-1423,0,0.00997509,"the sliding-window overlap size to 256 and max answer length to 80 during training, so as to get more positive instances. Moreover, since the Doc2dial dataset size is relatively small, we pre-trained the model on other QA datasets and then fine-tuned on Doc2dial. To this end, we selected SQuAD 1.1, because it is a widely used span-extraction dataset, and CoQA, because of its similar task structure, where models must answer questions based on both dialogue history and document-based context. 3.3.1 3.2 Baselines For Subtask1, the baseline model is the BERTlarge-uncased-whole-word-masking model (Devlin et al., 2019). A span-extraction head is added on top of BERT, and the model is fine-tuned on the Doc2Dial knowledge identification dataset. For each example, an entire document is used as the context and the reverse concatenated dialogue history is used as the question. For Subtask2, the baseline model is the BARTlarge-CNN (Lewis et al., 2020) model: a pre-trained BART model is first fine-tuned on the CNN summarization task, then fine-tuned on Doc2Dial response generation dataset. The entire document and full dialogue history are used as the context and the model is trained to generate the next dialogue r"
2021.dialdoc-1.14,2020.coling-main.238,0,0.0206711,"Missing"
2021.dialdoc-1.14,D19-6003,1,0.902488,"Missing"
2021.dialdoc-1.14,N18-1185,1,0.895658,"Missing"
2021.dialdoc-1.14,W18-6319,0,0.0214155,"baseline, indicating the usefulness of the carefully chosen hyper-parameters. However, when we combined the larger overlap stride with pre-training on CoQA or SQuAD, we did not see further improvement; we leave the further investigation of this issue to future work. F1 EM 63.80 69.73 70.89 72.15 72.74 51.79 54.91 56.31 57.18 58.53 Table 2: Model performance on Doc2Dial sub-task2. “SS” means span selection and “DI” means additional decoder input. Result and Analysis For Subtask1, we report f1-score and exact-match score on the dev set for our proposed method. For Subtask2, we report SacreBleu (Post, 2018) on the dev set. Finally, we report the test set results achieved with our best model, for both tasks. 4.1 Table 1: Model performance on Doc2Dial sub-task1. Here “Post.” means post-processing. SacreBleu 17.69 18.82 24.86 31.61 27.87 Sub-task2: Response Generation The results for Subtask2 are shown in Table 2. We see that when using the selected span of text, instead of the full document, we achieved a small improvement on Bleu score; when using the groundtruth grounding span, we got a large improvement. This verified our hypothesis that shorter input will help the model generate relevant respo"
2021.dialdoc-1.14,D16-1264,0,0.028323,"al-oriented Document-grounded Dialogue Systems Xi Chen¶ ∗, Faner Lin¶∗ , Yeju Zhou¶∗ , Kaixin Ma¶ , Jonathan Francis¶§ , Eric Nyberg¶ , Alessandro Oltramari§ ¶ Language Technologies Institute, Carnegie Mellon University § Human-Machine Collaboration, Bosch Research Pittsburgh {xc3, fanerl, yejuz, kaixinm, jmf1, ehn}@cs.cmu.edu, alessandro.oltramari@us.bosch.com Abstract various baseline models, and we developed and evaluated our proposed solutions. Some improvement strategies we tried include post-processing, hyper-parameter tuning, and pre-training on other well-known datasets such as SQuAD (Rajpurkar et al., 2016). We found that with carefully-selected hyperparameters, and with pre-processing and postprocessing heuristics, the baseline model’s performance can be significantly improved: our best model is able to out-perform the provided baseline by 10.5+ f1-score on the test-dev split for Subtask1, and our best model for Subtask2 out-performed the baseline by 11+ SacreBleu score on test-dev. In this paper, we describe our systems for solving the two Doc2Dial shared task: knowledge identification and response generation. We proposed several pre-processing and postprocessing methods, and we experimented w"
2021.dialdoc-1.14,Q19-1016,0,0.253473,"language. We experiment with ∗ * Equal contribution; alphabetized by surname https://doc2dial.github.io/ workshop2021/shared.html 1 2 Related Work There are many previous works that study the problem of dialogue-based question answering. Some of them only focused on answering the questions based on dialogue history alone (Ma et al., 2018; Li et al., 2020), while, for others, the dialogue and question-answer pairs are based on a document (Choi et al., 2018). Most of these tasks are extractive in nature, meaning that the exact answer can be located in the document or dialogue. Among them, CoQA (Reddy et al., 2019) is the most similar task to Doc2Dial dataset. The main objective of the CoQA challenge is to measure machine learning models’ ability to comprehend text and answer related questions that appear in a conversation; also, because some answers may not appear explicitly in the document, the model may be required to synthesize the answer based on evidence. The two sub-tasks we study in this paper differ from those described above—mainly in terms of dataset attributes. The Doc2Dial dataset mostly contains long documents and dialogues that inter-connect with each other. Moreover, the ground-truth ans"
2021.emnlp-main.445,2021.eacl-main.192,0,0.028258,"e task, but suffers from overfitting and limited generalization to novel answers. Prompting methods have lower accuracy, but tend to show higher robustness to “adversarial” splits. Extending the models by prefix-tuning represents a “sweet spot” between task accuracy, generalization, and robustness. Machine commonsense reasoning has recently gained new traction, largely due to a collection of diverse benchmarks (Talmor et al., 2019; Bhagavatula et al., 2019; Sap et al., 2019) and the successful application of language modeling methods on these benchmarks (Ma et al., 2019; Shwartz et al., 2020; Bauer and Bansal, 2021). The most widely adopted approach to solve these commonsense reasoning tasks is by fine-tuning large pre-trained language models (LMs) (Devlin et al., 2019; Liu et al., 2019) on the task-specific training data. Meanwhile, it has been shown that language models are able to acquire certain commonsense background 2 Related Work knowledge, during their pre-training on large textual data (Petroni et al., 2019; Davison et al., 2019; Prior works probe the commonsense knowledge Ma et al., 2021). In light of these findings and learned by the LMs. Davison et al. (2019) mined 5474 Proceedings of the 202"
2021.emnlp-main.445,2020.emnlp-main.84,0,0.0246061,"istic perturbations. Elazar et al. (2021) posit that while LMs can learn to perform well on commonsense tasks, their commonsense reasoning ability mostly comes from fine-tuning on the task data. Some works have sought to uncover what models learn through training on question answering datasets, exposing various dataset artifacts in the process (Jia and Liang, 2017; Kaushik and Lipton, 2018; Pugaliya et al., 2019). Welbl et al. (2020) found that models trained on the SQuAD2.0 dataset (Rajpurkar et al., 2018) are insensitive to the meaningful changes in the question and predict the same answer. Ko et al. (2020) found that BERT easily picks up the position bias in the SQuAD dataset (Rajpurkar et al., 2016) and models’ performance can drop by more than 50 points on f1-score when training on a biased subset. Sen and Saffari (2020) analyzed the model’s ability to generalize, by training on 5 different QA datasets, and found that no single dataset is robust to perturbations in the questions. Shah et al. (2020) tested models, trained on several multiple-choice QA datasets, and showed that they are largely relying on dataset biases. Previous work mostly studies the language models, as-is, or evaluated mode"
2021.emnlp-main.445,2020.acl-main.703,0,0.0431335,"wledge has already been encoded in the model parameters. However, to our knowledge, no comprehensive comparison exists between these model updating strategies. In this paper, we pose the question: What do models learn from commonsense reasoning datasets? We consider three representative learning methods: regular fine-tuning, model extension with prefixtuning (Li and Liang, 2021), and model prompting with Autoprompt (Shin et al., 2020). We apply them to two representative model classes: the autoregressive language model GPT-2 (Radford et al., 2019) and sequence-to-sequence language model BART (Lewis et al., 2020). We conduct thorough evaluation on the generative evaluation benchmarks ProtoQA (Boratko et al., 2020) and CommonGen (Lin et al., 2020a), by training on different partitions of the training data. Our experiments show that fine-tuning performs best, by learning both the content and the structure of the task, but suffers from overfitting and limited generalization to novel answers. Prompting methods have lower accuracy, but tend to show higher robustness to “adversarial” splits. Extending the models by prefix-tuning represents a “sweet spot” between task accuracy, generalization, and robustness"
2021.emnlp-main.445,2021.eacl-main.156,0,0.0743221,"Missing"
2021.emnlp-main.445,N19-1421,0,0.0292823,"(Lin et al., 2020a), by training on different partitions of the training data. Our experiments show that fine-tuning performs best, by learning both the content and the structure of the task, but suffers from overfitting and limited generalization to novel answers. Prompting methods have lower accuracy, but tend to show higher robustness to “adversarial” splits. Extending the models by prefix-tuning represents a “sweet spot” between task accuracy, generalization, and robustness. Machine commonsense reasoning has recently gained new traction, largely due to a collection of diverse benchmarks (Talmor et al., 2019; Bhagavatula et al., 2019; Sap et al., 2019) and the successful application of language modeling methods on these benchmarks (Ma et al., 2019; Shwartz et al., 2020; Bauer and Bansal, 2021). The most widely adopted approach to solve these commonsense reasoning tasks is by fine-tuning large pre-trained language models (LMs) (Devlin et al., 2019; Liu et al., 2019) on the task-specific training data. Meanwhile, it has been shown that language models are able to acquire certain commonsense background 2 Related Work knowledge, during their pre-training on large textual data (Petroni et al., 2019; D"
2021.emnlp-main.445,2020.findings-emnlp.103,0,0.0359545,"es in RICA (Zhou et al., 2020) show that LMs perform similar to random guessing in the zero-shot setting, they are heavily impacted by statistical biases, and are not robust to linguistic perturbations. Elazar et al. (2021) posit that while LMs can learn to perform well on commonsense tasks, their commonsense reasoning ability mostly comes from fine-tuning on the task data. Some works have sought to uncover what models learn through training on question answering datasets, exposing various dataset artifacts in the process (Jia and Liang, 2017; Kaushik and Lipton, 2018; Pugaliya et al., 2019). Welbl et al. (2020) found that models trained on the SQuAD2.0 dataset (Rajpurkar et al., 2018) are insensitive to the meaningful changes in the question and predict the same answer. Ko et al. (2020) found that BERT easily picks up the position bias in the SQuAD dataset (Rajpurkar et al., 2016) and models’ performance can drop by more than 50 points on f1-score when training on a biased subset. Sen and Saffari (2020) analyzed the model’s ability to generalize, by training on 5 different QA datasets, and found that no single dataset is robust to perturbations in the questions. Shah et al. (2020) tested models, tra"
2021.emnlp-main.445,2020.tacl-1.37,0,0.0385863,"e, it has been shown that language models are able to acquire certain commonsense background 2 Related Work knowledge, during their pre-training on large textual data (Petroni et al., 2019; Davison et al., 2019; Prior works probe the commonsense knowledge Ma et al., 2021). In light of these findings and learned by the LMs. Davison et al. (2019) mined 5474 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5474–5483 c November 7–11, 2021. 2021 Association for Computational Linguistics commonsense knowledge from LMs, using templates with masked tokens; Richardson and Sabharwal (2020) designed diagnostic tasks to probe LMs’ knowledge of definitions and taxonomic reasoning. The LAMA probes (Petroni et al., 2019) demonstrate that LMs can largely recover knowledge in existing (commonsense) knowledge graphs: they could thus be queried/prompted directly as knowledge bases (Shwartz et al., 2020; Shin et al., 2020). Ettinger (2020) diagnoses the BERT model, finding that it struggles with complex inference, role-based event prediction, and grasping the contextual impacts of negation. The logical commonsense probes in RICA (Zhou et al., 2020) show that LMs perform similar to random"
bilotti-nyberg-2006-evaluation,W03-0908,1,\N,Missing
bilotti-nyberg-2006-evaluation,H05-1117,0,\N,Missing
bilotti-nyberg-2006-evaluation,N04-1030,0,\N,Missing
C94-1012,1993.tmi-1.28,0,\N,Missing
C94-1012,C92-2113,0,\N,Missing
C94-1012,1991.mtsummit-papers.9,1,\N,Missing
cavalli-sforza-etal-2000-challenges,hakkani-etal-1998-english,1,\N,Missing
cavalli-sforza-etal-2000-challenges,C92-3168,1,\N,Missing
D08-1099,N07-1066,1,0.809616,"he Ephyra pattern matching approach learns textual patterns that relate question key terms to possible answers and applies these patterns to candidate sentences to extract factoid answers. The semantic approach generates a semantic representation of the question that is based on predicate-argument structures and extracts answer candidates from similar structures in the corpus. The source code of the answer extractors is included in OpenEphyra, an open source release of the system.2 The answer candidates from these extractors are combined and ranked by a statistical answer selection framework (Ko et al., 2007), which estimates the probability of an answer based on a number of answer validation and similarity features. Validation features use resources such as gazetteers and Wikipedia to verify an answer, whereas similarity features measure the syntactic and semantic similarity to other candidates, e.g. using string distance measures and WordNet relations. 2.2 Set Expander for Any Language (SEAL) Set expansion (SE) refers to expanding a given partial set of objects into a more complete set. SEAL3 (Wang and Cohen, 2007) is a SE system which accepts input elements (seeds) of some target set St and aut"
D17-1085,D14-1162,0,0.134371,"syntactic information of both context and question that are fed into the model. To gain an insight of how the encoding works, consider a sentence which syntactic tree consists of four nodes (o1, o2, o3, o4). A specific word is represented to be a sequence of nodes from its leave all the way to the root. We cover how this process work in detail in Section 3.1.1 and 3.1.2. Another input that will be fed into deep learning model is the embedding information for words and characters respectively. There are many ways to convert words in a sentence into a highdimensional embedding. We choose GloVe (Pennington et al., 2014b) to obtain a pre-trained and fixed vector for each word. Instead of using a fixed embedding, we use Convolutional Neural Networks (CNN) to model character level embedding, which values can be changed during training (Kim, 2014). To integrate both embeddings into the deep neural model, we feed the concatenation of them for the question and the context to be the input of the model. VP PP VBZ acts IN NP as the project coordinator Figure 1: The constituency tree of context “the architect or engineer acts as the project coordinator” “The Annual Conference” being the subject of “the basic unit of"
D17-1085,E17-1104,0,0.0132906,"the boundaries of the answers. Similar approaches can be used to encode other tree structures such as knowledge graphs and ontology relations. This work opened several potential new lines of research: 1) In the experiments of our paper we utilized the BiDAF model to retrieve answers from the context. Since there are no structures in the BiDAF models to specifically optimize for syntactic information, an attention mechanism that is designed for to utilize syntactic information should be studied. 2) Another direction of research is to incorporate SEST with deeper neural networks such as VD-CNN (Conneau et al., 2017) to improve learning capacity for syntactic embedding. 3) Tree structured information such as knowledge graphs and ontology structure should be studied and improve question answering tasks using similar techniques to the ones proposed in the paper. Related Work Reading Comprehension. Reading comprehension is a challenging task in NLP research. Since the release of the SQuAD data set, many works have been done to construct models on this massive question answering data set. Rajpurkar et. al. are among the first authors to explore the SQuAD. They used logistic regression with pos tagging informa"
D17-1085,D16-1264,0,0.68021,"n boost the performance of algorithms for the machine comprehension. We evaluate our approach using a state-of-the-art neural attention model on the SQuAD dataset. Experimental results demonstrate that our model can accurately identify the syntactic boundaries of the sentences and extract answers that are syntactically coherent over the baseline methods. 1 Introduction Whose role is to design the works, prepare the specifications and produce construction drawings, administer the contract, tender the works, and manage the works from inception to completion? Reading comprehension such as SQuAD (Rajpurkar et al., 2016) or NewsQA (Trischler et al., 2016) requires identifying a span from a given context, which is an extension to the traditional question answering task, aiming at responding questions posed by human with natural language (Nyberg et al., 2002; Ferrucci et al., 2010; Liu, 2017; Yang, 2017). Many works have been proposed to leverage deep neural networks for such question answering tasks, most of which involve learning the query-aware context representations (Dhingra et al., 2016; Seo et al., 2017; Wang and Jiang, 2016; Xiong et al., 2017). Although deep learning based methods demonstrated great po"
D17-1085,P15-1150,0,0.0997421,"Missing"
D17-1085,D14-1181,0,0.0892337,"be a sequence of nodes from its leave all the way to the root. We cover how this process work in detail in Section 3.1.1 and 3.1.2. Another input that will be fed into deep learning model is the embedding information for words and characters respectively. There are many ways to convert words in a sentence into a highdimensional embedding. We choose GloVe (Pennington et al., 2014b) to obtain a pre-trained and fixed vector for each word. Instead of using a fixed embedding, we use Convolutional Neural Networks (CNN) to model character level embedding, which values can be changed during training (Kim, 2014). To integrate both embeddings into the deep neural model, we feed the concatenation of them for the question and the context to be the input of the model. VP PP VBZ acts IN NP as the project coordinator Figure 1: The constituency tree of context “the architect or engineer acts as the project coordinator” “The Annual Conference” being the subject of “the basic unit of organization within the UMC” provides a critical clue for the model to skip over a large chunk of the text when answering the question “What is the basic unit of organization within the UMC”. As we show in the analysis section, a"
D17-1085,P14-5010,0,0.0161971,"cussed in detail in Section 4.5. In addition, a non-terminal node at a particular position in the syntactic sequence defines the begin and end indices of a phrase in the context. By measuring the similarity between syntactic seStructural Embedding of Syntactic Tree We detail the procedures of two alternative implementation of our methods: the Structural Embedding of Constituency Trees model (SECT) and the Structural Embedding of Dependency Trees model (SEDT). We assume that the syntactic information has already been generated in the preprocessing step using tools such as the Stanford CoreNLP (Manning et al., 2014). 3.1 Structural Embedding of Constituency Trees (SECT) Syntactic Sequence Extraction We first extract a syntactic collection C(p) for each word p, which consists of a set of nodes {o1 , o2 , . . . , od−1 , od } in the syntactic parse tree T . Each node oi can be a word, a grammatical category (e.g., part-of-speech tagging), or a dependency link label, depending on the type of syntactic tree we use. To construct syntactic embeddings, the first thing we need to do is to define a specific processing order A over the syntactic collection C(p), in which way we can extract a syntactic sequence S(p)"
D17-1228,W11-0609,0,0.118393,"Missing"
D17-1228,P16-1154,0,0.0159742,"ot i’m building a wall, right now Kennedy bot today, i am asking the congress for a new program to make a new effort to increase the tax privileges and to stimulate Table 1: Example responses from our Star Wars, Hillary, Trump, and Kennedy bots with scented conversation models. Introduction Neural encoder-decoder models have demonstrated great promise in many sequence generation tasks, including neural machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Luong et al., 2015; Wu et al., 2016), image captioning (Xu et al., 2015), summarization (Rush et al., 2015; Gu et al., 2016; Kikuchi et al., 2016), and conversation generation (Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Shao et al., 2017; Li et al., 2015). These encouraging early successes have motivated research interest in training more natural-sounding conversational systems based on large volumes of open-domain human-to-human interactions. In order to create more human-like patterns of conversation, the agents need to have recognizable (and tunable) style, just as individual humans do, and also need to accept guidance from separate information processing modules in order to increase qualit"
D17-1228,D16-1140,0,0.0227497,"wall, right now Kennedy bot today, i am asking the congress for a new program to make a new effort to increase the tax privileges and to stimulate Table 1: Example responses from our Star Wars, Hillary, Trump, and Kennedy bots with scented conversation models. Introduction Neural encoder-decoder models have demonstrated great promise in many sequence generation tasks, including neural machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Luong et al., 2015; Wu et al., 2016), image captioning (Xu et al., 2015), summarization (Rush et al., 2015; Gu et al., 2016; Kikuchi et al., 2016), and conversation generation (Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Shao et al., 2017; Li et al., 2015). These encouraging early successes have motivated research interest in training more natural-sounding conversational systems based on large volumes of open-domain human-to-human interactions. In order to create more human-like patterns of conversation, the agents need to have recognizable (and tunable) style, just as individual humans do, and also need to accept guidance from separate information processing modules in order to increase quality of responses. In an e"
D17-1228,D16-1168,0,0.0250985,"with 64 chat contexts spanning a range of topics in politic, science, and technology: the sort of questions we might ask in an entertaining political debate.5 To test the model’s ability to control output topic in section 7.4.3, we also created one hint per question. 7.2 Network Setup and Implementation Our encoder and decoder RNNs contains twolayer stacked LSTMs. Each LSTM layer has a memory size of 500. The network weights are randomly initialized using a uniform distribution (−0.08, 0.08), and are trained with the ADAM optimizer (Kingma and Ba, 2014), with 3 http://www.presidency.ucsb.edu/ Koncel-Kedziorski et al. (2016) also uses Star Wars scripts to test theme rewriting of algebra word problems. 5 See the Supplementary material. 4 an initial learning rate of 0.002. Gradients were clipped so their norm does not exceed 5. Each mini-batch contains 200 answers and their questions. The words of input sentences were first converted to 300-dimensional vector representations learned from the RNN based language modeling tool word2vec (Mikolov et al., 2013). The beginning and end of each passage are also padded with a special boundary symbol. During decoding, our model generates 500 candidate samples in parallel, the"
D17-1228,P16-1094,1,0.881067,"he line of research initiated by (Ritter et al., 2011) and (Vinyals and Le, 2015) who treat generation of conversational dialog as a data-drive statistical machine translation (SMT) problem. Sordoni et al. (2015) extended (Ritter et al., 2011) by re-scoring SMT outputs using a neural encoder-decoder model conditioned on conversation history. Recently, researchers have used neural encoder-decoder models to directly generate responses in an end-to-end fashion without relying on SMT phrase tables(Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Shao et al., 2017; Li et al., 2015). Li et al. (2016) defined a “persona” as the character that an artificial agent, as actor, plays or performs during conversational interactions. Their dataset requires user identification for all speakers in the training set, while our methods treat the base data (millions of twitter conversations) as unlabeled, and the target persona is defined simply by a relatively small sample of their speech. In this sense, the persona can be any set of text data. In our experiments, for example, we used a generic Star Wars character that was based on the entire set of Star Wars scripts (in addition to 46 million base con"
D17-1228,D16-1230,0,0.166864,"Missing"
D17-1228,D15-1166,0,0.416556,"agic solo. Hillary bot i’m running for president, i’m going to be talking about some of these things Trump bot i’m building a wall, right now Kennedy bot today, i am asking the congress for a new program to make a new effort to increase the tax privileges and to stimulate Table 1: Example responses from our Star Wars, Hillary, Trump, and Kennedy bots with scented conversation models. Introduction Neural encoder-decoder models have demonstrated great promise in many sequence generation tasks, including neural machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Luong et al., 2015; Wu et al., 2016), image captioning (Xu et al., 2015), summarization (Rush et al., 2015; Gu et al., 2016; Kikuchi et al., 2016), and conversation generation (Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Shao et al., 2017; Li et al., 2015). These encouraging early successes have motivated research interest in training more natural-sounding conversational systems based on large volumes of open-domain human-to-human interactions. In order to create more human-like patterns of conversation, the agents need to have recognizable (and tunable) style, just as individual humans do,"
D17-1228,D11-1054,0,0.0762747,"ne translation and quickly achieved state-of-the-art results (Bahdanau et al., 2014; Luong et al., 2015). As an extension, the attention mechanism enables the decoder to revisit the input sequence’s hidden states and dynamically collects information needed for each decoding step. Specifically, our conversation model is established based on a combination of the models of (Bahdanau et al., 2014) and (Luong et al., 2015) that we found to be effective. In section 3, we describe the attention-based neural encoder-decoder model we used in detail. This work follows the line of research initiated by (Ritter et al., 2011) and (Vinyals and Le, 2015) who treat generation of conversational dialog as a data-drive statistical machine translation (SMT) problem. Sordoni et al. (2015) extended (Ritter et al., 2011) by re-scoring SMT outputs using a neural encoder-decoder model conditioned on conversation history. Recently, researchers have used neural encoder-decoder models to directly generate responses in an end-to-end fashion without relying on SMT phrase tables(Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Shao et al., 2017; Li et al., 2015). Li et al. (2016) defined a “persona” as the character"
D17-1228,D15-1044,0,0.036099,"hese things Trump bot i’m building a wall, right now Kennedy bot today, i am asking the congress for a new program to make a new effort to increase the tax privileges and to stimulate Table 1: Example responses from our Star Wars, Hillary, Trump, and Kennedy bots with scented conversation models. Introduction Neural encoder-decoder models have demonstrated great promise in many sequence generation tasks, including neural machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Luong et al., 2015; Wu et al., 2016), image captioning (Xu et al., 2015), summarization (Rush et al., 2015; Gu et al., 2016; Kikuchi et al., 2016), and conversation generation (Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Shao et al., 2017; Li et al., 2015). These encouraging early successes have motivated research interest in training more natural-sounding conversational systems based on large volumes of open-domain human-to-human interactions. In order to create more human-like patterns of conversation, the agents need to have recognizable (and tunable) style, just as individual humans do, and also need to accept guidance from separate information processing modules in order t"
D17-1228,P15-1152,0,0.255575,"Missing"
D17-1228,N15-1020,1,0.953863,"gram to make a new effort to increase the tax privileges and to stimulate Table 1: Example responses from our Star Wars, Hillary, Trump, and Kennedy bots with scented conversation models. Introduction Neural encoder-decoder models have demonstrated great promise in many sequence generation tasks, including neural machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Luong et al., 2015; Wu et al., 2016), image captioning (Xu et al., 2015), summarization (Rush et al., 2015; Gu et al., 2016; Kikuchi et al., 2016), and conversation generation (Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Shao et al., 2017; Li et al., 2015). These encouraging early successes have motivated research interest in training more natural-sounding conversational systems based on large volumes of open-domain human-to-human interactions. In order to create more human-like patterns of conversation, the agents need to have recognizable (and tunable) style, just as individual humans do, and also need to accept guidance from separate information processing modules in order to increase quality of responses. In an extreme case, an agent may be micro-managed by a human user who uses the n"
D17-1228,D15-1199,0,0.0284613,"p(S|T ) empirically perform much better than p(T |S) on the relevance ranking task. Therefore, we directly apply Bayes’ rule to Equation 1, as in statistical machine translation (Brown et al., 1993), and use:  Tˆ = arg max log p(S|T ) + log p(T )}. (2) T Since p(T |S) is empirically biased towards p(T ), in practice, this objective also resembles the Maximum Mutual Information (MMI) objective function in (Li et al., 2015). The challenge now is to develop an effective search algorithm for a target words sequence that maximize the product in Equation 2. Here, we follow a similar process as in (Wen et al., 2015) which generates multiple target hypotheses with stochastic sampling based on p(T |S), and then ranks them with the objective function 2 above. However, as also observed by (Shao et al., 2017), step-by-step naive sampling can accumulate errors as the sequence gets longer. To reduce language errors of stochastic sampling, we introduce a sample selector to choose the next token among N stochastically sampled tokens based on the predicted output word distributions. The sample selector, which is a multilayer perceptron in our experiments, takes the following features: 1) the log-probability of cur"
D17-1228,P15-2116,1,0.862309,"Missing"
D17-1228,J93-2003,0,\N,Missing
D17-1315,P16-1160,0,0.0344896,"ur data driven approach. Also, their focus is on brand names. Hiranandani et al. (2017) have proposed an approach to recommend brand names based on brand/product description. However, they consider only a limited number of features like memorability and readability. Smith et al. (2014) devise an approach to generate portmanteaus, which requires user-defined weights for attributes like sounding good. Generating a portmanteau from two root words can be viewed as a S2S problem. Recently, neural approaches have been used for S2S problems (Sutskever et al., 2014) such as MT. Ling et al. (2015) and Chung et al. (2016) have shown that character-level neural sequence models work as well as word-level ones for language modelling and MT. Zoph and Knight (2016) propose S2S models for multi-source MT, which have multi-sequence inputs, similar to our case. 7 Conclusion We have proposed an end-to-end neural system to model portmanteau generation. Our experiments show the efficacy of proposed system in predicting portmanteaus given the root words. We conclude that pre-training character embeddings on the English vocabulary helps the model. Through human evaluation we show that our model’s predictions are superior t"
D17-1315,N15-1021,0,0.486256,"odern Hebrew BatEl (1996); Berman (1989) and Spanish Pi˜neros (2004). Their short length makes them ideal for headlines and brandnames (Gabler, 2015). Unlike better-defined morphological phenomenon such as inflection and derivation, portmanteau generation ∗ * denotes equal contribution is difficult to capture using a set of rules. For instance, Shaw et al. (2014) state that the composition of the portmanteau from its root words depends on several factors, two important ones being maintaining prosody and retaining character segments from the root words, especially the head. An existing work by Deri and Knight (2015) aims to solve the problem of predicting portmanteau using a multi-tape FST model, which is datadriven, unlike prior approaches. Their methods rely on a grapheme to phoneme converter, which takes into account the phonetic features of the language, but may not be available or accurate for non-dictionary words, or low resource languages. Prior works, such as Faruqui et al. (2016), have demonstrated the efficacy of neural approaches for morphological tasks such as inflection. We hypothesize that such neural methods can (1) provide a simpler and more integrated end-to-end framework than multiple F"
D17-1315,N16-1077,1,0.680948,"composition of the portmanteau from its root words depends on several factors, two important ones being maintaining prosody and retaining character segments from the root words, especially the head. An existing work by Deri and Knight (2015) aims to solve the problem of predicting portmanteau using a multi-tape FST model, which is datadriven, unlike prior approaches. Their methods rely on a grapheme to phoneme converter, which takes into account the phonetic features of the language, but may not be available or accurate for non-dictionary words, or low resource languages. Prior works, such as Faruqui et al. (2016), have demonstrated the efficacy of neural approaches for morphological tasks such as inflection. We hypothesize that such neural methods can (1) provide a simpler and more integrated end-to-end framework than multiple FSTs used in the previous work, and (2) automatically capture features such as phonetic similarity through the use of character embeddings, removing the need for explicit 2917 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2917–2922 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics grapheme-"
D17-1315,W04-3250,0,0.073361,"Missing"
D17-1315,P12-1074,0,0.426566,"Missing"
D17-1315,N16-1004,0,0.0265837,"based on brand/product description. However, they consider only a limited number of features like memorability and readability. Smith et al. (2014) devise an approach to generate portmanteaus, which requires user-defined weights for attributes like sounding good. Generating a portmanteau from two root words can be viewed as a S2S problem. Recently, neural approaches have been used for S2S problems (Sutskever et al., 2014) such as MT. Ling et al. (2015) and Chung et al. (2016) have shown that character-level neural sequence models work as well as word-level ones for language modelling and MT. Zoph and Knight (2016) propose S2S models for multi-source MT, which have multi-sequence inputs, similar to our case. 7 Conclusion We have proposed an end-to-end neural system to model portmanteau generation. Our experiments show the efficacy of proposed system in predicting portmanteaus given the root words. We conclude that pre-training character embeddings on the English vocabulary helps the model. Through human evaluation we show that our model’s predictions are superior to the baseline. We have also released our dataset and code6 to encourage further research on the phenomenon of portmanteaus. We also release"
D19-5818,Q18-1023,0,0.0259056,"and a pointer-generator decoder (See et al., 2017) to synthesize the answer. The model also applies common sense knowledge from an external knowledge base ConceptNet (Speer et al., 2016). The model encodes the context and question using Bi-LSTM layers, and BiDAF attention (Seo et al., 2016), then applies self-attention (Cheng et al., 2016) to perform multihop reasoning. The context is also attended by an encoded commonsense representation. Finally, the decoder generates the answer sequence and copies key spans from the context. This model has achieved promising performance on the NarrativeQA (Kocisky et al., 2018) and WikiHop (Welbl et al., 2018) datasets. We choose this model to test how it generalizes to extractive datasets and whether common sense knowledge is helpful for other QA tasks. We use the official implementation of this model. 6 We also focus on diversity when selecting models. Each of the models described in this section is developed for a different task and they have relatively heterogeneous architecture. We specifically chose models that had strong performance on at least one popular QA dataset, particularly the ones used in this study. Some of the models were not designed to handle the"
D19-5818,D18-1454,0,0.0332797,"Missing"
D19-5818,P18-1161,0,0.054736,"Missing"
D19-5818,D16-1053,0,0.0853644,"Missing"
D19-5818,P18-2124,0,0.0417442,"es a larger proportion of question-answer pairs. In addition, our quantitative analysis scales to larger data sizes. We focus on characterizing model outputs and errors, and in the process, make inferences about the MRC challenges. We adopt both automatic and manual analysis of QA pairs across all dataset-model pairs. We do not focus on explainability in this study, although we aim to conclude why a model performs in a certain way throughout our analysis. 3 well-understood, and it tests a model’s tolerance for paraphrasing and coreferences between the question and context. Although SQuAD 2.0 (Rajpurkar et al., 2018) is the most recent version of this dataset, we focus on SQuAD 1.1 because our selected models are not designed to handle the unanswerable questions in SQuAD 2.0. HotpotQA (Yang et al., 2018) is similar to SQuAD but includes additional linguistic phenomena. HotpotQA stresses multihop reasoning, which requires a model to aggregate information from multiple relevant passages to locate the answers. It also contains questions that require a model to compare two entities and select the correct one. We use the distractor version of HotpotQA, where 10 passages are provided per question; two of the pa"
D19-5818,D16-1264,0,0.0377389,"questions and corresponding documents returned by the search engine as contexts. We include MSMARCO as the only dataset that requires models to freely generate answer sequences instead of selecting a span. Although most of the models we test are span-based, we aim to evaluate how well the models adapt to a different answer type. Datasets We selected four datasets for evaluating model performance, each of which we describe briefly. We chose datasets that are relatively well-known and test a variety of non-overlapping capabilities. Table 1 summarizes key characteristics for the datasets. SQuAD (Rajpurkar et al., 2016) is one of the first large-scale extractive question answering datasets. We include SQuAD in this study because it is 126 4 Models et al., 2018) is mainly aimed at improving OpenDomain Question Answering. The model employs a paragraph selector to filter out noisy paragraphs and a paragraph reader to extract the correct answer from those denoised paragraphs. The paragraph selector encodes all paragraphs and the question using LSTM layers and self-attention. A paragraph reader then estimates a probability distribution over all possible spans. This architecture is shown to be effective on many op"
D19-5818,W18-2602,0,0.155388,"ia Crowd Span 113K SearchQA Web Jeopardy Span 140K MSMARCO Web Bing Free-form 1.01M Based on our findings, we conclude with some guidelines which future researchers can benefit from while building new models and datasets. 2 Related Work Table 1: Dataset Summary Wadhwa et al. (2018) explored the performance of several MRC models on SQuAD and inferred common areas of difficulty. Kaushik and Lipton (2018) examined model performance across several MRC datasets, including SQuAD. This study questioned the effective difficulty of MRC tasks by varying the amount of input data available to the models. Rondeau and Hazen (2018) presented a systematic approach for identifying the most salient features for a question’s difficulty on SQuAD. They define question categories based on the number of models that could get the correct output on the question. Sugawara et al. (2017) analyzed 6 MRC datasets on the metrics of prerequisite skills and readability, which are defined from a human’s perspective. Feng et al. (2018) explored model explainability on MRC and other tasks by reducing input spans until a given model failed to generate a correct prediction. Talmor and Berant (2019) investigated generalization and transferabil"
D19-5818,D18-1407,0,0.0346105,"Missing"
D19-5818,P17-1099,0,0.0153985,"graphs and the question using LSTM layers and self-attention. A paragraph reader then estimates a probability distribution over all possible spans. This architecture is shown to be effective on many open-domain datasets like QuasarT(Dhingra et al., 2017), SearchQA(Dunn et al., 2017), TriviaQA(Joshi et al., 2017) and CuratedTREC(Baudis and Sediv´y, 2015). We use the official implementation of this model.5 CommonSenseMultihop(CSM) (Bauer et al., 2018) generates an answer sequence rather than selecting a span. It uses an attention mechanism to reason over context and a pointer-generator decoder (See et al., 2017) to synthesize the answer. The model also applies common sense knowledge from an external knowledge base ConceptNet (Speer et al., 2016). The model encodes the context and question using Bi-LSTM layers, and BiDAF attention (Seo et al., 2016), then applies self-attention (Cheng et al., 2016) to perform multihop reasoning. The context is also attended by an encoded commonsense representation. Finally, the decoder generates the answer sequence and copies key spans from the context. This model has achieved promising performance on the NarrativeQA (Kocisky et al., 2018) and WikiHop (Welbl et al., 2"
D19-5818,P17-1147,0,0.0225077,"this study because it is 126 4 Models et al., 2018) is mainly aimed at improving OpenDomain Question Answering. The model employs a paragraph selector to filter out noisy paragraphs and a paragraph reader to extract the correct answer from those denoised paragraphs. The paragraph selector encodes all paragraphs and the question using LSTM layers and self-attention. A paragraph reader then estimates a probability distribution over all possible spans. This architecture is shown to be effective on many open-domain datasets like QuasarT(Dhingra et al., 2017), SearchQA(Dunn et al., 2017), TriviaQA(Joshi et al., 2017) and CuratedTREC(Baudis and Sediv´y, 2015). We use the official implementation of this model.5 CommonSenseMultihop(CSM) (Bauer et al., 2018) generates an answer sequence rather than selecting a span. It uses an attention mechanism to reason over context and a pointer-generator decoder (See et al., 2017) to synthesize the answer. The model also applies common sense knowledge from an external knowledge base ConceptNet (Speer et al., 2016). The model encodes the context and question using Bi-LSTM layers, and BiDAF attention (Seo et al., 2016), then applies self-attention (Cheng et al., 2016) to p"
D19-5818,D18-1546,0,0.0215683,"Machine Reading for Question Answering, pages 125–136 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics Dataset Data Source Answer Size SQuAD Wikipedia Crowd Span 100K HotpotQA Wikipedia Crowd Span 113K SearchQA Web Jeopardy Span 140K MSMARCO Web Bing Free-form 1.01M Based on our findings, we conclude with some guidelines which future researchers can benefit from while building new models and datasets. 2 Related Work Table 1: Dataset Summary Wadhwa et al. (2018) explored the performance of several MRC models on SQuAD and inferred common areas of difficulty. Kaushik and Lipton (2018) examined model performance across several MRC datasets, including SQuAD. This study questioned the effective difficulty of MRC tasks by varying the amount of input data available to the models. Rondeau and Hazen (2018) presented a systematic approach for identifying the most salient features for a question’s difficulty on SQuAD. They define question categories based on the number of models that could get the correct output on the question. Sugawara et al. (2017) analyzed 6 MRC datasets on the metrics of prerequisite skills and readability, which are defined from a human’s perspective. Feng et"
D19-5818,P17-1075,0,0.025725,"set Summary Wadhwa et al. (2018) explored the performance of several MRC models on SQuAD and inferred common areas of difficulty. Kaushik and Lipton (2018) examined model performance across several MRC datasets, including SQuAD. This study questioned the effective difficulty of MRC tasks by varying the amount of input data available to the models. Rondeau and Hazen (2018) presented a systematic approach for identifying the most salient features for a question’s difficulty on SQuAD. They define question categories based on the number of models that could get the correct output on the question. Sugawara et al. (2017) analyzed 6 MRC datasets on the metrics of prerequisite skills and readability, which are defined from a human’s perspective. Feng et al. (2018) explored model explainability on MRC and other tasks by reducing input spans until a given model failed to generate a correct prediction. Talmor and Berant (2019) investigated generalization and transferability of 10 MRC datasets and analyzed factors that contribute to these characteristics. Our study casts a broader net by testing four MRC datasets against four models. The study tests a greater range of linguistic phenomena and examines a larger prop"
D19-5818,P19-1485,0,0.0190564,"mount of input data available to the models. Rondeau and Hazen (2018) presented a systematic approach for identifying the most salient features for a question’s difficulty on SQuAD. They define question categories based on the number of models that could get the correct output on the question. Sugawara et al. (2017) analyzed 6 MRC datasets on the metrics of prerequisite skills and readability, which are defined from a human’s perspective. Feng et al. (2018) explored model explainability on MRC and other tasks by reducing input spans until a given model failed to generate a correct prediction. Talmor and Berant (2019) investigated generalization and transferability of 10 MRC datasets and analyzed factors that contribute to these characteristics. Our study casts a broader net by testing four MRC datasets against four models. The study tests a greater range of linguistic phenomena and examines a larger proportion of question-answer pairs. In addition, our quantitative analysis scales to larger data sizes. We focus on characterizing model outputs and errors, and in the process, make inferences about the MRC challenges. We adopt both automatic and manual analysis of QA pairs across all dataset-model pairs. We"
D19-5818,W18-2610,1,0.428606,"lable at https://github. com/jamesrt95/Neural-QA-Eval ∗ * Equal contribution 125 Proceedings of the Second Workshop on Machine Reading for Question Answering, pages 125–136 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics Dataset Data Source Answer Size SQuAD Wikipedia Crowd Span 100K HotpotQA Wikipedia Crowd Span 113K SearchQA Web Jeopardy Span 140K MSMARCO Web Bing Free-form 1.01M Based on our findings, we conclude with some guidelines which future researchers can benefit from while building new models and datasets. 2 Related Work Table 1: Dataset Summary Wadhwa et al. (2018) explored the performance of several MRC models on SQuAD and inferred common areas of difficulty. Kaushik and Lipton (2018) examined model performance across several MRC datasets, including SQuAD. This study questioned the effective difficulty of MRC tasks by varying the amount of input data available to the models. Rondeau and Hazen (2018) presented a systematic approach for identifying the most salient features for a question’s difficulty on SQuAD. They define question categories based on the number of models that could get the correct output on the question. Sugawara et al. (2017) analyzed"
D19-5818,Q18-1021,0,0.0224301,"e et al., 2017) to synthesize the answer. The model also applies common sense knowledge from an external knowledge base ConceptNet (Speer et al., 2016). The model encodes the context and question using Bi-LSTM layers, and BiDAF attention (Seo et al., 2016), then applies self-attention (Cheng et al., 2016) to perform multihop reasoning. The context is also attended by an encoded commonsense representation. Finally, the decoder generates the answer sequence and copies key spans from the context. This model has achieved promising performance on the NarrativeQA (Kocisky et al., 2018) and WikiHop (Welbl et al., 2018) datasets. We choose this model to test how it generalizes to extractive datasets and whether common sense knowledge is helpful for other QA tasks. We use the official implementation of this model. 6 We also focus on diversity when selecting models. Each of the models described in this section is developed for a different task and they have relatively heterogeneous architecture. We specifically chose models that had strong performance on at least one popular QA dataset, particularly the ones used in this study. Some of the models were not designed to handle the challenges presented by one or m"
D19-5818,D18-1259,0,0.0231082,"nferences about the MRC challenges. We adopt both automatic and manual analysis of QA pairs across all dataset-model pairs. We do not focus on explainability in this study, although we aim to conclude why a model performs in a certain way throughout our analysis. 3 well-understood, and it tests a model’s tolerance for paraphrasing and coreferences between the question and context. Although SQuAD 2.0 (Rajpurkar et al., 2018) is the most recent version of this dataset, we focus on SQuAD 1.1 because our selected models are not designed to handle the unanswerable questions in SQuAD 2.0. HotpotQA (Yang et al., 2018) is similar to SQuAD but includes additional linguistic phenomena. HotpotQA stresses multihop reasoning, which requires a model to aggregate information from multiple relevant passages to locate the answers. It also contains questions that require a model to compare two entities and select the correct one. We use the distractor version of HotpotQA, where 10 passages are provided per question; two of the passages are relevant and the remaining eight contain keywords that appear in the question. We selected HotpotQA to test how well models handle consistently challenging multihop and comparison"
D19-6003,W15-1405,0,0.0499029,"Missing"
D19-6003,J87-3004,0,0.770412,"onal linguists, cognitive psychologists (see for instance (Davis, 2014)): at the high level, we can identify declarative commonsense, whose scope encompassess factual knowledge, e.g., ‘the sky is blue’, ‘Paris is in France’; taxonomic knowledge, e.g., ‘football players are athletes’, ‘cats are mammals’; relational knowledge, e.g., ‘the nose is part of the skull’, ‘handwriting requires a hand and a writing instrument’; procedural commonsense, which includes prescriptive knowledge, e.g., ‘one needs an oven before baking cakes’, ‘the electricity should be off while the switch is being repaired’ (Hobbs et al., 1987); sentiment knowledge, e.g., ‘rushing to the hospital makes people worried’, ‘being in vacation makes people relaxed’; and metaphorical knowledge (e.g., ‘time flies’, ‘raining cats and dogs’). We believe that it is important to identifiy the most appropriate commonsense knowledge type required for specific tasks, in order to get better downstream performance. Once the knowledge type is identified, we can then select the appropriate knowledge-base(s), and the suitable neural integration mechanisms (e.g., attention-based injection, pre-training, or auxiliary training objectives). Non-extractive"
D19-6003,P98-1013,0,0.112276,"Missing"
D19-6003,D17-1082,0,0.0293941,"ultiple commonsense datasets. Our results and analysis show that attention-based injection seems to be a preferable choice for knowledge integration and that the degree of domain overlap, between knowledge bases and datasets, plays a crucial role in determining model success. 1 Introduction With the recent success of large pre-trained language models (Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019; Liu et al., 2019), model performance has reached or surpassed human-level capability on many previous question-answering (QA) benchmarks (Hermann et al., 2015; Rajpurkar et al., 2016; Lai et al., 2017). However, these benchmarks do not directly challenge model reasoning capability, as they require only marginal use of external knowledge to select the correct answer, i.e., all the evidence required to solve questions in these benchmarks is explicit in the context lexical space. Efforts have been made towards building more challenging datasets that, by design, require models to synthesize external commonsense ∗ Work was done during an internship at Bosch Research. 22 Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing, pages 22–32 c Hongkong, China, Novem"
D19-6003,D18-1454,0,0.0518136,"Missing"
D19-6003,P19-1470,0,0.0297685,"ATOMIC, whose focus is on folk psychology and related general implications; we could frame our goal as evaluating whether ATOMIC can provide relevant knowledge to help answer these questions. However, one challenge to this strategy is that heads and tails of knowledge triples in ATOMIC are short sentences or verb phrases, while rare words and person-references are reduced to blanks and PersonX/PersonY, respectively. This calls for a new matching procedure, different from the ConceptNet extraction strategy, for eliciting ATOMIC-specific relations: we rely on the recently-published COMET model (Bosselut et al., 2019) to generate new ATOMIC relations, with intermediate phrasal resolutions. In particular, we first segmented all dialogues, questions, and answer-options into sentences. We further segment long sentences into sub-sentences, using commas as seperators. Because only verb-phrases satisfy the definition of an “event” in ATOMIC (i.e., relations are only invoked by verbs), we remove all sentences/subsentences that do not contain any verb. Next, we use a pre-trained COMET model (Bosselut et al., 2019) to generate all possible ATOMIC relations, for all candidate sentences/sub-sentences and we use greed"
D19-6003,P19-1028,0,0.0269193,"g door is convenient for two direction travel, but it also serves as a security measure at a what? Answer choices: A. Bank*; B. Library; C. Department Store; D. Mall; E. New York Commonsense knowledge integration has also received a lot of attention on many other tasks. Tandon et al. (2018) proposed to use commonsense knowledge as hard/soft constraints to bias the neural model’s prediction on a procedural text comprehension task. Ma et al. (2018) proposed to used embedded affective commonsense knowledge inside LSTM cell to control the information flow in each gate for sentiment analysis task. Li and Srikumar (2019) presented a framework to convert declarative knowlegde into first-order logic that enhance neural networks’ training and prediction. Peters et al. (2019) and Levine et al. (2019) both tried to injecting knowlegde into language models by pretraining on knowledge bases. Table 2: An example from the CommonsenseQA dataset; the asterisk (*) denotes the correct answer. 2 Related Work It has been recognized that many recent QA tasks require external knowledge or commonsense to solve, and numerous efforts have been made in injecting commonsense in neural models. Bauer 1 From a terminological standpoi"
D19-6003,D19-1282,0,0.191571,"Missing"
D19-6003,2021.ccl-1.108,0,0.290683,"Missing"
D19-6003,P18-1076,0,0.0571807,"l., 2019) datasets. An example from DREAM that requires commonsense is shown in Table 1, and an example from CommonsenseQA is shown in Table 2. Our experimental results and analysis suggest that attention-based injection is preferable for knowledge integration and that the degree of domain overlap, between knowledge-base and dataset, is vital to model success.1 et al. (2018) introduced a pipeline for extracting grounded multi-hop commonsense relation paths from ConceptNet and proposed to inject commonsense knowledge into neural models’ intermediate representations, using attention. Similarly, Mihaylov and Frank (2018) also proposed to extract relevant knowledge triples from ConceptNet and use Key-Value Retrieval (Miller et al., 2016) to gather information from knowledge to enhance the neural representation. Zhong et al. (2018) proposed to pre-train a scoring function using knowledge triples from ConceptNet, to model the direct and indirect relation between concepts. This scoring function was then fused with QA models to make the final prediction. Pan et al. (2019a) introduced an entity discovery and linking system to identify the most salient entities in the question and answer-options. Wikipedia abstracts"
D19-6003,D16-1147,0,0.0355405,"is shown in Table 2. Our experimental results and analysis suggest that attention-based injection is preferable for knowledge integration and that the degree of domain overlap, between knowledge-base and dataset, is vital to model success.1 et al. (2018) introduced a pipeline for extracting grounded multi-hop commonsense relation paths from ConceptNet and proposed to inject commonsense knowledge into neural models’ intermediate representations, using attention. Similarly, Mihaylov and Frank (2018) also proposed to extract relevant knowledge triples from ConceptNet and use Key-Value Retrieval (Miller et al., 2016) to gather information from knowledge to enhance the neural representation. Zhong et al. (2018) proposed to pre-train a scoring function using knowledge triples from ConceptNet, to model the direct and indirect relation between concepts. This scoring function was then fused with QA models to make the final prediction. Pan et al. (2019a) introduced an entity discovery and linking system to identify the most salient entities in the question and answer-options. Wikipedia abstracts of these entities are then extracted and appended to the reference documents to provide additional information. Weiss"
D19-6003,Q19-1014,0,0.051972,"19. 2019 Association for Computational Linguistics Accordingly, in this work we conduct a comparison study of different knowledge bases and knowledge integration methods, and we evaluate model performance on two multiple-choice QA datasets that explicitly require commonsense reasoning. In particular, we used ConceptNet (Speer et al., 2016) and the recently-introduced ATOMIC (Sap et al., 2019) knowledge resources, integrating them with the Option Comparison Network model (OCN; Ran et al. (2019)), a recent stateof-the-art model for multiple choice QA tasks. We evalutate our models on the DREAM (Sun et al., 2019) and CommonsenseQA (Talmor et al., 2019) datasets. An example from DREAM that requires commonsense is shown in Table 1, and an example from CommonsenseQA is shown in Table 2. Our experimental results and analysis suggest that attention-based injection is preferable for knowledge integration and that the degree of domain overlap, between knowledge-base and dataset, is vital to model success.1 et al. (2018) introduced a pipeline for extracting grounded multi-hop commonsense relation paths from ConceptNet and proposed to inject commonsense knowledge into neural models’ intermediate representation"
D19-6003,S18-1119,0,0.0483568,"Missing"
D19-6003,N19-1421,0,0.0465816,"l Linguistics Accordingly, in this work we conduct a comparison study of different knowledge bases and knowledge integration methods, and we evaluate model performance on two multiple-choice QA datasets that explicitly require commonsense reasoning. In particular, we used ConceptNet (Speer et al., 2016) and the recently-introduced ATOMIC (Sap et al., 2019) knowledge resources, integrating them with the Option Comparison Network model (OCN; Ran et al. (2019)), a recent stateof-the-art model for multiple choice QA tasks. We evalutate our models on the DREAM (Sun et al., 2019) and CommonsenseQA (Talmor et al., 2019) datasets. An example from DREAM that requires commonsense is shown in Table 1, and an example from CommonsenseQA is shown in Table 2. Our experimental results and analysis suggest that attention-based injection is preferable for knowledge integration and that the degree of domain overlap, between knowledge-base and dataset, is vital to model success.1 et al. (2018) introduced a pipeline for extracting grounded multi-hop commonsense relation paths from ConceptNet and proposed to inject commonsense knowledge into neural models’ intermediate representations, using attention. Similarly, Mihaylov"
D19-6003,D19-5804,0,0.377257,"om ConceptNet and proposed to inject commonsense knowledge into neural models’ intermediate representations, using attention. Similarly, Mihaylov and Frank (2018) also proposed to extract relevant knowledge triples from ConceptNet and use Key-Value Retrieval (Miller et al., 2016) to gather information from knowledge to enhance the neural representation. Zhong et al. (2018) proposed to pre-train a scoring function using knowledge triples from ConceptNet, to model the direct and indirect relation between concepts. This scoring function was then fused with QA models to make the final prediction. Pan et al. (2019a) introduced an entity discovery and linking system to identify the most salient entities in the question and answer-options. Wikipedia abstracts of these entities are then extracted and appended to the reference documents to provide additional information. Weissenborn et al. (2018) proposed a strategy of dynamically refining word embeddings by reading input text as well as external knowledge, such as ConceptNet and Wikipedia abstracts. More recently, Lin et al. (2019) proposed to extract subgraphs from ConceptNet and embed the knowledge using Graph Convolutional Networks (Kipf and Welling, 2"
D19-6003,D18-1006,0,0.0646146,": How does the woman feel about driving to work? Answer choices: A. She doesn’t mind it as the road conditions are good.* B. She is unhappy to drive such a long way everyday. C. She is tired of driving in heavy traffic. Table 1: An example from the DREAM dataset; the asterisk (*) denotes the correct answer. Question: A revolving door is convenient for two direction travel, but it also serves as a security measure at a what? Answer choices: A. Bank*; B. Library; C. Department Store; D. Mall; E. New York Commonsense knowledge integration has also received a lot of attention on many other tasks. Tandon et al. (2018) proposed to use commonsense knowledge as hard/soft constraints to bias the neural model’s prediction on a procedural text comprehension task. Ma et al. (2018) proposed to used embedded affective commonsense knowledge inside LSTM cell to control the information flow in each gate for sentiment analysis task. Li and Srikumar (2019) presented a framework to convert declarative knowlegde into first-order logic that enhance neural networks’ training and prediction. Peters et al. (2019) and Levine et al. (2019) both tried to injecting knowlegde into language models by pretraining on knowledge bases."
D19-6003,D19-1005,0,0.0503086,"Missing"
D19-6003,P17-1018,0,0.0785365,"Missing"
D19-6003,D16-1264,0,0.0662131,"across benchmarks from multiple commonsense datasets. Our results and analysis show that attention-based injection seems to be a preferable choice for knowledge integration and that the degree of domain overlap, between knowledge bases and datasets, plays a crucial role in determining model success. 1 Introduction With the recent success of large pre-trained language models (Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019; Liu et al., 2019), model performance has reached or surpassed human-level capability on many previous question-answering (QA) benchmarks (Hermann et al., 2015; Rajpurkar et al., 2016; Lai et al., 2017). However, these benchmarks do not directly challenge model reasoning capability, as they require only marginal use of external knowledge to select the correct answer, i.e., all the evidence required to solve questions in these benchmarks is explicit in the context lexical space. Efforts have been made towards building more challenging datasets that, by design, require models to synthesize external commonsense ∗ Work was done during an internship at Bosch Research. 22 Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing, pages 22–32 c Hon"
D19-6003,D18-1009,0,0.0293637,"Carnegie Mellon University † Department of Mechanical Engineering, College of Engineering, Carnegie Mellon University § Intelligent IoT, Bosch Research and Technology Center (Pittsburgh, USA) {kaixinm, jmf1, qlv, ehn}@cs.cmu.edu alessandro.oltramari@us.bosch.com Abstract knowledge and leverage more sophisticated reasoning mechanisms (Zhang et al., 2018; Ostermann et al., 2018), showing that the previous stateof-the-art models often struggle to solve these newer tasks reliably. As a result, commonsense has received a lot of attention in other areas as well, such as natural language inference (Zellers et al., 2018b, 2019) and visual question answering (Zellers et al., 2018a). Despite the importance of commonsense knowledge, however, previous work on QA methods takes a coarse-grained view of commonsense, without considering the subtle differences across the various knowledge types and resources. Such differences have been discussed at length in AI by philosophers, computational linguists, cognitive psychologists (see for instance (Davis, 2014)): at the high level, we can identify declarative commonsense, whose scope encompassess factual knowledge, e.g., ‘the sky is blue’, ‘Paris is in France’; taxonomic"
D19-6003,P19-1472,0,0.04887,"Missing"
D19-6003,C98-1013,0,\N,Missing
D19-6003,N19-1423,0,\N,Missing
H01-1039,H92-1022,0,0.109179,"(currently, person names) in the body of the document, and adds appropriate annotations to the document. IVEAnnotator. For each named entity (person) annotation, checks a networked database for supplemental information about that individual. An interface to a database of information about individuals, publications, and organizations, created as part of the Information Validation and Evaluation project at CMU [12]. Implemented using Java Database Connectivity (JDBC). BrillAnnotator. Accepts a user-defined annotation (e.g., PASSAGE) and adds a new annotation created by calling the Brill Tagger [1] on the associated text. Implemented via a TCP/IP socket protocol which accesses a remote instance of the tagger running as a network service. ChartAnnotator. Accepts a user-defined annotation, and adds new annotations based on the results of bottom-up chart parsing with a user-defined grammar. The user can select which linguistic categories (e.g., NP VP, etc.) are to be annotated. RegexpAnnotator. Annotates passages which match a userdefined regular expression. 4.4 Transformer Nodes BrillTransformer. Similar to the BrillAnnotator (see above), but operates directly on the document body (does n"
H01-1039,X96-1039,0,0.12709,"olkits or class libraries for specific types of IR or NLP problems. Examples include the SMART system for indexing and retrieval [17], the FIRE [18] and InfoGrid [15] class models for information retrieval applications, and the ATTICS [11] system for text categorization and machine learning. Some prior work has also focused on the user interface, notably FireWorks [9] and SketchTrieve [9]2 . Other systems such as GATE [4] and Corelli [20] have centered on specific approaches to NLP applications. The Tipster II architecture working group summarized the requirements for an ideal IR architecture [6], which include: Standardization. Specify a standard set of functions and interfaces for information services.  Rapid Deployment. Speed up the initial development of new applications. This work is supported by National Science Foundation (KDI) grant number 9873009.  For further discussion on how these systems compare with the present work, see Section 7. Maintainability. Use standardized modules to support plugand-play updates. Flexibility. Enhance performance by allowing novel combinations of existing components. Evaluation. Isolate and test specific modules side-by-side in the same applica"
H01-1039,C92-2082,0,0.00713694,"tations) in a popup viewer window. Named Entity Association. A node chain which performs named-entity annotation using a phi-square measure[3], producin a MatrixDocument object (a user-defined Document subclass, which represents the association matrix). Note that the addition of a specialized Document subclass does not require recompilation of IIM (although the user must take care that specialized document objects are properly handled by user-defined nodes). Question Answering. A node chain which answers “What is” questions by querying the web for relevant documents, finding relevant passages [8, 10], and synthesizing answers from the results of various regular expression matches3. 6. PERFORMANCE In order to support accurate side-by-side evaluation of different modules, IIM implements two kinds of instrumentation for runtime performance data: Per-Node Run Time. The ChainRunner and Box classes automatically maintain run-time statistics for every node in a chain (including user-defined nodes). These statistics are printed at the end of every run. Node-Specific Statistics. For user-defined nodes, it may be useful to report task-specific statistics (e.g., for an Annotator, the total number of"
H01-1039,A97-1036,0,0.0718561,"on, it is also important to provide a graphical interface for effective task visualization and realtime control. Prior architecture-related work has focused on toolkits or class libraries for specific types of IR or NLP problems. Examples include the SMART system for indexing and retrieval [17], the FIRE [18] and InfoGrid [15] class models for information retrieval applications, and the ATTICS [11] system for text categorization and machine learning. Some prior work has also focused on the user interface, notably FireWorks [9] and SketchTrieve [9]2 . Other systems such as GATE [4] and Corelli [20] have centered on specific approaches to NLP applications. The Tipster II architecture working group summarized the requirements for an ideal IR architecture [6], which include: Standardization. Specify a standard set of functions and interfaces for information services.  Rapid Deployment. Speed up the initial development of new applications. This work is supported by National Science Foundation (KDI) grant number 9873009.  For further discussion on how these systems compare with the present work, see Section 7. Maintainability. Use standardized modules to support plugand-play updates. Flexi"
ide-etal-2014-language,windhouwer-2012-relcat,0,\N,Missing
ide-etal-2014-language,cieri-etal-2014-new,1,\N,Missing
ide-etal-2014-language,piperidis-2012-meta,0,\N,Missing
ide-etal-2014-language,cassidy-etal-2014-alveo,0,\N,Missing
ide-etal-2014-language,J08-3010,0,\N,Missing
ide-etal-2014-language,W14-5211,1,\N,Missing
ide-etal-2014-language,P13-1166,0,\N,Missing
ide-etal-2014-language,W14-5204,1,\N,Missing
ko-etal-2006-analyzing,geutner-etal-2002-design,0,\N,Missing
ko-etal-2006-exploiting,N03-1022,0,\N,Missing
ko-etal-2006-exploiting,P04-3018,1,\N,Missing
kupsc-etal-2004-pronominal,J90-4001,0,\N,Missing
kupsc-etal-2004-pronominal,1995.iwpt-1.15,0,\N,Missing
kupsc-etal-2004-pronominal,W98-1119,0,\N,Missing
kupsc-etal-2004-pronominal,C88-1021,0,\N,Missing
kupsc-etal-2004-pronominal,briscoe-carroll-2002-robust,0,\N,Missing
kupsc-etal-2004-pronominal,P95-1017,0,\N,Missing
kupsc-etal-2004-pronominal,W02-1028,0,\N,Missing
kupsc-etal-2004-pronominal,J01-4004,0,\N,Missing
kupsc-etal-2004-pronominal,2001.mtsummit-papers.43,1,\N,Missing
mitamura-etal-2002-kantoo,C92-3168,0,\N,Missing
mitamura-etal-2002-kantoo,1991.mtsummit-papers.9,1,\N,Missing
N03-4010,N01-1005,0,0.0248439,"be combined dynamically to determine the optimal answer. For more complex questions, a more flexible and powerful control mechanism is required. For example, LCC (D. Moldovan and Surdeanu, 2002) has implemented feedback loops which ensure that processing constraints are met by retrieving more documents or expanding question terms. The LCC system includes a passage retrieval loop, a lexico-semantic loop and a logic proving loop. The IBM PIQUANT system (Carroll et al., 2002) combines knowledge-based agents using predictive annotation with a statistical approach based on a maximum entropy model (Ittycheriah et al., 2001). Both the LCC and IBM systems represent a departure from the standard pipelined approach to QA architecture, and both work well for straightforward factoid questions. Nevertheless, both approaches incorporate a pre-determined set of processing steps or strategies, and have limited ability to reason about new types of questions not previously encountered. Practically useful question answering in non-factoid domains (e.g., intelligence analysis) requires more sophisticated question decomposition, reasoning, and answer synthesis. For these hard questions, QA architectures must define relationshi"
N04-4016,P01-1012,0,\N,Missing
N07-1066,ko-etal-2006-exploiting,1,0.892666,"input variables. Logistic P (correct(Ai )|Q, A1 , ..., An ) (1) ≈ P (correct(Ai )|val1 (Ai ), ..., valK1 (Ai ), sim1 (Ai ), ..., simK2 (Ai )) K2 K1 P P λk simk (Ai )) βk valk (Ai ) + exp(α0 + k=1 k=1 = K2 K1 P P λk simk (Ai )) βk valk (Ai ) + 1 + exp(α0 + k=1 k=1 where, simk (Ai ) = N X sim0k (Ai , Aj ). j=1(j6=i) ~ ~λ = argmax α ~ , β, ~ ~λ α ~ ,β, Nj R X X logP (correct(Ai )|val1 (Ai ), ..., valK1 (Ai ), sim1 (Ai ), ..., simK2 (Ai )) (2) j=1 i=1 regression has been successfully employed in many applications including multilingual document merging (Si and Callan, 2005). In our previous work (Ko et al., 2006), we showed that logistic regression performed well in merging three resources to validate answers to location and proper name questions. We extended this approach to combine multiple similarity features with multiple answer validation features. The extended framework estimates the probability that an answer candidate is correct given the degree of answer correctness and the amount of supporting evidence provided in a set of answer candidates (Equation 1). In Equation 1, each valk (Ai ) is a feature function used to produce an answer validity score for an answer candidate Ai . Each sim0k (Ai ,"
N07-1066,N03-1022,0,0.0446566,"there are redundant answers (“Shanghai”, as above) or several answers which represent a single instance (e.g. “Clinton, Bill” and “William Jefferson Clinton”) in the candidate list, how much should we boost the answer candidate scores? To address the first issue, several answer selection approaches have used semantic resources. One of the most common approaches relies on WordNet, CYC and gazetteers for answer validation or answer reranking; answer candidates are pruned or discounted if they are not found within a resource’s hierarchy corresponding to the expected answer type (Xu et al., 2003; Moldovan et al., 2003; Prager et al., 2004). In addition, the Web has been used for answer reranking by exploiting search engine results produced by queries containing the answer candidate and question keywords (Magnini et al., 2002), and Wikipedia’s structured information has been used for answer type checking (Buscaldi and Rosso, 2006). To use more than one resource for answer type checking of location questions, Schlobach et al. (2004) combined WordNet with geographical databases. However, in their experiments the combination actually hurt performance because of the increased semantic ambiguity that accompanies"
N07-1066,buscaldi-rosso-2006-mining,0,\N,Missing
N09-3010,C04-1121,0,0.0902152,"on Learning with Indirect Feature Voting Shilpa Arora and Eric Nyberg Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213, USA {shilpaa,ehn}@cs.cmu.edu Abstract using features like syntactic path from constituent to predicate improves performance of a semantic parser. However, often such features are “handcrafted” by domain experts and do not generalize to other tasks and domains. In this work, we propose a general graph representation for automatically extracting structured features from tokens and prior annotations such as part of speech, dependency triples, etc. Gamon (2004) shows that an approach using a large set of structured features and a feature selection procedure performs better than an approach that uses a few “handcrafted” features. Our hypothesis is that structured features are important for supervised annotation learning and can be automatically derived from tokens and prior annotations. We test our hypothesis and present our results for opinion mining from product reviews. We demonstrate that a supervised annotation learning approach using structured features derived from tokens and prior annotations performs better than a bag of words approach. We p"
N09-3010,N06-1041,0,0.0213443,"erformance can be achieved with less labeled data.We present our results on two annotation learning tasks for opinion mining from product and movie reviews. 1 Introduction Interactive Annotation Learning is a supervised approach to learning annotations with the goal of minimizing the total annotation cost. In this work, we demonstrate that with additional supervision per example, such as distinguishing discriminant features, same performance can be achieved with less annotated data. Supervision for simple features has been explored in the literature (Raghavan et al., 2006; Druck et al., 2008; Haghighi and Klein, 2006). In this work, we propose an approach that seeks supervision from the user on structured features. Features that capture the linguistic structure in text such as n-grams and syntactic patterns, referred to as structured features in this work, have been found to be useful for supervised learning of annotations. For example, Pradhan et al. (2004) show that 55 Deriving features from the annotation graph gives us a large number of very sparse features. Feature selection based on class association scores such as mutual information and chi-square have often been used to identify the most discrimina"
N09-3010,P03-1054,0,0.00895737,"Missing"
N09-3010,N04-1019,0,0.0131421,"ture, we plan to investigate further the benefit of using higher degree subgraph features for opinion mining from the movie review data. Comparing ranking of features:We also compared the features that the rationales capture to what the oracle will vote for as the most relevant features. Features are ranked based on chi-square scores used in feature selection. We compare the ranked list of features from RT D(1, 0), BT D and OT D and use a weighted F-measure score for evaluating the top 100 ranked features by each approach. This measure is inspired by the Pyramid measure used in Summarization (Nenkova and Passonneau, 2004). Instead of using counts in calculating F-measure, we used the chi-square score assigned to the features by the oracle dataset, in order to give more weight to the more discriminant features. As can be seen from Table 4, RT D(1, 0) outperforms BT D in capturing the important features when the datasize set is small (&lt; 300) and this difference is significant. Beyond 300 examples, as the data size increases, BT D outperforms RT D(1, 0). This implies that the rationales alone are able to capture the most relevant features when the dataset is small. 100 200 300 400 500 600 700 RO 47.70 53.80 57.68"
N09-3010,P04-1035,0,0.00469508,"tion to select the top 5000 features. As can be seen in Table 1, an approach using degree − 0 features, i.e. unigrams, part of speech and dependency triples together, outperforms using any of those features alone and this difference is significant. Using degree − 1 features with two nodes and an edge improves performance further. However, using degree−0 features in addition to degree−1 features does not improve performance. This suggests that when using higher degree features, we may leave out the features with lower degree that they subsume. 57 3.1 Data and Experimental Setup The data set by Pang and Lee (2004) consists of 2000 movie reviews (1000-pos, 1000-neg) from the IMDb review archive. Zaidan et al. (2007) provide rationales for 1800 reviews (900-pos, 900-neg). The annotation guidelines for marking rationales are described in (Zaidan et al., 2007). An example of a rationale is: “the movie is so badly put together that even the most casual viewer may notice the miserable pacing and stray plot threads”. For a test dataset of 200 reviews, randomly selected from 1800 reviews, we varied the training data size from 50 to 500 reviews, adding 50 reviews at a time. Training examples were randomly selec"
N09-3010,N04-1030,0,0.0373971,"tional supervision per example, such as distinguishing discriminant features, same performance can be achieved with less annotated data. Supervision for simple features has been explored in the literature (Raghavan et al., 2006; Druck et al., 2008; Haghighi and Klein, 2006). In this work, we propose an approach that seeks supervision from the user on structured features. Features that capture the linguistic structure in text such as n-grams and syntactic patterns, referred to as structured features in this work, have been found to be useful for supervised learning of annotations. For example, Pradhan et al. (2004) show that 55 Deriving features from the annotation graph gives us a large number of very sparse features. Feature selection based on class association scores such as mutual information and chi-square have often been used to identify the most discriminant features (Manning et al., 2008). However, these scores are calculated from labeled data and they are not very meaningful when the dataset is small. Supervised feature selection, i.e. asking the user to vote for the most discriminant features, has been used as an alternative when the training dataset is small. Raghavan et al. (2006) and Druck"
N09-3010,W00-1308,0,0.196819,"Missing"
N09-3010,N07-1033,0,0.202495,"Missing"
N09-3010,D08-1004,0,\N,Missing
nyberg-etal-2002-deriving,nyberg-mitamura-2000-kantoo,1,\N,Missing
nyberg-etal-2002-deriving,C92-3168,1,\N,Missing
nyberg-etal-2002-deriving,1991.mtsummit-papers.9,1,\N,Missing
nyberg-mitamura-2000-kantoo,C92-3168,1,\N,Missing
nyberg-mitamura-2000-kantoo,1991.mtsummit-papers.9,1,\N,Missing
P04-3018,lin-2002-web,0,0.0248125,"asingly critical to QA performance. While on-line resources such as the Web, WordNet, gazetteers, and encyclopedias are becoming more prevalent, no system-independent study has quantified their impact on the QA task. This paper focuses on several resources and their inherent potential to provide answers, without concentrating on a particular QA system or component. The goal is to quantify and bound the potential impact of these resources on the QA process. Eric Nyberg Carnegie Mellon University ehn@cs.cmu.edu produce more correct, confident answers (Clarke et al., 2001; Dumais et al., 2002). (Lin, 2002) presents two different approaches to using the Web: accessing the structured data and mining the unstructured data. Due to their complementary nature of these approaches, hybrid systems are likely to perform better (Lin and Katz, 2003). Definitional questions (“What is X?”, “Who is X?”) are especially compatible with structured resources such as gazetteers and encyclopedias. The top performing definitional systems (Xu et al., 2003) at TREC extract kernel facts similar to a question profile built using structured and semistructured resources: WordNet (Miller et al., 1990), Merriam-Webster dict"
P04-3018,P02-1054,0,0.0604621,"are covered well by these resources. 8 The Web as a Resource An increasing number of QA systems are using the web as a resource. Since the Web is orders of magnitude larger than local corpora, answers occur frequently in simple contexts, which is more conducive to retrieval and extraction of correct, confident answers (Clarke et al., 2001; Dumais et al., 2002; Lin and Katz, 2003). The web has been employed for pattern acquisition (Ravichandran et al., 2003), document retrieval (Dumais et al., 2002), query expansion (Yang et al., 2003), structured information extraction, and answer validation (Magnini et al., 2002) . Some of these approaches enhance existing QA systems, while others simplify the question answering task, allowing a less complex approach to find correct answers. 8.1 Web Documents Instead of searching a local corpus, some QA systems retrieve relevant documents from the web (Xu et al., 2003). Since the density of relevant web documents can be higher than the density of relevant local documents, answer extraction may be more successful from the web. For a TREC evaluation, answers found on the web must also be mapped to relevant documents in the local corpus. Web Retrieval Performance For QA"
P04-3018,N03-1022,0,0.0374274,"Missing"
P04-3018,W03-1201,0,0.0250427,"Missing"
P04-3018,N03-2029,0,0.0153578,"y well. Although the process and object types are broad answer types, the coverage is still reasonably good. As expected, the definition and person-bio answer types are covered well by these resources. 8 The Web as a Resource An increasing number of QA systems are using the web as a resource. Since the Web is orders of magnitude larger than local corpora, answers occur frequently in simple contexts, which is more conducive to retrieval and extraction of correct, confident answers (Clarke et al., 2001; Dumais et al., 2002; Lin and Katz, 2003). The web has been employed for pattern acquisition (Ravichandran et al., 2003), document retrieval (Dumais et al., 2002), query expansion (Yang et al., 2003), structured information extraction, and answer validation (Magnini et al., 2002) . Some of these approaches enhance existing QA systems, while others simplify the question answering task, allowing a less complex approach to find correct answers. 8.1 Web Documents Instead of searching a local corpus, some QA systems retrieve relevant documents from the web (Xu et al., 2003). Since the density of relevant web documents can be higher than the density of relevant local documents, answer extraction may be more successfu"
P07-1099,P03-1003,0,0.317782,"Missing"
P07-1099,N07-1066,1,0.842291,"Missing"
P07-1099,N03-1022,0,0.0850901,"Missing"
P07-1099,P06-1054,1,0.825811,"e same way to analyze Wikipedia documents. The idf score was calculated using word statistics from Japanese Yomiuri newspaper corpus and the NTCIR Chinese corpus. Google: The same algorithm was applied to analyze Japanese and Chinese snippets returned from Google. But we restricted the language to Chinese or Japanese so that Google returned only Chinese or Japanese documents. To calculate the distance between an answer candidate and question keywords, segmentation was done with linguistic tools. For Japanese, Chasen4 was used. For Chinese segmentation, a maximum-entropy based parser was used (Wang et al., 2006). 3) Manual Filtering Other than the features mentioned above, we manually created many rules for numeric and temporal questions to filter out invalid answers. For example, when the question is looking for a year as an answer, an answer candidate which contains only the month receives a score of -1. Otherwise, the score is 0. 4.2 Answer Similarity Features The same features used for English were applied to calculate the similarity of Chinese/Japanese answer candidates. To identify synonyms, Wikipedia were used for both Chinese and Japanese. EIJIRO dictionary was used to obtain Japanese synonym"
P07-1099,buscaldi-rosso-2006-mining,0,\N,Missing
P14-1024,W07-0103,0,0.295543,"eses about metaphoric language could be tested more easily at a larger scale with automation. However, metaphor detection is a hard problem. On one hand, there is a subjective component: humans may disagree whether a particular expression is used metaphorically or not, as there is no clear-cut semantic distinction between figurative and metaphorical language (Shutova, 2010). On the other, metaphors can be domain- and contextdependent.1 Previous work has focused on metaphor identification in English, using both extensive manuallycreated linguistic resources (Mason, 2004; Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Turney et al., 2011; Broadwell et al., 2013) and corpus-based approaches (Birke and Sarkar, 2007; Shutova et al., 2013; Neuman et al., 2013; Shutova and Sun, 2013; Hovy et al., 2013). We build on this foundation and also extend metaphor detection into other languages in which few resources may exist. Our work makes the following contributions: (1) we develop a new state-of-the-art English metaphor detection system that uses conceptual semantic features, such as a degree of abstractness and semantic supersenses;2 (2) we create new metaphor-annotated corpora for Russian and English;3 (3) using"
P14-1024,W06-1670,0,0.0180673,"Missing"
P14-1024,levin-etal-2014-resources,1,0.819369,"selection of the training samples. Thus, we trust that annotator judgments were not biased towards the cases that the system is trained to process. Multilingual test sets We collect and annotate metaphoric and literal test sentences in four languages. Thus, we compile eight test datasets, four for SVO relations, and four for AN relations. Each dataset has an equal number of metaphors and non-metaphors, i.e., the datasets are balanced. English (EN) and Russian (RU) datasets have been compiled by our team and are publicly available. Spanish (ES) and Farsi (FA) datasets are published elsewhere (Levin et al., 2014). Table 1 lists test set sizes. EN RU ES FA SVO AN 222 240 220 44 200 200 120 320 5 5.1 Experiments English experiments Our task, as defined in Section 2, is to classify SVO and AN relations as either metaphoric or literal. We first conduct a 10-fold cross-validation experiment on the training set defined in Section 4.1. We represent each candidate relation using the features described in Section 3.2, and evaluate performance of the three feature categories and their combinations. This is done by computing an accuracy in the 10-fold cross validation. Experimental results are given in Table 2,"
P14-1024,D10-1004,0,0.013166,"ets are associated with the supersense noun.body. Therefore, the value of the feature noun.body is 4/38 ≈ 0.11. 3.3 4 Cross-lingual feature projection Datasets In this section we describe a training and testing dataset as well a data collection procedure. 4.1 English training sets To train an SVO metaphor classifier, we employ the TroFi (Trope Finder) dataset.17 TroFi includes 3,737 manually annotated English sentences from the Wall Street Journal (Birke and Sarkar, 2007). Each sentence contains either literal or metaphorical use for one of 50 English verbs. First, we use a dependency parser (Martins et al., 2010) to extract subject-verb-object (SVO) relations. Then, we filter extracted relations to eliminate parsing-related errors, and relations with verbs which are not in the TroFi verb list. After filtering, there are 953 metaphorical and 656 literal SVO relations which we use as a training set. In the case of AN relations, we construct and make publicly available a training set containing 884 metaphorical AN pairs and 884 pairs with literal meaning. It was collected by two annotators using public resources (collections of metaphors on the web). At least one additional person carefully examined and"
P14-1024,E14-1049,1,0.391097,"metaphorical and 656 literal SVO relations which we use as a training set. In the case of AN relations, we construct and make publicly available a training set containing 884 metaphorical AN pairs and 884 pairs with literal meaning. It was collected by two annotators using public resources (collections of metaphors on the web). At least one additional person carefully examined and culled the collected metaphors, by removing duplicates, weak metaphors, and metaphorical phrases (such as Vector space word representations. We employ 64-dimensional vector-space word representations constructed by Faruqui and Dyer (2014).14 Vector construction algorithm is a variation on traditional latent semantic analysis (Deerwester et al., 1990) that uses multilingual information to produce representations in which synonymous words have similar vectors. The vectors were curacy during cross-validation. 12 For the full taxonomy see http://www.sfs. uni-tuebingen.de/lsd/adjectives.shtml 13 http://www.cs.cmu.edu/˜ytsvetko/ adj-supersenses.tar.gz 14 http://www.cs.cmu.edu/˜mfaruqui/soft. html 15 http://www.statmt.org/wmt11/ http://www.babylon.com 17 http://www.cs.sfu.ca/˜anoop/students/ jbirke/ 16 251 drowning students) whose in"
P14-1024,J04-1002,0,0.0944408,"ly or not. Second, scientific hypotheses about metaphoric language could be tested more easily at a larger scale with automation. However, metaphor detection is a hard problem. On one hand, there is a subjective component: humans may disagree whether a particular expression is used metaphorically or not, as there is no clear-cut semantic distinction between figurative and metaphorical language (Shutova, 2010). On the other, metaphors can be domain- and contextdependent.1 Previous work has focused on metaphor identification in English, using both extensive manuallycreated linguistic resources (Mason, 2004; Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Turney et al., 2011; Broadwell et al., 2013) and corpus-based approaches (Birke and Sarkar, 2007; Shutova et al., 2013; Neuman et al., 2013; Shutova and Sun, 2013; Hovy et al., 2013). We build on this foundation and also extend metaphor detection into other languages in which few resources may exist. Our work makes the following contributions: (1) we develop a new state-of-the-art English metaphor detection system that uses conceptual semantic features, such as a degree of abstractness and semantic supersenses;2 (2) we create new metaphor-"
P14-1024,D11-1006,0,0.0129329,"l et al., 2013) and corpus-based approaches (Birke and Sarkar, 2007; Shutova et al., 2013; Neuman et al., 2013; Shutova and Sun, 2013; Hovy et al., 2013). We build on this foundation and also extend metaphor detection into other languages in which few resources may exist. Our work makes the following contributions: (1) we develop a new state-of-the-art English metaphor detection system that uses conceptual semantic features, such as a degree of abstractness and semantic supersenses;2 (2) we create new metaphor-annotated corpora for Russian and English;3 (3) using a paradigm of model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2013; Kozhenikov and Titov, 2013), we provide support for the hypothesis that metaphors are concepWe show that it is possible to reliably discriminate whether a syntactic construction is meant literally or metaphorically using lexical semantic features of the words that participate in the construction. Our model is constructed using English resources, and we obtain state-of-the-art performance relative to previous work in this language. Using a model transfer approach by pivoting through a bilingual dictionary, we show our model can identify metaphoric expressions in othe"
P14-1024,W06-3506,0,0.227977,"cond, scientific hypotheses about metaphoric language could be tested more easily at a larger scale with automation. However, metaphor detection is a hard problem. On one hand, there is a subjective component: humans may disagree whether a particular expression is used metaphorically or not, as there is no clear-cut semantic distinction between figurative and metaphorical language (Shutova, 2010). On the other, metaphors can be domain- and contextdependent.1 Previous work has focused on metaphor identification in English, using both extensive manuallycreated linguistic resources (Mason, 2004; Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Turney et al., 2011; Broadwell et al., 2013) and corpus-based approaches (Birke and Sarkar, 2007; Shutova et al., 2013; Neuman et al., 2013; Shutova and Sun, 2013; Hovy et al., 2013). We build on this foundation and also extend metaphor detection into other languages in which few resources may exist. Our work makes the following contributions: (1) we develop a new state-of-the-art English metaphor detection system that uses conceptual semantic features, such as a degree of abstractness and semantic supersenses;2 (2) we create new metaphor-annotated corpora for R"
P14-1024,W97-0802,0,0.298022,"membership in different supersenses are represented by feature vectors, where each element corresponds to one supersense. For example, the word head (when used as a noun) participates in 33 synsets, three of which are related to the supersense noun.body. The value of the feature corresponding to this supersense is 3/33 ≈ 0.09. Supersenses of adjectives. WordNet lacks coarse-grained semantic categories for adjectives. To divide adjectives into groups, Tsvetkov et al. (2014) use 13 top-level classes from the adapted taxonomy of Hundsnurscher and Splett (1982), which is incorporated in GermaNet (Hamp and Feldweg, 1997). For example, the top-level classes in GermaNet include: adj.feeling (e.g., willing, pleasant, cheerful); adj.substance (e.g., dry, ripe, creamy); adj.spatial (e.g., adjacent, gigantic).12 For each adjective type in WordNet, they produce a vector with a classifier posterior probabilities corresponding to degrees of membership of this word in one of the 13 semantic classes,13 similar to the feature vectors we build for nouns and verbs. For example, for a word calm the top-2 categories (with the first and second highest degrees of membership) are adj.behavior and adj.feeling. For languages othe"
P14-1024,W13-0907,0,0.748517,"may disagree whether a particular expression is used metaphorically or not, as there is no clear-cut semantic distinction between figurative and metaphorical language (Shutova, 2010). On the other, metaphors can be domain- and contextdependent.1 Previous work has focused on metaphor identification in English, using both extensive manuallycreated linguistic resources (Mason, 2004; Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Turney et al., 2011; Broadwell et al., 2013) and corpus-based approaches (Birke and Sarkar, 2007; Shutova et al., 2013; Neuman et al., 2013; Shutova and Sun, 2013; Hovy et al., 2013). We build on this foundation and also extend metaphor detection into other languages in which few resources may exist. Our work makes the following contributions: (1) we develop a new state-of-the-art English metaphor detection system that uses conceptual semantic features, such as a degree of abstractness and semantic supersenses;2 (2) we create new metaphor-annotated corpora for Russian and English;3 (3) using a paradigm of model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2013; Kozhenikov and Titov, 2013), we provide support for the hypothesis that metaphors are concepWe show that"
P14-1024,P12-1092,0,0.00373356,"babilities of classifier predictions. We binarize these posteriors into abstractconcrete (or imageable-unimageable) boolean indicators using pre-defined thresholds.11 Perfor• Vector space word representations. Vector space word representations learned using unsupervised algorithms are often effective features in supervised learning methods (Turian et al., 2010). In particular, many such representations are designed to capture lexical semantic properties and are quite effective features in semantic processing, including named entity recognition (Turian et al., 2009), word sense disambiguation (Huang et al., 2012), and lexical entailment (Baroni et al., 2012). In a recent study, Mikolov et al. (2013) reveal an interesting cross-lingual property of distributed word representations: there is a strong similarity between the vector spaces across languages that can be easily captured by linear mapping. Thus, vector space models can also be seen as vectors of (latent) semantic concepts, that preserve their “meaning” across languages. 3 Classification using Random Forests 8 See Theorem 1.2 in (Breiman, 2001) for details. In our experiments, random forests model slightly outperformed logistic regression and SV"
P14-1024,N13-1076,1,0.7392,"lkit to train our classifiers (Pedregosa et al., 2011). Supersenses are particularly attractive features for metaphor detection: coarse sense taxonomies can be viewed as semantic concepts, and since concept mapping is a process in which metaphors are born, we expect different supersense co-occurrences in metaphoric and literal combinations. In “drinks gasoline”, for example, mapping to supersenses would yield a pair &lt;verb.consumption, noun.substance>, contrasted with &lt;verb.consumption, noun.food> for “drinks juice”. In addition, this coarse semantic categorization is preserved in translation (Schneider et al., 2013), which makes supersense features suitable for cross-lingual approaches such as ours. 3.2 Feature extraction Abstractness and imageability. The MRC psycholinguistic database is a large dictionary listing linguistic and psycholinguistic attributes obtained experimentally (Wilson, 1988).10 It includes, among other data, 4,295 words rated by the degrees of abstractness and 1,156 words rated by the imageability. Similarly to Tsvetkov et al. (2013), we use a logistic regression classifier to propagate abstractness and imageability scores from MRC ratings to all words for which we have vector space"
P14-1024,N13-1118,0,0.140905,"tive component: humans may disagree whether a particular expression is used metaphorically or not, as there is no clear-cut semantic distinction between figurative and metaphorical language (Shutova, 2010). On the other, metaphors can be domain- and contextdependent.1 Previous work has focused on metaphor identification in English, using both extensive manuallycreated linguistic resources (Mason, 2004; Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Turney et al., 2011; Broadwell et al., 2013) and corpus-based approaches (Birke and Sarkar, 2007; Shutova et al., 2013; Neuman et al., 2013; Shutova and Sun, 2013; Hovy et al., 2013). We build on this foundation and also extend metaphor detection into other languages in which few resources may exist. Our work makes the following contributions: (1) we develop a new state-of-the-art English metaphor detection system that uses conceptual semantic features, such as a degree of abstractness and semantic supersenses;2 (2) we create new metaphor-annotated corpora for Russian and English;3 (3) using a paradigm of model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2013; Kozhenikov and Titov, 2013), we provide support for the hypothesis that metaphors ar"
P14-1024,P13-1117,0,0.0225815,"e and Sarkar, 2007; Shutova et al., 2013; Neuman et al., 2013; Shutova and Sun, 2013; Hovy et al., 2013). We build on this foundation and also extend metaphor detection into other languages in which few resources may exist. Our work makes the following contributions: (1) we develop a new state-of-the-art English metaphor detection system that uses conceptual semantic features, such as a degree of abstractness and semantic supersenses;2 (2) we create new metaphor-annotated corpora for Russian and English;3 (3) using a paradigm of model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2013; Kozhenikov and Titov, 2013), we provide support for the hypothesis that metaphors are concepWe show that it is possible to reliably discriminate whether a syntactic construction is meant literally or metaphorically using lexical semantic features of the words that participate in the construction. Our model is constructed using English resources, and we obtain state-of-the-art performance relative to previous work in this language. Using a model transfer approach by pivoting through a bilingual dictionary, we show our model can identify metaphoric expressions in other languages. We provide results on three new test sets"
P14-1024,shutova-teufel-2010-metaphor,0,0.121436,"Missing"
P14-1024,C10-1113,0,0.331036,"6 0.4 EN (area = 0.92) ES (area = 0.73) FA (area = 0.83) RU (area = 0.8) 0.2 0.0 0.0 0.2 0.4 0.6 False Positive Rate 0.8 1.0 (b) AN Figure 2: Cross-lingual experiment: ROC curves for classifiers trained on the English data using a combination of all features, and applied to SVO and AN metaphoric and literal relations in four test languages: English, Russian, Spanish, and Farsi. 6 For a historic overview and a survey of common approaches to metaphor detection, we refer the reader to recent reviews by Shutova et al. (Shutova, 2010; Shutova et al., 2013). Here we focus only on recent approaches. Shutova et al. (2010) proposed a bottom-up method: one starts from a set of seed metaphors and seeks phrases where verbs and/or nouns belong to the same cluster as verbs or nouns in seed examples. Turney et al. (2011) show how abstractness scores could be used to detect metaphorical AN phrases. Neuman et al. (2013) describe a Concrete Category Overlap algorithm, where co-occurrence statistics and Turney’s abstractness scores are used to determine WordNet supersenses that correspond to literal usage of a given adjective or verb. For example, given an adjective, we can learn that it modifies concrete nouns that usua"
P14-1024,J13-2003,0,0.144988,"ard problem. On one hand, there is a subjective component: humans may disagree whether a particular expression is used metaphorically or not, as there is no clear-cut semantic distinction between figurative and metaphorical language (Shutova, 2010). On the other, metaphors can be domain- and contextdependent.1 Previous work has focused on metaphor identification in English, using both extensive manuallycreated linguistic resources (Mason, 2004; Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Turney et al., 2011; Broadwell et al., 2013) and corpus-based approaches (Birke and Sarkar, 2007; Shutova et al., 2013; Neuman et al., 2013; Shutova and Sun, 2013; Hovy et al., 2013). We build on this foundation and also extend metaphor detection into other languages in which few resources may exist. Our work makes the following contributions: (1) we develop a new state-of-the-art English metaphor detection system that uses conceptual semantic features, such as a degree of abstractness and semantic supersenses;2 (2) we create new metaphor-annotated corpora for Russian and English;3 (3) using a paradigm of model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2013; Kozhenikov and Titov, 2013), we provide"
P14-1024,P10-1071,0,0.0435314,"ine translation, dialog systems, sentiment analysis, and text analytics, etc.) would have access to a potentially useful high-level bit of information about whether something is to be understood literally or not. Second, scientific hypotheses about metaphoric language could be tested more easily at a larger scale with automation. However, metaphor detection is a hard problem. On one hand, there is a subjective component: humans may disagree whether a particular expression is used metaphorically or not, as there is no clear-cut semantic distinction between figurative and metaphorical language (Shutova, 2010). On the other, metaphors can be domain- and contextdependent.1 Previous work has focused on metaphor identification in English, using both extensive manuallycreated linguistic resources (Mason, 2004; Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Turney et al., 2011; Broadwell et al., 2013) and corpus-based approaches (Birke and Sarkar, 2007; Shutova et al., 2013; Neuman et al., 2013; Shutova and Sun, 2013; Hovy et al., 2013). We build on this foundation and also extend metaphor detection into other languages in which few resources may exist. Our work makes the following contributions:"
P14-1024,W13-0909,0,0.256984,"the scope of metaphor identification by including nominal metaphoric relations as well as explore techniques for incorporating contextual features, which can play a key role in identifying certain kinds of metaphors. Second, cross-lingual model transfer can be improved with more careful cross-lingual feature projection. Broadwell et al. (2013) argue that metaphors are highly imageable words that do not belong to a discussion topic. To implement this idea, they extend MRC imageability scores to all dictionary words using links among WordNet supersenses (mostly hypernym and hyponym relations). Strzalkowski et al. (2013) carry out experiments in a specific (government-related) domain for four languages: English, Spanish, Farsi, and Russian. Strzalkowski et al. (2013) explain the algorithm only for English and say that is the same for Spanish, Farsi, and Russian. Because they heavily rely on WordNet and availability of imageability scores, their approach may not be applicable to low-resource languages. Hovy et al. (2013) applied tree kernels to metaphor detection. Their method also employs WordNet supersenses, but it is not clear from the description whether WordNet is essential or can be replaced with some ot"
P14-1024,Q13-1001,0,0.0499428,"Missing"
P14-1024,W13-0906,1,0.94438,"tance>, contrasted with &lt;verb.consumption, noun.food> for “drinks juice”. In addition, this coarse semantic categorization is preserved in translation (Schneider et al., 2013), which makes supersense features suitable for cross-lingual approaches such as ours. 3.2 Feature extraction Abstractness and imageability. The MRC psycholinguistic database is a large dictionary listing linguistic and psycholinguistic attributes obtained experimentally (Wilson, 1988).10 It includes, among other data, 4,295 words rated by the degrees of abstractness and 1,156 words rated by the imageability. Similarly to Tsvetkov et al. (2013), we use a logistic regression classifier to propagate abstractness and imageability scores from MRC ratings to all words for which we have vector space representations. More specifically, we calculate the degree of abstractness and imageability of all English items that have a vector space representation, using vector elements as features. We train two separate classifiers for abstractness and imageability on a seed set of words from the MRC database. Degrees of abstractness and imageability are posterior probabilities of classifier predictions. We binarize these posteriors into abstractconcr"
P14-1024,tsvetkov-etal-2014-augmenting-english,1,0.424069,"tions. Supersenses of nouns and verbs. A lexical item can belong to several synsets, which are associated with different supersenses. Degrees of membership in different supersenses are represented by feature vectors, where each element corresponds to one supersense. For example, the word head (when used as a noun) participates in 33 synsets, three of which are related to the supersense noun.body. The value of the feature corresponding to this supersense is 3/33 ≈ 0.09. Supersenses of adjectives. WordNet lacks coarse-grained semantic categories for adjectives. To divide adjectives into groups, Tsvetkov et al. (2014) use 13 top-level classes from the adapted taxonomy of Hundsnurscher and Splett (1982), which is incorporated in GermaNet (Hamp and Feldweg, 1997). For example, the top-level classes in GermaNet include: adj.feeling (e.g., willing, pleasant, cheerful); adj.substance (e.g., dry, ripe, creamy); adj.spatial (e.g., adjacent, gigantic).12 For each adjective type in WordNet, they produce a vector with a classifier posterior probabilities corresponding to degrees of membership of this word in one of the 13 semantic classes,13 similar to the feature vectors we build for nouns and verbs. For example, f"
P14-1024,P10-1040,0,0.00568192,"sh items that have a vector space representation, using vector elements as features. We train two separate classifiers for abstractness and imageability on a seed set of words from the MRC database. Degrees of abstractness and imageability are posterior probabilities of classifier predictions. We binarize these posteriors into abstractconcrete (or imageable-unimageable) boolean indicators using pre-defined thresholds.11 Perfor• Vector space word representations. Vector space word representations learned using unsupervised algorithms are often effective features in supervised learning methods (Turian et al., 2010). In particular, many such representations are designed to capture lexical semantic properties and are quite effective features in semantic processing, including named entity recognition (Turian et al., 2009), word sense disambiguation (Huang et al., 2012), and lexical entailment (Baroni et al., 2012). In a recent study, Mikolov et al. (2013) reveal an interesting cross-lingual property of distributed word representations: there is a strong similarity between the vector spaces across languages that can be easily captured by linear mapping. Thus, vector space models can also be seen as vectors"
P14-1024,D11-1063,0,0.173781,"could be tested more easily at a larger scale with automation. However, metaphor detection is a hard problem. On one hand, there is a subjective component: humans may disagree whether a particular expression is used metaphorically or not, as there is no clear-cut semantic distinction between figurative and metaphorical language (Shutova, 2010). On the other, metaphors can be domain- and contextdependent.1 Previous work has focused on metaphor identification in English, using both extensive manuallycreated linguistic resources (Mason, 2004; Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Turney et al., 2011; Broadwell et al., 2013) and corpus-based approaches (Birke and Sarkar, 2007; Shutova et al., 2013; Neuman et al., 2013; Shutova and Sun, 2013; Hovy et al., 2013). We build on this foundation and also extend metaphor detection into other languages in which few resources may exist. Our work makes the following contributions: (1) we develop a new state-of-the-art English metaphor detection system that uses conceptual semantic features, such as a degree of abstractness and semantic supersenses;2 (2) we create new metaphor-annotated corpora for Russian and English;3 (3) using a paradigm of model"
P14-1024,E12-1004,0,\N,Missing
P14-1024,W07-0104,0,\N,Missing
P15-2116,D14-1067,0,0.0341218,"atures extraction and engineering over parsing trees are automated in (Severyn and Moschitti, 2013). Besides syntactic approaches, lexical semantic model (Yih et al., 2013) is also used to select answer sentences. This model is to pair semantically related words based on word relations including synonymy/antonymy, hypernymy/hyponymy and general semantic word similarity. There were also prior efforts in deep learning neural networks to question answering. Yih et al. (2014) focused on answering single-relation factual questions by a semantic similarity model using convolutional neural networks. Bordes et al. (2014) jointly embedded words and knowledge base constituents into same vector space to measure the relevance of question and answer sentences in that space. Iyyer et al. (2014) worked on the quiz bowl task, which is an application of recursive neural networks for factoid question answering over paragraphs. The correct answers are identified from a relatively small fixed set of candidate answers which are in the form of entities instead of sentences. 3 Network Architecture ht = H(Wxh xt + Whh ht−1 + bh ) (1) yt = Why ht + by (2) where the W denotes weight matrices, the b denotes bias vectors and H(·"
P15-2116,C10-1131,0,0.857892,"Missing"
P15-2116,D14-1070,0,0.0573013,"Missing"
P15-2116,D07-1003,0,0.770712,"ly, given an input sequence x = (x1 , x2 , . . . , xT ), a conventional RNN updates the hidden vector sequence h = (h1 , h2 , . . . , hT ) and output vector sequence y = (y1 , y2 , . . . , yT ) from t = 1 to T as follows: Prior to this work there were other approaches to address the sentence selection task. The majority of previous approaches focused on syntactic matching between questions and answers. Punyakanok et al. (2004) and Cui et al. (2005) were among the earliest to propose the general tree matching methods based on tree-edit distance. Subsequent to these two papers, the approach in (Wang et al., 2007) use quasi-synchronous grammar to match each pair of question and sentence by their dependency trees. Later, tree kernel function together with a logistic regression model (Heilman and Smith, 2010) or Conditional Random Fields models (Wang and Manning, 2010; Yao et al., 2013) with extracted feature were adopted to learn the associations between question and answer. Recently, discriminative tree-edit features extraction and engineering over parsing trees are automated in (Severyn and Moschitti, 2013). Besides syntactic approaches, lexical semantic model (Yih et al., 2013) is also used to select"
P15-2116,D14-1220,0,0.00568744,"selection is that it can be potentially used to predict answer quality in community QA sites. The techniques developed from this task might also be beneficial to the emerging real-time user-oriented QA tasks such as TREC LiveQA. However, usergenerated content can be noisy and hard to parse with off-the-shelf NLP tools. Therefore, methods that requires less syntactic features are desirable. Recently, neural network-based distributed sentence modeling has been found successful in many natural language processing tasks such as word sense disambiguation (McCarthy et al., 2004), discourse parsing (Li et al., 2014), machine translation (Sutskever et al., 2014; Cho et al., 2014), and paraphrase detection (Socher et al., 2011). In this paper, we present an approach that leverages the power of deep neural network to address the answer sentence selection problem for question answering. Our method employs stacked bidirectional Long Short-Term Memory (BLSTM) to sequentially read the words from question and answer sentences, and then output their relevance scores. The full system, when combined with keywords matching, outperforms previous approaches without using any syntactic parsing or external knowledge res"
P15-2116,P04-1036,0,0.00772977,"em, another benefit of the answer sentence selection is that it can be potentially used to predict answer quality in community QA sites. The techniques developed from this task might also be beneficial to the emerging real-time user-oriented QA tasks such as TREC LiveQA. However, usergenerated content can be noisy and hard to parse with off-the-shelf NLP tools. Therefore, methods that requires less syntactic features are desirable. Recently, neural network-based distributed sentence modeling has been found successful in many natural language processing tasks such as word sense disambiguation (McCarthy et al., 2004), discourse parsing (Li et al., 2014), machine translation (Sutskever et al., 2014; Cho et al., 2014), and paraphrase detection (Socher et al., 2011). In this paper, we present an approach that leverages the power of deep neural network to address the answer sentence selection problem for question answering. Our method employs stacked bidirectional Long Short-Term Memory (BLSTM) to sequentially read the words from question and answer sentences, and then output their relevance scores. The full system, when combined with keywords matching, outperforms previous approaches without using any syntac"
P15-2116,N13-1106,0,0.681727,"Missing"
P15-2116,P13-1171,0,0.91416,"ers, the approach in (Wang et al., 2007) use quasi-synchronous grammar to match each pair of question and sentence by their dependency trees. Later, tree kernel function together with a logistic regression model (Heilman and Smith, 2010) or Conditional Random Fields models (Wang and Manning, 2010; Yao et al., 2013) with extracted feature were adopted to learn the associations between question and answer. Recently, discriminative tree-edit features extraction and engineering over parsing trees are automated in (Severyn and Moschitti, 2013). Besides syntactic approaches, lexical semantic model (Yih et al., 2013) is also used to select answer sentences. This model is to pair semantically related words based on word relations including synonymy/antonymy, hypernymy/hyponymy and general semantic word similarity. There were also prior efforts in deep learning neural networks to question answering. Yih et al. (2014) focused on answering single-relation factual questions by a semantic similarity model using convolutional neural networks. Bordes et al. (2014) jointly embedded words and knowledge base constituents into same vector space to measure the relevance of question and answer sentences in that space."
P15-2116,P14-2105,0,0.0583927,"t al., 2013) with extracted feature were adopted to learn the associations between question and answer. Recently, discriminative tree-edit features extraction and engineering over parsing trees are automated in (Severyn and Moschitti, 2013). Besides syntactic approaches, lexical semantic model (Yih et al., 2013) is also used to select answer sentences. This model is to pair semantically related words based on word relations including synonymy/antonymy, hypernymy/hyponymy and general semantic word similarity. There were also prior efforts in deep learning neural networks to question answering. Yih et al. (2014) focused on answering single-relation factual questions by a semantic similarity model using convolutional neural networks. Bordes et al. (2014) jointly embedded words and knowledge base constituents into same vector space to measure the relevance of question and answer sentences in that space. Iyyer et al. (2014) worked on the quiz bowl task, which is an application of recursive neural networks for factoid question answering over paragraphs. The correct answers are identified from a relatively small fixed set of candidate answers which are in the form of entities instead of sentences. 3 Netwo"
P15-2116,D13-1044,0,0.606607,"neral tree matching methods based on tree-edit distance. Subsequent to these two papers, the approach in (Wang et al., 2007) use quasi-synchronous grammar to match each pair of question and sentence by their dependency trees. Later, tree kernel function together with a logistic regression model (Heilman and Smith, 2010) or Conditional Random Fields models (Wang and Manning, 2010; Yao et al., 2013) with extracted feature were adopted to learn the associations between question and answer. Recently, discriminative tree-edit features extraction and engineering over parsing trees are automated in (Severyn and Moschitti, 2013). Besides syntactic approaches, lexical semantic model (Yih et al., 2013) is also used to select answer sentences. This model is to pair semantically related words based on word relations including synonymy/antonymy, hypernymy/hyponymy and general semantic word similarity. There were also prior efforts in deep learning neural networks to question answering. Yih et al. (2014) focused on answering single-relation factual questions by a semantic similarity model using convolutional neural networks. Bordes et al. (2014) jointly embedded words and knowledge base constituents into same vector space"
P15-2116,N10-1145,0,\N,Missing
P15-2116,D14-1179,0,\N,Missing
P19-1606,D16-1032,0,0.0381036,"rarchical multi task approach to perform structure aware generation. Comprehending Food: Recent times have seen large scale datasets in food, such as Recipe1M (Marin et al., 2018), Food-101 (Bossard et al., 2014).Food recognition (Arora et al., 2019) addresses understanding food from a vision perspective. Salvador et al. (2018) worked on generating cooking instructions by inferring ingredients from an image. Zhou et al. (2018) proposed a method to generate procedure segments for YouCook2 data. In NLP domain, this is studied as generating procedural text by including ingredients as checklists (Kiddon et al., 2016) or treating the recipe as a flow graph (Mori et al., 2014). Our work is at the intersection of two modalities (language and vision) by generating procedural text for recipes from a sequence of images. (Bosselut et al., 2017) worked on reasoning non-mentioned causal effects thereby improving the understanding and generation of procedural text for cooking recipes. This is done by dynamically tracking entities by modeling actions using state transformers. Visual Story Telling: Research at the intersection of language and vision is accelerating with tasks like image captioning (Hossain et al., 20"
P19-1606,P18-1082,0,0.0352561,"ated to phases of procedural text as described in the following sections. Planning while writing content: A major challenge faced by neural text generation (Lu et al., 2018) while generating long sequences is the inability to maintain structure, contravening the coherence of the overall generated text. This aspect was also observed in various tasks like summarization (Liu et al., 2018), story generation (Fan et al., 2019). Pre-selecting content and planning to generate accordingly was explored by Puduppully et al. (2018) and Lukin et al. (2015) in contrast to generate as you proceed paradigm. Fan et al. (2018) adapt a hierarchical approach to generate a premise and then stories to improve coherence and fluency. Yao et al. (2018) experimented with static and dynamic schema to realize the entire storyline before generating. However, in this work we propose a hierarchical multi task approach to perform structure aware generation. Comprehending Food: Recent times have seen large scale datasets in food, such as Recipe1M (Marin et al., 2018), Food-101 (Bossard et al., 2014).Food recognition (Arora et al., 2019) addresses understanding food from a vision perspective. Salvador et al. (2018) worked on gener"
P19-1606,W15-4627,0,0.0252274,"epresentations explicitly. These specialized set of events are correlated to phases of procedural text as described in the following sections. Planning while writing content: A major challenge faced by neural text generation (Lu et al., 2018) while generating long sequences is the inability to maintain structure, contravening the coherence of the overall generated text. This aspect was also observed in various tasks like summarization (Liu et al., 2018), story generation (Fan et al., 2019). Pre-selecting content and planning to generate accordingly was explored by Puduppully et al. (2018) and Lukin et al. (2015) in contrast to generate as you proceed paradigm. Fan et al. (2018) adapt a hierarchical approach to generate a premise and then stories to improve coherence and fluency. Yao et al. (2018) experimented with static and dynamic schema to realize the entire storyline before generating. However, in this work we propose a hierarchical multi task approach to perform structure aware generation. Comprehending Food: Recent times have seen large scale datasets in food, such as Recipe1M (Marin et al., 2018), Food-101 (Bossard et al., 2014).Food recognition (Arora et al., 2019) addresses understanding foo"
P19-1606,P19-1254,0,0.106845,"Missing"
P19-1606,D18-1117,0,0.0934197,"ation of procedural text for cooking recipes. This is done by dynamically tracking entities by modeling actions using state transformers. Visual Story Telling: Research at the intersection of language and vision is accelerating with tasks like image captioning (Hossain et al., 2019), visual question answering (Wu et al., 2017), visual dialog (Das et al., 2017; Mostafazadeh et al., 2017; De Vries et al., 2017; de Vries et al., 2018). ViST (Huang et al., 2016) is a sequential vision to language task demonstrating differences between descriptions in isolation and stories in sequences. Similarly, Gella et al. (2018) created VideoStory dataset from videos on social media with the task of generating a multi-sentence story captions for them. Smilevski et al. (2018) proposed a late fusion based model for ViST challenge. Kim et al. (2018) attained the highest scores on human readability in this task by attending to both global and local contexts. We use this as our baseline model and propose two techniques on top of this baseline to impose structure needed for procedural text. 3 Data Description We identified two how-to blogs from: instructables.comand snapguide.com, comprising stepwise instructions (images a"
P19-1606,mori-etal-2014-flow,0,0.0703972,"ration. Comprehending Food: Recent times have seen large scale datasets in food, such as Recipe1M (Marin et al., 2018), Food-101 (Bossard et al., 2014).Food recognition (Arora et al., 2019) addresses understanding food from a vision perspective. Salvador et al. (2018) worked on generating cooking instructions by inferring ingredients from an image. Zhou et al. (2018) proposed a method to generate procedure segments for YouCook2 data. In NLP domain, this is studied as generating procedural text by including ingredients as checklists (Kiddon et al., 2016) or treating the recipe as a flow graph (Mori et al., 2014). Our work is at the intersection of two modalities (language and vision) by generating procedural text for recipes from a sequence of images. (Bosselut et al., 2017) worked on reasoning non-mentioned causal effects thereby improving the understanding and generation of procedural text for cooking recipes. This is done by dynamically tracking entities by modeling actions using state transformers. Visual Story Telling: Research at the intersection of language and vision is accelerating with tasks like image captioning (Hossain et al., 2019), visual question answering (Wu et al., 2017), visual di"
P19-1606,I17-1047,0,0.0300203,"ersection of two modalities (language and vision) by generating procedural text for recipes from a sequence of images. (Bosselut et al., 2017) worked on reasoning non-mentioned causal effects thereby improving the understanding and generation of procedural text for cooking recipes. This is done by dynamically tracking entities by modeling actions using state transformers. Visual Story Telling: Research at the intersection of language and vision is accelerating with tasks like image captioning (Hossain et al., 2019), visual question answering (Wu et al., 2017), visual dialog (Das et al., 2017; Mostafazadeh et al., 2017; De Vries et al., 2017; de Vries et al., 2018). ViST (Huang et al., 2016) is a sequential vision to language task demonstrating differences between descriptions in isolation and stories in sequences. Similarly, Gella et al. (2018) created VideoStory dataset from videos on social media with the task of generating a multi-sentence story captions for them. Smilevski et al. (2018) proposed a late fusion based model for ViST challenge. Kim et al. (2018) attained the highest scores on human readability in this task by attending to both global and local contexts. We use this as our baseline model an"
P19-1606,N18-1049,0,0.0230175,"work does not deal with multiple phases being a part of a single step). Phases may be ‘listing ingredients’, ‘baking’, ‘garnishing’ etc., The key idea of the SSiD model is to incorporate the sequence of phases in the decoder to impose structure during text generation There are two sources of supervision to drive the model: (1) multimodal dataset M = {I, T} from Section 3, (2) unimodal textual recipes2 U to learn phase sequences. Finer phases are learnt using clustering followed by an FSM. Clustering: K-Means clustering is performed on the sentence embeddings with compositional ngram features (Pagliardini et al., 2018) on each step of the recipe in U. Aligning with our intu6042 2 www.ffts.com/recipes.htm ition, when k is 3, it is observed that these clusters roughly indicate categories of desserts, drinks and main course foods (pizza, quesadilla etc,). However, we need to find out finer categories of the phases corresponding to the phases in the recipes. We use k-means clustering to obtain the categories of these phases. We experimented with different number of phases P as shown in Table 2. For example, let an example recipe comprise of 4 steps i.e, a sequence of 4 images. At this point, each recipe can be"
P19-1606,P02-1040,0,0.104097,"l need: 5 pounds of Preheat oven to 450 F. Mix dry Place a mat on the baking pan chicken wings,  cup all purpose ingredients in the dry ziplock bag. and spread butter evenly on it. flour,  tsp salt, 2 tsp of paprika, melted butter, silicon mat, baking pan. Figure 3: Comparison of generated storyboards for Easy Oven Baked Crispy Chicken Wings Models Glocal SSiD (hard phases) SSiD (hard states) SSiD (soft phases) SSiL (soft phases) BLEU 10.74 11.49 11.93 13.91 16.38 METEOR 0.25 0.24 0.25 0.29 0.31 ROUGE-L 0.31 0.31 0.31 0.32 0.34 Table 3: Evaluation of storyboarding recipes 2. The BLEU score (Papineni et al., 2002) is the highest when P is 40 and S is 100. Fixing these values, we compare the models proposed in Table 3. The models with hard phases and hard states are not as stable as the one with soft phases since backprop affects the impact of the scaffolded phases. Upon manual inspection, a key observation is that for SSiD model, most of the recipes followed a similar structure. It seemed to be conditioned on a global structure learnt from all recipes rather than the current input. However, SSiL model seems to generate recipe that is conditioned on the structure of that particular example. Human Evalua"
W03-0908,J95-4004,0,0.00615965,"punct) (ortho ?) (root ?) (tokens 6))) (qa ( (gap ( (atype temporal) (path (*MULT* adjunct object)))) (qtype entity))) (root found) (subject ( (BBN-name person) (Brill-pos NNP) (cat n) (definite +) (gen-pn +) (human +) (number sg) (ortho &quot;Wendy’s&quot;) (person third) (proper-noun +) (root wendy) (tokens 3))) (tense past) (tokens 5)) Figure 1: When was Wendy’s founded: KANTOO fstructure 3.1 Questions The question analysis consists of two steps: lexical processing and syntactic parsing. For the lexical processing step, we have integrated several external resources: the Brill part-of-speech tagger (Brill, 1995), BBN IdentiFinder (BBN, 2000) (to tag named entities such as proper names, time expressions, numbers, etc.), WordNet (Fellbaum, 1998) (for semantic categorization), and the KANTOO Lexifier (Nyberg and Mitamura, 2000) (to access a syntactic lexicon for verb valence information). The hand-written grammars employed in the project are based on the Lexical Functional Grammar (LFG) formalism (Bresnan, 1982), and are used with the KANTOO parser (Nyberg and Mitamura, 2000). The parser outputs a functional structure (f-structure) which specifies the grammatical functions of question components, e.g.,"
W03-0908,P03-1003,0,0.0321005,"th information extraction (IE) techniques, modified to be applicable to unrestricted texts. Although semantics-poor techniques, such as surface pattern matching (Soubbotin, 2002; Ravichandran and Hovy, 2002) or statistical methods (Ittycheriah et al., 2002), have been successful in answering factoid questions, more complex tasks require a consideration of text meaning. This requirement has motivated work on QA systems to incorporate knowledge processing components such as semantic representation, ontologies, reasoning and inference engines, e.g., (Moldovan et al., 2003), (Hovy et al., 2002), (Chu-Carroll et al., 2003). Since world knowledge databases for open-domain tasks are unavailable, alternative approaches for meaning representation must be adopted. In this paper, we present our preliminary approach to semantics-based answer detection in the JAVELIN QA system (Nyberg et al., 2003). In contrast to other QA systems, we are trying to realize a formal model for a lightweight semantics-based opendomain question answering. We propose a constrained semantic representation as well as an explicit unification 1 The authors appear in alphabetical order. System Components The JAVELIN system consists of four basic"
W03-0908,1995.iwpt-1.15,0,0.0160458,"exicon for verb valence information). The hand-written grammars employed in the project are based on the Lexical Functional Grammar (LFG) formalism (Bresnan, 1982), and are used with the KANTOO parser (Nyberg and Mitamura, 2000). The parser outputs a functional structure (f-structure) which specifies the grammatical functions of question components, e.g., subject, object, adjunct, etc. As illustrated in Fig. 1, the resulting f-structure provides a deep, detailed syntactic analysis of the question. 3.2 Passages Passages selected by the retrieval engine are processed by the Link Grammar parser (Grinberg et al., 1995). The parser uses a lexicalized grammar which specifies links, i.e., grammatical functions, and provides a constituent structure as output. The parser covers a wide range of syntactic constructions and is robust: it can skip over unrecognized fragments of text, and is able to handle unknown words. An example of the passage analysis produced by the Link Parser is presented in Fig. 2. Links are treated as predicates which relate various arguments. For example, O in Fig. 2 indicates that Wendy’s is an object of the verb founded. In parallel to the Link parser, passages are tagged with the BBN Ide"
W03-0908,W01-1203,0,0.030826,"linguistic analysis: the question analysis and passage understanding modules (Question Analyzer and Information Extractor, respectively). The relevant aspects of syntactic processing in both modules are presented in Section 3, whereas the semantic representation is introduced in Section 4. 3 Parsing The system employs two different parsing techniques: a chart parser with hand-written grammars for question analysis, and a lexicalized, broad coverage skipping parser for passage analysis. For question analysis, parsing serves two goals: to identify the finest answer focus (Moldovan et al., 2000; Hermjakob, 2001), and to produce a grammatical analysis (f-structure) for questions. Due to the lack of publicly available parsers which have suitable coverage of question forms, we have manually developed a set of grammars to achieve these goals. On the other hand, the limited coverage and ambiguity in these grammars made adopting the same approach for passage analysis inefficient. In effect, we use two distinct parsers which provide two syntactic representations, including grammatical functions. These syntactic structures are then transformed into a common semantic representation discussed in Section 4. ( ("
W03-0908,P00-1071,0,0.0261933,"mponents which support linguistic analysis: the question analysis and passage understanding modules (Question Analyzer and Information Extractor, respectively). The relevant aspects of syntactic processing in both modules are presented in Section 3, whereas the semantic representation is introduced in Section 4. 3 Parsing The system employs two different parsing techniques: a chart parser with hand-written grammars for question analysis, and a lexicalized, broad coverage skipping parser for passage analysis. For question analysis, parsing serves two goals: to identify the finest answer focus (Moldovan et al., 2000; Hermjakob, 2001), and to produce a grammatical analysis (f-structure) for questions. Due to the lack of publicly available parsers which have suitable coverage of question forms, we have manually developed a set of grammars to achieve these goals. On the other hand, the limited coverage and ambiguity in these grammars made adopting the same approach for passage analysis inefficient. In effect, we use two distinct parsers which provide two syntactic representations, including grammatical functions. These syntactic structures are then transformed into a common semantic representation discussed"
W03-0908,nyberg-mitamura-2000-kantoo,1,0.819708,"-pn +) (human +) (number sg) (ortho &quot;Wendy’s&quot;) (person third) (proper-noun +) (root wendy) (tokens 3))) (tense past) (tokens 5)) Figure 1: When was Wendy’s founded: KANTOO fstructure 3.1 Questions The question analysis consists of two steps: lexical processing and syntactic parsing. For the lexical processing step, we have integrated several external resources: the Brill part-of-speech tagger (Brill, 1995), BBN IdentiFinder (BBN, 2000) (to tag named entities such as proper names, time expressions, numbers, etc.), WordNet (Fellbaum, 1998) (for semantic categorization), and the KANTOO Lexifier (Nyberg and Mitamura, 2000) (to access a syntactic lexicon for verb valence information). The hand-written grammars employed in the project are based on the Lexical Functional Grammar (LFG) formalism (Bresnan, 1982), and are used with the KANTOO parser (Nyberg and Mitamura, 2000). The parser outputs a functional structure (f-structure) which specifies the grammatical functions of question components, e.g., subject, object, adjunct, etc. As illustrated in Fig. 1, the resulting f-structure provides a deep, detailed syntactic analysis of the question. 3.2 Passages Passages selected by the retrieval engine are processed by"
W03-0908,P02-1006,0,0.0160179,"egies; Sections 4 and 5 describe our preliminary semantic representation and the unification framework which assigns confidence values to answer candidates. The final section contains a summary and future plans. 2 1 Introduction Modern Question Answering (QA) systems aim at providing answers to natural language questions in an opendomain context. This task is usually achieved by combining information retrieval (IR) with information extraction (IE) techniques, modified to be applicable to unrestricted texts. Although semantics-poor techniques, such as surface pattern matching (Soubbotin, 2002; Ravichandran and Hovy, 2002) or statistical methods (Ittycheriah et al., 2002), have been successful in answering factoid questions, more complex tasks require a consideration of text meaning. This requirement has motivated work on QA systems to incorporate knowledge processing components such as semantic representation, ontologies, reasoning and inference engines, e.g., (Moldovan et al., 2003), (Hovy et al., 2002), (Chu-Carroll et al., 2003). Since world knowledge databases for open-domain tasks are unavailable, alternative approaches for meaning representation must be adopted. In this paper, we present our preliminary"
W08-1801,C04-1100,0,0.0490085,"Missing"
W08-1801,N04-1030,0,0.0371576,"tic representation for the sentence, John loves Mary. Note that John is identified as the A RG 0, the agent, or doer, of the love action. Mary is identified as the A RG 1, the patient, or to whom the love action is being done. Both John and Mary are also identified as P ERSON named entity types. The common representation in OpenEphyra is a verb predicate-argument structure, augmented with named entity types, in which verb arguments are labeled with semantic roles in the style of PropBank (Kingsbury et al., 2002). This feature requires the separate download2 of a semantic parser called ASSERT (Pradhan et al., 2004), which was trained on PropBank. See Figure 1 for an example representation for the sentence, John loves Mary. OpenEphyra comes packaged with standard baseline methods for answer extraction and selection. For example, it extracts answers from retrieved text based on named entity instances matching the expected answer type as determined by the question analysis module. It can also look for predicate-argument structures that match the question structure, and can extract the argument corresponding to the argument in the question representing the interrogative phrase. OpenEphyra’s default answer s"
W08-1801,A97-1004,0,0.0214824,"concludes with recommendations for further research in bringing text retrieval and answer extraction closer together. 2 Improving Text Retrieval in Isolation 2.2 This section documents an attempt to improve the performance of a QA system by substituting its existing text retrieval component with for highprecision retrieval system capable of checking linguistic and semantic constraints at retrieval time. The corpus used in this experiment is the AQUAINT corpus (Graff, 2002), the standard corpus for the TREC3 QA evaluations held in 2002 through 2005. The corpus was prepared using MXTerminator (Reynar and Ratnaparkhi, 1997) for sentence segmentation, BBN Identifinder (Bikel et al., 1999) for named entity recognition, as well as the aforementioned ASSERT for identification of verb predicate-argument structures and PropBank-style semantic role labeling of the arguments. The test collection consists of 109 questions from the QA track at TREC 2002 with extensive document-level relevance judgments (Bilotti et al., 2004; Lin and Katz, 2006) over the AQUAINT corpus. A set of sentence-level judgments was pre2.1 The OpenEphyra QA System OpenEphyra is the freely-available, open-source version of the Ephyra1 QA system (Sch"
W08-1801,buscaldi-rosso-2006-mining,0,\N,Missing
W09-1903,N09-3010,1,0.82093,"perience, programming experience, etc. We used the first 200 movie reviews from the dataset provided by Zaidan et al. (2007), with an equal distribution of positive and negative examples. Each group annotated 25 movie reviews randomly selected from the 200 reviews and all annotators in each group annotated all 25 reviews. In addition to voting positive or negative for a review, annotators also annotated rationales (Zaidan et al., 2007), spans of text in the review that support their vote. Rationales can be used to guide the model by identifying the most discriminant features. In related work (Arora and Nyberg, 2009), we ascertain that with rationales the same performance can be achieved with less annotated data. The annotation task with rationales involved a variety of user actions: voting positive or negative, highlighting spans of text and adding rationale annotations. We used the same annotation guidelines as Zaidan et al. (2007). The data has been made available for research purposes1 . Figure 1 shows a screenshot of the GUI used. We performed an analysis of our data similar to that conducted by Settles et al. (2008). We address the following main questions. Are the annotation times variable enough?"
W09-1903,W04-2412,0,0.0934527,"Missing"
W09-1903,W00-1306,0,0.0297815,"calculated exactly before querying the user. For example, in biological experiments it might be calculable from the cost of the equipment and the material used (King et al., 2004). In NLP, sometimes a simplifying assumption is made that the annotation cost for an example can be measured in terms of its length (e.g. seconds of voicemail annotated (Kapoor et al., 2007); number of tokens annotated (Tomanek et al., 2007)). Another assumption is that the number of user annotation actions can be used as a proxy for annotation cost of an example (e.g. number of brackets added for parsing a sentence (Hwa, 2000); number of clicks for correcting named entities (Kristjannson et al., 2004)). While these are important factors in determining the annotation cost, none of them alone can fully substitute for the actual annotation cost. For example, a short sentence with a lot of embedded clauses may be more costly to annotate than a longer sentence with simpler grammatical structure. Similarly, a short sentence with multiple verbs and discontinuous arguments may take more time to annotate with semantic roles than a longer sentence with a single verb and simple subject-verb-object structure (Carreras and M´ar"
W09-1903,ringger-etal-2008-assessing,0,0.693434,"annotator and annotation task characteristics are important for developing an accurate estimator, and argue that both correlation coefficient and root mean square error should be used for evaluating annotation cost estimators. 1 Introduction Active Learning is the process of selectively querying the user to annotate examples with the goal of minimizing the total annotation cost. Annotation cost has been traditionally measured in terms of the number of examples annotated, but it has been widely acknowledged that different examples may require different annotation effort (Settles et al., 2008; Ringger et al., 2008). Ideally, we would use actual human annotation cost for evaluating selective sampling strategies, but this will require conducting several user studies, one per strategy on the same dataset. Alternatively, we may be able to simulate the real user by an annotation cost estimator that can then be used to evaluate several selective sampling strategies without having to run a new user study each time. An annotation cost estimator models the characteristics that can 18 differentiate the examples in terms of their annotation time. The characteristics that strongly correlate with the annotation time"
W09-1903,D07-1051,0,0.224082,"relate with the annotation time can be used as a criterion in selective sampling strategies to minimize the total annotation cost. In some domains, the annotation cost of an example is known or can be calculated exactly before querying the user. For example, in biological experiments it might be calculable from the cost of the equipment and the material used (King et al., 2004). In NLP, sometimes a simplifying assumption is made that the annotation cost for an example can be measured in terms of its length (e.g. seconds of voicemail annotated (Kapoor et al., 2007); number of tokens annotated (Tomanek et al., 2007)). Another assumption is that the number of user annotation actions can be used as a proxy for annotation cost of an example (e.g. number of brackets added for parsing a sentence (Hwa, 2000); number of clicks for correcting named entities (Kristjannson et al., 2004)). While these are important factors in determining the annotation cost, none of them alone can fully substitute for the actual annotation cost. For example, a short sentence with a lot of embedded clauses may be more costly to annotate than a longer sentence with simpler grammatical structure. Similarly, a short sentence with multi"
W09-1903,H05-1044,0,0.045174,"Missing"
W09-1903,N07-1033,0,0.0117728,"used.We then present our experimental setup followed by a discussion of our results. 3.1 Annotation Methodology and Data Analysis In this work, we estimate the annotation cost for a movie review classification task. The data we used were collected as part of a graduate course. Twenty annotators (students and instructors) were grouped into five groups of four each. The groups were created such that each group had similar variance in annotator characteristics such as department, educational experience, programming experience, etc. We used the first 200 movie reviews from the dataset provided by Zaidan et al. (2007), with an equal distribution of positive and negative examples. Each group annotated 25 movie reviews randomly selected from the 200 reviews and all annotators in each group annotated all 25 reviews. In addition to voting positive or negative for a review, annotators also annotated rationales (Zaidan et al., 2007), spans of text in the review that support their vote. Rationales can be used to guide the model by identifying the most discriminant features. In related work (Arora and Nyberg, 2009), we ascertain that with rationales the same performance can be achieved with less annotated data. Th"
W10-0216,N09-3010,1,0.9353,"s, there are also features that provide new information. It is these features that we aim to capture. In this work, we propose an evolutionary approach that constructs complex features from subgraphs extracted from an annotation graph. A constant number of these features are added to the unigram feature space, adding much of the representational benefits without the computational cost of a drastic increase in feature space size. In the remainder of the paper, we review prior work on features commonly used for sentiment analysis. We then describe the annotation graph representation proposed by Arora and Nyberg (2009). Following this, we describe the frequent subgraph mining algorithm proposed in Yan and Han (2002), and used in this work to extract frequent subgraphs from the annotation graphs. We then introduce our novel feature evolution approach, and discuss our experimental setup and results. Subgraph features combined with the feature evolution approach gives promising results, with an improvement in performance over the baseline. 2 Related Work Some of the recent work in sentiment analysis has shown that structured features (features that capture syntactic patterns in text), such as n-grams, dependen"
W10-0216,C04-1121,0,0.197588,"hown that structured features (features that capture syntactic patterns in text), such as n-grams, dependency relations, etc., improve performance beyond Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 131–139, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics the bag of words approach. Arora et al. (2009) show that deep syntactic scope features constructed from transitive closure of dependency relations give significant improvement for identifying types of claims in product reviews. Gamon (2004) found that using deep linguistic features derived from phrase structure trees and part of speech annotations yields significant improvements on the task of predicting satisfaction ratings in customer feedback data. Wilson et al. (2004) use syntactic clues derived from dependency parse tree as features for predicting the intensity of opinion phrases1 . Structured features that capture linguistic patterns are often hand crafted by domain experts (Wilson et al., 2005) after careful examination of the data. Thus, they do not always generalize well across datasets and domains. This also requires a"
W10-0216,P09-2079,1,0.857854,"Missing"
W10-0216,P03-1054,0,0.00192253,"erimental Setup Data: The dataset consists of snippets from Rotten Tomatoes (Pang and Lee, 2005) 6 . It consists of 10662 snippets/sentences total with equal number positive and negative sentences (5331 each). This dataset was created and used by Pang and Lee (2005) to train a classifier for identifying positive sentences in a full length review. We use the first 8000 (4000 positive, 4000 negative) sentences as training data and evaluate on remaining 2662 (1331 positive, 1331 negative) sentences. We added part of speech and dependency triple annotations to this data using the Stanford parser (Klein and Manning, 2003). Annotation Graph: For the annotation graph representation, we used Unigrams (U), Part of Speech (P) and Dependency Relation Type (D) as labels for the nodes, and ParentOfGov and ParentOfDep as labels for the edges. For a dependency triple such as “amod good movie”, five nodes are added to the annotation graph as shown in Figure 4(a). ParentOfGov and ParentOfDep edges are added from the 6 http://www.cs.cornell.edu/people/pabo/ movie-review-data/rt-polaritydata.tar.gz 136 P_NN ParentofDep posQ ParentofGov X X (c) Figure 4: Annotation graph and a feature subgraph for dependency triple annotatio"
W10-0216,W02-1011,0,0.0140129,"‘good amod movie’, may be represented as a d amod relation between the head word ‘movie’ and its modifier ‘good’, or as a node d amod with edges ParentOfGov and ParentOfDep to the head and the modifier words. An example of an annotation graph is shown in Figure 1. The instance in Figure 1 describes a movie review comment, ‘interesting, but not compelling.’. The words ‘interesting’ and ‘compelling’ both have positive prior polarity, however, the phrase expresses negative sentiment towards the movie. Heuristics for special handling of negation have been proposed in the literature. For example, Pang et al. (2002) append every word following a negation, until a punctuation, with a ‘NOT’ . Applying a similar technique to our example gives us two sentiment bearing features, one positive (‘interesting’) and one negative (‘NOT-compelling’), and the model may not be as sure about the predicted label, since there is both positive and negative sentiment present. In Figure 2, we show three discriminating subgraph features derived from the annotation graph in Figure 1. These subgraph features capture the negative sentiment in our example phrase. The first feature in 2(a) captures the pattern using dependency re"
W10-0216,P04-1035,0,0.0413547,"and highly correlated features to our feature space through subgraphs, we believe that a stricter constraint must be placed on correlation between features. To accomplish this, we can set our correlation penalty cutoff to 0.3, lower than the 0.5 cutoff used in prior work. Results for both settings are reported. Baselines: To the best of our knowledge, there is no supervised machine learning result published on this dataset. We compare our results with the following baselines: • Unigram-only Baseline: In sentiment analysis, unigram-only features have been a strong baseline (Pang et al., 2002; Pang and Lee, 2004). We only use unigrams that occur in at least two sentences of the training data same as Matsumoto et al. (2005). We also filter out stop words using a small stop word list9 . • χ2 Baseline: For our training data, after filtering infrequent unigrams and stop words, we get 7 http://svmlight.joachims.org/ 8 Full movie review data by Pang et al. (2002) 9 http://nlp.stanford.edu/ IR-book/html/htmledition/ dropping-common-terms-stop-words-1.html (with one modification: removed ‘will’, added ‘this’) 137 8424 features. Adding subgraph features increases the total number of features to 44, 161, a fact"
W10-0216,P05-1015,0,0.0981203,"(for inverse alignment). We assign a penalty for any correlation past a cutoff. This function is labeled CC (correlation constraint) in our fitness function. Our fitness function therefore is: Fitness = F 1 + P P + CC P_NN P_JJ (a) P_NN ParentofGov D_amod ParentofDep U_good (b) (5) 8 D_amod 6 P_JJ Experiments and Results posQ We evaluate our approach on a sentiment classification task, where the goal is to classify a movie review sentence as expressing positive or negative sentiment towards the movie. 6.1 Data and Experimental Setup Data: The dataset consists of snippets from Rotten Tomatoes (Pang and Lee, 2005) 6 . It consists of 10662 snippets/sentences total with equal number positive and negative sentences (5331 each). This dataset was created and used by Pang and Lee (2005) to train a classifier for identifying positive sentences in a full length review. We use the first 8000 (4000 positive, 4000 negative) sentences as training data and evaluate on remaining 2662 (1331 positive, 1331 negative) sentences. We added part of speech and dependency triple annotations to this data using the Stanford parser (Klein and Manning, 2003). Annotation Graph: For the annotation graph representation, we used Uni"
W10-0216,W06-1652,0,0.0711577,"e use gSpan to mine frequent subgraphs from the annotation graph. 5 Feature Construction using Genetic Programming A challenge to overcome when adding expressiveness to the feature space for any text classification problem is the rapid increase in the feature space size. Among this large set of new features, most are not predictive or are very weak predictors, and only a few carry novel information that improves classification performance. Because of this, adding more complex features often gives no improvement or even worsens performance as the feature space’s signal is drowned out by noise. Riloff et al. (2006) propose a feature subsumption approach to address this issue. They define a hierarchy for features based on the information they represent. A complex feature is only added if its discriminative power is a delta above the discriminative power of all its simpler forms. In this work, we use a Genetic Programming (Koza, 1992) based approach which evaluates interactions between fea3 http://www.cs.ucsb.edu/˜xyan/software/ gSpan.htm, http://www.kyb.mpg.de/bs/people/ nowozin/gboost/ 134 tures and evolves complex features from them. The advantage of the genetic programing based approach over feature s"
W10-0216,H05-1044,0,0.131366,"structed from transitive closure of dependency relations give significant improvement for identifying types of claims in product reviews. Gamon (2004) found that using deep linguistic features derived from phrase structure trees and part of speech annotations yields significant improvements on the task of predicting satisfaction ratings in customer feedback data. Wilson et al. (2004) use syntactic clues derived from dependency parse tree as features for predicting the intensity of opinion phrases1 . Structured features that capture linguistic patterns are often hand crafted by domain experts (Wilson et al., 2005) after careful examination of the data. Thus, they do not always generalize well across datasets and domains. This also requires a significant amount of time and resources. By automatically deriving structured features, we might be able to learn new annotations faster. Matsumoto et al. (2005) propose an approach that uses frequent sub-sequence and sub-tree mining approaches (Asai et al., 2002; Pei et al., 2004) to derive structured features such as word sub-sequences and dependency sub-trees. They show that these features outperform bag-of-words features for a sentiment classification task and"
W10-0216,N09-2010,1,\N,Missing
W11-0313,N09-3010,1,0.848041,"al., 2008) is limited to simple features like unigrams. However, unigrams are limited in the linguistic phenomena they can capture. Structured features such as dependency relations, paths in syntactic parse trees, etc., are often needed for learning the target concept (Pradhan et al., 2004; Joshi and Ros´e, 2009). It is not clear how direct feature feedback can be extended straightforwardly to structured features, as they are difficult to present visually for feedback and may require special expertise to comprehend. An alternative approach is to seek indirect feedback on structured features (Arora and Nyberg, 2009) by asking the user to highlight spans of text, called rationales, that support the instance label (Zaidan et al., 2007). For example, when classifying the sentiment of a movie review, rationales are spans of text in the review that support the sentiment label for the review. Introduction Linear classifiers model the response as a weighted linear combination of the features in input instances. A supervised approach to learning a linear classifier involves learning the weights for the features from labeled data. A large number of labeled instances may be needed to determine the class associatio"
W11-0313,W10-0216,1,0.840177,"Missing"
W11-0313,D09-1009,0,0.0176526,"g the weights for the features from labeled data. A large number of labeled instances may be needed to determine the class association of the features and learn accurate weights for them. Alternatively, the user may directly label the features. For example, for a sentiment classification task, the user may label features, such as words or phrases, Assuming a fixed cost per unit of work, it might be cheaper to ask the user to label a few features, i.e. identify relevant features and their class association, than to label several instances. Prior work (Raghavan et al., 2006; Druck et al., 2008; Druck et al., 2009; Zaidan et al., 2007) has shown that a combination of instance and feature labeling can be used to reduce the total annotation cost required to learn the target concept. However, the benefit from feature feedback may vary across learning problems. If we can estimate the benefit from feature feedback for a 106 Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 106–114, c Portland, Oregon, USA, 23–24 June 2011. 2011 Association for Computational Linguistics given problem, we can minimize the total annotation cost for achieving the desired performance by se"
W11-0313,P09-2079,0,0.0527389,"Missing"
W11-0313,P03-1054,0,0.00295238,"lassification task has low feature complexity and benefited the most from feature feedback. We compare our results on this task and the sentiment classification task on the movie review datasets. 4.2 time to annotate as instances without feature feedback. Thus, we measure the labeled data in terms of the number of annotation cost units which may mean different number of labeled instances based on the annotation strategy. We used two feature configurations of “unigram only” and “unigram+dependency triples”. The unigram and dependency annotations are derived from the Stanford Dependency Parser (Klein and Manning, 2003). Rationales by definition are spans of text in a review that convey the sentiment of the reviewer and hence are the part of the document most relevant for the classification task. In order to vary the amount of irrelevant text, we vary the amount of text (measured in terms of the number of characters) around the rationales that is included in the instance representation. We call this the slack around rationales. When using the rationales with or without the slack, only features that overlap with the rationales (and the slack, if used) are used to represent the instance. Since we only have rat"
W11-0313,P05-1015,0,0.095488,"Missing"
W11-0313,N04-1030,0,0.169296,"he feedback is solicited and incorporated into the model. We extend the complexity measures proposed in the literature and propose some new ones to categorize learning problems, and find that they are strong indicators of the benefit from feature feedback. 1 Direct feedback on a list of features (Raghavan et al., 2006; Druck et al., 2008) is limited to simple features like unigrams. However, unigrams are limited in the linguistic phenomena they can capture. Structured features such as dependency relations, paths in syntactic parse trees, etc., are often needed for learning the target concept (Pradhan et al., 2004; Joshi and Ros´e, 2009). It is not clear how direct feature feedback can be extended straightforwardly to structured features, as they are difficult to present visually for feedback and may require special expertise to comprehend. An alternative approach is to seek indirect feedback on structured features (Arora and Nyberg, 2009) by asking the user to highlight spans of text, called rationales, that support the instance label (Zaidan et al., 2007). For example, when classifying the sentiment of a movie review, rationales are spans of text in the review that support the sentiment label for the"
W11-0313,N07-1033,0,0.28617,"capture. Structured features such as dependency relations, paths in syntactic parse trees, etc., are often needed for learning the target concept (Pradhan et al., 2004; Joshi and Ros´e, 2009). It is not clear how direct feature feedback can be extended straightforwardly to structured features, as they are difficult to present visually for feedback and may require special expertise to comprehend. An alternative approach is to seek indirect feedback on structured features (Arora and Nyberg, 2009) by asking the user to highlight spans of text, called rationales, that support the instance label (Zaidan et al., 2007). For example, when classifying the sentiment of a movie review, rationales are spans of text in the review that support the sentiment label for the review. Introduction Linear classifiers model the response as a weighted linear combination of the features in input instances. A supervised approach to learning a linear classifier involves learning the weights for the features from labeled data. A large number of labeled instances may be needed to determine the class association of the features and learn accurate weights for them. Alternatively, the user may directly label the features. For exam"
W16-3104,P11-2121,0,\N,Missing
W16-3104,C12-1084,0,\N,Missing
W16-5202,P13-1166,0,0.0336972,"Missing"
W16-5202,gilmanov-etal-2014-swift,0,0.0168661,"allowing for their immediate inclusion in workflows supporting sophisticated applications as well as evaluation of their performance side-by-side with comparable components. Although many contributors host their own contributed services (which are called from within the LAPPS Grid), where necessary the LAPPS Grid provides hosting to ensure that software remains available to the community. Recently contributed tools include all core tools from University of Darmstadt’s DKPro3 , the AIFdb services for Argumentation analysis4 (Lawrence et al., 2012), the SWIFT Aligner for cross-lingual transfer (Gilmanov et al., 2014), the EDISON feature extraction framework5 (Sammons et al., 2016) and other tools available from the University of Illinois (e.g., semantic role labelers, entity extractors), among others. In addition, several of the basic components produced by the ARIEL team working within DARPA’s Low Resource Languages for Emergent Incidents (LORELEI) program have been integrated into the LAPPS Grid, which include tools and data to support a wide array of under-resourced languages. The LAPPS Grid has been adopted by a Mellon-funded project at the University of Illinois, which is utilizing the platform to ap"
W16-5202,W14-5204,1,0.848346,"s in views, where each view contains metadata that spells out the information contained in that view, including information necessary to determine compatibility with other tools and data. Semantic interoperability is achieved via references to definitions in the Web Services Exchange Vocabulary (WSEV). The WSEV has been built bottom up, driven by the needs of components in the LAPPS Grid and closely following standard practice in the field as well as adopting, where possible, existing terminology and type systems. Both LIF and the WSEV are described in detail elsewhere (Verhagen et al., 2016; Ide et al., 2014; Ide et al., 2016). Another distinctive feature of the LAPPS Grid is its Open Advancement (OA) Evaluation system, a sophisticated environment that was used to develop IBM’s Jeopardy-winning Watson. OA can be simultaneously applied to multiple variant workflows involving alternative tools for a given task, and the results are evaluated and displayed so that the best possible configuration is readily apparent. Similarly, the weak links in a chain are easily detected, which can lead to module-specific improvements that affect the entire process. The inputs, tools, parameters and settings used fo"
W16-5202,N16-3019,0,0.0252091,"considerable programming effort to add or modify components. Similarly, the Natural Language Toolkit (NLTK) requires Python programming and effectively limits the user to the tools that are built-in to the system. In contrast, modules can be easily added to the LAPPS Grid by wrapping them as a service, using provided templates; and, more importantly, no programming experience or technical expertise is required, since workflows are constructed using the Galaxy project’s workflow management framework. This makes the LAPPS Grid ideal for instructional use. The recently introduced Kathaa system (Mohanty et al., 2016) provides functionality similar to the LAPPS Grid, but allows modules to be interfaced only if compatible with one another–i.e., there is no attempt to standardize inputs and outputs among modules, so that mixing and matching of different tools that perform the same function is limited. The LAPPS Grid’s Open Advancement evaluation modules, which exploit the ability to construct alternative pipelines in order produce statistics identifying the most effective tool sequence and/or components accounting for the largest proportion of error, are also unique; Kathaa in contrast has only basic evaluat"
W16-5202,L16-1645,0,0.0188181,"histicated applications as well as evaluation of their performance side-by-side with comparable components. Although many contributors host their own contributed services (which are called from within the LAPPS Grid), where necessary the LAPPS Grid provides hosting to ensure that software remains available to the community. Recently contributed tools include all core tools from University of Darmstadt’s DKPro3 , the AIFdb services for Argumentation analysis4 (Lawrence et al., 2012), the SWIFT Aligner for cross-lingual transfer (Gilmanov et al., 2014), the EDISON feature extraction framework5 (Sammons et al., 2016) and other tools available from the University of Illinois (e.g., semantic role labelers, entity extractors), among others. In addition, several of the basic components produced by the ARIEL team working within DARPA’s Low Resource Languages for Emergent Incidents (LORELEI) program have been integrated into the LAPPS Grid, which include tools and data to support a wide array of under-resourced languages. The LAPPS Grid has been adopted by a Mellon-funded project at the University of Illinois, which is utilizing the platform to apply sophisticated HLT text mining methods to the HathiTrust Resea"
W16-6640,de-marneffe-etal-2006-generating,0,0.0355787,"Missing"
W16-6640,J05-1004,0,0.00941446,"ecking if their structure is the same, that is, if the subtrees’ labels are syntactically equivalent and the number of children is the same (as suggested by Wang and Neumann Proceedings of The 9th International Natural Language Generation conference, pages 242–243, c Edinburgh, UK, September 5-8 2016. 2016 Association for Computational Linguistics (2007)). Then, QG A SP checks, for each token pair, if they match. For the lexical match, lemmas are obtained from WordNet. The semantic match is based on the SRL predicted verb and a verb dictionary. This dictionary is the mapping between PropBank (Palmer et al., 2005), VerbNet (Kipper et al., 2000) and FrameNet (Baker et al., 2003), gathered from SemLink1 . If two verbs belong to the same set in any of the resources, they are considered to match. It is also considered a semantic match if two nonverb tokens belong to the same synset, from WordNet (Miller, 1995), or if two Named Entitiess (NEs) have the same type, according to Stanford Named Entity Recognition (NER). 3 Evaluation We tested QG A SP on the Engarte corpus2 . We used Engarte’s 32 revised triples labeled as true. These triples were then used both for PA and QG, and tested in a leave one out appro"
W16-6640,D14-1045,0,0.0144506,"et al. (2010)), but to the best of our knowledge QG A SP is the first system that relies on the lexical, syntactic and semantic information in both the Pattern Acquisition (PA) and the Question Generation (QG) steps. 2 QG A SP overview Figure 1 illustrates QG A SP architecture. 2.1 Pattern Acquisition Our seeds are triples constituted by a question, its answer (optional), and a snippet that could answer that question. The question and the snippet from each seed are processed by the Stanford syntactic and dependency parsers (de Marneffe et al., 2006), and MatePlus Semantic Role Labeler (SRL) (Roth and Woodsend, 2014). A pattern is a bidirectional 242 Figure 1: QG A SP overview mapping between subtrees of the question and the correspondent snippet. 2.2 Question Generation Given a sentence, QG A SP starts by parsing it, exactly as before; then it matches the previously learned patterns with the obtained structures. 2.3 The Matching Step The matching step is the same, both in the PA and QG stage. Considering that a loose matching strategy will result in many patterns and questions, thus introducing noise, whereas a too restrict approach will end up in too specific patterns and low variability of questions, Q"
W16-6640,W07-1406,0,0.0642699,"Missing"
W17-2307,P03-1003,0,0.124858,"Missing"
W17-2307,W16-3104,1,0.829838,"brary of Medicine (NLM) provides MEDLINE, a gigantic database of 23 million references to biomedical journal papers. Approximately 200,000 articles 1 from this database have been cited since 2015. The rapid growth of information in this centralized repository makes it difficult for medical researchers to manually find an exact answer for a question 1 https://www.nlm.nih.gov/bsd/medline_ lang_distr.html 58 Proceedings of the BioNLP 2017 workshop, pages 58–66, c Vancouver, Canada, August 4, 2017. 2017 Association for Computational Linguistics yes/no questions, we use one of the winning systems (Yang et al., 2016) from the 2015 edition of the BioASQ challenge, open-sourced after the conclusion of the challenge 2 . We build on standard techniques such as Maximal Marginal Relevance (Carbonell and Goldstein, 1998) and Sentence Compression (Filippova et al., 2015) and incorporate domain-specific knowledge using biomedical ontologies such as the UMLS metathesaurus and SNOMEDCT (Stearns et al., 2001) to build an ideal answer generator for biomedical questions. We also experiment with several similarity metrics such as jaccard similarity and a novel word embedding based tf-idf (w2v tf-idf) similarity metric w"
W17-2307,N03-1020,0,0.439421,"Missing"
W17-2307,D15-1042,0,\N,Missing
W17-4902,P03-1021,0,0.0263479,"S (128-128), M E (192-192) and L (256-256). Across models, we find that the M E configuration performs better in terms of highest validation BLEU. We also find that larger configurations (384-384 & 512-512) fail to converge or perform very poorly 8 . Here, we report results only for the M E configuration for all the models. For all our models, we picked the best saved model over 15 epochs which has the highest validation BLEU. To train statistical machine translation (SMT) baselines, we use publicly available open-source toolkit MOSES (Koehn et al., 2007), along with the GIZA++ word aligner (Och, 2003), as was done in (Xu et al., 2012). For training the targetside LM component, we use the lmplz toolkit within MOSES to train a 4-gram LM. We also use MERT (Och, 2003), available as part of MOSES, to tune on the validation set. For fairness of comparison, it is necessary to use the pairwise dictionary and PTB while training the SMT models as well - the most obvious way for this is to use the dictionary and PTB as additional training data for the alignment component and the target-side LM respectively. We experiment with several SMT models, ablating for the use of both PTB and dictionary. In 8,"
W17-4902,P11-1020,0,0.00999304,"test-time we use greedy decoding to find the most likely target sentence9 . We also experiment with a post-processing strategy which replaces UNKs in the target output with the highest aligned (maximum attention) source word. We find that this gives a small jump in BLEU of about 0.1-0.2 for all neural models 10 . Our best model, for instance, gets a jump of 0.14 to reach a BLEU of 31.26 from 31.12. Evaluation Our primary evaluation metric is BLEU (Papineni et al., 2002) . We compute BLEU using the freely available and very widely used perl script7 from the MOSES decoder. We also report PINC (Chen and Dolan, 2011), which originates from paraphrase evaluation liter6 Training and Parameters We use a minibatch-size of 32 and the ADAM optimizer (Kingma and Ba, 2014) with learning rate 0.001, momentum parameters 0.9 and 0.999, and  = 10−8 . All our implementations are written in Python using Tensorflow 1.1.0 framework. For every model, we experimented with two configurations of embedding and LSTM size S (128-128), M E (192-192) and L (256-256). Across models, we find that the M E configuration performs better in terms of highest validation BLEU. We also find that larger configurations (384-384 & 512-512) f"
W17-4902,P02-1040,0,0.0978547,"eover, a dictionary cannot easily capture one-to-many mappings as well as long-range dependencies 6 . 7.2.3 7.4 Off-the-shelf SMT 7.5 7 Decoding At test-time we use greedy decoding to find the most likely target sentence9 . We also experiment with a post-processing strategy which replaces UNKs in the target output with the highest aligned (maximum attention) source word. We find that this gives a small jump in BLEU of about 0.1-0.2 for all neural models 10 . Our best model, for instance, gets a jump of 0.14 to reach a BLEU of 31.26 from 31.12. Evaluation Our primary evaluation metric is BLEU (Papineni et al., 2002) . We compute BLEU using the freely available and very widely used perl script7 from the MOSES decoder. We also report PINC (Chen and Dolan, 2011), which originates from paraphrase evaluation liter6 Training and Parameters We use a minibatch-size of 32 and the ADAM optimizer (Kingma and Ba, 2014) with learning rate 0.001, momentum parameters 0.9 and 0.999, and  = 10−8 . All our implementations are written in Python using Tensorflow 1.1.0 framework. For every model, we experimented with two configurations of embedding and LSTM size S (128-128), M E (192-192) and L (256-256). Across models, we"
W17-4902,D14-1162,0,0.0865762,"s on all training sentences. We also experiment with adding additional data from PTB (Marcus et al., 1993) for better learning of embeddings. Additionally we leverage a dictionary mapping tokens from Shakespearean English to modern English. We consider four distinct strategies to train the embeddings. In the cases where we use external text data, we first train the embeddings using both the external data and training data, and then for the same number of iterations on training data alone, to ensure adaptation. Note that we do not directly use off-the-shelf pretrained embeddings such as Glove (Pennington et al., 2014) and Word2Vec (Mikolov et al., 2013) since we need to learn embeddings for novel word forms (and also different word senses for extant word forms) on the Original side. Table 2 shows some statistics from the training split of the dataset. In general, the Original side has longer sentences and a larger vocabulary. The slightly higher entropy of the Original side’s frequency distribution indicates that the frequencies are more spread out over words. Intuitively, the large number of shared word types indicates that sharing the representation between Original and Modern sides could provide some be"
W17-4902,D17-1315,1,0.814755,"ine translation (MT), , summarization (Rush et al., 2015), etc. In the context of MT, various settings such as multisource MT (Zoph and Knight, 2016) and MT with external information (Sennrich et al., 2016) have been explored. Distinct from all of these, our work attempts to solve a Modern English → Shakespearean English style transformation task. Although closely related to both paraphrasing and MT, our task has some differentiating characteristics such as considerable source-target overlap in vocabulary and grammar (unlike MT), and different source and target language (unlike paraphrasing). Gangal et al. (2017) have proposed a neural sequence-to-sequence solution for generating a portmanteau given two English root-words. Though their task also involves large overlap in target and input, they do not employ any special copying mechanism. Unlike text simplification and summarization, our task does not involve shortening content length. Figure 2: Attention matrices from a Copy (top) and a simple S2S (bottom) model respectively on the input sentence “Holy Saint Francis, this is a drastic change!” . < s &gt; and < /s &gt; are start and stop characters. Darker cells are higher-valued. semble the ground truth mor"
W17-4902,D15-1044,0,0.0389034,"life . then she is just a life of life, let me life out of life . then the window will let day in, and life out . Table 1: Examples from dataset showing modern paraphrases (M ODERN) of few sentences from Shakespeare’s plays (O RIGINAL). We also show transformation of modern text to Shakespearean text from our models (C OPY, S IMPLE S2S and S TAT). ∗ * denotes equal contribution https://github.com/harsh19/Shakespearizing-ModernEnglish 1 10 Proceedings of the Workshop on Stylistic Variation, pages 10–19 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics (Rush et al., 2015) , brand naming (Hiranandani et al., 2017), text expansion (Srinivasan et al., 2017), etc. However, there is a dearth of automated solutions for adapting text quickly to different styles. We consider the problem of transforming text written in modern English text to Shakepearean style English. For the sake of brevity and clarity of exposition, we henceforth refer to the Shakespearean sentences/side as Original and the modern English paraphrases as Modern. Unlike traditional domain or style transfer, our task is made more challenging by the fact that the two styles employ diachronically dispara"
W17-4902,P07-2045,0,0.0118972,"mented with two configurations of embedding and LSTM size S (128-128), M E (192-192) and L (256-256). Across models, we find that the M E configuration performs better in terms of highest validation BLEU. We also find that larger configurations (384-384 & 512-512) fail to converge or perform very poorly 8 . Here, we report results only for the M E configuration for all the models. For all our models, we picked the best saved model over 15 epochs which has the highest validation BLEU. To train statistical machine translation (SMT) baselines, we use publicly available open-source toolkit MOSES (Koehn et al., 2007), along with the GIZA++ word aligner (Och, 2003), as was done in (Xu et al., 2012). For training the targetside LM component, we use the lmplz toolkit within MOSES to train a 4-gram LM. We also use MERT (Och, 2003), available as part of MOSES, to tune on the validation set. For fairness of comparison, it is necessary to use the pairwise dictionary and PTB while training the SMT models as well - the most obvious way for this is to use the dictionary and PTB as additional training data for the alignment component and the target-side LM respectively. We experiment with several SMT models, ablatin"
W17-4902,P17-1099,0,0.0260369,"stical machine translation to transform text to target style. On the other hand our method is an endto-end trainable neural network. Saha Roy et al (2015) leverage different language models based on geolocation and occupation to align a text to specific style. However, their work is limited to addition of adjectives and adverbs. Our method can handle more generic transformations including addition and deletion of words. Pointer networks (Vinyals et al., 2015) allow the use of input-side words directly as output in a neural S2S model, and have been used for tasks like extractive summarization (See et al., 2017) (Zeng et al., 2016) and question answering (Wang and Jiang, 2016). However, pointer networks cannot generate words not present in the input. A mixture model of recurrent neural network and pointer We have demonstrated the transformation to Shakespearean style English only. Methods have to be explored to achieve other stylistic variations corresponding to formality and politeness of text, usage of fancier words and expressions, etc. We release our code publicly to foster further research on stylistic transformations on text. 12 . 12 https://github.com/harsh19/Shakespearizing-ModernEnglish 17 A"
W17-4902,N16-1005,0,0.11106,"se due to our preprocessing. Although this slightly affects BLEU, it helps prevent token occurrences getting split due to capitalization. 16 network has been shown to achieve good performance on language modeling task (Merity et al., 2016). S2S neural models, first proposed by Sutskever et al. (2014), and enhanced with a attention mechanism by Bahdanau et al. (2014), have yielded state-of-the-art results for machine translation (MT), , summarization (Rush et al., 2015), etc. In the context of MT, various settings such as multisource MT (Zoph and Knight, 2016) and MT with external information (Sennrich et al., 2016) have been explored. Distinct from all of these, our work attempts to solve a Modern English → Shakespearean English style transformation task. Although closely related to both paraphrasing and MT, our task has some differentiating characteristics such as considerable source-target overlap in vocabulary and grammar (unlike MT), and different source and target language (unlike paraphrasing). Gangal et al. (2017) have proposed a neural sequence-to-sequence solution for generating a portmanteau given two English root-words. Though their task also involves large overlap in target and input, they d"
W17-4902,C12-1177,0,0.469547,"wn and belike), novel grammatical constructions (two second person forms - thou (informal) and you (formal) (Brown et al., 1960)), semantically drifted senses (e.g fetches is a synonym of excuses) and non-standard orthography (Rayson et al., 2007). Additionally, there is a domain difference since the Shakespearean play sentences are from a dramatic screenplay whereas the parallel modern English sentences are meant to be simplified explanation for high-school students. Prior works in this field leverage a language model for the target style, achieving transformation either using phrase tables (Xu et al., 2012), or by inserting relevant adjectives and adverbs (Saha Roy et al., 2015). Such works have limited scope in the type of transformations that can be achieved. Moreover, statistical and rule MT based systems do not provide a direct mechanism to a) share word representation information between source and target sides b) incorporating constraints between words into word representations in end-to-end fashion. Neural sequence-tosequence models, on the other hand, provide such flexibility. Our main contributions are as follows: # Word Tokens # Word Types Average Sentence Length Entropy (Type.Dist) ∩"
W17-4902,N16-1004,0,0.0346923,"Copy+SL configurations. 11 All neural outputs are lowercase due to our preprocessing. Although this slightly affects BLEU, it helps prevent token occurrences getting split due to capitalization. 16 network has been shown to achieve good performance on language modeling task (Merity et al., 2016). S2S neural models, first proposed by Sutskever et al. (2014), and enhanced with a attention mechanism by Bahdanau et al. (2014), have yielded state-of-the-art results for machine translation (MT), , summarization (Rush et al., 2015), etc. In the context of MT, various settings such as multisource MT (Zoph and Knight, 2016) and MT with external information (Sennrich et al., 2016) have been explored. Distinct from all of these, our work attempts to solve a Modern English → Shakespearean English style transformation task. Although closely related to both paraphrasing and MT, our task has some differentiating characteristics such as considerable source-target overlap in vocabulary and grammar (unlike MT), and different source and target language (unlike paraphrasing). Gangal et al. (2017) have proposed a neural sequence-to-sequence solution for generating a portmanteau given two English root-words. Though their tas"
W17-4902,J93-2004,0,\N,Missing
W17-5545,D13-1160,0,0.0657811,"a seed lexicon and a generative grammar, we pair logical forms with mixed text-image representations and ask crowdworkers to paraphrase and confirm the plausibility of the queries that they generated. We use this method to build a semantic parsing dataset from scratch for a dialog agent in a smart-home simulation. We find evidence that this dataset, which we have named S MART H OME, is demonstrably more lexically diverse and difficult to parse than existing domain-specific semantic parsing datasets. 1 Figure 1: Crowdsourcing pipeline for building semantic parsers for new domains Mooney, 1996; Berant et al., 2013; Branavan et al., 2009; Azaria et al., 2016; Gulwani and Marron, 2014; Krishnamurthy and Kollar, 2013). Orienting a dialogue-capable intelligent system is accomplished by training its semantic parser with utterances that capture the nuances of the domain. An inherent challenge lies in building datasets that have enough lexical diversity for granting the system robustness against natural language variation in query-based dialogue. With the advent of datadriven methods for semantic parsing (Dong and Lapata, 2016; Jia and Liang, 2016), constructing such realistic and sufficient-sized dialog data"
W17-5545,P09-1010,0,0.0240886,"generative grammar, we pair logical forms with mixed text-image representations and ask crowdworkers to paraphrase and confirm the plausibility of the queries that they generated. We use this method to build a semantic parsing dataset from scratch for a dialog agent in a smart-home simulation. We find evidence that this dataset, which we have named S MART H OME, is demonstrably more lexically diverse and difficult to parse than existing domain-specific semantic parsing datasets. 1 Figure 1: Crowdsourcing pipeline for building semantic parsers for new domains Mooney, 1996; Berant et al., 2013; Branavan et al., 2009; Azaria et al., 2016; Gulwani and Marron, 2014; Krishnamurthy and Kollar, 2013). Orienting a dialogue-capable intelligent system is accomplished by training its semantic parser with utterances that capture the nuances of the domain. An inherent challenge lies in building datasets that have enough lexical diversity for granting the system robustness against natural language variation in query-based dialogue. With the advent of datadriven methods for semantic parsing (Dong and Lapata, 2016; Jia and Liang, 2016), constructing such realistic and sufficient-sized dialog datasets for specific domai"
W17-5545,W10-2903,0,0.0864785,"Missing"
W17-5545,P16-1004,0,0.023426,"1: Crowdsourcing pipeline for building semantic parsers for new domains Mooney, 1996; Berant et al., 2013; Branavan et al., 2009; Azaria et al., 2016; Gulwani and Marron, 2014; Krishnamurthy and Kollar, 2013). Orienting a dialogue-capable intelligent system is accomplished by training its semantic parser with utterances that capture the nuances of the domain. An inherent challenge lies in building datasets that have enough lexical diversity for granting the system robustness against natural language variation in query-based dialogue. With the advent of datadriven methods for semantic parsing (Dong and Lapata, 2016; Jia and Liang, 2016), constructing such realistic and sufficient-sized dialog datasets for specific domains becomes especially important, and is often the bottleneck for applying semantic parsers to new tasks. Introduction Semantic parsing is the task of mapping natural language utterances to their underlying meaning representations. This is an essential component for many tasks that require understanding natural language dialogue (Woods, 1977; Zelle and Wang et al. (2015) propose a methodology for efficient creation of semantic parsing data that starts with the set of target logical forms,"
W17-5545,P16-1002,0,0.0946799,"ne for building semantic parsers for new domains Mooney, 1996; Berant et al., 2013; Branavan et al., 2009; Azaria et al., 2016; Gulwani and Marron, 2014; Krishnamurthy and Kollar, 2013). Orienting a dialogue-capable intelligent system is accomplished by training its semantic parser with utterances that capture the nuances of the domain. An inherent challenge lies in building datasets that have enough lexical diversity for granting the system robustness against natural language variation in query-based dialogue. With the advent of datadriven methods for semantic parsing (Dong and Lapata, 2016; Jia and Liang, 2016), constructing such realistic and sufficient-sized dialog datasets for specific domains becomes especially important, and is often the bottleneck for applying semantic parsers to new tasks. Introduction Semantic parsing is the task of mapping natural language utterances to their underlying meaning representations. This is an essential component for many tasks that require understanding natural language dialogue (Woods, 1977; Zelle and Wang et al. (2015) propose a methodology for efficient creation of semantic parsing data that starts with the set of target logical forms, and ∗ *The indicated a"
W17-5545,Q13-1016,0,0.0830323,"entations and ask crowdworkers to paraphrase and confirm the plausibility of the queries that they generated. We use this method to build a semantic parsing dataset from scratch for a dialog agent in a smart-home simulation. We find evidence that this dataset, which we have named S MART H OME, is demonstrably more lexically diverse and difficult to parse than existing domain-specific semantic parsing datasets. 1 Figure 1: Crowdsourcing pipeline for building semantic parsers for new domains Mooney, 1996; Berant et al., 2013; Branavan et al., 2009; Azaria et al., 2016; Gulwani and Marron, 2014; Krishnamurthy and Kollar, 2013). Orienting a dialogue-capable intelligent system is accomplished by training its semantic parser with utterances that capture the nuances of the domain. An inherent challenge lies in building datasets that have enough lexical diversity for granting the system robustness against natural language variation in query-based dialogue. With the advent of datadriven methods for semantic parsing (Dong and Lapata, 2016; Jia and Liang, 2016), constructing such realistic and sufficient-sized dialog datasets for specific domains becomes especially important, and is often the bottleneck for applying semant"
W17-5545,P15-1129,0,0.385032,"Abhilasha Ravichander1∗, Thomas Manzini1∗, Matthias Grabmair1 Graham Neubig1 , Jonathan Francis12 , Eric Nyberg1 1 Language Technologies Institute, Carnegie Mellon University 2 Robert Bosch LLC, Corporate Sector Research and Advanced Engineering {aravicha, tmanzini, mgrabmai, gneubig, ehn}@cs.cmu.edu jon.francis@us.bosch.com Abstract Building dialogue interfaces for realworld scenarios often entails training semantic parsers starting from zero examples. How can we build datasets that better capture the variety of ways users might phrase their queries, and what queries are actually realistic? Wang et al. (2015) proposed a method to build semantic parsing datasets by generating canonical utterances using a grammar and having crowdworkers paraphrase them into natural wording. A limitation of this approach is that it induces bias towards using similar language as the canonical utterances. In this work, we present a methodology that elicits meaningful and lexically diverse queries from users for semantic parsing tasks. Starting from a seed lexicon and a generative grammar, we pair logical forms with mixed text-image representations and ask crowdworkers to paraphrase and confirm the plausibility of the q"
W17-5545,N13-1103,0,0.177964,"Missing"
W17-5545,P11-1060,0,0.187332,"Missing"
W17-5545,W16-6644,0,0.0547291,"gested cannot be used to generate all the queries we may want to support in a new domain, and (3) there is no check on the correctness or naturalness of the canonical utterances themselves, which may not be logically plausible. This is problematic as even unlikely canonical utterances can be paraphrased fluently. 2 In this paper, we propose and evaluate a new approach for creating lexically diverse and plausible utterances for semantic parsing (Figure 1.). Firstly, inspired by the use of images in the creation of datasets for paraphrasing (Lin et al., 2014) or for natural language generation (Novikova et al., 2016), we seek to reduce this linguistic bias by using a lexicon consisting of images. Secondly, a generative grammar, which is tailored to the domain, combines these images to form mixed text-image representations. Using these two approaches, we retain many of the advantages of existing approaches such as ease of supervision and completeness of the dataset, with the added bonus of promoting lexical diversity in the natural language utterances, and supporting queries relevant to our domain. Finally, we add a simple step within the crowdsourcing experiment where crowd-workers evaluate the plausibili"
W17-5545,E17-1052,0,0.0456458,"Missing"
W17-5545,P15-1085,0,0.0354576,"Missing"
W18-1001,P17-1018,0,0.0684989,"Missing"
W18-1001,D17-1215,0,0.0233824,"inference-oriented reading comprehension. We use parallel passages from different sources for generating reasoning questions which encourage systems to gain a deeper understanding of language, and become robust to variations in style and topic. We include examples from our initial pilot study in Table 6. • High Lexical Overlap - Incorrect Sentence: The models tend to pick answer spans from sentences which have high lexical overlap with the question. We observe that this accounts for the largest chunk of errors across all models (example 2). Our observations are consistent with the findings of Jia and Liang (2017). The models often simply resolve the referential expression in the question to its corresponding entity. In example 1, the models resolve “organisation” in the question to “The UN” due to high lexical similarity. • Incorrect Answer Boundaries: This is the second most frequently observed error, where the answers generated are almost correct, but models face issues in appropriately defining answer boundaries (example 3). R-Net and DrQA, on average, produce shorter answers. BiDAF tends to produce longer answers. • Missing Logical Inference: Models are sometimes unable to make certain logical con"
W18-1001,P17-1147,0,0.0813222,"Missing"
W18-1001,D16-1264,0,0.252533,"to generalize indicates certain shortcomings. We believe that it is important to develop benchmarks which give a realistic sense of a system’s RC capabilities. Thus, our goal in this paper is two-fold: Proof of Concept: We propose a method to create an RC dataset that assesses a model’s ability to: • move beyond lexical pattern matching between the question and passage, • infer the correct answers to questions which contain referring expressions, and • generalize to different language styles. Analysis of Existing Models: We test three end-to-end neural MRC models, which perform well on SQuAD (Rajpurkar et al., 2016), on a few question-answer pairs generated using our methodology. We demonstrate that it is indeed difficult for these systems to answer such questions, also indicating their tendencies to resort to shallow pattern matching and overfit to training data. In this paper, we investigate the tendency of end-to-end neural Machine Reading Comprehension (MRC) models to match shallow patterns rather than perform inference-oriented reasoning on RC benchmarks. We aim to test the ability of these systems to answer questions which focus on referential inference. We propose ParallelQA, a strategy to formula"
W18-1001,D13-1020,0,0.0161674,"to derive simple neural baseline architectures can achieve comparable results. Our experiments also indicate that pattern matching can work well on these datasets. Inference, an important RC skill (Spearritt, 1972; Strange, 1980), is the ability to understand the meaning of text without all the information being stated explicitly. Table 5, Section A describes the types of inference that we may encounter while comprehending a passage along with the cues that ∗ 2 Existing Datasets In this work, we focus on datasets with multiword spans as answers rather than cloze-style RC datasets like MCTest (Richardson et al., 2013), CNN / Daily Mail (Hermann et al., 2015) and Children’s Book Test (Weston et al., 2015). The Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) was one of the first large scale RC datasets (over 100k QA pairs), where the answer to each question is a span in the given passage. For its collection, different sets of crowd-workers were asked to formulate questions and answers using passages obtained from ∼500 Wikipedia articles. However, this resulted in the questions having similar word patterns to the sentences containing the answers. We empirically demonstrate this in Table 1"
W18-1001,P17-1096,0,0.0337878,"Missing"
W18-2312,D14-1162,0,0.0811049,"pairs for a particular question to train a three-way neural classifier to predict if the relationship between the two is entailment, contradiction or neither. It is worth noting here that the embedding transformation techniques that we implemented are not specific to the NLI tasks and, in fact, enable transfer learning of a much broader set of tasks on smaller datasets like BioASQ by using the pre1. The number of assertion-sentence pairs in BioASQ is too few to train the textual entailment model effectively. 2. The models that are pre-trained on SNLI (Bowman et al., 2015) datasets use GLOVE (Pennington et al., 2014) embeddings that cannot be used for biomedical corpora which have quite different characteristics and vocabulary compared to the corpora that GLOVE was trained on. However, we have pre-trained embeddings available that were trained on PubMed and PMC texts along with Wikipedia articles (Pyysalo et al., 2013). To leverage these embeddings, we implemented an embedding-transformation methodology to projecting the PubMed embeddings to GLOVE embedding space and then fine tune the pre-trained InferSent on the BioASQ dataset for textual entailment. The hypothesis is that, since both the embeddings had"
W18-2312,D15-1075,0,0.0355495,"ence embeddings of the assertion-sentence pairs for a particular question to train a three-way neural classifier to predict if the relationship between the two is entailment, contradiction or neither. It is worth noting here that the embedding transformation techniques that we implemented are not specific to the NLI tasks and, in fact, enable transfer learning of a much broader set of tasks on smaller datasets like BioASQ by using the pre1. The number of assertion-sentence pairs in BioASQ is too few to train the textual entailment model effectively. 2. The models that are pre-trained on SNLI (Bowman et al., 2015) datasets use GLOVE (Pennington et al., 2014) embeddings that cannot be used for biomedical corpora which have quite different characteristics and vocabulary compared to the corpora that GLOVE was trained on. However, we have pre-trained embeddings available that were trained on PubMed and PMC texts along with Wikipedia articles (Pyysalo et al., 2013). To leverage these embeddings, we implemented an embedding-transformation methodology to projecting the PubMed embeddings to GLOVE embedding space and then fine tune the pre-trained InferSent on the BioASQ dataset for textual entailment. The hypo"
W18-2312,W17-2337,0,0.0808783,"ranking algorithms to generate the final predictions. Introduction In the era of ever advancing medical sciences and the age of the internet, a remarkable amount of medical literature is constantly being posted online. This has led to a need for an effective retrieval and indexing system which can allow us to extract meaningful information from these vast knowledge sources. One of the most effective and natural ways to leverage this huge amount of data 4. We improve upon the MMR framework for relevant sentence selection from the chosen snippets that was introduced in the work of Chandu et al. (2017). We experiment with a number of more informative similarity metrics to replace and improve upon the baseline Jaccard similarity metric. 109 Proceedings of the BioNLP 2018 workshop, pages 109–117 c Melbourne, Australia, July 19, 2018. 2018 Association for Computational Linguistics 2 Relevant Literature self. The 5b version of this dataset consists of 1,799 questions in 3 distinct categories: Biomedical Question answering has always been a hot topic of research among the QA community at large due to the relative significance of the problem and the challenge of dealing with a non standard vocabu"
W18-2312,W17-2307,1,0.839086,"nd supervised ranking algorithms to generate the final predictions. Introduction In the era of ever advancing medical sciences and the age of the internet, a remarkable amount of medical literature is constantly being posted online. This has led to a need for an effective retrieval and indexing system which can allow us to extract meaningful information from these vast knowledge sources. One of the most effective and natural ways to leverage this huge amount of data 4. We improve upon the MMR framework for relevant sentence selection from the chosen snippets that was introduced in the work of Chandu et al. (2017). We experiment with a number of more informative similarity metrics to replace and improve upon the baseline Jaccard similarity metric. 109 Proceedings of the BioNLP 2018 workshop, pages 109–117 c Melbourne, Australia, July 19, 2018. 2018 Association for Computational Linguistics 2 Relevant Literature self. The 5b version of this dataset consists of 1,799 questions in 3 distinct categories: Biomedical Question answering has always been a hot topic of research among the QA community at large due to the relative significance of the problem and the challenge of dealing with a non standard vocabu"
W18-2312,P17-1099,0,0.0371143,"l answer type questions on the BioASQ dataset. For exact answers, we incorporate neural entailment models along with a novel embedding transformation technique for answering yes/no questions, and employ LeToR ranking models to answer factoid/list based questions. For ideal answers, we improve the IR component of extractive summarization. Although this improves ROUGE scores considerably, the human readability aspect of the generated summary answer is not greatly improved. As future directions, we believe that effective abstractive summarization based approaches like Pointer Generator Networks (See et al., 2017) and Reinforcement Learning based techniques (Paulus et al., 2017) would improve the human readability of ideal answers. We aim to continue our research in this direction to achieve a good balance between ROUGE score and human readability. Learning To Rank In order to rank the candidate entities in a supervised way, we use a ranking classifier based on the features described in 5.2.2. For ranking, we choose point-wise ranking classifiers over pairwise and list-wise, because it yields similar results to ranking methods with a less time-consuming and computationally expensive approach. We use a"
W18-2312,P05-1022,0,0.285486,"Missing"
W18-2312,D17-1070,0,0.0170719,"ons for all yes/no questions. As a simple extension to this, we can also create negative assertions by using not along with the auxiliary verbs. 5.1.2 Recognizing Textual Entailment The primary goal of our NLI module is to infer if any of the sentences among the answer snippets entails or contradicts the assertion posed by the question. We segmented the answer snippets for each question to produce a set of assertionsentence pairs. To then evaluate if these assertions can be inferred or refuted from the sentences, we built a Recognizing Textual Entailment (RTE) model using the InferSent model (Conneau et al., 2017), which computes sentence embeddings for every sentence and has been shown to work well on NLI tasks. In training InferSent, we experienced two major challenges: W ∗ = arg minkW Ep |− Eg |k W subject to the constraint that W is orthogonal. The solution to this optimization problem is given by using the singular value decomposition of Eg |Ep , i.e.W ∗ = U V |where Eg |Ep = U ΣV |With this simple linear transformation, we then computed the transformed embeddings for all the words in the PubMed embeddings that are not present in the GLOVE embeddings. We also explore a non-linear transformation us"
W18-2312,W17-2309,0,0.0405859,"Missing"
W18-2312,W04-1013,0,0.0179173,"Hence, we present a Natural Language Inference (NLI)-based system that learns if the assertions made by the questions are true in the context of the documents. As a part of this system, we first generate assertions from questions and evaluate the entailment or contradiction of these assertions using a Recognizing Textual Entailment (RTE) model. We then use these entailment scores for all the sentences in the snippets or documents to heuristically evaluate if the answer to the yes/no question. Evaluation The pipeline described above is primarily designed to improve the ROUGE evaluation metric (Lin, 2004). Although a higher ROUGE score does not necessarily reflect improved human readability, MMR can improve readability by reducing redundancy in generated answers. Results for ideal answers for Task 5 phase b are shown in Table 1. We also compare our results with other state of the art approaches in Table 4. 5 Yes/No type questions Exact answers Exact answers represent the subset of the BioASQ task where the responses are not structured paragraphs, but instead either a single entity (yes/no types) or a combination of named entities (factoid or list types) that compose the correct reply to the gi"
W18-2312,W18-5300,0,0.246742,"Missing"
W18-2610,P17-1171,0,0.01401,"ts collection, different sets of crowd-workers formulated questions and answers using passages obtained from ∼500 Wikipedia articles. The answer to each question is a span in the given passage, and many effective neural QA models have been developed for this dataset. Our main focus in this work is to perform comparative subjective and empirical analysis of errors in answer predictions by four top performing models on the SQuAD leaderboard1 . We focused on Bi-Directional Attention Flow (BiDAF) (Seo et al., 2016), Gated Self-Matching Networks (R-Net) (Wang et al., 2017), Document Reader (DrQA) (Chen et al., 2017), MultiParagraph Reading Comprehension (DocQA) (Clark and Gardner, 2017), and the Logistic Regression baseline model (Rajpurkar et al., 2016) We mainly choose these models since they have comparable high performance on the evaluation metrics and it is easy to replicate their results due to availability of open source implementations. The task of Question Answering has gained prominence in the past few decades for testing the ability of machines to understand natural language. Large datasets for Machine Reading have led to the development of neural models that cater to deeper language understan"
W18-2610,P16-1145,0,0.0701043,"Missing"
W18-2610,P17-1147,0,0.0339647,"er. 1 Introduction Machine Reading is a task in which a model reads a piece of text and attempts to formally represent it or performs a downstream task like Question Answering (QA). Neural approaches to the latter have gained a lot of prominence especially owing to the recent spur in developing and publicly releasing large datasets on Machine Reading and Comprehension (MRC). These datasets are created from different underlying sources such as web resources in MS MARCO (Nguyen et al., 2016); trivia and web in QUASAR-S and QUASAR-T (Dhingra et al., 2017), SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017); news articles in CNN/Daily Mail (Chen et al.), NewsQA (Trischler et al., 2016) and stories in NarrativeQA (Koˇcisk`y 1 https://rajpurkar.github.io/ SQuAD-explorer/ 89 Proceedings of the Workshop on Machine Reading for Question Answering, pages 89–97 c Melbourne, Australia, July 19, 2018. 2018 Association for Computational Linguistics Document Reader (DrQA): This model, proposed by Chen et al. (2017), focuses on answering open-domain factoid questions using Wikipedia, but also performs well on SQuAD (skipping the document retrieval stage). Its implementation4 has paragraph and question encodi"
W18-2610,D16-1264,0,0.359202,"nature and properties of questions and answers in these datasets. Based on the dataset, certain neural models capitalize on these biases while others are unable to. The ability to generalize across different sources and domains is a desirable characteristic for any machine reading system. Evaluating and analyzing systems on QA tasks can lead to insights for advancements in machine reading and natural language understanding, and Pe˜nas et al. (2011) have also previously worked on this. One of the first large MRC datasets (over 100k QA pairs) is the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016). For its collection, different sets of crowd-workers formulated questions and answers using passages obtained from ∼500 Wikipedia articles. The answer to each question is a span in the given passage, and many effective neural QA models have been developed for this dataset. Our main focus in this work is to perform comparative subjective and empirical analysis of errors in answer predictions by four top performing models on the SQuAD leaderboard1 . We focused on Bi-Directional Attention Flow (BiDAF) (Seo et al., 2016), Gated Self-Matching Networks (R-Net) (Wang et al., 2017), Document Reader ("
W18-2610,W18-1001,1,0.80719,"Missing"
W18-2610,P17-1018,0,0.0572344,"aset (SQuAD) (Rajpurkar et al., 2016). For its collection, different sets of crowd-workers formulated questions and answers using passages obtained from ∼500 Wikipedia articles. The answer to each question is a span in the given passage, and many effective neural QA models have been developed for this dataset. Our main focus in this work is to perform comparative subjective and empirical analysis of errors in answer predictions by four top performing models on the SQuAD leaderboard1 . We focused on Bi-Directional Attention Flow (BiDAF) (Seo et al., 2016), Gated Self-Matching Networks (R-Net) (Wang et al., 2017), Document Reader (DrQA) (Chen et al., 2017), MultiParagraph Reading Comprehension (DocQA) (Clark and Gardner, 2017), and the Logistic Regression baseline model (Rajpurkar et al., 2016) We mainly choose these models since they have comparable high performance on the evaluation metrics and it is easy to replicate their results due to availability of open source implementations. The task of Question Answering has gained prominence in the past few decades for testing the ability of machines to understand natural language. Large datasets for Machine Reading have led to the development of neural mo"
W18-5307,P11-3002,0,0.026911,"ks of text. The algorithm for ordering the blocks of texts combines document heuristics with our Similarity Ordering algorithm. We first order the blocks by their length (the number of sentences in teh block). For blocks of equal length, we calculate the similarity of each block with the last fixed sen3.2 Algorithms and Techniques 3.2.1 Similarity Ordering The intuition behind the Similarity Ordering algorithm is that sentences that have similar content should appear consecutively so that the generated answer is not jumping back and forth between topics. Our implementation is based on work by Zhang (2011), which discusses the use of similarity metrics at two levels - first to cluster sentences, and then to order them within a cluster - which can lead to big improvements in coherency and readability. We apply this approach to the BQA 58 tence. Hence, given the last sentence of the preceding block, we select the next block first by its length, and then by the similarity of the block with the preceding sentence. If there is no single longest block to begin the answer, then we select the longest block that is most similar to the entire answer. This algorithm is tuned for specific goals with respec"
W18-5307,W17-2307,1,0.934539,"Extraction Meets Abstraction: Ideal Answer Generation for Biomedical Questions Yutong Li∗, Nicholas Gekakis∗, Qiuze Wu∗, Boyue Li∗, Khyathi Raghavi Chandu, Eric Nyberg Language Technologies Institute, Carnegie Mellon University {anareshk,hkesavam,madhurad,pkalwad,kchandu,teruko,ehn}@cs.cmu.edu Abstract icantly reducing the effort required to locate the most relevant information in a large corpus. Our goal is to build an effective BQA system to generate coherent, query-oriented, non-redundant, human-readable summaries for biomedical questions. Our approach is based on an extractive BQA system (Chandu et al., 2017) which performed well on automatic metrics (ROUGE) in the 5th edition of the BioASQ challenge. However, owing to the extractive nature of this system, it suffers from problems in human readability and coherence. In particular, extractive summaries which concatenate the most relevant text units from multiple documents are often incoherent to the reader, especially when the answer sentences jump back and forth between topics. Although the existing extractive approach explicitly attempts to reduce redundancy at the sentence level (via SoftMMR), stitching together existing sentences always admits"
W18-5307,C10-1037,0,0.0168118,"y improve the performance of the Block Ordering algorithm. We note that the Block ordering algorithm performed well in producing highquality, coherent answers; although the develop4.1 Methodology Given a set of candidate sentences generated by the pipeline for each summary, the sentence fusion module operates in two steps: 1) the candidate set is expanded to include fused sentences, and 2) sentences are selected from the expanded set to produce a new summary. 4.1.1 Expansion of Candidate Set To generate fused sentences, we begin by building upon previous work on multiple-sentence compression (Filippova, 2010), in which a directed word graph is used to express sentence structures. The word graph is constructed by iteratively adding candidate sentences. All words in the first sentence are added to the graph by creating a sequence of word nodes. A word in the following sentence is then mapped onto an existing word node if and only if it is the same word, with the same part of speech. Our assumption is that a shared node in the word graph is likely to refer to the same entity or event across sentences. We then find a K-possible fused sentence by searching for the K-shortest path within the word 60 gra"
W18-5307,W09-1802,0,0.0413717,"ur fusion technique is able to extract important information and formulate it into complete sentences, producing a new summary containing the following sentence: ‘Mir-155 targets the chromatin protein jarid2 to regulate proinflammatory cytokine expression in th17 cells.’ 4.1.2 Selecting Sentences from Candidate Set The fusion module is also able to compress multiple sentences into one, with minor grammatical errors. For example: The next step is to select sentences from the candidate set and produce a new summary. An Integer Linear Program (ILP) problem is formulated as follows, according to (Gillick and Favre, 2009): max y,z M ∑ N ∑ i=1 wi zi , such that M ∑ Sentence 1: ‘The RESID Database is a comprehensive collection of annotations and structures for protein post-translational modifications including N-terminal, C-terminal and peptide chain cross-link modifications[1].’ Sentence 2: ‘The RESID Database contains supplemental information on post-translational modifications for the standardized annotations appearing in the PIR-International Protein Sequence Database[2]’ Aij yj ≥ zi , Aij yj ≤ zi , j=1 lj yj ≤ L, yj ∈ {0, 1}, zi ∈ {0, 1} j=1 our approach produces the fused sentence: In the equation, zi is a"
W18-5307,P11-1049,0,0.0220585,"h by creating a sequence of word nodes. A word in the following sentence is then mapped onto an existing word node if and only if it is the same word, with the same part of speech. Our assumption is that a shared node in the word graph is likely to refer to the same entity or event across sentences. We then find a K-possible fused sentence by searching for the K-shortest path within the word 60 graph. Definition of the edge weights follows from the original paper (Filippova, 2010): w(eij ) = diverse set of concepts. We follow the convention of using bigrams as a surrogate for concepts (Taylor Berg-Kirkpatrick and Klein, 2011; Dan Gillick and Hakkani-Tur, 2008), and bigram counts as initial weights. Variable Aij indicates whether concept i appears in sentence j, and variable yj indicates if a sentence j is selected or not. ∑f req(i)+f req(j)−1 s∈S dif f (s,i,j) f req(i) × f req(j) where dif f (s, i, j) is the difference between the offset positions of word i and j in sentence s. Intuitively, we want to promote a connection between two word nodes with close distance, and between nodes that have multiple paths between them. We also prefer a compression path that goes through the most frequent no-stop nodes to emphas"
W18-5310,W17-2307,1,0.866427,"when compared with either approach used in isolation. The dataset we use for development of the current work is released as a part of the sixth edition of the annual BioASQ challenge (Tsatsaronis et al., 2012). The main categories of answers in this data include summary, factoid, list and yes/no. There are a total of 2,251 questions, each of which is accompanied by a list of relevant documents and a list of relevant snippets extracted from each of these documents. Our model is an extension to the highest ROUGE scoring model in the final test batch of the fifth edition of the BioASQ challenge (Chandu et al., 2017), which is based on Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998). In addition, we attempted abstractive techniques that are scoped to improve the readability and coherence aspect of the problem. We made 4 submissions to the challenge. The paper is organized as follows: Section 2 describes our overall system architecture and the implementation details. Experiments and results are discussed in Section 3 followed by conclusion and future work in 4. The ever-increasing magnitude of biomedical information sources makes it difficult and time-consuming for a human researcher to fi"
W18-5310,P17-1099,0,0.0242833,"e and abstractive summarization. Extractive summarization works by selecting the most relevant sentences in a document to generate the summary (Allahyari et al., 2017). The summaries generated using this technique generally obtain high ROUGE scores (Lin, 2004) due to the high n-gram overlap between the generated summary and the ideal answer. Abstractive summarization on the other hand works by generating the summary word by word as opposed to picking sentences in the case of extractive summarization. Recent advances in abstractive summarization using Pointer Generator Coverage (PGC) networks (See et al., 2017) have shown that neural sequence to sequence models can generate abstractive sumRelevanceScore (1) = scoreBM 25 (idealanswer, snippet) 2.2.3 RankSVM RankSVM(Cao et al., 2006) is a pairwise LETOR approach towards ranking of documents. Each pair of snippets was taken for a question and was labeled as -1 if the second snippet was ranked higher and +1 if the second snippet was ranked lower. In a pairwise approach there is an overhead of maintaining the metadata as we need to know which set of snippets are going into the SVM as input for validation of the model. Consider F (Q, S1 ) as a feature rep"
W18-5310,W12-0201,0,0.0300876,"icroarrays have been used to assess DNA replication timing in a variety of eukaryotic organisms”, the clause “in a variety of eukaryotic organisms” would be missed in the phase II of ontology creation. But in Phase II, we convert such that the verb “assess” has an attribute “{in, nodexyz }” where nodexyz is the node pertaining to the CUI of “eukaryotic organisms”. 2.1.2 Graph Creation Graph Framework: All PubMed abstracts are tokenized and relations are extracted from them. These are added as relations in the graph. We create custom data structures for the Nodes and Edges(Relations) in Neo4j (Webber, 2012). Every relation has attributes which are comma separated values of PubMed ID, location within the abstract. This is stored in order to retrieve the exact sentence that was used to create a particular relation. We hypothesize that this can improve in getting relevant snippets across the abstracts. Phase I: Ontology Creation with UMLS Concepts. Part of speech tagging is the most intuitive way of approaching the problem of extracting the relations from a given text. An initial strategy of forming the Subject Verb Object (SVO) triplets was formed based on a left-right parsing of the text. For thi"
W18-5310,P15-1128,0,0.0301019,"tion. However, common NLP tools aren’t easily leveraged on biomedical text, due to dramatic differences in the structure and content of the sentences. There exist tools for relation extraction in sub-domains such as Bacteria (Duclos et al., 2007) and disease-cause ontologies (Schriml et al., 2011), but these methods heavily rely on the presence of specific words or features at the sentence level, and cannot be easily scaled to general biotext. Most neural methods for training relation extractors require a large (O(106 )) corpus of labelled examples, which is not available for general biotext (Yih et al., 2015). In order to explore the use of ontology-based retrieval, we developed a novel Ontology-Based Information Retrieval Although a large amount of biomedical text is available in resources such as NLM (NIH, 2018), it can be difficult to leverage in the absence of supervised or automatic labeling (annotation) of the unstructured text content. Our hypothesis is that an Ontology-based retrieval module which utilizes entity and relation extraction techniques to represent and compare the content of questions and candidate answers can improve the recall of answerbearing documents from unstructured sour"
W18-5310,P14-5010,0,0.00251535,"relations is depicted in Figure 4. RE approach, which is described below. The base architecture for the RE module is depicted in Figure 3. The following 4 steps are employed for extracting relations from a sentence: 1. Noun Phrase Chunking: The sentence is parsed using the TreeTagger POS tagger (Schmid, 1995) to obtain all the Noun Chunks that form the potential nodes of the graph. For our purpose, the nodes of the graph are all Medical and Named Entities. In order to perform this, the potential nodes are passed through a Medical Entity Recogniser (GRAM-CNN) (Zhu et al.) and the Stanford NER (Manning et al., 2014) discarding the chunks that are not recognized. For an example, let us consider the following sentence: ‘Genomic microarrays have been used to assess DNA replication timing in a variety of eukaryotic organisms.’ which extract the following noun chunks: ‘Genomic Microarrays’, ‘DNA Replication Timing’ and ‘Eukaryotic Organisms’. 2. Relation Extraction: This step comprises of 2 sub parts. (2a) RE using Predicate Argument Structures: The Predicate Argument Structure (PAS) for the sentence, obtained using the Enju parser (Miyao et al., 2008), is further parsed in order to obtain possible relations"
W18-5310,P08-1006,0,0.01494,"ecogniser (GRAM-CNN) (Zhu et al.) and the Stanford NER (Manning et al., 2014) discarding the chunks that are not recognized. For an example, let us consider the following sentence: ‘Genomic microarrays have been used to assess DNA replication timing in a variety of eukaryotic organisms.’ which extract the following noun chunks: ‘Genomic Microarrays’, ‘DNA Replication Timing’ and ‘Eukaryotic Organisms’. 2. Relation Extraction: This step comprises of 2 sub parts. (2a) RE using Predicate Argument Structures: The Predicate Argument Structure (PAS) for the sentence, obtained using the Enju parser (Miyao et al., 2008), is further parsed in order to obtain possible relations for the graph. Possible relations are those that contain arguments related through a verb or a preposition. (2b) RE transformation through transitivity: Transitivity is performed on relations obtained from the Enju parser in order to ensure that the arguments of the relations represent medical or named entities in the graph. The potential nodes are passed through the NER and MER. Nodes that are not tagged or recognized by either undergo a transitive transformation to give way to new relations. For the example mentioned, the following re"
W19-5041,W19-5039,0,0.443147,"e of Natural Language Inference (NLI) in passage retrieval, answer selection and answer re-ranking to advance open-domain question answering. (Tari et al., 2007) shows effective use of UMLS (Bodenreider, 2004), a Unified Medical Language System to asses passage relevancy through semantic relatedness. All these methods work well independently, but to the best of our knowledge, there hasn’t been much work in using NLI and RQE systems in tandem for the tasks of filtering and re-ranking. Dataset & Evaluation The dataset for re-ranking and filtering has been provided by the MediQA Shared task (Ben Abacha et al., 2019) in ACL-BioNLP 2019 workshop. It consists of medical questions and their associated answers retrieved by CHiQA 2 . The training dataset consists of 208 questions while the validation and test datasets have 25 and 150 questions respectivley. Each question has upto 10 candidate answers, with each answer having the following attributes : 1. SystemRank: rank. As noted in (Romanov and Shivade, 2018), the task of Natural Language Inference is not domain agnostic, and thus is not able to transfer well to other domains. The authors use a gradient boosting classifier (Mason et al., 2000) with a variety"
W19-5041,P17-1152,0,0.0602761,"while the validation and test datasets have 25 and 150 questions respectivley. Each question has upto 10 candidate answers, with each answer having the following attributes : 1. SystemRank: rank. As noted in (Romanov and Shivade, 2018), the task of Natural Language Inference is not domain agnostic, and thus is not able to transfer well to other domains. The authors use a gradient boosting classifier (Mason et al., 2000) with a variety of hand crafted features for baselines. They then use Infersent (Conneau et al., 2017) as a sentence encoder. The paper also reports results on the ESIM Model (Chen et al., 2017) but with no visible improvements. They also discuss transfer learning and external knowledge based methods. It corresponds to CHiQA’s 2. ReferenceRank: It corresponds to the correct rank. 3. ReferenceScore: This is an additional score that is provided only in the training and validation sets, which corresponds to the manual judgment/rating of the answer [4: Excellent, 3: Correct but Incomplete, 2: Related, 1: Incorrect]. For the answer classification task, answers with scores 1 and 2 are considered as incorrect (label 0), and answers with scores 3 and 4 are considered as correct (label 1). Th"
W19-5041,D17-1070,0,0.167715,"Missing"
W19-5041,P06-1114,0,0.308079,"randomly sorted. Karan and Shefali took ownership of the NLI module while Sheetal and Prashant worked on the RQE module. Hemant researched and implemented the Question-Answering system including baseline and multi-task learning. Sheetal and Hemant worked on scraping data from icliniq. Karan and Prashant helped with integration of NLI and RQE module respectively into the multi-task system. 1 https://github.com/google-research/bert/issues/27 389 Proceedings of the BioNLP 2019 workshop, pages 389–398 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics 3 challenges. (Harabagiu and Hickl, 2006) successfully shows the use of Natural Language Inference (NLI) in passage retrieval, answer selection and answer re-ranking to advance open-domain question answering. (Tari et al., 2007) shows effective use of UMLS (Bodenreider, 2004), a Unified Medical Language System to asses passage relevancy through semantic relatedness. All these methods work well independently, but to the best of our knowledge, there hasn’t been much work in using NLI and RQE systems in tandem for the tasks of filtering and re-ranking. Dataset & Evaluation The dataset for re-ranking and filtering has been provided by th"
W19-5041,P19-1139,0,0.0302928,"ncorrectly Predicting Contradiction on Test Set The model also fails while trying to differentiate between statements that are neutral versus those that entail each other. The model generally relies on lexical overlap between the hypothesis and the premise, and in cases, when it is unable to find one, falls back to assigning the label as neutral as shown in Figure 4. For the RQE task, we observe that our model la395 them randomly during training so that the model learns the semantic representation even without the medical entities. Masking entities has been shown to generalize better in ERNIE(Zhang et al., 2019) in comparison to BERT(Devlin et al., 2018). For the re-ranking and filtering tasks we look into the macro-trends and investigate what qualifies as tougher problems for both the tasks. From Figure 7, it is clear that lower ranked valid answers are generally harder answers for filtering. Observing the valid answers with low ranks, we see that they generally have only 1-2 relevant sentences each, which might be hard for the model especially in cases where the answers have a lot of sentences. Similar analysis for the filtering tasks based on the Figure 4: NLI model Incorrectly Predicting Neutral"
W19-5041,P19-1441,0,0.225157,"main to improve domain specific IR and QA systems. The challenge consists of three tasks which are evaluated separately. The first task is the Natural Language Inference (NLI) task which focuses on determining whether a natural language hypothesis can be inferred from a natural language premise. The second task is to recognize question entailment (RQE) between a pair of questions. The third task is to filter and improve the ranking of automatically retrieved answers. For the NLI and RQE tasks, we use transfer learning on prevalent pre-trained models like BERT (Devlin et al., 2018) and MT-DNN (Liu et al., 2019). These models play a pivotal role to gain deeper semantic understanding of the content for the final task (filtering and re-ranking) of the challenge (Demszky et al., 2018). Besides using usual techniques for candidate answer selection and re-ranking, we use features obtained from NLI and RQE models. We majorly concentrate on the novel multi-task approach in this paper. We also succinctly describe our NLI and RQE models and their performance on the final leaderboard. Parallel deep learning architectures like finetuned BERT and MT-DNN, have quickly become the state of the art, bypassing previo"
W19-5041,P14-5010,0,0.00250207,"delineate their medical issues, which are then paraphrased as short queries by medical experts. The user queries are treated as CHQs whereas the paraphrased queries are treated as FAQs. We extract 9,958 positive examples and generate an equal number of negative examples by random sampling. The average CHQ length is 180 tokens whereas the average FAQ length is 11 tokens. In addition, the expert answers are used to augment the MediQUAD corpus (Ben Abacha and Demner-Fushman, 2019). 4 answers having “Updated by:” are removed. A coreference resolution is run on each answer using Stanford CoreNLP (Manning et al., 2014) and all the entity-mentions are replaced with their corresponding names. 4.3 For each question in the training set we get upto N entailing questions (along with their scores and embeddings) and answers with a threshold T for confidence using RQE module. We use this system both in the baseline and the multi-task learning system. The complete process is highlighted in Figure 1. 4.4 We use pretrained RQE and NLI modules as feature extractors to compute best entailed questions and best candidate answers in our proposed pipeline. 1. Answer Source (One-hot) 2. Answer Length In Sentences 3. ChiQA Ra"
W19-5041,D18-1187,0,0.346482,"been much work in using NLI and RQE systems in tandem for the tasks of filtering and re-ranking. Dataset & Evaluation The dataset for re-ranking and filtering has been provided by the MediQA Shared task (Ben Abacha et al., 2019) in ACL-BioNLP 2019 workshop. It consists of medical questions and their associated answers retrieved by CHiQA 2 . The training dataset consists of 208 questions while the validation and test datasets have 25 and 150 questions respectivley. Each question has upto 10 candidate answers, with each answer having the following attributes : 1. SystemRank: rank. As noted in (Romanov and Shivade, 2018), the task of Natural Language Inference is not domain agnostic, and thus is not able to transfer well to other domains. The authors use a gradient boosting classifier (Mason et al., 2000) with a variety of hand crafted features for baselines. They then use Infersent (Conneau et al., 2017) as a sentence encoder. The paper also reports results on the ESIM Model (Chen et al., 2017) but with no visible improvements. They also discuss transfer learning and external knowledge based methods. It corresponds to CHiQA’s 2. ReferenceRank: It corresponds to the correct rank. 3. ReferenceScore: This is an"
W19-5048,W19-5039,0,0.214687,"fic resources. Inspired by their observations, we explore several techniques of augmenting domain-specific features with the state-of-the-art methods. We hope that the deep neural networks will help the model learn about the task itself and the domain-specific features will assist the model in tacking the issues associated with such specialized domains. For instance, the medical domain has a distinct sublanguage (Friedman et al., 2002) and it presents challenges such as abbreviations, inconsistent spellings, relationship between drugs, diseases, symptoms. Introduction The ACL-BioNLP 2019 (Ben Abacha et al., 2019) shared task focuses on improving the following three tasks for medical domain: 1) Natural Language Inference (NLI) 2) Recognizing Question Entailment (RQE) and 3) Question-Answering reranking system. Our team has made submissions to all the three tasks. We note that in this work we focus more on the task 1 and task 2 as improvements in these two tasks reflect directly on the task 3. However, as per the shared task guidelines, we do submit one model for the task 3 to complete our submission. Our approach for both task 1 and task 2 is based on the state-of-the-art natural language understanding"
W19-5048,D15-1075,0,0.0639317,"hekhar Bannihatti Kumar ∗ Ashwin Srinivasan∗ Aditi Chaudhary∗ James Route Teruko Mitamura Eric Nyberg {vbkumar, ashwinsr, aschaudh, jroute, teruko, ehn}@cs.cmu.edu Language Technologies Institute Carnegie Mellon University Abstract the efficacy of learning universal language representations in providing a decent warm start to a task-specific model, by leveraging large amounts of unlabeled data. MT-DNN uses BERT as the encoder and uses MTL to fine-tune the multiple taskspecific layers. This model has obtained stateof-the-art results on several natural language understanding tasks such as SNLI (Bowman et al., 2015), SciTail (Khot et al., 2018) and hence forms the basis of our approach. For the task 3, we use a simple model to combine the task 1 and task 2 models as shown in §2.5. This paper presents the submissions by Team Dr.Quad to the ACL-BioNLP 2019 shared task on Textual Inference and Question Entailment in the Medical Domain. Our system is based on the prior work Liu et al. (2019) which uses a multi-task objective function for textual entailment. In this work, we explore different strategies for generalizing state-of-the-art language understanding models to the specialized medical domain. Our resu"
W19-5048,P19-1441,0,0.213043,"unlabeled data. MT-DNN uses BERT as the encoder and uses MTL to fine-tune the multiple taskspecific layers. This model has obtained stateof-the-art results on several natural language understanding tasks such as SNLI (Bowman et al., 2015), SciTail (Khot et al., 2018) and hence forms the basis of our approach. For the task 3, we use a simple model to combine the task 1 and task 2 models as shown in §2.5. This paper presents the submissions by Team Dr.Quad to the ACL-BioNLP 2019 shared task on Textual Inference and Question Entailment in the Medical Domain. Our system is based on the prior work Liu et al. (2019) which uses a multi-task objective function for textual entailment. In this work, we explore different strategies for generalizing state-of-the-art language understanding models to the specialized medical domain. Our results on the shared task demonstrate that incorporating domain knowledge through data augmentation is a powerful strategy for addressing challenges posed by specialized domains such as medicine. 1 As discussed above, state-of-the-art models using deep neural networks have shown significant performance gains across various natural language processing (NLP) tasks. However, their g"
W19-5048,N18-1202,0,0.0561994,"ts in these two tasks reflect directly on the task 3. However, as per the shared task guidelines, we do submit one model for the task 3 to complete our submission. Our approach for both task 1 and task 2 is based on the state-of-the-art natural language understanding model MT-DNN (Liu et al., 2019), which combines the strength of multi-task learning (MTL) and language model pre-training. MTL in deep networks has shown performance gains when related tasks are trained together resulting in better generalization to new domains (Ruder, 2017). Recent works such as BERT (Devlin et al., 2018), ELMO (Peters et al., 2018) have shown ∗ Our resulting models perform fairly on the unseen test data of the ACL-MediQA shared task. On Task 1, our best model achieves +14.1 gain above the baseline. On Task 2, our five-model ensemble achieved +12.6 gain over the baseline and for Task 3 our model achieves a a +4.9 gain. equal contribution 453 Proceedings of the BioNLP 2019 workshop, pages 453–461 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics 2 Approach Entailment Contradiction Neutral In this section, we first present our base model MT-DNN (Liu et al., 2019) which we use for both Task 1"
W19-5048,D18-1187,0,0.0772083,"Missing"
W19-5049,W14-3405,0,0.0299029,"5.0 75.2 75.0 Q1 (CHQ): Hello doctor, I do not have a white half moon on my nails. Is there any thyroid issue? If yes, please suggest some treatment.” Q2 (FAQ): Does the absence of the white half moon on nails indicate a thyroid problem? Gold Label: True Table 2: Baseline precision, recall and F1 values for RQE 5 Proposed Approach 5.1 Additional Datasets 5.1.4 Our hypothesis is that these parallel datasets will help our multi-task neural model capture salient biomedical features to help our main NLI and RQE tasks. 5.1.1 The dataset released by the Genetic and Rare diseases information center (Roberts et al., 2014) allows our model to learn question type information necessary for the RQE task. It contains 3137 questions each of which has one of 13 unique labels. Since the question type is an important handcrafted feature while considering traditional ML approaches for the RQE task, we use this dataset so that our multi-task model can leverage this information. The merit of this approach is shown in Table 3. PubMed RCT The Pubmed RCT dataset contains 2.3m sentences from 200k PubMed abstracts of randomized controlled trial (RCT) articles. We use the smaller subset of the sentences from 20k abstracts. The"
W19-5049,D18-1187,0,0.1053,"Missing"
W19-5049,D17-1070,0,0.0646386,"2.2 Biomedical Textual Inference The initial approaches for predicting inference relations between two sentences in the medical domain involved several neural architectures. (Ro∗ 1 Related Work *denotes equal contribution https://www.nih.gov/ 462 Proceedings of the BioNLP 2019 workshop, pages 462–470 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics 3.0.2 manov and Shivade, 2018) details the curation of the MedNLI dataset, and describes multiple baseline approaches. A Feature-based, Bag-of-Words (BOW), the ESIM model (Chen et al., 2016) and the InferSent model (Conneau et al., 2017) being among them. 2.3 RQE The RQE dataset comprises of consumer health questions (CHQs) received by the National Library of Medicine and frequently asked questions (FAQs) collected from the National Institutes of Health (NIH) websites (Ben Abacha and DemnerFushman, 2017). Biomedical Question Entailment • Training Set: 8,588 medical question pairs The initial work (Ben Abacha and DemnerFushman, 2017), in addition to creating the working dataset for RQE, uses handcrafted lexical and semantic features as an input to traditional machine learning models like SVM, Logistic Regression, and Naive Bay"
W19-5049,P19-1441,0,0.0604767,"Missing"
