2020.lrec-1.613,Recommendation Chart of Domains for Cross-Domain Sentiment Analysis: Findings of A 20 Domain Study,2020,22,0,3,0,17881,akash sheoran,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Cross-domain sentiment analysis (CDSA) helps to address the problem of data scarcity in scenarios where labelled data for a domain (known as the target domain) is unavailable or insufficient. However, the decision to choose a domain (known as the source domain) to leverage from is, at best, intuitive. In this paper, we investigate text similarity metrics to facilitate source domain selection for CDSA. We report results on 20 domains (all possible pairs) using 11 similarity metrics. Specifically, we compare CDSA performance with these metrics for different domain-pairs to enable the selection of a suitable source domain, given a target domain. These metrics include two novel metrics for evaluating domain adaptability to help source domain selection of labelled data and utilize word and sentence-based embeddings as metrics for unlabelled data. The goal of our experiments is a recommendation chart that gives the K best source domains for CDSA for a given target domain. We show that the best K source domains returned by our similarity metrics have a precision of over 50{\%}, for varying values of K."
W19-5015,A Comparison of Word-based and Context-based Representations for Classification Problems in Health Informatics,2019,28,0,1,1,17882,aditya joshi,Proceedings of the 18th BioNLP Workshop and Shared Task,0,"Distributed representations of text can be used as features when training a statistical classifier. These representations may be created as a composition of word vectors or as context-based sentence vectors. We compare the two kinds of representations (word versus context) for three classification problems: influenza infection classification, drug usage classification and personal health mention classification. For statistical classifiers trained for each of these problems, context-based representations based on ELMo, Universal Sentence Encoder, Neural-Net Language Model and FLAIR are better than Word2Vec, GloVe and the two adapted using the MESH ontology. There is an improvement of 2-4{\%} in the accuracy when these context-based representations are used instead of word-based representations."
W19-1309,{``}When Numbers Matter!{''}: Detecting Sarcasm in Numerical Portions of Text,2019,0,1,4,0,23488,abhijeet dubey,"Proceedings of the Tenth Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"Research in sarcasm detection spans almost a decade. However a particular form of sarcasm remains unexplored: sarcasm expressed through numbers, which we estimate, forms about 11{\%} of the sarcastic tweets in our dataset. The sentence {`}Love waking up at 3 am{'} is sarcastic because of the number. In this paper, we focus on detecting sarcasm in tweets arising out of numbers. Initially, to get an insight into the problem, we implement a rule-based and a statistical machine learning-based (ML) classifier. The rule-based classifier conveys the crux of the numerical sarcasm problem, namely, incongruity arising out of numbers. The statistical ML classifier uncovers the indicators i.e., features of such sarcasm. The actual system in place, however, are two deep learning (DL) models, CNN and attention network that obtains an F-score of 0.93 and 0.91 on our dataset of tweets containing numbers. To the best of our knowledge, this is the first line of research investigating the phenomenon of sarcasm arising out of numbers, culminating in a detector thereof."
U19-1008,Red-faced {ROUGE}: Examining the Suitability of {ROUGE} for Opinion Summary Evaluation,2019,-1,-1,2,0,3120,wenyi tay,Proceedings of the The 17th Annual Workshop of the Australasian Language Technology Association,0,"One of the most common metrics to automatically evaluate opinion summaries is ROUGE, a metric developed for text summarisation. ROUGE counts the overlap of word or word units between a candidate summary against reference summaries. This formulation treats all words in the reference summary equally.In opinion summaries, however, not all words in the reference are equally important. Opinion summarisation requires to correctly pair two types of semantic information: (1) aspect or opinion target; and (2) polarity of candidate and reference summaries. We investigate the suitability of ROUGE for evaluating opin-ion summaries of online reviews. Using three simulation-based experiments, we evaluate the behaviour of ROUGE for opinion summarisation on the ability to match aspect and polarity. We show that ROUGE cannot distinguish opinion summaries of similar or opposite polarities for the same aspect. Moreover,ROUGE scores have significant variance under different configuration settings. As a result, we present three recommendations for future work that uses ROUGE to evaluate opinion summarisation."
U19-1020,Does Multi-Task Learning Always Help?: An Evaluation on Health Informatics,2019,0,0,1,1,17882,aditya joshi,Proceedings of the The 17th Annual Workshop of the Australasian Language Technology Association,0,"Multi-Task Learning (MTL) has been an attractive approach to deal with limited labeled datasets or leverage related tasks, for a variety of NLP problems. We examine the benefit of MTL for three specific pairs of health informatics tasks that deal with: (a) overlapping symptoms for the same classification problem (personal health mention classification for influenza and for a set of symptoms); (b) overlapping medical concepts for related classification problems (vaccine usage and drug usage detection); and, (c) related classification problems (vaccination intent and vaccination relevance detection). We experiment with a simple neural architecture: a shared layer followed by task-specific dense layers. The novelty of this work is that it compares alternatives for shared layers for these pairs of tasks. While our observations agree with the promise of MTL as compared to single-task learning, for health informatics, we show that the benefit also comes with caveats in terms of the choice of shared layers and the relatedness between the participating tasks."
U19-1026,Overview of the 2019 {ALTA} Shared Task: Sarcasm Target Identification,2019,-1,-1,2,0,11574,diego molla,Proceedings of the The 17th Annual Workshop of the Australasian Language Technology Association,0,"We present an overview of the 2019 ALTA shared task. This is the 10th of the series of shared tasks organised by ALTA since 2010. The task was to detect the target of sarcastic comments posted on social media. We intro- duce the task, describe the data and present the results of baselines and participants. This year{'}s shared task was particularly challenging and no participating systems improved the re- sults of our baseline."
P19-1108,Figurative Usage Detection of Symptom Words to Improve Personal Health Mention Detection,2019,18,0,2,0,25614,adith iyer,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Personal health mention detection deals with predicting whether or not a given sentence is a report of a health condition. Past work mentions errors in this prediction when symptom words, i.e., names of symptoms of interest, are used in a figurative sense. Therefore, we combine a state-of-the-art figurative usage detection with CNN-based personal health mention detection. To do so, we present two methods: a pipeline-based approach and a feature augmentation-based approach. The introduction of figurative usage detection results in an average improvement of 2.21{\%} F-score of personal health mention detection, in the case of the feature augmentation-based approach. This paper demonstrates the promise of using figurative usage detection to improve personal health mention detection."
W18-5911,Shot Or Not: Comparison of {NLP} Approaches for Vaccination Behaviour Detection,2018,0,1,1,1,17882,aditya joshi,Proceedings of the 2018 {EMNLP} Workshop {SMM}4{H}: The 3rd Social Media Mining for Health Applications Workshop {\\&} Shared Task,0,"Vaccination behaviour detection deals with predicting whether or not a person received/was about to receive a vaccine. We present our submission for vaccination behaviour detection shared task at the SMM4H workshop. Our findings are based on three prevalent text classification approaches: rule-based, statistical and deep learning-based. Our final submissions are: (1) an ensemble of statistical classifiers with task-specific features derived using lexicons, language processing tools and word embeddings; and, (2) a LSTM classifier with pre-trained language models."
L18-1424,Sarcasm Target Identification: Dataset and An Introductory Approach,2018,0,1,1,1,17882,aditya joshi,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
W17-5201,Detecting Sarcasm Using Different Forms Of Incongruity,2017,0,0,1,1,17882,aditya joshi,"Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"Sarcasm is a form of verbal irony that is intended to express contempt or ridicule. Often quoted as a challenge to sentiment analysis, sarcasm involves use of words of positive or no polarity to convey negative sentiment. Incongruity has been observed to be at the heart of sarcasm understanding in humans. Our work in sarcasm detection identifies different forms of incongruity and employs different machine learning techniques to capture them. This talk will describe the approach, datasets and challenges in sarcasm detection using different forms of incongruity. We identify two forms of incongruity: incongruity which can be understood based on the target text and common background knowledge, and incongruity which can be understood based on the target text and additional, specific context. The former involves use of sentiment-based features, word embeddings, and topic models. The latter involves creation of author{'}s historical context based on their historical data, and creation of conversational context for sarcasm detection of dialogue."
D17-3002,Computational Sarcasm,2017,-1,-1,2,0,382,pushpak bhattacharyya,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts,0,"Sarcasm is a form of verbal irony that is intended to express contempt or ridicule. Motivated by challenges posed by sarcastic text to sentiment analysis, computational approaches to sarcasm have witnessed a growing interest at NLP forums in the past decade. Computational sarcasm refers to automatic approaches pertaining to sarcasm. The tutorial will provide a bird{'}s-eye view of the research in computational sarcasm for text, while focusing on significant milestones.The tutorial begins with linguistic theories of sarcasm, with a focus on incongruity: a useful notion that underlies sarcasm and other forms of figurative language. Since the most significant work in computational sarcasm is sarcasm detection: predicting whether a given piece of text is sarcastic or not, sarcasm detection forms the focus hereafter. We begin our discussion on sarcasm detection with datasets, touching on strategies, challenges and nature of datasets. Then, we describe algorithms for sarcasm detection: rule-based (where a specific evidence of sarcasm is utilised as a rule), statistical classifier-based (where features are designed for a statistical classifier), a topic model-based technique, and deep learning-based algorithms for sarcasm detection. In case of each of these algorithms, we refer to our work on sarcasm detection and share our learnings. Since information beyond the text to be classified, contextual information is useful for sarcasm detection, we then describe approaches that use such information through conversational context or author-specific context.We then follow it by novel areas in computational sarcasm such as sarcasm generation, sarcasm v/s irony classification, etc. We then summarise the tutorial and describe future directions based on errors reported in past work. The tutorial will end with a demonstration of our work on sarcasm detection.This tutorial will be of interest to researchers investigating computational sarcasm and related areas such as computational humour, figurative language understanding, emotion and sentiment sentiment analysis, etc. The tutorial is motivated by our continually evolving survey paper of sarcasm detection, that is available on arXiv at: Joshi, Aditya, Pushpak Bhattacharyya, and Mark James Carman. {``}Automatic Sarcasm Detection: A Survey.{''} arXiv preprint arXiv:1602.03426 (2016)."
W16-5001,{`}Who would have thought of that!{'}: A Hierarchical Topic Model for Extraction of Sarcasm-prevalent Topics and Sarcasm Detection,2016,20,4,1,1,17882,aditya joshi,Proceedings of the Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics ({E}x{P}ro{M}),0,"Topic Models have been reported to be beneficial for aspect-based sentiment analysis. This paper reports the first topic model for sarcasm detection, to the best of our knowledge. Designed on the basis of the intuition that sarcastic tweets are likely to have a mixture of words of both sentiments as against tweets with literal sentiment (either positive or negative), our hierarchical topic model discovers sarcasm-prevalent topics and topic-level sentiment. Using a dataset of tweets labeled using hashtags, the model estimates topic-level, and sentiment-level distributions. Our evaluation shows that topics such as {`}work{'}, {`}gun laws{'}, {`}weather{'} are sarcasm-prevalent topics. Our model is also able to discover the mixture of sentiment-bearing words that exist in a text of a given sentiment-related label. Finally, we apply our model to predict sarcasm in tweets. We outperform two prior work based on statistical classifiers with specific features, by around 25{\%}."
W16-2111,How Do Cultural Differences Impact the Quality of Sarcasm Annotation?: A Case Study of {I}ndian Annotators and {A}merican Text,2016,-1,-1,1,1,17882,aditya joshi,"Proceedings of the 10th {SIGHUM} Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities",0,None
W16-0415,Political Issue Extraction Model: A Novel Hierarchical Topic Model That Uses Tweets By Political And Non-Political Authors,2016,26,9,1,1,17882,aditya joshi,"Proceedings of the 7th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"People often use social media to discuss opinions, including political ones. We refer to relevant topics in these discussions as political issues, and the alternate stands towards these topics as political positions. We present a Political Issue Extraction (PIE) model that is capable of discovering political issues and positions from an unlabeled dataset of tweets. A strength of this model is that it uses twitter timelines of political and non-political authors, and affiliation information of only political authors. The model estimates word-specific distributions (that denote political issues and positions) and hierarchical author/group-specific distributions (that show how these issues divide people). Our experiments using a dataset of 2.4 million tweets from the US show that this model effectively captures the desired properties (with respect to words and groups) of political discussions. We also evaluate the two components of the model by experimenting with: (a) Use to alternate strategies to classify words, and (b) Value addition due to incorporation of group membership information. Estimated distributions are then used to predict political affiliation with 68% accuracy."
U16-1013,How Challenging is Sarcasm versus Irony Classification?: A Study With a Dataset from {E}nglish Literature,2016,-1,-1,1,1,17882,aditya joshi,Proceedings of the Australasian Language Technology Association Workshop 2016,0,None
L16-1349,"That{'}ll Do Fine!: A Coarse Lexical Resource for {E}nglish-{H}indi {MT}, Using Polylingual Topic Models",2016,16,1,2,0.512821,8127,diptesh kanojia,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Parallel corpora are often injected with bilingual lexical resources for improved Indian language machine translation (MT). In absence of such lexical resources, multilingual topic models have been used to create coarse lexical resources in the past, using a Cartesian product approach. Our results show that for morphologically rich languages like Hindi, the Cartesian product approach is detrimental for MT. We then present a novel {`}sentential{'} approach to use this coarse lexical resource from a multilingual topic model. Our coarse lexical resource when injected with a parallel corpus outperforms a system trained using parallel corpus and a good quality lexical resource. As demonstrated by the quality of our coarse lexical resource and its benefit to MT, we believe that our sentential approach to create such a resource will help MT for resource-constrained languages."
K16-1015,Harnessing Sequence Labeling for Sarcasm Detection in Dialogue from {TV} Series {`}{F}riends{'},2016,32,18,1,1,17882,aditya joshi,Proceedings of The 20th {SIGNLL} Conference on Computational Natural Language Learning,0,"This paper is a novel study that views sarcasm detection in dialogue as a sequence labeling task, where a dialogue is made up of a sequence of utterances. We create a manuallylabeled dataset of dialogue from TV series xe2x80x98Friendsxe2x80x99 annotated with sarcasm. Our goal is to predict sarcasm in each utterance, using sequential nature of a scene. We show performance gain using sequence labeling as compared to classification-based approaches. Our experiments are based on three sets of features, one is derived from information in our dataset, the other two are from past works. Two sequence labeling algorithms (SVM-HMM and SEARN) outperform three classification algorithms (SVM, Naive Bayes) for all these feature sets, with an increase in F-score of around 4%. Our observations highlight the viability of sequence labeling techniques for sarcasm detection of dialogue."
D16-1104,Are Word Embedding-based Features Useful for Sarcasm Detection?,2016,11,27,1,1,17882,aditya joshi,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
C16-1234,Towards Sub-Word Level Compositions for Sentiment Analysis of {H}indi-{E}nglish Code Mixed Text,2016,26,18,1,1,17882,aditya joshi,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Sentiment analysis (SA) using code-mixed data from social media has several applications in opinion mining ranging from customer satisfaction to social campaign analysis in multilingual societies. Advances in this area are impeded by the lack of a suitable annotated dataset. We introduce a Hindi-English (Hi-En) code-mixed dataset for sentiment analysis and perform empirical analysis comparing the suitability and performance of various state-of-the-art SA methods in social media. In this paper, we introduce learning sub-word level representations in our LSTM (Subword-LSTM) architecture instead of character-level or word-level representations. This linguistic prior in our architecture enables us to learn the information about sentiment value of important morphemes. This also seems to work well in highly noisy text containing misspellings as shown in our experiments which is demonstrated in morpheme-level feature maps learned by our model. Also, we hypothesize that encoding this linguistic prior in the Subword-LSTM architecture leads to the superior performance. Our system attains accuracy 4-5{\%} greater than traditional approaches on our dataset, and also outperforms the available system for sentiment analysis in Hi-En code-mixed text by 18{\%}."
W15-5912,A temporal expression recognition system for medical documents by,2015,0,0,2,0,36369,naman gupta,Proceedings of the 12th International Conference on Natural Language Processing,0,None
W15-5945,Using Multilingual Topic Models for Improved Alignment in {E}nglish-{H}indi {MT},2015,18,1,2,0.512821,8127,diptesh kanojia,Proceedings of the 12th International Conference on Natural Language Processing,0,None
W15-2905,Your Sentiment Precedes You: Using an author{'}s historical tweets to predict sarcasm,2015,11,41,2,0,36874,anupam khattri,"Proceedings of the 6th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"Sarcasm understanding may require information beyond the text itself, as in the case of xe2x80x98I absolutely love this restaurant!xe2x80x99 which may be sarcastic, depending on the contextual situation. We present the first quantitative evidence to show that historical tweets by an author can provide additional context for sarcasm detection. Our sarcasm detection approach uses two components: a contrast-based predictor (that identifies if there is a sentiment contrast within a target tweet), and a historical tweet-based predictor (that identifies if the sentiment expressed towards an entity in the target tweet agrees with sentiment expressed by the author towards that entity in the past)."
S15-2098,{S}entibase: Sentiment Analysis in {T}witter on a Budget,2015,19,3,2,0,37255,satarupa guha,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"Like SemEval 2013 and 2014, the task Sentiment Analysis in Twitter found a place in this yearxe2x80x99s SemEval too and attracted an unprecedented number of participations. This task comprises of four sub-tasks. We participated in subtask 2 xe2x80x94 Message polarity classification. Although we lie a few notches down from the top system, we present a very simple yet effective approach to handle this problem that can be implemented in a single day!"
S15-2129,{SIEL}: Aspect Based Sentiment Analysis in Reviews,2015,29,5,2,0,37255,satarupa guha,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"Following the footsteps of SemEval-2014 Task 4 (Pontiki et al., 2014), SemEval-2015 too had a task dedicated to aspect-level sentiment analysis (Pontiki et al., 2015), which saw participation from over 25 teams. In Aspectbased Sentiment Analysis, the aim is to identify the aspects of entities and the sentiment expressed for each aspect. In this paper, we present a detailed description of our system, that stood 4th in Aspect Category subtask (slot 1), 7th in Opinion Target Expression subtask (slot 2) and 8th in Sentiment Polarity subtask (slot 3) on the Restaurant datasets."
P15-2100,A Computational Approach to Automatic Prediction of Drunk-Texting,2015,13,5,1,1,17882,aditya joshi,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Alcohol abuse may lead to unsociable behavior such as crime, drunk driving, or privacy leaks. We introduce automatic drunk-texting prediction as the task of identifying whether a text was written when under the influence of alcohol. We experiment with tweets labeled using hashtags as distant supervision. Our classifiers use a set of N-gram and stylistic features to detect drunk tweets. Our observations present the first quantitative evidence that text contains signals that can be exploited to detect drunk-texting."
P15-2124,Harnessing Context Incongruity for Sarcasm Detection,2015,16,101,1,1,17882,aditya joshi,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,The relationship between context incongruity and sarcasm has been studied in linguistics. We present a computational system that harnesses context incongruity as a basis for sarcasm detection. Our statistical sarcasm classifiers incorporate two kinds of incongruity features: explicit and implicit. We show the benefit of our incongruity features for two text forms tweets and discussion forum posts. Our system also outperforms two past works (with Fscore improvement of 10-20%). We also show how our features can capture intersentential incongruity.
W14-2623,A cognitive study of subjectivity extraction in sentiment annotation,2014,7,8,2,0,23266,abhijit mishra,"Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"Existing sentiment analysers are weak AI systems: they try to capture the functionality of human sentiment detection faculty, without worrying about how such faculty is realized in the hardware of the human. These analysers are agnostic of the actual cognitive processes involved. This, however, does not deliver when applications demand order of magnitude facelift in accuracy, as well as insight into characteristics of sentiment detection process. In this paper, we present a cognitive study of sentiment detection from the perspective of strong AI. We study the sentiment detection process of a set of human xe2x80x9csentiment readersxe2x80x9d. Using eye-tracking, we show that on the way to sentiment detection, humans first extract subjectivity. They focus attention on a subset of sentences before arriving at the overall sentiment. This they do either through xe2x80x9danticipationxe2x80x9d where sentences are skipped during the first pass of reading, or through xe2x80x9dhomingxe2x80x9d where a subset of the sentences are read over multiple passes, or through both. xe2x80x9dHomingxe2x80x9d behaviour is also observed at the sub-sentence level in complex sentiment phenomena like sarcasm."
P14-2007,Measuring Sentiment Annotation Complexity of Text,2014,21,13,1,1,17882,aditya joshi,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"The effort required for a human annotator to detect sentiment is not uniform for all texts, irrespective of his/her expertise. We aim to predict a score that quantifies this effort, using linguistic properties of the text. Our proposed metric is called Sentiment Annotation Complexity (SAC). As for training data, since any direct judgment of complexity by a human annotator is fraught with subjectivity, we rely on cognitive evidence from eye-tracking. The sentences in our dataset are labeled with SAC scores derived from eye-fixation duration. Using linguistic features and annotated SACs, we train a regressor that predicts the SAC with a best mean error rate of 22.02% for five-fold cross-validation. We also study the correlation between a human annotatorxe2x80x99s perception of complexity and a machinexe2x80x99s confidence in polarity determination. The merit of our work lies in (a) deciding the sentiment annotation cost in, for example, a crowdsourcing setting, (b) choosing the right classifier for sentiment prediction."
I13-2006,Making Headlines in {H}indi: Automatic {E}nglish to {H}indi News Headline Translation,2013,5,1,1,1,17882,aditya joshi,The Companion Volume of the Proceedings of {IJCNLP} 2013: System Demonstrations,0,"News headlines exhibit stylistic peculiarities. The goal of our translation engine xe2x80x98Making Headlines in Hindixe2x80x99 is to achieve automatic translation of English news headlines to Hindi while retaining the Hindi news headline styles. There are two central modules of our engine: the modified translation unit based on Moses and a co-occurrencebased post-processing unit. The modified translation unit provides two machine translation (MT) models: phrase-based and factor-based (both using in-domain data). In addition, a co-occurrence-based post-processing option may be turned on by a user. Our evaluation shows that this engine handles some linguistic phenomena observed in Hindi news headlines."
ar-etal-2012-cost,Cost and Benefit of Using {W}ord{N}et Senses for Sentiment Analysis,2012,7,4,2,0.5,37448,balamurali ar,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Typically, accuracy is used to represent the performance of an NLP system. However, accuracy attainment is a function of investment in annotation. Typically, the more the amount and sophistication of annotation, higher is the accuracy. However, a moot question is ''''''``is the accuracy improvement commensurate with the cost incurred in annotation''''''''? We present an economic model to assess the marginal benefit accruing from increase in cost of annotation. In particular, as a case in point we have chosen the sentiment analysis (SA) problem. In SA, documents normally are polarity classified by running them through classifiers trained on document vectors constructed from lexeme features, i.e., words. If, however, instead of words, one uses word senses (synset ids in wordnets) as features, the accuracy improves dramatically. But is this improvement significant enough to justify the cost of annotation? This question, to the best of our knowledge, has not been investigated with the seriousness it deserves. We perform a cost benefit study based on a vendor-machine model. By setting up a cost price, selling price and profit scenario, we show that although extra cost is incurred in sense annotation, the profit margin is high, justifying the cost."
C12-2008,Cross-Lingual Sentiment Analysis for {I}ndian Languages using Linked {W}ord{N}ets,2012,13,38,2,0.5,37448,balamurali ar,Proceedings of {COLING} 2012: Posters,0,"Cross-Lingual Sentiment Analysis (CLSA) is the task of predicting the polarity of the opinion expressed in a text in a language Ltest using a classifier trained on the corpus of another language Lt rain. Popular approaches use Machine Translation (MT) to convert the test document in Ltest to Lt rain and use the classifier of Lt rain. However, MT systems do not exist for most pairs of languages and even if they do, their translation accuracy is low. So we present an alternative approach to CLSA using WordNet senses as features for supervised sentiment classification. A document in Ltest is tested for polarity through a classifier trained on sense marked and polarity labeled corpora of Lt rain. The crux of the idea is to use the linked WordNets of two languages to bridge the language gap. We report our results on two widely spoken Indian languages, Hindi (450 million speakers) and Marathi (72 million speakers), which do not have an MT system between them. The sense-based approach gives a CLSA accuracy of 72% and 84% for Hindi and Marathi sentiment classification respectively. This is an improvement of 14%-15% over an approach that uses a bilingual dictionary."
W11-1717,Robust Sense-based Sentiment Classification,2011,19,9,2,0,37448,balamurali ar,Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis ({WASSA} 2.011),0,"The new trend in sentiment classification is to use semantic features for representation of documents. We propose a semantic space based on WordNet senses for a supervised document-level sentiment classifier. Not only does this show a better performance for sentiment classification, it also opens opportunities for building a robust sentiment classifier. We examine the possibility of using similarity metrics defined on WordNet to address the problem of not finding a sense in the training corpus. Using three popular similarity metrics, we replace unknown synsets in the test set with a similar synset from the training set. An improvement of 6.2% is seen with respect to baseline using this approach."
P11-4022,{C}-Feel-It: A Sentiment Analyzer for Micro-blogs,2011,9,25,1,1,17882,aditya joshi,Proceedings of the {ACL}-{HLT} 2011 System Demonstrations,0,"Social networking and micro-blogging sites are stores of opinion-bearing content created by human users. We describe C-Feel-It, a system which can tap opinion content in posts (called tweets) from the micro-blogging website, Twitter. This web-based system categorizes tweets pertaining to a search string as positive, negative or objective and gives an aggregate sentiment score that represents a sentiment snapshot for a search string. We present a qualitative evaluation of this system based on a human-annotated tweet corpus."
D11-1100,Harnessing {W}ord{N}et Senses for Supervised Sentiment Classification,2011,19,26,2,0,37448,balamurali ar,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,"Traditional approaches to sentiment classification rely on lexical features, syntax-based features or a combination of the two. We propose semantic features using word senses for a supervised document-level sentiment classifier. To highlight the benefit of sense-based features, we compare word-based representation of documents with a sense-based representation where WordNet senses of the words are used as features. In addition, we highlight the benefit of senses by presenting a part-of-speech-wise effect on sentiment classification. Finally, we show that even if a WSD engine disambiguates between a limited set of words in a document, a sentiment classifier still performs better than what it does in absence of sense annotation. Since word senses used as features show promise, we also examine the possibility of using similarity metrics defined on WordNet to address the problem of not finding a sense in the training corpus. We perform experiments using three popular similarity metrics to mitigate the effect of unknown synsets in a test corpus by replacing them with similar synsets from the training corpus. The results show promising improvement with respect to the baseline."
