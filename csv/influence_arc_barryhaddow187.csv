2009.mtsummit-papers.8,J09-1002,0,0.0894987,"Missing"
2009.mtsummit-papers.8,W00-0507,0,0.122889,"anslation output. A user study validates the types of assistance and provides insight into the human translation process. 1 Introduction While machine translation has made tremendous progress over the last years, this progress has made little inroads into tools for human translators. Although it has become frequent practice in the industry to provide human translators with machine translation output for post-editing, typically no deeper integration of machine translation and human translation is found in translation agencies. An interesting new approach was pioneered by the TransType project (Langlais et al., 2000). The machine translation system makes sentence completion predictions in an interactive machine translation setting. The users may accept them or override them by typing in their own translations, which triggers new suggestions by the tool (Barrachina et al., 2009). But also other information of the machine translation system may be useful for the human translator, such as alternative translations for the input words and phrases. We developed the web-based translation tool Caitra that offers various types of assistance and carried out a study involving ten human translators, whose interaction"
2009.mtsummit-papers.8,macklovitch-2006-transtype2,0,0.305779,"creasing interest in the process studies of translation (Fraser, 1996). Such studies of user activity data focused on key strokes and considered statistics such as revision ratios (Buchweitz and Alves, 2006). Carl et al. (2008) presents a study that also uses eye trackers. StudBarry Haddow School of Informatics University of Edinburgh bhaddow@inf.ed.ac.uk ies may also make use of think aloud protocols (J¨aa¨ skel¨ainen, 2001) in which the translator narrates the thought process behind her actions. The interactive machine translation assistance (that we present in Section 2.1) was evaluated by Macklovitch (2006) with an emphasis on the user experience. Our user study is an extension of this prior work. It extends to different and novel types of assistance. We use a relatively large corpus and a large number of test subjects. 2 Types of Assistance Caitra is implemented as a web-based clientserver architecture, using Ajax Web 2.0 technologies. The machine translation back-end is powered by the Moses decoder. The tool is delivered over the web to allow for easier user studies, but also to expose it to a wider community to gather additional feedback. You can find the tool online at http://www.caitra.org/"
2012.amta-papers.25,P07-2045,0,0.0030871,"nnotator agreement figures we present below suggests that judges were pleased with the conditions offered and worked conscientiously. Restricting judges to a bilingual country appears to be important. We tried removing this condition, and obtained faster turnaround time but much poorerquality results, with weak inter-annotator agreement and many anomalous judgements suggesting that judges lacked fluency in one or the other language or were not taking the job seriously. 3.2 Training Data and SMT Systems The SMT baseline system was a phrase-based system trained with the standard Moses pipeline (Koehn et al., 2007), using GIZA++ (Och and Ney, 2000) for word alignment and SRILM (Stolcke, 2002) for the estimation of 5-gram Kneser-Ney smoothed (Kneser and Ney, 1995) language models. For training the translation and lexicalised reordering models we used the releases of europarl and news-commentary provided for the WMT12 shared task (Callison-Burch et al., 2012), together with a dataset from the ACCEPT project consisting mainly of technical product manuals and marketing materials. This last data set covers the same topics as the forums we wish to translate (so it may be considered as “in-domain”) but it is a"
2012.amta-papers.25,2005.mtsummit-papers.11,0,0.0214548,"ges. In this paper, we will only consider the automatic stages of the translation process in the French-toEnglish translation pair; we wish to translate French forum data for the benefit of English-speaking users. This rapidly exposes a mismatch between training and test data at the level of register. Forum posts are typically informal in tone. The vast majority of available aligned French/English training data is however formal: a typical example, which we will use in the rest of the paper as our primary resource, is the proceedings of the European parliament, the ubiquitous Europarl corpus (Koehn, 2005). Similar problems would have arisen if we had used other corpora, e.g. the UN corpus2 , Callison-Burch’s giga corpus3 or the Canadian Hansard corpus4 . French is a language where the gap between formal and informal usage is large. (For purposes of comparison, it is much larger than in English, though perhaps not as large as in Arabic). We will focus on two immediate problems, verb forms and questions. French, like most European languages (English is the major exception) has two secondperson pronouns, the formal vous and the informal tu (accusative form te, elided to t’ before a vowel). Each p"
2012.amta-papers.25,J04-2003,0,0.0997549,"Missing"
2012.amta-papers.25,P00-1056,0,0.10894,"nt below suggests that judges were pleased with the conditions offered and worked conscientiously. Restricting judges to a bilingual country appears to be important. We tried removing this condition, and obtained faster turnaround time but much poorerquality results, with weak inter-annotator agreement and many anomalous judgements suggesting that judges lacked fluency in one or the other language or were not taking the job seriously. 3.2 Training Data and SMT Systems The SMT baseline system was a phrase-based system trained with the standard Moses pipeline (Koehn et al., 2007), using GIZA++ (Och and Ney, 2000) for word alignment and SRILM (Stolcke, 2002) for the estimation of 5-gram Kneser-Ney smoothed (Kneser and Ney, 1995) language models. For training the translation and lexicalised reordering models we used the releases of europarl and news-commentary provided for the WMT12 shared task (Callison-Burch et al., 2012), together with a dataset from the ACCEPT project consisting mainly of technical product manuals and marketing materials. This last data set covers the same topics as the forums we wish to translate (so it may be considered as “in-domain”) but it is almost exclusively in the formal re"
2012.amta-papers.9,W10-1705,0,0.0209992,"ut back off to the decomposed model for unknown word forms. Interpolated backoff models combine surface and factored translation models, relying more heavily on the surface models for frequent words, and more heavily on the factored models for the rare words. We show that using interpolated backoff improves translation quality, especially of rare nouns and adjectives. 2 Related Work Factored translation models (Koehn and Hoang, 2007) were introduced to overcome data sparsity in morphologically rich languages. Positive results have been reported for languages such as Czech, Turkish, or German (Bojar and Kos, 2010; Yeniterzi and Oflazer, 2010; Koehn et al., 2010). The idea of pooling the evidence of morphologically related words is similar to the automatic clustering of phrases (Kuhn et al., 2010). The popular Arabic–English language pair has received attention in the context of source language morphology reduction. Most work in this area involves splitting off affixes from complex Arabic words that translate into English words of their own (Sadat and Habash, 2006; Popovi´c and Ney, 2004). A concentrated effort on reducing out-ofvocabulary words in Arabic is reported by Habash (2008), which includes th"
2012.amta-papers.9,W11-2103,1,0.923519,"ntrated effort on reducing out-ofvocabulary words in Arabic is reported by Habash (2008), which includes the application of stemming, as we do here. However, in our work, we also address the translation of rare words and use a more complex factored decomposed model for the handling of unknown words. Backoff to stemmed models was explored by Yang and Kirchhoff (2006). Corpus Sentences Europarl News Comm. News Test 11 1,739,154 136,227 3,003 Words English German 48,446,385 45,974,070 3,373,154 3,443,348 75,762 73,726 Table 1: Size of corpora used in experiments. Data from WMT 2011 shared tasks (Callison-Burch et al., 2011). The idea of interpolated backoff stems from language modelling, where it is used in smoothing methods such as Witten-Bell (Witten and Bell, 1991) and Kneser-Ney (Kneser and Ney, 1995). See Chen and Goodman (1998) for an overview. Smoothing methods were previously used by Foster et al. (2006) to discount rare translations, but not in combination with backoff methods. 3 Anatomy of Lexical Sparsity Before we dive into the details of our method, let us first gather some empirical insights into the problem we address. Our work is motivated by overcoming lexical sparsity in corpora of morphologica"
2012.amta-papers.9,P05-1066,1,0.830553,"See Figure 5 for an example of this process. we subtract a fixed number D from each count when deriving probabilities for observed translations e We carry out all our experiments on the German– English language pair, relying on data made available for the 2011 Workshop for Statistical Machine Translation (Callison-Burch et al., 2011). Training data is from European Parliament proceedings and collected news commentaries. The test set consists of a collection of news stories. As is common for this language set, we perform compound splitting (Koehn and Knight, 2003) and syntactic prereordering (Collins et al., 2005). We annotate input words and output words with all three factors (surface, lemma, morphology). This allows us to use 5-gram lemma and 7-gram morphology sequence models to support language modeling. The lexicalized reordering model is based on lemmata, so we can avoid inconsistencies between its use for translations from the joint and decomposed factored translation models. Word alignment is also performed on lemmata instead of surface forms. Phrase length is limited to four words, otherwise default Moses parameters are used. The fullyfactored phrase-based model outperforms a pure surface form"
2012.amta-papers.9,W06-1607,0,0.0248538,"ackoff to stemmed models was explored by Yang and Kirchhoff (2006). Corpus Sentences Europarl News Comm. News Test 11 1,739,154 136,227 3,003 Words English German 48,446,385 45,974,070 3,373,154 3,443,348 75,762 73,726 Table 1: Size of corpora used in experiments. Data from WMT 2011 shared tasks (Callison-Burch et al., 2011). The idea of interpolated backoff stems from language modelling, where it is used in smoothing methods such as Witten-Bell (Witten and Bell, 1991) and Kneser-Ney (Kneser and Ney, 1995). See Chen and Goodman (1998) for an overview. Smoothing methods were previously used by Foster et al. (2006) to discount rare translations, but not in combination with backoff methods. 3 Anatomy of Lexical Sparsity Before we dive into the details of our method, let us first gather some empirical insights into the problem we address. Our work is motivated by overcoming lexical sparsity in corpora of morphologically rich languages. But how big is the portion of rare words in the test set and do we translate them significantly worse? We examined these questions on the German–English language pair, given the News Commentary and Europarl training corpora and the WMT 2011 test set (corpus sizes are given"
2012.amta-papers.9,P08-2015,0,0.0373354,", or German (Bojar and Kos, 2010; Yeniterzi and Oflazer, 2010; Koehn et al., 2010). The idea of pooling the evidence of morphologically related words is similar to the automatic clustering of phrases (Kuhn et al., 2010). The popular Arabic–English language pair has received attention in the context of source language morphology reduction. Most work in this area involves splitting off affixes from complex Arabic words that translate into English words of their own (Sadat and Habash, 2006; Popovi´c and Ney, 2004). A concentrated effort on reducing out-ofvocabulary words in Arabic is reported by Habash (2008), which includes the application of stemming, as we do here. However, in our work, we also address the translation of rare words and use a more complex factored decomposed model for the handling of unknown words. Backoff to stemmed models was explored by Yang and Kirchhoff (2006). Corpus Sentences Europarl News Comm. News Test 11 1,739,154 136,227 3,003 Words English German 48,446,385 45,974,070 3,373,154 3,443,348 75,762 73,726 Table 1: Size of corpora used in experiments. Data from WMT 2011 shared tasks (Callison-Burch et al., 2011). The idea of interpolated backoff stems from language model"
2012.amta-papers.9,W11-2145,0,0.0435195,"Missing"
2012.amta-papers.9,D11-1125,0,0.0692783,"Missing"
2012.amta-papers.9,W10-1715,1,0.823527,"rd forms. Interpolated backoff models combine surface and factored translation models, relying more heavily on the surface models for frequent words, and more heavily on the factored models for the rare words. We show that using interpolated backoff improves translation quality, especially of rare nouns and adjectives. 2 Related Work Factored translation models (Koehn and Hoang, 2007) were introduced to overcome data sparsity in morphologically rich languages. Positive results have been reported for languages such as Czech, Turkish, or German (Bojar and Kos, 2010; Yeniterzi and Oflazer, 2010; Koehn et al., 2010). The idea of pooling the evidence of morphologically related words is similar to the automatic clustering of phrases (Kuhn et al., 2010). The popular Arabic–English language pair has received attention in the context of source language morphology reduction. Most work in this area involves splitting off affixes from complex Arabic words that translate into English words of their own (Sadat and Habash, 2006; Popovi´c and Ney, 2004). A concentrated effort on reducing out-ofvocabulary words in Arabic is reported by Habash (2008), which includes the application of stemming, as we do here. However,"
2012.amta-papers.9,D07-1091,1,0.966644,"BLEU (German–English) over phrase-based models, due to the better translation of rare nouns and adjectives. 1 Introduction Morphologically rich languages pose a special challenge to statistical machine translation. One aspect of the problem is the generative process yielding many surface forms from a single lemma, causing sparse data problems in model estimation, affecting both the translation model and the language model. Another aspect is the prediction of the correct morphological features which may require larger syntactic or even semantic context to resolve. Factored translation models (Koehn and Hoang, 2007) were proposed as a formalism to address these challenges. This modeling framework allows for arbitrary decomposition and enrichment of phrasebased translation models. For morphologically rich languages, one application of this framework is the decomposition of phrase translation into two translation steps, one for lemmata and one for morphological properties, and a generation step to produce the target surface form. While such factored translation models increase robustness by basing statistics on the more frequent lemmata instead of the sparser surface forms, they do make strong independence"
2012.amta-papers.9,P07-2045,1,0.0174468,"thods. 3 Anatomy of Lexical Sparsity Before we dive into the details of our method, let us first gather some empirical insights into the problem we address. Our work is motivated by overcoming lexical sparsity in corpora of morphologically rich languages. But how big is the portion of rare words in the test set and do we translate them significantly worse? We examined these questions on the German–English language pair, given the News Commentary and Europarl training corpora and the WMT 2011 test set (corpus sizes are given in Table 1). We trained a phrase-based translation model using Moses (Koehn et al., 2007) with mostly default parameters (for more details, please check the experimental section). 3.1 Computation of Source Word Translation Precision The question, if a (potentially rare) input word has been translated correctly, does unfortunately not have a straight-forward answer: while target words can be compared against a reference translation, source words need to first tracked to their target word translations (if any), which then in turn can be compared against a reference. We proceed as follows (see Figure 1). We record the word alignment within the phrase mappings, to closely track which"
2012.amta-papers.9,E03-1076,1,0.802171,"he surface generation probability is almost always 1. See Figure 5 for an example of this process. we subtract a fixed number D from each count when deriving probabilities for observed translations e We carry out all our experiments on the German– English language pair, relying on data made available for the 2011 Workshop for Statistical Machine Translation (Callison-Burch et al., 2011). Training data is from European Parliament proceedings and collected news commentaries. The test set consists of a collection of news stories. As is common for this language set, we perform compound splitting (Koehn and Knight, 2003) and syntactic prereordering (Collins et al., 2005). We annotate input words and output words with all three factors (surface, lemma, morphology). This allows us to use 5-gram lemma and 7-gram morphology sequence models to support language modeling. The lexicalized reordering model is based on lemmata, so we can avoid inconsistencies between its use for translations from the joint and decomposed factored translation models. Word alignment is also performed on lemmata instead of surface forms. Phrase length is limited to four words, otherwise default Moses parameters are used. The fullyfactored"
2012.amta-papers.9,N03-1017,1,0.0484067,"ords increases, but not at the same rate as the corpus increase. There are still significant number of rare nouns left — roughly a third occur less than 32 times. It is worthwhile to point out that nouns carry a substantial amount of meaning and their mistranslation is typically more serious than a dropped determiner or punctuation token. Translating them well is important. INPUT OUTPUT word word lemma lemma morphology morphology Figure 3: Factored translation model: Phrase translation is decomposed into a number of mapping steps. 4 Method Our method involves a traditional phrase-based model (Koehn et al., 2003) and a factored translation model (Koehn and Hoang, 2007). The traditional phrase based model is estimated using statistics on phrase mappings found in an automatically word-aligned parallel corpus. 4.1 Decomposed Factored Model The factored translation model decomposes the translation of a phrase into a number of mapping steps. See Figure 3 for an illustration. The decomposition involves two translation steps (between lemmata and between morphologically features) and two generation steps (from lemma to morphologically features and for the generation of the surface from both). Formally, we int"
2012.amta-papers.9,C10-1069,0,0.0490081,"Missing"
2012.amta-papers.9,popovic-ney-2004-towards,0,0.196254,"Missing"
2012.amta-papers.9,P06-1001,0,0.0284577,"overcome data sparsity in morphologically rich languages. Positive results have been reported for languages such as Czech, Turkish, or German (Bojar and Kos, 2010; Yeniterzi and Oflazer, 2010; Koehn et al., 2010). The idea of pooling the evidence of morphologically related words is similar to the automatic clustering of phrases (Kuhn et al., 2010). The popular Arabic–English language pair has received attention in the context of source language morphology reduction. Most work in this area involves splitting off affixes from complex Arabic words that translate into English words of their own (Sadat and Habash, 2006; Popovi´c and Ney, 2004). A concentrated effort on reducing out-ofvocabulary words in Arabic is reported by Habash (2008), which includes the application of stemming, as we do here. However, in our work, we also address the translation of rare words and use a more complex factored decomposed model for the handling of unknown words. Backoff to stemmed models was explored by Yang and Kirchhoff (2006). Corpus Sentences Europarl News Comm. News Test 11 1,739,154 136,227 3,003 Words English German 48,446,385 45,974,070 3,373,154 3,443,348 75,762 73,726 Table 1: Size of corpora used in experiments."
2012.amta-papers.9,C00-2105,0,0.0392933,"Missing"
2012.amta-papers.9,E06-1006,0,0.0157478,"ceived attention in the context of source language morphology reduction. Most work in this area involves splitting off affixes from complex Arabic words that translate into English words of their own (Sadat and Habash, 2006; Popovi´c and Ney, 2004). A concentrated effort on reducing out-ofvocabulary words in Arabic is reported by Habash (2008), which includes the application of stemming, as we do here. However, in our work, we also address the translation of rare words and use a more complex factored decomposed model for the handling of unknown words. Backoff to stemmed models was explored by Yang and Kirchhoff (2006). Corpus Sentences Europarl News Comm. News Test 11 1,739,154 136,227 3,003 Words English German 48,446,385 45,974,070 3,373,154 3,443,348 75,762 73,726 Table 1: Size of corpora used in experiments. Data from WMT 2011 shared tasks (Callison-Burch et al., 2011). The idea of interpolated backoff stems from language modelling, where it is used in smoothing methods such as Witten-Bell (Witten and Bell, 1991) and Kneser-Ney (Kneser and Ney, 1995). See Chen and Goodman (1998) for an overview. Smoothing methods were previously used by Foster et al. (2006) to discount rare translations, but not in com"
2012.amta-papers.9,P10-1047,0,0.0135194,"composed model for unknown word forms. Interpolated backoff models combine surface and factored translation models, relying more heavily on the surface models for frequent words, and more heavily on the factored models for the rare words. We show that using interpolated backoff improves translation quality, especially of rare nouns and adjectives. 2 Related Work Factored translation models (Koehn and Hoang, 2007) were introduced to overcome data sparsity in morphologically rich languages. Positive results have been reported for languages such as Czech, Turkish, or German (Bojar and Kos, 2010; Yeniterzi and Oflazer, 2010; Koehn et al., 2010). The idea of pooling the evidence of morphologically related words is similar to the automatic clustering of phrases (Kuhn et al., 2010). The popular Arabic–English language pair has received attention in the context of source language morphology reduction. Most work in this area involves splitting off affixes from complex Arabic words that translate into English words of their own (Sadat and Habash, 2006; Popovi´c and Ney, 2004). A concentrated effort on reducing out-ofvocabulary words in Arabic is reported by Habash (2008), which includes the application of stemming, as"
2012.iwslt-evaluation.4,2012.iwslt-evaluation.1,0,0.0377376,"Abstract 2.1. Acoustic modelling This paper describes the University of Edinburgh (UEDIN) systems for the IWSLT 2012 Evaluation. We participated in the ASR (English), MT (English-French, German-English) and SLT (English-French) tracks. 1. Introduction We report on experiments carried out for the development of automatic speech recognition (ASR), machine translation (MT) and spoken language translation (SLT) systems on the datasets of the International Workshop on Spoken Language Translation (IWSLT) 2012. Details about the evaluation campaign and the different evaluation tracks can be found in [1]. For the ASR track, we focused on the use of adaptive tandem features derived from deep neural networks, trained on both in-domain data from TED talks [2], and out-of-domain data from a corpus of meetings. Our experiments for the MT track compare approaches to data ﬁltering and phrase table adaptation and focus on adaptation by adding sparse lexicalised features. We explore different tuning setups on in-domain and mixed-domain systems. For the SLT track, we carried out experiments with a punctuation insertion system as an intermediate step between speech recognition and machine translation, f"
2012.iwslt-evaluation.4,2012.eamt-1.60,0,0.0411397,"(English), MT (English-French, German-English) and SLT (English-French) tracks. 1. Introduction We report on experiments carried out for the development of automatic speech recognition (ASR), machine translation (MT) and spoken language translation (SLT) systems on the datasets of the International Workshop on Spoken Language Translation (IWSLT) 2012. Details about the evaluation campaign and the different evaluation tracks can be found in [1]. For the ASR track, we focused on the use of adaptive tandem features derived from deep neural networks, trained on both in-domain data from TED talks [2], and out-of-domain data from a corpus of meetings. Our experiments for the MT track compare approaches to data ﬁltering and phrase table adaptation and focus on adaptation by adding sparse lexicalised features. We explore different tuning setups on in-domain and mixed-domain systems. For the SLT track, we carried out experiments with a punctuation insertion system as an intermediate step between speech recognition and machine translation, focussing on pre- and post-processing steps and comparing different tuning sets. 2. Automatic Speech Recognition (ASR) In this section we describe the 2012"
2012.iwslt-evaluation.4,P05-1066,1,0.826144,"Missing"
2012.iwslt-evaluation.4,2012.iwslt-papers.17,1,0.866553,"Missing"
2012.iwslt-evaluation.4,D11-1033,0,0.0820468,"Missing"
2012.iwslt-evaluation.4,W12-3154,1,0.896939,"Missing"
2012.iwslt-evaluation.4,2011.iwslt-evaluation.14,0,0.0925714,"Missing"
2012.iwslt-evaluation.4,E03-1076,1,0.840572,"Missing"
2012.iwslt-evaluation.4,P07-2045,1,\N,Missing
2012.iwslt-evaluation.4,W10-1711,0,\N,Missing
2012.iwslt-evaluation.4,2010.iwslt-evaluation.22,0,\N,Missing
2012.iwslt-evaluation.4,2011.iwslt-evaluation.18,0,\N,Missing
2012.iwslt-papers.17,D10-1044,0,0.0509215,"Missing"
2012.iwslt-papers.17,P07-2045,1,0.0231833,"Missing"
2012.iwslt-papers.17,D09-1074,0,0.171381,"Missing"
2012.iwslt-papers.17,D09-1022,0,0.0663516,"Missing"
2012.iwslt-papers.17,P03-1021,0,0.052503,"Missing"
2012.iwslt-papers.17,P02-1040,0,0.101407,"Missing"
2012.iwslt-papers.17,P12-1002,0,0.0240912,"Missing"
2012.iwslt-papers.17,P12-1048,0,0.0682546,"Missing"
2012.iwslt-papers.17,D07-1080,0,0.0207495,"Missing"
2012.iwslt-papers.17,eisele-chen-2010-multiun,0,\N,Missing
2012.iwslt-papers.17,D11-1033,0,\N,Missing
2012.iwslt-papers.17,P11-2080,0,\N,Missing
2012.iwslt-papers.17,N09-1025,0,\N,Missing
2012.iwslt-papers.17,P12-2023,0,\N,Missing
2012.iwslt-papers.17,W07-0717,0,\N,Missing
2012.iwslt-papers.17,2012.eamt-1.60,0,\N,Missing
2014.amta-researchers.11,D11-1033,0,0.0494963,"Overlap) with their gain over the respective baseline (bottom of each block). The best system on the mixed test set is marked in bold.. **: p ≤ 0.01, *: p ≤ 0.05 mark significantly better scores compared to the respective baseline. is equal or better than the best model in Table 4, the performance on T ED falls short of that model by ∼0.6 B LEU. This is likely due to the fact that adding Europarl data is particularly harmful for translating T ED documents. Therefore, in future work we will look at combining the adaptation approaches studied here with data selection methods such as the work of Axelrod et al. (2011). 7.4 Qualitative evaluation In this section, we analyse some concrete output examples that visualise the differences in the translations produced by the different models for training condition 1. Figure 2 shows two input and reference sentences with their translations under the unadapted baseline, the domainadapted model and the model with both domain-adapted and topic-adapted features9 . In the first example, the baseline system does not translate the source verb remontent appropriately. This is fixed by the domain-adapted model and in addition, the topic-adapted model finds a contextually b"
2014.amta-researchers.11,W11-1014,0,0.0591169,"Missing"
2014.amta-researchers.11,2010.amta-papers.16,0,0.111511,"n and language models (Foster and Kuhn, 2007; Matsoukas et al., 2009; Foster et al., 2010; Sennrich, 2012) to specific target domains that are usually known in advance, for example the news domain. An extension of the standard domain adaptation task is multi-domain adaptation where a translation system is adapted to several known target domains (see for example Cui et al. (2013)). In cases where the target domains are not assumed to be known, dedicated domain classifiers can be trained and used to automatically predict the target domain and choose an appropriate model based on the prediction (Banerjee et al., 2010). Recently, there has been increased work on the application of topic modelling to translation model adaptation (Gong et al., 2011; Su et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Hasler et al., 2014a) in an attempt to move away from the notion of a domain as the source of a corpus. Topic adaptation techniques build on the assumption that the origin of sentences and documents is unknown at test time, and the unsupervised nature of topic models is useful for detecting structure across corpus boundaries in training sets to adapt to diverse test sets. While domain adaptation t"
2014.amta-researchers.11,2012.eamt-1.60,0,0.061737,"Missing"
2014.amta-researchers.11,D13-1107,0,0.0134723,"e stronger of two training conditions. 1 Introduction Domain adaptation is a very active area of research in statistical machine translation (SMT) and there is a large and growing body of work on different techniques to adapt translation and language models (Foster and Kuhn, 2007; Matsoukas et al., 2009; Foster et al., 2010; Sennrich, 2012) to specific target domains that are usually known in advance, for example the news domain. An extension of the standard domain adaptation task is multi-domain adaptation where a translation system is adapted to several known target domains (see for example Cui et al. (2013)). In cases where the target domains are not assumed to be known, dedicated domain classifiers can be trained and used to automatically predict the target domain and choose an appropriate model based on the prediction (Banerjee et al., 2010). Recently, there has been increased work on the application of topic modelling to translation model adaptation (Gong et al., 2011; Su et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Hasler et al., 2014a) in an attempt to move away from the notion of a domain as the source of a corpus. Topic adaptation techniques build on the assumption tha"
2014.amta-researchers.11,P12-2023,0,0.0699077,"Missing"
2014.amta-researchers.11,D10-1044,0,0.0607935,"and topic adaptation approaches can be beneficial and that topic representations can be used to predict the domain of a test document. Our best combined model yields gains of up to 0.82 B LEU over a domain-adapted translation system and up to 1.67 B LEU over an unadapted system, measured on the stronger of two training conditions. 1 Introduction Domain adaptation is a very active area of research in statistical machine translation (SMT) and there is a large and growing body of work on different techniques to adapt translation and language models (Foster and Kuhn, 2007; Matsoukas et al., 2009; Foster et al., 2010; Sennrich, 2012) to specific target domains that are usually known in advance, for example the news domain. An extension of the standard domain adaptation task is multi-domain adaptation where a translation system is adapted to several known target domains (see for example Cui et al. (2013)). In cases where the target domains are not assumed to be known, dedicated domain classifiers can be trained and used to automatically predict the target domain and choose an appropriate model based on the prediction (Banerjee et al., 2010). Recently, there has been increased work on the application of top"
2014.amta-researchers.11,W07-0717,0,0.210003,"ata. We show empirically that combining domain and topic adaptation approaches can be beneficial and that topic representations can be used to predict the domain of a test document. Our best combined model yields gains of up to 0.82 B LEU over a domain-adapted translation system and up to 1.67 B LEU over an unadapted system, measured on the stronger of two training conditions. 1 Introduction Domain adaptation is a very active area of research in statistical machine translation (SMT) and there is a large and growing body of work on different techniques to adapt translation and language models (Foster and Kuhn, 2007; Matsoukas et al., 2009; Foster et al., 2010; Sennrich, 2012) to specific target domains that are usually known in advance, for example the news domain. An extension of the standard domain adaptation task is multi-domain adaptation where a translation system is adapted to several known target domains (see for example Cui et al. (2013)). In cases where the target domains are not assumed to be known, dedicated domain classifiers can be trained and used to automatically predict the target domain and choose an appropriate model based on the prediction (Banerjee et al., 2010). Recently, there has"
2014.amta-researchers.11,D11-1084,0,0.0243366,"hat are usually known in advance, for example the news domain. An extension of the standard domain adaptation task is multi-domain adaptation where a translation system is adapted to several known target domains (see for example Cui et al. (2013)). In cases where the target domains are not assumed to be known, dedicated domain classifiers can be trained and used to automatically predict the target domain and choose an appropriate model based on the prediction (Banerjee et al., 2010). Recently, there has been increased work on the application of topic modelling to translation model adaptation (Gong et al., 2011; Su et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Hasler et al., 2014a) in an attempt to move away from the notion of a domain as the source of a corpus. Topic adaptation techniques build on the assumption that the origin of sentences and documents is unknown at test time, and the unsupervised nature of topic models is useful for detecting structure across corpus boundaries in training sets to adapt to diverse test sets. While domain adaptation techniques rely on a given, hard clustering of the data, topic adaptation aims to induce a soft clustering that is more suited to t"
2014.amta-researchers.11,E14-1035,1,0.831418,"dard domain adaptation task is multi-domain adaptation where a translation system is adapted to several known target domains (see for example Cui et al. (2013)). In cases where the target domains are not assumed to be known, dedicated domain classifiers can be trained and used to automatically predict the target domain and choose an appropriate model based on the prediction (Banerjee et al., 2010). Recently, there has been increased work on the application of topic modelling to translation model adaptation (Gong et al., 2011; Su et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Hasler et al., 2014a) in an attempt to move away from the notion of a domain as the source of a corpus. Topic adaptation techniques build on the assumption that the origin of sentences and documents is unknown at test time, and the unsupervised nature of topic models is useful for detecting structure across corpus boundaries in training sets to adapt to diverse test sets. While domain adaptation techniques rely on a given, hard clustering of the data, topic adaptation aims to induce a soft clustering that is more suited to the task. While topic models are very useful for detecting and grouping the semantic diffe"
2014.amta-researchers.11,W14-3358,1,0.942461,"dard domain adaptation task is multi-domain adaptation where a translation system is adapted to several known target domains (see for example Cui et al. (2013)). In cases where the target domains are not assumed to be known, dedicated domain classifiers can be trained and used to automatically predict the target domain and choose an appropriate model based on the prediction (Banerjee et al., 2010). Recently, there has been increased work on the application of topic modelling to translation model adaptation (Gong et al., 2011; Su et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Hasler et al., 2014a) in an attempt to move away from the notion of a domain as the source of a corpus. Topic adaptation techniques build on the assumption that the origin of sentences and documents is unknown at test time, and the unsupervised nature of topic models is useful for detecting structure across corpus boundaries in training sets to adapt to diverse test sets. While domain adaptation techniques rely on a given, hard clustering of the data, topic adaptation aims to induce a soft clustering that is more suited to the task. While topic models are very useful for detecting and grouping the semantic diffe"
2014.amta-researchers.11,P13-2122,0,0.0778447,"in. An extension of the standard domain adaptation task is multi-domain adaptation where a translation system is adapted to several known target domains (see for example Cui et al. (2013)). In cases where the target domains are not assumed to be known, dedicated domain classifiers can be trained and used to automatically predict the target domain and choose an appropriate model based on the prediction (Banerjee et al., 2010). Recently, there has been increased work on the application of topic modelling to translation model adaptation (Gong et al., 2011; Su et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Hasler et al., 2014a) in an attempt to move away from the notion of a domain as the source of a corpus. Topic adaptation techniques build on the assumption that the origin of sentences and documents is unknown at test time, and the unsupervised nature of topic models is useful for detecting structure across corpus boundaries in training sets to adapt to diverse test sets. While domain adaptation techniques rely on a given, hard clustering of the data, topic adaptation aims to induce a soft clustering that is more suited to the task. While topic models are very useful for detecting and groupi"
2014.amta-researchers.11,D11-1125,0,0.0338854,"perimental setup All of the test corpora contain document boundaries which allows us to consider document context during translation and switch translation and language models at document boundaries. While the domain-adapted baselines use gold domain labels, we use automatically predicted domains when combining domain-adapted and topic-adapted models7 . We use a tuning set containing data from all three test domains and tune a single set of feature weights for all portions of the test set. Translation quality is evaluated using the average feature weights of three optimisation runs with P RO (Hopkins and May, 2011). We use the mteval-v13a.pl script to compute case-insensitive B LEU scores and use bootstrap resampling (Koehn, 2004) to measure significance of the B LEU scores on the mixed test set. 6 This 7 Note trend was observed by Banchs and Costa-juss`a (2011) for vectors derived from Latent Semantic Indexing. that topic adaptation does not rely on domain labels. Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 144 6.1 Unadapted baseline system Our baseline is a phrase-based French-English system trained on the concatenation of all parallel data f"
2014.amta-researchers.11,W04-3250,1,0.621116,"tion and switch translation and language models at document boundaries. While the domain-adapted baselines use gold domain labels, we use automatically predicted domains when combining domain-adapted and topic-adapted models7 . We use a tuning set containing data from all three test domains and tune a single set of feature weights for all portions of the test set. Translation quality is evaluated using the average feature weights of three optimisation runs with P RO (Hopkins and May, 2011). We use the mteval-v13a.pl script to compute case-insensitive B LEU scores and use bootstrap resampling (Koehn, 2004) to measure significance of the B LEU scores on the mixed test set. 6 This 7 Note trend was observed by Banchs and Costa-juss`a (2011) for vectors derived from Latent Semantic Indexing. that topic adaptation does not rely on domain labels. Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 144 6.1 Unadapted baseline system Our baseline is a phrase-based French-English system trained on the concatenation of all parallel data for condition 1 and 2, respectively. It was built with the Moses toolkit (Koehn et al., 2007) using the 14 standard cor"
2014.amta-researchers.11,D09-1074,0,0.0770157,"y that combining domain and topic adaptation approaches can be beneficial and that topic representations can be used to predict the domain of a test document. Our best combined model yields gains of up to 0.82 B LEU over a domain-adapted translation system and up to 1.67 B LEU over an unadapted system, measured on the stronger of two training conditions. 1 Introduction Domain adaptation is a very active area of research in statistical machine translation (SMT) and there is a large and growing body of work on different techniques to adapt translation and language models (Foster and Kuhn, 2007; Matsoukas et al., 2009; Foster et al., 2010; Sennrich, 2012) to specific target domains that are usually known in advance, for example the news domain. An extension of the standard domain adaptation task is multi-domain adaptation where a translation system is adapted to several known target domains (see for example Cui et al. (2013)). In cases where the target domains are not assumed to be known, dedicated domain classifiers can be trained and used to automatically predict the target domain and choose an appropriate model based on the prediction (Banerjee et al., 2010). Recently, there has been increased work on t"
2014.amta-researchers.11,2012.iwslt-papers.14,0,0.0137319,"c mixture for the test context. The second feature, Joint-conditional, estimates the joint probability of a target phrase and a test context given a source phrase. It is factorised as the (baseline) probability of a target phrase given a source phrase and the probability of the test context given the source and target phrase. The latter is approximated by the probility of the test context topic mixture given the phrase pair topic mixture, which is further approximated by the cosine similarity between the two topic mixtures. The Target-unigrams feature is inspired by the lazy MDI adaptation of Ruiz and Federico (2012) and measures the probability ratio of a word under the document topic mixture versus under the baseline model1 . We include an additional term to measure the topical relevance of a word by comparing against its probability under the asymmetric topic 0 of the PPT model2 . Sim-phrasePair measures the cosine similarity of a phrase pair topic vector and the topic vector of a test context. Sim-targetPhrase is similar but uses an average topic vector over all phrase pairs with the same target phrase. Sim-targetWord instead replaces the phrase pair topic vector with the word topic vector of the word"
2014.amta-researchers.11,E12-1055,0,0.197487,"approaches can be beneficial and that topic representations can be used to predict the domain of a test document. Our best combined model yields gains of up to 0.82 B LEU over a domain-adapted translation system and up to 1.67 B LEU over an unadapted system, measured on the stronger of two training conditions. 1 Introduction Domain adaptation is a very active area of research in statistical machine translation (SMT) and there is a large and growing body of work on different techniques to adapt translation and language models (Foster and Kuhn, 2007; Matsoukas et al., 2009; Foster et al., 2010; Sennrich, 2012) to specific target domains that are usually known in advance, for example the news domain. An extension of the standard domain adaptation task is multi-domain adaptation where a translation system is adapted to several known target domains (see for example Cui et al. (2013)). In cases where the target domains are not assumed to be known, dedicated domain classifiers can be trained and used to automatically predict the target domain and choose an appropriate model based on the prediction (Banerjee et al., 2010). Recently, there has been increased work on the application of topic modelling to t"
2014.amta-researchers.11,P13-1082,0,0.0149742,"hers in recent years. Xu et al. (2007) tune domain-specific features weights and build domain-specific language models. They use the perplexity of in-domain language models to classify test documents and select the appropriate weights and models per document. Banerjee et al. (2010) train domain-specific translation models and use SVMs to detect the domain of an input sentence to route it to a domain-specific model. Wang et al. (2012) follow a slightly different approach by re-using the same translation model for all domains and tuning domain-specific features weights with modified objectives. Sennrich et al. (2013) adapt the four standard translation model features to unsupervised clusters of the development data obtained by k-means clustering. Another line of research aims to improve topic modelling by encoding domain information via a Dirichlet Forest Prior (Andrzejewski et al., 2009). By specifying Must-Link and CannotLink relations between words, topic modelling is guided to either separate words into different topics or merge them into the same topic. While the idea of combining domain and topic adaptation within the same model is appealing, the model requires manually constructed lists of words an"
2014.amta-researchers.11,P12-1048,0,0.01972,"wn in advance, for example the news domain. An extension of the standard domain adaptation task is multi-domain adaptation where a translation system is adapted to several known target domains (see for example Cui et al. (2013)). In cases where the target domains are not assumed to be known, dedicated domain classifiers can be trained and used to automatically predict the target domain and choose an appropriate model based on the prediction (Banerjee et al., 2010). Recently, there has been increased work on the application of topic modelling to translation model adaptation (Gong et al., 2011; Su et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Hasler et al., 2014a) in an attempt to move away from the notion of a domain as the source of a corpus. Topic adaptation techniques build on the assumption that the origin of sentences and documents is unknown at test time, and the unsupervised nature of topic models is useful for detecting structure across corpus boundaries in training sets to adapt to diverse test sets. While domain adaptation techniques rely on a given, hard clustering of the data, topic adaptation aims to induce a soft clustering that is more suited to the task. While to"
2014.amta-researchers.11,2012.amta-papers.18,0,0.0131325,"thors 139 efficient architecture that combines online and offline computation. 2 Related work Domain classification for multi-domain adaptation has been the focus of several researchers in recent years. Xu et al. (2007) tune domain-specific features weights and build domain-specific language models. They use the perplexity of in-domain language models to classify test documents and select the appropriate weights and models per document. Banerjee et al. (2010) train domain-specific translation models and use SVMs to detect the domain of an input sentence to route it to a domain-specific model. Wang et al. (2012) follow a slightly different approach by re-using the same translation model for all domains and tuning domain-specific features weights with modified objectives. Sennrich et al. (2013) adapt the four standard translation model features to unsupervised clusters of the development data obtained by k-means clustering. Another line of research aims to improve topic modelling by encoding domain information via a Dirichlet Forest Prior (Andrzejewski et al., 2009). By specifying Must-Link and CannotLink relations between words, topic modelling is guided to either separate words into different topics"
2014.amta-researchers.11,2007.mtsummit-papers.68,0,0.0319857,"ting to topics. By predicting the domain label of test documents, we can combine both approaches to translate unlabelled documents from different genres and topics. We show that domain and topic adaptation can be complementary and that finding the right balance between the two could lead to a more Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 139 efficient architecture that combines online and offline computation. 2 Related work Domain classification for multi-domain adaptation has been the focus of several researchers in recent years. Xu et al. (2007) tune domain-specific features weights and build domain-specific language models. They use the perplexity of in-domain language models to classify test documents and select the appropriate weights and models per document. Banerjee et al. (2010) train domain-specific translation models and use SVMs to detect the domain of an input sentence to route it to a domain-specific model. Wang et al. (2012) follow a slightly different approach by re-using the same translation model for all domains and tuning domain-specific features weights with modified objectives. Sennrich et al. (2013) adapt the four"
2014.amta-researchers.11,W13-2201,1,\N,Missing
2015.mtsummit-papers.19,D11-1033,0,0.07366,"Missing"
2015.mtsummit-papers.19,2010.amta-papers.16,0,0.133002,"Missing"
2015.mtsummit-papers.19,J96-1002,0,0.0774689,"f reserving the in-domain devel2 http://www.statmt.org/wmt15/translation-task.html Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 243 opment sets for tuning source LM interpolation weights. Furthermore, we argue that source-side scores of the interpolated LMs employed in our classifier may to some extent resemble targetside scores of the interpolated LMs which are applied in the respective domain-adapted SMT systems. 5.2 Maximum Entropy Classifiers Maximum entropy text classifiers can utilize a larger number of features in order to predict the label (Berger et al., 1996). We incorporate features from single words, pairs of adjacent words, the first word of the sentence, and the last word of the sentence. The model is trained with L-BFGS (Byrd et al., 1995) and regularized using a Gaussian prior. We build maximum entropy (ME) classifiers under two different training conditions: using the MT development sets (which are rather small) as training data, and using selected other corpora as training data (which might not always exactly match what is defined as in-domain to the MT systems, as the development sets essentially constitute the domains). In a further flav"
2015.mtsummit-papers.19,W09-0432,0,0.0237446,"ch and Ney, 2002) on an in-domain development set (Pecina et al., 2012). • Model combination (of language models, translation models, or reordering models) via interpolation or other schemes, e.g. phrase table fill-up (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Nakov, 2008; Bisazza et al., 2011; Niehues and Waibel, 2012; Chen et al., 2013). • Data selection (Moore and Lewis, 2010; Axelrod et al., 2011). • Instance weighting (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012). • Further exploitation of in-domain monolingual data (Ueffing et al., 2007; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009; Lambert et al., 2011). • Domain-specific features, e.g. binary features indicating the provenance of phrase pairs as implemented in the open-source Moses toolkit (Durrani et al., 2013b) or “domain augmentation” (Clark et al., 2012). However, few authors have tackled the question of how to benefit from domain adaptation in scenarios where a domain label of the input is not present. An important aspect of our approach to multi-domain MT is the need for domain classification. Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 2"
2015.mtsummit-papers.19,2011.iwslt-evaluation.18,0,0.0660573,"d English→Greek language pairs using training corpora of diverse origin, totalling tens of millions of parallel sentences. 2 Related Work A significant amount of research on domain adaptation for SMT has been conducted in recent years. Some methods which are commonly used are: • Tuning of the decoder model weights (Och and Ney, 2002) on an in-domain development set (Pecina et al., 2012). • Model combination (of language models, translation models, or reordering models) via interpolation or other schemes, e.g. phrase table fill-up (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Nakov, 2008; Bisazza et al., 2011; Niehues and Waibel, 2012; Chen et al., 2013). • Data selection (Moore and Lewis, 2010; Axelrod et al., 2011). • Instance weighting (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012). • Further exploitation of in-domain monolingual data (Ueffing et al., 2007; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009; Lambert et al., 2011). • Domain-specific features, e.g. binary features indicating the provenance of phrase pairs as implemented in the open-source Moses toolkit (Durrani et al., 2013b) or “domain augmentation” (Clark et al., 2012). However, fe"
2015.mtsummit-papers.19,2012.eamt-1.60,0,0.144845,"Missing"
2015.mtsummit-papers.19,2013.iwslt-evaluation.1,0,0.0173438,"e than any of the domain-adapted systems on two out of four language pairs (En→De: +0.3; En→It: -0.2; En→Pt: +0.9; En→El: -0.2). Apart from English→Portuguese, the differences are small. Multi-domain SMT clearly outperforms mixed-domain SMT for English→Portuguese (up to +1.0 on all) and English→Greek (up to +0.6 on all). The choice of the domain classifier barely matters wrt. translation quality. Due to its compact model, the MEdev classifier would for instance be a reasonable choice despite not providing the highest classification accuracy. 8 MT English→Italian: +4.3 points B LEU on tst2013 (Cettolo et al., 2013). MT English→Portuguese: +2.8 points B LEU on tst2014 (Cettolo et al., 2014). 9 http://matrix.statmt.org Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 249 20 15 15 10 10 5 5 German 26 .3 26 BLEU 21 .9 21 .7 21 .9 22 .7 22 .5 22 .5 25 20 0 .2 26 .8 26 .8 8 30 28 . .1 .0 27 .2 27 .2 27 .0 27 .7 27 25 BLEU 25 .6 30 25 30 6 35 32 35 .6 32 . 40 .0 40 .8 Mixed-domain-tuned Multi-domain Oracle-domain 31 TED-tuned Europarl-tuned News-tuned Italian 0 Portuguese Greek Figure 1: B LEU scores on a concatenation of all test sets. Compared to oracle-domain SMT, wh"
2015.mtsummit-papers.19,N13-1114,0,0.0120964,"rpora of diverse origin, totalling tens of millions of parallel sentences. 2 Related Work A significant amount of research on domain adaptation for SMT has been conducted in recent years. Some methods which are commonly used are: • Tuning of the decoder model weights (Och and Ney, 2002) on an in-domain development set (Pecina et al., 2012). • Model combination (of language models, translation models, or reordering models) via interpolation or other schemes, e.g. phrase table fill-up (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Nakov, 2008; Bisazza et al., 2011; Niehues and Waibel, 2012; Chen et al., 2013). • Data selection (Moore and Lewis, 2010; Axelrod et al., 2011). • Instance weighting (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012). • Further exploitation of in-domain monolingual data (Ueffing et al., 2007; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009; Lambert et al., 2011). • Domain-specific features, e.g. binary features indicating the provenance of phrase pairs as implemented in the open-source Moses toolkit (Durrani et al., 2013b) or “domain augmentation” (Clark et al., 2012). However, few authors have tackled the question of how to"
2015.mtsummit-papers.19,N12-1047,0,0.0462838,"ty and word penalty. • Distance-based distortion cost. • A hierarchical lexicalized reordering model (Galley and Manning, 2008). • A 5-gram operation sequence model (Durrani et al., 2013a). • Seven binary features indicating absolute occurrence count classes of phrase pairs. • Sparse phrase length features. • Sparse lexical features for the top 200 words. • A 5-gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). We discard singleton n-grams of order three and higher. Feature weights are optimized to maximize B LEU (Papineni et al., 2002) with batch MIRA (Cherry and Foster, 2012) on 1000-best lists. We prune the phrase table to a maximum of 100 5 http://dumps.wikimedia.org 6 https://github.com/bwbaugh/wikipedia-extractor Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 245 best translation options per distinct source side and apply a minimum score threshold of 0.0001 on the source-to-target phrase translation probability. We use cube pruning in decoding. Pop limit and stack limit are set to 1000 for tuning and to 5000 for testing. We disallow reordering over punctuation. Furthermore, Minimum Bayes Risk decoding is employed for"
2015.mtsummit-papers.19,2012.amta-papers.4,0,0.040486,"akov, 2008; Bisazza et al., 2011; Niehues and Waibel, 2012; Chen et al., 2013). • Data selection (Moore and Lewis, 2010; Axelrod et al., 2011). • Instance weighting (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012). • Further exploitation of in-domain monolingual data (Ueffing et al., 2007; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009; Lambert et al., 2011). • Domain-specific features, e.g. binary features indicating the provenance of phrase pairs as implemented in the open-source Moses toolkit (Durrani et al., 2013b) or “domain augmentation” (Clark et al., 2012). However, few authors have tackled the question of how to benefit from domain adaptation in scenarios where a domain label of the input is not present. An important aspect of our approach to multi-domain MT is the need for domain classification. Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 241 Xu et al. (2007) perform domain classification for a Chinese→English task. The domains are newswire and newsgroup. The classifiers operate on whole documents rather than on individual sentences. The authors propose two techniques for domain classification. Th"
2015.mtsummit-papers.19,N13-1001,0,0.1814,"ster and Kuhn, 2007; Koehn and Schroeder, 2007; Nakov, 2008; Bisazza et al., 2011; Niehues and Waibel, 2012; Chen et al., 2013). • Data selection (Moore and Lewis, 2010; Axelrod et al., 2011). • Instance weighting (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012). • Further exploitation of in-domain monolingual data (Ueffing et al., 2007; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009; Lambert et al., 2011). • Domain-specific features, e.g. binary features indicating the provenance of phrase pairs as implemented in the open-source Moses toolkit (Durrani et al., 2013b) or “domain augmentation” (Clark et al., 2012). However, few authors have tackled the question of how to benefit from domain adaptation in scenarios where a domain label of the input is not present. An important aspect of our approach to multi-domain MT is the need for domain classification. Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 241 Xu et al. (2007) perform domain classification for a Chinese→English task. The domains are newswire and newsgroup. The classifiers operate on whole documents rather than on individual sentences. The authors prop"
2015.mtsummit-papers.19,W13-2212,1,0.871039,"Missing"
2015.mtsummit-papers.19,2011.iwslt-evaluation.1,0,0.0159256,"on political matters from parliamentary proceedings. News texts are written news articles. TED talks, Europarl, and News could be described as “genres”. We denote them as domains throughout this paper because the term “domain” is well established in related machine translation research literature and often used in a broad sense. TED talks, Europarl, and News have been highly relevant domains in recent machine translation research. The International Workshop on Spoken Language Translation1 (IWSLT) hosts a yearly open evaluation campaign which focuses on the translation of TED talks since 2011 (Federico et al., 2011). The European Parliament Proceedings Parallel Corpus (Koehn, 2005) has been an influential resource for machine translation research ever since its first release over 1 http://www.iwslt.org Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 242 a decade ago. It is freely available and includes parallel text for 21 European languages. Test sets and training data that enables research on machine translation of texts from the News domain have regularly been released for the shared translation task of the Workshop on Statistical Machine Translation2 (WMT). T"
2015.mtsummit-papers.19,D10-1044,0,0.0372183,"in adaptation for SMT has been conducted in recent years. Some methods which are commonly used are: • Tuning of the decoder model weights (Och and Ney, 2002) on an in-domain development set (Pecina et al., 2012). • Model combination (of language models, translation models, or reordering models) via interpolation or other schemes, e.g. phrase table fill-up (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Nakov, 2008; Bisazza et al., 2011; Niehues and Waibel, 2012; Chen et al., 2013). • Data selection (Moore and Lewis, 2010; Axelrod et al., 2011). • Instance weighting (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012). • Further exploitation of in-domain monolingual data (Ueffing et al., 2007; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009; Lambert et al., 2011). • Domain-specific features, e.g. binary features indicating the provenance of phrase pairs as implemented in the open-source Moses toolkit (Durrani et al., 2013b) or “domain augmentation” (Clark et al., 2012). However, few authors have tackled the question of how to benefit from domain adaptation in scenarios where a domain label of the input is not present. An important aspect of our approach t"
2015.mtsummit-papers.19,W07-0717,0,0.047497,"on the English→German, English→Italian, English→Portuguese, and English→Greek language pairs using training corpora of diverse origin, totalling tens of millions of parallel sentences. 2 Related Work A significant amount of research on domain adaptation for SMT has been conducted in recent years. Some methods which are commonly used are: • Tuning of the decoder model weights (Och and Ney, 2002) on an in-domain development set (Pecina et al., 2012). • Model combination (of language models, translation models, or reordering models) via interpolation or other schemes, e.g. phrase table fill-up (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Nakov, 2008; Bisazza et al., 2011; Niehues and Waibel, 2012; Chen et al., 2013). • Data selection (Moore and Lewis, 2010; Axelrod et al., 2011). • Instance weighting (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012). • Further exploitation of in-domain monolingual data (Ueffing et al., 2007; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009; Lambert et al., 2011). • Domain-specific features, e.g. binary features indicating the provenance of phrase pairs as implemented in the open-source Moses toolkit (Durrani et al., 201"
2015.mtsummit-papers.19,W06-1607,0,0.0720378,"the News Crawl corpora provided for the WMT 2015 shared translation task. Plain text was obtained from the Wikipedia XML dumps with the Wikipedia Extractor6 tool. Statistics of the additional monolingual training corpora are presented in Table 2. 6.2 Machine Translation Systems Word alignments are created by aligning the data in both directions and symmetrizing the two trained alignments (Och and Ney, 2003; Koehn et al., 2003). We extract phrases up to a maximum length of five. The MT systems comprise these features: • Phrase translation log-probabilities, smoothed with Good-Turing smoothing (Foster et al., 2006), and lexical translation log-probabilities in both directions. • Phrase penalty and word penalty. • Distance-based distortion cost. • A hierarchical lexicalized reordering model (Galley and Manning, 2008). • A 5-gram operation sequence model (Durrani et al., 2013a). • Seven binary features indicating absolute occurrence count classes of phrase pairs. • Sparse phrase length features. • Sparse lexical features for the top 200 words. • A 5-gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). We discard singleton n-grams of order three and higher. Feature wei"
2015.mtsummit-papers.19,D08-1089,0,0.0234049,"ual training corpora are presented in Table 2. 6.2 Machine Translation Systems Word alignments are created by aligning the data in both directions and symmetrizing the two trained alignments (Och and Ney, 2003; Koehn et al., 2003). We extract phrases up to a maximum length of five. The MT systems comprise these features: • Phrase translation log-probabilities, smoothed with Good-Turing smoothing (Foster et al., 2006), and lexical translation log-probabilities in both directions. • Phrase penalty and word penalty. • Distance-based distortion cost. • A hierarchical lexicalized reordering model (Galley and Manning, 2008). • A 5-gram operation sequence model (Durrani et al., 2013a). • Seven binary features indicating absolute occurrence count classes of phrase pairs. • Sparse phrase length features. • Sparse lexical features for the top 200 words. • A 5-gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). We discard singleton n-grams of order three and higher. Feature weights are optimized to maximize B LEU (Papineni et al., 2002) with batch MIRA (Cherry and Foster, 2012) on 1000-best lists. We prune the phrase table to a maximum of 100 5 http://dumps.wikimedia.org 6 https"
2015.mtsummit-papers.19,W08-0509,0,0.0165542,"respective LM yields the maximum LM score. Overall, we end up with four variations: MEtrain Classifier trained on medium-sized training corpora with the basic set of features. MEtrain+lm Classifier trained on medium-sized training corpora with the basic set of features plus source LM indicator features. MEdev Classifier trained on the MT development sets with the basic set of features. MEdev+lm Classifier trained on the MT development sets with the basic set of features plus source LM indicator features. 6 Experimental Setup We use Moses (Koehn et al., 2007) for machine translation, MGIZA++ (Gao and Vogel, 2008) to train word alignments, KenLM (Heafield, 2011) for LM training and scoring, SRILM (Stolcke, 2002) for LM interpolation, and the Stanford Classifier3 for maximum entropy text classification. We present experimental results on English→German, English→Italian, English→Portuguese, and English→Greek translation tasks. 6.1 Training Data Our SMT systems are trained with the following bilingual corpora: • • • • • • • • TED from WIT3 (Cettolo et al., 2012) Europarl (Koehn, 2005) JRC-Acquis 3.0 (Steinberger et al., 2006) DGT’s Translation Memory (Steinberger et al., 2012) as distributed in OPUS (Tied"
2015.mtsummit-papers.19,W12-3154,1,0.854639,"the same domain which can be employed for training and tuning. The adaptation task is then defined as utilizing a small amount of in-domain training resources effectively in order to learn system parameters that are more appropriate for translating in-domain input. The in-domain training resources constitute a minor fraction of the overall training data only, the majority of which has a domain mismatch with the designated application. The downside of systems that have been highly tweaked towards the characteristics of a single domain is a diminished translation quality on out-of-domain data (Haddow and Koehn, 2012). Online translation systems, on the other hand, are usually designed for open-domain scenarios where the domain of the input text is not predefined. Being able to take advantage of the benefits of domain adaptation while not having to compromise quality on out-of-domain data would be desirable for online systems. Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 240 A viable utilization of domain adaptation approaches in open-domain online translation systems comes in two components: • A number of different parameter sets, each tuned to optimize transla"
2015.mtsummit-papers.19,W11-2123,0,0.0283971,"e end up with four variations: MEtrain Classifier trained on medium-sized training corpora with the basic set of features. MEtrain+lm Classifier trained on medium-sized training corpora with the basic set of features plus source LM indicator features. MEdev Classifier trained on the MT development sets with the basic set of features. MEdev+lm Classifier trained on the MT development sets with the basic set of features plus source LM indicator features. 6 Experimental Setup We use Moses (Koehn et al., 2007) for machine translation, MGIZA++ (Gao and Vogel, 2008) to train word alignments, KenLM (Heafield, 2011) for LM training and scoring, SRILM (Stolcke, 2002) for LM interpolation, and the Stanford Classifier3 for maximum entropy text classification. We present experimental results on English→German, English→Italian, English→Portuguese, and English→Greek translation tasks. 6.1 Training Data Our SMT systems are trained with the following bilingual corpora: • • • • • • • • TED from WIT3 (Cettolo et al., 2012) Europarl (Koehn, 2005) JRC-Acquis 3.0 (Steinberger et al., 2006) DGT’s Translation Memory (Steinberger et al., 2012) as distributed in OPUS (Tiedemann, 2012) OPUS European Central Bank (ECB) OPU"
2015.mtsummit-papers.19,2005.mtsummit-papers.11,0,0.144636,"ews articles. TED talks, Europarl, and News could be described as “genres”. We denote them as domains throughout this paper because the term “domain” is well established in related machine translation research literature and often used in a broad sense. TED talks, Europarl, and News have been highly relevant domains in recent machine translation research. The International Workshop on Spoken Language Translation1 (IWSLT) hosts a yearly open evaluation campaign which focuses on the translation of TED talks since 2011 (Federico et al., 2011). The European Parliament Proceedings Parallel Corpus (Koehn, 2005) has been an influential resource for machine translation research ever since its first release over 1 http://www.iwslt.org Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 242 a decade ago. It is freely available and includes parallel text for 21 European languages. Test sets and training data that enables research on machine translation of texts from the News domain have regularly been released for the shared translation task of the Workshop on Statistical Machine Translation2 (WMT). The WMT “newstest” corpora have become important test sets to measur"
2015.mtsummit-papers.19,P07-2045,1,0.0125752,"an associated source LM indicator feature fires if the respective LM yields the maximum LM score. Overall, we end up with four variations: MEtrain Classifier trained on medium-sized training corpora with the basic set of features. MEtrain+lm Classifier trained on medium-sized training corpora with the basic set of features plus source LM indicator features. MEdev Classifier trained on the MT development sets with the basic set of features. MEdev+lm Classifier trained on the MT development sets with the basic set of features plus source LM indicator features. 6 Experimental Setup We use Moses (Koehn et al., 2007) for machine translation, MGIZA++ (Gao and Vogel, 2008) to train word alignments, KenLM (Heafield, 2011) for LM training and scoring, SRILM (Stolcke, 2002) for LM interpolation, and the Stanford Classifier3 for maximum entropy text classification. We present experimental results on English→German, English→Italian, English→Portuguese, and English→Greek translation tasks. 6.1 Training Data Our SMT systems are trained with the following bilingual corpora: • • • • • • • • TED from WIT3 (Cettolo et al., 2012) Europarl (Koehn, 2005) JRC-Acquis 3.0 (Steinberger et al., 2006) DGT’s Translation Memory"
2015.mtsummit-papers.19,N03-1017,0,0.00929731,"raining corpora are presented in Table 1. For language modeling on the target side, we furthermore add monolingual corpora from recent (April 2015) Wikipedia database dumps5 and—for German—the News Crawl corpora provided for the WMT 2015 shared translation task. Plain text was obtained from the Wikipedia XML dumps with the Wikipedia Extractor6 tool. Statistics of the additional monolingual training corpora are presented in Table 2. 6.2 Machine Translation Systems Word alignments are created by aligning the data in both directions and symmetrizing the two trained alignments (Och and Ney, 2003; Koehn et al., 2003). We extract phrases up to a maximum length of five. The MT systems comprise these features: • Phrase translation log-probabilities, smoothed with Good-Turing smoothing (Foster et al., 2006), and lexical translation log-probabilities in both directions. • Phrase penalty and word penalty. • Distance-based distortion cost. • A hierarchical lexicalized reordering model (Galley and Manning, 2008). • A 5-gram operation sequence model (Durrani et al., 2013a). • Seven binary features indicating absolute occurrence count classes of phrase pairs. • Sparse phrase length features. • Sparse lexical featur"
2015.mtsummit-papers.19,W07-0733,0,0.0947904,"Missing"
2015.mtsummit-papers.19,W11-2132,0,0.289447,"et al., 2012). • Model combination (of language models, translation models, or reordering models) via interpolation or other schemes, e.g. phrase table fill-up (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Nakov, 2008; Bisazza et al., 2011; Niehues and Waibel, 2012; Chen et al., 2013). • Data selection (Moore and Lewis, 2010; Axelrod et al., 2011). • Instance weighting (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012). • Further exploitation of in-domain monolingual data (Ueffing et al., 2007; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009; Lambert et al., 2011). • Domain-specific features, e.g. binary features indicating the provenance of phrase pairs as implemented in the open-source Moses toolkit (Durrani et al., 2013b) or “domain augmentation” (Clark et al., 2012). However, few authors have tackled the question of how to benefit from domain adaptation in scenarios where a domain label of the input is not present. An important aspect of our approach to multi-domain MT is the need for domain classification. Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 241 Xu et al. (2007) perform domain classification fo"
2015.mtsummit-papers.19,D08-1076,0,0.0236215,"are evaluated. The authors show that a pipeline with the SVM classifier is effective in multi-domain translation. Wang et al. (2012) distinguish generic and patent domain data in experiments on 20 language pairs. For domain classification, the authors rely on averaged perceptron classifiers with various phrase-based features. The machine translation development sets serve as training data for the classifiers. An interesting aspect of their translation experiments is that they utilize a multi-domain optimization in order to jointly tune weights for all domains in a single run of lattice MERT (Macherey et al., 2008). In a related strand of research, source-side text classifiers have recently been employed in order to detect Arabic dialects and select SMT systems accordingly (Salloum et al., 2014; Mansour et al., 2014). 3 Text Domains Our application scenario is an online translation service with the requirement to provide highquality translation not only of texts from a single domain, but of a wider range of text types. We therefore study a use case where the translation system is supposed to perform well on the following domains: TED talks, Europarl, and News. These three domains are fairly coarsegraine"
2015.mtsummit-papers.19,2014.amta-researchers.26,0,0.035468,"pairs. For domain classification, the authors rely on averaged perceptron classifiers with various phrase-based features. The machine translation development sets serve as training data for the classifiers. An interesting aspect of their translation experiments is that they utilize a multi-domain optimization in order to jointly tune weights for all domains in a single run of lattice MERT (Macherey et al., 2008). In a related strand of research, source-side text classifiers have recently been employed in order to detect Arabic dialects and select SMT systems accordingly (Salloum et al., 2014; Mansour et al., 2014). 3 Text Domains Our application scenario is an online translation service with the requirement to provide highquality translation not only of texts from a single domain, but of a wider range of text types. We therefore study a use case where the translation system is supposed to perform well on the following domains: TED talks, Europarl, and News. These three domains are fairly coarsegrained. Different documents from one of the domains are mostly not consistent regarding the covered topics. While all three domains comprise heterogeneous topics, the domains are set apart from each other by mea"
2015.mtsummit-papers.19,2012.iwslt-papers.7,0,0.0639241,"in recent years. Some methods which are commonly used are: • Tuning of the decoder model weights (Och and Ney, 2002) on an in-domain development set (Pecina et al., 2012). • Model combination (of language models, translation models, or reordering models) via interpolation or other schemes, e.g. phrase table fill-up (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Nakov, 2008; Bisazza et al., 2011; Niehues and Waibel, 2012; Chen et al., 2013). • Data selection (Moore and Lewis, 2010; Axelrod et al., 2011). • Instance weighting (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012). • Further exploitation of in-domain monolingual data (Ueffing et al., 2007; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009; Lambert et al., 2011). • Domain-specific features, e.g. binary features indicating the provenance of phrase pairs as implemented in the open-source Moses toolkit (Durrani et al., 2013b) or “domain augmentation” (Clark et al., 2012). However, few authors have tackled the question of how to benefit from domain adaptation in scenarios where a domain label of the input is not present. An important aspect of our approach to multi-domain MT is the need for domain cl"
2015.mtsummit-papers.19,2011.eamt-1.19,0,0.0209504,"-2012 sets (En→De) and on newstest2009 (En→It). We use newssyscomb2009 as an English→Italian News domain test set for lack of other English-Italian News test sets. Note that newssyscomb2009 is a small set of only 502 sentences. No News test data was available to us for the English→Portuguese and English→Greek language pairs, so we experiment with only two domains (TED and Europarl) on these tasks. The Portuguese TED development and test sets are Brazilian Portuguese whereas the Europarl sets are European Portuguese. The two Portuguese dialects have a number of differences in written language. Marujo et al. (2011) give a brief overview. 6.2.2 Domain-Adapted SMT For our domain adaptation experiments, we first tune the systems with the features described above on the respective in-domain development set (TED-tuned, Europarl-tuned, News-tuned). We next replace the large baseline LM with a domain-specific interpolated LM (+ LM interp.). We then add binary features indicating the provenance of phrase pairs (+ LM interp. + indicator feat.). 6.2.3 Mixed-Domain SMT We build mixed-domain SMT systems by tuning on a development corpus containing samples of texts from all domains. We include a balanced amount of d"
2015.mtsummit-papers.19,D09-1074,0,0.0508134,"ount of research on domain adaptation for SMT has been conducted in recent years. Some methods which are commonly used are: • Tuning of the decoder model weights (Och and Ney, 2002) on an in-domain development set (Pecina et al., 2012). • Model combination (of language models, translation models, or reordering models) via interpolation or other schemes, e.g. phrase table fill-up (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Nakov, 2008; Bisazza et al., 2011; Niehues and Waibel, 2012; Chen et al., 2013). • Data selection (Moore and Lewis, 2010; Axelrod et al., 2011). • Instance weighting (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012). • Further exploitation of in-domain monolingual data (Ueffing et al., 2007; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009; Lambert et al., 2011). • Domain-specific features, e.g. binary features indicating the provenance of phrase pairs as implemented in the open-source Moses toolkit (Durrani et al., 2013b) or “domain augmentation” (Clark et al., 2012). However, few authors have tackled the question of how to benefit from domain adaptation in scenarios where a domain label of the input is not present. An important asp"
2015.mtsummit-papers.19,P10-2041,0,0.0521304,"s of millions of parallel sentences. 2 Related Work A significant amount of research on domain adaptation for SMT has been conducted in recent years. Some methods which are commonly used are: • Tuning of the decoder model weights (Och and Ney, 2002) on an in-domain development set (Pecina et al., 2012). • Model combination (of language models, translation models, or reordering models) via interpolation or other schemes, e.g. phrase table fill-up (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Nakov, 2008; Bisazza et al., 2011; Niehues and Waibel, 2012; Chen et al., 2013). • Data selection (Moore and Lewis, 2010; Axelrod et al., 2011). • Instance weighting (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012). • Further exploitation of in-domain monolingual data (Ueffing et al., 2007; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009; Lambert et al., 2011). • Domain-specific features, e.g. binary features indicating the provenance of phrase pairs as implemented in the open-source Moses toolkit (Durrani et al., 2013b) or “domain augmentation” (Clark et al., 2012). However, few authors have tackled the question of how to benefit from domain adaptation in scenari"
2015.mtsummit-papers.19,W08-0320,0,0.0269359,"ortuguese, and English→Greek language pairs using training corpora of diverse origin, totalling tens of millions of parallel sentences. 2 Related Work A significant amount of research on domain adaptation for SMT has been conducted in recent years. Some methods which are commonly used are: • Tuning of the decoder model weights (Och and Ney, 2002) on an in-domain development set (Pecina et al., 2012). • Model combination (of language models, translation models, or reordering models) via interpolation or other schemes, e.g. phrase table fill-up (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Nakov, 2008; Bisazza et al., 2011; Niehues and Waibel, 2012; Chen et al., 2013). • Data selection (Moore and Lewis, 2010; Axelrod et al., 2011). • Instance weighting (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012). • Further exploitation of in-domain monolingual data (Ueffing et al., 2007; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009; Lambert et al., 2011). • Domain-specific features, e.g. binary features indicating the provenance of phrase pairs as implemented in the open-source Moses toolkit (Durrani et al., 2013b) or “domain augmentation” (Clark et a"
2015.mtsummit-papers.19,2012.amta-papers.19,0,0.106597,"ge pairs using training corpora of diverse origin, totalling tens of millions of parallel sentences. 2 Related Work A significant amount of research on domain adaptation for SMT has been conducted in recent years. Some methods which are commonly used are: • Tuning of the decoder model weights (Och and Ney, 2002) on an in-domain development set (Pecina et al., 2012). • Model combination (of language models, translation models, or reordering models) via interpolation or other schemes, e.g. phrase table fill-up (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Nakov, 2008; Bisazza et al., 2011; Niehues and Waibel, 2012; Chen et al., 2013). • Data selection (Moore and Lewis, 2010; Axelrod et al., 2011). • Instance weighting (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012). • Further exploitation of in-domain monolingual data (Ueffing et al., 2007; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009; Lambert et al., 2011). • Domain-specific features, e.g. binary features indicating the provenance of phrase pairs as implemented in the open-source Moses toolkit (Durrani et al., 2013b) or “domain augmentation” (Clark et al., 2012). However, few authors have tackled the"
2015.mtsummit-papers.19,P02-1038,0,0.261406,"cross all domains. However, a high-quality generic system with a single parameter set that does not depend on a domain label is appealing. In the empirical part of this paper, we compare multi-domain and mixed-domain SMT on the English→German, English→Italian, English→Portuguese, and English→Greek language pairs using training corpora of diverse origin, totalling tens of millions of parallel sentences. 2 Related Work A significant amount of research on domain adaptation for SMT has been conducted in recent years. Some methods which are commonly used are: • Tuning of the decoder model weights (Och and Ney, 2002) on an in-domain development set (Pecina et al., 2012). • Model combination (of language models, translation models, or reordering models) via interpolation or other schemes, e.g. phrase table fill-up (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Nakov, 2008; Bisazza et al., 2011; Niehues and Waibel, 2012; Chen et al., 2013). • Data selection (Moore and Lewis, 2010; Axelrod et al., 2011). • Instance weighting (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012). • Further exploitation of in-domain monolingual data (Ueffing et al., 2007; Bertoldi and Fede"
2015.mtsummit-papers.19,J03-1002,0,0.00454929,"of all bilingual training corpora are presented in Table 1. For language modeling on the target side, we furthermore add monolingual corpora from recent (April 2015) Wikipedia database dumps5 and—for German—the News Crawl corpora provided for the WMT 2015 shared translation task. Plain text was obtained from the Wikipedia XML dumps with the Wikipedia Extractor6 tool. Statistics of the additional monolingual training corpora are presented in Table 2. 6.2 Machine Translation Systems Word alignments are created by aligning the data in both directions and symmetrizing the two trained alignments (Och and Ney, 2003; Koehn et al., 2003). We extract phrases up to a maximum length of five. The MT systems comprise these features: • Phrase translation log-probabilities, smoothed with Good-Turing smoothing (Foster et al., 2006), and lexical translation log-probabilities in both directions. • Phrase penalty and word penalty. • Distance-based distortion cost. • A hierarchical lexicalized reordering model (Galley and Manning, 2008). • A 5-gram operation sequence model (Durrani et al., 2013a). • Seven binary features indicating absolute occurrence count classes of phrase pairs. • Sparse phrase length features. •"
2015.mtsummit-papers.19,P02-1040,0,0.0960693,"ities in both directions. • Phrase penalty and word penalty. • Distance-based distortion cost. • A hierarchical lexicalized reordering model (Galley and Manning, 2008). • A 5-gram operation sequence model (Durrani et al., 2013a). • Seven binary features indicating absolute occurrence count classes of phrase pairs. • Sparse phrase length features. • Sparse lexical features for the top 200 words. • A 5-gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). We discard singleton n-grams of order three and higher. Feature weights are optimized to maximize B LEU (Papineni et al., 2002) with batch MIRA (Cherry and Foster, 2012) on 1000-best lists. We prune the phrase table to a maximum of 100 5 http://dumps.wikimedia.org 6 https://github.com/bwbaugh/wikipedia-extractor Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 245 best translation options per distinct source side and apply a minimum score threshold of 0.0001 on the source-to-target phrase translation probability. We use cube pruning in decoding. Pop limit and stack limit are set to 1000 for tuning and to 5000 for testing. We disallow reordering over punctuation. Furthermore, Mi"
2015.mtsummit-papers.19,C12-1135,0,0.0349806,"Missing"
2015.mtsummit-papers.19,2014.eamt-1.39,0,0.0710152,"Missing"
2015.mtsummit-papers.19,P14-2125,0,0.0205524,"iments on 20 language pairs. For domain classification, the authors rely on averaged perceptron classifiers with various phrase-based features. The machine translation development sets serve as training data for the classifiers. An interesting aspect of their translation experiments is that they utilize a multi-domain optimization in order to jointly tune weights for all domains in a single run of lattice MERT (Macherey et al., 2008). In a related strand of research, source-side text classifiers have recently been employed in order to detect Arabic dialects and select SMT systems accordingly (Salloum et al., 2014; Mansour et al., 2014). 3 Text Domains Our application scenario is an online translation service with the requirement to provide highquality translation not only of texts from a single domain, but of a wider range of text types. We therefore study a use case where the translation system is supposed to perform well on the following domains: TED talks, Europarl, and News. These three domains are fairly coarsegrained. Different documents from one of the domains are mostly not consistent regarding the covered topics. While all three domains comprise heterogeneous topics, the domains are set apart"
2015.mtsummit-papers.19,2009.mtsummit-posters.17,0,0.0569352,"main development set (Pecina et al., 2012). • Model combination (of language models, translation models, or reordering models) via interpolation or other schemes, e.g. phrase table fill-up (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Nakov, 2008; Bisazza et al., 2011; Niehues and Waibel, 2012; Chen et al., 2013). • Data selection (Moore and Lewis, 2010; Axelrod et al., 2011). • Instance weighting (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012). • Further exploitation of in-domain monolingual data (Ueffing et al., 2007; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009; Lambert et al., 2011). • Domain-specific features, e.g. binary features indicating the provenance of phrase pairs as implemented in the open-source Moses toolkit (Durrani et al., 2013b) or “domain augmentation” (Clark et al., 2012). However, few authors have tackled the question of how to benefit from domain adaptation in scenarios where a domain label of the input is not present. An important aspect of our approach to multi-domain MT is the need for domain classification. Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 241 Xu et al. (2007) perform d"
2015.mtsummit-papers.19,2012.amta-papers.21,0,0.246487,"has been conducted in recent years. Some methods which are commonly used are: • Tuning of the decoder model weights (Och and Ney, 2002) on an in-domain development set (Pecina et al., 2012). • Model combination (of language models, translation models, or reordering models) via interpolation or other schemes, e.g. phrase table fill-up (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Nakov, 2008; Bisazza et al., 2011; Niehues and Waibel, 2012; Chen et al., 2013). • Data selection (Moore and Lewis, 2010; Axelrod et al., 2011). • Instance weighting (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012). • Further exploitation of in-domain monolingual data (Ueffing et al., 2007; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009; Lambert et al., 2011). • Domain-specific features, e.g. binary features indicating the provenance of phrase pairs as implemented in the open-source Moses toolkit (Durrani et al., 2013b) or “domain augmentation” (Clark et al., 2012). However, few authors have tackled the question of how to benefit from domain adaptation in scenarios where a domain label of the input is not present. An important aspect of our approach to multi-domain MT i"
2015.mtsummit-papers.19,steinberger-etal-2012-dgt,0,0.0441212,"Missing"
2015.mtsummit-papers.19,steinberger-etal-2006-jrc,0,0.0787518,"Missing"
2015.mtsummit-papers.19,tiedemann-2012-parallel,0,0.0570125,"Missing"
2015.mtsummit-papers.19,2012.amta-papers.18,0,0.0451164,"security in the area of computing. Empirical results on Chinese→English and English→Chinese tasks are presented. The authors build a Support Vector Machine (SVM) classifier using Term Frequency Inverse Sentence Frequency features over bigrams of stemmed content words. Classification is carried out on the level of individual sentences. The SVM is trained on the SMT training corpora (∼226k sentences in total). Several setups with different domain-adapted and domain-agnostic systems are evaluated. The authors show that a pipeline with the SVM classifier is effective in multi-domain translation. Wang et al. (2012) distinguish generic and patent domain data in experiments on 20 language pairs. For domain classification, the authors rely on averaged perceptron classifiers with various phrase-based features. The machine translation development sets serve as training data for the classifiers. An interesting aspect of their translation experiments is that they utilize a multi-domain optimization in order to jointly tune weights for all domains in a single run of lattice MERT (Macherey et al., 2008). In a related strand of research, source-side text classifiers have recently been employed in order to detect"
2015.mtsummit-papers.19,2007.mtsummit-papers.68,0,0.705796,"chwenk and Senellart, 2009; Lambert et al., 2011). • Domain-specific features, e.g. binary features indicating the provenance of phrase pairs as implemented in the open-source Moses toolkit (Durrani et al., 2013b) or “domain augmentation” (Clark et al., 2012). However, few authors have tackled the question of how to benefit from domain adaptation in scenarios where a domain label of the input is not present. An important aspect of our approach to multi-domain MT is the need for domain classification. Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 241 Xu et al. (2007) perform domain classification for a Chinese→English task. The domains are newswire and newsgroup. The classifiers operate on whole documents rather than on individual sentences. The authors propose two techniques for domain classification. Their first technique is based on interpolated LMs: a general-domain LM is interpolated with LMs which were trained on in-domain development sets, resulting in a number of domain-specific interpolated LMs. The interpolation weight is heuristically chosen. The classifier computes LM perplexities over input documents and assigns the domain with the lowest per"
2020.acl-main.417,bojar-etal-2012-joy,0,0.0615774,"Missing"
2020.acl-main.417,P91-1022,0,0.752852,"ctors may be recentered around the mean vector for a web domain (Germann, 2016) Document alignment quality can be improved with additional features such ratio of shared links, similarity of link URLs, ratio of shared images, binary feature indicating if the documents are linked, DOM structure similarity (Espl`a-Gomis et al., 2016), same numbers (Papavassiliou et al., 2016), or same named entities (Lohar et al., 2016). Guo et al. (2019) introduce the use of document embeddings, constructed from sentence embeddings, to the document alignment task. 2.3 Sentence Alignment Early sentence aligners (Brown et al., 1991; Gale and Church, 1993) use scoring functions based only on the number of words or characters in each sentence and alignment algorithms based on dynamic programming. Europarl, for example, used metadata to align paragraphs, typically consisting of 2-5 sentences, and using Gale and Church (1993)’s method to align sentences within corresponding paragraphs. Later work added lexical features and heuristics to speed up search, such as limiting the search space to be near the diagonal (Moore, 2002; Varga et al., 2005). More recent work introduced scoring methods that use MT to get both documents in"
2020.acl-main.417,buck-etal-2014-n,1,0.887657,"Missing"
2020.acl-main.417,W16-2347,1,0.931151,"set of patterns for language marking or simple Levenshtein distance (Le et al., 2016). Content matching requires crossing the language barrier at some point, typically by using bilingual dictionaries or translating one of the documents into the other document’s language (Uszkoreit et al., 2010). Documents may be represented by vectors over word frequencies, typically td-idf-weighted. Vectors may also be constructed over bigrams (Dara and Lin, 2016) or even higher order n-grams 4 http://opus.lingfil.uu.se/ (Uszkoreit et al., 2010). The vectors are then typically matched with cosine similarity (Buck and Koehn, 2016a). The raw vectors may be recentered around the mean vector for a web domain (Germann, 2016) Document alignment quality can be improved with additional features such ratio of shared links, similarity of link URLs, ratio of shared images, binary feature indicating if the documents are linked, DOM structure similarity (Espl`a-Gomis et al., 2016), same numbers (Papavassiliou et al., 2016), or same named entities (Lohar et al., 2016). Guo et al. (2019) introduce the use of document embeddings, constructed from sentence embeddings, to the document alignment task. 2.3 Sentence Alignment Early sente"
2020.acl-main.417,W11-1218,0,0.0324271,"Missing"
2020.acl-main.417,W16-2365,1,0.928849,"set of patterns for language marking or simple Levenshtein distance (Le et al., 2016). Content matching requires crossing the language barrier at some point, typically by using bilingual dictionaries or translating one of the documents into the other document’s language (Uszkoreit et al., 2010). Documents may be represented by vectors over word frequencies, typically td-idf-weighted. Vectors may also be constructed over bigrams (Dara and Lin, 2016) or even higher order n-grams 4 http://opus.lingfil.uu.se/ (Uszkoreit et al., 2010). The vectors are then typically matched with cosine similarity (Buck and Koehn, 2016a). The raw vectors may be recentered around the mean vector for a web domain (Germann, 2016) Document alignment quality can be improved with additional features such ratio of shared links, similarity of link URLs, ratio of shared images, binary feature indicating if the documents are linked, DOM structure similarity (Espl`a-Gomis et al., 2016), same numbers (Papavassiliou et al., 2016), or same named entities (Lohar et al., 2016). Guo et al. (2019) introduce the use of document embeddings, constructed from sentence embeddings, to the document alignment task. 2.3 Sentence Alignment Early sente"
2020.acl-main.417,2012.eamt-1.60,0,0.0178318,"2.1 Acquisition Efforts Most publicly available parallel corpora are the result of targeted efforts to extract the translations from a specific source. The French–English Canadian Hansards3 were used in the earliest work on statistical machine translation. A similar popular corpus is Europarl (Koehn, 2005), used throughout the WMT evaluation campaign. Multi-lingual web sites are attractive targets. Rafalovitch and Dale (2009); Ziemski et al. (2015) extract data from the United Nations, T¨ager (2011) from European Patents, Lison and Tiedemann (2016) from a collection of TV and movie subtitles. Cettolo et al. (2012) explain the creation of a multilingual parallel corpus of subtitles from the TED Talks website which is popular due to its use in the IWSLT evaluation campaign. 3 https://www.isi.edu/natural-language/ download/hansard/ 4555 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4555–4567 c July 5 - 10, 2020. 2020 Association for Computational Linguistics There are also various efforts targeted at a single language pair. Martin et al. (2003) build a parallel corpus for Inuktitut–English. Utiyama and Isahara (2003); Fukushima et al. (2006) worked on creat"
2020.acl-main.417,P05-1074,0,0.166175,"s Translation. We report on methods to create the largest publicly available parallel corpora by crawling the web, using open source software. We empirically compare alternative methods and publish benchmark data sets for sentence alignment and sentence pair filtering. We also describe the parallel corpora released and evaluate their quality and their usefulness to create machine translation systems. 1 2 Introduction Parallel corpora are essential for building highquality machine translation systems and have found uses in many other natural language applications, such as learning paraphrases (Bannard and Callison-Burch, 2005; Hu et al., 2019) or cross-lingual projection of language tools (Yarowsky et al., 2001). We report on work to create the largest publicly available parallel corpora by crawling hundreds of thousands of web sites, using open source tools. The processing pipeline consists of the steps: crawling, text extraction, document alignment, sentence alignment, and sentence pair filtering. We describe these steps in detail in Sections 4–8. For some of these steps we evaluate several methods empirically in terms of their impact on machine translation quality. We provide the data resources used in these ev"
2020.acl-main.417,W19-5435,1,0.894745,"Missing"
2020.acl-main.417,2005.mtsummit-papers.11,1,0.214853,"modular pipeline that allows harvesting parallel corpora from multilingual websites or from preexisting or historical web crawls such as the one available as part of the Internet Archive.2 1 2 https://github.com/bitextor/bitextor https://archive.org/ Related Work While the idea of mining the web for parallel data has been already pursued in the 20th century (Resnik, 1999), the most serious efforts have been limited to large companies such as Google (Uszkoreit et al., 2010) and Microsoft (Rarrick et al., 2011), or targeted efforts on specific domains such as the Canadian Hansards and Europarl (Koehn, 2005). The book Bitext Alignment (Tiedemann, 2011) describes some of the challenges in greater detail. 2.1 Acquisition Efforts Most publicly available parallel corpora are the result of targeted efforts to extract the translations from a specific source. The French–English Canadian Hansards3 were used in the earliest work on statistical machine translation. A similar popular corpus is Europarl (Koehn, 2005), used throughout the WMT evaluation campaign. Multi-lingual web sites are attractive targets. Rafalovitch and Dale (2009); Ziemski et al. (2015) extract data from the United Nations, T¨ager (201"
2020.acl-main.417,W19-5404,1,0.891381,"Missing"
2020.acl-main.417,W18-6453,1,0.89611,"Missing"
2020.acl-main.417,W19-5438,0,0.038302,"Missing"
2020.acl-main.417,W16-2371,0,0.0501587,"Missing"
2020.acl-main.417,I08-2120,0,0.0456513,"age/ download/hansard/ 4555 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4555–4567 c July 5 - 10, 2020. 2020 Association for Computational Linguistics There are also various efforts targeted at a single language pair. Martin et al. (2003) build a parallel corpus for Inuktitut–English. Utiyama and Isahara (2003); Fukushima et al. (2006) worked on creating Japanese–English corpora. Uchiyama and Isahara (2007) report on the efforts to build a Japanese–English patent corpus and Macken et al. (2007) on efforts on a broad-based Dutch–English corpus. Li and Liu (2008) mine the web for a Chinese–English corpus. A large Czech–English corpus from various sources was collected (Bojar et al., 2010), linguistically annotated (Bojar et al., 2012), and has been continuously extended to over 300 million words (Bojar et al., 2016). All these efforts rely on methods and implementations that are quite specific for each use case, not documented in great detail, and not publicly available. A discussion of the pitfalls during the construction of parallel corpora is given by Kaalep and Veskis (2007). A large collection of corpora is maintained at the OPUS web site4 (Tiede"
2020.acl-main.417,L16-1147,0,0.0317824,"t (Tiedemann, 2011) describes some of the challenges in greater detail. 2.1 Acquisition Efforts Most publicly available parallel corpora are the result of targeted efforts to extract the translations from a specific source. The French–English Canadian Hansards3 were used in the earliest work on statistical machine translation. A similar popular corpus is Europarl (Koehn, 2005), used throughout the WMT evaluation campaign. Multi-lingual web sites are attractive targets. Rafalovitch and Dale (2009); Ziemski et al. (2015) extract data from the United Nations, T¨ager (2011) from European Patents, Lison and Tiedemann (2016) from a collection of TV and movie subtitles. Cettolo et al. (2012) explain the creation of a multilingual parallel corpus of subtitles from the TED Talks website which is popular due to its use in the IWSLT evaluation campaign. 3 https://www.isi.edu/natural-language/ download/hansard/ 4555 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4555–4567 c July 5 - 10, 2020. 2020 Association for Computational Linguistics There are also various efforts targeted at a single language pair. Martin et al. (2003) build a parallel corpus for Inuktitut–English."
2020.acl-main.417,W16-2372,0,0.0140892,"ra and Lin, 2016) or even higher order n-grams 4 http://opus.lingfil.uu.se/ (Uszkoreit et al., 2010). The vectors are then typically matched with cosine similarity (Buck and Koehn, 2016a). The raw vectors may be recentered around the mean vector for a web domain (Germann, 2016) Document alignment quality can be improved with additional features such ratio of shared links, similarity of link URLs, ratio of shared images, binary feature indicating if the documents are linked, DOM structure similarity (Espl`a-Gomis et al., 2016), same numbers (Papavassiliou et al., 2016), or same named entities (Lohar et al., 2016). Guo et al. (2019) introduce the use of document embeddings, constructed from sentence embeddings, to the document alignment task. 2.3 Sentence Alignment Early sentence aligners (Brown et al., 1991; Gale and Church, 1993) use scoring functions based only on the number of words or characters in each sentence and alignment algorithms based on dynamic programming. Europarl, for example, used metadata to align paragraphs, typically consisting of 2-5 sentences, and using Gale and Church (1993)’s method to align sentences within corresponding paragraphs. Later work added lexical features and heuris"
2020.acl-main.417,2007.mtsummit-papers.42,0,0.0236436,"e in the IWSLT evaluation campaign. 3 https://www.isi.edu/natural-language/ download/hansard/ 4555 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4555–4567 c July 5 - 10, 2020. 2020 Association for Computational Linguistics There are also various efforts targeted at a single language pair. Martin et al. (2003) build a parallel corpus for Inuktitut–English. Utiyama and Isahara (2003); Fukushima et al. (2006) worked on creating Japanese–English corpora. Uchiyama and Isahara (2007) report on the efforts to build a Japanese–English patent corpus and Macken et al. (2007) on efforts on a broad-based Dutch–English corpus. Li and Liu (2008) mine the web for a Chinese–English corpus. A large Czech–English corpus from various sources was collected (Bojar et al., 2010), linguistically annotated (Bojar et al., 2012), and has been continuously extended to over 300 million words (Bojar et al., 2016). All these efforts rely on methods and implementations that are quite specific for each use case, not documented in great detail, and not publicly available. A discussion of the pitfalls during the construction of parallel corpora is given by Kaalep and Veskis (2007). A la"
2020.acl-main.417,W03-0320,0,0.154143,"ons, T¨ager (2011) from European Patents, Lison and Tiedemann (2016) from a collection of TV and movie subtitles. Cettolo et al. (2012) explain the creation of a multilingual parallel corpus of subtitles from the TED Talks website which is popular due to its use in the IWSLT evaluation campaign. 3 https://www.isi.edu/natural-language/ download/hansard/ 4555 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4555–4567 c July 5 - 10, 2020. 2020 Association for Computational Linguistics There are also various efforts targeted at a single language pair. Martin et al. (2003) build a parallel corpus for Inuktitut–English. Utiyama and Isahara (2003); Fukushima et al. (2006) worked on creating Japanese–English corpora. Uchiyama and Isahara (2007) report on the efforts to build a Japanese–English patent corpus and Macken et al. (2007) on efforts on a broad-based Dutch–English corpus. Li and Liu (2008) mine the web for a Chinese–English corpus. A large Czech–English corpus from various sources was collected (Bojar et al., 2010), linguistically annotated (Bojar et al., 2012), and has been continuously extended to over 300 million words (Bojar et al., 2016). All these e"
2020.acl-main.417,moore-2002-fast,0,0.25649,"nce embeddings, to the document alignment task. 2.3 Sentence Alignment Early sentence aligners (Brown et al., 1991; Gale and Church, 1993) use scoring functions based only on the number of words or characters in each sentence and alignment algorithms based on dynamic programming. Europarl, for example, used metadata to align paragraphs, typically consisting of 2-5 sentences, and using Gale and Church (1993)’s method to align sentences within corresponding paragraphs. Later work added lexical features and heuristics to speed up search, such as limiting the search space to be near the diagonal (Moore, 2002; Varga et al., 2005). More recent work introduced scoring methods that use MT to get both documents into the same language (Sennrich and Volk, 2010) or use pruned phrase tables from a statistical MT system (Gomes and Lopes, 2016). Both methods “anchor” highprobability 1–1 alignments in the search space and then fill in and refine alignments. They later propose an extension (Sennrich and Volk, 2011) in which an SMT system is bootstrapped from an initial alignment and then used in Bleualign. Vecalign (Thompson and Koehn, 2019) is a sentence alignment method that relies on bilingual sentence emb"
2020.acl-main.417,J05-4003,0,0.157471,"Our work exploits web sites that provide roughly the same content in multiple languages, leading us to the assumption to find pairs of web pages which are translations of each other, with translated sentences following the same order. This assumption does not hold in less consistently translated web content such as Wikipedia, or accidental parallel sentence found in news stories about the same subject matter written in multiple languages. There have been increasing efforts to mine sentence pairs from large pools of multi-lingual text, which are treated as unstructured bags of sen4557 tences. Munteanu and Marcu (2005) use document retrieval and a maximum entropy classifier to identify parallel sentence pairs in a multi-lingual collection of news stories. Bilingual sentence embeddings (Guo et al., 2018) and multilingual sentence embeddings (Artetxe and Schwenk, 2018) were tested on their ability to reconstruct parallel corpora. This lead to work to construct WikiMatrix, a large corpus of parallel sentences from Wikipedia (Schwenk et al., 2019) based on cosine distance of their crosslingual sentence embeddings. 3 Identifying Multi-Lingual Web Sites Since the start of the collection effort in 2015, we identif"
2020.amta-research.12,P19-1126,0,0.0452508,"er to finish their utterance. However online MT is less well supported, and is complicated by the reordering which is often necessary in translation, and by the use of encoder-decoder models which assume sight of the whole source sentence. Some systems for online SLT rely on the streaming approach to translation, perhaps inspired by human interpreters. In this approach, the MT system is modified to translate incrementally, and on each update from ASR it will decide whether to update its translation, or wait for further ASR output (Cho and Esipova, 2016; Ma et al., 2019; Zheng et al., 2019a,b; Arivazhagan et al., 2019). The difficulty with the streaming approach is that the system has to choose between committing to a particular choice of translation output, or waiting for further updates from ASR, and does not have the option to revise an incorrect choice. Furthermore, all the streaming approaches referenced above require specialised training of the MT system, and modified inference algorithms. To address the issues above, we construct our online SLT system using the retranslation approach (Niehues et al., 2018; Arivazhagan et al., 2020a), which is less studied but Proceedings of the 14th Conference of the"
2020.amta-research.12,2020.iwslt-1.27,0,0.0993883,"utput (Cho and Esipova, 2016; Ma et al., 2019; Zheng et al., 2019a,b; Arivazhagan et al., 2019). The difficulty with the streaming approach is that the system has to choose between committing to a particular choice of translation output, or waiting for further updates from ASR, and does not have the option to revise an incorrect choice. Furthermore, all the streaming approaches referenced above require specialised training of the MT system, and modified inference algorithms. To address the issues above, we construct our online SLT system using the retranslation approach (Niehues et al., 2018; Arivazhagan et al., 2020a), which is less studied but Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 123 more straightforward. It can be implemented using any standard MT toolkit (such as Marian (Junczys-Dowmunt et al., 2018) which is highly optimised for speed) and using the latest advances in text-to-text translation. The idea of retranslation is that we produce a new translation of the current sentence every time a partial sentence is received from the ASR system. Thus, the translation of each sentence prefix is in"
2020.amta-research.12,N12-1048,0,0.0703659,"Missing"
2020.amta-research.12,E17-1099,0,0.0118916,"ing the system make probes of possible extensions to the source prefix, and observing how stable the translation of these probes is – instability in the translation requires a larger mask. Our method requires no modifications to the underlying MT system, and has no effect on translation quality. 2 Related Work Early work on incremental MT used prosody (Bangalore et al., 2012) or lexical cues (Rangarajan Sridhar et al., 2013) to make the translate-or-wait decision. The first work on incremental neural MT used confidence to decide whether to wait or translate (Cho and Esipova, 2016), whilst in (Gu et al., 2017) they learn the translation schedule with reinforcement learning. In Ma et al. (2019), they address simultaneous translation using a transformer (Vaswani et al., 2017) model with a modified attention mechanism, which is trained on prefixes. They introduce the idea of wait-k, where the translation does not consider the final k words of the input. This work was extended by Zheng et al. (2019b,a), where a “delay” token is added to the target vocabulary so the model can learn when to wait, through being trained by imitation learning. The MILk attention (Arivazhagan et al., 2019) also provides a wa"
2020.amta-research.12,P18-4020,0,0.084089,"Missing"
2020.amta-research.12,P07-2045,0,0.00525882,"providing a more complete experimental picture than in Arivazhagan et al. (2020a), and demonstrating the adverse effect of biased beam search on quality. For these experiments we use data released for the IWSLT MT task (Cettolo et al., 2017), in both English→German and English→Chinese. We consider a simulated ASR system, which supplies the gold transcripts to the MT system one token at a time2 . For training we use the TED talk data, with dev2010 as heldout and tst2010 as test set. The raw data set sizes are 206112 sentences (en-de) and 231266 sentences (en-zh). We preprocess using the Moses (Koehn et al., 2007) tokenizer and truecaser (for English and German) and jieba3 for Chinese. We apply BPE (Sennrich et al., 2016) jointly with 90k merge operations. For our MT system, we use the transformer-base architecture (Vaswani et al., 2017) as implemented by Nematus (Sennrich et al., 2017). We use 256 sentence mini-batches, and a 4000 iteration warm-up in training. As we mentioned in the introduction, we did experiment with prefix training (using both alignment-based and length-based truncation) and found that this improved the translation of prefixes, but generally degraded translation for full sentences"
2020.amta-research.12,P19-1289,0,0.0874469,"ntally, instead of waiting for the speaker to finish their utterance. However online MT is less well supported, and is complicated by the reordering which is often necessary in translation, and by the use of encoder-decoder models which assume sight of the whole source sentence. Some systems for online SLT rely on the streaming approach to translation, perhaps inspired by human interpreters. In this approach, the MT system is modified to translate incrementally, and on each update from ASR it will decide whether to update its translation, or wait for further ASR output (Cho and Esipova, 2016; Ma et al., 2019; Zheng et al., 2019a,b; Arivazhagan et al., 2019). The difficulty with the streaming approach is that the system has to choose between committing to a particular choice of translation output, or waiting for further updates from ASR, and does not have the option to revise an incorrect choice. Furthermore, all the streaming approaches referenced above require specialised training of the MT system, and modified inference algorithms. To address the issues above, we construct our online SLT system using the retranslation approach (Niehues et al., 2018; Arivazhagan et al., 2020a), which is less stu"
2020.amta-research.12,W18-6319,0,0.0918075,"er of words the system has output, and the number of words expected, given the length of the source prefix received, and the ratio between source and target length. Formally, AL for source and target sentences S and T is defined as: AL(S, T ) = τ (t − 1)|S| 1X g(t) − τ t=1 |T | where τ is the number of target words generated by the time the whole source sentence is received, g(t) is the number of source words processed when the target hypothesis first reaches a length of t tokens. In our implementation, we calculate the AL at token (not subword) level with the standard tokenizer in sacreBLEU (Post, 2018), meaning that for Chinese output we calculate AL on characters. This metric differs from the one used in Arivazhagan et al. (2020a)1 , where latency is defined as the mean time between a source word being received and the translation of that source word being finalised. We argue against this definition, because it conflates latency and 1 In the presentation of this paper at ICASSP, the authors used a latency metric similar to the one used here, and different to the one they used in the paper Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October"
2020.amta-research.12,N13-1023,0,0.0588387,"Missing"
2020.amta-research.12,E17-3017,1,0.870478,"Missing"
2020.amta-research.12,P16-1162,1,0.187282,"verse effect of biased beam search on quality. For these experiments we use data released for the IWSLT MT task (Cettolo et al., 2017), in both English→German and English→Chinese. We consider a simulated ASR system, which supplies the gold transcripts to the MT system one token at a time2 . For training we use the TED talk data, with dev2010 as heldout and tst2010 as test set. The raw data set sizes are 206112 sentences (en-de) and 231266 sentences (en-zh). We preprocess using the Moses (Koehn et al., 2007) tokenizer and truecaser (for English and German) and jieba3 for Chinese. We apply BPE (Sennrich et al., 2016) jointly with 90k merge operations. For our MT system, we use the transformer-base architecture (Vaswani et al., 2017) as implemented by Nematus (Sennrich et al., 2017). We use 256 sentence mini-batches, and a 4000 iteration warm-up in training. As we mentioned in the introduction, we did experiment with prefix training (using both alignment-based and length-based truncation) and found that this improved the translation of prefixes, but generally degraded translation for full sentences. Since prefix translation can also be improved using the masking and biasing techniques, and the former does"
2020.amta-research.12,D19-1137,0,0.0437367,"f waiting for the speaker to finish their utterance. However online MT is less well supported, and is complicated by the reordering which is often necessary in translation, and by the use of encoder-decoder models which assume sight of the whole source sentence. Some systems for online SLT rely on the streaming approach to translation, perhaps inspired by human interpreters. In this approach, the MT system is modified to translate incrementally, and on each update from ASR it will decide whether to update its translation, or wait for further ASR output (Cho and Esipova, 2016; Ma et al., 2019; Zheng et al., 2019a,b; Arivazhagan et al., 2019). The difficulty with the streaming approach is that the system has to choose between committing to a particular choice of translation output, or waiting for further updates from ASR, and does not have the option to revise an incorrect choice. Furthermore, all the streaming approaches referenced above require specialised training of the MT system, and modified inference algorithms. To address the issues above, we construct our online SLT system using the retranslation approach (Niehues et al., 2018; Arivazhagan et al., 2020a), which is less studied but Proceedings"
2020.eamt-1.53,D19-1081,1,0.842597,"erpreter would not be available for capacity reasons. The 43 languages are 24 EU official languages and 19 others, spoken between Morocco and Kazachstan. With our other user partner, alfatraining, we connect our system with an online meeting platform, alfaview®. 2.2 Other Research Topics The most visible application goal of live subtitling is supported by our advancements in the related areas. We research into document-level machine translation to enable conference participants to translate documents between all the 43 languages in high-quality, taking inter-sentential phenomena into account (Voita et al., 2019a; Voita et al., 2019b; Vojtˇechov´a et al., 2019; Popel et al., 2019; Rysov´a et al., 2019). We research into multilingual machine translation to reduce the cost of targeting many languages at once, and to leverage multiple language variants of the source for higher quality (Zhang et al., 2019; Zhang and Sennrich, 2019). To face challenges of simultaneous translation, such as robustness to noise, out-of-vocabulary words, domain adaptation, and non-standard accents (Mach´acˇ ek et al., 2019), latency and quality trade-off, we aim to improve automatic speech recognition. We also explore cascade"
2020.eamt-1.53,P19-1116,1,0.81532,"erpreter would not be available for capacity reasons. The 43 languages are 24 EU official languages and 19 others, spoken between Morocco and Kazachstan. With our other user partner, alfatraining, we connect our system with an online meeting platform, alfaview®. 2.2 Other Research Topics The most visible application goal of live subtitling is supported by our advancements in the related areas. We research into document-level machine translation to enable conference participants to translate documents between all the 43 languages in high-quality, taking inter-sentential phenomena into account (Voita et al., 2019a; Voita et al., 2019b; Vojtˇechov´a et al., 2019; Popel et al., 2019; Rysov´a et al., 2019). We research into multilingual machine translation to reduce the cost of targeting many languages at once, and to leverage multiple language variants of the source for higher quality (Zhang et al., 2019; Zhang and Sennrich, 2019). To face challenges of simultaneous translation, such as robustness to noise, out-of-vocabulary words, domain adaptation, and non-standard accents (Mach´acˇ ek et al., 2019), latency and quality trade-off, we aim to improve automatic speech recognition. We also explore cascade"
2020.eamt-1.53,W19-5355,1,0.883897,"Missing"
2020.eamt-1.53,D19-1083,1,0.761049,"st visible application goal of live subtitling is supported by our advancements in the related areas. We research into document-level machine translation to enable conference participants to translate documents between all the 43 languages in high-quality, taking inter-sentential phenomena into account (Voita et al., 2019a; Voita et al., 2019b; Vojtˇechov´a et al., 2019; Popel et al., 2019; Rysov´a et al., 2019). We research into multilingual machine translation to reduce the cost of targeting many languages at once, and to leverage multiple language variants of the source for higher quality (Zhang et al., 2019; Zhang and Sennrich, 2019). To face challenges of simultaneous translation, such as robustness to noise, out-of-vocabulary words, domain adaptation, and non-standard accents (Mach´acˇ ek et al., 2019), latency and quality trade-off, we aim to improve automatic speech recognition. We also explore cascaded and fully end-to-end neural spoken language translation (Pham et al., 2019; Nguyen et al., 2019; Nguyen et al., 2020) and co-organize shared tasks at WMT and IWSLT. 2.3 Automatic Minuting The last objective of our project is an automatic system for structured summaries of meetings. It is a ch"
2020.emnlp-main.187,E14-1049,0,0.0292768,"d the language coverage. However, our method primarily differs as it is mainly based in linear algebra, encodes information from both sources since the beginning, and can deal with a small number of shared entries (e.g. 23 from LW ) to compute robust representations. There has been very little work on adopting typology knowledge for NMT. There is not a deep integration of the topics (Ponti et al., 2019), but one shallow and prominent case is the ranking method (Lin et al., 2019) that we analysed in §6. Finally, CCA and its variants have been previously used to derive embeddings at word-level (Faruqui and Dyer, 2014; Dhillon et al., 2015; Osborne et al., 2016). Kudugunta et al. (2019) also used SVCCA but to inspect sentence-level representations, where they uncover relevant insights about language similarity that are aligned with our results in §5. However, as far as we know, this is the first time a CCA-based method has been used to compute language-level representations. 9 Takeaways and practical tool We summarise our key findings as follows: • SVCCA can fuse linguistic typology KB entries with NMT-learned embeddings without diminishing the originally encoded typological and genetic similarity of langu"
2020.emnlp-main.187,2020.acl-main.560,0,0.044494,"VCCA can fuse linguistic typology KB entries with NMT-learned embeddings without diminishing the originally encoded typological and genetic similarity of languages. • Our method is a robust alternative for identifying clusters and choosing related languages for multilingual transfer in NMT. The advantage is notable when it is not feasible to pretrain a ranking model or learn embeddings from a massive multilingual system. Assessing new languages is an important ability, given that most of them do not have even enough monolingual corpora to learn embeddings from multilingual language modelling (Joshi et al., 2020). • Factored language embeddings encode more information to agglomerate related languages than the initial pseudo-token setting. Furthermore, we make our code available as an open-source tool9 , together with our LT factoredembeddings, to compute multi-view language representations using SVCCA. We enable the option to use other language vectors from lang2vec (Phonology or Phonetic Inventory) as the KB-source, and to upload new task-learned embeddings from different settings, such as one-to-many or many-to-many NMT, and also multilingual language modelling. Besides, given a list of languages to"
2020.emnlp-main.187,W18-2716,0,0.0132958,"and training. Similar to Tan et al. (2019), we train small transformer models (Vaswani et al., 2017). We jointly learn 90k shared sub-words with the byte pair encoding (Sennrich et al., 2016) algorithm built in SentencePiece (Kudo and Richardson, 2018). We also oversample all the training data of the less-resourced languages in each cluster, and shuffle them proportionally in all batches. We use Nematus (Sennrich et al., 2017) only to extract the factored language embeddings from the TED-53 corpus (LT ). Besides, given the large number of experiments, we also choose the efficient Marian NMT (Junczys-Dowmunt et al., 2018) toolkit for training the rest of systems. With Marian NMT, we only use the basic pseudo-token setting for identifying the source language, as we did not need to retrieve new language embeddings after training. Besides, we allow the Marian NMT framework to automatically determine the minibatch size given the sentence-length and available memory (mini-batch-fit parameter). We train our models with up to four NVIDIA P100 GPUs using Adam optimiser (Kingma and Ba, 2014) with default parameters (β1 = 0.9, β2 = 0.98, ε = 10−9 ) and early stopping at 5 validation steps for the cross-entropy metric. F"
2020.emnlp-main.187,E17-2002,0,0.436869,"2004). The two-step transformation of SVD followed by CCA is called singular vector canonical correlation analysis (SVCCA; Raghu et al., 2017) in the context of understanding the representation learning throughout neural network layers. That being said, we use SVCCA to get language representations and not to inspect a neural architecture.2 3 Methodology and research questions To embed linguistic typology knowledge in dense representations for a broad set of languages, we employ SVCCA (§2) with the following sources: KB view. We employ the language vectors from the URIEL and lang2vec database (Littell et al., 2017). Precisely, we work with the k-NN vectors of the Syntax feature class (US ; 103 feats.), that are composed of binary features encoded from WALS (Dryer and Haspelmath, 2013). (NMT) Learned view. Firstly, we exploit the NMT-learned embeddings from the Bible (LB ; 512 dim.) (Malaviya et al., 2017). Up to 731 entries are available in lang2vec that intersects with US . They were trained in a many-to-English NMT model with a pseudo-token identifying the source language at the beginning of every input sentence. Secondly, we take the many-to-English language embeddings learned for the language cluste"
2020.emnlp-main.187,D17-1268,0,0.524204,"ons and not to inspect a neural architecture.2 3 Methodology and research questions To embed linguistic typology knowledge in dense representations for a broad set of languages, we employ SVCCA (§2) with the following sources: KB view. We employ the language vectors from the URIEL and lang2vec database (Littell et al., 2017). Precisely, we work with the k-NN vectors of the Syntax feature class (US ; 103 feats.), that are composed of binary features encoded from WALS (Dryer and Haspelmath, 2013). (NMT) Learned view. Firstly, we exploit the NMT-learned embeddings from the Bible (LB ; 512 dim.) (Malaviya et al., 2017). Up to 731 entries are available in lang2vec that intersects with US . They were trained in a many-to-English NMT model with a pseudo-token identifying the source language at the beginning of every input sentence. Secondly, we take the many-to-English language embeddings learned for the language clustering task on multilingual NMT (LW ; 256 dim.) (Tan et al., 2019), where they use 23 languages of the WIT3 corpus (Cettolo et al., 2012). One main difference for the latter is the use of factors in the architecture, meaning that the embedding of every input token was concatenated with the embedde"
2020.emnlp-main.187,N15-1036,0,0.0314384,"ANK shows the accumulated training size (in thousands) for the top-3 candidates, whereas with SVCCA we approximate the amount of data and include the number of languages between brackets. 8 However, we do not answer what multilingual NMT really transfers to the low-resource languages. We left that question for further research, together with optimising the k number of languages or the amount of data per each language. Related work For language-level representations, URIEL and lang2vec (Littell et al., 2017) allow a straightforward extraction of typological binary features from different KBs. Murawaki (2015, 2017, 2018) exploits them to build latent language representations with independent binary variables. Language features are encoded from data-driven tasks as well, such as NMT (Malaviya et al., 2017) or language ¨ modelling (Tsvetkov et al., 2016; Ostling and Tiedemann, 2017; Bjerva and Augenstein, 2018b) 2398 with complementary linguistic-related target tasks (Bjerva and Augenstein, 2018a). Our approach is most similar to Bjerva et al. (2019a), as they build a generative model from typological features and use language embeddings, extracted from factored language modelling at character-leve"
2020.emnlp-main.187,I17-1046,0,0.327368,"Missing"
2020.emnlp-main.187,D18-1468,0,0.0459798,"Missing"
2020.emnlp-main.187,W18-6319,0,0.0174961,"ems. With Marian NMT, we only use the basic pseudo-token setting for identifying the source language, as we did not need to retrieve new language embeddings after training. Besides, we allow the Marian NMT framework to automatically determine the minibatch size given the sentence-length and available memory (mini-batch-fit parameter). We train our models with up to four NVIDIA P100 GPUs using Adam optimiser (Kingma and Ba, 2014) with default parameters (β1 = 0.9, β2 = 0.98, ε = 10−9 ) and early stopping at 5 validation steps for the cross-entropy metric. Finally, the sacreBLEU version string (Post, 2018) is as follows: BLEU+case.mixed+numrefs.1+smooth.exp +tok.13a+version.1.3.7. Clustering settings. We first list the baselines and our approaches, with the number of clusters/models between brackets: 1. Individual [53]: Pairwise model per language. 2. Massive [1]: A single model for all languages. 3. Language families [20]: Based on historical linguistics. We divide the 33 Indo-European languages into 7 branches. Moreover, 11 groups only have one language. 4. KB [3]: US (Syntax) tends to agglomerate large clusters (with 4-13-33 languages), behaving similar to a massive model (Fig. 2c). 5. Learn"
2020.emnlp-main.187,N18-2084,0,0.211501,"se 23 languages of the WIT3 corpus (Cettolo et al., 2012). One main difference for the latter is the use of factors in the architecture, meaning that the embedding of every input token was concatenated with the embedded pseudo-token that identifies the source language. The second difference is the neural architecture used to extract the embeddings: the former use a recurrent neural network, whereas the latter a small transformer model (Vaswani et al., 2017). Finally, we train a new set of embeddings (LT ) that we extracted from the 53 languages of the TED corpus (many-to-English) processed by Qi et al. (2018), using the approach of Tan et al. (2019).3 What knowledge do we represent? Each source embeds specialised knowledge to assess language relatedness. The KB vectors can measure typological similarity, whereas task-learned embeddings correlates with other kinds of language relationships (e.g. genetic) (Bjerva et al., 2019b). To analyse whether each kind of knowledge is induced with SVCCA, we assess the tasks of typological feature prediction (§4) and reconstruction of a language phylogeny (§5). What is the benefit for multilingual NMT (and NLP)? Language-level representations can evaluate the di"
2020.emnlp-main.187,P17-1049,0,0.0476411,"), there is a positive correlation between the language distances in a phylogenetic tree and a pairwise distance-matrix of task-learned representations. Our goal therefore 4 In other words, for SVCCA, it is difficult to deal with the noise provided in the learned embeddings. In Figures 6a and 6b of the Appendix, we observe noisy agglomerations in the dendrograms (obtained by clustering different language representations), which is preserved after the fusing with the KB vectors through SVCCA as we can see in Fig. 6c) Inference of a phylogenetic tree Experimental design. Based on previous work (Rabinovich et al., 2017), we take a tree of 17 IndoEuropean languages (Serva and Petroni, 2008) as a Gold Standard (GS), which is shown in Figure 1a.5 We also use agglomerative clustering with variance minimisation (Ward Jr, 1963) as linkage, but we employ cosine similarity as Bjerva et al. (2019b). We also consider a concatenation (⊕) of the KB and NMT-learned views as a baseline. It is essential to highlight that none of the NMTlearned and ⊕ vectors have all the 17 language entries of the GS. Therefore, we can already see one of the significant advantages of the SVCCA vectors, as we are able to represent “unknown”"
2020.emnlp-main.187,W18-6327,0,0.0488129,"we showed that the knowledge and language relationship encoded in both sources is preserved in the combined representation. Moreover, our approach offers important advantages because we can evaluate projected languages with entries in only one of the views and can easily extend the language coverage. The benefits are noticeable in multilingual NMT tasks, like language clustering and ranking related languages for multilingual transfer. We plan to study how to deeply incorporate our typologically-enriched embeddings in multilingual NMT, where there are promising avenues in parameter selection (Sachan and Neubig, 2018) and generation (Platanios et al., 2018). Acknowledgments This work was supported by funding from the European Union’s Horizon 2020 research and innovation programme under grant agreements No 825299 (GoURMET) and the EPSRC fellowship grant EP/S001271/1 (MTStretch). Also, it was performed using resources provided by the Cambridge Service for Data Driven Discovery (CSD3) operated by the University of Cambridge Research Computing Service (http://www. csd3.cam.ac.uk/), provided by Dell EMC and Intel using Tier-2 funding from the Engineering and Physical Sciences Research Council (capital 2399 9 ht"
2020.emnlp-main.187,E17-3017,1,0.824589,"Missing"
2020.emnlp-main.187,P16-1162,1,0.180217,"o better evaluate the extensibility of clusters and because it is also used to train the L ANG R ANK model. The list of languages, set sizes and other details are included in Appendix A. Before preprocessing the text, we drop any sentences from the training sets which overlap with any of the test sets. Since we are building many-toEnglish multilingual systems, this is important, as any such overlap will bias the results. Model and training. Similar to Tan et al. (2019), we train small transformer models (Vaswani et al., 2017). We jointly learn 90k shared sub-words with the byte pair encoding (Sennrich et al., 2016) algorithm built in SentencePiece (Kudo and Richardson, 2018). We also oversample all the training data of the less-resourced languages in each cluster, and shuffle them proportionally in all batches. We use Nematus (Sennrich et al., 2017) only to extract the factored language embeddings from the TED-53 corpus (LT ). Besides, given the large number of experiments, we also choose the efficient Marian NMT (Junczys-Dowmunt et al., 2018) toolkit for training the rest of systems. With Marian NMT, we only use the basic pseudo-token setting for identifying the source language, as we did not need to r"
2020.emnlp-main.187,D19-1089,0,0.150491,"Missing"
2020.emnlp-main.187,P19-1583,0,0.0172706,"06 0.4 0.125 former, we observe that many of the smaller mul0.4 0.04 0.100 0.2 tilingual models outperform the translation accu0.2 2 6 10 14 18 2 6 10 14 18 2 6 10 14 18 2 6 10 14 18 racy of the massive system. The result suggests # clusters # #clusters clusters # clusters that the amount of data is not the most important confound for supporting multilingual transfer in a Figure 4: Silhouette analysis for the LT ∗ embeddings trained using an initial pseudo-token (left) and the LB low-resource language, which is aligned with the Bible vectors (right). Both cases present a downtrend literature (Wang and Neubig, 2019). curve with scores below 0.2. The hierarchies of LT ∗ Comparing the two ranking approaches, we oband LB are shown in Figures 6b and 6a (in the Apserve that SVCCA approximates the performance pendix), respectively. of L ANG R ANK in most of the cases. We note that L ANG R ANK prefers related languages with large datasets, as it only requires three candidates 7 Factors over initial pseudo-tokens to group around half a million training samples, whereas SVCCA suggests to include from three to We additionally argue that the configuration used ten languages to reach a similar amount of paral- to co"
2020.emnlp-main.187,E17-2102,0,\N,Missing
2020.emnlp-main.187,J19-3005,0,\N,Missing
2020.emnlp-main.187,D18-1039,0,\N,Missing
2020.emnlp-main.187,N19-1156,0,\N,Missing
2020.emnlp-main.187,Q16-1030,0,\N,Missing
2020.emnlp-main.187,D19-1167,0,\N,Missing
2020.emnlp-main.187,2012.eamt-1.60,0,\N,Missing
2020.emnlp-main.187,W18-0207,0,\N,Missing
2020.emnlp-main.6,W07-0718,1,0.764038,". The use of reverse-created test sets was not the only concern raised by Läubli et al. (2018) and Toral et al. (2018). Both used more context than the original sentence-level evaluation in Hassan et al. (2018), Läubli et al. (2018) now asking human judges to assess entire documents, and Toral et al. (2018) involving assessment of MT output sentences in the order that they appeared in original documents. Furthermore, in contrast to the use of Direct Assessment (Graham et al., 2016) by Hassan et al. (2018), both reassessments used relative ranking, a method formerly used in WMT for evaluation (Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016), but now abandoned, partly due to low inter-annotator agreement. Therefore, although both re-evaluations improved the methodology employed in two respects, by eliminating reverse-created test data and including more context, both potentially include other sources of inaccuracy, such as lack of reliability of human judges when human evaluation takes the form of relative ranking. Furthermore, Toral et al. (2018) employ Trueskill to reach the conclusion that the MT system in question has not achieved human performance, and alth"
2020.emnlp-main.6,W19-5301,1,0.910427,"Missing"
2020.emnlp-main.6,W08-0309,1,0.862244,"Missing"
2020.emnlp-main.6,W10-1703,1,0.865007,"Missing"
2020.emnlp-main.6,W12-3102,1,0.905186,"Missing"
2020.emnlp-main.6,W11-2103,1,0.87397,"Missing"
2020.emnlp-main.6,W19-5204,0,0.0608855,"18) and Toral et al. (2018) used only test data that originated in the source language. Inspired by this work, other authors considered the effect of the 50/50 set-up on evaluation using WMT data. Edunov et al. (2019) questioned whether improvements in performance due to backtranslation were just an artifact of the test set construction. They found that, whilst back-translation had a disproportionately large positive effect on BLEU for reverse-created test sets, human evaluation showed that back-translation did indeed provide robust improvements to MT for forwardcreated text. Related to this, Freitag et al. (2019) also showed BLEU to be misleading on the reversecreated part of the test sets, when analysing why their automatic post-editing (APE) method produced improved translations according to human evaluation, but not according to BLEU. Given the concern in the community about using reversecreated test sets, the organisers of the WMT19 news translation task used only forward-created sentences in all their test sets (Barrault et al., 2019). In this current paper we provide detailed evidence to justify this decision. We note that Zhang and Toral (2019) also provide analysis of the effect of reverse-cre"
2020.emnlp-main.6,J12-4004,0,0.0612623,"Missing"
2020.emnlp-main.6,D18-1512,0,0.115616,"Missing"
2020.emnlp-main.6,P02-1040,0,0.115043,"ems participating in WMT-15 to WMT-18, as well as differences in human DA scores for systems participating in WMT-17 to WMT-18. The absence of systems in the upper-left and lower-right quadrants reassuringly shows that although extreme changes in BLEU and human scores do occur when test set creation direction is altered, the changes are at least somewhat systematic in the sense that when a difference in scores occurs (a drop or increase BLEU Besides human evaluation, the performance of MT systems is often measured using automatic metrics, the most common of which remains to be the BLEU score (Papineni et al., 2002). Figure 2 shows a box plot of absolute differences in BLEU scores for systems (reverse BLEU − forward BLEU) participating in WMT news translation tasks from 2015 to 2018. Counter expectation there is a clear mix of positive and negative BLEU score differences for several language pairs. Comparison of BLEU scores is not as straightforward as human evaluation however, and there are further consideration to be made before drawing conclusions from the mix of positive and negative absolute BLEU score differences described above. For example, the fact that splitting the test 76 Kendall 1.0 1.0 1.0"
2020.emnlp-main.6,W18-6312,0,0.155882,"of a human evaluation previously criticized for including reverse-created test data that claimed human parity of Chinese to English MT. We reveal insights into additional potential sources of inaccuracy of conclusions beyond the presence of translationese with the aim of preventing future inaccuracies. 2 spect to human evaluation, without considering its differing effect on automatic evaluation. Also, they do not consider the problem of statistical power in human evaluation, which we raise below. The use of reverse-created test sets was not the only concern raised by Läubli et al. (2018) and Toral et al. (2018). Both used more context than the original sentence-level evaluation in Hassan et al. (2018), Läubli et al. (2018) now asking human judges to assess entire documents, and Toral et al. (2018) involving assessment of MT output sentences in the order that they appeared in original documents. Furthermore, in contrast to the use of Direct Assessment (Graham et al., 2016) by Hassan et al. (2018), both reassessments used relative ranking, a method formerly used in WMT for evaluation (Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016), but now abandoned, p"
2020.emnlp-main.6,W19-5208,0,0.182034,"nts to MT for forwardcreated text. Related to this, Freitag et al. (2019) also showed BLEU to be misleading on the reversecreated part of the test sets, when analysing why their automatic post-editing (APE) method produced improved translations according to human evaluation, but not according to BLEU. Given the concern in the community about using reversecreated test sets, the organisers of the WMT19 news translation task used only forward-created sentences in all their test sets (Barrault et al., 2019). In this current paper we provide detailed evidence to justify this decision. We note that Zhang and Toral (2019) also provide analysis of the effect of reverse-created test sets on WMT evaluation campaigns. However they focus only on the effect of translationese with re73 REV FWD tion of statistical power. In most MT human evaluations, it is not feasible to evaluate the full test set of sentences for all systems and it is common to instead evaluate a sample of translations, usually drawn at random from the test data. In current WMT evaluations, for example, translations of all test sentences produced by all participating systems are pooled and a random sample is humanevaluated. This method ensures that"
2020.emnlp-main.615,N19-1071,1,0.849973,"incorporate prior knowledge into NMT. Zhang et al. (2017) exploit linguistic real-valued features, such as dictionaries or length ratios, to construct the distribution for regularizing the TM’s posteriors. Recently, Ren et al. (2019) used posterior regularization for unsupervised NMT, by employing an SMT model, which is robust to noisy data, as a prior over a neural TM to guide it in the iterative back-translation process. Finally, LMs have been used in a similar fashion as priors over latent text sequences in discrete latent variable models (Miao and Blunsom, 2016; Havrylov and Titov, 2017; Baziotis et al., 2019). 7 Conclusions In this work, we present a simple approach for incorporating knowledge from monolingual data to NMT . Specifically, we use a LM trained on targetside monolingual data, to regularize the output distributions of a TM. This method is more efficient than alternative approaches that used pretrained LM s, because it is not required during inference. Also, we avoid the translation errors introduced by LM -fusion, because the TM is able to deviate from the prior when needed. We empirically show that while this method works by simply changing the training objective, it achieves better r"
2020.emnlp-main.615,J93-2003,0,0.117719,"Missing"
2020.emnlp-main.615,D16-1139,0,0.0250432,"et al. (2019), GPT-2; Radford et al. (2019)), without compromising speed or efficiency. 3.1 Relation to Knowledge Distillation The regularization term in Eq. (1) resembles knowledge distillation (KD) (Ba and Caruana, 2014; Bucila et al., 2006; Hinton et al., 2015), where the soft output probabilities of a big teacher model are used to train a small compact student model, by minhard target label smoothing language model Figure 1: Targets with LS and LM-prior. imizing their DKL . However, in standard KD the teacher is trained on the same task as the student, like in KD for machine translation (Kim and Rush, 2016). However, the proposed LM-prior is trained on a different task that requires only monolingual data, unlike TM teachers that require parallel data. We exploit this connection to KD and following Hinton et al. (2015) we use a softmaxtemperature parameter τ ≥ 1 to control the smoothi /τ ) ness of the output distributions pi = Pexp(s , j exp(sj /τ )) where si is the un-normalized score of each word i (i.e., logit). Higher values of τ produce smoother distributions. Intuitively, this controls how much information encoded in the tail of the LM’s distributions, we expose to the TM. Specifically, a w"
2020.emnlp-main.615,W18-6319,0,0.0133939,"r the “Base + Prior (3M)” model. 4.1 Experiments We compare the proposed LM-prior with other approaches that incorporate a pretrained LM or regularize the outputs of the TM. First, we consider a vanilla NMT baseline without LS. Next, we compare with fusion techniques, namely shallow-fusion (Gulcehre et al., 2015) and POSTNORM (Stahlberg et al., 2018), which in the original paper outperformed other fusion methods. We also separately compare with label smoothing (LS), because it is another regularization method that uses soft targets. We report detokenized case-sensitive BLEU using sacre- BLEU (Post, 2018)4 , and decode with beam search of size 5. The LMs are fixed during training for both POSTNORM and the prior. We tune the hyper-parameters of each method on the DE→EN dev-set. We set the interpolation weight for shallow-fusion to β=0.1, the smoothing parameter for LS to α = 0.1. For the LM-prior we set the regularization weight to λ=0.5 and the temperature for LKL to τ =2. 4.2 Results First, we use in all methods LMs trained on the same amount of monolingual data, which is 3M sentences. We used the total amount of available Turkish monolingual data (3M) as the lowest common denominator. This i"
2020.emnlp-main.615,E17-2025,0,0.0342767,"scores for LMs trained on each language’s monolingual data, computed on a small heldout validation set per language. TM s. Table 2 lists all their hyperparameters. For the TMs we found that constraining their capacity and applying strong regularization was crucial, otherwise they suffered from over-fitting. We also found that initializing all weights with glorotuniform (Glorot and Bengio, 2010) initialization and using pre-norm residual connections (Xiong et al., 2020; Nguyen and Salazar, 2019), improved stability. We also tied the embedding and the output (projection) layers of the decoders (Press and Wolf, 2017; Inan et al., 2017). We optimized our models with Adam (Kingma and Ba, 2015) with a learning rate of 0.0002 and a linear warmup for the first 8K steps, followed by inverted squared decay and with mini-batches with 5000 tokens per batch. We evaluated each model on the dev set every 5000 batches, by decoding using greedy sampling, and stopped training if the BLEU score did not increase after 10 iterations. For the LM training we followed the same optimization process as for the TMs. However, we use Transformer-large configuration, in order to obtain a powerful LM-prior. Crucially, we did not ap"
2020.emnlp-main.615,W17-3204,0,0.0284764,"e LM can be viewed as teaching the TM about the target language. The proposed approach does not compromise decoding speed, because the LM is used only at training time, unlike previous work that requires it during inference. We present an analysis on the effects that different methods have on the distributions of the TM. Results on two low-resource machine translation datasets show clear improvements even with limited monolingual data. 1 Introduction Neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) relies heavily on large parallel corpora (Koehn and Knowles, 2017) and needs careful hyperparameter tuning, in order to work in low-resource settings (Sennrich and Zhang, 2019). A popular approach for addressing data scarcity is to exploit abundant monolingual corpora via data augmentation techniques, such as back-translation (Sennrich et al., 2016). Although back-translation usually leads to significant performance gains (Hoang et al., 2018), it requires training separate models and expensive translation of large amounts of monolingual data. However, when faced with lack of training data, a more principled approach is to consider exploiting prior informatio"
2020.emnlp-main.615,D18-2012,0,0.0225969,". As monolingual data for English and German we use the News Crawls 2016 articles (Bojar et al., 2016) and for Turkish we concatenate all the available News Crawls data from 2010-2018, which contain 3M sentences. For English and German we subsample 3M sentences to match the Turkish data, as well as 30M to measure the effect of stronger LM s. We remove sentences longer than 50 words. Pre-processing We perform punctuation normalization and truecasing and remove pairs, in which either of the sentences has more than 60 words or length ratio over 1.5. The text is tokenized with sentencepiece (SPM; Kudo and Richardson (2018)) with the “unigram” model. For each language we learn a separate SPM model with 16K symbols, trained on its respective side of the parallel data. For English, we train SPM on the concatenation of the English-side of the training data from each dataset, in order to have a single English vocabulary and be able to re-use the same LM. Model Configuration In all experiments, we use the Transformer architecture for both the LMs and 1 2 http://www.statmt.org/wmt18/translation-task.html http://opus.nlpl.eu/SETIMES2.php value TM LM 512 1024 6 8 0.3 1024 4096 6 16 0.3 Table 2: Hyperparameters of the TM"
2020.emnlp-main.615,D15-1166,0,0.100369,"Missing"
2020.emnlp-main.615,D16-1031,0,0.0294066,"roaches that have used posterior regularization to incorporate prior knowledge into NMT. Zhang et al. (2017) exploit linguistic real-valued features, such as dictionaries or length ratios, to construct the distribution for regularizing the TM’s posteriors. Recently, Ren et al. (2019) used posterior regularization for unsupervised NMT, by employing an SMT model, which is robust to noisy data, as a prior over a neural TM to guide it in the iterative back-translation process. Finally, LMs have been used in a similar fashion as priors over latent text sequences in discrete latent variable models (Miao and Blunsom, 2016; Havrylov and Titov, 2017; Baziotis et al., 2019). 7 Conclusions In this work, we present a simple approach for incorporating knowledge from monolingual data to NMT . Specifically, we use a LM trained on targetside monolingual data, to regularize the output distributions of a TM. This method is more efficient than alternative approaches that used pretrained LM s, because it is not required during inference. Also, we avoid the translation errors introduced by LM -fusion, because the TM is able to deviate from the prior when needed. We empirically show that while this method works by simply cha"
2020.emnlp-main.615,D17-1039,0,0.0238213,"-fusion, but with the LM used also during training, instead of used just in inference, and interpolating with λ=1. Fusion methods face the same computational limitation as noisy channel, since the LM needs to be used during inference. Also, probability interpolation methods, such as shallow fusion or POSTNORM , use a fixed weight for all time-steps, which can lead to translation errors. Gated fusion (Gulcehre et al., 2015; Sriram et al., 2018) is more flexible, but requires changing the network architecture. Other Approaches Transfer-learning is another approach for exploiting pretrained LMs. Ramachandran et al. (2017), first proposed to use LMs trained on monolingual corpora to initialize the encoder and decoder of a TM. Skorokhodov et al. (2018) extended this idea to Transformer architectures (Vaswani et al., 2017). This approach requires the TM to have identical architecture to the LM, which can be a limitation if the LM is huge. Domhan and Hieber (2017) used language modeling as extra signal, by training the decoder of a TM also as a LM on target-side monolingual data. Sennrich et al. (2016) replaced the source with a NULL token, while training on monolingual data. Both, reported mixed results, with mar"
2020.emnlp-main.615,P16-1009,1,0.881381,"ave on the distributions of the TM. Results on two low-resource machine translation datasets show clear improvements even with limited monolingual data. 1 Introduction Neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) relies heavily on large parallel corpora (Koehn and Knowles, 2017) and needs careful hyperparameter tuning, in order to work in low-resource settings (Sennrich and Zhang, 2019). A popular approach for addressing data scarcity is to exploit abundant monolingual corpora via data augmentation techniques, such as back-translation (Sennrich et al., 2016). Although back-translation usually leads to significant performance gains (Hoang et al., 2018), it requires training separate models and expensive translation of large amounts of monolingual data. However, when faced with lack of training data, a more principled approach is to consider exploiting prior information. Language models (LM) trained on target-side monolingual data have been used for years as priors in statistical machine translation (SMT) (Brown et al., 1993) via the noisy channel model. This approach has been adopted to NMT, with the neural noisy channel (Yu et al., 2017; Yee et a"
2020.emnlp-main.615,P19-1021,0,0.102559,"coding speed, because the LM is used only at training time, unlike previous work that requires it during inference. We present an analysis on the effects that different methods have on the distributions of the TM. Results on two low-resource machine translation datasets show clear improvements even with limited monolingual data. 1 Introduction Neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) relies heavily on large parallel corpora (Koehn and Knowles, 2017) and needs careful hyperparameter tuning, in order to work in low-resource settings (Sennrich and Zhang, 2019). A popular approach for addressing data scarcity is to exploit abundant monolingual corpora via data augmentation techniques, such as back-translation (Sennrich et al., 2016). Although back-translation usually leads to significant performance gains (Hoang et al., 2018), it requires training separate models and expensive translation of large amounts of monolingual data. However, when faced with lack of training data, a more principled approach is to consider exploiting prior information. Language models (LM) trained on target-side monolingual data have been used for years as priors in statisti"
2020.emnlp-main.615,W18-2205,0,0.0156839,"ce the same computational limitation as noisy channel, since the LM needs to be used during inference. Also, probability interpolation methods, such as shallow fusion or POSTNORM , use a fixed weight for all time-steps, which can lead to translation errors. Gated fusion (Gulcehre et al., 2015; Sriram et al., 2018) is more flexible, but requires changing the network architecture. Other Approaches Transfer-learning is another approach for exploiting pretrained LMs. Ramachandran et al. (2017), first proposed to use LMs trained on monolingual corpora to initialize the encoder and decoder of a TM. Skorokhodov et al. (2018) extended this idea to Transformer architectures (Vaswani et al., 2017). This approach requires the TM to have identical architecture to the LM, which can be a limitation if the LM is huge. Domhan and Hieber (2017) used language modeling as extra signal, by training the decoder of a TM also as a LM on target-side monolingual data. Sennrich et al. (2016) replaced the source with a NULL token, while training on monolingual data. Both, reported mixed results, with marginal gains. 3 Language Model Prior We propose to move the LM out of the TM and use it as a prior over its decoder, by employing po"
2020.emnlp-main.615,W18-6321,0,0.282985,"oisy channel model. This approach has been adopted to NMT, with the neural noisy channel (Yu et al., 2017; Yee et al., 2019). However, neural noisy channel models face a computational challenge, because they model the “reverse translation probability” p(x|y). Specifically, they require multiple passes over the source sentence x as they generate the target sentence y, or sophisticated architectures to reduce the passes. LM s have also been used in NMT for reweighting the predictions of translation models (TM), or as additional context, via LM-fusion (Gulcehre et al., 2015; Sriram et al., 2018; Stahlberg et al., 2018). But, as the LM is required during decoding, it adds a significant computation overhead. Another challenge is balancing the TM and the LM, whose ratio is either fixed (Stahlberg et al., 2018) or requires changing the model architecture (Gulcehre et al., 2015; Sriram et al., 2018). In this work, we propose to use a LM trained on target-side monolingual corpora as a weakly informative prior. We add a regularization term, which drives the output distributions of the TM to be probable under the distributions of the LM. This gives flexibility to the TM, by enabling it to deviate from the LM when n"
2020.emnlp-main.615,D19-1571,0,0.0790326,"., 2016). Although back-translation usually leads to significant performance gains (Hoang et al., 2018), it requires training separate models and expensive translation of large amounts of monolingual data. However, when faced with lack of training data, a more principled approach is to consider exploiting prior information. Language models (LM) trained on target-side monolingual data have been used for years as priors in statistical machine translation (SMT) (Brown et al., 1993) via the noisy channel model. This approach has been adopted to NMT, with the neural noisy channel (Yu et al., 2017; Yee et al., 2019). However, neural noisy channel models face a computational challenge, because they model the “reverse translation probability” p(x|y). Specifically, they require multiple passes over the source sentence x as they generate the target sentence y, or sophisticated architectures to reduce the passes. LM s have also been used in NMT for reweighting the predictions of translation models (TM), or as additional context, via LM-fusion (Gulcehre et al., 2015; Sriram et al., 2018; Stahlberg et al., 2018). But, as the LM is required during decoding, it adds a significant computation overhead. Another cha"
2020.emnlp-main.615,P17-1139,0,0.0174501,"nto the future”. However, in our work we address a different problem (low-resource NMT) and have different motivation. Also, we consider auto-regressive LMs as priors, which have clear interpretation, unlike BERT that is not strictly a LM and requires bidirectional context. Note that, large pretrained LMs, such as BERT or GPT-2, have not yet achieved the transformative results in NMT that we observe in natural language understanding tasks (e.g., GLUE benchmark (Wang et al., 2019)). There are also other approaches that have used posterior regularization to incorporate prior knowledge into NMT. Zhang et al. (2017) exploit linguistic real-valued features, such as dictionaries or length ratios, to construct the distribution for regularizing the TM’s posteriors. Recently, Ren et al. (2019) used posterior regularization for unsupervised NMT, by employing an SMT model, which is robust to noisy data, as a prior over a neural TM to guide it in the iterative back-translation process. Finally, LMs have been used in a similar fashion as priors over latent text sequences in discrete latent variable models (Miao and Blunsom, 2016; Havrylov and Titov, 2017; Baziotis et al., 2019). 7 Conclusions In this work, we pre"
2020.emnlp-main.615,W16-2301,1,\N,Missing
2020.emnlp-main.615,N19-1423,0,\N,Missing
2020.emnlp-main.615,D19-5611,0,\N,Missing
2020.emnlp-main.615,D19-5603,0,\N,Missing
2020.findings-emnlp.230,N18-1008,0,0.0196101,"ng studies on ST used a cascade of separately trained ASR and MT systems (Ney, 1999). Despite its simplicity, this approach inevitably suffers from mistakes made by ASR models, and is error prone. Research in this direction often focuses on strategies capable of mitigating the mismatch between ASR output and MT input, such as representing ASR outputs with lattices (Saleem et al., 2004; Mathias and Byrne, 2006; Zhang et al., 2019a; Beck et al., 2019), injecting synthetic ASR errors for robust MT (Tsvetkov et al., 2014; Cheng et al., 2018) and differentiable cascade modeling (Kano et al., 2017; Anastasopoulos and Chiang, 2018; Sperber et al., 2019). In contrast to cascading, another option is to perform direct speech-to-text translation. Duong et al. (2016) and B´erard et al. (2016) employ the attentional encoder-decoder model (Bahdanau et al., 2015) for E2E ST without accessing any intermediate transcriptions. E2E ST opens the way to bridging the modality gap directly, but it is data-hungry, sample-inefficient and often underperforms cascade models especially in low-resource settings (Bansal et al., 2018). This led researchers to explore solutions ranging from efficient neural architecture design (Karita et al.,"
2020.findings-emnlp.230,N19-1006,0,0.0749518,"mproves model convergence; feature selection makes training more stable. Compared to other models, the curve of ST with AFS is much smoother, suggesting its better regularization effect. We then investigate the effect of training data size, and show the results in Figure 7. Overall, we do not observe higher data efficiency by feature selection on low-resource settings. But instead, our results suggest that feature selection delivers larger performance improvement when more training data is available. With respect to data efficiency, ASR pretraining seems to be more important (Figure 7, left) (Bansal et al., 2019; Stoian et al., 2020). Com2538 10 17.5 # Selected Features 20 BLEU BLEU 15.0 12.5 10.0 7.5 18 16 ST + ASR-PT ST + Fixed Rate ST+AFSt ST + AFSt,f 14 ST ST + ASR-PT 5.0 2.5 50000 12 50000 6 4 2 100000 150000 200000 # Training Samples MuST-C En-De. We split the original training data into non-overlapped five subsets, and train different models with accumulated subsets. Results are reported on the test set. Note that we perform ASR pretraining on the original dataset. λ = 0.5, k = 6. 2500 Frequency 6000 4000 2000 0.5 1.0 1.5 Attention Weight 2.0 0.6 AFSt AFSt,f Fixed Rate 1.5 2.0 0 10 20 30 40 50"
2020.findings-emnlp.230,P19-1284,1,0.874511,"Missing"
2020.findings-emnlp.230,D19-5304,0,0.0916558,"in tokenization and letter case. To ease future cross-paper comparison, we provide SacreBLEU (Post, 2018)4 for our models. 5 Related Work Speech Translation Pioneering studies on ST used a cascade of separately trained ASR and MT systems (Ney, 1999). Despite its simplicity, this approach inevitably suffers from mistakes made by ASR models, and is error prone. Research in this direction often focuses on strategies capable of mitigating the mismatch between ASR output and MT input, such as representing ASR outputs with lattices (Saleem et al., 2004; Mathias and Byrne, 2006; Zhang et al., 2019a; Beck et al., 2019), injecting synthetic ASR errors for robust MT (Tsvetkov et al., 2014; Cheng et al., 2018) and differentiable cascade modeling (Kano et al., 2017; Anastasopoulos and Chiang, 2018; Sperber et al., 2019). In contrast to cascading, another option is to perform direct speech-to-text translation. Duong et al. (2016) and B´erard et al. (2016) employ the attentional encoder-decoder model (Bahdanau et al., 2015) for E2E ST without accessing any intermediate transcriptions. E2E ST opens the way to bridging the modality gap directly, but it is data-hungry, sample-inefficient and often underperforms casc"
2020.findings-emnlp.230,P18-1163,0,0.0603518,"LEU (Post, 2018)4 for our models. 5 Related Work Speech Translation Pioneering studies on ST used a cascade of separately trained ASR and MT systems (Ney, 1999). Despite its simplicity, this approach inevitably suffers from mistakes made by ASR models, and is error prone. Research in this direction often focuses on strategies capable of mitigating the mismatch between ASR output and MT input, such as representing ASR outputs with lattices (Saleem et al., 2004; Mathias and Byrne, 2006; Zhang et al., 2019a; Beck et al., 2019), injecting synthetic ASR errors for robust MT (Tsvetkov et al., 2014; Cheng et al., 2018) and differentiable cascade modeling (Kano et al., 2017; Anastasopoulos and Chiang, 2018; Sperber et al., 2019). In contrast to cascading, another option is to perform direct speech-to-text translation. Duong et al. (2016) and B´erard et al. (2016) employ the attentional encoder-decoder model (Bahdanau et al., 2015) for E2E ST without accessing any intermediate transcriptions. E2E ST opens the way to bridging the modality gap directly, but it is data-hungry, sample-inefficient and often underperforms cascade models especially in low-resource settings (Bansal et al., 2018). This led researchers"
2020.findings-emnlp.230,D19-1223,0,0.0367555,"Missing"
2020.findings-emnlp.230,N19-1202,0,0.284852,"Missing"
2020.findings-emnlp.230,N16-1109,0,0.62118,"−0.4 play P L is EY1 IH1 Z not NAA1 just JH IH0 S T CH child ’s AY1 L DT S games G EY1 M Z Figure 1: Example illustrating our motivation. We plot the amplitude and frequency spectrum of an audio segment (top), paired with its time-aligned words and phonemes (bottom). Information inside an audio stream is not uniformly distributed. We propose to dynamically capture speech features corresponding to informative signals (red rectangles) to improve ST. Introduction End-to-end (E2E) speech translation (ST), a paradigm that directly maps audio to a foreign text, has been gaining popularity recently (Duong et al., 2016; B´erard et al., 2016; Bansal et al., 2018; Di Gangi et al., 2019; Wang et al., 2019). Based on the attentional encoder-decoder framework (Bahdanau et al., 2015), it optimizes model parameters under direct translation supervision. This end-toend paradigm avoids the problem of error propagation that is inherent in cascade models where an automatic speech recognition (ASR) model and 1 We release our source code at https://github. com/bzhangGo/zero. a machine translation (MT) model are chained together. Nonetheless, previous work still reports that E2E ST delivers inferior performance compared t"
2020.findings-emnlp.230,L18-1001,0,0.276493,"hat correspond to git > 0, and pass them similarly as done with word embeddings to the ST encoder. We employ sinusoidal positional encoding to distinguish features at different positions. Except for the input to the ST encoder, our E2E ST follows the standard encoder-decoder translation model (MST in Eq. 10) and is optimized with LMLE alone as in Eq. 9. Intuitively, AFS bridges the gap between ASR output and MT input by selecting transcriptaligned speech features. 4 Experiments Datasets and Preprocessing We experiment with two benchmarks: the Augmented LibriSpeech dataset (LibriSpeech En-Fr) (Kocabiyikoglu et al., 2018) and the multilingual MuST-C dataset (MuSTC) (Di Gangi et al., 2019). LibriSpeech En-Fr is 3 Other candidate gating models, like linear mapping upon mean-pooled encoder outputs, delivered worse performance in our preliminary experiments. collected by aligning e-books in French with English utterances of LibriSpeech, further augmented with French translations offered by Google Translate. We use the 100 hours clean training set for training, including 47K utterances to train ASR models and double the size for ST models after concatenation with the Google translations. We report results on the te"
2020.findings-emnlp.230,P07-2045,0,0.0118427,"mon test set, whose size ranges from 2502 (Es) to 2641 (De) utterances. For all datasets, we extract 40-dimensional logMel filterbanks with a step size of 10ms and window size of 25ms as the acoustic features. We expand these features with their first and second-order derivatives, and stabilize them using mean subtraction and variance normalization. We stack the features corresponding to three consecutive frames without overlapping to the left, resulting in the final 360-dimensional acoustic input. For transcriptions and translations, we tokenize and truecase all the text using Moses scripts (Koehn et al., 2007). We train subword models (Sennrich et al., 2016) on each dataset with a joint vocabulary size of 16K to handle rare words, and share the model for ASR, MT and ST. We train all models without removing punctuation. Model Settings and Baselines We adopt the Transformer architecture (Vaswani et al., 2017) for all tasks, including MASR (Eq. 6), MAFS (Eq. 8) and MST (Eq. 10). The encoder and decoder consist of 6 identical layers, each including a self-attention sublayer, a cross-attention sublayer (decoder alone) and a feedforward sublayer. We employ the base setting for experiments: hidden size d"
2020.findings-emnlp.230,P02-1040,0,0.106095,"Missing"
2020.findings-emnlp.230,W18-6319,0,0.0501918,"Missing"
2020.findings-emnlp.230,P16-1162,1,0.24597,"to 2641 (De) utterances. For all datasets, we extract 40-dimensional logMel filterbanks with a step size of 10ms and window size of 25ms as the acoustic features. We expand these features with their first and second-order derivatives, and stabilize them using mean subtraction and variance normalization. We stack the features corresponding to three consecutive frames without overlapping to the left, resulting in the final 360-dimensional acoustic input. For transcriptions and translations, we tokenize and truecase all the text using Moses scripts (Koehn et al., 2007). We train subword models (Sennrich et al., 2016) on each dataset with a joint vocabulary size of 16K to handle rare words, and share the model for ASR, MT and ST. We train all models without removing punctuation. Model Settings and Baselines We adopt the Transformer architecture (Vaswani et al., 2017) for all tasks, including MASR (Eq. 6), MAFS (Eq. 8) and MST (Eq. 10). The encoder and decoder consist of 6 identical layers, each including a self-attention sublayer, a cross-attention sublayer (decoder alone) and a feedforward sublayer. We employ the base setting for experiments: hidden size d = 512, attention head 8 and feedforward size 2048"
2020.findings-emnlp.230,Q19-1020,0,0.0126597,"f separately trained ASR and MT systems (Ney, 1999). Despite its simplicity, this approach inevitably suffers from mistakes made by ASR models, and is error prone. Research in this direction often focuses on strategies capable of mitigating the mismatch between ASR output and MT input, such as representing ASR outputs with lattices (Saleem et al., 2004; Mathias and Byrne, 2006; Zhang et al., 2019a; Beck et al., 2019), injecting synthetic ASR errors for robust MT (Tsvetkov et al., 2014; Cheng et al., 2018) and differentiable cascade modeling (Kano et al., 2017; Anastasopoulos and Chiang, 2018; Sperber et al., 2019). In contrast to cascading, another option is to perform direct speech-to-text translation. Duong et al. (2016) and B´erard et al. (2016) employ the attentional encoder-decoder model (Bahdanau et al., 2015) for E2E ST without accessing any intermediate transcriptions. E2E ST opens the way to bridging the modality gap directly, but it is data-hungry, sample-inefficient and often underperforms cascade models especially in low-resource settings (Bansal et al., 2018). This led researchers to explore solutions ranging from efficient neural architecture design (Karita et al., 2019; Di Gangi et al.,"
2020.findings-emnlp.230,E14-1065,0,0.0202094,"ison, we provide SacreBLEU (Post, 2018)4 for our models. 5 Related Work Speech Translation Pioneering studies on ST used a cascade of separately trained ASR and MT systems (Ney, 1999). Despite its simplicity, this approach inevitably suffers from mistakes made by ASR models, and is error prone. Research in this direction often focuses on strategies capable of mitigating the mismatch between ASR output and MT input, such as representing ASR outputs with lattices (Saleem et al., 2004; Mathias and Byrne, 2006; Zhang et al., 2019a; Beck et al., 2019), injecting synthetic ASR errors for robust MT (Tsvetkov et al., 2014; Cheng et al., 2018) and differentiable cascade modeling (Kano et al., 2017; Anastasopoulos and Chiang, 2018; Sperber et al., 2019). In contrast to cascading, another option is to perform direct speech-to-text translation. Duong et al. (2016) and B´erard et al. (2016) employ the attentional encoder-decoder model (Bahdanau et al., 2015) for E2E ST without accessing any intermediate transcriptions. E2E ST opens the way to bridging the modality gap directly, but it is data-hungry, sample-inefficient and often underperforms cascade models especially in low-resource settings (Bansal et al., 2018)."
2020.findings-emnlp.230,2020.acl-main.344,0,0.240923,"ription. To improve speech encoding, we apply logarithmic penalty on attention to enforce short-range dependency (Di Gangi et al., 2019) and use trainable positional embedding with a maximum length of 2048. Apart from LMLE , we augment the training objective with the connectionist temporal classification (Graves et al., 2006, CTC) loss LCTC as in Eq. 5. Note η = 1 − γ. The CTC loss is applied to the encoder outputs, guiding them to align with their corresponding transcription (sub)words and improving the encoder’s robustness (Karita et al., 2019). Following previous work (Karita et al., 2019; Wang et al., 2020), we set γ to 0.3. (7)  3. Train ST model with pretrained and frozen ASR and AFS submodules until convergence:  ing encoder and decoder respectively. F (·) denotes the AFS approach, and F E means freezing the ASR encoder and the AFS module during training. Note that our framework puts no constraint on the architecture of the encoder and decoder in any task, although we adopt the multi-head dot-product attention network (Vaswani et al., 2017) for our experiments. Note that our model only requires pair-wise training corpora, (X, Y ) for ASR, and (X, Z) for ST. Lt0 (X) = n X i=1 1 − p(git = 0|"
2020.findings-emnlp.230,2021.findings-acl.255,1,0.952716,"Missing"
2020.findings-emnlp.230,P19-1649,0,0.290045,"zero. a machine translation (MT) model are chained together. Nonetheless, previous work still reports that E2E ST delivers inferior performance compared to cascade methods (Niehues et al., 2019). We study one reason for the difficulty of training E2E ST models, namely the uneven spread of information in the speech signal, as visualized in Figure 1, and the consequent difficulty of extracting informative features. Features corresponding to uninformative signals, such as pauses or noise, increase the input length and bring in unmanageable noise for ST. This increases the difficulty of learning (Zhang et al., 2019b; Na et al., 2019) and reduces translation performance. In this paper, we propose adaptive feature selection (AFS) for ST to explicitly eliminate uninformative features. Figure 2 shows the overall architecture. We employ a pretrained ASR encoder to induce contextual speech features, followed by an ST encoder bridging the gap between speech and translation modalities. AFS is inserted in-between them to select a subset of features for ST encoding (see red rectangles in Figure 1). To ensure that the selected features are well-aligned to transcriptions, we pretrain AFS on ASR. AFS estimates the i"
2020.findings-emnlp.375,N15-1124,1,0.853486,"verage ratings attributed to translations sampled from large test sets, and although such methodology does allow application of statistical significance testing to identify potentially meaningful differences in system performance, they do not provide any insight into the reasons behind a significantly higher score or the degree to which systems perform better when translating individual segments. Furthermore, DA score distributions produced in the human evaluation of the news task are based on individual DA scores that alone cannot be relied upon to reflect the quality of individual segments (Graham et al., 2015). Past work, has however provided a means of running a DA human evaluation in such a way that DA scores accurately reflect the performance of a system on a given individual segment (Graham 1 DA is also used in other task evaluations such as Video Captioning and Multilingual Surface Realisation (Awad et al., 2019; Graham et al., 2018; Mille et al., 2019). 4199 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4199–4207 c November 16 - 20, 2020. 2020 Association for Computational Linguistics et al., 2015). This method comes with the trade-off of requiring substantially"
2020.findings-emnlp.375,W19-5301,1,0.889077,"Missing"
2020.findings-emnlp.375,D18-1512,0,0.0479597,"Missing"
2020.findings-emnlp.375,W19-5302,1,0.82035,"Missing"
2020.findings-emnlp.375,D19-6301,1,0.787469,"etter when translating individual segments. Furthermore, DA score distributions produced in the human evaluation of the news task are based on individual DA scores that alone cannot be relied upon to reflect the quality of individual segments (Graham et al., 2015). Past work, has however provided a means of running a DA human evaluation in such a way that DA scores accurately reflect the performance of a system on a given individual segment (Graham 1 DA is also used in other task evaluations such as Video Captioning and Multilingual Surface Realisation (Awad et al., 2019; Graham et al., 2018; Mille et al., 2019). 4199 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4199–4207 c November 16 - 20, 2020. 2020 Association for Computational Linguistics et al., 2015). This method comes with the trade-off of requiring substantially more repeat assessments per segment than the test set level evaluation generally run, for example, to evaluate all primary submissions in the WMT news task. In this work we demonstrate how this method has the potential to be employed as a secondary method of evaluation in WMT tasks for a smaller subset of systems to provide segment-level insight into w"
2020.findings-emnlp.375,W19-5333,0,0.0132682,"++ + ++ + + ++ + + ++ + + + + ++ + + ++ + + + 0.000 0 25 50 adequacy 75 100 20 40 60 80 100 Facebook-FAIR (src) Figure 3: Density plot of sample of 540 accurate segment-level DA scores for German to English translation new translation for the top-performing system, FACEBOOK -FAIR, in WMT-19 versus the human translator where in the official results the system beat human performance; Human denotes evaluation of segments translated by the creator of the standard WMT reference translations Figures 1, 2 and 3 include density plots for human translation and the top-performing FACEBOOK -FAIR system (Ng et al., 2019) for the same 540 translated segments from WMT-19 for the three language pairs we investigate. For German to English and English to German translation in Figures 2 and 3 a similar pattern emerges in terms of comparison of human and machine-translated segments, as for both a slightly larger proportion of FACEBOOK -FAIR translations are scored high compared to the human translator – as can be seen from the higher red peak close to the extreme right of both plots indicating that the machine produces a marginally higher number of translations with higher levels of adequacy. For English to Russian"
2020.findings-emnlp.375,W18-6312,0,0.016085,"ments. 2 Related Work Over the past number of years, machine translation has been biting at the heels of human translation for a small number of language pairs. Beginning with the first claims that machines have surpassed human quality of translation for Chinese to English news text, conclusions received with some skepticism and even controversy (Hassan et al., 2018), as claims of human performance resulted in re-evaluations that scrutinized the methodology applied, highlighting the influence of reverse-created test data and lack of wider document context in evaluations (L¨aubli et al., 2018; Toral et al., 2018). Despite re-evaluations taking somewhat more care to eliminate such sources of inaccuracies, they additionally included some potential issues of their own, such as employing somewhat outdated human evaluation methodologies, non-standard methods of statistical significance testing and lack of planning evaluations in terms of statistical power. Graham et al. (2019, 2020), on the other hand re-run the evaluation, identify and fix remaining causes of error, and subsequently confirm that, on the overall level of the test set, with increased scrutiny on evaluation procedures, conclusions of human p"
2020.findings-emnlp.375,2020.emnlp-main.6,1,0.816952,"Missing"
2020.iwltp-1.3,W19-6723,1,0.787839,"Missing"
2020.iwltp-1.3,2020.iwltp-1.7,1,0.806176,"Missing"
2020.iwltp-1.7,2012.eamt-1.60,0,0.030274,"(Ha et al., 2016; Ha et al., 2017; Johnson et al., 2017) where a single system is able to translate from and to multiple languages. This approach has many advantages: • It leverages the large availability of multi-way, multilingual corpora in European languages such as the corpus of European Parliament documents and speeches’ transcription (Europarl) (Koehn, 2005), the collection of legislative texts of the European Union (JRCAcquis) (Steinberger et al., 2006) or the texts extracted from the document of European Constitution (EUconst) as well as the WIT3 corpus extracted from TED talks (TED) (Cettolo et al., 2012). • It uses the multilingual information to help improve the translation of the language pairs which are considered as low-resource languages in some domains. Our research has shown that our multilingual translation system maintains parity with the translation quality of systems trained on individual language pairs on the same small amount of data. • In practice, having a small number of multilingual systems to cover all language pairs significantly reduces the development and deployment efforts compared with having one system for each pair. 5. While each of the components (ASR, punctuation, M"
2020.iwltp-1.7,2015.iwslt-papers.8,1,0.871161,"Missing"
2020.iwltp-1.7,Q17-1024,0,0.0913848,"Missing"
2020.iwltp-1.7,2005.mtsummit-papers.11,0,0.143289,"f that sentence is displayed and never changed again by the update mechanism, to under 5 seconds. Machine Translation System With the ultimate goal of featuring a translation system for all EUROSAI languages, we opt for the multilingual approach (Ha et al., 2016; Ha et al., 2017; Johnson et al., 2017) where a single system is able to translate from and to multiple languages. This approach has many advantages: • It leverages the large availability of multi-way, multilingual corpora in European languages such as the corpus of European Parliament documents and speeches’ transcription (Europarl) (Koehn, 2005), the collection of legislative texts of the European Union (JRCAcquis) (Steinberger et al., 2006) or the texts extracted from the document of European Constitution (EUconst) as well as the WIT3 corpus extracted from TED talks (TED) (Cettolo et al., 2012). • It uses the multilingual information to help improve the translation of the language pairs which are considered as low-resource languages in some domains. Our research has shown that our multilingual translation system maintains parity with the translation quality of systems trained on individual language pairs on the same small amount of"
2020.iwltp-1.7,W19-5337,1,0.607584,"al conferences and remote conferencing) is described in the respective sections below. Our multilingual systems are based on the neural sequenceto-sequence with attention framework (Bahdanau et al., 2014) and shares the internal representation across languages (Pham et al., 2017). At present, we have one manyto-many Transformer model (Vaswani et al., 2017) providing translation between all pairings of 36 languages, along with several specialized models focused on subsets of languages, in particular the project’s primary languages of English, Czech, and German, see i.a. (Popel and Bojar, 2018; Popel et al., 2019). The resulting multilingual models after training can be used immediately in deployment or can go through a language adaptation step. This language adaptation is simply continuing training the multilingual model on the data of a specific language pair for a few epochs in order to improve the individual translation performance. While we need to do this language adaptation for every single language pair in our system, it is a trivial job since we could automate the process with the same settings and it takes only a little of time and computing resources to reach decent performances. 4.2. Practi"
2020.iwltp-1.7,steinberger-etal-2006-jrc,0,0.163597,"Missing"
2020.wmt-1.1,2020.nlpcovid19-2.5,1,0.796341,"tails of the evaluation. 4.1.1 Covid Test Suite TICO-19 The TICO-19 test suite was developed to evaluate how well can MT systems handle the newlyemerged topic of COVID-19. Accurate automatic translation can play an important role in facilitating communication in order to protect at-risk populations and combat the infodemic of misinformation, as described by the World Health Organization. The test suite has no corresponding paper so its authors provided an analysis of the outcomes directly here. The submitted systems were evaluated using the test set from the recently-released TICO-19 dataset (Anastasopoulos et al., 2020). The dataset provides manually created translations of COVID19 related data. The test set consists of PubMed articles (678 sentences from 5 scientific articles), patient-medical professional conversations (104 sentences), as well as related Wikipedia articles (411 sentences), announcements (98 sentences from Wikisource), and news items (67 sentences from Wikinews), for a total of 2100 sentences. Table 15 outlines the BLEU scores by each submitted system in the English-to-X directions, also breaking down the results per domain. The analysis shows that some systems are significantly more prepar"
2020.wmt-1.1,2020.wmt-1.6,0,0.0647231,"AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua Universit"
2020.wmt-1.1,2020.wmt-1.38,0,0.0746945,"Missing"
2020.wmt-1.1,2020.wmt-1.54,1,0.802974,"Missing"
2020.wmt-1.1,W07-0718,1,0.671054,"Missing"
2020.wmt-1.1,W08-0309,1,0.762341,"Missing"
2020.wmt-1.1,W12-3102,1,0.500805,"Missing"
2020.wmt-1.1,2020.lrec-1.461,0,0.0795779,"Missing"
2020.wmt-1.1,2012.eamt-1.60,0,0.124643,"tted systems, for those where the authors provided such details. The training corpus for Inuktitut↔English is the recently released Nunavut Hansard Inuktitut– English Parallel Corpus 3.0 (Joanis et al., 2020). For the Japanese↔English tasks, we added several freely available parallel corpora to the training data. It includes JParaCrawl v2.0 (Morishita et al., 2020), a large web-based parallel corpus, Japanese-English Subtitle Corpus (JESC) (Pryzant et al., 2017), the Kyoto Free Translation Task (KFTT) corpus (Neubig, 2011), constructed from the Kyoto-related Wikipedia articles, and TED Talks (Cettolo et al., 2012). The monolingual data we provided was similar to last year’s, with a 2019 news crawl added to all the news corpora. In addition, we provided versions of the news corpora for Czech, English and German, with both the document and paragraph structure retained. In other words, we did not apply sentence splitting to these corpora, and we retained the document boundaries and text ordering of the originals. Training, development, and test data for Pashto↔English and Khmer↔English are shared with the Parallel Corpus Filtering Shared Task (Koehn et al., 2020). The training data mostly comes from OPUS"
2020.wmt-1.1,2020.wmt-1.3,0,0.0731913,"on Machine Translation (WMT20)1 was held online with EMNLP 2020 and hosted a number of shared tasks on various aspects of machine translation. This conference built on 14 previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; CallisonBurch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017, 2018; Barrault et al., 2019). This year we conducted several official tasks. We report in this paper on the news and similar translation tasks. Additional shared tasks are described in separate papers in these proceedings: • automatic post-editing (Chatterjee et al., 2020) • biomedical translation (Bawden et al., 2020b) • chat translation (Farajian et al., 2020) • lifelong learning (Barrault et al., 2020) 1 Makoto Morishita NTT Santanu Pal WIPRO AI Abstract 1 Philipp Koehn JHU http://www.statmt.org/wmt20/ 1 Proceedings of the 5th Conference on Machine Translation (WMT), pages 1–55 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics as “direct assessment”) that we explored in the previous years with convincing results in terms of the trade-off between annotation effort and reliable distinctions between systems. The primary objectives o"
2020.wmt-1.1,2020.wmt-1.8,0,0.0898111,"D D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Know"
2020.wmt-1.1,2009.freeopmt-1.3,0,0.088081,"ve (CONTRASTIVE) or primary (PRIMARY), and the BLEU, RIBES and TER results. The scores are sorted by BLEU. In general, primary systems tend to be better than contrastive systems, as expected, but there are some exceptions. This year we recived major number of participants for the case of Indo-Aryan language group NUST-FJWU NUST-FJWU system is an extension of state-of-the-art Transformer model with hierarchical attention networks to incorporate contextual information. During training the model used back-translation. Prompsit This team is participating with a rulebased system based on Apertium (Forcada et al., 2009-11). Apertium is a free/open-source platform for developing rule-based machine translation systems and language technology that was first released in 2005. Apertium is hosted in Github where both language data and code are licensed under the GNU GPL. It is a research and business platform with a very active community that loves small languages. Language pairs are at a very different level of development and output quality in the platform, depending on two main variables: how much funded or in-kind effort has 32 5.4 i.e. Hindi–Marathi (in both directions). We received 22 submissions from 14 te"
2020.wmt-1.1,2020.wmt-1.80,0,0.0933589,"airs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages. Chi-kiu Lo NRC Masaaki Nagata NTT Marcos Zampieri Rochester Institute of Technology metrics (Mathur et al., 2020) parallel corpus filtering (Koehn et al., 2020) quality estimation (Specia et al., 2020a) robustness (Specia et al., 2020b) unsupervised and very low-resource translation (Fraser, 2020) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We included 22 translation directions this year, with translation between English and each of Chinese, Czech, German and Russian, as well as French to and from German being repeated from last year, and English to and from Inuktitut, Japanese, Polish and Tamil being new for this year. Furthermore, English to and from Khmer and Pashto were included, using the same test sets as in the corpus filtering task. Th"
2020.wmt-1.1,W19-5204,0,0.0543621,"Missing"
2020.wmt-1.1,2020.emnlp-main.5,0,0.0410594,"luation of out-ofEnglish translations, HITs were generated using the same method as described for the SR+DC evaluation of into-English translations in Section 3.2.1 with minor modifications. Source-based DA allows to include human references in the evaluation as another system to provide an estimate of human performance. Human references were added to the pull of system outputs prior to sampling documents for tasks generation. If multiple references are available, which is the case for English→German (3 alternative reference translations, including 1 generated using the paraphrasing method of Freitag et al. (2020)) and English→Chinese (2 translations), each reference is assessed individually. Since the annotations are made by researchers and professional translators who ensure a betTable 11: Amount of data collected in the WMT20 manual document- and segment-level evaluation campaigns for bilingual/source-based evaluation out of English and nonEnglish pairs. et al., 2020; Laubli et al., 2020). It differs from SR+DC DA introduced in WMT19 (Bojar et al., 2019), and still used in into-English human evaluation this year, where a single segment from a document is provided on a screen at a time, followed by s"
2020.wmt-1.1,W18-3931,1,0.874637,"ese improvements to increased capacity (which allows increased sensitivity to long-range relations) of the models. 5 Similar Language Translation Most shared tasks at WMT (e.g. News, Biomedical) have historically dealt with translating texts from and to English. In recent years, we observed a growing interest in training systems to translate between languages other than English. This includes a number of papers applying MT to translate between pairs of closely-related languages, national language varieties, and dialects 27 of the same language (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018; Popovi´c et al., 2020). To address this topic, the first Similar Language Translation (SLT) shared task at WMT 2019 has been organized. It featured data from three pairs of closely-related languages from different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (IndoAryan languages). Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic, we organize, for the second time at WMT, this shared task to evaluate the performance of state-of-the-art translation systems on trans"
2020.wmt-1.1,2020.wmt-1.18,0,0.0913945,"2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ubiqus (Hernandez and Nguyen, 2020) University of Edinburgh (Bawden et al., 2020a; Germann, 2020) University of Edinburgh and Charles University (Germann et al., 2020) Université du Québec à Montréal (no associated paper) ByteDance AI Lab (Wu et al., 2020a) WeChat (Meng et al., 2020) Baseline System from Biomedical Task (Bawden et al., 2020b) American University of Beirut (no associated paper) Zoho Corporation (no associated paper) Table 6: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion"
2020.wmt-1.1,2020.wmt-1.43,0,0.0835067,"Missing"
2020.wmt-1.1,2020.wmt-1.9,0,0.0939415,"Missing"
2020.wmt-1.1,2020.wmt-1.19,0,0.0674131,"Missing"
2020.wmt-1.1,2009.mtsummit-btm.6,0,0.103443,"Missing"
2020.wmt-1.1,W13-2305,1,0.929934,"work which can be well applied to different translation. directions. Techniques used in the submitted systems include optional multilingual pre-training (mRASP) for low resource languages, very deep Transformer or dynamic convolution models up to 50 encoder layers, iterative backtranslation, knowledge distillation, model ensemble and development set fine-tuning. The key ingredient of the process seems the strong focus on diversification of the (synthetic) training data, using multiple scalings of the Transformer model 3.1 Direct Assessment Since running a comparison of direct assessments (DA, Graham et al., 2013, 2014, 2016) and relative ranking in 2016 (Bojar et al., 2016) and verifying a high correlation of system rankings for the two methods, as well as the advantages of DA, such as quality controlled crowd-sourcing and linear growth relative to numbers of submissions, we have employed DA as the primary mechanism for evaluating systems. With DA human evaluation, 15 human assessors are asked to rate a given translation by how adequately it expresses the meaning of the corresponding reference translation or source language input on an analogue scale, which corresponds to an underlying absolute 0–100"
2020.wmt-1.1,E14-1047,1,0.888167,"Missing"
2020.wmt-1.1,2020.lrec-1.312,1,0.804196,"A screenshot of OCELoT is shown in Figure 5. For presentation of the results, systems are treated as either constrained or unconstrained, depending on whether their models were trained only on the provided data. Since we do not know how they were built, the online systems are treated as unconstrained during the automatic and human evaluations. In the rest of this section, we provide brief details of the submitted systems, for those where the authors provided such details. The training corpus for Inuktitut↔English is the recently released Nunavut Hansard Inuktitut– English Parallel Corpus 3.0 (Joanis et al., 2020). For the Japanese↔English tasks, we added several freely available parallel corpora to the training data. It includes JParaCrawl v2.0 (Morishita et al., 2020), a large web-based parallel corpus, Japanese-English Subtitle Corpus (JESC) (Pryzant et al., 2017), the Kyoto Free Translation Task (KFTT) corpus (Neubig, 2011), constructed from the Kyoto-related Wikipedia articles, and TED Talks (Cettolo et al., 2012). The monolingual data we provided was similar to last year’s, with a 2019 news crawl added to all the news corpora. In addition, we provided versions of the news corpora for Czech, Engli"
2020.wmt-1.1,2020.emnlp-main.6,1,0.839606,"shown in Table 3, where the first and second are simple merges or splits, whereas the third is a rare case of more complex reordering. We leave a detailed analysis of the translators’ treatment of paragraph-split data for future work. development set is provided, it is a mixture of both “source-original” and “target-original” texts, in order to maximise its size, although the original language is always marked in the sgm file, except for Inuktitut↔English. The consequences of directionality in test sets has been discussed recently in the literature (Freitag et al., 2019; Laubli et al., 2020; Graham et al., 2020), and the conclusion is that it can have an effect on detrimental effect on the accuracy of system evaluation. We use “source-original” parallel sentences wherever possible, on the basis that it is the more realistic scenario for practical MT usage. Exception: the test sets for the two Inuktitut↔English translation directions contain the same data, without regard to original direction. For most news text in the test and development sets, English was the original language and Inuktitut the translation, while the parliamentary data mixes the two directions. The origins of the news test documents"
2020.wmt-1.1,2020.wmt-1.11,0,0.0940191,"N GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) S"
2020.wmt-1.1,D19-1632,1,0.881933,"ent and paragraph structure retained. In other words, we did not apply sentence splitting to these corpora, and we retained the document boundaries and text ordering of the originals. Training, development, and test data for Pashto↔English and Khmer↔English are shared with the Parallel Corpus Filtering Shared Task (Koehn et al., 2020). The training data mostly comes from OPUS (software localization, Tatoeba, Global Voices), the Bible, and specialprepared corpora from TED Talks and the Jehova Witness web site (JW300). The development and test sets were created as part of the Flores initiative (Guzmán et al., 2019) by professional translation of Wikipedia content with careful vetting of the translations. Please refer the to the Parallel Corpus Filtering Shared Task overview paper for details on these corpora. 2.3.1 AFRL (Gwinnup and Anderson, 2020) AFRL - SYSCOMB 20 is a system combination consisting of two Marian transformer ensembles, one OpenNMT transformer system and a Moses phrase-based system. AFRL - FINETUNE is an OpenNMT transformer system fine-tuned on newstest2014-2017. 2.3.2 (Xv, 2020) ARIEL XV is a Transformer base model trained with the Sockeye sequence modeling toolkit usSome statistics ab"
2020.wmt-1.1,2020.wmt-1.12,1,0.754946,"Missing"
2020.wmt-1.1,2020.wmt-1.13,0,0.0737827,"2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ubiqus (Hernandez and Nguyen, 2020) University of Edinburgh (Bawden et al., 2020a; Germann, 2020) University of Edinburgh and Charles University (Germann et al., 2020) Université du Québec à Montréal (no associated paper) ByteDance AI Lab (Wu et al., 2020a) WeChat (Meng et al"
2020.wmt-1.1,2020.wmt-1.20,0,0.057602,"Missing"
2020.wmt-1.1,2020.wmt-1.14,1,0.820019,"set, and the origlang tag indicates the original source language. 9 Team AFRL ARIEL XV CUNI DCU D EEP M IND D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Mari"
2020.wmt-1.1,2020.wmt-1.39,1,0.812433,"kables was collected in the first phase of the annotation, which amounted to 4k assessments across the systems. The second annotation phase with 6.5k assessments compared markable translations, always checking outputs of all the 13 competing MT systems but still considering the document-level context of each of them. Among other things, the observations indicate that the better the system, the lower the variance in manual scores. Markables annotation then confirms that frequent errors like bad translation of a term need not be the most severe and conversely, 4.1.3 Gender Coreference and Bias (Kocmi et al., 2020) The test suite by Kocmi et al. (2020) focuses on the gender bias in professions (e.g. physician, teacher, secretary) for the translation from English into Czech, German, Polish and Russian. These nouns are ambiguous with respect to gender in English but exhibit gender in the examined target languages. The test suite is based on the fact that a pronoun referring to the ambiguous noun can reveal the gender of the noun in the English source sentence. Once disambiguated, the gender needs to be preserved in translation. To correctly translate the given noun, the translation system thus has to corr"
2020.wmt-1.1,2020.wmt-1.53,0,0.089538,"Missing"
2020.wmt-1.1,2020.wmt-1.78,1,0.815194,"rence on Machine Translation (WMT) 2020. In the news task, participants were asked to build machine translation systems for any of 11 language pairs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages. Chi-kiu Lo NRC Masaaki Nagata NTT Marcos Zampieri Rochester Institute of Technology metrics (Mathur et al., 2020) parallel corpus filtering (Koehn et al., 2020) quality estimation (Specia et al., 2020a) robustness (Specia et al., 2020b) unsupervised and very low-resource translation (Fraser, 2020) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We included 22 translation directions this year, with translation between English and each of Chinese, Czech, German and Russian, as well as French to and from German being repeated from last year, and English to and from Inuktitut, Japanese, Polish and Tamil being new fo"
2020.wmt-1.1,W17-1208,0,0.0524248,"ttribute all these improvements to increased capacity (which allows increased sensitivity to long-range relations) of the models. 5 Similar Language Translation Most shared tasks at WMT (e.g. News, Biomedical) have historically dealt with translating texts from and to English. In recent years, we observed a growing interest in training systems to translate between languages other than English. This includes a number of papers applying MT to translate between pairs of closely-related languages, national language varieties, and dialects 27 of the same language (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018; Popovi´c et al., 2020). To address this topic, the first Similar Language Translation (SLT) shared task at WMT 2019 has been organized. It featured data from three pairs of closely-related languages from different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (IndoAryan languages). Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic, we organize, for the second time at WMT, this shared task to evaluate the performance of state-of-the-art tr"
2020.wmt-1.1,2020.wmt-1.21,0,0.0791885,"Missing"
2020.wmt-1.1,2020.wmt-1.23,0,0.0607352,"020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ubiqus (Hernandez and Nguyen, 2020) University of Edinburgh (Bawden et al., 2020a; Germann, 2020) University of Edinburgh and Charles University (Germann et al., 2"
2020.wmt-1.1,2020.wmt-1.77,1,0.84299,"slation task, both organised alongside the Conference on Machine Translation (WMT) 2020. In the news task, participants were asked to build machine translation systems for any of 11 language pairs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages. Chi-kiu Lo NRC Masaaki Nagata NTT Marcos Zampieri Rochester Institute of Technology metrics (Mathur et al., 2020) parallel corpus filtering (Koehn et al., 2020) quality estimation (Specia et al., 2020a) robustness (Specia et al., 2020b) unsupervised and very low-resource translation (Fraser, 2020) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We included 22 translation directions this year, with translation between English and each of Chinese, Czech, German and Russian, as well as French to and from German being repeated from last year, and English to and from Inu"
2020.wmt-1.1,2020.wmt-1.24,0,0.0435945,"Missing"
2020.wmt-1.1,D18-1512,0,0.05415,"Missing"
2020.wmt-1.1,2020.wmt-1.47,1,0.740067,"Missing"
2020.wmt-1.1,W18-3601,1,0.891679,", we have employed DA as the primary mechanism for evaluating systems. With DA human evaluation, 15 human assessors are asked to rate a given translation by how adequately it expresses the meaning of the corresponding reference translation or source language input on an analogue scale, which corresponds to an underlying absolute 0–100 rating scale.5 No sentence or document length restriction is applied during manual evaluation. Direct Assessment is also employed for evaluation of video captioning systems at TRECvid (Graham et al., 2018; Awad et al., 2019) and multilingual surface realisation (Mille et al., 2018, 2019). 3.1.1 tion 2, most of our test sets do not include reversecreated sentence pairs, except when there were resource constraints on the creation of the test sets. 3.1.3 Prior to WMT19, the issue of including document context was raised within the community (Läubli et al., 2018; Toral et al., 2018) and at WMT19 a range of DA styles were subsequently tested that included document context. In WMT19, two options were run, firstly, an evaluation that included the document context “+DC” (with document context), and secondly, a variation that omitted document context “−DC” (without document con"
2020.wmt-1.1,2020.wmt-1.27,0,0.247738,"he original source language. 9 Team AFRL ARIEL XV CUNI DCU D EEP M IND D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans"
2020.wmt-1.1,D19-6301,1,0.888512,"Missing"
2020.wmt-1.1,W18-6424,0,0.0431841,"(Kocmi, 2020) combines transfer learning from a high-resource language pair Czech–English into the low-resource Inuktitut-English with an additional backtranslation step. Surprising behaviour is noticed when using synthetic data, which can be possibly attributed to a narrow domain of training and test data. The system is the Transformer model in a constrained submission. 2.3.3 Charles University (CUNI) CUNI-D OC T RANSFORMER (Popel, 2020) is similar to the sentence-level version (CUNI-T2T2018, CUBBITT), but trained on sequences with multiple sentences of up to 3000 characters. CUNI-T2T-2018 (Popel, 2018), also called CUBBITT, is exactly the same system as in WMT2018. It is the Transformer model trained according to Popel and Bojar (2018) plus a novel concat-regime backtranslation with checkpoint averaging (Popel et al., 2020), tuned separately for CZ-domain and non CZ-domain articles, possibly handling also translation-direction (“translationese”) issues. For cs→en also a coreference preprocessing was used adding the female-gender CUNI-T RANSFORMER (Popel, 2020) is similar to the WMT2018 version of CUBBITT, but with 12 encoder layers instead of 6 and trained on CzEng 2.0 instead of CzEng 1.7."
2020.wmt-1.1,2020.wmt-1.25,0,0.094349,"Missing"
2020.wmt-1.1,2020.wmt-1.28,0,0.0792624,"cument in the test set, and the origlang tag indicates the original source language. 9 Team AFRL ARIEL XV CUNI DCU D EEP M IND D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 20"
2020.wmt-1.1,2020.lrec-1.443,1,0.79707,"er their models were trained only on the provided data. Since we do not know how they were built, the online systems are treated as unconstrained during the automatic and human evaluations. In the rest of this section, we provide brief details of the submitted systems, for those where the authors provided such details. The training corpus for Inuktitut↔English is the recently released Nunavut Hansard Inuktitut– English Parallel Corpus 3.0 (Joanis et al., 2020). For the Japanese↔English tasks, we added several freely available parallel corpora to the training data. It includes JParaCrawl v2.0 (Morishita et al., 2020), a large web-based parallel corpus, Japanese-English Subtitle Corpus (JESC) (Pryzant et al., 2017), the Kyoto Free Translation Task (KFTT) corpus (Neubig, 2011), constructed from the Kyoto-related Wikipedia articles, and TED Talks (Cettolo et al., 2012). The monolingual data we provided was similar to last year’s, with a 2019 news crawl added to all the news corpora. In addition, we provided versions of the news corpora for Czech, English and German, with both the document and paragraph structure retained. In other words, we did not apply sentence splitting to these corpora, and we retained t"
2020.wmt-1.1,2020.wmt-1.48,0,0.090424,"Missing"
2020.wmt-1.1,2020.wmt-1.49,0,0.0519886,"Missing"
2020.wmt-1.1,W19-6712,0,0.0573476,"tly crawled multilingual parallel corpora from Indian government websites (Haddow and Kirefu, 2020; Siripragada et al., 2020), the Tanzil corpus (Tiedemann, 2009), the Pavlick dicParagraph-split Test Sets For the language pairs English↔Czech, English↔German and English→Chinese, we provided the translators with paragraph-split texts, instead of sentence-split texts. We did this in order to provide the translators with greater freedom and, hopefully, to improve the quality of the translation. Allowing translators to merge and split sentences removes one of the “translation shifts” identified by Popovic (2019), which can make translations create solely for MT evaluation different from translations produced for other purposes. We first show some descriptive statistics of the source texts, for Czech, English and German, in 3 Europarl Parallel Corpus Czech ↔ English German ↔ English Polish↔ English German ↔ French Sentences 645,241 1,825,745 632,435 1,801,076 Words 14,948,900 17,380,340 48,125,573 50,506,059 14,691,199 16,995,232 47,517,102 55,366,136 Distinct words 172,452 63,289 371,748 113,960 170,271 62,694 368,585 134,762 News Commentary Parallel Corpus Czech ↔ English 248,927 5,570,734 6,156,063"
2020.wmt-1.1,2020.wmt-1.26,0,0.0845151,"Missing"
2020.wmt-1.1,2020.vardial-1.10,0,0.0933804,"Missing"
2020.wmt-1.1,W18-6301,0,0.038239,"Missing"
2020.wmt-1.1,2020.wmt-1.51,0,0.0917045,"Missing"
2020.wmt-1.1,2020.wmt-1.50,1,0.78567,"Missing"
2020.wmt-1.1,2020.wmt-1.52,0,0.0485419,"Missing"
2020.wmt-1.1,P19-1164,0,0.0581517,"26 26.37 25.51 24.82 28.33 23.33 21.13 21.96 20.43 22.90 22.58 21.90 22.17 22.17 20.53 19.40 20.01 40.44 32.39 30.39 37.04 32.27 27.54 25.97 26.09 46.38 37.30 36.05 35.96 33.76 33.07 27.20 27.07 Table 15: TICO-19 test suite results on the English-to-X WMT20 translation directions. 26 4.1.5 antecedent (a less common direction of information flow), and then correctly express the noun in the target language. The success of the MT system in this test can be established automatically, whenever the gender of the target word can be automatically identified. Kocmi et al. (2020) build upon the WinoMT (Stanovsky et al., 2019) test set, which provides exactly the necessary type of sentences containing an ambiguous profession noun and a personal pronoun which unambiguously (for the human eye) refers to it based the situation described. When extending WinMT with Czech and Polish, Stanovsky et al. have to disregard some test patterns but the principle remains. The results indicate that all MT systems fail in this test, following gender bias (stereotypical patterns attributing the masculine gender to some professions and feminine gender to others) rather than the coreference link. Word Sense Disambiguation (Scherrer et"
2020.wmt-1.1,2020.wmt-1.31,0,0.0881792,"morphological segmentation of the polysynthetic Inuktitut, testing rule-based, supervised, semi-supervised as well as unsupervised word segmentation methods, (2) whether or not adding data from a related language (Greenlandic) helps, and (3) whether contextual word embeddings (XLM) improve translation. G RONINGEN - ENIU use Transformer implemented in Marian with the default setting, improving the performance also with tagged backtranslation, domain-specific data, ensembling and finetuning. 2.3.7 DONG - NMT (no associated paper) No description provided. 2.3.8 ENMT (Kim et al., 2020) Kim et al. (2020) base their approach on transferring knowledge of domain and linguistic characteristics by pre-training the encoder-decoder model with large amount of in-domain monolingual data through unsupervised and supervised prediction task. The model is then fine-tuned with parallel data and in-domain synthetic data, generated with iterative back-translation. For additional gain, final results are generated with an ensemble model and re-ranked with averaged models and language models. G RONINGEN - ENTAM (Dhar et al., 2020) study the effects of various techniques such as linguistically motivated segmenta"
2020.wmt-1.1,2020.wmt-1.32,0,0.0839317,"- NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ub"
2020.wmt-1.1,2020.wmt-1.33,0,0.080166,"Missing"
2020.wmt-1.1,2020.wmt-1.34,0,0.0803867,"Missing"
2020.wmt-1.1,2020.wmt-1.35,0,0.0951745,"ss web site (JW300). The development and test sets were created as part of the Flores initiative (Guzmán et al., 2019) by professional translation of Wikipedia content with careful vetting of the translations. Please refer the to the Parallel Corpus Filtering Shared Task overview paper for details on these corpora. 2.3.1 AFRL (Gwinnup and Anderson, 2020) AFRL - SYSCOMB 20 is a system combination consisting of two Marian transformer ensembles, one OpenNMT transformer system and a Moses phrase-based system. AFRL - FINETUNE is an OpenNMT transformer system fine-tuned on newstest2014-2017. 2.3.2 (Xv, 2020) ARIEL XV is a Transformer base model trained with the Sockeye sequence modeling toolkit usSome statistics about the training and test materials are given in Figures 1, 2, 3 and 4. 4 8 ARIEL XV https://github.com/AppraiseDev/OCELoT English I English II English III Chinese Czech German Inuktitut Japanese Polish Russian Tamil ABC News (2), All Africa (5), Brisbane Times (1), CBS LA (1), CBS News (1), CNBC (3), CNN (2), Daily Express (1), Daily Mail (2), Fox News (1), Gateway (1), Guardian (3), Huffington Post (2), London Evening Standard (2), Metro (2), NDTV (7), RTE (7), Reuters (4), STV (2), S"
2020.wmt-1.1,2020.wmt-1.55,0,0.0877526,"Missing"
2020.wmt-1.1,P17-4012,0,0.0273892,"o SJTU-NICT using large XLM model to improve NMT but the exact relation is unclear. 2.3.14 H UAWEI TSC (Wei et al., 2020a) H UAWEI TSC use Transformer-big with a further increased model size, focussing on standard techniques of careful pre-processing and filtering, back-translation and forward translation, including self-training, i.e. translating one of the sides of the original parallel data. Ensembling of individual training runs is used in the forward as well as backward translation, and single models are created from the ensembles using knowledge distillation. The submission uses THUNMT (Zhang et al., 2017) open-source engine. 2.3.19 N IU T RANS (Zhang et al., 2020) N IU T RANS gain their performance from focussed attention to six areas: (1) careful data preprocessing and filtering, (2) iterative back-translation to generate additional training data, (3) using different model architectures, such as wider and/or deeper models, relative position representation and relative length, to enhance the diversity of translations, (4) iterative knowledge distillation by in-domain monolingual data, (5) iterative finetuning for domain adaptation using small training batches, (6) rule-based post-processing of"
2020.wmt-1.1,P98-2238,0,0.590812,"and punctuation, and we tend to attribute all these improvements to increased capacity (which allows increased sensitivity to long-range relations) of the models. 5 Similar Language Translation Most shared tasks at WMT (e.g. News, Biomedical) have historically dealt with translating texts from and to English. In recent years, we observed a growing interest in training systems to translate between languages other than English. This includes a number of papers applying MT to translate between pairs of closely-related languages, national language varieties, and dialects 27 of the same language (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018; Popovi´c et al., 2020). To address this topic, the first Similar Language Translation (SLT) shared task at WMT 2019 has been organized. It featured data from three pairs of closely-related languages from different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (IndoAryan languages). Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic, we organize, for the second time at WMT, this shared task to evaluate th"
2020.wmt-1.1,2020.wmt-1.41,1,0.814291,"Missing"
2021.acl-long.200,N18-1118,1,0.930138,"translation cohesion and coherence has long been posited (Hardmeier et al., 2012; Xiong and Zhang, 2013). With the rapid development of neural MT and also available document-level textual datasets, research in this direction gained great popularity. Recent efforts often focus on either advanced contextual neural architecture development (Tiedemann and Scherrer, 2017; Kuang et al., 2018; Miculicich et al., 2018; Zhang et al., 2018, 2020c; Kang et al., 2020; Chen et al., 2020; Ma et al., 2020a; Zheng et al., 2020) and/or improved analysis and evaluation targeted at specific discourse phenomena (Bawden et al., 2018; L¨aubli et al., 2018; Guillou et al., 2018; Voita et al., 2019; Kim et al., 2019; Cai and Xiong, 2020). We follow this research line, and adapt the concatenation-based contextual model (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Lopes et al., 2020) to ST. Our main interest lies in exploring the impact of context on ST. Developing dedicated contextual models for ST is beyond the scope of this study, which we leave to future work. Context-aware ST extends the sentence-level ST towards streaming ST which allows models to access unlimited previous audio inputs. Instead of improving conte"
2021.acl-long.200,2020.iwdp-1.3,0,0.0248615,"). With the rapid development of neural MT and also available document-level textual datasets, research in this direction gained great popularity. Recent efforts often focus on either advanced contextual neural architecture development (Tiedemann and Scherrer, 2017; Kuang et al., 2018; Miculicich et al., 2018; Zhang et al., 2018, 2020c; Kang et al., 2020; Chen et al., 2020; Ma et al., 2020a; Zheng et al., 2020) and/or improved analysis and evaluation targeted at specific discourse phenomena (Bawden et al., 2018; L¨aubli et al., 2018; Guillou et al., 2018; Voita et al., 2019; Kim et al., 2019; Cai and Xiong, 2020). We follow this research line, and adapt the concatenation-based contextual model (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Lopes et al., 2020) to ST. Our main interest lies in exploring the impact of context on ST. Developing dedicated contextual models for ST is beyond the scope of this study, which we leave to future work. Context-aware ST extends the sentence-level ST towards streaming ST which allows models to access unlimited previous audio inputs. Instead of improving contextual modeling, many studies on streaming ST aim at developing better sentence/word segmentation policie"
2021.acl-long.200,2020.autosimtrans-1.5,0,0.0295807,"rk is inspired by pioneer studies on contextaware textual MT. Context beyond the current sentence carries information whose importance for translation cohesion and coherence has long been posited (Hardmeier et al., 2012; Xiong and Zhang, 2013). With the rapid development of neural MT and also available document-level textual datasets, research in this direction gained great popularity. Recent efforts often focus on either advanced contextual neural architecture development (Tiedemann and Scherrer, 2017; Kuang et al., 2018; Miculicich et al., 2018; Zhang et al., 2018, 2020c; Kang et al., 2020; Chen et al., 2020; Ma et al., 2020a; Zheng et al., 2020) and/or improved analysis and evaluation targeted at specific discourse phenomena (Bawden et al., 2018; L¨aubli et al., 2018; Guillou et al., 2018; Voita et al., 2019; Kim et al., 2019; Cai and Xiong, 2020). We follow this research line, and adapt the concatenation-based contextual model (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Lopes et al., 2020) to ST. Our main interest lies in exploring the impact of context on ST. Developing dedicated contextual models for ST is beyond the scope of this study, which we leave to future work. Context-aware ST"
2021.acl-long.200,N19-1202,0,0.0590194,"Missing"
2021.acl-long.200,N13-1073,0,0.0189828,".29 62.08 Table 1: Case-sensitive tokenized BLEU and APT for different models and settings on MuST-C En-De test set. Numbers in bracket denote document-based BLEU. lp: the length penalty for beam search decoding. “w/o Cyn ”: models that are trained without target-side context. Best results are highlighted in bold. Note C = 2, λ = 0.5 and lp = 0.6 by default. erwise specified. We use APT (Miculicich Werlen and Popescu-Belis, 2017), the accuracy of pronoun translation, as an approximate proxy for documentlevel evaluation. Word alignment required by APT is automatically extracted via fast align (Dyer et al., 2013) with the strategy “grow-diag-final-and”. 5.2 Results on MuST-C En-De Does context improve translation? Yes, but the decoding method matters for context-aware ST. Table 1 summarizes the results. Our model with IMED outperforms Baseline by +0.48 BLEU (significant at p &lt; 0.05)6 and +1.79 APT (1→5), clearly showing the benefits from contextual modeling. Although SWBD-Cons yields worse sentencebased BLEU (-0.27, 1→4), it still beats Baseline in document-based BLEU (+0.58) and pronoun translation (+0.17 APT). The reason behind this inferior BLEU partially lies in misaligned translation (see Table 8"
2021.acl-long.200,D14-1140,0,0.0403613,"Missing"
2021.acl-long.200,W18-6435,0,0.0268638,"Missing"
2021.acl-long.200,D12-1108,0,0.0351747,"gs in textual MT (Miculicich et al., 2018; Huo et al., 2020). In addition, context also improves the translation of homophones. • ST models with contexts suffer less from (artificial) audio segmentation errors. • Contextual modeling improves translation quality and reduces latency and flicker for simultaneous translation under re-translation strategy (Arivazhagan et al., 2020a). 2 Related Work Our work is inspired by pioneer studies on contextaware textual MT. Context beyond the current sentence carries information whose importance for translation cohesion and coherence has long been posited (Hardmeier et al., 2012; Xiong and Zhang, 2013). With the rapid development of neural MT and also available document-level textual datasets, research in this direction gained great popularity. Recent efforts often focus on either advanced contextual neural architecture development (Tiedemann and Scherrer, 2017; Kuang et al., 2018; Miculicich et al., 2018; Zhang et al., 2018, 2020c; Kang et al., 2020; Chen et al., 2020; Ma et al., 2020a; Zheng et al., 2020) and/or improved analysis and evaluation targeted at specific discourse phenomena (Bawden et al., 2018; L¨aubli et al., 2018; Guillou et al., 2018; Voita et al., 2"
2021.acl-long.200,2020.wmt-1.71,0,0.109365,"textaware ST model here for both types of translation – that’s why we call it in-model ensemble. We adopt Transformer (Vaswani et al., 2017) for experiments with the MuST-C dataset (Di Gangi et al., 2019). We study the impact of context on translation in different settings. Our results demonstrate the effectiveness of contextual modeling. Our main findings are summarized below: • Incorporating context improves overall translation quality (+0.18-2.61 BLEU) and benefits pronoun translation across different language pairs, resonating with previous findings in textual MT (Miculicich et al., 2018; Huo et al., 2020). In addition, context also improves the translation of homophones. • ST models with contexts suffer less from (artificial) audio segmentation errors. • Contextual modeling improves translation quality and reduces latency and flicker for simultaneous translation under re-translation strategy (Arivazhagan et al., 2020a). 2 Related Work Our work is inspired by pioneer studies on contextaware textual MT. Context beyond the current sentence carries information whose importance for translation cohesion and coherence has long been posited (Hardmeier et al., 2012; Xiong and Zhang, 2013). With the rap"
2021.acl-long.200,2020.emnlp-main.206,0,0.0710008,"Missing"
2021.acl-long.200,2020.emnlp-main.175,0,0.0284592,"Related Work Our work is inspired by pioneer studies on contextaware textual MT. Context beyond the current sentence carries information whose importance for translation cohesion and coherence has long been posited (Hardmeier et al., 2012; Xiong and Zhang, 2013). With the rapid development of neural MT and also available document-level textual datasets, research in this direction gained great popularity. Recent efforts often focus on either advanced contextual neural architecture development (Tiedemann and Scherrer, 2017; Kuang et al., 2018; Miculicich et al., 2018; Zhang et al., 2018, 2020c; Kang et al., 2020; Chen et al., 2020; Ma et al., 2020a; Zheng et al., 2020) and/or improved analysis and evaluation targeted at specific discourse phenomena (Bawden et al., 2018; L¨aubli et al., 2018; Guillou et al., 2018; Voita et al., 2019; Kim et al., 2019; Cai and Xiong, 2020). We follow this research line, and adapt the concatenation-based contextual model (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Lopes et al., 2020) to ST. Our main interest lies in exploring the impact of context on ST. Developing dedicated contextual models for ST is beyond the scope of this study, which we leave to future wor"
2021.acl-long.200,D19-6503,0,0.0268863,"Missing"
2021.acl-long.200,C18-1050,0,0.0644363,"Missing"
2021.acl-long.200,D18-1512,1,0.89201,"Missing"
2021.acl-long.200,2020.eamt-1.24,0,0.178603,"fforts often focus on either advanced contextual neural architecture development (Tiedemann and Scherrer, 2017; Kuang et al., 2018; Miculicich et al., 2018; Zhang et al., 2018, 2020c; Kang et al., 2020; Chen et al., 2020; Ma et al., 2020a; Zheng et al., 2020) and/or improved analysis and evaluation targeted at specific discourse phenomena (Bawden et al., 2018; L¨aubli et al., 2018; Guillou et al., 2018; Voita et al., 2019; Kim et al., 2019; Cai and Xiong, 2020). We follow this research line, and adapt the concatenation-based contextual model (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Lopes et al., 2020) to ST. Our main interest lies in exploring the impact of context on ST. Developing dedicated contextual models for ST is beyond the scope of this study, which we leave to future work. Context-aware ST extends the sentence-level ST towards streaming ST which allows models to access unlimited previous audio inputs. Instead of improving contextual modeling, many studies on streaming ST aim at developing better sentence/word segmentation policies to avoid segmentation errors that greatly hurt translation (Matusov et al., 2007; Rangarajan Sridhar et al., 2013; IranzoS´anchez et al., 2020; Zhang an"
2021.acl-long.200,2020.acl-main.321,0,0.0156967,"ioneer studies on contextaware textual MT. Context beyond the current sentence carries information whose importance for translation cohesion and coherence has long been posited (Hardmeier et al., 2012; Xiong and Zhang, 2013). With the rapid development of neural MT and also available document-level textual datasets, research in this direction gained great popularity. Recent efforts often focus on either advanced contextual neural architecture development (Tiedemann and Scherrer, 2017; Kuang et al., 2018; Miculicich et al., 2018; Zhang et al., 2018, 2020c; Kang et al., 2020; Chen et al., 2020; Ma et al., 2020a; Zheng et al., 2020) and/or improved analysis and evaluation targeted at specific discourse phenomena (Bawden et al., 2018; L¨aubli et al., 2018; Guillou et al., 2018; Voita et al., 2019; Kim et al., 2019; Cai and Xiong, 2020). We follow this research line, and adapt the concatenation-based contextual model (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Lopes et al., 2020) to ST. Our main interest lies in exploring the impact of context on ST. Developing dedicated contextual models for ST is beyond the scope of this study, which we leave to future work. Context-aware ST extends the sent"
2021.acl-long.200,D18-1325,0,0.111413,"that we use the same contextaware ST model here for both types of translation – that’s why we call it in-model ensemble. We adopt Transformer (Vaswani et al., 2017) for experiments with the MuST-C dataset (Di Gangi et al., 2019). We study the impact of context on translation in different settings. Our results demonstrate the effectiveness of contextual modeling. Our main findings are summarized below: • Incorporating context improves overall translation quality (+0.18-2.61 BLEU) and benefits pronoun translation across different language pairs, resonating with previous findings in textual MT (Miculicich et al., 2018; Huo et al., 2020). In addition, context also improves the translation of homophones. • ST models with contexts suffer less from (artificial) audio segmentation errors. • Contextual modeling improves translation quality and reduces latency and flicker for simultaneous translation under re-translation strategy (Arivazhagan et al., 2020a). 2 Related Work Our work is inspired by pioneer studies on contextaware textual MT. Context beyond the current sentence carries information whose importance for translation cohesion and coherence has long been posited (Hardmeier et al., 2012; Xiong and Zhang,"
2021.acl-long.200,W17-4802,0,0.0152439,") 22.97 (28.29) 22.94 (28.11) 61.89 63.51 62.76 11 12 3 w/o Cyn 5 w/o Cyn 21.12 (26.17) 20.72 (25.43) 59.51 58.18 13 14 3 w/o Baseline Initial. 5 w/o Baseline Initial. 21.75 (27.15) 21.97 (27.20) 62.29 62.08 Table 1: Case-sensitive tokenized BLEU and APT for different models and settings on MuST-C En-De test set. Numbers in bracket denote document-based BLEU. lp: the length penalty for beam search decoding. “w/o Cyn ”: models that are trained without target-side context. Best results are highlighted in bold. Note C = 2, λ = 0.5 and lp = 0.6 by default. erwise specified. We use APT (Miculicich Werlen and Popescu-Belis, 2017), the accuracy of pronoun translation, as an approximate proxy for documentlevel evaluation. Word alignment required by APT is automatically extracted via fast align (Dyer et al., 2013) with the strategy “grow-diag-final-and”. 5.2 Results on MuST-C En-De Does context improve translation? Yes, but the decoding method matters for context-aware ST. Table 1 summarizes the results. Our model with IMED outperforms Baseline by +0.48 BLEU (significant at p &lt; 0.05)6 and +1.79 APT (1→5), clearly showing the benefits from contextual modeling. Although SWBD-Cons yields worse sentencebased BLEU (-0.27, 1→4"
2021.acl-long.200,P02-1040,0,0.109788,"Missing"
2021.acl-long.200,W18-6319,0,0.0237995,"Missing"
2021.acl-long.200,N13-1023,0,0.194178,"nd Scherrer, 2017; Bawden et al., 2018; Lopes et al., 2020) to ST. Our main interest lies in exploring the impact of context on ST. Developing dedicated contextual models for ST is beyond the scope of this study, which we leave to future work. Context-aware ST extends the sentence-level ST towards streaming ST which allows models to access unlimited previous audio inputs. Instead of improving contextual modeling, many studies on streaming ST aim at developing better sentence/word segmentation policies to avoid segmentation errors that greatly hurt translation (Matusov et al., 2007; Rangarajan Sridhar et al., 2013; IranzoS´anchez et al., 2020; Zhang and Zhang, 2020; Arivazhagan et al., 2020b). Very recently, Ma et al. (2020b) proposed a memory augmented Transformer encoder for streaming ST, where the previous audio features are summarized into a growing continuous memory to improve the model’s context awareness. Despite its success, this method ignores the target-side context, which turns out to have significant positive impact on ST in our experiments. Our study still relies on oracle sentence segmentation of the audio. The most related work to ours is (Gaido et al., 2020), which also investigated con"
2021.acl-long.200,W17-4702,1,0.848263,"re ST. yn denotes the n-th target sentence in a document; xn denotes the speech encodings extracted from the n-th audio segment. We use dashed gray box to indicate the concatenation operation. “&lt;s>”: sentence separator symbol. Document-level context often offers extra informative clues that could improve the understanding of individual sentences. Such clues have been proven effective for textual machine translation (MT), particularly in handling translation errors specific to discourse phenomena, such as inaccurate coreference of pronouns (Guillou, 2016) and mistranslation of ambiguous words (Rios et al., 2017). Besides, ensuring consistency in translation is virtually impossible without document-level context as well (Voita et al., 2019). Analogous to MT, speech translation (ST) also suffers from these translation issues, and super-sentential context could in fact be more valuable to ST because 1) homophones and acoustic noise bring additional ambiguity to ST, and 2) a common use case in ST is simultaneous translation, where the system has to output translations of sentence fragments, and may have to predict future input to account for word order differences between the source and target language ("
2021.acl-long.200,P16-1162,1,0.404175,"Missing"
2021.acl-long.200,W17-4811,0,0.0254226,"and flicker for simultaneous translation under re-translation strategy (Arivazhagan et al., 2020a). 2 Related Work Our work is inspired by pioneer studies on contextaware textual MT. Context beyond the current sentence carries information whose importance for translation cohesion and coherence has long been posited (Hardmeier et al., 2012; Xiong and Zhang, 2013). With the rapid development of neural MT and also available document-level textual datasets, research in this direction gained great popularity. Recent efforts often focus on either advanced contextual neural architecture development (Tiedemann and Scherrer, 2017; Kuang et al., 2018; Miculicich et al., 2018; Zhang et al., 2018, 2020c; Kang et al., 2020; Chen et al., 2020; Ma et al., 2020a; Zheng et al., 2020) and/or improved analysis and evaluation targeted at specific discourse phenomena (Bawden et al., 2018; L¨aubli et al., 2018; Guillou et al., 2018; Voita et al., 2019; Kim et al., 2019; Cai and Xiong, 2020). We follow this research line, and adapt the concatenation-based contextual model (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Lopes et al., 2020) to ST. Our main interest lies in exploring the impact of context on ST. Developing dedicat"
2021.acl-long.200,P19-1116,1,0.896393,"Missing"
2021.acl-long.200,P18-1117,1,0.841621,"wer this question by studying the impact of incorrect context on our model. We replace the correct source context with some random audio segments from the same document, and randomly select the target context from previous translations during decoding. Intuitively, the performance of our model should be intact if it ignores the context. Note that we trained our model with correct contexts but test it with random contexts here. Results in Table 2 show that the randomized context, either source- or target-side, hurts the performance of our model in both BLEU and APT, similar to the findings in (Voita et al., 2018), and the translation of pronouns suffers more (> -1.6 APT). Compared to SWBD, the incorrect context has more negative impact on IMED, resulting in worse performance than Baseline (Table 1), although IMED also uses sentence-level translation. We ascribe this to the target prefix constraint in IMED which makes translation errors at early decoding much easier to propagate. We observe that the incorrect target context acts similarly to its source counterpart under IMED, albeit its selection scope is much smaller (only limited to the translated segments), and combining both contexts leads to a sli"
2021.acl-long.200,2020.acl-main.344,0,0.0208695,"ty to ST, and 2) a common use case in ST is simultaneous translation, where the system has to output translations of sentence fragments, and may have to predict future input to account for word order differences between the source and target language (Grissom II et al., 2014). Both for ambiguity from the acoustic signal, and operating on small sentence fragments, we hypothesize that access to extra context2 will be beneficial. Although recent studies on ST have achieved promising results with end-to-end (E2E) models (Anastasopoulos and Chiang, 2018; Di Gangi et al., 2019; Zhang et al., 2020a; Wang et al., 2020; Dong et al., 2020), nevertheless, they mainly focus on sentence-level translation. One practical challenge when scaling up sentence-level E2E ST to the document-level is the encoding of very long audio segments, which can easily hit the computational bottleneck, especially with Transformers (Vaswani et al., 2017). So far, the research question of whether and how contextual information benefits E2E ST has received little attention. In this paper, we answer this question through extensive experiments by exploring a concatenation1 Source code is available at https://github.com/ bzhangGo/zero. 2"
2021.acl-long.200,2020.findings-emnlp.230,1,0.889467,"ng additional ambiguity to ST, and 2) a common use case in ST is simultaneous translation, where the system has to output translations of sentence fragments, and may have to predict future input to account for word order differences between the source and target language (Grissom II et al., 2014). Both for ambiguity from the acoustic signal, and operating on small sentence fragments, we hypothesize that access to extra context2 will be beneficial. Although recent studies on ST have achieved promising results with end-to-end (E2E) models (Anastasopoulos and Chiang, 2018; Di Gangi et al., 2019; Zhang et al., 2020a; Wang et al., 2020; Dong et al., 2020), nevertheless, they mainly focus on sentence-level translation. One practical challenge when scaling up sentence-level E2E ST to the document-level is the encoding of very long audio segments, which can easily hit the computational bottleneck, especially with Transformers (Vaswani et al., 2017). So far, the research question of whether and how contextual information benefits E2E ST has received little attention. In this paper, we answer this question through extensive experiments by exploring a concatenation1 Source code is available at https://github.c"
2021.acl-long.200,2021.findings-acl.255,1,0.835314,"Missing"
2021.acl-long.200,D18-1049,0,0.0359043,"Missing"
2021.acl-long.200,2020.emnlp-main.81,0,0.0507902,"Missing"
2021.acl-long.200,2020.autosimtrans-1.1,0,0.196645,"., 2020) to ST. Our main interest lies in exploring the impact of context on ST. Developing dedicated contextual models for ST is beyond the scope of this study, which we leave to future work. Context-aware ST extends the sentence-level ST towards streaming ST which allows models to access unlimited previous audio inputs. Instead of improving contextual modeling, many studies on streaming ST aim at developing better sentence/word segmentation policies to avoid segmentation errors that greatly hurt translation (Matusov et al., 2007; Rangarajan Sridhar et al., 2013; IranzoS´anchez et al., 2020; Zhang and Zhang, 2020; Arivazhagan et al., 2020b). Very recently, Ma et al. (2020b) proposed a memory augmented Transformer encoder for streaming ST, where the previous audio features are summarized into a growing continuous memory to improve the model’s context awareness. Despite its success, this method ignores the target-side context, which turns out to have significant positive impact on ST in our experiments. Our study still relies on oracle sentence segmentation of the audio. The most related work to ours is (Gaido et al., 2020), which also investigated contextualized translation and showed that contextaware"
2021.acl-long.200,2021.acl-demo.7,0,0.0628604,"Missing"
2021.eacl-demos.32,P19-1126,0,0.361622,"Missing"
2021.eacl-demos.32,D14-1140,0,0.362092,"Missing"
2021.eacl-demos.32,2020.iwslt-1.27,0,0.152889,"among the research partners in our project, e.g. Hindi. The scientific motivation for our efforts is to find an approach that allows to assemble laboratory system components to a practically usable product and to document the problems on this journey. 3 Related Systems Live spoken language translation has been continuously studied for decades, see e.g. Osterholtz et al. (1992); F¨ugen et al. (2008); Bangalore et al. (2012). Recent systems differ in whether they provide revisions to their previous output (M¨uller et al., 2016; Niehues et al., 2016; Dessloch et al., 2018; Niehues et al., 2018; Arivazhagan et al., 2020), or whether they only append output tokens (Grissom II et al., 2014; Gu et al., 2017; Arivazhagan et al., 2019; Press and Smith, 2018; Xiong et al., 2019; Ma et al., 2019; Zheng et al., 2019). M¨uller et al. (2016) were probably the first to allow output revision when they find a better translation. Zenkel et al. (2018) released a simpler setup as an open-source toolkit consisting of a neural speech recognition system, a sentence segmentation system, and an attention-based translation system providing also some pre-trained models for their tasks. (Zenkel et al., 2018) evaluated only the quali"
2021.eacl-demos.32,E17-1099,0,0.269847,"rts is to find an approach that allows to assemble laboratory system components to a practically usable product and to document the problems on this journey. 3 Related Systems Live spoken language translation has been continuously studied for decades, see e.g. Osterholtz et al. (1992); F¨ugen et al. (2008); Bangalore et al. (2012). Recent systems differ in whether they provide revisions to their previous output (M¨uller et al., 2016; Niehues et al., 2016; Dessloch et al., 2018; Niehues et al., 2018; Arivazhagan et al., 2020), or whether they only append output tokens (Grissom II et al., 2014; Gu et al., 2017; Arivazhagan et al., 2019; Press and Smith, 2018; Xiong et al., 2019; Ma et al., 2019; Zheng et al., 2019). M¨uller et al. (2016) were probably the first to allow output revision when they find a better translation. Zenkel et al. (2018) released a simpler setup as an open-source toolkit consisting of a neural speech recognition system, a sentence segmentation system, and an attention-based translation system providing also some pre-trained models for their tasks. (Zenkel et al., 2018) evaluated only the quality of the output translations using BLEU and WER metrics. Zheng et al. (2019) propose"
2021.eacl-demos.32,N12-1048,0,0.134304,"m the secured networks of the labs so it usually does not run into firewall issues. tions of the EU and nearby countries. Experimentally, we include also other languages based on available systems among the research partners in our project, e.g. Hindi. The scientific motivation for our efforts is to find an approach that allows to assemble laboratory system components to a practically usable product and to document the problems on this journey. 3 Related Systems Live spoken language translation has been continuously studied for decades, see e.g. Osterholtz et al. (1992); F¨ugen et al. (2008); Bangalore et al. (2012). Recent systems differ in whether they provide revisions to their previous output (M¨uller et al., 2016; Niehues et al., 2016; Dessloch et al., 2018; Niehues et al., 2018; Arivazhagan et al., 2020), or whether they only append output tokens (Grissom II et al., 2014; Gu et al., 2017; Arivazhagan et al., 2019; Press and Smith, 2018; Xiong et al., 2019; Ma et al., 2019; Zheng et al., 2019). M¨uller et al. (2016) were probably the first to allow output revision when they find a better translation. Zenkel et al. (2018) released a simpler setup as an open-source toolkit consisting of a neural speec"
2021.eacl-demos.32,2020.eamt-1.53,1,0.493793,"Missing"
2021.eacl-demos.32,C18-2020,1,0.876378,"Missing"
2021.eacl-demos.32,2020.iwslt-1.25,1,0.823962,"Missing"
2021.eacl-demos.32,N16-3017,1,0.800534,"Missing"
2021.eacl-demos.32,2020.acl-main.148,1,0.8252,"tem in end-to-end fashion and face engineering problems and technical issues on all layers from sound acquisition through network connections, worker configuration to subtitle presentation. • We are currently running a user study with nonGerman speakers watching German videos with our online subtitles, see Section 7.1. We aim to measure the comprehension loss caused by different subtitling options, latency or flicker. 42 languages (Johnson et al., 2017). The models are mostly Transformers (Vaswani et al., 2017) but we improve their performance in massively multilingual setting by extra depth (Zhang et al., 2020). 5.3 Interplay of ASR and MT Connecting ASR and MT systems is not straightforward because MT systems assume input in the form of complete sentences. We follow the strategy of Niehues et al. (2016), first inserting punctuation into the stream of tokens coming from ASR (Tilk and Alum¨ae, 2016), breaking it up at full stops and sending individual sentences to MT, either as unfinished sentence prefixes, or complete sentences. We are using re-translation, as ASR or punctuation updates are received. Currently, the main problem is that punctuation prediction does not have access to the sound any mor"
2021.eacl-demos.32,D19-1137,0,0.29802,"product and to document the problems on this journey. 3 Related Systems Live spoken language translation has been continuously studied for decades, see e.g. Osterholtz et al. (1992); F¨ugen et al. (2008); Bangalore et al. (2012). Recent systems differ in whether they provide revisions to their previous output (M¨uller et al., 2016; Niehues et al., 2016; Dessloch et al., 2018; Niehues et al., 2018; Arivazhagan et al., 2020), or whether they only append output tokens (Grissom II et al., 2014; Gu et al., 2017; Arivazhagan et al., 2019; Press and Smith, 2018; Xiong et al., 2019; Ma et al., 2019; Zheng et al., 2019). M¨uller et al. (2016) were probably the first to allow output revision when they find a better translation. Zenkel et al. (2018) released a simpler setup as an open-source toolkit consisting of a neural speech recognition system, a sentence segmentation system, and an attention-based translation system providing also some pre-trained models for their tasks. (Zenkel et al., 2018) evaluated only the quality of the output translations using BLEU and WER metrics. Zheng et al. (2019) proposed a new approach with a delay-based heuristic. The model decides to read more input (or wait for it) or wri"
2021.eacl-demos.32,2020.findings-emnlp.349,0,0.0269839,"text available, the user does not see sufficient number of words to let the brain “make up” or reconstruct the original meaning from pieces. The short-term memory of recently processed text does not seem to be sufficient for this type recovery, while seeing the words in larger context gives the user a better chance. The last step in an SLT system is the delivery of the translated content to the user. Our goal stops at the textual representation, i.e. we do not include speech synthesis and delivery of the sound, which would bring yet another set of design decisions and open problems, see e.g. Zheng et al. (2020). We experiment with two different views for our text output, both implemented as web applications. The “subtitle view” is optimized toward minimal use of screen space. Only two lines of text are available which leaves room either for e.g. a streamed video of the session or the slides, or for many languages displayed at once, if the screen is intended for a multi-lingual audience. The “paragraph view” provides more textual context to the user. 7.1 Subtitle View The subtitle view offers a simple interface with a HLS stream of the video or slides and one or more subtitles streams. Section 7.1 pr"
2021.eacl-demos.32,W19-5337,1,0.807801,"latency and hypotheses updates, as in KIT Lecture Translator (M¨uller et al., 2016). We use the hybrid ASR models based on Janus from KIT Lecture Translator, for German and English, as well as recent neural sequence-to-sequence ASR models trained on the same data (Nguyen et al., 2020). For Czech ASR, we use a Kaldi hybrid model trained on a Corpus of Czech Parliament Plenary Hearings (Kratochv´ıl et al., 2019). Czech sequence-to-sequence ASR is a work in progress. 5.2 MT Systems in ELITR We use bilingual NMT models for some high resource and well-studied language pairs e.g. for English-Czech (Popel et al., 2019; Wetesko et al., 2019). For other targets, we use multi-target models, e.g. an English-centric universal model for ELITR Flexible Architecture We always strive for the best performance for each considered language pair. With the perpetual com272 Index Name auto-iwslt2020-antrecorp(ASR) auto-iwslt2020-antrecorp(MT) auto-iwslt2020-antrecorp(MT) auto-asr-english-auditing(ASR) auto-asr-english-auditing(MT) auto-asr-english-auditing(MT) auto-iwslt2020-khanacademy(ASR) Worker en-EU-lecture KIT-s2s rb-EU fromEN-en to 41 rb-EU fromEN-en to 41 en-EU-lecture KIT-s2s rb-EU fromEN-en to 41 rb-EU fromEN-e"
2021.eacl-demos.9,2020.eamt-1.53,1,0.78716,"Missing"
2021.eacl-demos.9,2021.eacl-demos.32,1,0.748319,"Missing"
2021.eacl-demos.9,P19-1126,0,0.026489,"Missing"
2021.eacl-demos.9,W08-0509,0,0.0767354,"e preceding “Unternehmen” (company) was available only at 1062. For “unser”, SLT EV selects the expected time as the maximum between 895 (its expectation time under proportional delay) and 961 (the time that its aligned source word “our” appeared) . In other words, SLT EV gives more time to the SLT system to display the “unser” because its aligned word is output a bit later than the proportional expectation of “unser”. Under the alignment-based delay, we do not expect that the word will be output earlier than its alignment indicates. Technically, we rely on automatic word alignments by MGIZA (Gao and Vogel, 2008) which is a multi-threaded version of GIZA++ (Och and Ney, 2003), aligning the completed segments of the golden source transcript and the reference translation. The effect of alignment errors on the reliability of the evaluation is yet to be explored. 4.2.3 Multi-Reference Delay Calculation With multiple references, we create a separate table T for each and calculate the delay of each segment individually, taking the minimum across all references. The final delay is the sum of these minima. We use this strategy for both delay calculation methods and both segmentation strategies introduced abov"
2021.eacl-demos.9,D14-1140,0,0.0387859,"Missing"
2021.eacl-demos.9,N12-1048,0,0.0274606,"defined measures with a normalized Flicker in our work. We also propose a new averaging of older measures. A preliminary version of SLT EV was used in the IWSLT 2020 SHARED TASK. Moreover, a growing collection of test datasets directly accessible by SLT EV are provided for system evaluation comparable across papers. 1 Introduction Spoken Language Translation (SLT), i.e. translation of human speech across languages, is an application at least as important as Machine Translation (MT). Many approaches have been examined so far, ranging from translation of transcript chunks (F¨ugen et al., 2008; Bangalore et al., 2012) to fully end-to-end, speech-to-speech neural systems, (Jia et al., 2019). In recent years, simultaneous translation systems aim at behavior similar to human interpreters, digesting and producing an infinite sequence of words. Some systems (Grissom II et al., 2014; Gu et al., 2017; Arivazhagan et al., 2019b; Press and Smith, 2018; Xiong et al., 2019; Ma et al., 2019; Zheng et al., 2019) do not consider any revision of their outputs and can be evaluated 2 Related Work SLT EV is designed to be versatile enough to score automatic SLT as well as transcribed human interpretation. Shimizu et al. (20"
2021.eacl-demos.9,E17-1099,0,0.0611745,"ion comparable across papers. 1 Introduction Spoken Language Translation (SLT), i.e. translation of human speech across languages, is an application at least as important as Machine Translation (MT). Many approaches have been examined so far, ranging from translation of transcript chunks (F¨ugen et al., 2008; Bangalore et al., 2012) to fully end-to-end, speech-to-speech neural systems, (Jia et al., 2019). In recent years, simultaneous translation systems aim at behavior similar to human interpreters, digesting and producing an infinite sequence of words. Some systems (Grissom II et al., 2014; Gu et al., 2017; Arivazhagan et al., 2019b; Press and Smith, 2018; Xiong et al., 2019; Ma et al., 2019; Zheng et al., 2019) do not consider any revision of their outputs and can be evaluated 2 Related Work SLT EV is designed to be versatile enough to score automatic SLT as well as transcribed human interpretation. Shimizu et al. (2014) are probably the first to score human interpretation with automatic 1 https://github.com/ELITR/SLTev 71 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations, pages 71–79 April 19 - 23, 2021. ©2021 As"
2021.eacl-demos.9,J03-1002,0,0.0437663,"r “unser”, SLT EV selects the expected time as the maximum between 895 (its expectation time under proportional delay) and 961 (the time that its aligned source word “our” appeared) . In other words, SLT EV gives more time to the SLT system to display the “unser” because its aligned word is output a bit later than the proportional expectation of “unser”. Under the alignment-based delay, we do not expect that the word will be output earlier than its alignment indicates. Technically, we rely on automatic word alignments by MGIZA (Gao and Vogel, 2008) which is a multi-threaded version of GIZA++ (Och and Ney, 2003), aligning the completed segments of the golden source transcript and the reference translation. The effect of alignment errors on the reliability of the evaluation is yet to be explored. 4.2.3 Multi-Reference Delay Calculation With multiple references, we create a separate table T for each and calculate the delay of each segment individually, taking the minimum across all references. The final delay is the sum of these minima. We use this strategy for both delay calculation methods and both segmentation strategies introduced above. 4.3 Flicker to Assess Stability For systems that revise their"
2021.eacl-demos.9,P04-1077,0,0.119574,"(i.e., the number of corrections) by measuring the overlap between consecutive updates. As soon as a word is changed, all the following words are counted as updated, suggesting than any word change forces the user to reread all the rest. Gu et al. (2017) consider two versions of delay when assessing their reinforcement learning based simultaneous SLT model: Average Proportion (of waiting compared to producing words) and Consecutive Wait (the silence duration so far), and prescribe a target value for each of them to steer the learning, also balancing it with quality estimated by smoothed BLEU (Lin and Och, 2004). Since their model does not allow corrections, they do not require a measure of stability. The delay measures of (Gu et al., 2017) were criticised by Ma et al. (2019) when they introduced their wait-k model. They defined a measure Average Lag (AL), which measures how far, in words, the translation is behind an ideal wait-k model. Since wait-k does not allow corrections, they do not need a stability measure. AL was improved by Cherry and Foster (2019), with Differentiable Average Lag (DAL), which not only is differentiable, but fixes some undesirable behaviour of AL around sentence boundaries."
2021.eacl-demos.9,P02-1040,0,0.124861,"50 Good 65 Good mor 119 Good morning how 195 Good morning. How are you? 102 Good morning. 218 How are you? I 195 How are you? 239 I am ... ... (a) Time-stamped transcript P P P P C P C P ... 60 80 130 201 201 220 220 245 ... 0 0 0 0 0 102 102 195 ... 50 Gut 65 Guten Morgen! 119 Guten wie morgen 195 Guten Morgen! Wie geht es dir? 102 Guten Morgen! 218 Wie geht es dir? Ich 195 Wie geht es dir? 239 Ich bin ... ... (b) SLT candidate output Figure 1: Example of SLT EV file formats. All timestamps in centiseconds. measures but they segment the output manually and assess only the quality using BLEU (Papineni et al., 2002), WER (Matusov et al., 2005), TER (Snover et al., 2006), and RIBES (Isozaki et al., 2010). Most SLT evaluations require sentence segmentation of the candidate to match the reference one. Using mwerSegmenter (Matusov et al., 2005), they re-segment the candidate automatically, minimizing WER against the reference. We complement this approach with time-based segmentation. Niehues et al. (2016) introduced the retranslation approach to simultaneous SLT, and define latency based on the time between a word expected and actually displayed, considering only the final version of the word, not early revi"
2021.eacl-demos.9,W18-6319,0,0.0232859,"nt of revision. Trading these qualities for one another is again possible: It is obvious that if a system creates the translations with a longer Delay or revises them more (higher Flicker), the quality of the final translation (i.e., the output text) can be better. Given the existence of three evaluation criteria and a multitude of possible definitions for each of them, the need for some robust and standard metrics to evaluate SLT is inevitable. Recently, the MT community tackled a similar problem (i.e., the inconsistency in the reporting of BLEU scores) by introducing a tool named sacreBLEU (Post, 2018) with a canonical implementation of the widely user metric. In this work, we propose SLT EV,1 an open-source tool to calculate the quality of SLT systems based on three different criteria: translation quality, latency, and stability, in a standardized way. Furthermore, we complement SLT EV with a growing collection of freelyavailable test sets for Automatic Speech Recognition (ASR), MT and SLT for a number of languages, so that these technologies can be evaluated in comparable settings, similarly to what the WMT news test sets (Barrault et al., 2020) offer in MT. Automatic evaluation of Machin"
2021.eacl-demos.9,shimizu-etal-2014-collection,0,0.0219904,"lore et al., 2012) to fully end-to-end, speech-to-speech neural systems, (Jia et al., 2019). In recent years, simultaneous translation systems aim at behavior similar to human interpreters, digesting and producing an infinite sequence of words. Some systems (Grissom II et al., 2014; Gu et al., 2017; Arivazhagan et al., 2019b; Press and Smith, 2018; Xiong et al., 2019; Ma et al., 2019; Zheng et al., 2019) do not consider any revision of their outputs and can be evaluated 2 Related Work SLT EV is designed to be versatile enough to score automatic SLT as well as transcribed human interpretation. Shimizu et al. (2014) are probably the first to score human interpretation with automatic 1 https://github.com/ELITR/SLTev 71 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations, pages 71–79 April 19 - 23, 2021. ©2021 Association for Computational Linguistics P P P P C P C P ... 0 0 0 0 0 102 102 195 ... 50 Good 65 Good mor 119 Good morning how 195 Good morning. How are you? 102 Good morning. 218 How are you? I 195 How are you? 239 I am ... ... (a) Time-stamped transcript P P P P C P C P ... 60 80 130 201 201 220 220 245 ... 0 0 0 0 0 1"
2021.eacl-demos.9,2020.emnlp-demos.19,0,0.0488099,"ability. For evaluation, they check the output of ASR and the output of the MT system as recorded over time in their simple logging system. They assess the quality, latency, and stability (i.e., Flicker). The quality is estimated using BLEU after mwerSegmenter re-segmentation. For the assessment of latency (translation lag) and stability (the number of erased tokens in temporary translations per final target token, “Normalized Erasure” in the paper), they do not use any segmentation at all and instead calculate the scores for ten-minute long audio chunks. The closest to our work is SIMULEVAL (Ma et al., 2020), a client-server toolkit measuring the latency of SLT including any network effects between the evaluated system (client) and the mock user (SIMULEVAL server). SIMULEVAL offers a nice visualization interface but the required clientserver approach may be unsuitable for research prototypes solving SLT only partially. Most importantly, updates of output (Flicker) are not supported and no test set for reproducible scoring is provided. 3 Input Formats SLT EV can evaluate separate ASR and MT systems as well as cascaded and end-to-end SLT systems. We focus on SLT here. Three input files are used for"
2021.eacl-demos.9,2006.amta-papers.25,0,0.205111,". How are you? 102 Good morning. 218 How are you? I 195 How are you? 239 I am ... ... (a) Time-stamped transcript P P P P C P C P ... 60 80 130 201 201 220 220 245 ... 0 0 0 0 0 102 102 195 ... 50 Gut 65 Guten Morgen! 119 Guten wie morgen 195 Guten Morgen! Wie geht es dir? 102 Guten Morgen! 218 Wie geht es dir? Ich 195 Wie geht es dir? 239 Ich bin ... ... (b) SLT candidate output Figure 1: Example of SLT EV file formats. All timestamps in centiseconds. measures but they segment the output manually and assess only the quality using BLEU (Papineni et al., 2002), WER (Matusov et al., 2005), TER (Snover et al., 2006), and RIBES (Isozaki et al., 2010). Most SLT evaluations require sentence segmentation of the candidate to match the reference one. Using mwerSegmenter (Matusov et al., 2005), they re-segment the candidate automatically, minimizing WER against the reference. We complement this approach with time-based segmentation. Niehues et al. (2016) introduced the retranslation approach to simultaneous SLT, and define latency based on the time between a word expected and actually displayed, considering only the final version of the word, not early revisions. They did not provide any evaluation of stability"
2021.eacl-demos.9,2005.iwslt-1.19,0,0.116909,"morning how 195 Good morning. How are you? 102 Good morning. 218 How are you? I 195 How are you? 239 I am ... ... (a) Time-stamped transcript P P P P C P C P ... 60 80 130 201 201 220 220 245 ... 0 0 0 0 0 102 102 195 ... 50 Gut 65 Guten Morgen! 119 Guten wie morgen 195 Guten Morgen! Wie geht es dir? 102 Guten Morgen! 218 Wie geht es dir? Ich 195 Wie geht es dir? 239 Ich bin ... ... (b) SLT candidate output Figure 1: Example of SLT EV file formats. All timestamps in centiseconds. measures but they segment the output manually and assess only the quality using BLEU (Papineni et al., 2002), WER (Matusov et al., 2005), TER (Snover et al., 2006), and RIBES (Isozaki et al., 2010). Most SLT evaluations require sentence segmentation of the candidate to match the reference one. Using mwerSegmenter (Matusov et al., 2005), they re-segment the candidate automatically, minimizing WER against the reference. We complement this approach with time-based segmentation. Niehues et al. (2016) introduced the retranslation approach to simultaneous SLT, and define latency based on the time between a word expected and actually displayed, considering only the final version of the word, not early revisions. They did not provide"
2021.eacl-demos.9,N16-3017,0,0.0588397,"Missing"
2021.eacl-demos.9,D19-1137,0,0.0111475,"n speech across languages, is an application at least as important as Machine Translation (MT). Many approaches have been examined so far, ranging from translation of transcript chunks (F¨ugen et al., 2008; Bangalore et al., 2012) to fully end-to-end, speech-to-speech neural systems, (Jia et al., 2019). In recent years, simultaneous translation systems aim at behavior similar to human interpreters, digesting and producing an infinite sequence of words. Some systems (Grissom II et al., 2014; Gu et al., 2017; Arivazhagan et al., 2019b; Press and Smith, 2018; Xiong et al., 2019; Ma et al., 2019; Zheng et al., 2019) do not consider any revision of their outputs and can be evaluated 2 Related Work SLT EV is designed to be versatile enough to score automatic SLT as well as transcribed human interpretation. Shimizu et al. (2014) are probably the first to score human interpretation with automatic 1 https://github.com/ELITR/SLTev 71 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations, pages 71–79 April 19 - 23, 2021. ©2021 Association for Computational Linguistics P P P P C P C P ... 0 0 0 0 0 102 102 195 ... 50 Good 65 Good mor 11"
2021.findings-acl.261,K16-1002,0,0.100823,"Missing"
2021.findings-acl.261,W18-6315,0,0.0204696,", unlike masking. Introduction Neural machine translation (NMT) is notoriously data-hungry (Koehn and Knowles, 2017). To learn a strong model it requires large, high-quality and in-domain parallel data, which exist only for a few language-pairs. The most successful approach for improving low-resource NMT is backtranslation (Sennrich et al., 2016), that exploits abundant monolingual corpora to augment the parallel with synthetic data. However, in low-resource settings, it may fail to improve or even degrade translation quality if the initial model is not strong enough (Imankulova et al., 2017; Burlot and Yvon, 2018). Unsupervised pretraining is a complementary technique, that has revolutionized many natural language understanding (NLU) tasks (Wang et al., 2019). The dominant approach is to train a (large) model on a lot of unlabeled data using the masked language modeling (MLM; Devlin et al. (2019)) objective and then finetune it on a downstream task. Besides improving generalization, good initialization drastically reduces the need for labelled data. This paradigm has been applied recently to NMT yielding impressive results in low-resource settings, with models such as XLM (Conneau and Lample, 2019), MA"
2021.findings-acl.261,N19-1423,0,0.153631,"ow-resource NMT is backtranslation (Sennrich et al., 2016), that exploits abundant monolingual corpora to augment the parallel with synthetic data. However, in low-resource settings, it may fail to improve or even degrade translation quality if the initial model is not strong enough (Imankulova et al., 2017; Burlot and Yvon, 2018). Unsupervised pretraining is a complementary technique, that has revolutionized many natural language understanding (NLU) tasks (Wang et al., 2019). The dominant approach is to train a (large) model on a lot of unlabeled data using the masked language modeling (MLM; Devlin et al. (2019)) objective and then finetune it on a downstream task. Besides improving generalization, good initialization drastically reduces the need for labelled data. This paradigm has been applied recently to NMT yielding impressive results in low-resource settings, with models such as XLM (Conneau and Lample, 2019), MASS (Song et al., 2019) and BART/mBART (Lewis et al., 2020b; Liu et al., 2020), that adapt MLM to sequence-to-sequence architectures. Although pretraining alone is not enough to outperform backtranslation, it helps the initial model to produce synthetic data of sufficient quality, and com"
2021.findings-acl.261,D18-1045,0,0.0708412,"Missing"
2021.findings-acl.261,P17-2090,0,0.0168307,"on all permutations of other tokens in a sentence and Song et al. (2020) extend this to sequence-level pretraining for NLU. MARGE (Lewis et al., 2020a) explores multi-lingual pretraining for document-level NMT, by reconstructing texts from a set of retrieved relevant documents. Clark et al. (2020) propose the replaced token detection (RTD) objective for pretraining text encoders. They replace tokens with samples from a MLM and train the encoder as a discriminator to predict whether each word is real or fake. Similar ideas have been previously explored in NMT with contextual data augmentation (Fadaee et al., 2017; Kobayashi, 2018; Gao et al., 2019). 2957 1 Code at github.com/cbaziotis/nmt-pretraining-objectives 3 Pretraining Our pretraining model is a multilingual denoising sequence autoencoder, based on the Transformer (Vaswani et al., 2017). We assume access to a corpus of unpaired data, containing text in two languages A, B. Given a text sequence of N tokens x = hx1 , x2 , ..., xN i we first add noise to it and obtain its corrupted version x0 . An encoder transforms x0 into a sequence of contextualized representations h(x0 ) = hh1 , h2 , ..., hN i, which are given as input to the decoder, that prod"
2021.findings-acl.261,P19-1555,0,0.0152663,"a sentence and Song et al. (2020) extend this to sequence-level pretraining for NLU. MARGE (Lewis et al., 2020a) explores multi-lingual pretraining for document-level NMT, by reconstructing texts from a set of retrieved relevant documents. Clark et al. (2020) propose the replaced token detection (RTD) objective for pretraining text encoders. They replace tokens with samples from a MLM and train the encoder as a discriminator to predict whether each word is real or fake. Similar ideas have been previously explored in NMT with contextual data augmentation (Fadaee et al., 2017; Kobayashi, 2018; Gao et al., 2019). 2957 1 Code at github.com/cbaziotis/nmt-pretraining-objectives 3 Pretraining Our pretraining model is a multilingual denoising sequence autoencoder, based on the Transformer (Vaswani et al., 2017). We assume access to a corpus of unpaired data, containing text in two languages A, B. Given a text sequence of N tokens x = hx1 , x2 , ..., xN i we first add noise to it and obtain its corrupted version x0 . An encoder transforms x0 into a sequence of contextualized representations h(x0 ) = hh1 , h2 , ..., hN i, which are given as input to the decoder, that produces a reconstruction of x. The reco"
2021.findings-acl.261,D19-1632,0,0.0466467,"Missing"
2021.findings-acl.261,W17-5704,0,0.0232334,"h resemble real sentences, unlike masking. Introduction Neural machine translation (NMT) is notoriously data-hungry (Koehn and Knowles, 2017). To learn a strong model it requires large, high-quality and in-domain parallel data, which exist only for a few language-pairs. The most successful approach for improving low-resource NMT is backtranslation (Sennrich et al., 2016), that exploits abundant monolingual corpora to augment the parallel with synthetic data. However, in low-resource settings, it may fail to improve or even degrade translation quality if the initial model is not strong enough (Imankulova et al., 2017; Burlot and Yvon, 2018). Unsupervised pretraining is a complementary technique, that has revolutionized many natural language understanding (NLU) tasks (Wang et al., 2019). The dominant approach is to train a (large) model on a lot of unlabeled data using the masked language modeling (MLM; Devlin et al. (2019)) objective and then finetune it on a downstream task. Besides improving generalization, good initialization drastically reduces the need for labelled data. This paradigm has been applied recently to NMT yielding impressive results in low-resource settings, with models such as XLM (Conne"
2021.findings-acl.261,2020.emnlp-main.75,0,0.0801411,"Missing"
2021.findings-acl.261,2021.naacl-main.7,0,0.0768002,"Missing"
2021.iwslt-1.4,2020.iwslt-1.27,0,0.0900899,"ova, 2016; Ma et al., 2019) where the system appends the output to a growing hypothesis as new inputs are available, and re-translation (Niehues et al., 2016, 2018; Arivazhagan et al., 2020a,b), where, as the name suggests, the system re-translates the whole prefix on every update to a completely new output. Retranslation approach has the advantage that we can use an unmodified, general purpose, optimised MT engine with beam-search, but we have to address the problem of flicker. That is to say, the translation of a prefix may be changed by the translation of an extended prefix. Recent work by Arivazhagan et al. (2020a) has shown that, if measures are taken to mitigate flicker, then re-translation produces results comparable to streaming approach. Since the shared task does not permit any revision of a committed hypothesis (i.e. flicker is not allowed) we focus on adapting the re-translation approach for our submission without introducing any flicker into a growing hypothesis. We describe our submission to the IWSLT 2021 shared task1 on simultaneous text-to-text English-German translation. Our system is based on the re-translation approach where the agent re-translates the whole source prefix each time it"
2021.iwslt-1.4,W18-6319,0,0.0221494,"l, so we did not pursue it further. The validation data is also pre-processed similarly to the training set. Note that this preprocessed validation set is used at training for early stopping and not for reporting the validation scores in the Table 2. Corpus Europarl Rapid News Commentary OpenSubtitle TED corpus MuST-C.v2 Sentence pairs 1.79 M 1.45 M 0.35 M 22.51 M 206 K 248 K Table 1: Corpora used in training the systems 4 Result and Analysis We evaluate the model’s performance on the full sentence translation before doing actual simultaneous translation. For this evaluation we use SacreBLEU (Post, 2018) on the MuST-C.v2 and TED 2018 test sets. The results on full sentence is shown in the Table 2. We see there is a significant improve48 34 32 30 30 28 28 26 26 BLEU BLEU 32 24 24 22 22 lm+mask mask lm baseline 20 18 2 4 6 8 AL 10 12 14 lm+mask mask lm baseline 20 18 2 16 4 6 8 AL 10 12 14 16 (b) Beam size = 12, Normalization = 0.6 (a) Beam size = 12, Normalization = 1.0 Figure 3: BLEU vs AL plots for English-German with different beam sizes and length normalization. 34 lm+mask mask lm baseline 32 30 28 28 26 26 BLEU BLEU 30 lm+mask mask lm baseline 32 24 24 22 22 20 20 18 18 2 4 6 8 DAL 10 12"
2021.iwslt-1.4,N19-1202,0,0.0923389,"Missing"
2021.iwslt-1.4,W11-2123,0,0.0556675,"ion is more stable. COMMON from MuST-C.v2. As the there is a significant overlap between MuST-C.v2 and tst20{14,15,18}, we remove the overlaps from the MuST-C.v2 training data before training. For training, we use the Marian toolkit (JunczysDowmunt et al., 2018) with the ‘base’ transformer architecture (Vaswani et al., 2017). First, we train a model using the aforementioned pre-processed training data and then fine-tune the model using MuST-C.v2 training data which is more of a domain specific data for simultaneous translation task. To train the language model for stabilisation, we use KenLM (Heafield, 2011) to train a 6-gram language model on the source-side training data. We have shown the number of sentences in each corpus in Table 1. For preprocessing we rely only on SentencePiece tokenization (Kudo and Richardson, 2018); no other preprocessing tools are applied. We use a shared vocabulary size of 32k. Standard NMT models perform well when translation is done on a full sentence but as our approach is based on retranslation, we use training data that is a 1:1 mix of full sentences and prefix pairs (Niehues et al., 2018; Arivazhagan et al., 2020a). This ensures that our model can translate both"
2021.iwslt-1.4,2020.amta-research.12,1,0.752346,"ck of the language model (LM) score of the previous token and compares it with the score of the current token. If the LM score is higher than the previous token, it keeps reading more tokens and does a re-translation only when this condition is not met. Here the LM score is the log probability of the current token given the context. Though LM score doesn’t guarantee to find meaningful unit every time but this simple approach shows it is better than the baseline approach in terms of BLEU score. Our second method of stabilising the retranslation approach is based on the idea of dynamic masking (Yao and Haddow, 2020). The dynamic mask approach finds the stable part of the target prefix by comparing the translation of the current prefix, with the translation of an extension of the current prefix. The longest common prefix (LCP) of the two translations is taken as the stable part. Figure 1 shows how dynamic masking works in general. Yao and Haddow (2020) showed that using dynamic mask could give a better flickerlatency trade-off than using a fixed mask, without affecting the translation quality of full sentences. For our IWSLT submission, we generate the extended prefixes for dynamic mask simply by appendin"
2021.iwslt-1.4,P18-4020,1,0.869837,"Missing"
2021.iwslt-1.4,2020.emnlp-main.178,0,0.0386263,"input is consumed, the agent keeps performing WRITE operations until it reaches the end of the translated sentence. The WRITE operation involves re-translating the prefix S and finding the next output word w from output prefix T . If the output prefix T has a length longer than the committed hypothesis H, it picks the (i + 1)th word of T , else sends READ signal to the agent, i being the length of the current hypothesis. ing extra READs when inside an MU. An MU is a chunk of words that has a definite translation and can be translated independently without having to wait for more input words (Zhang et al., 2020). Our first method of detecting MUs relies on the language model (LM) score. The agent keeps track of the language model (LM) score of the previous token and compares it with the score of the current token. If the LM score is higher than the previous token, it keeps reading more tokens and does a re-translation only when this condition is not met. Here the LM score is the log probability of the current token given the context. Though LM score doesn’t guarantee to find meaningful unit every time but this simple approach shows it is better than the baseline approach in terms of BLEU score. Our s"
2021.iwslt-1.4,D18-2012,0,0.0224564,"use the Marian toolkit (JunczysDowmunt et al., 2018) with the ‘base’ transformer architecture (Vaswani et al., 2017). First, we train a model using the aforementioned pre-processed training data and then fine-tune the model using MuST-C.v2 training data which is more of a domain specific data for simultaneous translation task. To train the language model for stabilisation, we use KenLM (Heafield, 2011) to train a 6-gram language model on the source-side training data. We have shown the number of sentences in each corpus in Table 1. For preprocessing we rely only on SentencePiece tokenization (Kudo and Richardson, 2018); no other preprocessing tools are applied. We use a shared vocabulary size of 32k. Standard NMT models perform well when translation is done on a full sentence but as our approach is based on retranslation, we use training data that is a 1:1 mix of full sentences and prefix pairs (Niehues et al., 2018; Arivazhagan et al., 2020a). This ensures that our model can translate both full sentences and prefixes. To create prefix pairs, we first randomly choose a position in the source sentence and then take the proportionate length of the target sentence. Along with that we also add modified prefix p"
2021.iwslt-1.4,L16-1147,0,0.0229467,"ss the flicker caused by re-translation, but could end up gluing together incompatible fragments of the hypothesis. This problem can be worse when the output prefix T flickers too much. To improve translation quality, we employ two approaches which aim at detecting meaningful units (MU) and allow3 Experimental Details We use only the officially allowed IWSLT 2021 data sets. The training data include high quality English-German parallel data from WMT 2020 (Barrault et al., 2020), English-German data from MuST-C.v2 (Di Gangi et al., 2019), the TED corpus (Cettolo et al., 2012) and OpenSubtitle (Lison and Tiedemann, 2016). For development, we use the concatenation of IWSLT test sets from 2014 and 2015. We test on IWSLT 2018 test set and tst47 ASR S =ab translate T =pqr LCP extend S0 = a b c translate T∗ = p q T0 = p q s t Figure 1: Dynamic Masking. The string a b is provided as input to the agent (in a full SLT system it would come from ASR). The MT system then produces translations of the string and its extension, compares them, and outputs the longest common prefix (LCP) prefix extension prefix extension Source Back in New York, Back in New York, UNK Back in New York, I Back in New York, I UNK Translation Zu"
2021.iwslt-1.4,2020.emnlp-demos.19,0,0.19269,"t possible trade-off between these two measures. In the IWSLT 2021 shared task on simultaneous translation, the aim was to build and evaluate simultaneous SLT systems at three different latency regimes (low, medium and high), as measured using the Average Lagging (AL; Ma et al. (2019)). 1 2 Overview of Our Submission We participated in the English→German text-totext simultaneous task. Since we re-translate the incomplete input (know as a prefix) each time it is updated, our system will try to modify the translations produced from earlier prefixes. But as the task is evaluated using SimulEval (Ma et al., 2020) which does not permit the modification of committed output (also known as flickering), we use a simple approach to generate incremental output at each re-translation step. Concretely, we apply a method inspired by the wait-k streaming approach (Ma et al., 2019) in our re-translation system in the following manner. In the task, a simultaneous SLT system is implemented as an agent which must choose between https://iwslt.org/2021/ 46 Proceedings of the 18th International Conference on Spoken Language Translation, pages 46–51 Bangkok, Thailand (Online), August 5–6, 2021. ©2021 Association for Com"
2021.mtsummit-asltrw.3,2020.iwltp-1.7,1,0.815336,"Missing"
2021.mtsummit-asltrw.3,P19-1289,0,0.0279532,"Missing"
2021.mtsummit-asltrw.3,tiedemann-2012-parallel,0,0.0899064,"Missing"
2021.mtsummit-research.8,2020.wmt-1.5,1,0.795255,"en prepended to the source sentence to specify the output language. We use WMT data (see Section 4.1) for training and early stopping. Training of the from-scratch system. Training consists of fine-tuning a pretrained model with Pashto–English parallel data, using it to generate initial backtranslations which are combined with the parallel data and used to train another round of the model, starting again from a pretrained model. At this point, we include the first 220,000 sentence pairs of “Bytedance” filtered parallel data, sorted by filtering rank. Following similar work with English–Tamil (Bawden et al., 2020), we start with our mBART-like model and we fine-tune it in the Pashto→English direction with our parallel data. Then we use this model to backtranslate the Pashto monolingual data, generating a pseudo-parallel corpus which we combine with our true parallel corpus and use to train a English→Pashto model again starting from mBART. We use this model to backtranslate the first 5,000,000 monolingual English sentences (we also experimented with the full corpus, but found minimal difference), and we train another round of Pashto→English followed by another round of English→Pashto, both initialized f"
2021.mtsummit-research.8,W18-2716,0,0.065695,"Missing"
2021.mtsummit-research.8,D18-2012,0,0.0212368,"f these could be Pashto speakers. Pashto (also spelled Pukhto and Pakhto is an Iranian language of the Indo-European family and is grouped with other Iranian languages such as Persian, Dari, Tajiki, in spite of major linguistic diferences among them. Pashto is written with a unique enriched Perso-Arabic script with 45 letters and four diacritics. Translating between English and Pashto poses interesting challenges. Pashto has a richer morphology than that of English; the induced data sparseness may partly be remedied with segmentation in subword units tokenization models such as SentencePiece (Kudo and Richardson, 2018), as used in mBART50. There are Pashto categories in Pashto that do not overtly exist in English (such as verb aspect or the oblique case in general nouns) and categories in English that do not overtly exist in Pashto (such as definite and indefinite articles), which may pose a certain challenge when having to generate correct text in machine translation output. Due to the chronic political and social instability and conflict that Afghanistan has experienced in its recent history, the country features prominently in global news coverage. Closely following the developments there remains a key p"
2021.mtsummit-research.8,2020.tacl-1.47,0,0.318683,"he BBC and DW for a short period of time. Given the impact of the COVID-19 pandemic, a twomonth period was considered realistic. On 1 February 2021, BBC and DW revealed the chosen language to be Pashto. By completing and documenting how this challenge was addressed, we prove we are able to bootstrap a new high quality neural machine translation task within a very limited window of time. There has also been a considerable amount of recent interest in using pretrained language models for improving performance on downstream natural language processing tasks, especially in a low resource setting (Liu et al., 2020; Brown et al., 2020; Qiu et al., 2020), but how best to do this is still an open question. A key question in this work is how best to use training data which is not English (en) to Pashto (ps) translations. We experimented, on the one hand, with pretraining models on a high-resource language pair (German–English, one of the most studied high-resource language pairs) and, on the other hand, with fine-tuning an existing large pretrained translation model (mBART50) trained on parallel data involving English and 49 languages including Pashto (Tang et al., 2020). We show that both approaches perfo"
2021.mtsummit-research.8,W17-4770,0,0.0257416,"Missing"
2021.mtsummit-research.8,W18-6319,0,0.0127791,"tively small decrease in memory consumption: for example, the GPU memory requirements of mBART n–to–n at inference time (setting the maximum number of tokens per mini-batch to 100) moved from around 4 GB to around 3 GB. 5 Results and Discussion Tables 2 and 3 show BLEU and chrF2 scores, respectively, for the English to Pashto systems with different test sets. The evaluation metrics for the Google MT system are also included for reference purposes. Similarly, tables 4 and 5 show BLEU and chrF2 scores, respectively, for the Pashto to English systems. All the scores were computed with sacrebleu (Post, 2018). The test sets considered are the two in-house parallel sets created by BBC and DW (see Section 3) and the devtest set provided in the FLORES19 benchmark (2,698 sentences). 19 https://github.com/facebookresearch/flores 8 Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 99 Google from-scratch mBART50 + small + small, large + small, large, synthetic BBC test DW test FLORES devtest 35.03 20.00 19.42 22.55 25.27 25.38 24.65 15.06 15.30 17.50 19.13 17.88 21.54 14.90 14.59 14.77 17.71 17.08 Table 4: BLEU scores of the Pa"
D15-1248,P11-2031,0,0.0111348,"y in parallel text &lt; 5). For comparison with the state-of-the-art, we train a full system on our restructured representation, which incorporates all models and settings of our WMT 2015 submission system (Williams et al., 2015).7 Note that our WMT 2015 submission 5 We use the last position in the clause as default location, but put the particle before any subordinated and coordinated clauses, which occur in the Nachfeld (the ‘final field’ in topological field theory). 6 We use mteval-v13a.pl for comparability to official WMT results; all significance values reported are obtained with MultEval (Clark et al., 2011). 7 In contrast to our other systems in this paper, RDLM is trained on all monolingual data for the full system, and two models are added: a 5-gram Neural Network language model newstest2014 20.7 21.3 21.4 20.9 22.0 22.1 22.6 newstest2015 22.0 22.4 22.8 22.7 23.4 23.8 24.4 Table 2: English–German translation results (B LEU). Average of three optimization runs. system reference baseline +head binarization +split compounds +particle verbs compound 2841 845 798 1850 1992 sep. 553 96 157 160 333 particle verb pref. zu-infix 1195 176 847 71 858 106 877 94 953 169 Table 3: Number of compounds [that"
D15-1248,E12-1068,0,0.015318,"t.1 We also split linking elements, and represent them as a postmodifier of each non-final segment, including the empty string (&quot;&quot;). We use the same representation for noun compounds and adjective compounds. An example of the original2 and the proposed compound representation is shown in Figure 1. Importantly, the head of the compound is also the parent of the determiners and attributes in the noun phrase, which makes a bigram dependency language model sufficient to enforce agreement. Since we model morphosyntactic agreement within the main translation step, and not in a separate step as in (Fraser et al., 2012), we deem it useful that inflection is marked at the head of the compound. Consequently, we do not split off inflectional or derivational morphemes. For German particle verbs, we define a common representation that abstracts away from the various surface realizations (see Table 1). Separated 1 We follow prior work in leaving frequent words or subwords unsplit, which has a disambiguating effect. With more aggressive splitting, frequency information could be used for the structural disambiguation of internal structure. 2 The original dependency trees follow the annotation guidelines by Foth (200"
D15-1248,W10-1734,0,0.0465186,"normally a premodifying particle, appears as an infix in particle verbs. Table 1 shows an illustrating example. 2 root obja det subj sie erheben eine Handgepäckgebühr PPER VVFIN ART NN they charge a carry-on bag fee obja det A Dependency Representation of Compounds and Particle Verbs root mod subj The main focus of research on compound splitting has been on the splitting algorithm (Popovic et al., 2006; Nießen and Ney, 2000; Weller et al., 2014; Macherey et al., 2011). Our focus is not the splitting algorithm, but the representation of compounds. For splitting, we use an approach similar to (Fritzinger and Fraser, 2010), with segmentation candidates identified by a finite-state morphology (Schmid et al., 2004; Sennrich and Kunz, 2014), and statistical evidence from the training corpus to select a split (Koehn and Knight, 2003). German compounds are head-final, and premodifiers can be added recursively. Compounds are structurally ambiguous if there is more than one modifier. Consider the distinction between (Stadtteil)projekt (literally: ’(city part) project)’) and Stadt(teilprojekt) ’city sub-project’. We opt for a left-branching representation by default.1 We also split linking elements, and represent them"
D15-1248,P06-1121,0,0.444314,"mber and gender is enforced between eine ’a’ and Gebühr ’fee’, and selectional preference between erheben ’charge’ and Gebühr ’fee’. A flat representation, as is common in phrase-based SMT, does not encode these relationships, but a dependency representation does so through dependency links. In this paper, we investigate a dependency representation of morphologically segmented words for SMT. Our representation encodes syntactic and morphological structure jointly, allowing a single model to learn the translation of both. Specifically, we work with a string-to-tree model with GHKM-style rules (Galley et al., 2006), and a relational dependency language model (Sennrich, 2015). We focus on the representation of German syntax and morphology in an English-to-German system, and two morphologically complex word classes in German that are challenging for translation, compounds and particle verbs. German makes heavy use of compounding, and compounds such as Abwasserbehandlungsanlage ‘waste water treatment plant’ are translated into complex noun phrases in other languages, such as French station d’épuration des eaux résiduaires. German particle verbs are difficult to model because their surface realization diffe"
D15-1248,W14-4018,0,0.27665,"etical drawback. With head binarization, we find substantial improvements from compound splitting of 0.7–1.1 B LEU. On newstest2014, the improvement is almost twice of that reported in related work (Williams et al., 2014), which also uses a hierarchical representation of compounds, albeit one that does not allow for dependency modelling. Examples of correct, unseen compounds generated include Staubsauger|roboter ’vacuum cleaner robot’, Gravitation|s|wellen ’gravitational waves’, and NPD|-|verbot|s|verfahren ’NPD banning process’.8 (Vaswani et al., 2013), and soft source-syntactic constraints (Huck et al., 2014). 8 Note that Staubsauger, despite being a compound, is not 2084 Particle verb restructuring yields additional gains of 0.1–0.4 B LEU. One reason for the smaller effect of particle verb restructuring is that the difficult cases – separated particle verbs and those with infixation – are rarer than compounds, with 2841 rare compounds [that would be split by our compound splitter] in the reference texts, in contrast to 553 separated particle verbs, and 176 particle verbs with infixation, as Table 3 illustrates. If we only evaluate the sentences containing a particle verb with zu-infix in the refe"
D15-1248,E03-1076,0,0.529645,"lation quality of 1.4–1.8 B LEU in the WMT English–German translation task. 1 finite (sub.) bare infinitive to/zu-infinitive Table 1: Surface realizations of particle verb weggehen ’walk away’. they charge a carry-on bag fee. Introduction When translating between two languages that differ in their degree of morphological synthesis, syntactic structures in one language may be realized as morphological structures in the other. Machine Translation models that treat words as atomic units have poor learning capabilities for such translation units, and morphological segmentations are commonly used (Koehn and Knight, 2003). Like words in a sentence, the morphemes of a word have a hierarchical structure that is relevant in translation. For instance, compounds in Germanic languages are head-final, and the head is the segment that determines agreement within the noun phrase, and is relevant for selectional preferences of verbs. 1. sie erheben eine Hand|gepäck|gebühr. English/German example he walks away quickly er geht schnell weg [...] because he walks away quickly [...] weil er schnell weggeht he can walk away quickly er kann schnell weggehen he promises to walk away quickly er verspricht, schnell wegzugehen In"
D15-1248,P07-2045,0,0.00473569,"leave it to future research to determine if our limitation to projective trees affects translation quality, and how to produce nonprojective trees. 5 SMT experiments 5.1 Data and Models We train English–German string-to-tree SMT systems on the training data of the shared translation task of the Workshop on Statistical Machine Translation (WMT) 2015. The data set consists of 4.2 million sentence pairs of parallel data, and 160 million sentences of monolingual German data. We base our systems on that of Williams et al. (2014). It is a string-to-tree GHKM translation system implemented in Moses (Koehn et al., 2007), and using the dependency annotation by ParZu (Sennrich et al., 2013). Additionally, our baseline system contains a dependency language model (RDLM) (Sennrich, 2015), trained on the target-side of the parallel training data. We report case-sensitive B LEU scores on the newstest2014/5 test sets from WMT, averaged over 3 optimization runs of k-batch MIRA (Cherry and Foster, 2012) on a subset of newstest2008-12.6 We split all particle verbs and hyphenated compounds, but other compounds are only split if they are rare (frequency in parallel text &lt; 5). For comparison with the state-of-the-art, we"
D15-1248,P11-1140,0,0.0502404,"Missing"
D15-1248,C00-2162,0,0.676827,"er 2015. 2015 Association for Computational Linguistics. main clauses, but prefixed to the verb in subordinated clauses, or when the verb is non-finite. The infinitive marker zu ’to’, which is normally a premodifying particle, appears as an infix in particle verbs. Table 1 shows an illustrating example. 2 root obja det subj sie erheben eine Handgepäckgebühr PPER VVFIN ART NN they charge a carry-on bag fee obja det A Dependency Representation of Compounds and Particle Verbs root mod subj The main focus of research on compound splitting has been on the splitting algorithm (Popovic et al., 2006; Nießen and Ney, 2000; Weller et al., 2014; Macherey et al., 2011). Our focus is not the splitting algorithm, but the representation of compounds. For splitting, we use an approach similar to (Fritzinger and Fraser, 2010), with segmentation candidates identified by a finite-state morphology (Schmid et al., 2004; Sennrich and Kunz, 2014), and statistical evidence from the training corpus to select a split (Koehn and Knight, 2003). German compounds are head-final, and premodifiers can be added recursively. Compounds are structurally ambiguous if there is more than one modifier. Consider the distinction between (Stad"
D15-1248,2001.mtsummit-papers.45,0,0.074245,"st-modifiers, as erheben and gepäck do in Figure 3. 2083 1. non-finite verbs are concatenated with the particle, and zu-markers are infixed. 2. finite verbs that head a subordinated clause (identified by its dependency label) are concatenated with the particle. 3. finite verbs that head a main clause have the system baseline +split compounds +particle verbs head binarization +split compounds +particle verbs full system particle moved to the right clause bracket.5 Previous work on particle verb translation into German proposed to predict the position of particles with an n-gram language model (Nießen and Ney, 2001). Our rules have the advantage that they are informed by the syntax of the sentence and consider the finiteness of the verb. Our rules only produce projective trees. Verb particles may also appear in positions that violate projectivity, and we leave it to future research to determine if our limitation to projective trees affects translation quality, and how to produce nonprojective trees. 5 SMT experiments 5.1 Data and Models We train English–German string-to-tree SMT systems on the training data of the shared translation task of the Workshop on Statistical Machine Translation (WMT) 2015. The"
D15-1248,schmid-etal-2004-smor,0,0.0162513,"ing example. 2 root obja det subj sie erheben eine Handgepäckgebühr PPER VVFIN ART NN they charge a carry-on bag fee obja det A Dependency Representation of Compounds and Particle Verbs root mod subj The main focus of research on compound splitting has been on the splitting algorithm (Popovic et al., 2006; Nießen and Ney, 2000; Weller et al., 2014; Macherey et al., 2011). Our focus is not the splitting algorithm, but the representation of compounds. For splitting, we use an approach similar to (Fritzinger and Fraser, 2010), with segmentation candidates identified by a finite-state morphology (Schmid et al., 2004; Sennrich and Kunz, 2014), and statistical evidence from the training corpus to select a split (Koehn and Knight, 2003). German compounds are head-final, and premodifiers can be added recursively. Compounds are structurally ambiguous if there is more than one modifier. Consider the distinction between (Stadtteil)projekt (literally: ’(city part) project)’) and Stadt(teilprojekt) ’city sub-project’. We opt for a left-branching representation by default.1 We also split linking elements, and represent them as a postmodifier of each non-final segment, including the empty string (&quot;&quot;). We use the s"
D15-1248,sennrich-kunz-2014-zmorge,1,0.84052,"bja det subj sie erheben eine Handgepäckgebühr PPER VVFIN ART NN they charge a carry-on bag fee obja det A Dependency Representation of Compounds and Particle Verbs root mod subj The main focus of research on compound splitting has been on the splitting algorithm (Popovic et al., 2006; Nießen and Ney, 2000; Weller et al., 2014; Macherey et al., 2011). Our focus is not the splitting algorithm, but the representation of compounds. For splitting, we use an approach similar to (Fritzinger and Fraser, 2010), with segmentation candidates identified by a finite-state morphology (Schmid et al., 2004; Sennrich and Kunz, 2014), and statistical evidence from the training corpus to select a split (Koehn and Knight, 2003). German compounds are head-final, and premodifiers can be added recursively. Compounds are structurally ambiguous if there is more than one modifier. Consider the distinction between (Stadtteil)projekt (literally: ’(city part) project)’) and Stadt(teilprojekt) ’city sub-project’. We opt for a left-branching representation by default.1 We also split linking elements, and represent them as a postmodifier of each non-final segment, including the empty string (&quot;&quot;). We use the same representation for nou"
D15-1248,R13-1079,1,0.651787,"ctive trees affects translation quality, and how to produce nonprojective trees. 5 SMT experiments 5.1 Data and Models We train English–German string-to-tree SMT systems on the training data of the shared translation task of the Workshop on Statistical Machine Translation (WMT) 2015. The data set consists of 4.2 million sentence pairs of parallel data, and 160 million sentences of monolingual German data. We base our systems on that of Williams et al. (2014). It is a string-to-tree GHKM translation system implemented in Moses (Koehn et al., 2007), and using the dependency annotation by ParZu (Sennrich et al., 2013). Additionally, our baseline system contains a dependency language model (RDLM) (Sennrich, 2015), trained on the target-side of the parallel training data. We report case-sensitive B LEU scores on the newstest2014/5 test sets from WMT, averaged over 3 optimization runs of k-batch MIRA (Cherry and Foster, 2012) on a subset of newstest2008-12.6 We split all particle verbs and hyphenated compounds, but other compounds are only split if they are rare (frequency in parallel text &lt; 5). For comparison with the state-of-the-art, we train a full system on our restructured representation, which incorpor"
D15-1248,Q15-1013,1,0.873797,"selectional preference between erheben ’charge’ and Gebühr ’fee’. A flat representation, as is common in phrase-based SMT, does not encode these relationships, but a dependency representation does so through dependency links. In this paper, we investigate a dependency representation of morphologically segmented words for SMT. Our representation encodes syntactic and morphological structure jointly, allowing a single model to learn the translation of both. Specifically, we work with a string-to-tree model with GHKM-style rules (Galley et al., 2006), and a relational dependency language model (Sennrich, 2015). We focus on the representation of German syntax and morphology in an English-to-German system, and two morphologically complex word classes in German that are challenging for translation, compounds and particle verbs. German makes heavy use of compounding, and compounds such as Abwasserbehandlungsanlage ‘waste water treatment plant’ are translated into complex noun phrases in other languages, such as French station d’épuration des eaux résiduaires. German particle verbs are difficult to model because their surface realization differs depending on the finiteness of the verb and the type of cl"
D15-1248,D13-1140,0,0.0683442,"d representation, and that binarization compensates this theoretical drawback. With head binarization, we find substantial improvements from compound splitting of 0.7–1.1 B LEU. On newstest2014, the improvement is almost twice of that reported in related work (Williams et al., 2014), which also uses a hierarchical representation of compounds, albeit one that does not allow for dependency modelling. Examples of correct, unseen compounds generated include Staubsauger|roboter ’vacuum cleaner robot’, Gravitation|s|wellen ’gravitational waves’, and NPD|-|verbot|s|verfahren ’NPD banning process’.8 (Vaswani et al., 2013), and soft source-syntactic constraints (Huck et al., 2014). 8 Note that Staubsauger, despite being a compound, is not 2084 Particle verb restructuring yields additional gains of 0.1–0.4 B LEU. One reason for the smaller effect of particle verb restructuring is that the difficult cases – separated particle verbs and those with infixation – are rarer than compounds, with 2841 rare compounds [that would be split by our compound splitter] in the reference texts, in contrast to 553 separated particle verbs, and 176 particle verbs with infixation, as Table 3 illustrates. If we only evaluate the sen"
D15-1248,D07-1078,0,0.15192,"Missing"
D15-1248,W14-3324,1,0.817402,"ith infixed zu-marker. verb particles are reordered to be the closest premodifier of the verb. Prefixed particles and the zuinfix are identified by the finite-state-morphology, and split from the verb so that the particle is the closest, the zu marker the next-closest premodifier of the verb, as shown in Figure 2. Agreement, selectional preferences, and other phenomena involve the verb and its dependents, and the proposed representation retains these dependency links, but reduces data sparsity from affixation and avoids discontinuity of the verb and its particle. 3 Tree Binarization We follow Williams et al. (2014) and map dependency trees into a constituency representation, which allows for the extraction of GHKM-style translation rules (Galley et al., 2006). This conversion is lossless, and we can still apply a de2082 pendency language model (RDLM). Figure 3 (a) shows the constituency representation of the example in Figure 1. Our model should not only be able to produce new words productively, but also to memorize words it has observed during training. Looking at the compound Handgepäckgebühr in Figure 3 (a), we can see that it does not form a constituent, and cannot be extracted with GHKM extraction"
D15-1248,W15-3024,1,0.880521,"ge model (RDLM) (Sennrich, 2015), trained on the target-side of the parallel training data. We report case-sensitive B LEU scores on the newstest2014/5 test sets from WMT, averaged over 3 optimization runs of k-batch MIRA (Cherry and Foster, 2012) on a subset of newstest2008-12.6 We split all particle verbs and hyphenated compounds, but other compounds are only split if they are rare (frequency in parallel text &lt; 5). For comparison with the state-of-the-art, we train a full system on our restructured representation, which incorporates all models and settings of our WMT 2015 submission system (Williams et al., 2015).7 Note that our WMT 2015 submission 5 We use the last position in the clause as default location, but put the particle before any subordinated and coordinated clauses, which occur in the Nachfeld (the ‘final field’ in topological field theory). 6 We use mteval-v13a.pl for comparability to official WMT results; all significance values reported are obtained with MultEval (Clark et al., 2011). 7 In contrast to our other systems in this paper, RDLM is trained on all monolingual data for the full system, and two models are added: a 5-gram Neural Network language model newstest2014 20.7 21.3 21.4 2"
D15-1248,J10-4005,0,\N,Missing
D15-1248,W14-5709,0,\N,Missing
D15-1248,W15-3014,0,\N,Missing
D15-1248,N12-1047,0,\N,Missing
D16-1134,W13-2322,0,0.335895,"Missing"
D16-1134,W05-0909,0,0.252262,"Missing"
D16-1134,W13-2203,1,0.896683,"Missing"
D16-1134,W12-4204,1,0.908186,"Missing"
D16-1134,W11-2101,1,0.914645,"Missing"
D16-1134,W14-4005,0,0.0250442,"Missing"
D16-1134,P16-2013,0,0.0371161,"Missing"
D16-1134,W07-0738,0,0.0835328,"Missing"
D16-1134,N15-1124,0,0.125815,"Missing"
D16-1134,P15-1174,0,0.0311734,"Missing"
D16-1134,P07-2045,1,0.0114599,"Missing"
D16-1134,W05-0904,0,0.093592,"Missing"
D16-1134,W11-1002,0,0.0373567,"Missing"
D16-1134,lo-wu-2014-reliability,0,0.0405899,"Missing"
D16-1134,oepen-lonning-2006-discriminant,0,0.0919776,"Missing"
D16-1134,2006.amta-papers.25,0,0.300368,"Missing"
D16-1134,W15-3502,1,0.478779,"Missing"
D16-1134,P02-1040,0,\N,Missing
D17-1156,2012.eamt-1.60,0,0.0287279,"he corresponding columns of the out-of-domain parameter matrices. This can be alternatively seen as learning matrices of parameter differences between in-domain and out-of-domain models with standard dropout, starting from a zero initialization at the beginning of fine-tuning. Therefore, equation 2 becomes ˆ + ∆W · M∆W,i,j ) · hi,j vi,j = (W (3) ˆ is the fixed out-of-domain parameter where W matrix and ∆W is the parameter difference matrix to be learned and M∆W,i,j is a Bayesian dropout mask. 1490 Evaluation 31 We evaluate transfer learning on test sets from the IWSLT shared translation task (Cettolo et al., 2012). 3.1 Data and Methods Test sets consist of transcripts of TED talks and their translations; small amounts of in-domain training data are also provided. For English-toGerman we use IWSLT 2015 training data, while for English-to-Russian we use IWSLT 2014 training data. For the out-of-domain systems, we use training data from the WMT shared translation task,2 which is considered permissible for IWSLT tasks, including back-translations of monolingual training data (Sennrich et al., 2016b), i.e., automatic translations of data available only in target language “back” into the source language.3 . W"
D17-1156,P07-1033,0,0.223933,"Missing"
D17-1156,W04-3250,0,0.204806,"Missing"
D17-1156,2015.iwslt-evaluation.11,0,0.0702048,"rvised domain adaptation for neural machine translation where an existing model trained on a large out-of-domain dataset is adapted to a small in-domain dataset. Owing to the incremental nature of stochastic gradient-based training algorithms, a simple yet effective approach to transfer learning for neural networks is fine-tuning (Hinton and Salakhutdinov, 2006; Mesnil et al., 2012; Yosinski et al., 2014): to continue training an existing model which was trained on out-of-domain data with indomain training data. This strategy was also found to be very effective for neural machine translation (Luong and Manning, 2015; Sennrich et al., 2016b). In this scenario, overfitting is a major challenge. We investigate a number of techniques to reduce overfitting and improve transfer learning, including regularization techniques such as dropout and L2regularization towards an out-of-domain prior. In addition, we introduce tuneout, a novel regularization technique inspired by dropout. We apply these techniques, alone and in combination, to neural machine translation, obtaining improvements on IWSLT datasets for English→German and English→Russian. We also investigate the amounts of in-domain training data needed for d"
D17-1156,D15-1123,0,0.0184422,"xt is available for the specific application domain, and it is We investigate techniques where domain adaptation starts from a pre-trained out-domain model, and only needs to process the in-domain corpus. Since we do not need to process the large out-domain corpus during adaptation, this is suitable for scenarios where adaptation must be performed quickly or where the original outdomain corpus is not available. Other works consider techniques that jointly train on the outdomain and in-domain corpora, distinguishing them using specific input features (Daume III, 2007; Finkel and Manning, 2009; Wuebker et al., 2015). These techniques are largely orthogonal to 1489 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1489–1494 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics ours1 and can be used in combination. In fact, Chu et al. (2017) successfully apply fine-tuning in combination with joint training. inal out-of-domain model was also trained with dropout. 2 L2-norm regularization is widely used for machine learning and statistical models. For linear models, it corresponds to imposing a diagonal Gaussian prior with zero"
D17-1156,W16-2323,1,\N,Missing
D17-1156,E17-3017,1,\N,Missing
D17-1156,P16-1162,1,\N,Missing
D17-1156,P16-1009,1,\N,Missing
D17-1156,N09-1068,0,\N,Missing
E14-1035,P12-2023,0,0.700385,"er translations within the same data set. If these distributions overlap, then we expect topic adaptation to help separate them and yield better translations than an unadapted system. Topics can be of varying granularity and are therefore a flexible means to structure data that is not uniform enough to be modelled in its entirety. In recent years there have been several attempts to integrating topical information into SMT either by learning better word alignments (Zhao and Xing, 2006), by adapting translation features cross-domain (Su et al., 2012), or by dynamically adapting lexical weights (Eidelman et al., 2012) or adding sparse topic features (Hasler et al., 2012). Translating text from diverse sources poses a challenge to current machine translation systems which are rarely adapted to structure beyond corpus level. We explore topic adaptation on a diverse data set and present a new bilingual variant of Latent Dirichlet Allocation to compute topic-adapted, probabilistic phrase translation features. We dynamically infer document-specific translation probabilities for test sets of unknown origin, thereby capturing the effects of document context on phrase translations. We show gains of up to 1.26 BLEU"
E14-1035,W07-0717,0,0.0868419,"gin, thereby capturing the effects of document context on phrase translations. We show gains of up to 1.26 BLEU over the baseline and 1.04 over a domain adaptation benchmark. We further provide an analysis of the domain-specific data and show additive gains of our model in combination with other types of topic-adapted features. 1 Introduction In statistical machine translation (SMT), there has been a lot of interest in trying to incorporate information about the provenance of training examples in order to improve translations for specific target domains. A popular approach are mixture models (Foster and Kuhn, 2007) where each component contains data from a specific genre or domain. Mixture models can be trained for crossdomain adaption when the target domain is known or for dynamic adaptation when the target domain is inferred from the source text under translation. More recent domain adaptation methods employ corpus or instance weights to promote relevant training examples (Matsoukas et al., 2009; Foster et al., 2010) or do more radical data selection based on language model perplexity (Axelrod et al., 2011). In this work, we are interested in the dynamic adaptation case, which is challenging because w"
E14-1035,D11-1033,0,0.0183881,"er to improve translations for specific target domains. A popular approach are mixture models (Foster and Kuhn, 2007) where each component contains data from a specific genre or domain. Mixture models can be trained for crossdomain adaption when the target domain is known or for dynamic adaptation when the target domain is inferred from the source text under translation. More recent domain adaptation methods employ corpus or instance weights to promote relevant training examples (Matsoukas et al., 2009; Foster et al., 2010) or do more radical data selection based on language model perplexity (Axelrod et al., 2011). In this work, we are interested in the dynamic adaptation case, which is challenging because we cannot tune our model towards any specific domain. In previous literature, domains have often been loosely defined in terms of corpora, for example, news texts would be defined as belonging to We take a new approach to topic adaptation by estimating probabilistic phrase translation features in a completely Bayesian fashion. The motivation is that automatically identifying topics in the training data can help to select the appropriate translation of a source phrase in the context of a document. By"
E14-1035,2012.iwslt-papers.17,1,0.830437,"ibutions overlap, then we expect topic adaptation to help separate them and yield better translations than an unadapted system. Topics can be of varying granularity and are therefore a flexible means to structure data that is not uniform enough to be modelled in its entirety. In recent years there have been several attempts to integrating topical information into SMT either by learning better word alignments (Zhao and Xing, 2006), by adapting translation features cross-domain (Su et al., 2012), or by dynamically adapting lexical weights (Eidelman et al., 2012) or adding sparse topic features (Hasler et al., 2012). Translating text from diverse sources poses a challenge to current machine translation systems which are rarely adapted to structure beyond corpus level. We explore topic adaptation on a diverse data set and present a new bilingual variant of Latent Dirichlet Allocation to compute topic-adapted, probabilistic phrase translation features. We dynamically infer document-specific translation probabilities for test sets of unknown origin, thereby capturing the effects of document context on phrase translations. We show gains of up to 1.26 BLEU over the baseline and 1.04 over a domain adaptation b"
E14-1035,W11-1014,0,0.331415,"r baseline counterparts after tuning, replacing them yielded better results in combination with the other adapted features. We believe the reason could be that fewer phrase table features in total are easier to optimise. |t| 2 f (x) = , 1 + 1x CC 110K 818 1892 Table 1: Number of sentence pairs and documents (in brackets) in the French-English data sets. The training data has 2.7M English words per domain. More topic-adapted features lazy MDI Mixed 354K (6450) 2453 (39) 5664 (112) relevance (4) (5) The third feature is a document similarity feature, similar to the semantic feature described by Banchs and Costa-jussà (2011): docSimt = max(1 − JSD(vtrain doci , vtest doc )) (6) i where vtrain_doci and vtest_doc are document topic vector of training and test documents. Because topic 0 captures phrase pairs that are common to many documents, we exclude it from the topic vectors before computing similarities. 4.1 Feature combination We tried integrating the four topic-adapted features separately and in all possible combinations. As we will see in the results section, while all features improve over the baseline in isolation, the adapted translation feature P(t|s,d) is the strongest feature. For the features that hav"
E14-1035,D11-1125,0,0.0087326,"containing data from all three domains and train one language model on the concatenation of (equally sized) target sides of the training data. Word alignments are trained on the concatenation of all training data and fixed for all models. Our baseline (ALL) is a phrase-based FrenchEnglish system trained on the concatenation of all parallel data. It was built with the Moses toolkit (Koehn et al., 2007) using the 14 standard core features including a 5gram language model. Translation quality is evaluated on a large test set, using the average feature weights of three optimisation runs with PRO (Hopkins and May, 2011). We use the mteval-v13a.pl script to compute caseinsensitive BLEU. As domain-aware benchmark systems, we use the phrase table fill-up method (FILLUP) of Bisazza et al. (2011) which preserves the translation scores of phrases from the IN model and the linear mixture models (LINTM) of Sennrich (2012b) (both available in the Moses toolkit). For both systems, we build separate phrase tables for each domain and use a wrapper to decode tuning and test sets with domainspecific tables. Both benchmarks have an advanModel IN ALL Mixed 26.77 26.86 CC 18.76 19.61 NC 29.56 29.42 TED 32.47 31.88 Model Ted-"
E14-1035,2011.iwslt-evaluation.18,0,0.0695227,"e concatenation of all training data and fixed for all models. Our baseline (ALL) is a phrase-based FrenchEnglish system trained on the concatenation of all parallel data. It was built with the Moses toolkit (Koehn et al., 2007) using the 14 standard core features including a 5gram language model. Translation quality is evaluated on a large test set, using the average feature weights of three optimisation runs with PRO (Hopkins and May, 2011). We use the mteval-v13a.pl script to compute caseinsensitive BLEU. As domain-aware benchmark systems, we use the phrase table fill-up method (FILLUP) of Bisazza et al. (2011) which preserves the translation scores of phrases from the IN model and the linear mixture models (LINTM) of Sennrich (2012b) (both available in the Moses toolkit). For both systems, we build separate phrase tables for each domain and use a wrapper to decode tuning and test sets with domainspecific tables. Both benchmarks have an advanModel IN ALL Mixed 26.77 26.86 CC 18.76 19.61 NC 29.56 29.42 TED 32.47 31.88 Model Ted-half vs Ted-full CC-half vs CC-full NC-half vs NC-full Table 2: BLEU of in-domain and baseline models. Model Ted-IN vs ALL CC-IN vs ALL NC-IN vs ALL Avg JSD 0.15 0.17 0.13 Avg"
E14-1035,W04-3250,1,0.221354,"The botton row of the table indicates the relative improvement of the best topic-adapted model per domain over the ALL model. Using all four topic-adapted features yields an improvement of 0.81 BLEU on the mixed test set. The highest improvement on a given domain is achieved for TED with an increase of 1.26 BLEU. The smallest improvement is measured on the NC domain. This is in line with the observation that distributions in the NC in-domain table are most similar to the ALL table, therefore we would expect the smallest improvement for domain or topic adaptation. We used bootstrap resampling (Koehn, 2004) to measure significance on the mixed test set and marked all statistically significant results compared to the respective baselines with asterisk (*: p ≤ 0.01). There is quite a clear horizontal separation between documents of different domains, for example, topics 6, 8, 19 occur mostly in Ted, NC and CC documents respectively. The overall structure is very similar between training (top) and test (bottom) documents, which shows that test inference was successful in carrying over the information learned on training documents. There is also some degree of topic sharing across domains, for examp"
E14-1035,D10-1005,0,0.00711491,"en a conversation and training documents in an additional feature. This is similar to the work of Banchs and Costa-jussà (2011), both of which inspired our document similarity feature. Also related is the work of Sennrich (2012a) who explore mixturemodelling on unsupervised clusters for domain adaptation and Chen et al. (2013) who compute phrase pair features from vector space representations that capture domain similarity to a development set. Both are cross-domain adaptation approaches, though. Instances of multilingual topic models outside the field of MT include BoydGraber and Blei (2009; Boyd-Graber and Resnik (2010) who learn cross-lingual topic correspondences (but do not learn conditional distributions like our model does). In terms of model structure, our model is similar to BiTAM (Zhao and Xing, 2006) which is an LDA-style model to learn topicbased word alignments. The work of Carpuat and Wu (2007) is similar to ours in spirit, but they predict the most probable translation in a context at the token level while our adaptation operates at the type level of a document. many IT-related documents). The last example, démon, has three frequent translations in English: devil, demon and daemon. The last tran"
E14-1035,2012.iwslt-papers.14,0,0.052881,"atures make use of the topic mixtures learned by our bilingual topic model. The first feature is an adapted lexical weight, similar to the features in the work of Eidelman et al. (2012). Our feature is different in that we marginalise over topics to produce a single adapted feature where v[k] is the kth element of a document topic vector for document d and w(t|s,k) is a topic-dependent word translation probability: lex(t¯|s, ¯ d) = 5 5.1 1 ∏ { j|(i, j) ∈ a} ∑ ∑ w(t|s, k) · v[k] (3) i ∀(i, j)∈a k | {z } w(t|s) The second feature is a target unigram feature similar to the lazy MDI adaptation of Ruiz and Federico (2012). It includes an additional term that measures the relevance of a target word wi by comparing its document-specific probability Pdoc to its probability under the asymmetric topic 0: |t| Pdoc (wi ) Pdoc (wi ) )· f ( ) trgUnigramst = ∏ f ( (wi ) Ptopic0 (wi ) i=1 |Pbaseline {z } | {z } x>0 NC 103K 817 1878 TED 140K 818 1894 the log-linear model. We found that while adding the features worked well and yielded close to zero weights for their baseline counterparts after tuning, replacing them yielded better results in combination with the other adapted features. We believe the reason could be that"
E14-1035,2007.tmi-papers.6,0,0.032213,"ation and Chen et al. (2013) who compute phrase pair features from vector space representations that capture domain similarity to a development set. Both are cross-domain adaptation approaches, though. Instances of multilingual topic models outside the field of MT include BoydGraber and Blei (2009; Boyd-Graber and Resnik (2010) who learn cross-lingual topic correspondences (but do not learn conditional distributions like our model does). In terms of model structure, our model is similar to BiTAM (Zhao and Xing, 2006) which is an LDA-style model to learn topicbased word alignments. The work of Carpuat and Wu (2007) is similar to ours in spirit, but they predict the most probable translation in a context at the token level while our adaptation operates at the type level of a document. many IT-related documents). The last example, démon, has three frequent translations in English: devil, demon and daemon. The last translation refers to a computer process and would occur in an IT context. The topic-phrase probabilities reveal that its mostly likely translation as daemon occurs under topic 19 which clusters IT-related phrase pairs and is frequent in the CC corpus. These examples show that our model can disa"
E14-1035,2012.eamt-1.60,0,0.0433237,"opic vectors before computing similarities. 4.1 Feature combination We tried integrating the four topic-adapted features separately and in all possible combinations. As we will see in the results section, while all features improve over the baseline in isolation, the adapted translation feature P(t|s,d) is the strongest feature. For the features that have a counterpart in the baseline model (p(t|s,d) and lex(t|s,d)), we experimented with either adding or replacing them in 331 Experimental setup Data and baselines Our experiments were carried out on a mixed data set, containing the TED corpus (Cettolo et al., 2012), parts of the News Commentary corpus (NC) and parts of the Commoncrawl corpus (CC) from the WMT13 shared task (Bojar et al., 2013) as described in Table 1. We were guided by two constraints in chosing our data set. 1) the data has document boundaries and the content of each document is assumed to be topically related, 2) there is some degree of topical variation within each data set. In order to compare to domain adaptation approaches, we chose a setup with data from different corpora. We want to abstract away from adaptation effects that concern tuning of length penalties and language models"
E14-1035,P06-2124,0,0.183172,"We view topic adaptation as fine-grained domain adaptation with the implicit assumption that there can be multiple distributions over translations within the same data set. If these distributions overlap, then we expect topic adaptation to help separate them and yield better translations than an unadapted system. Topics can be of varying granularity and are therefore a flexible means to structure data that is not uniform enough to be modelled in its entirety. In recent years there have been several attempts to integrating topical information into SMT either by learning better word alignments (Zhao and Xing, 2006), by adapting translation features cross-domain (Su et al., 2012), or by dynamically adapting lexical weights (Eidelman et al., 2012) or adding sparse topic features (Hasler et al., 2012). Translating text from diverse sources poses a challenge to current machine translation systems which are rarely adapted to structure beyond corpus level. We explore topic adaptation on a diverse data set and present a new bilingual variant of Latent Dirichlet Allocation to compute topic-adapted, probabilistic phrase translation features. We dynamically infer document-specific translation probabilities for te"
E14-1035,D10-1044,0,\N,Missing
E14-1035,D09-1074,0,\N,Missing
E14-1035,E12-1055,0,\N,Missing
E14-1035,P13-2122,0,\N,Missing
E17-3017,W14-3346,0,0.0173175,"2016). Nematus is available under a permissive BSD license. By default, the training objective in Nematus is cross-entropy minimization on a parallel training corpus. Training is performed via stochastic gradient descent, or one of its variants with adaptive learning rate (Adadelta (Zeiler, 2012), RmsProp (Tieleman and Hinton, 2012), Adam (Kingma and Ba, 2014)). Additionally, Nematus supports minimum risk training (MRT) (Shen et al., 2016) to optimize towards an arbitrary, sentence-level loss function. Various MT metrics are supported as loss function, including smoothed sentence-level B LEU (Chen and Cherry, 2014), METEOR (Denkowski and Lavie, 2011), BEER (Stanojevic and Sima’an, 2014), and any interpolation of implemented metrics. To stabilize training, Nematus supports early stopping based on cross entropy, or an arbitrary loss function defined by the user. 4 Conclusion Acknowledgments This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreements 645452 (QT21), 644333 (TraMOOC), 644402 (HimL) and 688139 (SUMMA). Usability Features References In addition to the main algorithms to train and decode with an NMT model, Nematus includes fe"
E17-3017,W11-2107,0,0.0197012,"r a permissive BSD license. By default, the training objective in Nematus is cross-entropy minimization on a parallel training corpus. Training is performed via stochastic gradient descent, or one of its variants with adaptive learning rate (Adadelta (Zeiler, 2012), RmsProp (Tieleman and Hinton, 2012), Adam (Kingma and Ba, 2014)). Additionally, Nematus supports minimum risk training (MRT) (Shen et al., 2016) to optimize towards an arbitrary, sentence-level loss function. Various MT metrics are supported as loss function, including smoothed sentence-level B LEU (Chen and Cherry, 2014), METEOR (Denkowski and Lavie, 2011), BEER (Stanojevic and Sima’an, 2014), and any interpolation of implemented metrics. To stabilize training, Nematus supports early stopping based on cross entropy, or an arbitrary loss function defined by the user. 4 Conclusion Acknowledgments This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreements 645452 (QT21), 644333 (TraMOOC), 644402 (HimL) and 688139 (SUMMA). Usability Features References In addition to the main algorithms to train and decode with an NMT model, Nematus includes features aimed towards facilitating ex"
E17-3017,W14-3354,0,0.0551932,"Missing"
E17-3017,E17-2025,0,0.133573,"Missing"
E17-3017,W16-2209,1,0.131218,"axout before the softmax layer. • In both encoder and decoder word embedding layers, we do not use additional biases. • Compared to Look, Generate, Update decoder phases in Bahdanau et al. (2015), we implement Look, Update, Generate which drastically simplifies the decoder implementation (see Table 1). • Optionally, we perform recurrent Bayesian dropout (Gal, 2015). • Instead of a single word embedding at each source position, our input representations allows multiple features (or “factors”) at each time step, with the final embedding being the concatenation of the embeddings of each feature (Sennrich and Haddow, 2016). • We allow tying of embedding matrices (Press and Wolf, 2017; Inan et al., 2016). We will here describe some differences in more detail: available at https://github.com/rsennrich/nematus https://github.com/nyu-dl/dl4mt-tutorial 65 Proceedings of the EACL 2017 Software Demonstrations, Valencia, Spain, April 3-7 2017, pages 65–68 c 2017 Association for Computational Linguistics z0j being the reset and update gate activations. In this formulation, W0 , U0 , Wr0 , U0r , Wz0 , U0z are trained model parameters; σ is the logistic sigmoid activation function. The attention mechanism, ATT, inputs the"
E17-3017,W16-2323,1,0.216734,"Sutskever et al., 2014) has recently established itself as a new state-of-the art in machine translation. We present Nematus1 , a new toolkit for Neural Machine Translation. Nematus has its roots in the dl4mt-tutorial.2 We found the codebase of the tutorial to be compact, simple and easy to extend, while also producing high translation quality. These characteristics make it a good starting point for research in NMT. Nematus has been extended to include new functionality based on recent research, and has been used to build top-performing systems to last year’s shared translation tasks at WMT (Sennrich et al., 2016) and IWSLT (Junczys-Dowmunt and Birch, 2016). Nematus is implemented in Python, and based on the Theano framework (Theano Development Team, 2016). It implements an attentional encoder–decoder architecture similar to Bahdanau et al. (2015). Our neural network architecture differs in some aspect from theirs, and we will discuss differences in more detail. We will also describe additional functionality, aimed to enhance usability and performance, which has been implemented in Nematus. 1 2 Neural Network Architecture • In the decoder, we use a feedforward hidden layer with tanh non-linearity rathe"
E17-3017,P16-1159,0,0.0772511,"ll documented toolkit to support their research. The toolkit is by no means limited to research, and has been used to train MT systems that are currently in production (WIPO, 2016). Nematus is available under a permissive BSD license. By default, the training objective in Nematus is cross-entropy minimization on a parallel training corpus. Training is performed via stochastic gradient descent, or one of its variants with adaptive learning rate (Adadelta (Zeiler, 2012), RmsProp (Tieleman and Hinton, 2012), Adam (Kingma and Ba, 2014)). Additionally, Nematus supports minimum risk training (MRT) (Shen et al., 2016) to optimize towards an arbitrary, sentence-level loss function. Various MT metrics are supported as loss function, including smoothed sentence-level B LEU (Chen and Cherry, 2014), METEOR (Denkowski and Lavie, 2011), BEER (Stanojevic and Sima’an, 2014), and any interpolation of implemented metrics. To stabilize training, Nematus supports early stopping based on cross entropy, or an arbitrary loss function defined by the user. 4 Conclusion Acknowledgments This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreements 645452 (QT2"
haddow-alex-2008-exploiting,brants-2000-inter,0,\N,Missing
haddow-alex-2008-exploiting,W07-1023,0,\N,Missing
haddow-alex-2008-exploiting,W03-0424,0,\N,Missing
haddow-alex-2008-exploiting,W07-1019,1,\N,Missing
haddow-alex-2008-exploiting,W06-3322,0,\N,Missing
haddow-alex-2008-exploiting,W07-1009,1,\N,Missing
haddow-alex-2008-exploiting,W03-0419,0,\N,Missing
haddow-alex-2008-exploiting,alex-etal-2006-impact,1,\N,Missing
N13-1035,2011.mtsummit-papers.32,0,0.0637851,"Missing"
N13-1035,W09-0432,0,0.0185738,"ork of Foster and Kuhn (2007), linear interpolation weights were derived from different measures of distance between the training corpora, but this was not found to be successful. Optimising the weights to minimise perplexity, as described in the introduction, was found by later authors to be more useful (Foster et al., 2010; Sennrich, 2012), generally showing small improvements over the default approach of concatenating all training data. An alternative approach is to use log-linear interpolation, so that the interpolation weights can be easily optimised in tuning (Koehn and Schroeder, 2007; Bertoldi and Federico, 2009; Banerjee et al., 2011). However, this effectively multiplies the probabilities across phrase tables, which does not seem appropriate, especially for phrases absent from 1 table. 2.2 Tuning SMT Systems The standard SMT model scores translation hypotheses as a linear combination of features. The model score of a hypothesis e is then defined to be w · h(e, f, a) where w is a weight vector, and h(e, f, a) a vector of feature functions defined over source sentences (f ), hypotheses, and their alignments (a). The weights are normally optimised (tuned) to maximise BLEU on a heldout set (the tuning"
N13-1035,W07-0718,0,0.0304239,"ures interpolated at sentence level. In reality the phrase features 1 Since the phrase penalty feature is a constant across phrase pairs it is not interpolated, and so is classed with the the “other” features. The lexical scores, although not actually probabilities, are interpolated. 344 are interpolated at the phrase level, then combined to give the sentence level feature value. This makes the definition of the objective more complex than that shown above, but still optimisable using bounded LBFGS. 3 Experiments 3.1 Corpus and Baselines We ran experiments with data from the WMT shared tasks (Callison-Burch et al., 2007; Callison-Burch et al., 2012), as well as OpenSubtitles data2 released by the OPUS project (Tiedemann, 2009). The experiments targeted both the newscommentary (nc) and OpenSubtitles (st) domains, with nc-devtest2007 and nc-test2007 for tuning and testing in the nc domain, respectively, and corresponding 2000 sentence tuning and test sets selected from the st data. The newscommentary v7 corpus and a 200k sentence corpus selected from the remaining st data were used as in-domain training data for the respective domains, with europarl v7 (ep) used as out-of-domain training data in both cases. Th"
N13-1035,W12-3102,0,0.0973613,"Missing"
N13-1035,N12-1047,0,0.062389,"ombination of features. The model score of a hypothesis e is then defined to be w · h(e, f, a) where w is a weight vector, and h(e, f, a) a vector of feature functions defined over source sentences (f ), hypotheses, and their alignments (a). The weights are normally optimised (tuned) to maximise BLEU on a heldout set (the tuning set). The most popular algorithm for this weight optimisation is the line-search based MERT (Och, 2003), but recently other algorithms that support more features, such as PRO (Hopkins and May, 2011) or MIRA-based algorithms (Watanabe et al., 2007; Chiang et al., 2008; Cherry and Foster, 2012), have been introduced. All these algorithms assume that the model score is a linear function of the parameters w. However since the phrase table probabilities enter the score function in log form, if these probabilities are a linear interpolation, then the model score is not a linear function of the interpolation weights. We will show that PRO can be used 343 2.3 Pairwise Ranked Optimisation For weights w, and hypothesis pair (xαi , xβi ), the (model) score difference ∆sw i is given by:   β w α w β α ) − s (x ) ≡ w · h − h (2) ∆sw ≡ s (x i i i i i Then the core PRO algorithm updates the wei"
N13-1035,D08-1024,0,0.0442458,"otheses as a linear combination of features. The model score of a hypothesis e is then defined to be w · h(e, f, a) where w is a weight vector, and h(e, f, a) a vector of feature functions defined over source sentences (f ), hypotheses, and their alignments (a). The weights are normally optimised (tuned) to maximise BLEU on a heldout set (the tuning set). The most popular algorithm for this weight optimisation is the line-search based MERT (Och, 2003), but recently other algorithms that support more features, such as PRO (Hopkins and May, 2011) or MIRA-based algorithms (Watanabe et al., 2007; Chiang et al., 2008; Cherry and Foster, 2012), have been introduced. All these algorithms assume that the model score is a linear function of the parameters w. However since the phrase table probabilities enter the score function in log form, if these probabilities are a linear interpolation, then the model score is not a linear function of the interpolation weights. We will show that PRO can be used 343 2.3 Pairwise Ranked Optimisation For weights w, and hypothesis pair (xαi , xβi ), the (model) score difference ∆sw i is given by:   β w α w β α ) − s (x ) ≡ w · h − h (2) ∆sw ≡ s (x i i i i i Then the core PRO"
N13-1035,P11-2031,0,0.0604369,"e used the standard Moses (Koehn et al., 2007) training pipeline, in particular employing the usual 5 phrase features – forward and backward phrase probabilities, forward and backward lexical scores and a phrase penalty. The 5-gram Kneser-Ney smoothed language models were trained by SRILM (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime. The language model is always a linear interpolation of models estimated on the in- and outof-domain corpora, with weights tuned by SRILM’s perplexity minimisation3 . All experiments were run three times with BLEU scores averaged, as recommended by Clark et al. (2011). Performance was evaluated using case-insensitive BLEU (Papineni et al., 2002), as implemented in Moses. The baseline systems were tuned using the Moses version of PRO, a reimplementation of the original 2 www.opensubtitles.org Our method could also be applied to language model interpolation but we chose to focus on phrase tables in this paper. 3 algorithm using the sampling scheme recommended by Hopkins and May. We ran 15 iterations of PRO, choosing the weights that maximised BLEU on the tuning set. For the PRO training of the interpolated models, we used the same sampling scheme, with optim"
N13-1035,W07-0717,0,0.127834,"oop). The core algorithm samples pairs of hypotheses from the nbest lists (according to a specific procedure), and uses these samples to optimise the weight vector w. The core algorithm in PRO will now be explained in more detail. Suppose that the N sampled hypothesis pairs (xαi , xβi ) are indexed by i and have corresponding feature vectors pairs (hαi , hβi ). If the gain of a given hypothesis (we use smoothed sentence BLEU ) is given by the function g(x), then we define yi by yi ≡ sgn(g(xαi ) − g(xβi )) (1) 2.1 Optimising Phrase Table Interpolation Weights Previous Approaches In the work of Foster and Kuhn (2007), linear interpolation weights were derived from different measures of distance between the training corpora, but this was not found to be successful. Optimising the weights to minimise perplexity, as described in the introduction, was found by later authors to be more useful (Foster et al., 2010; Sennrich, 2012), generally showing small improvements over the default approach of concatenating all training data. An alternative approach is to use log-linear interpolation, so that the interpolation weights can be easily optimised in tuning (Koehn and Schroeder, 2007; Bertoldi and Federico, 2009;"
N13-1035,D10-1044,0,0.216249,"Missing"
N13-1035,W12-3154,1,0.863009,"55 pro-mix +0.91 +0.48 Table 1: Mean BLEU relative to in system for each data set. System names as in Figure 1 . 4 www.scipy.org 345 4 Discussion and Conclusions The results show that the pro-mix method is a viable way of tuning systems built with interpolated phrase tables, and performs better than the current perplexity minimisation method on one of two data sets used in experiments. On the other data set (st), the out-of-domain data makes much less difference to the system performance in general, most probably because the difference between the in and outof-domain data sets in much larger (Haddow and Koehn, 2012). Whilst the differences between promix and perplexity minimisation are not large on the nc test set (about +0.5 BLEU) the results have been demonstrated to apply across many language pairs. The advantage of the pro-mix method over other approaches is that it directly optimises the measure that we are interested in, rather than optimising an intermediate measure and hoping that translation performance improves. In this work we optimise for BLEU , but the same method could easily be used to optimise for any sentence-level translation metric. Acknowledgments The research leading to these results"
N13-1035,W11-2123,0,0.019732,"ain training data in both cases. The language pairs we tested were the WMT language pairs for nc (English (en) to and from Spanish (es), German (de), French (fr) and Czech (cs)), with Dutch (nl) substituted for de in the st experiments. To build phrase-based translation systems, we used the standard Moses (Koehn et al., 2007) training pipeline, in particular employing the usual 5 phrase features – forward and backward phrase probabilities, forward and backward lexical scores and a phrase penalty. The 5-gram Kneser-Ney smoothed language models were trained by SRILM (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime. The language model is always a linear interpolation of models estimated on the in- and outof-domain corpora, with weights tuned by SRILM’s perplexity minimisation3 . All experiments were run three times with BLEU scores averaged, as recommended by Clark et al. (2011). Performance was evaluated using case-insensitive BLEU (Papineni et al., 2002), as implemented in Moses. The baseline systems were tuned using the Moses version of PRO, a reimplementation of the original 2 www.opensubtitles.org Our method could also be applied to language model interpolation but we chose to focus"
N13-1035,D11-1125,0,0.204599,"by addressing two of its shortcomings. The first problem is that the perplexity is not well defined because of the differing coverage of the phrase tables, and their partial coverage of the phrases extracted from the heldout set. Secondly, perplexity may not correlate with the performance of the final SMT system. So, instead of optimising the interpolation weights for the indirect goal of translation model perplexity, we optimise them directly for translation performance. We do this by incorporating these weights into SMT tuning using a modified version of Pairwise Ranked Optimisation (PRO) (Hopkins and May, 2011). In experiments on two different domain adaptation problems and 8 language pairs, we show that our method achieves comparable or improved performance, when compared to the perplexity minimisation method. This is an encouraging result as it 342 Proceedings of NAACL-HLT 2013, pages 342–347, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics shows that PRO can be adapted to optimise translation parameters other than those in the standard linear model. to simultaneously optimise such non-linear parameters. 2 PRO is a batch tuning algorithm in the sense that there i"
N13-1035,W07-0733,0,0.673722,"ther, and/or from the test data, then the problem of combining these disparate data sets to create the best possible translation system is known as domain adaptation. One approach to domain adaptation is to build separate models for each training domain, then weight them to create a system tuned to the test domain. In SMT, a successful approach to building domain specific language models is to build one from each corpus, then linearly interpolate them, choosing weights that minimise the perplexity on a suitable heldout set of in-domain data. This method has been applied by many authors (e.g. (Koehn and Schroeder, 2007)), and is implemented in popular language modelling tools like IRSTLM (Federico et al., 2008) and SRILM (Stolcke, 2002). Similar interpolation techniques have been developed for translation model interpolation (Foster et al., 2010; Sennrich, 2012) for phrase-based systems but have not been as widely adopted, perhaps because the efficacy of the methods is not as clearcut. In this previous work, the authors used standard phrase extraction heuristics to extract phrases from a heldout set of parallel sentences, then tuned the translation model (i.e. the phrase table) interpolation weights to minim"
N13-1035,P03-1021,0,0.0898607,"e tables, which does not seem appropriate, especially for phrases absent from 1 table. 2.2 Tuning SMT Systems The standard SMT model scores translation hypotheses as a linear combination of features. The model score of a hypothesis e is then defined to be w · h(e, f, a) where w is a weight vector, and h(e, f, a) a vector of feature functions defined over source sentences (f ), hypotheses, and their alignments (a). The weights are normally optimised (tuned) to maximise BLEU on a heldout set (the tuning set). The most popular algorithm for this weight optimisation is the line-search based MERT (Och, 2003), but recently other algorithms that support more features, such as PRO (Hopkins and May, 2011) or MIRA-based algorithms (Watanabe et al., 2007; Chiang et al., 2008; Cherry and Foster, 2012), have been introduced. All these algorithms assume that the model score is a linear function of the parameters w. However since the phrase table probabilities enter the score function in log form, if these probabilities are a linear interpolation, then the model score is not a linear function of the interpolation weights. We will show that PRO can be used 343 2.3 Pairwise Ranked Optimisation For weights w,"
N13-1035,P02-1040,0,0.0920135,"lar employing the usual 5 phrase features – forward and backward phrase probabilities, forward and backward lexical scores and a phrase penalty. The 5-gram Kneser-Ney smoothed language models were trained by SRILM (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime. The language model is always a linear interpolation of models estimated on the in- and outof-domain corpora, with weights tuned by SRILM’s perplexity minimisation3 . All experiments were run three times with BLEU scores averaged, as recommended by Clark et al. (2011). Performance was evaluated using case-insensitive BLEU (Papineni et al., 2002), as implemented in Moses. The baseline systems were tuned using the Moses version of PRO, a reimplementation of the original 2 www.opensubtitles.org Our method could also be applied to language model interpolation but we chose to focus on phrase tables in this paper. 3 algorithm using the sampling scheme recommended by Hopkins and May. We ran 15 iterations of PRO, choosing the weights that maximised BLEU on the tuning set. For the PRO training of the interpolated models, we used the same sampling scheme, with optimisation of the model weights and interpolation weights implemented in Python us"
N13-1035,E12-1055,0,0.666107,"n weight them to create a system tuned to the test domain. In SMT, a successful approach to building domain specific language models is to build one from each corpus, then linearly interpolate them, choosing weights that minimise the perplexity on a suitable heldout set of in-domain data. This method has been applied by many authors (e.g. (Koehn and Schroeder, 2007)), and is implemented in popular language modelling tools like IRSTLM (Federico et al., 2008) and SRILM (Stolcke, 2002). Similar interpolation techniques have been developed for translation model interpolation (Foster et al., 2010; Sennrich, 2012) for phrase-based systems but have not been as widely adopted, perhaps because the efficacy of the methods is not as clearcut. In this previous work, the authors used standard phrase extraction heuristics to extract phrases from a heldout set of parallel sentences, then tuned the translation model (i.e. the phrase table) interpolation weights to minimise the perplexity of the interpolated model on this set of extracted phrases. In this paper, we try to improve on this perplexity optimisation of phrase table interpolation weights by addressing two of its shortcomings. The first problem is that"
N13-1035,D07-1080,0,0.0543461,"scores translation hypotheses as a linear combination of features. The model score of a hypothesis e is then defined to be w · h(e, f, a) where w is a weight vector, and h(e, f, a) a vector of feature functions defined over source sentences (f ), hypotheses, and their alignments (a). The weights are normally optimised (tuned) to maximise BLEU on a heldout set (the tuning set). The most popular algorithm for this weight optimisation is the line-search based MERT (Och, 2003), but recently other algorithms that support more features, such as PRO (Hopkins and May, 2011) or MIRA-based algorithms (Watanabe et al., 2007; Chiang et al., 2008; Cherry and Foster, 2012), have been introduced. All these algorithms assume that the model score is a linear function of the parameters w. However since the phrase table probabilities enter the score function in log form, if these probabilities are a linear interpolation, then the model score is not a linear function of the interpolation weights. We will show that PRO can be used 343 2.3 Pairwise Ranked Optimisation For weights w, and hypothesis pair (xαi , xβi ), the (model) score difference ∆sw i is given by:   β w α w β α ) − s (x ) ≡ w · h − h (2) ∆sw ≡ s (x i i i"
N13-1035,P07-2045,0,\N,Missing
N16-1005,2015.iwslt-papers.5,0,0.0232567,"2) have used a bilingual English–German corpus to automatically annotate the T-V distinction, and train a classifier to predict the address from monolingual English text. Applying a source-side classifier is potential future work, although we note that the baseline encoder– decoder NMT system already has some disambiguating power. Our T-V classification is more comprehensive, including more pronoun forms and imperative verbs. Previous research on neural language models has proposed including various types of extra information, such as topic, genre or document context (Mikolov and Zweig, 2012; Aransa et al., 2015; Ji et al., 2015; Wang and Cho, 2015). Our method is somewhat similar, with the main novel idea being that we can target specific phenomena, such as honorifics, via an automatic annotation of the target side of a parallel corpus. On the modelling side, our method is slightly different in that we pass the extra information to the encoder of an encoder–decoder network, rather than the (decoder) hidden layer or output layer. We found this to be very effective, but trying different architectures is potential future work. In rule-based machine translation, user options to control the level of poli"
N16-1005,D14-1179,0,0.022336,"Missing"
N16-1005,E12-1064,0,0.0189134,"LEU indicates that the T-V distinction is relevant for translation. We expect that the actual relevance for humans depends on the task. For gisting, we expect the T-V distinction to have little effect on comprehensibility. For professional translation that uses MT with postediting, producing the desired honorifics is likely to improve post-editing speed and satisfaction. In an evaluation of MT for subtitle translation, Etchegoyhen et al. (2014) highlight the production of the appropriate T-V form as “a limitation of MT technology” that was “often frustrat[ing]” to post-editors. 6 Related Work Faruqui and Pado (2012) have used a bilingual English–German corpus to automatically annotate the T-V distinction, and train a classifier to predict the address from monolingual English text. Applying a source-side classifier is potential future work, although we note that the baseline encoder– decoder NMT system already has some disambiguating power. Our T-V classification is more comprehensive, including more pronoun forms and imperative verbs. Previous research on neural language models has proposed including various types of extra information, such as topic, genre or document context (Mikolov and Zweig, 2012; Ar"
N16-1005,P15-1001,0,0.153295,"describe rules to automatically annotate the T-V distinction in German text. • we describe how to use target-side T-V annotation in NMT training to control the level of politeness at test time via side constraints. • we perform oracle experiments to demonstrate the impact of controlling politeness in NMT. 2 Background: Neural Machine Translation Attentional neural machine translation (Bahdanau et al., 2015) is the current state of the art for 35 Proceedings of NAACL-HLT 2016, pages 35–40, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics English→German (Jean et al., 2015b; Luong and Manning, 2015). We follow the neural machine translation architecture by Bahdanau et al. (2015), which we will briefly summarize here. However, our approach is not specific to this architecture. The neural machine translation system is implemented as an attentional encoder-decoder network. The encoder is a bidirectional neural network with gated recurrent units (Cho et al., 2014) that reads an input sequence x = (x1 , ..., xm ) and calculates − → −→ a forward sequence of hidden states (h1 , ..., hm ), ← − ←− and a backward sequence (h1 , ..., hm ). The hidden → − ← − states hj and"
N16-1005,2015.iwslt-evaluation.11,0,0.149954,"Missing"
N16-1005,I05-6011,0,0.0321512,"roach is simple and applicable to a wide range of NMT architectures and our experiments suggest that the incorporation of the side constraint as an extra source token is very effective. 4 Automatic Training Set Annotation Our approach relies on annotating politeness in the training set to obtain the politeness feature which we discussed previously. We choose a sentence-level annotation because a target-side honorific may have no word-level correspondence in the source. We will discuss the annotation of German as an example, but our method could be applied to other languages, such as Japanese (Nariyama et al., 2005). German has distinct pronoun forms for informal and polite address, as shown in Table 1. A further difference between informal and polite speech are imperative verbs, and the original imperative forms are considered informal. The polite alternative is to use 3rd person plural forms with subject in position 2: • Ruf mich zurück. (informal) (Call me back.) • Rufen Sie mich zurück. (polite) (Call you me back.) We automatically annotate politeness on a sentence level with rules based on a morphosyntactic annotation by ParZu (Sennrich et al., 2013). Sentences containing imperative verbs are labell"
N16-1005,R13-1079,1,0.389147,"Missing"
N16-1005,tiedemann-2012-parallel,0,0.0240309,"olite forms and (neutral) 3rd person forms by their capitalization. If a sentence matches rules for both classes, we label it as informal – we found that our lowestprecision rule is the annotation of sentence-initial Sie. All sentences without a match are considered neutral. 5 Evaluation Our empirical research questions are as follows: • can we control the production of honorifics in neural machine translation via side constraints? • how important is the T-V distinction for translation quality (as measured by B LEU)? 5.1 Data and Methods We perform English→German experiments on OpenSubtitles (Tiedemann, 2012)1 , a parallel corpus of movie subtitles. Machine translation is commonly used in the professional translation of movie subtitles in a post-editing workflow, and politeness is considered an open problem for subtitle translation (Etchegoyhen et al., 2014). We use OpenSubtitles2012 as training corpus, and random samples from OpenSubtitles2013 for testing. The training corpus consists of of 5.58 million sentence pairs, out of which we label 0.48 million sentence pairs as polite, and 1.09 million as informal. We train an attentional encoder-decoder NMT system using Groundhog2 (Bahdanau et al., 201"
N16-1005,P16-1162,1,\N,Missing
N18-1118,W16-2348,1,0.898526,"Missing"
N18-1118,W09-2404,0,0.253424,"Missing"
N18-1118,L16-1100,0,0.184876,"Missing"
N18-1118,W16-2360,0,0.064602,"Missing"
N18-1118,D17-1263,0,0.069335,"Missing"
N18-1118,E17-3017,1,0.865828,"Missing"
N18-1118,P16-1162,1,0.644258,"Missing"
N18-1118,P17-2031,0,0.068079,"Missing"
N18-1118,L16-1147,0,0.126825,"Missing"
N18-1118,W17-4811,0,0.207132,"Missing"
N18-1118,D17-1301,0,0.230729,"Missing"
N18-1118,N16-1004,0,0.0681389,"Missing"
N18-1118,P02-1040,0,0.10402,"Missing"
N18-1118,W17-4702,1,0.90194,"Missing"
P16-1009,W09-0432,0,0.169739,"obtain another improvement of 0.8–1.0 B LEU. Back-translation Quality for Synthetic Data One question that our previous experiments leave open is how the quality of the automatic backtranslation affects training with synthetic data. To investigate this question, we back-translate the same German monolingual corpus with three different German→English systems: • with our baseline system and greedy decoding 4.3 Contrast to Phrase-based SMT The back-translation of monolingual target data into the source language to produce synthetic parallel text has been previously explored for phrasebased SMT (Bertoldi and Federico, 2009; Lambert et al., 2011). While our approach is technically similar, synthetic parallel data fulfills novel roles • with our baseline system and beam search (beam size 12). This is the same system used for the experiments in Table 3. 10 We also experimented with higher ratios of monolingual data, but this led to decreased B LEU scores. 91 name training data baseline (Gülçehre et al., 2015) deep fusion (Gülçehre et al., 2015) baseline parallel parallelsynth parallel/parallelsynth Gigawordmono parallel/Gigawordmono Gigawordsynth parallel/Gigawordsynth instances tst2011 18.4 20.2 18.6 19.9 18.8 21"
P16-1009,P15-1001,0,0.225572,"quences of subword units (Sennrich et al., 2016), and can represent any additional training data with the existing network vocabulary that was learned on the parallel data. In all experiments, the network vocabulary remains fixed. 4.1.1 We evaluate NMT training on parallel text, and with additional monolingual data, on English↔German and Turkish→English, using training and test data from WMT 15 for English↔German, IWSLT 15 for English→German, and IWSLT 14 for Turkish→English. Data and Methods We use Groundhog3 as the implementation of the NMT system for all experiments (Bahdanau et al., 2015; Jean et al., 2015a). We generally follow the settings and training procedure described by Sennrich et al. (2016). For English↔German, we report case-sensitive B LEU on detokenized text with mteval-v13a.pl for comparison to official WMT and IWSLT results. For Turkish→English, we report case-sensitive B LEU on tokenized text with multi-bleu.perl for comparison to results by Gülçehre et al. (2015). Gülçehre et al. (2015) determine the network vocabulary based on the parallel training data, 3 English↔German We use all parallel training data provided by WMT 2015 (Bojar et al., 2015)4 . We use the News Crawl corpora"
P16-1009,J90-2002,0,0.474829,"Missing"
P16-1009,2012.eamt-1.60,0,0.0565125,"n et al., 2012). We also use early stopping, based on B LEU measured every three hours on tst2010, which we treat as development set. For Turkish→English, we use gradient clipping with threshold 5, following Gülçehre et al. (2015), in contrast to the threshold 1 that we use for English↔German, following Jean et al. (2015a). Table 2: Turkish→English training data. cabulary size is 90 000. We also perform experiments on the IWSLT 15 test sets to investigate a cross-domain setting.5 The test sets consist of TED talk transcripts. As indomain training data, IWSLT provides the WIT3 parallel corpus (Cettolo et al., 2012), which also consists of TED talks. 4.1.2 4.2 4.2.1 Results English→German WMT 15 Table 3 shows English→German results with WMT training and test data. We find that mixing parallel training data with monolingual data with a dummy source side in a ratio of 1-1 improves quality by 0.4–0.5 B LEU for the single system, 1 B LEU for the ensemble. We train the system for twice as long as the baseline to provide the training algorithm with a similar amount of parallel training instances. To ensure that the quality improvement is due to the monolingual training instances, and not just increased trainin"
P16-1009,P07-2045,1,0.0662497,"Table 8: Phrase-based SMT results (English→German) on WMT test sets (average of newstest201{4,5}), and IWSLT test sets (average of tst201{3,4,5}), and average B LEU gain from adding synthetic data for both PBSMT and NMT. 6 4 2 0 5 10 15 20 training time (training instances 25 30 ·106 ) Figure 1: Turkish→English training and development set (tst2010) cross-entropy as a function of training time (number of training instances) for different systems. in NMT. To explore the relative effectiveness of backtranslated data for phrase-based SMT and NMT, we train two phrase-based SMT systems with Moses (Koehn et al., 2007), using only WMTparallel , or both WMTparallel and WMTsynth_de for training the translation and reordering model. Both systems contain the same language model, a 5-gram Kneser-Ney model trained on all available WMT data. We use the baseline features described by Haddow et al. (2015). Results are shown in Table 8. In phrase-based SMT, we find that the use of back-translated training data has a moderate positive effect on the WMT test sets (+0.7 B LEU), but not on the IWSLT test sets. This is in line with the expectation that the main effect of back-translated data for phrasebased SMT is domain"
P16-1009,W11-2132,0,0.0362203,"f 0.8–1.0 B LEU. Back-translation Quality for Synthetic Data One question that our previous experiments leave open is how the quality of the automatic backtranslation affects training with synthetic data. To investigate this question, we back-translate the same German monolingual corpus with three different German→English systems: • with our baseline system and greedy decoding 4.3 Contrast to Phrase-based SMT The back-translation of monolingual target data into the source language to produce synthetic parallel text has been previously explored for phrasebased SMT (Bertoldi and Federico, 2009; Lambert et al., 2011). While our approach is technically similar, synthetic parallel data fulfills novel roles • with our baseline system and beam search (beam size 12). This is the same system used for the experiments in Table 3. 10 We also experimented with higher ratios of monolingual data, but this led to decreased B LEU scores. 91 name training data baseline (Gülçehre et al., 2015) deep fusion (Gülçehre et al., 2015) baseline parallel parallelsynth parallel/parallelsynth Gigawordmono parallel/Gigawordmono Gigawordsynth parallel/Gigawordsynth instances tst2011 18.4 20.2 18.6 19.9 18.8 21.2 7.2m 6m/6m 7.6m/7.6m"
P16-1009,D14-1179,0,0.0846514,"Missing"
P16-1009,2015.iwslt-evaluation.11,0,0.370228,"MorphTR 6 89 name training instances syntax-based (Sennrich and Haddow, 2015) Neural MT (Jean et al., 2015b) parallel 37m (parallel) +monolingual 49m (parallel) / 49m (monolingual) +synthetic 44m (parallel) / 36m (synthetic) B LEU newstest2014 newstest2015 single ens-4 single ens-4 22.6 24.4 22.4 19.9 20.4 22.8 23.6 20.4 21.4 23.2 24.6 22.7 23.8 25.7 26.5 Table 3: English→German translation performance (B LEU) on WMT training/test sets. Ens-4: ensemble of 4 models. Number of training instances varies due to differences in training time and speed. name 1 2 3 4 5 fine-tuning data instances NMT (Luong and Manning, 2015) (single model) NMT (Luong and Manning, 2015) (ensemble of 8) parallel +synthetic 2+WITmono_de WMTparallel / WITmono 200k/200k 2+WITsynth_de WITsynth 200k 2+WITparallel WIT 200k tst2013 29.4 31.4 25.2 26.5 26.6 28.2 30.4 B LEU tst2014 27.6 22.6 23.5 23.6 24.4 25.9 tst2015 30.1 24.0 25.5 25.4 26.7 28.4 Table 4: English→German translation performance (B LEU) on IWSLT test sets (TED talks). Single models. test sets, which are news texts. We investigate if monolingual training data is especially valuable if it can be used to adapt a model to a new genre or domain, specifically adapting a system tr"
P16-1009,D15-1166,0,0.659364,"lgorithm with a similar amount of parallel training instances. To ensure that the quality improvement is due to the monolingual training instances, and not just increased training time, we also continued training our baseline system for another week, but saw no improvements in B LEU. Including synthetic data during training is very effective, and yields an improvement over our baseline by 2.8–3.4 B LEU. Our best ensemble system also outperforms a syntax-based baseline (Sennrich and Haddow, 2015) by 1.2–2.1 B LEU. We also substantially outperform NMT results reported by Jean et al. (2015a) and Luong et al. (2015), who previously reported SOTA result.8 We note that the difference is particularly large for single systems, since our ensemble is not as diverse as that of Luong et al. (2015), who used 8 independently trained ensemble components, whereas we sampled 4 ensemble components from the same training run. Turkish→English We use data provided for the IWSLT 14 machine translation track (Cettolo et al., 2014), namely the WIT3 parallel corpus (Cettolo et al., 2012), which consists of TED talks, and the SETimes corpus (Tyers and Alperen, 2010).6 After removal of sentence pairs which contain empty lines"
P16-1009,W15-3013,1,0.372853,"06 ) Figure 1: Turkish→English training and development set (tst2010) cross-entropy as a function of training time (number of training instances) for different systems. in NMT. To explore the relative effectiveness of backtranslated data for phrase-based SMT and NMT, we train two phrase-based SMT systems with Moses (Koehn et al., 2007), using only WMTparallel , or both WMTparallel and WMTsynth_de for training the translation and reordering model. Both systems contain the same language model, a 5-gram Kneser-Ney model trained on all available WMT data. We use the baseline features described by Haddow et al. (2015). Results are shown in Table 8. In phrase-based SMT, we find that the use of back-translated training data has a moderate positive effect on the WMT test sets (+0.7 B LEU), but not on the IWSLT test sets. This is in line with the expectation that the main effect of back-translated data for phrasebased SMT is domain adaptation (Bertoldi and Federico, 2009). Both the WMT test sets and the News Crawl corpora which we used as monolingual data come from the same source, a web crawl of newspaper articles.11 In contrast, News Crawl is out-of-domain for the IWSLT test sets. In contrast to phrase-based"
P16-1009,N06-1020,0,0.0227411,"parameters are tuned on further parallel training data, but the language model parameters are fixed during the finetuning stage. Jean et al. (2015b) also report on experiments with reranking of NMT output with a 5-gram language model, but improvements are small (between 0.1–0.5 B LEU). The production of synthetic parallel texts bears resemblance to data augmentation techniques used in computer vision, where datasets are often augmented with rotated, scaled, or otherwise distorted variants of the (limited) training set (Rowley et al., 1996). Another similar avenue of research is selftraining (McClosky et al., 2006; Schwenk, 2008). The main difference is that self-training typically refers to scenario where the training set is enhanced with training instances with artificially produced output labels, whereas we start with human-produced output (i.e. the translation), and artificially produce an input. We expect that this is more robust towards noise in the automatic translation. Improving NMT with monolingual source data, following similar work on phrasebased SMT (Schwenk, 2008), remains possible future work. Domain adaptation of neural networks via continued training has been shown to be effective for"
P16-1009,2008.iwslt-papers.6,0,0.00923665,"n further parallel training data, but the language model parameters are fixed during the finetuning stage. Jean et al. (2015b) also report on experiments with reranking of NMT output with a 5-gram language model, but improvements are small (between 0.1–0.5 B LEU). The production of synthetic parallel texts bears resemblance to data augmentation techniques used in computer vision, where datasets are often augmented with rotated, scaled, or otherwise distorted variants of the (limited) training set (Rowley et al., 1996). Another similar avenue of research is selftraining (McClosky et al., 2006; Schwenk, 2008). The main difference is that self-training typically refers to scenario where the training set is enhanced with training instances with artificially produced output labels, whereas we start with human-produced output (i.e. the translation), and artificially produce an input. We expect that this is more robust towards noise in the automatic translation. Improving NMT with monolingual source data, following similar work on phrasebased SMT (Schwenk, 2008), remains possible future work. Domain adaptation of neural networks via continued training has been shown to be effective for neural language"
P16-1009,D15-1248,1,0.477278,"r the single system, 1 B LEU for the ensemble. We train the system for twice as long as the baseline to provide the training algorithm with a similar amount of parallel training instances. To ensure that the quality improvement is due to the monolingual training instances, and not just increased training time, we also continued training our baseline system for another week, but saw no improvements in B LEU. Including synthetic data during training is very effective, and yields an improvement over our baseline by 2.8–3.4 B LEU. Our best ensemble system also outperforms a syntax-based baseline (Sennrich and Haddow, 2015) by 1.2–2.1 B LEU. We also substantially outperform NMT results reported by Jean et al. (2015a) and Luong et al. (2015), who previously reported SOTA result.8 We note that the difference is particularly large for single systems, since our ensemble is not as diverse as that of Luong et al. (2015), who used 8 independently trained ensemble components, whereas we sampled 4 ensemble components from the same training run. Turkish→English We use data provided for the IWSLT 14 machine translation track (Cettolo et al., 2014), namely the WIT3 parallel corpus (Cettolo et al., 2012), which consists of T"
P16-1009,P16-1162,1,0.590859,"monolingual data set into English. The German→English system used for this is the baseline system (parallel). Translation took about a week on an NVIDIA Titan Black GPU. For experiments in German→English, we back-translate 4 200 000 monolingual English sentences into German, using the English→German system +synthetic. Note that we always use single models for backtranslation, not ensembles. We leave it to future work to explore how sensitive NMT training with synthetic data is to the quality of the backtranslation. We tokenize and truecase the training data, and represent rare words via BPE (Sennrich et al., 2016). Specifically, we follow Sennrich et al. (2016) in performing BPE on the joint vocabulary with 89 500 merge operations. The network voEvaluation 4.1 sentences 4 200 000 200 000 160 000 000 3 600 000 118 000 000 4 200 000 4 github.com/sebastien-j/LV_groundhog 88 http://www.statmt.org/wmt15/ dataset WIT SETimes Gigawordmono Gigawordsynth sentences 160 000 160 000 177 000 000 3 200 000 We found overfitting to be a bigger problem than with the larger English↔German data set, and follow Gülçehre et al. (2015) in using Gaussian noise (stddev 0.01) (Graves, 2011), and dropout on the output layer (p="
P16-1009,W15-4006,0,0.0304229,"Missing"
P16-1162,D15-1249,0,0.114088,": 2, &apos;n e w e s t &lt;/w&gt;&apos;:6, &apos;w i d e s t &lt;/w&gt;&apos;:3} num_merges = 10 for i in range(num_merges): pairs = get_stats(vocab) best = max(pairs, key=pairs.get) vocab = merge_vocab(best, vocab) print(best) r· lo lo w e r· → → → → r· lo low er· Figure 1: BPE merge operations learned from dictionary {‘low’, ‘lowest’, ‘newer’, ‘wider’}. gorithm 1. In practice, we increase efficiency by indexing all pairs, and updating data structures incrementally. The main difference to other compression algorithms, such as Huffman encoding, which have been proposed to produce a variable-length encoding of words for NMT (Chitnis and DeNero, 2015), is that our symbol sequences are still interpretable as subword units, and that the network can generalize to translate and produce new words (unseen at training time) on the basis of these subword units. Figure 1 shows a toy example of learned BPE operations. At test time, we first split words into sequences of characters, then apply the learned operations to merge the characters into larger, known symbols. This is applicable to any word, and allows for open-vocabulary networks with fixed symbol vocabularies.3 In our example, the OOV ‘lower’ would be segmented into ‘low er·’. 3 The only sym"
P16-1162,D14-1179,0,0.315754,"Missing"
P16-1162,W02-0603,0,0.0880881,"t of the k most frequent word types unsegmented. Only the unigram representation is truly open-vocabulary. However, the unigram representation performed poorly in preliminary experiments, and we report translation results with a bigram representation, which is empirically better, but unable to produce some tokens in the test set with the training set vocabulary. We report statistics for several word segmentation techniques that have proven useful in previous SMT research, including frequency-based compound splitting (Koehn and Knight, 2003), rulebased hyphenation (Liang, 1983), and Morfessor (Creutz and Lagus, 2002). We find that they only moderately reduce vocabulary size, and do not solve the unknown word problem, and we thus find them unsuitable for our goal of open-vocabulary translation without back-off dictionary. BPE meets our goal of being open-vocabulary, and the learned merge operations can be applied to the test set to obtain a segmentation with no unknown symbols.10 Its main difference from the character-level model is that the more compact representation of BPE allows for shorter sequences, and that the attention model operates on variable-length units.11 Table 1 shows BPE with 59 500 merge"
P16-1162,E14-4029,0,0.0798953,"ranslated independently, our NMT models show robustness towards oversplitting. 2 Subword Translation The main motivation behind this paper is that the translation of some words is transparent in 1716 pothesis in Sections 4 and 5. First, we discuss different subword representations. 3.1 Related Work For Statistical Machine Translation (SMT), the translation of unknown words has been the subject of intensive research. A large proportion of unknown words are names, which can just be copied into the target text if both languages share an alphabet. If alphabets differ, transliteration is required (Durrani et al., 2014). Character-based translation has also been investigated with phrase-based models, which proved especially successful for closely related languages (Vilar et al., 2007; Tiedemann, 2009; Neubig et al., 2012). The segmentation of morphologically complex words such as compounds is widely used for SMT, and various algorithms for morpheme segmentation have been investigated (Nießen and Ney, 2000; Koehn and Knight, 2003; Virpioja et al., 2007; Stallard et al., 2012). Segmentation algorithms commonly used for phrase-based SMT tend to be conservative in their splitting decisions, whereas we aim for an"
P16-1162,N13-1073,0,0.0814933,"s), and continue training each with a fixed embedding layer (as suggested by (Jean et al., 2015)) for 12 hours. We perform two independent training runs for each models, once with cut-off for gradient clipping (Pascanu et al., 2013) of 5.0, once with a cut-off of 1.0 – the latter produced better single models for most settings. We report results of the system that performed best on our development set (newstest2013), and of an ensemble of all 8 models. We use a beam size of 12 for beam search, with probabilities normalized by sentence length. We use a bilingual dictionary based on fast-align (Dyer et al., 2013). For our baseline, this serves as back-off dictionary for rare words. We also use the dictionary to speed up translation for all experiments, only performing the softmax over a filtered list of candidate translations (like Jean et al. (2015), we use K = 30000; K 0 = 10). 4.1 Subword statistics Apart from translation quality, which we will verify empirically, our main objective is to represent an open vocabulary through a compact fixed-size subword vocabulary, and allow for efficient training and decoding.8 Statistics for different segmentations of the Ger6 Clipped unigram precision is essenti"
P16-1162,W15-3013,1,0.108755,"ased system in terms of B LEU, but not in terms of CHR F3. Regarding other neural systems, Luong et al. (2015a) report a B LEU score of 25.9 on newstest2015, but we note that they use an ensemble of 8 independently trained models, and also report strong improvements from applying dropout, which we did not use. We are confident that our improvements to the translation of rare words are orthogonal to improvements achievable through other improvements in the network archi1720 tecture, training algorithm, or better ensembles. For English→Russian, the state of the art is the phrase-based system by Haddow et al. (2015). It outperforms our WDict baseline by 1.5 B LEU. The subword models are a step towards closing this gap, and BPE-J90k yields an improvement of 1.3 B LEU, and 2.0 CHR F3, over WDict. As a further comment on our translation results, we want to emphasize that performance variability is still an open problem with NMT. On our development set, we observe differences of up to 1 B LEU between different models. For single systems, we report the results of the model that performs best on dev (out of 8), which has a stabilizing effect, but how to control for randomness deserves further attention in futu"
P16-1162,P15-1001,0,0.284282,"z o.o. Samsung R&D Institute Poland. lem, and especially for languages with productive word formation processes such as agglutination and compounding, translation models require mechanisms that go below the word level. As an example, consider compounds such as the German Abwasser|behandlungs|anlange ‘sewage water treatment plant’, for which a segmented, variable-length representation is intuitively more appealing than encoding the word as a fixed-length vector. For word-level NMT models, the translation of out-of-vocabulary words has been addressed through a back-off to a dictionary look-up (Jean et al., 2015; Luong et al., 2015b). We note that such techniques make assumptions that often do not hold true in practice. For instance, there is not always a 1-to-1 correspondence between source and target words because of variance in the degree of morphological synthesis between languages, like in our introductory compounding example. Also, word-level models are unable to translate or generate unseen words. Copying unknown words into the target text, as done by (Jean et al., 2015; Luong et al., 2015b), is a reasonable strategy for names, but morphological changes and transliteration is often required, e"
P16-1162,D13-1176,0,0.0821851,"ansliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 B LEU, respectively. 1 Introduction Neural machine translation has recently shown impressive results (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). However, the translation of rare words is an open problem. The vocabulary of neural models is typically limited to 30 000–50 000 words, but translation is an open-vocabulary probThe research presented in this publication was conducted in cooperation with Samsung Electronics Polska sp. z o.o. Samsung R&D Institute Poland. lem, and especially for languages with productive word formation processes such as agglutination and compounding, translation models require mechanisms that go below the word level. As an example, consider compounds such as the"
P16-1162,E03-1076,0,0.106965,"ch. A large proportion of unknown words are names, which can just be copied into the target text if both languages share an alphabet. If alphabets differ, transliteration is required (Durrani et al., 2014). Character-based translation has also been investigated with phrase-based models, which proved especially successful for closely related languages (Vilar et al., 2007; Tiedemann, 2009; Neubig et al., 2012). The segmentation of morphologically complex words such as compounds is widely used for SMT, and various algorithms for morpheme segmentation have been investigated (Nießen and Ney, 2000; Koehn and Knight, 2003; Virpioja et al., 2007; Stallard et al., 2012). Segmentation algorithms commonly used for phrase-based SMT tend to be conservative in their splitting decisions, whereas we aim for an aggressive segmentation that allows for open-vocabulary translation with a compact network vocabulary, and without having to resort to back-off dictionaries. The best choice of subword units may be taskspecific. For speech recognition, phone-level language models have been used (Bazzi and Glass, 2000). Mikolov et al. (2012) investigate subword language models, and propose to use syllables. For multilingual segmen"
P16-1162,P07-2045,1,0.138461,"ranslation of rare and unseen words in neural machine translation by representing them via subword units? • Which segmentation into subword units performs best in terms of vocabulary size, text size, and translation quality? We perform experiments on data from the shared translation task of WMT 2015. For English→German, our training set consists of 4.2 million sentence pairs, or approximately 100 million tokens. For English→Russian, the training set consists of 2.6 million sentence pairs, or approximately 50 million tokens. We tokenize and truecase the data with the scripts provided in Moses (Koehn et al., 2007). We use newstest2013 as development set, and report results on newstest2014 and newstest2015. We report results with B LEU (mteval-v13a.pl), and CHR F3 (Popovi´c, 2015), a character n-gram F3 score which was found to correlate well with 4 In practice, we simply concatenate the source and target side of the training set to learn joint BPE. 5 Since the Russian training text also contains words that use the Latin alphabet, we also apply the Latin BPE operations. 1718 human judgments, especially for translations out of English (Stanojevi´c et al., 2015). Since our main claim is concerned with the"
P16-1162,D15-1176,0,0.0929974,"resort to back-off dictionaries. The best choice of subword units may be taskspecific. For speech recognition, phone-level language models have been used (Bazzi and Glass, 2000). Mikolov et al. (2012) investigate subword language models, and propose to use syllables. For multilingual segmentation tasks, multilingual algorithms have been proposed (Snyder and Barzilay, 2008). We find these intriguing, but inapplicable at test time. Various techniques have been proposed to produce fixed-length continuous word vectors based on characters or morphemes (Luong et al., 2013; Botha and Blunsom, 2014; Ling et al., 2015a; Kim et al., 2015). An effort to apply such techniques to NMT, parallel to ours, has found no significant improvement over word-based approaches (Ling et al., 2015b). One technical difference from our work is that the attention mechanism still operates on the level of words in the model by Ling et al. (2015b), and that the representation of each word is fixed-length. We expect that the attention mechanism benefits from our variable-length representation: the network can learn to place attention on different subword units at each step. Recall our introductory example Abwasserbehandlungsanlang"
P16-1162,W13-3512,0,0.0897464,"Missing"
P16-1162,D15-1166,0,0.660487,"Institute Poland. lem, and especially for languages with productive word formation processes such as agglutination and compounding, translation models require mechanisms that go below the word level. As an example, consider compounds such as the German Abwasser|behandlungs|anlange ‘sewage water treatment plant’, for which a segmented, variable-length representation is intuitively more appealing than encoding the word as a fixed-length vector. For word-level NMT models, the translation of out-of-vocabulary words has been addressed through a back-off to a dictionary look-up (Jean et al., 2015; Luong et al., 2015b). We note that such techniques make assumptions that often do not hold true in practice. For instance, there is not always a 1-to-1 correspondence between source and target words because of variance in the degree of morphological synthesis between languages, like in our introductory compounding example. Also, word-level models are unable to translate or generate unseen words. Copying unknown words into the target text, as done by (Jean et al., 2015; Luong et al., 2015b), is a reasonable strategy for names, but morphological changes and transliteration is often required, especially if alphabe"
P16-1162,P15-1002,0,0.130638,"Institute Poland. lem, and especially for languages with productive word formation processes such as agglutination and compounding, translation models require mechanisms that go below the word level. As an example, consider compounds such as the German Abwasser|behandlungs|anlange ‘sewage water treatment plant’, for which a segmented, variable-length representation is intuitively more appealing than encoding the word as a fixed-length vector. For word-level NMT models, the translation of out-of-vocabulary words has been addressed through a back-off to a dictionary look-up (Jean et al., 2015; Luong et al., 2015b). We note that such techniques make assumptions that often do not hold true in practice. For instance, there is not always a 1-to-1 correspondence between source and target words because of variance in the degree of morphological synthesis between languages, like in our introductory compounding example. Also, word-level models are unable to translate or generate unseen words. Copying unknown words into the target text, as done by (Jean et al., 2015; Luong et al., 2015b), is a reasonable strategy for names, but morphological changes and transliteration is often required, especially if alphabe"
P16-1162,P12-1018,0,0.0565115,"s in Sections 4 and 5. First, we discuss different subword representations. 3.1 Related Work For Statistical Machine Translation (SMT), the translation of unknown words has been the subject of intensive research. A large proportion of unknown words are names, which can just be copied into the target text if both languages share an alphabet. If alphabets differ, transliteration is required (Durrani et al., 2014). Character-based translation has also been investigated with phrase-based models, which proved especially successful for closely related languages (Vilar et al., 2007; Tiedemann, 2009; Neubig et al., 2012). The segmentation of morphologically complex words such as compounds is widely used for SMT, and various algorithms for morpheme segmentation have been investigated (Nießen and Ney, 2000; Koehn and Knight, 2003; Virpioja et al., 2007; Stallard et al., 2012). Segmentation algorithms commonly used for phrase-based SMT tend to be conservative in their splitting decisions, whereas we aim for an aggressive segmentation that allows for open-vocabulary translation with a compact network vocabulary, and without having to resort to back-off dictionaries. The best choice of subword units may be taskspe"
P16-1162,C00-2162,0,0.0990133,"ct of intensive research. A large proportion of unknown words are names, which can just be copied into the target text if both languages share an alphabet. If alphabets differ, transliteration is required (Durrani et al., 2014). Character-based translation has also been investigated with phrase-based models, which proved especially successful for closely related languages (Vilar et al., 2007; Tiedemann, 2009; Neubig et al., 2012). The segmentation of morphologically complex words such as compounds is widely used for SMT, and various algorithms for morpheme segmentation have been investigated (Nießen and Ney, 2000; Koehn and Knight, 2003; Virpioja et al., 2007; Stallard et al., 2012). Segmentation algorithms commonly used for phrase-based SMT tend to be conservative in their splitting decisions, whereas we aim for an aggressive segmentation that allows for open-vocabulary translation with a compact network vocabulary, and without having to resort to back-off dictionaries. The best choice of subword units may be taskspecific. For speech recognition, phone-level language models have been used (Bazzi and Glass, 2000). Mikolov et al. (2012) investigate subword language models, and propose to use syllables."
P16-1162,W15-3049,0,0.534349,"Missing"
P16-1162,E12-1015,0,0.0627825,"ignment model αij , which models the probability that yi is aligned to xj . The alignment model is a singlelayer feedforward neural network that is learned jointly with the rest of the network through backpropagation. A detailed description can be found in (Bahdanau et al., 2015). Training is performed on a parallel corpus with stochastic gradient descent. For translation, a beam search with small beam size is employed. 3 • cognates and loanwords. Cognates and loanwords with a common origin can differ in regular ways between languages, so that character-level translation rules are sufficient (Tiedemann, 2012). Example: claustrophobia (English) Klaustrophobie (German) Êëàóñòðîôîáèÿ (Klaustrofobiâ) (Russian) Neural Machine Translation • morphologically complex words. Words containing multiple morphemes, for instance formed via compounding, affixation, or inflection, may be translatable by translating the morphemes separately. Example: solar system (English) Sonnensystem (Sonne + System) (German) Naprendszer (Nap + Rendszer) (Hungarian) In an analysis of 100 rare tokens (not among the 50 000 most frequent types) in our German training data1 , the majority of tokens are potentially translatable from E"
P16-1162,W07-0705,0,0.0774644,"words is transparent in 1716 pothesis in Sections 4 and 5. First, we discuss different subword representations. 3.1 Related Work For Statistical Machine Translation (SMT), the translation of unknown words has been the subject of intensive research. A large proportion of unknown words are names, which can just be copied into the target text if both languages share an alphabet. If alphabets differ, transliteration is required (Durrani et al., 2014). Character-based translation has also been investigated with phrase-based models, which proved especially successful for closely related languages (Vilar et al., 2007; Tiedemann, 2009; Neubig et al., 2012). The segmentation of morphologically complex words such as compounds is widely used for SMT, and various algorithms for morpheme segmentation have been investigated (Nießen and Ney, 2000; Koehn and Knight, 2003; Virpioja et al., 2007; Stallard et al., 2012). Segmentation algorithms commonly used for phrase-based SMT tend to be conservative in their splitting decisions, whereas we aim for an aggressive segmentation that allows for open-vocabulary translation with a compact network vocabulary, and without having to resort to back-off dictionaries. The best"
P16-1162,2007.mtsummit-papers.65,0,0.105199,"f unknown words are names, which can just be copied into the target text if both languages share an alphabet. If alphabets differ, transliteration is required (Durrani et al., 2014). Character-based translation has also been investigated with phrase-based models, which proved especially successful for closely related languages (Vilar et al., 2007; Tiedemann, 2009; Neubig et al., 2012). The segmentation of morphologically complex words such as compounds is widely used for SMT, and various algorithms for morpheme segmentation have been investigated (Nießen and Ney, 2000; Koehn and Knight, 2003; Virpioja et al., 2007; Stallard et al., 2012). Segmentation algorithms commonly used for phrase-based SMT tend to be conservative in their splitting decisions, whereas we aim for an aggressive segmentation that allows for open-vocabulary translation with a compact network vocabulary, and without having to resort to back-off dictionaries. The best choice of subword units may be taskspecific. For speech recognition, phone-level language models have been used (Bazzi and Glass, 2000). Mikolov et al. (2012) investigate subword language models, and propose to use syllables. For multilingual segmentation tasks, multiling"
P16-1162,D15-1248,1,0.126623,"Missing"
P16-1162,P08-1084,0,0.0964265,"ion algorithms commonly used for phrase-based SMT tend to be conservative in their splitting decisions, whereas we aim for an aggressive segmentation that allows for open-vocabulary translation with a compact network vocabulary, and without having to resort to back-off dictionaries. The best choice of subword units may be taskspecific. For speech recognition, phone-level language models have been used (Bazzi and Glass, 2000). Mikolov et al. (2012) investigate subword language models, and propose to use syllables. For multilingual segmentation tasks, multilingual algorithms have been proposed (Snyder and Barzilay, 2008). We find these intriguing, but inapplicable at test time. Various techniques have been proposed to produce fixed-length continuous word vectors based on characters or morphemes (Luong et al., 2013; Botha and Blunsom, 2014; Ling et al., 2015a; Kim et al., 2015). An effort to apply such techniques to NMT, parallel to ours, has found no significant improvement over word-based approaches (Ling et al., 2015b). One technical difference from our work is that the attention mechanism still operates on the level of words in the model by Ling et al. (2015b), and that the representation of each word is f"
P16-1162,P12-2063,0,0.075538,"es, which can just be copied into the target text if both languages share an alphabet. If alphabets differ, transliteration is required (Durrani et al., 2014). Character-based translation has also been investigated with phrase-based models, which proved especially successful for closely related languages (Vilar et al., 2007; Tiedemann, 2009; Neubig et al., 2012). The segmentation of morphologically complex words such as compounds is widely used for SMT, and various algorithms for morpheme segmentation have been investigated (Nießen and Ney, 2000; Koehn and Knight, 2003; Virpioja et al., 2007; Stallard et al., 2012). Segmentation algorithms commonly used for phrase-based SMT tend to be conservative in their splitting decisions, whereas we aim for an aggressive segmentation that allows for open-vocabulary translation with a compact network vocabulary, and without having to resort to back-off dictionaries. The best choice of subword units may be taskspecific. For speech recognition, phone-level language models have been used (Bazzi and Glass, 2000). Mikolov et al. (2012) investigate subword language models, and propose to use syllables. For multilingual segmentation tasks, multilingual algorithms have been"
P16-1162,W15-3031,0,0.0681316,"Missing"
P16-1162,2009.eamt-1.3,0,0.0688114,"t in 1716 pothesis in Sections 4 and 5. First, we discuss different subword representations. 3.1 Related Work For Statistical Machine Translation (SMT), the translation of unknown words has been the subject of intensive research. A large proportion of unknown words are names, which can just be copied into the target text if both languages share an alphabet. If alphabets differ, transliteration is required (Durrani et al., 2014). Character-based translation has also been investigated with phrase-based models, which proved especially successful for closely related languages (Vilar et al., 2007; Tiedemann, 2009; Neubig et al., 2012). The segmentation of morphologically complex words such as compounds is widely used for SMT, and various algorithms for morpheme segmentation have been investigated (Nießen and Ney, 2000; Koehn and Knight, 2003; Virpioja et al., 2007; Stallard et al., 2012). Segmentation algorithms commonly used for phrase-based SMT tend to be conservative in their splitting decisions, whereas we aim for an aggressive segmentation that allows for open-vocabulary translation with a compact network vocabulary, and without having to resort to back-off dictionaries. The best choice of subwor"
W07-1009,W05-1306,0,0.0218436,"Missing"
W07-1009,W03-0424,0,0.0115224,"(Grover and Tobin, 2006). 5.3 Named Entity Tagging The C&C tagger, referred to earlier, forms the basis of the NER component of the TXM natural language processing (NLP) pipeline designed to detect entity relations and normalisations (Grover et al., 2007). The tagger, in common with many ML approaches to NER, reduces the entity recognition problem to a sequence tagging problem by using the BIO encoding of entities. As well as performing well on the CoNLL-2003 task, Maximum Entropy Markov Models have also been successful on biomedical NER tasks (Finkel et al., 2005). As the vanilla C&C tagger (Curran and Clark, 2003) is optimised for performance on newswire text, various modifications were applied to improve its performance for biomedical NER. Table 3 lists the extra features specifically designed for biomedical text. The C&C tagger was also extended using several gazetteers, including a protein, complex, experimental method and modification gazetteer, targeted at recognising entities occurring in the EPPI data. Further postprocessing specific to the EPPI data involves correcting boundaries of some hyphenated proteins and filtering out entities ending in punctuation. All experiments with the C&C tagger in"
W07-1009,grover-tobin-2006-rule,1,0.466158,"del (MEMM) tagger developed by Curran and 69 Clark (2003) (hereafter referred to as C&C ) for the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003), trained on the MedPost data (Smith et al., 2004). Information on lemmatisation, as well as abbreviations and their long forms, is added using the morpha lemmatiser (Minnen et al., 2000) and the ExtractAbbrev script of Schwartz and Hearst (2003), respectively. A lookup step uses ontological information to identify scientific and common English names of species. Finally, a rule-based chunker marks up noun and verb groups and their heads (Grover and Tobin, 2006). 5.3 Named Entity Tagging The C&C tagger, referred to earlier, forms the basis of the NER component of the TXM natural language processing (NLP) pipeline designed to detect entity relations and normalisations (Grover et al., 2007). The tagger, in common with many ML approaches to NER, reduces the entity recognition problem to a sequence tagging problem by using the BIO encoding of entities. As well as performing well on the CoNLL-2003 task, Maximum Entropy Markov Models have also been successful on biomedical NER tasks (Finkel et al., 2005). As the vanilla C&C tagger (Curran and Clark, 2003)"
W07-1009,W95-0107,0,0.0109149,"tuations, it may make sense to relax these restrictions, for example by allowing entities to be nested inside other entities, or allowing discontinuous entities. GENIA (Ohta et al., 2002) and BioInfer (Pyysalo et al., 2007) are examples of recently produced NE-annotated biomedical corpora where entities nest. Corpora in other domains, for example the ACE1 data, also contain nested entities. This paper compares techniques for recognising nested entities in biomedical text. The difficulty of this task is that the standard method for converting NER to a sequence tagging problem with BIOencoding (Ramshaw and Marcus, 1995), where each 1 http://www.nist.gov/speech/tests/ace/ index.htm token is assigned a tag to indicate whether it is at the beginning (B), inside (I), or outside (O) of an entity, is not directly applicable when tokens belong to more than one entity. Here we explore methods of reducing the nested NER problem to one or more BIO problems so that existing NER tools can be used. This paper is organised as follows. In Section 2, the problem of nested entities is introduced and motivated with examples from GENIA and our EPPI (enriched protein-protein interaction) data. Related work is reviewed in Sectio"
W07-1009,W06-2703,1,0.905139,"format.2 The outermost annotation of coordinated structures and any continuous entity mark-up within them is retained. For example, in “human interleukin-2 and -4” both the continuous embedded entity “human interleukin-2” and the entire string are marked as proteins. The markup for discontinuous embedded entities, like “human interleukin-4” in the previous example, is not retained, as they could be derived in a post-processing step once nested entities are recognised. 3 Related Work In previous work addressing nested entities, Shen et al. (2003), Zhang et al. (2004), Zhou et al. (2004), Zhou (2006), and Gu (2006) considered the GENIA 2 Both corpora are represented in XML with standoff annotation, potentionally allowing overlapping NE s. corpus, where nested entities are relatively frequent. All these studies ignore embedded entities occurring in coordinated structures and only retain their outermost annotation. Shen et al. (2003), Zhang et al. (2004), and Zhou et al. (2004) all report on a rulebased approach to dealing with nested NEs in the GENIA corpus (Version 3.0) in combination with a Hidden Markov Model (HMM) that first recognises innermost NEs. They use four basic hand-crafted pa"
W07-1009,W03-1307,0,0.0183391,"differently, but for this work they are all converted to a common format.2 The outermost annotation of coordinated structures and any continuous entity mark-up within them is retained. For example, in “human interleukin-2 and -4” both the continuous embedded entity “human interleukin-2” and the entire string are marked as proteins. The markup for discontinuous embedded entities, like “human interleukin-4” in the previous example, is not retained, as they could be derived in a post-processing step once nested entities are recognised. 3 Related Work In previous work addressing nested entities, Shen et al. (2003), Zhang et al. (2004), Zhou et al. (2004), Zhou (2006), and Gu (2006) considered the GENIA 2 Both corpora are represented in XML with standoff annotation, potentionally allowing overlapping NE s. corpus, where nested entities are relatively frequent. All these studies ignore embedded entities occurring in coordinated structures and only retain their outermost annotation. Shen et al. (2003), Zhang et al. (2004), and Zhou et al. (2004) all report on a rulebased approach to dealing with nested NEs in the GENIA corpus (Version 3.0) in combination with a Hidden Markov Model (HMM) that first recogni"
W07-1009,W06-3318,0,0.167316,"e outermost annotation of coordinated structures and any continuous entity mark-up within them is retained. For example, in “human interleukin-2 and -4” both the continuous embedded entity “human interleukin-2” and the entire string are marked as proteins. The markup for discontinuous embedded entities, like “human interleukin-4” in the previous example, is not retained, as they could be derived in a post-processing step once nested entities are recognised. 3 Related Work In previous work addressing nested entities, Shen et al. (2003), Zhang et al. (2004), Zhou et al. (2004), Zhou (2006), and Gu (2006) considered the GENIA 2 Both corpora are represented in XML with standoff annotation, potentionally allowing overlapping NE s. corpus, where nested entities are relatively frequent. All these studies ignore embedded entities occurring in coordinated structures and only retain their outermost annotation. Shen et al. (2003), Zhang et al. (2004), and Zhou et al. (2004) all report on a rulebased approach to dealing with nested NEs in the GENIA corpus (Version 3.0) in combination with a Hidden Markov Model (HMM) that first recognises innermost NEs. They use four basic hand-crafted patterns and a co"
W07-1009,W04-1213,0,0.12087,"Missing"
W07-1009,W04-3111,0,0.0140137,"Missing"
W07-1009,H05-1124,0,0.0607621,"Missing"
W07-1009,W00-1427,0,0.0109833,"through a sequence of preprocessing steps implemented using the LT- XML2 and LT- TTT2 tools (Grover et al., 2006) with the output of each step encoded in XML mark-up. Tokenisation and sentence splitting is followed by part-ofspeech tagging with the Maximum Entropy Markov Model (MEMM) tagger developed by Curran and 69 Clark (2003) (hereafter referred to as C&C ) for the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003), trained on the MedPost data (Smith et al., 2004). Information on lemmatisation, as well as abbreviations and their long forms, is added using the morpha lemmatiser (Minnen et al., 2000) and the ExtractAbbrev script of Schwartz and Hearst (2003), respectively. A lookup step uses ontological information to identify scientific and common English names of species. Finally, a rule-based chunker marks up noun and verb groups and their heads (Grover and Tobin, 2006). 5.3 Named Entity Tagging The C&C tagger, referred to earlier, forms the basis of the NER component of the TXM natural language processing (NLP) pipeline designed to detect entity relations and normalisations (Grover et al., 2007). The tagger, in common with many ML approaches to NER, reduces the entity recognition prob"
W07-1009,W03-0419,0,\N,Missing
W07-1019,grover-tobin-2006-rule,0,0.0146241,"assignment modules were implemented as part of an NLP pipeline based on the LT- XML2 architecture1 . The pipeline consists of tokenisation, lemmatisation, part-of-speech tagging, species word identification, abbreviation detection and chunking, named entiry recognition (NER) and relation extraction. The part-of-speech tagging uses the Curran and Clark POS tagger (Curran and Clark, 2003) trained on MedPost data (Smith et al., 2004), whilst the other preprocessing stages are all rule based. Tokenisation, species word identification and chunking were implemented in-house using the LTXML2 tools (Grover and Tobin, 2006), whilst abbreviation extraction used the Schwartz and Hearst abbreviation extractor (Schwartz and Hearst, 2003) and lemmatisation used morpha (Minnen et al., 2000). The NER module uses the Curran and Clark NER tagger (Curran and Clark, 2003), augmented with extra features tailored to the biomedical domain. Finally, a relation extractor based on a maximum entropy model and a set of shallow linguistic features is employed, as described in (Nielsen, 2006). 4.2 svmlight3 ) were tested. To choose an optimal feature set, an iterative greedy optimisation procedure was employed. A set of potential fe"
W07-1019,M98-1002,0,0.030101,"ed Work There has been much recent interest in extracting PPIs from abstracts and full text papers (Bunescu and Mooney, 2006; Giuliano et al., 2006; Plake et al., 2005; Blaschke and Valencia, 2002; Donaldson et al., 2003). In these systems however, the focus has been on extracting just the PPIs without attempts to enrich the PPIs with further information. Enriched PPIs can be seen as a type of biological event extraction (Alphonse et al., 2004; Wattarujeekrit et al., 2004), a technique for mapping entities found in text to roles in predefined templates which was made popular in the MUC tasks (Marsh and Perzanowski, 1998). There has also been work to enrich sentences with semantic categories (Shah and Bork, 2006) and qualitative dimensions such as polarity (Wilbur et al., 2006). Using NLP to aid in curation was addressed in the KDD 2002 Cup (Yeh et al., 2002), where participants attempted to extract records curatable with respect to the FlyBase database, and has been further studied by many groups (Xu et al., 2006; Karamanis et al., 2007; Ursing et al., 2001). The Protein-Protein Interaction task of the recent BioCreAtIvE challenge (Krallinger et al., 2007) was concerned with selecting papers and extracting in"
W07-1019,W00-1427,0,0.0163097,"ech tagging, species word identification, abbreviation detection and chunking, named entiry recognition (NER) and relation extraction. The part-of-speech tagging uses the Curran and Clark POS tagger (Curran and Clark, 2003) trained on MedPost data (Smith et al., 2004), whilst the other preprocessing stages are all rule based. Tokenisation, species word identification and chunking were implemented in-house using the LTXML2 tools (Grover and Tobin, 2006), whilst abbreviation extraction used the Schwartz and Hearst abbreviation extractor (Schwartz and Hearst, 2003) and lemmatisation used morpha (Minnen et al., 2000). The NER module uses the Curran and Clark NER tagger (Curran and Clark, 2003), augmented with extra features tailored to the biomedical domain. Finally, a relation extractor based on a maximum entropy model and a set of shallow linguistic features is employed, as described in (Nielsen, 2006). 4.2 svmlight3 ) were tested. To choose an optimal feature set, an iterative greedy optimisation procedure was employed. A set of potential features were implemented, with options to turn parts of the feature set on or off. The full feature set was then tested on the DEVTEST data with each of the feature"
W07-1019,P04-1043,0,0.0908186,"Missing"
W07-1019,W06-3322,0,0.114393,"g stages are all rule based. Tokenisation, species word identification and chunking were implemented in-house using the LTXML2 tools (Grover and Tobin, 2006), whilst abbreviation extraction used the Schwartz and Hearst abbreviation extractor (Schwartz and Hearst, 2003) and lemmatisation used morpha (Minnen et al., 2000). The NER module uses the Curran and Clark NER tagger (Curran and Clark, 2003), augmented with extra features tailored to the biomedical domain. Finally, a relation extractor based on a maximum entropy model and a set of shallow linguistic features is employed, as described in (Nielsen, 2006). 4.2 svmlight3 ) were tested. To choose an optimal feature set, an iterative greedy optimisation procedure was employed. A set of potential features were implemented, with options to turn parts of the feature set on or off. The full feature set was then tested on the DEVTEST data with each of the feature options knocked out in turn. After examining the scores on all possible feature knockouts, the one which offered the largest gain in performance was selected and removed permanently. The whole procedure was then repeated until knockouts produced no further gains in performance. The resulting"
W07-1019,W03-0424,0,0.178299,"words if both annotators decide to attach an attribute to a particular PPI, they generally agree about which one, scoring a micro-averaged overall F1 of 95.10 in this case. 4 Methods 4.1 Pipeline Processing The property and attribute assignment modules were implemented as part of an NLP pipeline based on the LT- XML2 architecture1 . The pipeline consists of tokenisation, lemmatisation, part-of-speech tagging, species word identification, abbreviation detection and chunking, named entiry recognition (NER) and relation extraction. The part-of-speech tagging uses the Curran and Clark POS tagger (Curran and Clark, 2003) trained on MedPost data (Smith et al., 2004), whilst the other preprocessing stages are all rule based. Tokenisation, species word identification and chunking were implemented in-house using the LTXML2 tools (Grover and Tobin, 2006), whilst abbreviation extraction used the Schwartz and Hearst abbreviation extractor (Schwartz and Hearst, 2003) and lemmatisation used morpha (Minnen et al., 2000). The NER module uses the Curran and Clark NER tagger (Curran and Clark, 2003), augmented with extra features tailored to the biomedical domain. Finally, a relation extractor based on a maximum entropy m"
W07-1019,E06-1051,0,\N,Missing
W08-0603,W03-0424,0,0.143818,"In particular, the iaa for intrasentential frag relations is very high, probably because many of these are very straightforward constructions such as “Fragment of Protein”. Intersentential relations are often less clear as they involve linking information between several sentences, for example using coreferences. Both corpora were pre-processed before re was applied. The pre-processing involved tokenisation, sentence boundary detection, lemmatising. part-ofspeech tagging, head word detection and chunking. The part-of-speech tagging uses the Curran & Clark maximum entropy Markov model tagger (Curran and Clark, 2003) trained on MedPost data (Smith et al., 2004), whilst the other preprocessing stages are all rule-based. The tokenisation, sentence boundary detection, head word identification and chunking components were implemented with the lt-xml2 tools (Grover and Tobin, 2006), and the lemmatisation used morpha (Minnen et al., 2000). 3.2 The Relation Extraction System Relation extraction is treated a classification problem, by generating candidate relations, and classifying them as either true or false. In the optimisation experiments described in this paper, Zhang Le’s maximum entropy (maxent) classifier"
W08-0603,D07-1024,0,0.0177117,"atable interactions. The extraction of protein-protein interactions has also been helped by the availability of annotated corpora, such as AIMed (Bunescu et al., 2005), which consists of around 1000 Medline abstracts annotated with proteins and their interactions. In common with the LLL corpus, the AIMed corpus only contains intra-sentential relations, and is somewhat smaller than the corpus used in the current work. In addition to the work by the corpus creators (Bunescu and Mooney, 2007), other authors have achieved good results on AIMed by making use of dependency parses in different ways (Erkan et al., 2007; Katrenko and Adriaans, 2006). It is not clear, however, how well these techniques would transfer to other, similar, re problems, and how much work would be involved in tuning the systems for a new problem. Supervised learning based on shallow syntactic features has also been applied to the biomedical domain, again focusing on protein-protein interactions (Nielsen, 2006; Giuliano et al., 2006). A systematic exploration of a set of such features for proteinprotein interaction extraction was recently provided by Jiang and Zhai (2007), who also used features derived from the Collins parser. They"
W08-0603,E06-1051,0,0.0288657,"s used in the current work. In addition to the work by the corpus creators (Bunescu and Mooney, 2007), other authors have achieved good results on AIMed by making use of dependency parses in different ways (Erkan et al., 2007; Katrenko and Adriaans, 2006). It is not clear, however, how well these techniques would transfer to other, similar, re problems, and how much work would be involved in tuning the systems for a new problem. Supervised learning based on shallow syntactic features has also been applied to the biomedical domain, again focusing on protein-protein interactions (Nielsen, 2006; Giuliano et al., 2006). A systematic exploration of a set of such features for proteinprotein interaction extraction was recently provided by Jiang and Zhai (2007), who also used features derived from the Collins parser. They did not, however, experiment with the automated optimisation of the feature sets. In the news domain, the best reported results on the ACE dataset1 have been achieved by a composite kernel which depends partially on a full parse, and partially on a collection of shallow syntactic features (Zhou et al., 2007). Aside from protein-protein interactions, there has been little work directed at other"
W08-0603,grover-tobin-2006-rule,0,0.105312,"several sentences, for example using coreferences. Both corpora were pre-processed before re was applied. The pre-processing involved tokenisation, sentence boundary detection, lemmatising. part-ofspeech tagging, head word detection and chunking. The part-of-speech tagging uses the Curran & Clark maximum entropy Markov model tagger (Curran and Clark, 2003) trained on MedPost data (Smith et al., 2004), whilst the other preprocessing stages are all rule-based. The tokenisation, sentence boundary detection, head word identification and chunking components were implemented with the lt-xml2 tools (Grover and Tobin, 2006), and the lemmatisation used morpha (Minnen et al., 2000). 3.2 The Relation Extraction System Relation extraction is treated a classification problem, by generating candidate relations, and classifying them as either true or false. In the optimisation experiments described in this paper, Zhang Le’s maximum entropy (maxent) classifier2 was used, since its performance was very competitive and its fast training time permitted extensive feature experimentation. The Gaussian prior was set to 0.1, and the maximum training iterations to 100. In order to assess the performance of the final system, max"
W08-0603,N07-1015,0,0.0312544,"s on AIMed by making use of dependency parses in different ways (Erkan et al., 2007; Katrenko and Adriaans, 2006). It is not clear, however, how well these techniques would transfer to other, similar, re problems, and how much work would be involved in tuning the systems for a new problem. Supervised learning based on shallow syntactic features has also been applied to the biomedical domain, again focusing on protein-protein interactions (Nielsen, 2006; Giuliano et al., 2006). A systematic exploration of a set of such features for proteinprotein interaction extraction was recently provided by Jiang and Zhai (2007), who also used features derived from the Collins parser. They did not, however, experiment with the automated optimisation of the feature sets. In the news domain, the best reported results on the ACE dataset1 have been achieved by a composite kernel which depends partially on a full parse, and partially on a collection of shallow syntactic features (Zhou et al., 2007). Aside from protein-protein interactions, there has been little work directed at other types of relations in the biomedical domain. Recent corpus annotation projects such as Genia (Kim et al., 2008) and BioInfer (Pyysalo et al."
W08-0603,P07-1075,0,0.0298793,"Missing"
W08-0603,W00-1427,0,0.241081,"ora were pre-processed before re was applied. The pre-processing involved tokenisation, sentence boundary detection, lemmatising. part-ofspeech tagging, head word detection and chunking. The part-of-speech tagging uses the Curran & Clark maximum entropy Markov model tagger (Curran and Clark, 2003) trained on MedPost data (Smith et al., 2004), whilst the other preprocessing stages are all rule-based. The tokenisation, sentence boundary detection, head word identification and chunking components were implemented with the lt-xml2 tools (Grover and Tobin, 2006), and the lemmatisation used morpha (Minnen et al., 2000). 3.2 The Relation Extraction System Relation extraction is treated a classification problem, by generating candidate relations, and classifying them as either true or false. In the optimisation experiments described in this paper, Zhang Le’s maximum entropy (maxent) classifier2 was used, since its performance was very competitive and its fast training time permitted extensive feature experimentation. The Gaussian prior was set to 0.1, and the maximum training iterations to 100. In order to assess the performance of the final system, maxent was compared with support vector machines (svm) using"
W08-0603,W06-3322,0,0.0283734,"than the corpus used in the current work. In addition to the work by the corpus creators (Bunescu and Mooney, 2007), other authors have achieved good results on AIMed by making use of dependency parses in different ways (Erkan et al., 2007; Katrenko and Adriaans, 2006). It is not clear, however, how well these techniques would transfer to other, similar, re problems, and how much work would be involved in tuning the systems for a new problem. Supervised learning based on shallow syntactic features has also been applied to the biomedical domain, again focusing on protein-protein interactions (Nielsen, 2006; Giuliano et al., 2006). A systematic exploration of a set of such features for proteinprotein interaction extraction was recently provided by Jiang and Zhai (2007), who also used features derived from the Collins parser. They did not, however, experiment with the automated optimisation of the feature sets. In the news domain, the best reported results on the ACE dataset1 have been achieved by a composite kernel which depends partially on a full parse, and partially on a collection of shallow syntactic features (Zhou et al., 2007). Aside from protein-protein interactions, there has been littl"
W08-0603,D07-1076,0,0.0329003,"biomedical domain, again focusing on protein-protein interactions (Nielsen, 2006; Giuliano et al., 2006). A systematic exploration of a set of such features for proteinprotein interaction extraction was recently provided by Jiang and Zhai (2007), who also used features derived from the Collins parser. They did not, however, experiment with the automated optimisation of the feature sets. In the news domain, the best reported results on the ACE dataset1 have been achieved by a composite kernel which depends partially on a full parse, and partially on a collection of shallow syntactic features (Zhou et al., 2007). Aside from protein-protein interactions, there has been little work directed at other types of relations in the biomedical domain. Recent corpus annotation projects such as Genia (Kim et al., 2008) and BioInfer (Pyysalo et al., 2007) include multiple types of relations, however many of the relation types are represented in fairly small quantities. In earlier work (Skounakis et al., 2003), the extraction of cell localisation relations was studied using an automatically created corpus. 1 http://www.nist.gov/speech/tests/ace/ 20 Methods 3.1 Corpora The ITI TXM corpora contain annotations relate"
W09-0429,P97-1003,0,0.176631,"mbr), the monotoneat-punctuation reordering constraint (mp, see Section 3.2), and bigger beam sizes. 2.4 BLEU (uncased) 16.6 20.6 20.6 20.9 20.9 21.7 22.0 22.1 22.3 Table 3: Results for German–English with the incremental addition of methods beyond a baseline trained on the parallel corpus English–German (ued’08: 12.1, best’08: 14.2) baseline + interpolated news LM + minimum Bayes risk decoding + monotone at punctuation + truecasing + morphological LM + big beam German–English For German–English, we additionally incorporated rule-based reordering — We parse the input using the Collins parser (Collins, 1997) and apply a set of reordering rules to re-arrange the German sentence so that it corresponds more closely English word order (Collins et al., 2005). compound splitting — We split German compound words (mostly nouns), based on the frequency of the words in the potential decompositions (Koehn and Knight, 2003a). part-of-speech language model — We use factored translation models (Koehn and Hoang, 2007) to also output part-of-speech tags with each word in a single phrase mapping and run a second n-gram model over them. The EnBLEU (uncased) 13.5 15.2 15.2 15.2 15.2 15.2 15.7 Table 4: Results for E"
W09-0429,P05-1066,1,0.591762,"0.9 21.7 22.0 22.1 22.3 Table 3: Results for German–English with the incremental addition of methods beyond a baseline trained on the parallel corpus English–German (ued’08: 12.1, best’08: 14.2) baseline + interpolated news LM + minimum Bayes risk decoding + monotone at punctuation + truecasing + morphological LM + big beam German–English For German–English, we additionally incorporated rule-based reordering — We parse the input using the Collins parser (Collins, 1997) and apply a set of reordering rules to re-arrange the German sentence so that it corresponds more closely English word order (Collins et al., 2005). compound splitting — We split German compound words (mostly nouns), based on the frequency of the words in the potential decompositions (Koehn and Knight, 2003a). part-of-speech language model — We use factored translation models (Koehn and Hoang, 2007) to also output part-of-speech tags with each word in a single phrase mapping and run a second n-gram model over them. The EnBLEU (uncased) 13.5 15.2 15.2 15.2 15.2 15.2 15.7 Table 4: Results for English–German with the incremental addition of methods beyiond a baseline trained on the parallel corpus glish part-of-speech tags are obtained usin"
W09-0429,P07-1019,0,0.153459,"Missing"
W09-0429,D07-1091,1,0.522879,"ng + monotone at punctuation + truecasing + morphological LM + big beam German–English For German–English, we additionally incorporated rule-based reordering — We parse the input using the Collins parser (Collins, 1997) and apply a set of reordering rules to re-arrange the German sentence so that it corresponds more closely English word order (Collins et al., 2005). compound splitting — We split German compound words (mostly nouns), based on the frequency of the words in the potential decompositions (Koehn and Knight, 2003a). part-of-speech language model — We use factored translation models (Koehn and Hoang, 2007) to also output part-of-speech tags with each word in a single phrase mapping and run a second n-gram model over them. The EnBLEU (uncased) 13.5 15.2 15.2 15.2 15.2 15.2 15.7 Table 4: Results for English–German with the incremental addition of methods beyiond a baseline trained on the parallel corpus glish part-of-speech tags are obtained using MXPOST (Ratnaparkhi, 1996). 2.5 English-German For English–German, we additionally incorporated a morphological language model the same way we incorporated a part-of-speech language model in the other translation direction. The morphological tags were o"
W09-0429,P07-2045,1,0.0207344,"lated them by optimizing perplexity on the provided tuning set. Perplexity numbers are shown in Table 1. Introduction The commitment of the University of Edinburgh to the WMT shared tasks is to provide a strong statistical machine translation baseline with our open source tools for all language pairs. We are again the only institution that participated in all tracks. The shared task is also an opportunity to incorporate novel contributions and test them against the best machine translation systems for these language pairs. In this paper we describe the speed improvements to the Moses decoder (Koehn et al., 2007), as well as a novel framework to specify reordering constraints with XML markup, which we tested with punctuation-based constraints. 2 ep nc 449 486 264 311 785 821 341 392 *:1475 1615 hung:2148 2.2 Truecasing Our traditional method to handle case is to lowercase all training data, and then have a separate recasing (or recapitalization) step. Last year, we used truecasing: all words are normalized to their natural case, e.g. the, John, eBay, meaning that only sentence-leading words may be changed to their most frequent form. To refine last year’s approach, we record the seen truecased instanc"
W09-0429,E03-1076,1,0.831356,"rman (ued’08: 12.1, best’08: 14.2) baseline + interpolated news LM + minimum Bayes risk decoding + monotone at punctuation + truecasing + morphological LM + big beam German–English For German–English, we additionally incorporated rule-based reordering — We parse the input using the Collins parser (Collins, 1997) and apply a set of reordering rules to re-arrange the German sentence so that it corresponds more closely English word order (Collins et al., 2005). compound splitting — We split German compound words (mostly nouns), based on the frequency of the words in the potential decompositions (Koehn and Knight, 2003a). part-of-speech language model — We use factored translation models (Koehn and Hoang, 2007) to also output part-of-speech tags with each word in a single phrase mapping and run a second n-gram model over them. The EnBLEU (uncased) 13.5 15.2 15.2 15.2 15.2 15.2 15.7 Table 4: Results for English–German with the incremental addition of methods beyiond a baseline trained on the parallel corpus glish part-of-speech tags are obtained using MXPOST (Ratnaparkhi, 1996). 2.5 English-German For English–German, we additionally incorporated a morphological language model the same way we incorporated a p"
W09-0429,P03-1040,1,0.654267,"rman (ued’08: 12.1, best’08: 14.2) baseline + interpolated news LM + minimum Bayes risk decoding + monotone at punctuation + truecasing + morphological LM + big beam German–English For German–English, we additionally incorporated rule-based reordering — We parse the input using the Collins parser (Collins, 1997) and apply a set of reordering rules to re-arrange the German sentence so that it corresponds more closely English word order (Collins et al., 2005). compound splitting — We split German compound words (mostly nouns), based on the frequency of the words in the potential decompositions (Koehn and Knight, 2003a). part-of-speech language model — We use factored translation models (Koehn and Hoang, 2007) to also output part-of-speech tags with each word in a single phrase mapping and run a second n-gram model over them. The EnBLEU (uncased) 13.5 15.2 15.2 15.2 15.2 15.2 15.7 Table 4: Results for English–German with the incremental addition of methods beyiond a baseline trained on the parallel corpus glish part-of-speech tags are obtained using MXPOST (Ratnaparkhi, 1996). 2.5 English-German For English–German, we additionally incorporated a morphological language model the same way we incorporated a p"
W09-0429,2007.mtsummit-papers.43,0,0.05017,"Missing"
W09-0429,C00-2105,0,0.0377708,"Missing"
W09-0429,W96-0213,0,\N,Missing
W09-1114,P08-1024,1,0.253533,"r an advantage on an appropriately optimised model. 4 Minimum risk training In the previous section, we described how our sampler can be used to search for the best translation under a variety of decoding criteria (max derivation, translation, and minimum risk). However, there appeared to be little benefit to marginalizing over the latent derivations. This is almost certainly a side effect of the MERT training approach that was used to construct the models so as to maximise the performance of the model on its single best derivation, without regard to the shape of the rest of the distribution (Blunsom et al., 2008). In this section we 107 describe a further application of the Gibbs sampler: to do unbiased minimum risk training. While there have been at least two previous attempts to do minimum risk training for MT, both approaches relied on biased k-best approximations (Smith and Eisner, 2006; Zens and Ney, 2007). Since we sample from the whole distribution, we will have a more accurate risk assessment. The risk, or expected loss, of a probabilistic translation model on a corpus D, defined with respect to a particular loss function `eˆ(e), where eˆ is the reference translation and e is a hypothesis tran"
W09-1114,J93-2003,0,0.0151602,"xpected risk training and decoding. 1 Introduction Statistical machine translation (SMT) poses the problem: given a foreign sentence f , find the translation e∗ that maximises the conditional posterior probability p(e|f ). This probabilistic formulation of translation has driven development of state-of-the-art systems which are able to learn from parallel corpora which were generated for other purposes — a direct result of employing a mathematical framework that we can reason about independently of any particular model. For example, we can train SMT models using maximum likelihood estimation (Brown et al., 1993; Och and Ney, 2000; Marcu and Wong, 2002). Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (Kumar and Byrne, 2004; Smith and 102 Eisner, 2006; Zens and Ney, 2007). We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009). Most models define multiple derivations for each translation; the probability of a translation is thus"
W09-1114,D08-1033,0,0.0826863,"n reason about independently of any particular model. For example, we can train SMT models using maximum likelihood estimation (Brown et al., 1993; Och and Ney, 2000; Marcu and Wong, 2002). Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (Kumar and Byrne, 2004; Smith and 102 Eisner, 2006; Zens and Ney, 2007). We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009). Most models define multiple derivations for each translation; the probability of a translation is thus the sum over all of its derivations. Unfortunately, finding the maximum probability translation is NPhard for all but the most trivial of models in this setting (Sima’an, 1996). It is thus necessary to resort to approximations for this sum and the search for its maximum e∗ . The most common of these approximations is the max-derivation approximation, which for many models can be computed in polynomial time via dynamic programming (DP). Though effective for some proble"
W09-1114,W06-1673,0,0.0295365,"rementally improves it via operators such as R ETRANS and M ERGE -S PLIT. It is also similar to the estimator of Marcu and Wong (2002), who employ the same operators to search the alignment space from a heuristic initialisation. Although the operators are similar, the use is different. These previous efforts employed their operators in a greedy hill-climbing search. In contrast, our operators are applied probabilistically, making them theoretically well-founded for a variety of inference problems. Our use of Gibbs sampling follows from its increasing use in Bayesian inference problems in NLP (Finkel et al., 2006; Johnson et al., 2007b). Most closely related is the work of DeNero et al. (2008), who derive a Gibbs sampler for phrase-based alignment, using it to infer phrase translation probabilities. The use of Monte Carlo techniques to calculate posteriors is similar to that of Chappelier and Rajman (2000) who use those techniques to find the best parse under models where the derivation and the parse are not isomorphic. 109 This research was supported in part by the GALE program of the Defense Advanced Research Projects Agency, Contract No. HR0011-06-2-001; and by the EuroMatrix project funded by the"
W09-1114,P01-1030,0,0.032787,"ns. 7 Conclusion We have described a Gibbs sampler for approximating two intractable problems in SMT: maximum translation decoding (and its variant, minimum risk decoding) and minimum risk training. By using Monte Carlo techniques we avoid the biases associated with the more commonly used DP based maxderivation (or k-best derivation) approximation. In doing so we provide a further tool to the translation community that we envision will allow the development and analysis of increasing theoretically well motivated techniques. Acknowledgments Related work Our sampler is similar to the decoder of Germann et al. (2001), which starts with an approximate solution and then incrementally improves it via operators such as R ETRANS and M ERGE -S PLIT. It is also similar to the estimator of Marcu and Wong (2002), who employ the same operators to search the alignment space from a heuristic initialisation. Although the operators are similar, the use is different. These previous efforts employed their operators in a greedy hill-climbing search. In contrast, our operators are applied probabilistically, making them theoretically well-founded for a variety of inference problems. Our use of Gibbs sampling follows from it"
W09-1114,D07-1103,0,0.222812,"on P (a, e|f ) and therefore to determine the maximum of this distribution, in other words the most likely derivation. Furthermore, we can marginalise over the alignments to estimate P (e|f ) ● ● For the German and French systems, the DEV 2006 set was used for model tuning and the TEST 2007 (in-domain) and NEWS - DEV 2009 B (out-of-domain) sets for testing. For the Arabic system, the MT 02 set (10 reference translations) was used for tuning and MT 03 (4 reference translations) was used for evaluation. To reduce the size of the phrase table, we used the association-score technique suggested by Johnson et al. (2007a). Translation quality is reported using case-insensitive BLEU (Papineni et al., 2002). KL Divergence ● ● ● 0.1 ● ● ● ● 0.01 ● ● 0.001 KL divergence ● ● ● ● ● 10 100 1000 10000 100000 ● 3.2 1000000 Iterations Figure 2: The KL divergence of the true posterior distribution and the distribution estimated by the Gibbs sampler at different numbers of iterations for the Arabic source sentence r}ys wzrA’ mAlyzyA yzwr Alflbyn (in English, The prime minister of Malaysia visits the Philippines). and so obtain the most likely translation. The Gibbs sampler can therefore be used as a decoder, either runn"
W09-1114,N07-1018,0,0.206505,"on P (a, e|f ) and therefore to determine the maximum of this distribution, in other words the most likely derivation. Furthermore, we can marginalise over the alignments to estimate P (e|f ) ● ● For the German and French systems, the DEV 2006 set was used for model tuning and the TEST 2007 (in-domain) and NEWS - DEV 2009 B (out-of-domain) sets for testing. For the Arabic system, the MT 02 set (10 reference translations) was used for tuning and MT 03 (4 reference translations) was used for evaluation. To reduce the size of the phrase table, we used the association-score technique suggested by Johnson et al. (2007a). Translation quality is reported using case-insensitive BLEU (Papineni et al., 2002). KL Divergence ● ● ● 0.1 ● ● ● ● 0.01 ● ● 0.001 KL divergence ● ● ● ● ● 10 100 1000 10000 100000 ● 3.2 1000000 Iterations Figure 2: The KL divergence of the true posterior distribution and the distribution estimated by the Gibbs sampler at different numbers of iterations for the Arabic source sentence r}ys wzrA’ mAlyzyA yzwr Alflbyn (in English, The prime minister of Malaysia visits the Philippines). and so obtain the most likely translation. The Gibbs sampler can therefore be used as a decoder, either runn"
W09-1114,N03-1017,1,0.202563,"s for probabilistic inference: 1. It typically differs from the true model maximum. 2. It often requires additional approximations in search, leading to further error. 3. It introduces restrictions on models, such as use of only local features. 4. It provides no good solution to compute the normalization factor Z(f ) required by many probabilistic algorithms. In this work, we solve these problems using a Monte Carlo technique with none of the above drawbacks. Our technique is based on a novel Gibbs sampler that draws samples from the posterior distribution of a phrase-based translation model (Koehn et al., 2003) but operates in linear time with respect to the number of input words (Section 2). We show Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 102–110, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics that it is effective for both decoding (Section 3) and minimum risk training (Section 4). 2 A Gibbs sampler for phrase-based translation models We begin by assuming a phrase-based translation model in which the input sentence, f , is segmented into phrases, which are sequences of adjacent words.1 Each foreign phrase is"
W09-1114,P07-2045,1,0.0245677,"en the distribution is too flat and the sampler spends too much time exploring unimportant probability regions. If it is too large, then the distribution is too peaked and the sampler may concentrate on a very narrow probability region. We optimised the scaling factor on a 200-sentence portion of the tuning set, finding that a multiplicative factor of 10 worked best for fr-en and a multiplicative factor of 6 for de-en. 3 The first experiment shows the effect of different initialisations and numbers of sampler iterations on max-derivation decoding performance of the sampler. The Moses decoder (Koehn et al., 2007) was used to generate the starting hypothesis, either in full DP max-derivation mode, or alternatively with restrictions on the features and reordering, or with zero weights to simulate a random initialisation, and the number of iterations varied from 100 to 200,000, with a 100 iteration burn-in in each case. Figure 3 shows the variation of model score with sampler iteration, for the different starting points, and for both language pairs. 3 We experimented with annealing, where the scale factor is gradually increased to sharpen the distribution while sampling. However, we found no improvements"
W09-1114,N04-1022,0,0.549837,"formulation of translation has driven development of state-of-the-art systems which are able to learn from parallel corpora which were generated for other purposes — a direct result of employing a mathematical framework that we can reason about independently of any particular model. For example, we can train SMT models using maximum likelihood estimation (Brown et al., 1993; Och and Ney, 2000; Marcu and Wong, 2002). Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (Kumar and Byrne, 2004; Smith and 102 Eisner, 2006; Zens and Ney, 2007). We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009). Most models define multiple derivations for each translation; the probability of a translation is thus the sum over all of its derivations. Unfortunately, finding the maximum probability translation is NPhard for all but the most trivial of models in this setting (Sima’an, 1996). It is thus necessary to resort to approximations for this sum and"
W09-1114,W02-1018,0,0.133102,"ntroduction Statistical machine translation (SMT) poses the problem: given a foreign sentence f , find the translation e∗ that maximises the conditional posterior probability p(e|f ). This probabilistic formulation of translation has driven development of state-of-the-art systems which are able to learn from parallel corpora which were generated for other purposes — a direct result of employing a mathematical framework that we can reason about independently of any particular model. For example, we can train SMT models using maximum likelihood estimation (Brown et al., 1993; Och and Ney, 2000; Marcu and Wong, 2002). Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (Kumar and Byrne, 2004; Smith and 102 Eisner, 2006; Zens and Ney, 2007). We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009). Most models define multiple derivations for each translation; the probability of a translation is thus the sum over all of its derivations. Unfo"
W09-1114,N06-1045,0,0.0117505,"led to a situation where entire classes of potentially useful features are not considered because they would be impractical to integrate into a DP based translation system. With the sampler this restriction is mitigated: any function of h(e, f, a) may participate in the translation model subject only to its own computability. Freed from the rusty manacles of dynamic programming, we anticipate development of many useful features. 6 To our knowledge, we are the first to apply Monte Carlo methods to maximum translation and minimum risk translation. Approaches to the former (Blunsom et al., 2008; May and Knight, 2006) rely on dynamic programming techniques which do not scale well without heuristic approximations, while approaches to the latter (Smith and Eisner, 2006; Zens et al., 2007) use biased k-best approximations. 7 Conclusion We have described a Gibbs sampler for approximating two intractable problems in SMT: maximum translation decoding (and its variant, minimum risk decoding) and minimum risk training. By using Monte Carlo techniques we avoid the biases associated with the more commonly used DP based maxderivation (or k-best derivation) approximation. In doing so we provide a further tool to the t"
W09-1114,C00-2163,0,0.042141,"g and decoding. 1 Introduction Statistical machine translation (SMT) poses the problem: given a foreign sentence f , find the translation e∗ that maximises the conditional posterior probability p(e|f ). This probabilistic formulation of translation has driven development of state-of-the-art systems which are able to learn from parallel corpora which were generated for other purposes — a direct result of employing a mathematical framework that we can reason about independently of any particular model. For example, we can train SMT models using maximum likelihood estimation (Brown et al., 1993; Och and Ney, 2000; Marcu and Wong, 2002). Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (Kumar and Byrne, 2004; Smith and 102 Eisner, 2006; Zens and Ney, 2007). We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009). Most models define multiple derivations for each translation; the probability of a translation is thus the sum over all o"
W09-1114,P03-1021,0,0.0606256,"nstructed a phrase-based translation model as described in Koehn et al. (2003), limiting the phrase length to 5. The target side of the parallel corpus was used to train a 3-gram language model. 2 The Arabic-English training data consists of the eTIRR corpus (LDC2004E72), the Arabic news corpus (LDC2004T17), the Ummah corpus (LDC2004T18), and the sentences with confidence c > 0.995 in the ISI automatically extracted web parallel corpus (LDC2006T02). 105 Translation performance For the experiments reported in this section, we used feature weights trained with minimum error rate training (MERT; Och, 2003) . Because MERT ignores the denominator in Equation 1, it is invariant with respect to the scale of the weight vector θ — the Moses implementation simply normalises the weight vector it finds by its `1 -norm. However, when we use these weights in a true probabilistic model, the scaling factor affects the behaviour of the model since it determines how peaked or flat the distribution is. If the scaling factor is too small, then the distribution is too flat and the sampler spends too much time exploring unimportant probability regions. If it is too large, then the distribution is too peaked and t"
W09-1114,P02-1040,0,0.0772047,"words the most likely derivation. Furthermore, we can marginalise over the alignments to estimate P (e|f ) ● ● For the German and French systems, the DEV 2006 set was used for model tuning and the TEST 2007 (in-domain) and NEWS - DEV 2009 B (out-of-domain) sets for testing. For the Arabic system, the MT 02 set (10 reference translations) was used for tuning and MT 03 (4 reference translations) was used for evaluation. To reduce the size of the phrase table, we used the association-score technique suggested by Johnson et al. (2007a). Translation quality is reported using case-insensitive BLEU (Papineni et al., 2002). KL Divergence ● ● ● 0.1 ● ● ● ● 0.01 ● ● 0.001 KL divergence ● ● ● ● ● 10 100 1000 10000 100000 ● 3.2 1000000 Iterations Figure 2: The KL divergence of the true posterior distribution and the distribution estimated by the Gibbs sampler at different numbers of iterations for the Arabic source sentence r}ys wzrA’ mAlyzyA yzwr Alflbyn (in English, The prime minister of Malaysia visits the Philippines). and so obtain the most likely translation. The Gibbs sampler can therefore be used as a decoder, either running in max-derivation and max-translation mode. Using the Gibbs sampler in this way mak"
W09-1114,C96-2215,0,0.0376306,"Missing"
W09-1114,P06-2101,0,0.637386,"ared to be little benefit to marginalizing over the latent derivations. This is almost certainly a side effect of the MERT training approach that was used to construct the models so as to maximise the performance of the model on its single best derivation, without regard to the shape of the rest of the distribution (Blunsom et al., 2008). In this section we 107 describe a further application of the Gibbs sampler: to do unbiased minimum risk training. While there have been at least two previous attempts to do minimum risk training for MT, both approaches relied on biased k-best approximations (Smith and Eisner, 2006; Zens and Ney, 2007). Since we sample from the whole distribution, we will have a more accurate risk assessment. The risk, or expected loss, of a probabilistic translation model on a corpus D, defined with respect to a particular loss function `eˆ(e), where eˆ is the reference translation and e is a hypothesis translation L= X X p(e|f )`eˆ(e) (3) hˆ e,f i∈D e This value can be trivially computed using equation (2). In this section, we are concerned with finding the parameters θ that minimise (3). Fortunately, with the log-linear parameterization of p(e|f ), L is differentiable with respect to"
W09-1114,N07-1062,0,0.101321,"of state-of-the-art systems which are able to learn from parallel corpora which were generated for other purposes — a direct result of employing a mathematical framework that we can reason about independently of any particular model. For example, we can train SMT models using maximum likelihood estimation (Brown et al., 1993; Och and Ney, 2000; Marcu and Wong, 2002). Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (Kumar and Byrne, 2004; Smith and 102 Eisner, 2006; Zens and Ney, 2007). We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009). Most models define multiple derivations for each translation; the probability of a translation is thus the sum over all of its derivations. Unfortunately, finding the maximum probability translation is NPhard for all but the most trivial of models in this setting (Sima’an, 1996). It is thus necessary to resort to approximations for this sum and the search for its maximum e∗ . The most common"
W09-1114,D07-1055,0,0.0381723,"th the sampler this restriction is mitigated: any function of h(e, f, a) may participate in the translation model subject only to its own computability. Freed from the rusty manacles of dynamic programming, we anticipate development of many useful features. 6 To our knowledge, we are the first to apply Monte Carlo methods to maximum translation and minimum risk translation. Approaches to the former (Blunsom et al., 2008; May and Knight, 2006) rely on dynamic programming techniques which do not scale well without heuristic approximations, while approaches to the latter (Smith and Eisner, 2006; Zens et al., 2007) use biased k-best approximations. 7 Conclusion We have described a Gibbs sampler for approximating two intractable problems in SMT: maximum translation decoding (and its variant, minimum risk decoding) and minimum risk training. By using Monte Carlo techniques we avoid the biases associated with the more commonly used DP based maxderivation (or k-best derivation) approximation. In doing so we provide a further tool to the translation community that we envision will allow the development and analysis of increasing theoretically well motivated techniques. Acknowledgments Related work Our sample"
W09-1114,P08-1012,0,0.00643602,"framework that we can reason about independently of any particular model. For example, we can train SMT models using maximum likelihood estimation (Brown et al., 1993; Och and Ney, 2000; Marcu and Wong, 2002). Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (Kumar and Byrne, 2004; Smith and 102 Eisner, 2006; Zens and Ney, 2007). We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009). Most models define multiple derivations for each translation; the probability of a translation is thus the sum over all of its derivations. Unfortunately, finding the maximum probability translation is NPhard for all but the most trivial of models in this setting (Sima’an, 1996). It is thus necessary to resort to approximations for this sum and the search for its maximum e∗ . The most common of these approximations is the max-derivation approximation, which for many models can be computed in polynomial time via dynamic programming (DP). Though effe"
W10-1715,P05-1033,0,0.098205,"it. Thus, all our experiments should be replicable with publicly available resources. Tree-Based Models A major extension of the capabilities of the Moses system is the accommodation of tree-based models (Hoang et al., 2009). While we have not yet carried out sufficient experimentation and optimization of the implementation, we took the occasion of the shared translation task as a opportunity to build large-scale systems using such models. We build two translation systems: One using tree-based models without additional linguistic annotation, which are known as hierarchical phrasebased models (Chiang, 2005), and another system that uses linguistic annotation on the target side, which are known under many names such as string-to-tree models or syntactified target models (Marcu et al., 2006). Both models are trained using a very similar pipeline as for the phrase model. The main difference is that the translation rules do not have to be contiguous phrases, but may contain gaps with are labeled and co-ordinated by non-terminal symbols. Decoding with such models requires a very different algorithm, which is related to syntactic chart parsing. In the target syntax model, the target gaps and the entir"
W10-1715,P05-1066,1,0.764923,"er sentence to about 8 sec/sentence. However, this resulted only in minimal gains, on average +0.03 BLEU. For details refer back to Table 1. German–English For the German–English language direction, we used two additional processing steps that have shown to be successful in the past, and again resulted in significant gains. We split large words based on word frequencies to tackle the problem of word compounds in German (Koehn and Knight, 2003). Secondly, we re-order the German input to the decoder (and the German side of the training data) to align more closely to the English target language (Collins et al., 2005). The two methods improve +0.58 and +0.52 over the baseline individually, and +1.34 when combined. See also Table 8. 3.5 109 Corpus Last year, due to time constraints, we were not able to use the billion word 109 corpus for the French–English language pairs. This is largest publicly available parallel corpus, and it does strain computing resources, for instance forcing us to use multi-threaded GIZA++ (Gao and Vogel, 2008). Table 7 shows the gains obtained from using this corpus in both the translation model and the language model opposed to a baseline system trained with otherwise the same set"
W10-1715,W06-1607,0,0.0794072,"Missing"
W10-1715,W08-0509,0,0.0249618,"ehn and Knight, 2003). Secondly, we re-order the German input to the decoder (and the German side of the training data) to align more closely to the English target language (Collins et al., 2005). The two methods improve +0.58 and +0.52 over the baseline individually, and +1.34 when combined. See also Table 8. 3.5 109 Corpus Last year, due to time constraints, we were not able to use the billion word 109 corpus for the French–English language pairs. This is largest publicly available parallel corpus, and it does strain computing resources, for instance forcing us to use multi-threaded GIZA++ (Gao and Vogel, 2008). Table 7 shows the gains obtained from using this corpus in both the translation model and the language model opposed to a baseline system trained with otherwise the same settings. For French–English we see large gains (+1.23), but not for English–French (+0.10). Our official submission for the French–English language pairs used these models. They did not include a part-of-speech language model and bigger beam sizes. 3.7 Translation Model Interpolation Finally, we explored a novel domain adaption method for the translation model. Since the interpolation of language models is very successful,"
W10-1715,W06-1606,0,0.0432787,"on of tree-based models (Hoang et al., 2009). While we have not yet carried out sufficient experimentation and optimization of the implementation, we took the occasion of the shared translation task as a opportunity to build large-scale systems using such models. We build two translation systems: One using tree-based models without additional linguistic annotation, which are known as hierarchical phrasebased models (Chiang, 2005), and another system that uses linguistic annotation on the target side, which are known under many names such as string-to-tree models or syntactified target models (Marcu et al., 2006). Both models are trained using a very similar pipeline as for the phrase model. The main difference is that the translation rules do not have to be contiguous phrases, but may contain gaps with are labeled and co-ordinated by non-terminal symbols. Decoding with such models requires a very different algorithm, which is related to syntactic chart parsing. In the target syntax model, the target gaps and the entire target phrase must map to constituents in the parse tree. This restriction may be relaxed by adding constituent labels such as DET + ADJ or NP DET to group neighboring constituents or"
W10-1715,I08-2089,1,0.821265,"lation: number of tokens, perplexity, and interpolation weight for the different corpora 2.1 Cased 25.25 25.23 19.47 20.74 24.20 23.83 14.68 14.63 Interpolated Language Model 2.2 The WMT training data exhibits an increasing diversity of corpora: Europarl, News Commentary, UN, 109 , News — and seven different sources within the Czeng corpus. It is well known that domain adaptation is an important step in optimizing machine translation systems. A relatively simple and straight-forward method is the linear interpolation of the language model, as we explored previously (Koehn and Schroeder, 2007; Schwenk and Koehn, 2008). We trained domain-specific language models separately and then linearly interpolated them using SRILM toolkit (Stolke, 2002) with weights opTruecasing As last year, we deal with uppercase and lowercase forms of the same words by truecasing the corpus. This means that we change each surface word occurrence of a word to its natural case, e.g., the, Europe. During truecasing, we change the first word of a sentence to its most frequent casing. During de-truecasing, we uppercase the first letter of the first word of a sentence. See Table 3 for the performance of this method. In this table, we com"
W10-1715,2009.iwslt-papers.4,1,0.831497,"and our ability to exploit them. We also saw gains from adding linguistic annotation (in form of 7-gram models over part-ofspeech tags) and promising results for tree-based models. At this point, we are quite satisfied being able to build competitive systems with these new models, which opens up major new research directions. Everything we described here is part of the open source Moses toolkit. Thus, all our experiments should be replicable with publicly available resources. Tree-Based Models A major extension of the capabilities of the Moses system is the accommodation of tree-based models (Hoang et al., 2009). While we have not yet carried out sufficient experimentation and optimization of the implementation, we took the occasion of the shared translation task as a opportunity to build large-scale systems using such models. We build two translation systems: One using tree-based models without additional linguistic annotation, which are known as hierarchical phrasebased models (Chiang, 2005), and another system that uses linguistic annotation on the target side, which are known under many names such as string-to-tree models or syntactified target models (Marcu et al., 2006). Both models are trained"
W10-1715,W09-0429,1,0.851718,"Missing"
W10-1715,W06-3119,0,0.12094,"Missing"
W10-1715,D07-1091,1,0.826346,"we find singleton 357,929,182 phrase pairs and 24,966,751 phrase pairs that occur twice. The Good Turing formula tells us to adapt singleton 24,966,751 counts to 357,929,182 = 0.14. This means for our degenerate example of a single occurrence of a single French phrase that its single English translation has probability 0.14 1 = 0.14 (we do not adjust the denominator). Good Turing smoothing of the translation table gives us a gain of +0.17 BLEU points on average, and improvements for 7 out of 8 language pairs. For details refer back to Table 1. 3.3 POS n-gram Model The factored model approach (Koehn and Hoang, 2007) allows us to integrate 7-gram models over part-of-speech tags. The part-of-speech tags are produced during decoding by the phrase mapping of surface words on the source side to a factored representation of surface words and their part-ofspeech tags on the target side in one translation step. We previously used this additional scoring component for the German–English language pairs with success. Thus we now applied to it all other language pairs (except for English–Czech due to the lack of a Czech part-of-speech tagger). We used the following part-of-speech taggers: • • • • English: mxpost1 Ge"
W10-1715,P07-2045,1,0.0151141,"the ACL Workshop for Statistical Machine Translation 2010 in all language pairs. We continued our efforts to integrate linguistic annotation into the translation process, using factored and treebased translation models. On average we outperformed our submission from last year by 2.16 BLEU points on the same newstest2009 test set. While the submitted system follows the factored phrase-based approach, we also built hierarchical and syntax-based models for the English–German language pair and report on its performance on the development test sets. All our systems are based on the Moses toolkit (Koehn et al., 2007). We achieved gains over the systems from last year by consistently exploiting all available training data, using large-scale domain-interpolated, and consistent use of the factored translation model to integrate n-gram models over speech tags. We also experimented with novel domain adaptation methods, with mixed results. 2 • cube pruning We used most of these setting in our submission last year (Koehn and Haddow, 2009). The main difference to our baseline system from the submission from last year is the use of additional training data: larger releases of the News Commentary, Europarl, Czeng,"
W10-1715,E03-1076,1,0.823663,"e decoder is quite fast, partly due to multithreaded decoding using 4 cores machines (Haddow, 2010). Increasing the beam sizes slowed down decoding speed from about 2 seconds per sentence to about 8 sec/sentence. However, this resulted only in minimal gains, on average +0.03 BLEU. For details refer back to Table 1. German–English For the German–English language direction, we used two additional processing steps that have shown to be successful in the past, and again resulted in significant gains. We split large words based on word frequencies to tackle the problem of word compounds in German (Koehn and Knight, 2003). Secondly, we re-order the German input to the decoder (and the German side of the training data) to align more closely to the English target language (Collins et al., 2005). The two methods improve +0.58 and +0.52 over the baseline individually, and +1.34 when combined. See also Table 8. 3.5 109 Corpus Last year, due to time constraints, we were not able to use the billion word 109 corpus for the French–English language pairs. This is largest publicly available parallel corpus, and it does strain computing resources, for instance forcing us to use multi-threaded GIZA++ (Gao and Vogel, 2008)."
W10-1715,W07-0733,1,0.872608,"Table 2: English LM interpolation: number of tokens, perplexity, and interpolation weight for the different corpora 2.1 Cased 25.25 25.23 19.47 20.74 24.20 23.83 14.68 14.63 Interpolated Language Model 2.2 The WMT training data exhibits an increasing diversity of corpora: Europarl, News Commentary, UN, 109 , News — and seven different sources within the Czeng corpus. It is well known that domain adaptation is an important step in optimizing machine translation systems. A relatively simple and straight-forward method is the linear interpolation of the language model, as we explored previously (Koehn and Schroeder, 2007; Schwenk and Koehn, 2008). We trained domain-specific language models separately and then linearly interpolated them using SRILM toolkit (Stolke, 2002) with weights opTruecasing As last year, we deal with uppercase and lowercase forms of the same words by truecasing the corpus. This means that we change each surface word occurrence of a word to its natural case, e.g., the, Europe. During truecasing, we change the first word of a sentence to its most frequent casing. During de-truecasing, we uppercase the first letter of the first word of a sentence. See Table 3 for the performance of this met"
W10-1756,D08-1023,0,0.0244609,"Missing"
W10-1756,D08-1064,0,0.088056,"While our initial formulation of minimum risk training is similar to that of Arun et al. (2009), in preliminary experiments we observed a tendency for translation performance on held-out data to quickly increase to a maximum and then plateau. Hypothesizing that we were being trapped in local maxima as G is non-convex, we decided to 4.2 Corpus sampling While the objective functions in Equations 5 and 4.1 use a sentence-level variant of BLEU, the model’s test-time performance is evaluated with corpus level BLEU. The lack of correlation between sentence-level BLEU and corpus BLEU is well-known (Chiang et al., 2008a). Therefore, in an effort to address this issue, we tried maximizing expected corpus BLEU directly. In other words, given a training corpus of the form hCF , CEˆ i where CF is a set of source sentences and CEˆ its corresponding reference translations, we consider a gain function defined on the 367 hypothesized translation CE of the input CF with respect to CEˆ . The objective in equation 5 therefore becomes: G= X P (CE |CF )BLEUCEˆ (CE ) f1 f2 f3 SAMPLE FROM P(e,a |f) (7) CE The pair (CE , CF ) is denoted as a corpus sample corresponding to a sequence (e1 , a1 ), . . . , (eN , aN ) of deriva"
W10-1756,D08-1024,0,0.287469,"While our initial formulation of minimum risk training is similar to that of Arun et al. (2009), in preliminary experiments we observed a tendency for translation performance on held-out data to quickly increase to a maximum and then plateau. Hypothesizing that we were being trapped in local maxima as G is non-convex, we decided to 4.2 Corpus sampling While the objective functions in Equations 5 and 4.1 use a sentence-level variant of BLEU, the model’s test-time performance is evaluated with corpus level BLEU. The lack of correlation between sentence-level BLEU and corpus BLEU is well-known (Chiang et al., 2008a). Therefore, in an effort to address this issue, we tried maximizing expected corpus BLEU directly. In other words, given a training corpus of the form hCF , CEˆ i where CF is a set of source sentences and CEˆ its corresponding reference translations, we consider a gain function defined on the 367 hypothesized translation CE of the input CF with respect to CEˆ . The objective in equation 5 therefore becomes: G= X P (CE |CF )BLEUCEˆ (CE ) f1 f2 f3 SAMPLE FROM P(e,a |f) (7) CE The pair (CE , CF ) is denoted as a corpus sample corresponding to a sequence (e1 , a1 ), . . . , (eN , aN ) of deriva"
W10-1756,J07-2003,0,0.0731257,"ebatable (Table 1), running Moses with minimum risk trained weights gives results that are in line with what we would expect - lattice MBR does systematically better than competing decoding algorithms. This suggests that the unbi7 Related Work Expected BLEU training for phrase-based models has been successfully attempted by (Smith and Eisner, 2006; Zens et al., 2007), however they both used biased n-best lists to approximate the posterior distribution. Li and Eisner (2009) present work on performing expected BLEU training with deterministic annealing on translation forests generated by Hiero (Chiang, 2007). Since BLEU does not factorize over the search graph, they use the linear approximation of Tromble et al. (2008) instead. Pauls et al. (2009) present an alternate training criterion over translation forests called CoBLEU, 8 372 up to 1081 as per Tromble et al. (2008) similar in spirit to expected BLEU training, but aimed to maximize the expected counts of n-grams appearing in reference translations. This training criterion is used in conjunction with consensus decoding (DeNero et al., 2009), a linear-time approximation of MBR. In contrast to the approaches above, the algorithms presented in t"
W10-1756,P09-1064,0,0.0820261,"Missing"
W10-1756,D07-1103,0,0.0254345,"Missing"
W10-1756,N03-1017,1,0.0361222,"s samples from the posterior distribution of the translation model. For the work presented in this paper, we use this sampler. The sampler produces a sequence of samples, S1N = (e1 , a1 ) . . . (eN , aN ), that are drawn from the distribution p(e, a|f ). These samples can be used to estimate the expectation of a function h(e, a, f ) as follows: N 1 X Ep(a,e|f ) [h] = lim h(ai , ei , f ) (3) N →∞ N Inference methods for MT i=1 3 We assume a phrase-based machine translation model, defined with a log-linear form, with feature function vector h and parametrized by weight vector θ, as described in Koehn et al. (2003). The input sentence, f , is segmented into phrases, which are sequences of adjacent words. Each source phrase is translated into the target language, to produce an output sentence e and an alignment a representing the mapping from source to target phrases. Phrases are allowed to be reordered. p(e, a|f ; θ) = P Gibbs sampling for phrase-based MT Decoding In this work, we are interested in performing MBR decoding with BLEU. We define the MBR decision rule following Tromble et al. (2008): X 0 0 e∗ = arg max BLEU e (e )p(e |f ) (4) e∈H e0 ∈E where H refers to the hypothesis space from which tr"
W10-1756,P07-2045,1,0.0173102,"Missing"
W10-1756,N04-1022,0,0.0315341,"Missing"
W10-1756,P09-1019,0,0.333748,"mming over the translations in the list. The second step is to find the correct scale factor for the scores using a hyper-parameter search over held-out data. This is needed because the model parameters for the first-pass decoder are normally learnt using MERT (Och, 2003), which is invariant under scaling of the scores. Both these steps are theoretically unsatisfactory methods of estimating the posterior probability distribution since the approximation to Z is an unbounded term and the scaling factor is an artificial way of inducing a probability distribution. Recently, (Tromble et al., 2008; Kumar et al., 2009) have shown that using a search lattice to improve the estimation of the true probability distribution can lead to improved MBR performance. However, these approaches still rely on MERT for training the base model, and in fact introduce several extra parameters which must also be estimated using either grid search or a second MERT run. The lattice pruning required to make these techniques tractable is quite drastic, and is in addition to the pruning already performed during the search. Such extensive pruning is liable to render any probability estimates heavily biased (Blunsom and Osborne, 200"
W10-1756,W09-1114,1,0.574952,"ond MERT run. The lattice pruning required to make these techniques tractable is quite drastic, and is in addition to the pruning already performed during the search. Such extensive pruning is liable to render any probability estimates heavily biased (Blunsom and Osborne, 2008; Bouchard-Cˆot´e et al., 2009). Here, we present a unified approach to training and decoding in a phrase-based translation model (Koehn et al., 2003) which keeps the objective constant across the translation pipeline and so obviates the need for any extra hyper-parameter fitting. We use the phrase-based Gibbs sampler of Arun et al. (2009) at training time to compute the gradient of our minimum risk training objective in order to apply first-order optimization techniques, We present a unified approach to performing minimum risk training and minimum Bayes risk (MBR) decoding with BLEU in a phrase-based model. Key to our approach is the use of a Gibbs sampler that allows us to explore the entire probability distribution and maintain a strict probabilistic formulation across the pipeline. We also describe a new sampling algorithm called corpus sampling which allows us at training time to use BLEU instead of an approximation thereo"
W10-1756,D09-1005,0,0.730952,"adually lowered as the optimization progresses according to some annealing schedule. Differentiating with respect to θk then shows that the annealed gradient is given by the following expression: XX ∂p (BLEUeˆ(e) − T (1 + log p)) ∂θ k e,a where  ∂p = hk − Ep(e,a|f ) [hk ] p(e, a|f ) ∂θk A high value of T leads the optimizer to find weights which describe a fairly flat distribution, whereas a lower value of T pushes the optimizer towards a more peaked distribution. We perform 10 to 20 iterations of SGD at each temperature. In their deterministic annealing formulation, (Smith and Eisner, 2006; Li and Eisner, 2009), express the parameterization of the distribution θ as γ θˆ (where γ is the scaling factor) and perform optimization in two steps, the first optimizing θˆ and the second optimizing γ. We experimented with this two stage optimization process, but found that simply performing an unconstrained optimization on θ gave better results. (6)  ∂p = hk − Ep(e,a|f ) [hk ] p(e, a|f ) ∂θk Since the gradient is expressed in terms of expectations of feature values, it can easily be calculated using the sampler and then first-order optimization techniques can be applied to find optimal values of θ. Because o"
W10-1756,P09-1067,0,0.0449904,"eover, experimental evidence on three language pairs shows that our training regime is more stable than MERT, able to generalize better and generally leads to improvement in translation when used with sampling based MBR (Section 5). An added benefit is that the trained weights also lead to better performance when used with a beam-search based decoder. 2 by the Viterbi maximum with respect to the true model maximum is unbounded. Second, the DP solution requires substantial pruning and restricts the use of non-local features. The latter problem persists even in the variational approximations of Li et al. (2009), which attempt to solve the former. 2.1 An alternate approximate inference method for phrase-based MT without any of the previously mentioned drawbacks is the Gibbs sampler (Geman and Geman, 1984) of Arun et al. (2009) which draws samples from the posterior distribution of the translation model. For the work presented in this paper, we use this sampler. The sampler produces a sequence of samples, S1N = (e1 , a1 ) . . . (eN , aN ), that are drawn from the distribution p(e, a|f ). These samples can be used to estimate the expectation of a function h(e, a, f ) as follows: N 1 X Ep(a,e|f ) [h] ="
W10-1756,P03-1021,0,0.13736,"Missing"
W10-1756,P02-1040,0,0.087488,"Missing"
W10-1756,D09-1147,0,0.329001,"BR does systematically better than competing decoding algorithms. This suggests that the unbi7 Related Work Expected BLEU training for phrase-based models has been successfully attempted by (Smith and Eisner, 2006; Zens et al., 2007), however they both used biased n-best lists to approximate the posterior distribution. Li and Eisner (2009) present work on performing expected BLEU training with deterministic annealing on translation forests generated by Hiero (Chiang, 2007). Since BLEU does not factorize over the search graph, they use the linear approximation of Tromble et al. (2008) instead. Pauls et al. (2009) present an alternate training criterion over translation forests called CoBLEU, 8 372 up to 1081 as per Tromble et al. (2008) similar in spirit to expected BLEU training, but aimed to maximize the expected counts of n-grams appearing in reference translations. This training criterion is used in conjunction with consensus decoding (DeNero et al., 2009), a linear-time approximation of MBR. In contrast to the approaches above, the algorithms presented in this paper are able to explore an unpruned search space. By using corpus sampling, we can perform minimum risk training with corpus BLEU rather"
W10-1756,P06-2101,0,0.732419,"ion he∗ , a∗ i. This approximation can be computed in polynomial time via dynamic programming (DP). Though fast and effective for many problems, it has two serious drawbacks for probabilistic inference. First, the error incurred 2 The ngram precision counts are smoothed by adding 0.01 for n > 1 366 4 Minimum Risk Training employ deterministic annealing (Rose, 1998) to smooth the objective function to ensure that the optimizer explored as large a region as possible of the space before it settled on an optimal weight set. Our instantiation of deterministic annealing (DA) is based on the work of Smith and Eisner (2006), and involves the addition of an entropic prior to the objective""in Equation 5 to give ! # X X Gˆ = p(e, a|f )BLEUeˆ(e) + T.H(p) In order to train models suitable for use with MaxTrans or MBR decoding, we need to employ a training method which takes account of the whole distribution. To this end, we employ minimum risk training to find weights θ for Equation 1 that minimize the expected loss on the training set. We consider two variants of minimum risk training: sentence sampling optimizes an objective defined at the sentence level and corpus sampling a corpusbased objective. 4.1 hˆ e,f i∈D S"
W10-1756,D08-1065,0,0.306714,"ned with a log-linear form, with feature function vector h and parametrized by weight vector θ, as described in Koehn et al. (2003). The input sentence, f , is segmented into phrases, which are sequences of adjacent words. Each source phrase is translated into the target language, to produce an output sentence e and an alignment a representing the mapping from source to target phrases. Phrases are allowed to be reordered. p(e, a|f ; θ) = P Gibbs sampling for phrase-based MT Decoding In this work, we are interested in performing MBR decoding with BLEU. We define the MBR decision rule following Tromble et al. (2008): X 0 0 e∗ = arg max BLEU e (e )p(e |f ) (4) e∈H e0 ∈E where H refers to the hypothesis space from which translations are chosen, E refers to the evidence space used for calculating risk and BLEU e (e0 ) is a gain function that indicates the reward of hypothesising e0 when the reference solution is e. To perform MBR decoding using the sampler, let the function h in Equation 3 be the indicator function h = δ(a, a ˆ)δ(e, eˆ). Then, Equation 3 provides an estimate of p(ˆ a, eˆ|f ), and using h = δ(e, eˆ) marginalizes over all derivations a0 , yielding an estimate of p(ˆ e|f ). MBR is computed"
W10-1756,D07-1055,0,0.34657,"aining over an unpruned search space. Our proposed corpus sampling technique, like MERT, is able to optimize corpus BLEU directly whereas alternate parameter estimation techniques usually employed in SMT optimize approximations of BLEU . Chiang et al. (2008b) accounts for the online nature of the MIRA optimization algorithm by smoothing the sentence-level BLEU precision counts of a translation with a weighted average of the precision counts of previously decoded sentences, thus approximating corpus BLEU. As for minimum risk training, prior implementations have either used sentence-level BLEU (Zens et al., 2007) or a linear approximation to BLEU (Smith and Eisner, 2006; Li and Eisner, 2009). At test time, the sampler works best as an MBR decoder, but also allows us to verify past claims about the benefits of marginalizing over alignments during decoding. We compare the sampler MBR decoder’s performance against MERToptimized Moses run under three different decoding regimes, finding that the sampler does as well or better on 4 out of 5 datasets. Our training and testing pipeline has the advantage of being able to handle a large number of both local and global features so we expect in the future to outp"
W10-1756,P08-1024,0,\N,Missing
W11-2130,W09-1114,1,0.87215,"dom scores assigned to hypotheses) showed very little difference in performance. Once the initial set of hypotheses for the new batch is created, the SampleRank innermost loop (lines 6-14 in Algorithm 1) proceeds by repeatedly choosing a sample hypothesis set (y 0 ) and an oracle hypothesis set (y + ), corresponding to the source side of the batch (x). Given the current hypothesis set ys−1 = (e1 , . . . , ek ), the sample and oracle are chosen as follows. Firstly, a hypothesis ej is selected randomly from ys−1 , and a neighbourhood of alternate hypotheses N 3 ej generated using operators from Arun et al. (2009) (explained shortly). Model scores are calculated for all the hypotheses in N , converted to probabilities using Equation (1), and a sample e0j taken from N using these probabilities. The sample hypothesis set (y 0 ) is then the current hypothesis set (ys−1 ) with ej replaced by e0j . The oracle is created, analogously Chiang et al. (2008), by choosing e+ j ∈ N to maximise the sum of gain (calculated on the batch) and model score. The oracle hypothesis set (y + ) is then ys−1 with ej replaced by e+ j . We now describe how the neighbourhood is chosen. Given a single hypothesis ej , a neighbourh"
W11-2130,W10-1756,1,0.834184,"as a metric – because evaluation of translation is so difficult, any reasonable gain function is likely to have a complex relationship with the model parameters. Gradient-based tuning methods, such as minimum risk training, have been investigated as possible alternatives to MERT. Expected BLEU is normally adopted as the objective since it is differentiable and so can be optimised by a form of stochastic gradient ascent. The feature expectations required for the gradient calculation can be obtained from n-best lists or lattices (Smith and Eisner, 2006; Li and Eisner, 2009), or using sampling (Arun et al., 2010), both of which can be computationally expensive. 261 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 261–271, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics Margin-based techniques such as perceptron training (Liang et al., 2006) and MIRA (Chiang et al., 2008; Watanabe et al., 2007) have also been shown to be able to tune MT systems and scale to large numbers of features, but these generally involve repeatedly decoding the tuning set (and so are expensive) and require sentence-level approximations to the BLEU objective. In"
W11-2130,P08-1024,0,0.101275,"before making a parameter weight update, as opposed to the current work where weights may be updated after every sample. One novel feature of Arun et al. (2010) is that they were able to train to directly maximise corpus BLEU, instead of its sentence-based approximation, although this only made a small difference to the results. The training methods in (Arun et al., 2010) are very resource intensive, with the experiments running for 48 hours on around 40 cores, on a pruned phrase table derived from Europarl, and a 3-gram language model. Instead of using expected BLEU as a training objective, Blunsom et al. (2008) trained their model to directly maximise the log-likelihood of the discriminative model, estimating feature expectations from a packed chart. Their model treats derivations as a latent variable, directly modelling the translation probability. Margin-based techniques have the advantage that they do not have to employ expensive and complex algorithms to calculate the feature expectations. Typically, either perceptron ((Liang et al., 2006), (Arun and Koehn, 2007)) or MIRA ((Watanabe et al., 2007), (Chiang et al., 2008)) is employed, but in both cases the idea is to repeatedly decode sentences fr"
W11-2130,D08-1024,0,0.702897,"ve since it is differentiable and so can be optimised by a form of stochastic gradient ascent. The feature expectations required for the gradient calculation can be obtained from n-best lists or lattices (Smith and Eisner, 2006; Li and Eisner, 2009), or using sampling (Arun et al., 2010), both of which can be computationally expensive. 261 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 261–271, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics Margin-based techniques such as perceptron training (Liang et al., 2006) and MIRA (Chiang et al., 2008; Watanabe et al., 2007) have also been shown to be able to tune MT systems and scale to large numbers of features, but these generally involve repeatedly decoding the tuning set (and so are expensive) and require sentence-level approximations to the BLEU objective. In this paper we present an alternative method of tuning MT systems known as SampleRank, which has certain advantages over other methods in use today. SampleRank operates by repeatedly sampling pairs of translation hypotheses (for a given source sentence) and updating the feature weights if the ranking induced by the MT model (1) i"
W11-2130,W09-0439,0,0.0745551,"ning) (Och, 2003), to maximise performance Philipp Koehn School of Informatics University of Edinburgh pkoehn@inf.ed.ac.uk as measured by an automated metric such as BLEU (Papineni et al., 2002). MERT training uses a parallel data set (known as the tuning set) consisting of about 1000-2000 sentences, distinct from the data set used to build the generative models. Optimising the weights in Equation (1) is often referred to as tuning the MT system, to differentiate it from the process of training the generative models. MERT’s inability to scale beyond 20-30 features, as well as its instability (Foster and Kuhn, 2009) have led to investigation into alternative ways of tuning MT systems. The development of tuning methods is complicated, however by, the use of BLEU as an objective function. This objective in its usual form is not differentiable, and has a highly non-convex error surface (Och, 2003). Furthermore BLEU is evaluated at the corpus level rather than at the sentence level, so tuning methods either have to consider the entire corpus, or resort to a sentencelevel approximation of BLEU. It is unlikely, however, that the difficulties in discriminative MT tuning are due solely to the use of BLEU as a me"
W11-2130,D07-1091,1,0.744359,"eature to fire for each bigram. Dummy phrases with parts-of-speech < S &gt; and </ S &gt; are inserted at the start and end of the sentence, and also used to construct phrase boundary features. The example in Figure 4 shows the phrase-boundary features from a typical hypothesis. The idea is similar to a part-of-speech language model, but discriminatively trained, and targeted at how phrases are joined together in the hypothesis. The target-side part-of-speech tags are added using the Brill tagger, and incorporated into the phrase table using the factored translation modelling capabilities of Moses (Koehn and Hoang, 2007). Adding the phrase boundary features to the WMTSMALL system increased the feature count from 8 to around 800. Training experiments were run for both the French-English and German-English models, using the same configuration as in Section 3.2, varying the number of cores (8 or 16) and the number of samples per sentence (100 or 500). Training times were similar to those for the WMT- SMALL system. The mean maximum scores on heldout are shown in Table 5. We suspect that these features are fixing some short range reordering problems which 27 26 27 26 27 Bleu 25 ● ● ● ●●● ● ● ●●● ● ● ● ●● ●●● ● ●●●"
W11-2130,W07-0733,1,0.831597,"side of this data set. The features used in the WMT- SMALL translation system were the five Moses translation features, a language model feature, a word penalty feature and a distortion distance feature. To build the WMT- LARGE translation system, both the ep11 data set and the nc11 data set were concatenated together before building the translation model out of the resulting corpus of about 2 million sentences. Separate 5-gram language models were built from the target side of the two data sets and then they were interpolated using weights chosen to minimise the perplexity on the tuning set (Koehn and Schroeder, 2007). In the WMT- LARGE system, the eight core features were supplemented with the six features of the lexicalised reordering model, which was trained on the same data as was used to build the translation model. Whilst a training set size of 2 million sentences would not normally be sufficient to build a competitive system for an MT shared task, it is sufficient to show that how SampleRank training performs on a realistic sized system, whilst still allowing for plenty of experimenation with the algorithm’s parameters. For tuning, the nc-devtest2007 was used, with the first half of nc-test2007 corp"
W11-2130,D09-1005,0,0.275889,"tuning are due solely to the use of BLEU as a metric – because evaluation of translation is so difficult, any reasonable gain function is likely to have a complex relationship with the model parameters. Gradient-based tuning methods, such as minimum risk training, have been investigated as possible alternatives to MERT. Expected BLEU is normally adopted as the objective since it is differentiable and so can be optimised by a form of stochastic gradient ascent. The feature expectations required for the gradient calculation can be obtained from n-best lists or lattices (Smith and Eisner, 2006; Li and Eisner, 2009), or using sampling (Arun et al., 2010), both of which can be computationally expensive. 261 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 261–271, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics Margin-based techniques such as perceptron training (Liang et al., 2006) and MIRA (Chiang et al., 2008; Watanabe et al., 2007) have also been shown to be able to tune MT systems and scale to large numbers of features, but these generally involve repeatedly decoding the tuning set (and so are expensive) and require sentence-level a"
W11-2130,P06-1096,0,0.374556,"Missing"
W11-2130,P03-1021,0,0.438086,"ine translation (PBMT), the standard approach is to express the probability distribution p(a, e|f ) (where f is the source sentence and (a, e) is the aligned target sentence) in terms of a linear model based on a small set of feature functions ! n X p(a, e|f ) ∝ exp wi hi (a, e, f ) (1) i=1 The feature functions {hi } typically include log probabilities of generative models such as translation, language and reordering, as well as nonprobabilistic features such as word, phrase and distortion penalties. The feature weights w = {wi } are normally trained using MERT (minimum error rate training) (Och, 2003), to maximise performance Philipp Koehn School of Informatics University of Edinburgh pkoehn@inf.ed.ac.uk as measured by an automated metric such as BLEU (Papineni et al., 2002). MERT training uses a parallel data set (known as the tuning set) consisting of about 1000-2000 sentences, distinct from the data set used to build the generative models. Optimising the weights in Equation (1) is often referred to as tuning the MT system, to differentiate it from the process of training the generative models. MERT’s inability to scale beyond 20-30 features, as well as its instability (Foster and Kuhn,"
W11-2130,P02-1040,0,0.081806,"sentence) in terms of a linear model based on a small set of feature functions ! n X p(a, e|f ) ∝ exp wi hi (a, e, f ) (1) i=1 The feature functions {hi } typically include log probabilities of generative models such as translation, language and reordering, as well as nonprobabilistic features such as word, phrase and distortion penalties. The feature weights w = {wi } are normally trained using MERT (minimum error rate training) (Och, 2003), to maximise performance Philipp Koehn School of Informatics University of Edinburgh pkoehn@inf.ed.ac.uk as measured by an automated metric such as BLEU (Papineni et al., 2002). MERT training uses a parallel data set (known as the tuning set) consisting of about 1000-2000 sentences, distinct from the data set used to build the generative models. Optimising the weights in Equation (1) is often referred to as tuning the MT system, to differentiate it from the process of training the generative models. MERT’s inability to scale beyond 20-30 features, as well as its instability (Foster and Kuhn, 2009) have led to investigation into alternative ways of tuning MT systems. The development of tuning methods is complicated, however by, the use of BLEU as an objective functio"
W11-2130,2010.amta-papers.31,0,0.0674338,"ate the parameter weights if the best hypothesis according to the model differs from some “oracle” sentence. The approaches differ in the way they compute the oracle sentence, as well as the way the weights are updated. Normally sentences are processed one-by-one, with a weight update after considering each sentence, and sentence BLEU is used as the objective. However Chiang et al. (2008) introduced an approximation to corpus BLEU by using a rolling history. Both papers on MIRA demonstrated its ability to extend to large numbers of features. In the only known application of SampleRank to SMT, Roth et al. (2010) deploys quite a different translation model to the usual phrase-based model, allowing overlapping phrases and implemented as a factor graph. Decoding is with a rather slow stochastic search and performance is quite poor, but this model, in common with the training algorithm presented in the current work, permits features which depend on the whole sentence. 5 Discussion and Conclusions The results presented in Table 6 show that SampleRank is a viable method of parameter tuning for phrase-based MT systems, beating MERT in many cases, and equalling it in others. It is also able to do what MERT c"
W11-2130,P06-2101,0,0.353458,"ies in discriminative MT tuning are due solely to the use of BLEU as a metric – because evaluation of translation is so difficult, any reasonable gain function is likely to have a complex relationship with the model parameters. Gradient-based tuning methods, such as minimum risk training, have been investigated as possible alternatives to MERT. Expected BLEU is normally adopted as the objective since it is differentiable and so can be optimised by a form of stochastic gradient ascent. The feature expectations required for the gradient calculation can be obtained from n-best lists or lattices (Smith and Eisner, 2006; Li and Eisner, 2009), or using sampling (Arun et al., 2010), both of which can be computationally expensive. 261 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 261–271, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics Margin-based techniques such as perceptron training (Liang et al., 2006) and MIRA (Chiang et al., 2008; Watanabe et al., 2007) have also been shown to be able to tune MT systems and scale to large numbers of features, but these generally involve repeatedly decoding the tuning set (and so are expensive) and re"
W11-2130,D07-1080,0,0.710828,"entiable and so can be optimised by a form of stochastic gradient ascent. The feature expectations required for the gradient calculation can be obtained from n-best lists or lattices (Smith and Eisner, 2006; Li and Eisner, 2009), or using sampling (Arun et al., 2010), both of which can be computationally expensive. 261 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 261–271, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics Margin-based techniques such as perceptron training (Liang et al., 2006) and MIRA (Chiang et al., 2008; Watanabe et al., 2007) have also been shown to be able to tune MT systems and scale to large numbers of features, but these generally involve repeatedly decoding the tuning set (and so are expensive) and require sentence-level approximations to the BLEU objective. In this paper we present an alternative method of tuning MT systems known as SampleRank, which has certain advantages over other methods in use today. SampleRank operates by repeatedly sampling pairs of translation hypotheses (for a given source sentence) and updating the feature weights if the ranking induced by the MT model (1) is different from the ran"
W11-2130,N10-1069,0,\N,Missing
W12-3139,D11-1033,0,0.22496,"Missing"
W12-3139,W09-0432,0,0.0272171,"6.2 Hierarchical 21.4 (–.2) 27.6 (–.3) 28.4 (–.5) 22.0 (–.4) 15.5 (–.4) 28.0 (–.8) 30.4 (–.4) 15.6 (–.6) Setup fr-en ep+nc +un en-fr ep+nc +un Table 6: Hierarchical phrase models vs. baseline phrasebased models. to their size. Table 6 shows inferior performance for all language pairs (by about half a BLEU point), although results for German–English are close (–0.2 BLEU). 5.2 Semi-Supervised Learning Other research groups have reported improvements using semi-supervised learning methods to create synthetic parallel data from monolingual data (Schwenk et al., 2008; Abdul-Rauf and Schwenk, 2009; Bertoldi and Federico, 2009; Lambert et al., 2011). The idea is to translate in-domain monolingual data with a baseline system and filter the result for use as an additional parallel corpus. Table 7 shows out results when trying to emulate the approach of Lambert et al. (2011). We translate the some of the 2011 monolingual news data (139 million words for French and 100 million words for English) from the target language into the source language with a baseline system trained on Europarl and News Commentary. Adding all the obtained data hurts (except for minimal improvements over a small French-English system). When we"
W12-3139,W11-2103,1,0.888939,"Missing"
W12-3139,J07-2003,0,0.0177689,"never lead to worse results on the sampled n-best lists. This method (PRO-MERT in the table) applied here, however, did not lead to significantly different results than plain MERT. 5 4 MERT 21.7 (1.01) 29.1 (1.02) 24.2 (1.03) 16.0 (1.00) 29.3 (0.98) 31.5 (0.98) 17.4 (0.97) What did not Work Not everything we tried worked out. Notably, two promising directions — hierarchical models and semi-supervised learning — did not yield any improvements. It is not clear if we failed or if the methods failed, but we will investigate this further in future work. 5.1 Hierarchical Models Hierarchical models (Chiang, 2007) have been supported already for a few years by Moses, and they give significantly better performance for Chinese– English over phrase-based models. While we have not yet seen benefits for many other language pairs, the eight language pairs of WMT12 allowed us to compare these two models more extensively, also in view of recent enhancements resulting in better search accuracy. Since hierarchical models are much larger (roughly 10 times bigger), we trained hierarchical models on downsized training data for most language pairs. For Spanish and French, this excludes UN and GigaFrEn; for Czech som"
W12-3139,W11-2123,0,0.0275481,", but Model 1 sampling made no difference for the Spanish systems, and was harmful for the French systems. 318 Better Language Models In previous years, we were not able to make use of the monolingual LDC Gigaword corpora due to lack of sufficiently powerful computing resources. These corpora exist for English (4.3 billion words), Spanish (1.1 billion words), and French (0.8 billion words). With the acquisition of large memory machines2 , we were now able to train language models on this data. Use of these large language models during decoding is aided by more efficient storage and inference (Heafield, 2011). Still, even with that much RAM it is not possible to train a language model with SRILM (Stolke, 2002) in one pass. Hence, we broke up the training corpus by source (New York Times, Washington Post, ...) and trained separate language model for each. The largest individual corpus was the English New York Times portion which consists of 1.5 billion words and took close to 100GB of RAM. We also trained individual language models for each year of WMT12’s monolingual corpus. We interpolated the language models using the SRILM toolkit. The toolkit has a limit of 10 language models to be merged at o"
W12-3139,D11-1125,0,0.0119275,"er tuning set (7567 sentences) by combining newstest 2008 to 2010. 4.1 Better Tuning Bigger Tuning Sets In recent experiments, mainly geared towards using much larger feature sets, we learned that larger tuning sets may give better and more stable results. We tested this hypothesis here as well. By concatenating the sets from three years (20082010), we constructed a tuning set of 7567 sentences per language. Table 4 shows that we gain on average about +0.2 BLEU points. 4.2 Pairwise Ranked Optimization We recently added an implementation of the pairwise ranked optimization (PRO) tuning method (Hopkins and May, 2011) to Moses as an alternative to Och’s (2003) minimum error rate training (MERT). We checked if this method gives us better results. Table 5 shows a mixed picture. PRO gives slightly shorter translations, probably because it optimises sentence rather than corpus BLEU, which has a noticeable effect on the BLEU score. For 2 language pairs we see better results, for 4 worse, and for 1 there is no difference. On other data and lan319 PRO 21.9 (1.00) +.2 29.1 (1.01) ±.0 24.5 (1.00) +.3 15.7 (0.96) –.3 28.9 (0.96) –.4 31.3 (0.97) –.2 16.9 (0.92) –.5 PRO-MERT 21.7 (1.01) ±.0 29.1 (1.02) ±.0 24.2 (1.03)"
W12-3139,P07-2045,1,0.0200373,"r en-es LP fr-en en-fr Abstract We report on findings of exploiting large data sets for translation modeling, language modeling and tuning for the development of competitive machine translation systems for eight language pairs. 1 Introduction We report on experiments carried out for the development of competitive systems on the datasets of the 2012 Workshop on Statistical Machine Translation. Our main focus was directed on the effective use of all the available training data during training of translation and language models and tuning. We use the open source machine translation system Moses (Koehn et al., 2007) and other standard open source tools, hence all our experiments are straightforwardly replicable1 . Compared to all single system submissions by participants of the workshop we achieved the best BLEU scores for four language pairs (es-en, en-es, cs-en, en-cs), the 2nd best results for two language pairs (fr-en, de-en), as well as a 3rd place (en-de) and a 5th place (en-fr) for the remaining pairs. We improved upon this in the post-evaluation period for some of the language pairs by more systematically applying our methods. During the development of our system, we saw most gains from using lar"
W12-3139,W11-2132,0,0.0271331,"7.6 (–.3) 28.4 (–.5) 22.0 (–.4) 15.5 (–.4) 28.0 (–.8) 30.4 (–.4) 15.6 (–.6) Setup fr-en ep+nc +un en-fr ep+nc +un Table 6: Hierarchical phrase models vs. baseline phrasebased models. to their size. Table 6 shows inferior performance for all language pairs (by about half a BLEU point), although results for German–English are close (–0.2 BLEU). 5.2 Semi-Supervised Learning Other research groups have reported improvements using semi-supervised learning methods to create synthetic parallel data from monolingual data (Schwenk et al., 2008; Abdul-Rauf and Schwenk, 2009; Bertoldi and Federico, 2009; Lambert et al., 2011). The idea is to translate in-domain monolingual data with a baseline system and filter the result for use as an additional parallel corpus. Table 7 shows out results when trying to emulate the approach of Lambert et al. (2011). We translate the some of the 2011 monolingual news data (139 million words for French and 100 million words for English) from the target language into the source language with a baseline system trained on Europarl and News Commentary. Adding all the obtained data hurts (except for minimal improvements over a small French-English system). When we filtered out half of th"
W12-3139,P03-1021,0,0.00997651,"Missing"
W12-3139,2008.iwslt-evaluation.9,0,0.0545166,"Missing"
W12-3139,W11-2158,0,0.0618264,"Missing"
W12-3139,E09-1003,0,\N,Missing
W12-3139,D08-1076,0,\N,Missing
W12-3154,D11-1033,0,0.252188,"vera and Juan, 2007; Koehn and Schroeder, 2007) is the best approach. Also, other authors (Foster et al., 2010; Niehues and Waibel, 2010; Shah et al., 2010) have tried to weight the input sentences or extracted phrases before the phrase tables are built. In this type of approach, the main problem is how to train the weights of the sentences or phrases, and each of the papers has followed a different approach. Instead of weighting the out-of-domain data, some authors have investigated data selection methods for domain adaptation (Yasuda et al., 2008; Mansour et al., 2011; Schwenk et al., 2011; Axelrod et al., 2011). This is effectively the same as using a 1-0 weighting for input sentences, but has the advantage that it is usually easier to tune a threshold than it is to train weights for all input sentences or phrases. The other advantage of doing data selection is that it can potentially remove noisy (e.g. incorrectly aligned) data. However it will be seen later in this paper that out-of-domain data can usually contribute something useful to the translation system, so the 1-0 weighting of data-selection may be somewhat heavy-handed. 3 Experiments 3.1 mented using the Moses multi-bleu.pl script. Corpora"
W12-3154,2011.iwslt-evaluation.18,0,0.359249,"mixed results, with the overall conclusion being that it is difficult to predict how best to include out-of-domain data in the PBMT training pipeline. Unlike in the current work, Duh et al. do not separate phrase extraction and scoring in order to analyse the effect of domain on them separately. They make the point that adding extra out-of-domain data 423 may degrade translation by introducing unwanted lexical ambiguity, showing anecdotal evidence for this. Similar arguments were presented in (Sennrich, 2012). A recent paper which does attempt to tease apart phrase extraction and scoring is (Bisazza et al., 2011). In this work, the authors try to improve a system trained on in-domain data by including extra entries (termed “fill-up”) from out-of-domain data – this is similar to the nc+epE and st+epE systems in Section 3.4. It is shown by Bisazza et al. that this fill-up technique has a similar effect to using MERT to weight the in and out-of domain phrase tables. In the experiments in Section 3.4 we confirm that fillup techniques mostly provide better results than using a concatenation of in and out-of domain data. There has been quite a lot of work on finding ways of weighting in and out-of domain da"
W12-3154,W07-0722,0,0.0412156,"on 3.4 we confirm that fillup techniques mostly provide better results than using a concatenation of in and out-of domain data. There has been quite a lot of work on finding ways of weighting in and out-of domain data for SMT (as opposed to simply concatenating the data sets), both for language and translation modelling. Interpolating language models using perplexity is fairly well-established (e.g. Koehn and Schroeder (2007)), but for phrase-tables it is unclear whether perplexity minimisation (Foster et al., 2010; Sennrich, 2012) or linear or log-linear interpolation (Foster and Kuhn, 2007; Civera and Juan, 2007; Koehn and Schroeder, 2007) is the best approach. Also, other authors (Foster et al., 2010; Niehues and Waibel, 2010; Shah et al., 2010) have tried to weight the input sentences or extracted phrases before the phrase tables are built. In this type of approach, the main problem is how to train the weights of the sentences or phrases, and each of the papers has followed a different approach. Instead of weighting the out-of-domain data, some authors have investigated data selection methods for domain adaptation (Yasuda et al., 2008; Mansour et al., 2011; Schwenk et al., 2011; Axelrod et al., 201"
W12-3154,P11-2031,0,0.0163226,"systems consisted of phrase tables and lexicalised reordering tables estimated using the standard Moses (Koehn et al., 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime. Separate language models were built on the target side of the in-domain and out-ofdomain training data, then linearly interpolated using SRILM to minimise perplexity on the tuning set (e.g. Koehn and Schroeder (2007)). Tuning of models used minimum error rate training (Och, 2003), repeated 3 times and averaged (Clark et al., 2011). Performance is evaluated using caseinsensitive BLEU (Papineni et al., 2002), as imple1 www.opensubtitles.org 424 Europarl (ep) News Commentary (nc) Subtitles (st) Language pairs en↔fr en↔es en↔de en↔cs en↔nl en↔fr en↔es en↔de en↔cs en↔fr en↔es en↔nl en↔cs train tune test 1.8M 1.8M 1.7M 460k 1.8M 114k 130k 135k 122k 200k 200k 200k 200k n/a n/a n/a n/a n/a 1000 1000 1000 1000 2000 2000 2000 2000 n/a n/a n/a n/a n/a 2000 2000 2000 2000 2000 2000 2000 2000 Table 1: Summary of the data sets used, with approximate sentence counts 3.2 Comparing In-domain and Out-of-domain Data The aim of this secti"
W12-3154,W07-0717,0,0.319034,"he experiments in Section 3.4 we confirm that fillup techniques mostly provide better results than using a concatenation of in and out-of domain data. There has been quite a lot of work on finding ways of weighting in and out-of domain data for SMT (as opposed to simply concatenating the data sets), both for language and translation modelling. Interpolating language models using perplexity is fairly well-established (e.g. Koehn and Schroeder (2007)), but for phrase-tables it is unclear whether perplexity minimisation (Foster et al., 2010; Sennrich, 2012) or linear or log-linear interpolation (Foster and Kuhn, 2007; Civera and Juan, 2007; Koehn and Schroeder, 2007) is the best approach. Also, other authors (Foster et al., 2010; Niehues and Waibel, 2010; Shah et al., 2010) have tried to weight the input sentences or extracted phrases before the phrase tables are built. In this type of approach, the main problem is how to train the weights of the sentences or phrases, and each of the papers has followed a different approach. Instead of weighting the out-of-domain data, some authors have investigated data selection methods for domain adaptation (Yasuda et al., 2008; Mansour et al., 2011; Schwenk et al., 20"
W12-3154,D10-1044,0,0.346701,"Missing"
W12-3154,W11-2123,0,0.0271344,"ining corpus was selected from the remaining data. Using test sets from both news-commentary and OpenSubtitles gives two domain adaptation tasks, where in both cases the out-of-domain data is europarl, a significantly larger training set than the indomain data. The three data sets in use in this paper are summarised in Table 1. The translation systems consisted of phrase tables and lexicalised reordering tables estimated using the standard Moses (Koehn et al., 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime. Separate language models were built on the target side of the in-domain and out-ofdomain training data, then linearly interpolated using SRILM to minimise perplexity on the tuning set (e.g. Koehn and Schroeder (2007)). Tuning of models used minimum error rate training (Och, 2003), repeated 3 times and averaged (Clark et al., 2011). Performance is evaluated using caseinsensitive BLEU (Papineni et al., 2002), as imple1 www.opensubtitles.org 424 Europarl (ep) News Commentary (nc) Subtitles (st) Language pairs en↔fr en↔es en↔de en↔cs en↔nl en↔fr en↔es en↔de en↔cs en↔fr en↔es en↔n"
W12-3154,W07-0733,1,0.583028,"pE systems in Section 3.4. It is shown by Bisazza et al. that this fill-up technique has a similar effect to using MERT to weight the in and out-of domain phrase tables. In the experiments in Section 3.4 we confirm that fillup techniques mostly provide better results than using a concatenation of in and out-of domain data. There has been quite a lot of work on finding ways of weighting in and out-of domain data for SMT (as opposed to simply concatenating the data sets), both for language and translation modelling. Interpolating language models using perplexity is fairly well-established (e.g. Koehn and Schroeder (2007)), but for phrase-tables it is unclear whether perplexity minimisation (Foster et al., 2010; Sennrich, 2012) or linear or log-linear interpolation (Foster and Kuhn, 2007; Civera and Juan, 2007; Koehn and Schroeder, 2007) is the best approach. Also, other authors (Foster et al., 2010; Niehues and Waibel, 2010; Shah et al., 2010) have tried to weight the input sentences or extracted phrases before the phrase tables are built. In this type of approach, the main problem is how to train the weights of the sentences or phrases, and each of the papers has followed a different approach. Instead of wei"
W12-3154,2011.iwslt-papers.5,0,0.0606385,"ear interpolation (Foster and Kuhn, 2007; Civera and Juan, 2007; Koehn and Schroeder, 2007) is the best approach. Also, other authors (Foster et al., 2010; Niehues and Waibel, 2010; Shah et al., 2010) have tried to weight the input sentences or extracted phrases before the phrase tables are built. In this type of approach, the main problem is how to train the weights of the sentences or phrases, and each of the papers has followed a different approach. Instead of weighting the out-of-domain data, some authors have investigated data selection methods for domain adaptation (Yasuda et al., 2008; Mansour et al., 2011; Schwenk et al., 2011; Axelrod et al., 2011). This is effectively the same as using a 1-0 weighting for input sentences, but has the advantage that it is usually easier to tune a threshold than it is to train weights for all input sentences or phrases. The other advantage of doing data selection is that it can potentially remove noisy (e.g. incorrectly aligned) data. However it will be seen later in this paper that out-of-domain data can usually contribute something useful to the translation system, so the 1-0 weighting of data-selection may be somewhat heavy-handed. 3 Experiments 3.1 mented"
W12-3154,2010.eamt-1.29,0,0.0437338,"domain data. There has been quite a lot of work on finding ways of weighting in and out-of domain data for SMT (as opposed to simply concatenating the data sets), both for language and translation modelling. Interpolating language models using perplexity is fairly well-established (e.g. Koehn and Schroeder (2007)), but for phrase-tables it is unclear whether perplexity minimisation (Foster et al., 2010; Sennrich, 2012) or linear or log-linear interpolation (Foster and Kuhn, 2007; Civera and Juan, 2007; Koehn and Schroeder, 2007) is the best approach. Also, other authors (Foster et al., 2010; Niehues and Waibel, 2010; Shah et al., 2010) have tried to weight the input sentences or extracted phrases before the phrase tables are built. In this type of approach, the main problem is how to train the weights of the sentences or phrases, and each of the papers has followed a different approach. Instead of weighting the out-of-domain data, some authors have investigated data selection methods for domain adaptation (Yasuda et al., 2008; Mansour et al., 2011; Schwenk et al., 2011; Axelrod et al., 2011). This is effectively the same as using a 1-0 weighting for input sentences, but has the advantage that it is usual"
W12-3154,W99-0604,0,0.0919105,"kely to apply to other types of SMT systems. Furthermore, we will mainly be concerned with the effect of domain on the translation model, since it depends on parallel data which is more likely to be in short supply than monolingual data, and domain adaptation for language modelling has been more thoroughly studied. The effect of a shift of domain in the parallel data is complicated by the fact that training a translation model is a multi-stage process. First the parallel data is word-aligned, normally using the IBM models (Brown et al., 1994), then phrases are extracted using some heuristics (Och et al., 1999) and scored using a maximum likelihood estimate. Since the effect of domain may be felt at the alignment stage, the extraction stage, or the scoring stage, we have designed experiments to try to tease these apart. Experiments comparing the effect of domain at the alignment stage with the extraction and scoring stages have already been presented by (Duh et al., 2010), so we focus more on the differences between extraction and scoring. In other words, we examine whether adding more data (in or out-of domain) helps improve coverage of the phrase table, or helps improve the scoring of phrases. A f"
W12-3154,P03-1021,0,0.00721265,"are summarised in Table 1. The translation systems consisted of phrase tables and lexicalised reordering tables estimated using the standard Moses (Koehn et al., 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime. Separate language models were built on the target side of the in-domain and out-ofdomain training data, then linearly interpolated using SRILM to minimise perplexity on the tuning set (e.g. Koehn and Schroeder (2007)). Tuning of models used minimum error rate training (Och, 2003), repeated 3 times and averaged (Clark et al., 2011). Performance is evaluated using caseinsensitive BLEU (Papineni et al., 2002), as imple1 www.opensubtitles.org 424 Europarl (ep) News Commentary (nc) Subtitles (st) Language pairs en↔fr en↔es en↔de en↔cs en↔nl en↔fr en↔es en↔de en↔cs en↔fr en↔es en↔nl en↔cs train tune test 1.8M 1.8M 1.7M 460k 1.8M 114k 130k 135k 122k 200k 200k 200k 200k n/a n/a n/a n/a n/a 1000 1000 1000 1000 2000 2000 2000 2000 n/a n/a n/a n/a n/a 2000 2000 2000 2000 2000 2000 2000 2000 Table 1: Summary of the data sets used, with approximate sentence counts 3.2 Comparing In"
W12-3154,P02-1040,0,0.105515,"ated using the standard Moses (Koehn et al., 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime. Separate language models were built on the target side of the in-domain and out-ofdomain training data, then linearly interpolated using SRILM to minimise perplexity on the tuning set (e.g. Koehn and Schroeder (2007)). Tuning of models used minimum error rate training (Och, 2003), repeated 3 times and averaged (Clark et al., 2011). Performance is evaluated using caseinsensitive BLEU (Papineni et al., 2002), as imple1 www.opensubtitles.org 424 Europarl (ep) News Commentary (nc) Subtitles (st) Language pairs en↔fr en↔es en↔de en↔cs en↔nl en↔fr en↔es en↔de en↔cs en↔fr en↔es en↔nl en↔cs train tune test 1.8M 1.8M 1.7M 460k 1.8M 114k 130k 135k 122k 200k 200k 200k 200k n/a n/a n/a n/a n/a 1000 1000 1000 1000 2000 2000 2000 2000 n/a n/a n/a n/a n/a 2000 2000 2000 2000 2000 2000 2000 2000 Table 1: Summary of the data sets used, with approximate sentence counts 3.2 Comparing In-domain and Out-of-domain Data The aim of this section is to provide both a qualitative and quantitative comparison of the three"
W12-3154,W11-2158,0,0.0188191,"ter and Kuhn, 2007; Civera and Juan, 2007; Koehn and Schroeder, 2007) is the best approach. Also, other authors (Foster et al., 2010; Niehues and Waibel, 2010; Shah et al., 2010) have tried to weight the input sentences or extracted phrases before the phrase tables are built. In this type of approach, the main problem is how to train the weights of the sentences or phrases, and each of the papers has followed a different approach. Instead of weighting the out-of-domain data, some authors have investigated data selection methods for domain adaptation (Yasuda et al., 2008; Mansour et al., 2011; Schwenk et al., 2011; Axelrod et al., 2011). This is effectively the same as using a 1-0 weighting for input sentences, but has the advantage that it is usually easier to tune a threshold than it is to train weights for all input sentences or phrases. The other advantage of doing data selection is that it can potentially remove noisy (e.g. incorrectly aligned) data. However it will be seen later in this paper that out-of-domain data can usually contribute something useful to the translation system, so the 1-0 weighting of data-selection may be somewhat heavy-handed. 3 Experiments 3.1 mented using the Moses multi-"
W12-3154,E12-1055,0,0.212758,"scoring stages. Extensive experiments on 4 different data sets, and 10 different language pairs show mixed results, with the overall conclusion being that it is difficult to predict how best to include out-of-domain data in the PBMT training pipeline. Unlike in the current work, Duh et al. do not separate phrase extraction and scoring in order to analyse the effect of domain on them separately. They make the point that adding extra out-of-domain data 423 may degrade translation by introducing unwanted lexical ambiguity, showing anecdotal evidence for this. Similar arguments were presented in (Sennrich, 2012). A recent paper which does attempt to tease apart phrase extraction and scoring is (Bisazza et al., 2011). In this work, the authors try to improve a system trained on in-domain data by including extra entries (termed “fill-up”) from out-of-domain data – this is similar to the nc+epE and st+epE systems in Section 3.4. It is shown by Bisazza et al. that this fill-up technique has a similar effect to using MERT to weight the in and out-of domain phrase tables. In the experiments in Section 3.4 we confirm that fillup techniques mostly provide better results than using a concatenation of in and o"
W12-3154,W10-1759,0,0.0154759,"en quite a lot of work on finding ways of weighting in and out-of domain data for SMT (as opposed to simply concatenating the data sets), both for language and translation modelling. Interpolating language models using perplexity is fairly well-established (e.g. Koehn and Schroeder (2007)), but for phrase-tables it is unclear whether perplexity minimisation (Foster et al., 2010; Sennrich, 2012) or linear or log-linear interpolation (Foster and Kuhn, 2007; Civera and Juan, 2007; Koehn and Schroeder, 2007) is the best approach. Also, other authors (Foster et al., 2010; Niehues and Waibel, 2010; Shah et al., 2010) have tried to weight the input sentences or extracted phrases before the phrase tables are built. In this type of approach, the main problem is how to train the weights of the sentences or phrases, and each of the papers has followed a different approach. Instead of weighting the out-of-domain data, some authors have investigated data selection methods for domain adaptation (Yasuda et al., 2008; Mansour et al., 2011; Schwenk et al., 2011; Axelrod et al., 2011). This is effectively the same as using a 1-0 weighting for input sentences, but has the advantage that it is usually easier to tune a"
W12-3154,I08-2088,0,0.00929917,"or linear or log-linear interpolation (Foster and Kuhn, 2007; Civera and Juan, 2007; Koehn and Schroeder, 2007) is the best approach. Also, other authors (Foster et al., 2010; Niehues and Waibel, 2010; Shah et al., 2010) have tried to weight the input sentences or extracted phrases before the phrase tables are built. In this type of approach, the main problem is how to train the weights of the sentences or phrases, and each of the papers has followed a different approach. Instead of weighting the out-of-domain data, some authors have investigated data selection methods for domain adaptation (Yasuda et al., 2008; Mansour et al., 2011; Schwenk et al., 2011; Axelrod et al., 2011). This is effectively the same as using a 1-0 weighting for input sentences, but has the advantage that it is usually easier to tune a threshold than it is to train weights for all input sentences or phrases. The other advantage of doing data selection is that it can potentially remove noisy (e.g. incorrectly aligned) data. However it will be seen later in this paper that out-of-domain data can usually contribute something useful to the translation system, so the 1-0 weighting of data-selection may be somewhat heavy-handed. 3 E"
W12-3154,J93-2003,0,\N,Missing
W12-3154,W09-0401,1,\N,Missing
W12-3154,P07-2045,1,\N,Missing
W12-3154,2010.iwslt-papers.5,0,\N,Missing
W13-2201,W13-2205,0,0.0583032,"Missing"
W13-2201,S13-1034,0,0.0277746,"Missing"
W13-2201,C12-1008,0,0.0091484,"ignments is used to extract quantitative (amount and distribution of the alignments) and qualitative (importance of the aligned terms) features under the assumption that alignment information can help tasks where sentencelevel semantic relations need to be identified (Souza et al., 2013). Three similar EnglishSpanish systems are built and used to provide pseudo-references (Soricut et al., 2012) and back-translations, from which automatic MT evaluation metrics could be computed and used as features. DFKI (T1.2, T1.3): DFKI’s submission for Task 1.2 was based on decomposing rankings into pairs (Avramidis, 2012), where the best system for each pair was predicted with Logistic Regression (LogReg). For GermanEnglish, LogReg was trained with Stepwise Feature Selection (Hosmer, 1989) on two feature sets: Feature Set 24 includes basic counts augmented with PCFG parsing features (number of VPs, alternative parses, parse probability) on both source and target sentences (Avramidis et al., 2011), and pseudo-reference METEOR score; the most successful set, Feature Set 33 combines those 24 features with the 17 baseline features. For English-Spanish, LogReg was used with L2 Regularisation (Lin et al., 2007) and"
W13-2201,W13-2206,0,0.0913052,"ool of Data Analysis (Borisov et al., 2013) Carnegie Mellon University (Ammar et al., 2013) CMU - TREE - TO - TREE CU - BOJAR , Charles University in Prague (Bojar et al., 2013) CU - DEPFIX , CU - TAMCHYNA CU - KAREL , CU - ZEMAN CU - PHRASEFIX , Charles University in Prague (B´ılek and Zeman, 2013) Charles University in Prague (Galuˇscˇ a´ kov´a et al., 2013) CU - TECTOMT DCU DCU - FDA DCU - OKITA DESRT ITS - LATL JHU KIT LIA LIMSI MES -* OMNIFLUENT PROMT QCRI - MES QUAERO RWTH SHEF STANFORD TALP - UPC TUBITAK UCAM UEDIN , Dublin City University (Rubino et al., 2013a) Dublin City University (Bicici, 2013a) Dublin City University (Okita et al., 2013) Universit`a di Pisa (Miceli Barone and Attardi, 2013) University of Geneva Johns Hopkins University (Post et al., 2013) Karlsruhe Institute of Technology (Cho et al., 2013) Universit´e d’Avignon (Huet et al., 2013) LIMSI (Allauzen et al., 2013) Munich / Edinburgh / Stuttgart (Durrani et al., 2013a; Weller et al., 2013) SAIC (Matusov and Leusch, 2013) PROMT Automated Translations Solutions Qatar / Munich / Edinburgh / Stuttgart (Sajjad et al., 2013) QUAERO (Peitz et al., 2013a) RWTH Aachen (Peitz et al., 2013b) University of Sheffield Stanford Univ"
W13-2201,W13-2240,0,0.0186069,"Missing"
W13-2201,W13-2242,0,0.220429,"ool of Data Analysis (Borisov et al., 2013) Carnegie Mellon University (Ammar et al., 2013) CMU - TREE - TO - TREE CU - BOJAR , Charles University in Prague (Bojar et al., 2013) CU - DEPFIX , CU - TAMCHYNA CU - KAREL , CU - ZEMAN CU - PHRASEFIX , Charles University in Prague (B´ılek and Zeman, 2013) Charles University in Prague (Galuˇscˇ a´ kov´a et al., 2013) CU - TECTOMT DCU DCU - FDA DCU - OKITA DESRT ITS - LATL JHU KIT LIA LIMSI MES -* OMNIFLUENT PROMT QCRI - MES QUAERO RWTH SHEF STANFORD TALP - UPC TUBITAK UCAM UEDIN , Dublin City University (Rubino et al., 2013a) Dublin City University (Bicici, 2013a) Dublin City University (Okita et al., 2013) Universit`a di Pisa (Miceli Barone and Attardi, 2013) University of Geneva Johns Hopkins University (Post et al., 2013) Karlsruhe Institute of Technology (Cho et al., 2013) Universit´e d’Avignon (Huet et al., 2013) LIMSI (Allauzen et al., 2013) Munich / Edinburgh / Stuttgart (Durrani et al., 2013a; Weller et al., 2013) SAIC (Matusov and Leusch, 2013) PROMT Automated Translations Solutions Qatar / Munich / Edinburgh / Stuttgart (Sajjad et al., 2013) QUAERO (Peitz et al., 2013a) RWTH Aachen (Peitz et al., 2013b) University of Sheffield Stanford Univ"
W13-2201,W13-2207,0,0.0357036,"Missing"
W13-2201,W11-2104,0,0.00810596,"Missing"
W13-2201,P10-2016,1,0.826191,"Missing"
W13-2201,W13-2208,1,0.743011,"Missing"
W13-2201,P11-1022,0,0.112625,"the intended application. In Task 2 we tested binary word-level classification in a post-editing setting. If such annotation is presented through a user interface we imagine that words marked as incorrect would be hidden from the editor, highlighted as possibly wrong or that a list of alternatives would we generated. With respect to the poor improvements over trivial baselines, we consider that the results for word-level prediction could be mostly connected to limitations of the datasets provided, which are very small for word-level prediction, as compared to successful previous work such as (Bach et al., 2011). Despite the limited amount of training data, several systems were able to predict dubious words (binary variant of the task), showing that this can be a promising task. Extending the granularity even further by predicting the actual editing action necessary for a word yielded less positive results than the binary setting. We cannot directly compare sentence- and word-level results. However, since sentence-level predictions can benefit from more information available and therefore more signal on which the prediction is based, the natural conclusion is that, if there is a choice in the predict"
W13-2201,W13-2209,0,0.034542,"Missing"
W13-2201,W13-2241,1,0.0862471,"Missing"
W13-2201,W07-0718,1,0.697635,"to five alternative translations for a given source sentence, prediction of post-editing time for a sentence, and prediction of word-level scores for a given translation (correct/incorrect and types of edits). The datasets included English-Spanish and GermanEnglish news translations produced by a number of machine translation systems. This marks the second year we have conducted this task. Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at ACL 2013. This workshop builds on seven previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012). This year we conducted three official tasks: a translation task, a human evaluation of translation results, and a quality estimation task.1 In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Spanish, and Russian. The Russian translation tasks were new this year, and were also the most popular. The system outputs for each task were evaluated both automatically and manually."
W13-2201,W11-2103,1,0.5768,"Missing"
W13-2201,W13-2243,1,0.754887,"Missing"
W13-2201,W13-2213,0,0.0301468,"Missing"
W13-2201,P05-1022,0,0.0401846,"parses of the source and target sentence, the positions of the phrases with the lowest and highest probability and future cost estimate in the translation, the counts of phrases in the decoding graph whose probability or whether the future cost estimate is higher/lower than their standard deviation, counts of verbs and determiners, etc. The second submission (pls8) was trained with Partial Least Squares regression (Stone and Brooks, 1990) including more glass-box features. The following NLP tools were used in feature extraction: the Brown English Wall-StreetJournal-trained statistical parser (Charniak and Johnson, 2005), a Lexical Functional Grammar parser (XLE), together with a hand-crafted Lexical Functional Grammar, the English ParGram grammar (Kaplan et al., 2004), and the TreeTagger part-of-speech tagger (Schmidt, 1994) with off-the-shelf publicly available pre-trained tagging models for English and Spanish. For pseudoreference features, the Bing, Moses and Systran translation systems were used. The Mallet toolkit (McCallum, 2002) was used to build the topic models and features based on a grammar checker were extracted with LanguageTool.16 FBK-Uedin (T1.1, T1.3): The submissions explored features built"
W13-2201,W13-2210,0,0.0366902,"Missing"
W13-2201,W13-2212,1,0.1814,"Missing"
W13-2201,W13-2214,0,0.0302242,"Missing"
W13-2201,W13-2217,0,0.0313645,"Missing"
W13-2201,W13-2215,0,0.0174795,"Missing"
W13-2201,W13-2253,0,0.055006,"Missing"
W13-2201,W13-2246,0,0.0627442,"Missing"
W13-2201,N06-1058,0,0.0556187,"of the official test set size, and provide 4311 unique references. On average, one sentence in our set has 2.94±2.17 unique reference translations. Table 10 provides a histogram. It is well known that automatic MT evaluation methods perform better with more references, because a single one may not confirm a correct part of MT output. This issue is more severe for morphologically rich languages like Czech where about 1/3 of MT output was correct but not confirmed by the reference (Bojar et al., 2010). Advanced evaluation methods apply paraphrasing to smooth out some of the lexical divergence (Kauchak and Barzilay, 2006; Snover et al., 2009; Denkowski and Lavie, 2010). Simpler techniques such as lemmatizing are effective for morphologically rich languages (Tantug et al., 2008; Kos and Bojar, 2009) but they will lose resolution once the systems start performing generally well. WMTs have taken the stance that a big enough test set with just a single reference should compensate for the lack of other references. We use our post-edited reference translations to check this assumption for BLEU and NIST as implemented in mteval-13a (international tokenization switched on, which is not the default setting). We run ma"
W13-2201,W13-2248,0,0.0488662,"Missing"
W13-2201,2012.iwslt-papers.5,1,0.674636,"ts are somewhat artificially more diverse; in narrow domains, source sentences can repeat and even appear verbatim in the training data, and in natural test sets with multiple references, short sentences can receive several identical translations. For each probe, we measure the Spearman’s rank correlation coefficient ρ of the ranks proposed by BLEU or NIST and the manual ranks. We use the same implementation as applied in the WMT13 Shared Metrics Task (Mach´acˇ ek and Bojar, 2013). Note that the WMT13 metrics task still uses the WMT12 evaluation method ignoring ties, not the expected wins. As Koehn (2012) shows, the two methods do not differ much. Overall, the correlation is strongly impacted by Figure 5: Correlation of BLEU and WMT13 manual ranks for English→Czech translation Figure 6: Correlation of NIST and WMT13 manual ranks for English→Czech translation the particular choice of test sentences and reference translations. By picking sentences randomly, similarly or equally sized test sets can reach different correlations. Indeed, e.g. for a test set of about 1500 distinct sentences selected from the 3000-sentence official test set (1 reference translation), we obtain correlations for BLEU b"
W13-2201,W13-2202,1,0.761984,"Missing"
W13-2201,W06-3114,1,0.635019,"ntence, ranking of up to five alternative translations for a given source sentence, prediction of post-editing time for a sentence, and prediction of word-level scores for a given translation (correct/incorrect and types of edits). The datasets included English-Spanish and GermanEnglish news translations produced by a number of machine translation systems. This marks the second year we have conducted this task. Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at ACL 2013. This workshop builds on seven previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012). This year we conducted three official tasks: a translation task, a human evaluation of translation results, and a quality estimation task.1 In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Spanish, and Russian. The Russian translation tasks were new this year, and were also the most popular. The system outputs for each task were evaluated bot"
W13-2201,W13-2219,0,0.0360343,"Missing"
W13-2201,W13-2220,0,0.017834,"Missing"
W13-2201,W13-2255,0,0.0136612,"Missing"
W13-2201,W12-3113,0,0.012942,"Missing"
W13-2201,W13-2247,0,0.0208658,"Missing"
W13-2201,W13-2221,1,0.762444,"Missing"
W13-2201,W13-2228,0,0.0270572,"Missing"
W13-2201,W13-2224,0,0.0611782,"Missing"
W13-2201,2013.mtsummit-papers.21,1,0.577491,"ve learning to reduce the training set size (and therefore the annotation effort). The initial set features contains all black box and glass box features available within the Q U E ST framework (Specia et al., 2013) for the dataset at hand (160 in total for Task 1.1, and 80 for Task 1.3). The query selection strategy for active learning is based on the informativeness of the instances using Information Density, a measure that leverages between the variance among instances and how dense the region (in the feature space) where the instance is located is. To perform feature selection, following (Shah et al., 2013) features are ranked by the Gaussian Process 23 algorithm according to their learned length scales, which can be interpreted as the relevance of such feature for the model. This information was used for feature selection by discarding the lowest ranked (least useful) ones. based on empirical results found in (Shah et al., 2013), the top 25 features for both models were selected and used to retrain the same regression algorithm. Semantic Roles could bring marginally better accuracy. TCD-CNGL (T1.1) and TCD-DCU-CNGL (T1.3): The system is based on features which are commonly used for style classi"
W13-2201,2012.amta-papers.13,0,0.0208135,"Missing"
W13-2201,W13-2250,0,0.0320711,"Missing"
W13-2201,W13-2225,0,0.0376682,"Missing"
W13-2201,P13-1135,1,0.207781,"Missing"
W13-2201,W13-2226,1,0.759686,"Missing"
W13-2201,2006.amta-papers.25,0,0.295541,"ation Based on the data of Task 1.3, we define Task 2, a word-level annotation task for which participants are asked to produce a label for each token that indicates whether the word should be changed by a post-editor or kept in the final translation. We consider the following two sets of labels for prediction: Sentence-level Quality Estimation Task 1.1 Predicting Post-editing Distance This task is similar to the quality estimation task in WMT12, but with one important difference in the scoring variant: instead of using the post-editing effort scores in the [1, 2, 3, 4, 5] range, we use HTER (Snover et al., 2006) as quality score. This score is to be interpreted as the minimum edit distance between the machine translation and its manually post-edited version, and its range is [0, 1] (0 when no edit needs to be made, and 1 when all words need to be edited). Two variants of the results could be submitted in the shared task: • Binary classification: a keep/change label, the latter meaning that the token should be corrected in the post-editing process. • Multi-class classification: a label specifying the edit action that should be performed on the token (keep as is, delete, or substitute). 6.3 Datasets Ta"
W13-2201,W13-2227,0,0.0389834,"Missing"
W13-2201,W09-0441,0,0.0271208,"ze, and provide 4311 unique references. On average, one sentence in our set has 2.94±2.17 unique reference translations. Table 10 provides a histogram. It is well known that automatic MT evaluation methods perform better with more references, because a single one may not confirm a correct part of MT output. This issue is more severe for morphologically rich languages like Czech where about 1/3 of MT output was correct but not confirmed by the reference (Bojar et al., 2010). Advanced evaluation methods apply paraphrasing to smooth out some of the lexical divergence (Kauchak and Barzilay, 2006; Snover et al., 2009; Denkowski and Lavie, 2010). Simpler techniques such as lemmatizing are effective for morphologically rich languages (Tantug et al., 2008; Kos and Bojar, 2009) but they will lose resolution once the systems start performing generally well. WMTs have taken the stance that a big enough test set with just a single reference should compensate for the lack of other references. We use our post-edited reference translations to check this assumption for BLEU and NIST as implemented in mteval-13a (international tokenization switched on, which is not the default setting). We run many probes, randomly p"
W13-2201,W13-2230,0,0.0166145,"Missing"
W13-2201,W12-3118,1,0.609118,"1.1, T1.3): The submissions explored features built on MT engine resources including automatic word alignment, n-best candidate translation lists, back-translations and word posterior probabilities. Information about word alignments is used to extract quantitative (amount and distribution of the alignments) and qualitative (importance of the aligned terms) features under the assumption that alignment information can help tasks where sentencelevel semantic relations need to be identified (Souza et al., 2013). Three similar EnglishSpanish systems are built and used to provide pseudo-references (Soricut et al., 2012) and back-translations, from which automatic MT evaluation metrics could be computed and used as features. DFKI (T1.2, T1.3): DFKI’s submission for Task 1.2 was based on decomposing rankings into pairs (Avramidis, 2012), where the best system for each pair was predicted with Logistic Regression (LogReg). For GermanEnglish, LogReg was trained with Stepwise Feature Selection (Hosmer, 1989) on two feature sets: Feature Set 24 includes basic counts augmented with PCFG parsing features (number of VPs, alternative parses, parse probability) on both source and target sentences (Avramidis et al., 2011"
W13-2201,P13-2135,0,0.148019,"the difference between selecting the best or the worse translation. 19 ID CMU CNGL DCU DCU-SYMC DFKI FBK-UEdin LIG LIMSI LORIA SHEF TCD-CNGL TCD-DCU-CNGL UMAC UPC Participating team Carnegie Mellon University, USA (Hildebrand and Vogel, 2013) Centre for Next Generation Localization, Ireland (Bicici, 2013b) Dublin City University, Ireland (Almaghout and Specia, 2013) Dublin City University & Symantec, Ireland (Rubino et al., 2013b) German Research Centre for Artificial Intelligence, Germany (Avramidis and Popovic, 2013) Fondazione Bruno Kessler, Italy & University of Edinburgh, UK (Camargo de Souza et al., 2013) Laboratoire d’Informatique Grenoble, France (Luong et al., 2013) Laboratoire d’Informatique pour la M´ecanique et les Sciences de l’Ing´enieur, France (Singh et al., 2013) Lorraine Laboratory of Research in Computer Science and its Applications, France (Langlois and Smaili, 2013) University of Sheffield, UK (Beck et al., 2013) Trinity College Dublin & CNGL, Ireland (Moreau and Rubino, 2013) Trinity College Dublin, Dublin City University & CNGL, Ireland (Moreau and Rubino, 2013) University of Macau, China (Han et al., 2013) Universitat Politecnica de Catalunya, Spain (Formiga et al., 2013b) Ta"
W13-2201,2011.eamt-1.12,1,0.741789,"ar (CCG) features: CCG supertag language model perplexity and log probability, the number of maximal CCG constituents in the translation output which are the highestprobability minimum number of CCG constituents that span the translation output, the percentage of CCG argument mismatches between each subsequent CCG supertags, the percentage of CCG argument mismatches between each subsequent CCG maximal categories and the minimum number of phrases detected in the translation output. A second submission uses the aforementioned CCG features combined with 80 features from Q U E ST as described in (Specia, 2011). For the CCG features, the C&C parser was used to parse the translation output. Moses was used to build the phrase table from the SMT training corpus with maximum phrase length set to 7. The language model of supertags was built using the SRILM toolkit. As learning algorithm, Logistic Regression as provided by the SCIKIT- LEARN toolkit was used. The training data was prepared by converting each ranking of translation outputs to a set of pairwise comparisons according to the approach proposed by Avramidis et al. (2011). The rankings were generated back from pairwise comparisons predicted by th"
W13-2201,P13-4014,1,0.118757,"Missing"
W13-2201,W13-2229,0,0.0371208,"Missing"
W13-2201,tantug-etal-2008-bleu,0,0.0131442,"a histogram. It is well known that automatic MT evaluation methods perform better with more references, because a single one may not confirm a correct part of MT output. This issue is more severe for morphologically rich languages like Czech where about 1/3 of MT output was correct but not confirmed by the reference (Bojar et al., 2010). Advanced evaluation methods apply paraphrasing to smooth out some of the lexical divergence (Kauchak and Barzilay, 2006; Snover et al., 2009; Denkowski and Lavie, 2010). Simpler techniques such as lemmatizing are effective for morphologically rich languages (Tantug et al., 2008; Kos and Bojar, 2009) but they will lose resolution once the systems start performing generally well. WMTs have taken the stance that a big enough test set with just a single reference should compensate for the lack of other references. We use our post-edited reference translations to check this assumption for BLEU and NIST as implemented in mteval-13a (international tokenization switched on, which is not the default setting). We run many probes, randomly picking the test set size (number of distinct sentences) and the number of distinct references per sentence. Note that such test sets are s"
W13-2201,W10-1751,0,\N,Missing
W13-2201,de-marneffe-etal-2006-generating,0,\N,Missing
W13-2201,W13-2249,0,\N,Missing
W13-2201,N04-1013,0,\N,Missing
W13-2201,W02-1001,0,\N,Missing
W13-2201,W12-3102,1,\N,Missing
W13-2201,P12-3024,0,\N,Missing
W13-2201,W09-0401,1,\N,Missing
W13-2201,W13-2222,0,\N,Missing
W13-2201,W10-1711,0,\N,Missing
W13-2201,2010.iwslt-evaluation.22,0,\N,Missing
W13-2201,W10-1703,1,\N,Missing
W13-2201,W08-0309,1,\N,Missing
W13-2201,W13-2218,0,\N,Missing
W13-2201,P13-1004,1,\N,Missing
W13-2201,2013.mtsummit-papers.9,0,\N,Missing
W13-2201,W13-2216,1,\N,Missing
W13-2201,W13-2244,0,\N,Missing
W13-2203,W12-4204,0,0.54368,"valuate machine translation is not new. Gim´enez and M`arquez (2007) proposed using automatically assigned semantic role labels as a feature in a combined MT metric. The main difference between this application of semantic roles and MEANT is that arguments for specific verbs are taken into account, instead of just applying the subset agent, patient and benefactor. This idea would probably help human annotators to handle sentences with passives, copulas and other constructions which do not easily match the most basic arguments. On the other hand, verb specific arguments are language dependent. Bojar and Wu (2012), applying HMEANT to English-to-Czech MT output, identified a number of problems with HMEANT, and suggested a variety of improvements. In some respects, this work is very similar, except that our goal is to evaluate HMEANT along a range of intrinsic properties, to determine how useful the metric really is to evaluation campaigns such as the workshop on machine translation. 3 Evaluation with HMEANT 3.1 Annotation Procedure The goal of the HMEANT metric is to capture essential semantic content, but still be simple and fast. There are two stages to the annotation, the first of which is semantic r"
W13-2203,W07-0718,1,0.869138,"ion, there is still no consensus on how to evaluate machine translation based on human judgements. (Hutchins and Somers, 1992; Przybocki et al., 2009). One obvious approach is to ask annotators to rate translation candidates on a numerical scale. Under the DARPA TIDES program, the Linguistic Data Consortium (2002) developed an evaluation scheme that relies on two five-point scales representing fluency and adequacy. This was also the human evaluation scheme used in the annual MT competitions sponsored by NIST (2005). In an analysis of human evaluation results for the WMT ’07 workshop, however, Callison-Burch et al. (2007) found high correlation between fluency and adequacy scores assigned by individual annotators, suggesting that human annotators are not able to separate these two evaluation dimensions easily. Furthermore these absolute scores show low inter-annotator agreement. Instead of giving absolute quality assessments, annotators appeared to be using their ratings to rank translation candidates according to their overall preference for one over the other. In line with these findings, Callison-Burch et al. (2007) proposed to let annotators rank translation candidates directly, without asking them to assi"
W13-2203,P11-1023,0,0.125845,"on Street Edinburgh, EH8 9AB, UK Abstract tion might be useful for different purposes. If the MT is going to be the basis of a human translator’s work-flow, then post-editing effort seems like a natural fit. However, for people using MT for gisting, what we really want is some measure of how much meaning has been retained. We clearly need a metric which tries to answer the question, how much of the meaning does the translation capture. In this paper, we explore the use of human evaluation metrics which attempt to capture the extent of this meaning retention. In particular, we consider HMEANT (Lo and Wu, 2011a), a metric that uses semantic role labels to measure how much of the “who, why, when, where” has been preserved. For HMEANT evaluation, annotators are instructed to identify verbs as heads of semantic frames. Then they attach role fillers to the heads and finally they align heads and role fillers in the candidate translation with those in a reference translation. In a series of papers, Lo and Wu (2010, 2011b,a, 2012) explored a number of questions, evaluating HMEANT by using correlation statistics to compare it to judgements of human adequacy and contrastive evaluations. Given the drawbacks"
W13-2203,lo-wu-2010-evaluating,0,0.0158008,"ing does the translation capture. In this paper, we explore the use of human evaluation metrics which attempt to capture the extent of this meaning retention. In particular, we consider HMEANT (Lo and Wu, 2011a), a metric that uses semantic role labels to measure how much of the “who, why, when, where” has been preserved. For HMEANT evaluation, annotators are instructed to identify verbs as heads of semantic frames. Then they attach role fillers to the heads and finally they align heads and role fillers in the candidate translation with those in a reference translation. In a series of papers, Lo and Wu (2010, 2011b,a, 2012) explored a number of questions, evaluating HMEANT by using correlation statistics to compare it to judgements of human adequacy and contrastive evaluations. Given the drawbacks of those evaluation measures, which we discuss in Sec. 2, they could just as well have been evaluating the human adequacy and contrastive judgements using HMEANT. Human evaluation metrics need to be judged on other intrinsic qualities, which we describe below. The aim of this paper is to evaluate the effectiveness of HMEANT, with the goal of using it to judge the relative merits of different MT systems,"
W13-2203,W11-1002,0,0.330392,"on Street Edinburgh, EH8 9AB, UK Abstract tion might be useful for different purposes. If the MT is going to be the basis of a human translator’s work-flow, then post-editing effort seems like a natural fit. However, for people using MT for gisting, what we really want is some measure of how much meaning has been retained. We clearly need a metric which tries to answer the question, how much of the meaning does the translation capture. In this paper, we explore the use of human evaluation metrics which attempt to capture the extent of this meaning retention. In particular, we consider HMEANT (Lo and Wu, 2011a), a metric that uses semantic role labels to measure how much of the “who, why, when, where” has been preserved. For HMEANT evaluation, annotators are instructed to identify verbs as heads of semantic frames. Then they attach role fillers to the heads and finally they align heads and role fillers in the candidate translation with those in a reference translation. In a series of papers, Lo and Wu (2010, 2011b,a, 2012) explored a number of questions, evaluating HMEANT by using correlation statistics to compare it to judgements of human adequacy and contrastive evaluations. Given the drawbacks"
W13-2203,N12-1017,0,0.0331991,"eded by a human to convert the machine translation so as to convey the same meaning as the reference. This type of evaluation is of some use when one is using MT to aid human translation (although the relationship between number of edits and actual effort is not straightforward (Koponen, 2012)), but it is not so helpful when one’s task is gisting. The number of edits need not correlate with the severity of the semantic differences between the two sentences. The loss of a negative, for instance, is only one edit away from the original, but the semantics change completely. Alternatively, HyTER (Dreyer and Marcu, 2012) is an annotation tool which allows a user to create an exponential number of correct translations for a given sentence. These references are then efficiently exploited to compare with machine translation output. The authors argue that the current metrics fail simply because they have access to sets of reference translations which are simply too small. However, the fact is that even if one does have access to large numbers of translations, it is very difficult to determine whether the reference correctly captures the essential semantic content of the references. The idea of using semantic role"
W13-2203,W12-4206,0,0.0163075,"evaluation, as it is essential for building better automatic metrics, and therefore a more fundamental problem. The overall HMEANT score for MT evaluation is computed as the f-score from the counts of matches of frames and their role fillers between the reference and the MT output. Unmatched frames are excluded from the calculation together with all their corresponding roles. In recognition that preservation of some types of semantic relations may be more important than others for a human to understand a sentence, one may want to weight them differently in the computation of the HMEANT score. Lo and Wu (2012) train weights for each role filler type to optimise correlation with human adequacy judgements. As an unsupervised alternative, they suggest weighting roles according to their frequency as approximation to their importance. Since the main focus of the current paper is the annotation of the actions, roles and alignments that HMEANT depends on, we do not explore such different weight-setting schemes, but set the weights uniformly, with the exception of a partial alignment, which is given a weight of 0.5. HMEANT is thus defined as follows: 4 4.1 Systems and Data Sets We performed HMEANT evaluati"
W13-2203,P08-4006,1,0.829538,"espective sentence. They were then presented with the output of several machine translation systems for the same source sentence, one system at a time, with the reference translation and its annotations visible in the left half of the screen (cf. Fig. 1). For each system, the annotators were asked to annotate semantic frames and slot fillers in the translation first, and then align them with frame heads and slot fillers in the human reference translation. Annotations and alignment were performed with Edi-HMEANT2 , a web-based annotation tool for HMEANT that we developed on the basis of Yawat (Germann, 2008). The tool allows the alignment of slots from different semantic frames, and the alignment of slots of different types; however, such alignments are not considered in the computation of the final HMEANT score. The annotation guidelines were essentially those used in Bojar and Wu (2012), with some additional English examples, and a complete set of German examples. For ease of comparison with prior work, we used the same set of semantic role labels as Bojar and Wu (2012), shown in Table 1. Given the restriction that the head of a frame can consist of only one word, a convention was made that all"
W13-2203,W12-3101,0,0.0132441,"02), METEOR (Lavie and Denkowski, 2009) and TER (Snover et al., 2009b), which have greatly accelerated progress in MT research, rely on shallow surface properties of the translations, and only indirectly capture whether or not the translation preserves the meaning. This has meant that 53 sent a subset of the competing systems, and these rankings must be combined with other annotators judgements on five other system outputs to compute an overall ranking. The methodology for interpreting the contrastive evaluations has been the subject of much recent debate in the community (Bojar et al., 2011; Lopez, 2012). There has been some effort to overcome these problems. HTER (Snover et al., 2009a) is a metric which counts the number of edits needed by a human to convert the machine translation so as to convey the same meaning as the reference. This type of evaluation is of some use when one is using MT to aid human translation (although the relationship between number of edits and actual effort is not straightforward (Koponen, 2012)), but it is not so helpful when one’s task is gisting. The number of edits need not correlate with the severity of the semantic differences between the two sentences. The lo"
W13-2203,J05-1004,0,0.0443523,"s used for the WMT shared task, it is still reasonably efficient considering the fine-grained nature of the evaluation. On average, annotators evaluated about 10 sentences per hour. 2 Related Work Even though the idea that machine translation requires a semantic representation of the translated content is as old as the idea of computer-based translation itself (Weaver, 1955), it has not been until recently that people have begun to combine statistical models with semantic representations. Jones et al. (2012), for example, represent meaning as directed acyclic graphs and map these to PropBank (Palmer et al., 2005) style dependencies. To evaluate such approaches properly, we need evaluation metrics that capture the accuracy of the translation. Current automatic metrics of machine translation, such as BLEU (Papineni et al., 2002), METEOR (Lavie and Denkowski, 2009) and TER (Snover et al., 2009b), which have greatly accelerated progress in MT research, rely on shallow surface properties of the translations, and only indirectly capture whether or not the translation preserves the meaning. This has meant that 53 sent a subset of the competing systems, and these rankings must be combined with other annotator"
W13-2203,C12-1083,0,0.0311324,"uage. Efficiency Whilst HMEANT evaluation will never be as fast as, for example, the contrastive judgements used for the WMT shared task, it is still reasonably efficient considering the fine-grained nature of the evaluation. On average, annotators evaluated about 10 sentences per hour. 2 Related Work Even though the idea that machine translation requires a semantic representation of the translated content is as old as the idea of computer-based translation itself (Weaver, 1955), it has not been until recently that people have begun to combine statistical models with semantic representations. Jones et al. (2012), for example, represent meaning as directed acyclic graphs and map these to PropBank (Palmer et al., 2005) style dependencies. To evaluate such approaches properly, we need evaluation metrics that capture the accuracy of the translation. Current automatic metrics of machine translation, such as BLEU (Papineni et al., 2002), METEOR (Lavie and Denkowski, 2009) and TER (Snover et al., 2009b), which have greatly accelerated progress in MT research, rely on shallow surface properties of the translations, and only indirectly capture whether or not the translation preserves the meaning. This has mea"
W13-2203,P02-1040,0,0.101424,"a that machine translation requires a semantic representation of the translated content is as old as the idea of computer-based translation itself (Weaver, 1955), it has not been until recently that people have begun to combine statistical models with semantic representations. Jones et al. (2012), for example, represent meaning as directed acyclic graphs and map these to PropBank (Palmer et al., 2005) style dependencies. To evaluate such approaches properly, we need evaluation metrics that capture the accuracy of the translation. Current automatic metrics of machine translation, such as BLEU (Papineni et al., 2002), METEOR (Lavie and Denkowski, 2009) and TER (Snover et al., 2009b), which have greatly accelerated progress in MT research, rely on shallow surface properties of the translations, and only indirectly capture whether or not the translation preserves the meaning. This has meant that 53 sent a subset of the competing systems, and these rankings must be combined with other annotators judgements on five other system outputs to compute an overall ranking. The methodology for interpreting the contrastive evaluations has been the subject of much recent debate in the community (Bojar et al., 2011; Lop"
W13-2203,W12-3123,0,0.0290208,"ts to compute an overall ranking. The methodology for interpreting the contrastive evaluations has been the subject of much recent debate in the community (Bojar et al., 2011; Lopez, 2012). There has been some effort to overcome these problems. HTER (Snover et al., 2009a) is a metric which counts the number of edits needed by a human to convert the machine translation so as to convey the same meaning as the reference. This type of evaluation is of some use when one is using MT to aid human translation (although the relationship between number of edits and actual effort is not straightforward (Koponen, 2012)), but it is not so helpful when one’s task is gisting. The number of edits need not correlate with the severity of the semantic differences between the two sentences. The loss of a negative, for instance, is only one edit away from the original, but the semantics change completely. Alternatively, HyTER (Dreyer and Marcu, 2012) is an annotation tool which allows a user to create an exponential number of correct translations for a given sentence. These references are then efficiently exploited to compare with machine translation output. The authors argue that the current metrics fail simply bec"
W13-2203,W09-0441,0,0.049194,"translated content is as old as the idea of computer-based translation itself (Weaver, 1955), it has not been until recently that people have begun to combine statistical models with semantic representations. Jones et al. (2012), for example, represent meaning as directed acyclic graphs and map these to PropBank (Palmer et al., 2005) style dependencies. To evaluate such approaches properly, we need evaluation metrics that capture the accuracy of the translation. Current automatic metrics of machine translation, such as BLEU (Papineni et al., 2002), METEOR (Lavie and Denkowski, 2009) and TER (Snover et al., 2009b), which have greatly accelerated progress in MT research, rely on shallow surface properties of the translations, and only indirectly capture whether or not the translation preserves the meaning. This has meant that 53 sent a subset of the competing systems, and these rankings must be combined with other annotators judgements on five other system outputs to compute an overall ranking. The methodology for interpreting the contrastive evaluations has been the subject of much recent debate in the community (Bojar et al., 2011; Lopez, 2012). There has been some effort to overcome these problems."
W13-2203,Y12-1062,0,0.0303318,"Missing"
W13-2203,W07-0738,0,\N,Missing
W13-2203,W12-3129,0,\N,Missing
W13-2203,P13-1023,0,\N,Missing
W13-2203,W11-2101,0,\N,Missing
W13-2212,D07-1090,0,0.0383023,"4.02 30.04 22.70 25.70 31.87 24.00 17.95 20.06 28.76 30.03 33.87 29.66 15.81 18.35 23.75 18.44 The large language model was then quantized to 10 bits and compressed to 643 GB with KenLM (Heafield, 2011), loaded onto a machine with 1 TB RAM, and used as an additional feature in unconstrained French–English, Spanish–English, and Czech–English submissions. This additional language model is the only difference between our final constrained and unconstrained submissions; no additional parallel data was used. Results are shown in Table 18. Improvement from large language models is not a new result (Brants et al., 2007); the primary contribution is estimating on a single machine. +OSM 2012 2013 24.11 +.26 26.83 +.29 30.96 +.19 31.46 +.37 34.51 +.49 30.94 +.90 23.03 +.33 25.79 +.09 32.33 +.46 24.33 +.33 18.02 +.07 20.26 +.20 29.36 +.60 30.39 +.36 34.44 +.57 30.10 +.44 16.16 +.35 18.62 +.27 24.05 +.30 18.84 +.40 Constrained Unconstrained ∆ fr-en 31.46 32.24 +.78 es-en 30.59 31.37 +.78 cs-en 27.38 28.16 +.78 24.33 25.14 +.81 ru-en Table 18: Gain on newstest2013 from the unconstrained language model. Our time on shared machines with 1 TB is limited so Russian–English was run after the deadline and German–English"
W13-2212,W12-3102,1,0.0720198,"ut we did not use the factored decomposition of translation options into multiple mapping steps, since this usually lead to much slower systems with usually worse results. A good place, however, for factored decomposition is the handling of rare and unknown source words which have more frequent morphological variants (Koehn and Haddow, 2012a). Here, we used only factored backoff for unknown words, giving gains in BLEU of +.12 for German–English. Initial System Development We start with systems (Haddow and Koehn, 2012) that we developed for the 2012 Workshop on Statistical Machine Translation (Callison-Burch et al., 2012). The notable features of these systems are: • Moses phrase-based models with mostly default settings • training on all available parallel data, including the large UN parallel data, the FrenchEnglish 109 parallel data and the LDC Gigaword data 1.2 Tuning with k-best MIRA In preparation for training with sparse features, we moved away from MERT which is known to fall 114 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 114–121, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics apart with many more than a couple of dozen features. Ins"
W13-2212,N12-1047,0,0.0890278,"hese systems are: • Moses phrase-based models with mostly default settings • training on all available parallel data, including the large UN parallel data, the FrenchEnglish 109 parallel data and the LDC Gigaword data 1.2 Tuning with k-best MIRA In preparation for training with sparse features, we moved away from MERT which is known to fall 114 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 114–121, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics apart with many more than a couple of dozen features. Instead, we used k-best MIRA (Cherry and Foster, 2012). For the different language pairs, we saw improvements in BLEU of -.05 to +.39, with an average of +.09. There was only a minimal change in the length ratio (Table 1) de-en fr-en es-en cs-en en-de en-fr en-es en-cs avg MERT 22.11 (1.010) 30.00 (1.023) 30.42 (1.021) 25.54 (1.022) 16.08 (0.995) 29.26 (0.980) 31.92 (0.985) 17.38 (0.967) – k-best MIRA 22.10 (1.008) 30.11 (1.026) 30.63 (1.020) 25.49 (1.024) 16.04 (1.001) 29.65 (0.982) 31.95 (0.985) 17.42 (0.974) – The lexical features were restricted to the 50 most frequent words. All these features together only gave minor improvements (Table 3)."
W13-2212,N09-1025,0,0.025899,"f domain features: Translation Table Smoothing with Kneser-Ney Discounting de-en fr-en es-en cs-en en-de en-fr en-es en-cs avg sparse 22.02 30.24 30.61 25.49 15.93 29.81 32.02 17.28 – Table 3: Sparse features Table 1: Tuning with k-best MIRA instead of MERT (cased BLEU scores with length ratio) 1.3 baseline 22.10 30.11 30.63 25.49 16.04 29.65 31.95 17.42 – Sparse Features A significant extension of the Moses system over the last couple of years was the support for large numbers of sparse features. This year, we tested this capability on our big WMT systems. First, we used features proposed by Chiang et al. (2009): #d base. 2 22.10 4 30.11 3 30.63 9 25.49 2 16.122 4 29.65 3 31.95 9 17.42 – indicator 22.14 +.04 30.34 +.23 30.88 +.25 25.58 +.09 16.14 +.02 29.75 +.10 32.06 +.11 17.45 +.03 +.11 ratio 22.07 –.03 30.29 +.18 30.64 +.01 25.58 +.09 15.96 –.16 29.71 +.05 32.13 +.18 17.35 –.07 +.03 subset 22.12 +.02 30.15 +.04 30.82 +.19 25.46 –.03 16.01 –.11 29.70 +.05 32.02 +.07 17.44 +.02 +.03 Table 4: Sparse domain features When combining the domain features and the other sparse features, we see roughly additive gains (Table 5). We use the domain indicator feature and the other sparse features in subsequent e"
W13-2212,P05-1066,1,0.569778,"Missing"
W13-2212,P11-1105,1,0.298607,"us (translation and reordering) decisions spanning across phrasal boundaries thus overcoming the problematic phrasal independence assumption in the phrase-based model. In the OSM model, the reordering decisions influence lexical selection and vice versa. Lexical generation is strongly coupled with reordering thus improving the overall reordering mechanism. We used the modified version of the OSM model (Durrani et al., 2013b) that additionally handles discontinuous and unaligned target MTUs3 . We borrow 4 count-based supportive features, the Gap, Open Gap, Gap-width and Deletion penalties from Durrani et al. (2011). Inst. Wt (scale) – 33.98 ±.00 23.13 –.06 31.62 –.05 28.63 –.04 34.03 +.03 15.89 +.11 23.72 –.06 Table 14: Comparison of MML filtering and weighting with baseline. The MML uses monolingual news as in-domain, and selects from all training data after alignment.The weighting uses the MML weights, optionally downscaled by 10, then exponentiated. Baselines are as Table 13. Training: During training, each bilingual sentence pair is deterministically converted to a unique sequence of operations. Please refer to Durrani et al. (2011) for a list of operations and the conversion algorithm and see Figur"
W13-2212,D10-1044,0,0.0468666,"Training with new data (newstest2012 scores) 2 We adopted a number of changes that improved our baseline system by an average of +.30, see Table 10 for a breakdown. Domain Adaptation Techniques We explored two additional domain adaptation techniques: phrase table interpolation and modified Moore-Lewis filtering. method factored backoff kbest MIRA sparse features and domain indicator tuning with 25 iterations maximum phrase length 5 unpruned 4-gram LM translation table limit 100 total 2.1 Phrase Table Interpolation We experimented with phrase-table interpolation using perplexity minimisation (Foster et al., 2010; Sennrich, 2012). In particular, we used the implementation released with Sennrich (2012) and available in Moses, comparing both the naive and modified interpolation methods from that paper. For each language pair, we took the alignments created from all the data concatenated, built separate phrase tables from each of the individual corpora, and interpolated using each method. The results are shown in Table 13 Table 10: Summary of impact of changes Minor improvements that we did not adopt was avoiding reducing maximum phrase length to 5 (average +.03) and tuning with 1000-best lists (+.02). T"
W13-2212,N13-1035,1,0.80094,"was found to be better. The modified interpolation was not possible in fr↔en as it uses to much RAM. The final experiment of the initial system development phase was to train the systems on the new data, adding newstest2011 to the tuning set (now 10,068 sentences). Table 12 reports the gains on newstest2012 due to added data, indicating very clearly that valuable new data resources became available this year. The results from the phrase-table interpolation are quite mixed, and we only used the technique 117 for the final system in en-es. An interpolation based on PRO has recently been shown (Haddow, 2013) to improve on perplexity minimisation is some cases, but the current implementation of this method is limited to 2 phrase-tables, so we did not use it in this evaluation. 2.2 Figure 1: Bilingual Sentence with Alignments sequence of operations (o1 , o2 , . . . , oJ ) and learn a Markov model over this sequence as: Modified Moore-Lewis Filtering posm (F, E, A) = p(oJ1 ) = In last year’s evaluation (Koehn and Haddow, 2012b) we had some success with modified Moore-Lewis filtering (Moore and Lewis, 2010; Axelrod et al., 2011) of the training data. This year we conducted experiments in most of the"
W13-2212,W12-3154,1,0.0809865,"ems for the German–English language pairs to include POS and morphological target sequence models. But we did not use the factored decomposition of translation options into multiple mapping steps, since this usually lead to much slower systems with usually worse results. A good place, however, for factored decomposition is the handling of rare and unknown source words which have more frequent morphological variants (Koehn and Haddow, 2012a). Here, we used only factored backoff for unknown words, giving gains in BLEU of +.12 for German–English. Initial System Development We start with systems (Haddow and Koehn, 2012) that we developed for the 2012 Workshop on Statistical Machine Translation (Callison-Burch et al., 2012). The notable features of these systems are: • Moses phrase-based models with mostly default settings • training on all available parallel data, including the large UN parallel data, the FrenchEnglish 109 parallel data and the LDC Gigaword data 1.2 Tuning with k-best MIRA In preparation for training with sparse features, we moved away from MERT which is known to fall 114 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 114–121, c Sofia, Bulgaria, August 8-9, 2013"
W13-2212,W11-2123,1,0.674625,"do not go Generate Source Only (ja) Ich gehe ja ↓ nicht I do not go Jump Forward Ich gehe ja nicht ↓ I do not go Generate (zum, to the) . . . gehe ja nicht zum ↓ . . . not go to the Generate (haus, house) . . . ja nicht zum haus ↓ . . . go to the house Table 15: Step-wise Generation of Figure 1 LP newstest de-en fr-en es-en cs-en ru-en en-de en-fr en-es en-cs en-ru Baseline 2012 2013 23.85 26.54 30.77 31.09 34.02 30.04 22.70 25.70 31.87 24.00 17.95 20.06 28.76 30.03 33.87 29.66 15.81 18.35 23.75 18.44 The large language model was then quantized to 10 bits and compressed to 643 GB with KenLM (Heafield, 2011), loaded onto a machine with 1 TB RAM, and used as an additional feature in unconstrained French–English, Spanish–English, and Czech–English submissions. This additional language model is the only difference between our final constrained and unconstrained submissions; no additional parallel data was used. Results are shown in Table 18. Improvement from large language models is not a new result (Brants et al., 2007); the primary contribution is estimating on a single machine. +OSM 2012 2013 24.11 +.26 26.83 +.29 30.96 +.19 31.46 +.37 34.51 +.49 30.94 +.90 23.03 +.33 25.79 +.09 32.33 +.46 24.33"
W13-2212,P13-2121,1,0.843378,"reaks down the gains over the final system from Section 1 from using the operation sequence models (OSM), modified Moore-Lewis filtering (MML), fixing a bug with the sparse lexical features (Sparse-Lex Bugfix), and instance weighting (Instance Wt.), translation model combination (TM-Combine), and use of the huge language model (ClueWeb09 LM). Huge Language Models To overcome the memory limitations of SRILM, we implemented modified Kneser-Ney (Kneser and Ney, 1995; Chen and Goodman, 1998) smoothing from scratch using disk-based streaming algorithms. This open-source4 tool is described fully by Heafield et al. (2013). We used it to estimate an unpruned 5–gram language model on web pages from ClueWeb09.5 The corpus was preprocessed by removing spam (Cormack et al., 2011), selecting English documents, splitting sentences, deduplicating, tokenizing, and truecasing. Estimation on the remaining 126 billion tokens took 2.8 days on a single machine with 140 GB RAM (of which 123 GB was used at peak) and six hard drives in a RAID5 configuration. Statistics about the resulting model are shown in Table 17. 4 5 Summary Acknowledgments Thanks to Miles Osborne for preprocessing the ClueWeb09 corpus. The research leadin"
W13-2212,D07-1103,0,0.0605594,"Missing"
W13-2212,2012.amta-papers.9,1,0.770604,"anguage model trained on 126 billion tokens with a new training tool (Section 4). 1 1.1 Factored Backoff (German–English) We have consistently used factored models in past WMT systems for the German–English language pairs to include POS and morphological target sequence models. But we did not use the factored decomposition of translation options into multiple mapping steps, since this usually lead to much slower systems with usually worse results. A good place, however, for factored decomposition is the handling of rare and unknown source words which have more frequent morphological variants (Koehn and Haddow, 2012a). Here, we used only factored backoff for unknown words, giving gains in BLEU of +.12 for German–English. Initial System Development We start with systems (Haddow and Koehn, 2012) that we developed for the 2012 Workshop on Statistical Machine Translation (Callison-Burch et al., 2012). The notable features of these systems are: • Moses phrase-based models with mostly default settings • training on all available parallel data, including the large UN parallel data, the FrenchEnglish 109 parallel data and the LDC Gigaword data 1.2 Tuning with k-best MIRA In preparation for training with sparse f"
W13-2212,W12-3139,1,0.0637938,"anguage model trained on 126 billion tokens with a new training tool (Section 4). 1 1.1 Factored Backoff (German–English) We have consistently used factored models in past WMT systems for the German–English language pairs to include POS and morphological target sequence models. But we did not use the factored decomposition of translation options into multiple mapping steps, since this usually lead to much slower systems with usually worse results. A good place, however, for factored decomposition is the handling of rare and unknown source words which have more frequent morphological variants (Koehn and Haddow, 2012a). Here, we used only factored backoff for unknown words, giving gains in BLEU of +.12 for German–English. Initial System Development We start with systems (Haddow and Koehn, 2012) that we developed for the 2012 Workshop on Statistical Machine Translation (Callison-Burch et al., 2012). The notable features of these systems are: • Moses phrase-based models with mostly default settings • training on all available parallel data, including the large UN parallel data, the FrenchEnglish 109 parallel data and the LDC Gigaword data 1.2 Tuning with k-best MIRA In preparation for training with sparse f"
W13-2212,D07-1091,1,0.809959,"Missing"
W13-2212,E03-1076,1,0.320668,"Missing"
W13-2212,2012.iwslt-papers.7,0,0.0521392,"Missing"
W13-2212,P10-2041,0,0.0792206,"Missing"
W13-2212,2001.mtsummit-papers.68,0,0.0249874,"e model, and domain adaptation techniques. We also report on utilizing a huge language model trained on 126 billion tokens. Note that while our final 2012 systems included subsampling of training data with modified Moore-Lewis filtering (Axelrod et al., 2011), we did not use such filtering at the starting point of our development. We will report on such filtering in Section 2. Moreover, our system development initially used the WMT 2012 data condition, since it took place throughout 2012, and we switched to WMT 2013 training data at a later stage. In this section, we report cased BLEU scores (Papineni et al., 2001) on newstest2011. The annual machine translation evaluation campaign for European languages organized around the ACL Workshop on Statistical Machine Translation offers the opportunity to test recent advancements in machine translation in large data condition across several diverse language pairs. Building on our own developments and external contributions to the Moses open source toolkit, we carried out extensive experiments that, by early indications, led to a strong showing in the evaluation campaign. We would like to stress especially two contributions: the use of the new operation sequence"
W13-2212,E12-1055,0,0.0108725,"ta (newstest2012 scores) 2 We adopted a number of changes that improved our baseline system by an average of +.30, see Table 10 for a breakdown. Domain Adaptation Techniques We explored two additional domain adaptation techniques: phrase table interpolation and modified Moore-Lewis filtering. method factored backoff kbest MIRA sparse features and domain indicator tuning with 25 iterations maximum phrase length 5 unpruned 4-gram LM translation table limit 100 total 2.1 Phrase Table Interpolation We experimented with phrase-table interpolation using perplexity minimisation (Foster et al., 2010; Sennrich, 2012). In particular, we used the implementation released with Sennrich (2012) and available in Moses, comparing both the naive and modified interpolation methods from that paper. For each language pair, we took the alignments created from all the data concatenated, built separate phrase tables from each of the individual corpora, and interpolated using each method. The results are shown in Table 13 Table 10: Summary of impact of changes Minor improvements that we did not adopt was avoiding reducing maximum phrase length to 5 (average +.03) and tuning with 1000-best lists (+.02). The improvements d"
W13-2212,D11-1033,0,\N,Missing
W13-2212,P02-1040,0,\N,Missing
W13-2212,P13-2071,1,\N,Missing
W13-2212,N13-1001,1,\N,Missing
W13-2816,P11-2031,0,0.01261,"in the observed translation input sentence is produced while the writer has a particular “true” word wi ∈ Ci in mind, where Ci is the set of words confusable with w ˆi . For the sake of simplicity, we assume that within a confusion set, all “true word” options are equally likely, i.e., p(w ˆi |wi = x) = 1 for x ∈ C . The writer chooses the next i |Ci | word wi+1 according to the conditional word bigram probability p(wi+1 |wi ). 3.3 Automatic evaluation (BLEU) Due to the relatively small size of the evaluation set and instability inherent in minimum error rate training (Foster and Kuhn, 2009; Clark et al., 2011), results of individual tuning and evaluation runs can be unreliable. We therefore preformed multiple tuning and evaluation runs for each system (baseline, rule-based and weighted graph). To illustrate the precision of the BLEU score on our data sets, we plot in Fig. 2 for each individual tuning run the BLEU score achieved on the tuning set (x-axis) against the performance on the evaluation set (y-axis). The variance along the x-axis for each system is due to search errors in parameter optimization. Since the search space is not convex, the tuning process can get stuck in local maxima. The app"
W13-2816,C12-2032,0,0.0222513,"translations produced by the SMT engine from plain and corrected versions. confusions. The second is an engineering method: we use a commercial pronunciation-generation tool to generate a homophone dictionary, then use this dictionary to turn the input into a weighted graph where each word is replaced by a weighted disjunction of homophones. Related, though less elaborate, work has been reported by Bertoldi et al. (2010), who address spelling errors using a character-level confusion network based on common character confusions in typed English and test them on artificially created noisy data. Formiga and Fonollosa (2012) also used character-based models to correct spelling on informally written English data. The two approaches in the present paper exploit fundamentally different knowledge sources in trying to identify and correct homophone errors. The rule-based method relies exclusively on source-side information, encoding patterns indicative of common French homophone confusions. The weighted graph method shifts the balance to the target side; the choice between potential homophone alternatives is made primarily by the target language model, though the source language weights and the translation model are a"
W13-2816,W09-0439,0,0.0299907,"ose that each word w ˆi in the observed translation input sentence is produced while the writer has a particular “true” word wi ∈ Ci in mind, where Ci is the set of words confusable with w ˆi . For the sake of simplicity, we assume that within a confusion set, all “true word” options are equally likely, i.e., p(w ˆi |wi = x) = 1 for x ∈ C . The writer chooses the next i |Ci | word wi+1 according to the conditional word bigram probability p(wi+1 |wi ). 3.3 Automatic evaluation (BLEU) Due to the relatively small size of the evaluation set and instability inherent in minimum error rate training (Foster and Kuhn, 2009; Clark et al., 2011), results of individual tuning and evaluation runs can be unreliable. We therefore preformed multiple tuning and evaluation runs for each system (baseline, rule-based and weighted graph). To illustrate the precision of the BLEU score on our data sets, we plot in Fig. 2 for each individual tuning run the BLEU score achieved on the tuning set (x-axis) against the performance on the evaluation set (y-axis). The variance along the x-axis for each system is due to search errors in parameter optimization. Since the search space is not convex, the tuning process can get stuck in"
W13-2816,2005.mtsummit-papers.11,0,0.00505547,"s trained from a small bicorpus of domain language. With automatic evaluation, the weighted graph method yields an improvement of about +0.63 BLEU points, while the rulebased method scores about the same as the baseline. On contrastive manual evaluation, both methods give highly significant improvements (p < 0.0001) and score about equally when compared against each other. 1 Introduction and motivation The data used to train Statistical Machine Translation (SMT) systems is most often taken from the proceedings of large multilingual organisations, the generic example being the Europarl corpus (Koehn, 2005); for academic evaluation exercises, the test data may well also be taken from the same source. Texts of this kind are carefully cleaned-up formal language. However, real MT systems often need to handle text from very different genres, which as usual causes problems. This paper addresses a problem common in domains containing informally written text: spelling errors based on homophone confusions. Concretely, the work reported was carried out in the context of the ACCEPT project, which deals with the increasingly important topic of translating online forum posts; the experiments we describe wer"
W13-2816,N10-1064,0,0.0214978,"ins on ne rec¸oit pas l’alerte). ... (at least we do not recoit alert). .. (at least it does not receive the alert). Figure 1: Examples of homophone errors in French forum data, contrasting English translations produced by the SMT engine from plain and corrected versions. confusions. The second is an engineering method: we use a commercial pronunciation-generation tool to generate a homophone dictionary, then use this dictionary to turn the input into a weighted graph where each word is replaced by a weighted disjunction of homophones. Related, though less elaborate, work has been reported by Bertoldi et al. (2010), who address spelling errors using a character-level confusion network based on common character confusions in typed English and test them on artificially created noisy data. Formiga and Fonollosa (2012) also used character-based models to correct spelling on informally written English data. The two approaches in the present paper exploit fundamentally different knowledge sources in trying to identify and correct homophone errors. The rule-based method relies exclusively on source-side information, encoding patterns indicative of common French homophone confusions. The weighted graph method s"
W13-2816,2012.amta-papers.25,1,0.781932,", achieves an average BLEU score of 42.47 on this set. 3.1 The rule-based approach Under the ACCEPT project, a set of lightweight pre-editing rules have been developed specifically for the Symantec Forum translation task. Some of the rules are automatic (direct reformulations); others present the user with a set of suggestions. The evaluations described in Gerlach et al. (2013) demonstrate that pre-editing with the rules has a significant positive effect on the quality of SMTbased translation. The implemented rules address four main phenomena: differences between informal and formal language (Rayner et al., 2012), differences between local French and English word-order, elThe set of Acrolinx pre-editing rules potentially relevant to resolution of homophone errors was applied to the devtest b set test corpus (Section 2.1). In order to be able to make a fair comparison with the weighted-graph method, we only used rules with a unique suggestion, which could be run automatically. Applying these rules produced 430 changed words in the test corpus, but did not change the average BLEU score significantly (42.38). Corrections made with a human in the loop, used as “oracle” input for the SMT system, by the 111"
W13-2816,bredenkamp-etal-2000-looking,0,\N,Missing
W13-2816,P07-2045,0,\N,Missing
W13-2816,P00-1056,0,\N,Missing
W13-5303,W06-2810,0,0.0193329,"Missing"
W13-5303,2010.amta-papers.5,0,0.0301593,"language varieties) offer themselves to exploit the linguistic proximity in order to overcome the usual scarcity of parallel data. Nakov and Tiedemann (2012) take advantage of the great overlap in vocabulary and the strong syntactic and lexical similarity between Bulgarian and Macedonian. They develop an SMT system for this language pair by employing a combination of character and word level translation models, outperforming a phrasebased word-level baseline. Regarding MT of dialects, Zbib et al. (2012) use crowdsourcing to build Levantine-English and Egyptian-English parallel corpora; while Sawaf (2010) normalizes non-standard, spontaneous and dialect Arabic into Modern Standard Arabic to achieve translations into English. A considerable amount of work has been done on extracting parallel sentences from comparable corpora, i.e. a set of documents in different languages that contains similar information. Munteanu and Marcu (2005) use a Maximum Entropy classifier trained on parallel sentences to determine if a sentence pair is parallel or not. Based on techniques of Information Retrieval, Abdul-Rauf and Schwenk (2011) use the translations of a SMT system in order to find the corresponding para"
W13-5303,J93-2003,0,0.0399014,"r is parallel or not. Based on techniques of Information Retrieval, Abdul-Rauf and Schwenk (2011) use the translations of a SMT system in order to find the corresponding parallel sentences from the targetlanguage side of the comparable corpus. Smith et al. (2010) explore Wikipedia to extract parallel sentences where, once they achieve an alignment at the document level by taking advantage of the structure of this online encyclopedia, they train Conditional Random Fields to tackle the task of sentence alignment. Tillmann and Xu (2009) extract sentence pairs by a model based on the IBM Model-1 (Brown et al., 1993) and perform training on parallel data. With the exception of Munteanu and Marcu (2005), where bootstrapping techniques find application, these methods require (and presuppose the existence of) a certain amount of resources (i.e. parallel data or lexicon coverage) not available for some languages or varieties. (1) AG: Ja, ich weiß es doch. VD: J˚ a, i waas s e. ‘yes, I know it anyway.’ In an early stage, we were interested in finding a way to align these parallel sentences on a word-by-word basis, in order to simultaneously generate lexical resources comprising morphology and morpho-syntactic"
W13-5303,N10-1063,0,0.0139778,"tandard Arabic to achieve translations into English. A considerable amount of work has been done on extracting parallel sentences from comparable corpora, i.e. a set of documents in different languages that contains similar information. Munteanu and Marcu (2005) use a Maximum Entropy classifier trained on parallel sentences to determine if a sentence pair is parallel or not. Based on techniques of Information Retrieval, Abdul-Rauf and Schwenk (2011) use the translations of a SMT system in order to find the corresponding parallel sentences from the targetlanguage side of the comparable corpus. Smith et al. (2010) explore Wikipedia to extract parallel sentences where, once they achieve an alignment at the document level by taking advantage of the structure of this online encyclopedia, they train Conditional Random Fields to tackle the task of sentence alignment. Tillmann and Xu (2009) extract sentence pairs by a model based on the IBM Model-1 (Brown et al., 1993) and perform training on parallel data. With the exception of Munteanu and Marcu (2005), where bootstrapping techniques find application, these methods require (and presuppose the existence of) a certain amount of resources (i.e. parallel data"
W13-5303,D07-1091,0,0.0324684,"as gender, case, person, number etc.). Given that usually the two translations are syntactically very similar, with little re8 source and the target sentence are approximately the same. Second, non-adjacent insertion-deletion pairs with a distance measure below the threshold are marked as valid alignments. That way the algorithm that by itself provides only linear alignments is also capable to capture some non-local alignments resulting from syntactic re-ordering. With regard to SMT and contemplating the immanent problem of data sparsity, it seems obvious that a factorized translation model (Koehn and Hoang, 2007) will have certain advantages over a translation model that only considers full word forms. This, however, requires the generation of lexical resources for both language varieties. For the source language (AG) such resources already exist. The question is, if and how the lexical information stemming from the source language can be transferred onto the target language. Our word alignment is capable of identifying cognates. However, these cognates will only cover certain word forms out of more complex morphological paradigms. Given that for AG, the lemma and the information about the paradigm ca"
W13-5303,2012.eamt-1.37,0,0.0120697,"o our data because they either require a substantial amount of data to reliably estimate statistical models, i.e. at least 10k sentence pairs, such as the Microsoft Bilingual Aligner (Moore, 2002). But also the number of sentences to be aligned must be almost 11 equal – with a ratio of 1:6 it was not possible to achieve any reliable results at all. Additionally, the sentences in the parallel texts are presupposed to occur in the same order, which does not apply to the Wikipedia articles under consideration. Similar requirements hold for the Hunalign tool (Varga et al., 2005). Finally, LEXACC (Stefanescu et al., 2012) is a parallel sentence extractor for comparable corpora based on Information Retrieval, but again, certain resources are required beforehand, such as a GIZA++ dictionary created from existing parallel documents. The main obstacle to using any of these algorithms is that the texts in the BAR Wikipedia obey widely differing and mostly adhoc orthographic conventions, which are not consistent for a given dialectal variety, even within a single article. In our situation, we had to develop an alignment method that relies only on the linguistic proximity between the two varieties. Baseline (lcase) S"
W13-5303,2009.eamt-1.3,0,0.0178622,"ng set. The idea here is that the character-level models are built from “noisy” training data, containing many German-Viennese word-pairs which either represent lexical differences, or are the result of bad alignments. In order to extract the cognates we ran GIZA++ alignment on the combined TRAIN and DEV corpora, extracted all source-target token pairs that were aligned, converted the pairs to the BARSUBST representation (see section 5.1), and filtered using the log-normalised Levenshtein distance. Character-level Models In earlier work on MT for closely-related languages (Vilar et al., 2007; Tiedemann, 2009; Nakov and Tiedemann, 2012), it has been shown that character-level translation models can be effective. These character-level models are also built using phrase-based Moses, but allowing it to treat single characters or groups of characters as “tokens”. In the unigram character-level model, we treat each character as a separate token by inserting a space between each of them, and using a special character (||) to indicate word boundaries. For the bigram character-level model, the “tokens” are pairs of adjacent characters, with the same word boundary character as in the unigram model. Table 1"
W13-5303,N09-2024,0,0.0146047,"mum Entropy classifier trained on parallel sentences to determine if a sentence pair is parallel or not. Based on techniques of Information Retrieval, Abdul-Rauf and Schwenk (2011) use the translations of a SMT system in order to find the corresponding parallel sentences from the targetlanguage side of the comparable corpus. Smith et al. (2010) explore Wikipedia to extract parallel sentences where, once they achieve an alignment at the document level by taking advantage of the structure of this online encyclopedia, they train Conditional Random Fields to tackle the task of sentence alignment. Tillmann and Xu (2009) extract sentence pairs by a model based on the IBM Model-1 (Brown et al., 1993) and perform training on parallel data. With the exception of Munteanu and Marcu (2005), where bootstrapping techniques find application, these methods require (and presuppose the existence of) a certain amount of resources (i.e. parallel data or lexicon coverage) not available for some languages or varieties. (1) AG: Ja, ich weiß es doch. VD: J˚ a, i waas s e. ‘yes, I know it anyway.’ In an early stage, we were interested in finding a way to align these parallel sentences on a word-by-word basis, in order to simul"
W13-5303,moore-2002-fast,0,0.010506,"-level translation outperforms the word-level translation, but the backoff offers the best performance of all. The BLEU scores are relatively high compared to the 5.1 Sentence Extraction Our sentence alignment algorithm is primarily based on string-edit distance measures. There exist several open-source alignment tools for extracting parallel sentences from bilingual corpora. However, none of them is applicable to our data because they either require a substantial amount of data to reliably estimate statistical models, i.e. at least 10k sentence pairs, such as the Microsoft Bilingual Aligner (Moore, 2002). But also the number of sentences to be aligned must be almost 11 equal – with a ratio of 1:6 it was not possible to achieve any reliable results at all. Additionally, the sentences in the parallel texts are presupposed to occur in the same order, which does not apply to the Wikipedia articles under consideration. Similar requirements hold for the Hunalign tool (Varga et al., 2005). Finally, LEXACC (Stefanescu et al., 2012) is a parallel sentence extractor for comparable corpora based on Information Retrieval, but again, certain resources are required beforehand, such as a GIZA++ dictionary c"
W13-5303,W07-0705,0,0.0262422,"ates from the training set. The idea here is that the character-level models are built from “noisy” training data, containing many German-Viennese word-pairs which either represent lexical differences, or are the result of bad alignments. In order to extract the cognates we ran GIZA++ alignment on the combined TRAIN and DEV corpora, extracted all source-target token pairs that were aligned, converted the pairs to the BARSUBST representation (see section 5.1), and filtered using the log-normalised Levenshtein distance. Character-level Models In earlier work on MT for closely-related languages (Vilar et al., 2007; Tiedemann, 2009; Nakov and Tiedemann, 2012), it has been shown that character-level translation models can be effective. These character-level models are also built using phrase-based Moses, but allowing it to treat single characters or groups of characters as “tokens”. In the unigram character-level model, we treat each character as a separate token by inserting a space between each of them, and using a special character (||) to indicate word boundaries. For the bigram character-level model, the “tokens” are pairs of adjacent characters, with the same word boundary character as in the unigr"
W13-5303,J05-4003,0,0.0229536,"or this language pair by employing a combination of character and word level translation models, outperforming a phrasebased word-level baseline. Regarding MT of dialects, Zbib et al. (2012) use crowdsourcing to build Levantine-English and Egyptian-English parallel corpora; while Sawaf (2010) normalizes non-standard, spontaneous and dialect Arabic into Modern Standard Arabic to achieve translations into English. A considerable amount of work has been done on extracting parallel sentences from comparable corpora, i.e. a set of documents in different languages that contains similar information. Munteanu and Marcu (2005) use a Maximum Entropy classifier trained on parallel sentences to determine if a sentence pair is parallel or not. Based on techniques of Information Retrieval, Abdul-Rauf and Schwenk (2011) use the translations of a SMT system in order to find the corresponding parallel sentences from the targetlanguage side of the comparable corpus. Smith et al. (2010) explore Wikipedia to extract parallel sentences where, once they achieve an alignment at the document level by taking advantage of the structure of this online encyclopedia, they train Conditional Random Fields to tackle the task of sentence"
W13-5303,N12-1006,0,0.0257693,"and the Viennese orthography of a sentence from our corpus. Background Pairs of closely related languages (or language varieties) offer themselves to exploit the linguistic proximity in order to overcome the usual scarcity of parallel data. Nakov and Tiedemann (2012) take advantage of the great overlap in vocabulary and the strong syntactic and lexical similarity between Bulgarian and Macedonian. They develop an SMT system for this language pair by employing a combination of character and word level translation models, outperforming a phrasebased word-level baseline. Regarding MT of dialects, Zbib et al. (2012) use crowdsourcing to build Levantine-English and Egyptian-English parallel corpora; while Sawaf (2010) normalizes non-standard, spontaneous and dialect Arabic into Modern Standard Arabic to achieve translations into English. A considerable amount of work has been done on extracting parallel sentences from comparable corpora, i.e. a set of documents in different languages that contains similar information. Munteanu and Marcu (2005) use a Maximum Entropy classifier trained on parallel sentences to determine if a sentence pair is parallel or not. Based on techniques of Information Retrieval, Abd"
W13-5303,P12-2059,0,0.139545,"V documentaries and free interview recordings of dialect speakers. The transcripts were manually translated into both AG and VD, the latter being vacuous in most cases. This way we could ensure that (rarely occurring) switchings into the standard would not end up in the target model. A typical example looks as follows, where AG and VD refer to the standard and the Viennese orthography of a sentence from our corpus. Background Pairs of closely related languages (or language varieties) offer themselves to exploit the linguistic proximity in order to overcome the usual scarcity of parallel data. Nakov and Tiedemann (2012) take advantage of the great overlap in vocabulary and the strong syntactic and lexical similarity between Bulgarian and Macedonian. They develop an SMT system for this language pair by employing a combination of character and word level translation models, outperforming a phrasebased word-level baseline. Regarding MT of dialects, Zbib et al. (2012) use crowdsourcing to build Levantine-English and Egyptian-English parallel corpora; while Sawaf (2010) normalizes non-standard, spontaneous and dialect Arabic into Modern Standard Arabic to achieve translations into English. A considerable amount o"
W13-5303,P00-1056,0,0.158351,"tuning the MT system parameters and the third for testing during system development. The last was reserved for final testing. The relative sizes of the three section is shown in Table 1. Section Sentences TRAIN 4909 600 600 600 DEV DEVTEST TEST Tokens AG VD 39108 40031 4775 4882 4712 4803 4841 4943 Table 1: Corpus sizes (untokenised) 4.2 Word-level Models 4.4 The word-level models are standard phrase-based models built using Moses. The parallel text is tokenised using the Moses tokeniser for German, then it is all lowercased. This parallel text is then aligned in both directions using GIZA++ (Och and Ney, 2000) and the alignments are symmetrised using the ”grow-diag-final-and” heuristic. The aligned parallel text is then used to estimate a translation table using the standard Moses heuristics, and a 3-gram language model built on the target side of the parallel text using SRILM with Kneser-Ney smoothing. The translation and language models are then combined with a distancebased reordering model and their weights optimised for BLEU using MERT on the DEV corpus. 4.3 Backoff Models After observing the performance of word and character-level models, we decided to try to combine them into a backoff model"
W13-5303,P07-2045,0,\N,Missing
W14-3302,bojar-etal-2014-hindencorp,1,0.801715,"Missing"
W14-3302,W13-2242,0,0.189283,"Missing"
W14-3302,W14-3305,0,0.0227897,"Missing"
W14-3302,W14-3326,1,0.729814,"Missing"
W14-3302,W14-3313,0,0.0328356,"Missing"
W14-3302,W14-3342,0,0.0770851,"Missing"
W14-3302,2012.iwslt-papers.5,1,0.897779,"Each system SJ in the pool {Sj } is represented by an associated relative ability µj and a variance σa2 (fixed across all systems) which serve as the parameters of a Gaussian distribution. Samples from this distribution represent the quality of sentence translations, with higher quality samples having higher values. Pairwise annotations (S1 , S2 , π) are generated according to the following process: Method 1: Expected Wins (EW) Introduced for WMT13, the E XPECTED W INS has an intuitive score demonstrated to be accurate in ranking systems according to an underlying model of “relative ability” (Koehn, 2012a). The idea is to gauge the probability that a system Si will be ranked better than another system randomly chosen from a pool of opponents {Sj : j 6= i}. If we define the function win(A, B) as the number of times system A is ranked better than system B, 19 1. Select two systems S1 and S2 from the pool of systems {Sj } This score is then used to sort the systems and produce the ranking. 2. Draw two “translations”, adding random 2 to simulate Gaussian noise with variance σobs the subjectivity of the task and the differences among annotators: 3.4 Method Selection We have three methods which, pr"
W14-3302,W06-3114,1,0.176114,"estimation of machine translation quality, and a metrics task. This year, 143 machine translation systems from 23 institutions were submitted to the ten translation directions in the standard translation task. An additional 6 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had four subtasks, with a total of 10 teams, submitting 57 entries. 1 Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at ACL 2014. This workshop builds on eight previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013). This year we conducted four official tasks: a translation task, a quality estimation task, a metrics task1 and a medical translation task. In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Hindi, and Russian. The Hindi translation tasks were new this year, providing a lesser resourced data condition on a challenging languag"
W14-3302,W12-3101,0,0.0505246,"Missing"
W14-3302,P02-1040,0,0.10401,"inuous-space language model is also used in a post-processing step for each system. POSTECH submitted a phrase-based SMT system and query translation system for the DE–EN language pair in both subtasks. They analysed three types of query formation, generated query translation candidates using term-to-term dictionaries and a phrase-based system, and then scored them using a co-occurrence word frequency measure to select the best candidate. UEDIN applied the Moses phrase-based system to 5.5 Results MT quality in the Medical Translation Task is evaluated using automatic evaluation metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006), PER (Tillmann et al., 1997), and CDER (Leusch et al., 2006). BLEU scores are reported as percentage and all error rates are reported as one minus the original value, also as percentage, so that all metrics are in the 0-100 range, and higher scores indicate better translations. The main reason for not conducting human evaluation, as it happens in the standard Trans45 original ID BLEU normalized truecased normalized lowercased 1-TER 1-PER 1-CDER BLEU 1-TER 1-PER 1-CDER Czech→English CUNI 29.64 29.79±1.07 47.45±1.15 61.64±1.06 52.18±0.98 31.68±1.14 49.84±1.10 64.38±1."
W14-3302,W14-3328,0,0.0383178,"Missing"
W14-3302,W14-3301,1,0.485672,"guage pair by translating newspaper articles and provided training data. 2.1 2.2 As in past years we provided parallel corpora to train translation models, monolingual corpora to train language models, and development sets to tune system parameters. Some training corpora were identical from last year (Europarl4 , United Nations, French-English 109 corpus, CzEng, Common Crawl, Russian-English Wikipedia Headlines provided by CMU), some were updated (Russian-English parallel data provided by Yandex, News Commentary, monolingual data), and a new corpus was added (HindiEnglish corpus, Bojar et al. (2014)), Hindi-English Wikipedia Headline corpus). Some statistics about the training materials are given in Figure 1. Test data The test data for this year’s task was selected from news stories from online sources, as before. However, we changed our method to create the test sets. In previous years, we took equal amounts of source sentences from all six languages involved (around 500 sentences each), and translated them into all other languages. While this produced a multi-parallel test corpus that could be also used for language pairs (such as Czech-Russian) that we did not include in the evaluati"
W14-3302,W14-3330,0,0.0464303,"Missing"
W14-3302,W14-3320,0,0.0387654,"Missing"
W14-3302,W14-3317,0,0.0337314,"Missing"
W14-3302,W14-3343,1,0.726267,"Finally, with respect to out-of-domain (different 41 It is interesting to mention the indirect use of human translations by USHEFF for Tasks 1.1-1.3: given a translation for a source segment, all other translations for the same segment were used as pseudo-references. Apart from when this translation was actually the human translation, the human translation was effectively used as a reference. While this reference was mixed with 23 other pseudo-references (other machine translations) for the feature computations, these features led to significant gains in performance over the baseline features Scarton and Specia (2014). We believe that more investigation is needed for human translation quality prediction. Tasks dedicated to this type of data at both sentence- and word-level in the next editions of this shared task would be a possible starting point. The acquisition of such data is however much more costly, as it is arguably hard to find examples of low quality human translation, unless specific settings, such as translation learner corpora, are considered. text domain and MT system) test data, for Task 1.1, none of the papers submitted included experiments. (Shah and Specia, 2014) applied the models trained"
W14-3302,W02-1001,0,\N,Missing
W14-3302,E06-1031,0,\N,Missing
W14-3302,W12-3102,1,\N,Missing
W14-3302,P13-2135,0,\N,Missing
W14-3302,W14-3311,0,\N,Missing
W14-3302,W14-3314,0,\N,Missing
W14-3302,W09-0401,1,\N,Missing
W14-3302,W14-3319,0,\N,Missing
W14-3302,W14-3344,0,\N,Missing
W14-3302,W14-3327,0,\N,Missing
W14-3302,W07-0718,1,\N,Missing
W14-3302,P11-1132,0,\N,Missing
W14-3302,W13-2248,0,\N,Missing
W14-3302,W14-3307,0,\N,Missing
W14-3302,W10-1703,1,\N,Missing
W14-3302,W14-3323,0,\N,Missing
W14-3302,P13-4014,1,\N,Missing
W14-3302,W08-0309,1,\N,Missing
W14-3302,W14-3340,1,\N,Missing
W14-3302,W14-3312,0,\N,Missing
W14-3302,P13-1139,0,\N,Missing
W14-3302,W14-3310,1,\N,Missing
W14-3302,P13-1004,1,\N,Missing
W14-3302,W14-3304,0,\N,Missing
W14-3302,W14-3321,0,\N,Missing
W14-3302,W14-3308,0,\N,Missing
W14-3302,uresova-etal-2014-multilingual,1,\N,Missing
W14-3302,W14-3336,1,\N,Missing
W14-3302,W14-3331,0,\N,Missing
W14-3302,W14-3332,0,\N,Missing
W14-3302,W14-3325,0,\N,Missing
W14-3302,W14-3329,0,\N,Missing
W14-3302,W14-3315,0,\N,Missing
W14-3302,W13-2201,1,\N,Missing
W14-3302,W14-3339,0,\N,Missing
W14-3302,W14-3322,1,\N,Missing
W14-3302,W14-3303,0,\N,Missing
W14-3302,W14-3318,0,\N,Missing
W14-3302,W14-3341,0,\N,Missing
W14-3302,W11-2101,1,\N,Missing
W14-3302,W14-3338,1,\N,Missing
W14-3309,D11-1033,0,0.150733,"Missing"
W14-3309,2013.iwslt-evaluation.3,1,0.854043,"8 10.39 20.85 19.39 30.82 19.67 10.52 +0.25 +0.55 +0.09 +0.89 +0.13 27.44 26.42 31.64 24.45 15.48 27.34 26.42 31.76 24.63 15.26 ∆ -0.10 ±0.00 +0.12 +0.18 -0.22 Table 1: Using Word Clusters in Phrase-based and OSM models – B0 = System without Clusters, +Cid = with Cluster We also trained OSM models over POS and morph tags. For the English-to-German system we added an OSM model over [pos, morph] (source:pos, target:morph) and for the Germanto-English system we added an OSM model over [morph,pos] (source:morph, target:pos), a configuration that was found to work best in our previous experiments (Birch et al., 2013). Table 2 shows gains from additionally using OSM models over POS/morph tags. Lang B0 +OSMp,m ∆ en-de de-en 20.44 27.24 20.60 27.44 +0.16 +0.20 Unsupervised Transliteration Model Pair Training OOV B0 +Tr ∆ ru-en en-ru hi-en en-hi 232K 232K 38K 38K 1356 681 503 394 24.63 19.67 14.67 11.76 25.06 19.91 15.48 12.83 +0.41 +0.24 +0.81 +1.07 Table 3: Using Unsupervised Transliteration Model – Training = Extracted Transliteration Corpus (types), OOV = Out-of-vocabulary words (tokens) B0 = System without Transliteration, +Tr = Transliterating OOVs Table 3 shows the number (types) of transliteration pai"
W14-3309,D08-1023,0,0.0296702,".ed.ac.uk Kenneth Heafield Computer Science Department Stanford University heafield@cs.stanford.edu Abstract Our baseline systems are based on the setup described in (Durrani et al., 2013b) that we used for the Eighth Workshop on Statistical Machine Translation (Bojar et al., 2013). The notable features of these systems are described in the following section. The experiments that we carried out for this year’s translation task are described in the following sections. (Durrani et al., 2013a) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for GermanEnglish language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram mod"
W14-3309,W13-2201,1,0.872446,"Missing"
W14-3309,buck-etal-2014-n,1,0.885994,"Missing"
W14-3309,E14-4029,1,0.915819,"tatistical Machine Translation (WMT). We participated in all language pairs. We have improved upon our 2013 system by i) using generalized representations, specifically automatic word clusters for translations out of English, ii) using unsupervised character-based models to translate unknown words in RussianEnglish and Hindi-English pairs, iii) synthesizing Hindi data from closely-related Urdu data, and iv) building huge language on the common crawl corpus. 1 Translation Task Baseline Using Generalized Word Representations We explored the use of automatic word clusters in phrase-based models (Durrani et al., 2014a). We computed the clusters with GIZA++’s mkcls (Och, 1999) on the source and target side of the parallel training corpus. Clusters are word classes that are optimized to reduce n-gram perplexity. By generating a cluster identifier for each output word, we are able to add an n-gram model We trained our systems with the following settings: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a l"
W14-3309,N09-1025,0,0.082017,"Missing"
W14-3309,P11-1105,1,0.910095,"Missing"
W14-3309,P05-1066,1,0.793483,"limit of 6, maximum phrase-length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for GermanEnglish language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic-preordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) for German-to-English systems. We used trivia tokenizer for tokenizing Hindi. The systems were tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. We used newstest 2013 for the dev experiments. For RussianEnglish pairs news-test 2012 was used for tuning and for Hindi-English pairs, we divided the newsdev 2014 into two halves, used the first half for tuning and second for dev experiments. 1.1 1.2 This paper describes the University of Edinburgh’s (UEDIN) phrase-based submissions to the tr"
W14-3309,P13-2071,1,0.866776,"s for WMT-14 Nadir Durrani Barry Haddow Philipp Koehn School of Informatics University of Edinburgh {dnadir,bhaddow,pkoehn}@inf.ed.ac.uk Kenneth Heafield Computer Science Department Stanford University heafield@cs.stanford.edu Abstract Our baseline systems are based on the setup described in (Durrani et al., 2013b) that we used for the Eighth Workshop on Statistical Machine Translation (Bojar et al., 2013). The notable features of these systems are described in the following section. The experiments that we carried out for this year’s translation task are described in the following sections. (Durrani et al., 2013a) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for GermanEnglish langua"
W14-3309,W14-3310,1,0.89194,"Missing"
W14-3309,W13-2212,1,0.838117,"s for WMT-14 Nadir Durrani Barry Haddow Philipp Koehn School of Informatics University of Edinburgh {dnadir,bhaddow,pkoehn}@inf.ed.ac.uk Kenneth Heafield Computer Science Department Stanford University heafield@cs.stanford.edu Abstract Our baseline systems are based on the setup described in (Durrani et al., 2013b) that we used for the Eighth Workshop on Statistical Machine Translation (Bojar et al., 2013). The notable features of these systems are described in the following section. The experiments that we carried out for this year’s translation task are described in the following sections. (Durrani et al., 2013a) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for GermanEnglish langua"
W14-3309,D08-1089,0,0.557258,"d models (Durrani et al., 2014a). We computed the clusters with GIZA++’s mkcls (Och, 1999) on the source and target side of the parallel training corpus. Clusters are word classes that are optimized to reduce n-gram perplexity. By generating a cluster identifier for each output word, we are able to add an n-gram model We trained our systems with the following settings: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) 97 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 97–104, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics over these identifiers as an additional scoring function. The inclusion of such an additional factor is trivial given the factored model implementation (Koehn and Hoang, 2007) of Moses (Koehn et al., 2007). The n-gram model is trained in the similar way as the regular language model. We trained domain-specific language models separately and then linearly interp"
W14-3309,2014.eamt-1.17,1,0.831092,"The Hindi-English segment of this corpus is a subset of parallel data made available for the translation task but is completely disjoint from the Urdu-English segment. We initially trained a Urdu-to-Hindi SMT system using a very tiny EMILLE1 corpus (Baker Table 2: Using POS and Morph Tags in OSM models – B0 = Baseline, +OSMp,m = POS/Morph-based OSM 1 EMILLE corpus contains roughly 12000 sentences of Hindi and Urdu comparable data. From these we were able to sentence align 7000 sentences to build an Urdu-to-Hindi system. 98 using transliteration and triangulated phrase-tables are presented in Durrani and Koehn (2014). Using our best Urdu-to-Hindi system, we translated the Urdu part of the multi-indic corpus to form HindiEnglish parallel data. Table 4 shows results from using the synthesized Hindi-English corpus in isolation (Syn) and on top of the baseline system (B0 + Syn). et al., 2002). But we found this system to be useless for translating the Urdu part of Indic data due to domain mismatch and huge number of OOV words (approximately 310K tokens). To reduce sparsity we synthesized additional phrase-tables using interpolation and transliteration. Interpolation: We trained two phrase translation tables p"
W14-3309,W11-2123,1,0.79041,"ntations We explored the use of automatic word clusters in phrase-based models (Durrani et al., 2014a). We computed the clusters with GIZA++’s mkcls (Och, 1999) on the source and target side of the parallel training corpus. Clusters are word classes that are optimized to reduce n-gram perplexity. By generating a cluster identifier for each output word, we are able to add an n-gram model We trained our systems with the following settings: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) 97 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 97–104, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics over these identifiers as an additional scoring function. The inclusion of such an additional factor is trivial given the factored model implementation (Koehn and Hoang, 2007) of Moses (Koehn et al., 2007). The n-gram model is trained in the similar way as the regular language model"
W14-3309,C14-1041,1,0.92685,"tatistical Machine Translation (WMT). We participated in all language pairs. We have improved upon our 2013 system by i) using generalized representations, specifically automatic word clusters for translations out of English, ii) using unsupervised character-based models to translate unknown words in RussianEnglish and Hindi-English pairs, iii) synthesizing Hindi data from closely-related Urdu data, and iv) building huge language on the common crawl corpus. 1 Translation Task Baseline Using Generalized Word Representations We explored the use of automatic word clusters in phrase-based models (Durrani et al., 2014a). We computed the clusters with GIZA++’s mkcls (Och, 1999) on the source and target side of the parallel training corpus. Clusters are word classes that are optimized to reduce n-gram perplexity. By generating a cluster identifier for each output word, we are able to add an n-gram model We trained our systems with the following settings: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a l"
W14-3309,P13-2121,1,0.908258,"Missing"
W14-3309,P07-1019,0,0.118146,"sed for the Eighth Workshop on Statistical Machine Translation (Bojar et al., 2013). The notable features of these systems are described in the following section. The experiments that we carried out for this year’s translation task are described in the following sections. (Durrani et al., 2013a) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for GermanEnglish language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic-preordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) for German-to-English systems. We used trivia tokenizer for tokenizing Hindi. The systems w"
W14-3309,E99-1010,0,0.288751,"pairs. We have improved upon our 2013 system by i) using generalized representations, specifically automatic word clusters for translations out of English, ii) using unsupervised character-based models to translate unknown words in RussianEnglish and Hindi-English pairs, iii) synthesizing Hindi data from closely-related Urdu data, and iv) building huge language on the common crawl corpus. 1 Translation Task Baseline Using Generalized Word Representations We explored the use of automatic word clusters in phrase-based models (Durrani et al., 2014a). We computed the clusters with GIZA++’s mkcls (Och, 1999) on the source and target side of the parallel training corpus. Clusters are word classes that are optimized to reduce n-gram perplexity. By generating a cluster identifier for each output word, we are able to add an n-gram model We trained our systems with the following settings: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) 97 Proc"
W14-3309,2005.mtsummit-papers.11,1,0.0781164,"Language Models Our unconstrained submissions use an additional language model trained on web pages from the 2012, 2013, and winter 2013 CommonCrawl.2 The additional language model is the only difference between the constrained and unconstrained submissions; we did not use additional parallel data. These language models were trained on text provided by the CommonCrawl foundation, which they converted to UTF-8 after stripping HTML. Languages were detected using the Compact Language Detection 23 and, except for Hindi where we lack tools, sentences were split with the Europarl sentence splitter (Koehn, 2005). All text was then deduplicated, minimizing the impact of boilerplate, such as social media sharing buttons. We then tokenized and truecased the text as usual. Statistics are shown in Table 5. A full description of the pipeline, including a public data release, appears in Buck et al. (2014). Transliteration: Urdu and Hindi are written in different scripts (Arabic and Devanagri respectively). We added a transliteration component to our Urdu-to-Hindi system. An unsupervised transliteration model is learned from the wordalignments of Urdu-Hindi parallel data. We were able to extract around 2800"
W14-3309,W12-3152,0,0.0414194,"on, +Tr = Transliterating OOVs Table 3 shows the number (types) of transliteration pairs extracted using unsupervised mining, number of OOV words (tokens) in each pair and the gains achieved by transliterating unknown words. 1.4 Synthesizing Hindi Data from Urdu Hindi and Urdu are closely related language pairs that share grammatical structure and have a large overlap in vocabulary. This provides a strong motivation to transform any Urdu-English parallel data into Hindi-English by translating the Urdu part into Hindi. We made use of the Urdu-English segment of the Indic multi-parallel corpus (Post et al., 2012) which contains roughly 87K sentence pairs. The Hindi-English segment of this corpus is a subset of parallel data made available for the translation task but is completely disjoint from the Urdu-English segment. We initially trained a Urdu-to-Hindi SMT system using a very tiny EMILLE1 corpus (Baker Table 2: Using POS and Morph Tags in OSM models – B0 = Baseline, +OSMp,m = POS/Morph-based OSM 1 EMILLE corpus contains roughly 12000 sentences of Hindi and Urdu comparable data. From these we were able to sentence align 7000 sentences to build an Urdu-to-Hindi system. 98 using transliteration and t"
W14-3309,W13-2228,1,0.84762,"ne, discontinuous, swap), and one that distinguishes the discontinuous orientations to the left and right. Table 8 shows slight improvements with these models, so we used them in our baseline. Russian-English: We tried to improve wordalignments by integrating a transliteration submodel into GIZA++ word aligner. The probability of a word pair is calculated as an interpolation of the transliteration probability and translation probability stored in the t-table of the different alignment models used by the GIZA++ aligner. This interpolation is done for all iterations of all alignment models (See Sajjad et al. (2013) for details). Due to shortage of time we could only run it for Russian-to-English. The improved alignments gave a gain of +0.21 on news-test 2013 and +0.40 on news-test 2014. Threshold filtering of phrase table: We experimented with discarding some phrase table entry due to their low probability. We found that phrase translations with the phrase translation probability 100 φ(f |e)&lt;10−4 can be safely discarded with almost no change in translations. However, discarding phrase translations with the inverse phrase translation probability φ(e|f )&lt;10−4 is more risky, especially with morphologically"
W14-3309,D07-1091,1,0.922736,"n the following sections. (Durrani et al., 2013a) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for GermanEnglish language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic-preordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) for German-to-English systems. We used trivia tokenizer for tokenizing Hindi. The systems were tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. We used newstest 2013 for the dev experiments. For RussianEnglish pairs news-test 2012 was used for tuning and for Hindi-English pa"
W14-3309,I08-2089,1,0.83166,"edings of the Ninth Workshop on Statistical Machine Translation, pages 97–104, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics over these identifiers as an additional scoring function. The inclusion of such an additional factor is trivial given the factored model implementation (Koehn and Hoang, 2007) of Moses (Koehn et al., 2007). The n-gram model is trained in the similar way as the regular language model. We trained domain-specific language models separately and then linearly interpolated them using SRILM with weights optimized on the tuning set (Schwenk and Koehn, 2008). We also trained OSM models over cluster-ids (?). The lexically driven OSM model falls back to very small context sizes of two to three operations due to data sparsity. Learning operation sequences over cluster-ids enables us to learn richer translation and reordering patterns that can generalize better in sparse data conditions. Table 1 shows gains from adding target LM and OSM models over cluster-ids. Using word clusters was found more useful translating from English-to-*. from English Lang de cs fr ru hi 1.3 Last year, our Russian-English systems performed badly on the human evaluation. In"
W14-3309,J82-2005,0,0.208115,"Missing"
W14-3309,N07-1061,0,0.119594,"Missing"
W14-3309,E03-1076,1,0.896383,"est translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for GermanEnglish language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic-preordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) for German-to-English systems. We used trivia tokenizer for tokenizing Hindi. The systems were tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. We used newstest 2013 for the dev experiments. For RussianEnglish pairs news-test 2012 was used for tuning and for Hindi-English pairs, we divided the newsdev 2014 into two halves, used the first half for tuning and second for dev experiments. 1.1 1.2 This paper describes the University of Edinburgh’s (UEDIN) phrase-based submissions to the translation and medical translation shared tasks o"
W14-3309,P07-1108,0,0.0994085,"Missing"
W14-3309,N04-1022,0,0.220347,"d in (Durrani et al., 2013b) that we used for the Eighth Workshop on Statistical Machine Translation (Bojar et al., 2013). The notable features of these systems are described in the following section. The experiments that we carried out for this year’s translation task are described in the following sections. (Durrani et al., 2013a) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for GermanEnglish language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic-preordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) for German-to-English systems. We used trivia tokeni"
W14-3309,P10-2041,0,0.127106,"Missing"
W14-3309,P12-2059,0,0.0266614,". We added a transliteration component to our Urdu-to-Hindi system. An unsupervised transliteration model is learned from the wordalignments of Urdu-Hindi parallel data. We were able to extract around 2800 transliteration pairs. To learn a richer transliteration model, we additionally fed the interpolated phrase-table, as described above, to the transliteration miner. We were able to mine additional 21000 transliteration pairs and built a Urdu-Hindi character-based model from it. The transliteration module can be used to translate the 50K OOV words but previous research (Durrani et al., 2010; Nakov and Tiedemann, 2012) has shown that transliteration is useful for more than just translating OOV words when translating closely related language pairs. To fully capitalize on the large overlap in Hindi–Urdu vocabulary, we transliterated each word in the Urdu test-data into Hindi and produced a phrase-table with 100-best transliterations. The two synthesized (triangulated and transliterated) phrase-tables are then used along with the baseline Urdu-to-Hindi phrase-table in a log-linear model. Detailed results on Urdu-toHindi baseline and improvements obtained from Lang en de fr ru cs hi Lines (B) Tokens (B) 59.13 3"
W14-3358,P13-1126,0,0.0656518,"re learned over training sentences, they have to compare the current test sentence to the latent vector of every training instance associated with a translation unit. The highest similarity value is then used as a feature value. Instead, our model learns latent distributional representations of phrase pairs that can be directly compared to test contexts and are likely to be more robust. Because context words of a phrase pair are tied together in the distributional representations, we can use sparse priors to cluster context words associated with the same phrase pair into few topics. Recently, Chen et al. (2013) have proposed a vector space model for domain adaptation where phrase pairs are assigned vectors that are defined in terms of the training corpora. A similar vector is built for an in-domain development set and the similarity to the development set is used as a feature during translation. While their vector representations are similar to our latent topic represen3 Phrase pair topic model (PPT) Our proposed model aims to capture the relationship between phrase pairs and source words that frequently occur in the local context of a phrase pair, that is, context words occurring in the same senten"
W14-3358,D10-1113,0,0.029345,"cument. The score of the next word in a sequence is computed as the sum of the scores of both networks, but they do not consider alternative ways of combining contextual information. Most work in the WSD literature has modelled disambiguation using a limited window of context around the word to disambiguate. Cai et al. (2007), Boyd-graber and Blei (2007) and Li et al. (2010) further tried to integrate the notion of latent topics to address the sparsity problem of the lexicalised features typically used in WSD classifiers. The most closely related work in the area of sense disambiguation is by Dinu and Lapata (2010) who propose a disambiguation method for solving lexical similarity and substitution tasks. They measure word similarity in context by learning distributions over senses for each target word in the form of lower-dimensional distributional representations. Before computing word similarities, they contextualise the global sense distribution of a word using the sense distribution of words in the test context, thereby shifting the sense distribution towards the test context. We adopt a similar distributional representation, but argue that our representation does not need this disambiguation step b"
W14-3358,P12-2023,0,0.0789407,"ion model dynamically to a given test context by measuring their similarity. We show that combining information from both local and global test contexts helps to improve lexical selection and outperforms a baseline system by up to 1.15 B LEU. We test our topic-adapted model on a diverse data set containing documents from three different domains and achieve competitive performance in comparison with two supervised domain-adapted systems. 1 More recent work in SMT uses latent representations of the document context to dynamically adapt the translation model with either monolingual topic models (Eidelman et al., 2012; Hewavitharana et al., 2013) or bilingual topic models (Hasler et al., 2014), thereby allowing the translation system to disambiguate source phrases using document context. Eidelman et al. (2012) also apply a topic model to each test sentence and find that sentence context is sufficient for picking good translations, but they do not attempt to combine sentence and document level information. Sentence-level topic adaptation for SMT has also been employed by Hasler et al. (2012). Other approaches to topic adaptation for SMT include Zhao and Xing (2007) and Tam et al. (2008), both of which use a"
W14-3358,W11-1014,0,0.216649,"Missing"
W14-3358,H92-1045,0,0.547336,"Missing"
W14-3358,2011.iwslt-evaluation.18,0,0.122144,"Missing"
W14-3358,2012.iwslt-papers.17,1,0.848445,"esentations of the document context to dynamically adapt the translation model with either monolingual topic models (Eidelman et al., 2012; Hewavitharana et al., 2013) or bilingual topic models (Hasler et al., 2014), thereby allowing the translation system to disambiguate source phrases using document context. Eidelman et al. (2012) also apply a topic model to each test sentence and find that sentence context is sufficient for picking good translations, but they do not attempt to combine sentence and document level information. Sentence-level topic adaptation for SMT has also been employed by Hasler et al. (2012). Other approaches to topic adaptation for SMT include Zhao and Xing (2007) and Tam et al. (2008), both of which use adapted lexical weights. Introduction The task of lexical selection plays an important role in statistical machine translation (SMT). It strongly depends on context and is particularly difficult when the domain of a test document is unknown, for example when translating web documents from diverse sources. Selecting translations of words or phrases that preserve the sense of the source words is closely related to the field of word sense disambiguation (WSD), which has been studie"
W14-3358,W13-2201,1,0.825427,"domain. While a C C document contains 29.1 sentences on average, documents from N C and T ED are on average more than twice as long. The length of a document could have an influence on how reliable global topic information is but also on how important it is to have information from both local and global test contexts. Data Train Dev Test Data and experimental setup Our experiments were carried out on a mixed French-English data set containing the T ED corpus (Cettolo et al., 2012), parts of the News Commentary corpus (N C) and parts of the Commoncrawl corpus (C C) from the WMT13 shared task (Bojar et al., 2013) as described in Table 1. To ensure that the baseline model does not have an implicit preference for any particular domain, we selected subsets of the N C and C C corpora such that the training data contains 2.7M English words per domain. We were guided by two constraints in chosing our data set in order to simulate an environment where very diverse documents have to be translated, which is a typical scenario for web translation engines: 1) the data has document boundaries and the content of each document is assumed to be topically related, 2) there is some degree of topical variation within e"
W14-3358,E14-1035,1,0.921877,"e show that combining information from both local and global test contexts helps to improve lexical selection and outperforms a baseline system by up to 1.15 B LEU. We test our topic-adapted model on a diverse data set containing documents from three different domains and achieve competitive performance in comparison with two supervised domain-adapted systems. 1 More recent work in SMT uses latent representations of the document context to dynamically adapt the translation model with either monolingual topic models (Eidelman et al., 2012; Hewavitharana et al., 2013) or bilingual topic models (Hasler et al., 2014), thereby allowing the translation system to disambiguate source phrases using document context. Eidelman et al. (2012) also apply a topic model to each test sentence and find that sentence context is sufficient for picking good translations, but they do not attempt to combine sentence and document level information. Sentence-level topic adaptation for SMT has also been employed by Hasler et al. (2012). Other approaches to topic adaptation for SMT include Zhao and Xing (2007) and Tam et al. (2008), both of which use adapted lexical weights. Introduction The task of lexical selection plays an i"
W14-3358,D07-1109,0,0.0336596,"k on neural language models, Huang et al. (2012) combine the scores of two neural networks modelling the word embeddings of previous words in a sequence as well as those of words from the surrounding document by averaging over all word embeddings occurring in the same document. The score of the next word in a sequence is computed as the sum of the scores of both networks, but they do not consider alternative ways of combining contextual information. Most work in the WSD literature has modelled disambiguation using a limited window of context around the word to disambiguate. Cai et al. (2007), Boyd-graber and Blei (2007) and Li et al. (2010) further tried to integrate the notion of latent topics to address the sparsity problem of the lexicalised features typically used in WSD classifiers. The most closely related work in the area of sense disambiguation is by Dinu and Lapata (2010) who propose a disambiguation method for solving lexical similarity and substitution tasks. They measure word similarity in context by learning distributions over senses for each target word in the form of lower-dimensional distributional representations. Before computing word similarities, they contextualise the global sense distri"
W14-3358,P13-2122,0,0.126372,"o a given test context by measuring their similarity. We show that combining information from both local and global test contexts helps to improve lexical selection and outperforms a baseline system by up to 1.15 B LEU. We test our topic-adapted model on a diverse data set containing documents from three different domains and achieve competitive performance in comparison with two supervised domain-adapted systems. 1 More recent work in SMT uses latent representations of the document context to dynamically adapt the translation model with either monolingual topic models (Eidelman et al., 2012; Hewavitharana et al., 2013) or bilingual topic models (Hasler et al., 2014), thereby allowing the translation system to disambiguate source phrases using document context. Eidelman et al. (2012) also apply a topic model to each test sentence and find that sentence context is sufficient for picking good translations, but they do not attempt to combine sentence and document level information. Sentence-level topic adaptation for SMT has also been employed by Hasler et al. (2012). Other approaches to topic adaptation for SMT include Zhao and Xing (2007) and Tam et al. (2008), both of which use adapted lexical weights. Intro"
W14-3358,D07-1108,0,0.0816148,"Missing"
W14-3358,2007.tmi-papers.6,0,0.0678035,"hine translation (SMT). It strongly depends on context and is particularly difficult when the domain of a test document is unknown, for example when translating web documents from diverse sources. Selecting translations of words or phrases that preserve the sense of the source words is closely related to the field of word sense disambiguation (WSD), which has been studied extensively in the past. Most approaches to WSD model context at the sentence level and do not take the wider context of a word into account. Some of the ideas from the field of WSD have been adapted for machine translation (Carpuat and Wu, 2007b; Carpuat and Wu, 2007a; Chan et al., 2007). For example, Carpuat and Wu (2007a) extend word sense disambiguation to phrase sense disambiguation and In this paper, we present a topic model that learns latent distributional representations of the context of a phrase pair which can be applied to both local and global contexts at test time. We introduce similarity features that compare latent representations of phrase pair types to test contexts to disambiguate senses for improved lexical selection. We also propose different strategies for combining local and global topical context and show that"
W14-3358,D11-1125,0,0.0595404,"Missing"
W14-3358,P12-1092,0,0.020399,"orkshop on Statistical Machine Translation, pages 445–456, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics 2 Related work tations, their model has no notion of structure beyond corpus boundaries and is adapted towards a single target domain (cross-domain). Instead, our model learns the latent topical structure automatically and the translation model is adapted dynamically to each test instance. We are not aware of prior work in the field of MT that investigates combinations of local and global context. In their recent work on neural language models, Huang et al. (2012) combine the scores of two neural networks modelling the word embeddings of previous words in a sequence as well as those of words from the surrounding document by averaging over all word embeddings occurring in the same document. The score of the next word in a sequence is computed as the sum of the scores of both networks, but they do not consider alternative ways of combining contextual information. Most work in the WSD literature has modelled disambiguation using a limited window of context around the word to disambiguate. Cai et al. (2007), Boyd-graber and Blei (2007) and Li et al. (2010)"
W14-3358,D07-1007,0,0.0808304,"hine translation (SMT). It strongly depends on context and is particularly difficult when the domain of a test document is unknown, for example when translating web documents from diverse sources. Selecting translations of words or phrases that preserve the sense of the source words is closely related to the field of word sense disambiguation (WSD), which has been studied extensively in the past. Most approaches to WSD model context at the sentence level and do not take the wider context of a word into account. Some of the ideas from the field of WSD have been adapted for machine translation (Carpuat and Wu, 2007b; Carpuat and Wu, 2007a; Chan et al., 2007). For example, Carpuat and Wu (2007a) extend word sense disambiguation to phrase sense disambiguation and In this paper, we present a topic model that learns latent distributional representations of the context of a phrase pair which can be applied to both local and global contexts at test time. We introduce similarity features that compare latent representations of phrase pair types to test contexts to disambiguate senses for improved lexical selection. We also propose different strategies for combining local and global topical context and show that"
W14-3358,W09-2404,0,0.211019,"Missing"
W14-3358,2012.eamt-1.60,0,0.0933124,"are trained on the concatenation of all training data and fixed for all models. Table 2 shows the average length of a document for each domain. While a C C document contains 29.1 sentences on average, documents from N C and T ED are on average more than twice as long. The length of a document could have an influence on how reliable global topic information is but also on how important it is to have information from both local and global test contexts. Data Train Dev Test Data and experimental setup Our experiments were carried out on a mixed French-English data set containing the T ED corpus (Cettolo et al., 2012), parts of the News Commentary corpus (N C) and parts of the Commoncrawl corpus (C C) from the WMT13 shared task (Bojar et al., 2013) as described in Table 1. To ensure that the baseline model does not have an implicit preference for any particular domain, we selected subsets of the N C and C C corpora such that the training data contains 2.7M English words per domain. We were guided by two constraints in chosing our data set in order to simulate an environment where very diverse documents have to be translated, which is a typical scenario for web translation engines: 1) the data has document"
W14-3358,P07-1005,0,0.0897345,"context and is particularly difficult when the domain of a test document is unknown, for example when translating web documents from diverse sources. Selecting translations of words or phrases that preserve the sense of the source words is closely related to the field of word sense disambiguation (WSD), which has been studied extensively in the past. Most approaches to WSD model context at the sentence level and do not take the wider context of a word into account. Some of the ideas from the field of WSD have been adapted for machine translation (Carpuat and Wu, 2007b; Carpuat and Wu, 2007a; Chan et al., 2007). For example, Carpuat and Wu (2007a) extend word sense disambiguation to phrase sense disambiguation and In this paper, we present a topic model that learns latent distributional representations of the context of a phrase pair which can be applied to both local and global contexts at test time. We introduce similarity features that compare latent representations of phrase pair types to test contexts to disambiguate senses for improved lexical selection. We also propose different strategies for combining local and global topical context and show that using clues from both levels of contexts is"
W14-3358,W04-3250,1,0.65101,"results on the documents from each domain. While all topic settings yield improvements over the baseline, the largest improvement on the mixed test set (+0.48 B LEU) is achieved with 50 topics. Topic adaptation is most effective on the T ED portion of the test set where the increase in B LEU is 0.59. 7.2 Mixed -26.86 *27.15 *27.19 *27.34 *27.26 Table 3: B LEU scores of baseline system + phrSim-local feature for different numbers of topics. In this section we present experimental results of our model with different context settings and against different baselines. We used bootstrap resampling (Koehn, 2004) to measure significance on the mixed test set and marked all statistically significant results compared to the respective baselines with asterisk (*: p ≤ 0.01). 7.1 Model Baseline 10 topics 20 topics 50 topics 100 topics Model Baseline 10 topics 20 topics 50 topics 100 topics Mixed -26.86 *27.30 *27.34 *27.27 *27.24 CC 19.61 20.01 20.07 20.12 19.95 NC 29.42 29.61 29.56 29.48 29.66 T ED 31.88 32.64 32.71 32.55 32.52 &gt;Baseline +0.48 +0.51 +0.24 +0.83 Table 4: B LEU scores of baseline system + phrSim-global feature for different numbers of topics. 7.3 Relation to properties of test documents To"
W14-3358,P10-1116,0,0.0236438,"ng et al. (2012) combine the scores of two neural networks modelling the word embeddings of previous words in a sequence as well as those of words from the surrounding document by averaging over all word embeddings occurring in the same document. The score of the next word in a sequence is computed as the sum of the scores of both networks, but they do not consider alternative ways of combining contextual information. Most work in the WSD literature has modelled disambiguation using a limited window of context around the word to disambiguate. Cai et al. (2007), Boyd-graber and Blei (2007) and Li et al. (2010) further tried to integrate the notion of latent topics to address the sparsity problem of the lexicalised features typically used in WSD classifiers. The most closely related work in the area of sense disambiguation is by Dinu and Lapata (2010) who propose a disambiguation method for solving lexical similarity and substitution tasks. They measure word similarity in context by learning distributions over senses for each target word in the form of lower-dimensional distributional representations. Before computing word similarities, they contextualise the global sense distribution of a word usin"
W14-3358,E12-1055,0,0.15772,"Missing"
W15-3001,W05-0909,0,0.0473932,"asks 1 and 2 provide the same dataset with English-Spanish translations generated by the statistical machine translation (SMT) system, while Task 3 provides two different datasets, for two language pairs: English-German (EN-DE) and German-English (DE-EN) translations taken from all participating systems in WMT13 (Bojar et al., 2013). These datasets were annotated with different labels for quality: for Tasks 1 and 2, the labels were automatically derived from the post-editing of the machine translation output, while for Task 3, scores were computed based on reference translations using Meteor (Banerjee and Lavie, 2005). Any external resource, including additional quality estimation training data, could be used by participants (no distinction between open and close tracks was made). As presented in Section 4.1, participants were also provided with a baseline set of features for each task, and a software package to extract these and other quality estimation features and perform model learning, with suggested methods for all levels of prediction. Participants, described in Section 4.2, could submit up to two systems for each task. Data used to build MT systems or internal system information (such as model scor"
W15-3001,2011.mtsummit-papers.35,0,0.348896,"Missing"
W15-3001,W13-2241,1,0.851171,"Missing"
W15-3001,P13-2097,1,0.801912,"Missing"
W15-3001,W13-2242,0,0.0386206,"Missing"
W15-3001,W14-3339,0,0.0452066,"Missing"
W15-3001,W15-3035,0,0.054515,"Missing"
W15-3001,W11-2103,1,0.524514,"Missing"
W15-3001,W13-2201,1,0.499374,"Missing"
W15-3001,W14-3302,1,0.498006,"Missing"
W15-3001,W14-3340,1,0.54686,"Missing"
W15-3001,W15-3007,0,0.0166876,"Missing"
W15-3001,W15-3006,1,0.827804,"Missing"
W15-3001,W07-0718,1,0.664541,"om 24 institutions were submitted to the ten translation directions in the standard translation task. An additional 7 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had three subtasks, with a total of 10 teams, submitting 34 entries. The pilot automatic postediting task had a total of 4 teams, submitting 7 entries. 1 Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at EMNLP 2015. This workshop builds on eight previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014). This year we conducted five official tasks: a translation task, a quality estimation task, a metrics task, a tuning task1 , and a automatic postediting task. In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Finnish, and Russian. The Finnish translation 1 The metrics and tuning tasks are reported in separate papers (Stanojevi´c et al., 2015a,b)."
W15-3001,2014.amta-researchers.13,0,0.0200349,"Missing"
W15-3001,W08-0309,1,0.406319,"2. machine translation and automatic evaluation or prediction of translation quality. 2 Overview of the Translation Task The recurring task of the workshop examines translation between English and other languages. As in the previous years, the other languages include German, French, Czech and Russian. Finnish replaced Hindi as the special language this year. Finnish is a lesser resourced language compared to the other languages and has challenging morphological properties. Finnish represents also a different language family that we had not tackled since we included Hungarian in 2008 and 2009 (Callison-Burch et al., 2008, 2009). We created a test set for each language pair by translating newspaper articles and provided training data, except for French, where the test set was drawn from user-generated comments on the news articles. 2.1 2.3 We received 68 submissions from 24 institutions. The participating institutions and their entry names are listed in Table 2; each system did not necessarily appear in all translation tasks. We also included 1 commercial off-the-shelf MT system and 6 online statistical MT systems, which we anonymized. For presentation of the results, systems are treated as either constrained"
W15-3001,W15-3025,1,0.914094,"teps aimed at removing duplicates and those triplets in which any of the elements (source, target, post-edition) was either too long or too short compared to the others, or included tags or special problematic symbols. The main reason for random sampling was to induce some homogeneity across the three datasets and, in turn, Automatic Post-editing Task This year WMT hosted for the first time a shared task on automatic post-editing (APE) for machine translation. The task requires to automatically correct the errors present in a machine translated text. As pointed out in Parton et al. (2012) and Chatterjee et al. (2015b), from the application point of view, APE components would make it possible to: • Improve MT output by exploiting information unavailable to the decoder, or by per22 The original triplets were provided by Unbabel (https: //unbabel.com/). 28 in terms of Translation Error Rate (TER) (Snover et al., 2006a), an evaluation metric commonly used in MT-related tasks (e.g. in quality estimation) to measure the minimum edit distance between an automatic translation and a reference translation.23 Systems are ranked based on the average TER calculated on the test set by using the TERcom24 software: lowe"
W15-3001,W10-1703,1,0.163282,"Missing"
W15-3001,P15-2026,1,0.797338,"teps aimed at removing duplicates and those triplets in which any of the elements (source, target, post-edition) was either too long or too short compared to the others, or included tags or special problematic symbols. The main reason for random sampling was to induce some homogeneity across the three datasets and, in turn, Automatic Post-editing Task This year WMT hosted for the first time a shared task on automatic post-editing (APE) for machine translation. The task requires to automatically correct the errors present in a machine translated text. As pointed out in Parton et al. (2012) and Chatterjee et al. (2015b), from the application point of view, APE components would make it possible to: • Improve MT output by exploiting information unavailable to the decoder, or by per22 The original triplets were provided by Unbabel (https: //unbabel.com/). 28 in terms of Translation Error Rate (TER) (Snover et al., 2006a), an evaluation metric commonly used in MT-related tasks (e.g. in quality estimation) to measure the minimum edit distance between an automatic translation and a reference translation.23 Systems are ranked based on the average TER calculated on the test set by using the TERcom24 software: lowe"
W15-3001,W12-3102,1,0.571688,"ator agreement, both for inter- and intra-annotator agreement scores. not included to make the graphs viewable). The plots cleary suggest that a fair comparison of systems of different kinds cannot rely on automatic scores. Rule-based systems receive a much lower BLEU score than statistical systems (see for instance English–German, e.g., PROMT- RULE). The same is true to a lesser degree for statistical syntax-based systems (see English–German, UEDIN - SYNTAX ) and online systems that were not tuned to the shared task (see Czech–English, CU TECTO vs. the cluster of tuning task systems TT*). 4 (Callison-Burch et al., 2012; Bojar et al., 2013, 2014), with tasks including both sentence and word-level estimation, using new training and test datasets, and an additional task: document-level prediction. The goals of this year’s shared task were: • Advance work on sentence- and wordlevel quality estimation by providing larger datasets. • Investigate the effectiveness of quality labels, features and learning methods for documentlevel prediction. Quality Estimation Task • Explore differences between sentence-level and document-level prediction. The fourth edition of the WMT shared task on quality estimation (QE) of mac"
W15-3001,W15-3008,0,0.0135172,"secondary key. The results for the scoring variant are presented in Table 9, sorted from best to worst by using the MAE metric scores as primary key and the RMSE metric scores as secondary key. Pearson’s r coefficients for all systems against HTER is given in Table 10. As discussed in (Graham, 2015), the results according to this metric can rank participating systems differently. In particular, we note the SHEF/GP submission, are which is deemed significantly worse than the baseline system according to MAE, but substantially better than the baseline according to Pearson’s correlation. Graham (2015) argues that the use of MAE as evaluation score for quality estimation tasks is inadequate, as MAE is very sensitive to variance. This means that a system that outputs predictions with high variance is more likely to have high MAE score, even if the distribution follows that of the true labels. Interestingly, according to Pearson’s correlation, the systems are • Source sentence (English). • Automatic translation (Spanish). • Manual post-edition of the automatic translation. • Word-level binary (“OK”/“BAD”) labelling of the automatic translation. The binary labels for the datasets were acquired"
W15-3001,W09-0401,1,0.251315,"Missing"
W15-3001,W15-3009,0,0.0460438,"Missing"
W15-3001,W15-3010,0,0.035772,"Missing"
W15-3001,P10-4002,0,0.00861255,"t sentences and number of punctuation marks in source and target sentences. All other features were averaged. The implementation for document-level feature extraction is available in Q U E ST ++ (Specia et al., 2015).12 These features were then used to train a SVR algorithm with RBF kernel using the SCIKIT- LEARN toolkit. The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. • Source token aligned to the target token, its left and right contexts of one word. The alignments were produced with the force align.py script, which is part of cdec (Dyer et al., 2010). It allows to align new parallel data with a pre-trained alignment model built with the cdec word aligner (fast align). The alignment model was trained on the Europarl corpus (Koehn, 2005). • Boolean dictionary features: whether target token is a stopword, a punctuation mark, a proper noun, a number. 4.2 Participants Table 7 lists all participating teams submitting systems to any of the tasks. Each team was allowed up to two submissions for each task and language pair. In the descriptions below, participation in specific tasks is denoted by a task identifier. • Target language model features:"
W15-3001,W15-3011,0,0.0373266,"Missing"
W15-3001,W15-3036,0,0.0738999,"Missing"
W15-3001,W15-3012,0,0.0168435,"secondary key. The results for the scoring variant are presented in Table 9, sorted from best to worst by using the MAE metric scores as primary key and the RMSE metric scores as secondary key. Pearson’s r coefficients for all systems against HTER is given in Table 10. As discussed in (Graham, 2015), the results according to this metric can rank participating systems differently. In particular, we note the SHEF/GP submission, are which is deemed significantly worse than the baseline system according to MAE, but substantially better than the baseline according to Pearson’s correlation. Graham (2015) argues that the use of MAE as evaluation score for quality estimation tasks is inadequate, as MAE is very sensitive to variance. This means that a system that outputs predictions with high variance is more likely to have high MAE score, even if the distribution follows that of the true labels. Interestingly, according to Pearson’s correlation, the systems are • Source sentence (English). • Automatic translation (Spanish). • Manual post-edition of the automatic translation. • Word-level binary (“OK”/“BAD”) labelling of the automatic translation. The binary labels for the datasets were acquired"
W15-3001,W15-4903,0,0.0623526,"Missing"
W15-3001,W15-3013,1,0.843414,"Missing"
W15-3001,W11-2123,0,0.0240468,"27,101 5,966 8,816 SRC 13,701 3,765 5,307 Lemmas TGT PE 7,624 7,689 2,810 2,819 3,778 3,814 Table 18: Data statistics. and classifying each word of a sentence as good or bad. An automatic translation to be post-edited is first decoded by our SPE system, then fed into one of the classifiers identified as SVM180feat and RNN. The HTER estimator selects the translation with the lower score while the binary word-level classifier selects the translation with the fewer amount of bad tags. The official evaluation of the shared task show an advantage of the RNN approach compared to SVM. KenLM toolkit (Heafield, 2011) for standard ngram modeling with an n-gram length of 5. Finally, the APE system was tuned on the development set, optimizing TER with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison, computed for both evaluation modalities (case sensitive/insensitive), are also reported in Tables 20 and 21. For each submitted run, the statistical significance of performance differences with respect to the baseline and the re-implementation of Simard et al. (2007) is calculated with the bootstrap test (Koehn, 2004). 5.2 FBK. The two runs submitted by FBK (Chatterjee e"
W15-3001,W08-0509,0,0.0268564,"ere collected by means of a crowdsourcing platform developed by the data provider. Monolingual translation as another term of comparison. To get further insights about the progress with respect to previous APE methods, participants’ results are also analysed with respect to another term of comparison: a reimplementation of the state-of-the-art approach firstly proposed by Simard et al. (2007).27 For this purpose, a phrase-based SMT system based on Moses (Koehn et al., 2007) is used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the Test data (1, 817 instances) consists of (source, target) pairs having similar characteristics of those in the training set. Human post-editions of the test target instances were left apart to measure system performance. The data creation procedure adopted, as well as the origin and the domain of the texts pose specific challenges to the participating systems. As discussed in Section 5.4, the results of this pilot task can be partially explained in light of such challenges. This dataset, however, has three major advantages that made it sui"
W15-3001,W15-3014,0,0.0344469,"Missing"
W15-3001,P15-1174,0,0.0105686,"415 for test. Since no human annotation exists for the quality of entire paragraphs (or documents), Meteor against reference translations was used as quality label for this task. Meteor was calculated using its implementation within the Asyia toolkit, with the following settings: exact match, tokenised and case insensitive (Gim´enez and M`arquez, 2010). guage pairs. All systems were significantly better than the baseline. However, the difference between the baseline system and all submissions was much lower in the scoring evaluation than in the ranking evaluation. Following the suggestion in (Graham, 2015), Table 16 shows an alternative ranking of systems considering Pearson’s r correlation results. The alternative ranking differs from the official ranking in terms of MAE: for EN-DE, RTMDCU/RTM-FS-SVR is no longer in the winning group, while for DE-EN, USHEF/QUEST-DISCBO and USAAR-USHEF/BFF did not show statistically significant difference against the baseline. However, as with Task 1 these results are the same as the official ones in terms of DeltaAvg. 4.6 Discussion In what follows, we discuss the main findings of this year’s shared task based on the goals we had previously identified for it."
W15-3001,W04-3250,1,0.485885,"f the RNN approach compared to SVM. KenLM toolkit (Heafield, 2011) for standard ngram modeling with an n-gram length of 5. Finally, the APE system was tuned on the development set, optimizing TER with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison, computed for both evaluation modalities (case sensitive/insensitive), are also reported in Tables 20 and 21. For each submitted run, the statistical significance of performance differences with respect to the baseline and the re-implementation of Simard et al. (2007) is calculated with the bootstrap test (Koehn, 2004). 5.2 FBK. The two runs submitted by FBK (Chatterjee et al., 2015a) are based on combining the statistical phrase-based post-editing approach proposed by Simard et al. (2007) and its most significant variant proposed by B´echara et al. (2011). The APE systems are built-in an incremental manner. At each stage of the APE pipeline, the best configuration of a component is decided and then used in the next stage. The APE pipeline begins with the selection of the best language model from several language models trained on different types and quantities of data. The next stage addresses the possible"
W15-3001,2005.mtsummit-papers.11,1,0.0759255,"(Specia et al., 2015).12 These features were then used to train a SVR algorithm with RBF kernel using the SCIKIT- LEARN toolkit. The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. • Source token aligned to the target token, its left and right contexts of one word. The alignments were produced with the force align.py script, which is part of cdec (Dyer et al., 2010). It allows to align new parallel data with a pre-trained alignment model built with the cdec word aligner (fast align). The alignment model was trained on the Europarl corpus (Koehn, 2005). • Boolean dictionary features: whether target token is a stopword, a punctuation mark, a proper noun, a number. 4.2 Participants Table 7 lists all participating teams submitting systems to any of the tasks. Each team was allowed up to two submissions for each task and language pair. In the descriptions below, participation in specific tasks is denoted by a task identifier. • Target language model features: – The order of the highest order n-gram which starts or ends with the target token. – Backoff behaviour of the n-grams (ti−2 , ti−1 , ti ), (ti−1 , ti , ti+1 ), (ti , ti+1 , ti+2 ), where"
W15-3001,W15-3039,1,0.80659,"Missing"
W15-3001,J10-4005,0,0.0619684,"Missing"
W15-3001,J82-2005,0,0.818658,"Missing"
W15-3001,W14-3342,0,0.0353204,"t although the system is referred to as “baseline”, it is in fact a strong system. It has proved robust across a range of language pairs, MT systems, and text domains for predicting various forms of post-editing effort (Callison-Burch et al., 2012; Bojar et al., 2013, 2014). Word-level baseline system: For Task 2, the baseline features were extracted with the M AR MOT tool9 . For the baseline system we used a number of features that have been found the most informative in previous research on word-level quality estimation. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 25 features: Baseline systems Sentence-level baseline system: For Task 1, Q U E ST7 (Specia et al., 2013) was used to extract 17 MT system-independent features from the source and translation (target) files and parallel corpora: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), but the length of a sentence might influence the probability of a word being incorrect. • Number of tokens in the source and target sentences. • Ave"
W15-3001,W13-2248,0,0.0824189,"Missing"
W15-3001,W06-3114,1,0.427665,"translation systems from 24 institutions were submitted to the ten translation directions in the standard translation task. An additional 7 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had three subtasks, with a total of 10 teams, submitting 34 entries. The pilot automatic postediting task had a total of 4 teams, submitting 7 entries. 1 Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at EMNLP 2015. This workshop builds on eight previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014). This year we conducted five official tasks: a translation task, a quality estimation task, a metrics task, a tuning task1 , and a automatic postediting task. In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Finnish, and Russian. The Finnish translation 1 The metrics and tuning tasks are reported in separate papers (S"
W15-3001,W15-3015,0,0.0443946,"Missing"
W15-3001,W15-3016,0,0.0442638,"Missing"
W15-3001,W15-3037,0,0.0800739,"Missing"
W15-3001,P03-1021,0,0.0587969,"utomatic translation to be post-edited is first decoded by our SPE system, then fed into one of the classifiers identified as SVM180feat and RNN. The HTER estimator selects the translation with the lower score while the binary word-level classifier selects the translation with the fewer amount of bad tags. The official evaluation of the shared task show an advantage of the RNN approach compared to SVM. KenLM toolkit (Heafield, 2011) for standard ngram modeling with an n-gram length of 5. Finally, the APE system was tuned on the development set, optimizing TER with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison, computed for both evaluation modalities (case sensitive/insensitive), are also reported in Tables 20 and 21. For each submitted run, the statistical significance of performance differences with respect to the baseline and the re-implementation of Simard et al. (2007) is calculated with the bootstrap test (Koehn, 2004). 5.2 FBK. The two runs submitted by FBK (Chatterjee et al., 2015a) are based on combining the statistical phrase-based post-editing approach proposed by Simard et al. (2007) and its most significant variant proposed by B´echara"
W15-3001,padro-stanilovsky-2012-freeling,0,0.011024,"o rankings are not identical, none of the systems was particularly penalized by the case sensitive evaluation. Indeed, individual differences in the two modes are always close to the same value (∼ 0.7 TER difference) measured for the two baselines. USAAR-SAPE. The USAAR-SAPE system (Pal et al., 2015b) is designed with three basic components: corpus preprocessing, hybrid word alignment and a state-of-the-art phrase-based SMT system integrated with the hybrid word alignment. The preprocessing of the training corpus is carried out by stemming the Spanish MT output and the PE data using Freeling (Padr and Stanilovsky, 2012). The hybrid word alignment method combines different kinds of word alignment: GIZA++ word alignment with the 31 ID Baseline FBK Primary LIMSI Primary USAAR-SAPE LIMSI Contrastive Abu-MaTran Primary FBK Contrastive (Simard et al., 2007) Abu-MaTran Contrastive Avg. TER 22.913 23.228 23.331 23.426 23.573 23.639 23.649 23.839 24.715 ID Baseline LIMSI Primary FBK Primary USAAR-SAPE Abu-MaTran Primary LIMSI Contrastive FBK Contrastive (Simard et al., 2007) Abu-MaTran Contrastive Table 20: Official results for the WMT15 Automatic Post-editing task – average TER (↓) case sensitive. Table 21: Official"
W15-3001,W15-3038,0,0.0668224,"Missing"
W15-3001,W13-2814,0,0.0155219,"ject (Prompsit) Fondazione Bruno Kessler, Italy (Chatterjee et al., 2015a) Laboratoire d’Informatique pour la M´ecanique et les Sciences de l’Ing´enieur, France (Wisniewski et al., 2015) Saarland University, Germany & Jadavpur University, India (Pal et al., 2015b) Table 19: Participants in the WMT15 Automatic Post-editing pilot task. grow-diag-final-and (GDFA) heuristic (Koehn, 2010), SymGiza++ (Junczys-Dowmunt and Szal, 2011), the Berkeley aligner (Liang et al., 2006), and the edit distance-based aligners (Snover et al., 2006a; Lavie and Agarwal, 2007). These different word alignment tables (Pal et al., 2013) are combined by a mathematical union method. For the phrase-based SMT system various maximum phrase lengths for the translation model and n–gram settings for the language model are used. The best results in terms of BLEU (Papineni et al., 2002) score are achieved by a maximum phrase length of 7 for the translation model and a 5-gram language model. the model. These features measure the similarity and the reliability of the translation options and help to improve the precision of the resulting APE system. LIMSI. For the first edition of the APE shared task LIMSI submitted two systems (Wisniews"
W15-3001,W07-0734,0,0.0277649,"Abu-MaTran FBK LIMSI USAAR-SAPE Participating team Abu-MaTran Project (Prompsit) Fondazione Bruno Kessler, Italy (Chatterjee et al., 2015a) Laboratoire d’Informatique pour la M´ecanique et les Sciences de l’Ing´enieur, France (Wisniewski et al., 2015) Saarland University, Germany & Jadavpur University, India (Pal et al., 2015b) Table 19: Participants in the WMT15 Automatic Post-editing pilot task. grow-diag-final-and (GDFA) heuristic (Koehn, 2010), SymGiza++ (Junczys-Dowmunt and Szal, 2011), the Berkeley aligner (Liang et al., 2006), and the edit distance-based aligners (Snover et al., 2006a; Lavie and Agarwal, 2007). These different word alignment tables (Pal et al., 2013) are combined by a mathematical union method. For the phrase-based SMT system various maximum phrase lengths for the translation model and n–gram settings for the language model are used. The best results in terms of BLEU (Papineni et al., 2002) score are achieved by a maximum phrase length of 7 for the translation model and a 5-gram language model. the model. These features measure the similarity and the reliability of the translation options and help to improve the precision of the resulting APE system. LIMSI. For the first edition of"
W15-3001,W15-3017,0,0.0328586,"Missing"
W15-3001,W15-3026,0,0.0488223,"Missing"
W15-3001,W15-3040,1,0.889327,"HIDDEN Participating team Dublin City University, Ireland and University of Sheffield, UK (Logacheva et al., 2015) Heidelberg University, Germany (Kreutzer et al., 2015) Lorraine Laboratory of Research in Computer Science and its Applications, France (Langlois, 2015) Dublin City University, Ireland (Bicici et al., 2015) Shenyang Aerospace University, China (Shang et al., 2015) University of Sheffield Team 1, UK (Shah et al., 2015) Alicant University, Spain (Espl`a-Gomis et al., 2015a) Ghent University, Belgium (Tezcan et al., 2015) University of Sheffield, UK and Saarland University, Germany (Scarton et al., 2015a) University of Sheffield, UK (Scarton et al., 2015a) Undisclosed Table 7: Participants in the WMT15 quality estimation shared task. one from the official training data. Pseudoreferences were produced by three online systems. These features measure the intersection between n-gram sets of the target sentence and of the pseudo-references. Three sets of features were extracted from each online system, and a fourth feature was extracted measuring the inter-agreement among the three online systems and the target system. and fine-tuned for the quality estimation classification task by back-propagat"
W15-3001,W15-4916,1,0.80858,"Missing"
W15-3001,P02-1040,0,0.108957,"ndia (Pal et al., 2015b) Table 19: Participants in the WMT15 Automatic Post-editing pilot task. grow-diag-final-and (GDFA) heuristic (Koehn, 2010), SymGiza++ (Junczys-Dowmunt and Szal, 2011), the Berkeley aligner (Liang et al., 2006), and the edit distance-based aligners (Snover et al., 2006a; Lavie and Agarwal, 2007). These different word alignment tables (Pal et al., 2013) are combined by a mathematical union method. For the phrase-based SMT system various maximum phrase lengths for the translation model and n–gram settings for the language model are used. The best results in terms of BLEU (Papineni et al., 2002) score are achieved by a maximum phrase length of 7 for the translation model and a 5-gram language model. the model. These features measure the similarity and the reliability of the translation options and help to improve the precision of the resulting APE system. LIMSI. For the first edition of the APE shared task LIMSI submitted two systems (Wisniewski et al., 2015). The first one is based on the approach of Simard et al. (2007) and considers the APE task as a monolingual translation between a translation hypothesis and its post-edition. This straightforward approach does not succeed in imp"
W15-3001,2012.eamt-1.34,0,0.0799883,"Missing"
W15-3001,W15-3023,0,0.0267499,"Missing"
W15-3001,W15-3018,0,0.0412931,"Missing"
W15-3001,W15-3041,1,0.847113,"Missing"
W15-3001,potet-etal-2012-collection,0,0.011237,"Missing"
W15-3001,W15-3042,0,0.0858502,"Missing"
W15-3001,W15-3019,0,0.0362545,"Missing"
W15-3001,N07-1064,0,0.644928,"nsitive) are reported in Tables 20 and 21. • The target (TGT) is a tokenized Spanish translation of the source, produced by an unknown MT system; • The human post-edition (PE) is a manuallyrevised version of the target. PEs were collected by means of a crowdsourcing platform developed by the data provider. Monolingual translation as another term of comparison. To get further insights about the progress with respect to previous APE methods, participants’ results are also analysed with respect to another term of comparison: a reimplementation of the state-of-the-art approach firstly proposed by Simard et al. (2007).27 For this purpose, a phrase-based SMT system based on Moses (Koehn et al., 2007) is used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the Test data (1, 817 instances) consists of (source, target) pairs having similar characteristics of those in the training set. Human post-editions of the test target instances were left apart to measure system performance. The data creation procedure adopted, as well as the origin and the domain of the texts pose specifi"
W15-3001,W15-3022,0,0.0629217,"Missing"
W15-3001,P15-4020,1,0.0689693,"://github.com/lspecia/quest 14 http://scikit-learn.org/ https://github.com/qe-team/marmot • Target token, its left and right contexts of one word. Document-level baseline system: For Task 3, the baseline features for sentence-level prediction were used. These are aggregated by summing or averaging their values for the entire document. Features that were summed: number of tokens in the source and target sentences and number of punctuation marks in source and target sentences. All other features were averaged. The implementation for document-level feature extraction is available in Q U E ST ++ (Specia et al., 2015).12 These features were then used to train a SVR algorithm with RBF kernel using the SCIKIT- LEARN toolkit. The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. • Source token aligned to the target token, its left and right contexts of one word. The alignments were produced with the force align.py script, which is part of cdec (Dyer et al., 2010). It allows to align new parallel data with a pre-trained alignment model built with the cdec word aligner (fast align). The alignment model was trained on the Europarl corpus (Koehn, 2005). • Boole"
W15-3001,W15-3027,0,0.0625354,"Missing"
W15-3001,P13-4014,1,0.762876,"Missing"
W15-3001,2013.mtsummit-papers.15,0,0.055239,"these rules is based on an analysis of the most frequent error corrections and aims at: i) predicting word case; ii) predicting exclamation and interrogation marks; and iii) predicting verbal endings. Experiments with this approach show that this system also hurts translation quality. An in-depth analysis revealed that this negative result is mainly explained by two reasons: i) most of the post-edition operations are nearly unique, which makes very difficult to generalize from a small amount of data; and ii) even when they are not, the high variability of post-editing, already pointed out by Wisniewski et al. (2013), results in predicting legitimate corrections that have not been made by the annotators, therefore preventing from improving over the baseline. 5.3 Results The official results achieved by the participating systems are reported in Tables 20 and 21. The seven runs submitted are sorted based on the average TER they achieve on test data. Table 20 shows the results computed in case sensitive mode, while Table 21 provides scores computed in the case insensitive mode. Both rankings reveal an unexpected outcome: none of the submitted runs was able to beat the baselines (i.e. average TER scores of 22"
W15-3001,W15-3032,1,0.810291,"Missing"
W15-3001,W15-3031,1,0.376914,"Missing"
W15-3001,W15-3020,1,0.808362,"Missing"
W15-3001,W15-3043,0,0.0443554,"Missing"
W15-3001,W15-3021,0,0.038175,"Missing"
W15-3001,P07-2045,1,\N,Missing
W15-3001,W15-3004,0,\N,Missing
W15-3001,2015.eamt-1.4,0,\N,Missing
W15-3001,N06-1014,0,\N,Missing
W15-3001,2015.eamt-1.17,1,\N,Missing
W15-3001,2012.iwslt-papers.12,0,\N,Missing
W15-3013,D14-1132,0,0.0417768,"igated sparse lexicalized reordering features (Section 2.4) on the German-English language pair in both translation directions. Two methods for learning the weights of the sparse lexicalized reordering feature set have been compared: (1.) direct tuning in MIRA along with all other features in the model combination (sparse LR (MIRA)), and (2.) separate optimization with stochastic gradient descent (SGD) with a maximum expected B LEU objective (sparse LR (SGD)). For the latter variant, we used the MT tuning set for training (13 573 sentence pairs) and otherwise followed the approach outlined by Auli et al. (2014). We tuned the baseline feature weights with MIRA before SGD training and ran two final MIRA iterations after it. SGD training was stopped after 80 epochs. Empirical results for the German-English language pair are presented in Table 5. We observe minor gains of up to +0.2 points B LEU. The results are not consistent in the two translation directions: The MIRA-trained variant seems to perform better when translating from German, the SGD-trained variant when translating to German. However, in both cases the baseline score is almost identical to the best results with sparse lexicalized reorderin"
W15-3013,D11-1033,0,0.0915467,"Missing"
W15-3013,N12-1047,0,0.185989,"Missing"
W15-3013,N13-1003,0,0.0324335,"a to train 5gram language models with modified Kneser-Ney smoothing (Chen and Goodman, 1998). Typically, language models for each monolingual corpus were first trained using either KenLM (Heafield et al., 2013) or the SRILM toolkit (Stolcke, 2002) and then linearly interpolated using weights tuned to minimize perplexity on the development set. 3.4 Sparse Lexicalized Reordering Baseline Features We follow the standard approach to SMT of scoring translation hypotheses using a weighted linear combination of features. The core features of our We implemented sparse lexicalized reordering features (Cherry, 2013) in Moses and evaluated 127 Baseline (no clusters) Comprehensive setup w/o sparse features w/o language model w/o reordering model w/o operation sequence model de-en 28.0 28.5 (+.5) 28.2 (–.3) 28.3 (–.2) 28.5 (±.0) 28.3 (–.2) en-de 20.5 20.5 (±.0) 20.4 (–.1) 20.5 (±.0) 20.5 (±.0) 20.3 (–.1) cs-en 29.1 29.7 (+.6) 29.6 (–.1) 29.5 (–.2) 29.7 (±.0) en-cs 21.2 21.8 (+.6) 21.7 (–.1) 21.4 (–.4) 21.8 (±.0) 21.7 (–.1) ru-en 31.8 32.3 (+.5) 32.2 (–.1) 31.5 (–.8) 32.3 (±.0) 32.0 (–.3) en-ru 29.1 29.7 (+.6) 30.0 (+.3) 29.2 (–.6) 29.8 (+.1) 29.5 (–.2) avg ∆ +.5 –.2 –.4 ±.0 –.2 Table 1: Use of additional fe"
W15-3013,P05-1066,1,0.679903,"the OPUS (Tiedemann, 2012) par129 4.3 System Baseline Submitted BiLM source & combined & NPLM Czech↔English The development of the Czech↔English systems followed the ideas in Section 2.3, i.e., with a focus on word classes (50, 200, 600 classes) for all component models. We combined the test sets from 2008 to 2012 for tuning. No neural language model or bilingual language model was used. 4.4 Table 4: Experimental results (cased B LEU) for English→Russian averaged over newstest2013 and newstest2014. Russian↔English From German. For translation from German, we applied syntactic pre-reordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) in a preprocessing step on the source side. A rich set of translation factors was exploited in addition to word surface forms: Och clusters (50 classes), morphological tags, partof-speech tags, and word stems on the German side (Schmid, 2000), as well as Och clusters (50 classes), part-of-speech tags (Ratnaparkhi, 1996), and word stems (Porter, 1980) on the English side. The factors were utilized in the translation model and in OSMs. The lexicalized reordering model was trained on stems. Individual 7gram Och cluster LMs were trained with KenLM’s"
W15-3013,2014.iwslt-evaluation.7,1,0.858337,"mission we tested bilingual LMs on the French↔English tasks and on English→Russian task. For French↔English, we had resource issues4 in training such large Introduction The Edinburgh/JHU phrase-based translation systems for our participation in the WMT 2015 shared translation task1 are based on the open source Moses toolkit (Koehn et al., 2007). We built upon Edinburgh’s strong baselines from WMT submissions in previous years (Durrani et al., 2014a) as well as our recent research within the framework of other evaluation campaigns and projects such as IWSLT2 and EU-BRIDGE3 (Birch et al., 2014; Freitag et al., 2014a; Freitag et al., 2014b). We first discuss novel features that we integrated into our systems for the 2015 Edinburgh/JHU submission. Next we give a general system overview with details on our training pipeline and decoder configuration. We finally present empirical results for the individual language pairs and translation directions. 1 http://www.statmt.org/wmt15/ http://workshop2014.iwslt.org 3 http://www.eu-bridge.eu 4 These can now be addressed using the -mmap option to create a binarized version of the corpus which is then memory-mapped. 2 126 Proceedings of the Tenth Workshop on Statisti"
W15-3013,P14-1129,0,0.0621031,"Missing"
W15-3013,P13-2071,1,0.880126,"-based feature functions are used in all cases. B LEU scores on newstest2014 are reported. 4 model are a 5-gram LM score, phrase translation and lexical translation scores, word and phrase penalties, and a linear distortion score. The phrase translation probabilities are smoothed with GoodTuring smoothing (Foster et al., 2006). We used the hierarchical lexicalized reordering model (Galley and Manning, 2008) with 4 possible orientations (monotone, swap, discontinuous left and discontinuous right) in both left-to-right and rightto-left direction. We also used the operation sequence model (OSM) (Durrani et al., 2013) with 4 count based supportive features. We further employed domain indicator features (marking which training corpus each phrase pair was found in), binary phrase count indicator features, sparse phrase length features, and sparse source word deletion, target word insertion, and word translation features (limited to the top K words in each language, typically with K = 50). 3.5 In this section we describe peculiarities of individual systems and present experimental results. 4.1 French↔English Our submitted systems for the French-English language pair are quite similar for the two translation d"
W15-3013,D08-1089,0,0.29008,"ature functions based on Och clusters (see Section 2.3). The last four lines refer to ablation studies where one of the sets of clustered feature functions is removed from the comprehensive setup. Note that the word-based feature functions are used in all cases. B LEU scores on newstest2014 are reported. 4 model are a 5-gram LM score, phrase translation and lexical translation scores, word and phrase penalties, and a linear distortion score. The phrase translation probabilities are smoothed with GoodTuring smoothing (Foster et al., 2006). We used the hierarchical lexicalized reordering model (Galley and Manning, 2008) with 4 possible orientations (monotone, swap, discontinuous left and discontinuous right) in both left-to-right and rightto-left direction. We also used the operation sequence model (OSM) (Durrani et al., 2013) with 4 count based supportive features. We further employed domain indicator features (marking which training corpus each phrase pair was found in), binary phrase count indicator features, sparse phrase length features, and sparse source word deletion, target word insertion, and word translation features (limited to the top K words in each language, typically with K = 50). 3.5 In this"
W15-3013,W08-0509,0,0.0395025,"morphological tags, and basically no additional gains were observed due to the class based feature functions. 2.4 3 3.1 System Overview Preprocessing The training data was preprocessed using scripts from the Moses toolkit. We first normalized the data using the normalize-punctuation.perl script, then performed tokenization (using the -a option), and then truecasing. We did not perform any corpus filtering other than the standard Moses method, which removes sentence pairs with extreme length ratios. 3.2 Word Alignment For word alignment we used either fast_align (Dyer et al., 2013) or MGIZA++ (Gao and Vogel, 2008), followed by the standard grow-diag-final-and symmetrization heuristic. An empirical comparison of fast_align and MGIZA++ on the FinnishEnglish and English-Russian language pairs using the constrained data sets did not reveal any significant difference. 3.3 Language Model We used all available monolingual data to train 5gram language models with modified Kneser-Ney smoothing (Chen and Goodman, 1998). Typically, language models for each monolingual corpus were first trained using either KenLM (Heafield et al., 2013) or the SRILM toolkit (Stolcke, 2002) and then linearly interpolated using weig"
W15-3013,W14-3309,1,0.921919,"at test time the alignment is supplied by the decoder. The bilingual LM is trained using a feedforward neural network and we use the NPLM toolkit for this. Prior to submission we tested bilingual LMs on the French↔English tasks and on English→Russian task. For French↔English, we had resource issues4 in training such large Introduction The Edinburgh/JHU phrase-based translation systems for our participation in the WMT 2015 shared translation task1 are based on the open source Moses toolkit (Koehn et al., 2007). We built upon Edinburgh’s strong baselines from WMT submissions in previous years (Durrani et al., 2014a) as well as our recent research within the framework of other evaluation campaigns and projects such as IWSLT2 and EU-BRIDGE3 (Birch et al., 2014; Freitag et al., 2014a; Freitag et al., 2014b). We first discuss novel features that we integrated into our systems for the 2015 Edinburgh/JHU submission. Next we give a general system overview with details on our training pipeline and decoder configuration. We finally present empirical results for the individual language pairs and translation directions. 1 http://www.statmt.org/wmt15/ http://workshop2014.iwslt.org 3 http://www.eu-bridge.eu 4 These"
W15-3013,P13-2121,1,0.902744,"Missing"
W15-3013,P07-1019,0,0.0483912,"ed language models, we tested with 50 Och clusters, 200 Och clusters, and with both class-based LMs. For the bilingual LM, we created both “combined” (a 5-gram on the target and a 9-gram on the source) and “source” (1-gram on the target and 15-gram on Tuning Since our feature set (generally around 500 to 1000 features) was too large for MERT, we used k-best batch MIRA for tuning (Cherry and Foster, 2012). To speed up tuning we applied threshold pruning to the phrase table, based on the direct translation model probability. 3.6 Experimental Results Decoding In decoding we applied cube pruning (Huang and Chiang, 2007) with a stack size of 5000 (reduced to 1000 for tuning), Minimum Bayes Risk decoding (Kumar and Byrne, 2004), a maximum phrase length of 5, a distortion limit of 6, 100best translation options and the no-reorderingover-punctuation heuristic (Koehn and Haddow, 2009). 128 System Baseline Submitted 50 classes 200 classes 50+200 classes BiLM combined BiLM source & combined NPLM fr-en 33.0 32.7 32.8 32.9 32.9 32.9 33.2 33.0 System Baseline Submitted Without OPUS 50 classes 200 classes 50+200 classes BiLM combined BiLM source & combined NPLM en-fr 33.5 33.6 33.8 33.9 33.7 33.6 33.5 34.2 Table 2: Com"
W15-3013,E14-4029,1,0.939408,"at test time the alignment is supplied by the decoder. The bilingual LM is trained using a feedforward neural network and we use the NPLM toolkit for this. Prior to submission we tested bilingual LMs on the French↔English tasks and on English→Russian task. For French↔English, we had resource issues4 in training such large Introduction The Edinburgh/JHU phrase-based translation systems for our participation in the WMT 2015 shared translation task1 are based on the open source Moses toolkit (Koehn et al., 2007). We built upon Edinburgh’s strong baselines from WMT submissions in previous years (Durrani et al., 2014a) as well as our recent research within the framework of other evaluation campaigns and projects such as IWSLT2 and EU-BRIDGE3 (Birch et al., 2014; Freitag et al., 2014a; Freitag et al., 2014b). We first discuss novel features that we integrated into our systems for the 2015 Edinburgh/JHU submission. Next we give a general system overview with details on our training pipeline and decoder configuration. We finally present empirical results for the individual language pairs and translation directions. 1 http://www.statmt.org/wmt15/ http://workshop2014.iwslt.org 3 http://www.eu-bridge.eu 4 These"
W15-3013,N13-1073,0,0.0525479,"ure functions based on POS and morphological tags, and basically no additional gains were observed due to the class based feature functions. 2.4 3 3.1 System Overview Preprocessing The training data was preprocessed using scripts from the Moses toolkit. We first normalized the data using the normalize-punctuation.perl script, then performed tokenization (using the -a option), and then truecasing. We did not perform any corpus filtering other than the standard Moses method, which removes sentence pairs with extreme length ratios. 3.2 Word Alignment For word alignment we used either fast_align (Dyer et al., 2013) or MGIZA++ (Gao and Vogel, 2008), followed by the standard grow-diag-final-and symmetrization heuristic. An empirical comparison of fast_align and MGIZA++ on the FinnishEnglish and English-Russian language pairs using the constrained data sets did not reveal any significant difference. 3.3 Language Model We used all available monolingual data to train 5gram language models with modified Kneser-Ney smoothing (Chen and Goodman, 1998). Typically, language models for each monolingual corpus were first trained using either KenLM (Heafield et al., 2013) or the SRILM toolkit (Stolcke, 2002) and then"
W15-3013,E03-1076,1,0.79605,"Baseline Submitted BiLM source & combined & NPLM Czech↔English The development of the Czech↔English systems followed the ideas in Section 2.3, i.e., with a focus on word classes (50, 200, 600 classes) for all component models. We combined the test sets from 2008 to 2012 for tuning. No neural language model or bilingual language model was used. 4.4 Table 4: Experimental results (cased B LEU) for English→Russian averaged over newstest2013 and newstest2014. Russian↔English From German. For translation from German, we applied syntactic pre-reordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) in a preprocessing step on the source side. A rich set of translation factors was exploited in addition to word surface forms: Och clusters (50 classes), morphological tags, partof-speech tags, and word stems on the German side (Schmid, 2000), as well as Och clusters (50 classes), part-of-speech tags (Ratnaparkhi, 1996), and word stems (Porter, 1980) on the English side. The factors were utilized in the translation model and in OSMs. The lexicalized reordering model was trained on stems. Individual 7gram Och cluster LMs were trained with KenLM’s --discount_fallback --prune '0 0 1' parameters,"
W15-3013,P07-2045,1,0.00919205,"gned source token. At training time, the aligned source token is found from the automatic alignment, and at test time the alignment is supplied by the decoder. The bilingual LM is trained using a feedforward neural network and we use the NPLM toolkit for this. Prior to submission we tested bilingual LMs on the French↔English tasks and on English→Russian task. For French↔English, we had resource issues4 in training such large Introduction The Edinburgh/JHU phrase-based translation systems for our participation in the WMT 2015 shared translation task1 are based on the open source Moses toolkit (Koehn et al., 2007). We built upon Edinburgh’s strong baselines from WMT submissions in previous years (Durrani et al., 2014a) as well as our recent research within the framework of other evaluation campaigns and projects such as IWSLT2 and EU-BRIDGE3 (Birch et al., 2014; Freitag et al., 2014a; Freitag et al., 2014b). We first discuss novel features that we integrated into our systems for the 2015 Edinburgh/JHU submission. Next we give a general system overview with details on our training pipeline and decoder configuration. We finally present empirical results for the individual language pairs and translation d"
W15-3013,W06-1607,0,0.204091,"29.8 (+.1) 29.5 (–.2) avg ∆ +.5 –.2 –.4 ±.0 –.2 Table 1: Use of additional feature functions based on Och clusters (see Section 2.3). The last four lines refer to ablation studies where one of the sets of clustered feature functions is removed from the comprehensive setup. Note that the word-based feature functions are used in all cases. B LEU scores on newstest2014 are reported. 4 model are a 5-gram LM score, phrase translation and lexical translation scores, word and phrase penalties, and a linear distortion score. The phrase translation probabilities are smoothed with GoodTuring smoothing (Foster et al., 2006). We used the hierarchical lexicalized reordering model (Galley and Manning, 2008) with 4 possible orientations (monotone, swap, discontinuous left and discontinuous right) in both left-to-right and rightto-left direction. We also used the operation sequence model (OSM) (Durrani et al., 2013) with 4 count based supportive features. We further employed domain indicator features (marking which training corpus each phrase pair was found in), binary phrase count indicator features, sparse phrase length features, and sparse source word deletion, target word insertion, and word translation features"
W15-3013,W14-3310,1,0.838548,"mission we tested bilingual LMs on the French↔English tasks and on English→Russian task. For French↔English, we had resource issues4 in training such large Introduction The Edinburgh/JHU phrase-based translation systems for our participation in the WMT 2015 shared translation task1 are based on the open source Moses toolkit (Koehn et al., 2007). We built upon Edinburgh’s strong baselines from WMT submissions in previous years (Durrani et al., 2014a) as well as our recent research within the framework of other evaluation campaigns and projects such as IWSLT2 and EU-BRIDGE3 (Birch et al., 2014; Freitag et al., 2014a; Freitag et al., 2014b). We first discuss novel features that we integrated into our systems for the 2015 Edinburgh/JHU submission. Next we give a general system overview with details on our training pipeline and decoder configuration. We finally present empirical results for the individual language pairs and translation directions. 1 http://www.statmt.org/wmt15/ http://workshop2014.iwslt.org 3 http://www.eu-bridge.eu 4 These can now be addressed using the -mmap option to create a binarized version of the corpus which is then memory-mapped. 2 126 Proceedings of the Tenth Workshop on Statisti"
W15-3013,P10-2041,0,0.0404632,"Missing"
W15-3013,E99-1010,0,0.349917,"Missing"
W15-3013,2014.amta-researchers.3,0,0.061645,"rved a small improvement in translation performance. 2.3 Comprehensive Use of Word Classes In Edinburgh’s submission from the previous year, we used automatically generated word classes in additional language models and in additional operation sequence models (Durrani et al., 2014b). This year, we pushed the use of word classes into the remaining feature functions: the reordering model and the sparse word features. We generated Och clusters (Och, 1999) — a variant of Brown clusters — using mkcls. We have to choose a hyper parameter: the number of clusters. Our experiments and also prior work (Stewart et al., 2014) suggest that instead of committing to a single value, it is beneficial to use multiple numbers and use them in multiple feature functions concurrently. We used 50, 200, 600, and 2000 clusters, hence having 4 additional interpolated language models, 4 additional operation sequence models, 4 additional lexicalized reordering models, and 4 additional sets of sparse features. The feature functions for word classes were trained exactly the same way as the corresponding feature functions for words. For instance, this means that the word class language model required training of individual models on"
W15-3013,tiedemann-2012-parallel,0,0.0549009,"ploying dropout to prevent overfitting (Srivastava et al., 2014), enabling us to train the models for at least 2 epochs. We note that, as with French↔English, our application of bilingual LM did not result in significant improvement. Finnish and English are quite distantly related, but we can speculate that using words as a representation for Finnish is not appropriate. The NPLM, however, offers modest (+0.4) improvements over the baseline in both directions. Finnish↔English For the Finnish-English language pair we built systems using only the constrained data, and systems using all the OPUS (Tiedemann, 2012) par129 4.3 System Baseline Submitted BiLM source & combined & NPLM Czech↔English The development of the Czech↔English systems followed the ideas in Section 2.3, i.e., with a focus on word classes (50, 200, 600 classes) for all component models. We combined the test sets from 2008 to 2012 for tuning. No neural language model or bilingual language model was used. 4.4 Table 4: Experimental results (cased B LEU) for English→Russian averaged over newstest2013 and newstest2014. Russian↔English From German. For translation from German, we applied syntactic pre-reordering (Collins et al., 2005) and c"
W15-3013,D13-1140,0,0.0464299,"n of the University of Edinburgh and the Johns Hopkins University for the shared translation task of the EMNLP 2015 Tenth Workshop on Statistical Machine Translation (WMT 2015). We set up phrase-based statistical machine translation systems for all ten language pairs of this year’s evaluation campaign, which are English paired with Czech, Finnish, French, German, and Russian in both translation directions. 2.1 Neural Network LM with NPLM For some language pairs (notably French↔English and Finnish↔English) we experimented with feed-forward neural network language models using the NPLM toolkit (Vaswani et al., 2013). This toolkit enables such language models to be trained efficiently on large datasets, and provides a querying API which is fast enough to be used during decoding. NPLM is fully integrated into Moses, including appropriate wrapper scripts for training the language models within the Moses experiment management system. Novel research directions we investigated include: neural network language models and bilingual neural network language models, a comprehensive use of word classes, and sparse lexicalized reordering features. 1 Novel Methods 2.2 Bilingual Neural Network LM We also experimented w"
W15-3013,W15-3024,1,0.849652,"not the English. In fact French→English was the only language pair where NPLM did not improve B LEU after building the LM on all data. It is possible that the limited morphology of English means that the improved generalisation of the NPLM is not as helpful, and also that the conventional n-gram LM is already strong for this language pair. 4.2 fi-en 19.6 19.7 17.0 19.4 19.8 19.7 19.1 19.1 20.0 allel data. Our baselines include this extra data, but we also show results just using the constrained parallel data. We did not employ the morphological splitting as in Edinburgh’s syntax-based system (Williams et al., 2015) and consequently the English→Finnish systems performed poorly in development and we did not submit a phrase-based system for this pair. Our development setup was similar to French↔English; we used the newsdev2015 for tuning and test during system development (in 2-fold cross-validation) then for the submission and subsequent experiments we used the whole of newsdev2015 for tuning. Also in common with our work on French↔English, we performed several post-submission experiments to examine the effect of class-based language models, bilingual LM and NPLM. We show the results in Table 3. For train"
W15-3013,W09-0429,1,\N,Missing
W15-3013,N04-1022,0,\N,Missing
W15-3013,2014.iwslt-evaluation.6,1,\N,Missing
W15-3013,C14-1041,1,\N,Missing
W15-3013,W14-3324,1,\N,Missing
W16-2209,N06-2001,0,0.200365,"tem (Example 6) retains the original word order, which is highly unusual in English, especially for prose in the news domain. A syntactic annotation of the source sentence could support the attentional encoder-decoder in learning which words in the German source to attend (and translate) first. We will investigate the usefulness of linguistic features for the language pair German↔English, considering the following linguistic features: 2.1 Adding Input Features Our main innovation over the standard encoderdecoder architecture is that we represent the encoder input as a combination of features (Alexandrescu and Kirchhoff, 2006). We here show the equation for the forward states of the encoder (for the simple RNN case; consider (Bahdanau et al., 2015) for GRU): • lemmas • subword tags (see Section 3.2) • morphological features • POS tags • dependency labels → − − − → → −→ h j = tanh(W Exj + U h j−1 ) The inclusion of lemmas is motivated by the hope for a better generalization over inflectional variants of the same word form. The other linguistic features are motivated by disambiguation, as discussed in our introductory examples. 2 (1) where E ∈ Rm×Kx is a word embedding ma− → → − trix, W ∈ Rn×m , U ∈ Rn×n are weight m"
W16-2209,D14-1082,0,0.0870718,"onds to the full word. Our generalized model of the previous section supports an arbitrary number of input features. In this paper, we will focus on a number of wellknown linguistic features. Our main empirical question is if providing linguistic features to the encoder improves the translation quality of neural machine translation systems, or if the information emerges from training encoder-decoder models on raw text, making its inclusion via explicit features redundant. All linguistic features are predicted automatically; we use Stanford CoreNLP (Toutanova et al., 2003; Minnen et al., 2001; Chen and Manning, 2014) to annotate the English input for English→German, and ParZu (Sennrich et al., 2013) to annotate the German input for German→English. We here discuss the individual features in more detail. 3.1 3.3 Morphological Features For German→English, the parser annotates the German input with morphological features. Different word types have different sets of features – for instance, nouns have case, number and gender, while verbs have person, number, tense and aspect – and features may be underspecified. We treat the concatenation of all morphological features of a word, using a special symbol for unde"
W16-2209,D14-1179,0,0.105075,"Missing"
W16-2209,P16-1078,0,0.0585141,"o used in other tasks for which neural models have recently been employed, such as syntactic parsing (Chen and Manning, 2014). This paper addresses the question whether linguistic features on the source side are beneficial for neural machine translation. On the target side, linguistic features are harder to obtain for a generation task such as machine translation, since this would require incremental parsing of the hypotheses at test time, and this is possible future work. Among others, our model incorporates information from a dependency annotation, but is still a sequence-to-sequence model. Eriguchi et al. (2016) propose a tree-to-sequence model whose encoder computes vector representations for each phrase in the source tree. Their focus is on exploiting the (unlabelled) structure of a syntactic annotation, whereas we are focused on the disambiguation power of the functional dependency labels. Factored translation models are often used in phrase-based SMT (Koehn and Hoang, 2007) as a means to incorporate extra linguistic information. However, neural MT can provide a much more flexible mechanism for adding such information. Because phrase-based models cannot easily generalize to new feature combination"
W16-2209,N13-1090,0,0.0045048,"ance, nouns have case, number and gender, while verbs have person, number, tense and aspect – and features may be underspecified. We treat the concatenation of all morphological features of a word, using a special symbol for underspecified features, as a string, and treat each such string as a separate feature value. Lemma Using lemmas as input features guarantees sharing of information between word forms that share the same base form. In principle, neural models can learn that inflectional variants are semantically related, and represent them as similar points in the continuous vector space (Mikolov et al., 2013). However, while this has been demonstrated for high-frequency words, we expect that a lemmatized representation increases data efficiency; low-frequency variants may even be unknown to word-level models. With character- or subwordlevel models, it is unclear to what extent they can learn the similarity between low-frequency word forms that share a lemma, especially if the word forms are superficially dissimilar. Consider the following two German word forms, which share the lemma liegen ‘lie’: 3.4 POS Tags and Dependency Labels In our introductory examples, we motivated POS tags and dependency"
W16-2209,W15-3014,0,0.0288839,"glish source sentence in Example 1 (our translation in Example 2), a neural MT system (our baseline system from Section 4) mistranslates close as a verb, and produces the German verb schließen (Example 3), even though close is an adjective in this sentence, which has the German translation nah. Intuitively, partof-speech annotation of the English input could disambiguate between verb, noun, and adjective meanings of close. As a second example, consider the following German→English example: Introduction Neural machine translation has recently achieved impressive results (Bahdanau et al., 2015; Jean et al., 2015), while learning from raw, sentencealigned parallel text and using little in the way of external linguistic information.3 However, we hypothesize that various levels of linguistic annotation can be valuable for neural machine translation. Lemmatisation can reduce data sparse1 https://github.com/rsennrich/nematus https://github.com/rsennrich/ wmt16-scripts 3 Linguistic tools are most commonly used in preprocessing, e.g. for Turkish segmentation (Gülçehre et al., 2015). 2 4. Gefährlich ist die Route aber dennoch . dangerous is the route but still . 83 Proceedings of the First Conference on Machi"
W16-2209,D07-1091,0,0.0218021,"this would require incremental parsing of the hypotheses at test time, and this is possible future work. Among others, our model incorporates information from a dependency annotation, but is still a sequence-to-sequence model. Eriguchi et al. (2016) propose a tree-to-sequence model whose encoder computes vector representations for each phrase in the source tree. Their focus is on exploiting the (unlabelled) structure of a syntactic annotation, whereas we are focused on the disambiguation power of the functional dependency labels. Factored translation models are often used in phrase-based SMT (Koehn and Hoang, 2007) as a means to incorporate extra linguistic information. However, neural MT can provide a much more flexible mechanism for adding such information. Because phrase-based models cannot easily generalize to new feature combinations, the individual models either treat each feature combination as an atomic unit, resulting in data sparsity, or assume independence between features, for instance by having separate language models for words and POS tags. In contrast, we exploit the strong generalization ability of neural networks, and expect that even new feature combinations, e.g. a word that appears"
W16-2209,P16-1162,1,0.91322,"− − − → n → −→ h j = tanh(W ( Ek xjk ) + U h j−1 ) (2) k=1 where k is the vector concatenation, Ek ∈ are the feature embedding matrices, with P|F | k=1 mk = m, and Kk is the vocabulary size of the kth feature. In other words, we look up separate embedding vectors for each feature, which are then concatenated. The length of the concatenated vector matches the total embedding size, and all other parts of the model remain unchanged. Rmk ×Kk 4 https://github.com/nyu-dl/ dl4mt-tutorial 84 3 Linguistic Input Features a fixed symbol vocabulary, using a segmentation based on byte-pair encoding (BPE) (Sennrich et al., 2016c). We note that in BPE segmentation, some symbols are potentially ambiguous, and can either be a separate word, or a subword segment of a larger word. Also, text is represented as a sequence of subword units with no explicit word boundaries, but word boundaries are potentially helpful to learn which symbols to attend to, and when to forget information in the recurrent layers. We propose an annotation of subword structure similar to popular IOB format for chunking and named entity recognition, marking if a symbol in the text forms the beginning (B), inside (I), or end (E) of a word. A separate"
W16-2209,W15-3031,0,0.0302229,"Missing"
W16-2209,N03-1033,0,0.0679687,"separate tag (O) is used if a symbol corresponds to the full word. Our generalized model of the previous section supports an arbitrary number of input features. In this paper, we will focus on a number of wellknown linguistic features. Our main empirical question is if providing linguistic features to the encoder improves the translation quality of neural machine translation systems, or if the information emerges from training encoder-decoder models on raw text, making its inclusion via explicit features redundant. All linguistic features are predicted automatically; we use Stanford CoreNLP (Toutanova et al., 2003; Minnen et al., 2001; Chen and Manning, 2014) to annotate the English input for English→German, and ParZu (Sennrich et al., 2013) to annotate the German input for German→English. We here discuss the individual features in more detail. 3.1 3.3 Morphological Features For German→English, the parser annotates the German input with morphological features. Different word types have different sets of features – for instance, nouns have case, number and gender, while verbs have person, number, tense and aspect – and features may be underspecified. We treat the concatenation of all morphological featu"
W16-2209,W15-3049,0,0.0318322,"Missing"
W16-2209,W16-2327,1,0.886618,"ifferent features is not fully cumulative; we note that the information encoded in different features overlaps. For instance, both the dependency labels and the morphological features encode the distinction between German subjects and accusative objects, the former through different labels (subj and obja), the latter through grammatical case (nominative and accusative). We also evaluated adding linguistic features to a stronger baseline, which includes synthetic parallel training data. In addition, we compare our neural systems against phrase-based (PBSMT) and syntax-based (SBSMT) systems by (Williams et al., 2016), all of which make use of linguistic annotation on the source and/or target side. Results are shown in Table 4. For German→English, we observe similar improvements in the best development perplexity (45.2 → 44.1), test set B LEU (37.5→38.5) and CHR F3 (62.2 → 62.8). Our test set B LEU is on par to the best submitted system to this year’s WMT 16 shared translation task, which is similar to our baseline MT system, but which also uses a right-to-left decoder for reranking (Sennrich et al., 2016a). We expect that linguistic input features and bidirectional decoding are orthogonal, and that we cou"
W16-2209,W05-0908,0,0.0545676,"sagree, partly because they are highly sensitive to the length of the output. B LEU is precision-based, whereas CHR F3 considers both precision and recall, with a bias for recall. For B LEU, we also report whether differences between systems are statistically significant Table 1: Vocabulary size, and size of embedding layer of linguistic features, in system that includes all features, and contrastive experiments that add a single feature over the baseline. The embedding layer size of the word feature is set to bring the total size to 500. according to a bootstrap resampling significance test (Riezler and Maxwell, 2005). We train models for about a week, and report results for an ensemble of the 4 last saved models (with models saved every 12 hours). The ensemble serves to smooth the variance between single models. Decoding is performed with beam search with a beam size of 12. To ensure that performance improvements are not simply due to an increase in the number of model parameters, we keep the total size of the embedding layer fixed to 500. Table 1 lists the embedding size we use for linguistic features – the embedding layer size of the word-level feature varies, and is set to bring the total embedding lay"
W16-2209,N16-1004,0,0.062145,"Missing"
W16-2209,schmid-etal-2004-smor,0,0.0316331,"s and Dependency Labels In our introductory examples, we motivated POS tags and dependency labels as possible disambiguators. Each word is associated with one POS tag, and one dependency label. The latter is the label of the edge connecting a word to its syntactic head, or ’ROOT’ if the word has no syntactic head. 3.5 • liegt ‘lies’ (3.p.sg. present) • läge ‘lay’ (3.p.sg. subjunctive II) On Using Word-level Features in a Subword Model The lemmatisers we use are based on finite-state methods, which ensures a large coverage, even for infrequent word forms. We use the Zmorge analyzer for German (Schmid et al., 2004; Sennrich and Kunz, 2014), and the lemmatiser in the Stanford CoreNLP toolkit for English (Minnen et al., 2001). We segment rare words into subword units using BPE. The subword tags encode the segmentation of words into subword units, and need no further modification. All other features are originally word-level features. To annotate the segmented source text with features, we copy the word’s feature value to all its subword units. An example is shown in Figure 1. 3.2 4 Subword Tags Evaluation We evaluate our systems on the WMT16 shared translation task English↔German. The parallel In our exp"
W16-2209,sennrich-kunz-2014-zmorge,1,0.809645,"ls In our introductory examples, we motivated POS tags and dependency labels as possible disambiguators. Each word is associated with one POS tag, and one dependency label. The latter is the label of the edge connecting a word to its syntactic head, or ’ROOT’ if the word has no syntactic head. 3.5 • liegt ‘lies’ (3.p.sg. present) • läge ‘lay’ (3.p.sg. subjunctive II) On Using Word-level Features in a Subword Model The lemmatisers we use are based on finite-state methods, which ensures a large coverage, even for infrequent word forms. We use the Zmorge analyzer for German (Schmid et al., 2004; Sennrich and Kunz, 2014), and the lemmatiser in the Stanford CoreNLP toolkit for English (Minnen et al., 2001). We segment rare words into subword units using BPE. The subword tags encode the segmentation of words into subword units, and need no further modification. All other features are originally word-level features. To annotate the segmented source text with features, we copy the word’s feature value to all its subword units. An example is shown in Figure 1. 3.2 4 Subword Tags Evaluation We evaluate our systems on the WMT16 shared translation task English↔German. The parallel In our experiments, we operate on th"
W16-2209,R13-1079,1,0.410147,"trary number of input features. In this paper, we will focus on a number of wellknown linguistic features. Our main empirical question is if providing linguistic features to the encoder improves the translation quality of neural machine translation systems, or if the information emerges from training encoder-decoder models on raw text, making its inclusion via explicit features redundant. All linguistic features are predicted automatically; we use Stanford CoreNLP (Toutanova et al., 2003; Minnen et al., 2001; Chen and Manning, 2014) to annotate the English input for English→German, and ParZu (Sennrich et al., 2013) to annotate the German input for German→English. We here discuss the individual features in more detail. 3.1 3.3 Morphological Features For German→English, the parser annotates the German input with morphological features. Different word types have different sets of features – for instance, nouns have case, number and gender, while verbs have person, number, tense and aspect – and features may be underspecified. We treat the concatenation of all morphological features of a word, using a special symbol for underspecified features, as a string, and treat each such string as a separate feature v"
W16-2209,P16-1009,1,0.927518,"− − − → n → −→ h j = tanh(W ( Ek xjk ) + U h j−1 ) (2) k=1 where k is the vector concatenation, Ek ∈ are the feature embedding matrices, with P|F | k=1 mk = m, and Kk is the vocabulary size of the kth feature. In other words, we look up separate embedding vectors for each feature, which are then concatenated. The length of the concatenated vector matches the total embedding size, and all other parts of the model remain unchanged. Rmk ×Kk 4 https://github.com/nyu-dl/ dl4mt-tutorial 84 3 Linguistic Input Features a fixed symbol vocabulary, using a segmentation based on byte-pair encoding (BPE) (Sennrich et al., 2016c). We note that in BPE segmentation, some symbols are potentially ambiguous, and can either be a separate word, or a subword segment of a larger word. Also, text is represented as a sequence of subword units with no explicit word boundaries, but word boundaries are potentially helpful to learn which symbols to attend to, and when to forget information in the recurrent layers. We propose an annotation of subword structure similar to popular IOB format for chunking and named entity recognition, marking if a symbol in the text forms the beginning (B), inside (I), or end (E) of a word. A separate"
W16-2209,W16-2323,1,\N,Missing
W16-2301,W13-3520,0,0.0148252,"Missing"
W16-2301,2011.mtsummit-papers.35,0,0.440323,"Missing"
W16-2301,W15-3001,1,0.419439,"Missing"
W16-2301,W16-2305,0,0.0273073,"Missing"
W16-2301,L16-1662,0,0.00889007,"introduce a new set of features for the sentence-Level QE task. The features extracted include three alignment-based features, three bilingual embedding-based features, two embeddingbased features constrained on alignment links, as well as a set of 74 bigrams used as boolean features. The set of bigrams represents the most frequent bigrams in translations that have changed after the post-edition, and they are compiled by aligning translations to their post-editions provided in the WMT QE datasets. To produce these features, GIZA++ (Och and Ney, 2003) was used for word alignment and Multivec (Berard et al., 2016) was used for the bilingual model, which jointly learns distributed representations for source and target languages using a parallel corpus. To build the bilingual model, domain-specific data compiled from the resources made available for the WMT 16 IT-Domain shared task was used. As prediction model, a Linear Regression model using scikit-learn was built using a combination of QuEst++ baseline features and the new features proposed. tional features for both tasks. UNBABEL (Task 2): Two systems were submitted for the word-level task. UNBABEL 2 linear is a feature-based linear sequential model."
W16-2301,W11-2101,1,0.851508,"Missing"
W16-2301,W16-2306,0,0.0263259,"Missing"
W16-2301,W16-2382,0,0.0451767,"Missing"
W16-2301,W16-2302,1,0.346609,"Missing"
W16-2301,W16-2307,1,0.756182,"Missing"
W16-2301,W16-2308,0,0.0373021,"Missing"
W16-2301,buck-etal-2014-n,0,0.0150538,"Missing"
W16-2301,W13-2201,1,0.287085,"Missing"
W16-2301,W14-3302,1,0.399147,"Missing"
W16-2301,W14-3340,1,0.419045,"Missing"
W16-2301,W07-0718,1,0.759293,"oth automatically and manually. The human evaluation (§3) involves asking human judges to rank sentences output by anonymized systems. We obtained large numbers of rankings from researchers who contributed The quality estimation task had three subtasks, with a total of 14 teams, submitting 39 entries. The automatic post-editing task had a total of 6 teams, submitting 11 entries. 1 Introduction We present the results of the shared tasks of the First Conference on Statistical Machine Translation (WMT) held at ACL 2016. This conference builds on nine previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015). 131 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 131–198, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics to refine evaluation and estimation methodologies for machine translation. As before, all of the data, translations, and collected human judgments are publicly available.1 We hope these datasets serve as a valuable resource for research into statistical machine translation and automatic evaluation or prediction of translation quality. New"
W16-2301,W15-3025,1,0.888663,"used for the Task 2p. The submissions to the word-level task are modified in order to comply with the phrase-level task. sequences. Nevertheless, it is worth noticing the phraselevel QE systems introduced a number of interesting strategies that allowed them to outperform a strong baseline phrase-level model. Finally, we recall that the evaluation metric – word-level F1 mult – has difficulties to distinguish phrase-level systems. This suggests that we may need to find a different metric for evaluation of the phrase-level task, with phrase-level F1 -mult one of the candidates. 7 pointed out by Chatterjee et al. (2015b), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) post-editing effort; Automatic Post-editing Task This year WMT hosted the second round of the shared task on MT automatic post-editing (APE), which consists in automatically correcting"
W16-2301,P15-2026,1,0.895785,"used for the Task 2p. The submissions to the word-level task are modified in order to comply with the phrase-level task. sequences. Nevertheless, it is worth noticing the phraselevel QE systems introduced a number of interesting strategies that allowed them to outperform a strong baseline phrase-level model. Finally, we recall that the evaluation metric – word-level F1 mult – has difficulties to distinguish phrase-level systems. This suggests that we may need to find a different metric for evaluation of the phrase-level task, with phrase-level F1 -mult one of the candidates. 7 pointed out by Chatterjee et al. (2015b), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) post-editing effort; Automatic Post-editing Task This year WMT hosted the second round of the shared task on MT automatic post-editing (APE), which consists in automatically correcting"
W16-2301,W08-0309,1,0.809107,"Missing"
W16-2301,W16-2309,0,0.0305938,"Missing"
W16-2301,W10-1703,1,0.312318,"Missing"
W16-2301,W16-2336,0,0.0356983,"Missing"
W16-2301,W12-3102,1,0.235434,"ir native language. Consequently, health professionals may use the translated information to make clinical decisions impacting patients care. It is vital that translation systems do not contribute to the dissemination of incorrect clinical information. Therefore, the evaluation of biomedical translation systems should include an assessment at the document level indicating whether a translation conveyed erroneous clinical information. 6 Quality Estimation The fifth edition of the WMT shared task on quality estimation (QE) of machine translation (MT) builds on the previous editions of the task (Callison-Burch et al., 2012; Bojar et al., 2013, 2014, 2015), with “traditional” tasks at sentence and word levels, a new task for entire documents quality prediction, and a variant of the word-level task: phrase-level estimation. The goals of this year’s shared task were: • To advance work on sentence and wordlevel quality estimation by providing domainspecific, larger and professionally annotated datasets. • To analyse the effectiveness of different types of quality labels provided by humans for longer texts in document-level prediction. • To investigate quality estimation at a new level of granularity: phrases. Plan"
W16-2301,W16-2330,0,0.0328047,"Missing"
W16-2301,W16-2310,1,0.835279,"Missing"
W16-2301,W11-2103,1,0.65163,"Missing"
W16-2301,W16-2331,0,0.0203904,"cance test results for pairs of systems competing in the news domain translation task (en-ru), where a green cell denotes a significantly higher DA adequacy score for the system in a given row over the system in a given column. cs-en fi-en tr-en de-en ru-en ro-en 0.997 0.996 0.988 0.964 0.961 0.920 en-ru 0.975 DA Correlation with RR Table 11: Correlation between overall DA standardized mean adequacy scores and RR Trueskill scores. 150 4 IT Translation Task from the QTLeap project: HF&FCUL for Portuguese, UPV/EHU for Spanish and Basque, IICT-BAS for Bulgarian, CUNI for Czech and UG for Dutch). Duma and Menzel (2016) describe UHDS- DOC 2 VEC and UHBS- LMI (University of Hamburg). Pahari et al. (2016) describe JU-USAAR (Jadavpur University & Saarland University). Cuong et al. (2016) describe ILLC-U VA-S CORPIO (University of Amsterdam). IILC-U VA-DS is based on Hoang and Sima’an (2014). PROMT-RULE - BASED and PROMT-H YBRID systems were submitted by the PROMT LLC company and they are not described in any paper. QTL -M OSES is the standard Moses setup (MERT-tuned on the in-domain training data, but otherwise without any domain-adaptation) and serves as a baseline. The IT-domain translation task introduced th"
W16-2301,W15-3009,1,0.788321,"Missing"
W16-2301,P15-1174,1,0.28154,"endently inserted in another part of this sentence, i.e. to correct an unrelated error. The statistics of the datasets are outlined in Table 20. Evaluation Evaluation was performed against the true HTER label and/or ranking, using the following metrics: • Scoring: Pearson’s r correlation score (primary metric, official score for ranking submissions), Mean Average Error (MAE) and Root Mean Squared Error (RMSE). • Ranking: Spearman’s ρ rank correlation and DeltaAvg. Statistical significance on Pearson r and Spearman rho was computed using the William’s test, following the approach suggested in (Graham, 2015). Results Table 19 summarises the results for Task 1, ranking participating systems best to worst using Pearson’s r correlation as primary key. Spearman’s ρ correlation scores should be used to rank systems according to the ranking variant. We note that three systems have not submitted results ranking evaluation variant. 6.4 Task 2: Predicting word-level quality The goal of this task is to evaluate the extent to which we can detect word-level errors in MT output. Various classes of errors can be found in translations, but for this task we consider all error types together, aiming at making a b"
W16-2301,W16-2311,0,0.0122367,"016) PROMT Automated Translation Solutions (Molchanov and Bykov, 2016) QT21 System Combination (Peter et al., 2016b) RWTH Aachen (Peter et al., 2016a) ¨ TUBITAK (Bektas¸ et al., 2016) University of Edinburgh (Sennrich et al., 2016) University of Edinburgh (Williams et al., 2016) UEDIN - SYNTAX UEDIN - LMU UH -* USFD - RESCORING UUT YSDA ONLINE -[ A , B , F, G ] University of Edinburgh / University of Munich (Huck et al., 2016) University of Helsinki (Tiedemann et al., 2016) University of Sheffield (Blain et al., 2016) Uppsala University (Tiedemann et al., 2016) Yandex School of Data Analysis (Dvorkovich et al., 2016) Four online statistical machine translation systems Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. 136 Figure 2: Screenshot of the Appraise interface used in the human evaluation campaign. The annotator is presented with a source segment, a reference translation, and up to five outputs from competing systems (anonymized"
W16-2301,W13-2305,1,0.509145,"Missing"
W16-2301,W15-3036,0,0.0603245,"Missing"
W16-2301,E14-1047,1,0.831614,"Missing"
W16-2301,W16-2378,0,0.168663,"ts for pairs of systems competing in the news domain translation task (en-ru), where a green cell denotes a significantly higher DA adequacy score for the system in a given row over the system in a given column. cs-en fi-en tr-en de-en ru-en ro-en 0.997 0.996 0.988 0.964 0.961 0.920 en-ru 0.975 DA Correlation with RR Table 11: Correlation between overall DA standardized mean adequacy scores and RR Trueskill scores. 150 4 IT Translation Task from the QTLeap project: HF&FCUL for Portuguese, UPV/EHU for Spanish and Basque, IICT-BAS for Bulgarian, CUNI for Czech and UG for Dutch). Duma and Menzel (2016) describe UHDS- DOC 2 VEC and UHBS- LMI (University of Hamburg). Pahari et al. (2016) describe JU-USAAR (Jadavpur University & Saarland University). Cuong et al. (2016) describe ILLC-U VA-S CORPIO (University of Amsterdam). IILC-U VA-DS is based on Hoang and Sima’an (2014). PROMT-RULE - BASED and PROMT-H YBRID systems were submitted by the PROMT LLC company and they are not described in any paper. QTL -M OSES is the standard Moses setup (MERT-tuned on the in-domain training data, but otherwise without any domain-adaptation) and serves as a baseline. The IT-domain translation task introduced th"
W16-2301,W11-2123,0,0.0116622,"s were also evaluated against a reimplementation of the approach firstly proposed by Simard et al. (2007).36 Last year, in fact, this statistical post-editing approach represented the common backbone of all submissions (this is also reflected by the close results achieved by participants in the pilot task). For this purpose, a phrasebased SMT system based on Moses (Koehn et al., 2007) was used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heafield, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the APE system was tuned on Adam Mickiewicz University. This system is among the very first ones exploring the application of neural translation models to the APE task. In particular, it investigates the following aspects: i) the use of artificially-created postedited data to train the neural models, ii) the loglinear combination of monolingual and bilingual models in an ensemble-like manner, iii) the addition of task-specific features in the log-linear model to control the final output quality. Concerning the data, in addition"
W16-2301,W16-2384,0,0.0416841,"Missing"
W16-2301,W13-0805,0,0.0110515,"of words in the reference. Lower TER values indicate lower distance from the reference as a proxy for higher MT quality. 33 http://www.cs.umd.edu/˜snover/tercom/ 34 https://github.com/moses-smt/mosesdecoder/ blob/master/scripts/generic/multi-bleu.perl 27 The source sentences (together with their reference translations which were not used for the task) were provided by TAUS (https://www.taus.net/) and originally come from a unique IT vendor. 28 It consists of a phrase-based machine translation system leveraging generic and in-domain parallel training data and using a pre-reordering technique (Herrmann et al., 2013). It takes also advantages of POS and word class-based language models. 29 German native speakers working at Text&Form https: //www.textform.com/. 176 Train (12,000) Dev (1,000) Test (2,000) SRC 201,505 17,827 31,477 Tokens TGT 210,573 19,355 34,332 PE 214,720 19,763 35,276 Table 31: Types SRC TGT 9,328 14,185 2,931 3,333 3,908 4,695 Data statistics. APE@WMT15 APE@WMT16 (EN-ES, news, crowd) (EN-DE, IT, prof.) SRC 2.905 6.616 TGT 3.312 8.845 PE 3.085 8.245 Table 32: Repetition Rate (RR) of the WMT15 (EnglishSpanish, news domain, crowdsourced post-edits) and WMT16 (English-German, IT domain, pro"
W16-2301,W16-2315,1,0.824148,"Missing"
W16-2301,P07-2045,1,0.017773,"gets unmodified.35 Baseline results are reported in Table 34. 7.2 Participants Monolingual translation as another term of comparison. To get some insights about the progress with respect to the first pilot task, participating systems were also evaluated against a reimplementation of the approach firstly proposed by Simard et al. (2007).36 Last year, in fact, this statistical post-editing approach represented the common backbone of all submissions (this is also reflected by the close results achieved by participants in the pilot task). For this purpose, a phrasebased SMT system based on Moses (Koehn et al., 2007) was used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heafield, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the APE system was tuned on Adam Mickiewicz University. This system is among the very first ones exploring the application of neural translation models to the APE task. In particular, it investigates the following aspects: i) the use of artificially-created postedited data to train the neural models, ii)"
W16-2301,W16-2337,0,0.0376798,"Missing"
W16-2301,W16-2303,1,0.821661,"Missing"
W16-2301,L16-1582,1,0.790825,"Missing"
W16-2301,W16-2385,0,0.0304494,"Missing"
W16-2301,P16-2095,1,0.885979,"e package to extract these and other quality estimation features and perform model learning, with suggested methods for all levels of prediction. Participants, described in Section 6.2, could submit up to two systems for each task. Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Tas"
W16-2301,W15-3037,0,0.00849978,"stems were submitted for the word-level task. UNBABEL 2 linear is a feature-based linear sequential model. It uses the baseline features provided by the shared task organisers (with slight changes) conjoined with individual labels and pairs of consecutive labels. It also uses various syntactic dependency-based features (dependency relations, heads, and second-order structures like siblings and grandparents). The syntactic dependencies are predicted with TurboParser trained on the TIGER German treebank. UNBABEL 2 ensemble uses a stacked architecture, inspired by the last year’s QUETCH+ system (Kreutzer et al., 2015), which combines three neural systems: one feedforward and two recurrent ones. The predictions of these systems are added as additional features in the linear system above. The following external resources were used: part-of-speech tags and extra syntactic dependency information obtained with TurboTagger and TurboParser (Martins et al., 2013), trained on the Penn Treebank (for English) and on the version of the German TIGER corpus used in the SPMRL shared task (Seddah et al., 2014) for German. For the neural models, pretrained word embeddings from Polyglot (AlRfou et al., 2013) and those produ"
W16-2301,W14-3342,0,0.0237077,"Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Task 1, QuEst++13 (Specia et al., 2015) was used to extract 17 features from the SMT source/target language training corpus: • Number of tokens in source & target sentences. • Target token, its left and right contexts of one word."
W16-2301,W16-2318,0,0.0145349,"Missing"
W16-2301,N06-1014,0,0.0215966,"d on n previous translation and reordering decisions. This technique is able to model both local and long-range reorderings that are quite useful when dealing with the German language. To improve the capability of choosing the correct edit to process, eight new features are added to the loglinear model. These features capture the cost of deleting a phrase and different information on possible gaps in reordering operations. The monolingual alignments between the MT outputs and their post-edits are computed using different methods based on TER, METEOR (Snover et al., 2006) and Berkeley Aligner (Liang et al., 2006). Only the task data is used for these submissions. 7.3 179 TER/BLEU results ID AMU Primary AMU Contrastive FBK Contrastive FBK Primary USAAR Primary USAAR Constrastive CUNI Primary (Simard et al., 2007) Baseline DCU Contrastive JUSAAR Primary JUSAAR Contrastive DCU Primary Avg. TER 21.52 23.06 23.92 23.94 24.14 24.14 24.31 24.64 24.76 26.79 26.92 26.97 28.97 BLEU 67.65 66.09 64.75 64.75 64.10 64.00 63.32 63.47 62.11 58.60 59.44 59.18 55.19 tained this year by the top runs can only be reached by moving from the basic statistical MT backbone shared by all last year’s participants to new and mor"
W16-2301,P13-2109,0,0.0179517,"eads, and second-order structures like siblings and grandparents). The syntactic dependencies are predicted with TurboParser trained on the TIGER German treebank. UNBABEL 2 ensemble uses a stacked architecture, inspired by the last year’s QUETCH+ system (Kreutzer et al., 2015), which combines three neural systems: one feedforward and two recurrent ones. The predictions of these systems are added as additional features in the linear system above. The following external resources were used: part-of-speech tags and extra syntactic dependency information obtained with TurboTagger and TurboParser (Martins et al., 2013), trained on the Penn Treebank (for English) and on the version of the German TIGER corpus used in the SPMRL shared task (Seddah et al., 2014) for German. For the neural models, pretrained word embeddings from Polyglot (AlRfou et al., 2013) and those produced with a neural MT system (Bahdanau et al., 2014) were used. UGENT-LT3 (Task 1, Task 2): The submissions for the word-level task use 41 features in combination with the baseline feature set to train binary classifiers. The 41 additional features attempt to capture accuracy errors (concerned with the meaning transfer from the source to targe"
W16-2301,W16-2387,0,0.0253511,"Missing"
W16-2301,W16-2317,0,0.0414065,"Missing"
W16-2301,N13-1090,0,0.00833788,"Missing"
W16-2301,2015.iwslt-papers.4,1,0.737239,"Missing"
W16-2301,W16-2319,0,0.0339318,"Missing"
W16-2301,W16-2386,1,0.814188,"e package to extract these and other quality estimation features and perform model learning, with suggested methods for all levels of prediction. Participants, described in Section 6.2, could submit up to two systems for each task. Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Tas"
W16-2301,L16-1470,1,0.826657,"Missing"
W16-2301,P03-1021,0,0.0340784,"7 Tokens TGT 210,573 19,355 34,332 PE 214,720 19,763 35,276 Table 31: Types SRC TGT 9,328 14,185 2,931 3,333 3,908 4,695 Data statistics. APE@WMT15 APE@WMT16 (EN-ES, news, crowd) (EN-DE, IT, prof.) SRC 2.905 6.616 TGT 3.312 8.845 PE 3.085 8.245 Table 32: Repetition Rate (RR) of the WMT15 (EnglishSpanish, news domain, crowdsourced post-edits) and WMT16 (English-German, IT domain, professional posteditors) APE Task data. 7.1.3 PE 16,388 3,506 5,047 SRC 5,628 1,922 2,479 Lemmas TGT PE 11,418 13,244 2,686 2,806 3,753 4,050 the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Table 34. For each submitted run, the statistical significance of performance differences with respect to the baselines and the re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). Baseline The official baseline results are the TER and BLEU scores calculated by comparing the raw MT output with the human post-edits. In practice, the baseline APE system is a system that leaves all the test targets unmodified.35 Baseline results are reported in Table 34. 7.2 Participants Monolingual"
W16-2301,W16-2338,0,0.0374669,"Missing"
W16-2301,J03-1002,0,0.0268639,"word alignments and bilingual distributed representations to introduce a new set of features for the sentence-Level QE task. The features extracted include three alignment-based features, three bilingual embedding-based features, two embeddingbased features constrained on alignment links, as well as a set of 74 bigrams used as boolean features. The set of bigrams represents the most frequent bigrams in translations that have changed after the post-edition, and they are compiled by aligning translations to their post-editions provided in the WMT QE datasets. To produce these features, GIZA++ (Och and Ney, 2003) was used for word alignment and Multivec (Berard et al., 2016) was used for the bilingual model, which jointly learns distributed representations for source and target languages using a parallel corpus. To build the bilingual model, domain-specific data compiled from the resources made available for the WMT 16 IT-Domain shared task was used. As prediction model, a Linear Regression model using scikit-learn was built using a combination of QuEst++ baseline features and the new features proposed. tional features for both tasks. UNBABEL (Task 2): Two systems were submitted for the word-level tas"
W16-2301,W16-2321,0,0.0372138,"Missing"
W16-2301,W16-2388,1,0.748184,"Missing"
W16-2301,W16-2333,0,0.0259256,"Missing"
W16-2301,W15-3026,0,0.0339859,"Missing"
W16-2301,W16-2379,1,0.819672,"Missing"
W16-2301,W16-2334,1,0.811656,"Missing"
W16-2301,P02-1040,0,0.105557,", eventually, make the APE task more feasible by automatic systems. Other changes concern the language combination and the evaluation mode. As regards the languages, we moved from English-Spanish to English-German, which is one of the language pairs covered by the QT21 Project26 that supported data collection and post-editing. Concerning the evaluation, we changed from TER scores computed both in case-sensitive and caseinsensitive mode to a single ranking based on case sensitive measurements. Besides these changes the new round of the APE task included some extensions in the evaluation. BLEU (Papineni et al., 2002) has been introduced as a secondary evaluation metric to measure the improvements over the rough MT output. In addition, to gain further insights on final output quality, a subset of the outputs of the submitted systems has also been manually evaluated. Based on these changes and extensions, the goals of this year’s shared task were to: i) improve and stabilize the evaluation framework in view of future rounds, ii) analyze the effect on task 26 feasibility of data coming from a narrow domain, iii) analyze the effect of post-edits collected from professional translators, iv) analyze how humans"
W16-2301,W16-2389,0,0.0210658,"Missing"
W16-2301,W16-2390,0,0.0283246,"Missing"
W16-2301,W16-2322,0,0.0139204,"ng and evaluating the manual translations has settled into the following pattern. We ask human annotators to rank the outputs of five systems. From these rankings, we produce pairwise translation comparisons, and then evaluate them with a version of the TrueSkill algorithm adapted to our task. We refer to this approach (described in Section 3.4) as the relative ranking approach (RR), so named because the pairwise comparisons denote only relative ability between a pair of systems, and cannot be used to infer their absolute quality. These results are used to produce the official ranking for the WMT 2016 tasks. However, work in evaluation over the past few years has provided fresh insight into ways to collect direct assessments (DA) of machine translation quality. In this setting, annotators are asked to provide an assessment of the direct quality of the output of a system relative to a reference translation. In order to evaluate the potential of this approach for future WMT evaluations, we conducted a direct assessment evaluation in parallel. This evaluation, together with a comparison of the official results, is described in Section 3.5. 2.3 3.1 2.2 Training data As in past years we provide"
W16-2301,W16-2392,1,0.772695,"Missing"
W16-2301,L16-1649,1,0.704564,"): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1): The YSDA submission is based on a simple idea that the more complex the sentence is the more difficult it is to translate. For this purpose, it uses information provided by syntactic parsing (information from parsing trees,"
W16-2301,W16-2391,1,0.796337,"Missing"
W16-2301,2014.eamt-1.21,1,0.643048,"e extracted by averaging word embeddings in the document. The GP model was trained with two Rational Quadratic kernels (Rasmussen and Williams, 2006): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1): The YSDA submission is based on a simple idea that the more complex the sentence"
W16-2301,W15-3040,1,0.909092,"on-word corpus,18 with a vocabulary size of 527K words. Document embeddings are extracted by averaging word embeddings in the document. The GP model was trained with two Rational Quadratic kernels (Rasmussen and Williams, 2006): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1)"
W16-2301,2006.amta-papers.25,0,0.850121,"ombinations of several features. A regression model was training to predict BLEU as target metric instead HTER. The machine learning pipeline uses an SVR with RBF kernel to predict BLEU scores, followed by a linear SVR to predict HTER scores from BLEU scores. As external resources, the system uses a syntactic parser, pseudo-references and back-translation from web-scale MT system, and a web-scale language model. 6.3 Task 1: Predicting sentence-level quality This task consists in scoring (and ranking) translation sentences according to the percentage of their words that need to be fixed. HTER (Snover et al., 2006) is used as quality score, i.e. the minimum edit distance between the machine translation and its manually post-edited version in [0,1]. As in previous years, two variants of the results could be submitted: • Scoring: An absolute HTER score for each sentence translation, to be interpreted as an error metric: lower scores mean better translations. • Ranking: A ranking of sentence translations for all source sentences from best to worst. For this variant, it does not matter how the ranking is produced (from HTER predictions or by other means). The reference ranking is defined based on the true H"
W16-2301,W15-4916,1,0.824025,"Missing"
W16-2301,W16-2346,1,0.0602876,"Missing"
W16-2301,W16-2325,1,0.816227,"Missing"
W16-2301,W16-2393,0,0.033041,"Missing"
W16-2301,W16-2326,0,0.0221449,"Missing"
W16-2301,W16-2327,1,0.737603,"Missing"
W16-2301,W16-2328,0,0.0403946,"Missing"
W16-2301,C00-2137,0,0.0384283,"Missing"
W16-2301,D07-1091,1,\N,Missing
W16-2301,W09-0401,1,\N,Missing
W16-2301,P11-1105,0,\N,Missing
W16-2301,N07-1064,0,\N,Missing
W16-2301,W04-3250,1,\N,Missing
W16-2301,P15-4020,1,\N,Missing
W16-2301,W16-2377,1,\N,Missing
W16-2301,aziz-etal-2012-pet,1,\N,Missing
W16-2301,2012.eamt-1.31,0,\N,Missing
W16-2301,C14-1182,0,\N,Missing
W16-2301,W16-2316,0,\N,Missing
W16-2301,W16-2332,1,\N,Missing
W16-2301,W16-2320,1,\N,Missing
W16-2301,W16-2335,0,\N,Missing
W16-2301,W16-2329,0,\N,Missing
W16-2301,W16-2383,0,\N,Missing
W16-2301,W16-2381,1,\N,Missing
W16-2301,W16-2312,0,\N,Missing
W16-2301,P16-1162,1,\N,Missing
W16-2301,W13-2243,1,\N,Missing
W16-2301,W08-0509,0,\N,Missing
W16-2301,2015.eamt-1.17,1,\N,Missing
W16-2301,W14-6111,0,\N,Missing
W16-2301,2012.tc-1.5,1,\N,Missing
W16-2301,W16-2347,1,\N,Missing
W16-2301,W16-2314,0,\N,Missing
W16-2315,J07-2003,0,0.142484,"erarchical grammar that the system can use for serial concatenation of phrases as in monotonic phrase-based translation. In the Moses implementation, the decoder internally adds a sentence start terminal symbol &lt;s> and a sentence end terminal symbol &lt;/s> to the input before and after each sentence, respectively. Therefore, two more special rules S → h&lt;s>, &lt;s>i S → hS∼0 &lt;/s>, S∼0 &lt;/s>i 2.1 (2) are included which allow the decoder to finalize its translations. Hierarchical search is conducted with a customized version of the CYK+ parsing algorithm (Chappelier and Rajman, 1998) and cube pruning (Chiang, 2007). A hypergraph which represents the whole parsing space is built employing CYK+. Cube pruning operates in bottom-up topological order on this hypergraph and expands at most k derivations at each hypernode. All our experiments are run with the open source Moses implementation (Hoang et al., 2009) of the hierarchical phrase-based translation paradigm. 2 (1) System Overview Hierarchical Phrase-Based Translation In hierarchical phrase-based translation, a probabilistic synchronous context-free grammar is induced from bilingual training corpora. In addition to continuous lexical phrases as in stand"
W16-2315,W06-1607,0,0.0385299,"ted weight. Baseline Setup The features of our plain hierarchical phrase-based baseline are: CommonCrawl LM training data. A large Romanian CommonCrawl corpus has been released for the constrained track of the WMT16 shared task for machine translation of news. In our system, we utilize this corpus by adding it to the training data of the background LM. We append it to the concatenation of news2015, Europarl, and SETimes2 data and estimate a bigger background LM. • Rule translation log-probabilities in both target-to-source and source-to-target direction, smoothed with Good-Turing discounting (Foster et al., 2006). • Lexical translation log-probabilities in both target-to-source and source-to-target direction. • Seven binary features indicating absolute occurrence count classes of translation rules (with count classes 1, 2, 3, 4, 5-6, 7-10, >10). 6 Pruned individual LMs are trained with KenLM’s --prune '0 0 1' parameters. Weights for linear LM interpolation are optimized on newsdev2016_1. 313 Pruned vs. unpruned LMs. We compare pruned and unpruned language models. In the pruned versions of the models, singleton n-grams of order three and higher are discarded, whereas all n-grams are kept in the unprune"
W16-2315,W16-2304,0,0.12408,"cted from the parallel training corpus. Extracted rules of a standard hierarchical grammar are of the form X → hα, β ,∼ i where hα, β i 2.2 Data and Preprocessing Our system is trained using only permissible Romanian monolingual and English–Romanian parallel corpora provided by the organizers of the WMT16 shared task for machine translation of news: Europarl (Koehn, 2005), SETimes2 (Tyers and Alperen, 2010), News Crawl articles from 2015 (denoted as news2015 hereafter), and CommonCrawl (Buck et al., 2014). The target side of the data is preprocessed with tokro, LIMSI’s tokenizer for Romanian (Allauzen et al., 2016).5 The English source side is tokenized using the tokenizer.perl script from the Moses toolkit. Romanian and English sentences are both frequent-cased (with Moses’ truecase.perl). 5 https://perso.limsi.fr/aufrant/ software/tokro 312 • An indicator feature that fires on applications of the glue rule. • Word penalty. • Rule penalty. • A 5-gram language model. We split the development set newsdev2016 into two halves (newsdev2016_1 with the first 1000 sentences and newsdev2016_2 with the last 999 sentences). During the system building process, we measure progress by evaluating on newsdev2016_2 as"
W16-2315,2011.iwslt-evaluation.18,0,0.0230414,"e compare pruned and unpruned language models. In the pruned versions of the models, singleton n-grams of order three and higher are discarded, whereas all n-grams are kept in the unpruned versions. We follow the approach outlined by Huck et al. (2011) to augment the system with the synthetic parallel data. A foreground phrase table extracted from the human-generated parallel data is filled up with entries from a background phrase table extracted from the synthetic parallel data. An entry from the background table is only added if the foreground table does not already contain a similar entry (Bisazza et al., 2011). A binary feature distinguishes background phrases from foreground phrases. For the background phrase table, we extract only lexical phrases (i.e., phrases without non-terminals on their right-hand side) from the synthetic parallel data, no hierarchical phrases. The phrase length for entries of the background table is restricted to a maximum number of five terminal symbols on the source side. Lexical scores over the phrases extracted from synthetic data are calculated with a lexicon model learned from the human-generated parallel data, as proposed by Huck and Ney (2012). More hierarchical rul"
W16-2315,E14-2008,1,0.83418,"malized as a synchronous context-free grammar. We integrate several non-standard enhancements into our hierarchical phrase-based system and empirically evaluate their impact on translation quality. Our system is furthermore one component in a combination of systems by members of the HimL project and another EU-funded project, QT21.3 Measured in B LEU (Papineni et al., 2002), the QT21/HimL submission yields top translation quality amongst the shared task submissions.4 The QT21/HimL submission highlights the continued success of system combinations based on the Jane machine translation toolkit (Freitag et al., 2014a) in open evaluation campaigns (Freitag et al., 2013; Freitag et al., 2014b; Freitag et al., 2014c). A description of the QT21/HimL combined submission is given by Peter et al. (2016). This paper describes the hierarchical phrase-based machine translation system built jointly by the University of Edinburgh and the University of Munich (LMU) for the shared translation task at the ACL 2016 First Conference on Machine Translation (WMT16). The WMT16 Edinburgh/LMU system was trained for translation of news domain texts from English into Romanian. We participated in the shared task for machine tran"
W16-2315,J93-2003,0,0.0655029,"Missing"
W16-2315,W14-3310,1,0.882823,"malized as a synchronous context-free grammar. We integrate several non-standard enhancements into our hierarchical phrase-based system and empirically evaluate their impact on translation quality. Our system is furthermore one component in a combination of systems by members of the HimL project and another EU-funded project, QT21.3 Measured in B LEU (Papineni et al., 2002), the QT21/HimL submission yields top translation quality amongst the shared task submissions.4 The QT21/HimL submission highlights the continued success of system combinations based on the Jane machine translation toolkit (Freitag et al., 2014a) in open evaluation campaigns (Freitag et al., 2013; Freitag et al., 2014b; Freitag et al., 2014c). A description of the QT21/HimL combined submission is given by Peter et al. (2016). This paper describes the hierarchical phrase-based machine translation system built jointly by the University of Edinburgh and the University of Munich (LMU) for the shared translation task at the ACL 2016 First Conference on Machine Translation (WMT16). The WMT16 Edinburgh/LMU system was trained for translation of news domain texts from English into Romanian. We participated in the shared task for machine tran"
W16-2315,buck-etal-2014-n,0,0.0874284,"Missing"
W16-2315,2014.iwslt-evaluation.7,1,0.880814,"malized as a synchronous context-free grammar. We integrate several non-standard enhancements into our hierarchical phrase-based system and empirically evaluate their impact on translation quality. Our system is furthermore one component in a combination of systems by members of the HimL project and another EU-funded project, QT21.3 Measured in B LEU (Papineni et al., 2002), the QT21/HimL submission yields top translation quality amongst the shared task submissions.4 The QT21/HimL submission highlights the continued success of system combinations based on the Jane machine translation toolkit (Freitag et al., 2014a) in open evaluation campaigns (Freitag et al., 2013; Freitag et al., 2014b; Freitag et al., 2014c). A description of the QT21/HimL combined submission is given by Peter et al. (2016). This paper describes the hierarchical phrase-based machine translation system built jointly by the University of Edinburgh and the University of Munich (LMU) for the shared translation task at the ACL 2016 First Conference on Machine Translation (WMT16). The WMT16 Edinburgh/LMU system was trained for translation of news domain texts from English into Romanian. We participated in the shared task for machine tran"
W16-2315,N12-1047,0,0.074748,"; Koehn et al., 2003) to the two trained alignments. We extract synchronous context-free grammar rules that are consistent with the symmetrized word alignment from the parallel training data. We train 5-gram language models (LMs) with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). KenLM (Heafield, 2011) is employed for LM training and scoring, and SRILM (Stolcke, 2002) for linear LM interpolation. Our translation model incorporates a number of different features in a log-linear combination (Och and Ney, 2002). We tune the feature weights with batch k-best M IRA (Cherry and Foster, 2012) to maximize B LEU (Papineni et al., 2002) on a development set. We run M IRA for 25 iterations on 200-best lists. 2.4 2.5 Enhancements We now describe modifications that we apply on top of the baseline. The results of the empirical evaluation will be given in Section 3. Linear LM interpolation vs. individual LMs as features in the log-linear combination. Rather than employing a linearly interpolated LM, we integrate the individual LMs trained over the separate corpora (news2015, Europarl, SETimes2) directly into the log-linear feature combination of the system and let M IRA optimize their wei"
W16-2315,D08-1089,0,0.0533428,"f words covered by non-terminals at extraction time. Phrase orientation model. We implemented a feature in Moses that resembles the phrase orientation model for hierarchical machine translation as described by Huck et al. (2013). The Huck et al. (2013) implementation had been released as part of the Jane toolkit (Vilar et al., 2010; Vilar et al., 2012; Huck et al., 2012). Our new Moses implementation technically operates in almost the same manner, except for minor implementation differences. Similarly to the type of lexicalized reordering models that are in common use in phrase-based systems (Galley and Manning, 2008), our model estimates the probabilities of orientation classes for each phrase (or: rule) from the training data. We use three orientation classes: monotone, swap, and discontinuous.7 Larger development data. Since no dedicated unseen test set was available during system building, newsdev2016 was split into its first half (newsdev2016_1) and its second half (newsdev2016_2) so that we could tune on the first half and keep the second half untouched for evaluating progress in translation quality with the various enhancements. For the final system (our primary submission), we took the best configu"
W16-2315,W08-0509,0,0.0437082,"_1 is utilized for tuning. 2.3 We discard rules with non-terminals on their right-hand side if they are singletons in the training data. The baseline language model is a linear interpolation of three 5-gram LMs trained over the Romanian news2015, Europarl, and SETimes2 training data, respectively, with pruning of singleton n-grams of order three and higher.6 We run the Moses chart-based decoder with cube pruning, configured at a maximum chart span of 25 and otherwise default settings. Training and Tuning We create word alignments by aligning the bilingual data in both directions with MGIZA++ (Gao and Vogel, 2008). We use a sequence of IBM word alignment models (Brown et al., 1993) with five iterations of EM training (Dempster et al., 1977) of Model 1, three iterations of Model 3, and three iterations of Model 4. After EM, we obtain a symmetrized alignment by applying the grow-diag-final-and heuristic (Och and Ney, 2003; Koehn et al., 2003) to the two trained alignments. We extract synchronous context-free grammar rules that are consistent with the symmetrized word alignment from the parallel training data. We train 5-gram language models (LMs) with modified Kneser-Ney smoothing (Kneser and Ney, 1995;"
W16-2315,2005.mtsummit-papers.11,0,0.104257,"S is the start symbol of the grammar. The generic non-terminal X is used as a placeholder for the gaps within the right-hand side of hierarchical translation rules as well as on all left-hand sides of the translation rules that are extracted from the parallel training corpus. Extracted rules of a standard hierarchical grammar are of the form X → hα, β ,∼ i where hα, β i 2.2 Data and Preprocessing Our system is trained using only permissible Romanian monolingual and English–Romanian parallel corpora provided by the organizers of the WMT16 shared task for machine translation of news: Europarl (Koehn, 2005), SETimes2 (Tyers and Alperen, 2010), News Crawl articles from 2015 (denoted as news2015 hereafter), and CommonCrawl (Buck et al., 2014). The target side of the data is preprocessed with tokro, LIMSI’s tokenizer for Romanian (Allauzen et al., 2016).5 The English source side is tokenized using the tokenizer.perl script from the Moses toolkit. Romanian and English sentences are both frequent-cased (with Moses’ truecase.perl). 5 https://perso.limsi.fr/aufrant/ software/tokro 312 • An indicator feature that fires on applications of the glue rule. • Word penalty. • Rule penalty. • A 5-gram language"
W16-2315,W15-3013,1,0.839627,"h machine-translated English counterparts is utilized for lightly-supervised training (Schwenk, 2008) of our English→Romanian hierarchical system. 7 Using Moses’ Experiment Management System (EMS) (Koehn, 2010), the phrase orientation model for hierarchical machine translation can be activated by simply adding a line phrase-orientation = true to the [TRAINING] section of the EMS configuration file. 8 Whenever available, we typically attempt to use large development sets (in the order of a few thousand sentences), e.g. for Edinburgh’s phrase-based systems for the German– English language pair (Haddow et al., 2015). 314 en→ro newsdev2016_1 newsdev2016_2 newstest2016 baseline with interpolated LM over news2015, Europarl, SETimes2 + three individual LMs (replacing the interpolated LM) + background LM over concatenation of news2015, Europarl, SETimes2 + CommonCrawl LM training data in background LM + all LMs unpruned + more hierarchical rules + phrase orientation model + lightly-supervised training (contrastive submission system) + tuning on full newsdev2016 (primary submission system) 22.1 21.6 22.2 23.1 23.4 23.1 24.4 24.8 24.5 26.6 26.6 27.1 28.3 28.6 29.0 29.5 30.2 30.9 23.0 22.9 23.3 24.4 24.4 24.7 25"
W16-2315,P02-1038,0,0.264252,"lignment by applying the grow-diag-final-and heuristic (Och and Ney, 2003; Koehn et al., 2003) to the two trained alignments. We extract synchronous context-free grammar rules that are consistent with the symmetrized word alignment from the parallel training data. We train 5-gram language models (LMs) with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). KenLM (Heafield, 2011) is employed for LM training and scoring, and SRILM (Stolcke, 2002) for linear LM interpolation. Our translation model incorporates a number of different features in a log-linear combination (Och and Ney, 2002). We tune the feature weights with batch k-best M IRA (Cherry and Foster, 2012) to maximize B LEU (Papineni et al., 2002) on a development set. We run M IRA for 25 iterations on 200-best lists. 2.4 2.5 Enhancements We now describe modifications that we apply on top of the baseline. The results of the empirical evaluation will be given in Section 3. Linear LM interpolation vs. individual LMs as features in the log-linear combination. Rather than employing a linearly interpolated LM, we integrate the individual LMs trained over the separate corpora (news2015, Europarl, SETimes2) directly into th"
W16-2315,W11-2123,0,0.0606968,"alignment models (Brown et al., 1993) with five iterations of EM training (Dempster et al., 1977) of Model 1, three iterations of Model 3, and three iterations of Model 4. After EM, we obtain a symmetrized alignment by applying the grow-diag-final-and heuristic (Och and Ney, 2003; Koehn et al., 2003) to the two trained alignments. We extract synchronous context-free grammar rules that are consistent with the symmetrized word alignment from the parallel training data. We train 5-gram language models (LMs) with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). KenLM (Heafield, 2011) is employed for LM training and scoring, and SRILM (Stolcke, 2002) for linear LM interpolation. Our translation model incorporates a number of different features in a log-linear combination (Och and Ney, 2002). We tune the feature weights with batch k-best M IRA (Cherry and Foster, 2012) to maximize B LEU (Papineni et al., 2002) on a development set. We run M IRA for 25 iterations on 200-best lists. 2.4 2.5 Enhancements We now describe modifications that we apply on top of the baseline. The results of the empirical evaluation will be given in Section 3. Linear LM interpolation vs. individual"
W16-2315,J03-1002,0,0.0115423,"singleton n-grams of order three and higher.6 We run the Moses chart-based decoder with cube pruning, configured at a maximum chart span of 25 and otherwise default settings. Training and Tuning We create word alignments by aligning the bilingual data in both directions with MGIZA++ (Gao and Vogel, 2008). We use a sequence of IBM word alignment models (Brown et al., 1993) with five iterations of EM training (Dempster et al., 1977) of Model 1, three iterations of Model 3, and three iterations of Model 4. After EM, we obtain a symmetrized alignment by applying the grow-diag-final-and heuristic (Och and Ney, 2003; Koehn et al., 2003) to the two trained alignments. We extract synchronous context-free grammar rules that are consistent with the symmetrized word alignment from the parallel training data. We train 5-gram language models (LMs) with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). KenLM (Heafield, 2011) is employed for LM training and scoring, and SRILM (Stolcke, 2002) for linear LM interpolation. Our translation model incorporates a number of different features in a log-linear combination (Och and Ney, 2002). We tune the feature weights with batch k-best M IRA ("
W16-2315,2009.iwslt-papers.4,0,0.0920231,"sentence, respectively. Therefore, two more special rules S → h&lt;s>, &lt;s>i S → hS∼0 &lt;/s>, S∼0 &lt;/s>i 2.1 (2) are included which allow the decoder to finalize its translations. Hierarchical search is conducted with a customized version of the CYK+ parsing algorithm (Chappelier and Rajman, 1998) and cube pruning (Chiang, 2007). A hypergraph which represents the whole parsing space is built employing CYK+. Cube pruning operates in bottom-up topological order on this hypergraph and expands at most k derivations at each hypernode. All our experiments are run with the open source Moses implementation (Hoang et al., 2009) of the hierarchical phrase-based translation paradigm. 2 (1) System Overview Hierarchical Phrase-Based Translation In hierarchical phrase-based translation, a probabilistic synchronous context-free grammar is induced from bilingual training corpora. In addition to continuous lexical phrases as in standard phrase-based translation, hierarchical phrases with (usually) up to two non-terminals are extracted from the word-aligned parallel training data. The non-terminal set of a standard hierarchical grammar comprises two symbols which are shared by source and target: the initial symbol S and one"
W16-2315,P02-1040,0,0.0975049,"erarchical phrase-based translation (Chiang, 2005) for English→Romanian, a statistical machine translation paradigm that is closely related to phrasebased translation, but allows for phrases with gaps. Conceptionally, the translation model is formalized as a synchronous context-free grammar. We integrate several non-standard enhancements into our hierarchical phrase-based system and empirically evaluate their impact on translation quality. Our system is furthermore one component in a combination of systems by members of the HimL project and another EU-funded project, QT21.3 Measured in B LEU (Papineni et al., 2002), the QT21/HimL submission yields top translation quality amongst the shared task submissions.4 The QT21/HimL submission highlights the continued success of system combinations based on the Jane machine translation toolkit (Freitag et al., 2014a) in open evaluation campaigns (Freitag et al., 2013; Freitag et al., 2014b; Freitag et al., 2014c). A description of the QT21/HimL combined submission is given by Peter et al. (2016). This paper describes the hierarchical phrase-based machine translation system built jointly by the University of Edinburgh and the University of Munich (LMU) for the shar"
W16-2315,2012.amta-papers.8,1,0.924532,"ain a similar entry (Bisazza et al., 2011). A binary feature distinguishes background phrases from foreground phrases. For the background phrase table, we extract only lexical phrases (i.e., phrases without non-terminals on their right-hand side) from the synthetic parallel data, no hierarchical phrases. The phrase length for entries of the background table is restricted to a maximum number of five terminal symbols on the source side. Lexical scores over the phrases extracted from synthetic data are calculated with a lexicon model learned from the human-generated parallel data, as proposed by Huck and Ney (2012). More hierarchical rules. The baseline synchronous context-free grammar rules in the phrase table are extracted from the parallel training data with Moses’ default settings: a maximum of five symbols on the source side, a maximum span of ten words, and no right-hand side non-terminal at gaps that cover only a single word on the source side. We allow for extraction of more hierarchical rules by applying less strict rule extraction constraints: a maximum of ten symbols on the source side, a maximum span of twenty words, and no lower limit to the amount of words covered by non-terminals at extra"
W16-2315,W11-2211,1,0.857685,"exploring non-standard enhancements and non-default configuration settings such as: • Individual language models as features, rather than a single linearly interpolated language model; and another background language model estimated over concatenated corpora. • Large CommonCrawl language model training data. • Unpruned language models. • More hierarchical rules than in default systems, by means of imposing less strict extraction constraints. • A phrase orientation model for hierarchical translation (Huck et al., 2013). • Lightly-supervised training (Schwenk, 2008; Schwenk and Senellart, 2009; Huck et al., 2011). • Larger development data for tuning. S → hS∼0 X ∼1 , S∼0 X ∼1 i is incorporated into the hierarchical grammar that the system can use for serial concatenation of phrases as in monotonic phrase-based translation. In the Moses implementation, the decoder internally adds a sentence start terminal symbol &lt;s> and a sentence end terminal symbol &lt;/s> to the input before and after each sentence, respectively. Therefore, two more special rules S → h&lt;s>, &lt;s>i S → hS∼0 &lt;/s>, S∼0 &lt;/s>i 2.1 (2) are included which allow the decoder to finalize its translations. Hierarchical search is conducted with a cus"
W16-2315,2009.mtsummit-posters.17,0,0.0231433,"with a focus of interest on exploring non-standard enhancements and non-default configuration settings such as: • Individual language models as features, rather than a single linearly interpolated language model; and another background language model estimated over concatenated corpora. • Large CommonCrawl language model training data. • Unpruned language models. • More hierarchical rules than in default systems, by means of imposing less strict extraction constraints. • A phrase orientation model for hierarchical translation (Huck et al., 2013). • Lightly-supervised training (Schwenk, 2008; Schwenk and Senellart, 2009; Huck et al., 2011). • Larger development data for tuning. S → hS∼0 X ∼1 , S∼0 X ∼1 i is incorporated into the hierarchical grammar that the system can use for serial concatenation of phrases as in monotonic phrase-based translation. In the Moses implementation, the decoder internally adds a sentence start terminal symbol &lt;s> and a sentence end terminal symbol &lt;/s> to the input before and after each sentence, respectively. Therefore, two more special rules S → h&lt;s>, &lt;s>i S → hS∼0 &lt;/s>, S∼0 &lt;/s>i 2.1 (2) are included which allow the decoder to finalize its translations. Hierarchical search is"
W16-2315,W13-2258,1,0.952949,"nting the particularities of our hierarchical phrase-based system, with a focus of interest on exploring non-standard enhancements and non-default configuration settings such as: • Individual language models as features, rather than a single linearly interpolated language model; and another background language model estimated over concatenated corpora. • Large CommonCrawl language model training data. • Unpruned language models. • More hierarchical rules than in default systems, by means of imposing less strict extraction constraints. • A phrase orientation model for hierarchical translation (Huck et al., 2013). • Lightly-supervised training (Schwenk, 2008; Schwenk and Senellart, 2009; Huck et al., 2011). • Larger development data for tuning. S → hS∼0 X ∼1 , S∼0 X ∼1 i is incorporated into the hierarchical grammar that the system can use for serial concatenation of phrases as in monotonic phrase-based translation. In the Moses implementation, the decoder internally adds a sentence start terminal symbol &lt;s> and a sentence end terminal symbol &lt;/s> to the input before and after each sentence, respectively. Therefore, two more special rules S → h&lt;s>, &lt;s>i S → hS∼0 &lt;/s>, S∼0 &lt;/s>i 2.1 (2) are included wh"
W16-2315,2008.iwslt-papers.6,0,0.0482276,"e-based system, with a focus of interest on exploring non-standard enhancements and non-default configuration settings such as: • Individual language models as features, rather than a single linearly interpolated language model; and another background language model estimated over concatenated corpora. • Large CommonCrawl language model training data. • Unpruned language models. • More hierarchical rules than in default systems, by means of imposing less strict extraction constraints. • A phrase orientation model for hierarchical translation (Huck et al., 2013). • Lightly-supervised training (Schwenk, 2008; Schwenk and Senellart, 2009; Huck et al., 2011). • Larger development data for tuning. S → hS∼0 X ∼1 , S∼0 X ∼1 i is incorporated into the hierarchical grammar that the system can use for serial concatenation of phrases as in monotonic phrase-based translation. In the Moses implementation, the decoder internally adds a sentence start terminal symbol &lt;s> and a sentence end terminal symbol &lt;/s> to the input before and after each sentence, respectively. Therefore, two more special rules S → h&lt;s>, &lt;s>i S → hS∼0 &lt;/s>, S∼0 &lt;/s>i 2.1 (2) are included which allow the decoder to finalize its translat"
W16-2315,2015.mtsummit-papers.19,1,0.913146,"making documents originally written in English available to a large community of speakers in their native language, Romanian. Applications are for instance in the health care sector, where, as part of the Health in my Language project (HimL), several project partners intend to make public health information available in a wider variety of languages.2 The WMT task provides an interesting test bed for English→Romanian machine translation, though adaptation towards the specific domain (consumer health for HimL, rather than news) is also an important aspect that has to be considered in practice (Huck et al., 2015). We investigate the effectiveness of hierarchical phrase-based translation (Chiang, 2005) for English→Romanian, a statistical machine translation paradigm that is closely related to phrasebased translation, but allows for phrases with gaps. Conceptionally, the translation model is formalized as a synchronous context-free grammar. We integrate several non-standard enhancements into our hierarchical phrase-based system and empirically evaluate their impact on translation quality. Our system is furthermore one component in a combination of systems by members of the HimL project and another EU-fu"
W16-2315,N03-1017,0,0.0312287,"f order three and higher.6 We run the Moses chart-based decoder with cube pruning, configured at a maximum chart span of 25 and otherwise default settings. Training and Tuning We create word alignments by aligning the bilingual data in both directions with MGIZA++ (Gao and Vogel, 2008). We use a sequence of IBM word alignment models (Brown et al., 1993) with five iterations of EM training (Dempster et al., 1977) of Model 1, three iterations of Model 3, and three iterations of Model 4. After EM, we obtain a symmetrized alignment by applying the grow-diag-final-and heuristic (Och and Ney, 2003; Koehn et al., 2003) to the two trained alignments. We extract synchronous context-free grammar rules that are consistent with the symmetrized word alignment from the parallel training data. We train 5-gram language models (LMs) with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). KenLM (Heafield, 2011) is employed for LM training and scoring, and SRILM (Stolcke, 2002) for linear LM interpolation. Our translation model incorporates a number of different features in a log-linear combination (Och and Ney, 2002). We tune the feature weights with batch k-best M IRA (Cherry and Foster, 20"
W16-2315,W10-1738,1,0.878506,"on-terminal at gaps that cover only a single word on the source side. We allow for extraction of more hierarchical rules by applying less strict rule extraction constraints: a maximum of ten symbols on the source side, a maximum span of twenty words, and no lower limit to the amount of words covered by non-terminals at extraction time. Phrase orientation model. We implemented a feature in Moses that resembles the phrase orientation model for hierarchical machine translation as described by Huck et al. (2013). The Huck et al. (2013) implementation had been released as part of the Jane toolkit (Vilar et al., 2010; Vilar et al., 2012; Huck et al., 2012). Our new Moses implementation technically operates in almost the same manner, except for minor implementation differences. Similarly to the type of lexicalized reordering models that are in common use in phrase-based systems (Galley and Manning, 2008), our model estimates the probabilities of orientation classes for each phrase (or: rule) from the training data. We use three orientation classes: monotone, swap, and discontinuous.7 Larger development data. Since no dedicated unseen test set was available during system building, newsdev2016 was split into"
W16-2315,W16-2327,1,0.705098,"Missing"
W16-2315,P05-1033,0,\N,Missing
W16-2315,2013.iwslt-evaluation.16,1,\N,Missing
W16-2315,W16-2320,1,\N,Missing
W16-2320,W16-2304,1,0.833347,"factored word representation of the source and the target. On the source side we use the word surface form and two automatic word classes using 100 and 1,000 classes. On the Romanian side, we add the POS information as an additional word factor. 2 3.2 Preprocessing The data provided for the task was preprocessed once, by LIMSI, and shared with all the participants, in order to ensure consistency between systems. On the English side, preprocessing consists of tokenizing and truecasing using the Moses toolkit (Koehn et al., 2007). On the Romanian side, the data is tokenized using LIMSI’s tokro (Allauzen et al., 2016), a rulebased tokenizer that mainly normalizes diacritics and splits punctuation and clitics. This data is truecased in the same way as the English side. In addition, the Romanian sentences are also tagged, lemmatized, and chunked using the TTL tagger (Tufis¸ et al., 2008). 3 The LIMSI system uses NCODE (Crego et al., 2011), which implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Crego and Mari˜no, 2006; Mari˜no et al., 2006) that is closely related to the standard phrase-based approach (Zens et al., 2002). In this framework, translation is divided into two steps. T"
W16-2320,W05-0909,0,0.583183,"ions from multiple hypotheses which are obtained from different translation approaches, i.e., the systems described in the previous section. A system combination implementation developed at RWTH Aachen University (Freitag et al., 2014a) is used to combine the outputs of the different engines. The consensus translations outperform the individual hypotheses in terms of translation quality. The first step in system combination is the generation of confusion networks (CN) from I input translation hypotheses. We need pairwise alignments between the input hypotheses, which are obtained from METEOR (Banerjee and Lavie, 2005). The hypotheses are then reordered to match a selected skeleton hypothesis in terms of word ordering. We generate I different CNs, each having one of the input systems as the skeleton hypothesis, and the final lattice is the union of all I generated CNs. In Figure 1 an example of a confusion network with I = 4 input translations is depicted. Decoding of a confusion network finds the best path in the network. Each arc is assigned a score of a linear model combination of M different models, which includes word penalty, 3-gram language model trained on the input hypotheses, a binary primary syst"
W16-2320,P13-2071,1,0.873371,"odel trained on all available data. Words in the Common Crawl dataset that appear fewer than 500 times were replaced by UNK, and all singleton ngrams of order 3 or higher were pruned. We also use a 7-gram class-based language model, trained on the same data. 512 word Edinburgh Phrase-based System Edinburgh’s phrase-based system is built using the Moses toolkit, with fast align (Dyer et al., 2013) for word alignment, and KenLM (Heafield et al., 2013) for language model training. In our Moses setup, we use hierarchical lexicalized reordering (Galley and Manning, 2008), operation sequence model (Durrani et al., 2013), domain indicator features, and binned phrase count features. We use all available parallel data for the translation model, and all available Romanian text for the language model. We use two different 5-gram language models; one built from all the monolingual target text concatenated, without pruning, and one 3 USFD Phrase-based System 4 http://www.quest.dcs.shef.ac.uk/ quest_files/features_blackbox_baseline_ 17 https://github.com/rsennrich/nematus 348 the large building the large home a big huge house house a newsdev2016/1 and newsdev2016/2. The first part was used as development set while t"
W16-2320,D15-1129,1,0.847985,"training and evaluation. The language model uses 3 stacked LSTM layers, with 350 nodes each. The BJM has a projection layer, and computes a forLMU The LMU system integrates a discriminative rule selection model into a hierarchical SMT system, as described in (Tamchyna et al., 2014). The rule selection model is implemented using the highspeed classifier Vowpal Wabbit2 which is fully integrated in Moses’ hierarchical decoder. During decoding, the rule selection model is called at each rule application with syntactic context information as feature templates. The features are the same as used by Braune et al. (2015) in their string-to-tree system, including both lexical and soft source syntax features. The translation model features comprise the standard hierarchical features (Chiang, 2005) with an additional feature for the rule selection model (Braune et al., 2016). Before training, we reduce the number of translation rules using significance testing (Johnson et al., 2007). To extract the features of the rule selection model, we parse the English part of our 2 http://hunch.net/˜vw/ (VW). Implemented by John Langford and many others. 346 URLs, e-mail addresses, etc.). During translation a rule-based loc"
W16-2320,N13-1073,0,0.034805,"preordering model of (de Gispert et al., 2015). The preordering model is trained for 30 iterations on the full MGIZA-aligned training data. We use two language models, built using KenLM. The first is a 5-gram language model trained on all available data. Words in the Common Crawl dataset that appear fewer than 500 times were replaced by UNK, and all singleton ngrams of order 3 or higher were pruned. We also use a 7-gram class-based language model, trained on the same data. 512 word Edinburgh Phrase-based System Edinburgh’s phrase-based system is built using the Moses toolkit, with fast align (Dyer et al., 2013) for word alignment, and KenLM (Heafield et al., 2013) for language model training. In our Moses setup, we use hierarchical lexicalized reordering (Galley and Manning, 2008), operation sequence model (Durrani et al., 2013), domain indicator features, and binned phrase count features. We use all available parallel data for the translation model, and all available Romanian text for the language model. We use two different 5-gram language models; one built from all the monolingual target text concatenated, without pruning, and one 3 USFD Phrase-based System 4 http://www.quest.dcs.shef.ac.uk/ ques"
W16-2320,J04-2004,0,0.0194677,"en systems. On the English side, preprocessing consists of tokenizing and truecasing using the Moses toolkit (Koehn et al., 2007). On the Romanian side, the data is tokenized using LIMSI’s tokro (Allauzen et al., 2016), a rulebased tokenizer that mainly normalizes diacritics and splits punctuation and clitics. This data is truecased in the same way as the English side. In addition, the Romanian sentences are also tagged, lemmatized, and chunked using the TTL tagger (Tufis¸ et al., 2008). 3 The LIMSI system uses NCODE (Crego et al., 2011), which implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Crego and Mari˜no, 2006; Mari˜no et al., 2006) that is closely related to the standard phrase-based approach (Zens et al., 2002). In this framework, translation is divided into two steps. To translate a source sentence into a target sentence, the source sentence is first reordered according to a set of rewriting rules so as to reproduce the target word order. This generates a word lattice containing the most promising source permutations, which is then translated. Since the translation step is monotonic, this approach is able to rely on the n-gram assumption to decompose the joint probabilit"
W16-2320,2011.mtsummit-papers.30,0,0.020392,"-based System The RWTH hierarchical setup uses the open source translation toolkit Jane 2.3 (Vilar et al., 2010). Hierarchical phrase-based translation (HPBT) (Chiang, 2007) induces a weighted synchronous context-free grammar from parallel text. In addition to the contiguous lexical phrases, as used in phrase-based translation (PBT), hierarchical phrases with up to two gaps are also extracted. Our baseline model contains models with phrase translation probabilities and lexical smoothing probabilities in both translation directions, word and phrase penalty, and enhanced low frequency features (Chen et al., 2011). It also contains binary features to distinguish between hierarchical and non-hierarchical phrases, the glue rule, and rules with non-terminals at the boundaries. We use the cube pruning algorithm (Huang and Chiang, 2007) for decoding. The system uses three backoff language models (LM) that are estimated with the KenLM toolkit (Heafield et al., 2013) and are integrated into the decoder as separate models in the log-linear combination: a full 4-gram LM (trained on all data), a limited 5-gram LM (trained only on in-domain data), and a 7-gram word class language model (wcLM) (Wuebker et al., 201"
W16-2320,N12-1047,0,0.591808,"rescoring. The phrase-based system is trained on all available parallel training data. The phrase 345 more specifically its implementation in XenC (Rousseau, 2013). As a result, one third of the initial corpus is removed. Finally, we make a linear interpolation of these models, using the SRILM toolkit (Stolcke, 2002). 3.3 training data using the Berkeley parser (Petrov et al., 2006). For model prediction during tuning and decoding, we use parsed versions of the development and test sets. We train the rule selection model using VW and tune the weights of the translation model using batch MIRA (Cherry and Foster, 2012). The 5-gram language model is trained using KenLM (Heafield et al., 2013) on the Romanian part of the Common Crawl corpus concatenated with the Romanian part of the training data. LMU-CUNI The LMU-CUNI contribution is a constrained Moses phrase-based system. It uses a simple factored setting: our phrase table produces not only the target surface form but also its lemma and morphological tag. On the input, we include lemmas, POS tags and information from dependency parses (lemma of the parent node and syntactic relation), all encoded as additional factors. The main difference from a standard p"
W16-2320,E14-2008,1,0.72015,"tems are combined using RWTH’s system combination approach. The final submission shows an improvement of 1.0 B LEU compared to the best single system on newstest2016. 1 Introduction Quality Translation 21 (QT21) is a European machine translation research project with the aim 1 http://www.statmt.org/wmt16/ translation-task.html 344 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 344–355, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics mented in Jane, RWTH’s open source statistical machine translation toolkit (Freitag et al., 2014a). The Jane system combination is a mature implementation which previously has been successfully employed in other collaborative projects and for different language pairs (Freitag et al., 2013; Freitag et al., 2014b; Freitag et al., 2014c). In the remainder of the paper, we present the technical details of the QT21/HimL combined machine translation system and the experimental results obtained with it. The paper is structured as follows: We describe the common preprocessing used for most of the individual engines in Section 2. Section 3 covers the characteristics of the different individual en"
W16-2320,P05-1033,0,0.151933,"ative rule selection model into a hierarchical SMT system, as described in (Tamchyna et al., 2014). The rule selection model is implemented using the highspeed classifier Vowpal Wabbit2 which is fully integrated in Moses’ hierarchical decoder. During decoding, the rule selection model is called at each rule application with syntactic context information as feature templates. The features are the same as used by Braune et al. (2015) in their string-to-tree system, including both lexical and soft source syntax features. The translation model features comprise the standard hierarchical features (Chiang, 2005) with an additional feature for the rule selection model (Braune et al., 2016). Before training, we reduce the number of translation rules using significance testing (Johnson et al., 2007). To extract the features of the rule selection model, we parse the English part of our 2 http://hunch.net/˜vw/ (VW). Implemented by John Langford and many others. 346 URLs, e-mail addresses, etc.). During translation a rule-based localisation feature is applied. ward recurrent state encoding the source and target history, a backward recurrent state encoding the source future, and a third LSTM layer to combin"
W16-2320,J07-2003,0,0.558191,"this model is to better condition lexical choices by using the source context and to improve morphological and topical coherence by modeling the (limited left-hand side) target context. We also take advantage of the target factors by using a 7-gram language model trained on sequences of Romanian morphological tags. Finally, our system also uses a standard lexicalized reordering model. 3.4 3.5 RWTH Aachen University: Hierarchical Phrase-based System The RWTH hierarchical setup uses the open source translation toolkit Jane 2.3 (Vilar et al., 2010). Hierarchical phrase-based translation (HPBT) (Chiang, 2007) induces a weighted synchronous context-free grammar from parallel text. In addition to the contiguous lexical phrases, as used in phrase-based translation (PBT), hierarchical phrases with up to two gaps are also extracted. Our baseline model contains models with phrase translation probabilities and lexical smoothing probabilities in both translation directions, word and phrase penalty, and enhanced low frequency features (Chen et al., 2011). It also contains binary features to distinguish between hierarchical and non-hierarchical phrases, the glue rule, and rules with non-terminals at the bou"
W16-2320,W14-3310,1,0.909876,"tems are combined using RWTH’s system combination approach. The final submission shows an improvement of 1.0 B LEU compared to the best single system on newstest2016. 1 Introduction Quality Translation 21 (QT21) is a European machine translation research project with the aim 1 http://www.statmt.org/wmt16/ translation-task.html 344 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 344–355, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics mented in Jane, RWTH’s open source statistical machine translation toolkit (Freitag et al., 2014a). The Jane system combination is a mature implementation which previously has been successfully employed in other collaborative projects and for different language pairs (Freitag et al., 2013; Freitag et al., 2014b; Freitag et al., 2014c). In the remainder of the paper, we present the technical details of the QT21/HimL combined machine translation system and the experimental results obtained with it. The paper is structured as follows: We describe the common preprocessing used for most of the individual engines in Section 2. Section 3 covers the characteristics of the different individual en"
W16-2320,D14-1179,0,0.0138582,"Missing"
W16-2320,2014.iwslt-evaluation.7,1,0.925781,"tems are combined using RWTH’s system combination approach. The final submission shows an improvement of 1.0 B LEU compared to the best single system on newstest2016. 1 Introduction Quality Translation 21 (QT21) is a European machine translation research project with the aim 1 http://www.statmt.org/wmt16/ translation-task.html 344 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 344–355, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics mented in Jane, RWTH’s open source statistical machine translation toolkit (Freitag et al., 2014a). The Jane system combination is a mature implementation which previously has been successfully employed in other collaborative projects and for different language pairs (Freitag et al., 2013; Freitag et al., 2014b; Freitag et al., 2014c). In the remainder of the paper, we present the technical details of the QT21/HimL combined machine translation system and the experimental results obtained with it. The paper is structured as follows: We describe the common preprocessing used for most of the individual engines in Section 2. Section 3 covers the characteristics of the different individual en"
W16-2320,D08-1089,0,0.0211464,", built using KenLM. The first is a 5-gram language model trained on all available data. Words in the Common Crawl dataset that appear fewer than 500 times were replaced by UNK, and all singleton ngrams of order 3 or higher were pruned. We also use a 7-gram class-based language model, trained on the same data. 512 word Edinburgh Phrase-based System Edinburgh’s phrase-based system is built using the Moses toolkit, with fast align (Dyer et al., 2013) for word alignment, and KenLM (Heafield et al., 2013) for language model training. In our Moses setup, we use hierarchical lexicalized reordering (Galley and Manning, 2008), operation sequence model (Durrani et al., 2013), domain indicator features, and binned phrase count features. We use all available parallel data for the translation model, and all available Romanian text for the language model. We use two different 5-gram language models; one built from all the monolingual target text concatenated, without pruning, and one 3 USFD Phrase-based System 4 http://www.quest.dcs.shef.ac.uk/ quest_files/features_blackbox_baseline_ 17 https://github.com/rsennrich/nematus 348 the large building the large home a big huge house house a newsdev2016/1 and newsdev2016/2. T"
W16-2320,N15-1105,0,0.046766,"Missing"
W16-2320,W08-0509,0,0.06624,"probability 0.2, and also drop out full words with probability 0.1. We clip the gradient norm to 1.0 (Pascanu et al., 2013). We train the models with Adadelta (Zeiler, 2012), reshuffling the training corpus between epochs. We validate the model every 10 000 minibatches via B LEU on a validation set, and perform early stopping on B LEU. Decoding is performed with beam search with a beam size of 12. A more detailed description of the system, and more experimental results, can be found in (Sennrich et al., 2016a). 3.10 3.11 USFD’s phrase-based system is built using the Moses toolkit, with MGIZA (Gao and Vogel, 2008) for word alignment and KenLM (Heafield et al., 2013) for language model training. We use all available parallel data for the translation model. A single 5-gram language model is built using all the target side of the parallel data and a subpart of the monolingual Romanian corpora selected with Xenc-v2 (Rousseau, 2013). For the latter we use all the parallel data as in-domain data and the first half of newsdev2016 as development set. The feature weights are tuned with MERT (Och, 2003) on the first half of newsdev2016. The system produces distinct 1000-best lists, for which we extend the featur"
W16-2320,W16-2315,1,0.820309,"vs et al., 2012) that features language-specific data filtering and cleaning modules. Tilde’s system was trained on all available parallel data. Two language models are trained using KenLM (Heafield, 2011): 1) a 5-gram model using the Europarl and SETimes2 corpora, and 2) a 3-gram model using the Common Crawl corpus. We also apply a custom tokenization tool that takes into account specifics of the Romanian language and handles non-translatable entities (e.g., file paths, 347 up with entries from a background phrase table extracted from the automatically produced News Crawl 2015 parallel data. Huck et al. (2016) give a more in-depth description of the Edinburgh/LMU hierarchical machine translation system, along with detailed experimental results. 3.9 built from only News Crawl 2015, with singleton 3-grams and above pruned out. The weights of all these features and models are tuned with k-best MIRA (Cherry and Foster, 2012) on first the half of newsdev2016. In decoding, we use MBR (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) with a pop-limit of 5000, and the Moses ”monotone at punctuation” switch (to prevent reordering across punctuation) (Koehn and Haddow, 2009). Edinburgh Neural Sy"
W16-2320,D07-1103,0,0.032902,"bbit2 which is fully integrated in Moses’ hierarchical decoder. During decoding, the rule selection model is called at each rule application with syntactic context information as feature templates. The features are the same as used by Braune et al. (2015) in their string-to-tree system, including both lexical and soft source syntax features. The translation model features comprise the standard hierarchical features (Chiang, 2005) with an additional feature for the rule selection model (Braune et al., 2016). Before training, we reduce the number of translation rules using significance testing (Johnson et al., 2007). To extract the features of the rule selection model, we parse the English part of our 2 http://hunch.net/˜vw/ (VW). Implemented by John Langford and many others. 346 URLs, e-mail addresses, etc.). During translation a rule-based localisation feature is applied. ward recurrent state encoding the source and target history, a backward recurrent state encoding the source future, and a third LSTM layer to combine them. All layers have 350 nodes. The neural networks are implemented using an extension of the RWTHLM toolkit (Sundermeyer et al., 2014b). The parameter weights are optimized with MERT ("
W16-2320,W14-3360,0,0.0191919,"NMT system on newsdev2016/2, but lags behind on newstest2016. Removing the by itself weakest system shows a slight degradation on newsdev2016/2 and newstest2016, hinting that it still provides valuable information. Table 2 shows a comparison between all systems by scoring the translation output against each other in T ER and B LEU. We see that the neural networks outputs differ the most from all the other systems. Figure 1: System A: the large building; System B: the large home; System C: a big house; System D: a huge house; Reference: the big house. classes were generated using the method of Green et al. (2014). 4 System Combination System combination produces consensus translations from multiple hypotheses which are obtained from different translation approaches, i.e., the systems described in the previous section. A system combination implementation developed at RWTH Aachen University (Freitag et al., 2014a) is used to combine the outputs of the different engines. The consensus translations outperform the individual hypotheses in terms of translation quality. The first step in system combination is the generation of confusion networks (CN) from I input translation hypotheses. We need pairwise alig"
W16-2320,P07-2045,1,0.010375,", 2015) as well as neural network language and translation models. These models use a factored word representation of the source and the target. On the source side we use the word surface form and two automatic word classes using 100 and 1,000 classes. On the Romanian side, we add the POS information as an additional word factor. 2 3.2 Preprocessing The data provided for the task was preprocessed once, by LIMSI, and shared with all the participants, in order to ensure consistency between systems. On the English side, preprocessing consists of tokenizing and truecasing using the Moses toolkit (Koehn et al., 2007). On the Romanian side, the data is tokenized using LIMSI’s tokro (Allauzen et al., 2016), a rulebased tokenizer that mainly normalizes diacritics and splits punctuation and clitics. This data is truecased in the same way as the English side. In addition, the Romanian sentences are also tagged, lemmatized, and chunked using the TTL tagger (Tufis¸ et al., 2008). 3 The LIMSI system uses NCODE (Crego et al., 2011), which implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Crego and Mari˜no, 2006; Mari˜no et al., 2006) that is closely related to the standard phrase-based"
W16-2320,P13-2121,0,0.0645203,"Missing"
W16-2320,W11-2123,0,0.124165,"nslation is divided into two steps. To translate a source sentence into a target sentence, the source sentence is first reordered according to a set of rewriting rules so as to reproduce the target word order. This generates a word lattice containing the most promising source permutations, which is then translated. Since the translation step is monotonic, this approach is able to rely on the n-gram assumption to decompose the joint probability of a sentence pair into a sequence of bilingual units called tuples. We train three Romanian 4-gram language models, pruning all singletons with KenLM (Heafield, 2011). We use the in-domain monolingual corpus, the Romanian side of the parallel corpora and a subset of the (out-of-domain) Common Crawl corpus as training data. We select indomain sentences from the latter using the MooreLewis (Moore and Lewis, 2010) filtering method, Translation Systems Each group contributed one or more systems. In this section the systems are presented in alphabetic order. 3.1 LIMSI KIT The KIT system consists of a phrase-based machine translation system using additional models in rescoring. The phrase-based system is trained on all available parallel training data. The phras"
W16-2320,N04-1022,0,0.037676,"f the Romanian language and handles non-translatable entities (e.g., file paths, 347 up with entries from a background phrase table extracted from the automatically produced News Crawl 2015 parallel data. Huck et al. (2016) give a more in-depth description of the Edinburgh/LMU hierarchical machine translation system, along with detailed experimental results. 3.9 built from only News Crawl 2015, with singleton 3-grams and above pruned out. The weights of all these features and models are tuned with k-best MIRA (Cherry and Foster, 2012) on first the half of newsdev2016. In decoding, we use MBR (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) with a pop-limit of 5000, and the Moses ”monotone at punctuation” switch (to prevent reordering across punctuation) (Koehn and Haddow, 2009). Edinburgh Neural System Edinburgh’s neural machine translation system is an attentional encoder-decoder (Bahdanau et al., 2015), which we train with nematus.3 We use byte-pair-encoding (BPE) to achieve openvocabulary translation with a fixed vocabulary of subword symbols (Sennrich et al., 2016c). We produce additional parallel training data by automatically translating the monolingual Romanian News Crawl 2015 corpu"
W16-2320,2015.iwslt-papers.3,1,0.744353,"g. It uses two word-based n-gram language models and three additional non-word language models. Two of them are automatic word class-based (Och, 1999) language models, using 100 and 1,000 word classes. In addition, we use a POS-based language model. During decoding, we use a discriminative word lexicon (Niehues and Waibel, 2013) as well. We rescore the system output using a 300-best list. The weights are optimized on the concatenation of the development data and the SETimes2 dev set using the ListNet algorithm (Niehues et al., 2015). In rescoring, we add the source discriminative word lexica (Herrmann et al., 2015) as well as neural network language and translation models. These models use a factored word representation of the source and the target. On the source side we use the word surface form and two automatic word classes using 100 and 1,000 classes. On the Romanian side, we add the POS information as an additional word factor. 2 3.2 Preprocessing The data provided for the task was preprocessed once, by LIMSI, and shared with all the participants, in order to ensure consistency between systems. On the English side, preprocessing consists of tokenizing and truecasing using the Moses toolkit (Koehn e"
W16-2320,J06-4004,0,0.106202,"Missing"
W16-2320,2009.iwslt-papers.4,0,0.0984189,"target history, a backward recurrent state encoding the source future, and a third LSTM layer to combine them. All layers have 350 nodes. The neural networks are implemented using an extension of the RWTHLM toolkit (Sundermeyer et al., 2014b). The parameter weights are optimized with MERT (Och, 2003) towards the B LEU metric. 3.6 3.8 The UEDIN-LMU HPBT system is a hierarchical phrase-based machine translation system (Chiang, 2005) built jointly by the University of Edinburgh and LMU Munich. The system is based on the open source Moses implementation of the hierarchical phrase-based paradigm (Hoang et al., 2009). In addition to a set of standard features in a log-linear combination, a number of non-standard enhancements are employed to achieve improved translation quality. Specifically, we integrate individual language models trained over the separate corpora (News Crawl 2015, Europarl, SETimes2) directly into the log-linear combination of the system and let MIRA (Cherry and Foster, 2012) optimize their weights along with all other features in tuning, rather than relying on a single linearly interpolated language model. We add another background language model estimated over a concatenation of all Ro"
W16-2320,P10-2041,0,0.0238386,"ontaining the most promising source permutations, which is then translated. Since the translation step is monotonic, this approach is able to rely on the n-gram assumption to decompose the joint probability of a sentence pair into a sequence of bilingual units called tuples. We train three Romanian 4-gram language models, pruning all singletons with KenLM (Heafield, 2011). We use the in-domain monolingual corpus, the Romanian side of the parallel corpora and a subset of the (out-of-domain) Common Crawl corpus as training data. We select indomain sentences from the latter using the MooreLewis (Moore and Lewis, 2010) filtering method, Translation Systems Each group contributed one or more systems. In this section the systems are presented in alphabetic order. 3.1 LIMSI KIT The KIT system consists of a phrase-based machine translation system using additional models in rescoring. The phrase-based system is trained on all available parallel training data. The phrase 345 more specifically its implementation in XenC (Rousseau, 2013). As a result, one third of the initial corpus is removed. Finally, we make a linear interpolation of these models, using the SRILM toolkit (Stolcke, 2002). 3.3 training data using"
W16-2320,P07-1019,0,0.236745,"grammar from parallel text. In addition to the contiguous lexical phrases, as used in phrase-based translation (PBT), hierarchical phrases with up to two gaps are also extracted. Our baseline model contains models with phrase translation probabilities and lexical smoothing probabilities in both translation directions, word and phrase penalty, and enhanced low frequency features (Chen et al., 2011). It also contains binary features to distinguish between hierarchical and non-hierarchical phrases, the glue rule, and rules with non-terminals at the boundaries. We use the cube pruning algorithm (Huang and Chiang, 2007) for decoding. The system uses three backoff language models (LM) that are estimated with the KenLM toolkit (Heafield et al., 2013) and are integrated into the decoder as separate models in the log-linear combination: a full 4-gram LM (trained on all data), a limited 5-gram LM (trained only on in-domain data), and a 7-gram word class language model (wcLM) (Wuebker et al., 2013) trained on all data and with a output vocabulary of 143K words. The system produces 1000-best lists which are reranked using a LSTM-based (Hochreiter and Schmidhuber, 1997; Gers et al., 2000; Gers et al., 2003) language"
W16-2320,2012.amta-papers.19,1,0.778005,"common preprocessing used for most of the individual engines in Section 2. Section 3 covers the characteristics of the different individual engines, followed by a brief overview of our system combination approach (Section 4). We then summarize our empirical results in Section 5, showing that we achieve better translation quality than with any individual engine. Finally, in Section 6, we provide a statistical analysis of certain linguistic phenomena, specifically the prediction precision on morphological attributes. We conclude the paper with Section 7. table is adapted to the SETimes2 corpus (Niehues and Waibel, 2012). The system uses a prereordering technique (Rottmann and Vogel, 2007) in combination with lexical reordering. It uses two word-based n-gram language models and three additional non-word language models. Two of them are automatic word class-based (Och, 1999) language models, using 100 and 1,000 word classes. In addition, we use a POS-based language model. During decoding, we use a discriminative word lexicon (Niehues and Waibel, 2013) as well. We rescore the system output using a 300-best list. The weights are optimized on the concatenation of the development data and the SETimes2 dev set usin"
W16-2320,W11-2211,1,0.902733,"ty words, and no lower limit to the amount of words covered by right-hand side non-terminals at extraction time. We discard rules with non-terminals on their right-hand side if they are singletons in the training data. In order to promote better reordering decisions, we implemented a feature in Moses that resembles the phrase orientation model for hierarchical machine translation as described by Huck et al. (2013) and extend our system with it. The model scores orientation classes (monotone, swap, discontinuous) for each rule application in decoding. We finally follow the approach outlined by Huck et al. (2011) for lightly-supervised training of hierarchical systems. We automatically translate parts (1.2M sentences) of the monolingual Romanian News Crawl 2015 corpus to English with a Romanian→English phrase-based statistical machine translation system (Williams et al., 2016). The foreground phrase table extracted from the human-generated parallel data is filled RWTH Neural System The second system provided by the RWTH is an attention-based recurrent neural network similar to (Bahdanau et al., 2015). The implementation is based on Blocks (van Merri¨enboer et al., 2015) and Theano (Bergstra et al., 20"
W16-2320,W13-2264,1,0.852517,"stic phenomena, specifically the prediction precision on morphological attributes. We conclude the paper with Section 7. table is adapted to the SETimes2 corpus (Niehues and Waibel, 2012). The system uses a prereordering technique (Rottmann and Vogel, 2007) in combination with lexical reordering. It uses two word-based n-gram language models and three additional non-word language models. Two of them are automatic word class-based (Och, 1999) language models, using 100 and 1,000 word classes. In addition, we use a POS-based language model. During decoding, we use a discriminative word lexicon (Niehues and Waibel, 2013) as well. We rescore the system output using a 300-best list. The weights are optimized on the concatenation of the development data and the SETimes2 dev set using the ListNet algorithm (Niehues et al., 2015). In rescoring, we add the source discriminative word lexica (Herrmann et al., 2015) as well as neural network language and translation models. These models use a factored word representation of the source and the target. On the source side we use the word surface form and two automatic word classes using 100 and 1,000 classes. On the Romanian side, we add the POS information as an additio"
W16-2320,W13-2258,1,0.869431,"extraction, we impose less strict extraction constraints than the Moses defaults. We extract more hierarchical rules by allowing for a maximum of ten symbols on the source side, a maximum span of twenty words, and no lower limit to the amount of words covered by right-hand side non-terminals at extraction time. We discard rules with non-terminals on their right-hand side if they are singletons in the training data. In order to promote better reordering decisions, we implemented a feature in Moses that resembles the phrase orientation model for hierarchical machine translation as described by Huck et al. (2013) and extend our system with it. The model scores orientation classes (monotone, swap, discontinuous) for each rule application in decoding. We finally follow the approach outlined by Huck et al. (2011) for lightly-supervised training of hierarchical systems. We automatically translate parts (1.2M sentences) of the monolingual Romanian News Crawl 2015 corpus to English with a Romanian→English phrase-based statistical machine translation system (Williams et al., 2016). The foreground phrase table extracted from the human-generated parallel data is filled RWTH Neural System The second system prov"
W16-2320,E99-1010,0,0.040797,"ion 5, showing that we achieve better translation quality than with any individual engine. Finally, in Section 6, we provide a statistical analysis of certain linguistic phenomena, specifically the prediction precision on morphological attributes. We conclude the paper with Section 7. table is adapted to the SETimes2 corpus (Niehues and Waibel, 2012). The system uses a prereordering technique (Rottmann and Vogel, 2007) in combination with lexical reordering. It uses two word-based n-gram language models and three additional non-word language models. Two of them are automatic word class-based (Och, 1999) language models, using 100 and 1,000 word classes. In addition, we use a POS-based language model. During decoding, we use a discriminative word lexicon (Niehues and Waibel, 2013) as well. We rescore the system output using a 300-best list. The weights are optimized on the concatenation of the development data and the SETimes2 dev set using the ListNet algorithm (Niehues et al., 2015). In rescoring, we add the source discriminative word lexica (Herrmann et al., 2015) as well as neural network language and translation models. These models use a factored word representation of the source and th"
W16-2320,P03-1021,0,0.501814,". To extract the features of the rule selection model, we parse the English part of our 2 http://hunch.net/˜vw/ (VW). Implemented by John Langford and many others. 346 URLs, e-mail addresses, etc.). During translation a rule-based localisation feature is applied. ward recurrent state encoding the source and target history, a backward recurrent state encoding the source future, and a third LSTM layer to combine them. All layers have 350 nodes. The neural networks are implemented using an extension of the RWTHLM toolkit (Sundermeyer et al., 2014b). The parameter weights are optimized with MERT (Och, 2003) towards the B LEU metric. 3.6 3.8 The UEDIN-LMU HPBT system is a hierarchical phrase-based machine translation system (Chiang, 2005) built jointly by the University of Edinburgh and LMU Munich. The system is based on the open source Moses implementation of the hierarchical phrase-based paradigm (Hoang et al., 2009). In addition to a set of standard features in a log-linear combination, a number of non-standard enhancements are employed to achieve improved translation quality. Specifically, we integrate individual language models trained over the separate corpora (News Crawl 2015, Europarl, SE"
W16-2320,D14-1003,1,0.932306,"with the KenLM toolkit (Heafield et al., 2013) and are integrated into the decoder as separate models in the log-linear combination: a full 4-gram LM (trained on all data), a limited 5-gram LM (trained only on in-domain data), and a 7-gram word class language model (wcLM) (Wuebker et al., 2013) trained on all data and with a output vocabulary of 143K words. The system produces 1000-best lists which are reranked using a LSTM-based (Hochreiter and Schmidhuber, 1997; Gers et al., 2000; Gers et al., 2003) language model (Sundermeyer et al., 2012) and a LSTM-based bidirectional joined model (BJM) (Sundermeyer et al., 2014a). The models have a class-factored output layer (Goodman, 2001; Morin and Bengio, 2005) to speed up training and evaluation. The language model uses 3 stacked LSTM layers, with 350 nodes each. The BJM has a projection layer, and computes a forLMU The LMU system integrates a discriminative rule selection model into a hierarchical SMT system, as described in (Tamchyna et al., 2014). The rule selection model is implemented using the highspeed classifier Vowpal Wabbit2 which is fully integrated in Moses’ hierarchical decoder. During decoding, the rule selection model is called at each rule appli"
W16-2320,P06-1055,0,0.0121776,"anslation Systems Each group contributed one or more systems. In this section the systems are presented in alphabetic order. 3.1 LIMSI KIT The KIT system consists of a phrase-based machine translation system using additional models in rescoring. The phrase-based system is trained on all available parallel training data. The phrase 345 more specifically its implementation in XenC (Rousseau, 2013). As a result, one third of the initial corpus is removed. Finally, we make a linear interpolation of these models, using the SRILM toolkit (Stolcke, 2002). 3.3 training data using the Berkeley parser (Petrov et al., 2006). For model prediction during tuning and decoding, we use parsed versions of the development and test sets. We train the rule selection model using VW and tune the weights of the translation model using batch MIRA (Cherry and Foster, 2012). The 5-gram language model is trained using KenLM (Heafield et al., 2013) on the Romanian part of the Common Crawl corpus concatenated with the Romanian part of the training data. LMU-CUNI The LMU-CUNI contribution is a constrained Moses phrase-based system. It uses a simple factored setting: our phrase table produces not only the target surface form but als"
W16-2320,2007.tmi-papers.21,0,0.0230136,"n 2. Section 3 covers the characteristics of the different individual engines, followed by a brief overview of our system combination approach (Section 4). We then summarize our empirical results in Section 5, showing that we achieve better translation quality than with any individual engine. Finally, in Section 6, we provide a statistical analysis of certain linguistic phenomena, specifically the prediction precision on morphological attributes. We conclude the paper with Section 7. table is adapted to the SETimes2 corpus (Niehues and Waibel, 2012). The system uses a prereordering technique (Rottmann and Vogel, 2007) in combination with lexical reordering. It uses two word-based n-gram language models and three additional non-word language models. Two of them are automatic word class-based (Och, 1999) language models, using 100 and 1,000 word classes. In addition, we use a POS-based language model. During decoding, we use a discriminative word lexicon (Niehues and Waibel, 2013) as well. We rescore the system output using a 300-best list. The weights are optimized on the concatenation of the development data and the SETimes2 dev set using the ListNet algorithm (Niehues et al., 2015). In rescoring, we add t"
W16-2320,P16-1161,1,0.729045,"omanian part of the training data. LMU-CUNI The LMU-CUNI contribution is a constrained Moses phrase-based system. It uses a simple factored setting: our phrase table produces not only the target surface form but also its lemma and morphological tag. On the input, we include lemmas, POS tags and information from dependency parses (lemma of the parent node and syntactic relation), all encoded as additional factors. The main difference from a standard phrasebased setup is the addition of a feature-rich discriminative translation model which is conditioned on both source- and target-side context (Tamchyna et al., 2016). The motivation for using this model is to better condition lexical choices by using the source context and to improve morphological and topical coherence by modeling the (limited left-hand side) target context. We also take advantage of the target factors by using a 7-gram language model trained on sequences of Romanian morphological tags. Finally, our system also uses a standard lexicalized reordering model. 3.4 3.5 RWTH Aachen University: Hierarchical Phrase-based System The RWTH hierarchical setup uses the open source translation toolkit Jane 2.3 (Vilar et al., 2010). Hierarchical phrase-"
W16-2320,tufis-etal-2008-racais,0,0.107217,"Missing"
W16-2320,P16-1009,1,0.78198,"and models are tuned with k-best MIRA (Cherry and Foster, 2012) on first the half of newsdev2016. In decoding, we use MBR (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) with a pop-limit of 5000, and the Moses ”monotone at punctuation” switch (to prevent reordering across punctuation) (Koehn and Haddow, 2009). Edinburgh Neural System Edinburgh’s neural machine translation system is an attentional encoder-decoder (Bahdanau et al., 2015), which we train with nematus.3 We use byte-pair-encoding (BPE) to achieve openvocabulary translation with a fixed vocabulary of subword symbols (Sennrich et al., 2016c). We produce additional parallel training data by automatically translating the monolingual Romanian News Crawl 2015 corpus into English (Sennrich et al., 2016b), which we combine with the original parallel data in a 1-to-1 ratio. We use minibatches of size 80, a maximum sentence length of 50, word embeddings of size 500, and hidden layers of size 1024. We apply dropout to all layers (Gal, 2015), with dropout probability 0.2, and also drop out full words with probability 0.1. We clip the gradient norm to 1.0 (Pascanu et al., 2013). We train the models with Adadelta (Zeiler, 2012), reshufflin"
W16-2320,P12-3008,0,0.0597108,"Missing"
W16-2320,P16-1162,1,0.259658,"and models are tuned with k-best MIRA (Cherry and Foster, 2012) on first the half of newsdev2016. In decoding, we use MBR (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) with a pop-limit of 5000, and the Moses ”monotone at punctuation” switch (to prevent reordering across punctuation) (Koehn and Haddow, 2009). Edinburgh Neural System Edinburgh’s neural machine translation system is an attentional encoder-decoder (Bahdanau et al., 2015), which we train with nematus.3 We use byte-pair-encoding (BPE) to achieve openvocabulary translation with a fixed vocabulary of subword symbols (Sennrich et al., 2016c). We produce additional parallel training data by automatically translating the monolingual Romanian News Crawl 2015 corpus into English (Sennrich et al., 2016b), which we combine with the original parallel data in a 1-to-1 ratio. We use minibatches of size 80, a maximum sentence length of 50, word embeddings of size 500, and hidden layers of size 1024. We apply dropout to all layers (Gal, 2015), with dropout probability 0.2, and also drop out full words with probability 0.1. We clip the gradient norm to 1.0 (Pascanu et al., 2013). We train the models with Adadelta (Zeiler, 2012), reshufflin"
W16-2320,W10-1738,1,0.885055,"rget-side context (Tamchyna et al., 2016). The motivation for using this model is to better condition lexical choices by using the source context and to improve morphological and topical coherence by modeling the (limited left-hand side) target context. We also take advantage of the target factors by using a 7-gram language model trained on sequences of Romanian morphological tags. Finally, our system also uses a standard lexicalized reordering model. 3.4 3.5 RWTH Aachen University: Hierarchical Phrase-based System The RWTH hierarchical setup uses the open source translation toolkit Jane 2.3 (Vilar et al., 2010). Hierarchical phrase-based translation (HPBT) (Chiang, 2007) induces a weighted synchronous context-free grammar from parallel text. In addition to the contiguous lexical phrases, as used in phrase-based translation (PBT), hierarchical phrases with up to two gaps are also extracted. Our baseline model contains models with phrase translation probabilities and lexical smoothing probabilities in both translation directions, word and phrase penalty, and enhanced low frequency features (Chen et al., 2011). It also contains binary features to distinguish between hierarchical and non-hierarchical ph"
W16-2320,P15-4020,1,0.815128,"data for the translation model. A single 5-gram language model is built using all the target side of the parallel data and a subpart of the monolingual Romanian corpora selected with Xenc-v2 (Rousseau, 2013). For the latter we use all the parallel data as in-domain data and the first half of newsdev2016 as development set. The feature weights are tuned with MERT (Och, 2003) on the first half of newsdev2016. The system produces distinct 1000-best lists, for which we extend the feature set with the 17 baseline black-box features from sentencelevel Quality Estimation (QE) produced with Quest++4 (Specia et al., 2015). The 1000-best lists are then reranked and the top-best hypothesis extracted using the nbest rescorer available within the Moses toolkit. 3.12 UvA We use a phrase-based machine translation system (Moses) with a distortion limit of 6 and lexicalized reordering. Before translation, the English source side is preordered using the neural preordering model of (de Gispert et al., 2015). The preordering model is trained for 30 iterations on the full MGIZA-aligned training data. We use two language models, built using KenLM. The first is a 5-gram language model trained on all available data. Words in"
W16-2320,W16-2327,1,0.84726,"Missing"
W16-2320,D13-1138,1,0.859072,"(Chen et al., 2011). It also contains binary features to distinguish between hierarchical and non-hierarchical phrases, the glue rule, and rules with non-terminals at the boundaries. We use the cube pruning algorithm (Huang and Chiang, 2007) for decoding. The system uses three backoff language models (LM) that are estimated with the KenLM toolkit (Heafield et al., 2013) and are integrated into the decoder as separate models in the log-linear combination: a full 4-gram LM (trained on all data), a limited 5-gram LM (trained only on in-domain data), and a 7-gram word class language model (wcLM) (Wuebker et al., 2013) trained on all data and with a output vocabulary of 143K words. The system produces 1000-best lists which are reranked using a LSTM-based (Hochreiter and Schmidhuber, 1997; Gers et al., 2000; Gers et al., 2003) language model (Sundermeyer et al., 2012) and a LSTM-based bidirectional joined model (BJM) (Sundermeyer et al., 2014a). The models have a class-factored output layer (Goodman, 2001; Morin and Bengio, 2005) to speed up training and evaluation. The language model uses 3 stacked LSTM layers, with 350 nodes each. The BJM has a projection layer, and computes a forLMU The LMU system integra"
W16-2320,2002.tmi-tutorials.2,0,0.0608664,"omanian side, the data is tokenized using LIMSI’s tokro (Allauzen et al., 2016), a rulebased tokenizer that mainly normalizes diacritics and splits punctuation and clitics. This data is truecased in the same way as the English side. In addition, the Romanian sentences are also tagged, lemmatized, and chunked using the TTL tagger (Tufis¸ et al., 2008). 3 The LIMSI system uses NCODE (Crego et al., 2011), which implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Crego and Mari˜no, 2006; Mari˜no et al., 2006) that is closely related to the standard phrase-based approach (Zens et al., 2002). In this framework, translation is divided into two steps. To translate a source sentence into a target sentence, the source sentence is first reordered according to a set of rewriting rules so as to reproduce the target word order. This generates a word lattice containing the most promising source permutations, which is then translated. Since the translation step is monotonic, this approach is able to rely on the n-gram assumption to decompose the joint probability of a sentence pair into a sequence of bilingual units called tuples. We train three Romanian 4-gram language models, pruning all"
W16-2323,P16-1009,1,0.795503,"d better ensembles. Decoding is performed with beam search with a beam size of 12. For some language pairs, we used the AmuNMT C++ decoder4 as a more efficient alternative to the theano implementation of the dl4mt tutorial. Introduction We participated in the WMT 2016 shared news translation task by building neural translation systems for four language pairs: English↔Czech, English↔German, English↔Romanian and English↔Russian. Our systems are based on an attentional encoder-decoder (Bahdanau et al., 2015), using BPE subword segmentation for open-vocabulary translation with a fixed vocabulary (Sennrich et al., 2016b). We experimented with using automatic back-translations of the 2.1 Byte-pair encoding (BPE) To enable open-vocabulary translation, we segment words via byte-pair encoding (BPE)5 (Sen1 We have released the implementation that we used for the experiments as an open source toolkit: https://github. com/rsennrich/nematus 2 We have released scripts, sample configs, synthetic training data and trained models: https://github.com/ rsennrich/wmt16-scripts 3 https://github.com/nyu-dl/ dl4mt-tutorial 4 https://github.com/emjotde/amunmt 5 https://github.com/rsennrich/ subword-nmt 371 Proceedings of the"
W16-2323,P16-1162,1,0.641761,"d better ensembles. Decoding is performed with beam search with a beam size of 12. For some language pairs, we used the AmuNMT C++ decoder4 as a more efficient alternative to the theano implementation of the dl4mt tutorial. Introduction We participated in the WMT 2016 shared news translation task by building neural translation systems for four language pairs: English↔Czech, English↔German, English↔Romanian and English↔Russian. Our systems are based on an attentional encoder-decoder (Bahdanau et al., 2015), using BPE subword segmentation for open-vocabulary translation with a fixed vocabulary (Sennrich et al., 2016b). We experimented with using automatic back-translations of the 2.1 Byte-pair encoding (BPE) To enable open-vocabulary translation, we segment words via byte-pair encoding (BPE)5 (Sen1 We have released the implementation that we used for the experiments as an open source toolkit: https://github. com/rsennrich/nematus 2 We have released scripts, sample configs, synthetic training data and trained models: https://github.com/ rsennrich/wmt16-scripts 3 https://github.com/nyu-dl/ dl4mt-tutorial 4 https://github.com/emjotde/amunmt 5 https://github.com/rsennrich/ subword-nmt 371 Proceedings of the"
W16-2323,W16-2316,1,\N,Missing
W16-2327,J07-2003,0,0.787531,"dual monolingual corpora, we first used lmplz (Heafield et al., 2013) to train count-based 5-gram language models with modified Kneser-Ney smoothing (Chen and Goodman, 1998). We then used the SRILM toolkit (Stolcke, 2002) to linearly interpolate the models http://ufal.mff.cuni.cz/treex 399 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 399–410, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics 2.6 using weights tuned to minimize perplexity on the development set. In decoding we applied cube pruning (Huang and Chiang, 2007) with a stack size of 5000 (reduced to 1000 for tuning), Minimum Bayes Risk decoding (Kumar and Byrne, 2004), a maximum phrase length of 5, a distortion limit of 6, 100best translation options and the no-reorderingover-punctuation heuristic (Koehn and Haddow, 2009). CommonCrawl LMs Our CommonCrawl language models were trained in the same way as the individual corpus-specific standard models, but were not linearly-interpolated with other LMs. Instead, the log probabilities of CommonCrawl LMs were added as separate features of the systems’ linear models. 3 Neural LMs For some of our phrase-based"
W16-2327,P11-2072,0,0.0157943,"ubject to restrictions on the size of the resulting tree fragment. We used the settings shown in Table 4, which were chosen empirically during the development of 2013’s systems (Nadejde et al., 2013). parameter rule depth node count rule size Syntax-based System Overview For all syntax-based systems, we used a string-totree model based on a synchronous context-free grammar (SCFG) with linguistically-motivated labels on the target side. 4.1 Preprocessing binarized 7 30 7 Further to the restrictions on rule composition, fully non-lexical unary rules were eliminated using the method described in Chung et al. (2011) and rules with scope greater than 3 (Hopkins and Langmead, 2010) were pruned from the translation grammar. Scope pruning makes parsing tractable without the need for grammar binarization. 4.5 Word Alignment Baseline Features Our core set of string-to-tree feature functions is unchanged from previous years. It includes the ngram language model’s log probability for the target string, the target word count, the rule count, and several pre-computed rule-specific scores. The rule-specific scores were: the direct and indirect translation probabilities; the direct and indirect lexical weights (Koeh"
W16-2327,P05-1066,0,0.077238,"s representing the decoding search space) on a concatenation of newssyscomb2009 and newstest2008–2012. BLEU 26.8 26.2 25.6 26.4 26.6 26.5 3.5 Table 2: Effect of each of the language models used in the English→Romanian system. The experiments are not cumulative, so we first try pruning the “all” language model, then go back to the unpruned version and remove each LM in turn, observing the effect. The submitted system used all four LMs, and the scores shown are uncased B LEU scores on newstest2016. 3.4 German→English For phrase-based translation from German, we applied syntactic pre-reordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) in a preprocessing step on the source side. The operation sequence model for the German→English phrase-based system was unpruned. We integrated three language models: an unpruned LM over all English data except the CommonCrawl monolingual corpus; a pruned LM over CommonCrawl; and a pruned LM over the monolingual News Crawl 2015 corpus. In addition to lexical smoothing with the standard lexicon models, we utilized a source-to-target IBM Model 1 (Brown et al., 1993) for sentence-level lexical scoring in a similar manner as described by Huck et al."
W16-2327,P14-1129,0,0.035129,"on limit of 6, 100best translation options and the no-reorderingover-punctuation heuristic (Koehn and Haddow, 2009). CommonCrawl LMs Our CommonCrawl language models were trained in the same way as the individual corpus-specific standard models, but were not linearly-interpolated with other LMs. Instead, the log probabilities of CommonCrawl LMs were added as separate features of the systems’ linear models. 3 Neural LMs For some of our phrase-based systems we experimented with feed-forward neural network language models, both trained on target n-grams only, and on “joint” or “bilingual” ngrams (Devlin et al., 2014; Le et al., 2012). For training these models we used the NPLM toolkit (Vaswani et al., 2013), for which we have now implemented gradient clipping to address numerical issues often encountered during training. 2.4 3.1 Phrase-based Experiments Finnish→English Similar to last year (Haddow et al., 2015), we built an unconstrained system for Finnish→English using data extracted from OPUS (Tiedemann, 2012). Our parallel training set was the same as we used previously, but the language model training set was extended with the addition of the news2015 monolingual corpus and the large WMT16 English Co"
W16-2327,P13-2071,0,0.020975,"a weighted linear combination of features. The core features of our model are a 5-gram LM score (i.e. log probability), phrase translation and lexical translation scores, word and phrase penalties, and a linear distortion score. The phrase translation probabilities are smoothed with Good-Turing smoothing (Foster et al., 2006). We used the hierarchical lexicalized reordering model (Galley and Manning, 2008) with 4 possible orientations (monotone, swap, discontinuous left and discontinuous right) in both left-to-right and right-to-left direction. We also used the operation sequence model (OSM) (Durrani et al., 2013) with 4 count based supportive features. We further employed domain indicator features (marking which training corpus each phrase pair was found in), binary phrase count indicator features, sparse phrase length features, and sparse source word deletion, target word insertion, and word translation features (limited to the top K words in each language, typically with K = 50). 2.5 Decoding Tuning Since our feature set (generally around 500 to 1000 features) was too large for MERT, we used k-best batch MIRA for tuning (Cherry and Foster, 2012). To speed up tuning we applied threshold pruning to th"
W16-2327,J93-2003,0,0.052624,"Missing"
W16-2327,D14-1082,0,0.163978,"ram Neural Network language model (NPLM) (Vaswani et al., 2013), a relational dependency language model (RDLM) (Sennrich, 2015), and soft source-syntactic constraints (Huck et al., 2014). The parameters of the model are tuned towards the linear interpolation of B LEU and the syntactic metric HWCM (Liu and Gildea, 2005; Sennrich, 2015). Trees are transformed through binarization and a hierarchical representation of morphologically complex words (Sennrich and Haddow, 2015). For the soft source-syntactic constraints, we annotate the source text with the Stanford Neural Network dependency parser (Chen and Manning, 2014), along with heuristic projectivization (Nivre and Nilsson, 2005). Results are shown in Table 8. We report results of last year’s system (Williams et al., 2015), which was ranked (joint) first at WMT 15. Our improvements this year stem from particle verb restructuring (Sennrich and Haddow, 2015), and the use of the new monolingual News Crawl 2015 corpus for In 4 cases, the system with constraints delivered much better translation, and three of those were overall improvement of the sentence structure. In 41 cases, the area was better for various reasons. Most frequently (16 cases), this was ind"
W16-2327,N13-1073,0,0.099487,"Missing"
W16-2327,N12-1047,0,0.356006,"rection. We also used the operation sequence model (OSM) (Durrani et al., 2013) with 4 count based supportive features. We further employed domain indicator features (marking which training corpus each phrase pair was found in), binary phrase count indicator features, sparse phrase length features, and sparse source word deletion, target word insertion, and word translation features (limited to the top K words in each language, typically with K = 50). 2.5 Decoding Tuning Since our feature set (generally around 500 to 1000 features) was too large for MERT, we used k-best batch MIRA for tuning (Cherry and Foster, 2012). To speed up tuning we applied threshold pruning to the phrase table, based on the direct translation model probability. 400 are quite understandable, e.g. source yös Intian on sanottu olevan kiinnostunut puolustusyhteistyösopimuksesta Japanin kanssa. base India is also said to be interested in puolustusyhteistyösopimuksesta with Japan. bpe India is also said to be interested in defence cooperation agreement with Japan. reference India is also reportedly hoping for a deal on defence collaboration between the two nations. However applying BPE to Finnish can also result in some rather odd trans"
W16-2327,W06-1607,0,0.0250361,"ts here, we used 100,000 BPE merges to create the model. Applying BPE to Finnish→English was clearly effective at addressing the unknown word problem, and in many cases the resulting translations Baseline Features We follow the standard approach to SMT of scoring translation hypotheses using a weighted linear combination of features. The core features of our model are a 5-gram LM score (i.e. log probability), phrase translation and lexical translation scores, word and phrase penalties, and a linear distortion score. The phrase translation probabilities are smoothed with Good-Turing smoothing (Foster et al., 2006). We used the hierarchical lexicalized reordering model (Galley and Manning, 2008) with 4 possible orientations (monotone, swap, discontinuous left and discontinuous right) in both left-to-right and right-to-left direction. We also used the operation sequence model (OSM) (Durrani et al., 2013) with 4 count based supportive features. We further employed domain indicator features (marking which training corpus each phrase pair was found in), binary phrase count indicator features, sparse phrase length features, and sparse source word deletion, target word insertion, and word translation features"
W16-2327,D10-1063,0,0.0144921,"fragment. We used the settings shown in Table 4, which were chosen empirically during the development of 2013’s systems (Nadejde et al., 2013). parameter rule depth node count rule size Syntax-based System Overview For all syntax-based systems, we used a string-totree model based on a synchronous context-free grammar (SCFG) with linguistically-motivated labels on the target side. 4.1 Preprocessing binarized 7 30 7 Further to the restrictions on rule composition, fully non-lexical unary rules were eliminated using the method described in Chung et al. (2011) and rules with scope greater than 3 (Hopkins and Langmead, 2010) were pruned from the translation grammar. Scope pruning makes parsing tractable without the need for grammar binarization. 4.5 Word Alignment Baseline Features Our core set of string-to-tree feature functions is unchanged from previous years. It includes the ngram language model’s log probability for the target string, the target word count, the rule count, and several pre-computed rule-specific scores. The rule-specific scores were: the direct and indirect translation probabilities; the direct and indirect lexical weights (Koehn et al., 2003); the monolingual PCFG probability of the tree fra"
W16-2327,P07-1019,0,0.0831252,"For individual monolingual corpora, we first used lmplz (Heafield et al., 2013) to train count-based 5-gram language models with modified Kneser-Ney smoothing (Chen and Goodman, 1998). We then used the SRILM toolkit (Stolcke, 2002) to linearly interpolate the models http://ufal.mff.cuni.cz/treex 399 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 399–410, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics 2.6 using weights tuned to minimize perplexity on the development set. In decoding we applied cube pruning (Huang and Chiang, 2007) with a stack size of 5000 (reduced to 1000 for tuning), Minimum Bayes Risk decoding (Kumar and Byrne, 2004), a maximum phrase length of 5, a distortion limit of 6, 100best translation options and the no-reorderingover-punctuation heuristic (Koehn and Haddow, 2009). CommonCrawl LMs Our CommonCrawl language models were trained in the same way as the individual corpus-specific standard models, but were not linearly-interpolated with other LMs. Instead, the log probabilities of CommonCrawl LMs were added as separate features of the systems’ linear models. 3 Neural LMs For some of our phrase-based"
W16-2327,P06-1121,0,0.0809512,"Missing"
W16-2327,2015.iwslt-evaluation.4,1,0.850731,"ms are given in Table 3. English→German For the English→German phrase-based system, we exploited several translation factors in addition to word surface forms, in particular: Och clusters (with 50 classes) and part-of-speech tags (Ratnaparkhi, 1996) on the English side, as well as Och clusters (50 classes), morphological tags, and part-of-speech tags on the German side (Schmid, 2000). Recent experiments for our IWSLT 2015 phrase-based system have reconfirmed that English→German translation quality can benefit from these factors when supplementary models over factored representations are used (Huck and Birch, 2015). For WMT16, we utilized the factors in the translation model, in operation sequence models, and in language models (for linearly interpolated 7-gram LMs over Och clusters and morphological tags). Sparse source word deletion, target word insertion, and word translation features were integrated over the top 200 word surface forms and over selected factors (source and target Och clusters, source part-of-speech tags and target morphological tags). An unpruned 5-gram LM over words that was trained on all German data except the CommonCrawl monolingual corpus was supplemented by a separate pruned LM"
W16-2327,N04-1035,0,0.0743322,"pus was made up of three parts: all the English monolingual medical data from WMT14 medical, WMT16 biomedical and EMEA (11M sentences); all the English LDC GigaWord data (180M sentences); and all the English general domain data from WMT16 (240M sentences). We used the monolingual data to build three different language models which were then linearly interpolated. System tuning was with the SCIELO development data provided for the biomedical task. 4 4.4 SCFG rules were extracted from the word-aligned parallel data using the Moses implementation (Williams and Koehn, 2012) of the GHKM algorithm (Galley et al., 2004, 2006). Minimal GHKM rules were composed into larger rules subject to restrictions on the size of the resulting tree fragment. We used the settings shown in Table 4, which were chosen empirically during the development of 2013’s systems (Nadejde et al., 2013). parameter rule depth node count rule size Syntax-based System Overview For all syntax-based systems, we used a string-totree model based on a synchronous context-free grammar (SCFG) with linguistically-motivated labels on the target side. 4.1 Preprocessing binarized 7 30 7 Further to the restrictions on rule composition, fully non-lexic"
W16-2327,W14-4018,1,0.849099,"s, improving overall sentence structure on average. Crazy Reordering 3 Table 7: Manual evaluation of translations as proposed by the English→Czech system with unification constraints vs. the same system without constraints. 5.2 English→German This year’s string-to-tree submission for English→German is similar to last year’s system (Williams et al., 2015). In addition to the baseline feature functions, it contains count-based 5-gram Neural Network language model (NPLM) (Vaswani et al., 2013), a relational dependency language model (RDLM) (Sennrich, 2015), and soft source-syntactic constraints (Huck et al., 2014). The parameters of the model are tuned towards the linear interpolation of B LEU and the syntactic metric HWCM (Liu and Gildea, 2005; Sennrich, 2015). Trees are transformed through binarization and a hierarchical representation of morphologically complex words (Sennrich and Haddow, 2015). For the soft source-syntactic constraints, we annotate the source text with the Stanford Neural Network dependency parser (Chen and Manning, 2014), along with heuristic projectivization (Nivre and Nilsson, 2005). Results are shown in Table 8. We report results of last year’s system (Williams et al., 2015), w"
W16-2327,D08-1089,0,0.179545,"Missing"
W16-2327,W08-0509,0,0.143531,"Missing"
W16-2327,2011.iwslt-papers.1,1,0.855699,"al., 2005) and compound splitting (Koehn and Knight, 2003) in a preprocessing step on the source side. The operation sequence model for the German→English phrase-based system was unpruned. We integrated three language models: an unpruned LM over all English data except the CommonCrawl monolingual corpus; a pruned LM over CommonCrawl; and a pruned LM over the monolingual News Crawl 2015 corpus. In addition to lexical smoothing with the standard lexicon models, we utilized a source-to-target IBM Model 1 (Brown et al., 1993) for sentence-level lexical scoring in a similar manner as described by Huck et al. (2011) for hierarchical systems. We tuned on the concatenation of newssyscomb2009 and newstest2008–2012. Unlike last year’s system (Haddow et al., 2015)—and different from the inverse translation direction (English→German)—we refrained from using any factors and instead set up a system that operates over surface form word representations only. In relation to last year’s system, we were able to maintain high translation quality as measured in B LEU despite the abandonment of factors. However, we suspect that human judgment scores may suffer a bit from the abandonment of a factored model. We decided t"
W16-2327,N10-1129,0,0.0414698,"Missing"
W16-2327,W15-3013,1,0.881832,"Missing"
W16-2327,E03-1076,0,0.0923236,"concatenation of newssyscomb2009 and newstest2008–2012. BLEU 26.8 26.2 25.6 26.4 26.6 26.5 3.5 Table 2: Effect of each of the language models used in the English→Romanian system. The experiments are not cumulative, so we first try pruning the “all” language model, then go back to the unpruned version and remove each LM in turn, observing the effect. The submitted system used all four LMs, and the scores shown are uncased B LEU scores on newstest2016. 3.4 German→English For phrase-based translation from German, we applied syntactic pre-reordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) in a preprocessing step on the source side. The operation sequence model for the German→English phrase-based system was unpruned. We integrated three language models: an unpruned LM over all English data except the CommonCrawl monolingual corpus; a pruned LM over CommonCrawl; and a pruned LM over the monolingual News Crawl 2015 corpus. In addition to lexical smoothing with the standard lexicon models, we utilized a source-to-target IBM Model 1 (Brown et al., 1993) for sentence-level lexical scoring in a similar manner as described by Huck et al. (2011) for hierarchical systems. We tuned on th"
W16-2327,N03-1017,0,0.0439096,"011) and rules with scope greater than 3 (Hopkins and Langmead, 2010) were pruned from the translation grammar. Scope pruning makes parsing tractable without the need for grammar binarization. 4.5 Word Alignment Baseline Features Our core set of string-to-tree feature functions is unchanged from previous years. It includes the ngram language model’s log probability for the target string, the target word count, the rule count, and several pre-computed rule-specific scores. The rule-specific scores were: the direct and indirect translation probabilities; the direct and indirect lexical weights (Koehn et al., 2003); the monolingual PCFG probability of the tree fragment from which the rule was extracted; and a rule As in the phrase-based models, we used fast_align for word alignment and the grow-diag-final-and heuristic for symmetrization. 4.3 unbinarized 5 20 5 Table 4: Parameter settings for rule composition. The parameters were relaxed for systems that used binarization to allow for the increase in tree node density. Except for English-Czech, which we describe separately in Section 5.1, preprocessing was similar to the phrase-based systems (Section 2.3). To parse the target-side of the training data,"
W16-2327,N04-1022,0,0.115351,"anguage models with modified Kneser-Ney smoothing (Chen and Goodman, 1998). We then used the SRILM toolkit (Stolcke, 2002) to linearly interpolate the models http://ufal.mff.cuni.cz/treex 399 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 399–410, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics 2.6 using weights tuned to minimize perplexity on the development set. In decoding we applied cube pruning (Huang and Chiang, 2007) with a stack size of 5000 (reduced to 1000 for tuning), Minimum Bayes Risk decoding (Kumar and Byrne, 2004), a maximum phrase length of 5, a distortion limit of 6, 100best translation options and the no-reorderingover-punctuation heuristic (Koehn and Haddow, 2009). CommonCrawl LMs Our CommonCrawl language models were trained in the same way as the individual corpus-specific standard models, but were not linearly-interpolated with other LMs. Instead, the log probabilities of CommonCrawl LMs were added as separate features of the systems’ linear models. 3 Neural LMs For some of our phrase-based systems we experimented with feed-forward neural network language models, both trained on target n-grams on"
W16-2327,N12-1005,0,0.0147426,"t translation options and the no-reorderingover-punctuation heuristic (Koehn and Haddow, 2009). CommonCrawl LMs Our CommonCrawl language models were trained in the same way as the individual corpus-specific standard models, but were not linearly-interpolated with other LMs. Instead, the log probabilities of CommonCrawl LMs were added as separate features of the systems’ linear models. 3 Neural LMs For some of our phrase-based systems we experimented with feed-forward neural network language models, both trained on target n-grams only, and on “joint” or “bilingual” ngrams (Devlin et al., 2014; Le et al., 2012). For training these models we used the NPLM toolkit (Vaswani et al., 2013), for which we have now implemented gradient clipping to address numerical issues often encountered during training. 2.4 3.1 Phrase-based Experiments Finnish→English Similar to last year (Haddow et al., 2015), we built an unconstrained system for Finnish→English using data extracted from OPUS (Tiedemann, 2012). Our parallel training set was the same as we used previously, but the language model training set was extended with the addition of the news2015 monolingual corpus and the large WMT16 English CommonCrawl corpus."
W16-2327,W05-0904,0,0.0750355,"e English→Czech system with unification constraints vs. the same system without constraints. 5.2 English→German This year’s string-to-tree submission for English→German is similar to last year’s system (Williams et al., 2015). In addition to the baseline feature functions, it contains count-based 5-gram Neural Network language model (NPLM) (Vaswani et al., 2013), a relational dependency language model (RDLM) (Sennrich, 2015), and soft source-syntactic constraints (Huck et al., 2014). The parameters of the model are tuned towards the linear interpolation of B LEU and the syntactic metric HWCM (Liu and Gildea, 2005; Sennrich, 2015). Trees are transformed through binarization and a hierarchical representation of morphologically complex words (Sennrich and Haddow, 2015). For the soft source-syntactic constraints, we annotate the source text with the Stanford Neural Network dependency parser (Chen and Manning, 2014), along with heuristic projectivization (Nivre and Nilsson, 2005). Results are shown in Table 8. We report results of last year’s system (Williams et al., 2015), which was ranked (joint) first at WMT 15. Our improvements this year stem from particle verb restructuring (Sennrich and Haddow, 2015)"
W16-2327,H05-1066,0,0.0224782,"s. We used randomly-chosen subsets of the previous years’ test data to speed up decoding. 5 5.1 system baseline + constraints HimL1 23.3 23.6 B LEU HimL2 Khresmoi 18.6 20.4 18.8 20.7 Table 5: Translation results on the development system for English→Czech with unification-based constraints. Cased B LEU scores are shown. They are averaged over three tuning runs (note that baseline weights are reused in the experiments with constraints). Syntax-based Experiments English→Czech For English→Czech, we used Treex to preprocess and parse the Czech-side of the training data. Treex uses the MST parser (McDonald et al., 2005), which produces dependency graphs with non-projective arcs. In order to extract SCFG rules, we first applied the following conversion process: i) the dependency graphs were projectivized using the Malt Parser, which implements the method described in Nivre and Nilsson (2005) (we used the ‘Head’ encoding scheme); ii) the projective dependency graphs were converted to CFG trees. In addition, we reduced the complex positional tags to simple POS tags by discarding the morphological attributes. The CFG trees were not binarized. We also experimented with unificationbased agreement and case governme"
W16-2327,W13-2221,1,0.819391,"e used the monolingual data to build three different language models which were then linearly interpolated. System tuning was with the SCIELO development data provided for the biomedical task. 4 4.4 SCFG rules were extracted from the word-aligned parallel data using the Moses implementation (Williams and Koehn, 2012) of the GHKM algorithm (Galley et al., 2004, 2006). Minimal GHKM rules were composed into larger rules subject to restrictions on the size of the resulting tree fragment. We used the settings shown in Table 4, which were chosen empirically during the development of 2013’s systems (Nadejde et al., 2013). parameter rule depth node count rule size Syntax-based System Overview For all syntax-based systems, we used a string-totree model based on a synchronous context-free grammar (SCFG) with linguistically-motivated labels on the target side. 4.1 Preprocessing binarized 7 30 7 Further to the restrictions on rule composition, fully non-lexical unary rules were eliminated using the method described in Chung et al. (2011) and rules with scope greater than 3 (Hopkins and Langmead, 2010) were pruned from the translation grammar. Scope pruning makes parsing tractable without the need for grammar binar"
W16-2327,W14-4011,1,0.900492,"Missing"
W16-2327,P05-1013,0,0.487034,"ased constraints. Cased B LEU scores are shown. They are averaged over three tuning runs (note that baseline weights are reused in the experiments with constraints). Syntax-based Experiments English→Czech For English→Czech, we used Treex to preprocess and parse the Czech-side of the training data. Treex uses the MST parser (McDonald et al., 2005), which produces dependency graphs with non-projective arcs. In order to extract SCFG rules, we first applied the following conversion process: i) the dependency graphs were projectivized using the Malt Parser, which implements the method described in Nivre and Nilsson (2005) (we used the ‘Head’ encoding scheme); ii) the projective dependency graphs were converted to CFG trees. In addition, we reduced the complex positional tags to simple POS tags by discarding the morphological attributes. The CFG trees were not binarized. We also experimented with unificationbased agreement and case government constraints (Williams and Koehn, 2011; Williams, 2014). Specifically, our constraints were designed to enforce: i) case, gender, and number agreement between nouns and pre-nominal adjectival modifiers; ii) number and person agreement between subjects and verbs; iii) case a"
W16-2327,Q15-1013,1,0.838097,"hypothesis better in a surprisingly larger span of words, improving overall sentence structure on average. Crazy Reordering 3 Table 7: Manual evaluation of translations as proposed by the English→Czech system with unification constraints vs. the same system without constraints. 5.2 English→German This year’s string-to-tree submission for English→German is similar to last year’s system (Williams et al., 2015). In addition to the baseline feature functions, it contains count-based 5-gram Neural Network language model (NPLM) (Vaswani et al., 2013), a relational dependency language model (RDLM) (Sennrich, 2015), and soft source-syntactic constraints (Huck et al., 2014). The parameters of the model are tuned towards the linear interpolation of B LEU and the syntactic metric HWCM (Liu and Gildea, 2005; Sennrich, 2015). Trees are transformed through binarization and a hierarchical representation of morphologically complex words (Sennrich and Haddow, 2015). For the soft source-syntactic constraints, we annotate the source text with the Stanford Neural Network dependency parser (Chen and Manning, 2014), along with heuristic projectivization (Nivre and Nilsson, 2005). Results are shown in Table 8. We repo"
W16-2327,P03-1021,0,0.0414024,"translation task. We used two test sets from the HimL project and the Khresmoi test set. Results with and without constraints are shown in Table 5. We used hard constraints and reused the baseline weights (re-tuning did not appear to give additional gains). rareness penalty. 4.6 Decoding Decoding for the string-to-tree models is based on Sennrich’s (2014) recursive variant of the CYK+ parsing algorithm combined with LM integration via cube pruning (Chiang, 2007). 4.7 Tuning The feature weights for the English→Czech and Finnish→English systems were tuned using the Moses implementation of MERT (Och, 2003). For the remaining systems we used k-best MIRA (Cherry and Foster, 2012) due to the use of sparse features. We used randomly-chosen subsets of the previous years’ test data to speed up decoding. 5 5.1 system baseline + constraints HimL1 23.3 23.6 B LEU HimL2 Khresmoi 18.6 20.4 18.8 20.7 Table 5: Translation results on the development system for English→Czech with unification-based constraints. Cased B LEU scores are shown. They are averaged over three tuning runs (note that baseline weights are reused in the experiments with constraints). Syntax-based Experiments English→Czech For English→Cze"
W16-2327,D15-1248,1,0.87645,"for English→German is similar to last year’s system (Williams et al., 2015). In addition to the baseline feature functions, it contains count-based 5-gram Neural Network language model (NPLM) (Vaswani et al., 2013), a relational dependency language model (RDLM) (Sennrich, 2015), and soft source-syntactic constraints (Huck et al., 2014). The parameters of the model are tuned towards the linear interpolation of B LEU and the syntactic metric HWCM (Liu and Gildea, 2005; Sennrich, 2015). Trees are transformed through binarization and a hierarchical representation of morphologically complex words (Sennrich and Haddow, 2015). For the soft source-syntactic constraints, we annotate the source text with the Stanford Neural Network dependency parser (Chen and Manning, 2014), along with heuristic projectivization (Nivre and Nilsson, 2005). Results are shown in Table 8. We report results of last year’s system (Williams et al., 2015), which was ranked (joint) first at WMT 15. Our improvements this year stem from particle verb restructuring (Sennrich and Haddow, 2015), and the use of the new monolingual News Crawl 2015 corpus for In 4 cases, the system with constraints delivered much better translation, and three of thos"
W16-2327,P06-1055,0,0.0370971,"ility of the tree fragment from which the rule was extracted; and a rule As in the phrase-based models, we used fast_align for word alignment and the grow-diag-final-and heuristic for symmetrization. 4.3 unbinarized 5 20 5 Table 4: Parameter settings for rule composition. The parameters were relaxed for systems that used binarization to allow for the increase in tree node density. Except for English-Czech, which we describe separately in Section 5.1, preprocessing was similar to the phrase-based systems (Section 2.3). To parse the target-side of the training data, we used the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007) for English, and the ParZu dependency parser (Sennrich et al., 2013) for German. Except where stated otherwise, we right-binarized the trees after parsing to increase rule coverage. 4.2 Rule Extraction Language Models As in the phrase-based systems (Section 2.3), we used linearly-interpolated language models as standard, with some systems adding Common403 In preliminary experiments we used a smaller training set, comprising 2 million sentence pairs sampled from OPUS and monolingual data from last year’s WMT translation task. We used two test sets from the HimL project"
W16-2327,P16-1162,1,0.597202,"rm any corpus filtering other than the standard Moses method, which removes sentence pairs with extreme length ratios, and sentences longer than 80 tokens. Introduction Edinburgh’s submissions to the WMT 2016 news translation task fall into two distinct groups: neural translation systems and statistical translation systems. In this paper, we describe the statistical systems, which includes a mix of phrase-based and syntax-based approaches. We also include a brief description of our phrase-based submission to the WMT16 biomedical translation task. Our neural systems are described separately in Sennrich et al. (2016a). In most cases, our statistical systems build on last year’s, incorporating recent modelling refinements and adding this year’s new training data. For Romanian—a new language this year—we paid particular attention to language-specific processing of diacritics. For English→Czech, we experimented with a string-to-tree system, first using Treex1 (formerly TectoMT; Popel and Žabokrtský, 2010) to produce Czech dependency parses, then converting them to constituency representation and extracting GHKM rules. In the next two sections, we describe the phrasebased systems, first describing the core s"
W16-2327,W11-2126,1,0.846339,"dependency graphs with non-projective arcs. In order to extract SCFG rules, we first applied the following conversion process: i) the dependency graphs were projectivized using the Malt Parser, which implements the method described in Nivre and Nilsson (2005) (we used the ‘Head’ encoding scheme); ii) the projective dependency graphs were converted to CFG trees. In addition, we reduced the complex positional tags to simple POS tags by discarding the morphological attributes. The CFG trees were not binarized. We also experimented with unificationbased agreement and case government constraints (Williams and Koehn, 2011; Williams, 2014). Specifically, our constraints were designed to enforce: i) case, gender, and number agreement between nouns and pre-nominal adjectival modifiers; ii) number and person agreement between subjects and verbs; iii) case agreement between prepositions and nouns; iv) use of nominative case for subject nouns. For every Czech word in the training data, we obtained a set of morphological analyses using MorphoDiTa (Straková et al., 2014). From these analyses, we constructed a lexicon of feature structures. For constraint extraction, we used handwritten rules along the lines of those d"
W16-2327,R13-1079,1,0.855359,"ased models, we used fast_align for word alignment and the grow-diag-final-and heuristic for symmetrization. 4.3 unbinarized 5 20 5 Table 4: Parameter settings for rule composition. The parameters were relaxed for systems that used binarization to allow for the increase in tree node density. Except for English-Czech, which we describe separately in Section 5.1, preprocessing was similar to the phrase-based systems (Section 2.3). To parse the target-side of the training data, we used the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007) for English, and the ParZu dependency parser (Sennrich et al., 2013) for German. Except where stated otherwise, we right-binarized the trees after parsing to increase rule coverage. 4.2 Rule Extraction Language Models As in the phrase-based systems (Section 2.3), we used linearly-interpolated language models as standard, with some systems adding Common403 In preliminary experiments we used a smaller training set, comprising 2 million sentence pairs sampled from OPUS and monolingual data from last year’s WMT translation task. We used two test sets from the HimL project and the Khresmoi test set. Results with and without constraints are shown in Table 5. We used"
W16-2327,W12-3150,1,0.844228,"M sentences of parallel data. Our monolingual corpus was made up of three parts: all the English monolingual medical data from WMT14 medical, WMT16 biomedical and EMEA (11M sentences); all the English LDC GigaWord data (180M sentences); and all the English general domain data from WMT16 (240M sentences). We used the monolingual data to build three different language models which were then linearly interpolated. System tuning was with the SCIELO development data provided for the biomedical task. 4 4.4 SCFG rules were extracted from the word-aligned parallel data using the Moses implementation (Williams and Koehn, 2012) of the GHKM algorithm (Galley et al., 2004, 2006). Minimal GHKM rules were composed into larger rules subject to restrictions on the size of the resulting tree fragment. We used the settings shown in Table 4, which were chosen empirically during the development of 2013’s systems (Nadejde et al., 2013). parameter rule depth node count rule size Syntax-based System Overview For all syntax-based systems, we used a string-totree model based on a synchronous context-free grammar (SCFG) with linguistically-motivated labels on the target side. 4.1 Preprocessing binarized 7 30 7 Further to the restri"
W16-2327,W14-3324,1,0.800794,"Missing"
W16-2327,W15-3024,1,0.879589,"rder. Since the hard unification constraints effectively only avoid some of the possible translations (i.e. reduce the search space), we conclude that having to obey mere agreement constraints helps to select a hypothesis better in a surprisingly larger span of words, improving overall sentence structure on average. Crazy Reordering 3 Table 7: Manual evaluation of translations as proposed by the English→Czech system with unification constraints vs. the same system without constraints. 5.2 English→German This year’s string-to-tree submission for English→German is similar to last year’s system (Williams et al., 2015). In addition to the baseline feature functions, it contains count-based 5-gram Neural Network language model (NPLM) (Vaswani et al., 2013), a relational dependency language model (RDLM) (Sennrich, 2015), and soft source-syntactic constraints (Huck et al., 2014). The parameters of the model are tuned towards the linear interpolation of B LEU and the syntactic metric HWCM (Liu and Gildea, 2005; Sennrich, 2015). Trees are transformed through binarization and a hierarchical representation of morphologically complex words (Sennrich and Haddow, 2015). For the soft source-syntactic constraints, we a"
W16-2327,P14-5003,0,0.0285107,"Missing"
W16-2327,tiedemann-2012-parallel,0,0.108338,". 3 Neural LMs For some of our phrase-based systems we experimented with feed-forward neural network language models, both trained on target n-grams only, and on “joint” or “bilingual” ngrams (Devlin et al., 2014; Le et al., 2012). For training these models we used the NPLM toolkit (Vaswani et al., 2013), for which we have now implemented gradient clipping to address numerical issues often encountered during training. 2.4 3.1 Phrase-based Experiments Finnish→English Similar to last year (Haddow et al., 2015), we built an unconstrained system for Finnish→English using data extracted from OPUS (Tiedemann, 2012). Our parallel training set was the same as we used previously, but the language model training set was extended with the addition of the news2015 monolingual corpus and the large WMT16 English CommonCrawl corpus. We used newsdev2015 for tuning, and newsdev2015 for testing during system development. One clear problem that we noted with our submission from last year was the large number of OOVs, which were then copied directly into the English output. This is undoubtedly due to the agglutinative nature of Finnish, and probably was the cause of our system being poorly judged by human evaluators,"
W16-2327,D13-1140,0,0.194779,"(Koehn and Haddow, 2009). CommonCrawl LMs Our CommonCrawl language models were trained in the same way as the individual corpus-specific standard models, but were not linearly-interpolated with other LMs. Instead, the log probabilities of CommonCrawl LMs were added as separate features of the systems’ linear models. 3 Neural LMs For some of our phrase-based systems we experimented with feed-forward neural network language models, both trained on target n-grams only, and on “joint” or “bilingual” ngrams (Devlin et al., 2014; Le et al., 2012). For training these models we used the NPLM toolkit (Vaswani et al., 2013), for which we have now implemented gradient clipping to address numerical issues often encountered during training. 2.4 3.1 Phrase-based Experiments Finnish→English Similar to last year (Haddow et al., 2015), we built an unconstrained system for Finnish→English using data extracted from OPUS (Tiedemann, 2012). Our parallel training set was the same as we used previously, but the language model training set was extended with the addition of the news2015 monolingual corpus and the large WMT16 English CommonCrawl corpus. We used newsdev2015 for tuning, and newsdev2015 for testing during system d"
W16-2327,N07-1051,0,\N,Missing
W16-2327,W09-0429,1,\N,Missing
W16-2327,P05-1045,0,\N,Missing
W16-2327,P13-2121,0,\N,Missing
W17-4710,W14-4012,0,0.19765,"Missing"
W17-4710,D14-1179,0,0.0569526,"Missing"
W17-4710,Q16-1027,0,0.426398,"schemes (Pascanu et al., 2014) that give rise to different, orthogonal, definitions of depth (Zhang et al., 2016) which can affect the model performance depending on a given task. This is further complicated in sequence-to-sequence models as they contain multiple sub-networks, recurrent or feed-forward, each of which can be deep in different ways, giving rise to a large number of possible configurations. In this work we focus on stacked and deep transition recurrent architectures as defined by Pascanu et al. (2014). Different types of stacked architectures have been successfully used for NMT (Zhou et al., 2016; Wu et al., 2016). However, there is a lack of empirical comparisons of different deep architectures. Deep transition architectures have been successfully used for language modeling (Zilly et al., 2016), but not for NMT so far. We evaluate these architectures, both alone and in combination, varying the connection scheme between the different components and their depth over the different dimensions, measuring the performance of the different configurations on the WMT news translation task.1 Related work includes that of Britz et al. (2017), who have performed an exploration of NMT architecture"
W17-4710,E17-2060,1,0.824839,"o models are large and their results are highlighted separately. tances, since each layer may lose information during forward computation or backpropagation. This may not be a significant issue in the encoder, as the attention mechanism provides short paths from any source word state to the decoder, but the decoder contains no such shortcuts between its states, therefore it might be possible that this negatively affects its ability to model long-distance relationships in the target text, such as subject–verb agreement. Here, we seek to answer this question by testing our models on Lingeval97 (Sennrich, 2017), a test set which provides contrastive translation pairs for different types of errors. For the example of subject-verb agreement, contrastive translations are created from a reference translation by changing the grammatical number of the verb, and we can measure how often the NMT model prefers the correct reference over the contrastive variant. In Figure 5, we show accuracy as a function of the distance between subject and verb. We find that information is successfully passed over long distances by the deep recurrent transition network. Even for decisions that require information to be carri"
W17-4710,E17-3017,1,0.827403,"der in this work are GRU (Cho et al., 2014a) sequence-to-sequence transducers (Sutskever et al., 2014; Cho et al., 2014b) with attention (Bahdanau et al., 2015). In this section we describe the baseline system and the variants that we evaluated. 2.1 Figure 1: Deep transition decoder GRU transition for the current time step is carried over as the ""state"" input of the first GRU transition for the next time step. Applying this architecture to NMT is a novel contribution. Baseline Architecture As our baseline, we use the NMT architecture implemented in Nematus, which is described in more depth by Sennrich et al. (2017b). We augment it with layer normalization (Ba et al., 2016), which we have found to both improve translation quality and make training considerably faster. For our discussion, it is relevant that the baseline architecture already exhibits two types of depth: 2.2.1 Deep Transition Encoder As in a baseline shallow Nematus system, the encoder is a bidirectional recurrent neural network. Let Ls be the encoder recurrence depth, then for the i-th source word in the forward direction the → − → − forward source word state h i ≡ h i,Ls is computed as:  →  → − − h i,1 = GRU1 xi , h i−1,Ls  →  → − −"
W17-4710,W17-4739,1,\N,Missing
W17-4717,W17-4758,0,0.0835228,"ese submissions investigate alternative machine learning models for the prediction of the HTER score on the sentencelevel task. Instead of directly predicting the HTER score, the systems use a single-layer perceptron with four outputs that jointly predict the number of each of the four distinct post-editing operations that are then used to calculate the HTER score. This also gives the possibility to correct invalid (e.g. negative) predicted values prior to the calculation of the HTER score. The two submissions use the baseline features and the EnglishGerman submission also uses features from (Avramidis, 2017a). JXNU (T1): The JXNU submissions use features extracted from a neural network, including embedding features and cross-entropy features of the source sentences and their machine translations. The sentence embedding features are extracted through global average pooling from word embedding, which are trained using the WORD 2 VEC toolkit. The sentence cross-entropy features are calculated by a recurrent neural network language model. They experimented with different sentence embedding dimensions of the source sentences and translation outputs, as well as different sizes of the training corpus."
W17-4717,W17-4772,0,0.0310859,"Missing"
W17-4717,W17-4759,0,0.0327025,"Missing"
W17-4717,L16-1356,1,0.770826,"or BAD. The ﬁrst two matrices are built from 300 dimension word vectors computed with pretrained in-domain word embeddings. They train their model over 100 iterations with the l2 norm as regulariser and using the forward-backward splitting algorithm (FOBOS) (Duchi and Singer, 2009) as optimisation method. They report results considering the word and its context versus the word in isolation, as well as variants with and without the gold labels at training time. Finally, for the phrase-level task, SHEF made use of predictions generated by BMAPS for task 2 and the phrase labelling approaches in (Blain et al., 2016). These approaches use the number of BAD word-level predictions in a phrase: an optimistic version labels the phrase as OK if at least half of the words in it are predicted to be OK, and a superpessimistic version labels the phrase as BAD if any word is in is predicted to be BAD. UHH (T1): The UHH-STK submission is based on sequence and tree kernels applied on the source and target input data for predicting the HTER score. The kernels use a backtranslation of the MT output into the source language as an additional input data representation. Further hand-crafted features were deﬁned in the form"
W17-4717,W17-4760,1,0.841464,"Missing"
W17-4717,W17-4755,1,0.0393895,"nd Conference on Statistical Machine Translation (WMT) held at EMNLP 2017. This conference builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (constraint condition). We held 14 translation tasks this year, between English and each of Chinese, Czech, German, Finnish, Latvian, Russian, and Turkish. The Latvian and Chinese translation tasks were new this year. Latvian is a lesser resourced data condition on challenging language pair"
W17-4717,W07-0718,1,0.696979,") • bandit learning (Sokolov et al., 2017) This paper presents the results of the WMT17 shared tasks, which included three machine translation (MT) tasks (news, biomedical, and multimodal), two evaluation tasks (metrics and run-time estimation of MT quality), an automatic post-editing task, a neural MT training task, and a bandit learning task. 1 Introduction We present the results of the shared tasks of the Second Conference on Statistical Machine Translation (WMT) held at EMNLP 2017. This conference builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news translation task (Section 2), part"
W17-4717,W08-0309,1,0.537902,"Missing"
W17-4717,W10-1703,1,0.603181,"Missing"
W17-4717,W12-3102,1,0.508067,"Missing"
W17-4717,W17-4761,0,0.0349831,"Missing"
W17-4717,W17-4723,0,0.0362034,"Missing"
W17-4717,W17-4724,1,0.839009,"Missing"
W17-4717,W11-2103,1,0.744276,"Missing"
W17-4717,W17-4773,1,0.839713,"Missing"
W17-4717,W15-3025,1,0.905105,"Instead of predicting the HTER score, the systems attempted to predict the number of each of the four postediting operations (add, replace, shift, delete) at the sentence level. However, this did not lead to positive results. In future editions of the task, we plan to make this detailed post-editing information available again and suggest clear ways of using it. 5 Automatic Post-editing Task The WMT shared task on MT automatic postediting (APE), this year at its third round at WMT, aims to evaluate systems for the automatic correction of errors in a machine translated text. As pointed out by (Chatterjee et al., 2015b), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) post-editing effort; • Adapt the output of a general-purpose MT system to the lexicon/style requested in a speciﬁc application domain. The third round of the APE task proposed to parti"
W17-4717,W17-4718,1,0.0662673,"builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (constraint condition). We held 14 translation tasks this year, between English and each of Chinese, Czech, German, Finnish, Latvian, Russian, and Turkish. The Latvian and Chinese translation tasks were new this year. Latvian is a lesser resourced data condition on challenging language pair. Chinese allowed us to co-operate with an ongoing evaluation campaign on Asian languages org"
W17-4717,W17-4725,0,0.0391077,"Missing"
W17-4717,W08-0509,0,0.00859402,"l post-editors) and WMT17 DE-EN (German-English, pharmacological domain, professional post-editors) APE task data. TER BLEU APE15 23.84 n/a APE16 24.76 62.11 APE17 EN-DE 24.48 62.49 APE17 DE-EN 15.55 79.54 Table 26: Translation quality (TER/BLEU of TGT and proportion of TGTs with TER=0) of the WMT15, WMT16, WMT17 EN-DE and WMT17 DE-EN data. Figure 7: TER distribution over the EN-DE test set Figure 8: TER distribution over the DE-EN test set TERcom27 software: lower average TER scores correspond to higher ranks. BLEU was computed using the multi-bleu.perl package28 available in MOSES. MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heaﬁeld, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the system was tuned on the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Tables 28-29. For each submitted run, the statistical signiﬁcance of performance differences with respect to the baseline and our re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). 5.1.3 Baselines Also this year, the ofﬁci"
W17-4717,W17-4726,0,0.0387381,"Missing"
W17-4717,W13-2305,1,0.899702,"(RR) approach, so named because the pairwise comparisons denote only relative ability between a pair of systems, and cannot be used to infer absolute quality. For example, RR can be used to discover which systems perform better than others, but RR does not provide any information about the absolute quality of system translations, i.e. it provides no information about how far a given system is from producing perfect output according to a human user. Work on evaluation over the past few years has provided fresh insight into ways to collect direct assessments (DA) of machine translation quality (Graham et al., 2013, 2014, 2016), and last year’s evaluation campaign included parallel assessment of a subset of News task language pairs evaluated with RR and DA. DA has some clear advantages over RR, namely the evaluation of absolute translation quality and the ability to carry out evaluations through quality controlled crowd-sourcing. As established last year (Bojar et al., 2016a), DA results (via crowd-sourcing) and RR results (produced by researchers) correlate strongly, with Pearson correlation ranging from 0.920 to 0.997 across several source languages into English and at 0.975 for English-to-Russian (th"
W17-4717,E14-1047,1,0.508986,"Missing"
W17-4717,N15-1124,1,0.658439,"Missing"
W17-4717,W13-0805,0,0.0140095,"ely contain 25,000 and 1,000 triplets, while the test set consists of 2,000 instances. Table 24 provides some basic statistics about the data (the same used for the sentence-level quality estimation task), which has been released by the European Project QT21 (Specia et al., 2017b).25 In addition, Tables 25 and 26 provide a view of the data from a task difﬁculty standpoint. Table 25 shows the repetition rate (RR) values of the data sets released in the three rounds 24 We used phrase-based MT systems trained with generic and in-domain parallel training data, leveraging prereordering techniques (Herrmann et al., 2013), and taking advantage of POS and word class-based language models. 25 For both language directions, the source sentences and reference translations were provided by TAUS (https://www.taus.net/). 197 EN-DE Train (23,000) Dev (1,000) Test (2,000) DE-EN Train (25,000) Dev (1,000) Test (2,000) SRC Tokens TGT PE SRC Types TGT PE SRC Lemmas TGT PE 384448 17827 65120 403306 19355 69812 411246 19763 71483 18220 2931 8061 27382 3333 9765 31652 3506 10502 10946 1922 2626 21959 2686 3976 25550 2806 4282 437833 17578 35087 453096 18130 36082 456163 18313 36480 29745 4426 6987 19866 3583 5391 19172 3642 5"
W17-4717,W17-4775,0,0.0513454,"Missing"
W17-4717,W17-4730,1,0.829861,"Missing"
W17-4717,W17-4731,0,0.029579,"Missing"
W17-4717,W17-4727,0,0.046917,"Missing"
W17-4717,W16-2378,0,0.0869978,"a manuallyrevised version of the target, done by professional translators. Test data consists of (source, target) pairs having similar characteristics of those in the training set. Human post-edits of the test target instances were left apart to measure system performance. English-German data were drawn from the Information Technology (IT) domain. Training and test sets respectively contain 11,000 and 2,000 triplets. The data released for the 2016 round of the task (15,000 instances) and the artiﬁcially generated post-editing triplets (4 million instances) used by last year’s winning system (Junczys-Dowmunt and Grundkiewicz, 2016) were also provided as additional training material. German-English data were drawn from the Pharmacological domain. Training and development sets respectively contain 25,000 and 1,000 triplets, while the test set consists of 2,000 instances. Table 24 provides some basic statistics about the data (the same used for the sentence-level quality estimation task), which has been released by the European Project QT21 (Specia et al., 2017b).25 In addition, Tables 25 and 26 provide a view of the data from a task difﬁculty standpoint. Table 25 shows the repetition rate (RR) values of the data sets rele"
W17-4717,W11-2123,0,0.00943973,"-editors) APE task data. TER BLEU APE15 23.84 n/a APE16 24.76 62.11 APE17 EN-DE 24.48 62.49 APE17 DE-EN 15.55 79.54 Table 26: Translation quality (TER/BLEU of TGT and proportion of TGTs with TER=0) of the WMT15, WMT16, WMT17 EN-DE and WMT17 DE-EN data. Figure 7: TER distribution over the EN-DE test set Figure 8: TER distribution over the DE-EN test set TERcom27 software: lower average TER scores correspond to higher ranks. BLEU was computed using the multi-bleu.perl package28 available in MOSES. MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heaﬁeld, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the system was tuned on the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Tables 28-29. For each submitted run, the statistical signiﬁcance of performance differences with respect to the baseline and our re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). 5.1.3 Baselines Also this year, the ofﬁcial baseline results are the TER and BLEU scores calculated by comparing the raw MT o"
W17-4717,I17-1013,0,0.0334345,"Missing"
W17-4717,W16-2384,0,0.0137431,"features are calculated by a recurrent neural network language model. They experimented with different sentence embedding dimensions of the source sentences and translation outputs, as well as different sizes of the training corpus. The experimental results show that the neural network features lead to signiﬁcant improvements over the baseline, and that combining the neural network features with baseline features leads to further improvement. POSTECH (T1, T2, T3): POSTECH’s submissions to the sentence/word/phrase-level QE tasks are based on predictor-estimator architecture (Kim et al., 2017; Kim and Lee, 2016), which is the two-stage end-to-end ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamburg, Germany (Duma a"
W17-4717,W17-4763,0,0.0300648,"ence cross-entropy features are calculated by a recurrent neural network language model. They experimented with different sentence embedding dimensions of the source sentences and translation outputs, as well as different sizes of the training corpus. The experimental results show that the neural network features lead to signiﬁcant improvements over the baseline, and that combining the neural network features with baseline features leads to further improvement. POSTECH (T1, T2, T3): POSTECH’s submissions to the sentence/word/phrase-level QE tasks are based on predictor-estimator architecture (Kim et al., 2017; Kim and Lee, 2016), which is the two-stage end-to-end ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamb"
W17-4717,W04-3250,1,0.380171,"age28 available in MOSES. MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heaﬁeld, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the system was tuned on the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Tables 28-29. For each submitted run, the statistical signiﬁcance of performance differences with respect to the baseline and our re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). 5.1.3 Baselines Also this year, the ofﬁcial baseline results are the TER and BLEU scores calculated by comparing the raw MT output with the human post-edits. In practice, the baseline APE system is a “donothing” system that leaves all the test targets unmodiﬁed. Baseline results, the same shown in Table 26, are also reported in Tables 28-29 for comparison with participants’ submissions. In continuity with the previous rounds, we used as additional term of comparison a reimplementation of the method ﬁrstly proposed by Simard et al. (2007). It relies on a phrasebased post-editing approach to t"
W17-4717,P07-2045,1,0.0149362,"t with the human post-edits. In practice, the baseline APE system is a “donothing” system that leaves all the test targets unmodiﬁed. Baseline results, the same shown in Table 26, are also reported in Tables 28-29 for comparison with participants’ submissions. In continuity with the previous rounds, we used as additional term of comparison a reimplementation of the method ﬁrstly proposed by Simard et al. (2007). It relies on a phrasebased post-editing approach to the task, which represented the common backbone of APE systems before the spread of neural solutions. The system is based on Moses (Koehn et al., 2007); translation and reordering models were estimated following the Moses protocol with default setup using 27 http://www.cs.umd.edu/˜snover/tercom/ https://github.com/moses-smt/mosesdecoder/ blob/master/scripts/generic/multi-bleu.perl 28 5.2 Participants Seven teams participated in the English-German task by submitting a total of ﬁfteen runs. Two of them also participated in the German-English task with ﬁve submitted runs. Participants are listed in Table 27, and a short description of their systems is provided in the following. Adam Mickiewicz University. AMU’s (ENDE) participation explores and"
W17-4717,C14-1017,0,0.0141812,"ord embeddings approach used by (Scarton et al., 2016) for document-level QE. Here in-domain word embeddings are used instead of embeddings obtained general purpose data (same as in task 2, below). Word embeddings were averaged to generate a single vector for each sentence. Source and target word embeddings were then concatenated with the baseline features and given to an SVM regressor for model building. For the word-level task SHEF investigated a new approach based on predicting the strength of the lexical relationships between the source and target sentences (BMAPS). Following the work by (Madhyastha et al., 2014), a bilinear model is trained from three matrices corresponding to the training data, the development set and a “truth” matrix between them, which is built from the word alignments and the gold labels to indicate which lexical items form a pair, and whether or not their lexical relation is OK or BAD. The ﬁrst two matrices are built from 300 dimension word vectors computed with pretrained in-domain word embeddings. They train their model over 100 iterations with the l2 norm as regulariser and using the forward-backward splitting algorithm (FOBOS) (Duchi and Singer, 2009) as optimisation method."
W17-4717,Q17-1015,0,0.0112193,"ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamburg, Germany (Duma and Menzel, 2017) Unbabel, Portugal (Martins et al., 2017b) Table 10: Participants in the WMT17 quality estimation shared task. neural QE model. The predictor-estimator architecture consists of two types of stacked neural network models: 1) a word prediction model based on bidirectional and bilingual recurrent neural network language model trained on additional large-scale parallel corpora and 2) a neural quality estimation model trained on quality-annotated noisy parallel corpora. To jointly learn the two-stage model, a stack propagation method was applied (Zhang and Weiss, 2016). In addition, a “multilevel model” was developed where a task-speciﬁc"
W17-4717,W16-2387,0,0.0126259,"the SMT parallel corps. These features were used to train a Support Vector Regression (SVR) algorithm using a Radial Basis Function (RBF) kernel within the SCIKITLEARN toolkit.13 The γ, � and C parameters were optimised via grid search with 5-fold cross validation on the training set, resulting in γ=0.01, � = 0.0825, C = 20. This baseline system has proved robust across a range of language pairs, MT systems, and text domains for predicting various In addition to that, 6 new features were included which contain combinations of other features, and which proved useful in (Kreutzer et al., 2015; Martins et al., 2016): 12 https://github.com/ghpaetzold/ questplusplus 13 http://scikit-learn.org/ 184 • • • • Target word + left context. Target word + right context. Target word + aligned source word. POS of target word + POS of aligned source word. • Target word + left context + source word. • Target word + right context + source word. • Punctuation features: The baseline system models the task as a sequence prediction problem using the LinearChain Conditional Random Fields (CRF) algorithm within the CRFSuite tool (Okazaki, 2007). The model was trained using passive-aggressive optimisation algorithm. We note th"
W17-4717,W17-4764,0,0.0178514,"ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamburg, Germany (Duma and Menzel, 2017) Unbabel, Portugal (Martins et al., 2017b) Table 10: Participants in the WMT17 quality estimation shared task. neural QE model. The predictor-estimator architecture consists of two types of stacked neural network models: 1) a word prediction model based on bidirectional and bilingual recurrent neural network language model trained on additional large-scale parallel corpora and 2) a neural quality estimation model trained on quality-annotated noisy parallel corpora. To jointly learn the two-stage model, a stack propagation method was applied (Zhang and Weiss, 2016). In addition, a “multilevel model” was developed where a task-speciﬁc"
W17-4717,W06-3114,1,0.104699,"g (Bojar et al., 2017b) • bandit learning (Sokolov et al., 2017) This paper presents the results of the WMT17 shared tasks, which included three machine translation (MT) tasks (news, biomedical, and multimodal), two evaluation tasks (metrics and run-time estimation of MT quality), an automatic post-editing task, a neural MT training task, and a bandit learning task. 1 Introduction We present the results of the shared tasks of the Second Conference on Statistical Machine Translation (WMT) held at EMNLP 2017. This conference builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news trans"
W17-4717,P03-1021,0,0.144477,"ed better than monolingual models. The code for these models is freely available.15 DCU (T2): DCU’s submission is an ensemble of neural MT systems with different input factors, designed to jointly tackle both the automatic post-editing and word-level QE. 15 https://github.com/patelrajnath/rnn4nlp 186 Word-level features which have proven effective for QE, such as part-of-speech tags and dependency labels are included as input factors to NMT systems. NMT systems using different input representations are ensembled together in a log-linear model which is tuned for the F1 -mult metric using MERT (Och, 2003). The output of the ensemble is a pseudo-reference that is then TER aligned with the original MT to obtain OK/BAD tags for each word in the MT hypothesis. DFKI (T1): These submissions investigate alternative machine learning models for the prediction of the HTER score on the sentencelevel task. Instead of directly predicting the HTER score, the systems use a single-layer perceptron with four outputs that jointly predict the number of each of the four distinct post-editing operations that are then used to calculate the HTER score. This also gives the possibility to correct invalid (e.g. negativ"
W17-4717,W15-3037,0,0.0138062,"n in the source side of the SMT parallel corps. These features were used to train a Support Vector Regression (SVR) algorithm using a Radial Basis Function (RBF) kernel within the SCIKITLEARN toolkit.13 The γ, � and C parameters were optimised via grid search with 5-fold cross validation on the training set, resulting in γ=0.01, � = 0.0825, C = 20. This baseline system has proved robust across a range of language pairs, MT systems, and text domains for predicting various In addition to that, 6 new features were included which contain combinations of other features, and which proved useful in (Kreutzer et al., 2015; Martins et al., 2016): 12 https://github.com/ghpaetzold/ questplusplus 13 http://scikit-learn.org/ 184 • • • • Target word + left context. Target word + right context. Target word + aligned source word. POS of target word + POS of aligned source word. • Target word + left context + source word. • Target word + right context + source word. • Punctuation features: The baseline system models the task as a sequence prediction problem using the LinearChain Conditional Random Fields (CRF) algorithm within the CRFSuite tool (Okazaki, 2007). The model was trained using passive-aggressive optimisatio"
W17-4717,P16-1160,0,0.0330991,"t attention (looking at information anywhere in the source sequence during decoding) and hard monotonic attention (looking at one encoder state at a time from left to right, thus being more conservative and faithful to the original input), which are combined in different ways in the case of multi-source models. The artiﬁcial data provided by JunczysDowmunt and Grundkiewicz (2016) are used to boost performance by increasing the size of the corpus used for training. Univerzita Karlova v Praze. CUNI’s (EN-DE) system is based on the character-to-character neural network architecture described in (Lee et al., 2016). This architecture was compared with the standard neural network architecture proposed by Bahdanau et al. (2014) which uses byte-pair encoding (Sennrich et al., 2015) for generating translation tokens. During the experiments, two setups have been compared for each architecture: i) a single encoder with SRC and MT sentences concatenated, and ii) a two-encoder system, where each SRC and MT sentence is fed to a separate encoder. The submitted system uses the two-encoder architecture with a character-level encoder and decoder. The initial state of the decoder is a weighted combination of the ﬁnal"
W17-4717,W17-4733,0,0.0368107,"Missing"
W17-4717,W17-4765,1,0.786172,"Missing"
W17-4717,L16-1582,1,0.768758,"T 183 translations labelled with task-speciﬁc labels. Participants were also provided with a baseline set of features for each task, and a software package to extract these and other quality estimation features and perform model learning (Section 4.1). Participants (Section 4.2) could submit up to two systems for each task. A discussion on the main goals and ﬁndings from this year’s task is given in Section 4.7. 4.1 Baseline systems forms of post-editing effort (2012; 2013; 2014; 2015; 2016a). Word-level baseline system: For Task 2, the baseline features were extracted with the M AR MOT tool (Logacheva et al., 2016). These are 28 features that have been deemed the most informative in previous research on word-level QE. 22 of them were taken from the feature set described in (Luong et al., 2014), and had also been used as a baseline feature set at WMT16: Sentence-level baseline system: For Task 1, Q U E ST ++12 (2015) was used to extract 17 MT system-independent features from the source and translation (target) ﬁles and parallel corpora: • Word count in the source and target sentences, and source and target token count ratio. Although these features are sentencelevel (i.e. their values will be the same fo"
W17-4717,C16-1241,0,0.0353857,"Missing"
W17-4717,W14-3342,0,0.0181229,"lity estimation features and perform model learning (Section 4.1). Participants (Section 4.2) could submit up to two systems for each task. A discussion on the main goals and ﬁndings from this year’s task is given in Section 4.7. 4.1 Baseline systems forms of post-editing effort (2012; 2013; 2014; 2015; 2016a). Word-level baseline system: For Task 2, the baseline features were extracted with the M AR MOT tool (Logacheva et al., 2016). These are 28 features that have been deemed the most informative in previous research on word-level QE. 22 of them were taken from the feature set described in (Luong et al., 2014), and had also been used as a baseline feature set at WMT16: Sentence-level baseline system: For Task 1, Q U E ST ++12 (2015) was used to extract 17 MT system-independent features from the source and translation (target) ﬁles and parallel corpora: • Word count in the source and target sentences, and source and target token count ratio. Although these features are sentencelevel (i.e. their values will be the same for all words in a sentence), the length of a sentence might inﬂuence the probability of a word being wrong. • Target token, its left and right contexts of 1 word. • Source word aligne"
W17-4717,P16-2046,0,0.0171489,"Missing"
W17-4717,W16-2379,0,0.0228966,"Missing"
W17-4717,P02-1040,0,0.119222,"Missing"
W17-4717,W16-2389,0,0.038207,"Missing"
W17-4717,W17-4736,0,0.0268357,"Missing"
W17-4717,W17-4737,0,0.0383776,"Missing"
W17-4717,W17-4738,0,0.0427074,"Missing"
W17-4717,W14-3301,1,0.655758,"Missing"
W17-4717,W16-2391,1,0.761669,"target sentences into sequences of character embeddings, and then passes them through a series of deep parallel stacked convolution/max pooling layers. The baseline features are provided through a multi-layer perceptron, 187 and then concatenated with the characterlevel information. Finally, the concatenation is passed onto another multi-layer perceptron and the very last layer outputs HTER values. The two submissions differ in the the use of standard (CNN+BASE-Single) and multi-task learning (CNN+BASE-Multi) for training. The QUEST-EMB submission follows the word embeddings approach used by (Scarton et al., 2016) for document-level QE. Here in-domain word embeddings are used instead of embeddings obtained general purpose data (same as in task 2, below). Word embeddings were averaged to generate a single vector for each sentence. Source and target word embeddings were then concatenated with the baseline features and given to an SVM regressor for model building. For the word-level task SHEF investigated a new approach based on predicting the strength of the lexical relationships between the source and target sentences (BMAPS). Following the work by (Madhyastha et al., 2014), a bilinear model is trained"
W17-4717,W16-2323,1,0.349233,"theses. Concatenated source + MT hypothesis are also used as an input representation for some models. The system makes extensive use of the synthetic training data provided by Junczys-Dowmunt and Grundkiewicz (2016), as well as min-risk training for ﬁne-tuning (Shen et al., 2016). The neural systems, which use different input representations but share the same output vocabulary, are then ensembled together in a loglinear model which is tuned for the TER metric using MERT. Fondazione Bruno Kessler. FBK’s (EN-DE & DE-EN) submission extends the existing NMT implementation in the Nematus toolkit (Sennrich et al., 2016) to train an ensemble of multi-source neural APE systems. Building on previous participations based on the phrase-based paradigm (Chatterjee et al., 2015a, 2016), and similar to (Libovick´y et al., 2016), such systems jointly learn from source and target information in order to increase robustness and precision of the automatic corrections. The n-best hypotheses produced by 200 this ensemble are further re-ranked using features based on the edit distance between the original MT output and each APE hypothesis, as well as other statistical models (n-gram language model and operation sequence mod"
W17-4717,P16-1159,0,0.0156896,"different input factors, designed to jointly tackle both the APE task and the Word-Level QE task. Word-Level features which have proven effective for QE, such as word-alignments, partof-speech tags, and dependency labels, are included as input factors to neural machine translation systems, which are trained to output PostEdited MT hypotheses. Concatenated source + MT hypothesis are also used as an input representation for some models. The system makes extensive use of the synthetic training data provided by Junczys-Dowmunt and Grundkiewicz (2016), as well as min-risk training for ﬁne-tuning (Shen et al., 2016). The neural systems, which use different input representations but share the same output vocabulary, are then ensembled together in a loglinear model which is tuned for the TER metric using MERT. Fondazione Bruno Kessler. FBK’s (EN-DE & DE-EN) submission extends the existing NMT implementation in the Nematus toolkit (Sennrich et al., 2016) to train an ensemble of multi-source neural APE systems. Building on previous participations based on the phrase-based paradigm (Chatterjee et al., 2015a, 2016), and similar to (Libovick´y et al., 2016), such systems jointly learn from source and target inf"
W17-4717,2006.amta-papers.25,0,0.817721,"trast to last year, we also provide datasets for two language pairs. The structure used for the data have been the same since WMT15. Each data instance consists of (i) a source sentence, (ii) its automatic translation into the target language, (iii) the manually post-edited version of the automatic translation, (iv) a free reference translation of the source sentence. Post-edits are used to extract labels for the 4.4 Task 1: Predicting sentence-level quality This task consists in scoring (and ranking) translation sentences according to the proportion of their words that need to be ﬁxed. HTER (Snover et al., 2006b) is used as quality score, i.e. the minimum edit distance between the machine translation and its manually post-edited version. Labels HTER labels were computed using the TERCOM tool16 with default settings (tokenised, case insensitive, exact matching only), with scores capped to 1. 188 16 http://www.cs.umd.edu/˜snover/tercom/ Evaluation Evaluation was performed against the true HTER label and/or ranking, using the following metrics: with an edit operation: insertion, deletion, substitution or no edit (correct word). We mark each edited word as BAD, and the remainingn as OK. • Scoring: Pears"
W17-4717,W17-4756,0,0.0529328,"Missing"
W17-4717,P15-4020,1,0.738308,"Missing"
W17-4717,P13-4014,1,0.634274,"Missing"
W17-4717,W17-4720,1,0.830567,"Missing"
W17-4717,W17-4776,0,0.0596806,"Missing"
W17-4717,W17-4777,1,0.832756,"Missing"
W17-4717,W17-4742,0,0.0342495,"Missing"
W17-4717,W17-4744,0,0.0607458,"Missing"
W17-4717,C00-2137,0,0.0420443,"ch edited word as BAD, and the remainingn as OK. • Scoring: Pearson’s r correlation score (primary metric, ofﬁcial score for ranking submissions), Mean Average Error (MAE) and Root Mean Squared Error (RMSE). Evaluation Analogously to the last year’s task, the primary evaluation metric is the multiplication of F1 -scores for the OK and BAD classes, denoted as F1 -mult. Unlike previously used F1 BAD score this metric is not biased towards “pessimistic” labellings. We also report F1 -scores for individual classes for completeness. We test the signiﬁcance of the results using randomisation tests (Yeh, 2000) with Bonferroni correction (Abdi, 2007). • Ranking: Spearman’s ρ rank correlation and DeltaAvg. Statistical signiﬁcance on Pearson r was computed using the William’s test.17 Results Tables 13 and 14 summarise the results for Task 1 on German–English and English– German datasets, respectively, ranking participating systems best to worst using Pearson’s r correlation as primary key. Spearman’s ρ correlation scores should be used to rank systems for the ranking variant. The top three systems are the same for both datasets, and the ranking of systems according to their performance is similar for"
W17-4717,W17-4745,1,0.825998,"Missing"
W17-4717,P16-1147,0,0.0102821,"Missing"
W17-4719,L16-1470,1,0.868974,"Missing"
W17-4719,C16-2064,0,0.0199749,"ach language pair, and implemented byte pair encoding (BPE) (subword units) in their systems (Wolk and Marasek, 2017). Only the official parallel text corpora and monolingual models for the challenge evaluation campaign were used to train language models, and to develop, tune, and test their system. PJIIT explored the use of domain adaptation techniques, symmetrized word alignment models, the unsupervised transliteration models and the KenLM language modeling tool. kyoto (Kyoto University). The system from the team from Kyoto University is based on two previous papers (Cromieres et al., 2016; Cromieres, 2016). The participants describe it as a classic neural machine translation (NMT) system, however, we do not have further information regarding the datasets that have been used to train and tune the system for the WMT challenge. uedin-nmt (University of Edinburgh). The systems from the University of Edinburgh used a NMT trained with Nematus, an attentional encoder-decoder (Sennrich et al., 2017). Their setup follows the one from last year. This team again built BPE-based models with parallel and back-translated monolingual training data. New approaches this year included the use of deep architectur"
W17-4719,W16-4616,0,0.0207529,"training settings for each language pair, and implemented byte pair encoding (BPE) (subword units) in their systems (Wolk and Marasek, 2017). Only the official parallel text corpora and monolingual models for the challenge evaluation campaign were used to train language models, and to develop, tune, and test their system. PJIIT explored the use of domain adaptation techniques, symmetrized word alignment models, the unsupervised transliteration models and the KenLM language modeling tool. kyoto (Kyoto University). The system from the team from Kyoto University is based on two previous papers (Cromieres et al., 2016; Cromieres, 2016). The participants describe it as a classic neural machine translation (NMT) system, however, we do not have further information regarding the datasets that have been used to train and tune the system for the WMT challenge. uedin-nmt (University of Edinburgh). The systems from the University of Edinburgh used a NMT trained with Nematus, an attentional encoder-decoder (Sennrich et al., 2017). Their setup follows the one from last year. This team again built BPE-based models with parallel and back-translated monolingual training data. New approaches this year included the use o"
W17-4719,W17-4754,0,0.123112,"Munich has participated with an en2de NMT system (Huck and Fraser, 2017). A distinctive feature of their system is a linguistically informed, cascaded target word segmentation approach. Fine-tuning for the domain of health texts was done using in-domain sections of the UFAL Medical Corpus v.1.0 as a training corpus. The learning rate was set to 0.00001, initialized with a pre-trained model, and optimized using only the in-domain medical data. The HimL tun13 UHH (University of Hamburg). All SMT models were developed using the Moses phrase-based MT toolkit and the Experiment Management System (Duma and Menzel, 2017). The preprocessing of the data consisted of tokenization, cleaning (6-80), lowercasing and normalizing punctuation. The tuning and the test sets were derived from WMT 2016 and WMT 2017. The SRILM toolkit https://lilt.com/ 236 LIMSI baseline. For additional comparison, we also provided the results of an en2fr Moses-based system prepared by Ive et al. for their participation in the WMT16 biomedical track, which reflects the state of the art for this language pair (Ive et al., 2016a). The system uses in-domain parallel data provided for the biomedical task in 2016, as well as additional in-domai"
W17-4719,federmann-2010-appraise,0,0.0291939,"run2 LMU PJIIT run1 PJIIT run2 PJIIT run3 uedin-nmt run1 uedin-nmt run2 UHH run1 UHH run2 UHH run3 cs 15.93* 22.79* - de 20.45* 27.57* 26.79 29.46* 21.88* 33.06* 18.71 19.80 19.66* fr 22.99* 31.79 31.89 33.36* pl 14.09* 14.32 10.75 14.34* 23.15* 19.87 - es 40.97 41.20 41.22* ro 10.56* 18.10* 29.32* 27.32 - Table 7: Results for the NHS test sets. * indicates the primary run as informed by the participants. native speakers of the languages and were either members of the participating teams or colleagues from the research community. The validation task was carried out using the Appraise tool15 (Federmann, 2010). For each pairwise comparison, we validated a total of 100 randomly-chosen sentence pairs. The validation consisted of reading the two sentences (A and B), i.e., translations from two systems or from the reference, and choosing one of the options below: The manual validation for the Scielo test sets is presented in Table 8, for the comparison of the only participating team (UHH) to the reference translation. For en2es, the automatic translation scored lower than the reference one in 53 out of 100 pairs, but could still beat the reference translation in 23 pairs. For en2pt, the automatic trans"
W17-4719,W17-4730,0,0.0242129,"uilt BPE-based models with parallel and back-translated monolingual training data. New approaches this year included the use of deep architectures, layer normalization, and more compact models due to weight-tying and improvements in BPE segmentations. Lilt (Lilt Inc.). The system from the Lilt Inc.13 uses an in-house implementation of a sequenceto-sequence model with Bahdanau-style attention. The final submissions are ensembles between models fine-tuned on different parts of the available data. LMU (Ludwig Maximilian University of Munich). LMU Munich has participated with an en2de NMT system (Huck and Fraser, 2017). A distinctive feature of their system is a linguistically informed, cascaded target word segmentation approach. Fine-tuning for the domain of health texts was done using in-domain sections of the UFAL Medical Corpus v.1.0 as a training corpus. The learning rate was set to 0.00001, initialized with a pre-trained model, and optimized using only the in-domain medical data. The HimL tun13 UHH (University of Hamburg). All SMT models were developed using the Moses phrase-based MT toolkit and the Experiment Management System (Duma and Menzel, 2017). The preprocessing of the data consisted of tokeni"
W17-4719,W16-2337,0,0.097915,". All SMT models were developed using the Moses phrase-based MT toolkit and the Experiment Management System (Duma and Menzel, 2017). The preprocessing of the data consisted of tokenization, cleaning (6-80), lowercasing and normalizing punctuation. The tuning and the test sets were derived from WMT 2016 and WMT 2017. The SRILM toolkit https://lilt.com/ 236 LIMSI baseline. For additional comparison, we also provided the results of an en2fr Moses-based system prepared by Ive et al. for their participation in the WMT16 biomedical track, which reflects the state of the art for this language pair (Ive et al., 2016a). The system uses in-domain parallel data provided for the biomedical task in 2016, as well as additional in-domain data14 and out-ofdomain data. However, we did not perform SOUL re-scoring. and Kneser-Ney discounting were used to estimate 5-gram language models (LM). For word alignment, GIZA++ with the default grow-diag-finaland alignment symmetrization method was used. Tuning of the SMT systems was performed with MERT. Commoncrawl and Wikipedia were used as general domain data for all language pairs except for EN/PT, where no Commoncrawl data was provided by WMT. As for the in-domain corpo"
W17-4719,W17-4739,1,\N,Missing
W17-4719,W17-4743,0,\N,Missing
W17-4739,P10-2041,0,0.0320685,"Missing"
W17-4739,buck-etal-2014-n,1,0.80914,"Missing"
W17-4739,W17-4705,0,0.0149621,"Missing"
W17-4739,D14-1179,0,0.00518473,"Missing"
W17-4739,P16-1009,1,0.524474,"ablative experiments, reporting on the effectivenes of layer normalization, deep architectures, and different ensembling techniques. 1 Introduction We participated in the WMT17 shared news translation task for 12 translation directions, translating between English and Czech, German, Latvian, Russian, Turkish and Chinese, and in the WMT17 shared biomedical translation task for English to Czech, German, Polish and Romanian.1 We submitted neural machine translation systems trained with Nematus (Sennrich et al., 2017). Our setup is based on techniques described in last year’s system description (Sennrich et al., 2016a), including the use of subword models (Sennrich et al., 1 2 Novelties Here we describe the main differences to last year’s systems. We provide trained models and training commands at http://data.statmt.org/wmt17_systems/ 389 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 389–399 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics 2.1 Subword Segmentation stacked architecture. Implementations of both of these architectures are available in Nematus. For completeness, we here reproduce the description of the"
W17-4739,P16-1162,1,\N,Missing
W17-4739,E17-2025,0,\N,Missing
W18-6320,2009.mtsummit-posters.5,0,0.123004,"Missing"
W18-6320,W15-4918,1,0.909017,"Missing"
W18-6320,W16-2301,1,0.829153,"Missing"
W18-6320,2001.mtsummit-papers.20,0,0.488483,"Missing"
W18-6320,1999.mtsummit-1.42,0,0.32113,"Missing"
W18-6320,W11-2123,0,0.0202271,"Missing"
W18-6320,E06-1032,0,0.205885,"Missing"
W18-6320,N07-2020,0,0.0997368,"Missing"
W18-6320,L16-1048,0,0.0241495,"Missing"
W18-6320,2014.eamt-1.40,0,0.0438492,"Missing"
W18-6320,2000.tc-1.5,0,0.320841,"Missing"
W18-6320,W15-2402,0,0.0382638,"Missing"
W18-6320,stymne-etal-2012-eye,0,0.038532,"Missing"
W18-6320,P07-2045,1,0.0112762,"Missing"
W18-6320,1993.tmi-1.22,0,0.82749,"Missing"
W18-6320,W11-2401,0,0.033505,"Missing"
W18-6320,2012.freeopmt-1.3,0,0.127684,"Missing"
W18-6320,weiss-ahrenberg-2012-error,0,0.0760185,"Missing"
W18-6320,N16-1125,0,0.0253847,"Missing"
W18-6401,W18-6432,1,0.779979,"Missing"
W18-6401,W07-0718,1,0.492999,"Participants were asked to build machine translation systems for any of 7 language pairs in both directions, to be evaluated on a test set of news stories. The main metric for this task is human judgment of translation quality. This year, we also opened up the task to additional test sets to probe specific aspects of translation. 1 Introduction The Third Conference on Machine Translation (WMT) held at EMNLP 20181 host a number of shared tasks on various aspects of machine translation. This conference builds on twelve previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a, 2017). This year we conducted several official tasks. We report in this paper on the news translation task. Additional shared tasks are described in separate papers in these proceedings: • biomedical translation (Neves et al., 2018), • multimodal machine translation (Barrault et al., 2018), • metrics (Ma et al., 2018), • quality estimation (Specia et al., 2018), • automatic post-editing (Chatterjee et al., 2018), and • parallel corpus filtering (Koehn et al., 2018b). In the news translation task (Section 2), participants we"
W18-6401,W11-2101,1,0.731598,"Missing"
W18-6401,W08-0309,1,0.64499,"Missing"
W18-6401,W10-1703,1,0.498736,"Missing"
W18-6401,W12-3102,1,0.647494,"Missing"
W18-6401,E14-2008,0,0.0242419,"n the WMT18 bitexts provided, including ParaCrawl. Some models employed pretrained word embeddings built on BPE’d corpora (Sennrich et al., 2016). A Marian transformer model performed right-to-left rescoring for this system. The third system is trained with Moses (Koehn et al., 2007), using the same data as the Marian system. Hierarchical reordering and Operation Sequence Model were employed. The 5-gram English language model was trained with KenLM (Heafield, 2011) on the same corpus as the AFRL WMT15 system with the same BPE used in the Marian systems. Lastly, RWTH Jane’s system combination (Freitag et al., 2014) was applied yielding approximately a +0.5 gain in BLEU. 2.3.3 CUNI-KOCMI (Kocmi et al., 2018) 2.3.6 FACEBOOK -FAIR ? (Edunov et al., 2018) FACEBOOK -FAIR is an ensemble of six selfattentional models with back-translation data according to Edunov et al. (2018). Synthetic sources are sampled instead of beam search, oversampling the real bitext at a rate of 16, i.e., each bitext is sampled 16 times more often per epoch than the back-translated data. At inference time, translations which are copies of the source are filtered out, replacing them with the output of a very small news-commentary only"
W18-6401,E17-2058,0,0.0576994,"Missing"
W18-6401,W18-6406,0,0.107817,"t (Junczys-Dowmunt, 2018) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2018) ModernMT, MMT s.r.l. (no associated paper) University of Tartu (Tars and Fishel, 2018) National Institute of Information and Communications Technology (Marie et al., 2018) Northeastern University / NiuTrans Co., Ltd. (Wang et al., 2018b) NLP Group, Nanjing University (no associated paper) NTT Corporation (Morishita et al., 2018) Bo˘gaziçi University (Biçici, 2018) PROMT LLC (Molchanov, 2018) RWTH Aachen (Schamper et al., 2018) RWTH Aachen (Graça et al., 2018) TALP, Technical University of Catalonia (Casas et al., 2018) Tencent (Wang et al., 2018a) Tilde (Pinnis et al., 2018) Ubiqus (no associated paper) University of Cambridge (Stahlberg et al., 2018) University of Edinburgh (Haddow et al., 2018) University of Maryland (Xu and Carpuat, 2018) Unisound (no associated paper) University of Tartu (Del et al., 2018) Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous y"
W18-6401,W13-2305,1,0.876146,"017 (English) and news 2011 (Chinese). Subwords (BPE) are used for both English and Chinese sentences. 3 Human Evaluation A human evaluation campaign is run each year to assess translation quality and to determine the final ranking of systems taking part in the competition. This section describes how preparation of evaluation data, collection of human assessments, and computation of the official results of the shared task was carried out this year. Work on evaluation over the past few years has provided fresh insight into ways to collect direct assessments (DA) of machine translation quality (Graham et al., 2013, 2014, 2016), and two years ago the evaluation campaign included parallel assessment of a subset of News task language pairs evaluated with relative ranking (RR) and DA. DA has some clear advantages over RR, namely the evaluation of absolute translation quality and the ability to carry out evaluations through quality controlled crowd-sourcing. As established in 2016 (Bojar et al., 2016a), DA results (via crowd-sourcing) and RR results (produced by researchers) correlate strongly, with Pearson correlation ranging from 0.920 to 0.997 across several source languages into English and at 0.975 for"
W18-6401,E14-1047,1,0.89975,"Missing"
W18-6401,W18-6407,1,0.888041,"ns Co., Ltd. (Wang et al., 2018b) NLP Group, Nanjing University (no associated paper) NTT Corporation (Morishita et al., 2018) Bo˘gaziçi University (Biçici, 2018) PROMT LLC (Molchanov, 2018) RWTH Aachen (Schamper et al., 2018) RWTH Aachen (Graça et al., 2018) TALP, Technical University of Catalonia (Casas et al., 2018) Tencent (Wang et al., 2018a) Tilde (Pinnis et al., 2018) Ubiqus (no associated paper) University of Cambridge (Stahlberg et al., 2018) University of Edinburgh (Haddow et al., 2018) University of Maryland (Xu and Carpuat, 2018) Unisound (no associated paper) University of Tartu (Del et al., 2018) Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. “?” indicates invited participation with a late submission, where the team is not considered a regular participant. 276 and Estonian. The system is based on the Transformer (Vaswani et al., 2017) implementation in OpenNMT-py (Klein et al., 2017). It is trained on filtered pa"
W18-6401,W18-6410,0,0.0609282,"ions, organized into 35 teams are listed in Table 2 and detailed in the rest of this section. Each system did not necessarily appear in all translation tasks. We also included 39 online MT systems (originating from 5 services), which we anonymized as ONLINE -A,B,F,G. For presentation of the results, systems are treated as either constrained or unconstrained, depending on whether their models were trained only on the provided data. Since we do not know how they were built, these online and commercial systems are treated as unconstrained during the automatic and human evaluations. 2.3.1 A ALTO (Grönroos et al., 2018) Aalto participated in the constrained condition of the multi-lingual subtrack, with a single system trained to translate from English to both Finnish 3 http://www.yandex.com/ Estonian Research Council institutional research grant IUT20-56: “Computational models of the Estonian Language” 4 5 As of Fall 2011, the proceedings of the European Parliament are no longer translated into all official languages. 273 Europarl Parallel Corpus German ↔ English Czech ↔ English Finnish ↔ English Estonian ↔ English Sentences 1,920,209 646,605 1,926,114 652,944 Words 50,486,398 53,008,851 14,946,399 17,376,43"
W18-6401,D18-1045,0,0.0609466,"Missing"
W18-6401,W11-2123,0,0.0087119,"t sets. The second is a Marian (Junczys-Dowmunt et al., 2018) system ensembling 5 Univ. Edinburgh “bi-deep” and 6 transformer models all trained on the WMT18 bitexts provided, including ParaCrawl. Some models employed pretrained word embeddings built on BPE’d corpora (Sennrich et al., 2016). A Marian transformer model performed right-to-left rescoring for this system. The third system is trained with Moses (Koehn et al., 2007), using the same data as the Marian system. Hierarchical reordering and Operation Sequence Model were employed. The 5-gram English language model was trained with KenLM (Heafield, 2011) on the same corpus as the AFRL WMT15 system with the same BPE used in the Marian systems. Lastly, RWTH Jane’s system combination (Freitag et al., 2014) was applied yielding approximately a +0.5 gain in BLEU. 2.3.3 CUNI-KOCMI (Kocmi et al., 2018) 2.3.6 FACEBOOK -FAIR ? (Edunov et al., 2018) FACEBOOK -FAIR is an ensemble of six selfattentional models with back-translation data according to Edunov et al. (2018). Synthetic sources are sampled instead of beam search, oversampling the real bitext at a rate of 16, i.e., each bitext is sampled 16 times more often per epoch than the back-translated da"
W18-6401,E17-3017,1,0.773822,"eriment, right-toleft reranking does not help. Another focus is 277 (SMT) submission for the Finnish morphology test suite (Burlot et al., 2018). given to data filtering through rules, translation model and language model including parallel data and monolingual data. The language model is based the Transformer architecture as well. The final system is trained with four different seeds and mixed data. 2.3.8 2.3.9 JHU (Koehn et al., 2018a) The JHU systems are the result of two relatively independent efforts on German–English language directions and Russian–English, using the Marian and Sockeye (Hieber et al., 2017) neural machine translation toolkits, respectively. The novel contributions are iterative back-translation (for German) and fine-tuning on test sets from prior years (for both languages). HY (Raganato et al., 2018; Hurskainen and Tiedemann, 2017) The University of Helsinki (HY) submitted four systems: HY-AH, HY-NMT, HY-NMT-2 STEP and HY-SMT. 2.3.10 JUCBNMT (Mahata et al., 2018) JUCBNMT is an encoder-decoder sequence-tosequence NMT model with character level encoding. The submission uses preprocessing like tokenization, truecasing and corpus cleaning. Both encoder and decoder use a single LSTM"
W18-6401,P17-4012,0,0.0266435,"ted paper) University of Tartu (Del et al., 2018) Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. “?” indicates invited participation with a late submission, where the team is not considered a regular participant. 276 and Estonian. The system is based on the Transformer (Vaswani et al., 2017) implementation in OpenNMT-py (Klein et al., 2017). It is trained on filtered parallel and filtered back-translated monolingual data. The main contribution is a novel cross-lingual Morfessor (Virpioja et al., 2013) segmentation using cognates extracted from the parallel data. The aim is to improve the consistency of the morphological segmentation. Aalto decode using an ensemble of 3 (et) or 8 (fi) models. 2.3.4 2.3.2 2.3.5 AFRL The CUNI-KOCMI submission focuses on the low-resource language neural machine translation (NMT). The final submission uses a method of transfer learning: the model is pretrained on a related high-resource language (her"
W18-6401,W18-6413,0,0.01703,"he constrained systems, however, the data, taking into account its relatively large size, was not factored. T ENCENT (Wang et al., 2018a) T ENCENT- ENSEMBLE (called TenTrans) is an improved NMT system on Transformer based on self-attention mechanism. In addition to the basic settings of Transformer training, T ENCENTENSEMBLE uses multi-model fusion techniques, multiple features reranking, different segmentation models and joint learning. Additionally, data selection strategies were adopted to fine-tune the trained system, achieving a stable performance improvement. An additional system paper (Hu et al., 2018) describes a non-primary submission. 2.3.29 TILDE 2.3.30 U BIQUS The U BIQUS -NMT system is probably developed by the Ubiqus company (www.ubiqus.com). No further information is available. 2.3.31 UCAM (Stahlberg et al., 2018) UCAM is a generalization of previous work (de Gispert et al., 2017) to multiple architectures. It is a system combination of two Transformer-like models, a recurrent model, a convolutional model, and a phrase-based SMT system. The output is probably dominated by the Transformer, and to some extend by the SMT system. (Pinnis et al., 2018) submitted four systems: TILDE - C -"
W18-6401,W18-6416,1,0.791378,"Missing"
W18-6401,W17-4730,0,0.0118952,". HY-SMT (Tiedemann et al., 2016) is the Helsinki SMT system submitted at WMT 2016 (the constrained-basic+back-translated version). The system was not retrained and it may thus suffer from poor lexical coverage on recent test data. The main motivation for including this baseline was to have a statistical machine translation 2.3.13 LMU- NMT (Huck et al., 2018) For the WMT18 news translation shared task, LMU Munich (Huck et al., 2018) has trained ba278 2.3.15 sic shallow attentional encoder-decoder systems (Bahdanau et al., 2014) with the Nematus toolkit (Sennrich et al., 2017), like last year (Huck et al., 2017a). LMU has participated with these NMT systems for the English–German language pair in both translation directions. The training data is a concatenation of Europarl, News Commentary, Common Crawl, and some synthetic data in the form of backtranslated monolingual news texts. The 2017 monolingual News Crawl is not employed, nor are the parallel Rapid and ParaCrawl corpora. The German data is preprocessed with a linguistically informed word segmentation technique (Huck et al., 2017b). By using a linguistically more sound word segmentation, advantages over plain BPE segmentation are expected in t"
W18-6401,W18-6417,1,0.821949,"as workshops and conferences (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a, 2017). This year we conducted several official tasks. We report in this paper on the news translation task. Additional shared tasks are described in separate papers in these proceedings: • biomedical translation (Neves et al., 2018), • multimodal machine translation (Barrault et al., 2018), • metrics (Ma et al., 2018), • quality estimation (Specia et al., 2018), • automatic post-editing (Chatterjee et al., 2018), and • parallel corpus filtering (Koehn et al., 2018b). In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We held 1 2 http://www.statmt.org/wmt18/ 272 http://statmt.org/wmt18/results.html Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 272–303 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64028 tions are also available for interactive visualization and comparison of diff"
W18-6401,W17-4706,0,0.0132191,". HY-SMT (Tiedemann et al., 2016) is the Helsinki SMT system submitted at WMT 2016 (the constrained-basic+back-translated version). The system was not retrained and it may thus suffer from poor lexical coverage on recent test data. The main motivation for including this baseline was to have a statistical machine translation 2.3.13 LMU- NMT (Huck et al., 2018) For the WMT18 news translation shared task, LMU Munich (Huck et al., 2018) has trained ba278 2.3.15 sic shallow attentional encoder-decoder systems (Bahdanau et al., 2014) with the Nematus toolkit (Sennrich et al., 2017), like last year (Huck et al., 2017a). LMU has participated with these NMT systems for the English–German language pair in both translation directions. The training data is a concatenation of Europarl, News Commentary, Common Crawl, and some synthetic data in the form of backtranslated monolingual news texts. The 2017 monolingual News Crawl is not employed, nor are the parallel Rapid and ParaCrawl corpora. The German data is preprocessed with a linguistically informed word segmentation technique (Huck et al., 2017b). By using a linguistically more sound word segmentation, advantages over plain BPE segmentation are expected in t"
W18-6401,W18-6430,0,0.299738,"of Helsinki (Raganato et al., 2018) Johns Hopkins University (Koehn et al., 2018a) Jadavpur University (Mahata et al., 2018) Karlsruhe Institute of Technology (Pham et al., 2018) Li Muze (no associated paper) LMU Munich (Huck et al., 2018) LMU Munich (Stojanovski et al., 2018) Microsoft (Junczys-Dowmunt, 2018) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2018) ModernMT, MMT s.r.l. (no associated paper) University of Tartu (Tars and Fishel, 2018) National Institute of Information and Communications Technology (Marie et al., 2018) Northeastern University / NiuTrans Co., Ltd. (Wang et al., 2018b) NLP Group, Nanjing University (no associated paper) NTT Corporation (Morishita et al., 2018) Bo˘gaziçi University (Biçici, 2018) PROMT LLC (Molchanov, 2018) RWTH Aachen (Schamper et al., 2018) RWTH Aachen (Graça et al., 2018) TALP, Technical University of Catalonia (Casas et al., 2018) Tencent (Wang et al., 2018a) Tilde (Pinnis et al., 2018) Ubiqus (no associated paper) University of Cambridge (Stahlberg et al., 2018) University of Edinburgh (Haddow et al., 2018) University of Maryland (Xu and Carpuat, 2018) Unisound (no associated paper) University of Tartu (Del et al., 2018) Table 2: Part"
W18-6401,W18-6427,0,0.0533796,"Missing"
W18-6401,W18-6428,0,0.0966755,"D U NISOUND U NSUP TARTU Institution Aalto University (Grönroos et al., 2018) Air Force Research Laboratory (Gwinnup et al., 2018) Alibaba Group (Deng et al., 2018) Charles University (Kocmi et al., 2018) Charles University (Popel, 2018) Facebook AI Research (Edunov et al., 2018) Global Tone Communication Technology (Bei et al., 2018) University of Helsinki (Raganato et al., 2018) Johns Hopkins University (Koehn et al., 2018a) Jadavpur University (Mahata et al., 2018) Karlsruhe Institute of Technology (Pham et al., 2018) Li Muze (no associated paper) LMU Munich (Huck et al., 2018) LMU Munich (Stojanovski et al., 2018) Microsoft (Junczys-Dowmunt, 2018) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2018) ModernMT, MMT s.r.l. (no associated paper) University of Tartu (Tars and Fishel, 2018) National Institute of Information and Communications Technology (Marie et al., 2018) Northeastern University / NiuTrans Co., Ltd. (Wang et al., 2018b) NLP Group, Nanjing University (no associated paper) NTT Corporation (Morishita et al., 2018) Bo˘gaziçi University (Biçici, 2018) PROMT LLC (Molchanov, 2018) RWTH Aachen (Schamper et al., 2018) RWTH Aachen (Graça et al., 2018) TALP, Technical University of Ca"
W18-6401,W18-6431,0,0.0938702,"nications Technology (Marie et al., 2018) Northeastern University / NiuTrans Co., Ltd. (Wang et al., 2018b) NLP Group, Nanjing University (no associated paper) NTT Corporation (Morishita et al., 2018) Bo˘gaziçi University (Biçici, 2018) PROMT LLC (Molchanov, 2018) RWTH Aachen (Schamper et al., 2018) RWTH Aachen (Graça et al., 2018) TALP, Technical University of Catalonia (Casas et al., 2018) Tencent (Wang et al., 2018a) Tilde (Pinnis et al., 2018) Ubiqus (no associated paper) University of Cambridge (Stahlberg et al., 2018) University of Edinburgh (Haddow et al., 2018) University of Maryland (Xu and Carpuat, 2018) Unisound (no associated paper) University of Tartu (Del et al., 2018) Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. “?” indicates invited participation with a late submission, where the team is not considered a regular participant. 276 and Estonian. The system is based on the Transformer (Vaswani et al., 2017) implement"
W18-6401,W16-2326,0,0.0278975,"Missing"
W18-6412,W18-6401,1,0.759353,"airs (both directions). For these experiments, we use the same training sets and data preparation as in our system submissions, but train the deep RNNs with a working memory of 10GB, validating every 1,000 steps, and testing for convergence with a patience of 10. We use exponential smoothing and show the results on a single smoothed model. From the results in Table 7 we see that the multihead/hop extension has a small positive effect on B LEU in most language pairs. 27.6 28.2 27.7 28.1 26.9 Table 5: Results for EN↔TR systems on official WMT test sets. human evaluation from the findings paper (Bojar et al., 2018). In terms of the clustering provided by the organisers, we were in the top constrained cluster (i.e. no significant difference was observed between ours and the best constrained system) for EN→CS, DE→EN, ET→EN, FI→EN, TR→EN and EN→TR, i.e. 6/14 language pairs. Nevertheless, Table 6 still shows that our systems generally lag behind the best submitted systems. This is contrast to the 2017 shared task, where we achieved the highest scores in most of the language pairs where we submitted systems. We hypothesise that other groups have taken fuller advantage of the transformer architecture, and als"
W18-6412,P18-1008,0,0.0363778,"with the raw data. 2.3 Model Architecture For this submission we considered two types of sequence-to-sequence architectures: a transformer (Vaswani et al., 2017) and a deep RNN, specifically the BiDeep GRU encoder-decoder (Miceli Bar400 one et al., 2017). Both architectures4 are implemented in the Marian open source neural machine translation framework (Junczys-Dowmunt et al., 2018). For the transformer architecture we used the “wmt2017-transformer” setup from the Marian example collection5 . We extended the RNN with with multi-head and multi-hop attention. Multi-head attention is similar to Chen et al. (2018), with an MLP attention mechanism using a single tanh hidden layer followed by one soft-max layer for each attention heads. We further include an optional projection layer on the attended context with layer normalisation in order to avoid increasing the total size of the attended context. Let C ∈ RNs ×de be the input sentence representation produced by the encoder, where Ns is the source sentence length and de is the top-level bidirectional encoder state dimension. Let s ∈ Rdd be an internal decoder state at some step. Then for source sentence position i we compute a vector of M attention weig"
W18-6412,W17-4715,1,0.879995,"experimented with the Transformer architecture (Vaswani et al., 2017), as implemented by Marian (Junczys-Dowmunt et al., 2018), as well as introducing a new variant on the deep RNN architectire (Section 2.3). Data selection and weighting For some language pairs, we experimented with different data selection schemes, motivated by the introduction of the noisy ParaCrawl corpora to the task (Section 2.1). We also applied weighting of different corpora to most language pairs, particularly DE↔EN (Section 3.5). Extensions to Back-translation For TR↔EN (Section 3.7) we used copied monolingual data (Currey et al., 2017a) and iterative back-translation. System Details 2.1 Data and Selection All our systems were constrained in the sense that they only used the supplied parallel data (including ParaCrawl) for training the systems. We also used the monolingual news crawls to create extra synthetic parallel data by back-translation, for all language pairs, and by copying monolingual data for TR↔EN. During training we generally used newsdev2016 or newstest2016 for validation, and newstest2017 for development testing (i.e. model selection), except for ZH↔EN, and ET↔EN, where we used the recent newsdev sets instead"
W18-6412,N13-1073,0,0.0722204,"Missing"
W18-6412,W16-2323,1,0.938501,"ntroduce new RNN-variant, mixed RNN/Transformer ensembles, data selection and weighting, and extensions to backtranslation. 1 2 In this section we describe the general properties of our systems, as well as some novel approaches that we tried this year such as data selection and a variant on the GRU-based RNN architecture. The specifics of our submissions for each language pair are described in Section 3. Introduction For the WMT18 news translation task, we were the only team to make submissions to all 14 language pairs. Our submissions built on our strong results of the WMT16 and WMT17 tasks (Sennrich et al., 2016a, 2017), in that we used neural machine translation (NMT) with byte-pair encoding (BPE) (Sennrich et al., 2016c), back-translation (Sennrich et al., 2016b) and deep RNNs (Miceli Barone et al., 2017). For this year’s submissions we experimented with new architectures, and new ways of data handling. In brief, the innovations that we introduced this year are: Architecture This year we experimented with the Transformer architecture (Vaswani et al., 2017), as implemented by Marian (Junczys-Dowmunt et al., 2018), as well as introducing a new variant on the deep RNN architectire (Section 2.3). Data"
W18-6412,P07-2045,0,0.00708596,"e of the ParaCrawl corpus from about 36 million sentence pairs to ca. 18 million sentence pairs. 10 5 0 0.0 0.2 0.4 0.6 Proportion of ParaCrawl 0.8 1.0 Figure 1: Result of translation perplexity filtering of ParaCrawl on 2 language pairs we monitored the performance on a validation set (newstest2016) and observed the point where translation quality started to deteriorate. We used the translation plausibility score at this point as the threshold for selecting data for training the final systems. 2.2 Preprocessing For most language pairs, our preprocessing setup consisted of the Moses pipeline (Koehn et al., 2007) of normalisation, tokenisation and truecasing, followed by byte-pair encoding (BPE) (Sennrich et al., 2016c). We generally applied joint BPE, with the number of merge operations set on a per-pair basis, detailed in Section 3. Different pipelines were used for processing the two languages written in non-Latin scripts (i.e. Chinese and Russian), also explained in Section 3. For some language pairs (those including Czech, Estonian, Finnish and German) we used the preprocessed data provided by the organisers (which is preprocessed up to truecasing), whilst for the others we started with the raw d"
W18-6412,P12-3005,0,0.0282872,"id identified a significant proportion of the data as these other two Slavic languages, but on inspecting a sample, they were found nearly always to be Czech. The issue with langid is that we just give it the text, without providing any prior knowledge, when in actual fact there is a strong prior that CzEng sentences are really Czech and English, by construction 25 fi-en et-en 20 15 Bleu Language Identifier Filtering This was applied to the CS↔EN and DE↔EN corpora, based on observations that CzEng, and ParaCrawl both contain sentence pairs in the “wrong” language. For CS↔EN we applied langid (Lui and Baldwin, 2012) to both sids of the data, removing any sentences whose English side is not labelled as English, or whose Czech is not labelled as Czech, Slovak or Slovenian3 . For DE↔EN, we just applied langid to ParaCrawl and retained only those pairs where each side was identified as the ‘correct’ language by langid. This reduced the size of the ParaCrawl corpus from about 36 million sentence pairs to ca. 18 million sentence pairs. 10 5 0 0.0 0.2 0.4 0.6 Proportion of ParaCrawl 0.8 1.0 Figure 1: Result of translation perplexity filtering of ParaCrawl on 2 language pairs we monitored the performance on a va"
W18-6412,W17-4710,1,0.858373,"ll as some novel approaches that we tried this year such as data selection and a variant on the GRU-based RNN architecture. The specifics of our submissions for each language pair are described in Section 3. Introduction For the WMT18 news translation task, we were the only team to make submissions to all 14 language pairs. Our submissions built on our strong results of the WMT16 and WMT17 tasks (Sennrich et al., 2016a, 2017), in that we used neural machine translation (NMT) with byte-pair encoding (BPE) (Sennrich et al., 2016c), back-translation (Sennrich et al., 2016b) and deep RNNs (Miceli Barone et al., 2017). For this year’s submissions we experimented with new architectures, and new ways of data handling. In brief, the innovations that we introduced this year are: Architecture This year we experimented with the Transformer architecture (Vaswani et al., 2017), as implemented by Marian (Junczys-Dowmunt et al., 2018), as well as introducing a new variant on the deep RNN architectire (Section 2.3). Data selection and weighting For some language pairs, we experimented with different data selection schemes, motivated by the introduction of the noisy ParaCrawl corpora to the task (Section 2.1). We also"
W18-6412,P10-2041,0,0.0719435,"Missing"
W18-6412,W17-4739,1,0.864199,"3 times the tokens on the other side, following Hassan et al. (2018). After preprocessing the corpus size was 23.6M sentences. We then applied BPE using 18,000 merge operations and we used the top 18,000 BPE segments as vocabulary. We augmented our data with backtranslated 4 The BiDeep GRU is obtainable using the -best-deep option. 5 https://github.com/marian-nmt/ marian-examples 401 6 The implementation of the multi-head and multi-hop attention architectures is available at: https://github. com/EdinburghNLP/marian-dev 7 https://marian-nmt.github.io 8 https://github.com/fxsjy/jieba ZH↔EN from Sennrich et al. (2017), which consists of 8.6M sentences for EN→ZH and 19.7M for ZH→EN. We trained using the BiDeep architecture with multi-head attention with 1 hop and 3 heads. We decoded using an ensemble of 5 L2R systems and a beam of 12 for EN→ZH and 6 L2R systems and a beam of 12 for ZH→EN. Due to time constraints, we were not able to train any of the systems to convergence. 3.2 Czech ↔ English After preprocessing, language filtering (see Sections 2.1 and 2.2), and removing any parallel sentences where neither side contains an ASCII letter, we were left with around 50M sentence pairs. We then learned a joint"
W18-6412,P16-1162,1,0.881886,"ntroduce new RNN-variant, mixed RNN/Transformer ensembles, data selection and weighting, and extensions to backtranslation. 1 2 In this section we describe the general properties of our systems, as well as some novel approaches that we tried this year such as data selection and a variant on the GRU-based RNN architecture. The specifics of our submissions for each language pair are described in Section 3. Introduction For the WMT18 news translation task, we were the only team to make submissions to all 14 language pairs. Our submissions built on our strong results of the WMT16 and WMT17 tasks (Sennrich et al., 2016a, 2017), in that we used neural machine translation (NMT) with byte-pair encoding (BPE) (Sennrich et al., 2016c), back-translation (Sennrich et al., 2016b) and deep RNNs (Miceli Barone et al., 2017). For this year’s submissions we experimented with new architectures, and new ways of data handling. In brief, the innovations that we introduced this year are: Architecture This year we experimented with the Transformer architecture (Vaswani et al., 2017), as implemented by Marian (Junczys-Dowmunt et al., 2018), as well as introducing a new variant on the deep RNN architectire (Section 2.3). Data"
W19-5301,W19-5424,1,0.858444,"Missing"
W19-5301,W19-5306,0,0.248769,"al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of Maryland (Briakou and Carpuat, 2019) (no associated paper) University of Sydney (Ding and Tao, 2019) (no associated"
W19-5301,D18-1549,0,0.116951,"s are available for this system. 2.5.7 BASELINE - RE - RERANK (no associated CUNI-T RANSFORMER -T2T2018 (Popel, 2018) is the exact same system as used last year. paper) BASELINE - RE - RERANK is a standard Transformer, with corpus filtering, pre-processing, postprocessing, averaging and ensembling as well as n-best list reranking. 2.5.8 CUNI-T RANSFORMER -M ARIAN (Popel et al., 2019) is a “reimplementation” of the last year’s system (Popel, 2018) in Marian (JunczysDowmunt et al., 2018). CA I RE (Liu et al., 2019) CUNI-U NSUPERVISED -NER- POST (Kvapilíková et al., 2019) follows the strategy of Artetxe et al. (2018), creating a seed phrase-based system where the phrase table is initialized from cross-lingual embedding mappings trained on monolingual data, followed by a neural machine translation system trained on synthetic parallel corpus. The synthetic corpus is produced by the seed phrase-based MT system or by a such a model refined through iterative back-translation. CUNI-U NSUPERVISED -NER- POST further focuses on the handling of named entities, i.e. the part of vocabulary where the cross-lingual embedding mapping suffer most. CA I RE is a hybrid system that took part only in the unsupervised track."
W19-5301,D18-1332,0,0.0215805,"et al., 2018). For English↔Gujarati, synthetic parallel data from two sources, backtranslation and pivoting through Hindi, is produced using unsupervised and semi-supervised NMT models, pre-trained using a cross-lingual language objective (Lample and Conneau, 2019) For German→English, the impact of vast amounts of back-translated training data on translation quality is studied, and some additional insights are gained over (Edunov et al., 2018). Towards the end of training, for German→English and Chinese↔English, the mini-batch size was increased up to fifty-fold by delaying gradient updates (Bogoychev et al., 2018) as an alternative to learning rate cooldown (Smith, 2018). For Chinese↔English, a comparison of different segmentation strategies showed that character-based decoding was superior to the translation of subwords when translating into Chinese. Pre-processing strategies were also investigated for English→Czech, showing that preprocessing can be simplified without loss to MT quality. UEDIN’s main findings on the Chinese↔English translation task are that character-level model on the Chinese side can be used when translating into Chinese to improve the BLEU score. The same does not hold when transl"
W19-5301,W19-5351,0,0.0505622,"Missing"
W19-5301,W19-5423,0,0.0419015,"Missing"
W19-5301,W18-6412,1,0.856623,"Missing"
W19-5301,W19-5305,0,0.0496912,"Missing"
W19-5301,W12-3102,1,0.474924,"Missing"
W19-5301,W19-5310,0,0.0432396,"Missing"
W19-5301,W19-5425,0,0.0236032,"ys-Dowmunt et al., 2018) and Phrase-based machine translation system (implemented with Moses) and for the Spanish-Portuguese task. The system combination included features formerly presented in (Marie and Fujita, 2018), including scores left-to-right and right-to-left, sentence level translation probabilities and language model scores. Also authors provide contrastive results with an unsupervised phrase-based MT system which achieves quite close results to their primary system. Authors associate high performance of the unsupervised system to the language similarity. Incomslav: Team INCOMSLAV (Chen and Avgustinova, 2019) by Saarlad University participated in the Czech to Polish translation task only. The team’s primary submission builds on a transformer-based NMT baseline with back translation which has been submitted one of their contrastive submission. Incomslav’s primary system is a phoneme-based system re-scored using their NMT baseline. A second contrastive submission builds our phrase-based SMT system combined with a joint BPE model. NITS-CNLP: The NITS-CNLP team (Laskar et al., 2019) by the National Institute of Technology Silchar in India submitted results to the HI-NE translation task in both directi"
W19-5301,W07-0718,1,0.530103,"ojar Charles University Yvette Graham Barry Haddow Dublin City University University of Edinburgh Philipp Koehn JHU / University of Edinburgh Mathias Müller University of Zurich Marta R. Costa-jussà Christian Federmann UPC Microsoft Cloud + AI Shervin Malmasi Harvard Medical School Santanu Pal Saarland University Matt Post JHU Abstract Introduction The Fourth Conference on Machine Translation (WMT) held at ACL 20191 hosts a number of shared tasks on various aspects of machine translation. This conference builds on 13 previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017, 2018). This year we conducted several official tasks. We report in this paper on the news and similar translation tasks. Additional shared tasks are described in separate papers in these proceedings: • biomedical translation (Bawden et al., 2019b) • automatic post-editing (Chatterjee et al., 2019) • metrics (Ma et al., 2019) • quality estimation (Fonseca et al., 2019) • parallel corpus filtering (Koehn et al., 2019) • robustness (Li et al., 2019b) In the news translation task (Section 2), participants were asked to tra"
W19-5301,P16-2058,1,0.819865,"ipated with the Transformer (Vaswani et al., 2017) implemented in the OpenNMT toolkit. They focused on word segmentation methods and compared a cognate-aware segmentation method, Cognate Morfessor (Grönroos et al., 2018), with character segmentation and unsupervised segmentation methods. As primary submission they submitted this Cognate Morfessor that optimizes subword segmentations consistently for cognates. They participated for all translation directions in Spanish-Portuguese and Czech-Polish, and this Cognate Morfessor performed better for Czech-Polish, while characterbased segmentations (Costa-jussà and Fonollosa, 2016), while much more inefficient, were superior for Spanish-Portuguese. UPC-TALP: The UPC-TALP team (Biesialska et al., 2019) by the Universitat Politècnica de Catalunya submitted a Transformer (implemented with Fairseq (Ott et al., 2019)) for the Czechto-Polish task and a Phrase-based system (implemented with Moses (Koehn et al., 2007)) for Spanish-to-Portuguese. They tested adding monolingual data to the NMT system by copying the same data on the source and target sides, with negative results. Also, their system combination based on sentence-level BLEU in back-translation 5.4 Conclusion of Simi"
W19-5301,W08-0309,1,0.659809,"Missing"
W19-5301,W18-3931,1,0.820211,"or they use English as a pivot language to translate between resource-poorer languages. The interest in English is reflected, for example, in the WMT translation tasks (e.g. News, Biomedical) which have always included language pairs in which texts are translated to and/or from English. With the widespread use of MT technology, there is more and more interest in training systems to translate between languages other than English. One evidence of this is the need of directly translating between pairs of similar languages, varieties, and dialects (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018). The main challenge is to take advantage of the similarity between languages to overcome the limitation given the low amount of available parallel data to produce an accurate output. Given the interest of the community in this topic we organize, for the first time at WMT, a shared task on ""Similar Language Translation"" to evaluate the performance of state-of-the-art translation systems on translating between pairs of languages from the same language family. We provide participants with training and testing data from three language pairs: Spanish - Portuguese (Romance languages), Czech - Polis"
W19-5301,W19-5312,0,0.0773587,"Missing"
W19-5301,W19-5313,0,0.0931699,"University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of"
W19-5301,W19-5314,0,0.0200465,"Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of Maryland (Briakou and Carpuat, 2019) (no associated paper) University of Sydney (Ding and Tao, 2019) (no associated paper) 8 participated in all language pairs. The translations from the Table 5: Participants in the shared translation task. Not all teams online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. 2.5.6 BTRANS only the middle sentence was considered for the final translation hypothesis, otherwise shorter context of two sentences or just a single sentence was used. Unfortunately, no details are available for this system. 2.5.7 BASELINE - RE - RERANK (no associ"
W19-5301,D18-1045,0,0.0285693,"xt was morphologically segmented with Apertium. The UEDIN systems are supervised NMT systems based on the transformer architecture and trained using Marian (Junczys-Dowmunt et al., 2018). For English↔Gujarati, synthetic parallel data from two sources, backtranslation and pivoting through Hindi, is produced using unsupervised and semi-supervised NMT models, pre-trained using a cross-lingual language objective (Lample and Conneau, 2019) For German→English, the impact of vast amounts of back-translated training data on translation quality is studied, and some additional insights are gained over (Edunov et al., 2018). Towards the end of training, for German→English and Chinese↔English, the mini-batch size was increased up to fifty-fold by delaying gradient updates (Bogoychev et al., 2018) as an alternative to learning rate cooldown (Smith, 2018). For Chinese↔English, a comparison of different segmentation strategies showed that character-based decoding was superior to the translation of subwords when translating into Chinese. Pre-processing strategies were also investigated for English→Czech, showing that preprocessing can be simplified without loss to MT quality. UEDIN’s main findings on the Chinese↔Engl"
W19-5301,W18-6410,0,0.0193718,"ormance can be found in Hindi-Nepali (both directions), where the best performing system is around 50 BLEU (53 for Hindi-to-Nepali and 49.1 for Nepali-toHindi), and the lowest entry is 1.4 for Hindi-toNepali and 0 for Nepali-to-Hindi. The lowest variance is for Polish-to-Czech and it may be because only two teams participated. UHelsinki: The University of Helsinki team (Scherrer et al., 2019) participated with the Transformer (Vaswani et al., 2017) implemented in the OpenNMT toolkit. They focused on word segmentation methods and compared a cognate-aware segmentation method, Cognate Morfessor (Grönroos et al., 2018), with character segmentation and unsupervised segmentation methods. As primary submission they submitted this Cognate Morfessor that optimizes subword segmentations consistently for cognates. They participated for all translation directions in Spanish-Portuguese and Czech-Polish, and this Cognate Morfessor performed better for Czech-Polish, while characterbased segmentations (Costa-jussà and Fonollosa, 2016), while much more inefficient, were superior for Spanish-Portuguese. UPC-TALP: The UPC-TALP team (Biesialska et al., 2019) by the Universitat Politècnica de Catalunya submitted a Transform"
W19-5301,W19-5317,0,0.114089,"ed paper) (Liu et al., 2019) Charles University (Popel et al., 2019; Kocmi and Bojar, 2019) and (Kvapilíková et al., 2019) Kumamoto University, Telkom University, Indonesian Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 201"
W19-5301,W19-5315,0,0.0266353,"Missing"
W19-5301,W19-5318,0,0.198338,"ance of the systems when translating from French to German seems to heavily depend on the 7 http://data.statmt.org/wmt19/ translation-task/dev.tgz 6 Systems MSRA.MADL eTranslation LIUM MLLP-UPV onlineA TartuNLP onlineB onlineY onlineG onlineX FULL 47.3 45.4 43.7 41.5 40.8 39.2 39.1 39.0 38.5 38.1 source FR 38.3 37.4 37.5 36.4 35.4 34.8 35.3 34.7 34.6 35.6 source DE 50.0 47.8 45.5 43.0 42.3 40.5 40.2 40.2 39.7 38.8 evaluations. In the rest of this sub-section, we provide brief details of the submitted systems, for those in cases where the authors provided such details. 2.5.1 AFRL - SYSCOMB 19 (Gwinnup et al., 2019) is a system combination of a Marian ensemble system, two distinct OpenNMT systems, a Sockeyebased Elastic Weight Consolidation system, and one Moses phrase-based system. Table 3: French→German Meteor scores. Systems MSRA.MADL LinguaCustodia MLLP_UPV Kyoto_University_T2T LIUM onlineY onlineB TartuNLP onlineA onlineX onlineG FULL 52.0 51.3 49.5 48.8 48.3 47.5 46.4 46.3 45.3 42.7 41.7 source FR 51.9 52.5 49.9 49.7 46.5 43.7 43.7 45.0 43.7 41.6 40.9 source DE 52.0 51.0 49.4 48.6 48.7 48.4 47.0 46.7 45.8 42.9 41.9 AFRL- EWC (Gwinnup et al., 2019) is a Sockeye Transformer system trained with the de"
W19-5301,W19-5316,0,0.109691,"boratory (Gwinnup et al., 2019) Apertium (Pirinen, 2019) Apprentice (Li and Specia, 2019) Aylien Ltd. (Hokamp et al., 2019) Baidu (Sun et al., 2019) (no associated paper) (no associated paper) (Liu et al., 2019) Charles University (Popel et al., 2019; Kocmi and Bojar, 2019) and (Kvapilíková et al., 2019) Kumamoto University, Telkom University, Indonesian Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of"
W19-5301,E14-1047,1,0.904335,"Missing"
W19-5301,W19-5427,0,0.0467733,"Missing"
W19-5301,W19-5322,1,0.807321,"Missing"
W19-5301,W19-5302,1,0.715203,"20191 hosts a number of shared tasks on various aspects of machine translation. This conference builds on 13 previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017, 2018). This year we conducted several official tasks. We report in this paper on the news and similar translation tasks. Additional shared tasks are described in separate papers in these proceedings: • biomedical translation (Bawden et al., 2019b) • automatic post-editing (Chatterjee et al., 2019) • metrics (Ma et al., 2019) • quality estimation (Fonseca et al., 2019) • parallel corpus filtering (Koehn et al., 2019) • robustness (Li et al., 2019b) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We 1 Christof Monz University of Amsterdam Marcos Zampieri University of Wolverhampton held 18 translation tasks this year, between English and each of Chinese, Czech (into Czech only), German, Finnish, Lithuanian, and Russian. New this year were Gujarati↔English and Kazakh↔English. B"
W19-5301,W19-5333,0,0.0923169,"Missing"
W19-5301,W19-5353,0,0.0658382,"Missing"
W19-5301,W19-5430,1,0.873847,"Missing"
W19-5301,W19-5431,0,0.0199622,"Universitat Politècnica de València (UPV) participated with a Transformer (implemented with FairSeq (Ott et al., 2019)) and a finetuning strategy for domain adaptaion in the task of Spanish-Portuguese. Fine-tunning on the development data provide improvements of almost 12 BLEU points, which may explain their clear best performance in the task for this language pair. As a contrastive system authors provided only for the Portuguese-to-Spanish a novel 2D alternating RNN model which did not respond so well when fine-tunning. UBC-NLP: Team UBC-NLP from the University of British Columbia in Canada (Przystupa and Abdul-Mageed, 2019) compared the performance of the LSTM plus attention (Bahdanau et al., 2015) and Transformer (Vaswani et al., 2017) (implemented in OpenNMT toolkit22 ) perform for the three tasks at hand. Authors use backtranslation to introduce monolingual data in their systems. LSTM plus attention outperformed Transformer for Hindi-Nepali, and viceversa for the other two tasks. As reported by the authors, Hindi-Nepali task provides much more shorter sentences than KYOTOUNIVERSITY: Kyoto University’s submission, listed simply as KYOTO in Table 25 for PT → ES task is based on transformer NMT system. They used"
W19-5301,P02-1040,0,0.11337,"ation of the source (CS), and a second encoder to encode sub-word (byte-pair-encoding) information of the source (CS). The results obtained by their system in translating from Czech→Polish and comment on the impact of out-of-domain test data in the performance of their system. UDSDFKI ranked second among ten teams in Czech– Polish translation. 5.3 Results We present results for the three language pairs, each of them in the two possible directions. For this first edition of the Similar Translation Task and differently from News task, evaluation was only performed on automatic basis using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) measures. Each language direction is reported in one different table which contain information of the team; type of system, either contrastive (C) or primary (P), and the BLEU and TER results. In general, primary systems tend to be better than contrastive systems, as expected, but there are some exceptions. Even if we are presenting 3 pairs of languages each pair belonging to the same family, translation quality in terms of BLEU varies signficantly. While the best systems for Spanish-Portuguese are above 64 BLEU and below 21 TER (see Tables 26 and 27), best syste"
W19-5301,W19-5354,0,0.0611791,"Missing"
W19-5301,W18-6486,0,0.0189853,"the agglutinative nature of Kazakh, (ii) data from an additional language (Russian), given the scarcity of English–Kazakh data and (iii) synthetic data for the source language filtered using language-independent sentence similarity. RUG _ KKEN _ MORFESSOR Tilde developed both constrained and unconstrained NMT systems for English-Lithuanian and Lithuanian-English using the Marian toolkit. All systems feature ensembles of four to five transformer models that were trained using the quasi-hyperbolic Adam optimiser (Ma and Yarats, 2018). Data for the systems were prepared using TildeMT filtering (Pinnis, 2018) and preprocessing (Pinnis et al., 2018) methods. For unconstrained systems, data were additionally filtered using dual conditional cross-entropy filtering (Junczys-Dowmunt, 2018a). All systems were trained using iterative back-translation (Rikters, 2018) and feature synthetic data that allows training NMT systems to support handling of unknown phenomena (Pinnis et al., 2017). During translation, automatic named entity and nontranslatable phrase post-editing were performed. For constrained systems, named entities and nontranslatable phrase lists were extracted from the parallel training data."
W19-5301,W19-5335,0,0.0408704,"Missing"
W19-5301,W19-5344,1,0.904781,"n Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas e"
W19-5301,W19-5346,0,0.197908,"rtium (Pirinen, 2019) Apprentice (Li and Specia, 2019) Aylien Ltd. (Hokamp et al., 2019) Baidu (Sun et al., 2019) (no associated paper) (no associated paper) (Liu et al., 2019) Charles University (Popel et al., 2019; Kocmi and Bojar, 2019) and (Kvapilíková et al., 2019) Kumamoto University, Telkom University, Indonesian Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communicatio"
W19-5301,W19-5341,0,0.0172601,"A,B,G,X,Y. For presentation of the results, systems are treated as either constrained or unconstrained, depending on whether their models were trained only on the provided data. Since we do not know how they were built, the online systems are treated as unconstrained during the automatic and human AYLIEN _ MULTILINGUAL (Hokamp et al., 2019) The Aylien research team built a Multilingual NMT system which is trained on all WMT2019 language pairs in all directions, then fine-tuned for a small number of iterations on Gujarati-English data only, including some self-backtranslated data. 2.5.5 BAIDU (Sun et al., 2019) Baidu systems are based on the Transformer architecture with several improvements. Data selection, back translation, data augmentation, knowledge distillation, domain adaptation, model ensemble and re-ranking are employed and proven effective in our experiments. 7 Team AFRL A PERTIUM - FIN - ENG A PPRENTICE - C AYLIEN _ MULTILINGUAL BAIDU BTRANS BASELINE - RE - RERANK CA I RE CUNI DBMS-KU DFKI - NMT E T RANSLATION FACEBOOK FAIR GTCOM H ELSINKI NLP IIITH-MT IITP JHU JUMT JU_S AARLAND KSAI K YOTO U NIVERSITY L INGUA C USTODIA LIUM LMU-NMT MLLP-UPV MS T RANSLATOR MSRA N IU T RANS NICT NRC PARFDA"
W19-5301,P16-1162,1,0.310296,"ssible, 2.5.13 E T RANSLATION (Oravecz et al., 2019) E T RANSLATION En-De E T RANSLATION ’s EnDe system is an ensemble of 3 base Transformers and a Transformer-type language model, trained from all available parallel data (cleaned up and filtered with dual conditional cross-entropy filtering) and with additional back-translated data generated 9 2.5.17 from monolingual news. Each Transformer model is fine tuned on previous years’ test sets. H ELSINKI NLP is a Transformer (Vaswani et al., 2017) style model implemented in OpenNMTpy using a variety of corpus filtering techniques, truecasing, BPE (Sennrich et al., 2016), backtranslation, ensembling and fine-tuning for domain adaptation. E T RANSLATION Fr-De The Fr-De system is an ensemble of 2 big Transformers (with size 8192 FFN layers). Back-translation data was selected using topic modelling techniques to tune the model towards the domain defined in the task. 2.5.18 En-Lt The En-Lt system is an ensemble of 2 big Transformers (as for Fr-De) and a Transformer type language model. The training data contains the Rapid corpus and the news domain back-translated data sets 2 times oversampled. E T RANSLATION 2.5.19 FACEBOOK FAIR (Ng et al., 2019) 2.5.20 JHU (Mar"
W19-5301,W19-5339,0,0.0767002,"Missing"
W19-5301,W19-5347,0,0.0328257,"Missing"
W19-5301,W19-5342,1,0.887858,"encia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of Maryland (Briakou and Carpuat, 2019) (no associated paper) University of Sydney (Ding and Tao, 2019) (no associated paper) 8 participated in all language pairs. The translations from the Table 5: Participants in the shared translation task. Not all teams online systems were not submitted by their respective companies but were obtained by us, and"
W19-5301,W19-5355,1,0.869964,"Missing"
W19-5301,W19-5350,0,0.0441915,"Missing"
W19-5301,P98-2238,0,0.38957,"n trained to translate texts from and to English or they use English as a pivot language to translate between resource-poorer languages. The interest in English is reflected, for example, in the WMT translation tasks (e.g. News, Biomedical) which have always included language pairs in which texts are translated to and/or from English. With the widespread use of MT technology, there is more and more interest in training systems to translate between languages other than English. One evidence of this is the need of directly translating between pairs of similar languages, varieties, and dialects (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018). The main challenge is to take advantage of the similarity between languages to overcome the limitation given the low amount of available parallel data to produce an accurate output. Given the interest of the community in this topic we organize, for the first time at WMT, a shared task on ""Similar Language Translation"" to evaluate the performance of state-of-the-art translation systems on translating between pairs of languages from the same language family. We provide participants with training and testing data from three language"
