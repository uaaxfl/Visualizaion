2014.amta-researchers.25,W14-1604,0,0.184955,"Missing"
2014.amta-researchers.25,W02-0505,0,0.0647627,"arcs. One might expect 0 BLEU for the baseline case, where we use no deromanization method at all. This is not so due to the nature of social media data. As indicated in Table 1, many non-Arabizi tokens, such as emoticons, URLs, Arabic words, and English code switches, occur throughout the data, often mixed into predominantly Arabizi segments. The Test 1 corpus contains a significantly larger percentage of such tokens than the Test 2 corpus. One might also expect higher overall BLEU scores at the bottom of Table 2, given the general track record of transliteration performance (Darwish, 2013; Al-Onaizan and Knight, 2002). We note that dialectical Arabic is in general not a written language, and as such there are many different spellings for words, even when rendered in Arabic script. Thus the task is closer to machine translation than classic transliteration (in that “correctness” is a squishy notion). Additionally, we did not specifically optimize our deromanizer for this intrinsic experiment, where we must decide whether or not to deromanize a possibly non-Arabizi word. Choosing incorrectly penalizes us here but should not impact extrinsic MT performance (evaluated in Section 4), due to our pipeline archite"
2014.amta-researchers.25,W12-4808,0,0.179886,"Missing"
2014.amta-researchers.25,P10-4002,0,0.0161489,"abic and English in Twitter dialects > h. may be pronounced as [g] (as in god), [Z] (as in vision), or [dZ] (as in juice), and that > the digraph “dj” is used in French for [dZ]. For a machine translation system to properly handle all textual language that can be called “Arabic,” it is essential to handle Arabizi as well as Arabic script. However, currently available machine translation systems either do not handle Arabizi, or at least do not handle it in any but the most limited of ways. In order to use any of the widely available open-source engines such as Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), or Joshua (Post et al., 2013), one would need to train on a substantial corpus of parallel Arabizi-English, which is not known to exist. Microsoft’s Bing Translator does not appear to handle Arabizi at all. Google Translate only attempts to handle Arabizi when characters are manually typed, letter by letter, into a translation box (i.e. not pasted), and thus cannot be used to translate Arabizi web pages or documents, or even more than a few paragraphs at once.2 Because much communication is done in Arabizi, particularly in social media contexts, there is a great need to translate such commun"
2014.amta-researchers.25,2010.amta-papers.12,0,0.0637696,"Missing"
2014.amta-researchers.25,W12-2109,0,0.233702,"Missing"
2014.amta-researchers.25,2012.amta-commercial.8,0,0.0776595,"Missing"
2014.amta-researchers.25,D07-1103,0,0.023449,"nual construction also includes a “w”-dropping sequence pair, which we elected not to add. Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 334 deromanization approach none manual manual + lm automatic + lm semi-automatic + lm B LEU Test 1 Test 2 18.2 0.3 20.1 1.7 21.5 2.9 25.6 7.7 25.8 8.0 Table 2: Deromanization performance (note: not machine translation performance) of manually and automatically constructed modules, measured as word-based BLEU against a reference deromanization. cipled and still automatic approach such as that taken by Johnson et al. (2007) may accomplish the same goal. 2.4 Intrinsic Evaluation Even though our wFST-based machine translation system architecture is designed such that we can persist multiple deromanization (and non-deromanization) possibilities, it is helpful to examine the Viterbi deromanization choices of our methods, both qualitatively and quantitatively. For quantitative evaluation, both intrinsic and extrinsic, we use two test corpora of sentence-aligned Arabizi-English social media data made available to us as part of DARPABOLT. Statistics of the corpora are shown in Table 1. The data also includes reference"
2014.amta-researchers.25,P07-2045,0,0.00412367,"es of Arabizi mixed with Arabic and English in Twitter dialects > h. may be pronounced as [g] (as in god), [Z] (as in vision), or [dZ] (as in juice), and that > the digraph “dj” is used in French for [dZ]. For a machine translation system to properly handle all textual language that can be called “Arabic,” it is essential to handle Arabizi as well as Arabic script. However, currently available machine translation systems either do not handle Arabizi, or at least do not handle it in any but the most limited of ways. In order to use any of the widely available open-source engines such as Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), or Joshua (Post et al., 2013), one would need to train on a substantial corpus of parallel Arabizi-English, which is not known to exist. Microsoft’s Bing Translator does not appear to handle Arabizi at all. Google Translate only attempts to handle Arabizi when characters are manually typed, letter by letter, into a translation box (i.e. not pasted), and thus cannot be used to translate Arabizi web pages or documents, or even more than a few paragraphs at once.2 Because much communication is done in Arabizi, particularly in social media contexts, there is a great nee"
2014.amta-researchers.25,W11-0301,0,0.0127111,"to a ya ( ø), which is also the equivalent of replacing the English vowel ‘a’ by ‘i’. 3 System Description As illustrated in Figure 2, our deromanization module is one component in a pipeline of processing that forms a machine translation system. Aside from the deromanization module, which we vary in the following experiments, our system is constant and built as follows: The preprocessing additionally consists of a regular expression-based tokenization and normalization module to separate punctuation, and a word morphological segmentation module based on the typebased unsupervised approach of Lee et al. (2011). The machine translation module is phrase based, in the style of Koehn et al. (2003), and is trained on informal Arabic-English parallel and monolingual data made available through DARPA BOLT. The post-processing consists of deterministic detokenization based on the output word sequence. The capitalizer is part of our pipeline, as noted in Figure 2, but since we do not evaluate cased translations it was turned off for these experiments. 4 Extrinsic Experiments In Table 3 we show the results of evaluating our informal Arabic-English MT system on the two aforementioned test sets while equipped"
2014.amta-researchers.25,J97-2003,0,0.0765792,"into Arabic script for use in search engines, such as Yamli (www.yamli.com). Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 330 Arabizi deromanization tokenization morphological segmentation detokenization capitalization translation English Figure 2: Schematic of our modular wFST-based machine translation system structure. The focus of this work is on the deromanization module. 2 Building an Arabizi-to-Arabic Converter The design of our phrase-based machine translation system is modular and uses weighted finitestate transducers (wFSTs) (Mohri, 1997) to propagate information from module to module. It can thus accept a weighted lattice of possible inputs and can generate a weighted lattice of possible outputs. Our Arabizi-to-Arabic converter is one module in a pipeline that tokenizes, analyzes, translates, and re-composes data in the process of generating a translation. A schematic overview of the modules in our translation system is shown in Figure 2. An advantage of this framework is that it allows us the opportunity to propagate ambiguity through the processing pipeline so that difficult decisions may be deferred to modules with better"
2014.amta-researchers.25,J03-1002,0,0.00455398,"rabizi length 1 1 1 2 2 2 3 3 3 3 4 4 4 4 Arabic length 0 1 2 1 2 3 1 2 3 4 1 2 3 4 automatic count 0 55 3 178 341 3 112 736 415 2 10 369 698 216 manual count 7 51 0 25 0 0 0 0 0 0 0 0 0 0 Figure 5: Distribution of Arabizi-to-Arabic character sequence lengths in automatic and manually generated approaches to wFST building. Entries in boldface indicate the subsets of the automatic or manual construction that were included in the semi-automatic construction. as sentence pairs and the characters as words, and estimated Arabizi-to-English character alignments using a standard GIZA implementation (Och and Ney, 2003) with reorderings inhibited. We then extracted character sequence pairs up to four characters in length per side that were also consistent with the character alignments, in accordance with standard practice for building phrase translation correspondence tables (Koehn et al., 2003). This resulted in a set of 3138 unique sequence pairs. We estimated conditional probabilities of Arabic given Arabizi by simple maximum likelihood. A portion of the learned table is shown on the right side of Figure 4. We can see that, in comparison to the manually constructed table on the left side of the figure, th"
2014.amta-researchers.25,I11-1109,0,0.057031,"Missing"
2014.amta-researchers.25,W13-2226,0,0.0159855,"lects > h. may be pronounced as [g] (as in god), [Z] (as in vision), or [dZ] (as in juice), and that > the digraph “dj” is used in French for [dZ]. For a machine translation system to properly handle all textual language that can be called “Arabic,” it is essential to handle Arabizi as well as Arabic script. However, currently available machine translation systems either do not handle Arabizi, or at least do not handle it in any but the most limited of ways. In order to use any of the widely available open-source engines such as Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), or Joshua (Post et al., 2013), one would need to train on a substantial corpus of parallel Arabizi-English, which is not known to exist. Microsoft’s Bing Translator does not appear to handle Arabizi at all. Google Translate only attempts to handle Arabizi when characters are manually typed, letter by letter, into a translation box (i.e. not pasted), and thus cannot be used to translate Arabizi web pages or documents, or even more than a few paragraphs at once.2 Because much communication is done in Arabizi, particularly in social media contexts, there is a great need to translate such communication, both for those wanting"
2020.acl-main.218,N19-1423,0,0.0162132,"Missing"
2020.acl-main.218,P19-1005,0,0.0123241,"l conversational systems that are at least partially related to the lack of sufficiently high quality yes-ands we deal with in this work; approaches that mitigate these problems vary. The majority of recent works focus on diversifying the responses by modifying the training and decoding objectives (Li et al., 2016a,b, 2017a, 2016c; Xu et al., 2017; Shao et al., 2017). Other methods introduce latent variables to encourage diversity (Serban et al., 2017; Zhao et al., 2017). Some explore methods of re-weighing training instances that encourage diversity (Liu et al., 2018; Lison and Bibauw, 2017; Du and Black, 2019). Our approach is complementary to all the modelbased approaches described here, as it simply deals with the production of a particularly useful corpus, that can be used to fine-tune on top of these methods. We provide a survey of publicly available textbased datasets frequently used for open-domain dialogue systems and discuss their limitations for our purpose of generating grounded responses (see Table 5 for an overview). DailyDialog is a collection of multi-turn dialogue with manually annotated emotion and intent labels (Li et al., 2017b). Danescu-Niculescu-Mizil and Lee (2011) created the"
2020.acl-main.218,N16-1014,0,0.218572,"ctors consciously abide by this principle during improv performances, there is a high proportion of these turns embedded in improv dialogue, which helps ensure scenes are coherent and interesting. 1 except for, on occasion, external stimulus such as a suggestion from the audience 2398 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2398–2413 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Open-domain neural dialogue systems, by contrast, specifically lack coherence and interestingness. They commonly repeat previous utterances (Li et al., 2016c) or generate non-committal, generic statements such as I don’t know that are logically coherent as a response but preempt further conversation (Sordoni et al., 2015; Serban et al., 2015; Li et al., 2016a). Either of these developments leads to a conversational black hole and discourages participation in further dialogue turns. This is a critical shortcoming for open-domain dialogue agents, which, unlike task-oriented dialogue systems, are not guided by specific objectives other than entertainment (Huang et al., 2020). It would behoove such systems to adopt the strategies improvisers include"
2020.acl-main.218,D16-1127,0,0.254245,"ctors consciously abide by this principle during improv performances, there is a high proportion of these turns embedded in improv dialogue, which helps ensure scenes are coherent and interesting. 1 except for, on occasion, external stimulus such as a suggestion from the audience 2398 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2398–2413 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Open-domain neural dialogue systems, by contrast, specifically lack coherence and interestingness. They commonly repeat previous utterances (Li et al., 2016c) or generate non-committal, generic statements such as I don’t know that are logically coherent as a response but preempt further conversation (Sordoni et al., 2015; Serban et al., 2015; Li et al., 2016a). Either of these developments leads to a conversational black hole and discourages participation in further dialogue turns. This is a critical shortcoming for open-domain dialogue agents, which, unlike task-oriented dialogue systems, are not guided by specific objectives other than entertainment (Huang et al., 2020). It would behoove such systems to adopt the strategies improvisers include"
2020.acl-main.218,D17-1230,0,0.616755,"del to generate single-turn responses for the yes-and prompts in the development set. We use nucleus sampling (Holtzman et al., 2020) for the decoding step to keep only the top tokens with a cumulative probability that together exceed 0.9, from which the next token is chosen with multinomial sampling. 4.1 Data Configurations For our experiments, we use several established dialogue datasets as baselines, namely Persona-chat (Zhang et al., 2018), Cornell (Danescu-NiculescuMizil and Lee, 2011) (the unfiltered corpus out of which we extract yes-ands, as described in Section 2.2), and DailyDialog (Li et al., 2017b). Each of these is an English-language open-domain casual conversation corpus with 100k–300k turns. For each of these datasets, we either simply finetune on that dataset, or fine-tune and then further 2403 Figure 5: Interface used by human evaluators to rank responses based on their quality as a yes-and, where a rank of 1 is most preferred. The correct ranking is shown for this example. The option ranked 1 is a yesbut: it does not reject a reality but rather rejects a suggestion and provides refining information that is most coherent to the prompt. fine-tune with SPOLIN. In another configura"
2020.acl-main.218,I17-1099,0,0.355293,"del to generate single-turn responses for the yes-and prompts in the development set. We use nucleus sampling (Holtzman et al., 2020) for the decoding step to keep only the top tokens with a cumulative probability that together exceed 0.9, from which the next token is chosen with multinomial sampling. 4.1 Data Configurations For our experiments, we use several established dialogue datasets as baselines, namely Persona-chat (Zhang et al., 2018), Cornell (Danescu-NiculescuMizil and Lee, 2011) (the unfiltered corpus out of which we extract yes-ands, as described in Section 2.2), and DailyDialog (Li et al., 2017b). Each of these is an English-language open-domain casual conversation corpus with 100k–300k turns. For each of these datasets, we either simply finetune on that dataset, or fine-tune and then further 2403 Figure 5: Interface used by human evaluators to rank responses based on their quality as a yes-and, where a rank of 1 is most preferred. The correct ranking is shown for this example. The option ranked 1 is a yesbut: it does not reject a reality but rather rejects a suggestion and provides refining information that is most coherent to the prompt. fine-tune with SPOLIN. In another configura"
2020.acl-main.218,W04-1013,0,0.0352336,"prompt. fine-tune with SPOLIN. In another configuration, we also fine-tune directly with SPOLIN on top of GPT-2. The original GPT-2 implementation prepends the personalities given in Persona-chat to the dialogue sequence input before tokenization. For fine-tuning to datasets apart from Persona-chat, we simply do not prepend any auxiliary information to the dialogue sequence input. 4.2 Human Evaluation Automatic metrics that rely on n-gram overlap, such as BLEU, ROUGE, and METEOR, are often used for generative models when there is little variability in the target output (Papineni et al., 2002; Lin, 2004; Banerjee and Lavie, 2005). However, there can be a wide variety of responses that qualify as a good yes-and, a problem common to opendomain generation tasks. An adequate evaluation of our models requires assessing the main yes-and criteria: agreement with the context and the quality of the new relevant contribution, both of which are not feasible with the aforementioned metrics. Therefore, we ask human evaluators to compare the quality of the yes-ands generated by various models and the actual response to the prompt in SPOLIN that is used as the input. We ask human evaluators to rank a set o"
2020.acl-main.218,W17-5546,0,0.0168716,"onses generated by neural conversational systems that are at least partially related to the lack of sufficiently high quality yes-ands we deal with in this work; approaches that mitigate these problems vary. The majority of recent works focus on diversifying the responses by modifying the training and decoding objectives (Li et al., 2016a,b, 2017a, 2016c; Xu et al., 2017; Shao et al., 2017). Other methods introduce latent variables to encourage diversity (Serban et al., 2017; Zhao et al., 2017). Some explore methods of re-weighing training instances that encourage diversity (Liu et al., 2018; Lison and Bibauw, 2017; Du and Black, 2019). Our approach is complementary to all the modelbased approaches described here, as it simply deals with the production of a particularly useful corpus, that can be used to fine-tune on top of these methods. We provide a survey of publicly available textbased datasets frequently used for open-domain dialogue systems and discuss their limitations for our purpose of generating grounded responses (see Table 5 for an overview). DailyDialog is a collection of multi-turn dialogue with manually annotated emotion and intent labels (Li et al., 2017b). Danescu-Niculescu-Mizil and Le"
2020.acl-main.218,L16-1147,0,0.218108,"only with another dialogue dataset, although in the case of DailyDialog, the average ranking improves only by at most 0.06 after fine-tuning with SPOLIN. However, even the responses generated by models trained with SPOLIN are not ranked as well as the actual responses in the development set, indicating our models are still inferior to professional human improviser quality. 5 Extracting from Other Corpora The approach to classifier-based mining we describe in Section 2.2 can naturally be applied to other dialogue corpora. We thus next consider mining the gigantic (441M sentence) OpenSubtitles (Lison and Tiedemann, 2016) collection. As OpenSubtitles contains undesirable material, such as subtitles for media with minimal dialogue, we instead mine from the (3.3M sentence) SubTle corpus (Ameixa et al., 2013), a preprocessed subset of OpenSubtitles that heuristically combines subtitle sequences into dialogue form. By iterating through half of this corpus, we collect more than 40,000 yes-ands from it alone, which, when added to SPOLIN, yields what we call SPOLIN-extended, which contains about 68,000 yes-ands, more than 2.5 times the size of the core SPOLIN. Heuristics for finding alternations mean that SubTle’s ut"
2020.acl-main.218,D18-1297,0,0.0222035,"non-committal responses generated by neural conversational systems that are at least partially related to the lack of sufficiently high quality yes-ands we deal with in this work; approaches that mitigate these problems vary. The majority of recent works focus on diversifying the responses by modifying the training and decoding objectives (Li et al., 2016a,b, 2017a, 2016c; Xu et al., 2017; Shao et al., 2017). Other methods introduce latent variables to encourage diversity (Serban et al., 2017; Zhao et al., 2017). Some explore methods of re-weighing training instances that encourage diversity (Liu et al., 2018; Lison and Bibauw, 2017; Du and Black, 2019). Our approach is complementary to all the modelbased approaches described here, as it simply deals with the production of a particularly useful corpus, that can be used to fine-tune on top of these methods. We provide a survey of publicly available textbased datasets frequently used for open-domain dialogue systems and discuss their limitations for our purpose of generating grounded responses (see Table 5 for an overview). DailyDialog is a collection of multi-turn dialogue with manually annotated emotion and intent labels (Li et al., 2017b). Danesc"
2020.acl-main.218,W15-4640,0,0.281412,"I’m not in the mood. I’m not in the mood. I’m in here. I’m just trying to make sure I can get a bagel. Oh, yeah, the guy who left the bagel. I can help you. The problem is that the bagels are burned. Table 4: Average human ranking of responses to prompts from Spontaneanation generated by models trained with SPOLIN, an existing dialog corpus, or both, based on the yes-and criteria. Rank is scaled from 1 to 4, lower is better. Dataset DailyDialog (Li et al., 2017b) Cornell Movie-Dialogs Corpus (Danescu-Niculescu-Mizil and Lee, 2011) Persona-chat (Zhang et al., 2018) The Ubuntu Dialogue Corpus (Lowe et al., 2015) Twitter Triple Conversations (Sordoni et al., 2015) OpenSubtitles (Lison and Tiedemann, 2016) SubTle (Eng) (Ameixa et al., 2013) London-Lund Corpus (Greenbaum and Svartvik, 1990) London-Lund Corpus 2 (Põldvere et al., 2017) SPOLIN (yes-and only) SPOLIN-extended (yes-and only) Source Crowdsourced Movie scripts Crowdsourced Ubuntu chat logs Social media Subtitles Subtitles Various sources Various sources Improv, Movie scripts Improv, Movie scripts, subtitles Size∗ 104K 304K 162K 7M 6K 441M sentences 3.3M pairs 500K words 500K words 26K pairs 68K pairs Table 5: A survey of publicly available Eng"
2020.acl-main.218,P02-1040,0,0.106756,"s most coherent to the prompt. fine-tune with SPOLIN. In another configuration, we also fine-tune directly with SPOLIN on top of GPT-2. The original GPT-2 implementation prepends the personalities given in Persona-chat to the dialogue sequence input before tokenization. For fine-tuning to datasets apart from Persona-chat, we simply do not prepend any auxiliary information to the dialogue sequence input. 4.2 Human Evaluation Automatic metrics that rely on n-gram overlap, such as BLEU, ROUGE, and METEOR, are often used for generative models when there is little variability in the target output (Papineni et al., 2002; Lin, 2004; Banerjee and Lavie, 2005). However, there can be a wide variety of responses that qualify as a good yes-and, a problem common to opendomain generation tasks. An adequate evaluation of our models requires assessing the main yes-and criteria: agreement with the context and the quality of the new relevant contribution, both of which are not feasible with the aforementioned metrics. Therefore, we ask human evaluators to compare the quality of the yes-ands generated by various models and the actual response to the prompt in SPOLIN that is used as the input. We ask human evaluators to r"
2020.acl-main.218,N15-1020,0,0.06986,"Missing"
2020.acl-main.218,D17-1065,0,0.0291445,"Missing"
2020.acl-main.218,P95-1026,0,0.639027,"al., 2017). These corpora were not collected with the criteria for yes-ands in mind. Even for datasets with dialogue taking place in a similar domain as improv, they naturally contain only a small proportion of yes-ands. However, the relatively large sizes of these datasets still make them useful for dialogue systems. They can be used effectively for grounded conversations if the yes-ands or other desirable dialogue acts can be filtered out or given higher weights in training to enforce their characteristics in the responses generated. Our data collection approach is similar to the method of Yarowsky (1995), which formalizes the bootstrapping mechanism of iteratively improving a classifier and label unlabeled data. The main difference from the Yarowsky algorithm and our approach is that, rather than using a fully automated process for increasing training data, we use a probability threshold to regulate recall, followed by human judgment to ensure high precision. Apart from Clark and Schaefer (1989) there have been other taxonomies of grounding. For example, Traum (1999) considers six categories; among these are acknowledge and continue, which, taken together, map nicely to yes-and. Magerko et al"
2020.acl-main.218,P18-1205,0,0.34479,"lected using the methods described in Section 2.2. The composition details of SPOLIN are shown in Table 3. 4 Experiments To evaluate the effect of SPOLIN on generating yes-and responses and thus improving generated dialogue quality, we train a common architecture with a variety of fine-tuning data configurations, both with and without SPOLIN. Specifically, for each data configuration we fine-tune a doublehead GPT-2 model (117M-parameter version based on the implementation by Wolf et al. (2019b)), which achieved state-of-the-art performance on Personachat for the ConvAI-2 dialogue competition (Zhang et al., 2018). We fine-tune the models using two learning objectives, which we weigh equally in calculating loss: 1. Predicting the next word. 2. Predicting the next correct candidate that best fits the dialogue given the dialogue history. The language modeling component uses pretrained weights from OpenAI, while the candidate classification head is trained from scratch. For evaluation, we use the language modeling component of the fine-tuned model to generate single-turn responses for the yes-and prompts in the development set. We use nucleus sampling (Holtzman et al., 2020) for the decoding step to keep"
2020.acl-main.218,P17-1061,0,0.0149498,"less otherwise specified. 6 Related Work Many works have identified the same issues of repetitive or non-committal responses generated by neural conversational systems that are at least partially related to the lack of sufficiently high quality yes-ands we deal with in this work; approaches that mitigate these problems vary. The majority of recent works focus on diversifying the responses by modifying the training and decoding objectives (Li et al., 2016a,b, 2017a, 2016c; Xu et al., 2017; Shao et al., 2017). Other methods introduce latent variables to encourage diversity (Serban et al., 2017; Zhao et al., 2017). Some explore methods of re-weighing training instances that encourage diversity (Liu et al., 2018; Lison and Bibauw, 2017; Du and Black, 2019). Our approach is complementary to all the modelbased approaches described here, as it simply deals with the production of a particularly useful corpus, that can be used to fine-tune on top of these methods. We provide a survey of publicly available textbased datasets frequently used for open-domain dialogue systems and discuss their limitations for our purpose of generating grounded responses (see Table 5 for an overview). DailyDialog is a collection"
2020.acl-main.218,W11-0609,0,\N,Missing
2020.emnlp-main.50,D13-1185,1,0.75481,"Missing"
2020.emnlp-main.50,P08-1090,1,0.872582,"Missing"
2020.emnlp-main.50,P09-1068,1,0.7437,"tory provides significant improvement to downstream end-to-end Information Extraction over a state-of-the-art joint neural extraction model, when used as additional global features to unfold instance graphs.1 1 Introduction Existing approaches to automated event extraction retain the overly simplistic assumption that events are atomic occurrences. Understanding events requires knowledge in the form of a repository of abstracted event schemas (complex event templates). Scripts (Schank and Abelson, 1977) encode frequently recurring event sequences, where events are ordered by temporal relation (Chambers and Jurafsky, 2009), causal relation (Mostafazadeh et al., 2016b), or narrative order (Jans et al., 2012). Event schemas have become increasingly important for natural language understanding tasks such as story 1 Our code and data are publicly available for research purpose at http://blender.cs.illinois.edu/software/ pathlm. ending prediction (Mostafazadeh et al., 2016a) and reading comprehension (Koˇcisk´y et al., 2018; Ostermann et al., 2019). Previous schema induction methods mostly ignore uncertainty, re-occurring events and multiple hypotheses, with limited attention to capture complex relations among event"
2020.emnlp-main.50,chambers-jurafsky-2010-database,1,0.801355,"Missing"
2020.emnlp-main.50,N13-1104,0,0.133398,"Missing"
2020.emnlp-main.50,N19-1423,0,0.0311453,"Missing"
2020.emnlp-main.50,N15-1165,0,0.026451,"Missing"
2020.emnlp-main.50,W16-1701,1,0.907272,"Missing"
2020.emnlp-main.50,P16-1025,1,0.902933,"Missing"
2020.emnlp-main.50,E12-1034,0,0.126036,"Missing"
2020.emnlp-main.50,W19-3311,0,0.0837118,"Missing"
2020.emnlp-main.50,Q18-1023,0,0.0569661,"Missing"
2020.emnlp-main.50,2020.acl-main.713,1,0.645499,"Missing"
2020.emnlp-main.50,N16-1098,1,0.812451,"ream end-to-end Information Extraction over a state-of-the-art joint neural extraction model, when used as additional global features to unfold instance graphs.1 1 Introduction Existing approaches to automated event extraction retain the overly simplistic assumption that events are atomic occurrences. Understanding events requires knowledge in the form of a repository of abstracted event schemas (complex event templates). Scripts (Schank and Abelson, 1977) encode frequently recurring event sequences, where events are ordered by temporal relation (Chambers and Jurafsky, 2009), causal relation (Mostafazadeh et al., 2016b), or narrative order (Jans et al., 2012). Event schemas have become increasingly important for natural language understanding tasks such as story 1 Our code and data are publicly available for research purpose at http://blender.cs.illinois.edu/software/ pathlm. ending prediction (Mostafazadeh et al., 2016a) and reading comprehension (Koˇcisk´y et al., 2018; Ostermann et al., 2019). Previous schema induction methods mostly ignore uncertainty, re-occurring events and multiple hypotheses, with limited attention to capture complex relations among events, other than temporal or causal relations."
2020.emnlp-main.50,W16-1007,1,0.880849,"ream end-to-end Information Extraction over a state-of-the-art joint neural extraction model, when used as additional global features to unfold instance graphs.1 1 Introduction Existing approaches to automated event extraction retain the overly simplistic assumption that events are atomic occurrences. Understanding events requires knowledge in the form of a repository of abstracted event schemas (complex event templates). Scripts (Schank and Abelson, 1977) encode frequently recurring event sequences, where events are ordered by temporal relation (Chambers and Jurafsky, 2009), causal relation (Mostafazadeh et al., 2016b), or narrative order (Jans et al., 2012). Event schemas have become increasingly important for natural language understanding tasks such as story 1 Our code and data are publicly available for research purpose at http://blender.cs.illinois.edu/software/ pathlm. ending prediction (Mostafazadeh et al., 2016a) and reading comprehension (Koˇcisk´y et al., 2018; Ostermann et al., 2019). Previous schema induction methods mostly ignore uncertainty, re-occurring events and multiple hypotheses, with limited attention to capture complex relations among events, other than temporal or causal relations."
2020.emnlp-main.50,P15-1019,0,0.145444,"Missing"
2020.emnlp-main.50,S19-1012,0,0.0429448,"ent schemas (complex event templates). Scripts (Schank and Abelson, 1977) encode frequently recurring event sequences, where events are ordered by temporal relation (Chambers and Jurafsky, 2009), causal relation (Mostafazadeh et al., 2016b), or narrative order (Jans et al., 2012). Event schemas have become increasingly important for natural language understanding tasks such as story 1 Our code and data are publicly available for research purpose at http://blender.cs.illinois.edu/software/ pathlm. ending prediction (Mostafazadeh et al., 2016a) and reading comprehension (Koˇcisk´y et al., 2018; Ostermann et al., 2019). Previous schema induction methods mostly ignore uncertainty, re-occurring events and multiple hypotheses, with limited attention to capture complex relations among events, other than temporal or causal relations. Temporal relations exist between almost all events, even those that are not semantically related; while research in identifying causal relations has been hobbled by low inter-annotator agreement (Hong et al., 2016). In this paper, we hypothesize that two events are connected when their entity arguments are coreferential or semantically related. For example, in Figure 1, (a) and (b)"
2020.emnlp-main.50,P17-1178,1,0.846469,"ument roles. We follow our recent work on ACE IE (Lin et al., 2020) to split the data. We consider the training set as historical data to train the LM, and the test set as our target data to induce schema for target scenarios. The instance graphs of the target data set are constructed from manual annotations. For historical data, we construct event instance graphs from both manual annotations (Historicalann ) and system extraction results (Historicalsys ) from the state-ofthe-art IE model (Lin et al., 2020). We perform cross-document entity coreference resolution by applying an entity linker (Pan et al., 2017) for both annotated and system generated instance graphs. Table 2 shows the data statistics. Split Historicalann Historicalsys Validation Target The cardinality for an instance graph and a schema will be the number of substructures in each, i.e., X |g|I = count(hvm , emn , vn i), hvm ,emn ,vn i∈g |s|S = 47,525 48,664 3,422 3,673 7,152 7,018 728 802 4,419 4,426 468 424 By extension, each path of length l=5 in a graph schema [φi , ψij , φj , ψjk , φk ] contains two consecutive triples hφi , ψij ,φj i, hφj , ψjk , φk i∈s, and a matched instance path contains two consecutive instance triples hvm ,"
2020.emnlp-main.50,K19-1051,0,0.111184,"Missing"
2020.emnlp-main.50,N18-1202,0,0.129853,"event but fails to extract the I NVESTIGATE C RIME triggered by “discovered” and its D EFENDANT argument “Mohammed A. Salameh”. Event graph scehmas can inform the model that a person who is arrested was usually investigated, our IE system can fix this missing error. Therefore we also conduct extrinsic evaluations and show the effectiveness of the induced schema repository in enhancing downstream end-to-end IE tasks. PART- WHOLE PLACE−1 −−−−−−−→ GPE −−−−−→ ATTACK. We train the path language model on two tasks: learning an auto-regressive language model (Ponte and Croft, 1998; Dai and Le, 2015; Peters et al., 2018; Radford et al.; Yang et al., 2019) to predict an edge or a node, given previous edges and nodes in a path, and a neighboring path classification task to predict how likely two paths co-occur. The path language model is trained from all the paths between two event instances from the same document, based on the assumption that events from the same document (especially news document) tell a coherent story. We propose two intrinsic evaluation metrics, instance coverage and instance coherence, to assess when event instance graphs are covered by each 685 In summary, we make the following novel con"
2020.emnlp-main.50,E14-1024,0,0.100189,"Missing"
2020.emnlp-main.50,D15-1195,0,0.203725,"Missing"
2020.emnlp-main.50,N16-1049,0,0.151836,"Missing"
2020.emnlp-main.50,W17-0901,0,0.0336757,"Missing"
2020.emnlp-main.50,W19-3404,0,0.0428587,"Missing"
2020.emnlp-main.703,D16-1125,1,0.840058,"space effectively restricts understanding to small grammars (Paul et al., 2018; Walter et al., 2013) or controlled dialog responses (Thomason et al., 2020). These efforts to translate language instructions to actions build towards using language for end-to-end, continuous control (WS4). Collaborative games have long served as a testbed for studying language (Werner and Dyer, 1991) and emergent communication (Schlangen, 2019a; Lazaridou et al., 2018; Chaabouni et al., 2020). Suhr et al. (2019a) introduced an environment for evaluating language understanding in the service of a shared goal, and Andreas and Klein (2016) use a visual paradigm for studying pragmatics. Such efforts help us examine how inductive biases and environmental pressures build towards socialization (WS5), even if full social context is still too difficult and expensive to be practical. Most of this research provides resources such as data, code, simulators and methodology for evaluating the multimodal content of linguistic representations (Schlangen, 2019b; Silberer and Lapata, 2014; Bruni et al., 2012). Moving forward, we encourage a broad re-examination of how NLP frames the relationship between meaning and context (Bender and Koller,"
2020.emnlp-main.703,2020.acl-main.463,0,0.195512,"s and Klein (2016) use a visual paradigm for studying pragmatics. Such efforts help us examine how inductive biases and environmental pressures build towards socialization (WS5), even if full social context is still too difficult and expensive to be practical. Most of this research provides resources such as data, code, simulators and methodology for evaluating the multimodal content of linguistic representations (Schlangen, 2019b; Silberer and Lapata, 2014; Bruni et al., 2012). Moving forward, we encourage a broad re-examination of how NLP frames the relationship between meaning and context (Bender and Koller, 2020) and how pretraining obfuscates our ability to measure generalization (Linzen, 2020). 7 Conclusions Our World Scopes are steep steps. WS5 implies a persistent agent experiencing time and a personalized set of experiences. confined to IID datasets that lack the structure in time from which humans draw correlations about long-range causal dependencies. What happens if a machine is allowed to participate consistently? This is difficult to test under current evaluation paradigms for generalization. Yet, this is the structure of generalization in human development: drawing analogies to episodic mem"
2020.emnlp-main.703,J92-4003,0,0.0781903,"Missing"
2020.emnlp-main.703,P12-1015,0,0.0358063,"., 2020). Suhr et al. (2019a) introduced an environment for evaluating language understanding in the service of a shared goal, and Andreas and Klein (2016) use a visual paradigm for studying pragmatics. Such efforts help us examine how inductive biases and environmental pressures build towards socialization (WS5), even if full social context is still too difficult and expensive to be practical. Most of this research provides resources such as data, code, simulators and methodology for evaluating the multimodal content of linguistic representations (Schlangen, 2019b; Silberer and Lapata, 2014; Bruni et al., 2012). Moving forward, we encourage a broad re-examination of how NLP frames the relationship between meaning and context (Bender and Koller, 2020) and how pretraining obfuscates our ability to measure generalization (Linzen, 2020). 7 Conclusions Our World Scopes are steep steps. WS5 implies a persistent agent experiencing time and a personalized set of experiences. confined to IID datasets that lack the structure in time from which humans draw correlations about long-range causal dependencies. What happens if a machine is allowed to participate consistently? This is difficult to test under current"
2020.emnlp-main.703,2020.acl-main.407,0,0.0170751,"ng (She et al., 2014) in the real world face challenging, continuous perception and control (Tellex et al., 2020). Consequently, research in this space effectively restricts understanding to small grammars (Paul et al., 2018; Walter et al., 2013) or controlled dialog responses (Thomason et al., 2020). These efforts to translate language instructions to actions build towards using language for end-to-end, continuous control (WS4). Collaborative games have long served as a testbed for studying language (Werner and Dyer, 1991) and emergent communication (Schlangen, 2019a; Lazaridou et al., 2018; Chaabouni et al., 2020). Suhr et al. (2019a) introduced an environment for evaluating language understanding in the service of a shared goal, and Andreas and Klein (2016) use a visual paradigm for studying pragmatics. Such efforts help us examine how inductive biases and environmental pressures build towards socialization (WS5), even if full social context is still too difficult and expensive to be practical. Most of this research provides resources such as data, code, simulators and methodology for evaluating the multimodal content of linguistic representations (Schlangen, 2019b; Silberer and Lapata, 2014; Bruni et"
2020.emnlp-main.703,P14-1113,0,0.0214947,"Missing"
2020.emnlp-main.703,W17-2810,0,0.0700925,"Missing"
2020.emnlp-main.703,J93-2004,0,0.0733849,"al These World Scopes go beyond text to consider the contextual foundations of language: grounding, embodiment, and social interaction. We describe a brief history and ongoing progression of how contextual information can factor into representations and tasks. We conclude with a discussion of how 8718 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8718–8735, c November 16–20, 2020. 2020 Association for Computational Linguistics 1 WS1: Corpora and Representations The story of data-driven language research begins with the corpus. The Penn Treebank (Marcus et al., 1993) is the canonical example of a clean subset of naturally generated language, processed and annotated for the purpose of studying representations. Such corpora and the model representations built from them exemplify WS1. Community energy was initially directed at finding formal linguistic structure, such as recovering syntax trees. Recent success on downstream tasks has not required such explicitly annotated signal, leaning instead on unstructured fuzzy representations. These representations span from dense word vectors (Mikolov et al., 2013) to contextualized pretrained representations (Peters"
2020.emnlp-main.703,E12-1076,0,0.0176926,"r new research into more challenging world modeling. Mottaghi et al. (2016) predicts the effects of forces on objects in images. Bakhtin et al. (2019) extends this physical reasoning to complex puzzles of cause and effect. Sun et al. (2019b,a) models scripts and actions, and alternative unsupervised training regimes (Bachman et al., 2019) open up research towards automatic concept formation. Advances in computer vision have enabled building semantic representations rich enough to interact with natural language. In the last decade of work descendant from image captioning (Farhadi et al., 2010; Mitchell et al., 2012), a myriad of tasks on visual question answering (Antol et al., 2015; Das et al., 2018; Yagcioglu et al., 2018), natural language and visual reasoning (Suhr et al., 2019b), visual commonsense (Zellers et al., 2019a), 8721 2 3 Or the 1,600 classes of Anderson et al. (2017). Torchvision/Detectron2 include dozens of trained models. and multilingual captioning/translation via video (Wang et al., 2019b) have emerged. These combined text and vision benchmarks are rich enough to train large-scale, multimodal transformers (Li et al., 2019a; Lu et al., 2019; Zhou et al., 2019) without language pretrain"
2020.emnlp-main.703,1985.tmi-1.17,0,0.223965,"achs et al., 1981; O’Grady, 2005; Vigliocco et al., 2014). Perception includes auditory, tactile, and visual input. Even restricted to purely linguistic signals, sarcasm, stress, and meaning can be implied through prosody. Further, tactile senses lend meaning, both physical (Sinapov et al., 2014; Thomason et al., 2016) and abstract, to concepts like heavy and soft. Visual perception is a rich signal for modeling a vastness of experiences in the world that cannot be documented by text alone (Harnad, 1990). For example, frames and scripts (Schank and Abelson, 1977; Charniak, 1977; Dejong, 1981; Mooney and Dejong, 1985) require understanding often unstated sets of pre- and post-conditions about the world. To borrow from Charniak (1977), how should we learn the meaning, method, and implications of painting? A web crawl of knowledge Eugene Charniak (A Framed PAINTING: The Representation of a Common Sense Knowledge Fragment 1977) from an exponential number of possible how-to, text-only guides and manuals (Bisk et al., 2020) is misdirected without some fundamental referents to which to ground symbols. Models must be able to watch and recognize objects, people, and activities to understand the language describing"
2020.emnlp-main.703,N18-1202,0,0.171411,"1993) is the canonical example of a clean subset of naturally generated language, processed and annotated for the purpose of studying representations. Such corpora and the model representations built from them exemplify WS1. Community energy was initially directed at finding formal linguistic structure, such as recovering syntax trees. Recent success on downstream tasks has not required such explicitly annotated signal, leaning instead on unstructured fuzzy representations. These representations span from dense word vectors (Mikolov et al., 2013) to contextualized pretrained representations (Peters et al., 2018; Devlin et al., 2019). Word representations have a long history predating the recent success of deep learning methods. Outside of NLP, philosophy (Austin, 1975) and linguistics (Lakoff, 1973; Coleman and Kay, 1981) recognized that meaning is flexible yet structured. Early experiments on neural networks trained with sequences of words (Elman, 1990; Bengio et al., 2003) suggested that vector representations could capture both syntax and semantics. Subsequent experiments with larger models, documents, and corpora have demonstrated that representations learned from text capture a great deal of in"
2020.emnlp-main.703,D18-1261,0,0.0266104,"perimentation with language starkly contrasts with the disembodied chat bots that are the focus of the current dialogue community (Roller et al., 2020; Adiwardana et al., 2020; Zhou et al., 2020; Chen et al., 2018; Serban et al., 2017), which often do not learn from individual experiences and whose environments are not persistent enough to learn the effects of actions. Theory of Mind When attempting to get what we want, we confront people who have their own desires and identities. The ability to consider the feelings and knowledge of others is now commonly referred to as the “Theory of Mind” (Nematzadeh et al., 2018). This paradigm has also been described under the “Speaker-Listener” model (Stephens et al., 2010), and a rich theory to describe this computationally is being actively developed under the Rational Speech Act Model (Frank and Goodman, 2012; Bergen et al., 2016). A series of challenges that attempt to address this fundamental aspect of communication have been introduced (Nematzadeh et al., 2018; Sap et al., 2019). These works are a great start towards deeper understanding, but static datasets can be problematic due to the risk of embedding spurious patterns and bias (de Vries et al., 2020; Le e"
2020.emnlp-main.703,P19-1506,0,0.0297579,"s rethinking existing tasks and investigating where their semantics can be expanded and grounded. This idea is not new (Chen and Mooney, 2008; Feng and Lapata, 2010; Bruni et al., 2014; Lazaridou et al., 2016) and has accelerated in the last few years. Elliott et al. (2016) reframes machine translation with visual observations, a trend extended into videos (Wang et al., 2019b). Regneri et al. (2013) introduce a foundational dataset aligning text descriptions and semantic annotations of actions with videos. Vision can even inform core tasks like syntax (Shi et al., 2019) and language modeling (Ororbia et al., 2019). Careful design is key, as visually augmented tasks can fail to require sensory perception (Thomason et al., 2019a). Language-guided, embodied agents invoke many of the challenges of WS4. Language-based navigation (Anderson et al., 2018) and task completion (Shridhar et al., 2020) in simulation environments ground language to actions, but even complex simulation action spaces can be discretized and enumerated. By contrast, language-guided robots that perform task completion (Tellex et al., 2014) and learning (She et al., 2014) in the real world face challenging, continuous perception and cont"
2020.emnlp-main.703,P16-1144,1,0.890919,"Missing"
2020.emnlp-main.703,N15-1082,0,0.0556621,"Missing"
2020.emnlp-main.703,D14-1162,0,0.0857475,"s in deep models. Traditionally, transfer learning relied on our understanding of model classes, such as English grammar. Domain adaptation simply required sufficient data to capture lexical variation, by assuming most higherlevel structure would remain the same. Unsupervised representations today capture deep associations across multiple domains, and can be used successfully transfer knowledge into surprisingly diverse contexts (Brown et al., 2020). These representations require scale in terms of both data and parameters. Concretely, Mikolov et al. (2013) trained on 1.6 billion tokens, while Pennington et al. (2014) scaled up to 840 billion tokens from Common Crawl. Recent approaches 1 A parallel discussion would focus on the hardware required to enable advances to higher World Scopes. Playstations (Pinto et al., 2009) and then GPUs (Krizhevsky et al., 2012) made many WS2 advances possible. Perception, interaction, and robotics leverage other new hardware. have made progress by substantially increasing the number of model parameters to better consume these vast quantities of data. Where Peters et al. (2018) introduced ELMo with ∼108 parameters, Transformer models (Vaswani et al., 2017) have continued to"
2020.emnlp-main.703,P18-2124,0,0.0547463,"Missing"
2020.emnlp-main.703,P19-1534,0,0.0307256,"rate language that does something to the world. Passive creation and evaluation of generated language separates generated utterances from their effects on other people, and while the latter is a rich learning signal it is inherently difficult to annotate. In order to learn the effects language has on the world, an agent must participate in linguistic activity, such as negotiation (Yang et al., 2019a; He et al., 2018; Lewis et al., 2017), collaboration (Chai et al., 2017), visual disambiguation (Anderson et al., 2018; Lazaridou et al., 2017; Liu and Chai, 2015), or providing emotional support (Rashkin et al., 2019). These activities require inferring mental states and social outcomes—a key area of interest in itself (Zadeh et al., 2019). What “lame” means in terms of discriminative information is always at question: it can be defined as “undesirable,” but what it tells one about the processes operating in the environment requires social context to determine (Bloom, 2002). It is the toddler’s social experimentation with “You’re so lame!” that gives the word weight and definite intent (Ornaghi et al., 2011). In other words, the discriminative signal for the most foundational part of a word’s meaning can o"
2020.emnlp-main.703,Q13-1003,0,0.0607394,"fool!” can be hurtful, while for others it may seem playful. Social knowledge is requisite for realistic understanding of sentiment in situated human contexts. 8725 Relevant recent work The move from WS2 to WS3 requires rethinking existing tasks and investigating where their semantics can be expanded and grounded. This idea is not new (Chen and Mooney, 2008; Feng and Lapata, 2010; Bruni et al., 2014; Lazaridou et al., 2016) and has accelerated in the last few years. Elliott et al. (2016) reframes machine translation with visual observations, a trend extended into videos (Wang et al., 2019b). Regneri et al. (2013) introduce a foundational dataset aligning text descriptions and semantic annotations of actions with videos. Vision can even inform core tasks like syntax (Shi et al., 2019) and language modeling (Ororbia et al., 2019). Careful design is key, as visually augmented tasks can fail to require sensory perception (Thomason et al., 2019a). Language-guided, embodied agents invoke many of the challenges of WS4. Language-based navigation (Anderson et al., 2018) and task completion (Shridhar et al., 2020) in simulation environments ground language to actions, but even complex simulation action spaces c"
2020.emnlp-main.703,P19-1180,0,0.0412109,"ent work The move from WS2 to WS3 requires rethinking existing tasks and investigating where their semantics can be expanded and grounded. This idea is not new (Chen and Mooney, 2008; Feng and Lapata, 2010; Bruni et al., 2014; Lazaridou et al., 2016) and has accelerated in the last few years. Elliott et al. (2016) reframes machine translation with visual observations, a trend extended into videos (Wang et al., 2019b). Regneri et al. (2013) introduce a foundational dataset aligning text descriptions and semantic annotations of actions with videos. Vision can even inform core tasks like syntax (Shi et al., 2019) and language modeling (Ororbia et al., 2019). Careful design is key, as visually augmented tasks can fail to require sensory perception (Thomason et al., 2019a). Language-guided, embodied agents invoke many of the challenges of WS4. Language-based navigation (Anderson et al., 2018) and task completion (Shridhar et al., 2020) in simulation environments ground language to actions, but even complex simulation action spaces can be discretized and enumerated. By contrast, language-guided robots that perform task completion (Tellex et al., 2014) and learning (She et al., 2014) in the real world fac"
2020.emnlp-main.703,D19-1454,0,0.0599647,"Missing"
2020.emnlp-main.703,P14-1068,1,0.867068,"al., 2018; Chaabouni et al., 2020). Suhr et al. (2019a) introduced an environment for evaluating language understanding in the service of a shared goal, and Andreas and Klein (2016) use a visual paradigm for studying pragmatics. Such efforts help us examine how inductive biases and environmental pressures build towards socialization (WS5), even if full social context is still too difficult and expensive to be practical. Most of this research provides resources such as data, code, simulators and methodology for evaluating the multimodal content of linguistic representations (Schlangen, 2019b; Silberer and Lapata, 2014; Bruni et al., 2012). Moving forward, we encourage a broad re-examination of how NLP frames the relationship between meaning and context (Bender and Koller, 2020) and how pretraining obfuscates our ability to measure generalization (Linzen, 2020). 7 Conclusions Our World Scopes are steep steps. WS5 implies a persistent agent experiencing time and a personalized set of experiences. confined to IID datasets that lack the structure in time from which humans draw correlations about long-range causal dependencies. What happens if a machine is allowed to participate consistently? This is difficult"
2020.emnlp-main.703,D19-1592,0,0.0554394,"Missing"
2020.emnlp-main.703,P18-1238,0,0.112804,"Missing"
2020.emnlp-main.703,W14-4313,1,0.780432,"re tasks like syntax (Shi et al., 2019) and language modeling (Ororbia et al., 2019). Careful design is key, as visually augmented tasks can fail to require sensory perception (Thomason et al., 2019a). Language-guided, embodied agents invoke many of the challenges of WS4. Language-based navigation (Anderson et al., 2018) and task completion (Shridhar et al., 2020) in simulation environments ground language to actions, but even complex simulation action spaces can be discretized and enumerated. By contrast, language-guided robots that perform task completion (Tellex et al., 2014) and learning (She et al., 2014) in the real world face challenging, continuous perception and control (Tellex et al., 2020). Consequently, research in this space effectively restricts understanding to small grammars (Paul et al., 2018; Walter et al., 2013) or controlled dialog responses (Thomason et al., 2020). These efforts to translate language instructions to actions build towards using language for end-to-end, continuous control (WS4). Collaborative games have long served as a testbed for studying language (Werner and Dyer, 1991) and emergent communication (Schlangen, 2019a; Lazaridou et al., 2018; Chaabouni et al., 202"
2020.emnlp-main.703,J08-1008,0,0.0602314,"time and a personalized set of experiences. confined to IID datasets that lack the structure in time from which humans draw correlations about long-range causal dependencies. What happens if a machine is allowed to participate consistently? This is difficult to test under current evaluation paradigms for generalization. Yet, this is the structure of generalization in human development: drawing analogies to episodic memories and gathering new data through non-independent experiments. As with many who have analyzed the history of NLP, its trends (Church, 2007), its maturation toward a science (Steedman, 2008), and its major challenges (Hirschberg and Manning, 2015; McClelland et al., 2019), we hope to provide momentum for a direction many are already heading. We call for and embrace the incremental, but purposeful, contextualization of language in human experience. With all that we have learned about what words can tell us and what they keep implicit, now is the time to ask: What tasks, representations, and inductive-biases will fill the gaps? Computer vision and speech recognition are mature enough for investigation of broader linguistic contexts (WS3). The robotics industry is rapidly developing"
2020.emnlp-main.703,D19-1218,0,0.0512018,"Missing"
2020.emnlp-main.703,P19-1644,0,0.137634,"easoning to complex puzzles of cause and effect. Sun et al. (2019b,a) models scripts and actions, and alternative unsupervised training regimes (Bachman et al., 2019) open up research towards automatic concept formation. Advances in computer vision have enabled building semantic representations rich enough to interact with natural language. In the last decade of work descendant from image captioning (Farhadi et al., 2010; Mitchell et al., 2012), a myriad of tasks on visual question answering (Antol et al., 2015; Das et al., 2018; Yagcioglu et al., 2018), natural language and visual reasoning (Suhr et al., 2019b), visual commonsense (Zellers et al., 2019a), 8721 2 3 Or the 1,600 classes of Anderson et al. (2017). Torchvision/Detectron2 include dozens of trained models. and multilingual captioning/translation via video (Wang et al., 2019b) have emerged. These combined text and vision benchmarks are rich enough to train large-scale, multimodal transformers (Li et al., 2019a; Lu et al., 2019; Zhou et al., 2019) without language pretraining (e.g. via conceptual captions (Sharma et al., 2018)) or further broadened to include audio (Tsai et al., 2019). Vision can also help ground speech signals (Srinivasa"
2020.emnlp-main.703,P06-1124,0,0.0482713,"ardware. have made progress by substantially increasing the number of model parameters to better consume these vast quantities of data. Where Peters et al. (2018) introduced ELMo with ∼108 parameters, Transformer models (Vaswani et al., 2017) have continued to scale by orders of magnitude between papers (Devlin et al., 2019; Radford et al., 2019; Zellers et al., 2019b) to ∼1011 (Brown et al., 2020). Current models are the next (impressive) step in language modeling which started with Good (1953), the weights of Kneser and Ney (1995); Chen and Goodman (1996), and the power-law distributions of Teh (2006). Modern approaches to learning dense representations allow us to better estimate these distributions from massive corpora. However, modeling lexical co-occurrence, no matter the scale, is still modeling the written world. Models constructed this way blindly search for symbolic co-occurences void of meaning. How can models yield both “impressive results” and “diminishing returns”? Language modeling— the modern workhorse of neural NLP systems—is a canonical example. Recent pretraining literature has produced results that few could have predicted, crowding leaderboards with “super-human"" accurac"
2020.emnlp-main.703,P19-1452,0,0.0131052,"e, in the limit, to everything humanity has ever written.1 We are no longer constrained to a single author or source, and the temptation for NLP is to believe everything that needs knowing can be learned from the written world. But, a large and noisy text corpus is still a text corpus. This move towards using large scale raw data has led to substantial advances in performance on existing and novel community benchmarks (Devlin et al., 2019; Brown et al., 2020). Scale in data and modeling has demonstrated that a single representation can discover both rich syntax and semantics without our help (Tenney et al., 2019). This change is perhaps best seen in transfer learning enabled by representations in deep models. Traditionally, transfer learning relied on our understanding of model classes, such as English grammar. Domain adaptation simply required sufficient data to capture lexical variation, by assuming most higherlevel structure would remain the same. Unsupervised representations today capture deep associations across multiple domains, and can be used successfully transfer knowledge into surprisingly diverse contexts (Brown et al., 2020). These representations require scale in terms of both data and pa"
2020.emnlp-main.703,N19-1197,1,0.927858,"the basis of action-oriented categories (Thelen and Smith, 1996) as children learn how to manipulate their perception by manipulating their environment. Language grounding enables an agent to connect words to these actionoriented categories for communication (Smith and Gasser, 2005), but requires action to fully discover such connections. Embodiment—situated action taking—is therefore a natural next broader context. An embodied agent, whether in a virtual world, such as a 2D Maze (MacMahon et al., 2006), a grid world (Chevalier-Boisvert et al., 2019), a simulated house (Anderson et al., 2018; Thomason et al., 2019b; Shridhar et al., 2020), or the real world (Tellex et al., 2011; Matuszek, 2018; Thomason et al., 2020; Tellex et al., 2020) must translate from language to action. Control and action taking open several new dimensions to understanding and actively learning about the world. Queries can be resolved via dialog-based exploration with a human interlocutor (Liu and Chai, 2015), even as new object properties, like texture and weight (Thomason et al., 2017), or feedback, like muscle activations (Moro and Kennington, 2018), become available. We see the need for embodied language with complex meaning"
2020.emnlp-main.703,P19-1656,0,0.0233734,"u et al., 2018), natural language and visual reasoning (Suhr et al., 2019b), visual commonsense (Zellers et al., 2019a), 8721 2 3 Or the 1,600 classes of Anderson et al. (2017). Torchvision/Detectron2 include dozens of trained models. and multilingual captioning/translation via video (Wang et al., 2019b) have emerged. These combined text and vision benchmarks are rich enough to train large-scale, multimodal transformers (Li et al., 2019a; Lu et al., 2019; Zhou et al., 2019) without language pretraining (e.g. via conceptual captions (Sharma et al., 2018)) or further broadened to include audio (Tsai et al., 2019). Vision can also help ground speech signals (Srinivasan et al., 2020; Harwath et al., 2019) to facilitate discovery of linguistic concepts (Harwath et al., 2020). At the same time, NLP resources contributed to the success of these vision backbones. Hierarchical semantic representations emerge from ImageNet classification pretraining partially due to class hypernyms owed to that dataset’s WordNet origins. For example, the person class sub-divides into many professions and hobbies, like firefighter, gymnast, and doctor. To differentiate such sibling classes, learned vectors can also encode lowe"
2020.emnlp-main.703,P10-1040,1,0.164233,"the recent success of deep learning methods. Outside of NLP, philosophy (Austin, 1975) and linguistics (Lakoff, 1973; Coleman and Kay, 1981) recognized that meaning is flexible yet structured. Early experiments on neural networks trained with sequences of words (Elman, 1990; Bengio et al., 2003) suggested that vector representations could capture both syntax and semantics. Subsequent experiments with larger models, documents, and corpora have demonstrated that representations learned from text capture a great deal of information about meaning in and out of context (Collobert and Weston, 2008; Turian et al., 2010; Mikolov et al., 2013; McCann et al., 2017). The intuition of such embedding representations, that context lends meaning, has long been acknowledged (Firth, 1957; Turney and Pantel, 2010). Earlier on, discrete, hierarchical representations, such as agglomerative clustering guided by mutual information (Brown et al., 1992), were constructed with some innate interpretability. A word’s position in such a hierarchy captures semantic and syntactic distinctions. When the Baum–Welch algorithm (Welch, 2003) is applied to unsupervised Hidden Markov Models, it assigns a class distribution to every word"
2020.emnlp-main.703,2020.cl-1.2,0,0.0416453,"Missing"
2020.emnlp-main.703,N19-1364,0,0.123027,"chniques, data collection paradigms, and tasks. We posit that the present success of representation learning approaches trained on large, text-only corpora requires the parallel tradition of research on the broader physical and social context of language to address the deeper questions of communication. Improvements in hardware and data collection have galvanized progress in NLP across many benchmark tasks. Impressive performance has been achieved in language modeling (Radford et al., 2019; Zellers et al., 2019b; Keskar et al., 2019) and span-selection question answering (Devlin et al., 2019; Yang et al., 2019b; Lan et al., 2020) through massive data and massive models. With models exceeding human performance on such tasks, now is an excellent time to reflect on a key question: Where is NLP going? In this paper, we consider how the data and world a language learner is exposed to define and constrains the scope of that learner’s semantics. Meaning does not arise from the statistical distribution of words, but from their use by people to communicate. Many of the assumptions and understandings on which communication relies lie outside of text. We must consider what is missing from models trained solel"
2020.emnlp-main.703,N10-1125,1,\N,Missing
2020.emnlp-main.703,D12-1110,0,\N,Missing
2020.emnlp-main.703,D08-1094,0,\N,Missing
2020.emnlp-main.703,P15-1135,1,\N,Missing
2020.emnlp-main.703,D16-1230,0,\N,Missing
2020.emnlp-main.703,P18-2103,0,\N,Missing
2020.emnlp-main.703,W18-5446,0,\N,Missing
2020.emnlp-main.703,P19-1388,0,\N,Missing
2020.emnlp-main.703,D17-1259,0,\N,Missing
2020.emnlp-main.703,D18-1256,0,\N,Missing
2020.emnlp-main.703,N19-1423,0,\N,Missing
2020.emnlp-main.703,D19-1598,0,\N,Missing
2020.findings-emnlp.273,D14-1181,0,0.0210765,"computer model, considering whether to fry using a fridge is no more ludicrous than considering whether to fry using a plate (which, to an untrained human cook, may be plausible, though is certainly not a good idea). Both actions can be discouraged by negative reinforcement, but a human only needs to learn not to do the latter. Furthermore, a computer player learning that one can chop carrots with a knife may not generalize that one can chop celery the same way, but a human surely will. There is existing work in learning to play text games with RL (Narasimhan et al., 2015; Yuan et al., 2018; Kim, 2014; Zahavy et al., 2018; Yin and May, 2019a; Tessler et al., 2019) but the standard pattern of incorporating large language models such as BERT (Devlin et al., 2019) has not yet been seen in current literature. It turns out that this integration is not trivial. Most models that use BERT and its ilk predominantly apply their results to supervised learning tasks that have training data with ground truth (Zellers et al., 2018; Wang et al., 2018) or at least, in the case of generation-based tasks like dialogue and translation, a corpus of desirable output to mimic (Wolf et al., 2019; Imamura and Sum"
2020.findings-emnlp.273,W02-0109,0,0.345954,"ing P2: [slice potato with knife |drop knife |drink water] Figure 2: The architecture of the DRRN model. Trajectories and actions are encoded by a CNN (in this case) and an LSTM into state and action representations, respectively, followed by a dense layer to compute the Q-values. On the bottom, we show a truncated example of dialogue from a text game in the cooking genre, with S1 and S2 representing the system’s descriptions, and P1 showing the player’s first actions in response to S1. S1 + P1 + S2 is an example of a trajectory. P2 shows a set of admissible actions. word tokenizer from NLTK (Loper and Bird, 2002) since GloVe embeddings are pre-determined and not compatible with BPE. We use a zero vector as the padding token and average of all word embeddings as the unknown token for CNN-GloVe. CNN uses a word embedding size of 64, while for CNNGloVe and BERT, we use the pre-trained word embedding size, i.e., 50 dimensions for CNN-GloVe (we choose this dimension because it is close to our CNN) and 768 for BERT (so does Transformer). 3.2 Action Representations A consequence of learning to play different games is that actions differ from one game to another. Vanilla DQNs, introduced by (Mnih et al., 2015"
2020.findings-emnlp.273,D15-1001,0,0.622749,"zation caused by a lack of commonsense. To a computer model, considering whether to fry using a fridge is no more ludicrous than considering whether to fry using a plate (which, to an untrained human cook, may be plausible, though is certainly not a good idea). Both actions can be discouraged by negative reinforcement, but a human only needs to learn not to do the latter. Furthermore, a computer player learning that one can chop carrots with a knife may not generalize that one can chop celery the same way, but a human surely will. There is existing work in learning to play text games with RL (Narasimhan et al., 2015; Yuan et al., 2018; Kim, 2014; Zahavy et al., 2018; Yin and May, 2019a; Tessler et al., 2019) but the standard pattern of incorporating large language models such as BERT (Devlin et al., 2019) has not yet been seen in current literature. It turns out that this integration is not trivial. Most models that use BERT and its ilk predominantly apply their results to supervised learning tasks that have training data with ground truth (Zellers et al., 2018; Wang et al., 2018) or at least, in the case of generation-based tasks like dialogue and translation, a corpus of desirable output to mimic (Wolf"
2020.findings-emnlp.273,D14-1162,0,0.0905119,"Missing"
2020.findings-emnlp.273,P16-1162,0,0.0475056,"Missing"
2020.findings-emnlp.273,W18-5446,0,0.06074,"Missing"
2020.findings-emnlp.273,D19-1280,0,0.0566779,"Missing"
2020.findings-emnlp.273,D18-1009,0,0.0671083,"not generalize that one can chop celery the same way, but a human surely will. There is existing work in learning to play text games with RL (Narasimhan et al., 2015; Yuan et al., 2018; Kim, 2014; Zahavy et al., 2018; Yin and May, 2019a; Tessler et al., 2019) but the standard pattern of incorporating large language models such as BERT (Devlin et al., 2019) has not yet been seen in current literature. It turns out that this integration is not trivial. Most models that use BERT and its ilk predominantly apply their results to supervised learning tasks that have training data with ground truth (Zellers et al., 2018; Wang et al., 2018) or at least, in the case of generation-based tasks like dialogue and translation, a corpus of desirable output to mimic (Wolf et al., 2019; Imamura and Sumita, 2019). For tasks suited to RL such as the exploration of and interaction with a world, there is no true target or even, initially, a corpus, and thus learning can only proceed iteratively via, e.g., exploration-exploitation (Mnih et al., 2013), which requires millions of training iterations to converge (Yin and May, 2019a; Narasimhan et al., 2017; Mnih et al., 2013). Integrating this process games games response act"
2020.findings-emnlp.352,W19-5301,0,0.043884,"Missing"
2020.findings-emnlp.352,W14-4012,0,0.154765,"Missing"
2020.findings-emnlp.352,D14-1179,0,0.0720199,"Missing"
2020.findings-emnlp.352,W17-3204,0,0.106789,"us on frequent classes while paying relatively less importance to infrequent classes. Frequency-based bias leads to poor recall of infrequent classes (Johnson and Khoshgoftaar, 2019). When a model is used in a domain mismatch scenario, i.e. where test and training set distributions do not match, model performance generally degrades. It is not surprising that frequency-biased classifiers show particular degradation in domain mismatch scenarios, as types that were infrequent in the training distribution and were ignored by the learning algorithm may appear with high frequency in the new domain. Koehn and Knowles (2017) showed empirical evidence of poor generalization of NMT to out-of-domain datasets. 3956 In other classification tasks, where each instance is classified independently, methods such as upsampling infrequent classes and down-sampling frequent classes are used. In NMT, since classification is done within the context of sequences, it is possible to accomplish the objective of balancing by altering sequence lengths. This can be done by choosing the level of subword segmentation (Sennrich et al., 2016). Quantification of Zipfian Imbalance: We use two statistics to quantify the imbalance of a traini"
2020.findings-emnlp.352,P18-1007,0,0.0256196,"d limitations are yet to be theoretically understood. 6.2 BPE Subwords Sennrich et al. (2016) introduce BPE as a simplified way to solve out-of-vocabulary (OOV) words without having to use a back-off dictionary for OOV words. They note that BPE improves the translation of not only the OOV words, but also some rare invocabulary words. The analysis by Morishita et al. (2018) is different than ours in that they view various vocabulary sizes as hierarchical features that are used in addition to a fixed vocabulary. Salesky et al. (2018) offer an efficient way to search BPE vocabulary size for NMT. Kudo (2018) use BPE as a regularization technique by introducing sampling based randomness to the BPE segmentation. To the best of our knowledge, no previous work exists that analyzes BPE’s effect on class imbalance. 6.3 Class Imbalance The class imbalance problem has been extensively studied in classical ML (Japkowicz and Stephen, 2002). In the medical domain Mazurowski et al. (2008) find that classifier performance deteriorates with even modest imbalance in the training data. Untreated class imbalance has been known to deteriorate the performance of image segmentation. Sudre et al. (2017) investigate t"
2020.findings-emnlp.352,L18-1548,0,0.0569952,"Missing"
2020.findings-emnlp.352,D18-1336,0,0.0336723,"Missing"
2020.findings-emnlp.352,D15-1166,0,0.0608679,"$EN of 0.5M and is provided in Appendix B along with visualizations on validation sets. The vocabulary sizes that achieved highest BLEU are indicated with dashed vertical lines, and the vocabulary our heuristic selects is indicated by dotted vertical lines. recall and class rank (⇢F,R ), indicating the poorer recall of low-frequency classes. 6 6.1 Related Work NMT Architectures Several variations of NMT models have been proposed and refined: Sutskever et al. (2014) and Cho et al. (2014b) introduce the RNN-based encoderdecoder model. Bahdanau et al. (2015) introduce the attention mechanism and Luong et al. (2015) propose several variations that became essential components of many future models. RNN modules, either LSTM (Hochreiter and Schmidhuber, 1997) or GRU (Cho et al., 2014a), have been popular choices for composing NMT encoders and decoders. The encoder uses bidirectional information, but the decoder is unidirectional, typically left-to-right, to facilitate autoregressive generation. Gehring et al. (2017) use a CNN architecture that outperforms RNN models. Vaswani et al. (2017) propose the Transformer, whose main components are feed-forward and attention networks. There are only a few models that"
2020.findings-emnlp.352,P11-1015,0,0.00671015,"a classification task in an autoregressive setting and analyze the limitations of both classification and autoregression components. Classifiers are known to perform better with balanced class distributions during training. Since the Zipfian nature of languages causes imbalanced classes, we explore its effect on NMT. We analyze the effect of various vocabulary sizes on NMT performance on multiple languages with many data sizes, and reveal an explanation for why certain vocabulary sizes are better than others.1 1 Introduction Natural language processing (NLP) tasks such as sentiment analysis (Maas et al., 2011; Zhang et al., 2015) and spam detection are modeled as classification tasks, where instances are independently labeled. Tasks such as part-of-speech tagging (Zeman et al., 2017) and named entity recognition (Tjong Kim Sang and De Meulder, 2003) are examples of structured classification tasks, where instance classification is decomposed into a sequence of per-token contextualized labels. We can similarly cast neural machine translation (NMT), an example of a natural language generation (NLG) task, as a form of structured classification, where an instance label (a translation) is generated as a"
2020.findings-emnlp.352,C18-1052,0,0.0190597,"ank. Reduction in class imbalance (D), as shown by the horizontal axis, generally reduces the bias as indicated by the reduction in magnitude of correlation. viewed as token classifiers with a different kind of feature extractor, whose strengths and limitations are yet to be theoretically understood. 6.2 BPE Subwords Sennrich et al. (2016) introduce BPE as a simplified way to solve out-of-vocabulary (OOV) words without having to use a back-off dictionary for OOV words. They note that BPE improves the translation of not only the OOV words, but also some rare invocabulary words. The analysis by Morishita et al. (2018) is different than ours in that they view various vocabulary sizes as hierarchical features that are used in addition to a fixed vocabulary. Salesky et al. (2018) offer an efficient way to search BPE vocabulary size for NMT. Kudo (2018) use BPE as a regularization technique by introducing sampling based randomness to the BPE segmentation. To the best of our knowledge, no previous work exists that analyzes BPE’s effect on class imbalance. 6.3 Class Imbalance The class imbalance problem has been extensively studied in classical ML (Japkowicz and Stephen, 2002). In the medical domain Mazurowski e"
2020.findings-emnlp.352,W18-6319,0,0.0215143,"Missing"
2020.findings-emnlp.352,E17-2025,0,0.0170388,"age the last 10 checkpoints, and use a beam size of 4 with length penalty of 0.6, similar to Vaswani et al. (2017). Since the vocabulary size hyperparameter is the focus of this analysis, we use a range of vocabulary sizes that include character vocabulary and BPE operations that yield vocabulary sizes between 500 and 64K types. A common practice, as seen in Vaswani et al. (2017)’s setup, is to jointly learn BPE for both source and target languages, which facilitates three-way weight sharing between the encoder’s input, the decoder’s input, and the output (i.e. classifier’s class) embeddings (Press and Wolf, 2017). However, to facilitate fine-grained analysis of vocabulary sizes and their effect on class imbalance, our models separately learn source and target vocabularies; weight sharing between the encoder’s 4 5 http://www.statmt.org/wmt19/translation-task.html 3958 https://github.com/alvations/sacremoses https://github.com/anoopkunchukuttan/indic nlp library Languages DE!EN EN!DE EN!HI EN!LT Training Sentences 30K Europarl v10 0.5M WMT13CommonCrawl 1M NewsCommentary v14 4.5M 0.5M IITB Training 1.3M Europarl v10 0.6M EN Toks 0.8M 12.9M 25.7M 116M 8M 21M 17M XX Toks 0.8M 12.2M 24.3M 109.8M 8.6M 22.5M"
2020.findings-emnlp.352,P16-1162,0,0.775747,"the finite set used for training. A sequence drawn from a Zipfian distribution is likely to have a large number of rare types, and these are likely to have not been seen in training. 2. Imbalanced Classes: There are a few extremely frequent types and many infrequent types, causing an extreme imbalance. Such an imbalance, in other domains where classifiers are used, has been known to cause undesired biases and severe performance degradation (Johnson and Khoshgoftaar, 2019). The use of subwords, that is, decomposition of word types into pieces, such as the widely used Byte Pair Encoding (BPE) (Sennrich et al., 2016) addresses the open-ended vocabulary problem by ultimately allowing a word to be represented as a sequence of characters if necessary. BPE has a single hyperparameter named merge operations that governs the vocabulary size. The effect of this hyperparameter is not well understood. In practice, it is either chosen arbitrarily or via trial-and-error (Salesky et al., 2018). 3955 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3955–3964 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Regarding the problem of imbalanced classes, Steedman (2008)"
2020.lrec-1.243,P15-1017,0,0.0888802,"for the nodes (words). We train a BiAffine Dependency Parser (Dozat and Manning, 2016) for a particular language using the Universal Dependency treebanks (Nivre et al., 2016), and then apply the dependency parser to sentences to obtain universal dependency trees. For fully connected graphs, we regard each token in a sentence as a node in the graph and there’s an edge between each pair of nodes. Then we apply Tree-LSTM encoder and Transformer encoder to generate word representations in the latent space, respectively. Tree-LSTM Encoder. We exploit the Child-Sum TreeLSTMs proposed by Tai et al. (2015). In contrast to the standard LSTM, here the memory cell updates of the TreeLSTM unit are dependent on the states of all children units. The Tree-LSTM unit selectively incorporates information from each child. Transformer Encoder. Our multi-layer bidirectional Transformer encoder is based on the architecture proposed in Vaswani et al. (2017), composed of a stack of N identical layers, where each layer has a multi-head self-attention sub-layer and a position-wise feed-forward sub-layer. Our implementation is identical to the original, except that here, crucially, we do not include positional en"
2020.lrec-1.243,doddington-etal-2004-automatic,0,0.27252,"Missing"
2020.lrec-1.243,L18-1245,0,0.141208,"n Extraction (IE) that aims to identify event triggers and arguments from unstructured texts and classify them into predefined categories. Compared to other IE tasks such as name tagging, the annotations for Event Extraction are more costly because they are structured and require a rich label space; full event structure annotation consists of its trigger span and type label as well as each of its one or more argument spans and role labels. Publicly available annotations for event extraction exist for only a few languages, such as English, Spanish, Chinese, and Arabic (Doddington et al., 2004; Getman et al., 2018). We propose a novel shareand-transfer framework to project training data for English only and test data for zero-event-resource languages into one common semantic space, so that we can train an event extractor on English annotations and apply it to target languages. Currently most successful cross-lingual transfer approaches for IE are limited to sequence labeling (Feng et al., 2018a; Xie et al., 2018; Zhang et al., 2018a; Lin et al., 2018). In contrast, event extraction requires transferring complex graph structures that contain triggers and arguments. For example, in Figure 1, the words fir"
2020.lrec-1.243,C16-1114,0,0.538714,"l., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b). These approaches incorporate languagespecific information, and thus require a substantial amount of annotations when adapted to a new language. Traditional multilingual approaches (Li et al., 2012; Wei et Figure 4: Visualization of Attention Weights for the First head of the Multi-head Attention Sublayer. al., 2017) to event extraction were all based on feature engineering. Recently, (Agerri et al., 2016; Danilova et al., 2014; Feng et al., 2018b) demonstrate methods for building multilingual event extraction systems. Hsi et al. (2016) have used language-independent features for event extraction for low-resource languages. (Lu and Nguyen, 2018) show that word sense disambiguation helps event detection via neural representation matching. (Liu et al., 2018a; Zhang et al., 2018b) propose event extraction by attention mechanism, e.g. the former use a gated multi-lingual attention technique. To the best of our knowledge, this is the first work to design a cross-lingual structure transfer framework to enable event extraction for a language without any event training data. 5. Conclusions and Future Work In this paper, we propose a"
2020.lrec-1.243,P08-1030,1,0.911889,"sonnel Business Manufacture 2,613 346 539 453 2,411 301 53 52 1,956 207 116 86 1,688 275 651 225 1,340 146 58 43 279 0 14 8 158 58 9 5 Table 2: Distribution of event types in various datasets (Number of event mentions). The statistics for English are from the training split, and the statistics for Spanish, Russian and Ukrainian are from testing splits. Hyperparameter Value word embedding size 300 hidden dimension size 768 filter size 768 number of head 12 number of layer 12 dropout 0.2 learning rate 0.003 batch size 16 of event types for each language. We follow the criteria in previous work (Ji and Grishman, 2008; Li et al., 2013) for evaluation. 3.2. Training Details Treebanks. We use the Version 2.3 treebanks released by Universal Dependencies 3 to train the dependency parsers. Tokenization. We use Spacy tokenization (Honnibal and Montani, 2017) for English and Spanish and the NLTK toktok tokenizer (Dehdari, 2014) for Russian and Ukrainian. Word Embedding. We use multilingual word embeddings released by Facebook Research (Lample et al., 2017) 4 . The algorithm aligns word embeddings of various languages, which are pre-trained from Wikipedia articles (Joulin et al., 2016) 5 , in a single vector space"
2020.lrec-1.243,D12-1092,0,0.0290845,"large number of supervised machine learning techniques have been used for event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b). These approaches incorporate languagespecific information, and thus require a substantial amount of annotations when adapted to a new language. Traditional multilingual approaches (Li et al., 2012; Wei et Figure 4: Visualization of Attention Weights for the First head of the Multi-head Attention Sublayer. al., 2017) to event extraction were all based on feature engineering. Recently, (Agerri et al., 2016; Danilova et al., 2014; Feng et al., 2018b) demonstrate methods for building multilingual event extraction systems. Hsi et al. (2016) have used language-independent features for event extraction for low-resource languages. (Lu and Nguyen, 2018) show that word sense disambiguation helps event detection via neural representation matching. (Liu et al., 2018a; Zhang et al., 2018b) propose"
2020.lrec-1.243,P13-1008,1,0.885221,"ture 2,613 346 539 453 2,411 301 53 52 1,956 207 116 86 1,688 275 651 225 1,340 146 58 43 279 0 14 8 158 58 9 5 Table 2: Distribution of event types in various datasets (Number of event mentions). The statistics for English are from the training split, and the statistics for Spanish, Russian and Ukrainian are from testing splits. Hyperparameter Value word embedding size 300 hidden dimension size 768 filter size 768 number of head 12 number of layer 12 dropout 0.2 learning rate 0.003 batch size 16 of event types for each language. We follow the criteria in previous work (Ji and Grishman, 2008; Li et al., 2013) for evaluation. 3.2. Training Details Treebanks. We use the Version 2.3 treebanks released by Universal Dependencies 3 to train the dependency parsers. Tokenization. We use Spacy tokenization (Honnibal and Montani, 2017) for English and Spanish and the NLTK toktok tokenizer (Dehdari, 2014) for Russian and Ukrainian. Word Embedding. We use multilingual word embeddings released by Facebook Research (Lample et al., 2017) 4 . The algorithm aligns word embeddings of various languages, which are pre-trained from Wikipedia articles (Joulin et al., 2016) 5 , in a single vector space. It learns a mapp"
2020.lrec-1.243,D14-1198,1,0.831152,"when the model generates the representation for the word “ранение (wound)”, “спину (back)” also contributes besides the word “ранен” itself. And “спину (back)” is an argument of the event mention triggered by “ранение (wound)” here. It indicates that the model successfully transfers structural information from the source language to the target language. 4. Related Work A large number of supervised machine learning techniques have been used for event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b). These approaches incorporate languagespecific information, and thus require a substantial amount of annotations when adapted to a new language. Traditional multilingual approaches (Li et al., 2012; Wei et Figure 4: Visualization of Attention Weights for the First head of the Multi-head Attention Sublayer. al., 2017) to event extraction were all based on feature engineering. Recently, (Agerri et al., 2016; Danilova et al"
2020.lrec-1.243,R11-1002,0,0.0219009,"visualized attention weights, we can clearly see when the model generates the representation for the word “ранение (wound)”, “спину (back)” also contributes besides the word “ранен” itself. And “спину (back)” is an argument of the event mention triggered by “ранение (wound)” here. It indicates that the model successfully transfers structural information from the source language to the target language. 4. Related Work A large number of supervised machine learning techniques have been used for event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b). These approaches incorporate languagespecific information, and thus require a substantial amount of annotations when adapted to a new language. Traditional multilingual approaches (Li et al., 2012; Wei et Figure 4: Visualization of Attention Weights for the First head of the Multi-head Attention Sublayer. al., 2017) to event extraction were all based on feature engineering. Recen"
2020.lrec-1.243,P18-1074,1,0.785986,"icly available annotations for event extraction exist for only a few languages, such as English, Spanish, Chinese, and Arabic (Doddington et al., 2004; Getman et al., 2018). We propose a novel shareand-transfer framework to project training data for English only and test data for zero-event-resource languages into one common semantic space, so that we can train an event extractor on English annotations and apply it to target languages. Currently most successful cross-lingual transfer approaches for IE are limited to sequence labeling (Feng et al., 2018a; Xie et al., 2018; Zhang et al., 2018a; Lin et al., 2018). In contrast, event extraction requires transferring complex graph structures that contain triggers and arguments. For example, in Figure 1, the words fire/ fired combined with different arguments indicate different event types. A transfer approach to IE with a typical sequence-based Long Short Term Memory (LSTM) encoder will incorporate languagespecific characteristics, such as word order, into word representations, reducing its effectiveness in transfer between two languages with quite different word orders. In this paper, we explore cross-lingual event transfer learning in a zero-resource"
2020.lrec-1.243,D18-1156,0,0.0860644,"nt of the event mention triggered by “ранение (wound)” here. It indicates that the model successfully transfers structural information from the source language to the target language. 4. Related Work A large number of supervised machine learning techniques have been used for event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b). These approaches incorporate languagespecific information, and thus require a substantial amount of annotations when adapted to a new language. Traditional multilingual approaches (Li et al., 2012; Wei et Figure 4: Visualization of Attention Weights for the First head of the Multi-head Attention Sublayer. al., 2017) to event extraction were all based on feature engineering. Recently, (Agerri et al., 2016; Danilova et al., 2014; Feng et al., 2018b) demonstrate methods for building multilingual event extraction systems. Hsi et al. (2016) have used language-independent features for event extr"
2020.lrec-1.243,D18-1517,0,0.0123448,"guagespecific information, and thus require a substantial amount of annotations when adapted to a new language. Traditional multilingual approaches (Li et al., 2012; Wei et Figure 4: Visualization of Attention Weights for the First head of the Multi-head Attention Sublayer. al., 2017) to event extraction were all based on feature engineering. Recently, (Agerri et al., 2016; Danilova et al., 2014; Feng et al., 2018b) demonstrate methods for building multilingual event extraction systems. Hsi et al. (2016) have used language-independent features for event extraction for low-resource languages. (Lu and Nguyen, 2018) show that word sense disambiguation helps event detection via neural representation matching. (Liu et al., 2018a; Zhang et al., 2018b) propose event extraction by attention mechanism, e.g. the former use a gated multi-lingual attention technique. To the best of our knowledge, this is the first work to design a cross-lingual structure transfer framework to enable event extraction for a language without any event training data. 5. Conclusions and Future Work In this paper, we propose a novel cross-lingual structure transfer framework for zero-resource event extraction. Experiments on three lang"
2020.lrec-1.243,P15-2060,0,0.107531,"пину (back)” also contributes besides the word “ранен” itself. And “спину (back)” is an argument of the event mention triggered by “ранение (wound)” here. It indicates that the model successfully transfers structural information from the source language to the target language. 4. Related Work A large number of supervised machine learning techniques have been used for event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b). These approaches incorporate languagespecific information, and thus require a substantial amount of annotations when adapted to a new language. Traditional multilingual approaches (Li et al., 2012; Wei et Figure 4: Visualization of Attention Weights for the First head of the Multi-head Attention Sublayer. al., 2017) to event extraction were all based on feature engineering. Recently, (Agerri et al., 2016; Danilova et al., 2014; Feng et al., 2018b) demonstrate methods for building multilingual event extract"
2020.lrec-1.243,N16-1034,0,0.0723032,"es besides the word “ранен” itself. And “спину (back)” is an argument of the event mention triggered by “ранение (wound)” here. It indicates that the model successfully transfers structural information from the source language to the target language. 4. Related Work A large number of supervised machine learning techniques have been used for event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b). These approaches incorporate languagespecific information, and thus require a substantial amount of annotations when adapted to a new language. Traditional multilingual approaches (Li et al., 2012; Wei et Figure 4: Visualization of Attention Weights for the First head of the Multi-head Attention Sublayer. al., 2017) to event extraction were all based on feature engineering. Recently, (Agerri et al., 2016; Danilova et al., 2014; Feng et al., 2018b) demonstrate methods for building multilingual event extraction systems. Hsi et a"
2020.lrec-1.243,L16-1262,0,0.0919272,"Missing"
2020.lrec-1.243,P15-1150,0,0.122428,"Missing"
2020.lrec-1.243,P17-2046,0,0.0487613,"Missing"
2020.lrec-1.243,D18-1034,0,0.0245405,"re argument spans and role labels. Publicly available annotations for event extraction exist for only a few languages, such as English, Spanish, Chinese, and Arabic (Doddington et al., 2004; Getman et al., 2018). We propose a novel shareand-transfer framework to project training data for English only and test data for zero-event-resource languages into one common semantic space, so that we can train an event extractor on English annotations and apply it to target languages. Currently most successful cross-lingual transfer approaches for IE are limited to sequence labeling (Feng et al., 2018a; Xie et al., 2018; Zhang et al., 2018a; Lin et al., 2018). In contrast, event extraction requires transferring complex graph structures that contain triggers and arguments. For example, in Figure 1, the words fire/ fired combined with different arguments indicate different event types. A transfer approach to IE with a typical sequence-based Long Short Term Memory (LSTM) encoder will incorporate languagespecific characteristics, such as word order, into word representations, reducing its effectiveness in transfer between two languages with quite different word orders. In this paper, we explore cross-lingual eve"
2020.lrec-1.243,N16-1033,0,0.0142582,"nerates the representation for the word “ранение (wound)”, “спину (back)” also contributes besides the word “ранен” itself. And “спину (back)” is an argument of the event mention triggered by “ранение (wound)” here. It indicates that the model successfully transfers structural information from the source language to the target language. 4. Related Work A large number of supervised machine learning techniques have been used for event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b). These approaches incorporate languagespecific information, and thus require a substantial amount of annotations when adapted to a new language. Traditional multilingual approaches (Li et al., 2012; Wei et Figure 4: Visualization of Attention Weights for the First head of the Multi-head Attention Sublayer. al., 2017) to event extraction were all based on feature engineering. Recently, (Agerri et al., 2016; Danilova et al., 2014; Feng et al., 2018"
2020.lrec-1.243,N18-5009,1,0.761275,"and role labels. Publicly available annotations for event extraction exist for only a few languages, such as English, Spanish, Chinese, and Arabic (Doddington et al., 2004; Getman et al., 2018). We propose a novel shareand-transfer framework to project training data for English only and test data for zero-event-resource languages into one common semantic space, so that we can train an event extractor on English annotations and apply it to target languages. Currently most successful cross-lingual transfer approaches for IE are limited to sequence labeling (Feng et al., 2018a; Xie et al., 2018; Zhang et al., 2018a; Lin et al., 2018). In contrast, event extraction requires transferring complex graph structures that contain triggers and arguments. For example, in Figure 1, the words fire/ fired combined with different arguments indicate different event types. A transfer approach to IE with a typical sequence-based Long Short Term Memory (LSTM) encoder will incorporate languagespecific characteristics, such as word order, into word representations, reducing its effectiveness in transfer between two languages with quite different word orders. In this paper, we explore cross-lingual event transfer learning"
2020.nlpcovid19-acl.4,P07-1056,0,0.36466,"Missing"
2020.nlpcovid19-acl.4,D19-1041,1,0.827068,"0.63 Past 0.08 0.37 Table 4: Percentage of sentences in tweets and policy that are in past or present/future tense. of their sentences, as shown in Table 45 . While lemmatization should mitigate that difference, it does not measurably improve the accuracy of our classifiers. Additionally, for the LogisticRegression classifier, we experimented with different vocabulary thresholds before choosing an optimal set, based on performance. Another set of features we consider are extracted events: we extract event arguments – agents and patients – and anchors using a BERT + BiLSTM neural architecture (Han et al., 2019), as well as the lemmatized version of these extracted events. We consider event-extraction after observing that tweets are significantly more likely to contain opinionated text – policy text has a median subjectivity of .23 while tweet text has a median subjectivity of .33.6 We hypothesize event-extraction can help transfer accuracy by abstracting the content of tweets from opinions. 3 3.1 Methodology Classification We test two classifiers: Logistic Regression on a TF-IDF normalized7 bag-of-words representation of each input document, and a pretrained RoBERTa-base model. We use Logistic Regre"
2020.nlpcovid19-acl.4,2020.emnlp-demos.2,0,0.0537572,"th views used for co-training. View #1 is the extracted events, View #2 is the text with event words removed. The labels generated from one view are iteratively added to another view’s training set. Then, the full text along with all added labels are used to train the final classifier. Not shown but tested as baseline views are: “noun-phrases”, “verb-phrases”, “random words”. RoBERTa-base pretrained model8 for our classification task, although we acknowledge that a more corpus-specific pretraining, like a Twitterspecific pretraining or a law-specific pretraining might achieve higher accuracy (Nguyen et al., 2020). 3.2 Co-Training Additionally, we test co-training as a method for increasing transfer accuracy. As formulated by Blum and Mitchell (1998), co-training is a method for increasing the accuracy of a classifier by using labels generated by other classifiers with different “views” of the data (i.e. non-overlapping feature sets). Previous work has found co-training advantageous in transfer learning tasks (Wan, 2009). The views we use are shown in Table 5. For View #1, we extract events using the method as described in Section 3.1, and for View #2, we leave the text without events as the other view"
2020.nlpcovid19-acl.4,W01-0501,0,0.299778,"his suggests that a more domain-specific pretrained model, like a Twitterspecific RoBERTa (Nguyen et al., 2020), might have even greater benefits from co-training, but we leave that to future work. We also leave to future work an exploration of further views that could increase co-training accuracy. Further engineering tricks, such as selective lemmatizing, might perform well as views. However, as indicated in Figure 4, event-extraction is a particularly useful view for imparting signal, relative to the baseline views we considered – baselines which have been, in fact, used in the literature (Pierce and Cardie, 2001). While it’s not immediately clear why this is the case, we hypothesize that extracting events as one view gives us the clearest conditional independence of views, which is necessary for co-training to be effective. Interestingly, both the “noun-phrase” baseline and the “verb-phrase” baseline degrade in performance over time for the transfer task (Figure 4b). It may be that these two tasks separate the views similarly: i.e., what is left over when extracting noun-phrases is mostly verb-phrases, and vice-versa. As shown in (Nigam and Ghani, 2000), if the accuracy of even the most confident co-t"
2020.nlpcovid19-acl.4,P09-1027,0,0.147512,"Missing"
2021.acl-long.381,2020.acl-main.421,0,0.0334573,"e AutoPrompt, we perform gradient-based optimization in the space of word embeddings which gives our model more degrees of freedom and eventually better performance on the downstream tasks (Section 6.2). In a more general sense, guiding an NLP model with special tokens appended to the input is an even older idea. In particular, multilingual neural machine translation models use special tokens in the input to control the target language (Ha et al., 2016; Johnson et al., 2017) or politeness of the translation (Sennrich et al., 2016). Another method to reprogram a BERT-based model is proposed by Artetxe et al. (2020), where a model tuned on an English version of a particular task is transformed to work in another language by changing only the embedding matrices. In parallel work, Li and Liang (2021) propose a similar method and successfully apply it on two text generation tasks. Apart from the different types of tasks and our characterization of the task as a form of Adversarial Reprogramming, the main difference between their approach and ours is that they use an additional parameterization trick to stabilize the training. 3 WARP We follow a setup similar to Elsayed et al. (2019) with some NLP-specific m"
2021.acl-long.381,S17-2001,0,0.0217323,"es, but the [MASK] token is always put between the sentences. For MNLI, we use matched accuracy as a validation metric and use the same model for the mismatched version. In our few-shot attempt for the RTE task, we use a different training and evaluation setup discussed in Section 5.2. QQP (Quora Question Pairs4 ) and MRPC (Microsoft Research Paraphrase Corpus, Dolan and Brockett, 2005) follow the same prompt pattern as NLI tasks. As a validation metric F1 score is used. STS-B (Semantic Textual Similarity Bench4 https://www.quora.com/q/quoradata/First-QuoraDataset-Release-Question-Pairs mark, Cer et al., 2017), unlike the other tasks in the benchmark, is formulated as a regression task. The prompt pattern is the same, but instead of introducing new embeddings for [V 1], [V 2], ..., [V C] verbalizer tokens, we add a regression head to the last hidden state of MLM head and use Mean Squares Error optimization objective, similar to (Liu et al., 2019). Pearson Correlation is used as the validation metric. During inference, we clip the scores within [1, 5]. We follow Liu et al. and train models for MRPC, STS-B, and RTE tasks initialized with the parameters from the best MNLI model but do not apply any ta"
2021.acl-long.381,2020.emnlp-main.20,0,0.028408,"al. and train models for MRPC, STS-B, and RTE tasks initialized with the parameters from the best MNLI model but do not apply any task-specific tricks to WNLI (Winograd Schema Challenge NLI, Levesque et al., 2011) and always predict the majority label. 4.2 Results Table 1 presents the results on the test set obtained from the GLUE evaluation server. Besides our best WARP models, we also include the human baselines, current state-of-the-art model (He et al., 2020), the regular fine-tuned pretrained model we use, and also include relatively small language models, including (Jiao et al., 2020), (Clark et al., 2020), (Houlsby et al., 2019). With the GLUE Score, WARP outperforms all the models that train less than 25 million parameters on the leaderboard. We explain the relatively strong WARP results on textual entailment tasks by the easier reformulation of such tasks. Likewise, we explain the relatively weak performance on CoLA by the difficulties of reformulating the 4925 train size Fine-Tuning Adapters Linear Classifier WARP0 WARP1 WARP2 WARP4 WARP8 WARPinit WARP20 WARPMNLI MNLI 392702 90.2 90.4 64.2 70.9 83.9 85.4 86.9 87.6 86.8 88.2 QNLI 104743 94.7 94.7 78.1 78.8 87.6 88.0 92.4 93.0 90.4 93.5 QQP 3"
2021.acl-long.381,W07-1401,0,0.0377134,"token after the sentence, and the trainable prompt tokens are both appended and prepended to the sentence. CoLA (Corpus of Linguistic Acceptability, Warstadt et al., 2019) is a single sentence classification task as well, so we treat both the same way with the only difference that as a validation metric we use accuracy for SST-2, and Matthew’s correlation for CoLA. MNLI (MultiNLI, Multi-Genre Natural Language Inference, Williams et al., 2018), QNLI (Question Natural Language Inference, Rajpurkar et al., 2016) and RTE (Recognizing Textual Entailment, Dagan et al., 2006; Bar Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009) are sentence pair classification tasks. Similar to Schick and Sch¨utze (2021a), we may have prompt tokens before, after and between the two sentences, but the [MASK] token is always put between the sentences. For MNLI, we use matched accuracy as a validation metric and use the same model for the mismatched version. In our few-shot attempt for the RTE task, we use a different training and evaluation setup discussed in Section 5.2. QQP (Quora Question Pairs4 ) and MRPC (Microsoft Research Paraphrase Corpus, Dolan and Brockett, 2005) follow the same prompt pattern as NL"
2021.acl-long.381,2020.findings-emnlp.372,0,0.145161,"e on the GLUE Leaderboard, outperforming all the other submissions that use up to three orders of magnitude more trainable parameters. We show that it is possible to inject knowledge into WARP models using manually designed initialization of the prompt, which is especially useful on tasks with a small number of examples. Moreover, WARP shows impressive few-shot performance on two tasks from the SuperGLUE benchmark with just 32 examples, outperforming GPT-3 results. Finally, we discuss the advantages of our method in real-life applications. 2 2.1 Related Work Towards Fewer Trainable Parameters Jiao et al. (2020) show that knowledge distillation may help reduce the size of their model 7.5 times while almost preserving the performance, but finetuning such models still requires storage of separate task-specific models. As seen in Section 6, this approach does not scale when we want to apply it to many tasks at once. Another approach, called Adapters (Houlsby et al., 2019; Pfeiffer et al., 2021), introduces new task-specific parameters that are added at every layer of the Transformer network. Only these newly initialized weights are trained, which allows separation of general and task-specific knowledge."
2021.acl-long.381,2021.ccl-1.108,0,0.0859599,"Missing"
2021.acl-long.381,D19-1525,0,0.0215845,"Figure 1. Then the perturbation parameter is trained to optimize the target classification task objective using the annotated image data. While in the case of image classification it is not obvious why adversarial reprogramming should ever work, e.g. why a network trained on ImageNet should have the capacity to solve MNIST when surrounded with a particular bitmap, for NLP tasks, there is more intuition. Many NLP tasks can be reformulated as language models, a shared space for both program and data. Adversarial reprogramming has been adapted to text classification tasks with LSTM networks in (Neekhara et al., 2019). They operate in the vocabulary space and reprogram a model trained for one task to perform another task. More recently, AutoPrompt (Shin et al., 2020a) attempts to find prompts for large language models automatically without adding any parameters to the model. Unlike AutoPrompt, we perform gradient-based optimization in the space of word embeddings which gives our model more degrees of freedom and eventually better performance on the downstream tasks (Section 6.2). In a more general sense, guiding an NLP model with special tokens appended to the input is an even older idea. In particular, mu"
2021.acl-long.381,N18-1202,0,0.0139256,"specific word embeddings that, when concatenated to the input text, instruct the language model to solve the specified task. Using up to 25K trainable parameters per task, this approach outperforms all existing methods with up to 25M trainable parameters on the public leaderboard of the GLUE benchmark. Our method, initialized with task-specific human-readable prompts, also works in a few-shot setting, outperforming GPT-3 on two SuperGLUE tasks with just 32 training samples. 1 Introduction Language model pretraining has had a tremendous impact on solving many natural language processing tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019). The most popular two approaches take a pretrained model and use a straightforward supervised learning objective. In the first approach, the parameters of the language model are frozen and a task-specific head is trained on top of them (Peters et al., 2018). The second approach fine-tunes all model parameters (Radford et al., 2018). The latter can sometimes yield better results (Peters et al., 2019), while the first one usually offers better stability for smaller datasets. The approach based on frozen features does not require stor"
2021.acl-long.381,W19-4302,0,0.0358372,"Missing"
2021.acl-long.381,2021.eacl-main.39,0,0.169973,"Missing"
2021.acl-long.381,2021.acl-long.353,0,0.238745,"Missing"
2021.acl-long.381,D16-1264,0,0.00951095,"ebank, Socher et al., 2013) is a single sentence binary classification task. For the prompt, we put a [MASK] token after the sentence, and the trainable prompt tokens are both appended and prepended to the sentence. CoLA (Corpus of Linguistic Acceptability, Warstadt et al., 2019) is a single sentence classification task as well, so we treat both the same way with the only difference that as a validation metric we use accuracy for SST-2, and Matthew’s correlation for CoLA. MNLI (MultiNLI, Multi-Genre Natural Language Inference, Williams et al., 2018), QNLI (Question Natural Language Inference, Rajpurkar et al., 2016) and RTE (Recognizing Textual Entailment, Dagan et al., 2006; Bar Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009) are sentence pair classification tasks. Similar to Schick and Sch¨utze (2021a), we may have prompt tokens before, after and between the two sentences, but the [MASK] token is always put between the sentences. For MNLI, we use matched accuracy as a validation metric and use the same model for the mismatched version. In our few-shot attempt for the RTE task, we use a different training and evaluation setup discussed in Section 5.2. QQP (Quora Question Pairs4 ) a"
2021.acl-long.381,2021.eacl-main.20,0,0.219805,"Missing"
2021.acl-long.381,2021.naacl-main.185,0,0.210297,"Missing"
2021.acl-long.381,N16-1005,0,0.0277247,"large language models automatically without adding any parameters to the model. Unlike AutoPrompt, we perform gradient-based optimization in the space of word embeddings which gives our model more degrees of freedom and eventually better performance on the downstream tasks (Section 6.2). In a more general sense, guiding an NLP model with special tokens appended to the input is an even older idea. In particular, multilingual neural machine translation models use special tokens in the input to control the target language (Ha et al., 2016; Johnson et al., 2017) or politeness of the translation (Sennrich et al., 2016). Another method to reprogram a BERT-based model is proposed by Artetxe et al. (2020), where a model tuned on an English version of a particular task is transformed to work in another language by changing only the embedding matrices. In parallel work, Li and Liang (2021) propose a similar method and successfully apply it on two text generation tasks. Apart from the different types of tasks and our characterization of the task as a form of Adversarial Reprogramming, the main difference between their approach and ours is that they use an additional parameterization trick to stabilize the trainin"
2021.acl-long.381,2020.emnlp-main.346,0,0.579852,"of image classification it is not obvious why adversarial reprogramming should ever work, e.g. why a network trained on ImageNet should have the capacity to solve MNIST when surrounded with a particular bitmap, for NLP tasks, there is more intuition. Many NLP tasks can be reformulated as language models, a shared space for both program and data. Adversarial reprogramming has been adapted to text classification tasks with LSTM networks in (Neekhara et al., 2019). They operate in the vocabulary space and reprogram a model trained for one task to perform another task. More recently, AutoPrompt (Shin et al., 2020a) attempts to find prompts for large language models automatically without adding any parameters to the model. Unlike AutoPrompt, we perform gradient-based optimization in the space of word embeddings which gives our model more degrees of freedom and eventually better performance on the downstream tasks (Section 6.2). In a more general sense, guiding an NLP model with special tokens appended to the input is an even older idea. In particular, multilingual neural machine translation models use special tokens in the input to control the target language (Ha et al., 2016; Johnson et al., 2017) or"
2021.acl-long.381,D13-1170,0,0.00581696,"tion server. The subscript next to TinyBERT corresponds to the number of layers in the model. WARP for RTE, STS-B and MRPC are intialized from the MNLI parameters. Results for WNLI are not shown, although they are counted in the averaged GLUE score (AVG column). The last column # shows the number of trainable parameters. WARP’s average performance is higher than all models with up to three orders of magnitude more trainable parameters. Fully fine-tuned RoBERTa and the current state-of-the-art method (DeBERT) score higher by 6.5 and 9.2 points, respectively. SST-2 (Sentence Sentiment Treebank, Socher et al., 2013) is a single sentence binary classification task. For the prompt, we put a [MASK] token after the sentence, and the trainable prompt tokens are both appended and prepended to the sentence. CoLA (Corpus of Linguistic Acceptability, Warstadt et al., 2019) is a single sentence classification task as well, so we treat both the same way with the only difference that as a validation metric we use accuracy for SST-2, and Matthew’s correlation for CoLA. MNLI (MultiNLI, Multi-Genre Natural Language Inference, Williams et al., 2018), QNLI (Question Natural Language Inference, Rajpurkar et al., 2016) and"
2021.acl-long.381,Q19-1040,0,0.0223638,"VG column). The last column # shows the number of trainable parameters. WARP’s average performance is higher than all models with up to three orders of magnitude more trainable parameters. Fully fine-tuned RoBERTa and the current state-of-the-art method (DeBERT) score higher by 6.5 and 9.2 points, respectively. SST-2 (Sentence Sentiment Treebank, Socher et al., 2013) is a single sentence binary classification task. For the prompt, we put a [MASK] token after the sentence, and the trainable prompt tokens are both appended and prepended to the sentence. CoLA (Corpus of Linguistic Acceptability, Warstadt et al., 2019) is a single sentence classification task as well, so we treat both the same way with the only difference that as a validation metric we use accuracy for SST-2, and Matthew’s correlation for CoLA. MNLI (MultiNLI, Multi-Genre Natural Language Inference, Williams et al., 2018), QNLI (Question Natural Language Inference, Rajpurkar et al., 2016) and RTE (Recognizing Textual Entailment, Dagan et al., 2006; Bar Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009) are sentence pair classification tasks. Similar to Schick and Sch¨utze (2021a), we may have prompt tokens before, after a"
2021.acl-long.381,N18-1101,0,0.016943,"by 6.5 and 9.2 points, respectively. SST-2 (Sentence Sentiment Treebank, Socher et al., 2013) is a single sentence binary classification task. For the prompt, we put a [MASK] token after the sentence, and the trainable prompt tokens are both appended and prepended to the sentence. CoLA (Corpus of Linguistic Acceptability, Warstadt et al., 2019) is a single sentence classification task as well, so we treat both the same way with the only difference that as a validation metric we use accuracy for SST-2, and Matthew’s correlation for CoLA. MNLI (MultiNLI, Multi-Genre Natural Language Inference, Williams et al., 2018), QNLI (Question Natural Language Inference, Rajpurkar et al., 2016) and RTE (Recognizing Textual Entailment, Dagan et al., 2006; Bar Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009) are sentence pair classification tasks. Similar to Schick and Sch¨utze (2021a), we may have prompt tokens before, after and between the two sentences, but the [MASK] token is always put between the sentences. For MNLI, we use matched accuracy as a validation metric and use the same model for the mismatched version. In our few-shot attempt for the RTE task, we use a different training and evalu"
2021.acl-long.561,P10-1106,0,0.0826427,"Missing"
2021.acl-long.561,2020.acl-main.145,0,0.350884,"ing, pages 7226–7235 August 1–6, 2021. ©2021 Association for Computational Linguistics 3.1 a) The Copiale cipher.3 b) The Borg cipher. Figure 1: Historical cipher examples. to a substitution table called the key. For example: the plaintext word “doors” would be enciphered to “KFFML” using the substitution table: Cipher K F M L Plain d o r s 3 Decipherment Model Inspired by character-level neural machine translation (NMT), we view decipherment as a sequenceto-sequence translation task. The motivation behind using a sequence-to-sequence model is: • The model can be trained on multilingual data (Gao et al., 2020), making it potentially possible to obtain end-to-end multilingual decipherment without relying on a separate language ID step. • Due to transcription challenges of historical ciphers (Section 5.4), ciphertext could be noisy. We would like the model to have the ability to recover from that noise by inserting, deleting, or substituting characters while generating plaintext. Sequence-to-sequence models seem to be good candidates for this task. 3 https://cl.lingfil.uu.se/~bea/ copiale/ To cast decipherment as a supervised translation task, we need training data, i.e. pairs of &lt;f1N , eM 1 &gt; to tra"
2021.acl-long.561,C14-1218,0,0.02495,"we experiment with a special type of noise. In this section, we address the challenging problem of solving substitution ciphers in which letters within each word have been randomly shuffled. Anagramming is a technique that can be used to further disguise substitution ciphers by permuting characters. Various theories about the mysterious Voynich Manuscript, for example, suggest that some anagramming scheme was used to encode the manuscript (Reddy and Knight, 2011). Hauer and Kondrak (2016) propose a two-step approach to solve this problem. First, they use their 1:1 substitution cipher solver (Hauer et al., 2014) to decipher the text. The solver is based on tree search for the key, guided by character-level and word-level n-gram language models. They adapt the solver by relaxing the letter order constraint in the key mutation component of the solver. They then re-arrange the resulting deciphered characters using a word trigram language model. 7231 7 Figure 4: The first 132 characters of the Borg cipher and its decipherment. Errors are underlined. Correct words are: pulegi, benedicti, crispe, ozimi, and feniculi. We try a one-step, end-to-end anagram decryption model. In our sequence-to-sequence formul"
2021.acl-long.561,Q16-1006,0,0.22724,"researchers (Pettersson and Megyesi, 2019; Megyesi et al., 2020). Decipherment of classical ciphers is an essential step to reveal the contents of those historical documents. In this work, we focus on solving 1:1 substitution ciphers. Current state-of-the-art methods use beam search and a neural language model to score candidate plaintext hypotheses for a given cipher (Kambhatla et al., 2018). However, this approach assumes that the target plaintext language is known. Other work that both identifies language and deciphers relies on a brute-force guess-and-check strategy (Knight et al., 2006; Hauer and Kondrak, 2016). We ask: Can we build an end-to-end model that deciphers directly without relying on a separate language ID step? The contributions of our work are: 2 The Decipherment Problem Decipherment conditions vary from one cipher to another. For example, some cleartext might be found along with the encrypted text, which gives a hint to the plaintext language of the cipher. In other cases, called known-plaintext attacks, some decoded material is found, which can be exploited to crack the rest of the encoded script. However, in a ciphertext-only attack, the focus of this paper, the cryptanalyst only has"
2021.acl-long.561,D18-1102,0,0.141363,"red documents from the early modern period. Example documents include encrypted letters, diplomatic correspondences, and books from secret societies (Figure 1). Previous work has made historical cipher collections available for researchers (Pettersson and Megyesi, 2019; Megyesi et al., 2020). Decipherment of classical ciphers is an essential step to reveal the contents of those historical documents. In this work, we focus on solving 1:1 substitution ciphers. Current state-of-the-art methods use beam search and a neural language model to score candidate plaintext hypotheses for a given cipher (Kambhatla et al., 2018). However, this approach assumes that the target plaintext language is known. Other work that both identifies language and deciphers relies on a brute-force guess-and-check strategy (Knight et al., 2006; Hauer and Kondrak, 2016). We ask: Can we build an end-to-end model that deciphers directly without relying on a separate language ID step? The contributions of our work are: 2 The Decipherment Problem Decipherment conditions vary from one cipher to another. For example, some cleartext might be found along with the encrypted text, which gives a hint to the plaintext language of the cipher. In o"
2021.acl-long.561,W11-1202,0,0.0316233,"more accurate than Kambhatla et al. (2018) for shorter, more difficult ciphers of lengths 16 and 32. In addition, our method provides the ability to train on multilingual data, which we use to attack ciphers with an unknown plaintext language as described in Section 5.3. 5.2 The inclusion of white space between words makes decipherment easier because word boundaries can give a strong clue to the cryptanalyst. In many historical ciphers, however, spaces are hidden. For example, in the Copiale cipher (Figure 1a), spaces are enciphered with special symbols just like other alphabetic characters (Knight et al., 2011). In other ciphers, spaces might be omitted from the plain text before enciphering, as was done in the Zodiac-408 cipher (Nuhn et al., 2013). We test our method in four scenarios: 1. Ciphers with spaces (comparable to Kambhatla et al. (2018)). 2. Ciphers with enciphered spaces. In this case, we treat space like other cipher characters during frequency encoding as described in Section 3.2. Cipher Length We first experiment with ciphers of length 256 using the approach described in Section 3.1 (i.e. we train a Transformer model on pairs of &lt;f1N , eM 1 &gt; without frequency encoding). As expected,"
2021.acl-long.561,P06-2065,0,0.233063,"ections available for researchers (Pettersson and Megyesi, 2019; Megyesi et al., 2020). Decipherment of classical ciphers is an essential step to reveal the contents of those historical documents. In this work, we focus on solving 1:1 substitution ciphers. Current state-of-the-art methods use beam search and a neural language model to score candidate plaintext hypotheses for a given cipher (Kambhatla et al., 2018). However, this approach assumes that the target plaintext language is known. Other work that both identifies language and deciphers relies on a brute-force guess-and-check strategy (Knight et al., 2006; Hauer and Kondrak, 2016). We ask: Can we build an end-to-end model that deciphers directly without relying on a separate language ID step? The contributions of our work are: 2 The Decipherment Problem Decipherment conditions vary from one cipher to another. For example, some cleartext might be found along with the encrypted text, which gives a hint to the plaintext language of the cipher. In other cases, called known-plaintext attacks, some decoded material is found, which can be exploited to crack the rest of the encoded script. However, in a ciphertext-only attack, the focus of this paper,"
2021.acl-long.561,2005.mtsummit-papers.11,0,0.400321,"Missing"
2021.acl-long.561,P13-1154,0,0.0501697,"Missing"
2021.acl-long.561,D14-1184,0,0.0434516,"Missing"
2021.acl-long.561,N19-4009,0,0.0278889,"fraction of incorrect symbols in the deciphered text. For space restoration experiments (Section 5.2), we use Translation Edit Rate (TER) (Snover et al., 2006), but on the 4 Our dataset is available at https://github.com/ NadaAldarrab/s2s-decipherment 7228 5 https://en.wikipedia.org/wiki/History character level. We define character-level TER as: TER = # of edits # of reference characters (1) where possible edits include the insertion, deletion, and substitution of single characters. When the ciphertext and plaintext have equal lengths, SER is equal to TER. We use FAIRSEQ to train our models (Ott et al., 2019). We mostly use the same hyperparameters as Gao et al. (2020) for character NMT, except that we set the maximum batch size to 10K tokens and use half precision floating point computation for faster training. The model has about 44M parameters. Training on a Tesla V100 GPU takes about 110 minutes per epoch. We train for 20 epochs. Decoding takes about 400 character tokens/s. We use a beam size of 100. Unless otherwise stated, we use 2M example ciphers to train, 3K ciphers for tuning, and 50 ciphers for testing in all experiments. We report the average SER on the 50 test ciphers of each experime"
2021.acl-long.561,W19-6126,0,0.0129454,"ce ciphers, and ciphers with noise, and demonstrate that our model is robust to these conditions. • We apply our model on synthetic ciphers as well as on the Borg cipher, a real historical cipher.1 We show that our multilingual model can crack the Borg cipher using the first 256 characters of the cipher. Introduction Libraries and archives have many enciphered documents from the early modern period. Example documents include encrypted letters, diplomatic correspondences, and books from secret societies (Figure 1). Previous work has made historical cipher collections available for researchers (Pettersson and Megyesi, 2019; Megyesi et al., 2020). Decipherment of classical ciphers is an essential step to reveal the contents of those historical documents. In this work, we focus on solving 1:1 substitution ciphers. Current state-of-the-art methods use beam search and a neural language model to score candidate plaintext hypotheses for a given cipher (Kambhatla et al., 2018). However, this approach assumes that the target plaintext language is known. Other work that both identifies language and deciphers relies on a brute-force guess-and-check strategy (Knight et al., 2006; Hauer and Kondrak, 2016). We ask: Can we b"
2021.acl-long.561,D08-1085,0,0.131591,"Missing"
2021.acl-long.561,W11-1511,0,0.0346215,"herment and can be easily corrected by Latin scholars who would be interested in such a text. Anagram Decryption To further test the capacity of our model, we experiment with a special type of noise. In this section, we address the challenging problem of solving substitution ciphers in which letters within each word have been randomly shuffled. Anagramming is a technique that can be used to further disguise substitution ciphers by permuting characters. Various theories about the mysterious Voynich Manuscript, for example, suggest that some anagramming scheme was used to encode the manuscript (Reddy and Knight, 2011). Hauer and Kondrak (2016) propose a two-step approach to solve this problem. First, they use their 1:1 substitution cipher solver (Hauer et al., 2014) to decipher the text. The solver is based on tree search for the key, guided by character-level and word-level n-gram language models. They adapt the solver by relaxing the letter order constraint in the key mutation component of the solver. They then re-arrange the resulting deciphered characters using a word trigram language model. 7231 7 Figure 4: The first 132 characters of the Borg cipher and its decipherment. Errors are underlined. Correc"
2021.acl-long.561,2006.amta-papers.25,0,0.213594,"Missing"
2021.dialdoc-1.11,P19-1620,0,0.23467,"ISI jonmay@isi.edu Abstract not changing these expectations, and users simply not possessing sufficient knowledge of the subject of interest to ask more challenging questions. Irrespective of the reason, one potential solution to this dilemma is to provide users with automatically generated suggested questions (SQs) to help users better understand QA system capabilities. Generating SQs is a specific form of question generation (QG), a long-studied task with many applied use cases – the most frequent purpose being data augmentation for mitigating the high sample complexity of neural QA models (Alberti et al., 2019a). However, the objective of such existing QG systems is to produce large quantities of question/answer pairs for training, which is contrary to that of SQs. The latter seeks to guide users in their research of a particular subject by producing engaging and understandable questions. To this end, we aim to generate questions that are self-explanatory and introductory. Self-explanatory questions require neither significant background knowledge nor access to documents used for QG to understand the SQ context. For example, existing QG systems may use the text “On December 13, 2013, Beyonc´e unexp"
2021.dialdoc-1.11,P17-1123,0,0.273971,"rt answer question generation performance can reach 28.1 BLEU-4. 2 Related Work QG has been studied in multiple application contexts (e.g., generating questions for reading comprehension tests (Heilman and Smith, 2010), generating questions about an image (Mostafazadeh et al., 2016), recommending questions with respect to a news article (Laban et al., 2020)), evaluating summaries (Deutsch et al., 2020; Wang et al., 2020), and using multiple methods (see (Pan et al., 2019) for a recent survey). Early neural models focused on sequence-to-sequence generation based solutions (Serban et al., 2016; Du et al., 2017). The primary directions for improving these early works generally fall into the categories of providing mechanisms to inject answer-aware information into the neural encoder-decoder architectures (Du and Cardie, 2018; Li et al., 2019; Liu et al., 2019; Wang et al., 2020; Sun et al., 2018), encoding larger portions of the answer document as context (Zhao et al., 2018; Tuan et al., 2020), and incorporating richer knowledge sources (Elsahar et al., 2018). These QG methods and the work described in this paper focus on using single-hop QA datasets such as SQuAD (Rajpurkar et al., 2016, 2018), We f"
2021.dialdoc-1.11,N18-1020,0,0.0154651,"s (see (Pan et al., 2019) for a recent survey). Early neural models focused on sequence-to-sequence generation based solutions (Serban et al., 2016; Du et al., 2017). The primary directions for improving these early works generally fall into the categories of providing mechanisms to inject answer-aware information into the neural encoder-decoder architectures (Du and Cardie, 2018; Li et al., 2019; Liu et al., 2019; Wang et al., 2020; Sun et al., 2018), encoding larger portions of the answer document as context (Zhao et al., 2018; Tuan et al., 2020), and incorporating richer knowledge sources (Elsahar et al., 2018). These QG methods and the work described in this paper focus on using single-hop QA datasets such as SQuAD (Rajpurkar et al., 2016, 2018), We further validate the generalization ability of our B ERT P GN model by creating an out-of-domain test set with the CNN/Daily Mail (Hermann et al., 2015). Without human-generated reference questions, automatic evaluation metrics such as BLEU are not usable. We propose to evaluate these questions with a pretrained QA system that produces two novel metrics. The first is answerability, mea82 NewsQA (Trischler et al., 2017; Hermann et al., 2015), and MS Marc"
2021.dialdoc-1.11,D19-1317,0,0.0121757,"tions about an image (Mostafazadeh et al., 2016), recommending questions with respect to a news article (Laban et al., 2020)), evaluating summaries (Deutsch et al., 2020; Wang et al., 2020), and using multiple methods (see (Pan et al., 2019) for a recent survey). Early neural models focused on sequence-to-sequence generation based solutions (Serban et al., 2016; Du et al., 2017). The primary directions for improving these early works generally fall into the categories of providing mechanisms to inject answer-aware information into the neural encoder-decoder architectures (Du and Cardie, 2018; Li et al., 2019; Liu et al., 2019; Wang et al., 2020; Sun et al., 2018), encoding larger portions of the answer document as context (Zhao et al., 2018; Tuan et al., 2020), and incorporating richer knowledge sources (Elsahar et al., 2018). These QG methods and the work described in this paper focus on using single-hop QA datasets such as SQuAD (Rajpurkar et al., 2016, 2018), We further validate the generalization ability of our B ERT P GN model by creating an out-of-domain test set with the CNN/Daily Mail (Hermann et al., 2015). Without human-generated reference questions, automatic evaluation metrics such as"
2021.dialdoc-1.11,N19-1028,0,0.0132739,"l (Hermann et al., 2015). Without human-generated reference questions, automatic evaluation metrics such as BLEU are not usable. We propose to evaluate these questions with a pretrained QA system that produces two novel metrics. The first is answerability, mea82 NewsQA (Trischler et al., 2017; Hermann et al., 2015), and MS Marco (Bajaj et al., 2016). However, there has also been recent interest in multi-hop QG settings (Yu et al., 2020; Gupta et al., 2020; Malon and Bai, 2020) by using multi-hop QA datasets including HotPotQA (Yang et al., 2018), TriviaQA (Joshi et al., 2017), and FreebaseQA (Jiang et al., 2019). Finally, there has been some recent interesting work regarding unsupervised QG, where the goal is to generate QA training data without an existing QG corpus to train better QA models (Lewis et al., 2019; Li et al., 2020). Most directly related to our work from a motivation perspective is recent research regarding providing SQs in the context of supporting a news chatbot (Laban et al., 2020). However, the focus of this work is not QG, where they essentially use a GPT-2 language model (Radford et al., 2019) trained on SQuAD data for QG and do not evaluate this component independently. Qi et al"
2021.dialdoc-1.11,2020.acl-main.600,0,0.0166625,"cs. The first is answerability, mea82 NewsQA (Trischler et al., 2017; Hermann et al., 2015), and MS Marco (Bajaj et al., 2016). However, there has also been recent interest in multi-hop QG settings (Yu et al., 2020; Gupta et al., 2020; Malon and Bai, 2020) by using multi-hop QA datasets including HotPotQA (Yang et al., 2018), TriviaQA (Joshi et al., 2017), and FreebaseQA (Jiang et al., 2019). Finally, there has been some recent interesting work regarding unsupervised QG, where the goal is to generate QA training data without an existing QG corpus to train better QA models (Lewis et al., 2019; Li et al., 2020). Most directly related to our work from a motivation perspective is recent research regarding providing SQs in the context of supporting a news chatbot (Laban et al., 2020). However, the focus of this work is not QG, where they essentially use a GPT-2 language model (Radford et al., 2019) trained on SQuAD data for QG and do not evaluate this component independently. Qi et al. (2020) generates questions for information-seeking but not focuses on introductory questions. Most directly related to our work from a conceptual perspective is regarding producing questions for long answer targets (Mish"
2021.dialdoc-1.11,P17-1147,0,0.165493,"and demonstrate that our model produces better SQs for news articles – with further confirmation via a human evaluation. 1 Introduction Question answering (QA) systems have experienced dramatic recent empirical improvements due to several factors including novel neural architectures (Chen and Yih, 2020), access to pre-trained contextualized embeddings (Devlin et al., 2019), and the development of large QA training corpora (Rajpurkar et al., 2016; Trischler et al., 2017; Yu et al., 2020). However, despite technological advancements that support more sophisticated questions (Yang et al., 2018; Joshi et al., 2017; Choi et al., 2018; Reddy et al., 2019), many consumers of QA technology in practice tend to ask simple factoid questions when engaging with these systems. Potential explanations for this phenomenon include low expectations set by previous QA systems, limited coverage for more complex questions ∗ Work was done as an intern at Amazon. 81 Proceedings of the 1st Workshop on Document-grounded Dialogue and Conversational Question Answering, pages 81–97 August 5–6, 2021. ©2021 Association for Computational Linguistics plasma help COVID patients?”). However, existing QG methods mostly generate quest"
2021.dialdoc-1.11,W18-6326,0,0.0317431,"Missing"
2021.dialdoc-1.11,W04-1013,0,0.0627149,"G, where they essentially use a GPT-2 language model (Radford et al., 2019) trained on SQuAD data for QG and do not evaluate this component independently. Qi et al. (2020) generates questions for information-seeking but not focuses on introductory questions. Most directly related to our work from a conceptual perspective is regarding producing questions for long answer targets (Mishra et al., 2020), which we contrast directly in Section 3. As QG is a generation task, automated evaluation frequently uses metrics such as BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007), and ROUGE (Lin, 2004). As these do not explicitly evaluate the requirements of our information-seeking use case, we also evaluate using the output of a trained QA system and conduct human annotator evaluations. 3 ? Copy Transformer BERT Encoder w p t w p t w p t w p t Attn BERT as a LM w p w p w p Figure 1: The B ERT P GN architecture. The input for the B ERT encoder is the context (w/p: word and position embeddinngs) with answer spans (or the whole context in the long answer setting) marked with the answer tagging (t: answer tagging embeddings). The decoder is a combination of B ERT as a language model (i.e. has"
2021.dialdoc-1.11,P16-1162,0,0.0484494,"Missing"
2021.dialdoc-1.11,W18-6301,0,0.0119432,"oint model (Alberti et al., 2019b) (Figure 2) for NQ question answering to evaluate our long answer question generation. The B ERTjoint model takes the combination a question and the corresponding context as an input, outputs the probability of answer spans and the probability of answer types. For a context of size n, it produces pstart and pend for each token, indicating whether this token is a start or end token of an answer span. It then chooses the answer span (i, j) where i &lt; j 4 Mishra et al. (2020) have not described the decoding method and possible beam size, but they use models from (Ott et al., 2018) that uses beam=4. 86 Du-17 best MSD B1 B4 ME RL 43.1 46.0 12.3 14.8 16.6 19.2 39.8 42.0 15 10 5 sans(MNQ) sans(MSD) sgra(MNQ) sgra(MSD) 0 Table 4: The performance of our answer-free baseline, compared with the best model from (Du et al., 2017). -5 -10 that maximizes pstart (i) · pend (j) as the probability of the answer. It also defines the probability of no answer to be pstart ([CLS]) · pend ([CLS]), i.e., an answer span that starts then stops at the [CLS] token. Furthermore, the B ERT-joint model computes the probability of types of the question— undetermined, long answer, short answer, and"
2021.dialdoc-1.11,P16-1056,0,0.0482585,"Missing"
2021.dialdoc-1.11,P02-1040,0,0.109981,"ions that tend to satisfy our self-explanatory and introductory requirements. To this end, we propose a novel B ERT-based Pointer-Generator Network (B ERT P GN) trained with the NQ dataset to generate introductory and self-explanatory questions as SQs. Using NQ, we start by creating a QG dataset that contains questions with both short and long answers. We train our B ERT P GN model with these two types of context-question pairs together. During inference, the model can generate either short- or long-answer questions as determined by the context. With automatic evaluation metrics such as BLEU (Papineni et al., 2002), we show that for long-answer question generation, our model can produce state-of-the-art performance with 20.1 BLEU-4, 6.2 higher than (Mishra et al., 2020), the current state-of-the-art on this dataset. The short answer question generation performance can reach 28.1 BLEU-4. 2 Related Work QG has been studied in multiple application contexts (e.g., generating questions for reading comprehension tests (Heilman and Smith, 2010), generating questions about an image (Mostafazadeh et al., 2016), recommending questions with respect to a news article (Laban et al., 2020)), evaluating summaries (Deu"
2021.dialdoc-1.11,D16-1264,0,0.349922,"e of SQ generation on the NQ dataset (20.1 BLEU-4). We further apply our model on out-of-domain news articles, evaluating with a QA system due to the lack of gold questions and demonstrate that our model produces better SQs for news articles – with further confirmation via a human evaluation. 1 Introduction Question answering (QA) systems have experienced dramatic recent empirical improvements due to several factors including novel neural architectures (Chen and Yih, 2020), access to pre-trained contextualized embeddings (Devlin et al., 2019), and the development of large QA training corpora (Rajpurkar et al., 2016; Trischler et al., 2017; Yu et al., 2020). However, despite technological advancements that support more sophisticated questions (Yang et al., 2018; Joshi et al., 2017; Choi et al., 2018; Reddy et al., 2019), many consumers of QA technology in practice tend to ask simple factoid questions when engaging with these systems. Potential explanations for this phenomenon include low expectations set by previous QA systems, limited coverage for more complex questions ∗ Work was done as an intern at Amazon. 81 Proceedings of the 1st Workshop on Document-grounded Dialogue and Conversational Question An"
2021.dialdoc-1.11,2020.acl-main.450,0,0.0190089,"long-answer question generation, our model can produce state-of-the-art performance with 20.1 BLEU-4, 6.2 higher than (Mishra et al., 2020), the current state-of-the-art on this dataset. The short answer question generation performance can reach 28.1 BLEU-4. 2 Related Work QG has been studied in multiple application contexts (e.g., generating questions for reading comprehension tests (Heilman and Smith, 2010), generating questions about an image (Mostafazadeh et al., 2016), recommending questions with respect to a news article (Laban et al., 2020)), evaluating summaries (Deutsch et al., 2020; Wang et al., 2020), and using multiple methods (see (Pan et al., 2019) for a recent survey). Early neural models focused on sequence-to-sequence generation based solutions (Serban et al., 2016; Du et al., 2017). The primary directions for improving these early works generally fall into the categories of providing mechanisms to inject answer-aware information into the neural encoder-decoder architectures (Du and Cardie, 2018; Li et al., 2019; Liu et al., 2019; Wang et al., 2020; Sun et al., 2018), encoding larger portions of the answer document as context (Zhao et al., 2018; Tuan et al., 2020), and incorporating"
2021.dialdoc-1.11,Q19-1016,0,0.0227516,"s better SQs for news articles – with further confirmation via a human evaluation. 1 Introduction Question answering (QA) systems have experienced dramatic recent empirical improvements due to several factors including novel neural architectures (Chen and Yih, 2020), access to pre-trained contextualized embeddings (Devlin et al., 2019), and the development of large QA training corpora (Rajpurkar et al., 2016; Trischler et al., 2017; Yu et al., 2020). However, despite technological advancements that support more sophisticated questions (Yang et al., 2018; Joshi et al., 2017; Choi et al., 2018; Reddy et al., 2019), many consumers of QA technology in practice tend to ask simple factoid questions when engaging with these systems. Potential explanations for this phenomenon include low expectations set by previous QA systems, limited coverage for more complex questions ∗ Work was done as an intern at Amazon. 81 Proceedings of the 1st Workshop on Document-grounded Dialogue and Conversational Question Answering, pages 81–97 August 5–6, 2021. ©2021 Association for Computational Linguistics plasma help COVID patients?”). However, existing QG methods mostly generate questions while reading the text corpus and t"
2021.dialdoc-1.11,P17-1099,0,0.0488228,"tio between questions and the context-answer pairs to avoid generating multiple questions based on the same context-answer. After removing questions that have no answers, there are 152,148 questions and 136,450 unique long answers. The average ratio between questions and long answers is around 1.1 questions per paragraph (ratios are in a range of 1 to 47). The average ratio is more reasonable for question generation, comparing to the SQuAD where there are 1.4 questions per sentence on average (Du et al., 2017). Question Decoding The transformer-based Pointer-Generator Network is derived from (See et al., 2017) with adaptations to support transformers (Vaswani et al., 2017). Denoting LN(·) as layer normalization, MHA(Q, K, V ) as the multi-head attention with three parameters—query, key, and value, FFN(·) as a linear function, and the decoder input at time t: Y (t) = {yj }tj=1 , the decoder self-attention at time t is given by (illustrated with a single-layer transformer simplification)     (t) AS = LN MHA Y (t) , Y (t) , Y (t) + Y (t) , the cross-attention between encoder and decoder is     (t) (t) (t) AC = LN MHA AS , H, H + AS , and the final decoder output is     (t) (t) O(t) = LN FF"
2021.emnlp-main.107,D13-1178,0,0.0184606,"trained Transformer language model based on the event chains (Section 4.1), or capture the sequences by training an RNN language model from scratch (Section 4.2). Once we obtain such a language model, we seek to use it to help with narrative prediction by predicting the continuation of an event chain. We can also leverage the fine-tuned model to support question answering regarding temporal orders of events. 4 Experiments It is difficult to directly evaluate the quality of event chains in an intrinsic way. Some works on event schemas ask human annotators to rate qualities of generated chains (Balasubramanian et al., 2013; Weber et al., 2018a,b), which can be subjective. We instead turn to extrinsic evaluation tasks that depend on implicitly understanding typical sequences 2 Those include Main Event (M1), Consequence (M2), Preof meaningful events in order to be completed usevious Event (C1), Current Context (C2), Historical Event (D1), Anecdotal Event (D2), Evaluation (D3), and Expectation (D4). fully. 1421 TORQUE (Ning et al., 2020), short for Temporal ORdering QUEstions, is a machine reading comprehension benchmark that requests a model to answer questions regarding temporally specified events (e.g., “What h"
2021.emnlp-main.107,D14-1159,0,0.054292,"Missing"
2021.emnlp-main.107,P08-1090,0,0.0941191,"is currently 1 Introduction difficult to gather enough data from other modaliHuman languages always communicate about ties to model real world “events,” and written text, evolving events. Hence, it is important for NLP especially in the news domain, seems to be our best systems to understand how events are procedurally option. described in text. In this context, identifying patPrevious attempts have been made to generate terns of event chains is important but challenging, event chains by modeling narratives in news, stories as it requires knowledge of inter-event relations and documentaries (Chambers and Jurafsky, 2008; such as temporal or causal relations. Modeling Weber et al., 2018b; Li et al., 2020). The problem high-quality event chain patterns from text would with such data is that the articles are usually a mixbe a first step toward the more general goal of ture of the true narrative flow with other content, 1 Our code and data are available at https://github. which serves to explain the context or provide side com/juvezxy/Salience-Event-Chain. information. Most prior approaches do not take 1418 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1418–1428 c"
2021.emnlp-main.107,D17-1168,0,0.384764,"a reference article. We hypothesize that models trained on unfiltered event chains are less likely to focus on the relevant events requested by the question, but this seeks to be improved by our filtered event chains. ROCStories (Mostafazadeh et al., 2016) is a narrative prediction dataset consisting of five-sentence short stories, where each sentence of a story contains a core event. A test set included with ROCStories contains two candidate endings to each partially complete story, where one of them is plausible. While prior work has successfully leveraged event chains to infer the endings (Chaturvedi et al., 2017), we believe a model trained on relevant chains of events should be able to better distinguish the relevant ending from the irrelevant one than a model trained on event chains that contain irrelevant events. Other works such as Sun et al. (2019) have directly tried to maximize performance on this corpus; we don’t seek to directly compete with that work here, but rather use ROCStories as a means of establishing that our isolation of important events at the event language model level does positively influence event ending prediction, indicating that the sequences we find do indeed contain more r"
2021.emnlp-main.107,2020.conll-1.43,1,0.874695,"ge models on temporal understanding of events, leading to state-of-the-art performance on answering temporal ordering questions. 2 Event Chains and Narratives connected in a temporal order, form linear event chains, which can be viewed as a form of linear representation of the progress of described events in a narrative. However, events that co-occur or are described together in an article are often not equally important. Some events are salient – they are relevant to the main topic of a text context, or play essential roles with regard to the central goal of an event chain (Liu et al., 2018; Chen et al., 2020). For example, from the paragraph in Figure 1, the events detained, alerted and questioned are salient components that constitute the progress of a described story. Other events may describe how the salient events came to be known or involve other events that happened alongside salient events but are not critical to understanding the core actions of the story. In Figure 1, said and establish are not salient. The goal of event salience detection systems (Liu et al., 2018; Jindal et al., 2020) is to identify events that are essential components of the narrative, which can help us filter out triv"
2021.emnlp-main.107,N18-1076,0,0.0198425,"-aware filtering on an example text. The words in italic are the events and the arrows between them show the temporal relation given by TEAR (pointing from previous to next). Salient events are shown in bold; the blue sentence is in the Main Event category whereas the red sentence is in the Evaluation category. event schema induction, which involves generating high-level representations of event relations and structures. The extracted chain patterns can in turn represent useful information for core natural language tasks such as question answering (Reddy et al., 2019), semantic role labeling (Cheng and Erk, 2018), story generation (Yao et al., 2019), and reading comprehension (Ostermann et al., 2019). Generally speaking, “events” correspond to what we perceive as happening around us. According to the theory of embodied cognition (Wilson, 2002), our understanding of the world is shaped by various aspects of our entire bodies, involving also our language comprehension. However, it is currently 1 Introduction difficult to gather enough data from other modaliHuman languages always communicate about ties to model real world “events,” and written text, evolving events. Hence, it is important for NLP especia"
2021.emnlp-main.107,2020.acl-main.478,0,0.516389,"Natural Language Processing, pages 1418–1428 c November 7–11, 2021. 2021 Association for Computational Linguistics into account the centrality or salience of events or the discourse structures that describe those events. Accordingly, overlooking such important discourse properties when choosing the events of a sequence introduces noise in the modeling of event chains, and causes trivial or irrelevant content to be captured and inferred in narrative understanding tasks. In this work, we explore the use of salience identification (Liu et al., 2018; Jindal et al., 2020) and discourse profiling (Choubey et al., 2020) to help isolate the main event chains from other distracting events, and show the effect on two recent datasets related to narrative understanding and temporal understanding. More specifically, we obtain event chains from documents and perform different methods of filtering, then build event language models, which we use to predict story ending events from the ROCStories dataset (Mostafazadeh et al., 2016) and answer temporal event questions from the TORQUE dataset (Ning et al., 2020). By comparing the use of event language models built on differently filtered event sources, we show that filt"
2021.emnlp-main.107,N18-2055,0,0.0608443,"Missing"
2021.emnlp-main.107,N19-1423,0,0.00540612,"ws articles with expertwritten abstracts. During training, we use the frame-based event mention annotation by Liu et al. (2018), and for inference on new articles, we extract the event mentions using TEAR. Each event mention is labeled as salient if its lemma appears in the associated abstract. The trained salience model can then assign a salience score between 0 and 1 to each event extracted from a document, and we use a threshold of 0.5 to perform the final filtering of the events. We adopt the joint structured event-relation extraction model from Han et al. (2019). It uses pretrained BERT (Devlin et al., 2019) embedding vectors of the input text, which are further fed into an RNN-based scoring function for both event and relation extraction. During the end-to-end training, a MAP inference framework sets up a global objective function using local scoring functions to get the optimal assignment of event and relation labels. To ensure that we obtain globally coherent assignments, several logical constraints are specified including event-relation consistency and symmetry/transitivity consistency, so that the output event graph is logically consistent. This end-toend model extracts both events and event"
2021.emnlp-main.107,D19-1041,0,0.0406084,"Missing"
2021.emnlp-main.107,2020.coling-main.10,0,0.141497,"of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1418–1428 c November 7–11, 2021. 2021 Association for Computational Linguistics into account the centrality or salience of events or the discourse structures that describe those events. Accordingly, overlooking such important discourse properties when choosing the events of a sequence introduces noise in the modeling of event chains, and causes trivial or irrelevant content to be captured and inferred in narrative understanding tasks. In this work, we explore the use of salience identification (Liu et al., 2018; Jindal et al., 2020) and discourse profiling (Choubey et al., 2020) to help isolate the main event chains from other distracting events, and show the effect on two recent datasets related to narrative understanding and temporal understanding. More specifically, we obtain event chains from documents and perform different methods of filtering, then build event language models, which we use to predict story ending events from the ROCStories dataset (Mostafazadeh et al., 2016) and answer temporal event questions from the TORQUE dataset (Ning et al., 2020). By comparing the use of event language models built on differ"
2021.emnlp-main.107,2020.emnlp-main.50,1,0.891181,"ays communicate about ties to model real world “events,” and written text, evolving events. Hence, it is important for NLP especially in the news domain, seems to be our best systems to understand how events are procedurally option. described in text. In this context, identifying patPrevious attempts have been made to generate terns of event chains is important but challenging, event chains by modeling narratives in news, stories as it requires knowledge of inter-event relations and documentaries (Chambers and Jurafsky, 2008; such as temporal or causal relations. Modeling Weber et al., 2018b; Li et al., 2020). The problem high-quality event chain patterns from text would with such data is that the articles are usually a mixbe a first step toward the more general goal of ture of the true narrative flow with other content, 1 Our code and data are available at https://github. which serves to explain the context or provide side com/juvezxy/Salience-Event-Chain. information. Most prior approaches do not take 1418 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1418–1428 c November 7–11, 2021. 2021 Association for Computational Linguistics into account the c"
2021.emnlp-main.107,2021.acl-long.555,0,0.0204948,"trate the effectiveness of the approach by 1425 using the produced event chains to train/fine-tune language models, which leads to improved performance on temporal understanding of events and narrative prediction. Our case study on salienceaware discourse parsing shows the advantage of combining event-level and sentence-level salience information. We plan to use these event chain patterns on other narrative understanding and generation tasks, such as constrained story generation (Peng et al., 2018), event script generation (Zhang et al., 2020; Lyu et al., 2021), and implicit event prediction (Lin et al., 2021; Zhou et al., 2021). Ethical Considerations This work does not present any direct societal consequence. The proposed method aims at providing high-quality extraction of event chains from documents with awareness of salience and discourse structures, but is only evaluated on English data and uses western notions of both salience and news discourse; event sequence extraction using data from other languages and cultures may not benefit from the methods shown here. The extracted event chain representations benefit narrative understanding and temporal understanding of events. Yet, real-world open"
2021.emnlp-main.107,2020.acl-main.713,0,0.0106549,"luation sentence. We may classify the members of van Dijk’s ontology into types that are core to understanding the event sequence of a story (e.g. Main Event) and types that are not (e.g. Evaluation), and further refine extracted event sequences. Though salience and discourse structures represent different perspectives of narrative analysis, they can be concurrently modeled in an event chain extraction system, leading to more effective filtering of mined event chain representations. Information extraction techniques have evolved to extract event mentions as well as their ordering information (Lin et al., 2020; Han et al., 2019; Wang et al., 2020), hence enabling the automated induc- 3 Method tion of raw event chains from text. Tools such as TEAR (Han et al., 2019) have been developed, To obtain the interesting event chains from a docwhich we can use to extract both the events and ument, we first use the TEAR tool by Han et al. temporal ordering from text documents, like those (2019) to generate a temporal relation graph. Then shown in Figure 1. These sequences of events, we apply different levels of filtering on the ex1419 Figure 2: System diagram of our approach along with an example. Solid lines"
2021.emnlp-main.107,2021.ccl-1.108,0,0.0375462,"Missing"
2021.emnlp-main.107,D18-1154,0,0.183614,"1418 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1418–1428 c November 7–11, 2021. 2021 Association for Computational Linguistics into account the centrality or salience of events or the discourse structures that describe those events. Accordingly, overlooking such important discourse properties when choosing the events of a sequence introduces noise in the modeling of event chains, and causes trivial or irrelevant content to be captured and inferred in narrative understanding tasks. In this work, we explore the use of salience identification (Liu et al., 2018; Jindal et al., 2020) and discourse profiling (Choubey et al., 2020) to help isolate the main event chains from other distracting events, and show the effect on two recent datasets related to narrative understanding and temporal understanding. More specifically, we obtain event chains from documents and perform different methods of filtering, then build event language models, which we use to predict story ending events from the ROCStories dataset (Mostafazadeh et al., 2016) and answer temporal event questions from the TORQUE dataset (Ning et al., 2020). By comparing the use of event language"
2021.emnlp-main.107,2021.inlg-1.19,0,0.0190159,"nces of events. Many works in this line ex- demonstrate the effectiveness of the approach by 1425 using the produced event chains to train/fine-tune language models, which leads to improved performance on temporal understanding of events and narrative prediction. Our case study on salienceaware discourse parsing shows the advantage of combining event-level and sentence-level salience information. We plan to use these event chain patterns on other narrative understanding and generation tasks, such as constrained story generation (Peng et al., 2018), event script generation (Zhang et al., 2020; Lyu et al., 2021), and implicit event prediction (Lin et al., 2021; Zhou et al., 2021). Ethical Considerations This work does not present any direct societal consequence. The proposed method aims at providing high-quality extraction of event chains from documents with awareness of salience and discourse structures, but is only evaluated on English data and uses western notions of both salience and news discourse; event sequence extraction using data from other languages and cultures may not benefit from the methods shown here. The extracted event chain representations benefit narrative understanding and tempor"
2021.emnlp-main.107,N16-1098,0,0.0815769,"content to be captured and inferred in narrative understanding tasks. In this work, we explore the use of salience identification (Liu et al., 2018; Jindal et al., 2020) and discourse profiling (Choubey et al., 2020) to help isolate the main event chains from other distracting events, and show the effect on two recent datasets related to narrative understanding and temporal understanding. More specifically, we obtain event chains from documents and perform different methods of filtering, then build event language models, which we use to predict story ending events from the ROCStories dataset (Mostafazadeh et al., 2016) and answer temporal event questions from the TORQUE dataset (Ning et al., 2020). By comparing the use of event language models built on differently filtered event sources, we show that filtering out distracting narrative information can indeed benefit the modeling of event relations, thus leading to more reliable extraction of useful event chain patterns. The main contributions in this work are threefold. (1) To support narrative understanding tasks with high-quality event chains, we develop a new event chain extraction method that is discourse-aware, and particularly, salience-aware. (2) We"
2021.emnlp-main.107,2020.emnlp-main.88,0,0.282836,"xplore the use of salience identification (Liu et al., 2018; Jindal et al., 2020) and discourse profiling (Choubey et al., 2020) to help isolate the main event chains from other distracting events, and show the effect on two recent datasets related to narrative understanding and temporal understanding. More specifically, we obtain event chains from documents and perform different methods of filtering, then build event language models, which we use to predict story ending events from the ROCStories dataset (Mostafazadeh et al., 2016) and answer temporal event questions from the TORQUE dataset (Ning et al., 2020). By comparing the use of event language models built on differently filtered event sources, we show that filtering out distracting narrative information can indeed benefit the modeling of event relations, thus leading to more reliable extraction of useful event chain patterns. The main contributions in this work are threefold. (1) To support narrative understanding tasks with high-quality event chains, we develop a new event chain extraction method that is discourse-aware, and particularly, salience-aware. (2) We show the effectiveness of salience-based and discourse-aware filtering of event"
2021.emnlp-main.107,S19-1012,0,0.0173607,"between them show the temporal relation given by TEAR (pointing from previous to next). Salient events are shown in bold; the blue sentence is in the Main Event category whereas the red sentence is in the Evaluation category. event schema induction, which involves generating high-level representations of event relations and structures. The extracted chain patterns can in turn represent useful information for core natural language tasks such as question answering (Reddy et al., 2019), semantic role labeling (Cheng and Erk, 2018), story generation (Yao et al., 2019), and reading comprehension (Ostermann et al., 2019). Generally speaking, “events” correspond to what we perceive as happening around us. According to the theory of embodied cognition (Wilson, 2002), our understanding of the world is shaped by various aspects of our entire bodies, involving also our language comprehension. However, it is currently 1 Introduction difficult to gather enough data from other modaliHuman languages always communicate about ties to model real world “events,” and written text, evolving events. Hence, it is important for NLP especially in the news domain, seems to be our best systems to understand how events are procedu"
2021.emnlp-main.107,K19-1051,0,0.0158117,"es, which viewed paragraphs as units of annotations, and developed models to predict discourse labels. Choubey et al. (2020) instead created a sentencelevel discourse structure corpus spanning different domains and sources, which was more helpful for a fine-grained identification of sentences relevant to the main topic. Insights in those studies constitute the two aspects of salience awareness that are characterized in our method, i.e., event-level and discourse-level salience. tract raw event chains and directly train a neural language model for narrative prediction (Chaturvedi et al., 2017; Peng et al., 2019) or script generation (Rudinger et al., 2015; Pichotta and Mooney, 2016; Weber et al., 2018b). Besides directly training, Zheng et al. (2020) proposed a unified finetuning architecture, where a masked language model was explicitly fine-tuned on the representations of event chains to model event elements. Li et al. (2020) proposed to learn to induce event schemas using an auto-regressive language model trained on salient paths on an event-event relation graph, which was also an attempt to reduce noise in constructing event schema. While that work uses a different kind of data structure, our wor"
2021.emnlp-main.107,W18-1505,1,0.84541,"- course profiling to filter out distracting events. We ing sequences of events. Many works in this line ex- demonstrate the effectiveness of the approach by 1425 using the produced event chains to train/fine-tune language models, which leads to improved performance on temporal understanding of events and narrative prediction. Our case study on salienceaware discourse parsing shows the advantage of combining event-level and sentence-level salience information. We plan to use these event chain patterns on other narrative understanding and generation tasks, such as constrained story generation (Peng et al., 2018), event script generation (Zhang et al., 2020; Lyu et al., 2021), and implicit event prediction (Lin et al., 2021; Zhou et al., 2021). Ethical Considerations This work does not present any direct societal consequence. The proposed method aims at providing high-quality extraction of event chains from documents with awareness of salience and discourse structures, but is only evaluated on English data and uses western notions of both salience and news discourse; event sequence extraction using data from other languages and cultures may not benefit from the methods shown here. The extracted event"
2021.emnlp-main.107,N18-1202,0,0.0457742,"ering procedure is to first apply discourse filtering, and then keep only salient events from the discourse-filtered sentences, but this may propagate errors in the process and filter away too many events. We instead use event salience information to improve the performance on the discourse parsing task. As shown in Figure 3, we modify the hierarchical biLSTM model from the work by Choubey et al. (2020). The input is a document consisting of sentences s1 , s2 , . . . , sn . Each sentence st , which is a sequence of tokens (wt1 , wt2 , . . . , wtk ), is first transformed to a sequence of ELMo (Peters et al., 2018) word representations, and then to hidden vectors (ht1 , ht2 , . . . , htk ) in the word-level bi-LSTM layer. Then, we concatenate the original intermediate sentence encoding St obtained from an softattention-weighted sum of the hidden vectors, with Et which is an average over the hidden vectors of salient events within the sentence. These concatenated vectors are fed into the sentence-level bi-LSTM layer to generate sentence encodings H1 , . . . , Ht , . . . , Hn , on top of which the final prediction layer and softmax are stacked, following the original model architecture by Choubey et al. ("
2021.emnlp-main.107,Q19-1016,0,0.0298144,"event chains and salienceaware and discourse-aware filtering on an example text. The words in italic are the events and the arrows between them show the temporal relation given by TEAR (pointing from previous to next). Salient events are shown in bold; the blue sentence is in the Main Event category whereas the red sentence is in the Evaluation category. event schema induction, which involves generating high-level representations of event relations and structures. The extracted chain patterns can in turn represent useful information for core natural language tasks such as question answering (Reddy et al., 2019), semantic role labeling (Cheng and Erk, 2018), story generation (Yao et al., 2019), and reading comprehension (Ostermann et al., 2019). Generally speaking, “events” correspond to what we perceive as happening around us. According to the theory of embodied cognition (Wilson, 2002), our understanding of the world is shaped by various aspects of our entire bodies, involving also our language comprehension. However, it is currently 1 Introduction difficult to gather enough data from other modaliHuman languages always communicate about ties to model real world “events,” and written text, evolving"
2021.emnlp-main.107,W18-4304,0,0.0275742,"ience identification. Choubey et al. (2018) studied how to find the dominant event(s) in a news article using mined event coreference relations. Liu et al. (2018) proposed a sequence tagging model for event salience detection, and also contributed a large-scale event salience corpus based on NYT. Jindal et al. (2020) tried to further capture representation of events and their interactions, and evaluated the modeling of event salience on extractive summarization. Recent studies on discourse structures, especially for news articles, were built on Van Dijk’s theory (van Dijk, 1988). For example, Yarlott et al. (2018) annotated a dataset of discourse structures, which viewed paragraphs as units of annotations, and developed models to predict discourse labels. Choubey et al. (2020) instead created a sentencelevel discourse structure corpus spanning different domains and sources, which was more helpful for a fine-grained identification of sentences relevant to the main topic. Insights in those studies constitute the two aspects of salience awareness that are characterized in our method, i.e., event-level and discourse-level salience. tract raw event chains and directly train a neural language model for narra"
2021.emnlp-main.107,D15-1195,0,0.0619513,"Missing"
2021.emnlp-main.107,N18-2015,0,0.0184328,"haturvedi et al., 2017; Srinivasan Dataset We evaluate the effectiveness of our extracted event chains on narrative prediction using the ROCStories dataset (Mostafazadeh et al., 2016). The training set contains 98,161 five-sentence stories, and the development and test sets each consist of 1,871 instances of four-sentence stories together with a correct and an incorrect ending. The goal is to predict the correct ending sentence given the two candidate choices. We use the development set but not the training set for supervised evaluation, following previous approaches (Chaturvedi et al., 2017; Srinivasan et al., 2018), as detailed below. 1423 Figure 4: Distribution of questions in the TORQUE dev set, showing only the most frequent prefixes related to Before, After or Co-occuring relations. For example, a total of 229 questions start with “What happened before”, which query the Before relation. 191+44 questions query the After relation, and 39+19 questions query the Cooccuring relation. We define these three categories of questions as “standard” type. This prefix matching is only a rough categorization of the question type. et al., 2018), i.e., on top of the language model, we train a binary classifier usin"
2021.emnlp-main.107,N19-1270,0,0.0304397,"Missing"
2021.emnlp-main.107,2020.emnlp-main.119,1,0.904665,"hains based on their PMI. Radinsky et al. (2012) and Radinsky and Horvitz (2013) extended such unsupervised event chain modeling to crossdocument scenarios and used the technology for news prediction and timeline construction. Berant et al. (2014) extracted relations among events and entities in biological processes to help solve a biological reading comprehension task using a structure matching method. More recently, Chen et al. (2020) attempted to infer the type of action and object associated with an event chain, which required recognition of the goal or intention extracted from the chain. Zhang et al. (2020) used a probabilistic graphical model to capture common patterns from analogous event chains, and induced new chains from those patterns. These works do not explore salience or discourse structures for event chains. As a case study of our proposed discourse parsing model from Section 3.3, we also compare our performance on the discourse type classification task 1424 Textual order TEAR order unsupervised supervised unsupervised supervised Event-sequence SemLM (Chaturvedi et al., 2017) 71.6 All events 61.3 72.5 63.2 73.3 Salient events 64.1 73.4 63.6 73.8 Discourse-filtered events 64.8 73.6 63.8"
2021.emnlp-main.107,2021.naacl-main.107,0,0.0273494,"eness of the approach by 1425 using the produced event chains to train/fine-tune language models, which leads to improved performance on temporal understanding of events and narrative prediction. Our case study on salienceaware discourse parsing shows the advantage of combining event-level and sentence-level salience information. We plan to use these event chain patterns on other narrative understanding and generation tasks, such as constrained story generation (Peng et al., 2018), event script generation (Zhang et al., 2020; Lyu et al., 2021), and implicit event prediction (Lin et al., 2021; Zhou et al., 2021). Ethical Considerations This work does not present any direct societal consequence. The proposed method aims at providing high-quality extraction of event chains from documents with awareness of salience and discourse structures, but is only evaluated on English data and uses western notions of both salience and news discourse; event sequence extraction using data from other languages and cultures may not benefit from the methods shown here. The extracted event chain representations benefit narrative understanding and temporal understanding of events. Yet, real-world open source articles may"
2021.emnlp-main.107,2020.emnlp-main.51,1,0.718245,"e members of van Dijk’s ontology into types that are core to understanding the event sequence of a story (e.g. Main Event) and types that are not (e.g. Evaluation), and further refine extracted event sequences. Though salience and discourse structures represent different perspectives of narrative analysis, they can be concurrently modeled in an event chain extraction system, leading to more effective filtering of mined event chain representations. Information extraction techniques have evolved to extract event mentions as well as their ordering information (Lin et al., 2020; Han et al., 2019; Wang et al., 2020), hence enabling the automated induc- 3 Method tion of raw event chains from text. Tools such as TEAR (Han et al., 2019) have been developed, To obtain the interesting event chains from a docwhich we can use to extract both the events and ument, we first use the TEAR tool by Han et al. temporal ordering from text documents, like those (2019) to generate a temporal relation graph. Then shown in Figure 1. These sequences of events, we apply different levels of filtering on the ex1419 Figure 2: System diagram of our approach along with an example. Solid lines indicate inference and dotted lines i"
2021.emnlp-main.107,D18-1413,0,0.147598,"liHuman languages always communicate about ties to model real world “events,” and written text, evolving events. Hence, it is important for NLP especially in the news domain, seems to be our best systems to understand how events are procedurally option. described in text. In this context, identifying patPrevious attempts have been made to generate terns of event chains is important but challenging, event chains by modeling narratives in news, stories as it requires knowledge of inter-event relations and documentaries (Chambers and Jurafsky, 2008; such as temporal or causal relations. Modeling Weber et al., 2018b; Li et al., 2020). The problem high-quality event chain patterns from text would with such data is that the articles are usually a mixbe a first step toward the more general goal of ture of the true narrative flow with other content, 1 Our code and data are available at https://github. which serves to explain the context or provide side com/juvezxy/Salience-Event-Chain. information. Most prior approaches do not take 1418 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1418–1428 c November 7–11, 2021. 2021 Association for Computational Linguistics"
2021.emnlp-main.132,2020.coling-main.381,0,0.0212,"the discovery of §4.2 can be taken Figure 4: Performance on the original language pair advantage of is mitigating catastrophic forgetting. after transfer. The original Fr–En parent model scores Catastrophic forgetting refers to the loss of pre- 35.0 BLEU on the Fr–En test set. {src,tgt}+xattn outperforms {src,tgt}+body on the parent task. viously acquired knowledge in the model during transfer to a new task. To the best of our knowledge, catastrophic forgetting in MT models has only been studied within the context of inter-domain adapta- 5.2 Zero-Shot Translation tion (Thompson et al., 2019; Gu and Feng, 2020), Another area where well-aligned embeddings from and not inter-lingual adaptation. the {src,tgt}+xattn setting can come in handy The effectiveness of the cross-lingual embed- is zero-shot translation. Since the source embed1760 dings are aligned, we, for instance, can replace the French embeddings in the Fr–Es model learned via tgt+xattn with German embeddings from the De–En model learned via src+xattn and form a De–Es translation model with no De–Es training or direct De–Fr alignment. We additionally build two more zero-shot systems in the same manner: Ro–Es (using transferred Ro–En and Fr–E"
2021.emnlp-main.132,2021.acl-long.381,1,0.730015,"{src,tgt}+xattn and {src,tgt}+randxattn. Bapna and Firat (2019) devise adapters for MT by inserting language pairspecific adapter parameters in the Transformer architecture. In the multilingual setting, they show that by fine-tuning adapters in a shared pretrained multilingual model, they can compensate for the performance drop of high-resource languages incurred by shared training. Philip et al. (2020) replace language pair-specific adapters with monolingual adapters, which enables adapting under the zero-shot setting. Another family of lightweight fine-tuning approaches (Li and Liang, 2021; Hambardzumyan et al., 2021; Lester et al., 2021), inspired by prompt tuning (Brown et al., 2020), also relies on updating a set of additional new parameters from scratch towards each downstream task. Such sets of parameters equal a very small fraction of the total parameters in the pretrained model. By contrast, our approach updates a subset of the model’s own parameters instead of adding new ones. We leave a comparison of the relative advantages and disadvantages of these approaches to future work. Module Freezing. In terms of restrictions introduced, our work is related to a group of recent works that freeze certain"
2021.emnlp-main.132,W18-6325,0,0.0165642,"ta and model that we use to materialize the analysis outlined in §2.2. 3.1 Methods We first provide the details of our transfer setup, and then describe the specific fine-tuning baselines and variants used in our experiments. General Setup. An important concern when transferring is initializing the embeddings of the new language. When initializing parameters in the child model, there are several ways to address the vocabulary mismatch between the parent and the child model: frequency-based assignment, random assignment (Zoph et al., 2016), joint (shared) vocabularies (Nguyen and Chiang, 2017; Kocmi and Bojar, 2018; Neubig and Hu, 2018; Gheini and May, 2019; Liu et al., 2020), and no assignment at all, which results in training randomly initialized embeddings (Aji et al., 2020). In our experiments, we choose to always use new random initialization for the new embeddings (including token embeddings, positional embeddings, and corresponding layer norm parameters). This decision is made to later let us study what happens to embeddings under each of the settings, independent of any pretraining artifacts that exist in them. For instance, when transferring from Fr–En to {Ro–En, Fr–Es}, respectively, all param"
2021.emnlp-main.132,D18-2012,0,0.0162025,"target) as child language pairs. Our Fr–En parent model is trained on the Europarl + Common Crawl subset of WMT14 Fr–En,5 which comprises 5,251,875 sentences. Details and statistics of the data for the child language pairs are provided in Table 1. Model Details. We use the Transformer base architecture (6 layers of encoder and decoder with model dimension of 512 and 8 attention heads) for all models, (Vaswani et al., 2017) and the Fairseq (Ott et al., 2019) toolkit for all our experiments. All models rely on BPE subword vocabularies (Sennrich et al., 2016) processed through the SentencePiece (Kudo and Richardson, 2018) BPE implementation. The vocabulary for the parent model consists of 32K French subwords on the source side, and 32K English subwords on the target side. The sizes of the vocabularies for child models are also reported in Table 1. We follow the advice from Gowda and May (2020) when deciding what vocabulary size to choose, i.e., we choose the maximum number of operations to ensure a minimum of 100 tokens per type. 4 Results and Analysis Our preliminary empirical results consist of five experiments for each of the child language pairs based on methods described in §3.1: scratch, {src,tgt}, {src,"
2021.emnlp-main.132,2021.emnlp-main.243,0,0.0723281,"Missing"
2021.emnlp-main.132,2020.acl-main.703,0,0.0293962,"vitskiy et al., 2021). In particular, transfer learn- three questions: 1) How powerful is cross-attention ing from large pretrained Transformer-based lan- alone in terms of adapting to the new language pair guage models has been widely adopted to train new while other modules are frozen? 2) How crucial are the cross-attention layers pretrained values with models: adapting models such as BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) regard to successful adaptation to the new task? and 3) Are there any qualitative differences in the for encoder-only tasks and models such as BART (Lewis et al., 2020) and mBART (Liu et al., 2020) learned representations when cross-attention is the only module that gets updated? for encoder-decoder tasks like machine translation To answer these questions, we compare mul(MT). This transfer learning is predominantly pertiple strategies of fine-tuning towards a new lanformed in the form of fine-tuning: using the values of several hundred million parameters from the pre- guage pair from a pretrained translation model that shares one language with the new pair. These are trained model to initialize a model and start training depicted in Figure 1: a) Ignoring the"
2021.emnlp-main.132,2021.acl-long.353,0,0.0250061,"edge by contrasting {src,tgt}+xattn and {src,tgt}+randxattn. Bapna and Firat (2019) devise adapters for MT by inserting language pairspecific adapter parameters in the Transformer architecture. In the multilingual setting, they show that by fine-tuning adapters in a shared pretrained multilingual model, they can compensate for the performance drop of high-resource languages incurred by shared training. Philip et al. (2020) replace language pair-specific adapters with monolingual adapters, which enables adapting under the zero-shot setting. Another family of lightweight fine-tuning approaches (Li and Liang, 2021; Hambardzumyan et al., 2021; Lester et al., 2021), inspired by prompt tuning (Brown et al., 2020), also relies on updating a set of additional new parameters from scratch towards each downstream task. Such sets of parameters equal a very small fraction of the total parameters in the pretrained model. By contrast, our approach updates a subset of the model’s own parameters instead of adding new ones. We leave a comparison of the relative advantages and disadvantages of these approaches to future work. Module Freezing. In terms of restrictions introduced, our work is related to a group of recen"
2021.emnlp-main.132,2020.tacl-1.47,0,0.254967,"icular, transfer learn- three questions: 1) How powerful is cross-attention ing from large pretrained Transformer-based lan- alone in terms of adapting to the new language pair guage models has been widely adopted to train new while other modules are frozen? 2) How crucial are the cross-attention layers pretrained values with models: adapting models such as BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) regard to successful adaptation to the new task? and 3) Are there any qualitative differences in the for encoder-only tasks and models such as BART (Lewis et al., 2020) and mBART (Liu et al., 2020) learned representations when cross-attention is the only module that gets updated? for encoder-decoder tasks like machine translation To answer these questions, we compare mul(MT). This transfer learning is predominantly pertiple strategies of fine-tuning towards a new lanformed in the form of fine-tuning: using the values of several hundred million parameters from the pre- guage pair from a pretrained translation model that shares one language with the new pair. These are trained model to initialize a model and start training depicted in Figure 1: a) Ignoring the pretrained from there. 1 par"
2021.emnlp-main.132,R19-1136,0,0.0471395,"Missing"
2021.emnlp-main.132,D18-1103,0,0.0138335,"e to materialize the analysis outlined in §2.2. 3.1 Methods We first provide the details of our transfer setup, and then describe the specific fine-tuning baselines and variants used in our experiments. General Setup. An important concern when transferring is initializing the embeddings of the new language. When initializing parameters in the child model, there are several ways to address the vocabulary mismatch between the parent and the child model: frequency-based assignment, random assignment (Zoph et al., 2016), joint (shared) vocabularies (Nguyen and Chiang, 2017; Kocmi and Bojar, 2018; Neubig and Hu, 2018; Gheini and May, 2019; Liu et al., 2020), and no assignment at all, which results in training randomly initialized embeddings (Aji et al., 2020). In our experiments, we choose to always use new random initialization for the new embeddings (including token embeddings, positional embeddings, and corresponding layer norm parameters). This decision is made to later let us study what happens to embeddings under each of the settings, independent of any pretraining artifacts that exist in them. For instance, when transferring from Fr–En to {Ro–En, Fr–Es}, respectively, all parameters are reused exce"
2021.emnlp-main.132,I17-2050,0,0.0174523,"ur experiments and the data and model that we use to materialize the analysis outlined in §2.2. 3.1 Methods We first provide the details of our transfer setup, and then describe the specific fine-tuning baselines and variants used in our experiments. General Setup. An important concern when transferring is initializing the embeddings of the new language. When initializing parameters in the child model, there are several ways to address the vocabulary mismatch between the parent and the child model: frequency-based assignment, random assignment (Zoph et al., 2016), joint (shared) vocabularies (Nguyen and Chiang, 2017; Kocmi and Bojar, 2018; Neubig and Hu, 2018; Gheini and May, 2019; Liu et al., 2020), and no assignment at all, which results in training randomly initialized embeddings (Aji et al., 2020). In our experiments, we choose to always use new random initialization for the new embeddings (including token embeddings, positional embeddings, and corresponding layer norm parameters). This decision is made to later let us study what happens to embeddings under each of the settings, independent of any pretraining artifacts that exist in them. For instance, when transferring from Fr–En to {Ro–En, Fr–Es},"
2021.emnlp-main.132,N19-4009,0,0.0179706,"additionally include Ha–En, Fr–Es, and Fr–De. We designate Fr–En as the parent language pair and Ro–En, Ja–En, De–En, Ha–En (new source), Fr–Es, Fr–De (new target) as child language pairs. Our Fr–En parent model is trained on the Europarl + Common Crawl subset of WMT14 Fr–En,5 which comprises 5,251,875 sentences. Details and statistics of the data for the child language pairs are provided in Table 1. Model Details. We use the Transformer base architecture (6 layers of encoder and decoder with model dimension of 512 and 8 attention heads) for all models, (Vaswani et al., 2017) and the Fairseq (Ott et al., 2019) toolkit for all our experiments. All models rely on BPE subword vocabularies (Sennrich et al., 2016) processed through the SentencePiece (Kudo and Richardson, 2018) BPE implementation. The vocabulary for the parent model consists of 32K French subwords on the source side, and 32K English subwords on the target side. The sizes of the vocabularies for child models are also reported in Table 1. We follow the advice from Gowda and May (2020) when deciding what vocabulary size to choose, i.e., we choose the maximum number of operations to ensure a minimum of 100 tokens per type. 4 Results and Anal"
2021.emnlp-main.132,2020.emnlp-main.361,0,0.0303184,"ime of fine-tuning, they are not able to reveal anything about the importance of pretrained modules. Our approach, however, enables highlighting the crucial role of the encoded translation knowledge by contrasting {src,tgt}+xattn and {src,tgt}+randxattn. Bapna and Firat (2019) devise adapters for MT by inserting language pairspecific adapter parameters in the Transformer architecture. In the multilingual setting, they show that by fine-tuning adapters in a shared pretrained multilingual model, they can compensate for the performance drop of high-resource languages incurred by shared training. Philip et al. (2020) replace language pair-specific adapters with monolingual adapters, which enables adapting under the zero-shot setting. Another family of lightweight fine-tuning approaches (Li and Liang, 2021; Hambardzumyan et al., 2021; Lester et al., 2021), inspired by prompt tuning (Brown et al., 2020), also relies on updating a set of additional new parameters from scratch towards each downstream task. Such sets of parameters equal a very small fraction of the total parameters in the pretrained model. By contrast, our approach updates a subset of the model’s own parameters instead of adding new ones. We l"
2021.emnlp-main.132,W18-6319,0,0.0185147,"We follow the advice from Gowda and May (2020) when deciding what vocabulary size to choose, i.e., we choose the maximum number of operations to ensure a minimum of 100 tokens per type. 4 Results and Analysis Our preliminary empirical results consist of five experiments for each of the child language pairs based on methods described in §3.1: scratch, {src,tgt}, {src,tgt}+body, {src,tgt}+xattn, and {src,tgt}+randxattn. Our core results, which rely on transferring from the Fr–En parent under each setting, are reported in Table 2. All scores are detokenized cased BLEU computed using S ACREBLEU (Post, 2018).6 across all six language pairs, suggesting that cross-attention is capable of taking advantage of encoded generic translation knowledge in the Transformer body to adapt to each child task. Performance gain from {src,tgt} and drop from {src,tgt}+body when changing the target language (i.e., Fr–Es and Fr–De) are more pronounced than when transferring the source. This is expected—when changing the target, two out of three cross-attention matrices (key and value matrices) are now exposed to a new language. When transferring source, only the query matrix is exposed to the new language. Storage. W"
2021.emnlp-main.132,N18-2084,0,0.0246657,") and Ro–De (using transferred Ro–En and Fr–De models). To put zero-shot scores in context, for each pair we also train a model from scratch: for De– Es using 294,216-sentence News Commentary v14 corpus, and for Ro–Es and Ro–De using 387,653sentence and 385,663-sentence Europarl corpora respectively. All scores are provided in Table 3. Zero-shot BLEU Supervised BLEU De–Es Ro–Es Ro–De 9.2 18.3 14.7 18.6 9.8 13.4 Table 3: Performance of zero-shot systems for three language pairs. De–Es is evaluated on newstest2013 test set. Ro–Es and Ro–De are evaluated on respective TED talks corpus test sets (Qi et al., 2018). In the case of De–Es, we train two additional models from scratch on 50,000- and 100,000- sentence subsets of the training corpus. These respectively score 7.2 and 12.0 BLEU on the newstest2013 De–Es test set (v.s. zero-shot performance of 9.2). Taken together, these results show that the zero-shot methods we obtain from crossattention-based transfer can yield reasonable translation models in the absence of parallel data. 6 Related Work Studying Cross-attention. Several recent works consider the importance of self- and cross-attention heads in the Transformer architecture (Voita et al., 2019"
2021.emnlp-main.132,P16-1162,0,0.0269261,"Ro–En, Ja–En, De–En, Ha–En (new source), Fr–Es, Fr–De (new target) as child language pairs. Our Fr–En parent model is trained on the Europarl + Common Crawl subset of WMT14 Fr–En,5 which comprises 5,251,875 sentences. Details and statistics of the data for the child language pairs are provided in Table 1. Model Details. We use the Transformer base architecture (6 layers of encoder and decoder with model dimension of 512 and 8 attention heads) for all models, (Vaswani et al., 2017) and the Fairseq (Ott et al., 2019) toolkit for all our experiments. All models rely on BPE subword vocabularies (Sennrich et al., 2016) processed through the SentencePiece (Kudo and Richardson, 2018) BPE implementation. The vocabulary for the parent model consists of 32K French subwords on the source side, and 32K English subwords on the target side. The sizes of the vocabularies for child models are also reported in Table 1. We follow the advice from Gowda and May (2020) when deciding what vocabulary size to choose, i.e., we choose the maximum number of operations to ensure a minimum of 100 tokens per type. 4 Results and Analysis Our preliminary empirical results consist of five experiments for each of the child language pai"
2021.emnlp-main.132,P19-1580,0,0.401579,"ay}@isi.edu Abstract Fine-tuning pretrained models often involves updating all parameters of the model without making We study the power of cross-attention in the a distinction between them based on their imporTransformer architecture within the context of transfer learning for machine translation, tance. However, copious recent studies have looked and extend the findings of studies into crossinto the relative cruciality of multi-headed self- and attention when training from scratch. We cross- attention layers when training an MT model conduct a series of experiments through finefrom scratch (Voita et al., 2019; Michel et al., 2019; tuning a translation model on data where eiYou et al., 2020). Cross-attention (also known as ther the source or target language has changed. encoder-decoder attention) layers are more imporThese experiments reveal that fine-tuning only tant than self-attention layers in the sense that they the cross-attention parameters is nearly as effective as fine-tuning all parameters (i.e., the result in more degradation in quality when pruned, entire translation model). We provide insights and hence, are more sensitive to pruning (Voita into why this is the case and observe that li"
2021.emnlp-main.132,N15-1104,0,0.0547712,"Missing"
2021.emnlp-main.132,2020.acl-main.687,0,0.374037,"eters of the model without making We study the power of cross-attention in the a distinction between them based on their imporTransformer architecture within the context of transfer learning for machine translation, tance. However, copious recent studies have looked and extend the findings of studies into crossinto the relative cruciality of multi-headed self- and attention when training from scratch. We cross- attention layers when training an MT model conduct a series of experiments through finefrom scratch (Voita et al., 2019; Michel et al., 2019; tuning a translation model on data where eiYou et al., 2020). Cross-attention (also known as ther the source or target language has changed. encoder-decoder attention) layers are more imporThese experiments reveal that fine-tuning only tant than self-attention layers in the sense that they the cross-attention parameters is nearly as effective as fine-tuning all parameters (i.e., the result in more degradation in quality when pruned, entire translation model). We provide insights and hence, are more sensitive to pruning (Voita into why this is the case and observe that limet al., 2019; Michel et al., 2019). Also, crossiting fine-tuning in this manner yi"
2021.emnlp-main.132,D16-1163,1,0.901483,"that is needed in our case relative to having to store a full new model for each adapted task. Our contributions are: 1) We empirically show the competitive performance of exclusively finetuning the cross-attention layers when contrasted with fine-tuning the entire Transformer body; 2) We show that when fine-tuning only the cross-attention layers, the new embeddings get aligned with the respective embeddings in the pretrained model. The same effect does not hold when fine-tuning the en2 tire Transformer body; 3) we demonstrate effective Freezing shared language embeddings is common practice (Zoph et al., 2016). application of this aligning artifact in mitigating 1755 catastrophic forgetting (Goodfellow et al., 2014) and zero-shot translation. 2 Cross-Attention Fine-Tuning for MT Fine-tuning pretrained Transformer models towards downstream tasks has pushed the limits of NLP, and MT has been no exception (Liu et al., 2020). Despite the prevalence of using pretrained Transformers, recent studies focus on investigating the importance of self- and cross- attention heads while training models from scratch (Voita et al., 2019; Michel et al., 2019; You et al., 2020). These studies verify the relative impor"
2021.emnlp-main.40,N19-1252,1,0.874966,"Missing"
2021.emnlp-main.40,D18-1045,0,0.0269659,"PDTB U KBP Mic. .05 -.69 -1.14 -2.17 Mac. .83 -1.41 .68 -2.94 Table 3: We run LinReg (LR) on the α weights from multitask trials to predict Micro and Macro F1-scores (i.e. LR(α) = Mic. F1-score, Mac. F1-score). LR coefficients (β) for each dataset show the effects of each dataset on the scores. E.g. increasing RST’s weight by +1 yields .5 Mic. F1-score improvement. len and Hoos, 2020). For each datapoint (xi , yi ) in our primary dataset, we generate k = 10 noisy samples (xi1 , yi ), ..., (xik , yi ). We use a samplingbased backtranslation function to generate augmentations for TDA and UDA. (Edunov et al., 2018).19 UDA is a form of semisupervised learning that propagates signal from labeled to unlabeled datapoints, making use of the manifold assumption in semi-supervised learning (Xie et al., 2020; Van Engelen and Hoos, 2020). UDA seeks to promote consistency between model predictions on unlabeled datapoints pθ (xi ) and their augmentations {pθ (xˆi )}kj=1 by minimizing their KL-divergence.20 To test our hypothesis that labeled information in the multitask setup helps us achieve higher accuracy, we perform the following ablation: we test using additional data that does not contain new label informati"
2021.emnlp-main.40,P19-1206,0,0.0170857,"blished, due in part to label correlations across tasks, which improve performance for underrepresented classes. We also offer an extensive review of additional techniques proposed to address resource-poor problems in NLP, and show that none of these approaches can improve classification accuracy in our setting1 . 1 Introduction Learning the discourse structure of a text has been shown to be helpful for diverse tasks such as event extraction (Choubey et al., 2020), sentiment analysis (Chenlo et al., 2014), natural language generation (Celikyilmaz et al., 2020), summarization (Lu et al., 2019; Isonuma et al., 2019), storyline discovery (Rehm et al., 2019), and even misinformation detection (Abbas, 2020; Zhou et al., 2020). However, even as recent advances in NLP allow us to achieve impressive results across a variety of tasks, discourse learning, a supervised learning task, faces the following challenges: (1) discourse datasets tend to be very class-imbalanced.2 (2) Discourse learning is a complex task: human annotators require training and conferencing to achieve moderate agreement (Das et al., 2017). (3) Discourse learning tends to be resource-poor, as annotation complexities make large-scale data col"
2021.emnlp-main.40,N18-1202,0,0.0388342,"Missing"
2021.emnlp-main.40,P16-2067,0,0.0604264,"Missing"
2021.naacl-main.254,D17-2014,0,0.0691519,"Missing"
2021.naacl-main.283,2020.acl-main.421,0,0.0358265,". (Tjong Kim Sang, 2002; Tjong Kim Sang and ∗ De Meulder, 2003). On the other hand, crossWork was started while the first author was a research intern at Adobe. lingual Natural Language Understanding (NLU) 3617 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3617–3632 June 6–11, 2021. ©2021 Association for Computational Linguistics tasks have gained less attention, with smaller benchmark datasets that cover a handful of languages and don’t truly model linguistic variety (Conneau et al., 2018; Artetxe et al., 2020). Natural Language Understanding tasks are critical for dialog systems, as they make up an integral part of the dialog pipeline. Understanding and improving the mechanism behind cross-lingual transfer for natural language understanding in dialog systems require evaluations on more challenging and typologically diverse benchmarks. Numerous approaches have attempted to build stronger cross-lingual representations on top of those multilingual models; however, most require parallel corpora (Wang et al., 2019; Lample and Conneau, 2019) and are biased towards highresource and balanced setups. This f"
2021.naacl-main.283,2020.repl4nlp-1.1,0,0.0435506,"learning is a technique used et al., 2019). to adapt a model trained on a downstream task in a The generalization of such representations has source language to directly generalize to the task in been extensively evaluated on traditional tasks such new languages. It aims to come up with common as Part-of-Speech (POS) tagging, Named Entity cross-lingual representations and leverages them to Recognition (NER) and Cross-lingual Document bridge the divide between resources to make any Classification (CLDC) (Ahmad et al., 2019; Wu NLP application scale to multiple languages. This and Dredze, 2019; Bari et al., 2020a; Schwenk is particularly useful for data-scarce scenarios, as it and Li, 2018), with ever-growing open commureduces the need for API calls implied by machine nity annotation efforts like Universal Dependentranslation or costly task-specific annotation for cies (Nivre et al., 2020) and CoNLL shared tasks new languages. (Tjong Kim Sang, 2002; Tjong Kim Sang and ∗ De Meulder, 2003). On the other hand, crossWork was started while the first author was a research intern at Adobe. lingual Natural Language Understanding (NLU) 3617 Proceedings of the 2021 Conference of the North American Chapter of t"
2021.naacl-main.283,D18-1398,0,0.115069,"oss-lingual representations on top of those multilingual models; however, most require parallel corpora (Wang et al., 2019; Lample and Conneau, 2019) and are biased towards highresource and balanced setups. This fuels the need for a method that doesn’t require explicit crosslingual alignment for faster adaptation to lowresource setups. Meta-learning, a method for “learning to learn”, has found favor especially among the computer vision and speech recognition communities (Nichol et al., 2018; Triantafillou et al., 2020; Winata et al., 2020). Meta-learning has been used for machine translation (Gu et al., 2018), few-shot relation classification (Gao et al., 2019), and on a variety of GLUE tasks (Dou et al., 2019). Recently, Nooralahzadeh et al. (2020) apply the MAML (Finn et al., 2017) algorithm to crosslingual transfer learning for XNLI (Conneau et al., 2018) and MLQA (Lewis et al., 2020), NLU tasks that are naturally biased towards machine translation-based solutions. Nooralahzadeh et al. are able to show improvement over strong multilingual models, including M-BERT. However, they mainly show the effects of meta-learning as a first step in a framework that relies on supervised finetuning, making i"
2021.naacl-main.283,D19-1252,0,0.0471437,"Missing"
2021.naacl-main.283,2020.clssts-1.5,0,0.161057,"are biased towards highresource and balanced setups. This fuels the need for a method that doesn’t require explicit crosslingual alignment for faster adaptation to lowresource setups. Meta-learning, a method for “learning to learn”, has found favor especially among the computer vision and speech recognition communities (Nichol et al., 2018; Triantafillou et al., 2020; Winata et al., 2020). Meta-learning has been used for machine translation (Gu et al., 2018), few-shot relation classification (Gao et al., 2019), and on a variety of GLUE tasks (Dou et al., 2019). Recently, Nooralahzadeh et al. (2020) apply the MAML (Finn et al., 2017) algorithm to crosslingual transfer learning for XNLI (Conneau et al., 2018) and MLQA (Lewis et al., 2020), NLU tasks that are naturally biased towards machine translation-based solutions. Nooralahzadeh et al. are able to show improvement over strong multilingual models, including M-BERT. However, they mainly show the effects of meta-learning as a first step in a framework that relies on supervised finetuning, making it difficult to properly compare and contrast both approaches. sification task, MTOD is a joint classification and sequence labelling task and i"
2021.naacl-main.283,2020.acl-main.653,0,0.0411571,"l alignment for faster adaptation to lowresource setups. Meta-learning, a method for “learning to learn”, has found favor especially among the computer vision and speech recognition communities (Nichol et al., 2018; Triantafillou et al., 2020; Winata et al., 2020). Meta-learning has been used for machine translation (Gu et al., 2018), few-shot relation classification (Gao et al., 2019), and on a variety of GLUE tasks (Dou et al., 2019). Recently, Nooralahzadeh et al. (2020) apply the MAML (Finn et al., 2017) algorithm to crosslingual transfer learning for XNLI (Conneau et al., 2018) and MLQA (Lewis et al., 2020), NLU tasks that are naturally biased towards machine translation-based solutions. Nooralahzadeh et al. are able to show improvement over strong multilingual models, including M-BERT. However, they mainly show the effects of meta-learning as a first step in a framework that relies on supervised finetuning, making it difficult to properly compare and contrast both approaches. sification task, MTOD is a joint classification and sequence labelling task and is more typologically diverse. TyDiQA is not a classification task, but we show how meta-learning can be applied usefully to it. We also show"
2021.naacl-main.283,2020.emnlp-main.484,0,0.0460222,"Missing"
2021.naacl-main.283,D19-1129,0,0.0634266,"Missing"
2021.naacl-main.283,K19-1061,1,0.840535,"Missing"
2021.naacl-main.283,D19-1575,0,0.0235775,"a handful of languages and don’t truly model linguistic variety (Conneau et al., 2018; Artetxe et al., 2020). Natural Language Understanding tasks are critical for dialog systems, as they make up an integral part of the dialog pipeline. Understanding and improving the mechanism behind cross-lingual transfer for natural language understanding in dialog systems require evaluations on more challenging and typologically diverse benchmarks. Numerous approaches have attempted to build stronger cross-lingual representations on top of those multilingual models; however, most require parallel corpora (Wang et al., 2019; Lample and Conneau, 2019) and are biased towards highresource and balanced setups. This fuels the need for a method that doesn’t require explicit crosslingual alignment for faster adaptation to lowresource setups. Meta-learning, a method for “learning to learn”, has found favor especially among the computer vision and speech recognition communities (Nichol et al., 2018; Triantafillou et al., 2020; Winata et al., 2020). Meta-learning has been used for machine translation (Gu et al., 2018), few-shot relation classification (Gao et al., 2019), and on a variety of GLUE tasks (Dou et al., 2019)."
2021.naacl-main.283,2020.acl-main.348,0,0.292112,"are biased towards highresource and balanced setups. This fuels the need for a method that doesn’t require explicit crosslingual alignment for faster adaptation to lowresource setups. Meta-learning, a method for “learning to learn”, has found favor especially among the computer vision and speech recognition communities (Nichol et al., 2018; Triantafillou et al., 2020; Winata et al., 2020). Meta-learning has been used for machine translation (Gu et al., 2018), few-shot relation classification (Gao et al., 2019), and on a variety of GLUE tasks (Dou et al., 2019). Recently, Nooralahzadeh et al. (2020) apply the MAML (Finn et al., 2017) algorithm to crosslingual transfer learning for XNLI (Conneau et al., 2018) and MLQA (Lewis et al., 2020), NLU tasks that are naturally biased towards machine translation-based solutions. Nooralahzadeh et al. are able to show improvement over strong multilingual models, including M-BERT. However, they mainly show the effects of meta-learning as a first step in a framework that relies on supervised finetuning, making it difficult to properly compare and contrast both approaches. sification task, MTOD is a joint classification and sequence labelling task and i"
2021.naacl-main.283,D19-1077,0,0.046422,"Missing"
2021.naacl-main.90,D18-1549,0,0.0229342,"h cases. Apart from the trained BLEURTmedian metric, M ACRO F1 scores higher than the others on CS-EN, and is competitive on CS-EN. M ACRO F1 is not the metric with highest IR task correlation in this setting, unlike in Section 3.3.1, however it is competitive with B LEU and C HR F1 , and thus a safe choice as a downstream task performance indicator. 4 Spotting Qualitative Differences Between Supervised and Unsupervised NMT with M ACRO F1 Unsupervised neural machine translation (UNMT) systems trained on massive monolingual data without parallel corpora have made significant progress recently (Artetxe et al., 2018; Lample et al., 2018a,b; Conneau and Lample, 2019; Song et al., 2019; Liu et al., 2020). In some cases, UNMT yields a B LEU score that is comparable with strong 11 supervised neural machine transla11 though not, generally, the strongest tion (SNMT) systems. In this section we leverage M ACRO F1 to investigate differences in the translations from UNMT and SNMT systems that have similar B LEU. We compare UNMT and SNMT for English ↔ German (EN-DE, DE-EN), English ↔ French (ENFR, FR-EN), and English ↔ Romanian (EN-RO, RO-EN). All our UNMT models are based on XLM (Conneau and Lample, 2019), pretra"
2021.naacl-main.90,W05-0909,0,0.27503,"ni et al., gram is used as its weight, such that rare n-grams attain relatively more importance than in BLEU. 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), and C HR F1 (Popovi´c, 2015). Model- We abandon this direction for two reasons: Firstly, based metrics have a significant number of pa- as noted in that work, large amounts of data are required to estimate n-gram statistics. Secondly, rameters and, sometimes, external resources that unequal weighing is a bias that is best suited to must be set prior to use. These include METEOR datasets where the weights are derived from, and (Banerjee and Lavie, 2005), BLEURT (Sellam et al., 2020), YiSi (Lo, 2019), ESIM (Mathur et al., such biases often do not generalize to other datasets. 2019), and BEER (Stanojevi´c and Sima’an, 2014). Therefore, unlike Doddington (2002), we assign equal weights to all n-gram classes, and in this Model-based metrics require significant effort and resources when adapting to a new language or do- work we limit our scope to unigrams only. main, while model-free metrics require only a test While B LEU is a precision-oriented measure, 1144 METEOR (Banerjee and Lavie, 2005) and CHRF (Popovi´c, 2015) include both precision and"
2021.naacl-main.90,W17-4755,0,0.0488912,"Missing"
2021.naacl-main.90,P19-3004,1,0.702824,"luent output for proper translations (Callison-Burch et al., 2007). 3.3.2 Europarl Datasets CLSSTS datasets contain queries in English (EN), and documents in many source languages along with their human translations, as well as querydocument relevance judgments. We use three source languages: Lithuanian (LT), Pashto (PS), and Bulgarian (BG). The performance of this CLIR task is evaluated using two IR measures: Actual Query Weighted Value (AQWV) and Mean Average Precision (MAP). AQWV 9 is derived from Actual Term Weighted Value (ATWV) metric (Wegmann et al., 2013). We use a single CLIR system (Boschee et al., 2019) with the same IR settings for all MT models in the set, and measure Kendall’s τ between MT and IR measures. The results, in Table 4, show that M ACRO F1 is the strongest indicator of CLIR downstream task performance in five out of six settings. AQWV and MAP have a similar trend in agreement to the MT metrics. C HR F1 and BLEURT, which are strong contenders when generated text is directly evaluated by humans, do not indicate We perform a similar analysis to Section 3.3.1 but on another cross-lingual task set up by Lignos et al. (2019) for Czech → English (CS-EN) and German → English (DE-EN), u"
2021.naacl-main.90,W07-0718,0,0.0741243,"bility of findings. An essential resource of this analysis is a dataset with human annotations for computing MT and IR performances. We conduct experiments on two datasets: firstly, on data from the 2020 workshop on Cross-Language Search and Summarization of Text and Speech (CLSSTS) (Zavorin et al., 2020), and secondly, on data originally from Europarl, prepared by Lignos et al. (2019) (Europarl). CLIR task performance as well as M ACRO F1 , as CLIR tasks require faithful meaning equivalence across the language boundary, and human translators can mistake fluent output for proper translations (Callison-Burch et al., 2007). 3.3.2 Europarl Datasets CLSSTS datasets contain queries in English (EN), and documents in many source languages along with their human translations, as well as querydocument relevance judgments. We use three source languages: Lithuanian (LT), Pashto (PS), and Bulgarian (BG). The performance of this CLIR task is evaluated using two IR measures: Actual Query Weighted Value (AQWV) and Mean Average Precision (MAP). AQWV 9 is derived from Actual Term Weighted Value (ATWV) metric (Wegmann et al., 2013). We use a single CLIR system (Boschee et al., 2019) with the same IR settings for all MT models"
2021.naacl-main.90,M92-1002,0,0.6672,"al imbalance of class distribution. BEER (Stanojevi´c and Sima’an, 2014) and METEOR (Denkowski and Lavie, 2011) make an explicit distinction between function and content words; such a distinction inherently captures frequency differences since function words are often frequent and content words are often infrequent types. However, doing so requires the construction of potentially expensive linguistic resources. This work does not make any explicit distinction and uses naturally occurring type counts to effect a similar result. 5.3 F-measure as an Evaluation Metric F-measure (Rijsbergen, 1979; Chinchor, 1992) is extensively used as an evaluation metric in classification tasks such as part-of-speech tagging, named entity recognition, and sentiment analysis (Derczynski, 2016). Viewing MT as a multi-class classifier is a relatively new paradigm (Gowda and May, 2020), and evaluating MT solely as a multiclass classifier as proposed in this work is not an established practice. However, we find that the F1 measure is sometimes used for various analyses when B LEU and others are inadequate: The compare-mt tool (Neubig et al., 2019) supports comparison of MT models based on F1 measure of individual types."
2021.naacl-main.90,W11-2107,0,0.0379661,"erefore, unlike Doddington (2002), we assign equal weights to all n-gram classes, and in this Model-based metrics require significant effort and resources when adapting to a new language or do- work we limit our scope to unigrams only. main, while model-free metrics require only a test While B LEU is a precision-oriented measure, 1144 METEOR (Banerjee and Lavie, 2005) and CHRF (Popovi´c, 2015) include both precision and recall, similar to our methods. However, neither of these measures try to address the natural imbalance of class distribution. BEER (Stanojevi´c and Sima’an, 2014) and METEOR (Denkowski and Lavie, 2011) make an explicit distinction between function and content words; such a distinction inherently captures frequency differences since function words are often frequent and content words are often infrequent types. However, doing so requires the construction of potentially expensive linguistic resources. This work does not make any explicit distinction and uses naturally occurring type counts to effect a similar result. 5.3 F-measure as an Evaluation Metric F-measure (Rijsbergen, 1979; Chinchor, 1992) is extensively used as an evaluation metric in classification tasks such as part-of-speech tagg"
2021.naacl-main.90,L16-1040,0,0.0290583,"ent words; such a distinction inherently captures frequency differences since function words are often frequent and content words are often infrequent types. However, doing so requires the construction of potentially expensive linguistic resources. This work does not make any explicit distinction and uses naturally occurring type counts to effect a similar result. 5.3 F-measure as an Evaluation Metric F-measure (Rijsbergen, 1979; Chinchor, 1992) is extensively used as an evaluation metric in classification tasks such as part-of-speech tagging, named entity recognition, and sentiment analysis (Derczynski, 2016). Viewing MT as a multi-class classifier is a relatively new paradigm (Gowda and May, 2020), and evaluating MT solely as a multiclass classifier as proposed in this work is not an established practice. However, we find that the F1 measure is sometimes used for various analyses when B LEU and others are inadequate: The compare-mt tool (Neubig et al., 2019) supports comparison of MT models based on F1 measure of individual types. Gowda and May (2020) use F1 of individual types to uncover frequency-based bias in MT models. Sennrich et al. (2016) use corpuslevel unigram F1 in addition to B LEU and"
2021.naacl-main.90,P17-1017,0,0.0892372,"the microaverage averages by token: ∑c∈V Fβ ;c ∣V ∣ ∑c∈V f (c) × Fβ ;c M ICRO Fβ = ∑c′ ∈V f (c′ ) M ACRO Fβ = Justification for M ACRO F1 In the following sections, we verify and justify the utility of M ACRO F1 while also offering a comparison with popular alternatives such as M ICRO F1 , B LEU, C HR F1 , and BLEURT. 4 We use Kendall’s rank correlation coefficient, τ, to compute the association between metrics and human judgments. Correlations with p-values smaller than α = 0.05 are considered to be statistically significant. 3.1 Data-to-Text: WebNLG We use the 2017 WebNLG Challenge dataset (Gardent et al., 2017; Shimorina, 2018) 5 to analyze the differences between micro- and macro- averaging. WebNLG is a task of generating English text for sets of triples extracted from DBPedia. Human annotations are available for a sample of 223 records each from nine NLG systems. The human 2 m P REDS(c) = ∑ C(c,h(i) ) i=1 m R EFS(c) = ∑ C(c,y(i) ) i=1 m M ATCH(c) = ∑ min{C(c,h(i) ),C(c,y(i) )} i=1 We consider Fβ ;c for c ∈/ Vh∩y to be 0. We use k = 1. When k → ∞, M ICRO Fβ → M ACRO Fβ . 4 B LEU and C HR F1 scores reported in this work are computed with S ACRE B LEU; see the Appendix for details. BLEURT scores are"
2021.naacl-main.90,2020.findings-emnlp.352,1,0.899527,"y B LEU cannot, especially regarding the adequacy of generated text, and provide a novel approach to qualitative analysis of the effect of metrics choice on quantitative evaluation (Section 4). where f (c) = R EFS(c) + k for smoothing factor k. 3 We scale M ACRO Fβ and M ICRO Fβ values to percentile, similar to B LEU, for the sake of easier readability. 2 3 NMT as Classification Neural machine translation (NMT) models are often viewed as pairs of encoder-decoder networks. Viewing NMT as such is useful in practice for implementation; however, such a view is inadequate for theoretical analysis. Gowda and May (2020) provide a high-level view of NMT as two fundamental ML components: an autoregressor and a classifier. Specifically, NMT is viewed as a multi-class classifier that operates on representations from an autoregressor. We may thus consider classifier-based evaluation metrics. Consider a test corpus, T = {(x(i) ,h(i) ,y(i) )∣i = 1,2,3...m} where x(i) , h(i) , and y(i) are source, system hypothesis, and reference translation, respectively. Let x = {x(i) ∀i} and similar for h and y. Let Vh ,Vy ,Vh∩y , and V be the vocabulary of h, the vocabulary of y, Vh ∩Vy , and Vh ∪Vy , respectively. For each clas"
2021.naacl-main.90,2005.mtsummit-papers.11,0,0.025685,"nd measure Kendall’s τ between MT and IR measures. The results, in Table 4, show that M ACRO F1 is the strongest indicator of CLIR downstream task performance in five out of six settings. AQWV and MAP have a similar trend in agreement to the MT metrics. C HR F1 and BLEURT, which are strong contenders when generated text is directly evaluated by humans, do not indicate We perform a similar analysis to Section 3.3.1 but on another cross-lingual task set up by Lignos et al. (2019) for Czech → English (CS-EN) and German → English (DE-EN), using publicly available data from the Europarl v7 corpus (Koehn, 2005). This task differs from the CLSSTS task (Section 3.3.1) in several ways. Firstly, MT metrics are computed on test sets from the news domain, whereas IR metrics are from the Europarl domain. The domains are thus intentionally mismatched between MT and IR tests. Secondly, since there are no queries specifically created for the Europarl domain, GOV2 TREC topics 701–850 are used as domain-relevant English queries. And lastly, since there are no query-document relevance human judgments for the chosen query and document sets, the documents retrieved by BM25 (Jones et al., 2000) on the English set f"
2021.naacl-main.90,D19-1353,1,0.910978,"s steady progress over the years, and outperforms others in 2019. stronger correlation with the IR metric(s) is more useful than the ones with weaker correlations. 4. Repeat the above steps on many languages to verify the generalizability of findings. An essential resource of this analysis is a dataset with human annotations for computing MT and IR performances. We conduct experiments on two datasets: firstly, on data from the 2020 workshop on Cross-Language Search and Summarization of Text and Speech (CLSSTS) (Zavorin et al., 2020), and secondly, on data originally from Europarl, prepared by Lignos et al. (2019) (Europarl). CLIR task performance as well as M ACRO F1 , as CLIR tasks require faithful meaning equivalence across the language boundary, and human translators can mistake fluent output for proper translations (Callison-Burch et al., 2007). 3.3.2 Europarl Datasets CLSSTS datasets contain queries in English (EN), and documents in many source languages along with their human translations, as well as querydocument relevance judgments. We use three source languages: Lithuanian (LT), Pashto (PS), and Bulgarian (BG). The performance of this CLIR task is evaluated using two IR measures: Actual Query"
2021.naacl-main.90,2020.tacl-1.47,0,0.013828,"s on CS-EN, and is competitive on CS-EN. M ACRO F1 is not the metric with highest IR task correlation in this setting, unlike in Section 3.3.1, however it is competitive with B LEU and C HR F1 , and thus a safe choice as a downstream task performance indicator. 4 Spotting Qualitative Differences Between Supervised and Unsupervised NMT with M ACRO F1 Unsupervised neural machine translation (UNMT) systems trained on massive monolingual data without parallel corpora have made significant progress recently (Artetxe et al., 2018; Lample et al., 2018a,b; Conneau and Lample, 2019; Song et al., 2019; Liu et al., 2020). In some cases, UNMT yields a B LEU score that is comparable with strong 11 supervised neural machine transla11 though not, generally, the strongest tion (SNMT) systems. In this section we leverage M ACRO F1 to investigate differences in the translations from UNMT and SNMT systems that have similar B LEU. We compare UNMT and SNMT for English ↔ German (EN-DE, DE-EN), English ↔ French (ENFR, FR-EN), and English ↔ Romanian (EN-RO, RO-EN). All our UNMT models are based on XLM (Conneau and Lample, 2019), pretrained by Yang (2020). We choose SNMT models with similar B LEU on common test sets by eit"
2021.naacl-main.90,W19-5358,0,0.0673115,"ke BLEU would consider each of the errors above to be equally wrong. The source of model-based metrics’ (e.g. BLEURT) correlative superiority over model-free metrics (e.g. BLEU) appears to be the former’s ability to focus evaluation on adequacy, while the latter are overly focused on fluency. BLEU and 1 Introduction most other generation metrics consider each output Model-based metrics for evaluating machine trans- token equally. Since natural language is dominated by a few high-count types, an MT model that conlation such as BLEURT (Sellam et al., 2020), ESIM (Mathur et al., 2019), and YiSi (Lo, 2019) have re- centrates on getting its if s, ands and buts right will cently attracted attention due to their superior cor- benefit from BLEU in the long run more than one relation with human judgments (Ma et al., 2019). that gets its xylophones, peripatetics, and defenestrates right. Can we derive a metric with the However, B LEU (Papineni et al., 2002) remains the most widely used corpus-level MT metric. It corre- discriminating power of BLEURT that does not share its bias or expense and is as interpretable as lates reasonably well with human judgments, and moreover is easy to understand and che"
2021.naacl-main.90,W18-6450,0,0.0427812,"Missing"
2021.naacl-main.90,W19-5302,0,0.0163066,"lity to focus evaluation on adequacy, while the latter are overly focused on fluency. BLEU and 1 Introduction most other generation metrics consider each output Model-based metrics for evaluating machine trans- token equally. Since natural language is dominated by a few high-count types, an MT model that conlation such as BLEURT (Sellam et al., 2020), ESIM (Mathur et al., 2019), and YiSi (Lo, 2019) have re- centrates on getting its if s, ands and buts right will cently attracted attention due to their superior cor- benefit from BLEU in the long run more than one relation with human judgments (Ma et al., 2019). that gets its xylophones, peripatetics, and defenestrates right. Can we derive a metric with the However, B LEU (Papineni et al., 2002) remains the most widely used corpus-level MT metric. It corre- discriminating power of BLEURT that does not share its bias or expense and is as interpretable as lates reasonably well with human judgments, and moreover is easy to understand and cheap to cal- BLEU? As it turns out, the metric may already exist and culate, requiring only reference translations in the be in common use. Information extraction and target language. By contrast, model-based metrics"
2021.naacl-main.90,P19-1269,0,0.0206621,"al biases; model-free metrics like BLEU would consider each of the errors above to be equally wrong. The source of model-based metrics’ (e.g. BLEURT) correlative superiority over model-free metrics (e.g. BLEU) appears to be the former’s ability to focus evaluation on adequacy, while the latter are overly focused on fluency. BLEU and 1 Introduction most other generation metrics consider each output Model-based metrics for evaluating machine trans- token equally. Since natural language is dominated by a few high-count types, an MT model that conlation such as BLEURT (Sellam et al., 2020), ESIM (Mathur et al., 2019), and YiSi (Lo, 2019) have re- centrates on getting its if s, ands and buts right will cently attracted attention due to their superior cor- benefit from BLEU in the long run more than one relation with human judgments (Ma et al., 2019). that gets its xylophones, peripatetics, and defenestrates right. Can we derive a metric with the However, B LEU (Papineni et al., 2002) remains the most widely used corpus-level MT metric. It corre- discriminating power of BLEURT that does not share its bias or expense and is as interpretable as lates reasonably well with human judgments, and moreover is easy"
2021.naacl-main.90,2020.acl-main.448,0,0.0431109,"avored alternatives, appears in the list for B LEU. This indicates relatively bad judgment on the part of B LEU. One case of good judgment from M ACRO F1 and bad judgment from B LEU regarding truncation is shown in Table 8. From our qualitative examinations, M ACRO F1 is better than B LEU at discriminating against untranslations and trucations in UNMT. The case is similar for FR-EN and RO-EN, except that ROEN has more untranslations for both SNMT and UNMT, possibly due to the smaller training data. Complete tables and annotated sentences are in the Appendix, in Section C. set with references. Mathur et al. (2020) have recently evaluated the utility of popular metrics and recommend the use of either C HR F1 or a model-based metric instead of B LEU. We compare our M ACRO F1 and M ICRO F1 metrics with B LEU, C HR F1 , and BLEURT (Sellam et al., 2020). While Mathur et al. (2020) use Pearson’s correlation coefficient (r) to quantify the correlation between automatic evaluation metrics and human judgements, we instead use Kendall’s rank coefficient (τ), since τ is more robust to outliers than r (Croux and Dehon, 2010). 5.2 Rare Words are Important That natural language word types roughly follow a Zipfian di"
2021.naacl-main.90,N19-4007,0,0.0170959,"ilar result. 5.3 F-measure as an Evaluation Metric F-measure (Rijsbergen, 1979; Chinchor, 1992) is extensively used as an evaluation metric in classification tasks such as part-of-speech tagging, named entity recognition, and sentiment analysis (Derczynski, 2016). Viewing MT as a multi-class classifier is a relatively new paradigm (Gowda and May, 2020), and evaluating MT solely as a multiclass classifier as proposed in this work is not an established practice. However, we find that the F1 measure is sometimes used for various analyses when B LEU and others are inadequate: The compare-mt tool (Neubig et al., 2019) supports comparison of MT models based on F1 measure of individual types. Gowda and May (2020) use F1 of individual types to uncover frequency-based bias in MT models. Sennrich et al. (2016) use corpuslevel unigram F1 in addition to B LEU and C HR F, however, corpus-level F1 is computed as M ICRO F1 . To the best of our knowledge, there is no previous work that clearly formulates the differences between micro- and macro- averages, and justifies the use of M ACRO F1 for MT evaluation. tic adequacy is a key discriminating factor, macroaveraged measures such as M ACRO F1 are better at judging th"
2021.naacl-main.90,N19-4009,0,0.015479,"cuments, whereas Domain=In+Ext are where IR scores are computed on a larger set of documents that is a superset of segments on which MT scores are computed. Bold values are the best correlations achieved in a row-wise setting; values with × are not significant at α = 0.05. B LEU M ACRO F1 M ICRO F1 C HR F1 BT CS-EN .850 .867 .850 .850 .900 DE-EN .900 .900 .900 .912 .917  BT .867 .900 Table 5: Europarl CLIR task: Kendall’s τ between  are short for MT metrics and RBO. BT and BT BLEURTmean and BLEURTmedian. All correlations are significant at α = 0.05. et al., 2017) implemented in the FAIRSeq (Ott et al., 2019) toolkit. For each of CS-EN and DE-EN, a total of 16 NMT models that are based on different quantities of training data and BPE hyperparameter values are used. The results in Table 5 show that BLEURT has the highest correlation in both cases. Apart from the trained BLEURTmedian metric, M ACRO F1 scores higher than the others on CS-EN, and is competitive on CS-EN. M ACRO F1 is not the metric with highest IR task correlation in this setting, unlike in Section 3.3.1, however it is competitive with B LEU and C HR F1 , and thus a safe choice as a downstream task performance indicator. 4 Spotting Qu"
2021.naacl-main.90,P02-1040,0,0.113389,"metrics consider each output Model-based metrics for evaluating machine trans- token equally. Since natural language is dominated by a few high-count types, an MT model that conlation such as BLEURT (Sellam et al., 2020), ESIM (Mathur et al., 2019), and YiSi (Lo, 2019) have re- centrates on getting its if s, ands and buts right will cently attracted attention due to their superior cor- benefit from BLEU in the long run more than one relation with human judgments (Ma et al., 2019). that gets its xylophones, peripatetics, and defenestrates right. Can we derive a metric with the However, B LEU (Papineni et al., 2002) remains the most widely used corpus-level MT metric. It corre- discriminating power of BLEURT that does not share its bias or expense and is as interpretable as lates reasonably well with human judgments, and moreover is easy to understand and cheap to cal- BLEU? As it turns out, the metric may already exist and culate, requiring only reference translations in the be in common use. Information extraction and target language. By contrast, model-based metrics other areas concerned with classification have long require tuning on thousands of examples of human used both micro averaging, which tre"
2021.naacl-main.90,W15-3049,0,0.0445979,"Missing"
2021.naacl-main.90,W98-1218,0,0.392679,"rics and recommend the use of either C HR F1 or a model-based metric instead of B LEU. We compare our M ACRO F1 and M ICRO F1 metrics with B LEU, C HR F1 , and BLEURT (Sellam et al., 2020). While Mathur et al. (2020) use Pearson’s correlation coefficient (r) to quantify the correlation between automatic evaluation metrics and human judgements, we instead use Kendall’s rank coefficient (τ), since τ is more robust to outliers than r (Croux and Dehon, 2010). 5.2 Rare Words are Important That natural language word types roughly follow a Zipfian distribution is a well known phenomenon (Zipf, 1949; Powers, 1998). The frequent types are mainly so-called “stop words,” function words, and other low-information types, while most content words are infrequent types. To counter this natu5 Related Work ral frequency-based imbalance, statistics such as inverted document frequency (IDF) are commonly 5.1 MT Metrics used to weigh the input words in applications such Many metrics have been proposed for MT eval- as information retrieval (Jones, 1972). In NLG uation, which we broadly categorize into model- tasks such as MT, where words are the output of a classifier, there has been scant effort to address free or m"
2021.naacl-main.90,2020.acl-main.704,0,0.317703,"or new domains and languages. Furthermore, their decisions are inherently non-transparent and appear to reflect unwelcome biases. We explore the simple type-based classifier metric, M ACRO F1 , and study its applicability to MT evaluation. We find that M ACRO F1 is competitive on direct assessment, and outperforms others in indicating downstream cross-lingual information retrieval task performance. Further, we show that M ACRO F1 can be used to effectively compare supervised and unsupervised neural machine translation, and reveal significant qualitative differences in the methods’ outputs. 1 (Sellam et al., 2020). Model-based metric scores are also opaque and can hide undesirable biases, as can be seen in Table 1. Reference: Hypothesis: Reference: Hypothesis: You must be a doctor. must be a doctor. He -0.735 Joe -0.975 Sue -1.043 She -1.100 It is the greatest country in the world. is the greatest country in the world. France -0.022 America -0.060 Russia -0.161 Canada -0.309 Table 1: A demonstration of BLEURT’s internal biases; model-free metrics like BLEU would consider each of the errors above to be equally wrong. The source of model-based metrics’ (e.g. BLEURT) correlative superiority over model-fre"
2021.naacl-main.90,P16-1162,0,0.01777,"gging, named entity recognition, and sentiment analysis (Derczynski, 2016). Viewing MT as a multi-class classifier is a relatively new paradigm (Gowda and May, 2020), and evaluating MT solely as a multiclass classifier as proposed in this work is not an established practice. However, we find that the F1 measure is sometimes used for various analyses when B LEU and others are inadequate: The compare-mt tool (Neubig et al., 2019) supports comparison of MT models based on F1 measure of individual types. Gowda and May (2020) use F1 of individual types to uncover frequency-based bias in MT models. Sennrich et al. (2016) use corpuslevel unigram F1 in addition to B LEU and C HR F, however, corpus-level F1 is computed as M ICRO F1 . To the best of our knowledge, there is no previous work that clearly formulates the differences between micro- and macro- averages, and justifies the use of M ACRO F1 for MT evaluation. tic adequacy is a key discriminating factor, macroaveraged measures such as M ACRO F1 are better at judging the generation quality of MT models (Section 3.2). We have found that another popular metric, C HR F1 , also performs well on direct assessment, however, being an implicitly micro-averaged meas"
2021.naacl-main.90,2006.amta-papers.25,0,0.0471348,"ones, 1972). In NLG uation, which we broadly categorize into model- tasks such as MT, where words are the output of a classifier, there has been scant effort to address free or model-based. Model-free metrics compute scores based on translations but have no signifi- the imbalance. Doddington (2002) is the only work we know of in which the ‘information’ of an ncant parameters or hyperparameters that must be tuned a priori; these include B LEU (Papineni et al., gram is used as its weight, such that rare n-grams attain relatively more importance than in BLEU. 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), and C HR F1 (Popovi´c, 2015). Model- We abandon this direction for two reasons: Firstly, based metrics have a significant number of pa- as noted in that work, large amounts of data are required to estimate n-gram statistics. Secondly, rameters and, sometimes, external resources that unequal weighing is a bias that is best suited to must be set prior to use. These include METEOR datasets where the weights are derived from, and (Banerjee and Lavie, 2005), BLEURT (Sellam et al., 2020), YiSi (Lo, 2019), ESIM (Mathur et al., such biases often do not generalize to other datasets. 2019), and BEER ("
2021.naacl-main.90,W14-3354,0,0.0602988,"Missing"
2021.naacl-main.90,2020.clssts-1.2,0,0.0107241,"dian agreements across the years. Judging based on the number of wins, M ACRO F1 has steady progress over the years, and outperforms others in 2019. stronger correlation with the IR metric(s) is more useful than the ones with weaker correlations. 4. Repeat the above steps on many languages to verify the generalizability of findings. An essential resource of this analysis is a dataset with human annotations for computing MT and IR performances. We conduct experiments on two datasets: firstly, on data from the 2020 workshop on Cross-Language Search and Summarization of Text and Speech (CLSSTS) (Zavorin et al., 2020), and secondly, on data originally from Europarl, prepared by Lignos et al. (2019) (Europarl). CLIR task performance as well as M ACRO F1 , as CLIR tasks require faithful meaning equivalence across the language boundary, and human translators can mistake fluent output for proper translations (Callison-Burch et al., 2007). 3.3.2 Europarl Datasets CLSSTS datasets contain queries in English (EN), and documents in many source languages along with their human translations, as well as querydocument relevance judgments. We use three source languages: Lithuanian (LT), Pashto (PS), and Bulgarian (BG)."
D07-1038,N04-1035,1,0.884736,"than that used to obtain the word-for-word alignments). Weights for the second model are then set, typically by counting and smoothing, and this weighted model is used for translation. Realistic approaches scale to large data sets and have yielded better BLEU performance than their idealistic counterparts, but there is a disconnect between the first model (hereafter, the alignment model) and the second (the translation model). Examples of realistic systems are the phrase-based ATS system of Och and Ney (2004), the phrasal-syntax hybrid system Hiero (Chiang, 2005), and the GHKM syntax system (Galley et al., 2004; Galley et al., 2006). For an alignment model, most of these use the Aachen HMM approach (Vogel et al., 1996), the implementation of IBM Model 4 in GIZA++ (Och and Ney, 2000) or, more recently, the semi-supervised EMD algorithm (Fraser and Marcu, 2006). The two-model approach of the realistic path has undeniable empirical advantages and scales to large data sets, but new research tends to focus on development of higher order translation models that are informed only by low-order alignments. We would like to add the analytic power gained from modern translation models to the underlying alignme"
D07-1038,P06-1121,1,0.861846,"tain the word-for-word alignments). Weights for the second model are then set, typically by counting and smoothing, and this weighted model is used for translation. Realistic approaches scale to large data sets and have yielded better BLEU performance than their idealistic counterparts, but there is a disconnect between the first model (hereafter, the alignment model) and the second (the translation model). Examples of realistic systems are the phrase-based ATS system of Och and Ney (2004), the phrasal-syntax hybrid system Hiero (Chiang, 2005), and the GHKM syntax system (Galley et al., 2004; Galley et al., 2006). For an alignment model, most of these use the Aachen HMM approach (Vogel et al., 1996), the implementation of IBM Model 4 in GIZA++ (Och and Ney, 2000) or, more recently, the semi-supervised EMD algorithm (Fraser and Marcu, 2006). The two-model approach of the realistic path has undeniable empirical advantages and scales to large data sets, but new research tends to focus on development of higher order translation models that are informed only by low-order alignments. We would like to add the analytic power gained from modern translation models to the underlying alignment model without sacri"
D07-1038,P01-1030,1,0.236925,"odels. This leads to extraction of more useful linguistic patterns and improved BLEU scores on translation experiments in Chinese and Arabic. 1 Methods of statistical MT Roughly speaking, there are two paths commonly taken in statistical machine translation (Figure 1). The idealistic path uses an unsupervised learning algorithm such as EM (Demptser et al., 1977) to learn parameters for some proposed translation model from a bitext training corpus, and then directly translates using the weighted model. Some examples of the idealistic approach are the direct IBM word model (Berger et al., 1994; Germann et al., 2001), the phrase-based approach of Marcu and Wong (2002), and the syntax approaches of Wu (1996) and Yamada and Knight (2001). Idealistic approaches are conceptually simple and thus easy to relate to observed phenomena. However, as more parameters are added to the model the idealistic approach has not scaled well, for it is increasingly difficult to incorporate large amounts of training data efficiently over an increasingly large search space. Additionally, the EM procedure has a tendency to overfit its training data when the input units have varying explanatory powers, such as variable-size phras"
D07-1038,N04-1014,1,0.587779,"RDS 9,864,294 GIZA CORPUS C HINESE WORDS 7,520,779 RE - ALIGNMENT EXPERIMENT TYPE RULES TUNE TEST baseline initial adjusted 19,138,252 18,698,549 26,053,341 39.08 39.49 39.76 37.77 38.39 38.69 Table 2: A comparison of Chinese BLEU performance between the GIZA baseline (no re-alignment), realignment as proposed in Section 3.2, and re-alignment as modified in Section 5.4 algorithm (Galley et al., 2004) to obtain a rule set for each bootstrap alignment. Now we need an EM algorithm for learning Ythe parameters of the rule set that maximize p(tree, string). Such an algorithm is precorpus sented by Graehl and Knight (2004). The algorithm consists of two components: D ERIV, which is a procedure for constructing a packed forest of derivation trees of rules that explain a (tree, string) bitext corpus given that corpus and a rule set, and T RAIN, which is an iterative parameter-setting procedure. We initially attempted to use the top-down D E RIV algorithm of Graehl and Knight (2004), but as the constraints of the derivation forests are largely lexical, too much time was spent on exploring deadends. Instead we build derivation forests using the following sequence of operations: 1. Binarize rules using the synchrono"
D07-1038,W02-1018,0,0.0498816,"istic patterns and improved BLEU scores on translation experiments in Chinese and Arabic. 1 Methods of statistical MT Roughly speaking, there are two paths commonly taken in statistical machine translation (Figure 1). The idealistic path uses an unsupervised learning algorithm such as EM (Demptser et al., 1977) to learn parameters for some proposed translation model from a bitext training corpus, and then directly translates using the weighted model. Some examples of the idealistic approach are the direct IBM word model (Berger et al., 1994; Germann et al., 2001), the phrase-based approach of Marcu and Wong (2002), and the syntax approaches of Wu (1996) and Yamada and Knight (2001). Idealistic approaches are conceptually simple and thus easy to relate to observed phenomena. However, as more parameters are added to the model the idealistic approach has not scaled well, for it is increasingly difficult to incorporate large amounts of training data efficiently over an increasingly large search space. Additionally, the EM procedure has a tendency to overfit its training data when the input units have varying explanatory powers, such as variable-size phrases or variable-height trees. The realistic path also"
D07-1038,P00-1056,0,0.402881,"nslation. Realistic approaches scale to large data sets and have yielded better BLEU performance than their idealistic counterparts, but there is a disconnect between the first model (hereafter, the alignment model) and the second (the translation model). Examples of realistic systems are the phrase-based ATS system of Och and Ney (2004), the phrasal-syntax hybrid system Hiero (Chiang, 2005), and the GHKM syntax system (Galley et al., 2004; Galley et al., 2006). For an alignment model, most of these use the Aachen HMM approach (Vogel et al., 1996), the implementation of IBM Model 4 in GIZA++ (Och and Ney, 2000) or, more recently, the semi-supervised EMD algorithm (Fraser and Marcu, 2006). The two-model approach of the realistic path has undeniable empirical advantages and scales to large data sets, but new research tends to focus on development of higher order translation models that are informed only by low-order alignments. We would like to add the analytic power gained from modern translation models to the underlying alignment model without sacrificing the efficiency and empirical gains of the two-model approach. By adding the 360 Proceedings of the 2007 Joint Conference on Empirical Methods in N"
D07-1038,J04-4002,0,0.0114153,"elds a set of patterns or rules for a second translation model (which often has a wider parameter space than that used to obtain the word-for-word alignments). Weights for the second model are then set, typically by counting and smoothing, and this weighted model is used for translation. Realistic approaches scale to large data sets and have yielded better BLEU performance than their idealistic counterparts, but there is a disconnect between the first model (hereafter, the alignment model) and the second (the translation model). Examples of realistic systems are the phrase-based ATS system of Och and Ney (2004), the phrasal-syntax hybrid system Hiero (Chiang, 2005), and the GHKM syntax system (Galley et al., 2004; Galley et al., 2006). For an alignment model, most of these use the Aachen HMM approach (Vogel et al., 1996), the implementation of IBM Model 4 in GIZA++ (Och and Ney, 2000) or, more recently, the semi-supervised EMD algorithm (Fraser and Marcu, 2006). The two-model approach of the realistic path has undeniable empirical advantages and scales to large data sets, but new research tends to focus on development of higher order translation models that are informed only by low-order alignments."
D07-1038,H94-1028,0,0.060612,"Missing"
D07-1038,J93-2003,0,0.0369719,"exercise for the reader). A string-totree decoder constructs a derivation forest of derivation trees where the right sides of the rules in a tree, taken together, explain a candidate source sentence. It then outputs the English tree corresponding to the highest-scoring derivation in the forest. 3 Introducing syntax into the alignment model We now lay the ground for a syntactically motivated alignment model. We begin by reviewing an alignment model commonly seen in realistic MT systems and compare it to a syntactically-aware alignment model. 3.1 The traditional IBM alignment model IBM Model 4 (Brown et al., 1993) learns a set of 4 probability tables to compute p(f |e) given a foreign sentence f and its target translation e via the following (greatly simplified) generative story: NPB PP NPB NNP POS taiwan ’s R1: NN IN surplus in x0:NPB R2: x1:NN → NPB NNP POS taiwan ’s PP NN IN NP-C trade between NPB ¥ x1 DT CD NNS the two shores R10: x0:NPB R11: R10: x0:NNP x0:NPB → x0 NPB R17: POS R12: NNP → Ñl x0:IN R4: IN → R13: ó x1:NP-C → PP IN ó x0 ¥ R5: NP-C → between Ü NNP x0:POS taiwan Ñl ’s x0:NP-C → x0 PP IN x0:NP-C in → x1 x0 NP-C x0:NPB PP IN R5: x1:PP x0:NPB R6: → x1 x0 → x0 NPB R19: in in x1:NN R18: PO"
D07-1038,P05-1033,0,0.111985,"el (which often has a wider parameter space than that used to obtain the word-for-word alignments). Weights for the second model are then set, typically by counting and smoothing, and this weighted model is used for translation. Realistic approaches scale to large data sets and have yielded better BLEU performance than their idealistic counterparts, but there is a disconnect between the first model (hereafter, the alignment model) and the second (the translation model). Examples of realistic systems are the phrase-based ATS system of Och and Ney (2004), the phrasal-syntax hybrid system Hiero (Chiang, 2005), and the GHKM syntax system (Galley et al., 2004; Galley et al., 2006). For an alignment model, most of these use the Aachen HMM approach (Vogel et al., 1996), the implementation of IBM Model 4 in GIZA++ (Och and Ney, 2000) or, more recently, the semi-supervised EMD algorithm (Fraser and Marcu, 2006). The two-model approach of the realistic path has undeniable empirical advantages and scales to large data sets, but new research tends to focus on development of higher order translation models that are informed only by low-order alignments. We would like to add the analytic power gained from mo"
D07-1038,D07-1079,1,0.347425,"in Viterbi derivation trees, then use the annotated partial alignments to obtain Viterbi alignments. 4. Use the new alignments as input to the MT system described below. 5.2 The MT system setup A truly idealistic MT system would directly apply the rule weight parameters learned via EM to a machine translation task. As mentioned in Section 1, we maintain the two-model, or realistic approach. Below we briefly describe the translation model, focusing on comparison with the previously described alignment model. Galley et al. (2006) provides a more complete description of the translation model and DeNeefe et al. (2007) provides a more complete description of the end-to-end translation pipeline. Although in principle the re-alignment model and translation model learn parameter weights over the same rule space, in practice we limit the rules used for re-alignment to the set of smallest rules that explain the training corpus and are consistent with the bootstrap alignments. This is a compromise made to reduce the search space for EM. The translation model learns multiple derivations of rules consistent with the re-alignments for each sentence, and learns (a) Chinese re-alignment corpus has 9,864,294 English an"
D07-1038,P06-1097,0,0.184644,"better BLEU performance than their idealistic counterparts, but there is a disconnect between the first model (hereafter, the alignment model) and the second (the translation model). Examples of realistic systems are the phrase-based ATS system of Och and Ney (2004), the phrasal-syntax hybrid system Hiero (Chiang, 2005), and the GHKM syntax system (Galley et al., 2004; Galley et al., 2006). For an alignment model, most of these use the Aachen HMM approach (Vogel et al., 1996), the implementation of IBM Model 4 in GIZA++ (Och and Ney, 2000) or, more recently, the semi-supervised EMD algorithm (Fraser and Marcu, 2006). The two-model approach of the realistic path has undeniable empirical advantages and scales to large data sets, but new research tends to focus on development of higher order translation models that are informed only by low-order alignments. We would like to add the analytic power gained from modern translation models to the underlying alignment model without sacrificing the efficiency and empirical gains of the two-model approach. By adding the 360 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp."
D07-1038,P03-1021,0,0.0114356,"Missing"
D07-1038,C96-2141,0,0.592313,"by counting and smoothing, and this weighted model is used for translation. Realistic approaches scale to large data sets and have yielded better BLEU performance than their idealistic counterparts, but there is a disconnect between the first model (hereafter, the alignment model) and the second (the translation model). Examples of realistic systems are the phrase-based ATS system of Och and Ney (2004), the phrasal-syntax hybrid system Hiero (Chiang, 2005), and the GHKM syntax system (Galley et al., 2004; Galley et al., 2006). For an alignment model, most of these use the Aachen HMM approach (Vogel et al., 1996), the implementation of IBM Model 4 in GIZA++ (Och and Ney, 2000) or, more recently, the semi-supervised EMD algorithm (Fraser and Marcu, 2006). The two-model approach of the realistic path has undeniable empirical advantages and scales to large data sets, but new research tends to focus on development of higher order translation models that are informed only by low-order alignments. We would like to add the analytic power gained from modern translation models to the underlying alignment model without sacrificing the efficiency and empirical gains of the two-model approach. By adding the 360 P"
D07-1038,P96-1021,0,0.0471829,"on experiments in Chinese and Arabic. 1 Methods of statistical MT Roughly speaking, there are two paths commonly taken in statistical machine translation (Figure 1). The idealistic path uses an unsupervised learning algorithm such as EM (Demptser et al., 1977) to learn parameters for some proposed translation model from a bitext training corpus, and then directly translates using the weighted model. Some examples of the idealistic approach are the direct IBM word model (Berger et al., 1994; Germann et al., 2001), the phrase-based approach of Marcu and Wong (2002), and the syntax approaches of Wu (1996) and Yamada and Knight (2001). Idealistic approaches are conceptually simple and thus easy to relate to observed phenomena. However, as more parameters are added to the model the idealistic approach has not scaled well, for it is increasingly difficult to incorporate large amounts of training data efficiently over an increasingly large search space. Additionally, the EM procedure has a tendency to overfit its training data when the input units have varying explanatory powers, such as variable-size phrases or variable-height trees. The realistic path also learns a model of translation, but uses"
D07-1038,J97-3002,0,0.106154,"erative parameter-setting procedure. We initially attempted to use the top-down D E RIV algorithm of Graehl and Knight (2004), but as the constraints of the derivation forests are largely lexical, too much time was spent on exploring deadends. Instead we build derivation forests using the following sequence of operations: 1. Binarize rules using the synchronous binarization algorithm for tree-to-string transducers described in Zhang et al. (2006). 2. Construct a parse chart with a CKY parser simultaneously constrained on the foreign string and English tree, similar to the bilingual parsing of Wu (1997) 1 . 3. Recover all reachable edges by traversing the chart, starting from the topmost entry. Since the chart is constructed bottom-up, leaf lexical constraints are encountered immediately, resulting in a narrower search space and faster running time than the top-down D ERIV algorithm for this application. Derivation forest construction takes around 400 hours of cumulative machine time (4processor machines) for Chinese. The actual running of EM iterations (which directly implements the T RAIN algorithm of Graehl and Knight (2004)) 1 In the cases where a rule is not synchronous-binarizable stan"
D07-1038,P01-1067,1,0.346039,"in Chinese and Arabic. 1 Methods of statistical MT Roughly speaking, there are two paths commonly taken in statistical machine translation (Figure 1). The idealistic path uses an unsupervised learning algorithm such as EM (Demptser et al., 1977) to learn parameters for some proposed translation model from a bitext training corpus, and then directly translates using the weighted model. Some examples of the idealistic approach are the direct IBM word model (Berger et al., 1994; Germann et al., 2001), the phrase-based approach of Marcu and Wong (2002), and the syntax approaches of Wu (1996) and Yamada and Knight (2001). Idealistic approaches are conceptually simple and thus easy to relate to observed phenomena. However, as more parameters are added to the model the idealistic approach has not scaled well, for it is increasingly difficult to incorporate large amounts of training data efficiently over an increasingly large search space. Additionally, the EM procedure has a tendency to overfit its training data when the input units have varying explanatory powers, such as variable-size phrases or variable-height trees. The realistic path also learns a model of translation, but uses that model only to obtain Vi"
D07-1038,N06-1033,1,0.658697,"re for constructing a packed forest of derivation trees of rules that explain a (tree, string) bitext corpus given that corpus and a rule set, and T RAIN, which is an iterative parameter-setting procedure. We initially attempted to use the top-down D E RIV algorithm of Graehl and Knight (2004), but as the constraints of the derivation forests are largely lexical, too much time was spent on exploring deadends. Instead we build derivation forests using the following sequence of operations: 1. Binarize rules using the synchronous binarization algorithm for tree-to-string transducers described in Zhang et al. (2006). 2. Construct a parse chart with a CKY parser simultaneously constrained on the foreign string and English tree, similar to the bilingual parsing of Wu (1997) 1 . 3. Recover all reachable edges by traversing the chart, starting from the topmost entry. Since the chart is constructed bottom-up, leaf lexical constraints are encountered immediately, resulting in a narrower search space and faster running time than the top-down D ERIV algorithm for this application. Derivation forest construction takes around 400 hours of cumulative machine time (4processor machines) for Chinese. The actual runnin"
D07-1038,J04-4004,0,\N,Missing
D07-1038,J08-3004,1,\N,Missing
D07-1038,D08-1076,0,\N,Missing
D11-1125,D10-1059,0,0.0245254,"performs well on high-dimensionality candidate spaces is the MIRA-based approach used by Watanabe et al. (2007) and Chiang et al. (2008b; 2009). Unfortunately, this approach requires a complex architecture that diverges significantly from the MERT approach, and consequently has not been widely adopted. Our goal is to achieve the same performance with minimal modification to MERT. With MERT as a starting point, we have a choice: modify candidate generation, optimization, or both. Although alternative candidate generation methods have been proposed (Macherey et al., 2008; Chiang et al., 2008b; Chatterjee and Cancedda, 2010), we will restrict ourselves to MERT-style candidate generation, in order to minimize divergence from the established MERT tuning architecture. Instead, we focus on the optimization phase. 4.1 Basic Approach While intuitive, the MERT optimization module focuses attention on Hw ’s best policy, and not on its overall prowess at ranking policies. We will create an optimization module that directly addresses Hw ’s ability to rank policies in the hope that this more holistic approach will generalize better to unseen data. Assume that the gold scoring function G decomposes in the following way: G(p)"
D11-1125,D08-1064,0,0.057311,"g as Ranking Mark Hopkins and Jonathan May SDL Language Weaver Los Angeles, CA 90045 {mhopkins,jmay}@sdl.com Abstract We offer a simple, effective, and scalable method for statistical machine translation parameter tuning based on the pairwise approach to ranking (Herbrich et al., 1999). Unlike the popular MERT algorithm (Och, 2003), our pairwise ranking optimization (PRO) method is not limited to a handful of parameters and can easily handle systems with thousands of features. Moreover, unlike recent approaches built upon the MIRA algorithm of Crammer and Singer (2003) (Watanabe et al., 2007; Chiang et al., 2008b), PRO is easy to implement. It uses off-the-shelf linear binary classifier software and can be built on top of an existing MERT framework in a matter of hours. We establish PRO’s scalability and effectiveness by comparing it to MERT and MIRA and demonstrate parity on both phrase-based and syntax-based systems in a variety of language pairs, using large scale data scenarios. 1 Introduction The MERT algorithm (Och, 2003) is currently the most popular way to tune the parameters of a statistical machine translation (MT) system. MERT is well-understood, easy to implement, and runs quickly, but ca"
D11-1125,D08-1024,0,0.648823,"Missing"
D11-1125,N09-1025,0,0.378108,"Missing"
D11-1125,P10-1146,0,0.00603213,"innovation. Several researchers have attempted to address this weakness. Recently, Watanabe et al. (2007) and Chiang et al. (2008b) have developed tuning methods using the MIRA algorithm (Crammer and Singer, 2003) as a nucleus. The MIRA technique of Chiang et al. has been shown to perform well on large-scale tasks with hundreds or thousands of features (2009). However, the technique is complex and architecturally quite different from MERT. Tellingly, in the entire proceedings of ACL 2010 (Hajiˇc et al., 2010), only one paper describing a statistical MT system cited the use of MIRA for tuning (Chiang, 2010), while 15 used MERT.1 Here we propose a simpler approach to tuning that scales similarly to high-dimensional feature spaces. We cast tuning as a ranking problem (Chen et al., 2009), where the explicit goal is to learn to correctly rank candidate translations. Specifically, we follow the pairwise approach to ranking (Herbrich et al., 1999; Freund et al., 2003; Burges et al., 2005; Cao et al., 2007), in which the ranking problem is reduced to the binary classification task of deciding between candidate translation pairs. Of primary concern to us is the ease of adoption of our proposed technique"
D11-1125,W06-1628,0,0.00923834,"Missing"
D11-1125,N04-1035,1,0.552222,"We now detail the three language pairs, two feature scenarios, and two MT models used for our experiments. For each language pair and each MT model we used MERT, MIRA, and PRO to tune with a standard set of baseline features, and used the latter two methods to tune with an extended set of features.8 At the end of every experiment we used the final feature weights to decode a held-out test set and evaluated it with case-sensitive B LEU. The results are in Table 1. 5.1 Systems We used two systems, each based on a different MT model. Our syntax-based system (hereafter, SBMT) follows the model of Galley et al. (2004). Our 8 MERT could not run to a satisfactory completion in any extended feature scenario; as implied in the synthetic data experiment of Section 3, the algorithm makes poor choices for its weights and this leads to low-quality k-best lists and dismal performance, near 0 B LEU in every iteration. 1357 phrase-based system (hereafter, PBMT) follows the model of Och and Ney (2004). In both systems we learn alignments with GIZA++ (Och and Ney, 2000) using IBM Model 4; for Urdu-English and Chinese-English we merged alignments with the refined method, and for Arabic-English we merged with the union m"
D11-1125,H05-1012,0,0.00610831,"polated the weights w0 learned by the classifier in iteration t with those from iteration t − 1 by a factor of Ψ, such that wt = Ψ · w0 + (1 − Ψ) · wt−1 . We found Ψ = 0.1 gave good performance across the board. 20 MERT PRO TUNE 19 18 17 TEST 0 5 10 15 20 Iteration 25 30 Figure 6: Tune and test curves of five repetitions of the same Urdu-English PBMT baseline feature experiment. PRO is more stable than MERT. multi-class stochastic gradient descent to learn feature weights for an MT model. Och and Ney (2002) used maximum entropy to tune feature weights but did not compare pairs of derivations. Ittycheriah and Roukos (2005) used a maximum entropy classifier to train an alignment model using hand-labeled data. Xiong et al. (2006) also used a maximum entropy classifier, in this case to train the reordering component of their MT model. Lattice- and hypergraphbased variants of MERT (Macherey et al., 2008; Kumar et al., 2009) are more stable than traditional MERT, but also require significant engineering efforts. 7 Conclusion We have described a simple technique for tuning an MT system that is on par with the leading techniques, exhibits reliable behavior, scales gracefully to high-dimension feature spaces, and is re"
D11-1125,P07-2045,0,0.0605762,"ing between candidate translation pairs. Of primary concern to us is the ease of adoption of our proposed technique. Because of this, we adhere as closely as possible to the established MERT architecture and use freely available machine learning software. The end result is a technique that scales and performs just as well as MIRA-based tuning, but which can be implemented in a couple of hours by anyone with an existing MERT implementation. Mindful that many would-be enhancements to the 1 The remainder either did not specify their tuning method (though a number of these used the Moses toolkit (Koehn et al., 2007), which uses MERT for tuning) or, in one case, set weights by hand. 1352 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1352–1362, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics state-of-the-art are false positives that only show improvement in a narrowly defined setting or with limited data, we validate our claims on both syntax and phrase-based systems, using multiple language pairs and large data sets. We describe tuning in abstract and somewhat formal terms in Section 2, describe the MERT algorithm"
D11-1125,P09-1019,0,0.0328863,"petitions of the same Urdu-English PBMT baseline feature experiment. PRO is more stable than MERT. multi-class stochastic gradient descent to learn feature weights for an MT model. Och and Ney (2002) used maximum entropy to tune feature weights but did not compare pairs of derivations. Ittycheriah and Roukos (2005) used a maximum entropy classifier to train an alignment model using hand-labeled data. Xiong et al. (2006) also used a maximum entropy classifier, in this case to train the reordering component of their MT model. Lattice- and hypergraphbased variants of MERT (Macherey et al., 2008; Kumar et al., 2009) are more stable than traditional MERT, but also require significant engineering efforts. 7 Conclusion We have described a simple technique for tuning an MT system that is on par with the leading techniques, exhibits reliable behavior, scales gracefully to high-dimension feature spaces, and is remarkably easy to implement. We have demonstrated, via a litany of experiments, that our claims are valid and that this technique is widely applicable. It is our hope that the adoption of PRO tuning leads to fewer headaches during tuning and motivates advanced MT feature engineering research. Acknowledg"
D11-1125,P06-1096,0,0.560472,"Missing"
D11-1125,D08-1076,0,0.295468,"ost prominent example of a tuning method that performs well on high-dimensionality candidate spaces is the MIRA-based approach used by Watanabe et al. (2007) and Chiang et al. (2008b; 2009). Unfortunately, this approach requires a complex architecture that diverges significantly from the MERT approach, and consequently has not been widely adopted. Our goal is to achieve the same performance with minimal modification to MERT. With MERT as a starting point, we have a choice: modify candidate generation, optimization, or both. Although alternative candidate generation methods have been proposed (Macherey et al., 2008; Chiang et al., 2008b; Chatterjee and Cancedda, 2010), we will restrict ourselves to MERT-style candidate generation, in order to minimize divergence from the established MERT tuning architecture. Instead, we focus on the optimization phase. 4.1 Basic Approach While intuitive, the MERT optimization module focuses attention on Hw ’s best policy, and not on its overall prowess at ranking policies. We will create an optimization module that directly addresses Hw ’s ability to rank policies in the hope that this more holistic approach will generalize better to unseen data. Assume that the gold sc"
D11-1125,N03-5008,0,0.0422022,"figuration. The tune and test B LEU at each iteration is depicted in Figure 6. The standard deviation of the final test B LEU of MERT was 0.13 across the five experiment instances, while PRO had a standard deviation of just 0.05. 6 Related Work Several works (Shen et al., 2004; Cowan et al., 2006; Watanabe et al., 2006) have used discriminative techniques to re-rank k-best lists for MT. Tillmann and Zhang (2005) used a customized form of 1360 Urdu-English PBMT tuning stability 21 4-ref BLEU using the support vector machine module of WEKA (Hall et al., 2009) as well as the Stanford classifier (Manning and Klein, 2003). We ran for up to 30 iterations and used the same k and stopping criterion as was used for MERT, though variability of sampling precluded list convergence. While MERT and MIRA use each iteration’s final weights as a starting point for hill-climbing the next iteration, the pairwise ranking approach has no explicit tie to previous iterations. To incorporate such stability into our process we interpolated the weights w0 learned by the classifier in iteration t with those from iteration t − 1 by a factor of Ψ, such that wt = Ψ · w0 + (1 − Ψ) · wt−1 . We found Ψ = 0.1 gave good performance across"
D11-1125,N03-2021,0,0.0425316,"policy’s candidate translations: Hw (p) = i∈I hw (i, p(i)). As can be seen in Figure 1, using w = [−2, 1], Hw (p1 ) = 9 and Hw (p2 ) = −8. The goal of tuning is to learn a weight vector w such that Hw (p) assigns a high score to good policies, and a low score to bad policies.2 To do so, we need information about which policies are good and which are bad. This information is provided by a “gold” scoring function G that maps each policy to a real-valued score. Typically this gold function is B LEU (Papineni et al., 2002), though there are several common alternatives (Lavie and Denkowski, 2009; Melamed et al., 2003; Snover et al., 2006; Chiang et al., 2008a). We want to find a weight vector w such that Hw behaves “similarly” to G on a candidate space s. We assume a loss function ls (Hw , G) which returns the real-valued loss of using scoring function Hw when the gold scoring function is G and the candidate space is s. Thus, we may say the goal of tuning is to find the weight vector w that minimizes loss. 3 MERT In general, the candidate space may have infinitely many source sentences, as well as infinitely many candidate translations per source sentence. In practice, tuning optimizes over a finite subse"
D11-1125,P00-1056,0,0.0693495,"s are in Table 1. 5.1 Systems We used two systems, each based on a different MT model. Our syntax-based system (hereafter, SBMT) follows the model of Galley et al. (2004). Our 8 MERT could not run to a satisfactory completion in any extended feature scenario; as implied in the synthetic data experiment of Section 3, the algorithm makes poor choices for its weights and this leads to low-quality k-best lists and dismal performance, near 0 B LEU in every iteration. 1357 phrase-based system (hereafter, PBMT) follows the model of Och and Ney (2004). In both systems we learn alignments with GIZA++ (Och and Ney, 2000) using IBM Model 4; for Urdu-English and Chinese-English we merged alignments with the refined method, and for Arabic-English we merged with the union method. 5.2 Data Table 2 notes the sizes of the datasets used in our experiments. All tune and test data have four English reference sets for the purposes of scoring. Data lines Train words lines Tune words lines Test words U-E 515K 2.2M 923 16K 938 18K A-E 6.5M 175M 1994 65K 1357 47K C-E 7.9M 173M 1615 42K 1357 37K Table 2: Data sizes for the experiments reported in this paper (English words shown). Class baseline target word discount node coun"
D11-1125,P02-1038,0,0.358557,"proach has no explicit tie to previous iterations. To incorporate such stability into our process we interpolated the weights w0 learned by the classifier in iteration t with those from iteration t − 1 by a factor of Ψ, such that wt = Ψ · w0 + (1 − Ψ) · wt−1 . We found Ψ = 0.1 gave good performance across the board. 20 MERT PRO TUNE 19 18 17 TEST 0 5 10 15 20 Iteration 25 30 Figure 6: Tune and test curves of five repetitions of the same Urdu-English PBMT baseline feature experiment. PRO is more stable than MERT. multi-class stochastic gradient descent to learn feature weights for an MT model. Och and Ney (2002) used maximum entropy to tune feature weights but did not compare pairs of derivations. Ittycheriah and Roukos (2005) used a maximum entropy classifier to train an alignment model using hand-labeled data. Xiong et al. (2006) also used a maximum entropy classifier, in this case to train the reordering component of their MT model. Lattice- and hypergraphbased variants of MERT (Macherey et al., 2008; Kumar et al., 2009) are more stable than traditional MERT, but also require significant engineering efforts. 7 Conclusion We have described a simple technique for tuning an MT system that is on par w"
D11-1125,J04-4002,0,0.216035,"d-out test set and evaluated it with case-sensitive B LEU. The results are in Table 1. 5.1 Systems We used two systems, each based on a different MT model. Our syntax-based system (hereafter, SBMT) follows the model of Galley et al. (2004). Our 8 MERT could not run to a satisfactory completion in any extended feature scenario; as implied in the synthetic data experiment of Section 3, the algorithm makes poor choices for its weights and this leads to low-quality k-best lists and dismal performance, near 0 B LEU in every iteration. 1357 phrase-based system (hereafter, PBMT) follows the model of Och and Ney (2004). In both systems we learn alignments with GIZA++ (Och and Ney, 2000) using IBM Model 4; for Urdu-English and Chinese-English we merged alignments with the refined method, and for Arabic-English we merged with the union method. 5.2 Data Table 2 notes the sizes of the datasets used in our experiments. All tune and test data have four English reference sets for the purposes of scoring. Data lines Train words lines Tune words lines Test words U-E 515K 2.2M 923 16K 938 18K A-E 6.5M 175M 1994 65K 1357 47K C-E 7.9M 173M 1615 42K 1357 37K Table 2: Data sizes for the experiments reported in this paper"
D11-1125,P03-1021,0,0.894037,"asily handle systems with thousands of features. Moreover, unlike recent approaches built upon the MIRA algorithm of Crammer and Singer (2003) (Watanabe et al., 2007; Chiang et al., 2008b), PRO is easy to implement. It uses off-the-shelf linear binary classifier software and can be built on top of an existing MERT framework in a matter of hours. We establish PRO’s scalability and effectiveness by comparing it to MERT and MIRA and demonstrate parity on both phrase-based and syntax-based systems in a variety of language pairs, using large scale data scenarios. 1 Introduction The MERT algorithm (Och, 2003) is currently the most popular way to tune the parameters of a statistical machine translation (MT) system. MERT is well-understood, easy to implement, and runs quickly, but can behave erratically and does not scale beyond a handful of features. This lack of scalability is a significant weakness, as it inhibits systems from using more than a couple dozen features to discriminate between candidate translations and stymies feature development innovation. Several researchers have attempted to address this weakness. Recently, Watanabe et al. (2007) and Chiang et al. (2008b) have developed tuning m"
D11-1125,P02-1040,0,0.120119,"vector x(i, j). This scoring function extends to a policy p by summing the cost of each P of the policy’s candidate translations: Hw (p) = i∈I hw (i, p(i)). As can be seen in Figure 1, using w = [−2, 1], Hw (p1 ) = 9 and Hw (p2 ) = −8. The goal of tuning is to learn a weight vector w such that Hw (p) assigns a high score to good policies, and a low score to bad policies.2 To do so, we need information about which policies are good and which are bad. This information is provided by a “gold” scoring function G that maps each policy to a real-valued score. Typically this gold function is B LEU (Papineni et al., 2002), though there are several common alternatives (Lavie and Denkowski, 2009; Melamed et al., 2003; Snover et al., 2006; Chiang et al., 2008a). We want to find a weight vector w such that Hw behaves “similarly” to G on a candidate space s. We assume a loss function ls (Hw , G) which returns the real-valued loss of using scoring function Hw when the gold scoring function is G and the candidate space is s. Thus, we may say the goal of tuning is to find the weight vector w that minimizes loss. 3 MERT In general, the candidate space may have infinitely many source sentences, as well as infinitely man"
D11-1125,2010.amta-papers.31,0,0.0185473,"s a function from the nonnegative real numbers to the real interval [0, 1]. but added noise to each feature vector, drawn from a zero-mean Gaussian with a standard deviation of 500. The results of the noisy synthetic experiments, also in Figure 3 (the lines labeled “Noisy”), show that the pairwise ranking approach is less successful than before at learning w∗ at high dimensionality, but still greatly outperforms MERT. 4.3 Discussion The idea of learning from difference vectors also lies at the heart of the MIRA-based approaches (Watanabe et al., 2007; Chiang et al., 2008b) and the approach of Roth et al. (2010), which, similar to our method, uses sampling to select vectors. Here, we isolate these aspects of those approaches to create a simpler tuning technique that closely mirrors the ubiquitous MERT architecture. Among other simplifications, we abstract away the choice of MIRA as the classification method (our approach can use any classification technique that learns a separating hyperplane), and we eliminate the need for oracle translations. An important observation is that B LEU does not satisfy the decomposability assumption of Equation (1). An advantage of MERT is that it can directly optimize"
D11-1125,N04-1023,0,0.120075,"Missing"
D11-1125,2006.amta-papers.25,0,0.0382281,"anslations: Hw (p) = i∈I hw (i, p(i)). As can be seen in Figure 1, using w = [−2, 1], Hw (p1 ) = 9 and Hw (p2 ) = −8. The goal of tuning is to learn a weight vector w such that Hw (p) assigns a high score to good policies, and a low score to bad policies.2 To do so, we need information about which policies are good and which are bad. This information is provided by a “gold” scoring function G that maps each policy to a real-valued score. Typically this gold function is B LEU (Papineni et al., 2002), though there are several common alternatives (Lavie and Denkowski, 2009; Melamed et al., 2003; Snover et al., 2006; Chiang et al., 2008a). We want to find a weight vector w such that Hw behaves “similarly” to G on a candidate space s. We assume a loss function ls (Hw , G) which returns the real-valued loss of using scoring function Hw when the gold scoring function is G and the candidate space is s. Thus, we may say the goal of tuning is to find the weight vector w that minimizes loss. 3 MERT In general, the candidate space may have infinitely many source sentences, as well as infinitely many candidate translations per source sentence. In practice, tuning optimizes over a finite subset of source sentences"
D11-1125,P05-1069,0,0.00913318,"Missing"
D11-1125,2006.iwslt-evaluation.14,0,0.0646286,"Missing"
D11-1125,D07-1080,0,0.653898,"arge scale data scenarios. 1 Introduction The MERT algorithm (Och, 2003) is currently the most popular way to tune the parameters of a statistical machine translation (MT) system. MERT is well-understood, easy to implement, and runs quickly, but can behave erratically and does not scale beyond a handful of features. This lack of scalability is a significant weakness, as it inhibits systems from using more than a couple dozen features to discriminate between candidate translations and stymies feature development innovation. Several researchers have attempted to address this weakness. Recently, Watanabe et al. (2007) and Chiang et al. (2008b) have developed tuning methods using the MIRA algorithm (Crammer and Singer, 2003) as a nucleus. The MIRA technique of Chiang et al. has been shown to perform well on large-scale tasks with hundreds or thousands of features (2009). However, the technique is complex and architecturally quite different from MERT. Tellingly, in the entire proceedings of ACL 2010 (Hajiˇc et al., 2010), only one paper describing a statistical MT system cited the use of MIRA for tuning (Chiang, 2010), while 15 used MERT.1 Here we propose a simpler approach to tuning that scales similarly to"
D11-1125,P06-1066,0,0.0329082,"h that wt = Ψ · w0 + (1 − Ψ) · wt−1 . We found Ψ = 0.1 gave good performance across the board. 20 MERT PRO TUNE 19 18 17 TEST 0 5 10 15 20 Iteration 25 30 Figure 6: Tune and test curves of five repetitions of the same Urdu-English PBMT baseline feature experiment. PRO is more stable than MERT. multi-class stochastic gradient descent to learn feature weights for an MT model. Och and Ney (2002) used maximum entropy to tune feature weights but did not compare pairs of derivations. Ittycheriah and Roukos (2005) used a maximum entropy classifier to train an alignment model using hand-labeled data. Xiong et al. (2006) also used a maximum entropy classifier, in this case to train the reordering component of their MT model. Lattice- and hypergraphbased variants of MERT (Macherey et al., 2008; Kumar et al., 2009) are more stable than traditional MERT, but also require significant engineering efforts. 7 Conclusion We have described a simple technique for tuning an MT system that is on par with the leading techniques, exhibits reliable behavior, scales gracefully to high-dimension feature spaces, and is remarkably easy to implement. We have demonstrated, via a litany of experiments, that our claims are valid an"
D11-1125,C04-1072,0,\N,Missing
D15-1136,P13-2009,0,0.017486,"ta that is much narrower and an order of magnitude smaller than that of AMR, primarily the Geoquery corpus (Zelle and Mooney, 1996). The WASP system of Wong and Mooney (2006) uses hierarchical SMT techniques and does not apply semanticspecific improvements; its extension (Wong and Mooney, 2007) incorporates a target-side reordering component much like the one presented in Section 4.4. Jones et al. (2012a) cast semantic parsing as an instance of hyperedge replacement grammar transduction; like this work they use an IBM model-influenced alignment algorithm and a GHKM-based extraction algorithm. Andreas et al. (2013) use phrase-based and hierarchical SMT techniques on Geoquery. Like this work, they perform a transformation of the input semantic representation so that it is amenable to use in an existing machine translation system. However, they are unable to reach the state of the art in performance. Li et al. (2013) directly address GHKM’s word-toterminal alignment requirement by extending that algorithm to handle word-to-node alignment. Our SBMT system is grounded in the theory of tree transducers, which were applied to the task of semantic parsing by Jones et al. (2011; 2012b). Semantic parsing in gene"
D15-1136,W13-2322,1,0.831018,"ng Representation Using Syntax-Based Machine Translation Michael Pust, Ulf Hermjakob, Kevin Knight, Daniel Marcu, Jonathan May Information Sciences Institute Computer Science Department University of Southern California {pust, ulf, knight, marcu, jonmay}@isi.edu Abstract polarit soldier 0 ARG die-01 The soldier was not afraid of dying. The soldier was not afraid to die. The soldier did not fear death. Figure 1: An Abstract Meaning Representation (AMR) with several English renderings. Introduction Abstract Meaning Representation (AMR) is a compact, readable, whole-sentence semantic annotation (Banarescu et al., 2013). It includes entity identification and typing, PropBank semantic roles (Kingsbury and Palmer, 2002), individual entities playing multiple roles, as well as treatments of modality, negation, etc. AMR abstracts in numerous ways, e.g., by assigning the same conceptual structure to fear (v), fear (n), and afraid (adj). Figure 1 gives an example. AMR parsing is a new research problem, with only a few papers published to date (Flanigan et al., 2014; Wang et al., 2015) and a publicly available corpus of more than 10,000 English/AMR pairs.1 New research problems can be tackled either by developing ne"
D15-1136,J93-2003,0,0.0479423,"training, development, and test splits specified in the AMR corpus (Table 1). The training set is used for rule extraction, language modeling, and statistical rule feature calculation. The development set is used both for parameter optimization and qualitatively for hill-climbing. The test set is held out blind for evaluation. We preprocess the English with a simple rule-based tokenizer and, except where noted, lowercase all data. We obtain English–AMR alignments by using the unsupervised alignment approach of Pourdamghani et al. (2014), which linearizes the AMR and then applies the models of Brown et al. (1993) with an additional symmetrization constraint. All parsing results reported in this work are obtained with the Smatch 1.0 software (Cai and Knight, 2013). We compare our results to those of Flanigan et al. (2014) on the AMR 1.0 data splits; we run that work’s JAMR software according to the provided instructions.3 We also compare our results to published scores in the recent work of Wang et al. (2015). Their work uses slightly different data than that used here 4 but in practice we have not seen significant variation in results. 3 https://github.com/jflanigan/jamr LDC2013E117, a pre-released ve"
D15-1136,P13-2131,1,0.81492,"tion that is suitable for string-to-tree SBMT rule extraction and decoding (Section 4.1). 2 LDC Catalog number 2014T12 See e.g. the related work section of Huck et al. (2014). 1143 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1143–1154, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. Target Nodes Edges Alignments SBMT tree labeled unlabeled words to leaves Children Accuracy Metric ordered B LEU (Papineni et al., 2002) AMR parsing graph unlabeled labeled words to leaves + words to edges unordered Smatch (Cai and Knight, 2013) Corpus Training Development Test Tokens 218,021 29,484 30,263 Table 1: Data splits of AMR 1.0, used in this work. Tokens are English, after tokenization. Figure 2: Differences between AMR parsing and syntax-based machine translation (SBMT). 2. Proposing a target-side reordering technique that leverages the fact that child nodes in AMR are unordered (Section 4.4). 3. Introducing a hierarchical AMR-specific language model to ensure generation of likely parent-child relationships (Section 5). 4. Integrating several semantic knowledge sources into the task (Section 6). 5. Developing tuning method"
D15-1136,A00-2018,0,0.379684,"Missing"
D15-1136,P96-1041,0,0.0575384,"e SBMT system proceeds as follows. Given a corpus of (source string, target tree, source-target word alignment) sentence translation training tuples and a corpus of (source, target, score) sentence translation tuning tuples: 1. Rule extraction: A grammar of string-totree rules is induced from training tuples using the GHKM algorithm (Galley et al., 2004; Galley et al., 2006). 2. Local feature calculation: Statistical and indicator features, as described by Chiang et al. (2009), are calculated over the rule grammar. 3. Language model calculation: A KneserNey-interpolated 5-gram language model (Chen and Goodman, 1996) is learned from the yield of the target training trees. 4. Decoding: A beamed bottom-up chart decoder (Pust and Knight, 2009; Hopkins and Langmead, 2010) calculates the optimal derivations given a source string and feature parameter set. 5. Tuning: Feature parameters are optimized using the MIRA learning approach (Chiang et al., 2009) to maximize the objective, typically B LEU (Papineni et al., 2002), associated with a tuning corpus. We initially use this system with no modifications and pretend that English–AMR is a language pair indistinct from any other. 3 Data and Comparisons We use Engli"
D15-1136,N09-1025,1,0.453031,"Missing"
D15-1136,P97-1003,0,0.0336152,"acceptable to GHKM, and hence an entire end-to-end AMR parser may now be built with SBMT tools, the resulting parser does not exhibit very good performance (Table 3, first line). The trees we are learning on are exceedingly flat, and thus yield rules that do not generalize sufficiently. Rules produced from the top of the tree in Figure 3c, such as that in Figure 4a, are only appropriate for cases where fear-01 has exactly three roles: ARG0 (agent), ARG1 (patient), and polarity. We follow the lead of Wang et al. (2010), who in turn were influenced by similar approaches in monolingual parsing (Collins, 1997; Charniak, 2000), and re-structure trees at nodes with more 5 In Figure 3 the negative polarity marker ‘-’ is a string. Disconnected referents labeled ‘*’ are treated as AMR instances with no roles. than three children (i.e. instances with more than one role), to allow generalization of flat structures. However, our trees are unlike syntactic constituent trees in that they do not have labeled nonterminal nodes, so we have no natural choice of an intermediate (“bar”) label. We must choose a meaningful label to characterize an instance and its roles. We initially choose the concept label, resul"
D15-1136,P05-1045,0,0.00345268,"semantic parsing, we have not yet discussed the use of any semantic resources. In this section we rectify that omission. 6.1 Rules from Numerical Quantities and Named Entities While the majority of string-to-tree rules in SBMT systems are extracted from aligned parallel data, it is common practice to dynamically generate additional rules to handle the translation of dates and numerical quantities, as these follow common patterns and are easily detected at decode-time. We follow this practice here, and additionally detect person names at decode-time using the Stanford Named Entity Recognizer (Finkel et al., 2005). We use cased, tokenized source data to build the decode-time rules. We add indicator features to these rules so that our tuning methods can decide how favorable the resources are. We leave as future work the incorporation of named-entity rules for other classes, since most available namedentity recognition beyond person names is at a granularity level that is incompatible with AMR (e.g. we can recognize ‘Location’ but not distinguish between ‘City’ and ‘Country’). 6.2 Hierarchical Semantic Categories fear-01 die-01 ARG1 * Figure 5: Final modification of the AMR data; semantically clustered p"
D15-1136,P14-1134,0,0.298361,"(AMR) with several English renderings. Introduction Abstract Meaning Representation (AMR) is a compact, readable, whole-sentence semantic annotation (Banarescu et al., 2013). It includes entity identification and typing, PropBank semantic roles (Kingsbury and Palmer, 2002), individual entities playing multiple roles, as well as treatments of modality, negation, etc. AMR abstracts in numerous ways, e.g., by assigning the same conceptual structure to fear (v), fear (n), and afraid (adj). Figure 1 gives an example. AMR parsing is a new research problem, with only a few papers published to date (Flanigan et al., 2014; Wang et al., 2015) and a publicly available corpus of more than 10,000 English/AMR pairs.1 New research problems can be tackled either by developing new algorithms/techniques (Flanigan et al., 2014; Wang et al., 2015) or by adapting existing algorithms/techniques to the problem at hand. In this paper, we investigate the second approach. The AMR parsing problem bears a strong formal resemblance to syntax-based machine translation (SBMT) of the string-to-tree variety, in that 1 ARG1 ins t t ins - 1 fear-01 y G 1 st in AR We present a parser for Abstract Meaning Representation (AMR). We treat E"
D15-1136,N04-1035,1,0.67464,"constitute lightweight changes on a baseline SBMT system, we achieve state-of-the-art AMR parsing results. We next describe our baseline, and then describe how we adapt it to AMR parsing. 2 Sentences 10,313 1,368 1,371 Syntax-Based Machine Translation Our baseline SBMT system proceeds as follows. Given a corpus of (source string, target tree, source-target word alignment) sentence translation training tuples and a corpus of (source, target, score) sentence translation tuning tuples: 1. Rule extraction: A grammar of string-totree rules is induced from training tuples using the GHKM algorithm (Galley et al., 2004; Galley et al., 2006). 2. Local feature calculation: Statistical and indicator features, as described by Chiang et al. (2009), are calculated over the rule grammar. 3. Language model calculation: A KneserNey-interpolated 5-gram language model (Chen and Goodman, 1996) is learned from the yield of the target training trees. 4. Decoding: A beamed bottom-up chart decoder (Pust and Knight, 2009; Hopkins and Langmead, 2010) calculates the optimal derivations given a source string and feature parameter set. 5. Tuning: Feature parameters are optimized using the MIRA learning approach (Chiang et al.,"
D15-1136,P06-1121,1,0.857425,"ht changes on a baseline SBMT system, we achieve state-of-the-art AMR parsing results. We next describe our baseline, and then describe how we adapt it to AMR parsing. 2 Sentences 10,313 1,368 1,371 Syntax-Based Machine Translation Our baseline SBMT system proceeds as follows. Given a corpus of (source string, target tree, source-target word alignment) sentence translation training tuples and a corpus of (source, target, score) sentence translation tuning tuples: 1. Rule extraction: A grammar of string-totree rules is induced from training tuples using the GHKM algorithm (Galley et al., 2004; Galley et al., 2006). 2. Local feature calculation: Statistical and indicator features, as described by Chiang et al. (2009), are calculated over the rule grammar. 3. Language model calculation: A KneserNey-interpolated 5-gram language model (Chen and Goodman, 1996) is learned from the yield of the target training trees. 4. Decoding: A beamed bottom-up chart decoder (Pust and Knight, 2009; Hopkins and Langmead, 2010) calculates the optimal derivations given a source string and feature parameter set. 5. Tuning: Feature parameters are optimized using the MIRA learning approach (Chiang et al., 2009) to maximize the"
D15-1136,J02-3001,0,0.0353382,"sting machine translation system. However, they are unable to reach the state of the art in performance. Li et al. (2013) directly address GHKM’s word-toterminal alignment requirement by extending that algorithm to handle word-to-node alignment. Our SBMT system is grounded in the theory of tree transducers, which were applied to the task of semantic parsing by Jones et al. (2011; 2012b). Semantic parsing in general and AMR parsing specifically can be considered a subsumption of many semantic resolution sub-tasks, e.g. named entity recognition (Nadeau and Sekine, 2007), semantic role labeling (Gildea and Jurafsky, 2002), word sense disambiguation (Navigli, 2009) and relation finding (Bach and Badaskar, 2007). 10 Conclusion By restructuring our AMRs we are able to convert a sophisticated SBMT engine into a baseline semantic parser with little additional effort. By further restructuring our data to appropriately model the behavior we want to capture, we are able to rapidly achieve state-of-the-art results. Finally, by incorporating novel language models and external semantic resources, we are able to increase quality even more. This is not the last word on AMR parsing, as fortunately, machine translation techn"
D15-1136,D10-1063,0,0.0114874,"nd a corpus of (source, target, score) sentence translation tuning tuples: 1. Rule extraction: A grammar of string-totree rules is induced from training tuples using the GHKM algorithm (Galley et al., 2004; Galley et al., 2006). 2. Local feature calculation: Statistical and indicator features, as described by Chiang et al. (2009), are calculated over the rule grammar. 3. Language model calculation: A KneserNey-interpolated 5-gram language model (Chen and Goodman, 1996) is learned from the yield of the target training trees. 4. Decoding: A beamed bottom-up chart decoder (Pust and Knight, 2009; Hopkins and Langmead, 2010) calculates the optimal derivations given a source string and feature parameter set. 5. Tuning: Feature parameters are optimized using the MIRA learning approach (Chiang et al., 2009) to maximize the objective, typically B LEU (Papineni et al., 2002), associated with a tuning corpus. We initially use this system with no modifications and pretend that English–AMR is a language pair indistinct from any other. 3 Data and Comparisons We use English–AMR data from the AMR 1.0 corpus, LDC Catalog number 2014T12. In contrast to narrow-domain data sources that are often used in work related to semantic"
D15-1136,W14-3362,0,0.0118261,"re widely available, anyone wishing to generate AMR from text need only follow our recipe and retrain an existing framework with relevant data to quickly obtain state-of-the-art results. Since SBMT and AMR parsing are, in fact, distinct tasks, as outlined in Figure 2, to adapt the SBMT parsing framework to AMR parsing, we develop novel representations and techniques. Some of our key ideas include: 1. Introducing an AMR-equivalent representation that is suitable for string-to-tree SBMT rule extraction and decoding (Section 4.1). 2 LDC Catalog number 2014T12 See e.g. the related work section of Huck et al. (2014). 1143 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1143–1154, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. Target Nodes Edges Alignments SBMT tree labeled unlabeled words to leaves Children Accuracy Metric ordered B LEU (Papineni et al., 2002) AMR parsing graph unlabeled labeled words to leaves + words to edges unordered Smatch (Cai and Knight, 2013) Corpus Training Development Test Tokens 218,021 29,484 30,263 Table 1: Data splits of AMR 1.0, used in this work. Tokens are English, after tokenization"
D15-1136,U11-1005,0,0.0154014,"KM-based extraction algorithm. Andreas et al. (2013) use phrase-based and hierarchical SMT techniques on Geoquery. Like this work, they perform a transformation of the input semantic representation so that it is amenable to use in an existing machine translation system. However, they are unable to reach the state of the art in performance. Li et al. (2013) directly address GHKM’s word-toterminal alignment requirement by extending that algorithm to handle word-to-node alignment. Our SBMT system is grounded in the theory of tree transducers, which were applied to the task of semantic parsing by Jones et al. (2011; 2012b). Semantic parsing in general and AMR parsing specifically can be considered a subsumption of many semantic resolution sub-tasks, e.g. named entity recognition (Nadeau and Sekine, 2007), semantic role labeling (Gildea and Jurafsky, 2002), word sense disambiguation (Navigli, 2009) and relation finding (Bach and Badaskar, 2007). 10 Conclusion By restructuring our AMRs we are able to convert a sophisticated SBMT engine into a baseline semantic parser with little additional effort. By further restructuring our data to appropriately model the behavior we want to capture, we are able to rapi"
D15-1136,C12-1083,1,0.768754,"Missing"
D15-1136,H90-1020,0,0.139969,"s the optimal derivations given a source string and feature parameter set. 5. Tuning: Feature parameters are optimized using the MIRA learning approach (Chiang et al., 2009) to maximize the objective, typically B LEU (Papineni et al., 2002), associated with a tuning corpus. We initially use this system with no modifications and pretend that English–AMR is a language pair indistinct from any other. 3 Data and Comparisons We use English–AMR data from the AMR 1.0 corpus, LDC Catalog number 2014T12. In contrast to narrow-domain data sources that are often used in work related to semantic parsing (Price, 1990; Zelle, 1995; Kuhlmann et al., 2004), the AMR corpus covers a broad range of news and web forum data. We use the training, development, and test splits specified in the AMR corpus (Table 1). The training set is used for rule extraction, language modeling, and statistical rule feature calculation. The development set is used both for parameter optimization and qualitatively for hill-climbing. The test set is held out blind for evaluation. We preprocess the English with a simple rule-based tokenizer and, except where noted, lowercase all data. We obtain English–AMR alignments by using the unsup"
D15-1136,P12-1051,0,0.0106212,"parsing task by modifying training data and adding lightweight AMRspecific features. Several other recent works have used a machine translation approach to semantic parsing, but all have been applied to domain data that is much narrower and an order of magnitude smaller than that of AMR, primarily the Geoquery corpus (Zelle and Mooney, 1996). The WASP system of Wong and Mooney (2006) uses hierarchical SMT techniques and does not apply semanticspecific improvements; its extension (Wong and Mooney, 2007) incorporates a target-side reordering component much like the one presented in Section 4.4. Jones et al. (2012a) cast semantic parsing as an instance of hyperedge replacement grammar transduction; like this work they use an IBM model-influenced alignment algorithm and a GHKM-based extraction algorithm. Andreas et al. (2013) use phrase-based and hierarchical SMT techniques on Geoquery. Like this work, they perform a transformation of the input semantic representation so that it is amenable to use in an existing machine translation system. However, they are unable to reach the state of the art in performance. Li et al. (2013) directly address GHKM’s word-toterminal alignment requirement by extending tha"
D15-1136,N09-2036,1,0.774328,"ation training tuples and a corpus of (source, target, score) sentence translation tuning tuples: 1. Rule extraction: A grammar of string-totree rules is induced from training tuples using the GHKM algorithm (Galley et al., 2004; Galley et al., 2006). 2. Local feature calculation: Statistical and indicator features, as described by Chiang et al. (2009), are calculated over the rule grammar. 3. Language model calculation: A KneserNey-interpolated 5-gram language model (Chen and Goodman, 1996) is learned from the yield of the target training trees. 4. Decoding: A beamed bottom-up chart decoder (Pust and Knight, 2009; Hopkins and Langmead, 2010) calculates the optimal derivations given a source string and feature parameter set. 5. Tuning: Feature parameters are optimized using the MIRA learning approach (Chiang et al., 2009) to maximize the objective, typically B LEU (Papineni et al., 2002), associated with a tuning corpus. We initially use this system with no modifications and pretend that English–AMR is a language pair indistinct from any other. 3 Data and Comparisons We use English–AMR data from the AMR 1.0 corpus, LDC Catalog number 2014T12. In contrast to narrow-domain data sources that are often use"
D15-1136,kingsbury-palmer-2002-treebank,0,0.217756,"ht, Daniel Marcu, Jonathan May Information Sciences Institute Computer Science Department University of Southern California {pust, ulf, knight, marcu, jonmay}@isi.edu Abstract polarit soldier 0 ARG die-01 The soldier was not afraid of dying. The soldier was not afraid to die. The soldier did not fear death. Figure 1: An Abstract Meaning Representation (AMR) with several English renderings. Introduction Abstract Meaning Representation (AMR) is a compact, readable, whole-sentence semantic annotation (Banarescu et al., 2013). It includes entity identification and typing, PropBank semantic roles (Kingsbury and Palmer, 2002), individual entities playing multiple roles, as well as treatments of modality, negation, etc. AMR abstracts in numerous ways, e.g., by assigning the same conceptual structure to fear (v), fear (n), and afraid (adj). Figure 1 gives an example. AMR parsing is a new research problem, with only a few papers published to date (Flanigan et al., 2014; Wang et al., 2015) and a publicly available corpus of more than 10,000 English/AMR pairs.1 New research problems can be tackled either by developing new algorithms/techniques (Flanigan et al., 2014; Wang et al., 2015) or by adapting existing algorithm"
D15-1136,D07-1038,1,0.835975,"Missing"
D15-1136,P02-1040,0,0.0972587,"vel representations and techniques. Some of our key ideas include: 1. Introducing an AMR-equivalent representation that is suitable for string-to-tree SBMT rule extraction and decoding (Section 4.1). 2 LDC Catalog number 2014T12 See e.g. the related work section of Huck et al. (2014). 1143 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1143–1154, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. Target Nodes Edges Alignments SBMT tree labeled unlabeled words to leaves Children Accuracy Metric ordered B LEU (Papineni et al., 2002) AMR parsing graph unlabeled labeled words to leaves + words to edges unordered Smatch (Cai and Knight, 2013) Corpus Training Development Test Tokens 218,021 29,484 30,263 Table 1: Data splits of AMR 1.0, used in this work. Tokens are English, after tokenization. Figure 2: Differences between AMR parsing and syntax-based machine translation (SBMT). 2. Proposing a target-side reordering technique that leverages the fact that child nodes in AMR are unordered (Section 4.4). 3. Introducing a hierarchical AMR-specific language model to ensure generation of likely parent-child relationships (Section"
D15-1136,D14-1048,1,0.817579,", 2004), the AMR corpus covers a broad range of news and web forum data. We use the training, development, and test splits specified in the AMR corpus (Table 1). The training set is used for rule extraction, language modeling, and statistical rule feature calculation. The development set is used both for parameter optimization and qualitatively for hill-climbing. The test set is held out blind for evaluation. We preprocess the English with a simple rule-based tokenizer and, except where noted, lowercase all data. We obtain English–AMR alignments by using the unsupervised alignment approach of Pourdamghani et al. (2014), which linearizes the AMR and then applies the models of Brown et al. (1993) with an additional symmetrization constraint. All parsing results reported in this work are obtained with the Smatch 1.0 software (Cai and Knight, 2013). We compare our results to those of Flanigan et al. (2014) on the AMR 1.0 data splits; we run that work’s JAMR software according to the provided instructions.3 We also compare our results to published scores in the recent work of Wang et al. (2015). Their work uses slightly different data than that used here 4 but in practice we have not seen significant variation i"
D15-1136,J10-2004,1,0.846218,"m this SBMT-compliant rewrite. 4.2 Tree Restructuring While the transformation in Figure 3c is acceptable to GHKM, and hence an entire end-to-end AMR parser may now be built with SBMT tools, the resulting parser does not exhibit very good performance (Table 3, first line). The trees we are learning on are exceedingly flat, and thus yield rules that do not generalize sufficiently. Rules produced from the top of the tree in Figure 3c, such as that in Figure 4a, are only appropriate for cases where fear-01 has exactly three roles: ARG0 (agent), ARG1 (patient), and polarity. We follow the lead of Wang et al. (2010), who in turn were influenced by similar approaches in monolingual parsing (Collins, 1997; Charniak, 2000), and re-structure trees at nodes with more 5 In Figure 3 the negative polarity marker ‘-’ is a string. Disconnected referents labeled ‘*’ are treated as AMR instances with no roles. than three children (i.e. instances with more than one role), to allow generalization of flat structures. However, our trees are unlike syntactic constituent trees in that they do not have labeled nonterminal nodes, so we have no natural choice of an intermediate (“bar”) label. We must choose a meaningful labe"
D15-1136,N15-1040,0,0.404884,"lish renderings. Introduction Abstract Meaning Representation (AMR) is a compact, readable, whole-sentence semantic annotation (Banarescu et al., 2013). It includes entity identification and typing, PropBank semantic roles (Kingsbury and Palmer, 2002), individual entities playing multiple roles, as well as treatments of modality, negation, etc. AMR abstracts in numerous ways, e.g., by assigning the same conceptual structure to fear (v), fear (n), and afraid (adj). Figure 1 gives an example. AMR parsing is a new research problem, with only a few papers published to date (Flanigan et al., 2014; Wang et al., 2015) and a publicly available corpus of more than 10,000 English/AMR pairs.1 New research problems can be tackled either by developing new algorithms/techniques (Flanigan et al., 2014; Wang et al., 2015) or by adapting existing algorithms/techniques to the problem at hand. In this paper, we investigate the second approach. The AMR parsing problem bears a strong formal resemblance to syntax-based machine translation (SBMT) of the string-to-tree variety, in that 1 ARG1 ins t t ins - 1 fear-01 y G 1 st in AR We present a parser for Abstract Meaning Representation (AMR). We treat Englishto-AMR convers"
D15-1136,N10-1000,0,0.0447018,"Missing"
D15-1136,P15-1095,0,0.503983,"Missing"
D15-1136,N06-1056,0,0.0542131,"labels and restructures to resolve discrepancies between dependency standards and AMR’s specification. In contrast to these works, we use a single-pass approach and re-use existing machine translation architecture, adapting to the AMR parsing task by modifying training data and adding lightweight AMRspecific features. Several other recent works have used a machine translation approach to semantic parsing, but all have been applied to domain data that is much narrower and an order of magnitude smaller than that of AMR, primarily the Geoquery corpus (Zelle and Mooney, 1996). The WASP system of Wong and Mooney (2006) uses hierarchical SMT techniques and does not apply semanticspecific improvements; its extension (Wong and Mooney, 2007) incorporates a target-side reordering component much like the one presented in Section 4.4. Jones et al. (2012a) cast semantic parsing as an instance of hyperedge replacement grammar transduction; like this work they use an IBM model-influenced alignment algorithm and a GHKM-based extraction algorithm. Andreas et al. (2013) use phrase-based and hierarchical SMT techniques on Geoquery. Like this work, they perform a transformation of the input semantic representation so that"
D15-1136,P07-1121,0,0.024563,"ese works, we use a single-pass approach and re-use existing machine translation architecture, adapting to the AMR parsing task by modifying training data and adding lightweight AMRspecific features. Several other recent works have used a machine translation approach to semantic parsing, but all have been applied to domain data that is much narrower and an order of magnitude smaller than that of AMR, primarily the Geoquery corpus (Zelle and Mooney, 1996). The WASP system of Wong and Mooney (2006) uses hierarchical SMT techniques and does not apply semanticspecific improvements; its extension (Wong and Mooney, 2007) incorporates a target-side reordering component much like the one presented in Section 4.4. Jones et al. (2012a) cast semantic parsing as an instance of hyperedge replacement grammar transduction; like this work they use an IBM model-influenced alignment algorithm and a GHKM-based extraction algorithm. Andreas et al. (2013) use phrase-based and hierarchical SMT techniques on Geoquery. Like this work, they perform a transformation of the input semantic representation so that it is amenable to use in an existing machine translation system. However, they are unable to reach the state of the art"
D15-1136,N03-1031,0,\N,Missing
D16-1163,W15-3001,0,0.0163026,"ent models is 0.5 with a decay rate of 0.9 when the development perplexity does not improve. The child models are all trained for 100 epochs. We re-scale the gradient when the gradient norm of all parameters is greater than 5. The initial parameter range is [-0.08, +0.08]. We also initialize our forget-gate biases to 1 as specified by J´ozefowicz et al. (2015) and Gers et al. (2000). For decoding we use a beam search of width 12. 4.1 Transfer Results The results for our transfer learning method applied to the four languages above are in Table 2. The parent models were trained on the WMT 2015 (Bojar et al., 2015) French–English corpus for 5 epochs. Our baseline NMT systems (‘NMT’ row) all receive a large B LEU improvement when using the transfer method (the ‘Xfer’ row) with an average B LEU improvement of 5.6. Additionally, when we use unknown word replacement from Luong et al. (2015b) and ensemble together 8 models (the ‘Final’ row) we further improve upon our B LEU scores, bringing the average B LEU improvement to 7.5. Overall our method allows the NMT system to reach competitive scores and outperform the SBMT system in one of the four language pairs. Figure 2: Our NMT model architecture, showing si"
D16-1163,1997.mtsummit-plenaries.5,0,0.630738,"Missing"
D16-1163,N09-1025,1,0.225735,"ores along with the NMT baselines that do not use transfer. There is a large gap between the SBMT and NMT systems when our transfer method is not used. The SBMT system used in this paper is a stringto-tree statistical machine translation system (Galley et al., 2006; Galley et al., 2004). In this system there are two count-based 5-gram language models. One is trained on the English side of the WMT 2015 English–French dataset and the other is trained on the English side of the low-resource bitext. Additionally, the SBMT models use thousands of sparsely-occurring, lexicalized syntactic features (Chiang et al., 2009). For our NMT system, we use development sets for Hausa, Turkish, Uzbek, and Urdu to tune the learn1571 ing rate, parameter initialization range, dropout rate, and hidden state size for all the experiments. For training we use a minibatch size of 128, hidden state size of 1000, a target vocabulary size of 15K, and a source vocabulary size of 30K. The child models are trained with a dropout probability of 0.5, as in Zaremba et al. (2014). The common parent model is trained with a dropout probability of 0.2. The learning rate used for both child and parent models is 0.5 with a decay rate of 0.9"
D16-1163,P15-1166,0,0.261996,"analysis (Wang and Zheng, 2015). Deep learning models discover multiple levels of representation, some of which may be useful across tasks, which makes them particularly suited to transfer learning (Bengio, 2012). For example, Cires¸an et al. (2012) use a convolutional neural network to recognize handwritten characters and show positive effects of transfer between models for Latin and Chinese characters. Ours is the first study to apply transfer learning to neural machine translation. There has also been work on using data from multiple language pairs in NMT to improve performance. Recently, Dong et al. (2015) showed that sharing a source encoder for one language helps performance when using different target decoders Decoder NMT Xfer Final SBMT Hausa 16.8 21.3 24.0 23.7 Turkish 11.4 17.0 18.7 20.4 Uzbek 10.7 14.4 16.8 17.9 Urdu 5.2 13.8 14.5 17.9 Re-scorer Table 2: Our method significantly improves NMT results for None NMT Xfer LM Hausa 23.7 24.5 24.8 23.6 SBMT Decoder Turkish Uzbek 20.4 17.9 21.4 19.5 21.8 19.5 21.1 17.9 Urdu 17.9 18.2 19.1 18.2 the translation of low-resource languages into English. Results Table 3: Our transfer method applied to re-scoring output nshow test-set B LEU scores. The"
D16-1163,N16-1101,0,0.284449,"irst row shows the out transfer, and the ‘Xfer’ row shows results with transfer. The SBMT performance with no re-scoring and the other 3 rows ‘Final’ row shows B LEU after we ensemble 8 models and use show the performance after re-scoring with the selected model. unknown word replacement. Note: the ‘LM’ row shows the results when an RNN LM trained on the large English corpus was used to re-score. for different languages. In that paper the authors showed that using this framework improves performance for low-resource languages by incorporating a mix of low-resource and high-resource languages. Firat et al. (2016) used a similar approach, employing a separate encoder for each source language, a separate decoder for each target language, and a shared attention mechanism across all languages. They then trained these components jointly across multiple different language pairs to show improvements in a lower-resource setting. There are a few key differences between our work and theirs. One is that we are working with truly small amounts of training data. Dong et al. (2015) used a training corpus of about 8m English words for the low-resource experiments, and Firat et al. (2016) used from 2m to 4m words, wh"
D16-1163,N04-1035,1,0.184111,"g translation knowledge from parallel text. NMT systems have achieved competitive accuracy rates under large-data training conditions for language pairs This work was carried out while all authors were at USC’s Information Sciences Institute. *This author is currently at Google Brain. such as English–French. However, neural methods are data-hungry and learn poorly from low-count events. This behavior makes vanilla NMT a poor choice for low-resource languages, where parallel data is scarce. Table 1 shows that for 4 low-resource languages, a standard string-to-tree statistical MT system (SBMT) (Galley et al., 2004; Galley et al., 2006) strongly outperforms NMT, even when NMT uses the state-of-the-art local attention plus feedinput techniques from Luong et al. (2015a). In this paper, we describe a method for substantially improving NMT results on these languages. Our key idea is to first train a high-resource language pair, then use the resulting trained network (the parent model) to initialize and constrain training for our low-resource language pair (the child model). We find that we can optimize our results by fixing certain parameters of the parent model and letting the rest be fine-tuned by the chi"
D16-1163,P06-1121,1,0.286724,"ge from parallel text. NMT systems have achieved competitive accuracy rates under large-data training conditions for language pairs This work was carried out while all authors were at USC’s Information Sciences Institute. *This author is currently at Google Brain. such as English–French. However, neural methods are data-hungry and learn poorly from low-count events. This behavior makes vanilla NMT a poor choice for low-resource languages, where parallel data is scarce. Table 1 shows that for 4 low-resource languages, a standard string-to-tree statistical MT system (SBMT) (Galley et al., 2004; Galley et al., 2006) strongly outperforms NMT, even when NMT uses the state-of-the-art local attention plus feedinput techniques from Luong et al. (2015a). In this paper, we describe a method for substantially improving NMT results on these languages. Our key idea is to first train a high-resource language pair, then use the resulting trained network (the parent model) to initialize and constrain training for our low-resource language pair (the child model). We find that we can optimize our results by fixing certain parameters of the parent model and letting the rest be fine-tuned by the child model. We report NM"
D16-1163,N06-1014,0,0.0428891,"d the EngPerm–English learns to un-permute scrambled English sentences. The LM is a 2-layer LSTM RNN language model. Dictionary Initialization Using the transfer method, we always initialize input language embeddings for the child model with randomly-assigned embeddings from the parent (which has a different input language). A smarter method might be to initialize child embeddings with similar parent embeddings, where similarity is measured by word-to-word t-table probabilities. To get these probabilities we compose Uzbek–English and English–French t-tables obtained from the Berkeley Aligner (Liang et al., 2006). We see from Figure 4 that this dictionary-based assignment results in faster improvement in the early part of the training. However the final performance is similar to our standard model, indicating that the training is able to untangle the dictionary permutation introduced by randomly-assigned embeddings. 5.6 Transfer Model None French–English Parent English–English Parent EngPerm–English Parent LM Xfer Different Parent Models In the above experiments, we use a parent model trained on a large French–English corpus. One might hypothesize that our gains come from exploit1574 ing the English h"
D16-1163,D15-1166,0,0.858229,"is work was carried out while all authors were at USC’s Information Sciences Institute. *This author is currently at Google Brain. such as English–French. However, neural methods are data-hungry and learn poorly from low-count events. This behavior makes vanilla NMT a poor choice for low-resource languages, where parallel data is scarce. Table 1 shows that for 4 low-resource languages, a standard string-to-tree statistical MT system (SBMT) (Galley et al., 2004; Galley et al., 2006) strongly outperforms NMT, even when NMT uses the state-of-the-art local attention plus feedinput techniques from Luong et al. (2015a). In this paper, we describe a method for substantially improving NMT results on these languages. Our key idea is to first train a high-resource language pair, then use the resulting trained network (the parent model) to initialize and constrain training for our low-resource language pair (the child model). We find that we can optimize our results by fixing certain parameters of the parent model and letting the rest be fine-tuned by the child model. We report NMT improvements from transfer learning of 5.6 B LEU on average, and we provide an analysis of why the method works. The final NMT sys"
D16-1163,P15-1002,0,0.348052,"is work was carried out while all authors were at USC’s Information Sciences Institute. *This author is currently at Google Brain. such as English–French. However, neural methods are data-hungry and learn poorly from low-count events. This behavior makes vanilla NMT a poor choice for low-resource languages, where parallel data is scarce. Table 1 shows that for 4 low-resource languages, a standard string-to-tree statistical MT system (SBMT) (Galley et al., 2004; Galley et al., 2006) strongly outperforms NMT, even when NMT uses the state-of-the-art local attention plus feedinput techniques from Luong et al. (2015a). In this paper, we describe a method for substantially improving NMT results on these languages. Our key idea is to first train a high-resource language pair, then use the resulting trained network (the parent model) to initialize and constrain training for our low-resource language pair (the child model). We find that we can optimize our results by fixing certain parameters of the parent model and letting the rest be fine-tuned by the child model. We report NMT improvements from transfer learning of 5.6 B LEU on average, and we provide an analysis of why the method works. The final NMT sys"
D16-1163,D13-1140,0,0.0533372,"sfer learning helps French0 (13.3→20.0) more than it helps Uzbek (10.7→15.0). 4.2 5 Re-scoring Results We also use the NMT model with transfer learning as a feature when re-scoring output n-best lists (n = 1000) from the SBMT system. Table 3 shows the results of re-scoring. We compare re-scoring with transfer NMT to re-scoring with baseline (i.e. non-transfer) NMT and to re-scoring with a neural language model. The neural language model is an LSTM RNN with 2 layers and 1000 hidden states. It has a target vocabulary of 100K and is trained using noise-contrastive estimation (Mnih and Teh, 2012; Vaswani et al., 2013; Baltescu and Blunsom, 2015; Williams et al., 2015). Additionally, it is trained using dropout with a dropout probability of 0.2 as suggested by Zaremba et al. (2014). Re-scoring with the transfer NMT model yields an improvement of 1.1– 1.6 B LEU points above the strong SBMT system; we find that transfer NMT is a better re-scoring feature than baseline NMT or neural language models. In the next section, we describe a number of additional experiments designed to help us understand the contribution of the various components of our transfer model. 1572 Analysis We analyze the effects of using di"
D16-1163,2020.wmt-1.63,0,\N,Missing
D16-1163,2020.coling-tutorials.3,0,\N,Missing
D19-1030,D14-1164,0,0.0220708,"f manually annotated mentions. The system extracted entity mentions introduce noise and 318 (a) Target Language: Chinese (b) Target Language: Arabic Figure 3: Relation Extraction: Comparison between supervised monolingual GCN model and cross-lingual transfer learning. (a) Target Language: Chinese (b) Target Language: Arabic Figure 4: Event Argument Role Labeling: Comparison between supervised monolingual GCN model and crosslingual transfer learning. thus decrease the performance of the model, but the overall results are still promising. 4 et al., 2009; Surdeanu et al., 2012; Min et al., 2013; Angeli et al., 2014; Zeng et al., 2015; Quirk and Poon, 2017; Qin et al., 2018; Wang et al., 2018a) through knowledge bases (KBs), where entities and static relations are plentiful. Distant supervision is less applicable for the task of event extraction because very few dynamic events are included in KBs. These approaches, however, incorporate language-specific characteristics and thus are costly in requiring substantial amount of annotations to adapt to a new language (Chen and Vincent, 2012; Blessing and Schütze, 2012; Li et al., 2012; Danilova et al., 2014; Agerri et al., 2016; Hsi et al., 2016; Feng et al.,"
D19-1030,D12-1092,0,0.0655205,"Missing"
D19-1030,L18-1245,0,0.0189856,"so are reported predominantly in the low-resource language data sources available to that community. For example, though English language news will occasionally discuss the Person Aung San Suu Kyi, the vast majority of Physical-Located relations and Meeting events involving her are only reported locally in Burmese news, and thus, without Burmese relation and event extraction, a knowledge graph of this person will lack this available information. Unfortunately, publiclyavailable gold-standard annotations for relation and event extraction exist for only a few languages (Doddington et al., 2004; Getman et al., 2018), and Burmese is not among them. Compared to other IE tasks such as name tagging, the annotations for Relation and Event Extraction are also more costly to obtain, because they are structured and require a rich label space. Recent research (Lin et al., 2017) has found that relational facts are typically expressed by identifiable patterns within languages and has shown Introduction Advanced Information Extraction (IE) tasks entail predicting structures, such as relations between entities, and events involving entities. Given a pair of entity mentions, Relation Extraction aims to identify the re"
D19-1030,P14-1038,1,0.937832,"ces by limiting the number of negative samples to be no more than the number of positive samples for each document. For data preprocessing, we apply the Stanford CoreNLP toolkit (Manning et al., 2014) for Chinese word segmentation and English tokenization, and the API provided by UDPipe (Straka and s m2 yij log(σ(U r · [hm1 i ; hij ; hj ])) i=1 j=1 (1) where U r is a weight matrix. 316 Straková, 2017) for Arabic tokenization. We use UDPipe1 for POS tagging and dependency parsing for all three languages. We follow the following criteria in previous work (Ji and Grishman, 2008; Li et al., 2013; Li and Ji, 2014) for evaluation: performance by applying models trained with various combinations of training and test data from these three languages, as shown in Tables 3 and 4. We can see that the results are promising. For both tasks, the models trained from English are best, followed by Chinese, and then Arabic. We find that extraction task performance degrades as the accuracy of language-dependent tools (for sentence segmentation, POS tagging, dependency parsing) degrades. • A relation mention is considered correct if its relation type is correct, and the head offsets of the two related entity mention a"
D19-1030,C16-1114,0,0.359758,"al., 2013; Angeli et al., 2014; Zeng et al., 2015; Quirk and Poon, 2017; Qin et al., 2018; Wang et al., 2018a) through knowledge bases (KBs), where entities and static relations are plentiful. Distant supervision is less applicable for the task of event extraction because very few dynamic events are included in KBs. These approaches, however, incorporate language-specific characteristics and thus are costly in requiring substantial amount of annotations to adapt to a new language (Chen and Vincent, 2012; Blessing and Schütze, 2012; Li et al., 2012; Danilova et al., 2014; Agerri et al., 2016; Hsi et al., 2016; Feng et al., 2016). Related Work A large number of supervised machine learning techniques have been used for English event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015a; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b; Lu and Nguyen, 2018; Liu et al., 2018a; Zhang et al., 2018a, 2019). English relation extraction in the early days also followed supervise"
D19-1030,P13-1008,1,0.874134,"e training instances by limiting the number of negative samples to be no more than the number of positive samples for each document. For data preprocessing, we apply the Stanford CoreNLP toolkit (Manning et al., 2014) for Chinese word segmentation and English tokenization, and the API provided by UDPipe (Straka and s m2 yij log(σ(U r · [hm1 i ; hij ; hj ])) i=1 j=1 (1) where U r is a weight matrix. 316 Straková, 2017) for Arabic tokenization. We use UDPipe1 for POS tagging and dependency parsing for all three languages. We follow the following criteria in previous work (Ji and Grishman, 2008; Li et al., 2013; Li and Ji, 2014) for evaluation: performance by applying models trained with various combinations of training and test data from these three languages, as shown in Tables 3 and 4. We can see that the results are promising. For both tasks, the models trained from English are best, followed by Chinese, and then Arabic. We find that extraction task performance degrades as the accuracy of language-dependent tools (for sentence segmentation, POS tagging, dependency parsing) degrades. • A relation mention is considered correct if its relation type is correct, and the head offsets of the two relate"
D19-1030,P18-1074,1,0.816541,"2015; Nguyen and Grishman, 2018; Liu et al., 2018b; Lu and Nguyen, 2018; Liu et al., 2018a; Zhang et al., 2018a, 2019). English relation extraction in the early days also followed supervised paradigms (Li and Ji, 2014; Zeng et al., 2014; Nguyen and Grishman, 2015b; Miwa and Bansal, 2016; Pawar et al., 2017; Bekoulis et al., 2018; Wang et al., 2018b). Recent efforts have attempted to reduce annotation costs using distant supervision (Mintz Regardless of the recent successes in applying cross-lingual transfer learning to sequence labeling tasks, such as name tagging (e.g., (Mayhew et al., 2017; Lin et al., 2018; Huang et al., 2019)), only limited work has explored cross-lingual relation and event structure transfer. Most previous efforts working with cross-lingual structure trans319 fer rely on bilingual dictionaries (Hsi et al., 2016), parallel data (Chen and Ji, 2009; Kim et al., 2010; Qian et al., 2014) or machine translation (Faruqui and Kumar, 2015; Zou et al., 2018). Recent methods (Lin et al., 2017; Wang et al., 2018b) aggregate consistent patterns and complementary information across languages to enhance Relation Extraction, but they do so exploiting only distributional representations. Even"
D19-1030,P09-1113,0,0.169793,"Missing"
D19-1030,P16-1105,0,0.0446401,"ing techniques have been used for English event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015a; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b; Lu and Nguyen, 2018; Liu et al., 2018a; Zhang et al., 2018a, 2019). English relation extraction in the early days also followed supervised paradigms (Li and Ji, 2014; Zeng et al., 2014; Nguyen and Grishman, 2015b; Miwa and Bansal, 2016; Pawar et al., 2017; Bekoulis et al., 2018; Wang et al., 2018b). Recent efforts have attempted to reduce annotation costs using distant supervision (Mintz Regardless of the recent successes in applying cross-lingual transfer learning to sequence labeling tasks, such as name tagging (e.g., (Mayhew et al., 2017; Lin et al., 2018; Huang et al., 2019)), only limited work has explored cross-lingual relation and event structure transfer. Most previous efforts working with cross-lingual structure trans319 fer rely on bilingual dictionaries (Hsi et al., 2016), parallel data (Chen and Ji, 2009; Kim et"
D19-1030,N19-1112,0,0.0293072,"raction shares with Semantic Role Labeling (SRL) the task of assigning to each event argument its event role label, in the process of completing other tasks to extract the full event structure (assigning event types to predicates and more specific roles to arguments). Cross-lingual transfer has been very successful for SRL. Early attempts relied on word alignment (Van der Plas et al., 2011) or bilingual dictionaries (Kozhevnikov and Titov, 2013). Recent work incorporates universal dependencies (Prazák and Konopík, 2017) or multilingual word embeddings for Polyglot SRL (Mulcaire et al., 2018). Liu et al. (2019) and Mulcaire et al. (2019) exploit multi-lingual contextualized word embedding for SRL and other Polyglot NLP tasks including dependency parsing and name tagging. To the best of our knowledge, our work is the first to construct a cross-lingual structure transfer framework that combines language-universal symbolic representations and distributional representations for relation and event extraction over texts written in a language without any training data. GCN has been successfully applied to several individual monolingual NLP tasks, including relation extraction (Zhang et al., 2018b), event d"
D19-1030,N19-1392,0,0.0323235,"mantic Role Labeling (SRL) the task of assigning to each event argument its event role label, in the process of completing other tasks to extract the full event structure (assigning event types to predicates and more specific roles to arguments). Cross-lingual transfer has been very successful for SRL. Early attempts relied on word alignment (Van der Plas et al., 2011) or bilingual dictionaries (Kozhevnikov and Titov, 2013). Recent work incorporates universal dependencies (Prazák and Konopík, 2017) or multilingual word embeddings for Polyglot SRL (Mulcaire et al., 2018). Liu et al. (2019) and Mulcaire et al. (2019) exploit multi-lingual contextualized word embedding for SRL and other Polyglot NLP tasks including dependency parsing and name tagging. To the best of our knowledge, our work is the first to construct a cross-lingual structure transfer framework that combines language-universal symbolic representations and distributional representations for relation and event extraction over texts written in a language without any training data. GCN has been successfully applied to several individual monolingual NLP tasks, including relation extraction (Zhang et al., 2018b), event detection (Nguyen and Grishm"
D19-1030,D18-1156,0,0.172682,"Missing"
D19-1030,P18-2106,0,0.0942397,"Missing"
D19-1030,N15-1028,0,0.0215415,"symbolic features, such as a common labeled dependency path, as well as distributional features, such as multilingual word embeddings. Based on these language-universal representations, we then project all entity mentions, event triggers and their contexts into one multilingual common space. Unlike recent work on multilingual common space construction that makes use of linear mappings (Mikolov et al., 2013; Rothe et al., 2016; Zhang et al., 2016; Lazaridou et al., 2015; Xing et al., 2015; Smith et al., 2017) or canonical correlation analysis (CCA) (Ammar et al., 2016; Faruqui and Dyer, 2014; Lu et al., 2015) to transfer surface features across languages, our major innovation is to convert the text data into structured representations derived from universal dependency parses and enhanced with distributional information to capture individual entities as well 314 as the relations and events involving those entities, so we can share structural representations across multiple languages. Then we construct a novel cross-lingual structure transfer learning framework to project source language (SL) training data and target language (TL) test data into the common semantic space, so that we can train a rela"
D19-1030,N16-1034,0,0.107021,"nd thus are costly in requiring substantial amount of annotations to adapt to a new language (Chen and Vincent, 2012; Blessing and Schütze, 2012; Li et al., 2012; Danilova et al., 2014; Agerri et al., 2016; Hsi et al., 2016; Feng et al., 2016). Related Work A large number of supervised machine learning techniques have been used for English event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015a; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b; Lu and Nguyen, 2018; Liu et al., 2018a; Zhang et al., 2018a, 2019). English relation extraction in the early days also followed supervised paradigms (Li and Ji, 2014; Zeng et al., 2014; Nguyen and Grishman, 2015b; Miwa and Bansal, 2016; Pawar et al., 2017; Bekoulis et al., 2018; Wang et al., 2018b). Recent efforts have attempted to reduce annotation costs using distant supervision (Mintz Regardless of the recent successes in applying cross-lingual transfer learning to sequence labeling tasks, such as name tagging (e.g., (Mayhew"
D19-1030,D18-1517,0,0.0143756,"nguage (Chen and Vincent, 2012; Blessing and Schütze, 2012; Li et al., 2012; Danilova et al., 2014; Agerri et al., 2016; Hsi et al., 2016; Feng et al., 2016). Related Work A large number of supervised machine learning techniques have been used for English event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015a; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b; Lu and Nguyen, 2018; Liu et al., 2018a; Zhang et al., 2018a, 2019). English relation extraction in the early days also followed supervised paradigms (Li and Ji, 2014; Zeng et al., 2014; Nguyen and Grishman, 2015b; Miwa and Bansal, 2016; Pawar et al., 2017; Bekoulis et al., 2018; Wang et al., 2018b). Recent efforts have attempted to reduce annotation costs using distant supervision (Mintz Regardless of the recent successes in applying cross-lingual transfer learning to sequence labeling tasks, such as name tagging (e.g., (Mayhew et al., 2017; Lin et al., 2018; Huang et al., 2019)), only limited work has explored"
D19-1030,P14-5010,0,0.00571376,"Missing"
D19-1030,P15-2060,0,0.0927352,"e-specific characteristics and thus are costly in requiring substantial amount of annotations to adapt to a new language (Chen and Vincent, 2012; Blessing and Schütze, 2012; Li et al., 2012; Danilova et al., 2014; Agerri et al., 2016; Hsi et al., 2016; Feng et al., 2016). Related Work A large number of supervised machine learning techniques have been used for English event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015a; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b; Lu and Nguyen, 2018; Liu et al., 2018a; Zhang et al., 2018a, 2019). English relation extraction in the early days also followed supervised paradigms (Li and Ji, 2014; Zeng et al., 2014; Nguyen and Grishman, 2015b; Miwa and Bansal, 2016; Pawar et al., 2017; Bekoulis et al., 2018; Wang et al., 2018b). Recent efforts have attempted to reduce annotation costs using distant supervision (Mintz Regardless of the recent successes in applying cross-lingual transfer learning to sequence labeling tasks, such as name"
D19-1030,D17-1159,0,0.194902,"cy parses and enhanced with distributional information to capture individual entities as well 314 as the relations and events involving those entities, so we can share structural representations across multiple languages. Then we construct a novel cross-lingual structure transfer learning framework to project source language (SL) training data and target language (TL) test data into the common semantic space, so that we can train a relation or event extractor from SL annotations and apply the resulting extractor to TL texts. We adopt graph convolutional networks (GCN) (Kipf and Welling, 2017; Marcheggiani and Titov, 2017) to encode graph structures over the input data, applying graph convolution operations to generate entity and word representations in a latent space. In contrast to other encoders such a Tree-LSTM (Tai et al., 2015), GCN can cover more complete contextual information from dependency parses because, for each word, it captures all parse tree neighbors of the word, rather than just the child nodes of the word. Using this shared encoder, we treat the two tasks of relation extraction and event argument role labeling as mappings from the latent space to relation type and to event type and argument r"
D19-1030,W15-1506,0,0.0597738,"e-specific characteristics and thus are costly in requiring substantial amount of annotations to adapt to a new language (Chen and Vincent, 2012; Blessing and Schütze, 2012; Li et al., 2012; Danilova et al., 2014; Agerri et al., 2016; Hsi et al., 2016; Feng et al., 2016). Related Work A large number of supervised machine learning techniques have been used for English event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015a; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b; Lu and Nguyen, 2018; Liu et al., 2018a; Zhang et al., 2018a, 2019). English relation extraction in the early days also followed supervised paradigms (Li and Ji, 2014; Zeng et al., 2014; Nguyen and Grishman, 2015b; Miwa and Bansal, 2016; Pawar et al., 2017; Bekoulis et al., 2018; Wang et al., 2018b). Recent efforts have attempted to reduce annotation costs using distant supervision (Mintz Regardless of the recent successes in applying cross-lingual transfer learning to sequence labeling tasks, such as name"
D19-1030,P17-1178,1,0.854212,"re seen in packed emergency rooms) Figure 2: Multilingual common semantic space and cross-lingual structure transfer. that the consistency in patterns observed across languages can be used to improve relation extraction. Inspired by their results, we exploit languageuniversal features relevant to relation and event argument identification and classification, by way of both symbolic and distributional representations. For example, language-universal POS tagging and universal dependency parsing is available for 76 languages (Nivre et al., 2018), entity extraction is available for 282 languages (Pan et al., 2017), and multi-lingual word embeddings are available for 44 languages (Bojanowski et al., 2017; Joulin et al., 2018). As shown in Figure 2, even for distinct pairs of entity mentions (colored pink and blue, in both English and Russian), the structures share similar language-universal symbolic features, such as a common labeled dependency path, as well as distributional features, such as multilingual word embeddings. Based on these language-universal representations, we then project all entity mentions, event triggers and their contexts into one multilingual common space. Unlike recent work on mul"
D19-1030,E17-1077,0,0.0555006,"Missing"
D19-1030,D12-1042,0,0.0476729,"y extracted by Stanford CoreNLP instead of manually annotated mentions. The system extracted entity mentions introduce noise and 318 (a) Target Language: Chinese (b) Target Language: Arabic Figure 3: Relation Extraction: Comparison between supervised monolingual GCN model and cross-lingual transfer learning. (a) Target Language: Chinese (b) Target Language: Arabic Figure 4: Event Argument Role Labeling: Comparison between supervised monolingual GCN model and crosslingual transfer learning. thus decrease the performance of the model, but the overall results are still promising. 4 et al., 2009; Surdeanu et al., 2012; Min et al., 2013; Angeli et al., 2014; Zeng et al., 2015; Quirk and Poon, 2017; Qin et al., 2018; Wang et al., 2018a) through knowledge bases (KBs), where entities and static relations are plentiful. Distant supervision is less applicable for the task of event extraction because very few dynamic events are included in KBs. These approaches, however, incorporate language-specific characteristics and thus are costly in requiring substantial amount of annotations to adapt to a new language (Chen and Vincent, 2012; Blessing and Schütze, 2012; Li et al., 2012; Danilova et al., 2014; Agerri et al."
D19-1030,P11-2052,0,0.0755243,"Missing"
D19-1030,P15-1150,0,0.12426,"Missing"
D19-1030,prazak-konopik-2017-cross,0,0.0648181,"nhance Relation Extraction, but they do so exploiting only distributional representations. Event extraction shares with Semantic Role Labeling (SRL) the task of assigning to each event argument its event role label, in the process of completing other tasks to extract the full event structure (assigning event types to predicates and more specific roles to arguments). Cross-lingual transfer has been very successful for SRL. Early attempts relied on word alignment (Van der Plas et al., 2011) or bilingual dictionaries (Kozhevnikov and Titov, 2013). Recent work incorporates universal dependencies (Prazák and Konopík, 2017) or multilingual word embeddings for Polyglot SRL (Mulcaire et al., 2018). Liu et al. (2019) and Mulcaire et al. (2019) exploit multi-lingual contextualized word embedding for SRL and other Polyglot NLP tasks including dependency parsing and name tagging. To the best of our knowledge, our work is the first to construct a cross-lingual structure transfer framework that combines language-universal symbolic representations and distributional representations for relation and event extraction over texts written in a language without any training data. GCN has been successfully applied to several in"
D19-1030,D18-1248,0,0.0916018,"ise and 318 (a) Target Language: Chinese (b) Target Language: Arabic Figure 3: Relation Extraction: Comparison between supervised monolingual GCN model and cross-lingual transfer learning. (a) Target Language: Chinese (b) Target Language: Arabic Figure 4: Event Argument Role Labeling: Comparison between supervised monolingual GCN model and crosslingual transfer learning. thus decrease the performance of the model, but the overall results are still promising. 4 et al., 2009; Surdeanu et al., 2012; Min et al., 2013; Angeli et al., 2014; Zeng et al., 2015; Quirk and Poon, 2017; Qin et al., 2018; Wang et al., 2018a) through knowledge bases (KBs), where entities and static relations are plentiful. Distant supervision is less applicable for the task of event extraction because very few dynamic events are included in KBs. These approaches, however, incorporate language-specific characteristics and thus are costly in requiring substantial amount of annotations to adapt to a new language (Chen and Vincent, 2012; Blessing and Schütze, 2012; Li et al., 2012; Danilova et al., 2014; Agerri et al., 2016; Hsi et al., 2016; Feng et al., 2016). Related Work A large number of supervised machine learning techniques h"
D19-1030,P14-1055,0,0.115821,"l., 2017; Bekoulis et al., 2018; Wang et al., 2018b). Recent efforts have attempted to reduce annotation costs using distant supervision (Mintz Regardless of the recent successes in applying cross-lingual transfer learning to sequence labeling tasks, such as name tagging (e.g., (Mayhew et al., 2017; Lin et al., 2018; Huang et al., 2019)), only limited work has explored cross-lingual relation and event structure transfer. Most previous efforts working with cross-lingual structure trans319 fer rely on bilingual dictionaries (Hsi et al., 2016), parallel data (Chen and Ji, 2009; Kim et al., 2010; Qian et al., 2014) or machine translation (Faruqui and Kumar, 2015; Zou et al., 2018). Recent methods (Lin et al., 2017; Wang et al., 2018b) aggregate consistent patterns and complementary information across languages to enhance Relation Extraction, but they do so exploiting only distributional representations. Event extraction shares with Semantic Role Labeling (SRL) the task of assigning to each event argument its event role label, in the process of completing other tasks to extract the full event structure (assigning event types to predicates and more specific roles to arguments). Cross-lingual transfer has"
D19-1030,C18-1099,0,0.356373,"ise and 318 (a) Target Language: Chinese (b) Target Language: Arabic Figure 3: Relation Extraction: Comparison between supervised monolingual GCN model and cross-lingual transfer learning. (a) Target Language: Chinese (b) Target Language: Arabic Figure 4: Event Argument Role Labeling: Comparison between supervised monolingual GCN model and crosslingual transfer learning. thus decrease the performance of the model, but the overall results are still promising. 4 et al., 2009; Surdeanu et al., 2012; Min et al., 2013; Angeli et al., 2014; Zeng et al., 2015; Quirk and Poon, 2017; Qin et al., 2018; Wang et al., 2018a) through knowledge bases (KBs), where entities and static relations are plentiful. Distant supervision is less applicable for the task of event extraction because very few dynamic events are included in KBs. These approaches, however, incorporate language-specific characteristics and thus are costly in requiring substantial amount of annotations to adapt to a new language (Chen and Vincent, 2012; Blessing and Schütze, 2012; Li et al., 2012; Danilova et al., 2014; Agerri et al., 2016; Hsi et al., 2016; Feng et al., 2016). Related Work A large number of supervised machine learning techniques h"
D19-1030,P18-1046,0,0.0122437,"tions introduce noise and 318 (a) Target Language: Chinese (b) Target Language: Arabic Figure 3: Relation Extraction: Comparison between supervised monolingual GCN model and cross-lingual transfer learning. (a) Target Language: Chinese (b) Target Language: Arabic Figure 4: Event Argument Role Labeling: Comparison between supervised monolingual GCN model and crosslingual transfer learning. thus decrease the performance of the model, but the overall results are still promising. 4 et al., 2009; Surdeanu et al., 2012; Min et al., 2013; Angeli et al., 2014; Zeng et al., 2015; Quirk and Poon, 2017; Qin et al., 2018; Wang et al., 2018a) through knowledge bases (KBs), where entities and static relations are plentiful. Distant supervision is less applicable for the task of event extraction because very few dynamic events are included in KBs. These approaches, however, incorporate language-specific characteristics and thus are costly in requiring substantial amount of annotations to adapt to a new language (Chen and Vincent, 2012; Blessing and Schütze, 2012; Li et al., 2012; Danilova et al., 2014; Agerri et al., 2016; Hsi et al., 2016; Feng et al., 2016). Related Work A large number of supervised machine le"
D19-1030,N15-1104,0,0.0713422,"Missing"
D19-1030,E17-1110,0,0.0234348,"m extracted entity mentions introduce noise and 318 (a) Target Language: Chinese (b) Target Language: Arabic Figure 3: Relation Extraction: Comparison between supervised monolingual GCN model and cross-lingual transfer learning. (a) Target Language: Chinese (b) Target Language: Arabic Figure 4: Event Argument Role Labeling: Comparison between supervised monolingual GCN model and crosslingual transfer learning. thus decrease the performance of the model, but the overall results are still promising. 4 et al., 2009; Surdeanu et al., 2012; Min et al., 2013; Angeli et al., 2014; Zeng et al., 2015; Quirk and Poon, 2017; Qin et al., 2018; Wang et al., 2018a) through knowledge bases (KBs), where entities and static relations are plentiful. Distant supervision is less applicable for the task of event extraction because very few dynamic events are included in KBs. These approaches, however, incorporate language-specific characteristics and thus are costly in requiring substantial amount of annotations to adapt to a new language (Chen and Vincent, 2012; Blessing and Schütze, 2012; Li et al., 2012; Danilova et al., 2014; Agerri et al., 2016; Hsi et al., 2016; Feng et al., 2016). Related Work A large number of sup"
D19-1030,N16-1033,0,0.150323,"cluded in KBs. These approaches, however, incorporate language-specific characteristics and thus are costly in requiring substantial amount of annotations to adapt to a new language (Chen and Vincent, 2012; Blessing and Schütze, 2012; Li et al., 2012; Danilova et al., 2014; Agerri et al., 2016; Hsi et al., 2016; Feng et al., 2016). Related Work A large number of supervised machine learning techniques have been used for English event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015a; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b; Lu and Nguyen, 2018; Liu et al., 2018a; Zhang et al., 2018a, 2019). English relation extraction in the early days also followed supervised paradigms (Li and Ji, 2014; Zeng et al., 2014; Nguyen and Grishman, 2015b; Miwa and Bansal, 2016; Pawar et al., 2017; Bekoulis et al., 2018; Wang et al., 2018b). Recent efforts have attempted to reduce annotation costs using distant supervision (Mintz Regardless of the recent successes in applying cross-lingu"
D19-1030,N16-1091,0,0.0130887,"Joulin et al., 2018). As shown in Figure 2, even for distinct pairs of entity mentions (colored pink and blue, in both English and Russian), the structures share similar language-universal symbolic features, such as a common labeled dependency path, as well as distributional features, such as multilingual word embeddings. Based on these language-universal representations, we then project all entity mentions, event triggers and their contexts into one multilingual common space. Unlike recent work on multilingual common space construction that makes use of linear mappings (Mikolov et al., 2013; Rothe et al., 2016; Zhang et al., 2016; Lazaridou et al., 2015; Xing et al., 2015; Smith et al., 2017) or canonical correlation analysis (CCA) (Ammar et al., 2016; Faruqui and Dyer, 2014; Lu et al., 2015) to transfer surface features across languages, our major innovation is to convert the text data into structured representations derived from universal dependency parses and enhanced with distributional information to capture individual entities as well 314 as the relations and events involving those entities, so we can share structural representations across multiple languages. Then we construct a novel cross-"
D19-1030,D15-1203,0,0.025466,"mentions. The system extracted entity mentions introduce noise and 318 (a) Target Language: Chinese (b) Target Language: Arabic Figure 3: Relation Extraction: Comparison between supervised monolingual GCN model and cross-lingual transfer learning. (a) Target Language: Chinese (b) Target Language: Arabic Figure 4: Event Argument Role Labeling: Comparison between supervised monolingual GCN model and crosslingual transfer learning. thus decrease the performance of the model, but the overall results are still promising. 4 et al., 2009; Surdeanu et al., 2012; Min et al., 2013; Angeli et al., 2014; Zeng et al., 2015; Quirk and Poon, 2017; Qin et al., 2018; Wang et al., 2018a) through knowledge bases (KBs), where entities and static relations are plentiful. Distant supervision is less applicable for the task of event extraction because very few dynamic events are included in KBs. These approaches, however, incorporate language-specific characteristics and thus are costly in requiring substantial amount of annotations to adapt to a new language (Chen and Vincent, 2012; Blessing and Schütze, 2012; Li et al., 2012; Danilova et al., 2014; Agerri et al., 2016; Hsi et al., 2016; Feng et al., 2016). Related Work"
D19-1030,C14-1220,0,0.0417218,"Work A large number of supervised machine learning techniques have been used for English event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015a; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b; Lu and Nguyen, 2018; Liu et al., 2018a; Zhang et al., 2018a, 2019). English relation extraction in the early days also followed supervised paradigms (Li and Ji, 2014; Zeng et al., 2014; Nguyen and Grishman, 2015b; Miwa and Bansal, 2016; Pawar et al., 2017; Bekoulis et al., 2018; Wang et al., 2018b). Recent efforts have attempted to reduce annotation costs using distant supervision (Mintz Regardless of the recent successes in applying cross-lingual transfer learning to sequence labeling tasks, such as name tagging (e.g., (Mayhew et al., 2017; Lin et al., 2018; Huang et al., 2019)), only limited work has explored cross-lingual relation and event structure transfer. Most previous efforts working with cross-lingual structure trans319 fer rely on bilingual dictionaries (Hsi et a"
D19-1030,K17-3009,0,0.116769,"Missing"
D19-1030,N16-1156,0,0.0604827,"Missing"
D19-1030,D18-1244,0,0.136645,"versal, we first convert each tree node into a vector which is a concatenation of three language-universal representations at wordlevel: multilingual word embedding, POS embedding (Nivre et al., 2016), entity-type embedding, and dependency relation embedding. More details are reported in Section 3.2. Model Overview 2.3 GCN Encoder Structural information is important for relation extraction and event argument role labeling, thus we aim to generate contextualized word representations by leveraging neighbors in dependency trees for each node. Our GCN encoder is based on the monolingual design by Zhang et al. (2018b). The graphical sentence representation obtained from dependency parsing of a sentence with N tokens is converted into an N × N adjacency matrix A, with added self-connections at each node to help capture information about the current node itself, as in Kipf and Welling (2017). Here, Ai,j = 1 denotes the presence of a directed edge from node i to node j in the dependency tree. Initially, each node contains distributional information about the ith word, including word embedding xw i , embeddings for symbolic information including its POS tag xpi , dependency relation xdi and entity type xei ."
D19-1030,C18-1037,0,0.0763441,"have attempted to reduce annotation costs using distant supervision (Mintz Regardless of the recent successes in applying cross-lingual transfer learning to sequence labeling tasks, such as name tagging (e.g., (Mayhew et al., 2017; Lin et al., 2018; Huang et al., 2019)), only limited work has explored cross-lingual relation and event structure transfer. Most previous efforts working with cross-lingual structure trans319 fer rely on bilingual dictionaries (Hsi et al., 2016), parallel data (Chen and Ji, 2009; Kim et al., 2010; Qian et al., 2014) or machine translation (Faruqui and Kumar, 2015; Zou et al., 2018). Recent methods (Lin et al., 2017; Wang et al., 2018b) aggregate consistent patterns and complementary information across languages to enhance Relation Extraction, but they do so exploiting only distributional representations. Event extraction shares with Semantic Role Labeling (SRL) the task of assigning to each event argument its event role label, in the process of completing other tasks to extract the full event structure (assigning event types to predicates and more specific roles to arguments). Cross-lingual transfer has been very successful for SRL. Early attempts relied on word alignme"
D19-1625,N16-1071,0,0.0600412,"Missing"
D19-1625,P17-1152,0,0.0299558,"ams et al., 2018), the SemEval-2018 commonsense shared task (Ostermann et al., 2018), the Rochester Story Completion (ROCStories) corpus (Mostafazadeh et al., 2016), and the Situations with Adversarial Generations (SWAG) grounded inference corpus (Zellers et al., 2018). After their release, very large language models (LMs) were able to reach or surpass human-level performance on SNLI (Peters et al., 2018) and SWAG (Devlin et al., 2018). However, researchers have found inadequacies in these datasets and the models trained on them. Despite the strong performance of recent systems on SNLI (e.g., Chen et al., 2017; Parikh et al., 2016), Glockner et al. (2018) show that by making trivial changes to the test set, these methods suffered. Further, Pavlick and Callison-Burch (2016) show that state-of-the-art models for natural language inference fail on a task requiring only reasoning over adjective-noun relations. Relatedly, in the shared task to predict sentence endings of ROCStories, Schwartz et al. (2017) show that by incorporating style features, with only the answer choices as input, it is possible to reach near stateof-the-art performance. These results point to implicit bias baked into the data sets"
D19-1625,W15-0107,0,0.0304347,"18) showed that models based on distributional semantics without explicit external knowledge perform poorly at predicting physical plausibility of actions. Lucy and Gauthier (2017) investigate perceptual properties of distributional embeddings and suggest that part–whole properties like has legs are well encoded by embeddings. This may help explain why the simple word-based MLP models perform well without other sources of context. Rei et al. (2018) introduce an effective neural architecture for learning word-embedding based models for graded lexical entailment. Prior work (Bulat et al., 2016; Fagarasan et al., 2015) utilizes embeddings to predict real-world perceptual proper6053 Whole Part Adjective Label NLI premise NLI hypothesis armchair arm black Probably On the back of the president’s quaint black armchair there was emblazoned a half-sun, brilliant with its gilded rays. The armchair’s arm is black. vanity mirror white Impossible bench support wooden Unrelated A door to a bathroom half open and a white vanity. In front of me about five feet distance, stood a wooden bench. The vanity’s mirror is white. The bench’s support is wooden. Table 1: Example triples and retrieved premise sentences, with labels"
D19-1625,P17-1025,0,0.0199664,"h provides a focused evaluation, based on a specific task in commonsense reasoning. Gathering and validating data from crowd workers, we evaluate a number of approaches to performing these inferences, a three-way lexical entailment problem. We find that simple word embedding-based models perform adequately, but beneath humans, on this task, with recent large LM approaches (Devlin et al., 2018; Radford et al., 2018) providing only slight improvement over the purely lexical approach. 2 Related Work Other researchers have constructed datasets investigating similar ideas in commonsense reasoning. Forbes and Choi (2017) develop a dataset and methods for inferring physical commonsense knowledge from verb usage, showing it is possible to learn the physical implications of unseen verbs from a small seed set. Zhang et al. (2017) create a large dataset for general commonsense inference in the form of premise-hypothesis pairs, equipped with ordinal labels ranging from “impossible” to “very likely”. We adopt much of their methodology but for a targeted subset of commonsense reasoning. The SemEval 2018 Task 10 on Capturing Discriminative Attributes (Krebs et al., 2018) describes a similar lexical reasoning task invo"
D19-1625,P18-2103,0,0.0129536,"nsense shared task (Ostermann et al., 2018), the Rochester Story Completion (ROCStories) corpus (Mostafazadeh et al., 2016), and the Situations with Adversarial Generations (SWAG) grounded inference corpus (Zellers et al., 2018). After their release, very large language models (LMs) were able to reach or surpass human-level performance on SNLI (Peters et al., 2018) and SWAG (Devlin et al., 2018). However, researchers have found inadequacies in these datasets and the models trained on them. Despite the strong performance of recent systems on SNLI (e.g., Chen et al., 2017; Parikh et al., 2016), Glockner et al. (2018) show that by making trivial changes to the test set, these methods suffered. Further, Pavlick and Callison-Burch (2016) show that state-of-the-art models for natural language inference fail on a task requiring only reasoning over adjective-noun relations. Relatedly, in the shared task to predict sentence endings of ROCStories, Schwartz et al. (2017) show that by incorporating style features, with only the answer choices as input, it is possible to reach near stateof-the-art performance. These results point to implicit bias baked into the data sets. Rudinger et al. (2017) demonstrate similar s"
D19-1625,S13-1035,0,0.0354196,"and retrieved premise sentences, with labels, used for training word embedding-based models and language model fine-tuning. ties, and we expect an approach that leverages this will help solve this task, but we leave it to future work. 3 Candidate collection We seek to annotate examples of (whole, part, adjective) triples with answers to the question: “Does an hadjectivei hwholei have an hadjectivei hparti?” As a major part of our contribution, we provide an annotated dataset that is visually grounded, with relations mined from Visual Genome (Krishna et al., 2017) and Google Syntactic N-grams (Goldberg and Orwant, 2013). We provide an overview here, with details in Appendix A. 3.1 Part–whole relations Visual Genome (VG) is a large dataset of images annotated with objects, their attributes, and the relations between them. We start by considering all relationships in the VG dataset where the predicate is an underspecified has relation. We count the number of images in which a pair of objects appear in a has relation, and keep only those pairs appearing in at least three distinct images. 3.2 Adjectives We gather adjectives from both Google Syntactic N-grams and VG. From Syntactic N-grams, we count the occurrenc"
D19-1625,S18-1117,0,0.0272239,"ting similar ideas in commonsense reasoning. Forbes and Choi (2017) develop a dataset and methods for inferring physical commonsense knowledge from verb usage, showing it is possible to learn the physical implications of unseen verbs from a small seed set. Zhang et al. (2017) create a large dataset for general commonsense inference in the form of premise-hypothesis pairs, equipped with ordinal labels ranging from “impossible” to “very likely”. We adopt much of their methodology but for a targeted subset of commonsense reasoning. The SemEval 2018 Task 10 on Capturing Discriminative Attributes (Krebs et al., 2018) describes a similar lexical reasoning task involving triplets of words, though it focuses on finding attributes that distinguish two concepts, while in our work the adjective may well apply to both part and whole. Past work has also evaluated commonsense capabilities in neural models. Pavlick and CallisonBurch (2016) investigate the related problem of entailment in adjective-nouns, and show surprising negative results for neural NLI models. Wang et al. (2018) showed that models based on distributional semantics without explicit external knowledge perform poorly at predicting physical plausibi"
D19-1625,W17-2810,0,0.0165701,"lar lexical reasoning task involving triplets of words, though it focuses on finding attributes that distinguish two concepts, while in our work the adjective may well apply to both part and whole. Past work has also evaluated commonsense capabilities in neural models. Pavlick and CallisonBurch (2016) investigate the related problem of entailment in adjective-nouns, and show surprising negative results for neural NLI models. Wang et al. (2018) showed that models based on distributional semantics without explicit external knowledge perform poorly at predicting physical plausibility of actions. Lucy and Gauthier (2017) investigate perceptual properties of distributional embeddings and suggest that part–whole properties like has legs are well encoded by embeddings. This may help explain why the simple word-based MLP models perform well without other sources of context. Rei et al. (2018) introduce an effective neural architecture for learning word-embedding based models for graded lexical entailment. Prior work (Bulat et al., 2016; Fagarasan et al., 2015) utilizes embeddings to predict real-world perceptual proper6053 Whole Part Adjective Label NLI premise NLI hypothesis armchair arm black Probably On the bac"
D19-1625,N16-1098,0,0.0313962,"his knowledge may also help a visual agent reason about unseen objects: it knows a brick house does not have a brick door without needing to see the door. The past few years have seen a raft of data sets intended to test our ability to construct models with an understanding of commonsense knowledge. Standout examples are the Stanford Natural Language Inference (SNLI) and related MultiGenre Natural Language Inference (MNLI) corpora (Bowman et al., 2015; Williams et al., 2018), the SemEval-2018 commonsense shared task (Ostermann et al., 2018), the Rochester Story Completion (ROCStories) corpus (Mostafazadeh et al., 2016), and the Situations with Adversarial Generations (SWAG) grounded inference corpus (Zellers et al., 2018). After their release, very large language models (LMs) were able to reach or surpass human-level performance on SNLI (Peters et al., 2018) and SWAG (Devlin et al., 2018). However, researchers have found inadequacies in these datasets and the models trained on them. Despite the strong performance of recent systems on SNLI (e.g., Chen et al., 2017; Parikh et al., 2016), Glockner et al. (2018) show that by making trivial changes to the test set, these methods suffered. Further, Pavlick and Ca"
D19-1625,S18-1119,0,0.0250216,"e, *Research conducted while author was at USC/ISI the car may now be fast. This knowledge may also help a visual agent reason about unseen objects: it knows a brick house does not have a brick door without needing to see the door. The past few years have seen a raft of data sets intended to test our ability to construct models with an understanding of commonsense knowledge. Standout examples are the Stanford Natural Language Inference (SNLI) and related MultiGenre Natural Language Inference (MNLI) corpora (Bowman et al., 2015; Williams et al., 2018), the SemEval-2018 commonsense shared task (Ostermann et al., 2018), the Rochester Story Completion (ROCStories) corpus (Mostafazadeh et al., 2016), and the Situations with Adversarial Generations (SWAG) grounded inference corpus (Zellers et al., 2018). After their release, very large language models (LMs) were able to reach or surpass human-level performance on SNLI (Peters et al., 2018) and SWAG (Devlin et al., 2018). However, researchers have found inadequacies in these datasets and the models trained on them. Despite the strong performance of recent systems on SNLI (e.g., Chen et al., 2017; Parikh et al., 2016), Glockner et al. (2018) show that by making"
D19-1625,D16-1244,0,0.089233,"Missing"
D19-1625,P16-1204,0,0.0306828,"et al., 2016), and the Situations with Adversarial Generations (SWAG) grounded inference corpus (Zellers et al., 2018). After their release, very large language models (LMs) were able to reach or surpass human-level performance on SNLI (Peters et al., 2018) and SWAG (Devlin et al., 2018). However, researchers have found inadequacies in these datasets and the models trained on them. Despite the strong performance of recent systems on SNLI (e.g., Chen et al., 2017; Parikh et al., 2016), Glockner et al. (2018) show that by making trivial changes to the test set, these methods suffered. Further, Pavlick and Callison-Burch (2016) show that state-of-the-art models for natural language inference fail on a task requiring only reasoning over adjective-noun relations. Relatedly, in the shared task to predict sentence endings of ROCStories, Schwartz et al. (2017) show that by incorporating style features, with only the answer choices as input, it is possible to reach near stateof-the-art performance. These results point to implicit bias baked into the data sets. Rudinger et al. (2017) demonstrate similar systematic and social bias in SNLI, attributing it to the fact that hypothesis sentences were written by crowd workers. T"
D19-1625,D14-1162,0,0.0844078,"abel distribution is shown in Table 2. The dataset has 728 unique part nouns, 873 unique whole nouns, and 553 unique adjectives. 5 Inference baselines We now describe several basic approaches for solving these commonsense inference problems, which we intend as a baseline to be built upon by Word embedding models We approach the problem as categorical classification and train a multi-layer perceptron (MLP) model to classify inputs consisting of word embeddings for the whole, part, and adjective words. The MLP takes as input the concatenation of these three word embeddings, obtained from GloVe (Pennington et al., 2014), and applies a single hidden layer with ReLU activation before the final softmax layer which predicts the class label. 5.2 Adjective projection as NLI As we want to evaluate strong yet simple preexisting language understanding models on this task, we now describe a method for obtaining the direct prediction described above via conversion to a form suitable for inference in the style of the SNLI and MNLI datasets (Bowman et al., 2015; Williams et al., 2018), which consist of premise and hypothesis sentence pairs. We first form simple hypothesis sentences from the tuples using the fixed templat"
D19-1625,N18-1202,0,0.018836,"ls with an understanding of commonsense knowledge. Standout examples are the Stanford Natural Language Inference (SNLI) and related MultiGenre Natural Language Inference (MNLI) corpora (Bowman et al., 2015; Williams et al., 2018), the SemEval-2018 commonsense shared task (Ostermann et al., 2018), the Rochester Story Completion (ROCStories) corpus (Mostafazadeh et al., 2016), and the Situations with Adversarial Generations (SWAG) grounded inference corpus (Zellers et al., 2018). After their release, very large language models (LMs) were able to reach or surpass human-level performance on SNLI (Peters et al., 2018) and SWAG (Devlin et al., 2018). However, researchers have found inadequacies in these datasets and the models trained on them. Despite the strong performance of recent systems on SNLI (e.g., Chen et al., 2017; Parikh et al., 2016), Glockner et al. (2018) show that by making trivial changes to the test set, these methods suffered. Further, Pavlick and Callison-Burch (2016) show that state-of-the-art models for natural language inference fail on a task requiring only reasoning over adjective-noun relations. Relatedly, in the shared task to predict sentence endings of ROCStories, Schwartz et al."
D19-1625,P18-2101,0,0.0229796,"nd CallisonBurch (2016) investigate the related problem of entailment in adjective-nouns, and show surprising negative results for neural NLI models. Wang et al. (2018) showed that models based on distributional semantics without explicit external knowledge perform poorly at predicting physical plausibility of actions. Lucy and Gauthier (2017) investigate perceptual properties of distributional embeddings and suggest that part–whole properties like has legs are well encoded by embeddings. This may help explain why the simple word-based MLP models perform well without other sources of context. Rei et al. (2018) introduce an effective neural architecture for learning word-embedding based models for graded lexical entailment. Prior work (Bulat et al., 2016; Fagarasan et al., 2015) utilizes embeddings to predict real-world perceptual proper6053 Whole Part Adjective Label NLI premise NLI hypothesis armchair arm black Probably On the back of the president’s quaint black armchair there was emblazoned a half-sun, brilliant with its gilded rays. The armchair’s arm is black. vanity mirror white Impossible bench support wooden Unrelated A door to a bathroom half open and a white vanity. In front of me about f"
D19-1625,W17-1609,0,0.0462981,"Missing"
D19-1625,K17-1004,0,0.0175119,"s et al., 2018) and SWAG (Devlin et al., 2018). However, researchers have found inadequacies in these datasets and the models trained on them. Despite the strong performance of recent systems on SNLI (e.g., Chen et al., 2017; Parikh et al., 2016), Glockner et al. (2018) show that by making trivial changes to the test set, these methods suffered. Further, Pavlick and Callison-Burch (2016) show that state-of-the-art models for natural language inference fail on a task requiring only reasoning over adjective-noun relations. Relatedly, in the shared task to predict sentence endings of ROCStories, Schwartz et al. (2017) show that by incorporating style features, with only the answer choices as input, it is possible to reach near stateof-the-art performance. These results point to implicit bias baked into the data sets. Rudinger et al. (2017) demonstrate similar systematic and social bias in SNLI, attributing it to the fact that hypothesis sentences were written by crowd workers. The SWAG data set was specif6052 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 6052–6058, c Hong Kong, China, N"
D19-1625,N18-2049,0,0.119218,"their methodology but for a targeted subset of commonsense reasoning. The SemEval 2018 Task 10 on Capturing Discriminative Attributes (Krebs et al., 2018) describes a similar lexical reasoning task involving triplets of words, though it focuses on finding attributes that distinguish two concepts, while in our work the adjective may well apply to both part and whole. Past work has also evaluated commonsense capabilities in neural models. Pavlick and CallisonBurch (2016) investigate the related problem of entailment in adjective-nouns, and show surprising negative results for neural NLI models. Wang et al. (2018) showed that models based on distributional semantics without explicit external knowledge perform poorly at predicting physical plausibility of actions. Lucy and Gauthier (2017) investigate perceptual properties of distributional embeddings and suggest that part–whole properties like has legs are well encoded by embeddings. This may help explain why the simple word-based MLP models perform well without other sources of context. Rei et al. (2018) introduce an effective neural architecture for learning word-embedding based models for graded lexical entailment. Prior work (Bulat et al., 2016; Fag"
D19-1625,N18-1101,0,0.249084,"dshield, the car remains slow, whereas if the new part is an engine, *Research conducted while author was at USC/ISI the car may now be fast. This knowledge may also help a visual agent reason about unseen objects: it knows a brick house does not have a brick door without needing to see the door. The past few years have seen a raft of data sets intended to test our ability to construct models with an understanding of commonsense knowledge. Standout examples are the Stanford Natural Language Inference (SNLI) and related MultiGenre Natural Language Inference (MNLI) corpora (Bowman et al., 2015; Williams et al., 2018), the SemEval-2018 commonsense shared task (Ostermann et al., 2018), the Rochester Story Completion (ROCStories) corpus (Mostafazadeh et al., 2016), and the Situations with Adversarial Generations (SWAG) grounded inference corpus (Zellers et al., 2018). After their release, very large language models (LMs) were able to reach or surpass human-level performance on SNLI (Peters et al., 2018) and SWAG (Devlin et al., 2018). However, researchers have found inadequacies in these datasets and the models trained on them. Despite the strong performance of recent systems on SNLI (e.g., Chen et al., 2017"
D19-1625,D18-1009,0,0.0129478,"brick door without needing to see the door. The past few years have seen a raft of data sets intended to test our ability to construct models with an understanding of commonsense knowledge. Standout examples are the Stanford Natural Language Inference (SNLI) and related MultiGenre Natural Language Inference (MNLI) corpora (Bowman et al., 2015; Williams et al., 2018), the SemEval-2018 commonsense shared task (Ostermann et al., 2018), the Rochester Story Completion (ROCStories) corpus (Mostafazadeh et al., 2016), and the Situations with Adversarial Generations (SWAG) grounded inference corpus (Zellers et al., 2018). After their release, very large language models (LMs) were able to reach or surpass human-level performance on SNLI (Peters et al., 2018) and SWAG (Devlin et al., 2018). However, researchers have found inadequacies in these datasets and the models trained on them. Despite the strong performance of recent systems on SNLI (e.g., Chen et al., 2017; Parikh et al., 2016), Glockner et al. (2018) show that by making trivial changes to the test set, these methods suffered. Further, Pavlick and Callison-Burch (2016) show that state-of-the-art models for natural language inference fail on a task requi"
D19-1672,D16-1153,0,0.0186096,"mising direction to further improve the model performances. Our results can shed light on future research for improving cross-lingual NER. 1 Introduction Named Entity Recognition (NER) is an important NLP task that identifies the boundary and type of named entities (e.g., person, organization, location) in texts. However, for some languages, it is hard to obtain enough labeled data to build a fully supervised learning model. Cross-lingual transfer models, which train on high-resource languages and transfer to target languages are a promising direction for languages with little annotated data (Bharadwaj et al., 2016; Tsai et al., 2016; Pan et al., 2017; Yang et al., 2017; Mayhew et al., 2017; Wang et al., 2017; Cotterell and Duh, 2017; Feng et al., 2018; Xie et al., 2018; Zhou et al., 2019). ∗ The work was done when the first author worked as an intern at USC ISI. Cross-lingual NER models learn transferable knowledge mainly from four sources: translation (Mayhew et al., 2017; Feng et al., 2018), bilingual embeddings (Ni et al., 2017; Xie et al., 2018), phonetic alphabets (Bharadwaj et al., 2016) and multi-source learning (Mayhew et al., 2017; Lin et al., 2018; Zhou et al., 2019; Chen et al., 2019; Rahimi"
D19-1672,Q17-1010,0,0.031467,"a and replace numbers and URLs as “num” and “url” respectively. We use the training data to train our model, the development sets to select the best well-trained model, and the test sets to evaluate performance, where the training data is English and the development and test sets are from the target language. 3.2 Experimental Settings Bilingual Embedding. We use the 300dimensional bilingual word embeddings pretrained by MUSE (Conneau et al., 2018) and 300dimensional randomly initialized character embeddings. Specifically, we first collect monolingual pre-trained word embeddings from fastText (Bojanowski et al., 2017). We then align the embeddings using the MUSE supervised model. We normalize the inputs to MUSE and keep the other hyperparameters as defaults. We merge the aligned word embeddings and remove duplicates. Updating the embedding during training time will change the vector space of partial bilingual embeddings and therefore break the vector alignment. Thus, we freeze the embedding weights during the training step. For OOV (out-of-vocabulary) words, we use a randomly initialized 300 dimension vector within [−0.1, 0.1]. Reverse Translation. We follow the general translation step in Section 2. Speci"
D19-1672,P19-1299,0,0.0136879,"a (Bharadwaj et al., 2016; Tsai et al., 2016; Pan et al., 2017; Yang et al., 2017; Mayhew et al., 2017; Wang et al., 2017; Cotterell and Duh, 2017; Feng et al., 2018; Xie et al., 2018; Zhou et al., 2019). ∗ The work was done when the first author worked as an intern at USC ISI. Cross-lingual NER models learn transferable knowledge mainly from four sources: translation (Mayhew et al., 2017; Feng et al., 2018), bilingual embeddings (Ni et al., 2017; Xie et al., 2018), phonetic alphabets (Bharadwaj et al., 2016) and multi-source learning (Mayhew et al., 2017; Lin et al., 2018; Zhou et al., 2019; Chen et al., 2019; Rahimi et al., 2019). Translation can be applied at either the sentence level via machine translation or at the word and phrase level via application of bilingual dictionaries. Bilingual embeddings are a form of bilingual dictionaries; they constitute a projection from one pre-trained language representation into the same vector space as the other such that words with the same meaning have similar representations (Conneau et al., 2018). Phonetic alphabets enable different languages to share the same pronunciation characters so that the character-level knowledge is transferable between langua"
D19-1672,L16-1720,0,0.0134871,"ns. to the performance, the entities with two tokens achieve the higher scores, while the entities with more than 2 tokens decrease significantly ranging from 12.59 to 25.04 absolute percentages of F1 scores. The observation indicates that entities longer than two tokens are more difficult to infer. This might encourage us to balance the weight of long entities in our current evaluation method which ignores entity length when datasets have high volumes of long entities. 4 Case Study: Bengali Models CTNER CTNER+ CTNER* CTNER*+ Our Model F1 score 30.47 31.70 46.28 45.70 34.29 and online forums (Cieri et al., 2016). By contrast, the only data source of CoNLL data is from news articles (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). While transfer can help provide universal context clue information across languages there is no substitute for a resource of actual names. Language Spanish Dutch German Bengali The previous cross-lingual settings were only for European languages, which share similar alphabets. However, many languages use non-Latin orthography. In this work, we present a case study on Bengali, which does not use a Latin alphabet. We compare our proposed method with the translation"
D19-1672,I17-2016,0,0.0284126,"cross-lingual NER. 1 Introduction Named Entity Recognition (NER) is an important NLP task that identifies the boundary and type of named entities (e.g., person, organization, location) in texts. However, for some languages, it is hard to obtain enough labeled data to build a fully supervised learning model. Cross-lingual transfer models, which train on high-resource languages and transfer to target languages are a promising direction for languages with little annotated data (Bharadwaj et al., 2016; Tsai et al., 2016; Pan et al., 2017; Yang et al., 2017; Mayhew et al., 2017; Wang et al., 2017; Cotterell and Duh, 2017; Feng et al., 2018; Xie et al., 2018; Zhou et al., 2019). ∗ The work was done when the first author worked as an intern at USC ISI. Cross-lingual NER models learn transferable knowledge mainly from four sources: translation (Mayhew et al., 2017; Feng et al., 2018), bilingual embeddings (Ni et al., 2017; Xie et al., 2018), phonetic alphabets (Bharadwaj et al., 2016) and multi-source learning (Mayhew et al., 2017; Lin et al., 2018; Zhou et al., 2019; Chen et al., 2019; Rahimi et al., 2019). Translation can be applied at either the sentence level via machine translation or at the word and phrase"
D19-1672,P18-4003,1,0.876004,"via machine translation or at the word and phrase level via application of bilingual dictionaries. Bilingual embeddings are a form of bilingual dictionaries; they constitute a projection from one pre-trained language representation into the same vector space as the other such that words with the same meaning have similar representations (Conneau et al., 2018). Phonetic alphabets enable different languages to share the same pronunciation characters so that the character-level knowledge is transferable between languages that otherwise have different character sets, such as English and Bengali (Hermjakob et al., 2018). Multi-source learning is effective when multiple or similar language resources are available by learning shareable knowledge. For example, training a model on both English and Hindi can significantly improve the model performance on Bengali than only using English (Mayhew et al., 2017). However, there is little prior work with detailed analysis of how cross-lingual NER models transfer knowledge between languages on different levels. In this paper, we focus on a single-source zeroshot transfer setting where we transfer from English to target languages that have no annotated data. In our setti"
D19-1672,N04-1036,0,0.0437452,"Missing"
D19-1672,N16-1030,0,0.0667801,"ation for Computational Linguistics alphabets. Next, we conduct qualitative analyses to answer the following questions on how the model transfers knowledge under the cross-lingual settings: 1) does the source language syntax matter? 2) how do word and character embeddings affect the model transfer? We analyze how F1 scores differ across different entity lengths. Finally, we conduct a case study on Bengali. 2 Model NER models take a sequence of tokens as input and predict a sequence of labels such as person (PER) or location (LOC). In this paper, we adopt the neural architecture NER model from Lample et al. (2016); Ma and Hovy (2016). The model first combines pre-trained word and character embeddings as token representations, then feeds the representations to one layer of a Bidirectional Long Term Short Memory (Bi-LSTM) (Hochreiter and Schmidhuber, 1997), and finally predicts the outputs via a Conditional Random Field (CRF). We show the model architecture on left of Figure 1. However, languages express the same named entity in different words and characters. To bridge the barriers, we combine three strategies: bilingual embedding, reverse translation, and transliteration. Bilingual Embedding. Word embe"
D19-1672,P18-1074,0,0.0121823,"r languages with little annotated data (Bharadwaj et al., 2016; Tsai et al., 2016; Pan et al., 2017; Yang et al., 2017; Mayhew et al., 2017; Wang et al., 2017; Cotterell and Duh, 2017; Feng et al., 2018; Xie et al., 2018; Zhou et al., 2019). ∗ The work was done when the first author worked as an intern at USC ISI. Cross-lingual NER models learn transferable knowledge mainly from four sources: translation (Mayhew et al., 2017; Feng et al., 2018), bilingual embeddings (Ni et al., 2017; Xie et al., 2018), phonetic alphabets (Bharadwaj et al., 2016) and multi-source learning (Mayhew et al., 2017; Lin et al., 2018; Zhou et al., 2019; Chen et al., 2019; Rahimi et al., 2019). Translation can be applied at either the sentence level via machine translation or at the word and phrase level via application of bilingual dictionaries. Bilingual embeddings are a form of bilingual dictionaries; they constitute a projection from one pre-trained language representation into the same vector space as the other such that words with the same meaning have similar representations (Conneau et al., 2018). Phonetic alphabets enable different languages to share the same pronunciation characters so that the character-level kn"
D19-1672,P16-1101,0,0.0128327,"l Linguistics alphabets. Next, we conduct qualitative analyses to answer the following questions on how the model transfers knowledge under the cross-lingual settings: 1) does the source language syntax matter? 2) how do word and character embeddings affect the model transfer? We analyze how F1 scores differ across different entity lengths. Finally, we conduct a case study on Bengali. 2 Model NER models take a sequence of tokens as input and predict a sequence of labels such as person (PER) or location (LOC). In this paper, we adopt the neural architecture NER model from Lample et al. (2016); Ma and Hovy (2016). The model first combines pre-trained word and character embeddings as token representations, then feeds the representations to one layer of a Bidirectional Long Term Short Memory (Bi-LSTM) (Hochreiter and Schmidhuber, 1997), and finally predicts the outputs via a Conditional Random Field (CRF). We show the model architecture on left of Figure 1. However, languages express the same named entity in different words and characters. To bridge the barriers, we combine three strategies: bilingual embedding, reverse translation, and transliteration. Bilingual Embedding. Word embeddings are usually t"
D19-1672,D17-1269,0,0.0350975,"Missing"
D19-1672,P17-1135,0,0.0290309,"Missing"
D19-1672,P17-1178,1,0.851071,"el performances. Our results can shed light on future research for improving cross-lingual NER. 1 Introduction Named Entity Recognition (NER) is an important NLP task that identifies the boundary and type of named entities (e.g., person, organization, location) in texts. However, for some languages, it is hard to obtain enough labeled data to build a fully supervised learning model. Cross-lingual transfer models, which train on high-resource languages and transfer to target languages are a promising direction for languages with little annotated data (Bharadwaj et al., 2016; Tsai et al., 2016; Pan et al., 2017; Yang et al., 2017; Mayhew et al., 2017; Wang et al., 2017; Cotterell and Duh, 2017; Feng et al., 2018; Xie et al., 2018; Zhou et al., 2019). ∗ The work was done when the first author worked as an intern at USC ISI. Cross-lingual NER models learn transferable knowledge mainly from four sources: translation (Mayhew et al., 2017; Feng et al., 2018), bilingual embeddings (Ni et al., 2017; Xie et al., 2018), phonetic alphabets (Bharadwaj et al., 2016) and multi-source learning (Mayhew et al., 2017; Lin et al., 2018; Zhou et al., 2019; Chen et al., 2019; Rahimi et al., 2019). Translation can be ap"
D19-1672,D15-1064,1,0.821467,"we replace English words with target language words if the translation pairs exist in the bilingual dictionaries (Rolston and Kirchhoff, 2016). If a translation pair does not exist for a word, we keep the word unchanged in the training data. We use the pre-trained bilingual embedding to help select the best choice of polysemy. Model parameters. We use 300-dimension hidden states for both character and token level Bi-LSTMs. To prevent overfitting, we apply dropout (Srivastava et al., 2014) with a rate of 0.5 on outputs of the two Bi-LSTMs. We follow the Conditional Random Field (CRF) setup of Peng and Dredze (2015). We then randomly initialize the weights of the layers within [−0.1, 0.1]. We train the model for 200 epochs and optimize the parameters by Stochastic Gradient Descent (SGD) with momentum, gradient clipping, and learning rate decay. We set the learning rate (lr) and the decay rate (dr) as 0.01 and 0.05 respectively. We uplr date the learning rate by (1+(n+1)∗dr) after epoch n. We clip the gradients to the range of [-5.0, 5.0]. We measure performance by F1 score. 3.3 Baselines In this study, we compare our proposed method with three close works under the cross-lingual settings. We compare our"
D19-1672,P19-1015,0,0.0342167,", 2016; Tsai et al., 2016; Pan et al., 2017; Yang et al., 2017; Mayhew et al., 2017; Wang et al., 2017; Cotterell and Duh, 2017; Feng et al., 2018; Xie et al., 2018; Zhou et al., 2019). ∗ The work was done when the first author worked as an intern at USC ISI. Cross-lingual NER models learn transferable knowledge mainly from four sources: translation (Mayhew et al., 2017; Feng et al., 2018), bilingual embeddings (Ni et al., 2017; Xie et al., 2018), phonetic alphabets (Bharadwaj et al., 2016) and multi-source learning (Mayhew et al., 2017; Lin et al., 2018; Zhou et al., 2019; Chen et al., 2019; Rahimi et al., 2019). Translation can be applied at either the sentence level via machine translation or at the word and phrase level via application of bilingual dictionaries. Bilingual embeddings are a form of bilingual dictionaries; they constitute a projection from one pre-trained language representation into the same vector space as the other such that words with the same meaning have similar representations (Conneau et al., 2018). Phonetic alphabets enable different languages to share the same pronunciation characters so that the character-level knowledge is transferable between languages that otherwise hav"
D19-1672,W09-1119,0,0.0821579,"different European languages, Spanish, Dutch, English, and German. The data contains four types of named entities: person (PER), organization (ORG), location (LOC) and MISC. We use Bengali language data from LDC2015E13 (V2.1). To be consistent, we only keep the PER, ORG and LOC tags and ignore MISC tags from the Bengali corpus. In this study, we choose the BIOSE tag schema instead of standard BIO, where the B, I, O, S, E refer to the beginning, inside, outside, single and end of an entity, respectively. Previous work shows that BIOSE can learn a more expressive model than the BIO schema does (Ratinov and Roth, 2009). We lowercase the data and replace numbers and URLs as “num” and “url” respectively. We use the training data to train our model, the development sets to select the best well-trained model, and the test sets to evaluate performance, where the training data is English and the development and test sets are from the target language. 3.2 Experimental Settings Bilingual Embedding. We use the 300dimensional bilingual word embeddings pretrained by MUSE (Conneau et al., 2018) and 300dimensional randomly initialized character embeddings. Specifically, we first collect monolingual pre-trained word embe"
D19-1672,P19-1336,0,0.0467955,"Missing"
D19-1672,W02-2024,0,0.484143,"may not share the same characters, but some named entities in the multilingual corpora share phonetic similarities (Huang et al., 2004). To map multilingual corpora into the same character space and connect the phonetic similarity between entities, we employ Uroman1 (Hermjakob et al., 2018), which transliterates any language into something roughly pronounceable English spellings, while keeps the English words unchanged. We show some examples of Uroman in Table 1. 3 Experiments In this study, we first evaluate our proposed crosslingual method on the CoNLL 2002 and 2003 NER datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). We then conduct ablation studies to examine how the model learns transferable knowledge under the crosslingual settings. Finally, we conduct a case study on Bengali, a low resource language. 6396 1 https://www.isi.edu/˜ulf/uroman.html B-ORG I-ORG E-ORG O S-PER Hidden States Bilingual Embedding …. …. …. …. CRF …. Bi-LSTM …… g r u Translate …… n gruen Uroman grün Tokens Representations Character Bi-LSTM green Token Figure 1: Architecture of our proposed model. Each word is translated from the source. Each token representation contains two concatenated part"
D19-1672,W03-0419,0,0.483696,"Missing"
D19-1672,K16-1022,0,0.0665748,"her improve the model performances. Our results can shed light on future research for improving cross-lingual NER. 1 Introduction Named Entity Recognition (NER) is an important NLP task that identifies the boundary and type of named entities (e.g., person, organization, location) in texts. However, for some languages, it is hard to obtain enough labeled data to build a fully supervised learning model. Cross-lingual transfer models, which train on high-resource languages and transfer to target languages are a promising direction for languages with little annotated data (Bharadwaj et al., 2016; Tsai et al., 2016; Pan et al., 2017; Yang et al., 2017; Mayhew et al., 2017; Wang et al., 2017; Cotterell and Duh, 2017; Feng et al., 2018; Xie et al., 2018; Zhou et al., 2019). ∗ The work was done when the first author worked as an intern at USC ISI. Cross-lingual NER models learn transferable knowledge mainly from four sources: translation (Mayhew et al., 2017; Feng et al., 2018), bilingual embeddings (Ni et al., 2017; Xie et al., 2018), phonetic alphabets (Bharadwaj et al., 2016) and multi-source learning (Mayhew et al., 2017; Lin et al., 2018; Zhou et al., 2019; Chen et al., 2019; Rahimi et al., 2019). Tra"
D19-1672,I17-2065,1,0.855877,"arch for improving cross-lingual NER. 1 Introduction Named Entity Recognition (NER) is an important NLP task that identifies the boundary and type of named entities (e.g., person, organization, location) in texts. However, for some languages, it is hard to obtain enough labeled data to build a fully supervised learning model. Cross-lingual transfer models, which train on high-resource languages and transfer to target languages are a promising direction for languages with little annotated data (Bharadwaj et al., 2016; Tsai et al., 2016; Pan et al., 2017; Yang et al., 2017; Mayhew et al., 2017; Wang et al., 2017; Cotterell and Duh, 2017; Feng et al., 2018; Xie et al., 2018; Zhou et al., 2019). ∗ The work was done when the first author worked as an intern at USC ISI. Cross-lingual NER models learn transferable knowledge mainly from four sources: translation (Mayhew et al., 2017; Feng et al., 2018), bilingual embeddings (Ni et al., 2017; Xie et al., 2018), phonetic alphabets (Bharadwaj et al., 2016) and multi-source learning (Mayhew et al., 2017; Lin et al., 2018; Zhou et al., 2019; Chen et al., 2019; Rahimi et al., 2019). Translation can be applied at either the sentence level via machine translation"
D19-1672,D18-1034,0,0.06626,"ty Recognition (NER) is an important NLP task that identifies the boundary and type of named entities (e.g., person, organization, location) in texts. However, for some languages, it is hard to obtain enough labeled data to build a fully supervised learning model. Cross-lingual transfer models, which train on high-resource languages and transfer to target languages are a promising direction for languages with little annotated data (Bharadwaj et al., 2016; Tsai et al., 2016; Pan et al., 2017; Yang et al., 2017; Mayhew et al., 2017; Wang et al., 2017; Cotterell and Duh, 2017; Feng et al., 2018; Xie et al., 2018; Zhou et al., 2019). ∗ The work was done when the first author worked as an intern at USC ISI. Cross-lingual NER models learn transferable knowledge mainly from four sources: translation (Mayhew et al., 2017; Feng et al., 2018), bilingual embeddings (Ni et al., 2017; Xie et al., 2018), phonetic alphabets (Bharadwaj et al., 2016) and multi-source learning (Mayhew et al., 2017; Lin et al., 2018; Zhou et al., 2019; Chen et al., 2019; Rahimi et al., 2019). Translation can be applied at either the sentence level via machine translation or at the word and phrase level via application of bilingual d"
D19-6107,P16-2058,0,0.0183015,"and Italy.) 5 Related Work Cross-lingual Word Embedding Learning. Mikolov et al. (2013b) first notice that word embedding spaces have similar geometric arrangements across languages. They use this property to learn a linear mapping between two spaces. After that, several methods attempt to improve the mapping (Faruqui and Dyer, 2014; Xing et al., 2015; Lazaridou et al., 2015; Ammar et al., 2016; Artetxe et al., 2017; Smith et al., 2017). The measures used to compute similarity between a foreign word and an English word often include distributed monolingual representations on character-level (Costa-jussà and Fonollosa, 2016; Luong and Manning, 2016), subwordlevel (Anwarus Salam et al., 2012; Rei et al., Table 4: Examples of mined parallel sentences from Wikipedia. A portion of alignments are highlighted using the same colors. We randomly select 100 mined parallel sentence pairs for each of 3 language pairs, and ask linguistic experts to judge the quality of these sentence pairs (perfect, partial, or not parallel). The results are shown in Table 5. We can see that the quality of mined parallel sentence is promising and the quality of word and entity alignment is decent. Furthermore, we evaluate the quality of min"
D19-6107,W12-5604,0,0.0326989,"(2013b) first notice that word embedding spaces have similar geometric arrangements across languages. They use this property to learn a linear mapping between two spaces. After that, several methods attempt to improve the mapping (Faruqui and Dyer, 2014; Xing et al., 2015; Lazaridou et al., 2015; Ammar et al., 2016; Artetxe et al., 2017; Smith et al., 2017). The measures used to compute similarity between a foreign word and an English word often include distributed monolingual representations on character-level (Costa-jussà and Fonollosa, 2016; Luong and Manning, 2016), subwordlevel (Anwarus Salam et al., 2012; Rei et al., Table 4: Examples of mined parallel sentences from Wikipedia. A portion of alignments are highlighted using the same colors. We randomly select 100 mined parallel sentence pairs for each of 3 language pairs, and ask linguistic experts to judge the quality of these sentence pairs (perfect, partial, or not parallel). The results are shown in Table 5. We can see that the quality of mined parallel sentence is promising and the quality of word and entity alignment is decent. Furthermore, we evaluate the quality of mined parallel sentences extrinsically using a neural machine translati"
D19-6107,D07-1074,0,0.0882441,"ntion to Entity Probability: |A∗,m |, where A∗,m is a set of anchor links with anchor text m and Ae,m is a subset that links to entity e. Entity Type (Ling et al., 2015): P p(e|m) , where e 7→ t indicates that t is one of e’s e7→t p(e|m) entity types. Conditional probability p(e|m) can be estimated by Fprob (e|m). Fprob (e|m) Ftype (e|m, t) |A | Table 1: Mention disambiguation features. Coherence is driven by the assumption that if multiple mentions appear together within a context window, their referent entities are more likely to be strongly connected to each other in the KB. Previous work (Cucerzan, 2007; Milne and Witten, 2008; Hoffart et al., 2011; Ratinov et al., 2011; Cheng and Roth, 2013; Ceccarelli et al., 2013; Ling et al., 2015) considers the KB as a knowledge graph and models coherence based on the overlapped neighbors of two entities in the knowledge graph. These approaches heavily rely on explicit connections among entities in the knowledge graph and thus cannot capture the coherence between two entities that are implicitly connected. For example, two entities en/Mosquito and en/Cockroach only have very few overlapped neighbors in the knowledge graph, but they usually appear togeth"
D19-6107,P17-1042,0,0.0148444,"LEU score. Classical Chinese - Modern Chinese * ⾄⼆战之时，南斯拉夫屡败，终为德意志、义⼤利所分。 * 在⼆次世界⼤战期间，南斯拉夫多次战败，分别被德国、 意⼤利占领。 (During the World War II, Yugoslavia was defeated several times and was occupied by Germany and Italy.) 5 Related Work Cross-lingual Word Embedding Learning. Mikolov et al. (2013b) first notice that word embedding spaces have similar geometric arrangements across languages. They use this property to learn a linear mapping between two spaces. After that, several methods attempt to improve the mapping (Faruqui and Dyer, 2014; Xing et al., 2015; Lazaridou et al., 2015; Ammar et al., 2016; Artetxe et al., 2017; Smith et al., 2017). The measures used to compute similarity between a foreign word and an English word often include distributed monolingual representations on character-level (Costa-jussà and Fonollosa, 2016; Luong and Manning, 2016), subwordlevel (Anwarus Salam et al., 2012; Rei et al., Table 4: Examples of mined parallel sentences from Wikipedia. A portion of alignments are highlighted using the same colors. We randomly select 100 mined parallel sentence pairs for each of 3 language pairs, and ask linguistic experts to judge the quality of these sentence pairs (perfect, partial, or not p"
D19-6107,E14-1049,0,0.0966405,"Missing"
D19-6107,P18-1073,0,0.0156503,"wledge, this is the first work to incorporate joint entity and word embedding into parallel sentence mining. As a result the sentence pairs we include reliable alignment between entity mentions which are often out-of-vocabulary and ambiguous and thus receive poor alignment quality from previous methods. 2016; Sennrich et al., 2016; Yang et al., 2017), and bi-lingual word embedding (Madhyastha and España-Bonet, 2017). Recent attempts have shown that it is possible to derive cross-lingual word embedding from unaligned corpora in an unsupervised fashion (Zhang et al., 2017; Conneau et al., 2017; Artetxe et al., 2018). Another strategy for cross-lingual word embedding learning is to combine monolingual and cross-lingual training objectives (Zou et al., 2013; Klementiev et al., 2012; Luong et al., 2015; Ammar et al., 2016; Vuli´c et al., 2017). Compared to our direct mapping approach, these methods generally require large size of parallel data. Our work is largely inspired from (Conneau et al., 2017). However, our work focuses on better representing entities, which are fundamentally different from common words or phrases in many aspects as described in Section 1. Previous multilingual word embedding efforts"
D19-6107,W16-2506,0,0.0173554,"ition, considering each pair of Wikipedia articles connected by an inter-language link as comparable documents, we use this multilingual common space to represent sentences and extract many parallel sentence pairs. The novel contributions of this paper are: Traditional approaches to derive training data from Wikipedia usually replace each anchor link with its anchor text, for example, “apple is a technology company.”. These methods have two limitations: (1) Information loss: For example, the anchor text “apple” itself does not convey information such as the entity is a company; (2) Ambiguity (Faruqui et al., 2016): For example, the fruit sense and the company sense of “apple” mistakenly share one surface form. Similar to previous work (Wang et al., 2014; Tsai and Roth, 2016; Yamada et al., 2016), we replace each anchor link with its corresponding entity title, and thus treat each entity title as a unique word. For example, “en/Apple_Inc. is a technology company.”. Using this kind of data mix of entity titles and contextual words, we can learn joint embedding of entities and words. pear juice apple microsoft computer company ibm steve macintosh jobs • We develop a novel approach based on rich anchor lin"
D19-6107,W04-3208,0,0.0345301,"parallel). The results are shown in Table 5. We can see that the quality of mined parallel sentence is promising and the quality of word and entity alignment is decent. Furthermore, we evaluate the quality of mined parallel sentences extrinsically using a neural machine translation (NMT) model. We use the 5 https://github.com/tensorflow/ tensor2tensor 62 Parallel Sentence Mining. Automatic mining parallel sentences from comparable documents is an important and useful task to improve Statistical Machine Translation. Early efforts mainly exploited bilingual word dictionaries for bootstrapping (Fung and Cheung, 2004). Recent approaches are mainly based on bilingual word embeddings (Marie and Fujita, 2017) and sentence embeddings (Schwenk, 2018) to detect sentence pairs or continuous parallel segments (Hangya and Fraser, 2019). To the best of our knowledge, this is the first work to incorporate joint entity and word embedding into parallel sentence mining. As a result the sentence pairs we include reliable alignment between entity mentions which are often out-of-vocabulary and ambiguous and thus receive poor alignment quality from previous methods. 2016; Sennrich et al., 2016; Yang et al., 2017), and bi-li"
D19-6107,D18-1021,0,0.0360999,"Missing"
D19-6107,P19-1118,0,0.0197823,"ined parallel sentences extrinsically using a neural machine translation (NMT) model. We use the 5 https://github.com/tensorflow/ tensor2tensor 62 Parallel Sentence Mining. Automatic mining parallel sentences from comparable documents is an important and useful task to improve Statistical Machine Translation. Early efforts mainly exploited bilingual word dictionaries for bootstrapping (Fung and Cheung, 2004). Recent approaches are mainly based on bilingual word embeddings (Marie and Fujita, 2017) and sentence embeddings (Schwenk, 2018) to detect sentence pairs or continuous parallel segments (Hangya and Fraser, 2019). To the best of our knowledge, this is the first work to incorporate joint entity and word embedding into parallel sentence mining. As a result the sentence pairs we include reliable alignment between entity mentions which are often out-of-vocabulary and ambiguous and thus receive poor alignment quality from previous methods. 2016; Sennrich et al., 2016; Yang et al., 2017), and bi-lingual word embedding (Madhyastha and España-Bonet, 2017). Recent attempts have shown that it is possible to derive cross-lingual word embedding from unaligned corpora in an unsupervised fashion (Zhang et al., 2017"
D19-6107,D11-1072,0,0.0617505,"ere A∗,m is a set of anchor links with anchor text m and Ae,m is a subset that links to entity e. Entity Type (Ling et al., 2015): P p(e|m) , where e 7→ t indicates that t is one of e’s e7→t p(e|m) entity types. Conditional probability p(e|m) can be estimated by Fprob (e|m). Fprob (e|m) Ftype (e|m, t) |A | Table 1: Mention disambiguation features. Coherence is driven by the assumption that if multiple mentions appear together within a context window, their referent entities are more likely to be strongly connected to each other in the KB. Previous work (Cucerzan, 2007; Milne and Witten, 2008; Hoffart et al., 2011; Ratinov et al., 2011; Cheng and Roth, 2013; Ceccarelli et al., 2013; Ling et al., 2015) considers the KB as a knowledge graph and models coherence based on the overlapped neighbors of two entities in the knowledge graph. These approaches heavily rely on explicit connections among entities in the knowledge graph and thus cannot capture the coherence between two entities that are implicitly connected. For example, two entities en/Mosquito and en/Cockroach only have very few overlapped neighbors in the knowledge graph, but they usually appear together and have similar contexts in text. Using CL"
D19-6107,D13-1184,0,0.0273206,"r text m and Ae,m is a subset that links to entity e. Entity Type (Ling et al., 2015): P p(e|m) , where e 7→ t indicates that t is one of e’s e7→t p(e|m) entity types. Conditional probability p(e|m) can be estimated by Fprob (e|m). Fprob (e|m) Ftype (e|m, t) |A | Table 1: Mention disambiguation features. Coherence is driven by the assumption that if multiple mentions appear together within a context window, their referent entities are more likely to be strongly connected to each other in the KB. Previous work (Cucerzan, 2007; Milne and Witten, 2008; Hoffart et al., 2011; Ratinov et al., 2011; Cheng and Roth, 2013; Ceccarelli et al., 2013; Ling et al., 2015) considers the KB as a knowledge graph and models coherence based on the overlapped neighbors of two entities in the knowledge graph. These approaches heavily rely on explicit connections among entities in the knowledge graph and thus cannot capture the coherence between two entities that are implicitly connected. For example, two entities en/Mosquito and en/Cockroach only have very few overlapped neighbors in the knowledge graph, but they usually appear together and have similar contexts in text. Using CLEW embedding Z, the coherence score can be e"
D19-6107,C12-1089,0,0.0349723,"ment between entity mentions which are often out-of-vocabulary and ambiguous and thus receive poor alignment quality from previous methods. 2016; Sennrich et al., 2016; Yang et al., 2017), and bi-lingual word embedding (Madhyastha and España-Bonet, 2017). Recent attempts have shown that it is possible to derive cross-lingual word embedding from unaligned corpora in an unsupervised fashion (Zhang et al., 2017; Conneau et al., 2017; Artetxe et al., 2018). Another strategy for cross-lingual word embedding learning is to combine monolingual and cross-lingual training objectives (Zou et al., 2013; Klementiev et al., 2012; Luong et al., 2015; Ammar et al., 2016; Vuli´c et al., 2017). Compared to our direct mapping approach, these methods generally require large size of parallel data. Our work is largely inspired from (Conneau et al., 2017). However, our work focuses on better representing entities, which are fundamentally different from common words or phrases in many aspects as described in Section 1. Previous multilingual word embedding efforts including (Conneau et al., 2017) do not explicitly handle entity representations. Moreover, we perform comprehensive extrinsic evaluations based on down-stream NLP ap"
D19-6107,P15-1027,0,0.0565409,"Missing"
D19-6107,P11-1138,0,0.0513717,"nchor links with anchor text m and Ae,m is a subset that links to entity e. Entity Type (Ling et al., 2015): P p(e|m) , where e 7→ t indicates that t is one of e’s e7→t p(e|m) entity types. Conditional probability p(e|m) can be estimated by Fprob (e|m). Fprob (e|m) Ftype (e|m, t) |A | Table 1: Mention disambiguation features. Coherence is driven by the assumption that if multiple mentions appear together within a context window, their referent entities are more likely to be strongly connected to each other in the KB. Previous work (Cucerzan, 2007; Milne and Witten, 2008; Hoffart et al., 2011; Ratinov et al., 2011; Cheng and Roth, 2013; Ceccarelli et al., 2013; Ling et al., 2015) considers the KB as a knowledge graph and models coherence based on the overlapped neighbors of two entities in the knowledge graph. These approaches heavily rely on explicit connections among entities in the knowledge graph and thus cannot capture the coherence between two entities that are implicitly connected. For example, two entities en/Mosquito and en/Cockroach only have very few overlapped neighbors in the knowledge graph, but they usually appear together and have similar contexts in text. Using CLEW embedding Z, the co"
D19-6107,C16-1030,0,0.0342359,"Missing"
D19-6107,Q15-1023,0,0.0216008,"= 5, 10, 50. Following this work, we set K = 10. 3 Unsupervised Cross-lingual Entity Linking Downstream Applications We apply CLEW to enhance two important downstream tasks: Cross-lingual Entity Linking and Parallel Sentence Mining. Ftxt (e) = cos(vm , ze ) = 59 vm · z e kvm k kze k Feature Description Fprior (e) e,∗ Entity Prior: |A∗,∗ |, where Ae,∗ is a set of anchor links that link to entity e and A∗,∗ is all anchor links in the KB |Ae,m | Mention to Entity Probability: |A∗,m |, where A∗,m is a set of anchor links with anchor text m and Ae,m is a subset that links to entity e. Entity Type (Ling et al., 2015): P p(e|m) , where e 7→ t indicates that t is one of e’s e7→t p(e|m) entity types. Conditional probability p(e|m) can be estimated by Fprob (e|m). Fprob (e|m) Ftype (e|m, t) |A | Table 1: Mention disambiguation features. Coherence is driven by the assumption that if multiple mentions appear together within a context window, their referent entities are more likely to be strongly connected to each other in the KB. Previous work (Cucerzan, 2007; Milne and Witten, 2008; Hoffart et al., 2011; Ratinov et al., 2011; Cheng and Roth, 2013; Ceccarelli et al., 2013; Ling et al., 2015) considers the KB as"
D19-6107,P16-1100,0,0.020695,"-lingual Word Embedding Learning. Mikolov et al. (2013b) first notice that word embedding spaces have similar geometric arrangements across languages. They use this property to learn a linear mapping between two spaces. After that, several methods attempt to improve the mapping (Faruqui and Dyer, 2014; Xing et al., 2015; Lazaridou et al., 2015; Ammar et al., 2016; Artetxe et al., 2017; Smith et al., 2017). The measures used to compute similarity between a foreign word and an English word often include distributed monolingual representations on character-level (Costa-jussà and Fonollosa, 2016; Luong and Manning, 2016), subwordlevel (Anwarus Salam et al., 2012; Rei et al., Table 4: Examples of mined parallel sentences from Wikipedia. A portion of alignments are highlighted using the same colors. We randomly select 100 mined parallel sentence pairs for each of 3 language pairs, and ask linguistic experts to judge the quality of these sentence pairs (perfect, partial, or not parallel). The results are shown in Table 5. We can see that the quality of mined parallel sentence is promising and the quality of word and entity alignment is decent. Furthermore, we evaluate the quality of mined parallel sentences extr"
D19-6107,P18-2037,0,0.0256022,"and entity alignment is decent. Furthermore, we evaluate the quality of mined parallel sentences extrinsically using a neural machine translation (NMT) model. We use the 5 https://github.com/tensorflow/ tensor2tensor 62 Parallel Sentence Mining. Automatic mining parallel sentences from comparable documents is an important and useful task to improve Statistical Machine Translation. Early efforts mainly exploited bilingual word dictionaries for bootstrapping (Fung and Cheung, 2004). Recent approaches are mainly based on bilingual word embeddings (Marie and Fujita, 2017) and sentence embeddings (Schwenk, 2018) to detect sentence pairs or continuous parallel segments (Hangya and Fraser, 2019). To the best of our knowledge, this is the first work to incorporate joint entity and word embedding into parallel sentence mining. As a result the sentence pairs we include reliable alignment between entity mentions which are often out-of-vocabulary and ambiguous and thus receive poor alignment quality from previous methods. 2016; Sennrich et al., 2016; Yang et al., 2017), and bi-lingual word embedding (Madhyastha and España-Bonet, 2017). Recent attempts have shown that it is possible to derive cross-lingual w"
D19-6107,W15-1521,0,0.0248712,"ons which are often out-of-vocabulary and ambiguous and thus receive poor alignment quality from previous methods. 2016; Sennrich et al., 2016; Yang et al., 2017), and bi-lingual word embedding (Madhyastha and España-Bonet, 2017). Recent attempts have shown that it is possible to derive cross-lingual word embedding from unaligned corpora in an unsupervised fashion (Zhang et al., 2017; Conneau et al., 2017; Artetxe et al., 2018). Another strategy for cross-lingual word embedding learning is to combine monolingual and cross-lingual training objectives (Zou et al., 2013; Klementiev et al., 2012; Luong et al., 2015; Ammar et al., 2016; Vuli´c et al., 2017). Compared to our direct mapping approach, these methods generally require large size of parallel data. Our work is largely inspired from (Conneau et al., 2017). However, our work focuses on better representing entities, which are fundamentally different from common words or phrases in many aspects as described in Section 1. Previous multilingual word embedding efforts including (Conneau et al., 2017) do not explicitly handle entity representations. Moreover, we perform comprehensive extrinsic evaluations based on down-stream NLP applications including"
D19-6107,P16-1162,0,0.0101754,"ctionaries for bootstrapping (Fung and Cheung, 2004). Recent approaches are mainly based on bilingual word embeddings (Marie and Fujita, 2017) and sentence embeddings (Schwenk, 2018) to detect sentence pairs or continuous parallel segments (Hangya and Fraser, 2019). To the best of our knowledge, this is the first work to incorporate joint entity and word embedding into parallel sentence mining. As a result the sentence pairs we include reliable alignment between entity mentions which are often out-of-vocabulary and ambiguous and thus receive poor alignment quality from previous methods. 2016; Sennrich et al., 2016; Yang et al., 2017), and bi-lingual word embedding (Madhyastha and España-Bonet, 2017). Recent attempts have shown that it is possible to derive cross-lingual word embedding from unaligned corpora in an unsupervised fashion (Zhang et al., 2017; Conneau et al., 2017; Artetxe et al., 2018). Another strategy for cross-lingual word embedding learning is to combine monolingual and cross-lingual training objectives (Zou et al., 2013; Klementiev et al., 2012; Luong et al., 2015; Ammar et al., 2016; Vuli´c et al., 2017). Compared to our direct mapping approach, these methods generally require large s"
D19-6107,W17-2617,0,0.0388491,"Missing"
D19-6107,P17-2062,0,0.0219451,"el sentence is promising and the quality of word and entity alignment is decent. Furthermore, we evaluate the quality of mined parallel sentences extrinsically using a neural machine translation (NMT) model. We use the 5 https://github.com/tensorflow/ tensor2tensor 62 Parallel Sentence Mining. Automatic mining parallel sentences from comparable documents is an important and useful task to improve Statistical Machine Translation. Early efforts mainly exploited bilingual word dictionaries for bootstrapping (Fung and Cheung, 2004). Recent approaches are mainly based on bilingual word embeddings (Marie and Fujita, 2017) and sentence embeddings (Schwenk, 2018) to detect sentence pairs or continuous parallel segments (Hangya and Fraser, 2019). To the best of our knowledge, this is the first work to incorporate joint entity and word embedding into parallel sentence mining. As a result the sentence pairs we include reliable alignment between entity mentions which are often out-of-vocabulary and ambiguous and thus receive poor alignment quality from previous methods. 2016; Sennrich et al., 2016; Yang et al., 2017), and bi-lingual word embedding (Madhyastha and España-Bonet, 2017). Recent attempts have shown that"
D19-6107,N16-1072,0,0.0734668,"ntences and extract many parallel sentence pairs. The novel contributions of this paper are: Traditional approaches to derive training data from Wikipedia usually replace each anchor link with its anchor text, for example, “apple is a technology company.”. These methods have two limitations: (1) Information loss: For example, the anchor text “apple” itself does not convey information such as the entity is a company; (2) Ambiguity (Faruqui et al., 2016): For example, the fruit sense and the company sense of “apple” mistakenly share one surface form. Similar to previous work (Wang et al., 2014; Tsai and Roth, 2016; Yamada et al., 2016), we replace each anchor link with its corresponding entity title, and thus treat each entity title as a unique word. For example, “en/Apple_Inc. is a technology company.”. Using this kind of data mix of entity titles and contextual words, we can learn joint embedding of entities and words. pear juice apple microsoft computer company ibm steve macintosh jobs • We develop a novel approach based on rich anchor links in Wikipedia to learn crosslingual joint entity and word embedding, so that entity mentions across multiple languages are disambiguated and grounded into one un"
D19-6107,D17-1270,0,0.0491752,"Missing"
D19-6107,D14-1167,0,0.0273262,"ace to represent sentences and extract many parallel sentence pairs. The novel contributions of this paper are: Traditional approaches to derive training data from Wikipedia usually replace each anchor link with its anchor text, for example, “apple is a technology company.”. These methods have two limitations: (1) Information loss: For example, the anchor text “apple” itself does not convey information such as the entity is a company; (2) Ambiguity (Faruqui et al., 2016): For example, the fruit sense and the company sense of “apple” mistakenly share one surface form. Similar to previous work (Wang et al., 2014; Tsai and Roth, 2016; Yamada et al., 2016), we replace each anchor link with its corresponding entity title, and thus treat each entity title as a unique word. For example, “en/Apple_Inc. is a technology company.”. Using this kind of data mix of entity titles and contextual words, we can learn joint embedding of entities and words. pear juice apple microsoft computer company ibm steve macintosh jobs • We develop a novel approach based on rich anchor links in Wikipedia to learn crosslingual joint entity and word embedding, so that entity mentions across multiple languages are disambiguated and"
D19-6107,N15-1104,0,0.0600246,"Missing"
D19-6107,K16-1025,0,0.0200526,"any parallel sentence pairs. The novel contributions of this paper are: Traditional approaches to derive training data from Wikipedia usually replace each anchor link with its anchor text, for example, “apple is a technology company.”. These methods have two limitations: (1) Information loss: For example, the anchor text “apple” itself does not convey information such as the entity is a company; (2) Ambiguity (Faruqui et al., 2016): For example, the fruit sense and the company sense of “apple” mistakenly share one surface form. Similar to previous work (Wang et al., 2014; Tsai and Roth, 2016; Yamada et al., 2016), we replace each anchor link with its corresponding entity title, and thus treat each entity title as a unique word. For example, “en/Apple_Inc. is a technology company.”. Using this kind of data mix of entity titles and contextual words, we can learn joint embedding of entities and words. pear juice apple microsoft computer company ibm steve macintosh jobs • We develop a novel approach based on rich anchor links in Wikipedia to learn crosslingual joint entity and word embedding, so that entity mentions across multiple languages are disambiguated and grounded into one unified common space. wo"
D19-6107,D17-1150,0,0.0181583,"pping (Fung and Cheung, 2004). Recent approaches are mainly based on bilingual word embeddings (Marie and Fujita, 2017) and sentence embeddings (Schwenk, 2018) to detect sentence pairs or continuous parallel segments (Hangya and Fraser, 2019). To the best of our knowledge, this is the first work to incorporate joint entity and word embedding into parallel sentence mining. As a result the sentence pairs we include reliable alignment between entity mentions which are often out-of-vocabulary and ambiguous and thus receive poor alignment quality from previous methods. 2016; Sennrich et al., 2016; Yang et al., 2017), and bi-lingual word embedding (Madhyastha and España-Bonet, 2017). Recent attempts have shown that it is possible to derive cross-lingual word embedding from unaligned corpora in an unsupervised fashion (Zhang et al., 2017; Conneau et al., 2017; Artetxe et al., 2018). Another strategy for cross-lingual word embedding learning is to combine monolingual and cross-lingual training objectives (Zou et al., 2013; Klementiev et al., 2012; Luong et al., 2015; Ammar et al., 2016; Vuli´c et al., 2017). Compared to our direct mapping approach, these methods generally require large size of parallel data"
D19-6107,P17-1179,0,0.0157094,"and Fraser, 2019). To the best of our knowledge, this is the first work to incorporate joint entity and word embedding into parallel sentence mining. As a result the sentence pairs we include reliable alignment between entity mentions which are often out-of-vocabulary and ambiguous and thus receive poor alignment quality from previous methods. 2016; Sennrich et al., 2016; Yang et al., 2017), and bi-lingual word embedding (Madhyastha and España-Bonet, 2017). Recent attempts have shown that it is possible to derive cross-lingual word embedding from unaligned corpora in an unsupervised fashion (Zhang et al., 2017; Conneau et al., 2017; Artetxe et al., 2018). Another strategy for cross-lingual word embedding learning is to combine monolingual and cross-lingual training objectives (Zou et al., 2013; Klementiev et al., 2012; Luong et al., 2015; Ammar et al., 2016; Vuli´c et al., 2017). Compared to our direct mapping approach, these methods generally require large size of parallel data. Our work is largely inspired from (Conneau et al., 2017). However, our work focuses on better representing entities, which are fundamentally different from common words or phrases in many aspects as described in Section 1."
D19-6107,D13-1141,0,0.0414272,"ude reliable alignment between entity mentions which are often out-of-vocabulary and ambiguous and thus receive poor alignment quality from previous methods. 2016; Sennrich et al., 2016; Yang et al., 2017), and bi-lingual word embedding (Madhyastha and España-Bonet, 2017). Recent attempts have shown that it is possible to derive cross-lingual word embedding from unaligned corpora in an unsupervised fashion (Zhang et al., 2017; Conneau et al., 2017; Artetxe et al., 2018). Another strategy for cross-lingual word embedding learning is to combine monolingual and cross-lingual training objectives (Zou et al., 2013; Klementiev et al., 2012; Luong et al., 2015; Ammar et al., 2016; Vuli´c et al., 2017). Compared to our direct mapping approach, these methods generally require large size of parallel data. Our work is largely inspired from (Conneau et al., 2017). However, our work focuses on better representing entities, which are fundamentally different from common words or phrases in many aspects as described in Section 1. Previous multilingual word embedding efforts including (Conneau et al., 2017) do not explicitly handle entity representations. Moreover, we perform comprehensive extrinsic evaluations ba"
J08-3004,J00-1004,0,0.197688,"Missing"
J08-3004,C00-1007,0,0.0182227,"conversion to name transliteration. However, language problems like machine translation break this mold, because they involve massive re-ordering of symbols, and because the transformation processes seem sensitive to hierarchical tree structure. Recently, speciﬁc probabilistic tree-based models have been proposed not only for machine translation (Wu 1997; Alshawi, Bangalore, and Douglas 2000; Yamada and Knight 2001; Eisner 2003; Gildea 2003), but also for summarization (Knight and Marcu 2002), paraphrasing (Pang, Knight, and Marcu 2003), natural language generation (Langkilde and Knight 1998; Bangalore and Rambow 2000; Corston-Oliver et al. 2002), parsing, and language modeling (Baker 1979; Lari and Young 1990; Collins 1997; Chelba and Jelinek 2000; Charniak 2001; Klein ∗ ∗∗ † 1 Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: graehl@isi.edu. Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: knight@isi.edu. Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: jonmay@isi.edu. www.research.att.com/sw/tools/fsm and www.isi.edu/licensed-sw/carmel. Submission received: 30 October 2003; revised submission re"
J08-3004,P01-1017,0,0.0342892,"d because the transformation processes seem sensitive to hierarchical tree structure. Recently, speciﬁc probabilistic tree-based models have been proposed not only for machine translation (Wu 1997; Alshawi, Bangalore, and Douglas 2000; Yamada and Knight 2001; Eisner 2003; Gildea 2003), but also for summarization (Knight and Marcu 2002), paraphrasing (Pang, Knight, and Marcu 2003), natural language generation (Langkilde and Knight 1998; Bangalore and Rambow 2000; Corston-Oliver et al. 2002), parsing, and language modeling (Baker 1979; Lari and Young 1990; Collins 1997; Chelba and Jelinek 2000; Charniak 2001; Klein ∗ ∗∗ † 1 Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: graehl@isi.edu. Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: knight@isi.edu. Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: jonmay@isi.edu. www.research.att.com/sw/tools/fsm and www.isi.edu/licensed-sw/carmel. Submission received: 30 October 2003; revised submission received: 30 August 2007; accepted for publication: 20 October 2007. © 2008 Association for Computational Linguistics Computational Linguistics Volume"
J08-3004,P97-1003,0,0.209903,"olve massive re-ordering of symbols, and because the transformation processes seem sensitive to hierarchical tree structure. Recently, speciﬁc probabilistic tree-based models have been proposed not only for machine translation (Wu 1997; Alshawi, Bangalore, and Douglas 2000; Yamada and Knight 2001; Eisner 2003; Gildea 2003), but also for summarization (Knight and Marcu 2002), paraphrasing (Pang, Knight, and Marcu 2003), natural language generation (Langkilde and Knight 1998; Bangalore and Rambow 2000; Corston-Oliver et al. 2002), parsing, and language modeling (Baker 1979; Lari and Young 1990; Collins 1997; Chelba and Jelinek 2000; Charniak 2001; Klein ∗ ∗∗ † 1 Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: graehl@isi.edu. Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: knight@isi.edu. Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: jonmay@isi.edu. www.research.att.com/sw/tools/fsm and www.isi.edu/licensed-sw/carmel. Submission received: 30 October 2003; revised submission received: 30 August 2007; accepted for publication: 20 October 2007. © 2008 Association for Computational Ling"
J08-3004,W02-2105,0,0.00505275,"eration. However, language problems like machine translation break this mold, because they involve massive re-ordering of symbols, and because the transformation processes seem sensitive to hierarchical tree structure. Recently, speciﬁc probabilistic tree-based models have been proposed not only for machine translation (Wu 1997; Alshawi, Bangalore, and Douglas 2000; Yamada and Knight 2001; Eisner 2003; Gildea 2003), but also for summarization (Knight and Marcu 2002), paraphrasing (Pang, Knight, and Marcu 2003), natural language generation (Langkilde and Knight 1998; Bangalore and Rambow 2000; Corston-Oliver et al. 2002), parsing, and language modeling (Baker 1979; Lari and Young 1990; Collins 1997; Chelba and Jelinek 2000; Charniak 2001; Klein ∗ ∗∗ † 1 Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: graehl@isi.edu. Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: knight@isi.edu. Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: jonmay@isi.edu. www.research.att.com/sw/tools/fsm and www.isi.edu/licensed-sw/carmel. Submission received: 30 October 2003; revised submission received: 30 August 2007; accep"
J08-3004,P79-1022,0,0.810927,"simply the ﬁrst projection). For every weighted context-free grammar, there is an equivalent wRTG that generates its weighted derivation trees (whose yield is a string in the context-free language), and the yield of any regular tree language is a context-free string language (Gécseg and Steinby 1984). We can also interpret a regular tree grammar as a context-free string grammar with alphabet Σ ∪ {(, )}. wRTGs generate (ignoring weights) exactly the recognizable tree languages, which are sets of trees accepted by a non-transducing automaton version of T. This acceptor automaton is described in Doner (1970) and is actually a closer mechanical analogue to an FSA than is the rewrite-rule-based wRTG. RTGs are closed under intersection (Gécseg and Steinby 1984), and the constructive proof also applies to weighted wRTG intersection. There is a normal form for wRTGs analogous to that of regular grammars: Right-hand sides are a single terminal root with (optional) nonterminal children. What is sometimes called a forest in natural language generation (Langkilde 2000; Nederhof and Satta 2002) is a ﬁnite wRTG without loops—for all valid derivation trees, each nonterminal may only occur once in any path fr"
J08-3004,P02-1001,0,0.0718822,"Missing"
J08-3004,P03-2041,0,0.890805,"tware toolkits like the AT&T FSM Library and USC/ISI’s Carmel.1 Moreover, a surprising variety of problems are attackable with FSTs, from part-of-speech tagging to letter-to-sound conversion to name transliteration. However, language problems like machine translation break this mold, because they involve massive re-ordering of symbols, and because the transformation processes seem sensitive to hierarchical tree structure. Recently, speciﬁc probabilistic tree-based models have been proposed not only for machine translation (Wu 1997; Alshawi, Bangalore, and Douglas 2000; Yamada and Knight 2001; Eisner 2003; Gildea 2003), but also for summarization (Knight and Marcu 2002), paraphrasing (Pang, Knight, and Marcu 2003), natural language generation (Langkilde and Knight 1998; Bangalore and Rambow 2000; Corston-Oliver et al. 2002), parsing, and language modeling (Baker 1979; Lari and Young 1990; Collins 1997; Chelba and Jelinek 2000; Charniak 2001; Klein ∗ ∗∗ † 1 Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: graehl@isi.edu. Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: knight@isi.edu. Information Sciences Institute, 4676"
J08-3004,P03-1011,0,0.0923722,"s like the AT&T FSM Library and USC/ISI’s Carmel.1 Moreover, a surprising variety of problems are attackable with FSTs, from part-of-speech tagging to letter-to-sound conversion to name transliteration. However, language problems like machine translation break this mold, because they involve massive re-ordering of symbols, and because the transformation processes seem sensitive to hierarchical tree structure. Recently, speciﬁc probabilistic tree-based models have been proposed not only for machine translation (Wu 1997; Alshawi, Bangalore, and Douglas 2000; Yamada and Knight 2001; Eisner 2003; Gildea 2003), but also for summarization (Knight and Marcu 2002), paraphrasing (Pang, Knight, and Marcu 2003), natural language generation (Langkilde and Knight 1998; Bangalore and Rambow 2000; Corston-Oliver et al. 2002), parsing, and language modeling (Baker 1979; Lari and Young 1990; Collins 1997; Chelba and Jelinek 2000; Charniak 2001; Klein ∗ ∗∗ † 1 Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: graehl@isi.edu. Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: knight@isi.edu. Information Sciences Institute, 4676 Admiralty Way,"
J08-3004,N04-1014,1,0.39167,"Missing"
J08-3004,P03-1054,0,0.0551946,"Missing"
J08-3004,knight-al-onaizan-1998-translation,1,0.794918,"Missing"
J08-3004,N03-1017,0,0.11059,"Missing"
J08-3004,N03-1019,0,0.035142,"Missing"
J08-3004,A00-2023,0,0.0140971,"nizable tree languages, which are sets of trees accepted by a non-transducing automaton version of T. This acceptor automaton is described in Doner (1970) and is actually a closer mechanical analogue to an FSA than is the rewrite-rule-based wRTG. RTGs are closed under intersection (Gécseg and Steinby 1984), and the constructive proof also applies to weighted wRTG intersection. There is a normal form for wRTGs analogous to that of regular grammars: Right-hand sides are a single terminal root with (optional) nonterminal children. What is sometimes called a forest in natural language generation (Langkilde 2000; Nederhof and Satta 2002) is a ﬁnite wRTG without loops—for all valid derivation trees, each nonterminal may only occur once in any path from root to a leaf: ∀n ∈ N, t ∈ TΣ (N), h ∈ (paths × P)∗ : (n, ()) ⇒∗G (t, h) =⇒ pathst ({n} ) = ∅ RTGs produce tree sets equivalent to those produced by tree substitution grammars (TSGs) (Schabes 1990) up to relabeling. The relabeling is necessary because RTGs distinguish states and tree symbols, which are conﬂated in TSGs at the elementary tree root. Regular tree languages are strictly contained in tree sets of tree adjoining grammars (TAG; Joshi and Scha"
J08-3004,P98-1116,1,0.462175,"tagging to letter-to-sound conversion to name transliteration. However, language problems like machine translation break this mold, because they involve massive re-ordering of symbols, and because the transformation processes seem sensitive to hierarchical tree structure. Recently, speciﬁc probabilistic tree-based models have been proposed not only for machine translation (Wu 1997; Alshawi, Bangalore, and Douglas 2000; Yamada and Knight 2001; Eisner 2003; Gildea 2003), but also for summarization (Knight and Marcu 2002), paraphrasing (Pang, Knight, and Marcu 2003), natural language generation (Langkilde and Knight 1998; Bangalore and Rambow 2000; Corston-Oliver et al. 2002), parsing, and language modeling (Baker 1979; Lari and Young 1990; Collins 1997; Chelba and Jelinek 2000; Charniak 2001; Klein ∗ ∗∗ † 1 Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: graehl@isi.edu. Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: knight@isi.edu. Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: jonmay@isi.edu. www.research.att.com/sw/tools/fsm and www.isi.edu/licensed-sw/carmel. Submission received: 30 October"
J08-3004,W02-1018,0,0.0337006,"Missing"
J08-3004,P02-1015,0,0.0176924,"guages, which are sets of trees accepted by a non-transducing automaton version of T. This acceptor automaton is described in Doner (1970) and is actually a closer mechanical analogue to an FSA than is the rewrite-rule-based wRTG. RTGs are closed under intersection (Gécseg and Steinby 1984), and the constructive proof also applies to weighted wRTG intersection. There is a normal form for wRTGs analogous to that of regular grammars: Right-hand sides are a single terminal root with (optional) nonterminal children. What is sometimes called a forest in natural language generation (Langkilde 2000; Nederhof and Satta 2002) is a ﬁnite wRTG without loops—for all valid derivation trees, each nonterminal may only occur once in any path from root to a leaf: ∀n ∈ N, t ∈ TΣ (N), h ∈ (paths × P)∗ : (n, ()) ⇒∗G (t, h) =⇒ pathst ({n} ) = ∅ RTGs produce tree sets equivalent to those produced by tree substitution grammars (TSGs) (Schabes 1990) up to relabeling. The relabeling is necessary because RTGs distinguish states and tree symbols, which are conﬂated in TSGs at the elementary tree root. Regular tree languages are strictly contained in tree sets of tree adjoining grammars (TAG; Joshi and Schabes 1997), which generate"
J08-3004,W99-0604,0,0.296338,"Missing"
J08-3004,N03-1024,1,0.2473,"Missing"
J08-3004,C69-0101,0,0.730689,"ormation Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: knight@isi.edu. Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: jonmay@isi.edu. www.research.att.com/sw/tools/fsm and www.isi.edu/licensed-sw/carmel. Submission received: 30 October 2003; revised submission received: 30 August 2007; accepted for publication: 20 October 2007. © 2008 Association for Computational Linguistics Computational Linguistics Volume 34, Number 3 and Manning 2003). It is useful to understand generic algorithms that may support all these tasks and more. Rounds (1970) and Thatcher (1970) independently introduced tree transducers as a generalization of FSTs. Rounds was motivated by natural language: Recent developments in the theory of automata have pointed to an extension of the domain of deﬁnition of automata from strings to trees . . . parts of mathematical linguistics can be formalized easily in a tree-automaton setting . . . We investigate decision problems and closure properties. Our results should clarify the nature of syntax-directed translations and transformational grammars . . . (Rounds 1970) The Rounds/Thatcher tree transducer is very similar to"
J08-3004,J97-3002,0,0.0722114,"67) and forward–backward training (Baum and Eagon 1967), as well as software toolkits like the AT&T FSM Library and USC/ISI’s Carmel.1 Moreover, a surprising variety of problems are attackable with FSTs, from part-of-speech tagging to letter-to-sound conversion to name transliteration. However, language problems like machine translation break this mold, because they involve massive re-ordering of symbols, and because the transformation processes seem sensitive to hierarchical tree structure. Recently, speciﬁc probabilistic tree-based models have been proposed not only for machine translation (Wu 1997; Alshawi, Bangalore, and Douglas 2000; Yamada and Knight 2001; Eisner 2003; Gildea 2003), but also for summarization (Knight and Marcu 2002), paraphrasing (Pang, Knight, and Marcu 2003), natural language generation (Langkilde and Knight 1998; Bangalore and Rambow 2000; Corston-Oliver et al. 2002), parsing, and language modeling (Baker 1979; Lari and Young 1990; Collins 1997; Chelba and Jelinek 2000; Charniak 2001; Klein ∗ ∗∗ † 1 Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: graehl@isi.edu. Information Sciences Institute, 4676 Admiralty Way, Marina del R"
J08-3004,P01-1067,1,0.0800498,"on 1967), as well as software toolkits like the AT&T FSM Library and USC/ISI’s Carmel.1 Moreover, a surprising variety of problems are attackable with FSTs, from part-of-speech tagging to letter-to-sound conversion to name transliteration. However, language problems like machine translation break this mold, because they involve massive re-ordering of symbols, and because the transformation processes seem sensitive to hierarchical tree structure. Recently, speciﬁc probabilistic tree-based models have been proposed not only for machine translation (Wu 1997; Alshawi, Bangalore, and Douglas 2000; Yamada and Knight 2001; Eisner 2003; Gildea 2003), but also for summarization (Knight and Marcu 2002), paraphrasing (Pang, Knight, and Marcu 2003), natural language generation (Langkilde and Knight 1998; Bangalore and Rambow 2000; Corston-Oliver et al. 2002), parsing, and language modeling (Baker 1979; Lari and Young 1990; Collins 1997; Chelba and Jelinek 2000; Charniak 2001; Klein ∗ ∗∗ † 1 Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: graehl@isi.edu. Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: knight@isi.edu. Information Sciences Ins"
J08-3004,P02-1039,1,0.67228,"Missing"
J08-3004,J91-1004,0,\N,Missing
J08-3004,J97-4005,0,\N,Missing
J08-3004,1987.mtsummit-1.11,0,\N,Missing
J08-3004,C98-1112,1,\N,Missing
J08-3004,P96-1029,0,\N,Missing
J10-2004,P98-1006,0,0.0838333,"Missing"
J10-2004,J93-2003,0,0.0129373,"Missing"
J10-2004,A00-2018,0,0.126804,"yle trees. One striking fact about these trees is that they contain many ﬂat structures. For example, base noun phrases frequently have ﬁve or more direct children. It is well known in monolingual parsing research that these ﬂat structures cause problems. Although thousands of rewrite rules can be learned from the Penn Treebank, these rules still do not cover the new rewrites observed in held-out test data. For this reason, and to extract more general knowledge, many monolingual parsing models are markovized so that they can produce ﬂat structures incrementally and horizontally (Collins 1997; Charniak 2000). Other parsing systems binarize the training trees in a pre-processing step, then learn to model the binarized corpus (Petrov et al. 2006); after parsing, their results are ﬂattened back in a post-processing step. In addition, Johnson (1998b) shows that different types of tree structuring (e.g., the Chomsky adjunction representation vs. the Penn Treebank II representation) can have a large effect on the parsing performance of a PCFG estimated from these trees. We ﬁnd that ﬂat structures are also problematic for syntax-based machine translation. The rules we learn from tree/string/alignment tr"
J10-2004,J07-2003,0,0.240062,"putational Linguistics Volume 36, Number 2 Figure 3 Additional rules extracted from the learning case in Figure 1. store English words that appear at the left and right corners of the tree, as these are needed for computing the P(e) score when cells are combined. For CKY to work, all transducer rules must be broken down, or binarized, into rules that contain at most two variables—more efﬁcient search can be gained if this binarization produces rules that can be incrementally scored by the language model (Melamed, Satta, and Wellington 2004; Zhang et al. 2006). Finally, we employ cube pruning (Chiang 2007) for further efﬁciency in the search. When scoring translation candidates, we add several smaller models. One model rewards longer translation candidates, off-setting the language model’s desire for short output. Other models punish rules that drop Chinese content words or introduce spurious English content words. We also include lexical smoothing models (Gale and Sampson 1996; Good 1953) to help distinguish good low-count rules from bad lowcount rules. The ﬁnal score of a translation candidate is a weighted linear combination of log P(e), log P(e, c), and the scores from these additional smal"
J10-2004,D09-1037,0,0.100934,"Missing"
J10-2004,P97-1003,0,0.0914444,"ng, but in this work we concentrate only on target-language syntax. The target-language generation problem presents a difﬁcult challenge, whereas the source sentence is ﬁxed and usually already grammatical. To prepare training data for such a system, we begin with a bilingual text that has been automatically processed into segment pairs. We require that the segments be single sentences on the English side, whereas the corresponding Chinese segments may be sentences, sentence fragments, or multiple sentences. We then parse the English side of the bilingual text using a re-implementation of the Collins (1997) parsing model, which we train on the Penn English Treebank (Marcus, Santorini, and Marcinkiewicz 1993). Finally, we word-align the segment pairs according to IBM Model 4 (Brown et al. 1993). Figure 1 shows a sample (tree, string, alignment) triple. We build two generative statistical models from this data. First, we construct a smoothed n-gram language model (Kneser and Ney 1995; Stolcke 2002) out of the English side of the bilingual data. This model assigns a probability P(e) to any candidate translation, rewarding translations whose subsequences have been observed frequently in the training"
J10-2004,D07-1079,1,0.88608,"traction method assigns each unaligned Chinese word to a default rule in the derivation tree. We next follow Galley et al. (2006) in allowing unaligned Chinese words to participate in multiple translation rules. In this case, we obtain a derivation forest of minimal rules. Galley et al. show how to use EM to count rules over derivation forests and obtain Viterbi derivation trees of minimal rules. We also follow Galley et al. in collecting composed rules, namely, compositions of minimal rules. These larger rules have been shown to substantially improve translation accuracy (Galley et al. 2006; DeNeefe et al. 2007). Figure 3 shows some of the additional rules. With these models, we can decode a new Chinese sentence by enumerating and scoring all of the English trees that can be derived from it by rule. The score is a weighted product of P(e) and P(e, c). To search efﬁciently, we employ the CKY dynamicprogramming parsing algorithm (Yamada and Knight 2002; Galley et al. 2006). This algorithm builds English trees on top of Chinese spans. In each cell of the CKY matrix, we store the non-terminal symbol at the root of the English tree being built up. We also Figure 2 Minimal rules extracted from the learning"
J10-2004,P06-1121,1,0.821544,"and deletion (R2, R4), and tree traversal (R9, R7). The extracted rules for a given example form a derivation tree. We collect all rules over the entire bilingual corpus, and we normalize rule counts count(rule) in this way: P(rule) = count(LHS -root(rule)) . When we apply these probabilities to derive an English sentence e and a corresponding Chinese sentence c, we wind up computing the joint probability P(e, c). We smooth the rule counts with Good–Turing smoothing (Good 1953). This extraction method assigns each unaligned Chinese word to a default rule in the derivation tree. We next follow Galley et al. (2006) in allowing unaligned Chinese words to participate in multiple translation rules. In this case, we obtain a derivation forest of minimal rules. Galley et al. show how to use EM to count rules over derivation forests and obtain Viterbi derivation trees of minimal rules. We also follow Galley et al. in collecting composed rules, namely, compositions of minimal rules. These larger rules have been shown to substantially improve translation accuracy (Galley et al. 2006; DeNeefe et al. 2007). Figure 3 shows some of the additional rules. With these models, we can decode a new Chinese sentence by enu"
J10-2004,N04-1035,1,0.876041,"re 1 A sample learning case for the syntax-based machine translation system described in this article. 248 Wang et al. Re-structuring, Re-labeling, and Re-aligning SMT (Brown et al. 1993), our model operates in the English-to-Chinese direction— we envision a generative top–down process by which an English tree is gradually transformed (by probabilistic rules) into an observed Chinese string. We represent a collection of such rules as a tree transducer (Knight and Graehl 2005). In order to construct this transducer from parsed and word-aligned data, we use the GHKM rule extraction algorithm of Galley et al. (2004). This algorithm computes the unique set of minimal rules needed to explain any sentence pair in the data. Figure 2 shows all the minimal rules extracted from the example (tree, string, alignment) triple in Figure 1. Note that rules specify rotation (e.g., R1, R5), direct translation (R3, R10), insertion and deletion (R2, R4), and tree traversal (R9, R7). The extracted rules for a given example form a derivation tree. We collect all rules over the entire bilingual corpus, and we normalize rule counts count(rule) in this way: P(rule) = count(LHS -root(rule)) . When we apply these probabilities"
J10-2004,J99-4004,0,0.0216439,"ubstructure that yields NNP2 , NNP3 , and their translational equivalences. To obtain more factorizable sub-phrases, we need to parallel-binarize in both directions. 254 Wang et al. Re-structuring, Re-labeling, and Re-aligning Figure 7 Packed forest obtained by packing trees (3) and (6) in Figure 6. 3.2.2 Parallel Binarization. Simple binarizations transform a parse tree into another single parse tree. Parallel binarization transforms a parse tree into a binarization forest, packed to enable dynamic programming when we extract translation rules from it. Borrowing terms from parsing semirings (Goodman 1999), a packed forest is composed of additive forest nodes (⊕-nodes) and multiplicative forest nodes (⊗-nodes). In the binarization forest, a ⊗-node corresponds to a tree node in the unbinarized tree or a new tree node introduced during tree binarization; and this ⊗-node composes several ⊕-nodes, forming a one-level substructure that is observed in the unbinarized tree or in one of its binarized tree. A ⊕-node corresponds to alternative ways of binarizing the same tree node and it contains one or more ⊗-nodes. The same ⊕-node can appear in more than one place in the packed forest, enabling sharing"
J10-2004,N06-1031,1,0.897685,"a translation candidate is a weighted linear combination of log P(e), log P(e, c), and the scores from these additional smaller models. We obtain weights through minimum error-rate training (Och 2003). The system thus constructed performs fairly well at Chinese-to-English translation, as reﬂected in the NIST06 common evaluation of machine translation quality.1 However, it would be surprising if the parse structures and word alignments in our bilingual data were somehow perfectly suited to syntax-based SMT—we have so far used out-of-the-box tools like IBM Model 4 and a Treebank-trained parser. Huang and Knight (2006) already investigated whether different syntactic labels would be more appropriate for SMT, though their study was carried out on a weak baseline translation system. In this article, we take a broad view and investigate how changes to syntactic structures, syntactic labels, and word alignments can lead to substantial improvements in translation quality on top of a strong baseline. We design our methods around problems that arise in MT data whose parses and alignments use some Penn Treebankstyle annotations. We believe that some of the techniques will apply to other annotation schemes, but conc"
J10-2004,J98-4004,0,0.110894,"use problems. Although thousands of rewrite rules can be learned from the Penn Treebank, these rules still do not cover the new rewrites observed in held-out test data. For this reason, and to extract more general knowledge, many monolingual parsing models are markovized so that they can produce ﬂat structures incrementally and horizontally (Collins 1997; Charniak 2000). Other parsing systems binarize the training trees in a pre-processing step, then learn to model the binarized corpus (Petrov et al. 2006); after parsing, their results are ﬂattened back in a post-processing step. In addition, Johnson (1998b) shows that different types of tree structuring (e.g., the Chomsky adjunction representation vs. the Penn Treebank II representation) can have a large effect on the parsing performance of a PCFG estimated from these trees. We ﬁnd that ﬂat structures are also problematic for syntax-based machine translation. The rules we learn from tree/string/alignment triples often lack sufﬁcient generalization power. For example, consider the training samples in Figure 4. We should be able to learn enough from these two samples to translate the new phrase .W #L Z Æ{ 3ä VIKTOR CHERNOMYRDIN AND HIS COLL"
J10-2004,P03-1054,0,0.00631755,"in Figure 11(b) yields ungrammatical translations like he likes reading she does not like reading. Tree binarization enables the reuse of substructures, but causes over-generation of trees at the same time. We solve the coarse-nonterminal problem by reﬁning/re-labeling the training tree labels. Re-labeling is done by enriching the nonterminal label of each tree node based on its context information. Re-labeling has already been used in monolingual parsing research to improve parsing accuracy of PCFGs. We are interested in two types of re-labeling methods: Linguistically motivated re-labeling (Klein and Manning 2003; Johnson 1998b) enriches the labels of parser training trees using parent labels, head word tag labels, and/or sibling labels. Automatic category splitting (Petrov et al. 2006) reﬁnes a nonterminal Figure 10 MT output errors due to coarse Penn Treebank annotations. Oval nodes in (b) are rule overlapping nodes. Subtree (b) is formed by composing the LHSs of R20, R21, and R22. 262 Wang et al. Re-structuring, Re-labeling, and Re-aligning Figure 11 Tree binarization over-generalizes the parse tree. Translation rules R23 and R24 are acquired from binarized training trees, aiming for reuse of subst"
J10-2004,J08-3004,1,0.883026,"Missing"
J10-2004,W04-3250,0,0.0206993,"ecause the NIST08 evaluation set is a mix of newswire text and Web text, we also report the BLEU scores on the newswire portion. We use two 5-gram language models. One is trained on the English half of the bitext. The other is trained on one billion words of monolingual data. Kneser–Ney smoothing (Kneser and Ney 1995) is applied to both language models. Language models are represented using randomized data structures similar to those of Talbot and Osborne (2007) in decoding for efﬁcient RAM usage. To test the signiﬁcance of improvements over the baseline, we compute paired bootstrap p-values (Koehn 2004) for BLEU between the baseline system and each improved system. 3. Re-structuring Trees for Training Our translation system is trained on Chinese/English data, where the English side has been automatically parsed into Penn Treebank-style trees. One striking fact about these trees is that they contain many ﬂat structures. For example, base noun phrases frequently have ﬁve or more direct children. It is well known in monolingual parsing research that these ﬂat structures cause problems. Although thousands of rewrite rules can be learned from the Penn Treebank, these rules still do not cover the"
J10-2004,W02-1018,1,0.803481,"Missing"
J10-2004,J93-2004,0,0.0464715,"Missing"
J10-2004,D07-1038,1,0.870717,"Missing"
J10-2004,P04-1084,0,0.0245554,"Missing"
J10-2004,D08-1022,0,0.0550391,"t rules from the admissible nodes in the packed forest. Rules that can be extracted from the original unrestructured tree can be extracted from the packed forest as well. Parallel binarization results in parse forests. Thus translation rules need to be extracted from training data consisting of (e-forest, f, a)-tuples. 3.3 Extracting Translation Rules from (e-forest, f, a)-tuples Our algorithm to extract rules from (e-forest, f, a)-tuples is a natural generalization of the (e-parse, f, a)-based rule extraction algorithm in Galley et al. (2006). A similar problem is also elegantly addressed in Mi and Huang (2008) in detail. The forest-based rule extraction algorithm takes as input a (e-forest, f, a)-triple, and outputs a derivation forest (Galley et al. 2006), which consists of overlapping translation rules. The algorithm recursively traverses the e-forest top–down, extracts rules only at admissible e-forest 256 Wang et al. Re-structuring, Re-labeling, and Re-aligning nodes, and transforms e-forest nodes into synchronous derivation-forest nodes via the following two procedures, depending on which condition is met. r r Condition 1: If we reach an additive e-forest node, for each of its children, which"
J10-2004,J04-4002,0,0.187296,"Missing"
J10-2004,P03-1021,0,0.0115692,"idates, we add several smaller models. One model rewards longer translation candidates, off-setting the language model’s desire for short output. Other models punish rules that drop Chinese content words or introduce spurious English content words. We also include lexical smoothing models (Gale and Sampson 1996; Good 1953) to help distinguish good low-count rules from bad lowcount rules. The ﬁnal score of a translation candidate is a weighted linear combination of log P(e), log P(e, c), and the scores from these additional smaller models. We obtain weights through minimum error-rate training (Och 2003). The system thus constructed performs fairly well at Chinese-to-English translation, as reﬂected in the NIST06 common evaluation of machine translation quality.1 However, it would be surprising if the parse structures and word alignments in our bilingual data were somehow perfectly suited to syntax-based SMT—we have so far used out-of-the-box tools like IBM Model 4 and a Treebank-trained parser. Huang and Knight (2006) already investigated whether different syntactic labels would be more appropriate for SMT, though their study was carried out on a weak baseline translation system. In this art"
J10-2004,P06-1055,0,0.704636,"e ﬁve or more direct children. It is well known in monolingual parsing research that these ﬂat structures cause problems. Although thousands of rewrite rules can be learned from the Penn Treebank, these rules still do not cover the new rewrites observed in held-out test data. For this reason, and to extract more general knowledge, many monolingual parsing models are markovized so that they can produce ﬂat structures incrementally and horizontally (Collins 1997; Charniak 2000). Other parsing systems binarize the training trees in a pre-processing step, then learn to model the binarized corpus (Petrov et al. 2006); after parsing, their results are ﬂattened back in a post-processing step. In addition, Johnson (1998b) shows that different types of tree structuring (e.g., the Chomsky adjunction representation vs. the Penn Treebank II representation) can have a large effect on the parsing performance of a PCFG estimated from these trees. We ﬁnd that ﬂat structures are also problematic for syntax-based machine translation. The rules we learn from tree/string/alignment triples often lack sufﬁcient generalization power. For example, consider the training samples in Figure 4. We should be able to learn enough"
J10-2004,P07-1065,0,0.0152868,"is from the newswire domain, and we chose it to represent a wide period of time rather than a single year. We use the NIST08 evaluation set as our test set. Because the NIST08 evaluation set is a mix of newswire text and Web text, we also report the BLEU scores on the newswire portion. We use two 5-gram language models. One is trained on the English half of the bitext. The other is trained on one billion words of monolingual data. Kneser–Ney smoothing (Kneser and Ney 1995) is applied to both language models. Language models are represented using randomized data structures similar to those of Talbot and Osborne (2007) in decoding for efﬁcient RAM usage. To test the signiﬁcance of improvements over the baseline, we compute paired bootstrap p-values (Koehn 2004) for BLEU between the baseline system and each improved system. 3. Re-structuring Trees for Training Our translation system is trained on Chinese/English data, where the English side has been automatically parsed into Penn Treebank-style trees. One striking fact about these trees is that they contain many ﬂat structures. For example, base noun phrases frequently have ﬁve or more direct children. It is well known in monolingual parsing research that th"
J10-2004,C96-2141,0,0.0804986,"Missing"
J10-2004,D07-1078,1,0.89473,"Missing"
J10-2004,J97-3002,0,0.829465,"we notice that re-structuring tends to help MT accuracy more than re-labeling. We mentioned earlier that re-structuring overgeneralizes structures, but enables reuse of substructures. Results in Table 5 and Table 1 show substructure reuse mitigates structure over-generalization in our tree restructuring method. 5. Re-aligning (Tree, String) Pairs for Training So far, we have improved the English structures in our parsed, aligned training corpus. We now turn to improving the word alignments. Some MT systems use the same model for alignment and translation—examples include Brown et al. (1993), Wu (1997), Alshawi, Bangalore, and Douglas (1998), Yamada and Knight (2001, 2002), and Cohn and Blunsom (2009). Other systems use Brown et al. for alignment, then collect counts for a completely different model, such as Och and Ney Table 5 Impact of re-labeling methods on MT accuracy as measured by BLEU. Four-way splitting was carried out on EM-binarized trees; thus, it already beneﬁts from tree re-structuring. All p-values are computed against Baseline1. E XPERIMENT NIST08 BLEU p NIST08-NW BLEU p Baseline1 (no re-structuring and no re-labeling) Linguistically motivated re-labeling 29.12 29.57 — 0.029"
J10-2004,P01-1067,1,0.646868,"uracy more than re-labeling. We mentioned earlier that re-structuring overgeneralizes structures, but enables reuse of substructures. Results in Table 5 and Table 1 show substructure reuse mitigates structure over-generalization in our tree restructuring method. 5. Re-aligning (Tree, String) Pairs for Training So far, we have improved the English structures in our parsed, aligned training corpus. We now turn to improving the word alignments. Some MT systems use the same model for alignment and translation—examples include Brown et al. (1993), Wu (1997), Alshawi, Bangalore, and Douglas (1998), Yamada and Knight (2001, 2002), and Cohn and Blunsom (2009). Other systems use Brown et al. for alignment, then collect counts for a completely different model, such as Och and Ney Table 5 Impact of re-labeling methods on MT accuracy as measured by BLEU. Four-way splitting was carried out on EM-binarized trees; thus, it already beneﬁts from tree re-structuring. All p-values are computed against Baseline1. E XPERIMENT NIST08 BLEU p NIST08-NW BLEU p Baseline1 (no re-structuring and no re-labeling) Linguistically motivated re-labeling 29.12 29.57 — 0.029 35.33 35.85 — 0.050 Baseline2 (EM re-structuring but no re-labeli"
J10-2004,P02-1039,1,0.437148,"ts and obtain Viterbi derivation trees of minimal rules. We also follow Galley et al. in collecting composed rules, namely, compositions of minimal rules. These larger rules have been shown to substantially improve translation accuracy (Galley et al. 2006; DeNeefe et al. 2007). Figure 3 shows some of the additional rules. With these models, we can decode a new Chinese sentence by enumerating and scoring all of the English trees that can be derived from it by rule. The score is a weighted product of P(e) and P(e, c). To search efﬁciently, we employ the CKY dynamicprogramming parsing algorithm (Yamada and Knight 2002; Galley et al. 2006). This algorithm builds English trees on top of Chinese spans. In each cell of the CKY matrix, we store the non-terminal symbol at the root of the English tree being built up. We also Figure 2 Minimal rules extracted from the learning case in Figure 1 using the GHKM procedure. 249 Computational Linguistics Volume 36, Number 2 Figure 3 Additional rules extracted from the learning case in Figure 1. store English words that appear at the left and right corners of the tree, as these are needed for computing the P(e) score when cells are combined. For CKY to work, all transduce"
J10-2004,J02-1005,0,\N,Missing
J10-2004,D08-1033,0,\N,Missing
J10-2004,J12-2006,0,\N,Missing
J10-2004,C98-1006,0,\N,Missing
J10-2004,N06-1033,1,\N,Missing
K19-1061,P11-1113,0,0.028856,"h is a trigger for a MovementTransport event in the first sentence and for an End-Position event in the second sentence. Due to the complexity of the task and the difficulty in constructing a standard annotation scheme, there exists limited labeled data, for only a few languages. The earliest work has focused mainly on English, for which there are relatively many annotated sentences, and relies extensively on language-specific linguistic tools to extract the lexical and syntactic features that need to be computed as a pre-requisite for the task (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013). Simply generating annotated corpora for each In this work, we treat event trigger extraction as a sequence tagging problem and propose a cross-lingual framework for training it without any hand-crafted features. We experiment with different flavors of transfer learning from high-resourced to low-resourced languages and compare the performance of different multilingual embeddings for event trigger extraction. Our results show that training in a multilingual setting outperforms languagespecific models for both English and Chinese. Our work is the first to experiment with two"
K19-1061,Q17-1010,0,0.0609281,"mance for each event trigger extraction architecture. They are selected based on random search and performance on the validation dataset. For Bi-LSTM-Char-CRF, we train character embeddings using a single bi-LSTM layer with 100 hidden units and use another single layer of bi-LSTM with 300 hidden units to train on the concatenated word and char embeddings. We use a dropout rate of 0.5. We optimize using Adam with learning rate of 0.01, weight decay rate of 0.9, β1 = 0.7, β2 = 0.999 and  = 1e−8. For monolingual embeddings, we use 300dimensional word embeddings for EN, ZH, and AR from fastText (Bojanowski et al., 2017). For multilingual experiments, we use MUSE library 3 to train unsupervised alignments from ZH and AR to EN resulting in a unified vector space for the three languages. We use the same training hyperparameters across monolingual and multilingual training to ensure a fair comparison. For BERT-CRF, we train monolingual EN and ZH using cased BERT-Base and BERT-ZH models4 respectively and for all multilingual experiments, we use the recommended multi-cased BERT-Base model.5 All models were trained using 12 layers with 768 hidden size and 12 selfattention heads and 110 Million parameters. We fine t"
K19-1061,C16-1114,0,0.0818556,"e associated with multilingual training is what encouraged us to explore this methodology on event trigger extraction. To the best of our knowledge, there is no prior work adopting conventional or contextualized multilingual embeddings for event trigger detection. chine translation techniques or word alignment data. Feng et al. (2016) propose a languageindependent approach that doesn’t require any linguistic feature engineering. However, this approach still requires equally abundant labeled data for different languages and implies the need to train a new model for each language independently. Hsi et al. (2016) exploit both languagedependent and language-independent features in the form of universal features such as universal dependencies, limited bilingual dictionaries and aligned multilingual word embeddings to train a model with multiple languages. However, this work lags behind in terms of the neural approach used and doesn’t investigate the effectiveness of leveraging multiple source languages. Liu et al. (2018b) propose gated cross-lingual attention as a mechanism to exploit the inherent complementarity of multilingual data which helps with data scarcity and trigger disambiguation. However, th"
K19-1061,C12-1033,0,0.0583822,"Missing"
K19-1061,P08-1030,0,0.519183,"n disambiguating the word sense of leaving, which is a trigger for a MovementTransport event in the first sentence and for an End-Position event in the second sentence. Due to the complexity of the task and the difficulty in constructing a standard annotation scheme, there exists limited labeled data, for only a few languages. The earliest work has focused mainly on English, for which there are relatively many annotated sentences, and relies extensively on language-specific linguistic tools to extract the lexical and syntactic features that need to be computed as a pre-requisite for the task (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013). Simply generating annotated corpora for each In this work, we treat event trigger extraction as a sequence tagging problem and propose a cross-lingual framework for training it without any hand-crafted features. We experiment with different flavors of transfer learning from high-resourced to low-resourced languages and compare the performance of different multilingual embeddings for event trigger extraction. Our results show that training in a multilingual setting outperforms languagespecific models for both English and Chinese. O"
K19-1061,P15-1017,0,0.721951,", we observe a tendency of practitioners drifting away from linguistic features and more towards continuous distributed features that can be obtained without hand-engineering, based simply on publicly available corpora. Recently, approaches have tried to overcome the limitation of traditional lexical features, which can suffer from the data sparsity problem and inability to fully capture the semantics of the words, by making use of sequential modeling methods including variants of Recurrent Neural Networks (RNN) and Convolutional Neural Networks (CNN), and/or Conditional Random Fields (CRF). (Chen et al., 2015; Liu et al., 2016; Nguyen et al., 2016; Sha et al., 2018; Liu et al., 2018b). Existing approaches which take into consideration the cross-lingual aspect of event trigger extraction tend to either take inspiration from machine translation, distant supervision or multitasking. Machine translation is used by Liu et al. (2018a) to project monolingual text to parallel multilingual texts to model the confidence of clues provided by other languages. However, this approach suffers from error propagation of machine translation. Another approach relies on multilingual embeddings, which can be pre-train"
K19-1061,C12-1089,0,0.0421993,"布年底前要解散 ，所以使得... ”Since ’the sea of the moon’ has been announced to be disbanded before the end of the year, ... ” MUSE Movement: Transport Justice: Release-Parole Conflict: Demonstrate BERT(mono) BERT Personnel: End-Position Transaction: Transfer-Money Conflict: Attack BERT(multi all) O Justice:Fine O Conflict:Attack B-Business: DeclareBankruptcy Business: End-Org Table 3: Examples of trigger extraction mislabeled by MUSE but correctly labeled by BERT and those missed/mislabeled with monolingual training only and corrected with multilingual BERT model. cross-lingual document classification ((Klementiev et al., 2012); (Schwenk and Li, 2018)) named entity recognition (Xie et al., 2018). More recently, BERT was developed as an extension to the transformer architecture and achieved significant improvement in performance for many NLP tasks. The gain in performance associated with multilingual training is what encouraged us to explore this methodology on event trigger extraction. To the best of our knowledge, there is no prior work adopting conventional or contextualized multilingual embeddings for event trigger detection. chine translation techniques or word alignment data. Feng et al. (2016) propose a langua"
K19-1061,D11-1005,0,0.0121929,"ts for both English and Chinese show competitive performance with baselines on the ACE2005 benchmark even in the zero-shot learning scheme. Although results using MUSE are lower for English, they are on par with Chinese baselines and better for Arabic compared to BERT. Cross-lingual Tasks Cross-lingual embeddings are of practical usefulness in many tasks in natural language processing (NLP) and information extraction (IE). In each case, a model is trained on one language and transferred to unseen languages. Downstream applications on which they are applied include partof-speech (POS) tagging (Cohen et al., 2011), 663 We observe a generous boost in performance when English is added to the target language, and when all languages are combined together to train one cross-lingual model, especially for Arabic. Our result are promising compared to both feature-based approaches and cross-lingual approaches based on machine translation. deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnes"
K19-1061,N16-1030,0,0.0322079,"-art approach for sequence tagging based on bidirectional Long Short Term Memory (bi-LSTM) with word and character embeddings and a CRF layer on top of it. Then, we describe an approach that trains BERT with a CRF layer for the task. In both architectures, the input is in the form of BIO notation used to differentiate between the beginning (B), inside (I) and (O) for no associated trigger labels. 2.1.2 Character Embeddings Character embeddings are used to capture orthographic patterns and to deal with out-ofvocabulary words, especially in the cross-lingual setting. We follow the same setup as Lample et al. (2016) to obtain character embeddings using biLSTM. Specifically, we concatenate both character and word-level features and use a bi-LSTM on top of that. 2.1 2.1.3 CRF Layer The encoded character and word-level features are fed to a CRF layer to learn inter-dependencies between output trigger tags and find the optimal tag sequence. This layer simulates bi-LSTM in its use of past and future tags to predict the current tag. Following Lafferty et al. (2001), CRF layers define a transition matrix A and use a score Aij to model the transition from the ith state to the j th for a pair of consecutive time"
K19-1061,P13-1008,0,0.443286,"a MovementTransport event in the first sentence and for an End-Position event in the second sentence. Due to the complexity of the task and the difficulty in constructing a standard annotation scheme, there exists limited labeled data, for only a few languages. The earliest work has focused mainly on English, for which there are relatively many annotated sentences, and relies extensively on language-specific linguistic tools to extract the lexical and syntactic features that need to be computed as a pre-requisite for the task (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013). Simply generating annotated corpora for each In this work, we treat event trigger extraction as a sequence tagging problem and propose a cross-lingual framework for training it without any hand-crafted features. We experiment with different flavors of transfer learning from high-resourced to low-resourced languages and compare the performance of different multilingual embeddings for event trigger extraction. Our results show that training in a multilingual setting outperforms languagespecific models for both English and Chinese. Our work is the first to experiment with two event architecture"
K19-1061,P10-1081,0,0.451088,"rd sense of leaving, which is a trigger for a MovementTransport event in the first sentence and for an End-Position event in the second sentence. Due to the complexity of the task and the difficulty in constructing a standard annotation scheme, there exists limited labeled data, for only a few languages. The earliest work has focused mainly on English, for which there are relatively many annotated sentences, and relies extensively on language-specific linguistic tools to extract the lexical and syntactic features that need to be computed as a pre-requisite for the task (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013). Simply generating annotated corpora for each In this work, we treat event trigger extraction as a sequence tagging problem and propose a cross-lingual framework for training it without any hand-crafted features. We experiment with different flavors of transfer learning from high-resourced to low-resourced languages and compare the performance of different multilingual embeddings for event trigger extraction. Our results show that training in a multilingual setting outperforms languagespecific models for both English and Chinese. Our work is the first to e"
K19-1061,D18-1034,0,0.0240982,"nded before the end of the year, ... ” MUSE Movement: Transport Justice: Release-Parole Conflict: Demonstrate BERT(mono) BERT Personnel: End-Position Transaction: Transfer-Money Conflict: Attack BERT(multi all) O Justice:Fine O Conflict:Attack B-Business: DeclareBankruptcy Business: End-Org Table 3: Examples of trigger extraction mislabeled by MUSE but correctly labeled by BERT and those missed/mislabeled with monolingual training only and corrected with multilingual BERT model. cross-lingual document classification ((Klementiev et al., 2012); (Schwenk and Li, 2018)) named entity recognition (Xie et al., 2018). More recently, BERT was developed as an extension to the transformer architecture and achieved significant improvement in performance for many NLP tasks. The gain in performance associated with multilingual training is what encouraged us to explore this methodology on event trigger extraction. To the best of our knowledge, there is no prior work adopting conventional or contextualized multilingual embeddings for event trigger detection. chine translation techniques or word alignment data. Feng et al. (2016) propose a languageindependent approach that doesn’t require any linguistic feature en"
K19-1061,P19-1522,0,0.0323423,"s a subtask of event extraction which requires systems to detect and label the lexical instantiation of an event, known as a trigger. As an example, in the sentence ”John traveled to NYC for a meeting”, trav656 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 656–665 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics and right context (Devlin et al., 2019), was proposed, which unlike MUSE, provides contextualized word embeddings, and has been shown to achieve state-of-the-art performance on many NLP tasks. In particular, (Yang et al., 2019) propose a method based on BERT for enhancing event trigger and argument extraction by generating more labeled data. However, it has not been applied in the context of cross-lingual transfer learning. In this paper, we investigate the possibility of automatically learning effective features from data while relying on zero language-specific linguistic resources. Moreover, we explore the application of multilingual embeddings to the event trigger extraction task in a direct transfer of annotation scheme where ground truth is only needed for one language and can be used to predict labels in other"
K19-1061,D18-1156,0,0.0216899,"Missing"
K19-1061,N16-1034,0,0.480059,"ers drifting away from linguistic features and more towards continuous distributed features that can be obtained without hand-engineering, based simply on publicly available corpora. Recently, approaches have tried to overcome the limitation of traditional lexical features, which can suffer from the data sparsity problem and inability to fully capture the semantics of the words, by making use of sequential modeling methods including variants of Recurrent Neural Networks (RNN) and Convolutional Neural Networks (CNN), and/or Conditional Random Fields (CRF). (Chen et al., 2015; Liu et al., 2016; Nguyen et al., 2016; Sha et al., 2018; Liu et al., 2018b). Existing approaches which take into consideration the cross-lingual aspect of event trigger extraction tend to either take inspiration from machine translation, distant supervision or multitasking. Machine translation is used by Liu et al. (2018a) to project monolingual text to parallel multilingual texts to model the confidence of clues provided by other languages. However, this approach suffers from error propagation of machine translation. Another approach relies on multilingual embeddings, which can be pre-trained beforehand on large monolingual corp"
K19-1061,L18-1560,0,0.0233934,"e sea of the moon’ has been announced to be disbanded before the end of the year, ... ” MUSE Movement: Transport Justice: Release-Parole Conflict: Demonstrate BERT(mono) BERT Personnel: End-Position Transaction: Transfer-Money Conflict: Attack BERT(multi all) O Justice:Fine O Conflict:Attack B-Business: DeclareBankruptcy Business: End-Org Table 3: Examples of trigger extraction mislabeled by MUSE but correctly labeled by BERT and those missed/mislabeled with monolingual training only and corrected with multilingual BERT model. cross-lingual document classification ((Klementiev et al., 2012); (Schwenk and Li, 2018)) named entity recognition (Xie et al., 2018). More recently, BERT was developed as an extension to the transformer architecture and achieved significant improvement in performance for many NLP tasks. The gain in performance associated with multilingual training is what encouraged us to explore this methodology on event trigger extraction. To the best of our knowledge, there is no prior work adopting conventional or contextualized multilingual embeddings for event trigger detection. chine translation techniques or word alignment data. Feng et al. (2016) propose a languageindependent approach t"
L16-1067,P13-2131,1,0.824032,"9 68.7 23.4 R∗ 100.0 100.0 67.1 41.8 92.2 92.4 46.7 Figure 3: System name annotated as ‘PBMT base PRO’. Table 2: Performance of atom detector in terms of Precision, Recall, and F1 score, and reconstruction from survey response (R∗ ). 5. Baseline Evaluation Data From the collected data, five papers are used for development, and 62 are used for evaluation. Evaluation Metrics We evaluate system performance of atom detection with precision and recall. We approach the evaluation of the linked structured representation by transforming it into a directed acyclic graph and computing the Smatch score (Cai and Knight, 2013), previously used to evaluate the similarity between Abstract Meaning Representation (AMR) structures. 5.1. Atom Detection Evaluation Table 2 shows the performance of atom detection. As annotators do not tell us where the information is located, we match the annotated atoms to every substring in the structured text and present annotation recall from text as R∗ in Table 2. This presents a soft ceiling for our baseline approach. Finding annotated dataset name, size, and system name atoms was challenging due to abbreviations, PDF-to-text conversion errors, lexical diversity and name expansion, as"
L16-1067,councill-etal-2008-parscit,0,0.0238814,"3 4. St. Dev. 1.38 1.78 89.65 9.85 4.14 1.52 5.01 0.34 We present a pipelined pattern-based system that extracts individual atoms from a plain text logical representation of a machine translation paper and selects and links them into a structured representation. 4.1. Table 1: Structured text and survey response mean and standard deviation. Figure 2: Survey form to collect annotations. commercial TET system.2 As tables are often used to report experimental results, we pay special attention to their extraction. We extract tabular information using TableSeer3 (Liu et al., 2007). We use ParsCit4 (Councill et al., 2008) to derive the hierarchical structure of sections and subsections. We produce the final representation of papers with a system that combines the inputs of all three components. The process produces structured text, split into sections and subsections with parsed tables accompanied by captions, but does not include figures. 3.2. Structured Representation Annotation Annotators are presented with the papers in PDF format. Ideally, annotators would highlight relevant information in the text and link it to the structured representation. However, such linking is very time consuming. As an alternativ"
L16-1067,I11-1001,0,0.0369291,"Missing"
L16-1067,W06-1606,1,0.770132,"Missing"
L16-1067,W09-3607,0,0.0130031,"a structured representation of the experimental data (bottom). Two result values are associated with the PBMT system and two with the SPMT-Comb system. The RESULT values are connected to DATASETS via the test or train relation, and to an EXPERIMENT TYPE. We refer to the individual pieces of information that comprise the structured representation as atoms. In the example, there are 15 atoms, including Chinese-English, BLEU, 31.46 and 2002 NIST. 3. Data and Annotation In order to construct the dataset, we started by selecting papers related to Machine Translation from the ACL Anthology corpus (Radev et al., 2009) using keyword search and targeting of MT related workshops. For a random sample of 67 papers, we asked the annotators to provide a structured representation of experimental results as defined in the previous section. In order to aid structured information extraction, we automatically produced a structured text representation of each paper. The representation consists of plain text split into sections and subsections, as well as parsed tables. The annotated dataset is released alongside the paper1 to promote future research. In total, 1063 atoms were annotated. Additional dataset statistics ar"
L16-1067,P11-4002,0,0.394826,"Missing"
N06-1045,W98-1426,1,\N,Missing
N06-1045,N04-1035,1,\N,Missing
N06-1045,W05-1506,0,\N,Missing
N06-1045,C96-2215,0,\N,Missing
N06-1045,P04-1015,0,\N,Missing
N06-1045,P05-3025,1,\N,Missing
N06-1045,E03-1005,0,\N,Missing
N06-1045,C92-3126,0,\N,Missing
N16-1145,N09-1025,1,0.743713,"rate our LSTM as a rescoring feature on top of the output of a strong Arabic-English syntax-based string-to-tree statistical machine translation system (Galley et al., 2006; Galley et al., 2004). That system is trained on 208 million words of parallel Arabic-English data from a variety of sources, much of which is Egyptian discussion forum content. The Arabic side is morphologically segmented with MADA-ARZ (Habash et al., 2013). We use a standard set of features, including two conventional count-based language models, as well as thousands of sparsely-occurring, lexicalized syntactic features (Chiang et al., 2009). The system additionally uses a source-to-target feed-forward neural network translation model during decoding, analogous to the model of (Devlin et al., 2014). These features are tuned with MIRA (Chiang et al., 2009) on approximately 63,000 words of Arabic discussion forum text, along with three references. We evaluate this baseline on two test sets, each with approximately 34,000 words from the same genre used in tuning. We generate n-best lists (n = 1000) of unique translations for each sentence in the tuning set and re-rank the translations using an approach based on MERT (Och, 2003). We"
N16-1145,P14-1129,0,0.0646684,"et al., 2006; Galley et al., 2004). That system is trained on 208 million words of parallel Arabic-English data from a variety of sources, much of which is Egyptian discussion forum content. The Arabic side is morphologically segmented with MADA-ARZ (Habash et al., 2013). We use a standard set of features, including two conventional count-based language models, as well as thousands of sparsely-occurring, lexicalized syntactic features (Chiang et al., 2009). The system additionally uses a source-to-target feed-forward neural network translation model during decoding, analogous to the model of (Devlin et al., 2014). These features are tuned with MIRA (Chiang et al., 2009) on approximately 63,000 words of Arabic discussion forum text, along with three references. We evaluate this baseline on two test sets, each with approximately 34,000 words from the same genre used in tuning. We generate n-best lists (n = 1000) of unique translations for each sentence in the tuning set and re-rank the translations using an approach based on MERT (Och, 2003). We collapse all features other than language models, text length, and derivation size into a single feature, formed by taking the dot product of the previously lea"
N16-1145,N04-1035,1,0.464603,"s 2048 hidden units, with a target vocabulary size of 793,471. For training, we also use dropout to prevent overfitting. We follow Zaremba et al. (2014) for dropout locations, and we use a dropout rate of 0.2. The training is parallelized across 4 GPUs, such that each layer lies on its own GPU and communicates its activations to the next layer once it finishes its computation. 5.2 Statistical Machine Translation We incorporate our LSTM as a rescoring feature on top of the output of a strong Arabic-English syntax-based string-to-tree statistical machine translation system (Galley et al., 2006; Galley et al., 2004). That system is trained on 208 million words of parallel Arabic-English data from a variety of sources, much of which is Egyptian discussion forum content. The Arabic side is morphologically segmented with MADA-ARZ (Habash et al., 2013). We use a standard set of features, including two conventional count-based language models, as well as thousands of sparsely-occurring, lexicalized syntactic features (Chiang et al., 2009). The system additionally uses a source-to-target feed-forward neural network translation model during decoding, analogous to the model of (Devlin et al., 2014). These featur"
N16-1145,P06-1121,1,0.72422,", where each layer has 2048 hidden units, with a target vocabulary size of 793,471. For training, we also use dropout to prevent overfitting. We follow Zaremba et al. (2014) for dropout locations, and we use a dropout rate of 0.2. The training is parallelized across 4 GPUs, such that each layer lies on its own GPU and communicates its activations to the next layer once it finishes its computation. 5.2 Statistical Machine Translation We incorporate our LSTM as a rescoring feature on top of the output of a strong Arabic-English syntax-based string-to-tree statistical machine translation system (Galley et al., 2006; Galley et al., 2004). That system is trained on 208 million words of parallel Arabic-English data from a variety of sources, much of which is Egyptian discussion forum content. The Arabic side is morphologically segmented with MADA-ARZ (Habash et al., 2013). We use a standard set of features, including two conventional count-based language models, as well as thousands of sparsely-occurring, lexicalized syntactic features (Chiang et al., 2009). The system additionally uses a source-to-target feed-forward neural network translation model during decoding, analogous to the model of (Devlin et al"
N16-1145,N13-1044,0,0.0233637,"d across 4 GPUs, such that each layer lies on its own GPU and communicates its activations to the next layer once it finishes its computation. 5.2 Statistical Machine Translation We incorporate our LSTM as a rescoring feature on top of the output of a strong Arabic-English syntax-based string-to-tree statistical machine translation system (Galley et al., 2006; Galley et al., 2004). That system is trained on 208 million words of parallel Arabic-English data from a variety of sources, much of which is Egyptian discussion forum content. The Arabic side is morphologically segmented with MADA-ARZ (Habash et al., 2013). We use a standard set of features, including two conventional count-based language models, as well as thousands of sparsely-occurring, lexicalized syntactic features (Chiang et al., 2009). The system additionally uses a source-to-target feed-forward neural network translation model during decoding, analogous to the model of (Devlin et al., 2014). These features are tuned with MIRA (Chiang et al., 2009) on approximately 63,000 words of Arabic discussion forum text, along with three references. We evaluate this baseline on two test sets, each with approximately 34,000 words from the same genre"
N16-1145,D15-1166,0,0.0268234,"imple approach for handling large vocabularies effectively on the GPU. • Significantly improved perplexities (43.2) on the One Billion Word benchmark over Chelba et al. (2013) • Extrinsic machine translation improvement over a strong baseline. • Fast decoding times because in practice there is no need to normalize. 2 Long Short Term Memory Language Models In recent years, LSTMs (Hochreiter and Schmidhuber, 1997) have achieved state-of-the-art performance in many natural language tasks such as language modeling (Zaremba et al., 2014) and statistical machine translation (Sutskever et al., 2014; Luong et al., 2015). LSTMs were designed to have longer memories than standard RNNs, allowing them to exploit more context to make predictions. To compute word probabilities, the LSTM reads words left-to-right, updating its memory after each word and producing a hidden state h, which summarizes all of the history. For details on the architecture and computations of the LSTM, the reader can refer to (Zaremba et al., 2014). In this model the probability of word w given history u is P (w |u) = p(w |u) , Z(u) (1) where p(w |u) = exp Dw hT + bw is an unnormalized probability. Dw and bw are the output word embedding a"
N16-1145,P03-1021,0,0.0196923,"g et al., 2009). The system additionally uses a source-to-target feed-forward neural network translation model during decoding, analogous to the model of (Devlin et al., 2014). These features are tuned with MIRA (Chiang et al., 2009) on approximately 63,000 words of Arabic discussion forum text, along with three references. We evaluate this baseline on two test sets, each with approximately 34,000 words from the same genre used in tuning. We generate n-best lists (n = 1000) of unique translations for each sentence in the tuning set and re-rank the translations using an approach based on MERT (Och, 2003). We collapse all features other than language models, text length, and derivation size into a single feature, formed by taking the dot product of the previously learned feature and weight vectors. We then run a single iteration of MERT on the n-best lists to determine optimal weights for the collapsed feature, the uncollapsed features, and an LSTM feature formed by taking the score of the hypothesis according to the LSTM described in Section 5.1. We use the weights to rerank hypotheses from the n-best lists of the two test sets. We repeated this experiment, substituting instead a twolayer LST"
N16-1145,D13-1140,1,0.928366,"language models (LSTMs) are a class of RNNs that have been designed to model long histories and are easier to train than standard RNNs. LSTMs are currently the best performing language models on the Penn Treebank (PTB) dataset (Zaremba et al., 2014). The most common method for training LSTMs, maximum likelihood estimation (MLE), is prohibitively expensive for large vocabularies, as it involves time-intensive matrix-matrix multiplications. Noise-contrastive estimation (NCE) has been a successful alternative to train continuous space language models with large vocabularies (Mnih and Teh, 2012; Vaswani et al., 2013). However, NCE in its standard form is not suitable for GPUs, as the computations are not amenable to dense matrix operations. In this paper, we present a natural modification to the NCE objective function for language modeling that allows a very efficient GPU implementation. Using our new objective, we train large multi-layer LSTMs on the One Billion Word benchmark (Chelba et al., 2013), with its full 780k word vocabulary. We achieve significantly lower perplexities with a single model, while using only a sixth of the parameters of a very strong baseline model (Chelba et al., 2013). We releas"
N16-1145,N15-1083,0,\N,Missing
N18-1205,N06-1044,0,0.118345,"Missing"
N18-1205,C96-2215,0,0.603625,"Missing"
N18-5009,P16-1213,0,0.0310293,". In addition to the entity extraction and linking results, we also display the top 5 images for each entity retrieved from Google Image Search11 . In this way even when a user cannot read a document in a lowresource language, s/he will obtain a high-level summary of entities involved in the document. 6 Figure 4: Different Map Styles 7 Heatmap Visualization Related Work Some recent work has also focused on lowresource name tagging (Tsai et al., 2016; Littell et al., 2016; Zhang et al., 2016; Yang et al., 2017) and cross-lingual entity linking (McNamee et al., 2011; Spitkovsky and Chang, 2011; Sil and Florian, 2016), but the system demonstrated in this paper is the first publicly available end-to-end system to perform both tasks and all of the 282 Wikipedia languages. Using disaster monitoring as a use case, we detect the following ten topics from the input multilingual data based on translating 117 English disaster keywords via PanLex12 : (1) water supply, (2) food supply, (3) medical assistance, (4) terrorism or other extreme violence, (5) utilities, energy or sanitation, (6) evacuation, (7) shelter, (8) search and rescue, (9) civil unrest or widespread crime, and (10) infrastructure, as defined in the"
N18-5009,W13-2322,1,0.753485,"ifier} /entity linking amr /localize/{identifier} Description Retrieve the current server status, including supported languages, language identifiers, and the state (offline, online, or pending) of each model. Retrieve the current status of a given language. Main entry of the EDL system. Take input in either plain text or *.ltf format, tag names that are PER, ORG or LOC/GPE, and link them to Wikipedia. Transliterate a name to Latin script. Query based entity linking. Link each mention to KBs. English entity linking for Abstract Meaning Representation (AMR) style input (Pan et al., 2015). AMR (Banarescu et al., 2013) is a structured semantic representation scheme. The rich semantic knowledge in AMR boosts linking performance. Localize a LOC/GPE name based on GeoNames database. Table 1: RUN APIs description. APIs /status /status/{identifier} /train/{identifier} Description An alias of /status Query the current status of a model being trained. Train a new name tagging model for a language. A model id is automatically generated and returned based on model name, and time stamp. Table 2: TRAIN APIs description. Figure 1: Cross-lingual Entity Extraction and Linking Interface Figure 2: Cross-lingual Entity Extra"
N18-5009,K16-1022,0,0.0371881,"e test interface, where a user can select one of the 282 languages, enter a text or select an example document, and run the system. Figure 2 shows an output example. In addition to the entity extraction and linking results, we also display the top 5 images for each entity retrieved from Google Image Search11 . In this way even when a user cannot read a document in a lowresource language, s/he will obtain a high-level summary of entities involved in the document. 6 Figure 4: Different Map Styles 7 Heatmap Visualization Related Work Some recent work has also focused on lowresource name tagging (Tsai et al., 2016; Littell et al., 2016; Zhang et al., 2016; Yang et al., 2017) and cross-lingual entity linking (McNamee et al., 2011; Spitkovsky and Chang, 2011; Sil and Florian, 2016), but the system demonstrated in this paper is the first publicly available end-to-end system to perform both tasks and all of the 282 Wikipedia languages. Using disaster monitoring as a use case, we detect the following ten topics from the input multilingual data based on translating 117 English disaster keywords via PanLex12 : (1) water supply, (2) food supply, (3) medical assistance, (4) terrorism or other extreme violence,"
N18-5009,Q16-1026,0,0.0419127,"Missing"
N18-5009,N16-1029,1,0.928423,"on Sciences Institute {jonmay,knight}@isi.edu Abstract tity mention with a certain type. Our model is based on a bi-directional long short-term memory (LSTM) networks with a Conditional Random Fields (CRFs) layer (Chiu and Nichols, 2016). It is challenging to perform entity extraction across a massive variety of languages because most languages don’t have sufficient data to train a machine learning model. To tackle the low-resource challenge, we developed creative methods of deriving noisy training data from Wikipedia (Pan et al., 2017), exploiting non-traditional languageuniversal resources (Zhang et al., 2016) and crosslingual transfer learning (Cheung et al., 2017). We demonstrate E LISA -E DL, a state-of-the-art re-trainable system to extract entity mentions from low-resource languages, link them to external English knowledge bases, and visualize locations related to disaster topics on a world heatmap. We make all of our data sets1 , resources and system training and testing APIs2 publicly available for research purpose. 1 Introduction Our cross-lingual entity extraction, linking and localization system is capable of extracting named entities from unstructured text in any of 282 Wikipedia languag"
N18-5009,W16-2701,1,0.830496,"ages, and the TRAIN section provides a re-training function for users who want to train their own customized name tagging models using their own datasets. We also published our training and test data sets, as well as resources related to at morphology analysis and name translation at: https://elisa-ie.github.io/wikiann. Table 1 and Table 2 present the detailed functionality and usages of the APIs of these two sections. Besides the core components as described in Section 2 and Section 3, we also provide the APIs of additional components, including a re-trainable name transliteration component (Lin et al., 2016) and a universal name and word translation component based on word alignment derived from crossTable 3: Name Tagging Performance on Low-Resource Languages GeoNames and Wikipedia as their salience scores. Then we construct knowledge networks from source language texts, where each node represents a entity mention, and each link represents a sentence-level co-occurrence relation. If two mentions cooccur in the same sentence, we prefer their entity candidates in the GeoNames to share an administrative code and type, or be geographically close in the world, as measured in terms of latitude and long"
N18-5009,C16-1095,0,0.0142781,"here a user can select one of the 282 languages, enter a text or select an example document, and run the system. Figure 2 shows an output example. In addition to the entity extraction and linking results, we also display the top 5 images for each entity retrieved from Google Image Search11 . In this way even when a user cannot read a document in a lowresource language, s/he will obtain a high-level summary of entities involved in the document. 6 Figure 4: Different Map Styles 7 Heatmap Visualization Related Work Some recent work has also focused on lowresource name tagging (Tsai et al., 2016; Littell et al., 2016; Zhang et al., 2016; Yang et al., 2017) and cross-lingual entity linking (McNamee et al., 2011; Spitkovsky and Chang, 2011; Sil and Florian, 2016), but the system demonstrated in this paper is the first publicly available end-to-end system to perform both tasks and all of the 282 Wikipedia languages. Using disaster monitoring as a use case, we detect the following ten topics from the input multilingual data based on translating 117 English disaster keywords via PanLex12 : (1) water supply, (2) food supply, (3) medical assistance, (4) terrorism or other extreme violence, (5) utilities, energy"
N18-5009,I11-1029,0,0.0340278,"d run the system. Figure 2 shows an output example. In addition to the entity extraction and linking results, we also display the top 5 images for each entity retrieved from Google Image Search11 . In this way even when a user cannot read a document in a lowresource language, s/he will obtain a high-level summary of entities involved in the document. 6 Figure 4: Different Map Styles 7 Heatmap Visualization Related Work Some recent work has also focused on lowresource name tagging (Tsai et al., 2016; Littell et al., 2016; Zhang et al., 2016; Yang et al., 2017) and cross-lingual entity linking (McNamee et al., 2011; Spitkovsky and Chang, 2011; Sil and Florian, 2016), but the system demonstrated in this paper is the first publicly available end-to-end system to perform both tasks and all of the 282 Wikipedia languages. Using disaster monitoring as a use case, we detect the following ten topics from the input multilingual data based on translating 117 English disaster keywords via PanLex12 : (1) water supply, (2) food supply, (3) medical assistance, (4) terrorism or other extreme violence, (5) utilities, energy or sanitation, (6) evacuation, (7) shelter, (8) search and rescue, (9) civil unrest or widespre"
N18-5009,N15-1119,1,0.949944,"efforts reported from incident regions in low-resource languages. In the rest of the paper, we will present a comprehensive overview of the system components (Section 2 and Section 3), APIs (Section 4), interface3 (Section 5), and visualization4 (Section 6). 2 3 Entity Linking and Localization After we extract entity mentions, we link GPE and LOC mentions to GeoNames5 , and PER and ORG mentions to Wikipedia6 . We adopt the name translation approach described in (Pan et al., 2017) to translate each tagged entity mention into English, then we apply an unsupervised collective inference approach (Pan et al., 2015) to link each translated mention to the target KB. Figure 2 shows an example output of a Hausa document. The extracted entity mentions “Stephane Dujarric” and “birnin Bentiu” are linked to their corresponding entries in Wikipedia and GeoNames respectively. Compared to traditional entity linking, the unique challenge of linking to GeoNames is that it is very scarce, without rich linked structures or text descriptions. Only 500k out of 4.7 million entities in Wikipedia are linked to GeoNames. Therefore, we associate mentions with entities in the KBs in a collective manner, based on salience, sim"
N18-5009,P17-1178,1,0.947324,"er Polytechnic Institute {zhangb8,liny9,panx2,lud2,jih}@rpi.edu 2 Information Sciences Institute {jonmay,knight}@isi.edu Abstract tity mention with a certain type. Our model is based on a bi-directional long short-term memory (LSTM) networks with a Conditional Random Fields (CRFs) layer (Chiu and Nichols, 2016). It is challenging to perform entity extraction across a massive variety of languages because most languages don’t have sufficient data to train a machine learning model. To tackle the low-resource challenge, we developed creative methods of deriving noisy training data from Wikipedia (Pan et al., 2017), exploiting non-traditional languageuniversal resources (Zhang et al., 2016) and crosslingual transfer learning (Cheung et al., 2017). We demonstrate E LISA -E DL, a state-of-the-art re-trainable system to extract entity mentions from low-resource languages, link them to external English knowledge bases, and visualize locations related to disaster topics on a world heatmap. We make all of our data sets1 , resources and system training and testing APIs2 publicly available for research purpose. 1 Introduction Our cross-lingual entity extraction, linking and localization system is capable of ext"
N19-1252,D18-1549,0,0.0207217,"mote locales still remains. As an example, when international aid organizations respond to new disasters, they are often unable to deploy technology to understand local reports detailing specific events (Munro and Manning, 2012; Lewis et al., 2011). An inability to communicate with partner governments or civilian populations in a timely manner leads to preventable casualties. The lack of adequate labeled training data has been the major obstacle to expanding NLP’s outreach more multilingually. Developments in unsupervised techniques that require only monolingual corpora (Lample et al., 2018a; Artetxe et al., 2018) and the ability to leverage labeled resources in other languages have been proposed to address this issue (Das and Petrov, 2011; Duong et al., 2014; Ammar et al., 2016). Unfortunately, these 2428 Proceedings of NAACL-HLT 2019, pages 2428–2439 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics methods either do not work in practice on true lowresource cases or unrealistically assume the availability of some amount of supervision. Consider syntactic parsing as a prime example. Past editions of the CoNLL Shared Task on Multilingual Parsing (Zeman et a"
N19-1252,N10-1083,0,0.0864509,"Missing"
N19-1252,Q17-1010,0,0.0929895,"Missing"
N19-1252,J92-4003,0,0.758212,"laer Polytechnic Institute ♥ Information Sciences Institute, University of Southern California ronald.cardenas@matfyz.cz liny9@rpi.edu jih@rpi.edu jonmay@isi.edu Abstract Unsupervised part of speech (POS) tagging is often framed as a clustering problem, but practical taggers need to ground their clusters as well. Grounding generally requires reference labeled data, a luxury a low-resource language might not have. In this work, we describe an approach for low-resource unsupervised POS tagging that yields fully grounded output and requires no labeled training data. We find the classic method of Brown et al. (1992) clusters well in our use case and employ a decipherment-based approach to grounding. This approach presumes a sequence of cluster IDs is a ‘ciphertext’ and seeks a POS tag-tocluster ID mapping that will reveal the POS sequence. We show intrinsically that, despite the difficulty of the task, we obtain reasonable performance across a variety of languages. We also show extrinsically that incorporating our POS tagger into a name tagger leads to stateof-the-art tagging performance in Sinhalese and Kinyarwanda, two languages with nearly no labeled POS data available. We further demonstrate our tagg"
N19-1252,Q16-1026,0,0.0475058,"Missing"
N19-1252,D10-1056,0,0.0303461,"d, with few exceptions, on extensions to the standard HMM; most, in the form of appropriate priors over the HMM multinomial parameters (Goldwater and Griffiths, 2007; Johnson, 2007; Ganchev et al., 2009); others, by using logistic distributions instead of multinomial ones (Berg-Kirkpatrick et al., 2010; Stratos et al., 2016). However, these models still need to ground or map hidden states to actual POS tags to evaluate, and they inevitably resort to many-to-one or one-to-one accuracy scoring. Some previous work has been cautious in pointing out this ill-defined setting (Ravi and Knight, 2009; Christodoulopoulos et al., 2010), and we argue its inappropriateness for scenarios in which the test set is extremely small or even when no annotated reference corpus exists. Therefore, the problem of grounding the sequence of states or cluster IDs to POS tags without using any linguistic resource remains unsolved. We formulate this task as a decipherment problem. Decipherment aims to find a substitution table between alphabets or tokens of an encrypted code and a known language without the need of parallel corpora. The task has been successfully applied in alphabet mapping for lost languages (Snyder et al., 2010), and machi"
N19-1252,E03-1009,0,0.156984,"oy 102 features obtained from WALS9 related to word order and morphosyntactic alignment, further reduced to 50 dimensions using PCA. However, none these criteria correlates significantly to tagging accuracy, as we elaborate in Section 5.1. We instead try a combined approach. The likelihood of cluster ID replacement, Pθ (ˆ ci |pj ), ∀ˆ ci ∈ C, ∀pj in the tagset, is replaced by P ci |psj ) s∈S,s6=t Pθ (ˆ Pavg (ˆ ci |pj ) ∼ |S |− 1 https://github.com/percyliang/ brown-cluster 7 Optimized and implemented by M¨uller and Schuetze (2015). Available at http://cistern.cis.lmu.de/ marlin/ 8 As noted by Clark (2003) and Johnson (2007), in the limit, keeping each type (or, in the case of A - HMM , TOKEN in its own cluster will result in the maximum possible manyto-one (polysemic types prevent perfect accuracy when type clustering). where Pall (p) is a language model trained over the concatenation of POS sequences of all parent languages in S. We call this approach CIPHER - AVG. 4 Downstream Tasks 4.1 Name Tagging We experiment with the LSTM-CNN model proposed by Chiu and Nichols (2016), one of the 2431 9 https://wals.info/ Seq. Tagger BROWN MARLIN A - HMM E - KMEANS en 81.37 81.53 77.12 63.01 de 81.28 81."
N19-1252,P11-1061,0,0.208466,"o deploy technology to understand local reports detailing specific events (Munro and Manning, 2012; Lewis et al., 2011). An inability to communicate with partner governments or civilian populations in a timely manner leads to preventable casualties. The lack of adequate labeled training data has been the major obstacle to expanding NLP’s outreach more multilingually. Developments in unsupervised techniques that require only monolingual corpora (Lample et al., 2018a; Artetxe et al., 2018) and the ability to leverage labeled resources in other languages have been proposed to address this issue (Das and Petrov, 2011; Duong et al., 2014; Ammar et al., 2016). Unfortunately, these 2428 Proceedings of NAACL-HLT 2019, pages 2428–2439 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics methods either do not work in practice on true lowresource cases or unrealistically assume the availability of some amount of supervision. Consider syntactic parsing as a prime example. Past editions of the CoNLL Shared Task on Multilingual Parsing (Zeman et al., 2017, 2018) featured a category of target languages for which either little or no training data was provided. However, even"
N19-1252,P15-1081,0,0.022531,"no annotated reference corpus exists. Therefore, the problem of grounding the sequence of states or cluster IDs to POS tags without using any linguistic resource remains unsolved. We formulate this task as a decipherment problem. Decipherment aims to find a substitution table between alphabets or tokens of an encrypted code and a known language without the need of parallel corpora. The task has been successfully applied in alphabet mapping for lost languages (Snyder et al., 2010), and machine translation at the character (Pourdamghani and Knight, 2017) and token level (Ravi and Knight, 2011; Dou et al., 2015). For the task of POS tag grounding, the sequence of states or cluster IDs is modeled as an encrypted code to be deciphered back to a POS sequence. Furthermore, we tackle the problem from a ‘universal’ perspective by allowing the cipher learn from POS sequences from a varied pool of languages. Other recent work has declared a ‘radically universal’ mantra to language inclusivity. Hermjakob et al. (2018) presents a Romanizer that covers all writing systems known to Unicode. Pan et al. (2017) extends name tagging and linking capability to hundreds of languages by leveraging Wikipedia. Kirov et al"
N19-1252,D14-1096,0,0.116435,"understand local reports detailing specific events (Munro and Manning, 2012; Lewis et al., 2011). An inability to communicate with partner governments or civilian populations in a timely manner leads to preventable casualties. The lack of adequate labeled training data has been the major obstacle to expanding NLP’s outreach more multilingually. Developments in unsupervised techniques that require only monolingual corpora (Lample et al., 2018a; Artetxe et al., 2018) and the ability to leverage labeled resources in other languages have been proposed to address this issue (Das and Petrov, 2011; Duong et al., 2014; Ammar et al., 2016). Unfortunately, these 2428 Proceedings of NAACL-HLT 2019, pages 2428–2439 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics methods either do not work in practice on true lowresource cases or unrealistically assume the availability of some amount of supervision. Consider syntactic parsing as a prime example. Past editions of the CoNLL Shared Task on Multilingual Parsing (Zeman et al., 2017, 2018) featured a category of target languages for which either little or no training data was provided. However, even in the ‘no-resource’"
N19-1252,P15-1033,0,0.0569815,"Missing"
N19-1252,P17-2093,0,0.0203432,"different approach, Goldberg et al. (2008) obtain competitive performance with a classic HMM model by initializing the emission probability distribution with a mixture of language-specific, linguistically constrained distributions. However, both of these approaches are framed around the task of unsupervised POS disambiguation with a full dictionary (Merialdo, 1994). Previous work relaxes the full dictionary constraint by leveraging monolingual lexicons (Haghighi and Klein, 2006; Smith and Eisner, 2005; Merialdo, 1994; Ravi and Knight, 2009), multilingual tagged dictionaries (Li et al., 2012; Fang and Cohn, 2017), and parallel corpora (Duong et al., 2014; T¨ackstr¨om et al., 2013; Das and Petrov, 2011). In addition, previous work includes sequence models that do not rely on any resource besides raw text during training, namely unsupervised POS induction models. These models are based, with few exceptions, on extensions to the standard HMM; most, in the form of appropriate priors over the HMM multinomial parameters (Goldwater and Griffiths, 2007; Johnson, 2007; Ganchev et al., 2009); others, by using logistic distributions instead of multinomial ones (Berg-Kirkpatrick et al., 2010; Stratos et al., 2016"
N19-1252,P08-1085,0,0.030201,"a without any path constraint (a cluster ID could be mapped to any POS tag). In this sense, our approach applies EM to simplify the task (e.g. when using Brown clustering (Brown et al., 1992)), followed by another EM run to optimize cipher table parameters. Under this lens, the methods closest to our approach are those which attempt to reduce or constrain the parameter search space prior to running EM. For instance, Ravi and Knight (2009) explicitly search for the smallest model that explains the data using Integer Programming, and then use EM to set parameter values. In a different approach, Goldberg et al. (2008) obtain competitive performance with a classic HMM model by initializing the emission probability distribution with a mixture of language-specific, linguistically constrained distributions. However, both of these approaches are framed around the task of unsupervised POS disambiguation with a full dictionary (Merialdo, 1994). Previous work relaxes the full dictionary constraint by leveraging monolingual lexicons (Haghighi and Klein, 2006; Smith and Eisner, 2005; Merialdo, 1994; Ravi and Knight, 2009), multilingual tagged dictionaries (Li et al., 2012; Fang and Cohn, 2017), and parallel corpora"
N19-1252,P07-1094,0,0.121402,"der “zeroresource” conditions. In order to function, however, the model requires users to provide gold POS tags and word mappings from these languages into a common semantic space, using approaches that require parallel data (Guo et al., 2015). Indeed, the compulsion to use POS tag-labeled data in zero-resource circumstances extends to the vast, varied lines of research in unsupervised POS tagging itself! Every approach explored so far ultimately requires POS-annotated resources for the language being studied in order to produce a final, grounded output. Even the most conservative strategies (Goldwater and Griffiths, 2007; BergKirkpatrick et al., 2010; Stratos et al., 2016) that do not require any supervised signal during training still ultimately produce only ungrounded clusters, and require a reference annotated corpus to map the inferred clusters or states to actual POS tags. Making matters worse, evaluation is generally offered in terms of the ‘many-to-one’ or ‘one-toone’ analyses Johnson (2007). These metrics use a reference corpus to determine the optimal mapping of clusters to tags. While this evaluation approach is intuitively sensible for measuring cluster purity, to actually use such an output, an en"
N19-1252,P15-1119,0,0.166435,"her little or no training data was provided. However, even in the ‘no-resource’ scenario that most closely matches our use case, gold part-of-speech (POS) tags for test data were provided for the participants to use. Prior to these shared tasks, Ammar et al. (2016) proposed a variant of their main model, MALOPA, that was meant to produce reasonable parses for languages under “zeroresource” conditions. In order to function, however, the model requires users to provide gold POS tags and word mappings from these languages into a common semantic space, using approaches that require parallel data (Guo et al., 2015). Indeed, the compulsion to use POS tag-labeled data in zero-resource circumstances extends to the vast, varied lines of research in unsupervised POS tagging itself! Every approach explored so far ultimately requires POS-annotated resources for the language being studied in order to produce a final, grounded output. Even the most conservative strategies (Goldwater and Griffiths, 2007; BergKirkpatrick et al., 2010; Stratos et al., 2016) that do not require any supervised signal during training still ultimately produce only ungrounded clusters, and require a reference annotated corpus to map the"
N19-1252,N06-1041,0,0.0670558,"009) explicitly search for the smallest model that explains the data using Integer Programming, and then use EM to set parameter values. In a different approach, Goldberg et al. (2008) obtain competitive performance with a classic HMM model by initializing the emission probability distribution with a mixture of language-specific, linguistically constrained distributions. However, both of these approaches are framed around the task of unsupervised POS disambiguation with a full dictionary (Merialdo, 1994). Previous work relaxes the full dictionary constraint by leveraging monolingual lexicons (Haghighi and Klein, 2006; Smith and Eisner, 2005; Merialdo, 1994; Ravi and Knight, 2009), multilingual tagged dictionaries (Li et al., 2012; Fang and Cohn, 2017), and parallel corpora (Duong et al., 2014; T¨ackstr¨om et al., 2013; Das and Petrov, 2011). In addition, previous work includes sequence models that do not rely on any resource besides raw text during training, namely unsupervised POS induction models. These models are based, with few exceptions, on extensions to the standard HMM; most, in the form of appropriate priors over the HMM multinomial parameters (Goldwater and Griffiths, 2007; Johnson, 2007; Ganche"
N19-1252,C08-1042,0,0.0436749,"Missing"
N19-1252,P18-4003,1,0.811244,"been successfully applied in alphabet mapping for lost languages (Snyder et al., 2010), and machine translation at the character (Pourdamghani and Knight, 2017) and token level (Ravi and Knight, 2011; Dou et al., 2015). For the task of POS tag grounding, the sequence of states or cluster IDs is modeled as an encrypted code to be deciphered back to a POS sequence. Furthermore, we tackle the problem from a ‘universal’ perspective by allowing the cipher learn from POS sequences from a varied pool of languages. Other recent work has declared a ‘radically universal’ mantra to language inclusivity. Hermjakob et al. (2018) presents a Romanizer that covers all writing systems known to Unicode. Pan et al. (2017) extends name tagging and linking capability to hundreds of languages by leveraging Wikipedia. Kirov et al. (2016) has semiautomatically built inflectional paradigms for hundreds of languages. 7 Conclusion We present a POS tag grounding strategy based on decipherment that does not require human-labeled data to map states or clusters to actual POS tags and thus can be used in real-world situations requiring grounded POS tags. The decipherment model considers state or word cluster IDs of a CL as a cipher tex"
N19-1252,D07-1031,0,0.362308,"itself! Every approach explored so far ultimately requires POS-annotated resources for the language being studied in order to produce a final, grounded output. Even the most conservative strategies (Goldwater and Griffiths, 2007; BergKirkpatrick et al., 2010; Stratos et al., 2016) that do not require any supervised signal during training still ultimately produce only ungrounded clusters, and require a reference annotated corpus to map the inferred clusters or states to actual POS tags. Making matters worse, evaluation is generally offered in terms of the ‘many-to-one’ or ‘one-toone’ analyses Johnson (2007). These metrics use a reference corpus to determine the optimal mapping of clusters to tags. While this evaluation approach is intuitively sensible for measuring cluster purity, to actually use such an output, an entire annotated training corpus is required.1 It is not enough to simply rely on ungrounded clusters in real-world systems; grounded labels offer a sort of universal API between other resources such as rule-based modules that operate on certain word types or between resources built from other annotated high-resource language data. Since POS tag and parallel data resources for new lan"
N19-1252,L16-1498,0,0.0232341,"al., 2015). For the task of POS tag grounding, the sequence of states or cluster IDs is modeled as an encrypted code to be deciphered back to a POS sequence. Furthermore, we tackle the problem from a ‘universal’ perspective by allowing the cipher learn from POS sequences from a varied pool of languages. Other recent work has declared a ‘radically universal’ mantra to language inclusivity. Hermjakob et al. (2018) presents a Romanizer that covers all writing systems known to Unicode. Pan et al. (2017) extends name tagging and linking capability to hundreds of languages by leveraging Wikipedia. Kirov et al. (2016) has semiautomatically built inflectional paradigms for hundreds of languages. 7 Conclusion We present a POS tag grounding strategy based on decipherment that does not require human-labeled data to map states or clusters to actual POS tags and thus can be used in real-world situations requiring grounded POS tags. The decipherment model considers state or word cluster IDs of a CL as a cipher text to be deciphered back to a POS sequence. The model operates on top of Brown cluster IDs and requires a POS language model trained on annotated corpora of one or more PLs. Experimental results over a la"
N19-1252,W11-2164,0,0.0217165,"e use an unsupervised clustering method (Section 3.2) then reduce and ground the clusters using a decipherment approach informed by POS tag sequence data from many languages (Section 3.3). Introduction While cellular, satellite, and hardware advances have ensured that sophisticated NLP technology can reach all corners of the earth, the language barrier upon reaching remote locales still remains. As an example, when international aid organizations respond to new disasters, they are often unable to deploy technology to understand local reports detailing specific events (Munro and Manning, 2012; Lewis et al., 2011). An inability to communicate with partner governments or civilian populations in a timely manner leads to preventable casualties. The lack of adequate labeled training data has been the major obstacle to expanding NLP’s outreach more multilingually. Developments in unsupervised techniques that require only monolingual corpora (Lample et al., 2018a; Artetxe et al., 2018) and the ability to leverage labeled resources in other languages have been proposed to address this issue (Das and Petrov, 2011; Duong et al., 2014; Ammar et al., 2016). Unfortunately, these 2428 Proceedings of NAACL-HLT 2019,"
N19-1252,D12-1127,0,0.0474251,"Missing"
N19-1252,P13-2017,0,0.0530892,"Missing"
N19-1252,J94-2001,0,0.765507,"pt to reduce or constrain the parameter search space prior to running EM. For instance, Ravi and Knight (2009) explicitly search for the smallest model that explains the data using Integer Programming, and then use EM to set parameter values. In a different approach, Goldberg et al. (2008) obtain competitive performance with a classic HMM model by initializing the emission probability distribution with a mixture of language-specific, linguistically constrained distributions. However, both of these approaches are framed around the task of unsupervised POS disambiguation with a full dictionary (Merialdo, 1994). Previous work relaxes the full dictionary constraint by leveraging monolingual lexicons (Haghighi and Klein, 2006; Smith and Eisner, 2005; Merialdo, 1994; Ravi and Knight, 2009), multilingual tagged dictionaries (Li et al., 2012; Fang and Cohn, 2017), and parallel corpora (Duong et al., 2014; T¨ackstr¨om et al., 2013; Das and Petrov, 2011). In addition, previous work includes sequence models that do not rely on any resource besides raw text during training, namely unsupervised POS induction models. These models are based, with few exceptions, on extensions to the standard HMM; most, in the f"
N19-1252,N15-1055,0,0.0677038,"Missing"
N19-1252,P17-1178,1,0.84514,"hine translation at the character (Pourdamghani and Knight, 2017) and token level (Ravi and Knight, 2011; Dou et al., 2015). For the task of POS tag grounding, the sequence of states or cluster IDs is modeled as an encrypted code to be deciphered back to a POS sequence. Furthermore, we tackle the problem from a ‘universal’ perspective by allowing the cipher learn from POS sequences from a varied pool of languages. Other recent work has declared a ‘radically universal’ mantra to language inclusivity. Hermjakob et al. (2018) presents a Romanizer that covers all writing systems known to Unicode. Pan et al. (2017) extends name tagging and linking capability to hundreds of languages by leveraging Wikipedia. Kirov et al. (2016) has semiautomatically built inflectional paradigms for hundreds of languages. 7 Conclusion We present a POS tag grounding strategy based on decipherment that does not require human-labeled data to map states or clusters to actual POS tags and thus can be used in real-world situations requiring grounded POS tags. The decipherment model considers state or word cluster IDs of a CL as a cipher text to be deciphered back to a POS sequence. The model operates on top of Brown cluster IDs"
N19-1252,D17-1266,0,0.0275221,"ess for scenarios in which the test set is extremely small or even when no annotated reference corpus exists. Therefore, the problem of grounding the sequence of states or cluster IDs to POS tags without using any linguistic resource remains unsolved. We formulate this task as a decipherment problem. Decipherment aims to find a substitution table between alphabets or tokens of an encrypted code and a known language without the need of parallel corpora. The task has been successfully applied in alphabet mapping for lost languages (Snyder et al., 2010), and machine translation at the character (Pourdamghani and Knight, 2017) and token level (Ravi and Knight, 2011; Dou et al., 2015). For the task of POS tag grounding, the sequence of states or cluster IDs is modeled as an encrypted code to be deciphered back to a POS sequence. Furthermore, we tackle the problem from a ‘universal’ perspective by allowing the cipher learn from POS sequences from a varied pool of languages. Other recent work has declared a ‘radically universal’ mantra to language inclusivity. Hermjakob et al. (2018) presents a Romanizer that covers all writing systems known to Unicode. Pan et al. (2017) extends name tagging and linking capability to"
N19-1252,D15-1039,0,0.0209949,"(fr), Italian (it), Spanish (es), Japanese (ja), Czech (cs), Russian (ru), Arabic (ar), and Farsi (fa). For Swahili (sw), we use the Helsinki Corpus of Swahili 2.0.4 Overall in these experiments we cover 11 languages and 4 language families. In our dependency parsing experiments, we use the Universal Treebank v2.0 (McDonald et al., 2013) for en, de, fr, es, it, Portuguese (pt), and Swedish (sv). This set of treebanks is chosen instead of UD in order to obtain results comparable to those of previous work on simulated zeroresource parsing scenarios (Ammar et al., 2016; Zhang and Barzilay, 2015; Rasooli and Collins, 2015). In our name tagging experiments, we use monolingual texts for Sinhalese (si) and Kinyarwanda (rw) provided by DARPA’s Low Resource Languages for Emergent Incidents (LORELEI) Program during the 2018 Low Resource Human Languages Technologies (LoReHLT) evaluation. 3.2 While unrealistic for POS tagger performance purposes, many-to-one is a good choice for determining cluster ‘purity’ and provides a reasonable grounding upper bound. As the calculation of many-to-one does require labeled data, we constrain the use of these labels for development and will evaluate extrinsically using languages for"
N19-1252,P09-1057,0,0.134614,"agging pipeline can be interpreted as first reducing the vocabulary size to a fixed number of clusters, and then finding a cluster– POS tag mapping table that best explains the data without any path constraint (a cluster ID could be mapped to any POS tag). In this sense, our approach applies EM to simplify the task (e.g. when using Brown clustering (Brown et al., 1992)), followed by another EM run to optimize cipher table parameters. Under this lens, the methods closest to our approach are those which attempt to reduce or constrain the parameter search space prior to running EM. For instance, Ravi and Knight (2009) explicitly search for the smallest model that explains the data using Integer Programming, and then use EM to set parameter values. In a different approach, Goldberg et al. (2008) obtain competitive performance with a classic HMM model by initializing the emission probability distribution with a mixture of language-specific, linguistically constrained distributions. However, both of these approaches are framed around the task of unsupervised POS disambiguation with a full dictionary (Merialdo, 1994). Previous work relaxes the full dictionary constraint by leveraging monolingual lexicons (Hagh"
N19-1252,P11-1002,0,0.0987625,"odel. If we assume a deterministic pipelined clustering of words and a tag labeling model that does not depend on words, then for chosen cˆ, this becomes X argmax Pθ (p|c, w)Pθ (c|w) p c∈C |w| = argmax Pθ (p|ˆ c) p = argmax Pθ (ˆ c|p)Pθ (p) (1) p We call this model the cipher grounder. As presented it requires an estimate for Pθ (p) for the CL, which requires POS training data. Under the zero-resource scenario, we instead approximate Pθ (p) by the tag distribution of a PL. Then, the cipher table Pθ (ˆ c|p) can be trained using a noisy-channel, expectation-maximization (EM)based approach as in Ravi and Knight (2011). 3 POS Tagger construction We approach the search for optimal components in the two-step pipeline outlined in Section 2 in a cascaded manner. First, an optimal word clustering is determined by means of the many-to-one evaluation method. This method is explained well by Johnson (2007): “ ...deterministically map each hidden state to the POS tag it co-occurs most frequently with, and return the proportion of the resulting POS tags that are the same as the POS tags of the goldstandard corpus.” which we do not have any training data; see Section 5.2. Secondly, we search for the best approach to g"
N19-1252,P05-1044,0,0.108207,"the smallest model that explains the data using Integer Programming, and then use EM to set parameter values. In a different approach, Goldberg et al. (2008) obtain competitive performance with a classic HMM model by initializing the emission probability distribution with a mixture of language-specific, linguistically constrained distributions. However, both of these approaches are framed around the task of unsupervised POS disambiguation with a full dictionary (Merialdo, 1994). Previous work relaxes the full dictionary constraint by leveraging monolingual lexicons (Haghighi and Klein, 2006; Smith and Eisner, 2005; Merialdo, 1994; Ravi and Knight, 2009), multilingual tagged dictionaries (Li et al., 2012; Fang and Cohn, 2017), and parallel corpora (Duong et al., 2014; T¨ackstr¨om et al., 2013; Das and Petrov, 2011). In addition, previous work includes sequence models that do not rely on any resource besides raw text during training, namely unsupervised POS induction models. These models are based, with few exceptions, on extensions to the standard HMM; most, in the form of appropriate priors over the HMM multinomial parameters (Goldwater and Griffiths, 2007; Johnson, 2007; Ganchev et al., 2009); others,"
N19-1252,P10-1107,0,0.0331855,"hristodoulopoulos et al., 2010), and we argue its inappropriateness for scenarios in which the test set is extremely small or even when no annotated reference corpus exists. Therefore, the problem of grounding the sequence of states or cluster IDs to POS tags without using any linguistic resource remains unsolved. We formulate this task as a decipherment problem. Decipherment aims to find a substitution table between alphabets or tokens of an encrypted code and a known language without the need of parallel corpora. The task has been successfully applied in alphabet mapping for lost languages (Snyder et al., 2010), and machine translation at the character (Pourdamghani and Knight, 2017) and token level (Ravi and Knight, 2011; Dou et al., 2015). For the task of POS tag grounding, the sequence of states or cluster IDs is modeled as an encrypted code to be deciphered back to a POS sequence. Furthermore, we tackle the problem from a ‘universal’ perspective by allowing the cipher learn from POS sequences from a varied pool of languages. Other recent work has declared a ‘radically universal’ mantra to language inclusivity. Hermjakob et al. (2018) presents a Romanizer that covers all writing systems known to"
N19-1252,K17-3009,0,0.0657579,"Missing"
N19-1252,Q16-1018,0,0.181264,"the model requires users to provide gold POS tags and word mappings from these languages into a common semantic space, using approaches that require parallel data (Guo et al., 2015). Indeed, the compulsion to use POS tag-labeled data in zero-resource circumstances extends to the vast, varied lines of research in unsupervised POS tagging itself! Every approach explored so far ultimately requires POS-annotated resources for the language being studied in order to produce a final, grounded output. Even the most conservative strategies (Goldwater and Griffiths, 2007; BergKirkpatrick et al., 2010; Stratos et al., 2016) that do not require any supervised signal during training still ultimately produce only ungrounded clusters, and require a reference annotated corpus to map the inferred clusters or states to actual POS tags. Making matters worse, evaluation is generally offered in terms of the ‘many-to-one’ or ‘one-toone’ analyses Johnson (2007). These metrics use a reference corpus to determine the optimal mapping of clusters to tags. While this evaluation approach is intuitively sensible for measuring cluster purity, to actually use such an output, an entire annotated training corpus is required.1 It is no"
N19-1252,Q13-1001,0,0.0574881,"Missing"
N19-1252,D15-1213,0,0.0201838,"(en), German (de), French (fr), Italian (it), Spanish (es), Japanese (ja), Czech (cs), Russian (ru), Arabic (ar), and Farsi (fa). For Swahili (sw), we use the Helsinki Corpus of Swahili 2.0.4 Overall in these experiments we cover 11 languages and 4 language families. In our dependency parsing experiments, we use the Universal Treebank v2.0 (McDonald et al., 2013) for en, de, fr, es, it, Portuguese (pt), and Swedish (sv). This set of treebanks is chosen instead of UD in order to obtain results comparable to those of previous work on simulated zeroresource parsing scenarios (Ammar et al., 2016; Zhang and Barzilay, 2015; Rasooli and Collins, 2015). In our name tagging experiments, we use monolingual texts for Sinhalese (si) and Kinyarwanda (rw) provided by DARPA’s Low Resource Languages for Emergent Incidents (LORELEI) Program during the 2018 Low Resource Human Languages Technologies (LoReHLT) evaluation. 3.2 While unrealistic for POS tagger performance purposes, many-to-one is a good choice for determining cluster ‘purity’ and provides a reasonable grounding upper bound. As the calculation of many-to-one does require labeled data, we constrain the use of these labels for development and will evaluate extrin"
N19-1383,W16-1614,0,0.0301494,"16) and cross-lingual word embeddings (Fang and Cohn, 2017; Wang et al., 2017; Huang et al., 2018; Xie et al., 2018). The second is based on multitask learning via a weight sharing encoder (Yang et al., 2016, 2017; Lin et al., 2018). Compared to these studies, our approach not only automatically learns cross-lingual word embeddings without requiring any parallel resources, but also carefully extracts language-agnostic sequential features, yielding better performance. Adversarial training has also been extensively studied and applied for cross-lingual and crossdomain transfer. Several studies (Barone, 2016; Zhang et al., 2017c,b; Conneau et al., 2017; Chen and Cardie, 2018) explore adversarial training to automatically induce bilingual and multilingual word representations without using any parallel corpora or bilingual gazetteers. Adversarial training is also applied to extract languageagnostic (Chen et al., 2016; Zou et al., 2018; Wang and Pan, 2018; Kim et al., 2017a; Muis et al., 3830 2018; Cao et al., 2018) and domain-agnostic features (Kim et al., 2017b; Ganin et al., 2016; Tzeng et al., 2017; Chen et al., 2017; Li et al., 2017; Fu et al., 2017; Bousmalis et al., 2016; Shi et al., 2018) f"
N19-1383,D18-1017,0,0.0222598,"age-agnostic sequential features, yielding better performance. Adversarial training has also been extensively studied and applied for cross-lingual and crossdomain transfer. Several studies (Barone, 2016; Zhang et al., 2017c,b; Conneau et al., 2017; Chen and Cardie, 2018) explore adversarial training to automatically induce bilingual and multilingual word representations without using any parallel corpora or bilingual gazetteers. Adversarial training is also applied to extract languageagnostic (Chen et al., 2016; Zou et al., 2018; Wang and Pan, 2018; Kim et al., 2017a; Muis et al., 3830 2018; Cao et al., 2018) and domain-agnostic features (Kim et al., 2017b; Ganin et al., 2016; Tzeng et al., 2017; Chen et al., 2017; Li et al., 2017; Fu et al., 2017; Bousmalis et al., 2016; Shi et al., 2018) for cross-lingual and cross-domain adaptation. Compared with these methods, our approach combines both word-level and sentence-level adversarial training. 5 Conclusions and Future Work We design a new neural architecture which integrates multi-level adversarial transfer into a BiLSTM-CRF to improve low-resource name tagging. With word-level adversarial training, it can automatically project the source language i"
N19-1383,D18-1024,0,0.103202,"features from both t and s are further fed into a context encoder to better capture and refine contextual information and a conditional random field (CRF) (Lafferty et al., 2001) based name tagger. Next we show the details of each component in our architecture. 2.2 Word-level Adversarial Transfer To better leverage the resources from the source language, our first step is to construct a shared se3824 mantic space where the words from the source and target languages are semantically aligned. Without requiring any bilingual gazetteers, recent efforts (Zhang et al., 2017b; Conneau et al., 2017; Chen and Cardie, 2018) explore unsupervised approaches to learn cross-lingual word embeddings and achieve comparable performance to supervised methods. Following these studies, we perform word-level adversarial training to automatically align word representations from s and t. Formally, assume we are given pretrained monolingual word embeddings t t t N ×d t Vt = {v1 , v2 , ..., vN } ∈ R for t, and Vs = {vs1 , vs2 , ..., vsM } ∈ RM ×ds for s, where vti and vsj are the vector representations of words wit and wis from t and s, N and M denote the vocabulary sizes, dt and ds denote the embedding dimensionality of t and"
N19-1383,R11-1017,0,0.214416,"Missing"
N19-1383,W18-6125,0,0.156228,"et al., 2017, 2018; Devlin et al., 2018) to improve name tagging. In addition, several approaches (Zhang et al., 2016a, 2017a; AlBadrashiny et al., 2017) attempt to incorporate hand-crafted linguistic features into a Bi-LSTMCRF to improve low-resource name tagging performance. Recent attempts on cross-lingual transfer for name tagging can be divided into two categories: the first projects annotations from a source language to a target language via parallel corpora (Yarowsky et al., 2001; Wang and Manning, 2013; Wang et al., 2013; Zhang et al., 2016b; Fang and Cohn, 2016; Ehrmann et al., 2011; Enghoff et al., 2018; Ni et al., 2017), a bilingual gazetteer (Feng et al., 2017; Zirikly and Hagiwara, 2015), Wikipedia anchor links (Kim et al., 2012; Nothman et al., 2013; Tsai et al., 2016; Pan et al., 2017), and language universal representations, including Unicode bytes (Gillick et al., 2016) and cross-lingual word embeddings (Fang and Cohn, 2017; Wang et al., 2017; Huang et al., 2018; Xie et al., 2018). The second is based on multitask learning via a weight sharing encoder (Yang et al., 2016, 2017; Lin et al., 2018). Compared to these studies, our approach not only automatically learns cross-lingual word e"
N19-1383,K16-1018,0,0.0292479,"language models (Liu et al., 2018; Peters et al., 2017, 2018; Devlin et al., 2018) to improve name tagging. In addition, several approaches (Zhang et al., 2016a, 2017a; AlBadrashiny et al., 2017) attempt to incorporate hand-crafted linguistic features into a Bi-LSTMCRF to improve low-resource name tagging performance. Recent attempts on cross-lingual transfer for name tagging can be divided into two categories: the first projects annotations from a source language to a target language via parallel corpora (Yarowsky et al., 2001; Wang and Manning, 2013; Wang et al., 2013; Zhang et al., 2016b; Fang and Cohn, 2016; Ehrmann et al., 2011; Enghoff et al., 2018; Ni et al., 2017), a bilingual gazetteer (Feng et al., 2017; Zirikly and Hagiwara, 2015), Wikipedia anchor links (Kim et al., 2012; Nothman et al., 2013; Tsai et al., 2016; Pan et al., 2017), and language universal representations, including Unicode bytes (Gillick et al., 2016) and cross-lingual word embeddings (Fang and Cohn, 2017; Wang et al., 2017; Huang et al., 2018; Xie et al., 2018). The second is based on multitask learning via a weight sharing encoder (Yang et al., 2016, 2017; Lin et al., 2018). Compared to these studies, our approach not on"
N19-1383,P17-2093,0,0.0225366,"g can be divided into two categories: the first projects annotations from a source language to a target language via parallel corpora (Yarowsky et al., 2001; Wang and Manning, 2013; Wang et al., 2013; Zhang et al., 2016b; Fang and Cohn, 2016; Ehrmann et al., 2011; Enghoff et al., 2018; Ni et al., 2017), a bilingual gazetteer (Feng et al., 2017; Zirikly and Hagiwara, 2015), Wikipedia anchor links (Kim et al., 2012; Nothman et al., 2013; Tsai et al., 2016; Pan et al., 2017), and language universal representations, including Unicode bytes (Gillick et al., 2016) and cross-lingual word embeddings (Fang and Cohn, 2017; Wang et al., 2017; Huang et al., 2018; Xie et al., 2018). The second is based on multitask learning via a weight sharing encoder (Yang et al., 2016, 2017; Lin et al., 2018). Compared to these studies, our approach not only automatically learns cross-lingual word embeddings without requiring any parallel resources, but also carefully extracts language-agnostic sequential features, yielding better performance. Adversarial training has also been extensively studied and applied for cross-lingual and crossdomain transfer. Several studies (Barone, 2016; Zhang et al., 2017c,b; Conneau et al., 2017;"
N19-1383,2005.mtsummit-papers.11,0,0.102348,"Missing"
N19-1383,I17-2072,0,0.0448683,"gual and crossdomain transfer. Several studies (Barone, 2016; Zhang et al., 2017c,b; Conneau et al., 2017; Chen and Cardie, 2018) explore adversarial training to automatically induce bilingual and multilingual word representations without using any parallel corpora or bilingual gazetteers. Adversarial training is also applied to extract languageagnostic (Chen et al., 2016; Zou et al., 2018; Wang and Pan, 2018; Kim et al., 2017a; Muis et al., 3830 2018; Cao et al., 2018) and domain-agnostic features (Kim et al., 2017b; Ganin et al., 2016; Tzeng et al., 2017; Chen et al., 2017; Li et al., 2017; Fu et al., 2017; Bousmalis et al., 2016; Shi et al., 2018) for cross-lingual and cross-domain adaptation. Compared with these methods, our approach combines both word-level and sentence-level adversarial training. 5 Conclusions and Future Work We design a new neural architecture which integrates multi-level adversarial transfer into a BiLSTM-CRF to improve low-resource name tagging. With word-level adversarial training, it can automatically project the source language into a shared semantic space with the target language without requiring any comparable data or bilingual gazetteers. Moreover, considering the"
N19-1383,N16-1155,0,0.268781,"rce languages, the addition of English to the limited target language training data yields a considerably noisy corpus. However, by forcing the sequence encoder to extract language-agnostic features, our approach still achieves better performance than the monolingual baseline for most languages. All of these experiments demonstrate that our approach is more effective in leveraging annotations from other languages to improve target language name tagging. 3.5 Cross-lingual Transfer for High Resource Languages Language Dutch Spanish Model Lample et al. (2016) Yang et al. (2017) Lin et al. (2018) Gillick et al. (2016) Word-Adv1 Word-Adv2 Our Model (Bi-LSTM) F-score 81.74 85.19 85.71 82.84 85.87 86.43 86.87 Lample et al. (2016) Yang et al. (2017) Lin et al. (2018) Gillick et al. (2016) Word-Adv1 Word-Adv2 Our Model (Bi-LSTM) 85.75 85.77 85.02 82.95 85.92 85.84 86.41 Table 6: Comparison on cross-lingual transfer for Dutch and Spanish with various baselines: monolingual baseline (Lample et al. (2016)), multitask baselines (Yang et al. (2017) and Lin et al. (2018)), language universal representation baselines (Gillick et al. (2016), Word-Adv1 , Word-Adv2 ). We finally investigate the results when both the sour"
N19-1383,D18-1023,1,0.852801,"e first projects annotations from a source language to a target language via parallel corpora (Yarowsky et al., 2001; Wang and Manning, 2013; Wang et al., 2013; Zhang et al., 2016b; Fang and Cohn, 2016; Ehrmann et al., 2011; Enghoff et al., 2018; Ni et al., 2017), a bilingual gazetteer (Feng et al., 2017; Zirikly and Hagiwara, 2015), Wikipedia anchor links (Kim et al., 2012; Nothman et al., 2013; Tsai et al., 2016; Pan et al., 2017), and language universal representations, including Unicode bytes (Gillick et al., 2016) and cross-lingual word embeddings (Fang and Cohn, 2017; Wang et al., 2017; Huang et al., 2018; Xie et al., 2018). The second is based on multitask learning via a weight sharing encoder (Yang et al., 2016, 2017; Lin et al., 2018). Compared to these studies, our approach not only automatically learns cross-lingual word embeddings without requiring any parallel resources, but also carefully extracts language-agnostic sequential features, yielding better performance. Adversarial training has also been extensively studied and applied for cross-lingual and crossdomain transfer. Several studies (Barone, 2016; Zhang et al., 2017c,b; Conneau et al., 2017; Chen and Cardie, 2018) explore adversa"
N19-1383,N16-1030,0,0.138096,"glish is very different from these low-resource languages, the addition of English to the limited target language training data yields a considerably noisy corpus. However, by forcing the sequence encoder to extract language-agnostic features, our approach still achieves better performance than the monolingual baseline for most languages. All of these experiments demonstrate that our approach is more effective in leveraging annotations from other languages to improve target language name tagging. 3.5 Cross-lingual Transfer for High Resource Languages Language Dutch Spanish Model Lample et al. (2016) Yang et al. (2017) Lin et al. (2018) Gillick et al. (2016) Word-Adv1 Word-Adv2 Our Model (Bi-LSTM) F-score 81.74 85.19 85.71 82.84 85.87 86.43 86.87 Lample et al. (2016) Yang et al. (2017) Lin et al. (2018) Gillick et al. (2016) Word-Adv1 Word-Adv2 Our Model (Bi-LSTM) 85.75 85.77 85.02 82.95 85.92 85.84 86.41 Table 6: Comparison on cross-lingual transfer for Dutch and Spanish with various baselines: monolingual baseline (Lample et al. (2016)), multitask baselines (Yang et al. (2017) and Lin et al. (2018)), language universal representation baselines (Gillick et al. (2016), Word-Adv1 , Word-Ad"
N19-1383,P18-1074,1,0.845725,"artment, Rensselaer Polytechnic Institute {huangl7,jih}@rpi.edu 2 Information Sciences Institute, University of Southern California jonmay@isi.edu Abstract and Hagiwara, 2015), cross-lingual word embedding (Fang and Cohn, 2017; Wang et al., 2017; Huang et al., 2018), or cross-lingual Wikification (Kim et al., 2012; Nothman et al., 2013; Tsai et al., 2016; Pan et al., 2017), but these resources are still only available for dozens of languages. Recent efforts on multi-task learning model each language as one single task while all the tasks share the same encoding layer (Yang et al., 2016, 2017; Lin et al., 2018). These methods can transfer knowledge via the shared encoder without using bilingual resources. However, different languages usually have different underlying sequence structures, as shown in Figure 1. Without an explicit constraint, the encoder is not guaranteed to extract language-independent sequential features. Moreover, when the size of annotated resources is not balanced, the encoder is likely to be biased toward the resource-dominant language. We focus on improving name tagging for lowresource languages using annotations from related languages. Previous studies either directly project"
N19-1383,P16-1101,0,0.0759447,"statistics of these data sets. For fair comparison, we use the same pretrained word embeddings of English, Dutch and Spanish as Lin et al. (2018), while for each lowresource language we train their word embeddings using the documents from their LDC packages with FastText.6 Table 3 lists the key hyperparameters we used in our experiments. 3.2 Baselines We compare our methods with three categories of baseline methods:7 • Monolingual Name Tagging Using monolingual annotations only, the current state-of-theart name tagging model is the Bi-LSTM-CRF network (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016).8 • Multi-task Learning Lin et al. (2018) apply multi-task learning to boost name tagging performance by introducing additional annotations from source languages using a weight sharing context encoder across multiple languages. • Language Universal Representations We apply word adversarial transfer only to project the source language into the same semantic space as the target language, then train the name tagger on the annotations of source and target languages. Word-Adv1 refers to the approach which is directly trained on the combination of the anno4 The annotations are from: am (LDC2016E87)"
N19-1383,D17-1302,0,0.106268,"Missing"
N19-1383,P12-1073,0,0.0708481,"hiny et al., 2017) attempt to incorporate hand-crafted linguistic features into a Bi-LSTMCRF to improve low-resource name tagging performance. Recent attempts on cross-lingual transfer for name tagging can be divided into two categories: the first projects annotations from a source language to a target language via parallel corpora (Yarowsky et al., 2001; Wang and Manning, 2013; Wang et al., 2013; Zhang et al., 2016b; Fang and Cohn, 2016; Ehrmann et al., 2011; Enghoff et al., 2018; Ni et al., 2017), a bilingual gazetteer (Feng et al., 2017; Zirikly and Hagiwara, 2015), Wikipedia anchor links (Kim et al., 2012; Nothman et al., 2013; Tsai et al., 2016; Pan et al., 2017), and language universal representations, including Unicode bytes (Gillick et al., 2016) and cross-lingual word embeddings (Fang and Cohn, 2017; Wang et al., 2017; Huang et al., 2018; Xie et al., 2018). The second is based on multitask learning via a weight sharing encoder (Yang et al., 2016, 2017; Lin et al., 2018). Compared to these studies, our approach not only automatically learns cross-lingual word embeddings without requiring any parallel resources, but also carefully extracts language-agnostic sequential features, yielding bet"
N19-1383,P17-1135,0,0.53374,"andom fields (Bi-LSTM-CRF) (Lample et al., 2016; Huang et al., 2015; Ma and 3823 Proceedings of NAACL-HLT 2019, pages 3823–3833 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Target Language BPER ... ... Sequence Feature Encoder Convolutional Neural Networks Word Discriminator IPER Sequence Discriminator Context Encoder O CRF Name Tagger ... O BGPE Linear Projection ... Source Language Figure 2: Architecture overview. Hovy, 2016), integrated with multi-level adversarial transfer: (1) word level adversarial transfer, similar to Conneau et al. (2017), applying a projection function on the source language and a discriminator to distinguish each word of the target language from that of the source language, resulting in a bilingual shared semantic space; (2) sentence-level adversarial transfer, where a discriminator is trained to distinguish each sentence of the target language from that of the source language,2 and a sequence encoder is applied to each sentence of both languages to prevent the discriminator from correctly predicting the source of each sentence, yielding language-agnostic sequential features. These features can better facili"
N19-1383,P17-1178,1,0.875125,"uistic features into a Bi-LSTMCRF to improve low-resource name tagging performance. Recent attempts on cross-lingual transfer for name tagging can be divided into two categories: the first projects annotations from a source language to a target language via parallel corpora (Yarowsky et al., 2001; Wang and Manning, 2013; Wang et al., 2013; Zhang et al., 2016b; Fang and Cohn, 2016; Ehrmann et al., 2011; Enghoff et al., 2018; Ni et al., 2017), a bilingual gazetteer (Feng et al., 2017; Zirikly and Hagiwara, 2015), Wikipedia anchor links (Kim et al., 2012; Nothman et al., 2013; Tsai et al., 2016; Pan et al., 2017), and language universal representations, including Unicode bytes (Gillick et al., 2016) and cross-lingual word embeddings (Fang and Cohn, 2017; Wang et al., 2017; Huang et al., 2018; Xie et al., 2018). The second is based on multitask learning via a weight sharing encoder (Yang et al., 2016, 2017; Lin et al., 2018). Compared to these studies, our approach not only automatically learns cross-lingual word embeddings without requiring any parallel resources, but also carefully extracts language-agnostic sequential features, yielding better performance. Adversarial training has also been extensiv"
N19-1383,P17-1161,0,0.0332845,"0 0 0 0.2 0.4 0.6 0.8 1 Sample Rate of Source Training Data Figure 4: The impact of the size of annotations from source and target languages on Amharic name tagging. 4 Related Work Name tagging methods based on sequence labeling have been widely studied in recent years. Huang et al. (2015) and Lample et al. (2016) propose an effective Bi-LSTM-CRF architecture; the BiLSTM encodes previous and following contexts, and the CRF is used for tag prediction. Other studies incorporate a character-level CNN (Ma and Hovy, 2016), global contexts (Zhang et al., 2018), or language models (Liu et al., 2018; Peters et al., 2017, 2018; Devlin et al., 2018) to improve name tagging. In addition, several approaches (Zhang et al., 2016a, 2017a; AlBadrashiny et al., 2017) attempt to incorporate hand-crafted linguistic features into a Bi-LSTMCRF to improve low-resource name tagging performance. Recent attempts on cross-lingual transfer for name tagging can be divided into two categories: the first projects annotations from a source language to a target language via parallel corpora (Yarowsky et al., 2001; Wang and Manning, 2013; Wang et al., 2013; Zhang et al., 2016b; Fang and Cohn, 2016; Ehrmann et al., 2011; Enghoff et a"
N19-1383,N18-1202,0,0.0481708,"any comparable data or bilingual gazetteers. Moreover, considering the different underlying sequential structures among various languages, we further design a sentence-level adversarial transfer to encourage the sequence encoder to extract language-agnostic features. The experiments show that our approach achieves the state-of-the-art on both CoNLL data sets and 10 low-resource languages. In the future, we will further explore selecting the feature-consistent annotations from the source language and add to the target language, and explore unsupervised pretrained cross-lingual language models (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018; Lample and Conneau, 2019) for cross-lingual low resource name tagging. Acknowledgments This research is based upon work supported in part by U.S. DARPA LORELEI Program # HR001115-C-0115, and the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via contract # FA8650-17-C-9116, and ARL NS-CTA No. W911NF-09-2-0053. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied"
N19-1383,D18-1125,1,0.833352,"udies (Barone, 2016; Zhang et al., 2017c,b; Conneau et al., 2017; Chen and Cardie, 2018) explore adversarial training to automatically induce bilingual and multilingual word representations without using any parallel corpora or bilingual gazetteers. Adversarial training is also applied to extract languageagnostic (Chen et al., 2016; Zou et al., 2018; Wang and Pan, 2018; Kim et al., 2017a; Muis et al., 3830 2018; Cao et al., 2018) and domain-agnostic features (Kim et al., 2017b; Ganin et al., 2016; Tzeng et al., 2017; Chen et al., 2017; Li et al., 2017; Fu et al., 2017; Bousmalis et al., 2016; Shi et al., 2018) for cross-lingual and cross-domain adaptation. Compared with these methods, our approach combines both word-level and sentence-level adversarial training. 5 Conclusions and Future Work We design a new neural architecture which integrates multi-level adversarial transfer into a BiLSTM-CRF to improve low-resource name tagging. With word-level adversarial training, it can automatically project the source language into a shared semantic space with the target language without requiring any comparable data or bilingual gazetteers. Moreover, considering the different underlying sequential structures"
N19-1383,W02-2024,0,0.510491,"Missing"
N19-1383,W03-0419,0,0.373808,"Missing"
N19-1383,K16-1022,0,0.0721972,"glish is very different from these low-resource languages, the addition of English to the limited target language training data yields a considerably noisy corpus. However, by forcing the sequence encoder to extract language-agnostic features, our approach still achieves better performance than the monolingual baseline for most languages. All of these experiments demonstrate that our approach is more effective in leveraging annotations from other languages to improve target language name tagging. 3.5 Cross-lingual Transfer for High Resource Languages Language Dutch Spanish Model Lample et al. (2016) Yang et al. (2017) Lin et al. (2018) Gillick et al. (2016) Word-Adv1 Word-Adv2 Our Model (Bi-LSTM) F-score 81.74 85.19 85.71 82.84 85.87 86.43 86.87 Lample et al. (2016) Yang et al. (2017) Lin et al. (2018) Gillick et al. (2016) Word-Adv1 Word-Adv2 Our Model (Bi-LSTM) 85.75 85.77 85.02 82.95 85.92 85.84 86.41 Table 6: Comparison on cross-lingual transfer for Dutch and Spanish with various baselines: monolingual baseline (Lample et al. (2016)), multitask baselines (Yang et al. (2017) and Lin et al. (2018)), language universal representation baselines (Gillick et al. (2016), Word-Adv1 , Word-Ad"
N19-1383,I17-2065,0,0.240629,"andom fields (Bi-LSTM-CRF) (Lample et al., 2016; Huang et al., 2015; Ma and 3823 Proceedings of NAACL-HLT 2019, pages 3823–3833 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Target Language BPER ... ... Sequence Feature Encoder Convolutional Neural Networks Word Discriminator IPER Sequence Discriminator Context Encoder O CRF Name Tagger ... O BGPE Linear Projection ... Source Language Figure 2: Architecture overview. Hovy, 2016), integrated with multi-level adversarial transfer: (1) word level adversarial transfer, similar to Conneau et al. (2017), applying a projection function on the source language and a discriminator to distinguish each word of the target language from that of the source language, resulting in a bilingual shared semantic space; (2) sentence-level adversarial transfer, where a discriminator is trained to distinguish each sentence of the target language from that of the source language,2 and a sequence encoder is applied to each sentence of both languages to prevent the discriminator from correctly predicting the source of each sentence, yielding language-agnostic sequential features. These features can better facili"
N19-1383,P13-1106,0,0.0297461,"global contexts (Zhang et al., 2018), or language models (Liu et al., 2018; Peters et al., 2017, 2018; Devlin et al., 2018) to improve name tagging. In addition, several approaches (Zhang et al., 2016a, 2017a; AlBadrashiny et al., 2017) attempt to incorporate hand-crafted linguistic features into a Bi-LSTMCRF to improve low-resource name tagging performance. Recent attempts on cross-lingual transfer for name tagging can be divided into two categories: the first projects annotations from a source language to a target language via parallel corpora (Yarowsky et al., 2001; Wang and Manning, 2013; Wang et al., 2013; Zhang et al., 2016b; Fang and Cohn, 2016; Ehrmann et al., 2011; Enghoff et al., 2018; Ni et al., 2017), a bilingual gazetteer (Feng et al., 2017; Zirikly and Hagiwara, 2015), Wikipedia anchor links (Kim et al., 2012; Nothman et al., 2013; Tsai et al., 2016; Pan et al., 2017), and language universal representations, including Unicode bytes (Gillick et al., 2016) and cross-lingual word embeddings (Fang and Cohn, 2017; Wang et al., 2017; Huang et al., 2018; Xie et al., 2018). The second is based on multitask learning via a weight sharing encoder (Yang et al., 2016, 2017; Lin et al., 2018). Comp"
N19-1383,N15-1104,0,0.0139277,"gs t t t N ×d t Vt = {v1 , v2 , ..., vN } ∈ R for t, and Vs = {vs1 , vs2 , ..., vsM } ∈ RM ×ds for s, where vti and vsj are the vector representations of words wit and wis from t and s, N and M denote the vocabulary sizes, dt and ds denote the embedding dimensionality of t and s respectively. We then apply a mapping function f to project s into the same semantic space as t: e s = f (Vs ) = Vs U V (1) where U ∈ Rds ×dt is the transformation matrix. e s ∈ RM ×dt are the projected word embeddings V for s, and Θf = {θf } denotes the set of parameters to be optimized for f . Similar to Xing et al. (2015), Conneau et al. (2017), and Chen and Cardie (2018), we constrain the transformation matrix U to be orthogonal with singular value decomposition (SVD) to reduce the parameter search space: e s V⊤ U = AB⊤ , with AΣB⊤ = SVD(V s) (2) To automatically optimize the mapping function f without using extra bilingual signals, we introduce a multi-layer perceptron D as a word discriminator, which takes word embeddings of t and projected word embeddings of s as input features and outputs a single scalar. D(wi∗ ) represents the probability of wi∗ coming from t. The word discriminator is trained by minimiz"
N19-1383,H01-1035,0,0.148427,"ate a character-level CNN (Ma and Hovy, 2016), global contexts (Zhang et al., 2018), or language models (Liu et al., 2018; Peters et al., 2017, 2018; Devlin et al., 2018) to improve name tagging. In addition, several approaches (Zhang et al., 2016a, 2017a; AlBadrashiny et al., 2017) attempt to incorporate hand-crafted linguistic features into a Bi-LSTMCRF to improve low-resource name tagging performance. Recent attempts on cross-lingual transfer for name tagging can be divided into two categories: the first projects annotations from a source language to a target language via parallel corpora (Yarowsky et al., 2001; Wang and Manning, 2013; Wang et al., 2013; Zhang et al., 2016b; Fang and Cohn, 2016; Ehrmann et al., 2011; Enghoff et al., 2018; Ni et al., 2017), a bilingual gazetteer (Feng et al., 2017; Zirikly and Hagiwara, 2015), Wikipedia anchor links (Kim et al., 2012; Nothman et al., 2013; Tsai et al., 2016; Pan et al., 2017), and language universal representations, including Unicode bytes (Gillick et al., 2016) and cross-lingual word embeddings (Fang and Cohn, 2017; Wang et al., 2017; Huang et al., 2018; Xie et al., 2018). The second is based on multitask learning via a weight sharing encoder (Yang"
N19-1383,I17-1037,1,0.902957,"Missing"
N19-1383,N16-1029,1,0.848295,"s from source and target languages on Amharic name tagging. 4 Related Work Name tagging methods based on sequence labeling have been widely studied in recent years. Huang et al. (2015) and Lample et al. (2016) propose an effective Bi-LSTM-CRF architecture; the BiLSTM encodes previous and following contexts, and the CRF is used for tag prediction. Other studies incorporate a character-level CNN (Ma and Hovy, 2016), global contexts (Zhang et al., 2018), or language models (Liu et al., 2018; Peters et al., 2017, 2018; Devlin et al., 2018) to improve name tagging. In addition, several approaches (Zhang et al., 2016a, 2017a; AlBadrashiny et al., 2017) attempt to incorporate hand-crafted linguistic features into a Bi-LSTMCRF to improve low-resource name tagging performance. Recent attempts on cross-lingual transfer for name tagging can be divided into two categories: the first projects annotations from a source language to a target language via parallel corpora (Yarowsky et al., 2001; Wang and Manning, 2013; Wang et al., 2013; Zhang et al., 2016b; Fang and Cohn, 2016; Ehrmann et al., 2011; Enghoff et al., 2018; Ni et al., 2017), a bilingual gazetteer (Feng et al., 2017; Zirikly and Hagiwara, 2015), Wikipe"
N19-1383,K18-1009,1,0.889982,"Missing"
N19-1383,C16-1045,1,0.862131,"s from source and target languages on Amharic name tagging. 4 Related Work Name tagging methods based on sequence labeling have been widely studied in recent years. Huang et al. (2015) and Lample et al. (2016) propose an effective Bi-LSTM-CRF architecture; the BiLSTM encodes previous and following contexts, and the CRF is used for tag prediction. Other studies incorporate a character-level CNN (Ma and Hovy, 2016), global contexts (Zhang et al., 2018), or language models (Liu et al., 2018; Peters et al., 2017, 2018; Devlin et al., 2018) to improve name tagging. In addition, several approaches (Zhang et al., 2016a, 2017a; AlBadrashiny et al., 2017) attempt to incorporate hand-crafted linguistic features into a Bi-LSTMCRF to improve low-resource name tagging performance. Recent attempts on cross-lingual transfer for name tagging can be divided into two categories: the first projects annotations from a source language to a target language via parallel corpora (Yarowsky et al., 2001; Wang and Manning, 2013; Wang et al., 2013; Zhang et al., 2016b; Fang and Cohn, 2016; Ehrmann et al., 2011; Enghoff et al., 2018; Ni et al., 2017), a bilingual gazetteer (Feng et al., 2017; Zirikly and Hagiwara, 2015), Wikipe"
N19-1383,P17-1179,0,0.068792,"Missing"
N19-1383,D17-1207,0,0.138743,"ame tagger The language-agnostic sequential features from both t and s are further fed into a context encoder to better capture and refine contextual information and a conditional random field (CRF) (Lafferty et al., 2001) based name tagger. Next we show the details of each component in our architecture. 2.2 Word-level Adversarial Transfer To better leverage the resources from the source language, our first step is to construct a shared se3824 mantic space where the words from the source and target languages are semantically aligned. Without requiring any bilingual gazetteers, recent efforts (Zhang et al., 2017b; Conneau et al., 2017; Chen and Cardie, 2018) explore unsupervised approaches to learn cross-lingual word embeddings and achieve comparable performance to supervised methods. Following these studies, we perform word-level adversarial training to automatically align word representations from s and t. Formally, assume we are given pretrained monolingual word embeddings t t t N ×d t Vt = {v1 , v2 , ..., vN } ∈ R for t, and Vs = {vs1 , vs2 , ..., vsM } ∈ RM ×ds for s, where vti and vsj are the vector representations of words wit and wis from t and s, N and M denote the vocabulary sizes, dt and d"
N19-1383,P15-2064,0,0.0685169,"eral approaches (Zhang et al., 2016a, 2017a; AlBadrashiny et al., 2017) attempt to incorporate hand-crafted linguistic features into a Bi-LSTMCRF to improve low-resource name tagging performance. Recent attempts on cross-lingual transfer for name tagging can be divided into two categories: the first projects annotations from a source language to a target language via parallel corpora (Yarowsky et al., 2001; Wang and Manning, 2013; Wang et al., 2013; Zhang et al., 2016b; Fang and Cohn, 2016; Ehrmann et al., 2011; Enghoff et al., 2018; Ni et al., 2017), a bilingual gazetteer (Feng et al., 2017; Zirikly and Hagiwara, 2015), Wikipedia anchor links (Kim et al., 2012; Nothman et al., 2013; Tsai et al., 2016; Pan et al., 2017), and language universal representations, including Unicode bytes (Gillick et al., 2016) and cross-lingual word embeddings (Fang and Cohn, 2017; Wang et al., 2017; Huang et al., 2018; Xie et al., 2018). The second is based on multitask learning via a weight sharing encoder (Yang et al., 2016, 2017; Lin et al., 2018). Compared to these studies, our approach not only automatically learns cross-lingual word embeddings without requiring any parallel resources, but also carefully extracts language-"
N19-1383,D18-1034,0,0.0714901,"otations from a source language to a target language via parallel corpora (Yarowsky et al., 2001; Wang and Manning, 2013; Wang et al., 2013; Zhang et al., 2016b; Fang and Cohn, 2016; Ehrmann et al., 2011; Enghoff et al., 2018; Ni et al., 2017), a bilingual gazetteer (Feng et al., 2017; Zirikly and Hagiwara, 2015), Wikipedia anchor links (Kim et al., 2012; Nothman et al., 2013; Tsai et al., 2016; Pan et al., 2017), and language universal representations, including Unicode bytes (Gillick et al., 2016) and cross-lingual word embeddings (Fang and Cohn, 2017; Wang et al., 2017; Huang et al., 2018; Xie et al., 2018). The second is based on multitask learning via a weight sharing encoder (Yang et al., 2016, 2017; Lin et al., 2018). Compared to these studies, our approach not only automatically learns cross-lingual word embeddings without requiring any parallel resources, but also carefully extracts language-agnostic sequential features, yielding better performance. Adversarial training has also been extensively studied and applied for cross-lingual and crossdomain transfer. Several studies (Barone, 2016; Zhang et al., 2017c,b; Conneau et al., 2017; Chen and Cardie, 2018) explore adversarial training to au"
N19-1383,C18-1037,0,0.022651,"dings without requiring any parallel resources, but also carefully extracts language-agnostic sequential features, yielding better performance. Adversarial training has also been extensively studied and applied for cross-lingual and crossdomain transfer. Several studies (Barone, 2016; Zhang et al., 2017c,b; Conneau et al., 2017; Chen and Cardie, 2018) explore adversarial training to automatically induce bilingual and multilingual word representations without using any parallel corpora or bilingual gazetteers. Adversarial training is also applied to extract languageagnostic (Chen et al., 2016; Zou et al., 2018; Wang and Pan, 2018; Kim et al., 2017a; Muis et al., 3830 2018; Cao et al., 2018) and domain-agnostic features (Kim et al., 2017b; Ganin et al., 2016; Tzeng et al., 2017; Chen et al., 2017; Li et al., 2017; Fu et al., 2017; Bousmalis et al., 2016; Shi et al., 2018) for cross-lingual and cross-domain adaptation. Compared with these methods, our approach combines both word-level and sentence-level adversarial training. 5 Conclusions and Future Work We design a new neural architecture which integrates multi-level adversarial transfer into a BiLSTM-CRF to improve low-resource name tagging. With w"
P10-1108,J97-2003,0,0.260836,"hms for application of cascades of weighted tree transducers to weighted tree acceptors, connecting formal theory with actual practice. Additionally, we present novel on-the-fly variants of these algorithms, and compare their performance on a syntax machine translation cascade based on (Yamada and Knight, 2001). 1 • explicit algorithms for application of WTT cascades • novel algorithms for on-the-fly application of WTT cascades, and • experiments comparing the performance of these algorithms. 2 Motivation Weighted finite-state transducers have found recent favor as models of natural language (Mohri, 1997). In order to make actual use of systems built with these formalisms we must first calculate the set of possible weighted outputs allowed by the transducer given some input, which we call forward application, or the set of possible weighted inputs given some output, which we call backward application. After application we can do some inference on this result, such as determining its k highest weighted elements. We may also want to divide up our problems into manageable chunks, each represented by a transducer. As noted by Woods (1980), it is easier for designers to write several small transduc"
P10-1108,P09-1108,0,0.0890181,"t graph need not be built. A graphical representation of all three methods is presented in Figure 1. 3 Application of tree transducers Now let us revisit these strategies in the setting of trees and tree transducers. Imagine we have a tree or set of trees as input that can be represented as a weighted regular tree grammar3 (WRTG) and a WTT that can transform that input with some weight. We would like to know the k-best trees the WTT can produce as output for that input, along with their weights. We already know of several methods for acquiring k-best trees from a WRTG (Huang and Chiang, 2005; Pauls and Klein, 2009), so we then must ask if, analogously to the string case, WTTs preserve recognizability4 and we can form an application WRTG. Before we begin, however, we must define WTTs and WRTGs. 3.1 Preliminaries5 A ranked alphabet is a finite set Σ such that every member σ ∈ Σ has a rank rk(σ) ∈ N. We call Σ(k) ⊆ Σ, k ∈ N the set of those σ ∈ Σ such that rk(σ) = k. The set of variables is denoted X = {x1 , x2 , . . .} and is assumed to be disjoint from any ranked alphabet used in this paper. We use ⊥ to denote a symbol of rank 0 that is not in any ranked alphabet used in this paper. A tree t ∈ TΣ is deno"
P10-1108,J80-1001,0,0.74103,"cers have found recent favor as models of natural language (Mohri, 1997). In order to make actual use of systems built with these formalisms we must first calculate the set of possible weighted outputs allowed by the transducer given some input, which we call forward application, or the set of possible weighted inputs given some output, which we call backward application. After application we can do some inference on this result, such as determining its k highest weighted elements. We may also want to divide up our problems into manageable chunks, each represented by a transducer. As noted by Woods (1980), it is easier for designers to write several small transducers where each performs a simple transformation, rather than painstakingly construct a single complicated device. We would like to know, then, the result of transformation of input or output by a cascade of transducers, one operating after the other. As we will see, there are various strategies for approaching this problem. We will consider offline composition, bucket brigade application, and on-the-fly application. Application of cascades of weighted string transducers (WSTs) has been well-studied (Mohri, Strategies for the string ca"
P10-1108,W05-1506,0,0.0396786,"in that the entire result graph need not be built. A graphical representation of all three methods is presented in Figure 1. 3 Application of tree transducers Now let us revisit these strategies in the setting of trees and tree transducers. Imagine we have a tree or set of trees as input that can be represented as a weighted regular tree grammar3 (WRTG) and a WTT that can transform that input with some weight. We would like to know the k-best trees the WTT can produce as output for that input, along with their weights. We already know of several methods for acquiring k-best trees from a WRTG (Huang and Chiang, 2005; Pauls and Klein, 2009), so we then must ask if, analogously to the string case, WTTs preserve recognizability4 and we can form an application WRTG. Before we begin, however, we must define WTTs and WRTGs. 3.1 Preliminaries5 A ranked alphabet is a finite set Σ such that every member σ ∈ Σ has a rank rk(σ) ∈ N. We call Σ(k) ⊆ Σ, k ∈ N the set of those σ ∈ Σ such that rk(σ) = k. The set of variables is denoted X = {x1 , x2 , . . .} and is assumed to be disjoint from any ranked alphabet used in this paper. We use ⊥ to denote a symbol of rank 0 that is not in any ranked alphabet used in this pape"
P10-1108,P01-1067,1,0.821707,"n this work, presenting: Weighted tree transducers have been proposed as useful formal models for representing syntactic natural language processing applications, but there has been little description of inference algorithms for these automata beyond formal foundations. We give a detailed description of algorithms for application of cascades of weighted tree transducers to weighted tree acceptors, connecting formal theory with actual practice. Additionally, we present novel on-the-fly variants of these algorithms, and compare their performance on a syntax machine translation cascade based on (Yamada and Knight, 2001). 1 • explicit algorithms for application of WTT cascades • novel algorithms for on-the-fly application of WTT cascades, and • experiments comparing the performance of these algorithms. 2 Motivation Weighted finite-state transducers have found recent favor as models of natural language (Mohri, 1997). In order to make actual use of systems built with these formalisms we must first calculate the set of possible weighted outputs allowed by the transducer given some input, which we call forward application, or the set of possible weighted inputs given some output, which we call backward applicatio"
P13-1139,W12-3102,0,0.10768,"Missing"
P13-1139,W12-3101,0,0.698226,"heuristic accurately judges the systems to be equivalently good. Suppose however that we had duplicated B and had submitted it to the competition a second time as system C. Since B and C produce identical translations, they should always tie with one another. The expected value of O RIG WMT(A) would not change, but the expected value of O RIG WMT(B) would increase to 5/6, buoyed by its ties with system C. This vulnerability prompted (Bojar et al., 2011) to offer the following revision: B OJAR(s) = win(s) win(s) + loss(s) The following year, it was B OJAR’s turn to be criticized, this time by (Lopez, 2012): Superficially, this appears to be an improvement....couldn’t a system still be penalized simply by being compared to [good systems] more frequently than its competitors? On the other hand, couldn’t a system be rewarded simply by being compared against a bad system more frequently than its competitors? Lopez’s concern, while reasonable, is less obviously damning than (Bojar et al., 2011)’s criticism of O RIG WMT. It depends on whether the collected set of comparisons is small enough or biased enough to make the variance in competition significant. While this hypothesis is plausible, Lopez mak"
P13-1139,W11-2101,0,0.387287,"Missing"
P13-1139,W10-1703,0,0.0498106,"same source text. Ties are permitted. In Table 1, we show an example2 where a judge (we’ll call him “jdoe”) has ranked five translations of the French sentence “Il ne va pas.” Each such elicitation encodes ten pairwise comparisons, as shown in Table 2. For each competition track, WMT typically elicits between 5000 and 20000 comparisons. Once the elicitation process is complete, WMT faces a large database of comparisons and a question that must be answered: whose system is the best? 1 system bbn uedin jhu cmu kit Although in recent competitions, some of the judging has also been crowdsourced (Callison-Burch et al., 2010). 2 The example does not use actual system output. sys1 bbn bbn bbn bbn cmu cmu cmu jhu jhu kit sys2 cmu jhu kit uedin jhu kit uedin kit uedin uedin judge jdoe jdoe jdoe jdoe jdoe jdoe jdoe jdoe jdoe jdoe preference 1 1 1 1 2 1 2 1 0 2 Table 2: Pairwise comparisons encoded by Table 1. A preference of 0 means neither translation was preferred. Otherwise the preference specifies the preferred system. 2 A Ranking Problem For several years, WMT used the following heuristic for ranking the translation systems: O RIG WMT(s) = win(s) + tie(s) win(s) + tie(s) + loss(s) For system s, win(s) is the numb"
P17-1178,N15-1107,0,0.0299644,"s through cross-lingual topic transfer. For the first time, we propose to customize name annotations for specific downstream applications. Again, we use a cross-lingual knowledge transfer strategy to leverage the widely available English corpora to choose entities with specific Wikipedia topic categories (Section 2.5). Derive morphology analysis from Wikipedia markups. Another unique challenge for morphologically rich languages is to segment each token into its stemming form and affixes. Previous methods relied on either high-cost supervised learning (Roth et al., 2008; Mahmoudi et al., 2013; Ahlberg et al., 2015), or low-quality unsupervised learning (Gr¨onroos et al., 2014; Ruokolainen et al., 2016). We exploit Wikipedia markups to automatically learn affixes as language-specific features (Section 2.3). Mine word translations from cross-lingual links. Name translation is a crucial step to generate candidate entities in cross-lingual entity linking. Only a small percentage of names can be directly translated by matching against cross-lingual Wikipedia title pairs. Based on the observation that Wikipedia titles within any language tend to follow a consistent style and format, we propose an effective me"
P17-1178,C12-2005,0,0.0398197,"Missing"
P17-1178,R13-1005,0,0.0213967,"hods required labeled data and name transliteration. We share the same goal as (Sil and Florian, 2016) to extend cross-lingual entity linking to all languages in Wikipedia. They exploited Wikipedia links to train a supervised linker. We mine reliable word translations from cross-lingual Wikipedia titles, which enables us to adopt unsupervised English entity linking techniques such as (Pan et al., 2015) to directly link translated English name mentions to English KB. Efforts to save annotation cost for name tagging: Some previous work including (Ji and Grishman, 2006; Richman and Schone, 2008; Althobaiti et al., 2013) exploited semi-supervised methods to save annotation cost. We observed that self-training can provide further gains when the training data contains certain amount of noise. 6 Conclusions and Future Work We developed a simple yet effective framework that can extract names from 282 languages and link them to an English KB. This framework follows a fully automatic training and testing pipeline, without the needs of any manual annotations or knowledge from native speakers. We evaluated our framework on both Wikipedia articles and external formal and informal texts and obtained promising results."
P17-1178,E14-3012,0,0.0322345,"Missing"
P17-1178,P03-2031,0,0.129469,"t handle unknown names. We developed a new method to derive generalizable affixes for morphologically rich language based on Wikipedia markups. Wikipedia as background features for IE: Wikipedia pages have been used as additional features to improve various Information Extraction (IE) tasks, including name tagging (Kazama and Torisawa, 2007), coreference resolution (Paolo Ponzetto and Strube, 2006), relation extraction (Chan and Roth, 2010) and event extraction (Hogue et al., 2014). Other automatic name annotation generation methods have been proposed, including KB driven distant supervision (An et al., 2003; Mintz et al., 2009; Ren et al., 2015) and cross-lingual projection (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013; Wang and Manning, 2014; Zhang et al., 2016b). Multi-lingual name tagging: Some recent research (Zhang et al., 2016a; Littell et al., 2016; Tsai et al., 2016) under the DARPA LORELEI program focused on developing name tagging techniques for low-resource languages. These approaches require English annotations for projection (Tsai et al., 2016), some input from a native speaker, either through manual annotations (Littell et al., 2016), or a linguistic surve"
P17-1178,W13-2322,1,0.762364,"omputational Linguistics, pages 1946–1958 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1178 tr/Çin_K en/Comm 282 languages, and link them to an English KB (Wikipedia in this work). The major challenges and our new solutions are summarized as follows. Creating “Silver-standard” through crosslingual entity transfer. The first step is to classify English Wikipedia entries into certain entity types and then propagate these labels to other languages. We exploit the English Abstract Meaning Representation (AMR) corpus (Banarescu et al., 2013) which includes both name tagging and linking annotations for fine-grained entity types to train an automatic classifier. Furthermore, we exploit each entry’s properties in DBpedia as features and thus eliminate the need of language-specific features and resources such as part-of-speech tagging as in previous work (Section 2.2). Refine annotations through self-training. The initial annotations obtained from above are too incomplete and inconsistent. Previous work used name string match to propagate labels. In contrast, we apply self-training to label other mentions without links in Wikipedia a"
P17-1178,C10-1018,0,0.0173,"advantage of richer structures in the KBs. Previous work on Arabic name tagging (Althobaiti et al., 2014) extracted entity titles as a gazetteer for stemming, and thus it cannot handle unknown names. We developed a new method to derive generalizable affixes for morphologically rich language based on Wikipedia markups. Wikipedia as background features for IE: Wikipedia pages have been used as additional features to improve various Information Extraction (IE) tasks, including name tagging (Kazama and Torisawa, 2007), coreference resolution (Paolo Ponzetto and Strube, 2006), relation extraction (Chan and Roth, 2010) and event extraction (Hogue et al., 2014). Other automatic name annotation generation methods have been proposed, including KB driven distant supervision (An et al., 2003; Mintz et al., 2009; Ren et al., 2015) and cross-lingual projection (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013; Wang and Manning, 2014; Zhang et al., 2016b). Multi-lingual name tagging: Some recent research (Zhang et al., 2016a; Littell et al., 2016; Tsai et al., 2016) under the DARPA LORELEI program focused on developing name tagging techniques for low-resource languages. These approaches requir"
P17-1178,N13-1006,0,0.0254816,"d on Wikipedia markups. Wikipedia as background features for IE: Wikipedia pages have been used as additional features to improve various Information Extraction (IE) tasks, including name tagging (Kazama and Torisawa, 2007), coreference resolution (Paolo Ponzetto and Strube, 2006), relation extraction (Chan and Roth, 2010) and event extraction (Hogue et al., 2014). Other automatic name annotation generation methods have been proposed, including KB driven distant supervision (An et al., 2003; Mintz et al., 2009; Ren et al., 2015) and cross-lingual projection (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013; Wang and Manning, 2014; Zhang et al., 2016b). Multi-lingual name tagging: Some recent research (Zhang et al., 2016a; Littell et al., 2016; Tsai et al., 2016) under the DARPA LORELEI program focused on developing name tagging techniques for low-resource languages. These approaches require English annotations for projection (Tsai et al., 2016), some input from a native speaker, either through manual annotations (Littell et al., 2016), or a linguistic survey (Zhang et al., 2016a). Without using any manual annotations, our name taggers outperform previous methods on the same d"
P17-1178,I08-1071,0,0.089043,"Missing"
P17-1178,D15-1103,0,0.0422904,"Missing"
P17-1178,W10-2415,0,0.0132221,"ired from previous work that leveraged Wikipedia markups to train name taggers (Nothman et al., 2008; Dakka and Cucerzan, 2008; Mika et al., 2008; Ringland et al., 2009; Alotaibi and Lee, 2012; Nothman et al., 2013; Althobaiti et al., 2014). Most of these previous methods manually classified many English Wikipedia entries into pre-defined entity types. In contrast, our approach doesn’t need any manual annotations or language-specific features, while generates both coarse-grained and fine-grained types. Many fine-grained entity typing approaches (Fleischman and Hovy, 2002; Giuliano, 1953 2009; Ekbal et al., 2010; Ling and Weld, 2012; Yosef et al., 2012; Nakashole et al., 2013; Gillick et al., 2014; Yogatama et al., 2015; Del Corro et al., 2015) also created annotations based on Wikipedia anchor links. Our framework performs both name identification and typing and takes advantage of richer structures in the KBs. Previous work on Arabic name tagging (Althobaiti et al., 2014) extracted entity titles as a gazetteer for stemming, and thus it cannot handle unknown names. We developed a new method to derive generalizable affixes for morphologically rich language based on Wikipedia markups. Wikipedia as back"
P17-1178,C02-1130,0,0.0349275,"er standard generation: Our work was mainly inspired from previous work that leveraged Wikipedia markups to train name taggers (Nothman et al., 2008; Dakka and Cucerzan, 2008; Mika et al., 2008; Ringland et al., 2009; Alotaibi and Lee, 2012; Nothman et al., 2013; Althobaiti et al., 2014). Most of these previous methods manually classified many English Wikipedia entries into pre-defined entity types. In contrast, our approach doesn’t need any manual annotations or language-specific features, while generates both coarse-grained and fine-grained types. Many fine-grained entity typing approaches (Fleischman and Hovy, 2002; Giuliano, 1953 2009; Ekbal et al., 2010; Ling and Weld, 2012; Yosef et al., 2012; Nakashole et al., 2013; Gillick et al., 2014; Yogatama et al., 2015; Del Corro et al., 2015) also created annotations based on Wikipedia anchor links. Our framework performs both name identification and typing and takes advantage of richer structures in the KBs. Previous work on Arabic name tagging (Althobaiti et al., 2014) extracted entity titles as a gazetteer for stemming, and thus it cannot handle unknown names. We developed a new method to derive generalizable affixes for morphologically rich language base"
P17-1178,W09-1125,0,0.0774484,"Missing"
P17-1178,C14-1111,0,0.0173537,"Missing"
P17-1178,D14-1012,0,0.0209447,"Missing"
P17-1178,U14-1006,1,0.83456,"Previous work on Arabic name tagging (Althobaiti et al., 2014) extracted entity titles as a gazetteer for stemming, and thus it cannot handle unknown names. We developed a new method to derive generalizable affixes for morphologically rich language based on Wikipedia markups. Wikipedia as background features for IE: Wikipedia pages have been used as additional features to improve various Information Extraction (IE) tasks, including name tagging (Kazama and Torisawa, 2007), coreference resolution (Paolo Ponzetto and Strube, 2006), relation extraction (Chan and Roth, 2010) and event extraction (Hogue et al., 2014). Other automatic name annotation generation methods have been proposed, including KB driven distant supervision (An et al., 2003; Mintz et al., 2009; Ren et al., 2015) and cross-lingual projection (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013; Wang and Manning, 2014; Zhang et al., 2016b). Multi-lingual name tagging: Some recent research (Zhang et al., 2016a; Littell et al., 2016; Tsai et al., 2016) under the DARPA LORELEI program focused on developing name tagging techniques for low-resource languages. These approaches require English annotations for projection (Tsai"
P17-1178,P06-2055,1,0.710096,"2011) extended it to 21 languages. But their methods required labeled data and name transliteration. We share the same goal as (Sil and Florian, 2016) to extend cross-lingual entity linking to all languages in Wikipedia. They exploited Wikipedia links to train a supervised linker. We mine reliable word translations from cross-lingual Wikipedia titles, which enables us to adopt unsupervised English entity linking techniques such as (Pan et al., 2015) to directly link translated English name mentions to English KB. Efforts to save annotation cost for name tagging: Some previous work including (Ji and Grishman, 2006; Richman and Schone, 2008; Althobaiti et al., 2013) exploited semi-supervised methods to save annotation cost. We observed that self-training can provide further gains when the training data contains certain amount of noise. 6 Conclusions and Future Work We developed a simple yet effective framework that can extract names from 282 languages and link them to an English KB. This framework follows a fully automatic training and testing pipeline, without the needs of any manual annotations or knowledge from native speakers. We evaluated our framework on both Wikipedia articles and external formal"
P17-1178,J03-1002,0,0.0210511,"Missing"
P17-1178,D07-1073,0,0.0495625,"ions based on Wikipedia anchor links. Our framework performs both name identification and typing and takes advantage of richer structures in the KBs. Previous work on Arabic name tagging (Althobaiti et al., 2014) extracted entity titles as a gazetteer for stemming, and thus it cannot handle unknown names. We developed a new method to derive generalizable affixes for morphologically rich language based on Wikipedia markups. Wikipedia as background features for IE: Wikipedia pages have been used as additional features to improve various Information Extraction (IE) tasks, including name tagging (Kazama and Torisawa, 2007), coreference resolution (Paolo Ponzetto and Strube, 2006), relation extraction (Chan and Roth, 2010) and event extraction (Hogue et al., 2014). Other automatic name annotation generation methods have been proposed, including KB driven distant supervision (An et al., 2003; Mintz et al., 2009; Ren et al., 2015) and cross-lingual projection (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013; Wang and Manning, 2014; Zhang et al., 2016b). Multi-lingual name tagging: Some recent research (Zhang et al., 2016a; Littell et al., 2016; Tsai et al., 2016) under the DARPA LORELEI prog"
P17-1178,P12-1073,0,0.103702,"rich language based on Wikipedia markups. Wikipedia as background features for IE: Wikipedia pages have been used as additional features to improve various Information Extraction (IE) tasks, including name tagging (Kazama and Torisawa, 2007), coreference resolution (Paolo Ponzetto and Strube, 2006), relation extraction (Chan and Roth, 2010) and event extraction (Hogue et al., 2014). Other automatic name annotation generation methods have been proposed, including KB driven distant supervision (An et al., 2003; Mintz et al., 2009; Ren et al., 2015) and cross-lingual projection (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013; Wang and Manning, 2014; Zhang et al., 2016b). Multi-lingual name tagging: Some recent research (Zhang et al., 2016a; Littell et al., 2016; Tsai et al., 2016) under the DARPA LORELEI program focused on developing name tagging techniques for low-resource languages. These approaches require English annotations for projection (Tsai et al., 2016), some input from a native speaker, either through manual annotations (Littell et al., 2016), or a linguistic survey (Zhang et al., 2016a). Without using any manual annotations, our name taggers outperform previous met"
P17-1178,N16-1030,0,0.0776175,"Missing"
P17-1178,C16-1095,0,0.0821433,"Missing"
P17-1178,R13-1053,0,0.0239555,"). Customize annotations through cross-lingual topic transfer. For the first time, we propose to customize name annotations for specific downstream applications. Again, we use a cross-lingual knowledge transfer strategy to leverage the widely available English corpora to choose entities with specific Wikipedia topic categories (Section 2.5). Derive morphology analysis from Wikipedia markups. Another unique challenge for morphologically rich languages is to segment each token into its stemming form and affixes. Previous methods relied on either high-cost supervised learning (Roth et al., 2008; Mahmoudi et al., 2013; Ahlberg et al., 2015), or low-quality unsupervised learning (Gr¨onroos et al., 2014; Ruokolainen et al., 2016). We exploit Wikipedia markups to automatically learn affixes as language-specific features (Section 2.3). Mine word translations from cross-lingual links. Name translation is a crucial step to generate candidate entities in cross-lingual entity linking. Only a small percentage of names can be directly translated by matching against cross-lingual Wikipedia title pairs. Based on the observation that Wikipedia titles within any language tend to follow a consistent style and format, we"
P17-1178,P14-5010,0,0.0041107,"Missing"
P17-1178,I11-1029,0,0.137923,"6) under the DARPA LORELEI program focused on developing name tagging techniques for low-resource languages. These approaches require English annotations for projection (Tsai et al., 2016), some input from a native speaker, either through manual annotations (Littell et al., 2016), or a linguistic survey (Zhang et al., 2016a). Without using any manual annotations, our name taggers outperform previous methods on the same data sets for many languages. Multi-lingual entity linking: NIST TAC-KBP Tri-lingual entity linking (Ji et al., 2016) focused on three languages: English, Chinese and Spanish. (McNamee et al., 2011) extended it to 21 languages. But their methods required labeled data and name transliteration. We share the same goal as (Sil and Florian, 2016) to extend cross-lingual entity linking to all languages in Wikipedia. They exploited Wikipedia links to train a supervised linker. We mine reliable word translations from cross-lingual Wikipedia titles, which enables us to adopt unsupervised English entity linking techniques such as (Pan et al., 2015) to directly link translated English name mentions to English KB. Efforts to save annotation cost for name tagging: Some previous work including (Ji and"
P17-1178,P09-1113,0,0.0368323,"names. We developed a new method to derive generalizable affixes for morphologically rich language based on Wikipedia markups. Wikipedia as background features for IE: Wikipedia pages have been used as additional features to improve various Information Extraction (IE) tasks, including name tagging (Kazama and Torisawa, 2007), coreference resolution (Paolo Ponzetto and Strube, 2006), relation extraction (Chan and Roth, 2010) and event extraction (Hogue et al., 2014). Other automatic name annotation generation methods have been proposed, including KB driven distant supervision (An et al., 2003; Mintz et al., 2009; Ren et al., 2015) and cross-lingual projection (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013; Wang and Manning, 2014; Zhang et al., 2016b). Multi-lingual name tagging: Some recent research (Zhang et al., 2016a; Littell et al., 2016; Tsai et al., 2016) under the DARPA LORELEI program focused on developing name tagging techniques for low-resource languages. These approaches require English annotations for projection (Tsai et al., 2016), some input from a native speaker, either through manual annotations (Littell et al., 2016), or a linguistic survey (Zhang et al., 201"
P17-1178,P13-1146,0,0.00785015,"rain name taggers (Nothman et al., 2008; Dakka and Cucerzan, 2008; Mika et al., 2008; Ringland et al., 2009; Alotaibi and Lee, 2012; Nothman et al., 2013; Althobaiti et al., 2014). Most of these previous methods manually classified many English Wikipedia entries into pre-defined entity types. In contrast, our approach doesn’t need any manual annotations or language-specific features, while generates both coarse-grained and fine-grained types. Many fine-grained entity typing approaches (Fleischman and Hovy, 2002; Giuliano, 1953 2009; Ekbal et al., 2010; Ling and Weld, 2012; Yosef et al., 2012; Nakashole et al., 2013; Gillick et al., 2014; Yogatama et al., 2015; Del Corro et al., 2015) also created annotations based on Wikipedia anchor links. Our framework performs both name identification and typing and takes advantage of richer structures in the KBs. Previous work on Arabic name tagging (Althobaiti et al., 2014) extracted entity titles as a gazetteer for stemming, and thus it cannot handle unknown names. We developed a new method to derive generalizable affixes for morphologically rich language based on Wikipedia markups. Wikipedia as background features for IE: Wikipedia pages have been used as additio"
P17-1178,U08-1016,1,0.909684,"Missing"
P17-1178,N15-1119,1,0.926167,"in English KB Salience, Similarity and Coherence Comparison Linking Translated and Linked Mentions m1 t1 e1 m2 t2 e1 m3 t3 e2 m4 t4 e3 m5 t5 NIL m6 t6 NIL Figure 3: Cross-lingual Entity Linking Overview 3.2 Name Translation The cross-lingual Wikipedia title pairs, generated through crowd-sourcing, generally follow a consistent style and format in each language. From Table 2 we can see that the order of modifier and head word keeps consistent in Turkish and English titles. 8 http://www.darpa.mil/program/low-resource-languagesfor-emergent-incidents 1949 to the KB, similar to our previous work (Pan et al., 2015). The only difference is that we construct knowledge networks (KNs) g(ti ) for T based on their co-occurrence within a context window 9 instead of their AMR relations, because AMR parsing is not available for foreign languages. For each translated name mention ti , an initial list of candidate entities E(ti ) = {e1 , e2 , ..., ek } is generated based on a surface form dictionary mined from KB properties (e.g., redirects, names, aliases). If no surface form can be matched then we determine the mention as unlinkable. Then we construct KNs g(ej ) for each entity candidate ej in ti ’s entity candi"
P17-1178,N06-1025,0,0.0111175,"ms both name identification and typing and takes advantage of richer structures in the KBs. Previous work on Arabic name tagging (Althobaiti et al., 2014) extracted entity titles as a gazetteer for stemming, and thus it cannot handle unknown names. We developed a new method to derive generalizable affixes for morphologically rich language based on Wikipedia markups. Wikipedia as background features for IE: Wikipedia pages have been used as additional features to improve various Information Extraction (IE) tasks, including name tagging (Kazama and Torisawa, 2007), coreference resolution (Paolo Ponzetto and Strube, 2006), relation extraction (Chan and Roth, 2010) and event extraction (Hogue et al., 2014). Other automatic name annotation generation methods have been proposed, including KB driven distant supervision (An et al., 2003; Mintz et al., 2009; Ren et al., 2015) and cross-lingual projection (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013; Wang and Manning, 2014; Zhang et al., 2016b). Multi-lingual name tagging: Some recent research (Zhang et al., 2016a; Littell et al., 2016; Tsai et al., 2016) under the DARPA LORELEI program focused on developing name tagging techniques for low-"
P17-1178,P08-1001,0,0.0334855,"1 languages. But their methods required labeled data and name transliteration. We share the same goal as (Sil and Florian, 2016) to extend cross-lingual entity linking to all languages in Wikipedia. They exploited Wikipedia links to train a supervised linker. We mine reliable word translations from cross-lingual Wikipedia titles, which enables us to adopt unsupervised English entity linking techniques such as (Pan et al., 2015) to directly link translated English name mentions to English KB. Efforts to save annotation cost for name tagging: Some previous work including (Ji and Grishman, 2006; Richman and Schone, 2008; Althobaiti et al., 2013) exploited semi-supervised methods to save annotation cost. We observed that self-training can provide further gains when the training data contains certain amount of noise. 6 Conclusions and Future Work We developed a simple yet effective framework that can extract names from 282 languages and link them to an English KB. This framework follows a fully automatic training and testing pipeline, without the needs of any manual annotations or knowledge from native speakers. We evaluated our framework on both Wikipedia articles and external formal and informal texts and ob"
P17-1178,U09-1004,1,0.913527,"Missing"
P17-1178,P08-2030,0,0.0100737,"ntions (Section 2.4). Customize annotations through cross-lingual topic transfer. For the first time, we propose to customize name annotations for specific downstream applications. Again, we use a cross-lingual knowledge transfer strategy to leverage the widely available English corpora to choose entities with specific Wikipedia topic categories (Section 2.5). Derive morphology analysis from Wikipedia markups. Another unique challenge for morphologically rich languages is to segment each token into its stemming form and affixes. Previous methods relied on either high-cost supervised learning (Roth et al., 2008; Mahmoudi et al., 2013; Ahlberg et al., 2015), or low-quality unsupervised learning (Gr¨onroos et al., 2014; Ruokolainen et al., 2016). We exploit Wikipedia markups to automatically learn affixes as language-specific features (Section 2.3). Mine word translations from cross-lingual links. Name translation is a crucial step to generate candidate entities in cross-lingual entity linking. Only a small percentage of names can be directly translated by matching against cross-lingual Wikipedia title pairs. Based on the observation that Wikipedia titles within any language tend to follow a consisten"
P17-1178,P16-1213,0,0.0247317,"notations for projection (Tsai et al., 2016), some input from a native speaker, either through manual annotations (Littell et al., 2016), or a linguistic survey (Zhang et al., 2016a). Without using any manual annotations, our name taggers outperform previous methods on the same data sets for many languages. Multi-lingual entity linking: NIST TAC-KBP Tri-lingual entity linking (Ji et al., 2016) focused on three languages: English, Chinese and Spanish. (McNamee et al., 2011) extended it to 21 languages. But their methods required labeled data and name transliteration. We share the same goal as (Sil and Florian, 2016) to extend cross-lingual entity linking to all languages in Wikipedia. They exploited Wikipedia links to train a supervised linker. We mine reliable word translations from cross-lingual Wikipedia titles, which enables us to adopt unsupervised English entity linking techniques such as (Pan et al., 2015) to directly link translated English name mentions to English KB. Efforts to save annotation cost for name tagging: Some previous work including (Ji and Grishman, 2006; Richman and Schone, 2008; Althobaiti et al., 2013) exploited semi-supervised methods to save annotation cost. We observed that s"
P17-1178,K16-1022,0,0.117852,"luding name tagging (Kazama and Torisawa, 2007), coreference resolution (Paolo Ponzetto and Strube, 2006), relation extraction (Chan and Roth, 2010) and event extraction (Hogue et al., 2014). Other automatic name annotation generation methods have been proposed, including KB driven distant supervision (An et al., 2003; Mintz et al., 2009; Ren et al., 2015) and cross-lingual projection (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013; Wang and Manning, 2014; Zhang et al., 2016b). Multi-lingual name tagging: Some recent research (Zhang et al., 2016a; Littell et al., 2016; Tsai et al., 2016) under the DARPA LORELEI program focused on developing name tagging techniques for low-resource languages. These approaches require English annotations for projection (Tsai et al., 2016), some input from a native speaker, either through manual annotations (Littell et al., 2016), or a linguistic survey (Zhang et al., 2016a). Without using any manual annotations, our name taggers outperform previous methods on the same data sets for many languages. Multi-lingual entity linking: NIST TAC-KBP Tri-lingual entity linking (Ji et al., 2016) focused on three languages: English, Chinese and Spanish. (Mc"
P17-1178,P13-1106,0,0.0824147,"kups. Wikipedia as background features for IE: Wikipedia pages have been used as additional features to improve various Information Extraction (IE) tasks, including name tagging (Kazama and Torisawa, 2007), coreference resolution (Paolo Ponzetto and Strube, 2006), relation extraction (Chan and Roth, 2010) and event extraction (Hogue et al., 2014). Other automatic name annotation generation methods have been proposed, including KB driven distant supervision (An et al., 2003; Mintz et al., 2009; Ren et al., 2015) and cross-lingual projection (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013; Wang and Manning, 2014; Zhang et al., 2016b). Multi-lingual name tagging: Some recent research (Zhang et al., 2016a; Littell et al., 2016; Tsai et al., 2016) under the DARPA LORELEI program focused on developing name tagging techniques for low-resource languages. These approaches require English annotations for projection (Tsai et al., 2016), some input from a native speaker, either through manual annotations (Littell et al., 2016), or a linguistic survey (Zhang et al., 2016a). Without using any manual annotations, our name taggers outperform previous methods on the same data sets for many l"
P17-1178,Q14-1005,0,0.016415,"background features for IE: Wikipedia pages have been used as additional features to improve various Information Extraction (IE) tasks, including name tagging (Kazama and Torisawa, 2007), coreference resolution (Paolo Ponzetto and Strube, 2006), relation extraction (Chan and Roth, 2010) and event extraction (Hogue et al., 2014). Other automatic name annotation generation methods have been proposed, including KB driven distant supervision (An et al., 2003; Mintz et al., 2009; Ren et al., 2015) and cross-lingual projection (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013; Wang and Manning, 2014; Zhang et al., 2016b). Multi-lingual name tagging: Some recent research (Zhang et al., 2016a; Littell et al., 2016; Tsai et al., 2016) under the DARPA LORELEI program focused on developing name tagging techniques for low-resource languages. These approaches require English annotations for projection (Tsai et al., 2016), some input from a native speaker, either through manual annotations (Littell et al., 2016), or a linguistic survey (Zhang et al., 2016a). Without using any manual annotations, our name taggers outperform previous methods on the same data sets for many languages. Multi-lingual"
P17-1178,J90-1003,0,0.414992,"Missing"
P17-1178,P15-2048,0,0.0158903,"and Cucerzan, 2008; Mika et al., 2008; Ringland et al., 2009; Alotaibi and Lee, 2012; Nothman et al., 2013; Althobaiti et al., 2014). Most of these previous methods manually classified many English Wikipedia entries into pre-defined entity types. In contrast, our approach doesn’t need any manual annotations or language-specific features, while generates both coarse-grained and fine-grained types. Many fine-grained entity typing approaches (Fleischman and Hovy, 2002; Giuliano, 1953 2009; Ekbal et al., 2010; Ling and Weld, 2012; Yosef et al., 2012; Nakashole et al., 2013; Gillick et al., 2014; Yogatama et al., 2015; Del Corro et al., 2015) also created annotations based on Wikipedia anchor links. Our framework performs both name identification and typing and takes advantage of richer structures in the KBs. Previous work on Arabic name tagging (Althobaiti et al., 2014) extracted entity titles as a gazetteer for stemming, and thus it cannot handle unknown names. We developed a new method to derive generalizable affixes for morphologically rich language based on Wikipedia markups. Wikipedia as background features for IE: Wikipedia pages have been used as additional features to improve various Information E"
P17-1178,C12-2133,0,0.0297288,"Missing"
P17-1178,N16-1029,1,0.814483,"IE: Wikipedia pages have been used as additional features to improve various Information Extraction (IE) tasks, including name tagging (Kazama and Torisawa, 2007), coreference resolution (Paolo Ponzetto and Strube, 2006), relation extraction (Chan and Roth, 2010) and event extraction (Hogue et al., 2014). Other automatic name annotation generation methods have been proposed, including KB driven distant supervision (An et al., 2003; Mintz et al., 2009; Ren et al., 2015) and cross-lingual projection (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013; Wang and Manning, 2014; Zhang et al., 2016b). Multi-lingual name tagging: Some recent research (Zhang et al., 2016a; Littell et al., 2016; Tsai et al., 2016) under the DARPA LORELEI program focused on developing name tagging techniques for low-resource languages. These approaches require English annotations for projection (Tsai et al., 2016), some input from a native speaker, either through manual annotations (Littell et al., 2016), or a linguistic survey (Zhang et al., 2016a). Without using any manual annotations, our name taggers outperform previous methods on the same data sets for many languages. Multi-lingual entity linking: NIST"
P17-1178,C16-1045,1,0.911502,"Missing"
P18-4003,P07-1015,0,0.0477465,"in 21 sample languages are available on the demo start page and more are accessible as ranTable 6: Romanization with alternatives Table 7 includes examples of the Romanization rules in uroman, including n-to-m mappings. 2.6 Caching uroman caches token romanizations for speed. 16 useful in cross-lingual information retrieval. There is a body of work mapping text to phonetic representations. Deri and Knight (2016) use Wiktionary and Wikipedia resources to learn text-to-phoneme mappings. Phonetic representations are used in a number of end-to-end transliteration systems (Knight and Graehl, 1998; Yoon et al., 2007). Qian et al. (2010) describe the toolkit ScriptTranscriber, which extracts crosslingual transliteration pairs from comparable corpora. A core component of ScriptTranscriber maps text to an ASCII variant of the International Phonetic Alphabet (IPA). Andy Hu’s transliterator8 is a fairly universal romanizer in JavaScript, limited to romanizing one Unicode character at a time, without context. dom texts. After picking the first random text, additional random texts will be available from three corpora to choose from (small, large, and Wikipedia articles about the US). Users can then restrict the"
P18-4003,P16-1038,1,0.837726,"and script of their choice, optionally specify a language code, and then have uroman romanize the text. Additionally, the demo page includes sample texts in 290 languages in a wide variety of scripts. Texts in 21 sample languages are available on the demo start page and more are accessible as ranTable 6: Romanization with alternatives Table 7 includes examples of the Romanization rules in uroman, including n-to-m mappings. 2.6 Caching uroman caches token romanizations for speed. 16 useful in cross-lingual information retrieval. There is a body of work mapping text to phonetic representations. Deri and Knight (2016) use Wiktionary and Wikipedia resources to learn text-to-phoneme mappings. Phonetic representations are used in a number of end-to-end transliteration systems (Knight and Graehl, 1998; Yoon et al., 2007). Qian et al. (2010) describe the toolkit ScriptTranscriber, which extracts crosslingual transliteration pairs from comparable corpora. A core component of ScriptTranscriber maps text to an ASCII variant of the International Phonetic Alphabet (IPA). Andy Hu’s transliterator8 is a fairly universal romanizer in JavaScript, limited to romanizing one Unicode character at a time, without context. do"
P18-4003,P18-4011,1,0.852386,"ation of string-similarity metrics to texts from different scripts without the need and complexity of an intermediate phonetic representation. The tool is freely and publicly available as a Perl script suitable for inclusion in data processing pipelines and as an interactive demo web page. 1 Table 1: Example of Hindi and Urdu romanization Foreign scripts also present a massive cognitive barrier to humans who are not familiar with them. We devised a utility that allows people to translate text from languages they don’t know, using the same information available to a machine translation system (Hermjakob et al., 2018). We found that when we asked native English speakers to use this utility to translate text from languages such as Uyghur or Bengali to English, they strongly preferred working on the romanized version of the source language compared to its original form and indeed found using the native, unfamiliar script to be a nearly impossible task. Introduction String similarity is a useful feature in many natural language processing tasks. In machine translation, it can be used to improve the alignment of bitexts, and for low-resource languages with a related language of larger resources, it can help to"
P18-4003,W17-1414,0,0.01822,"shown when a user hovers over it). The romanization of the output at the demo site is mouse sensitive. Hovering over characters of either the original or romanized text, the page will highlight corresponding characters. See Figure 1 for an example. Hovering over the original text will also display additional information such as the Unicode name and any numeric value. To support this interactive demo site, the uroman package also includes fonts for Burmese, Tifinagh, Klingon, and Egyptian hieroglyphs, as they are sometimes missing from standard browser font packages. 4 5.2 Ji et al. (2017) and Mayfield et al. (2017) use uroman for named entity recognition. Mayhew et al. (2016) use uroman for (end-to-end) transliteration. Cheung et al. (2017) use uroman for machine translation of low-resource languages. uroman has also been used in our aforementioned translation utility (Hermjakob et al., 2018), where humans translate text to another language, with computer support, with high fluency in the target language (English), but no prior knowledge of the source language. uroman has been partially ported by third parties to Python and Java.9 Limitations and Future Work The current version of uroman has a few limit"
P18-4003,qian-etal-2010-python,0,0.0247378,"es are available on the demo start page and more are accessible as ranTable 6: Romanization with alternatives Table 7 includes examples of the Romanization rules in uroman, including n-to-m mappings. 2.6 Caching uroman caches token romanizations for speed. 16 useful in cross-lingual information retrieval. There is a body of work mapping text to phonetic representations. Deri and Knight (2016) use Wiktionary and Wikipedia resources to learn text-to-phoneme mappings. Phonetic representations are used in a number of end-to-end transliteration systems (Knight and Graehl, 1998; Yoon et al., 2007). Qian et al. (2010) describe the toolkit ScriptTranscriber, which extracts crosslingual transliteration pairs from comparable corpora. A core component of ScriptTranscriber maps text to an ASCII variant of the International Phonetic Alphabet (IPA). Andy Hu’s transliterator8 is a fairly universal romanizer in JavaScript, limited to romanizing one Unicode character at a time, without context. dom texts. After picking the first random text, additional random texts will be available from three corpora to choose from (small, large, and Wikipedia articles about the US). Users can then restrict the randomness in a spec"
P18-4011,P18-4003,1,0.74235,"examples for five Hungarian affixes and two Tagalog function words. Table 2: Texts in Uyghur, Amharic and Tibetan. Table 3: Grammar entries for Hungarian, Tagalog. We found that when we asked native English speakers to use the Chinese Room to translate text from languages such as Uyghur or Bengali to English, they strongly preferred working on a romanized version of the source language compared to its original form and indeed found using the native, unfamiliar script to be a nearly impossible task. By default, we therefore romanize non-Latinscript text, using the universal romanizer uroman2 (Hermjakob et al., 2018). The Chinese Room Editor includes the option to display the original text or both the original and romanized source text. The Uyghur text in Table 2 is romanized as yaponie fukushima 1-yadro elektir istansisining toet genratorlar guruppisi which facilitates the recognition of cognates. The grammar files have been built manually, typically drawing on external resources such as Wiktionary.3 The size is language specific, ranging from a few dozen entries to several hundred entries for extremely suffix-rich Hungarian. 2 2.7 Process Figure 1 provides an overview of the Chinese Room process. Given"
P18-4011,N10-1078,0,0.02333,"English describing earthquakes and disaster relief efforts. However, we had no parallel data dealing with this topic, and our use of an unrelated test set (see Figure 3) to estimate overall task performance was not reliable. We thus wanted to construct an in-domain Uyghur-English parallel corpus. In the scenario we were given a small number of one-hour sessions with a native informant (NI), a Uyghur native who spoke English and was not a linguistics or computer science expert. We initially asked the NI use the time to translate docu5 Related Work Callison-Burch (2005); Albrecht et al. (2009); Koehn (2010) and Trados5 have built computeraided translation systems for high-resource languages, with an emphasis on post-editing. Hu et al. (2011) describe a monolingual translation protocol that combines MT with not only monolingual target language speakers, but, unlike the Chinese Room, also monolingual source language speakers. 5 66 https://www.sdltrados.com Figure 3: MT performance on an out-of-domain corpus (‘test’) does not predict performance on the evaluation (’eval’) set but performance on our ‘domain’ data set which comprises NI translations and Chinese Room post-edits, is predictive. 6 Futur"
P18-4011,E09-1008,0,0.031659,"ocuments from Uyghur to English describing earthquakes and disaster relief efforts. However, we had no parallel data dealing with this topic, and our use of an unrelated test set (see Figure 3) to estimate overall task performance was not reliable. We thus wanted to construct an in-domain Uyghur-English parallel corpus. In the scenario we were given a small number of one-hour sessions with a native informant (NI), a Uyghur native who spoke English and was not a linguistics or computer science expert. We initially asked the NI use the time to translate docu5 Related Work Callison-Burch (2005); Albrecht et al. (2009); Koehn (2010) and Trados5 have built computeraided translation systems for high-resource languages, with an emphasis on post-editing. Hu et al. (2011) describe a monolingual translation protocol that combines MT with not only monolingual target language speakers, but, unlike the Chinese Room, also monolingual source language speakers. 5 66 https://www.sdltrados.com Figure 3: MT performance on an out-of-domain corpus (‘test’) does not predict performance on the evaluation (’eval’) set but performance on our ‘domain’ data set which comprises NI translations and Chinese Room post-edits, is predi"
P19-1293,D18-1549,0,0.0357829,", 2017), which suffer from a huge hypothesis space. Recent approaches to zero-shot machine translation include pivot-based methods (Chen et al., 2017; Zheng et al., 2017; Cheng et al., 2016) and multi-lingual NMT methods (Firat et al., 2016a,b; Johnson et al., 2017; Ha et al., 2016, 2017). These systems are zero-shot for a specific source/target language pair, but need parallel data from source to a pivot or multiple other languages. More recently, totally unsupervised NMT methods are introduced that use only monolingual data for training a machine translation system. Lample et al. (2018a,c), Artetxe et al. (2018), and 3057 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3057–3062 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Yang et al. (2018) use iterative back-translation to train MT models in both directions simultaneously. Their training takes place on massive monolingual data and requires a long time to train as well as careful tuning of hyperparameters. The closest unsupervised NMT work to ours is by Kim et al. (2018). Similar to us, they break translation into glossing and correction steps. However, the"
P19-1293,Q17-1010,0,0.333604,"s a hard task and needs to be tuned for every language. Previous zero-shot NMT work compensates for a lack of source/target parallel data by either using source/pivot parallel data, extremely large monolingual data, or artificially generated data. These requirements and techniques limit the methods’ applicability to real-world low-resource languages. Instead, in this paper we propose using parallel data from high-resource languages to learn ‘how to translate’ and apply the trained system to low resource settings. We use off-theshelf technologies to build word embeddings from monolingual data (Bojanowski et al., 2017) and learn a source-to-target bilingual dictionary using source and target embeddings (Lample et al., 2018b). Given a target language, we train sourceto-target dictionaries for a diverse set of highresource source languages, and use them to convert the source side of the parallel data to Translationese. We combine this parallel data and train a Translationese-to-target translator on it. Later, we can build source-to-target dictionaries on-demand, generate Translationese from source texts, and use the pre-trained system to rapidly produce machine translation for many languages without requiring"
P19-1293,P18-4011,1,0.899332,"lel data, with the source side converted into Translationese using Step 1. Introduction Quality of machine translation, especially neural MT, highly depends on the amount of available parallel data. For a handful of languages, where parallel data is abundant, MT quality has reached quite good performance (Wu et al., 2016; Hassan et al., 2018). However, the quality of translation rapidly deteriorates as the amount of parallel data decreases (Koehn and Knowles, 2017). Unfortunately, many languages have close to zero parallel texts. Translating texts from these languages requires new techniques. Hermjakob et al. (2018) presented a hybrid human/machine translation tool that uses lexical translation tables to gloss a translation and relies on human language and world models to propagate glosses into fluent translations. Inspired by that work, this work investigates the following The notion of separating adequacy from fluency components into a pipeline of operations dates back to the early days of MT and NLP research, where the inadequacy of word-by-word MT was first observed (Yngve, 1955; Oswald, 1952). A subfield of MT research that seeks to improve fluency given disfluent but adequate first-pass translation"
P19-1293,D18-1101,0,0.617894,"onolingual data for training a machine translation system. Lample et al. (2018a,c), Artetxe et al. (2018), and 3057 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3057–3062 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Yang et al. (2018) use iterative back-translation to train MT models in both directions simultaneously. Their training takes place on massive monolingual data and requires a long time to train as well as careful tuning of hyperparameters. The closest unsupervised NMT work to ours is by Kim et al. (2018). Similar to us, they break translation into glossing and correction steps. However, their correction step is trained on artificially generated noisy data aimed at simulating glossed source texts. Although this correction method helps, simulating noise caused by natural language phenomena is a hard task and needs to be tuned for every language. Previous zero-shot NMT work compensates for a lack of source/target parallel data by either using source/pivot parallel data, extremely large monolingual data, or artificially generated data. These requirements and techniques limit the methods’ applicab"
P19-1293,2005.mtsummit-papers.11,0,0.109419,"et al., 2017). These embeddings are used to train bilingual dictionaries. We select English as the target language. In order to avoid biasing the trained system toward a language or a specific type of parallel data, we use diverse parallel data on a diverse set of languages to train the Translationese to English system. We use Arabic, Czech, Dutch, Finnish, French, German, Italian, Russian, and Spanish as the set of out training languages. We use roughly 2 million sentence pairs per language and limit the length of the sentences to 100 tokens. For Dutch, Finnish, and Italian we use Europarl (Koehn, 2005) for parallel data. For Arabic we use MultiUN (Tiedemann, 2012). For French we use CommonCrawl. For German we use a mix of CommonCrawl (1.7M), and NewsCommentary (300K). The numbers in parentheses show the number of sentences for each dataset. For Spanish we use CommonCrawl (1.8M), and Europarl (200K). For Russian we use Yandex (1M), CommonCrawl (800K), and NewsCommentary (200K), and finally for Czech we use a mix of ParaCrawl (1M), Europarl (640K), NewsCommentary (200K), and CommonCrawl (160K). We train one model on these nine languages and apply it to test languages not in this set. Also, to"
P19-1293,P17-1176,0,0.0163683,"(Yngve, 1955; Oswald, 1952). A subfield of MT research that seeks to improve fluency given disfluent but adequate first-pass translation is automatic post-editing (APE) pioneered by Knight and Chander (1994). Much of the current APE work targets correction of black-box MT systems, which are presumed to be supervised. Early approaches to unsupervised machine translation include decipherment methods (Nuhn et al., 2013; Ravi and Knight, 2011; Pourdamghani and Knight, 2017), which suffer from a huge hypothesis space. Recent approaches to zero-shot machine translation include pivot-based methods (Chen et al., 2017; Zheng et al., 2017; Cheng et al., 2016) and multi-lingual NMT methods (Firat et al., 2016a,b; Johnson et al., 2017; Ha et al., 2016, 2017). These systems are zero-shot for a specific source/target language pair, but need parallel data from source to a pivot or multiple other languages. More recently, totally unsupervised NMT methods are introduced that use only monolingual data for training a machine translation system. Lample et al. (2018a,c), Artetxe et al. (2018), and 3057 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3057–3062 c Florence,"
P19-1293,W02-0902,1,0.5872,"our proposed pipeline includes a word-by-word translation of the source texts. This requires a source/target dictionary. Manually constructed dictionaries exist for many language pairs, however cleaning these dictionaries to get a word to word lexicon is not trivial, and these dictionaries often cover a small portion of the source vocabulary, focusing on stems and specifically excluding inflected variants. In order to have a comprehensive, word to word, inflected bi-lingual dictionary we look for automatically built ones. Automatic lexical induction is an active field of research (Fung, 1995; Koehn and Knight, 2002; Haghighi et al., 2008; Lample et al., 2018b). A popular method for automatic extraction of bilingual dictionaries is through building cross-lingual word embeddings. Finding a shared word representation space between two languages enables us to calculate the distance between word embeddings of source and target, which helps us to find translation candidates for each word. We follow this approach for building the bilingual dictionaries. For a given source and target language, we start by separately training source and target word embeddings S and T , and use the method introduced by Lample et"
P19-1293,W17-3204,0,0.0315501,"nput into a pseudo-translation or ‘Translationese’. 2. Translate the Translationese into target language, using a model built in advance from various parallel data, with the source side converted into Translationese using Step 1. Introduction Quality of machine translation, especially neural MT, highly depends on the amount of available parallel data. For a handful of languages, where parallel data is abundant, MT quality has reached quite good performance (Wu et al., 2016; Hassan et al., 2018). However, the quality of translation rapidly deteriorates as the amount of parallel data decreases (Koehn and Knowles, 2017). Unfortunately, many languages have close to zero parallel texts. Translating texts from these languages requires new techniques. Hermjakob et al. (2018) presented a hybrid human/machine translation tool that uses lexical translation tables to gloss a translation and relies on human language and world models to propagate glosses into fluent translations. Inspired by that work, this work investigates the following The notion of separating adequacy from fluency components into a pipeline of operations dates back to the early days of MT and NLP research, where the inadequacy of word-by-word MT w"
P19-1293,N16-1101,0,0.0299477,"n disfluent but adequate first-pass translation is automatic post-editing (APE) pioneered by Knight and Chander (1994). Much of the current APE work targets correction of black-box MT systems, which are presumed to be supervised. Early approaches to unsupervised machine translation include decipherment methods (Nuhn et al., 2013; Ravi and Knight, 2011; Pourdamghani and Knight, 2017), which suffer from a huge hypothesis space. Recent approaches to zero-shot machine translation include pivot-based methods (Chen et al., 2017; Zheng et al., 2017; Cheng et al., 2016) and multi-lingual NMT methods (Firat et al., 2016a,b; Johnson et al., 2017; Ha et al., 2016, 2017). These systems are zero-shot for a specific source/target language pair, but need parallel data from source to a pivot or multiple other languages. More recently, totally unsupervised NMT methods are introduced that use only monolingual data for training a machine translation system. Lample et al. (2018a,c), Artetxe et al. (2018), and 3057 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3057–3062 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Yang et al."
P19-1293,D16-1026,0,0.018138,"n disfluent but adequate first-pass translation is automatic post-editing (APE) pioneered by Knight and Chander (1994). Much of the current APE work targets correction of black-box MT systems, which are presumed to be supervised. Early approaches to unsupervised machine translation include decipherment methods (Nuhn et al., 2013; Ravi and Knight, 2011; Pourdamghani and Knight, 2017), which suffer from a huge hypothesis space. Recent approaches to zero-shot machine translation include pivot-based methods (Chen et al., 2017; Zheng et al., 2017; Cheng et al., 2016) and multi-lingual NMT methods (Firat et al., 2016a,b; Johnson et al., 2017; Ha et al., 2016, 2017). These systems are zero-shot for a specific source/target language pair, but need parallel data from source to a pivot or multiple other languages. More recently, totally unsupervised NMT methods are introduced that use only monolingual data for training a machine translation system. Lample et al. (2018a,c), Artetxe et al. (2018), and 3057 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3057–3062 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Yang et al."
P19-1293,W95-0114,0,0.23432,"rst step of our proposed pipeline includes a word-by-word translation of the source texts. This requires a source/target dictionary. Manually constructed dictionaries exist for many language pairs, however cleaning these dictionaries to get a word to word lexicon is not trivial, and these dictionaries often cover a small portion of the source vocabulary, focusing on stems and specifically excluding inflected variants. In order to have a comprehensive, word to word, inflected bi-lingual dictionary we look for automatically built ones. Automatic lexical induction is an active field of research (Fung, 1995; Koehn and Knight, 2002; Haghighi et al., 2008; Lample et al., 2018b). A popular method for automatic extraction of bilingual dictionaries is through building cross-lingual word embeddings. Finding a shared word representation space between two languages enables us to calculate the distance between word embeddings of source and target, which helps us to find translation candidates for each word. We follow this approach for building the bilingual dictionaries. For a given source and target language, we start by separately training source and target word embeddings S and T , and use the method"
P19-1293,P13-1154,0,0.0171821,"e notion of separating adequacy from fluency components into a pipeline of operations dates back to the early days of MT and NLP research, where the inadequacy of word-by-word MT was first observed (Yngve, 1955; Oswald, 1952). A subfield of MT research that seeks to improve fluency given disfluent but adequate first-pass translation is automatic post-editing (APE) pioneered by Knight and Chander (1994). Much of the current APE work targets correction of black-box MT systems, which are presumed to be supervised. Early approaches to unsupervised machine translation include decipherment methods (Nuhn et al., 2013; Ravi and Knight, 2011; Pourdamghani and Knight, 2017), which suffer from a huge hypothesis space. Recent approaches to zero-shot machine translation include pivot-based methods (Chen et al., 2017; Zheng et al., 2017; Cheng et al., 2016) and multi-lingual NMT methods (Firat et al., 2016a,b; Johnson et al., 2017; Ha et al., 2016, 2017). These systems are zero-shot for a specific source/target language pair, but need parallel data from source to a pivot or multiple other languages. More recently, totally unsupervised NMT methods are introduced that use only monolingual data for training a machi"
P19-1293,1983.tc-1.13,0,0.670265,"have close to zero parallel texts. Translating texts from these languages requires new techniques. Hermjakob et al. (2018) presented a hybrid human/machine translation tool that uses lexical translation tables to gloss a translation and relies on human language and world models to propagate glosses into fluent translations. Inspired by that work, this work investigates the following The notion of separating adequacy from fluency components into a pipeline of operations dates back to the early days of MT and NLP research, where the inadequacy of word-by-word MT was first observed (Yngve, 1955; Oswald, 1952). A subfield of MT research that seeks to improve fluency given disfluent but adequate first-pass translation is automatic post-editing (APE) pioneered by Knight and Chander (1994). Much of the current APE work targets correction of black-box MT systems, which are presumed to be supervised. Early approaches to unsupervised machine translation include decipherment methods (Nuhn et al., 2013; Ravi and Knight, 2011; Pourdamghani and Knight, 2017), which suffer from a huge hypothesis space. Recent approaches to zero-shot machine translation include pivot-based methods (Chen et al., 2017; Zheng et"
P19-1293,D17-1266,1,0.859975,"cy components into a pipeline of operations dates back to the early days of MT and NLP research, where the inadequacy of word-by-word MT was first observed (Yngve, 1955; Oswald, 1952). A subfield of MT research that seeks to improve fluency given disfluent but adequate first-pass translation is automatic post-editing (APE) pioneered by Knight and Chander (1994). Much of the current APE work targets correction of black-box MT systems, which are presumed to be supervised. Early approaches to unsupervised machine translation include decipherment methods (Nuhn et al., 2013; Ravi and Knight, 2011; Pourdamghani and Knight, 2017), which suffer from a huge hypothesis space. Recent approaches to zero-shot machine translation include pivot-based methods (Chen et al., 2017; Zheng et al., 2017; Cheng et al., 2016) and multi-lingual NMT methods (Firat et al., 2016a,b; Johnson et al., 2017; Ha et al., 2016, 2017). These systems are zero-shot for a specific source/target language pair, but need parallel data from source to a pivot or multiple other languages. More recently, totally unsupervised NMT methods are introduced that use only monolingual data for training a machine translation system. Lample et al. (2018a,c), Artetxe"
P19-1293,P11-1002,1,0.864862,"ing adequacy from fluency components into a pipeline of operations dates back to the early days of MT and NLP research, where the inadequacy of word-by-word MT was first observed (Yngve, 1955; Oswald, 1952). A subfield of MT research that seeks to improve fluency given disfluent but adequate first-pass translation is automatic post-editing (APE) pioneered by Knight and Chander (1994). Much of the current APE work targets correction of black-box MT systems, which are presumed to be supervised. Early approaches to unsupervised machine translation include decipherment methods (Nuhn et al., 2013; Ravi and Knight, 2011; Pourdamghani and Knight, 2017), which suffer from a huge hypothesis space. Recent approaches to zero-shot machine translation include pivot-based methods (Chen et al., 2017; Zheng et al., 2017; Cheng et al., 2016) and multi-lingual NMT methods (Firat et al., 2016a,b; Johnson et al., 2017; Ha et al., 2016, 2017). These systems are zero-shot for a specific source/target language pair, but need parallel data from source to a pivot or multiple other languages. More recently, totally unsupervised NMT methods are introduced that use only monolingual data for training a machine translation system."
P19-1293,P16-1162,0,0.0432095,"ple et al. (2018b) to find the cross-lingual embedding vectors. In order to create the dictionary we limit the size of the source and target (English) vocabulary 1 https://github.com/tensorflow/ tensor2tensor 2 https://github.com/facebookresearch/ fastText/blob/master/pretrained-vectors. md 3059 to 100K tokens. For each source token we find 20 nearest neighbors in the target language. We use a 5-gram language model trained on 4 billion tokens of Gigaword to select between the translation options for each token. We use Moses scripts for tokenizing and lowercasing the data. We do not apply BPE (Sennrich et al., 2016) on the data. In order to be comparable to Kim et al. (2018) we split German compound words only for the newstest2016 test data. We use the CharSplit3 python package for this purpose. We use tensor2tensor’s transformer base hyperparameters to train the transformer model on a single gpu for each language. 4 Experiments We report translation results on newstest2013 for Spanish, newstest2014 for French, and newstest2016 for Czech, German, Finnish, Romanian, and Russian. We also report results on the first 3000 sentences of GlobalVoices20154 for Dutch, Bulgarian, Danish, Indonesian, Polish, Portug"
P19-1293,tiedemann-2012-parallel,0,0.0115774,"l dictionaries. We select English as the target language. In order to avoid biasing the trained system toward a language or a specific type of parallel data, we use diverse parallel data on a diverse set of languages to train the Translationese to English system. We use Arabic, Czech, Dutch, Finnish, French, German, Italian, Russian, and Spanish as the set of out training languages. We use roughly 2 million sentence pairs per language and limit the length of the sentences to 100 tokens. For Dutch, Finnish, and Italian we use Europarl (Koehn, 2005) for parallel data. For Arabic we use MultiUN (Tiedemann, 2012). For French we use CommonCrawl. For German we use a mix of CommonCrawl (1.7M), and NewsCommentary (300K). The numbers in parentheses show the number of sentences for each dataset. For Spanish we use CommonCrawl (1.8M), and Europarl (200K). For Russian we use Yandex (1M), CommonCrawl (800K), and NewsCommentary (200K), and finally for Czech we use a mix of ParaCrawl (1M), Europarl (640K), NewsCommentary (200K), and CommonCrawl (160K). We train one model on these nine languages and apply it to test languages not in this set. Also, to test on each of the training languages, we train a model where"
P19-1293,P18-1005,0,0.0332431,"t al., 2016a,b; Johnson et al., 2017; Ha et al., 2016, 2017). These systems are zero-shot for a specific source/target language pair, but need parallel data from source to a pivot or multiple other languages. More recently, totally unsupervised NMT methods are introduced that use only monolingual data for training a machine translation system. Lample et al. (2018a,c), Artetxe et al. (2018), and 3057 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3057–3062 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Yang et al. (2018) use iterative back-translation to train MT models in both directions simultaneously. Their training takes place on massive monolingual data and requires a long time to train as well as careful tuning of hyperparameters. The closest unsupervised NMT work to ours is by Kim et al. (2018). Similar to us, they break translation into glossing and correction steps. However, their correction step is trained on artificially generated noisy data aimed at simulating glossed source texts. Although this correction method helps, simulating noise caused by natural language phenomena is a hard task and needs"
P19-3004,P06-1121,0,0.00985087,"ms with specific foreignlanguage terms, and a matching network to determine relevance. The model was optimized using a cross-entropy objective. In recent experiments, SEARCHER’s performance exceeded that of the term-level matching approach, improving AQWV (see Section 3) from 23.1 to 25.2 on the Somali MATERIAL evaluation corpus, even when translation is performed by state-of-the-art MT systems. Machine Translation Our low-resource MT architecture is a system combination (Heafield and Lavie, 2010) of a Transformer-based neural model (Vaswani et al., 2017) and a statistical syntax-based model (Galley et al., 2006), which bring complementary strengths, particularly in low-resource conditions. All models are trained with fewer than 2M words of parallel data.3 By contrast, in the WMT 2018 shared task (Bojar et al., 2018) most language pairs had 4M or more words, and many had more than 10M words. To further adapt to low-resource conditions, we augment our neural system with 14.5M words of crawled English region-relevant data with parallel Somali or Swahili obtained from backtranslation Transformer models (Sennrich et al., 2016a). Transformer model hyperparameters are “out-ofthe-box” except that the shared"
P19-3004,2010.amta-papers.34,0,0.02616,"al. 2017) for encoding foreign-language sentences, a query embedding matrix, an attention mechanism for aligning query terms with specific foreignlanguage terms, and a matching network to determine relevance. The model was optimized using a cross-entropy objective. In recent experiments, SEARCHER’s performance exceeded that of the term-level matching approach, improving AQWV (see Section 3) from 23.1 to 25.2 on the Somali MATERIAL evaluation corpus, even when translation is performed by state-of-the-art MT systems. Machine Translation Our low-resource MT architecture is a system combination (Heafield and Lavie, 2010) of a Transformer-based neural model (Vaswani et al., 2017) and a statistical syntax-based model (Galley et al., 2006), which bring complementary strengths, particularly in low-resource conditions. All models are trained with fewer than 2M words of parallel data.3 By contrast, in the WMT 2018 shared task (Bojar et al., 2018) most language pairs had 4M or more words, and many had more than 10M words. To further adapt to low-resource conditions, we augment our neural system with 14.5M words of crawled English region-relevant data with parallel Somali or Swahili obtained from backtranslation Tran"
P19-3004,D18-1023,0,0.0226189,"Missing"
P19-3004,P15-2070,0,0.0430599,"Missing"
P19-3004,P16-1009,0,0.0201176,"r-based neural model (Vaswani et al., 2017) and a statistical syntax-based model (Galley et al., 2006), which bring complementary strengths, particularly in low-resource conditions. All models are trained with fewer than 2M words of parallel data.3 By contrast, in the WMT 2018 shared task (Bojar et al., 2018) most language pairs had 4M or more words, and many had more than 10M words. To further adapt to low-resource conditions, we augment our neural system with 14.5M words of crawled English region-relevant data with parallel Somali or Swahili obtained from backtranslation Transformer models (Sennrich et al., 2016a). Transformer model hyperparameters are “out-ofthe-box” except that the shared Byte Pair Encoding (Sennrich et al., 2016b) vocabulary is set to approximately 8,000. 2.3 Cross-Lingual Information Retrieval We employ a combination of two approaches to cross-lingual information retrieval. The first relies on term-level matching in both the original document and its machine translation(s). Sourcelanguage matching is mediated via translation tables derived from the word alignments used by our syntax-based MT system. Terms are expanded using transformations of varying expected accuracy, e.g. stemm"
pighin-etal-2012-analysis,H05-1098,0,\N,Missing
pighin-etal-2012-analysis,P07-2045,0,\N,Missing
S16-1166,D15-1198,0,0.1684,"ngsbury and Palmer, 2002), individual entities playing multiple roles, as well as treatments of modality, negation, etc. AMR abstracts in numerous ways, e.g., by assigning the same conceptual structure to fear (v), fear (n), and afraid (adj). Figure 1 gives an example. With the recent public release of a sizeable corpus of English/AMR pairs (LDC2014T12), there has been substantial interest in creating parsers to recover this formalism from plain text. Several parsers were released in the past couple of years (Flanigan et al., 2014; Wang et al., 2015b; Werling et al., 2015; Wang et al., 2015a; Artzi et al., 2015; Pust et al., 2015). This body of work constitutes many diverse and interesting scientific contributions, but it is difficult to adequately determine which parser is numerically superior, due to heterogeneous evaluation decisions and the lack of a controlled blind evaluation. The purpose of this task, therefore, was to provide a competitive environment in which to determine one winner and award a trophy to said winner. 2 Training Data LDC released a new corpus of AMRs (LDC2015E86), created as part of the DARPA DEFT program, in August of 2015. The new corpus, which was annotated by teams at SD"
S16-1166,W13-2322,0,0.348907,"entation in the web forum portion. The top scoring systems scored 0.62 F1 according to the Smatch (Cai and Knight, 2013) evaluation heuristic. We show some sample sentences along with a comparison of system parses and perform quantitative ablative studies. 1 The soldier was not afraid of dying. The soldier was not afraid to die. The soldier did not fear death. Figure 1: An Abstract Meaning Representation (AMR) with several English renderings. Example borrowed from Pust et al. (2015). Introduction Abstract Meaning Representation (AMR) is a compact, readable, whole-sentence semantic annotation (Banarescu et al., 2013). It includes entity identification and typing, PropBank semantic roles (Kingsbury and Palmer, 2002), individual entities playing multiple roles, as well as treatments of modality, negation, etc. AMR abstracts in numerous ways, e.g., by assigning the same conceptual structure to fear (v), fear (n), and afraid (adj). Figure 1 gives an example. With the recent public release of a sizeable corpus of English/AMR pairs (LDC2014T12), there has been substantial interest in creating parsers to recover this formalism from plain text. Several parsers were released in the past couple of years (Flanigan e"
S16-1166,S16-1176,0,0.465341,"perimented with three sets of new features: 1) rich named entities, 2) a verbalization list, and 3) semantic role labels. They also used the RPI Wikifier to wikify the concepts in the AMR graph. 6.1.2 ICL-HD (Brandt et al., 2016) This team attempted to improve AMR parsing by exploiting preposition semantic role labeling information retrieved from a multi-layer feed-forward neural network. Prepositional semantics was included as features into CAMR. The inclusion of the features modified the behavior of CAMR when creating meaning representations triggered by prepositional semantics. 6.1.3 RIGA (Barzdins and Gosko, 2016) Besides developing a novel character-level neural translation based AMR parser, this team also 3 Four random restarts. A twelfth team, CUCLEAR, participated but produced invalid AMRs that could not be scored. 4 1065 extended the Smatch scoring tool with the C6.0 rule-based classifier to produce a human-readable report on the error patterns frequency observed in the scored AMR graphs. They improved CAMR by adding to it a manually crafted wrapper fixing the identified CAMR parser errors. A small further gain was achieved by combining the neural and CAMR+wrapper parsers in an ensemble. 6.1.4 M2L"
S16-1166,S16-1182,0,0.0818895,"Missing"
S16-1166,S16-1179,0,0.0598788,"l system description papers for more details. 6.1 CAMR-based systems A number of teams made use of the CAMR system from Wang et al. (2015a). These systems proved among the highest-scoring and had little variance from each other in terms of system score. 6.1.1 Brandeis / cemantix.org / RPI (Wang et al., 2016) This team, the originators of CAMR, started with their existing AMR parser and experimented with three sets of new features: 1) rich named entities, 2) a verbalization list, and 3) semantic role labels. They also used the RPI Wikifier to wikify the concepts in the AMR graph. 6.1.2 ICL-HD (Brandt et al., 2016) This team attempted to improve AMR parsing by exploiting preposition semantic role labeling information retrieved from a multi-layer feed-forward neural network. Prepositional semantics was included as features into CAMR. The inclusion of the features modified the behavior of CAMR when creating meaning representations triggered by prepositional semantics. 6.1.3 RIGA (Barzdins and Gosko, 2016) Besides developing a novel character-level neural translation based AMR parser, this team also 3 Four random restarts. A twelfth team, CUCLEAR, participated but produced invalid AMRs that could not be sc"
S16-1166,S16-1177,0,0.0589134,"Missing"
S16-1166,P13-2131,0,0.226181,"ntation (AMR) (Banarescu et al., 2013) graphs for a set of English sentences in the news and discussion forum domains. Eleven sites submitted valid systems. The availability of state-of-the-art baseline systems was a key factor in lowering the bar to entry; many submissions relied on CAMR (Wang et al., 2015b; Wang et al., 2015a) as a baseline system and added extensions to it to improve scores. The evaluation set was quite difficult to parse, particularly due to creative approaches to word representation in the web forum portion. The top scoring systems scored 0.62 F1 according to the Smatch (Cai and Knight, 2013) evaluation heuristic. We show some sample sentences along with a comparison of system parses and perform quantitative ablative studies. 1 The soldier was not afraid of dying. The soldier was not afraid to die. The soldier did not fear death. Figure 1: An Abstract Meaning Representation (AMR) with several English renderings. Example borrowed from Pust et al. (2015). Introduction Abstract Meaning Representation (AMR) is a compact, readable, whole-sentence semantic annotation (Banarescu et al., 2013). It includes entity identification and typing, PropBank semantic roles (Kingsbury and Palmer, 20"
S16-1166,P07-2009,0,0.0133216,"o transform a syntactic parse into a predicate logic based meaning representation, followed by conversion to the required Penman notation. This team developed a novel transition-based parsing algorithm using exact imitation learning, in which the parser learns a statistical model by imitating the actions of an expert on the training data. They used the imitation learning algorithm DAGGER to improve the performance, and applied an alpha-bound as a simple noise reduction technique. 6.2.4 The Meaning Factory (Bjerva et al., 2016) This team employed an existing open-domain semantic parser, Boxer (Curran et al., 2007), which produces semantic representations based on Discourse Representation Theory. As the meaning representations produced by Boxer are considerably different from AMRs, the team used a hybrid conversion method to map Boxer’s output to AMRs. This process involves lexical adaptation, a conver1066 6.2.6 UofR (Peng and Gildea, 2016) This team applied a synchronous-graphgrammar-based approach for string-to-AMR parsing. They applied Markov Chain Monte Carlo (MCMC) algorithms to learn Synchronous Hyperedge Replacement Grammar (SHRG) rules from a forest that represents likely derivations consistent"
S16-1166,P14-1134,0,0.656077,"al., 2013). It includes entity identification and typing, PropBank semantic roles (Kingsbury and Palmer, 2002), individual entities playing multiple roles, as well as treatments of modality, negation, etc. AMR abstracts in numerous ways, e.g., by assigning the same conceptual structure to fear (v), fear (n), and afraid (adj). Figure 1 gives an example. With the recent public release of a sizeable corpus of English/AMR pairs (LDC2014T12), there has been substantial interest in creating parsers to recover this formalism from plain text. Several parsers were released in the past couple of years (Flanigan et al., 2014; Wang et al., 2015b; Werling et al., 2015; Wang et al., 2015a; Artzi et al., 2015; Pust et al., 2015). This body of work constitutes many diverse and interesting scientific contributions, but it is difficult to adequately determine which parser is numerically superior, due to heterogeneous evaluation decisions and the lack of a controlled blind evaluation. The purpose of this task, therefore, was to provide a competitive environment in which to determine one winner and award a trophy to said winner. 2 Training Data LDC released a new corpus of AMRs (LDC2015E86), created as part of the DARPA D"
S16-1166,S16-1186,0,0.126723,"with novel techniques. 6.2.1 CLIP@UMD (Rao et al., 2016) This team developed a novel technique for AMR parsing that uses the Learning to Search (L2S) algorithm. They decomposed the AMR prediction problem into three problems—that of predicting the concepts, predicting the root, and predicting the relations between the predicted concepts. Using L2S allowed them to model the learning of concepts and relations in a unified framework which aims to minimize the loss over the entire predicted structure, as opposed to minimizing the loss over concepts and relations in two separate stages. 6.2.2 CMU (Flanigan et al., 2016) This team’s entry is a set of improvements to JAMR (Flanigan et al., 2014). The improvements are: a novel training loss function for structured prediction, new sources for concepts, improved features, and improvements to the rule-based aligner in Flanigan et al. (2014). The overall architecture of the system and the decoding algorithms for conBrandeis/cemantix.org/RPI CLIP@UMD CMU CU-NLP DynamicPower ICL-HD M2L RIGA Meaning Factory UCL+Sheffield UofR Determ. baseline JAMR baseline Full AMR 0.6195 0.4370 0.5636 0.6060 0.3706 0.6005 0.5952 0.6196 0.4702 0.5983 0.4985 0.2440 0.4965 Instances 0.7"
S16-1166,S16-1185,0,0.283604,"ous Hyperedge Replacement Grammar (SHRG) rules from a forest that represents likely derivations consistent with a fixed string-to-graph alignment (extracted using an automatic aligner). They make an analogy of string-to-AMR parsing to the task of phrase-based machine translation and came up with an efficient algorithm to learn graph grammars from string-graph pairs. They proposed an effective approximation strategy to resolve the complexity issue of graph compositions. Then they used the Earley algorithm with cube-pruning for AMR parsing given new sentences and the learned SHRG. 6.2.7 CU-NLP (Foland and Martin, 2016) This parser does not rely on a syntactic pre-parse, or heavily engineered features, and uses five recurrent neural networks as the key architectural components for estimating AMR graph structure. 7 Result Ablations We conduct several ablations to attempt to empirically determine what aspects of the AMR parsing task were more or less difficult for the various systems. 7.1 Impact of Wikification The AMR standard has recently been expanded to include wikification and the data used in this task reflected that expansion. Since this is a rather recent change to the standard and requires some kind o"
S16-1166,S16-1180,0,0.336956,"Missing"
S16-1166,kingsbury-palmer-2002-treebank,0,0.0799219,"h (Cai and Knight, 2013) evaluation heuristic. We show some sample sentences along with a comparison of system parses and perform quantitative ablative studies. 1 The soldier was not afraid of dying. The soldier was not afraid to die. The soldier did not fear death. Figure 1: An Abstract Meaning Representation (AMR) with several English renderings. Example borrowed from Pust et al. (2015). Introduction Abstract Meaning Representation (AMR) is a compact, readable, whole-sentence semantic annotation (Banarescu et al., 2013). It includes entity identification and typing, PropBank semantic roles (Kingsbury and Palmer, 2002), individual entities playing multiple roles, as well as treatments of modality, negation, etc. AMR abstracts in numerous ways, e.g., by assigning the same conceptual structure to fear (v), fear (n), and afraid (adj). Figure 1 gives an example. With the recent public release of a sizeable corpus of English/AMR pairs (LDC2014T12), there has been substantial interest in creating parsers to recover this formalism from plain text. Several parsers were released in the past couple of years (Flanigan et al., 2014; Wang et al., 2015b; Werling et al., 2015; Wang et al., 2015a; Artzi et al., 2015; Pust"
S16-1166,S16-1183,0,0.284901,"ning data. They used the imitation learning algorithm DAGGER to improve the performance, and applied an alpha-bound as a simple noise reduction technique. 6.2.4 The Meaning Factory (Bjerva et al., 2016) This team employed an existing open-domain semantic parser, Boxer (Curran et al., 2007), which produces semantic representations based on Discourse Representation Theory. As the meaning representations produced by Boxer are considerably different from AMRs, the team used a hybrid conversion method to map Boxer’s output to AMRs. This process involves lexical adaptation, a conver1066 6.2.6 UofR (Peng and Gildea, 2016) This team applied a synchronous-graphgrammar-based approach for string-to-AMR parsing. They applied Markov Chain Monte Carlo (MCMC) algorithms to learn Synchronous Hyperedge Replacement Grammar (SHRG) rules from a forest that represents likely derivations consistent with a fixed string-to-graph alignment (extracted using an automatic aligner). They make an analogy of string-to-AMR parsing to the task of phrase-based machine translation and came up with an efficient algorithm to learn graph grammars from string-graph pairs. They proposed an effective approximation strategy to resolve the compl"
S16-1166,D14-1048,0,0.171001,"from Ulf Hermjakob) used to produce the tokenized sentences in the training corpus. Table 2: Main Results: Mean of five runs of Smatch 2.0.2 with five restarts per run is shown; Standard deviation of F1 was about 0.0002 per system. • The AMR specification, used by annotators in producing the AMRs.1 4 • A deterministic, input-agnostic trivial baseline ‘parser’ courtesy of Ulf Hermjakob. • The JAMR parser (Flanigan et al., 2014) as a strong baseline. We provided setup scripts to process the released training data but otherwise provided the parser as is. • An unsupervised AMR-to-English aligner (Pourdamghani et al., 2014). • The same Smatch (Cai and Knight, 2013) scoring script used in the evaluation. • A Python AMR manipulation library, from Nathan Schneider. 1 https://github.com/kevincrawfordknight/ amr-guidelines/blob/master/amr.md. 1064 Evaluation Data For the specific purposes of this task, DEFT commissioned and LDC released an additional set of English sentences along with AMR annotations2 that had not been previously seen. This blind evaluation set consists of 1,053 sentences in a roughly 50/50 discussion forum/newswire split. The distribution of sentences by source is shown in Table 1. 5 Task Definitio"
S16-1166,D15-1136,1,0.802882,"ns to it to improve scores. The evaluation set was quite difficult to parse, particularly due to creative approaches to word representation in the web forum portion. The top scoring systems scored 0.62 F1 according to the Smatch (Cai and Knight, 2013) evaluation heuristic. We show some sample sentences along with a comparison of system parses and perform quantitative ablative studies. 1 The soldier was not afraid of dying. The soldier was not afraid to die. The soldier did not fear death. Figure 1: An Abstract Meaning Representation (AMR) with several English renderings. Example borrowed from Pust et al. (2015). Introduction Abstract Meaning Representation (AMR) is a compact, readable, whole-sentence semantic annotation (Banarescu et al., 2013). It includes entity identification and typing, PropBank semantic roles (Kingsbury and Palmer, 2002), individual entities playing multiple roles, as well as treatments of modality, negation, etc. AMR abstracts in numerous ways, e.g., by assigning the same conceptual structure to fear (v), fear (n), and afraid (adj). Figure 1 gives an example. With the recent public release of a sizeable corpus of English/AMR pairs (LDC2014T12), there has been substantial inter"
S16-1166,S16-1178,0,0.0280597,"esides developing a novel character-level neural translation based AMR parser, this team also 3 Four random restarts. A twelfth team, CUCLEAR, participated but produced invalid AMRs that could not be scored. 4 1065 extended the Smatch scoring tool with the C6.0 rule-based classifier to produce a human-readable report on the error patterns frequency observed in the scored AMR graphs. They improved CAMR by adding to it a manually crafted wrapper fixing the identified CAMR parser errors. A small further gain was achieved by combining the neural and CAMR+wrapper parsers in an ensemble. 6.1.4 M2L (Puzikov et al., 2016) This team attempted to improve upon CAMR by using a feed-forward neural network classification algorithm. They also experimented with various ways of enriching CAMR’s feature set. Unlike ICLHD and RIGA they were not able to benefit from feed-forward neural networks, but were able to benefit from feature enhancements. 6.2 Other Approaches The other teams either improved upon their existing AMR parsers, converted existing semantic parsing tools and pipelines into AMR, or constructed AMR parsers from scratch with novel techniques. 6.2.1 CLIP@UMD (Rao et al., 2016) This team developed a novel tec"
S16-1166,S16-1184,0,0.0254744,"Missing"
S16-1166,P15-2141,0,0.162589,"s Institute University of Southern California jonmay@isi.edu (f / fear-01 :polarity &quot;-&quot; :ARG0 ( s / soldier ) :ARG1 ( d / die-01 :ARG1 s )) Abstract In this report we summarize the results of the SemEval 2016 Task 8: Meaning Representation Parsing. Participants were asked to generate Abstract Meaning Representation (AMR) (Banarescu et al., 2013) graphs for a set of English sentences in the news and discussion forum domains. Eleven sites submitted valid systems. The availability of state-of-the-art baseline systems was a key factor in lowering the bar to entry; many submissions relied on CAMR (Wang et al., 2015b; Wang et al., 2015a) as a baseline system and added extensions to it to improve scores. The evaluation set was quite difficult to parse, particularly due to creative approaches to word representation in the web forum portion. The top scoring systems scored 0.62 F1 according to the Smatch (Cai and Knight, 2013) evaluation heuristic. We show some sample sentences along with a comparison of system parses and perform quantitative ablative studies. 1 The soldier was not afraid of dying. The soldier was not afraid to die. The soldier did not fear death. Figure 1: An Abstract Meaning Representation"
S16-1166,N15-1040,0,0.496346,"s Institute University of Southern California jonmay@isi.edu (f / fear-01 :polarity &quot;-&quot; :ARG0 ( s / soldier ) :ARG1 ( d / die-01 :ARG1 s )) Abstract In this report we summarize the results of the SemEval 2016 Task 8: Meaning Representation Parsing. Participants were asked to generate Abstract Meaning Representation (AMR) (Banarescu et al., 2013) graphs for a set of English sentences in the news and discussion forum domains. Eleven sites submitted valid systems. The availability of state-of-the-art baseline systems was a key factor in lowering the bar to entry; many submissions relied on CAMR (Wang et al., 2015b; Wang et al., 2015a) as a baseline system and added extensions to it to improve scores. The evaluation set was quite difficult to parse, particularly due to creative approaches to word representation in the web forum portion. The top scoring systems scored 0.62 F1 according to the Smatch (Cai and Knight, 2013) evaluation heuristic. We show some sample sentences along with a comparison of system parses and perform quantitative ablative studies. 1 The soldier was not afraid of dying. The soldier was not afraid to die. The soldier did not fear death. Figure 1: An Abstract Meaning Representation"
S16-1166,S16-1181,0,0.267627,"ore under consistent heuristic conditions.3 6 Participants and Results 11 teams participated in the task.4 Their systems and scores are shown in Table 2. Below are brief descriptions of each of the various systems, based on summaries provided by the system authors. Readers are encouraged to consult individual system description papers for more details. 6.1 CAMR-based systems A number of teams made use of the CAMR system from Wang et al. (2015a). These systems proved among the highest-scoring and had little variance from each other in terms of system score. 6.1.1 Brandeis / cemantix.org / RPI (Wang et al., 2016) This team, the originators of CAMR, started with their existing AMR parser and experimented with three sets of new features: 1) rich named entities, 2) a verbalization list, and 3) semantic role labels. They also used the RPI Wikifier to wikify the concepts in the AMR graph. 6.1.2 ICL-HD (Brandt et al., 2016) This team attempted to improve AMR parsing by exploiting preposition semantic role labeling information retrieved from a multi-layer feed-forward neural network. Prepositional semantics was included as features into CAMR. The inclusion of the features modified the behavior of CAMR when c"
S16-1166,P15-1095,0,0.0327789,"on and typing, PropBank semantic roles (Kingsbury and Palmer, 2002), individual entities playing multiple roles, as well as treatments of modality, negation, etc. AMR abstracts in numerous ways, e.g., by assigning the same conceptual structure to fear (v), fear (n), and afraid (adj). Figure 1 gives an example. With the recent public release of a sizeable corpus of English/AMR pairs (LDC2014T12), there has been substantial interest in creating parsers to recover this formalism from plain text. Several parsers were released in the past couple of years (Flanigan et al., 2014; Wang et al., 2015b; Werling et al., 2015; Wang et al., 2015a; Artzi et al., 2015; Pust et al., 2015). This body of work constitutes many diverse and interesting scientific contributions, but it is difficult to adequately determine which parser is numerically superior, due to heterogeneous evaluation decisions and the lack of a controlled blind evaluation. The purpose of this task, therefore, was to provide a competitive environment in which to determine one winner and award a trophy to said winner. 2 Training Data LDC released a new corpus of AMRs (LDC2015E86), created as part of the DARPA DEFT program, in August of 2015. The new co"
S17-2090,S16-1176,0,0.353888,"Missing"
S17-2090,W13-2322,0,0.367773,"ppreciate this. The advent of several systems that generate English text from AMR input (Flanigan et al., 2016b; Pourdamghani et al., 2016) inspired us to conduct a generation-based shared task from AMRs in the news/discussion forum domain. For the generation subtask, we solicited human judgments of sentence quality. We followed the precedent established by the Workshop in Machine Translation (Bojar et al., 2016) and used the Appraise solicitation system (Federmann, 2012), lightly modIntroduction Abstract Meaning Representation (AMR) is a compact, readable, whole-sentence semantic annotation (Banarescu et al., 2013). It includes entity identification and typing, PropBank semantic roles (Kingsbury and Palmer, 2002), individual entities playing multiple roles, as well as treatments of modality, negation, etc. AMR abstracts in numerous ways, e.g., by assigning the same conceptual structure to fear (v), fear (n), and afraid (adj). Figure 1 gives an example. In 2016 an AMR parsing shared task was held at SemEval (May, 2016). Task participants demonstrated several new directions in AMR parsing technology and also validated the strong performance of existing parsers. We sought, in 2017, to focus AMR parsing per"
S17-2090,W16-2301,0,0.0729238,"Missing"
S17-2090,P07-2045,0,0.00364823,"n. This subtask explored the suitability of that hypothesis. Given that AMRs do not capture non-semantic surface phenomena nor some essential properties of realized text such as tense, we incorporated human judgments into our evaluation, since automatic metrics against a single reference were practically guaranteed to be inadequate. 5.1 5.1.5 ISI This was an internal, non-trophy-eligible submission based on the work of Pourdamghani et al. (2016). It views generation as phrase based machine translation and learns a linearization of AMR such that the result can be used in an offthe-shelf Moses (Koehn et al., 2007) PBMT implementation. Systems Four teams participated in the task. We also included a submission from Pourdamghani et al. (2016) run by the organizer, though a priori declared that submission to be non-competitive due to a conflict of interest. Below we provide short summaries of each team’s approach. 5.1.1 5.2 Manual Evaluation We used Appraise (Federmann, 2012), an opensource system for manual evaluation of machine translation, to conduct a human evaluation of generation quality. The system asks human judges to rank randomly selected systems’ translations of sentences from the test corpus. T"
S17-2090,P13-2131,0,0.518239,"and four participated in the generation subtask. Along with a description of the task and the participants’ systems, we show various score ablations and some sample outputs. 1 The soldier was not afraid of dying. The soldier was not afraid to die. The soldier did not fear death. Figure 1: An Abstract Meaning Representation (AMR) with several English renderings. Example borrowed from Pust et al. (2015). formal compared to some of those evaluated in last year’s task, they are also very complex, and have many terms unique to the domain. An example is shown in Figure 2. We continue to use Smatch (Cai and Knight, 2013) as a metric for AMR parsing, but we perform additional ablative analysis using the approach proposed by Damonte et al. (2016). Along with parsing into AMR, it is important to encourage improvements in automatic generation of natural language (NL) text from AMR. Humans favor communication in NL. An AI that is able to parse text into AMR at a quality level indistinguishable from humans may be said to understand NL, but without the ability to render its own semantic representations into NL no human will ever be able to appreciate this. The advent of several systems that generate English text fro"
S17-2090,S17-2096,0,0.0798888,"Missing"
S17-2090,S16-1186,0,0.454156,"for AMR parsing, but we perform additional ablative analysis using the approach proposed by Damonte et al. (2016). Along with parsing into AMR, it is important to encourage improvements in automatic generation of natural language (NL) text from AMR. Humans favor communication in NL. An AI that is able to parse text into AMR at a quality level indistinguishable from humans may be said to understand NL, but without the ability to render its own semantic representations into NL no human will ever be able to appreciate this. The advent of several systems that generate English text from AMR input (Flanigan et al., 2016b; Pourdamghani et al., 2016) inspired us to conduct a generation-based shared task from AMRs in the news/discussion forum domain. For the generation subtask, we solicited human judgments of sentence quality. We followed the precedent established by the Workshop in Machine Translation (Bojar et al., 2016) and used the Appraise solicitation system (Federmann, 2012), lightly modIntroduction Abstract Meaning Representation (AMR) is a compact, readable, whole-sentence semantic annotation (Banarescu et al., 2013). It includes entity identification and typing, PropBank semantic roles (Kingsbury and"
S17-2090,S16-1166,1,0.676476,"d the Appraise solicitation system (Federmann, 2012), lightly modIntroduction Abstract Meaning Representation (AMR) is a compact, readable, whole-sentence semantic annotation (Banarescu et al., 2013). It includes entity identification and typing, PropBank semantic roles (Kingsbury and Palmer, 2002), individual entities playing multiple roles, as well as treatments of modality, negation, etc. AMR abstracts in numerous ways, e.g., by assigning the same conceptual structure to fear (v), fear (n), and afraid (adj). Figure 1 gives an example. In 2016 an AMR parsing shared task was held at SemEval (May, 2016). Task participants demonstrated several new directions in AMR parsing technology and also validated the strong performance of existing parsers. We sought, in 2017, to focus AMR parsing performance on the biomedical domain, for which a not insignificant but still relatively small training corpus had been produced. While sentences from this domain are quite 536 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 536–545, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics Interestingly, serpinE2 mRNA and protein were a"
S17-2090,N16-1087,0,0.393486,"for AMR parsing, but we perform additional ablative analysis using the approach proposed by Damonte et al. (2016). Along with parsing into AMR, it is important to encourage improvements in automatic generation of natural language (NL) text from AMR. Humans favor communication in NL. An AI that is able to parse text into AMR at a quality level indistinguishable from humans may be said to understand NL, but without the ability to render its own semantic representations into NL no human will ever be able to appreciate this. The advent of several systems that generate English text from AMR input (Flanigan et al., 2016b; Pourdamghani et al., 2016) inspired us to conduct a generation-based shared task from AMRs in the news/discussion forum domain. For the generation subtask, we solicited human judgments of sentence quality. We followed the precedent established by the Workshop in Machine Translation (Bojar et al., 2016) and used the Appraise solicitation system (Federmann, 2012), lightly modIntroduction Abstract Meaning Representation (AMR) is a compact, readable, whole-sentence semantic annotation (Banarescu et al., 2013). It includes entity identification and typing, PropBank semantic roles (Kingsbury and"
S17-2090,S17-2156,0,0.447122,"Missing"
S17-2090,D14-1048,0,0.0350246,".8 Parsing evaluation set Generation evaluation set (LDC2016R33) Domain News/Forum Biomedical Biomedical News/Form Train 36,521 5,452 N/A N/A Dev 1,368 500 N/A N/A Test 1,371 500 N/A N/A Eval N/A N/A 500 1,293 Table 1: A summary of data used in this task; split sizes indicate the number of AMRs per sub-corpus. Figure 3: The Appraise interface, adapted for AMR generation evaluation. • The JAMR (Flanigan et al., 2016b) generation system, as a strong generation baseline. asked to produce AMR graphs. Main results and ablative results are shown in Table 2. • An unsupervised AMR-to-English aligner (Pourdamghani et al., 2014).6 4.1 Five teams participated in the task, a noticeable decline from last year’s task, which saw eleven full participants. One team submitted two systems for a total of six distinct systems. Two teams were repeats from last year: CMU and RIGOTRIO (previously RIGA). Below are brief descriptions of each of the various systems, based on summaries provided by the system authors. Readers are encouraged to consult individual system description papers or relevant conference paper descriptions for more details. • The same Smatch (Cai and Knight, 2013) scoring script used in the evaluation.7 • A Pytho"
S17-2090,P14-1134,0,0.143029,"sks separately, in, respectively, Sections 4 and 5. Readers interested in only one of these subtasks should not feel compelled to read the other section. We will reconvene in Section 6 to conclude and discuss hardware, as we continue the tradition established last year in the awarding of trophies to the declared winners of each subtask. 2 3 Other Resources We made the following resources available to participants: • The tokenizer (from Ulf Hermjakob) used to produce the tokenized sentences in the training corpus.2 • The AMR specification, used by annotators in producing the AMRs.3 • The JAMR (Flanigan et al., 2014)4 and CAMR (Wang et al., 2015a)5 parsers, as strong parser baselines. Data 1 PMIDs 24651010, 11777939, and 15630473 http://alt.qcri.org/semeval2016/ task8/data/uploads/tokenizer.tar.gz 3 https://github.com/ kevincrawfordknight/amr-guidelines/blob/ master/amr.md 4 https://github.com/jflanigan/jamr 5 https://github.com/c-amr/camr 2 LDC released a new corpus of AMRs (LDC2016E25), created as part of the DARPA DEFT program, in March of 2016. The new corpus, which was annotated by teams at SDL, LDC, and the University of Colorado, and su537 Corpus LDC2016E25 Bio-AMR v0.8 Parsing evaluation set Gener"
S17-2090,W16-6603,0,0.298085,"perform additional ablative analysis using the approach proposed by Damonte et al. (2016). Along with parsing into AMR, it is important to encourage improvements in automatic generation of natural language (NL) text from AMR. Humans favor communication in NL. An AI that is able to parse text into AMR at a quality level indistinguishable from humans may be said to understand NL, but without the ability to render its own semantic representations into NL no human will ever be able to appreciate this. The advent of several systems that generate English text from AMR input (Flanigan et al., 2016b; Pourdamghani et al., 2016) inspired us to conduct a generation-based shared task from AMRs in the news/discussion forum domain. For the generation subtask, we solicited human judgments of sentence quality. We followed the precedent established by the Workshop in Machine Translation (Bojar et al., 2016) and used the Appraise solicitation system (Federmann, 2012), lightly modIntroduction Abstract Meaning Representation (AMR) is a compact, readable, whole-sentence semantic annotation (Banarescu et al., 2013). It includes entity identification and typing, PropBank semantic roles (Kingsbury and Palmer, 2002), individual ent"
S17-2090,S16-1185,0,0.0651592,"Missing"
S17-2090,D15-1136,1,0.880374,"nces in the biomedical domain. In the generation subtask, participants were asked to generate English sentences given AMR graphs in the news/forum domain. A total of five sites participated in the parsing subtask, and four participated in the generation subtask. Along with a description of the task and the participants’ systems, we show various score ablations and some sample outputs. 1 The soldier was not afraid of dying. The soldier was not afraid to die. The soldier did not fear death. Figure 1: An Abstract Meaning Representation (AMR) with several English renderings. Example borrowed from Pust et al. (2015). formal compared to some of those evaluated in last year’s task, they are also very complex, and have many terms unique to the domain. An example is shown in Figure 2. We continue to use Smatch (Cai and Knight, 2013) as a metric for AMR parsing, but we perform additional ablative analysis using the approach proposed by Damonte et al. (2016). Along with parsing into AMR, it is important to encourage improvements in automatic generation of natural language (NL) text from AMR. Humans favor communication in NL. An AI that is able to parse text into AMR at a quality level indistinguishable from hu"
S17-2090,S17-2159,0,0.218485,"Missing"
S17-2090,W14-3301,0,0.0384147,"Missing"
S17-2090,S17-2158,0,0.0996119,"Missing"
S17-2090,S17-2160,0,0.0987411,"Missing"
S17-2090,P15-2141,0,0.456698,"Sections 4 and 5. Readers interested in only one of these subtasks should not feel compelled to read the other section. We will reconvene in Section 6 to conclude and discuss hardware, as we continue the tradition established last year in the awarding of trophies to the declared winners of each subtask. 2 3 Other Resources We made the following resources available to participants: • The tokenizer (from Ulf Hermjakob) used to produce the tokenized sentences in the training corpus.2 • The AMR specification, used by annotators in producing the AMRs.3 • The JAMR (Flanigan et al., 2014)4 and CAMR (Wang et al., 2015a)5 parsers, as strong parser baselines. Data 1 PMIDs 24651010, 11777939, and 15630473 http://alt.qcri.org/semeval2016/ task8/data/uploads/tokenizer.tar.gz 3 https://github.com/ kevincrawfordknight/amr-guidelines/blob/ master/amr.md 4 https://github.com/jflanigan/jamr 5 https://github.com/c-amr/camr 2 LDC released a new corpus of AMRs (LDC2016E25), created as part of the DARPA DEFT program, in March of 2016. The new corpus, which was annotated by teams at SDL, LDC, and the University of Colorado, and su537 Corpus LDC2016E25 Bio-AMR v0.8 Parsing evaluation set Generation evaluation set (LDC2016"
S17-2090,N15-1040,0,0.525803,"Sections 4 and 5. Readers interested in only one of these subtasks should not feel compelled to read the other section. We will reconvene in Section 6 to conclude and discuss hardware, as we continue the tradition established last year in the awarding of trophies to the declared winners of each subtask. 2 3 Other Resources We made the following resources available to participants: • The tokenizer (from Ulf Hermjakob) used to produce the tokenized sentences in the training corpus.2 • The AMR specification, used by annotators in producing the AMRs.3 • The JAMR (Flanigan et al., 2014)4 and CAMR (Wang et al., 2015a)5 parsers, as strong parser baselines. Data 1 PMIDs 24651010, 11777939, and 15630473 http://alt.qcri.org/semeval2016/ task8/data/uploads/tokenizer.tar.gz 3 https://github.com/ kevincrawfordknight/amr-guidelines/blob/ master/amr.md 4 https://github.com/jflanigan/jamr 5 https://github.com/c-amr/camr 2 LDC released a new corpus of AMRs (LDC2016E25), created as part of the DARPA DEFT program, in March of 2016. The new corpus, which was annotated by teams at SDL, LDC, and the University of Colorado, and su537 Corpus LDC2016E25 Bio-AMR v0.8 Parsing evaluation set Generation evaluation set (LDC2016"
S17-2090,S16-1181,0,0.0943569,"Missing"
W15-1406,C04-1180,0,0.0555953,"edge base Parse Logical form converter LF Abductive reasoner Interpretation Conceptual metaphor domains CM extractor & scorer Figure 1: Abduction-based metaphor processing pipeline. basic natural language explanation of the conceptual metaphor identified by abduction. The effectiveness of this approach was validated by expert linguists for English and Russian metaphors. A diagram of the interpretation pipeline is shown in Figure 1. To process a text fragment containing a metaphor, this system generates logical forms (LFs) in the style of Hobbs (1985) by postprocessing the output of the Boxer (Bos et al., 2004) and Malt (Nivre et al., 2006) dependency parsers. A logical form is a conjunction of propositions, where argument links show the relationships among the constituents. An advantage of LFs over the direct use of dependency structures is that they generalize over syntax and they link arguments using long-distance dependencies. While this process is generally reliable, it can result in incorrect part-of-speech suffixes on predicates or inaccurate linking of arguments. Along with appropriate knowledge bases, the sentential logical forms are input to an engine for weighted abduction based on intege"
W15-1406,J91-1003,0,0.835879,"give deeper analysis, as discussed in section 5. This paper’s main contribution is the use of annotated collections of metaphors, describing seven target concepts in terms of 67 source concepts in four languages, to learn the lexical axioms needed for high-precision abductive metaphor mapping. 2 Related Work Metaphor has been studied extensively in the fields of linguistics, philosophy, and cognitive science (e.g., Lakoff and Johnson, 1980; Lakoff, 1992; Gentner et al., 2002). Computational research on metaphor has focused on the problems of (1) identifying linguistic metaphors in text (e.g., Fass, 1991; Birke and Sarkar, 2006; Shutova et al., 2010; Li and Sporleder, 2010; Tsvetkov et al., 2014) and (2) identifying the source and target concepts invoked by each linguistic metaphor. 50 Proceedings of the Third Workshop on Metaphor in NLP, pages 50–55, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics Knowledge-based approaches to identifying conceptual metaphors include that of Hobbs (1992), described in the following section, KARMA (Narayanan, 1997, 1999), and ATT-Meta (Barnden and Lee, 2002; Agerri et al., 2007). These have relied on the use of manually coded"
W15-1406,P85-1008,1,0.618125,"good memory, etc. 51 Text (LMs) Parser Boxer/Malt Knowledge base Parse Logical form converter LF Abductive reasoner Interpretation Conceptual metaphor domains CM extractor & scorer Figure 1: Abduction-based metaphor processing pipeline. basic natural language explanation of the conceptual metaphor identified by abduction. The effectiveness of this approach was validated by expert linguists for English and Russian metaphors. A diagram of the interpretation pipeline is shown in Figure 1. To process a text fragment containing a metaphor, this system generates logical forms (LFs) in the style of Hobbs (1985) by postprocessing the output of the Boxer (Bos et al., 2004) and Malt (Nivre et al., 2006) dependency parsers. A logical form is a conjunction of propositions, where argument links show the relationships among the constituents. An advantage of LFs over the direct use of dependency structures is that they generalize over syntax and they link arguments using long-distance dependencies. While this process is generally reliable, it can result in incorrect part-of-speech suffixes on predicates or inaccurate linking of arguments. Along with appropriate knowledge bases, the sentential logical forms"
W15-1406,W14-2305,1,0.602889,"ingual Metaphors Jonathan Gordon, Jerry R. Hobbs, and Jonathan May Information Sciences Institute University of Southern California Marina del Rey, CA {jgordon,hobbs,jonmay}@isi.edu Abstract Metaphor is a cognitive phenomenon exhibited in language, where one conceptual domain (the target) is thought of in terms of another (the source). The first level of metaphor interpretation is the mapping of linguistic metaphors to pairs of source and target concepts. Based on the abductive approach to metaphor interpretation proposed by Hobbs (1992) and implemented in the open-source Metaphor-ADP system (Ovchinnikova et al., 2014), we present work to automatically learn knowledge bases to support high-precision conceptual metaphor mapping in English, Spanish, Farsi, and Russian. 1 Introduction In everyday speech and text, people talk about one conceptual domain (the target) in terms of another (the source). According to Lakoff and Johnson (1980) and others, these linguistic metaphors (LMs) are an observable manifestation of our mental, conceptual metaphors (CMs). Computational research on metaphor is important: If natural-language systems treat metaphors at face value, meaning can be missed, resulting in absurd or triv"
W15-1406,N10-1039,0,0.0856645,"r’s main contribution is the use of annotated collections of metaphors, describing seven target concepts in terms of 67 source concepts in four languages, to learn the lexical axioms needed for high-precision abductive metaphor mapping. 2 Related Work Metaphor has been studied extensively in the fields of linguistics, philosophy, and cognitive science (e.g., Lakoff and Johnson, 1980; Lakoff, 1992; Gentner et al., 2002). Computational research on metaphor has focused on the problems of (1) identifying linguistic metaphors in text (e.g., Fass, 1991; Birke and Sarkar, 2006; Shutova et al., 2010; Li and Sporleder, 2010; Tsvetkov et al., 2014) and (2) identifying the source and target concepts invoked by each linguistic metaphor. 50 Proceedings of the Third Workshop on Metaphor in NLP, pages 50–55, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics Knowledge-based approaches to identifying conceptual metaphors include that of Hobbs (1992), described in the following section, KARMA (Narayanan, 1997, 1999), and ATT-Meta (Barnden and Lee, 2002; Agerri et al., 2007). These have relied on the use of manually coded knowledge, limiting their ability to scale across domains and language"
W15-1406,nivre-etal-2006-maltparser,0,0.0151617,"converter LF Abductive reasoner Interpretation Conceptual metaphor domains CM extractor & scorer Figure 1: Abduction-based metaphor processing pipeline. basic natural language explanation of the conceptual metaphor identified by abduction. The effectiveness of this approach was validated by expert linguists for English and Russian metaphors. A diagram of the interpretation pipeline is shown in Figure 1. To process a text fragment containing a metaphor, this system generates logical forms (LFs) in the style of Hobbs (1985) by postprocessing the output of the Boxer (Bos et al., 2004) and Malt (Nivre et al., 2006) dependency parsers. A logical form is a conjunction of propositions, where argument links show the relationships among the constituents. An advantage of LFs over the direct use of dependency structures is that they generalize over syntax and they link arguments using long-distance dependencies. While this process is generally reliable, it can result in incorrect part-of-speech suffixes on predicates or inaccurate linking of arguments. Along with appropriate knowledge bases, the sentential logical forms are input to an engine for weighted abduction based on integer linear programming (Inoue an"
W15-1406,C10-1113,0,0.0489568,"n section 5. This paper’s main contribution is the use of annotated collections of metaphors, describing seven target concepts in terms of 67 source concepts in four languages, to learn the lexical axioms needed for high-precision abductive metaphor mapping. 2 Related Work Metaphor has been studied extensively in the fields of linguistics, philosophy, and cognitive science (e.g., Lakoff and Johnson, 1980; Lakoff, 1992; Gentner et al., 2002). Computational research on metaphor has focused on the problems of (1) identifying linguistic metaphors in text (e.g., Fass, 1991; Birke and Sarkar, 2006; Shutova et al., 2010; Li and Sporleder, 2010; Tsvetkov et al., 2014) and (2) identifying the source and target concepts invoked by each linguistic metaphor. 50 Proceedings of the Third Workshop on Metaphor in NLP, pages 50–55, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics Knowledge-based approaches to identifying conceptual metaphors include that of Hobbs (1992), described in the following section, KARMA (Narayanan, 1997, 1999), and ATT-Meta (Barnden and Lee, 2002; Agerri et al., 2007). These have relied on the use of manually coded knowledge, limiting their ability to scale acr"
W15-1406,C12-2109,0,0.0376563,"Missing"
W15-1406,P14-1024,0,0.0466056,"the use of annotated collections of metaphors, describing seven target concepts in terms of 67 source concepts in four languages, to learn the lexical axioms needed for high-precision abductive metaphor mapping. 2 Related Work Metaphor has been studied extensively in the fields of linguistics, philosophy, and cognitive science (e.g., Lakoff and Johnson, 1980; Lakoff, 1992; Gentner et al., 2002). Computational research on metaphor has focused on the problems of (1) identifying linguistic metaphors in text (e.g., Fass, 1991; Birke and Sarkar, 2006; Shutova et al., 2010; Li and Sporleder, 2010; Tsvetkov et al., 2014) and (2) identifying the source and target concepts invoked by each linguistic metaphor. 50 Proceedings of the Third Workshop on Metaphor in NLP, pages 50–55, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics Knowledge-based approaches to identifying conceptual metaphors include that of Hobbs (1992), described in the following section, KARMA (Narayanan, 1997, 1999), and ATT-Meta (Barnden and Lee, 2002; Agerri et al., 2007). These have relied on the use of manually coded knowledge, limiting their ability to scale across domains and languages. As an alternative to"
W15-1406,W11-0124,1,\N,Missing
W15-1406,N10-1147,0,\N,Missing
W15-1406,E06-1042,0,\N,Missing
W15-1407,nivre-etal-2006-maltparser,0,\N,Missing
W15-1407,C10-1113,0,\N,Missing
W15-1407,N10-1147,0,\N,Missing
W15-1407,J91-1003,0,\N,Missing
W15-1407,E06-1042,0,\N,Missing
W15-1407,N10-1039,0,\N,Missing
W15-1407,shaikh-etal-2014-multi,0,\N,Missing
W15-1407,C14-1165,1,\N,Missing
W15-1407,W15-1406,1,\N,Missing
W15-1407,W13-2322,0,\N,Missing
W15-1407,W14-2305,1,\N,Missing
W15-1407,alonge-2006-italian,0,\N,Missing
W18-1505,P17-4008,1,0.80681,": An overview (upper) and an example (lower) of the proposed analyze-to-generate story framework. erate stories. Martin et al. (2017) train a recurrent encoder-decoder neural network (Sutskever et al., 2014) to predict the next event in the story. Despite significant progress in automatic story generation, there has been less emphasis on controllability: having a system takes human inputs and composes stories accordingly. With the recent successes on controllable generation of images (Chen et al., 2016; Siddharth et al., 2017; Lample et al., 2017), dialog responses (Wang et al., 2017), poems (Ghazvininejad et al., 2017), and different styles of text (Hu et al., 2017; Ficler and Goldberg, 2017; Shen et al., 2017; Fu et al., 2017). people would want to control a story generation system to produce interesting and personalized stories. This paper emphasizes the controllability aspect. We propose a completely data-driven approach towards controllable story generation by analyzing the existing story corpora. First, an analyzer extracts control factors from existing stories, and then a generator learns to generate stories according to the control factors. This creates an excellent interface for humans to interact:"
W18-1505,P17-4012,0,0.0146337,"ence of words ki = {ki,1 , ki,2 , . . . , ki,r } from each story xi . The ki s are ordered according to their order in the story. We adapt the RAKE algorithm (Rose et al., 2010) for keyword extraction, which builds document graphs and weights the importance of each word combining several word-level and graphlevel criteria. We extract the most important word from each sentence as the storyline. Generator. The generator for storylinecontrolled generation is also a conditional language model. Specifically, we employ the seq2seq model with attention (Bahdanau et al., 2014) implemented in OpenNMT (Klein et al., 2017). Specifically, the storyline words are encoded into vectors by a BiLSTM: hk = BiLST M (k) = → − ← − [ h k ; h k ], and the decoder generate each word according to the probability: att st = F (wt−1 , st−1 , ct ) r X ct = αtj hkj g() again denotes the softmax function, and V l denotes parameters that perform a linear transformation. F att () in Equation 5b denotes the computations of an LSTM-cell with attention mechanism, where the context vector ct is computed by an weighted summation of the storyline words vectors as in Equation 5c, and the weights are computed from some alignment function a("
W18-1505,D15-1167,0,0.0271169,"ending valence labels to facilitate the computation. Formally, we learn an embedding matrix E l to map each label lk into a vector: li = f v (xi ), where i indexes instances. Since there is no prior work on analyzing story ending valence, we build our own analyzer by collecting some annotations for story ending valence from Amazon Mechanical Turk (AMT) and building a supervised classifier. We employ an LSTM-based logistic regression classifier as it learns feature representations that capture long-term dependencies between the words, and has been shown efficient in text classification tasks (Tang et al., 2015). ekl = E l [lk ], 44 Agreement experiment Researcher vs. Researcher Turkers vs. Researcher Classifier vs. Turkers Always happyEnding where E l is a m × p matrix that maps each label (p of them) into a m-dimensional vector. The ending valence embeddings dimension are made the same as the word embedding dimension for simplicity. We add the ending valence as follows: p(wt |w1t−1 , l; θ) =  g(V F(el , F(wt−1 , ht−1 ))), t = s g(V F(wt−1 , ht−1 )), t = others valence. Labels are happyEnding, sadEnding, or cannotTell. The automatic classifier trained on 3980 turker annotated stories achieved much"
W18-1505,D17-1228,0,0.0622079,"Missing"
W18-1505,N16-1098,0,0.0786322,"→ − ← − [ h k ; h k ], and the decoder generate each word according to the probability: att st = F (wt−1 , st−1 , ct ) r X ct = αtj hkj g() again denotes the softmax function, and V l denotes parameters that perform a linear transformation. F att () in Equation 5b denotes the computations of an LSTM-cell with attention mechanism, where the context vector ct is computed by an weighted summation of the storyline words vectors as in Equation 5c, and the weights are computed from some alignment function a() as in Equation 5d. 3 Experimental Setup We conduct experiments on the ROCstories dataset (Mostafazadeh et al., 2016), which consists of 98,162 five-line stories for training, and 1871 stories each for the development and test sets. We treat the first four sentences of each story as the body and the last sentence as the ending. We build analyzers to annotate the ending valence and the storyline for every story, and train the two controlled generators with 98,162 annotated stories. 3.1 Ending Valence Annotation We conduct a three-stage data collection procedure to gather ending valence annotations and train a classifier to analyze the whole corpora. We classify all the stories into happyEnding, sadEnding, or"
W18-1505,W17-0911,0,0.0151733,"types of texts, such as novels, movies, and news articles. Automatic story generation efforts started as early as the 1970s with the TALE-SPIN system (Meehan, 1977). Early attempts in this field relied on symbolic planning (Meehan, 1977; Lebowitz, 1987; Turner, 1993; Bringsjord and Ferrucci, 1999; Perez and Sharples, 2001; Riedl and Young, 2010), casebased reasoning (Gervas et al., 2005), or generalizing knowledge from existing stories to assemble new ones (Swanson and Gordon, 2012; Li et al., 2013). In recent years, deep learning models are used to capture higher level structure in stories. Roemmele et al. (2017) use skip-thought vectors (Kiros et al., 2015) to encode sentences, and a Long Short-Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997) to gen1 Structured Control Factors Happy or sad endings. 43 Proceedings of the First Workshop on Storytelling, pages 43–49 c New Orleans, Louisiana, June 5, 2018. 2018 Association for Computational Linguistics Specifically, we use a bidirectional-LSTM to encode an input story into a sequence of vector representations hi = {hi,1 , hi,2 , · · · , hi,T }, where → − ← − hi = BiLST M (xi ) = [ h i ; h i ], T denotes the story length, [, :, ] denotes elem"
