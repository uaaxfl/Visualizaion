2020.acl-main.577,N19-1078,0,0.067223,"Missing"
2020.acl-main.577,C18-1139,0,0.128229,"Missing"
2020.acl-main.577,W07-1009,0,0.0601862,"and its siblings provided better language models that turned again into higher scores for NER. Lample et al. (2016) cast NER as transitionbased dependency parsing using a Stack-LSTM. They compare with a LSTM-CRF model which turns out to be a very strong baseline. Their transition-based system uses two transitions (shift and reduce) to mark the named entities and handles flat NER while our system has been designed to handle both nested and flat entities. Nested Named Entity Recognition. Early work on nested NER, motivated particularly by the GENIA corpus, includes (Shen et al., 2003; Beatrice Alex and Grover, 2007; Finkel and Manning, 2009). Finkel and Manning (2009) also proposed a constituency parsing-based approach. In the last years, we saw an increasing number of neural models targeting nested NER as well. Ju et al. (2018) suggested a LSTM-CRF model to predict nested named entities. Their algorithm iteratively continues until no further entities are predicted. Lin et al. (2019) tackle the problem in two steps: they first detect the entity head, and then they infer the entity boundaries as well as the category of the named entity. Strakov´a et al. (2019) tag the nested named entity by a sequence-to"
2020.acl-main.577,N18-1131,0,0.264782,"h turns out to be a very strong baseline. Their transition-based system uses two transitions (shift and reduce) to mark the named entities and handles flat NER while our system has been designed to handle both nested and flat entities. Nested Named Entity Recognition. Early work on nested NER, motivated particularly by the GENIA corpus, includes (Shen et al., 2003; Beatrice Alex and Grover, 2007; Finkel and Manning, 2009). Finkel and Manning (2009) also proposed a constituency parsing-based approach. In the last years, we saw an increasing number of neural models targeting nested NER as well. Ju et al. (2018) suggested a LSTM-CRF model to predict nested named entities. Their algorithm iteratively continues until no further entities are predicted. Lin et al. (2019) tackle the problem in two steps: they first detect the entity head, and then they infer the entity boundaries as well as the category of the named entity. Strakov´a et al. (2019) tag the nested named entity by a sequence-to-sequence model exploring combinations of context-based embeddings such as ELMo, BERT, and Flair. Zheng et al. (2019) use a boundary aware network to solve the nested NER. Similar to our work, Sohrab and Miwa (2018) Me"
2020.acl-main.577,P19-1066,0,0.0290515,"l exploring combinations of context-based embeddings such as ELMo, BERT, and Flair. Zheng et al. (2019) use a boundary aware network to solve the nested NER. Similar to our work, Sohrab and Miwa (2018) Methods Our model is inspired by the dependency parsing model of Dozat and Manning (2017). We use both word embeddings and character embeddings as input, and feed the output into a BiLSTM and finally to a biaffine classifier. Figure 1 shows an overview of the architecture. To encode words, we use both BERTLarge and fastText embeddings (Bojanowski et al., 2016). For BERT we follow the recipe of (Kantor and Globerson, 2019) to obtain the context dependent embeddings for a target token with 64 surrounding tokens each side. For the character-based word embeddings, we use a CNN to encode the characters of the tokens. The concatenation of the word and character-based word embeddings is feed into a BiLSTM to obtain the word representations (x). After obtaining the word representations from the BiLSTM, we apply two separate FFNNs to create different representations (hs /he ) for the start/end of the spans. Using different representations for the start/end of the spans allow the system to learn to identify the start/en"
2020.acl-main.577,N18-1079,0,0.112789,"Missing"
2020.acl-main.577,D18-1217,0,0.0320911,"SoTA. We provide the code as open source1 . 2 Related Work Flat Named Entity Recognition. The majority of flat NER models are based on a sequence labelling approach. Collobert et al. (2011) introduced a neural NER model that uses CNNs to encode tokens combined with a CRF layer for the classification. Many other neural systems followed this approach but used instead LSTMs to encode the input and a CRF for the prediction (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016). These latter models were later extended to use contextdependent embeddings such as ELMo (Peters et al., 2018). Clark et al. (2018) quite successfully used cross-view training (CVT) paired with multi-task learning. This method yields impressive gains for 1 The code is available at https://github.com/ juntaoy/biaffine-ner 6470 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6470–6476 c July 5 - 10, 2020. 2020 Association for Computational Linguistics enumerate exhaustively all possible spans up to a defined length by concatenating the LSTMs outputs for the start and end position and then using this to calculate a score for each span. Apart from the different network and word e"
2020.acl-main.577,P19-1511,0,0.382875,"ain difference between their model and ours is there for the use of biaffine model. Due to the biaffine model, we get a global view of the sentence while Sohrab and Miwa (2018) concatenates the output of the LSTMs of possible start and end positions up to a distinct length. Dozat and Manning (2017) demonstrated that the biaffine mapping performs significantly better than just the concatenation of pairs of LSTM outputs. Biaffine Classifier FFNN_Start FFNN_End BiLSTM BERT, fastText & Char Embeddings 3 Figure 1: The network architectures of our system. a number of NLP applications including NER. Devlin et al. (2019) invented BERT, a bidirectional transformer architecture for the training of language models. BERT and its siblings provided better language models that turned again into higher scores for NER. Lample et al. (2016) cast NER as transitionbased dependency parsing using a Stack-LSTM. They compare with a LSTM-CRF model which turns out to be a very strong baseline. Their transition-based system uses two transitions (shift and reduce) to mark the named entities and handles flat NER while our system has been designed to handle both nested and flat entities. Nested Named Entity Recognition. Early work"
2020.acl-main.577,D18-1124,0,0.0488677,"Missing"
2020.acl-main.577,N18-1202,0,0.180717,"Missing"
2020.acl-main.577,W03-1307,0,0.125749,"ing of language models. BERT and its siblings provided better language models that turned again into higher scores for NER. Lample et al. (2016) cast NER as transitionbased dependency parsing using a Stack-LSTM. They compare with a LSTM-CRF model which turns out to be a very strong baseline. Their transition-based system uses two transitions (shift and reduce) to mark the named entities and handles flat NER while our system has been designed to handle both nested and flat entities. Nested Named Entity Recognition. Early work on nested NER, motivated particularly by the GENIA corpus, includes (Shen et al., 2003; Beatrice Alex and Grover, 2007; Finkel and Manning, 2009). Finkel and Manning (2009) also proposed a constituency parsing-based approach. In the last years, we saw an increasing number of neural models targeting nested NER as well. Ju et al. (2018) suggested a LSTM-CRF model to predict nested named entities. Their algorithm iteratively continues until no further entities are predicted. Lin et al. (2019) tackle the problem in two steps: they first detect the entity head, and then they infer the entity boundaries as well as the category of the named entity. Strakov´a et al. (2019) tag the nest"
2020.acl-main.577,D18-1309,0,0.192973,"Missing"
2020.acl-main.577,P19-1527,0,0.235149,"Missing"
2020.acl-main.577,D17-1283,0,0.0938625,"Missing"
2020.acl-main.577,W02-2024,0,0.690669,"Missing"
2020.acl-main.577,D09-1015,0,\N,Missing
2020.acl-main.577,W03-0419,0,\N,Missing
2020.acl-main.577,W12-4501,0,\N,Missing
2020.acl-main.577,D15-1102,0,\N,Missing
2020.acl-main.577,N16-1030,0,\N,Missing
2020.acl-main.577,D17-1276,0,\N,Missing
2020.acl-main.577,N19-1308,0,\N,Missing
2020.acl-main.577,N19-1423,0,\N,Missing
2020.acl-main.577,D18-1019,0,\N,Missing
2020.acl-main.577,Q16-1026,0,\N,Missing
2020.acl-main.577,D19-1034,0,\N,Missing
2020.cllrd-1.4,W10-0722,0,0.012472,"the task. Users need to be motivated to rise to the challenge of difficult tasks and this is when financial incentives may prove to be too expensive on a large scale. The quality of the work produced by microworking, with appropriate post-processing, seems sufficient to train and evaluate statistical translation or transcription systems (Callison-Burch and Dredze, 2010; Marge et al., 2010). However, it varies from one task to another according to the defining parameters. Unsurprisingly, workers seem to have difficulty performing complex tasks, such as the evaluation of summarisation systems (Gillick and Liu, 2010). A task may be difficult for several reasons: the correct answer is difficult, but not impossible, to determine; the true interpretation is a difficult type of solution to determine; or that the answer is genuinely ambiguous and there is more 4.4. Limitations and Challenges One drawback to offering unconstrained inputs is that users use them in different ways. There is a risk of accounts being used for malicious content, spreading advertising or for spamming. Users have different expectations that may lead to segregation into groups and data not being entered in a fashion that is expected. A"
2020.cllrd-1.4,W11-0404,0,0.0297267,"to make annotations, with freetext comments allowed (although this is not the usual mode of interaction with the game). The pre-processing of text allows the interface to be constrained in this way, but is subject to errors in pre-processing that must also be fixed. The interface of microworking sites is also predefined and presents limitations that constitute an important issue for some tasks, for example, in annotating noun compound relations using a large taxonomy (Tratz and Hovy, 2010). In a word sense disambiguation task, considerable redesigns were required to get satisfactory results (Hong and Baker, 2011). These examples show how difficult it is to design tasks for crowdsourcing within a predefined system. The design of social network interfaces is dictated by the owners of the platforms, rather than the requester or the community of users and crowdsourcing efforts may be in conflict with other revenue-generating activities such as advertising. The interface design has an impact on the speed at which players can complete tasks, with clicking being faster than typing. A design decision to use radio buttons or freetext boxes can have a significant impact on performance (Aker et al., 2012) and re"
2020.cllrd-1.4,aker-etal-2012-assessing,1,0.776988,"ts (Hong and Baker, 2011). These examples show how difficult it is to design tasks for crowdsourcing within a predefined system. The design of social network interfaces is dictated by the owners of the platforms, rather than the requester or the community of users and crowdsourcing efforts may be in conflict with other revenue-generating activities such as advertising. The interface design has an impact on the speed at which players can complete tasks, with clicking being faster than typing. A design decision to use radio buttons or freetext boxes can have a significant impact on performance (Aker et al., 2012) and response times (Chamberlain and O’Reilly, 2014). Errors in the data constitute wasted effort and should be dealt with by bug testing the system rather than postprocessing. 4.2. than one plausible solution. The latter tasks can be rare, but are of the most interest to computational linguists and machine learning algorithms. In these cases the users need to have a thorough understanding of how to add their solutions and an unconstrained input option would capture data beyond what the interface may have been designed for; however, automatically processing these cases can be difficult. 4.3. C"
2020.cllrd-1.4,bunt-etal-2012-iso,0,0.0108293,"a minimum, with expert annotators only required to validate a small portion of the data (which is also likely to be the data of most interest them). Question answering systems attempt to learn how to answer a question automatically from a human, either from structured data or from processing natural language of existing conversations and dialogue. Here we are more interested in Community Question Answering (cQA), in which the crowd is the system that attempts to answer the question through natural language. Examples of cQA are sites such as StackOveflow3 and Yahoo Answers.4 Detailed schemas (Bunt et al., 2012) and rich feature sets (Agichtein et al., 2008) have been used to describe cQA dialogue and progress has been made to analyse this source of data automatically (Su et al., 2007). perience of progression through the game by scoring points, being assigned levels and recognising their effort. Systems are required to control the behaviour of players: to encourage them to concentrate on the tasks and to discourage them from malicious behaviour. Social computing and social networks Social computing has been described as ‘applications and services that facilitate collective action and social interact"
2020.cllrd-1.4,W10-0701,0,0.0208878,"identifying discourse-new (DN) markables, whereas discourse-old (DO) markables are more difficult (Chamberlain et al., 2016). This demonstrates that quality is not only affected by player motivation and interface design but also by the inherent difficulty of the task. Users need to be motivated to rise to the challenge of difficult tasks and this is when financial incentives may prove to be too expensive on a large scale. The quality of the work produced by microworking, with appropriate post-processing, seems sufficient to train and evaluate statistical translation or transcription systems (Callison-Burch and Dredze, 2010; Marge et al., 2010). However, it varies from one task to another according to the defining parameters. Unsurprisingly, workers seem to have difficulty performing complex tasks, such as the evaluation of summarisation systems (Gillick and Liu, 2010). A task may be difficult for several reasons: the correct answer is difficult, but not impossible, to determine; the true interpretation is a difficult type of solution to determine; or that the answer is genuinely ambiguous and there is more 4.4. Limitations and Challenges One drawback to offering unconstrained inputs is that users use them in di"
2020.cllrd-1.4,L16-1323,1,0.792316,"le and, although the majority of tasks were not hard, it is the uncommon difficult tasks that require the power of human computation. A less-constrained environment allows these difficult tasks to be solved in more organic ways compared to a fully constrained system. There is a clear difference in quality when we look at the difficulty of the tasks in Phrase Detectives. Looking separately at the agreement on each class of markable annotation, we observe near-expert quality for the simple task of identifying discourse-new (DN) markables, whereas discourse-old (DO) markables are more difficult (Chamberlain et al., 2016). This demonstrates that quality is not only affected by player motivation and interface design but also by the inherent difficulty of the task. Users need to be motivated to rise to the challenge of difficult tasks and this is when financial incentives may prove to be too expensive on a large scale. The quality of the work produced by microworking, with appropriate post-processing, seems sufficient to train and evaluate statistical translation or transcription systems (Callison-Burch and Dredze, 2010; Marge et al., 2010). However, it varies from one task to another according to the defining p"
2020.cllrd-1.4,N10-1024,0,0.0268982,"urcing by using the Web as a way of reaching large numbers of workers (often referred to as turkers) who get paid to complete small items of work called human intelligence tasks (HITs). This is typically very little, in the order of 0.01 to 0.20 US$ per HIT. A reported advantage of microworking is that the work is completed very fast. It is not uncommon for a HIT to be completed in minutes, but this is usually for simple tasks. In the case of more complex tasks, or tasks in which the worker needs to be more skilled, e.g. translating a sentence in an uncommon language, it can take much longer (Novotney and Callison-Burch, 2010). Microwork crowdsourcing is becoming a standard way of creating small-scale resources, but is prohibitively expensive to create large-scale resources. 2.1. Features of crowdsourcing tasks Crowdsourcing approaches can be distinguished by features related to the task. To clarify why these features apply to a particular approach an exemplar system is chosen for the approach that is perhaps the most prevalent or successful: Manual annotation is considered the benchmark where the task is completed by an expert; GalaxyZoo represents citizen science (although a detailed typology for citizen science"
2020.cllrd-1.4,N19-1176,1,0.86664,". 3.3. (which is flagged in a checkbox) and/or publish the comment with the corpus (in fact, all comments are published in the corpus, this flag is an indication that the administrator thought the comment was useful). Links to other comments on the same markable can be seen so they can all be dealt with at the same time. Consolidation of Unconstrained Input The constrained inputs from the players have been analysed in several ways, initially using majority voting for a collective decision making (Chamberlain et al., 2018), then with more advanced modelling through Mention-Pair Analysis (MPA) (Poesio et al., 2019). However, these techniques did not make use of any of the unconstrained data collected from the players. In order to make the unconstrained data into a more useful form it was consolidated semi-automatically (see Figure 6) and included in the corpora released for further research (Poesio et al., 2019). Each comment was classified initially by the player (by the type of skip they select) and then by an administrator. The administrator can then take action in relation to the comment, e.g., correcting markable boundaries 3.4. Data As of 18 Feb 2020 there were 114,353 skips and 76,883 comments ad"
2020.cllrd-1.4,P10-1070,0,0.0170104,"an contribute data to a crowdsourcing system. In Phrase Detectives the player is constrained to a set of 31 predefined options to make annotations, with freetext comments allowed (although this is not the usual mode of interaction with the game). The pre-processing of text allows the interface to be constrained in this way, but is subject to errors in pre-processing that must also be fixed. The interface of microworking sites is also predefined and presents limitations that constitute an important issue for some tasks, for example, in annotating noun compound relations using a large taxonomy (Tratz and Hovy, 2010). In a word sense disambiguation task, considerable redesigns were required to get satisfactory results (Hong and Baker, 2011). These examples show how difficult it is to design tasks for crowdsourcing within a predefined system. The design of social network interfaces is dictated by the owners of the platforms, rather than the requester or the community of users and crowdsourcing efforts may be in conflict with other revenue-generating activities such as advertising. The interface design has an impact on the speed at which players can complete tasks, with clicking being faster than typing. A"
2020.coling-main.315,P16-1061,0,0.02191,"19) introduced a multi-task network together with adversarial learning for underresourced NER. The evaluation on both cross-language and cross-domain settings shows that partially sharing the BiLSTM works better for cross-language transfer, while for cross-domain setting, the system performs better when the LSTM layers are fully shared. 2.3 Neural Coreference Resolution By contrast with bridging reference, coreference resolution has been extensively studied. Wiseman et al. (2015; Wiseman et al. (2016) first introduced a neural network-based approach to solving coreference in a non-linear way. Clark and Manning (2016) integrated reinforcement learning to let the model, optimized directly on the B3 scores. Lee et al. (2017) proposed a neural joint approach for mention detection and coreference resolution. Their model does not rely on parse trees; instead, the system learns to detect mentions by exploring the outputs of a BiLSTM. After the introduction of context dependent word embeddings such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), the Lee et al. (2017) system has been greatly improved by those embeddings (Lee et al., 2018; Kantor and Globerson, 2019) to achieve SoTA results. We use a"
2020.coling-main.315,P19-1595,0,0.0565616,"ences. As a consequence, a system designed for one corpus (e.g. ISNOTES) works poorly when applied to other corpora (e.g. ARRAU), and significant modifications are needed to make the system works equally well on different corpora (Roesiger, 2018). To tackle these challenges, we introduce a multi task learning-based neural model that learns bridging resolution together with coreference resolution. Multi task architectures have proven effective at exploiting the synergies between distinct but related tasks to in cases when only limited amounts of data are available for one or more of the tasks (Clark et al., 2019). Such an architecture should therefore be especially suited for our context, given that, linguistically, bridging reference resolution and coreference resolution are two distinct but closely related aspects of anaphora resolution, and indeed were often tackled together in early systems (Sidner, 1979; Vieira and Poesio, 2000). Using a neural network-based approach that minimises feature engineering enables the system to be more flexible on the choices of corpora. We mainly evaluate our system on the RST portion of the ARRAU corpus since it is the largest available resource, but we additionally"
2020.coling-main.315,I17-2016,0,0.019702,"Normally, the goal of multitask learning is to improve performance on all tasks; but in an under-resourced setting, the aim often is only to improve performance on the low resource task/language/domains (the target task). This is sometimes known as shared representation based transfer learning. Yang et al. (2017) applied transfer learning to sequence labelling tasks; the deep hierarchical recurrent neural network used in their work is fully/partially shared between the source and the target tasks. They demonstrated that SoTA performance can be achieved by using models trained on multi-tasks. Cotterell and Duh (2017) trained a neural NER system on a combination of high-/low-resource languages to improve NER for the low-resource languages. In their work, character-based embeddings are shared across the languages. Recently, Zhou et al. (2019) introduced a multi-task network together with adversarial learning for underresourced NER. The evaluation on both cross-language and cross-domain settings shows that partially sharing the BiLSTM works better for cross-language transfer, while for cross-domain setting, the system performs better when the LSTM layers are fully shared. 2.3 Neural Coreference Resolution By"
2020.coling-main.315,N19-1423,0,0.433742,"nd Poesio, 2000). Using a neural network-based approach that minimises feature engineering enables the system to be more flexible on the choices of corpora. We mainly evaluate our system on the RST portion of the ARRAU corpus since it is the largest available resource, but we additionally evaluate it on the TRAINS and PEAR portion of the ARRAU corpus, ISNOTES , BASHI and SCICORP corpus to demonstrate its tolerance of diversity. We start with a strong baseline for bridging adapted from the SoTA coreference architecture (Lee et al., 2018; Kantor and Globerson, 2019) enhanced by BERT embeddings (Devlin et al., 2019). We extend the system to multi-task learning by adding a coreference classifier that shares part of the network with the bridging classifier. In this way, we improve full bridging resolution and its subtasks (anaphor recognition and antecedent selection) by 6.5-7.3 p.p. respectively. But because the number of coreference examples is much larger than the number of bridging pairs, the dataset is highly imbalanced. We achieve further improvements of 1.7 p.p. and 6.6 p.p. on full bridging resolution and anaphor recognition by using undersampling during the training. This final system achieves SoT"
2020.coling-main.315,N13-1111,0,0.70444,"different from ARRAU, our system works equally well, achieving the new SoTA results on full bridging resolution and anaphor recognition for all six corpora as well as five corpora on antecedent selection. 2 Related Work 2.1 Bridging Reference Resolution Bridging reference resolution involves two subtasks: anaphor recognition and antecedent selection (Hou et al., 2018). Early work on bridging resolution mostly focused on definite bridging anaphors (Sidner, 1979; Vieira and Poesio, 2000; Lassalle and Denis, 2011), but later systems covered unrestricted antecedent selection (Poesio et al., 2004; Hou et al., 2013). Hou et al. (2013) introduced a model based on Markov logic networks and using an extensive set of features and constraints. They evaluated the system with both local and global features on ISNOTES, and showed that global features can greatly improve performance. The system was later extended in (Hou, 2018b; Hou, 2018a; Hou et al., 2018) to explore additional features from embeddings tailored for bridging resolution, to advanced antecedent candidate selection using the Penn Discourse Treebank (d-scope-salience). Recently, Hou (2020) framed the antecedent selection task as question answering,"
2020.coling-main.315,D14-1222,0,0.681962,"including e.g. GNOME (Poesio, 2004), ISNOTES (Markert et al., 2012), SCICORP (R¨osiger, 2016) and BASHI (R¨osiger, 2018), but they are all rather small, with at most around 1k examples of bridging reference. ARRAU (Poesio and Artstein, 2008; Uryupina et al., 2019) is much larger, but still contains only 5.5k bridging pairs. It is challenging to train a learning based system on that amount of data, particularly the new neural models. As a result, the current SoTA systems for full bridging resolution are still rule-based, employing a number of heuristic rules many of which are corpus-dependent (Hou et al., 2014; Roesiger et al., 2018). This is problematic at the light of the second challenge for work in this area: namely, that the definitions of bridging are different in these This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. 1 The code is available at https://github.com/juntaoy/dali-bridging Licence details: http:// 3534 Proceedings of the 28th International Conference on Computational Linguistics, pages 3534–3546 Barcelona, Spain (Online), December 8-13, 2020 different corpora (Roesiger et al., 2018). Existing corpora differ"
2020.coling-main.315,J18-2002,0,0.255658,"Evaluations with very different bridging corpora (ARRAU, ISNOTES, BASHI and SCICORP) suggest that our architecture works equally well on all corpora, and achieves the SoTA results on full bridging resolution for all corpora, outperforming the best reported results by up to 36.3 p.p..1 1 Introduction Anaphora resolution (Karttunen, 1976; Webber, 1979; Kamp and Reyle, 1993; Garnham, 2001; Poesio et al., 2016) is the aspect of language interpretation concerned with linking nominal expressions to entities in the context of interpretation (or discourse model). As illustrated by (1) (adapted from (Hou et al., 2018)), nominal expressions can be linked to the context in several ways: corefererence (linking [The Bakersfield Supermarket], [The business], [its]), bridging or associative reference (linking [the customers] to [the supermarket]) (Clark, 1975; Prince, 1981; Poesio et al., 2004; Hou et al., 2018), and discourse deixis (linking [the murder] to the event of murdering) (Webber, 1991; Kolhatkar et al., 2018). (1) [The Bakersfield Supermarket] went bankrupt last May. [The business] closed when [[its] old owner] was murdered by [robbers]. [The murder] saddened [the customers]. Bridging reference resolu"
2020.coling-main.315,C16-1177,0,0.107598,"oor was open.. ‘Lexical bridging’ is when the bridging reference could also be interpreted autonomously, such as Madrid in I went to Spain last year. I particularly liked Madrid. See (Poesio, 2004; Baumann and Riester, 2012; Markert et al., 2012; Hou et al., 2018; Uryupina et al., 2019) for a detailed discussion of the annotation schemes, and (Roesiger et al., 2018; Roesiger, 2018) for a discussion of the implications. 3535 ISNOTES corpus, hence perform less well on other corpora. The anaphor recognition subtask is usually solved as a part of the information status task (Markert et al., 2012; Hou, 2016; Hou et al., 2018). Recent systems for full bridging resolution include (Hou et al., 2014; Hou et al., 2018; Roesiger et al., 2018; Roesiger, 2018). Hou et al. (2014) proposed a rule-based system for full bridging resolution with the ISNOTES corpus, consisting of a rich system of rules motivated by linguistic knowledge. They also evaluated a learning-based system that uses the rules as features, but the learning-based system only outperforms the rule-based system’s F1 score by 0.1 percentage points. The rule-based system was later adapted by Roesiger et al. (2018); Roesiger (2018) for full br"
2020.coling-main.315,D18-1219,0,0.512393,"ecognition and antecedent selection (Hou et al., 2018). Early work on bridging resolution mostly focused on definite bridging anaphors (Sidner, 1979; Vieira and Poesio, 2000; Lassalle and Denis, 2011), but later systems covered unrestricted antecedent selection (Poesio et al., 2004; Hou et al., 2013). Hou et al. (2013) introduced a model based on Markov logic networks and using an extensive set of features and constraints. They evaluated the system with both local and global features on ISNOTES, and showed that global features can greatly improve performance. The system was later extended in (Hou, 2018b; Hou, 2018a; Hou et al., 2018) to explore additional features from embeddings tailored for bridging resolution, to advanced antecedent candidate selection using the Penn Discourse Treebank (d-scope-salience). Recently, Hou (2020) framed the antecedent selection task as question answering, and pre-trains the system with a large synthetic bridging corpus; this system achieves SoTA results on ISNOTES. But those systems are highly specialized for the 2 Roesiger et al use ‘referential bridging’ for the cases in which the bridging reference needs an antecedent in order to be interpretable, such as"
2020.coling-main.315,N18-2001,0,0.302826,"ecognition and antecedent selection (Hou et al., 2018). Early work on bridging resolution mostly focused on definite bridging anaphors (Sidner, 1979; Vieira and Poesio, 2000; Lassalle and Denis, 2011), but later systems covered unrestricted antecedent selection (Poesio et al., 2004; Hou et al., 2013). Hou et al. (2013) introduced a model based on Markov logic networks and using an extensive set of features and constraints. They evaluated the system with both local and global features on ISNOTES, and showed that global features can greatly improve performance. The system was later extended in (Hou, 2018b; Hou, 2018a; Hou et al., 2018) to explore additional features from embeddings tailored for bridging resolution, to advanced antecedent candidate selection using the Penn Discourse Treebank (d-scope-salience). Recently, Hou (2020) framed the antecedent selection task as question answering, and pre-trains the system with a large synthetic bridging corpus; this system achieves SoTA results on ISNOTES. But those systems are highly specialized for the 2 Roesiger et al use ‘referential bridging’ for the cases in which the bridging reference needs an antecedent in order to be interpretable, such as"
2020.coling-main.315,2020.acl-main.132,0,0.523314,"restricted antecedent selection (Poesio et al., 2004; Hou et al., 2013). Hou et al. (2013) introduced a model based on Markov logic networks and using an extensive set of features and constraints. They evaluated the system with both local and global features on ISNOTES, and showed that global features can greatly improve performance. The system was later extended in (Hou, 2018b; Hou, 2018a; Hou et al., 2018) to explore additional features from embeddings tailored for bridging resolution, to advanced antecedent candidate selection using the Penn Discourse Treebank (d-scope-salience). Recently, Hou (2020) framed the antecedent selection task as question answering, and pre-trains the system with a large synthetic bridging corpus; this system achieves SoTA results on ISNOTES. But those systems are highly specialized for the 2 Roesiger et al use ‘referential bridging’ for the cases in which the bridging reference needs an antecedent in order to be interpretable, such as the door in John walked towards the house. The door was open.. ‘Lexical bridging’ is when the bridging reference could also be interpreted autonomously, such as Madrid in I went to Spain last year. I particularly liked Madrid. See"
2020.coling-main.315,P19-1066,0,0.476159,"tackled together in early systems (Sidner, 1979; Vieira and Poesio, 2000). Using a neural network-based approach that minimises feature engineering enables the system to be more flexible on the choices of corpora. We mainly evaluate our system on the RST portion of the ARRAU corpus since it is the largest available resource, but we additionally evaluate it on the TRAINS and PEAR portion of the ARRAU corpus, ISNOTES , BASHI and SCICORP corpus to demonstrate its tolerance of diversity. We start with a strong baseline for bridging adapted from the SoTA coreference architecture (Lee et al., 2018; Kantor and Globerson, 2019) enhanced by BERT embeddings (Devlin et al., 2019). We extend the system to multi-task learning by adding a coreference classifier that shares part of the network with the bridging classifier. In this way, we improve full bridging resolution and its subtasks (anaphor recognition and antecedent selection) by 6.5-7.3 p.p. respectively. But because the number of coreference examples is much larger than the number of bridging pairs, the dataset is highly imbalanced. We achieve further improvements of 1.7 p.p. and 6.6 p.p. on full bridging resolution and anaphor recognition by using undersampling d"
2020.coling-main.315,Q18-1017,0,0.0211664,"inguistic knowledge. They also evaluated a learning-based system that uses the rules as features, but the learning-based system only outperforms the rule-based system’s F1 score by 0.1 percentage points. The rule-based system was later adapted by Roesiger et al. (2018); Roesiger (2018) for full bridging resolution on ARRAU, but since ARRAU follows a more general definition of bridging, most of the rules had to be changed. 2.2 Multi-task Learning for Under-Resourced Tasks Multi-task learning has been successfully used in several NLP applications (Collobert and Weston, 2008; Luong et al., 2016; Kiperwasser and Ballesteros, 2018; Clark et al., 2019). Normally, the goal of multitask learning is to improve performance on all tasks; but in an under-resourced setting, the aim often is only to improve performance on the low resource task/language/domains (the target task). This is sometimes known as shared representation based transfer learning. Yang et al. (2017) applied transfer learning to sequence labelling tasks; the deep hierarchical recurrent neural network used in their work is fully/partially shared between the source and the target tasks. They demonstrated that SoTA performance can be achieved by using models tr"
2020.coling-main.315,J18-3007,0,0.0835318,"et al., 2016) is the aspect of language interpretation concerned with linking nominal expressions to entities in the context of interpretation (or discourse model). As illustrated by (1) (adapted from (Hou et al., 2018)), nominal expressions can be linked to the context in several ways: corefererence (linking [The Bakersfield Supermarket], [The business], [its]), bridging or associative reference (linking [the customers] to [the supermarket]) (Clark, 1975; Prince, 1981; Poesio et al., 2004; Hou et al., 2018), and discourse deixis (linking [the murder] to the event of murdering) (Webber, 1991; Kolhatkar et al., 2018). (1) [The Bakersfield Supermarket] went bankrupt last May. [The business] closed when [[its] old owner] was murdered by [robbers]. [The murder] saddened [the customers]. Bridging reference resolution is the sub-task of anaphora resolution concerned with identifying and resolving bridging references, i.e., anaphoric references to non-identical associated antecedents. Bridging resolution is much less studied than the closely related sub-task of coreference resolution, which has received a lot of attention ((Pradhan et al., 2012; Wiseman et al., 2015; Lee et al., 2017; Lee et al., 2018), to ment"
2020.coling-main.315,D17-1018,0,0.331188,"ring) (Webber, 1991; Kolhatkar et al., 2018). (1) [The Bakersfield Supermarket] went bankrupt last May. [The business] closed when [[its] old owner] was murdered by [robbers]. [The murder] saddened [the customers]. Bridging reference resolution is the sub-task of anaphora resolution concerned with identifying and resolving bridging references, i.e., anaphoric references to non-identical associated antecedents. Bridging resolution is much less studied than the closely related sub-task of coreference resolution, which has received a lot of attention ((Pradhan et al., 2012; Wiseman et al., 2015; Lee et al., 2017; Lee et al., 2018), to mention just a few recent proposals). One reason for this is the lack of training data. Several corpora have been annotated with bridging reference, including e.g. GNOME (Poesio, 2004), ISNOTES (Markert et al., 2012), SCICORP (R¨osiger, 2016) and BASHI (R¨osiger, 2018), but they are all rather small, with at most around 1k examples of bridging reference. ARRAU (Poesio and Artstein, 2008; Uryupina et al., 2019) is much larger, but still contains only 5.5k bridging pairs. It is challenging to train a learning based system on that amount of data, particularly the new neura"
2020.coling-main.315,N18-2108,0,0.0896458,"1; Kolhatkar et al., 2018). (1) [The Bakersfield Supermarket] went bankrupt last May. [The business] closed when [[its] old owner] was murdered by [robbers]. [The murder] saddened [the customers]. Bridging reference resolution is the sub-task of anaphora resolution concerned with identifying and resolving bridging references, i.e., anaphoric references to non-identical associated antecedents. Bridging resolution is much less studied than the closely related sub-task of coreference resolution, which has received a lot of attention ((Pradhan et al., 2012; Wiseman et al., 2015; Lee et al., 2017; Lee et al., 2018), to mention just a few recent proposals). One reason for this is the lack of training data. Several corpora have been annotated with bridging reference, including e.g. GNOME (Poesio, 2004), ISNOTES (Markert et al., 2012), SCICORP (R¨osiger, 2016) and BASHI (R¨osiger, 2018), but they are all rather small, with at most around 1k examples of bridging reference. ARRAU (Poesio and Artstein, 2008; Uryupina et al., 2019) is much larger, but still contains only 5.5k bridging pairs. It is challenging to train a learning based system on that amount of data, particularly the new neural models. As a resu"
2020.coling-main.315,P12-1084,0,0.416673,"esolution is the sub-task of anaphora resolution concerned with identifying and resolving bridging references, i.e., anaphoric references to non-identical associated antecedents. Bridging resolution is much less studied than the closely related sub-task of coreference resolution, which has received a lot of attention ((Pradhan et al., 2012; Wiseman et al., 2015; Lee et al., 2017; Lee et al., 2018), to mention just a few recent proposals). One reason for this is the lack of training data. Several corpora have been annotated with bridging reference, including e.g. GNOME (Poesio, 2004), ISNOTES (Markert et al., 2012), SCICORP (R¨osiger, 2016) and BASHI (R¨osiger, 2018), but they are all rather small, with at most around 1k examples of bridging reference. ARRAU (Poesio and Artstein, 2008; Uryupina et al., 2019) is much larger, but still contains only 5.5k bridging pairs. It is challenging to train a learning based system on that amount of data, particularly the new neural models. As a result, the current SoTA systems for full bridging resolution are still rule-based, employing a number of heuristic rules many of which are corpus-dependent (Hou et al., 2014; Roesiger et al., 2018). This is problematic at th"
2020.coling-main.315,D14-1162,0,0.0854195,"ers et al., 2018) used by Lee et al. These systems have similar architecture and both do mention detection and coreference jointly. We only use the coreference part of the system, since for bridging resolution evaluation is usually on gold mentions. Our baseline system first creates representations for mentions using the output of a BiLSTM. The sentences of a document are encoded from both directions to obtain a representation for each token in the sentence. The BiLSTM takes as input the concatenated embeddings ((embt )Tt=1 ) of both word and 3536 character levels. For word embeddings, GloVe (Pennington et al., 2014) and BERT (Devlin et al., 2019) embeddings are used. Character embeddings are learned from a convolution neural network (CNN) during training. The tokens are represented by concatenated outputs from the forward and the backward LSTMs. The token representations (xt )Tt=1 are used together with head representations (hi ) to represent mentions (Mi ). The hi of a mention is obtained by applying attention over its token representations ({xbi , ..., xei }), where bi and ei are the indices of the start and the end of the mention respectively. Formally, we compute hi , Mi as follows: exp(αt ) αt = FFN"
2020.coling-main.315,N18-1202,0,0.0128467,"erence resolution has been extensively studied. Wiseman et al. (2015; Wiseman et al. (2016) first introduced a neural network-based approach to solving coreference in a non-linear way. Clark and Manning (2016) integrated reinforcement learning to let the model, optimized directly on the B3 scores. Lee et al. (2017) proposed a neural joint approach for mention detection and coreference resolution. Their model does not rely on parse trees; instead, the system learns to detect mentions by exploring the outputs of a BiLSTM. After the introduction of context dependent word embeddings such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), the Lee et al. (2017) system has been greatly improved by those embeddings (Lee et al., 2018; Kantor and Globerson, 2019) to achieve SoTA results. We use a simplified version of the model by (Lee et al., 2018; Kantor and Globerson, 2019) as baseline. 3 3.1 Methods The Single-Task Baseline System We use as our single-task baseline for bridging reference a simplified version of the SoTA coreference systems by Lee et al. (2018; Kantor and Globerson (2019), since bridging resolution is closely related to coreference: like coreference it requires establishing a link"
2020.coling-main.315,poesio-artstein-2008-anaphoric,1,0.93507,"ecedents. Bridging resolution is much less studied than the closely related sub-task of coreference resolution, which has received a lot of attention ((Pradhan et al., 2012; Wiseman et al., 2015; Lee et al., 2017; Lee et al., 2018), to mention just a few recent proposals). One reason for this is the lack of training data. Several corpora have been annotated with bridging reference, including e.g. GNOME (Poesio, 2004), ISNOTES (Markert et al., 2012), SCICORP (R¨osiger, 2016) and BASHI (R¨osiger, 2018), but they are all rather small, with at most around 1k examples of bridging reference. ARRAU (Poesio and Artstein, 2008; Uryupina et al., 2019) is much larger, but still contains only 5.5k bridging pairs. It is challenging to train a learning based system on that amount of data, particularly the new neural models. As a result, the current SoTA systems for full bridging resolution are still rule-based, employing a number of heuristic rules many of which are corpus-dependent (Hou et al., 2014; Roesiger et al., 2018). This is problematic at the light of the second challenge for work in this area: namely, that the definitions of bridging are different in these This work is licensed under a Creative Commons Attribu"
2020.coling-main.315,P04-1019,1,0.847882,"o 36.3 p.p..1 1 Introduction Anaphora resolution (Karttunen, 1976; Webber, 1979; Kamp and Reyle, 1993; Garnham, 2001; Poesio et al., 2016) is the aspect of language interpretation concerned with linking nominal expressions to entities in the context of interpretation (or discourse model). As illustrated by (1) (adapted from (Hou et al., 2018)), nominal expressions can be linked to the context in several ways: corefererence (linking [The Bakersfield Supermarket], [The business], [its]), bridging or associative reference (linking [the customers] to [the supermarket]) (Clark, 1975; Prince, 1981; Poesio et al., 2004; Hou et al., 2018), and discourse deixis (linking [the murder] to the event of murdering) (Webber, 1991; Kolhatkar et al., 2018). (1) [The Bakersfield Supermarket] went bankrupt last May. [The business] closed when [[its] old owner] was murdered by [robbers]. [The murder] saddened [the customers]. Bridging reference resolution is the sub-task of anaphora resolution concerned with identifying and resolving bridging references, i.e., anaphoric references to non-identical associated antecedents. Bridging resolution is much less studied than the closely related sub-task of coreference resolution,"
2020.coling-main.315,W04-0210,1,0.776862,"s]. Bridging reference resolution is the sub-task of anaphora resolution concerned with identifying and resolving bridging references, i.e., anaphoric references to non-identical associated antecedents. Bridging resolution is much less studied than the closely related sub-task of coreference resolution, which has received a lot of attention ((Pradhan et al., 2012; Wiseman et al., 2015; Lee et al., 2017; Lee et al., 2018), to mention just a few recent proposals). One reason for this is the lack of training data. Several corpora have been annotated with bridging reference, including e.g. GNOME (Poesio, 2004), ISNOTES (Markert et al., 2012), SCICORP (R¨osiger, 2016) and BASHI (R¨osiger, 2018), but they are all rather small, with at most around 1k examples of bridging reference. ARRAU (Poesio and Artstein, 2008; Uryupina et al., 2019) is much larger, but still contains only 5.5k bridging pairs. It is challenging to train a learning based system on that amount of data, particularly the new neural models. As a result, the current SoTA systems for full bridging resolution are still rule-based, employing a number of heuristic rules many of which are corpus-dependent (Hou et al., 2014; Roesiger et al.,"
2020.coling-main.315,W12-4501,0,0.140237,"(linking [the murder] to the event of murdering) (Webber, 1991; Kolhatkar et al., 2018). (1) [The Bakersfield Supermarket] went bankrupt last May. [The business] closed when [[its] old owner] was murdered by [robbers]. [The murder] saddened [the customers]. Bridging reference resolution is the sub-task of anaphora resolution concerned with identifying and resolving bridging references, i.e., anaphoric references to non-identical associated antecedents. Bridging resolution is much less studied than the closely related sub-task of coreference resolution, which has received a lot of attention ((Pradhan et al., 2012; Wiseman et al., 2015; Lee et al., 2017; Lee et al., 2018), to mention just a few recent proposals). One reason for this is the lack of training data. Several corpora have been annotated with bridging reference, including e.g. GNOME (Poesio, 2004), ISNOTES (Markert et al., 2012), SCICORP (R¨osiger, 2016) and BASHI (R¨osiger, 2018), but they are all rather small, with at most around 1k examples of bridging reference. ARRAU (Poesio and Artstein, 2008; Uryupina et al., 2019) is much larger, but still contains only 5.5k bridging pairs. It is challenging to train a learning based system on that am"
2020.coling-main.315,C18-1298,0,0.612667,"ME (Poesio, 2004), ISNOTES (Markert et al., 2012), SCICORP (R¨osiger, 2016) and BASHI (R¨osiger, 2018), but they are all rather small, with at most around 1k examples of bridging reference. ARRAU (Poesio and Artstein, 2008; Uryupina et al., 2019) is much larger, but still contains only 5.5k bridging pairs. It is challenging to train a learning based system on that amount of data, particularly the new neural models. As a result, the current SoTA systems for full bridging resolution are still rule-based, employing a number of heuristic rules many of which are corpus-dependent (Hou et al., 2014; Roesiger et al., 2018). This is problematic at the light of the second challenge for work in this area: namely, that the definitions of bridging are different in these This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. 1 The code is available at https://github.com/juntaoy/dali-bridging Licence details: http:// 3534 Proceedings of the 28th International Conference on Computational Linguistics, pages 3534–3546 Barcelona, Spain (Online), December 8-13, 2020 different corpora (Roesiger et al., 2018). Existing corpora differ in whether they attempt"
2020.coling-main.315,W18-0703,0,0.57504,"ent corpora (Roesiger et al., 2018). Existing corpora differ in whether they attempt to annotate only what Roesiger et al call referential bridging (as in ISNOTES), or the full range of bridging references, as in ARRAU.2 The ISNOTES, BASHI and SCICORP corpora consist mostly of referential bridging examples, while the ARRAU corpus contains both types of bridging references. As a consequence, a system designed for one corpus (e.g. ISNOTES) works poorly when applied to other corpora (e.g. ARRAU), and significant modifications are needed to make the system works equally well on different corpora (Roesiger, 2018). To tackle these challenges, we introduce a multi task learning-based neural model that learns bridging resolution together with coreference resolution. Multi task architectures have proven effective at exploiting the synergies between distinct but related tasks to in cases when only limited amounts of data are available for one or more of the tasks (Clark et al., 2019). Such an architecture should therefore be especially suited for our context, given that, linguistically, bridging reference resolution and coreference resolution are two distinct but closely related aspects of anaphora resolut"
2020.coling-main.315,L16-1275,0,0.380452,"Missing"
2020.coling-main.315,L18-1058,0,0.167767,"Missing"
2020.coling-main.315,J00-4003,1,0.763598,"l model that learns bridging resolution together with coreference resolution. Multi task architectures have proven effective at exploiting the synergies between distinct but related tasks to in cases when only limited amounts of data are available for one or more of the tasks (Clark et al., 2019). Such an architecture should therefore be especially suited for our context, given that, linguistically, bridging reference resolution and coreference resolution are two distinct but closely related aspects of anaphora resolution, and indeed were often tackled together in early systems (Sidner, 1979; Vieira and Poesio, 2000). Using a neural network-based approach that minimises feature engineering enables the system to be more flexible on the choices of corpora. We mainly evaluate our system on the RST portion of the ARRAU corpus since it is the largest available resource, but we additionally evaluate it on the TRAINS and PEAR portion of the ARRAU corpus, ISNOTES , BASHI and SCICORP corpus to demonstrate its tolerance of diversity. We start with a strong baseline for bridging adapted from the SoTA coreference architecture (Lee et al., 2018; Kantor and Globerson, 2019) enhanced by BERT embeddings (Devlin et al., 2"
2020.coling-main.315,P15-1137,0,0.575653,"to the event of murdering) (Webber, 1991; Kolhatkar et al., 2018). (1) [The Bakersfield Supermarket] went bankrupt last May. [The business] closed when [[its] old owner] was murdered by [robbers]. [The murder] saddened [the customers]. Bridging reference resolution is the sub-task of anaphora resolution concerned with identifying and resolving bridging references, i.e., anaphoric references to non-identical associated antecedents. Bridging resolution is much less studied than the closely related sub-task of coreference resolution, which has received a lot of attention ((Pradhan et al., 2012; Wiseman et al., 2015; Lee et al., 2017; Lee et al., 2018), to mention just a few recent proposals). One reason for this is the lack of training data. Several corpora have been annotated with bridging reference, including e.g. GNOME (Poesio, 2004), ISNOTES (Markert et al., 2012), SCICORP (R¨osiger, 2016) and BASHI (R¨osiger, 2018), but they are all rather small, with at most around 1k examples of bridging reference. ARRAU (Poesio and Artstein, 2008; Uryupina et al., 2019) is much larger, but still contains only 5.5k bridging pairs. It is challenging to train a learning based system on that amount of data, particul"
2020.coling-main.315,N16-1114,0,0.038705,"rce languages. In their work, character-based embeddings are shared across the languages. Recently, Zhou et al. (2019) introduced a multi-task network together with adversarial learning for underresourced NER. The evaluation on both cross-language and cross-domain settings shows that partially sharing the BiLSTM works better for cross-language transfer, while for cross-domain setting, the system performs better when the LSTM layers are fully shared. 2.3 Neural Coreference Resolution By contrast with bridging reference, coreference resolution has been extensively studied. Wiseman et al. (2015; Wiseman et al. (2016) first introduced a neural network-based approach to solving coreference in a non-linear way. Clark and Manning (2016) integrated reinforcement learning to let the model, optimized directly on the B3 scores. Lee et al. (2017) proposed a neural joint approach for mention detection and coreference resolution. Their model does not rely on parse trees; instead, the system learns to detect mentions by exploring the outputs of a BiLSTM. After the introduction of context dependent word embeddings such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), the Lee et al. (2017) system has been"
2020.coling-main.315,P19-1336,0,0.0376362,"Missing"
2020.coling-main.315,W18-0702,1,\N,Missing
2020.coling-main.315,C69-7001,0,\N,Missing
2020.coling-main.315,C69-6902,0,\N,Missing
2020.coling-main.538,P14-1005,0,0.0609052,"Missing"
2020.coling-main.538,P15-1136,0,0.0483011,"Missing"
2020.coling-main.538,P16-1061,0,0.0174977,"knowledge, this is the first reported result on unrestricted split-antecedent anaphora resolution. 2 2.1 Background Single-Antecedent Coreference Resolution Single-antecedent coreference resolution has been extensively studied. In the pre-neural period, both rule-based (Lee et al., 2013) and statistical models (Soon et al., 2001; Bj¨orkelund and Kuhn, 2014; Clark and Manning, 2015) were developed to resolve single-antecedent coreference resolution. Recently, Wiseman et al. (2015; Wiseman et al. (2016) first introduced a neural network-based approach to solving coreference in a non-linear way. Clark and Manning (2016) integrated reinforcement learning to let the model, optimising directly on the B3 scores. Lee et al. (2017) proposed a neural joint approach for mention detection and coreference resolution. Their model does not rely on parse trees; instead, the system learns to detect mentions by exploring the outputs of a BILSTM. After the introduction of contextual word embeddings such as ELMO (Peters et al., 2018) and BERT (Devlin et al., 2019), the Lee et al. (2017) system has been greatly improved by those embeddings (Lee et al., 2018; Kantor and Globerson, 2019; Joshi et al., 2019; Joshi et al., 2020)"
2020.coling-main.538,P19-1595,0,0.0340063,"Missing"
2020.coling-main.538,I17-2016,0,0.137536,"ly or after encountering the anaphor. (5) a. Michael met Peter and Maria in the pub. They had a great time. b. Michael watched Peter and Maria in the pub. They had a great time. Only two recent computational treatments of this type of anaphor exist (Vala et al., 2016; Zhou and Choi, 2018); they are discussed in Section 6. 2.3 Approaches for Under-Resourced Tasks Much research has focused on resolving under-resourced tasks via semi-supervised learning (Pekar et al., 2014; Yu and Bohnet, 2015; Kocijan et al., 2019; Hou, 2020) and shared representation based transfer learning (Yang et al., 2017; Cotterell and Duh, 2017; Zhou et al., 2019). Semi-supervised learning Semi-supervised methods use large unlabelled/automatically labelled data to enhance performance for under-resourced domains/languages. Early research focused on generating additional training data from automatically annotated text. Pekar et al. (2014), Yu et al. (2015) used cotraining/self-training for dependency parsing to leverage models trained on rich-resourced domain for under-resourced domains. Yu and Bohnet (2015) applied a confidence-based self-training approach to enhance parsing performance for 9 languages with parse trees automatically"
2020.coling-main.538,N19-1423,0,0.13978,"resolution. Recently, Wiseman et al. (2015; Wiseman et al. (2016) first introduced a neural network-based approach to solving coreference in a non-linear way. Clark and Manning (2016) integrated reinforcement learning to let the model, optimising directly on the B3 scores. Lee et al. (2017) proposed a neural joint approach for mention detection and coreference resolution. Their model does not rely on parse trees; instead, the system learns to detect mentions by exploring the outputs of a BILSTM. After the introduction of contextual word embeddings such as ELMO (Peters et al., 2018) and BERT (Devlin et al., 2019), the Lee et al. (2017) system has been greatly improved by those embeddings (Lee et al., 2018; Kantor and Globerson, 2019; Joshi et al., 2019; Joshi et al., 2020) to achieve SoTA results. But none of those SoTA systems can resolve split-antecedent anaphors. 2.2 Split-Antecedent Anaphora There is substantial research on resolving split-antecedent anaphors both in linguistics (Kamp and Reyle, 1993) and psychology (Murphy, 1984; Sanford and Lockhart, 1990; Kaup et al., 2002; Patson, 2014), but only a few early computational models (Eschenbach et al., 1989). This work was primarily concerned with"
2020.coling-main.538,E89-1022,0,0.83784,"Missing"
2020.coling-main.538,2020.acl-main.132,0,0.146283,"posals differed, e.g., on whether complex reference objects are created immediately or after encountering the anaphor. (5) a. Michael met Peter and Maria in the pub. They had a great time. b. Michael watched Peter and Maria in the pub. They had a great time. Only two recent computational treatments of this type of anaphor exist (Vala et al., 2016; Zhou and Choi, 2018); they are discussed in Section 6. 2.3 Approaches for Under-Resourced Tasks Much research has focused on resolving under-resourced tasks via semi-supervised learning (Pekar et al., 2014; Yu and Bohnet, 2015; Kocijan et al., 2019; Hou, 2020) and shared representation based transfer learning (Yang et al., 2017; Cotterell and Duh, 2017; Zhou et al., 2019). Semi-supervised learning Semi-supervised methods use large unlabelled/automatically labelled data to enhance performance for under-resourced domains/languages. Early research focused on generating additional training data from automatically annotated text. Pekar et al. (2014), Yu et al. (2015) used cotraining/self-training for dependency parsing to leverage models trained on rich-resourced domain for under-resourced domains. Yu and Bohnet (2015) applied a confidence-based self-tr"
2020.coling-main.538,D19-1588,0,0.0654186,"non-linear way. Clark and Manning (2016) integrated reinforcement learning to let the model, optimising directly on the B3 scores. Lee et al. (2017) proposed a neural joint approach for mention detection and coreference resolution. Their model does not rely on parse trees; instead, the system learns to detect mentions by exploring the outputs of a BILSTM. After the introduction of contextual word embeddings such as ELMO (Peters et al., 2018) and BERT (Devlin et al., 2019), the Lee et al. (2017) system has been greatly improved by those embeddings (Lee et al., 2018; Kantor and Globerson, 2019; Joshi et al., 2019; Joshi et al., 2020) to achieve SoTA results. But none of those SoTA systems can resolve split-antecedent anaphors. 2.2 Split-Antecedent Anaphora There is substantial research on resolving split-antecedent anaphors both in linguistics (Kamp and Reyle, 1993) and psychology (Murphy, 1984; Sanford and Lockhart, 1990; Kaup et al., 2002; Patson, 2014), but only a few early computational models (Eschenbach et al., 1989). This work was primarily concerned with explaining preferences and restrictions on split antecedent anaphora: for example, in (5a) they can refer to Michael, Peter and Maria, or to"
2020.coling-main.538,2020.tacl-1.5,0,0.0357459,"k and Manning (2016) integrated reinforcement learning to let the model, optimising directly on the B3 scores. Lee et al. (2017) proposed a neural joint approach for mention detection and coreference resolution. Their model does not rely on parse trees; instead, the system learns to detect mentions by exploring the outputs of a BILSTM. After the introduction of contextual word embeddings such as ELMO (Peters et al., 2018) and BERT (Devlin et al., 2019), the Lee et al. (2017) system has been greatly improved by those embeddings (Lee et al., 2018; Kantor and Globerson, 2019; Joshi et al., 2019; Joshi et al., 2020) to achieve SoTA results. But none of those SoTA systems can resolve split-antecedent anaphors. 2.2 Split-Antecedent Anaphora There is substantial research on resolving split-antecedent anaphors both in linguistics (Kamp and Reyle, 1993) and psychology (Murphy, 1984; Sanford and Lockhart, 1990; Kaup et al., 2002; Patson, 2014), but only a few early computational models (Eschenbach et al., 1989). This work was primarily concerned with explaining preferences and restrictions on split antecedent anaphora: for example, in (5a) they can refer to Michael, Peter and Maria, or to Peter and Maria, but"
2020.coling-main.538,P19-1066,0,0.412715,"to solving coreference in a non-linear way. Clark and Manning (2016) integrated reinforcement learning to let the model, optimising directly on the B3 scores. Lee et al. (2017) proposed a neural joint approach for mention detection and coreference resolution. Their model does not rely on parse trees; instead, the system learns to detect mentions by exploring the outputs of a BILSTM. After the introduction of contextual word embeddings such as ELMO (Peters et al., 2018) and BERT (Devlin et al., 2019), the Lee et al. (2017) system has been greatly improved by those embeddings (Lee et al., 2018; Kantor and Globerson, 2019; Joshi et al., 2019; Joshi et al., 2020) to achieve SoTA results. But none of those SoTA systems can resolve split-antecedent anaphors. 2.2 Split-Antecedent Anaphora There is substantial research on resolving split-antecedent anaphors both in linguistics (Kamp and Reyle, 1993) and psychology (Murphy, 1984; Sanford and Lockhart, 1990; Kaup et al., 2002; Patson, 2014), but only a few early computational models (Eschenbach et al., 1989). This work was primarily concerned with explaining preferences and restrictions on split antecedent anaphora: for example, in (5a) they can refer to Michael, Pet"
2020.coling-main.538,D19-1439,0,0.0198662,"aup et al., 2002). Proposals differed, e.g., on whether complex reference objects are created immediately or after encountering the anaphor. (5) a. Michael met Peter and Maria in the pub. They had a great time. b. Michael watched Peter and Maria in the pub. They had a great time. Only two recent computational treatments of this type of anaphor exist (Vala et al., 2016; Zhou and Choi, 2018); they are discussed in Section 6. 2.3 Approaches for Under-Resourced Tasks Much research has focused on resolving under-resourced tasks via semi-supervised learning (Pekar et al., 2014; Yu and Bohnet, 2015; Kocijan et al., 2019; Hou, 2020) and shared representation based transfer learning (Yang et al., 2017; Cotterell and Duh, 2017; Zhou et al., 2019). Semi-supervised learning Semi-supervised methods use large unlabelled/automatically labelled data to enhance performance for under-resourced domains/languages. Early research focused on generating additional training data from automatically annotated text. Pekar et al. (2014), Yu et al. (2015) used cotraining/self-training for dependency parsing to leverage models trained on rich-resourced domain for under-resourced domains. Yu and Bohnet (2015) applied a confidence-b"
2020.coling-main.538,L18-1065,0,0.0250566,"ositive examples, finding they are mainly due to three types of mistakes: single-antecedent coreference (the coreference chains were annotated as the split-antecedent), bridging reference (not required to be annotated), and other annotation mistakes. The first two types of mistakes are not harmful to our task as our third and fourth auxiliary corpora are created using those types of relations. 4 We use the gold single-antecedent clusters to ensure the selected antecedents belong to distinct gold cluster. https://github.com/dali-ambiguity/Phrase-Detectives-Corpus-2.1.4 6 The ParCorFull corpus (Lapshinova-Koltunski et al., 2018) also includes split-antecedent annotations, but the number of split-antecedent examples are too small to be used in our experiments. 7 A subset of the PD corpus comes with additional gold labels annotated by experts. 5 6117 Corpus Anaphors Type Data Quality Num of docs Num of Anaphors ARRAU TRAIN / DEV / TEST Split-antecedent anaphors Split-antecedent anaphors Split-antecedent anaphors Bridging anaphors Single-antecedent anaphors Gold Silver Noisy Gold Gold 211 / 30 / 60 165 467 213 462 507 / 80 / 110 507 6262 1059 30372 PD - SILVER PD - CROWD ELEMENT- OF SINGLE - COREF Table 1: Statistics ab"
2020.coling-main.538,J13-4004,0,0.146648,"Missing"
2020.coling-main.538,D17-1018,0,0.0432638,"Single-Antecedent Coreference Resolution Single-antecedent coreference resolution has been extensively studied. In the pre-neural period, both rule-based (Lee et al., 2013) and statistical models (Soon et al., 2001; Bj¨orkelund and Kuhn, 2014; Clark and Manning, 2015) were developed to resolve single-antecedent coreference resolution. Recently, Wiseman et al. (2015; Wiseman et al. (2016) first introduced a neural network-based approach to solving coreference in a non-linear way. Clark and Manning (2016) integrated reinforcement learning to let the model, optimising directly on the B3 scores. Lee et al. (2017) proposed a neural joint approach for mention detection and coreference resolution. Their model does not rely on parse trees; instead, the system learns to detect mentions by exploring the outputs of a BILSTM. After the introduction of contextual word embeddings such as ELMO (Peters et al., 2018) and BERT (Devlin et al., 2019), the Lee et al. (2017) system has been greatly improved by those embeddings (Lee et al., 2018; Kantor and Globerson, 2019; Joshi et al., 2019; Joshi et al., 2020) to achieve SoTA results. But none of those SoTA systems can resolve split-antecedent anaphors. 2.2 Split-Ant"
2020.coling-main.538,N18-2108,0,0.44049,"rk-based approach to solving coreference in a non-linear way. Clark and Manning (2016) integrated reinforcement learning to let the model, optimising directly on the B3 scores. Lee et al. (2017) proposed a neural joint approach for mention detection and coreference resolution. Their model does not rely on parse trees; instead, the system learns to detect mentions by exploring the outputs of a BILSTM. After the introduction of contextual word embeddings such as ELMO (Peters et al., 2018) and BERT (Devlin et al., 2019), the Lee et al. (2017) system has been greatly improved by those embeddings (Lee et al., 2018; Kantor and Globerson, 2019; Joshi et al., 2019; Joshi et al., 2020) to achieve SoTA results. But none of those SoTA systems can resolve split-antecedent anaphors. 2.2 Split-Antecedent Anaphora There is substantial research on resolving split-antecedent anaphors both in linguistics (Kamp and Reyle, 1993) and psychology (Murphy, 1984; Sanford and Lockhart, 1990; Kaup et al., 2002; Patson, 2014), but only a few early computational models (Eschenbach et al., 1989). This work was primarily concerned with explaining preferences and restrictions on split antecedent anaphora: for example, in (5a) th"
2020.coling-main.538,D18-1218,1,0.893946,"Missing"
2020.coling-main.538,W14-6105,1,0.764272,"e for they to refer to Peter and Maria (Kaup et al., 2002). Proposals differed, e.g., on whether complex reference objects are created immediately or after encountering the anaphor. (5) a. Michael met Peter and Maria in the pub. They had a great time. b. Michael watched Peter and Maria in the pub. They had a great time. Only two recent computational treatments of this type of anaphor exist (Vala et al., 2016; Zhou and Choi, 2018); they are discussed in Section 6. 2.3 Approaches for Under-Resourced Tasks Much research has focused on resolving under-resourced tasks via semi-supervised learning (Pekar et al., 2014; Yu and Bohnet, 2015; Kocijan et al., 2019; Hou, 2020) and shared representation based transfer learning (Yang et al., 2017; Cotterell and Duh, 2017; Zhou et al., 2019). Semi-supervised learning Semi-supervised methods use large unlabelled/automatically labelled data to enhance performance for under-resourced domains/languages. Early research focused on generating additional training data from automatically annotated text. Pekar et al. (2014), Yu et al. (2015) used cotraining/self-training for dependency parsing to leverage models trained on rich-resourced domain for under-resourced domains."
2020.coling-main.538,N15-1082,0,0.0811292,"inking nominal expressions (mentions) to entities in the discourse, so that mentions representing the same entity are grouped together in a ‘coreference chain’ (Poesio et al., 2016). As the performance of coreference models has substantially improved (Clark and Manning, 2015; Clark and Manning, 2016; Lee et al., 2017; Lee et al., 2018; Kantor and Globerson, 2019) in recent years, more attention is being devoted to more complex aspects of anaphoric reference– from the pronouns that require commonsense knowledge for their resolution studied in the Winograd Schema Challenge (Rahman and Ng, 2012; Peng et al., 2015) to pronouns that cannot be resolved purely on the basis of gender (Webster et al., 2018). Another limitation of state of the art systems is the assumption that anaphors can only have one antecedent. Plural anaphors with multiple antecedents (split antecedent anaphors) are not widely studied– in fact, such anaphors are not annotated in the most widely used coreference corpus, ONTONOTES (Pradhan et al., 2012). In ONTONOTES we find annotated cases of plural reference to plural antecedents, as in (1), or cases in which singular antecedents are conjoined so that a mention can be introduced for the"
2020.coling-main.538,D14-1162,0,0.0834203,"id our split-antecedent anaphora resolution. 3 3.1 Methods The Baseline System Our baseline is a simplified version of the SoTA coreference architecture by Lee et al. (2018), further developed by Kantor and Globerson (2019). In this model, mention detection and coreference are carried out jointly, but here we only use the coreference part, since we evaluate our model with gold mentions. Our baseline system first creates representations for mentions using the output of a BILSTM. The BILSTM takes as input the concatenated embeddings of both word and character levels. For word embeddings, GloVe (Pennington et al., 2014) and BERT (Devlin et al., 2019) embeddings are used. Character embeddings are learned from a convolution neural network (CNN) during training. The tokens are represented by concatenating outputs from the forward and the backward LSTMs. The token representations (xt )Tt=1 are used together with head representations (hi ) to represent mentions (Mi ). The hi of a mention is obtained by applying attention over its token representations ({xbi , ..., xei }), where bi and ei are the indices of the start and the end of the mention, respectively. Formally, we compute hi , Mi as follows: αt = FFNNα (xt"
2020.coling-main.538,N18-1202,0,0.0107088,"e single-antecedent coreference resolution. Recently, Wiseman et al. (2015; Wiseman et al. (2016) first introduced a neural network-based approach to solving coreference in a non-linear way. Clark and Manning (2016) integrated reinforcement learning to let the model, optimising directly on the B3 scores. Lee et al. (2017) proposed a neural joint approach for mention detection and coreference resolution. Their model does not rely on parse trees; instead, the system learns to detect mentions by exploring the outputs of a BILSTM. After the introduction of contextual word embeddings such as ELMO (Peters et al., 2018) and BERT (Devlin et al., 2019), the Lee et al. (2017) system has been greatly improved by those embeddings (Lee et al., 2018; Kantor and Globerson, 2019; Joshi et al., 2019; Joshi et al., 2020) to achieve SoTA results. But none of those SoTA systems can resolve split-antecedent anaphors. 2.2 Split-Antecedent Anaphora There is substantial research on resolving split-antecedent anaphors both in linguistics (Kamp and Reyle, 1993) and psychology (Murphy, 1984; Sanford and Lockhart, 1990; Kaup et al., 2002; Patson, 2014), but only a few early computational models (Eschenbach et al., 1989). This wo"
2020.coling-main.538,poesio-artstein-2008-anaphoric,1,0.77592,"a model focused on the resolution of the pronominal plural mentions they and them in a newly created corpus of fiction where most of the antecedents are from a fixed list of characters in the novel. Zhou and Choi (2018) proposed a model that addresses a wider range of anaphoric references to split antecedents, but limited only to references to main characters (mainly pronominal mentions) in a corpus of transcripts of the sitcom Friends. In this paper, we introduce the first model targeting the whole range of split-antecedent anaphora. We evaluate our system on the hand-annotated ARRAU corpus (Poesio and Artstein, 2008; Uryupina et al., 2020), which covers a range of anaphoric relations going from identity relations (including split antecedent anaphora), bridging reference, and discourse deixis, as well as different genres going from news to task-oriented dialogue. Since the task is complex, we focus on establishing links between (gold) plural anaphors and their split-antecedents, leaving the detection of split-antecedent anaphors for future work. We follow Vala et al. (2016) and evaluate on the setting that assumes the gold split-antecedent anaphors and the gold mentions are provided. Our baseline system i"
2020.coling-main.538,N19-1176,1,0.912754,"Missing"
2020.coling-main.538,W12-4501,0,0.0831456,"evoted to more complex aspects of anaphoric reference– from the pronouns that require commonsense knowledge for their resolution studied in the Winograd Schema Challenge (Rahman and Ng, 2012; Peng et al., 2015) to pronouns that cannot be resolved purely on the basis of gender (Webster et al., 2018). Another limitation of state of the art systems is the assumption that anaphors can only have one antecedent. Plural anaphors with multiple antecedents (split antecedent anaphors) are not widely studied– in fact, such anaphors are not annotated in the most widely used coreference corpus, ONTONOTES (Pradhan et al., 2012). In ONTONOTES we find annotated cases of plural reference to plural antecedents, as in (1), or cases in which singular antecedents are conjoined so that a mention can be introduced for the conjunction, as in (2). However, it is also possible to refer plurally to antecedents introduced by separate noun phrases, as in (3) or (4) (Eschenbach et al., 1989; Kamp and Reyle, 1993); such cases are not annotated in ONTONOTES. (1) The Jonesesi went to the park. Theyi had a good time. (2) John and Maryi went to the park. Theyi had a good time. (3) Johni met Maryj in the park. Theyi,j had a good chat . ("
2020.coling-main.538,D12-1071,0,0.176594,"linguistic task of linking nominal expressions (mentions) to entities in the discourse, so that mentions representing the same entity are grouped together in a ‘coreference chain’ (Poesio et al., 2016). As the performance of coreference models has substantially improved (Clark and Manning, 2015; Clark and Manning, 2016; Lee et al., 2017; Lee et al., 2018; Kantor and Globerson, 2019) in recent years, more attention is being devoted to more complex aspects of anaphoric reference– from the pronouns that require commonsense knowledge for their resolution studied in the Winograd Schema Challenge (Rahman and Ng, 2012; Peng et al., 2015) to pronouns that cannot be resolved purely on the basis of gender (Webster et al., 2018). Another limitation of state of the art systems is the assumption that anaphors can only have one antecedent. Plural anaphors with multiple antecedents (split antecedent anaphors) are not widely studied– in fact, such anaphors are not annotated in the most widely used coreference corpus, ONTONOTES (Pradhan et al., 2012). In ONTONOTES we find annotated cases of plural reference to plural antecedents, as in (1), or cases in which singular antecedents are conjoined so that a mention can b"
2020.coling-main.538,J01-4004,0,0.723591,"Missing"
2020.coling-main.538,P16-1216,0,0.273607,"tribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. 1 The code is available at https://github.com/juntaoy/dali-plural Licence details: http:// 6113 Proceedings of the 28th International Conference on Computational Linguistics, pages 6113–6125 Barcelona, Spain (Online), December 8-13, 2020 Early research on split antecedent anaphors mostly focused on the constraints on the construction of complex entities from singular entities There are two recent studies of split antecedent anaphora, both involving the creation of a new dataset, and focusing on a subset of the problem. Vala et al. (2016) proposed a model focused on the resolution of the pronominal plural mentions they and them in a newly created corpus of fiction where most of the antecedents are from a fixed list of characters in the novel. Zhou and Choi (2018) proposed a model that addresses a wider range of anaphoric references to split antecedents, but limited only to references to main characters (mainly pronominal mentions) in a corpus of transcripts of the sitcom Friends. In this paper, we introduce the first model targeting the whole range of split-antecedent anaphora. We evaluate our system on the hand-annotated ARRA"
2020.coling-main.538,Q18-1042,0,0.0552409,"epresenting the same entity are grouped together in a ‘coreference chain’ (Poesio et al., 2016). As the performance of coreference models has substantially improved (Clark and Manning, 2015; Clark and Manning, 2016; Lee et al., 2017; Lee et al., 2018; Kantor and Globerson, 2019) in recent years, more attention is being devoted to more complex aspects of anaphoric reference– from the pronouns that require commonsense knowledge for their resolution studied in the Winograd Schema Challenge (Rahman and Ng, 2012; Peng et al., 2015) to pronouns that cannot be resolved purely on the basis of gender (Webster et al., 2018). Another limitation of state of the art systems is the assumption that anaphors can only have one antecedent. Plural anaphors with multiple antecedents (split antecedent anaphors) are not widely studied– in fact, such anaphors are not annotated in the most widely used coreference corpus, ONTONOTES (Pradhan et al., 2012). In ONTONOTES we find annotated cases of plural reference to plural antecedents, as in (1), or cases in which singular antecedents are conjoined so that a mention can be introduced for the conjunction, as in (2). However, it is also possible to refer plurally to antecedents in"
2020.coling-main.538,P15-1137,0,0.365955,"Missing"
2020.coling-main.538,N16-1114,0,0.101284,"Missing"
2020.coling-main.538,W15-2138,1,0.829562,"to Peter and Maria (Kaup et al., 2002). Proposals differed, e.g., on whether complex reference objects are created immediately or after encountering the anaphor. (5) a. Michael met Peter and Maria in the pub. They had a great time. b. Michael watched Peter and Maria in the pub. They had a great time. Only two recent computational treatments of this type of anaphor exist (Vala et al., 2016; Zhou and Choi, 2018); they are discussed in Section 6. 2.3 Approaches for Under-Resourced Tasks Much research has focused on resolving under-resourced tasks via semi-supervised learning (Pekar et al., 2014; Yu and Bohnet, 2015; Kocijan et al., 2019; Hou, 2020) and shared representation based transfer learning (Yang et al., 2017; Cotterell and Duh, 2017; Zhou et al., 2019). Semi-supervised learning Semi-supervised methods use large unlabelled/automatically labelled data to enhance performance for under-resourced domains/languages. Early research focused on generating additional training data from automatically annotated text. Pekar et al. (2014), Yu et al. (2015) used cotraining/self-training for dependency parsing to leverage models trained on rich-resourced domain for under-resourced domains. Yu and Bohnet (2015)"
2020.coling-main.538,W15-2201,1,0.719895,"2.3 Approaches for Under-Resourced Tasks Much research has focused on resolving under-resourced tasks via semi-supervised learning (Pekar et al., 2014; Yu and Bohnet, 2015; Kocijan et al., 2019; Hou, 2020) and shared representation based transfer learning (Yang et al., 2017; Cotterell and Duh, 2017; Zhou et al., 2019). Semi-supervised learning Semi-supervised methods use large unlabelled/automatically labelled data to enhance performance for under-resourced domains/languages. Early research focused on generating additional training data from automatically annotated text. Pekar et al. (2014), Yu et al. (2015) used cotraining/self-training for dependency parsing to leverage models trained on rich-resourced domain for under-resourced domains. Yu and Bohnet (2015) applied a confidence-based self-training approach to enhance parsing performance for 9 languages with parse trees automatically annotated by models trained on small initial training data. Recently, another line of research focused on creating synthetic training data from unlabelled/automatically labelled data using some heuristic patterns. Kocijan et al. (2019) used Wikipedia to create WikiCREM, a large pronoun resolution dataset, using heu"
2020.coling-main.538,C18-1003,0,0.397618,"utational Linguistics, pages 6113–6125 Barcelona, Spain (Online), December 8-13, 2020 Early research on split antecedent anaphors mostly focused on the constraints on the construction of complex entities from singular entities There are two recent studies of split antecedent anaphora, both involving the creation of a new dataset, and focusing on a subset of the problem. Vala et al. (2016) proposed a model focused on the resolution of the pronominal plural mentions they and them in a newly created corpus of fiction where most of the antecedents are from a fixed list of characters in the novel. Zhou and Choi (2018) proposed a model that addresses a wider range of anaphoric references to split antecedents, but limited only to references to main characters (mainly pronominal mentions) in a corpus of transcripts of the sitcom Friends. In this paper, we introduce the first model targeting the whole range of split-antecedent anaphora. We evaluate our system on the hand-annotated ARRAU corpus (Poesio and Artstein, 2008; Uryupina et al., 2020), which covers a range of anaphoric relations going from identity relations (including split antecedent anaphora), bridging reference, and discourse deixis, as well as di"
2020.coling-main.538,P19-1336,0,0.0534972,"Missing"
2020.crac-1.11,2020.lrec-1.11,1,0.576016,"et al. (2014) defined a set of rules based on parse tree information to detect mentions, and utilized a latent tree representation to learn coreference chains. Similarly Björkelund and Kuhn (2014) adopted a tree representation approach to cluster mentions, but improved the learning strategy and introduced non-local features to capture more information about coreference relations. There have been other research studies related to anaphora resolution (Trabelsi et al., 2016; Bouzid et al., 2017; Beseiso and Al-Alwani, 2016; Abolohom and Omar, 2015), but they only considered pronominal anaphora. Aloraini and Poesio (2020) also considered a specific type of pronominal anaphora, zero-pronoun anaphora. All current approaches suffer from a number of limitations, one of which is that most of them rely on an extensive set of hand-chosen features. 3 3.1 System architecture The Baseline System We use the Lee et al. (2018) system as our baseline and replace their ELMo embeddings with the bert recipe of Kantor and Globerson (2019a). The input of the system is the concatenated embeddings ((embt )Tt=1 ) of both word and character levels. The word-level fastText (Bojanowski et al., 2016) and bert (Devlin et al., 2019) embe"
2020.crac-1.11,althobaiti-etal-2014-aranlp,1,0.805818,"rchitecture of our system. 3.2 Data Pre-processing Arabic is a morphologically rich language. Thus, training on Arabic texts that are not pre-processed properly can suffer from sparsity (various forms for the same word) and ambiguity (same form corresponding to multiple words). There are two reasons for these problems. First, certain letters can have different forms which are usually misspelled, such as the various forms of the letter “alif”. Second, the placement of diacritics on words which are assumed to be undiacritized (Habash and Sadat, 2006). Therefore, we follow the steps proposed in (Althobaiti et al., 2014) to pre-process the data. These steps include: • Normalizing the various forms of the letter ”alif” (إ,أ, )آto the letter ””ا. • Removing all diacritic marks. We show an example of an original and pre-processed sentence from OntoNotes 5.0 in Table 1. Preprocessing the data increases the overall performance of coreference system with 7 percentage points more as we will see in Section 5. 2 Candidate mentions are paired with all the mentions appeared before them (candidate antecedents) in the document. 102 End to end Word, Char BERT Embs or External Mention detector Head Emb BiLSTM Mentio"
2020.crac-1.11,2020.osact-1.2,0,0.2578,"a strong baseline system (Lee et al., 2018; Kantor and Globerson, 2019a), enhanced with contextual bert embeddings (Devlin et al., 2019). We then explored three methods for improving the model’s performance for Arabic. The first method is to pre-process Arabic words with heuristic rules. We follow Althobaiti et al. (2014) to normalize the letters with different forms, and removing all the diacritics. This results in a substantial improvement of 7 percentage points over our baseline. The second route is to replace multilingual bert with a bert model trained only on the Arabic texts (AraBERT) (Antoun et al., 2020). Multilingual bert is trained with 100+ languages; as a result, it is not optimized for any of them. As shown by Antoun et al. (2020), monolingual bert trained only on the Arabic texts has better performance on various nlp tasks. We found the same holds for coreference: using embeddings from monolingual bert, the model further improved the conll F1 by 4.8 percentage points. Our third step is to leverage the end-to-end system with a separately trained mention detector (Yu et al., 2020a). We show that a better mention detection performance can be achieved by using a separately trained mention d"
2020.crac-1.11,W11-1905,0,0.410974,"e-art English coreference model can be adapted to Arabic coreference leading to a substantial improvement in performance when compared to previous feature-based systems. 2 2.1 Related Work English Coreference Resolution Like with other natural language processing tasks, most state-of-the-art coreference resolution systems are evaluated on English data. Coreference resolution for English is an active area of research. Until the appearance of neural systems, state-of-the-art systems for English coreference resolution were either rule-based (Lee et al., 2011) or feature-based (Soon et al., 2001; Björkelund and Nugues, 2011; Fernandes et al., 2014; Björkelund and Kuhn, 2014; Clark and Manning, 2015). Wiseman et al. (2015) introduced a neural network-based approach to solving the task in a non-linear way. In their system, the heuristic features commonly used in linear models are transformed by a tanh function to be used as the mention representations. Clark and Manning (2016b) integrated reinforcement learning to let the model optimize directly on the B3 scores. Lee et al. (2017) first presented a neural joint approach for mention detection and coreference resolution. Their model does not rely on parse trees; ins"
2020.crac-1.11,P14-1005,0,0.691603,"reference, most recently using neural coreference approaches (Lee et al., 2017; Lee et al., 2018; Kantor and Globerson, 2019a; Joshi et al., 2019b; Joshi et al., 2019a; Yu et al., 2020b; Wu et al., 2020), leading to a significant increase in the performance of coreference resolvers for English. By contrast, there has been almost no research on Arabic coreference; the performance for Arabic coreference resolution has not improved much since the conll 2012 shared task, and in particular no neural architectures have been proposed–the current state-of-the-art system remains the model proposed in (Björkelund and Kuhn, 2014). In this paper we close this very obvious gap by proposing what to our knowledge is the first neural coreference resolver for Arabic.1 One explanation for this lack of research might simply be the lack of training data large enough for the task. Another explanation might be that Arabic is more problematic than English because of its rich morphology, its many dialects, and/or its high degree of ambiguity. We explore the first of these possibilities. Coreference resolution can be further divided into two subtasks–mention detection and mention clustering–as illustrated in Figure 1. In early work"
2020.crac-1.11,W12-4504,0,0.406308,"by pretrain the system first on the large question answering corpora. 2.2 Arabic Coreference Resolution There have been several studies of Arabic coreference resolution; in particular, several of the systems involved in the conll 2012 shared task attempted Arabic as well. li (2012) used syntactic parse trees to detect mentions, and compared pairs of mention based on their semantic and syntactic features. Zhekova and Kübler (2010) proposed a language independent module that requires only syntactic information and clusters mentions using the memory-based learner TiMBL (Daelemans et al., 2004). Chen and Ng (2012) detected mentions by employing named entity and language-dependent heuristics. They employed multiple sieves (Lee et al., 2011) for English and Chinese, but only used an exact match sieve for Arabic because other sieves did not provide better results. Björkelund and Nugues (2011) considered all noun phrases and possessive pronouns as mentions, and trained two types of classifier: logistic regression and decision trees. Stamborg et al. (2012) extracted all noun phrases, pronouns, and possessive pronouns as mentions. Then they applied (Björkelund and Nugues, 2011)’s solver which consists of var"
2020.crac-1.11,P15-1136,0,0.0192899,"a substantial improvement in performance when compared to previous feature-based systems. 2 2.1 Related Work English Coreference Resolution Like with other natural language processing tasks, most state-of-the-art coreference resolution systems are evaluated on English data. Coreference resolution for English is an active area of research. Until the appearance of neural systems, state-of-the-art systems for English coreference resolution were either rule-based (Lee et al., 2011) or feature-based (Soon et al., 2001; Björkelund and Nugues, 2011; Fernandes et al., 2014; Björkelund and Kuhn, 2014; Clark and Manning, 2015). Wiseman et al. (2015) introduced a neural network-based approach to solving the task in a non-linear way. In their system, the heuristic features commonly used in linear models are transformed by a tanh function to be used as the mention representations. Clark and Manning (2016b) integrated reinforcement learning to let the model optimize directly on the B3 scores. Lee et al. (2017) first presented a neural joint approach for mention detection and coreference resolution. Their model does not rely on parse trees; instead, the system learns to detect mentions by exploring the outputs of a bi-d"
2020.crac-1.11,D16-1245,0,0.0894764,"be the lack of training data large enough for the task. Another explanation might be that Arabic is more problematic than English because of its rich morphology, its many dialects, and/or its high degree of ambiguity. We explore the first of these possibilities. Coreference resolution can be further divided into two subtasks–mention detection and mention clustering–as illustrated in Figure 1. In early work, coreference’s two subtasks were usually carried out in a pipeline fashion (Soon et al., 2001; Fernandes et al., 2014; Björkelund and Kuhn, 2014; Wiseman et al., 2015; Wiseman et al., 2016; Clark and Manning, 2016a; Clark and Manning, 2016b), with candidate mentions selected prior the mention clustering step. Since Lee et al. (2017) introduced an end-to-end neural coreference architecture that achieved state of the art by carrying out the two tasks jointly, as first proposed by Daume and Marcu (2005), most state-of-the-art systems have followed this approach. However, no end-to-end solution was attempted for Arabic. We intend to explore whether an end-to-end solution would be practicable with a corpus of more limited size. This work is licensed under a Creative Commons Attribution 4.0 International Lic"
2020.crac-1.11,P16-1061,0,0.0676751,"be the lack of training data large enough for the task. Another explanation might be that Arabic is more problematic than English because of its rich morphology, its many dialects, and/or its high degree of ambiguity. We explore the first of these possibilities. Coreference resolution can be further divided into two subtasks–mention detection and mention clustering–as illustrated in Figure 1. In early work, coreference’s two subtasks were usually carried out in a pipeline fashion (Soon et al., 2001; Fernandes et al., 2014; Björkelund and Kuhn, 2014; Wiseman et al., 2015; Wiseman et al., 2016; Clark and Manning, 2016a; Clark and Manning, 2016b), with candidate mentions selected prior the mention clustering step. Since Lee et al. (2017) introduced an end-to-end neural coreference architecture that achieved state of the art by carrying out the two tasks jointly, as first proposed by Daume and Marcu (2005), most state-of-the-art systems have followed this approach. However, no end-to-end solution was attempted for Arabic. We intend to explore whether an end-to-end solution would be practicable with a corpus of more limited size. This work is licensed under a Creative Commons Attribution 4.0 International Lic"
2020.crac-1.11,L16-1170,0,0.028257,"r monolingual counterparts to achieve better. Therefore, recent research adopts the monolingual approach to pretrain bert, developing, e.g., CamemBERT for French (Martin et al., 2019), AlBERTo for Italian (Polignano et al., 2019), and others (Lee et al., 2020; Souza et al., 2019; Kuratov and Arkhipov, 2019). AraBERT (Antoun et al., 2020) is a monolingual bert model for Arabic which was pre-trained on a collection of Wikipedia and newspaper articles. There are two versions, AraBERT 0.1 and AraBERT 1.0, the difference being that the latter pretrained on the word morphemes obtained using Farasa (Darwish and Mubarak, 2016). The two versions yield relatively similar scores in various nlp tasks. In our experiments, we used AraBERT 0.1 because empirically it proved more compatible with the coreference resolution system. 3.4 Mention Detection Mention detection is a crucial part of the coreference resolution system, better candidate mentions usually lead to better overall performance. As suggested by Yu et al. (2020a), a separately trained mention detector can achieve a better mention detection performance when compared to its end-to-end counterpart. In this work, we adapt the state-of-the-art mention detector of Yu"
2020.crac-1.11,H05-1013,0,0.110651,"further divided into two subtasks–mention detection and mention clustering–as illustrated in Figure 1. In early work, coreference’s two subtasks were usually carried out in a pipeline fashion (Soon et al., 2001; Fernandes et al., 2014; Björkelund and Kuhn, 2014; Wiseman et al., 2015; Wiseman et al., 2016; Clark and Manning, 2016a; Clark and Manning, 2016b), with candidate mentions selected prior the mention clustering step. Since Lee et al. (2017) introduced an end-to-end neural coreference architecture that achieved state of the art by carrying out the two tasks jointly, as first proposed by Daume and Marcu (2005), most state-of-the-art systems have followed this approach. However, no end-to-end solution was attempted for Arabic. We intend to explore whether an end-to-end solution would be practicable with a corpus of more limited size. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. * Equal contribution. Listed by alphabetical order. 1 The code is available at https://github.com/juntaoy/aracoref License details: http:// 99 Proceedings of the 3rd Workshop on Computational Models of Reference, Anaphora and Coreference (CRAC 2020"
2020.crac-1.11,N19-1423,0,0.495451,"Figure 1: The first step in coreference resolution is mention detection. The detected mentions are underlined. The second step is mention clustering. We have two clusters {Obama, his, he} and {Clinton, her, she}. The mention detector might identify other words as mentions, but for simplicity we present only the mentions of the two clusters. The approach we followed to adapt the state-of-the-art English coreference resolution architecture to Arabic is as follows. We started with a strong baseline system (Lee et al., 2018; Kantor and Globerson, 2019a), enhanced with contextual bert embeddings (Devlin et al., 2019). We then explored three methods for improving the model’s performance for Arabic. The first method is to pre-process Arabic words with heuristic rules. We follow Althobaiti et al. (2014) to normalize the letters with different forms, and removing all the diacritics. This results in a substantial improvement of 7 percentage points over our baseline. The second route is to replace multilingual bert with a bert model trained only on the Arabic texts (AraBERT) (Antoun et al., 2020). Multilingual bert is trained with 100+ languages; as a result, it is not optimized for any of them. As shown by Ant"
2020.crac-1.11,doddington-etal-2004-automatic,0,0.0263316,"Missing"
2020.crac-1.11,W12-4502,0,0.0543429,"Missing"
2020.crac-1.11,N06-2013,0,0.106157,"t pairs predicted by the system. Figure 2 shows the proposed system architecture of our system. 3.2 Data Pre-processing Arabic is a morphologically rich language. Thus, training on Arabic texts that are not pre-processed properly can suffer from sparsity (various forms for the same word) and ambiguity (same form corresponding to multiple words). There are two reasons for these problems. First, certain letters can have different forms which are usually misspelled, such as the various forms of the letter “alif”. Second, the placement of diacritics on words which are assumed to be undiacritized (Habash and Sadat, 2006). Therefore, we follow the steps proposed in (Althobaiti et al., 2014) to pre-process the data. These steps include: • Normalizing the various forms of the letter ”alif” (إ,أ, )آto the letter ””ا. • Removing all diacritic marks. We show an example of an original and pre-processed sentence from OntoNotes 5.0 in Table 1. Preprocessing the data increases the overall performance of coreference system with 7 percentage points more as we will see in Section 5. 2 Candidate mentions are paired with all the mentions appeared before them (candidate antecedents) in the document. 102 End to end Wo"
2020.crac-1.11,P19-1356,0,0.0201675,"text الى ذلك كتبت مئات المقالات النقدية الادبية Table 1: An example on how we pre-process Arabic text. The letter ”alif” is normalized and all diacritic marks are removed. 3.3 Multilingual vs. monolingual BERT bert (Devlin et al., 2019) is a language representation model consisting of multiple stacked Transformers (Vaswani et al., 2017). bert was pretrained on a large amount of unlabeled text, and produces distributional vectors for words and contexts. Recently, it has been shown that bert can capture structural properties of a language, such as its surface, semantic, and syntactic aspects (Jawahar et al., 2019) which seems related to what we need for the coreference resolution. Therefore, we set bert to produce embeddings for the mentions. bert is available for English, Chinese, and there is a version for multiple languages, called multilingual bert 3 . Multilingual bert is publicly available and covers a wide range of languages including Arabic. Even though the multilingual version provides great results for many languages, it has been shown their monolingual counterparts to achieve better. Therefore, recent research adopts the monolingual approach to pretrain bert, developing, e.g., CamemBERT for"
2020.crac-1.11,D19-1588,0,0.0710804,"Missing"
2020.crac-1.11,P19-1066,0,0.0659973,"2016) . Coreference resolution is a difficult task that requires reasoning, context understanding, and background knowledge of real-world entities, and has driven research in both natural language processing and machine learning, particularly since the release of the ontonotes multilingual corpus providing annotated coreference data for Arabic, Chinese and English and used for the 2011 and 2012 conll shared tasks (Pradhan et al., 2012). Since then, there has been substantial research on English coreference, most recently using neural coreference approaches (Lee et al., 2017; Lee et al., 2018; Kantor and Globerson, 2019a; Joshi et al., 2019b; Joshi et al., 2019a; Yu et al., 2020b; Wu et al., 2020), leading to a significant increase in the performance of coreference resolvers for English. By contrast, there has been almost no research on Arabic coreference; the performance for Arabic coreference resolution has not improved much since the conll 2012 shared task, and in particular no neural architectures have been proposed–the current state-of-the-art system remains the model proposed in (Björkelund and Kuhn, 2014). In this paper we close this very obvious gap by proposing what to our knowledge is the first neu"
2020.crac-1.11,W11-1902,0,0.0609058,"Missing"
2020.crac-1.11,D17-1018,0,0.232153,"ntity into clusters (Poesio et al., 2016) . Coreference resolution is a difficult task that requires reasoning, context understanding, and background knowledge of real-world entities, and has driven research in both natural language processing and machine learning, particularly since the release of the ontonotes multilingual corpus providing annotated coreference data for Arabic, Chinese and English and used for the 2011 and 2012 conll shared tasks (Pradhan et al., 2012). Since then, there has been substantial research on English coreference, most recently using neural coreference approaches (Lee et al., 2017; Lee et al., 2018; Kantor and Globerson, 2019a; Joshi et al., 2019b; Joshi et al., 2019a; Yu et al., 2020b; Wu et al., 2020), leading to a significant increase in the performance of coreference resolvers for English. By contrast, there has been almost no research on Arabic coreference; the performance for Arabic coreference resolution has not improved much since the conll 2012 shared task, and in particular no neural architectures have been proposed–the current state-of-the-art system remains the model proposed in (Björkelund and Kuhn, 2014). In this paper we close this very obvious gap by pr"
2020.crac-1.11,N18-2108,0,0.0682779,"s (Poesio et al., 2016) . Coreference resolution is a difficult task that requires reasoning, context understanding, and background knowledge of real-world entities, and has driven research in both natural language processing and machine learning, particularly since the release of the ontonotes multilingual corpus providing annotated coreference data for Arabic, Chinese and English and used for the 2011 and 2012 conll shared tasks (Pradhan et al., 2012). Since then, there has been substantial research on English coreference, most recently using neural coreference approaches (Lee et al., 2017; Lee et al., 2018; Kantor and Globerson, 2019a; Joshi et al., 2019b; Joshi et al., 2019a; Yu et al., 2020b; Wu et al., 2020), leading to a significant increase in the performance of coreference resolvers for English. By contrast, there has been almost no research on Arabic coreference; the performance for Arabic coreference resolution has not improved much since the conll 2012 shared task, and in particular no neural architectures have been proposed–the current state-of-the-art system remains the model proposed in (Björkelund and Kuhn, 2014). In this paper we close this very obvious gap by proposing what to ou"
2020.crac-1.11,W12-4516,0,0.0575887,"Missing"
2020.crac-1.11,N18-1202,0,0.0143556,"em, the heuristic features commonly used in linear models are transformed by a tanh function to be used as the mention representations. Clark and Manning (2016b) integrated reinforcement learning to let the model optimize directly on the B3 scores. Lee et al. (2017) first presented a neural joint approach for mention detection and coreference resolution. Their model does not rely on parse trees; instead, the system learns to detect mentions by exploring the outputs of a bi-directional LSTM. Lee et al. (2018) is an extended version of Lee et al. (2017) mainly enhanced by using ELMo embeddings (Peters et al., 2018), in addition, the use of second-order inference enabled the system to explore partial entity level features and further improved the system by 0.4 percentage points. Later the model was further improved by Kantor and Globerson 100 (2019a) who use bert embeddings (Devlin et al., 2019) instead of ELMo embeddings. In these systems, both bert and ELMo embeddings are used in a pre-trained fashion. More recently, Joshi et al. (2019b) fine-tuned the bert model for coreference, resulting in a small further improvement. Later, Joshi et al. (2019a) introduces SpanBERT which is trained for tasks that in"
2020.crac-1.11,W12-4501,0,0.602049,"can tackle these challenges. 1 Introduction Coreference resolution is the task of grouping mentions in a text that refer to the same real-world entity into clusters (Poesio et al., 2016) . Coreference resolution is a difficult task that requires reasoning, context understanding, and background knowledge of real-world entities, and has driven research in both natural language processing and machine learning, particularly since the release of the ontonotes multilingual corpus providing annotated coreference data for Arabic, Chinese and English and used for the 2011 and 2012 conll shared tasks (Pradhan et al., 2012). Since then, there has been substantial research on English coreference, most recently using neural coreference approaches (Lee et al., 2017; Lee et al., 2018; Kantor and Globerson, 2019a; Joshi et al., 2019b; Joshi et al., 2019a; Yu et al., 2020b; Wu et al., 2020), leading to a significant increase in the performance of coreference resolvers for English. By contrast, there has been almost no research on Arabic coreference; the performance for Arabic coreference resolution has not improved much since the conll 2012 shared task, and in particular no neural architectures have been proposed–the"
2020.crac-1.11,J01-4004,0,0.779837,"edge is the first neural coreference resolver for Arabic.1 One explanation for this lack of research might simply be the lack of training data large enough for the task. Another explanation might be that Arabic is more problematic than English because of its rich morphology, its many dialects, and/or its high degree of ambiguity. We explore the first of these possibilities. Coreference resolution can be further divided into two subtasks–mention detection and mention clustering–as illustrated in Figure 1. In early work, coreference’s two subtasks were usually carried out in a pipeline fashion (Soon et al., 2001; Fernandes et al., 2014; Björkelund and Kuhn, 2014; Wiseman et al., 2015; Wiseman et al., 2016; Clark and Manning, 2016a; Clark and Manning, 2016b), with candidate mentions selected prior the mention clustering step. Since Lee et al. (2017) introduced an end-to-end neural coreference architecture that achieved state of the art by carrying out the two tasks jointly, as first proposed by Daume and Marcu (2005), most state-of-the-art systems have followed this approach. However, no end-to-end solution was attempted for Arabic. We intend to explore whether an end-to-end solution would be practica"
2020.crac-1.11,W12-4505,0,0.706817,"d a language independent module that requires only syntactic information and clusters mentions using the memory-based learner TiMBL (Daelemans et al., 2004). Chen and Ng (2012) detected mentions by employing named entity and language-dependent heuristics. They employed multiple sieves (Lee et al., 2011) for English and Chinese, but only used an exact match sieve for Arabic because other sieves did not provide better results. Björkelund and Nugues (2011) considered all noun phrases and possessive pronouns as mentions, and trained two types of classifier: logistic regression and decision trees. Stamborg et al. (2012) extracted all noun phrases, pronouns, and possessive pronouns as mentions. Then they applied (Björkelund and Nugues, 2011)’s solver which consists of various lexical and graph dependency features. Uryupina et al. (2012) adapted for Arabic the BART (Versley et al., 2008) coreference resolution system, which consists of five components: pre-processing pipeline, mention factory, feature extraction module, decoder and encoder. Fernandes et al. (2014) defined a set of rules based on parse tree information to detect mentions, and utilized a latent tree representation to learn coreference chains. Si"
2020.crac-1.11,W12-4515,1,0.802968,"and language-dependent heuristics. They employed multiple sieves (Lee et al., 2011) for English and Chinese, but only used an exact match sieve for Arabic because other sieves did not provide better results. Björkelund and Nugues (2011) considered all noun phrases and possessive pronouns as mentions, and trained two types of classifier: logistic regression and decision trees. Stamborg et al. (2012) extracted all noun phrases, pronouns, and possessive pronouns as mentions. Then they applied (Björkelund and Nugues, 2011)’s solver which consists of various lexical and graph dependency features. Uryupina et al. (2012) adapted for Arabic the BART (Versley et al., 2008) coreference resolution system, which consists of five components: pre-processing pipeline, mention factory, feature extraction module, decoder and encoder. Fernandes et al. (2014) defined a set of rules based on parse tree information to detect mentions, and utilized a latent tree representation to learn coreference chains. Similarly Björkelund and Kuhn (2014) adopted a tree representation approach to cluster mentions, but improved the learning strategy and introduced non-local features to capture more information about coreference relations."
2020.crac-1.11,P15-1137,0,0.117341,"tion for this lack of research might simply be the lack of training data large enough for the task. Another explanation might be that Arabic is more problematic than English because of its rich morphology, its many dialects, and/or its high degree of ambiguity. We explore the first of these possibilities. Coreference resolution can be further divided into two subtasks–mention detection and mention clustering–as illustrated in Figure 1. In early work, coreference’s two subtasks were usually carried out in a pipeline fashion (Soon et al., 2001; Fernandes et al., 2014; Björkelund and Kuhn, 2014; Wiseman et al., 2015; Wiseman et al., 2016; Clark and Manning, 2016a; Clark and Manning, 2016b), with candidate mentions selected prior the mention clustering step. Since Lee et al. (2017) introduced an end-to-end neural coreference architecture that achieved state of the art by carrying out the two tasks jointly, as first proposed by Daume and Marcu (2005), most state-of-the-art systems have followed this approach. However, no end-to-end solution was attempted for Arabic. We intend to explore whether an end-to-end solution would be practicable with a corpus of more limited size. This work is licensed under a Cre"
2020.crac-1.11,N16-1114,0,0.0184173,"research might simply be the lack of training data large enough for the task. Another explanation might be that Arabic is more problematic than English because of its rich morphology, its many dialects, and/or its high degree of ambiguity. We explore the first of these possibilities. Coreference resolution can be further divided into two subtasks–mention detection and mention clustering–as illustrated in Figure 1. In early work, coreference’s two subtasks were usually carried out in a pipeline fashion (Soon et al., 2001; Fernandes et al., 2014; Björkelund and Kuhn, 2014; Wiseman et al., 2015; Wiseman et al., 2016; Clark and Manning, 2016a; Clark and Manning, 2016b), with candidate mentions selected prior the mention clustering step. Since Lee et al. (2017) introduced an end-to-end neural coreference architecture that achieved state of the art by carrying out the two tasks jointly, as first proposed by Daume and Marcu (2005), most state-of-the-art systems have followed this approach. However, no end-to-end solution was attempted for Arabic. We intend to explore whether an end-to-end solution would be practicable with a corpus of more limited size. This work is licensed under a Creative Commons Attribut"
2020.crac-1.11,2020.acl-main.622,0,0.110812,"standing, and background knowledge of real-world entities, and has driven research in both natural language processing and machine learning, particularly since the release of the ontonotes multilingual corpus providing annotated coreference data for Arabic, Chinese and English and used for the 2011 and 2012 conll shared tasks (Pradhan et al., 2012). Since then, there has been substantial research on English coreference, most recently using neural coreference approaches (Lee et al., 2017; Lee et al., 2018; Kantor and Globerson, 2019a; Joshi et al., 2019b; Joshi et al., 2019a; Yu et al., 2020b; Wu et al., 2020), leading to a significant increase in the performance of coreference resolvers for English. By contrast, there has been almost no research on Arabic coreference; the performance for Arabic coreference resolution has not improved much since the conll 2012 shared task, and in particular no neural architectures have been proposed–the current state-of-the-art system remains the model proposed in (Björkelund and Kuhn, 2014). In this paper we close this very obvious gap by proposing what to our knowledge is the first neural coreference resolver for Arabic.1 One explanation for this lack of research"
2020.crac-1.11,2020.lrec-1.1,1,0.89054,"ing, context understanding, and background knowledge of real-world entities, and has driven research in both natural language processing and machine learning, particularly since the release of the ontonotes multilingual corpus providing annotated coreference data for Arabic, Chinese and English and used for the 2011 and 2012 conll shared tasks (Pradhan et al., 2012). Since then, there has been substantial research on English coreference, most recently using neural coreference approaches (Lee et al., 2017; Lee et al., 2018; Kantor and Globerson, 2019a; Joshi et al., 2019b; Joshi et al., 2019a; Yu et al., 2020b; Wu et al., 2020), leading to a significant increase in the performance of coreference resolvers for English. By contrast, there has been almost no research on Arabic coreference; the performance for Arabic coreference resolution has not improved much since the conll 2012 shared task, and in particular no neural architectures have been proposed–the current state-of-the-art system remains the model proposed in (Björkelund and Kuhn, 2014). In this paper we close this very obvious gap by proposing what to our knowledge is the first neural coreference resolver for Arabic.1 One explanation for th"
2020.crac-1.11,2020.lrec-1.2,1,0.828592,"ing, context understanding, and background knowledge of real-world entities, and has driven research in both natural language processing and machine learning, particularly since the release of the ontonotes multilingual corpus providing annotated coreference data for Arabic, Chinese and English and used for the 2011 and 2012 conll shared tasks (Pradhan et al., 2012). Since then, there has been substantial research on English coreference, most recently using neural coreference approaches (Lee et al., 2017; Lee et al., 2018; Kantor and Globerson, 2019a; Joshi et al., 2019b; Joshi et al., 2019a; Yu et al., 2020b; Wu et al., 2020), leading to a significant increase in the performance of coreference resolvers for English. By contrast, there has been almost no research on Arabic coreference; the performance for Arabic coreference resolution has not improved much since the conll 2012 shared task, and in particular no neural architectures have been proposed–the current state-of-the-art system remains the model proposed in (Björkelund and Kuhn, 2014). In this paper we close this very obvious gap by proposing what to our knowledge is the first neural coreference resolver for Arabic.1 One explanation for th"
2020.crac-1.11,S10-1019,0,0.0843056,"Missing"
2020.crac-1.3,P95-1017,0,0.625677,"ntion-based model, but combined their network with (Chen and Ng, 2016) features to resolve AZPs. Yin et al. (2019) applied the same heuristics in (Chen and Ng, 2015) to identify AZPs and applied a collaborative-filtering approach to resolve AZPs. Kong et al. (2019) identified AZPs using a learning-based classifier with semantic, lexical and syntactic features, and used coreferential chain information to improve AZP resolution. Other languages: There has been also a great deal of research on identification and resolution of AZPs, particularly in Japanese (Yoshimoto, 1988; Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2006; Iida et al., 2007; Sasano et al., 2008; Sasano et al., 2009; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Iida et al., 2015; Yoshino et al., 2013; Yamashiro et al., 2018), but also in other languages, including Korean (Han, 2004; Byron et al., 2006), Spanish (Ferrández and Peral, 2000; Rello and Ilisei, 2009), Portuguese (Rello et al., 2012), Romanian (Mihăilă et al., 2011), Bulgarian (Grigorova, 2013), and Sanskrit (Gopal and Jha, 2017). Iida and Poesio (2011) proposed the first multilingual approach"
2020.crac-1.3,Q17-1010,0,0.0246328,"Missing"
2020.crac-1.3,D13-1135,0,0.11175,"on and applied a rule-based approach that employed Hobbs algorithm (Hobbs, 1978) to resolve ZPs in the Chinese Treebank; however, did not attempt to automatically identify AZP. Yeh and Chen (2006) is another rule-based approach, for AZP resolution and also used a set of handengineered rules to identify AZPs. Zhao and Ng (2007), the first machine learning approach to Chinese AZPs identification and resolution, by applying decision trees incorporated with a set of syntactic and positional features. (Kong and Zhou, 2010) employed a tree kernel-based approach to AZP identification and resolution. Chen and Ng (2013) is an extension of (Zhao and Ng, 2007), they incorporated contextual features for AZP resolution and applied a combination of syntactic, lexical and other features for the identification. Chen and Ng (2014) proposed unsupervised techniques to resolve AZPs and applied a set of rules to identify AZP. Chen and Ng (2015) is another unsupervised approach on the AZP resolution. Recent approaches applying deep-learning neural networks include Chen and Ng (2016) trained a binary classifier to identify AZP and applied a feed-forward neural network to the AZP resolution; Yin et al. (2016) used (Chen an"
2020.crac-1.3,D14-1084,0,0.0826246,"rule-based approach, for AZP resolution and also used a set of handengineered rules to identify AZPs. Zhao and Ng (2007), the first machine learning approach to Chinese AZPs identification and resolution, by applying decision trees incorporated with a set of syntactic and positional features. (Kong and Zhou, 2010) employed a tree kernel-based approach to AZP identification and resolution. Chen and Ng (2013) is an extension of (Zhao and Ng, 2007), they incorporated contextual features for AZP resolution and applied a combination of syntactic, lexical and other features for the identification. Chen and Ng (2014) proposed unsupervised techniques to resolve AZPs and applied a set of rules to identify AZP. Chen and Ng (2015) is another unsupervised approach on the AZP resolution. Recent approaches applying deep-learning neural networks include Chen and Ng (2016) trained a binary classifier to identify AZP and applied a feed-forward neural network to the AZP resolution; Yin et al. (2016) used (Chen and Ng, 2016)’s classifier to identify AZPs. For AZP resolution, they employed an LSTM to represent AZP and two subnetworks (general encoder and local encoder) to capture contextlevel and word-level informatio"
2020.crac-1.3,P15-2053,0,0.0158593,"Ng (2007), the first machine learning approach to Chinese AZPs identification and resolution, by applying decision trees incorporated with a set of syntactic and positional features. (Kong and Zhou, 2010) employed a tree kernel-based approach to AZP identification and resolution. Chen and Ng (2013) is an extension of (Zhao and Ng, 2007), they incorporated contextual features for AZP resolution and applied a combination of syntactic, lexical and other features for the identification. Chen and Ng (2014) proposed unsupervised techniques to resolve AZPs and applied a set of rules to identify AZP. Chen and Ng (2015) is another unsupervised approach on the AZP resolution. Recent approaches applying deep-learning neural networks include Chen and Ng (2016) trained a binary classifier to identify AZP and applied a feed-forward neural network to the AZP resolution; Yin et al. (2016) used (Chen and Ng, 2016)’s classifier to identify AZPs. For AZP resolution, they employed an LSTM to represent AZP and two subnetworks (general encoder and local encoder) to capture contextlevel and word-level information of the candidates; Yin et al. (2017) also applied (Chen and Ng, 2016)’s classifier 23 to detect AZPs and propo"
2020.crac-1.3,P16-1074,0,0.0779396,"set of syntactic and positional features. (Kong and Zhou, 2010) employed a tree kernel-based approach to AZP identification and resolution. Chen and Ng (2013) is an extension of (Zhao and Ng, 2007), they incorporated contextual features for AZP resolution and applied a combination of syntactic, lexical and other features for the identification. Chen and Ng (2014) proposed unsupervised techniques to resolve AZPs and applied a set of rules to identify AZP. Chen and Ng (2015) is another unsupervised approach on the AZP resolution. Recent approaches applying deep-learning neural networks include Chen and Ng (2016) trained a binary classifier to identify AZP and applied a feed-forward neural network to the AZP resolution; Yin et al. (2016) used (Chen and Ng, 2016)’s classifier to identify AZPs. For AZP resolution, they employed an LSTM to represent AZP and two subnetworks (general encoder and local encoder) to capture contextlevel and word-level information of the candidates; Yin et al. (2017) also applied (Chen and Ng, 2016)’s classifier 23 to detect AZPs and proposed an improved deep memory network to resolve AZPs; and Liu et al. (2017), applied an attention-based neural network to resolve AZPs and en"
2020.crac-1.3,W19-4828,0,0.023292,"et al., 2017) (Kong et al., 2019) Our model (r=10) Settings 1: Gold Parse R P F1 50.6 55.1 52.8 72.4 42.3 53.4 75.1 50.1 60.1 63.5 65.3 64.4 70.1 59.4 64.3 90.7 55.8 69.1 Settings 2: System Parse R P F1 30.8 34.4 32.5 42.3 26.8 32.8 43.7 30.7 36.1 57.2 55.7 56.4 60.2 40.2 48.2 81.9 59.2 68.7 Table 4: AZP identification results for Chinese. The highest score is in bold. 5.3 Discussion BERT representations work interestingly well on AZPs even though empty categories have not been considered during the BERT’s pretraining. Recent works (Jawahar et al., 2019; Kovaleva et al., 2019; Goldberg, 2019; Clark et al., 2019) have shown that BERT learns various linguistic information such as, syntactic roles, coreference resolution, semantic relations and others. Our experimental results suggest that these information might be encoded in AZP contexts which make them distinctive. Current approaches for AZP identification evaluate under two settings: gold and system annotations because the task depend highly on the annotation quality of parse trees. In our experiments, gold settings for both Arabic and Chinese achieve outstanding results. In system parse, Chinese achieves results similar to its gold setting; however"
2020.crac-1.3,C90-2047,0,0.69578,"Missing"
2020.crac-1.3,P00-1022,0,0.160947,"eferential chain information to improve AZP resolution. Other languages: There has been also a great deal of research on identification and resolution of AZPs, particularly in Japanese (Yoshimoto, 1988; Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2006; Iida et al., 2007; Sasano et al., 2008; Sasano et al., 2009; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Iida et al., 2015; Yoshino et al., 2013; Yamashiro et al., 2018), but also in other languages, including Korean (Han, 2004; Byron et al., 2006), Spanish (Ferrández and Peral, 2000; Rello and Ilisei, 2009), Portuguese (Rello et al., 2012), Romanian (Mihăilă et al., 2011), Bulgarian (Grigorova, 2013), and Sanskrit (Gopal and Jha, 2017). Iida and Poesio (2011) proposed the first multilingual approach for AZP resolution. Current approaches suffer from one (or more) of the following. First, they assume AZPs are available; so they focus mainly on the resolution part. Second, they apply on a private or very small size corpus. Third, they rely on an extensive set of features or language-dependent rules to identify AZP. 3 Model To identify AZPs, context understanding and semant"
2020.crac-1.3,2009.mtsummit-caasl.4,0,0.0769278,"evaluation settings in Section 4. We show the results and discuss them in Section 5. We conclude in Section 6. Figure 1: Chinese ZPs appear before a VP node (left), and Arabic ZPs appear after the verb of a VP head (right).In OntoNotes 5.0, Chinese AZPs are annotated as *pro* and Arabic AZPs as *. 2 Related Work AZP identification task has been considered independently, but also as a prerequisite step before AZP resolution task because the detection has a heavy impact on the resolution (Kong et al., 2019). Arabic: There have been a few studies devoted to AZPs and empty categories in general. Green et al. (2009) proposed a conditional-random-field (CRF) sequence classifier to detect Arabic noun phrases, and captured ZPs implicitly. Bakr et al. (2009) applied a statistical approach to detect empty categories. Gabbard (2010) proposed a pipeline made of maximum entropy classifiers which jointly make a CRF to retrieve Arabic empty categories. Aloraini and Poesio (2020) proposed the first neural model for resolving Arabic AZP, but they did not consider the AZP identification step. As far as we know, no previous work has considered Arabic AZP identification. Chinese: Converse (2006) studied AZP resolution"
2020.crac-1.3,W04-0205,0,0.0926891,"ical and syntactic features, and used coreferential chain information to improve AZP resolution. Other languages: There has been also a great deal of research on identification and resolution of AZPs, particularly in Japanese (Yoshimoto, 1988; Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2006; Iida et al., 2007; Sasano et al., 2008; Sasano et al., 2009; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Iida et al., 2015; Yoshino et al., 2013; Yamashiro et al., 2018), but also in other languages, including Korean (Han, 2004; Byron et al., 2006), Spanish (Ferrández and Peral, 2000; Rello and Ilisei, 2009), Portuguese (Rello et al., 2012), Romanian (Mihăilă et al., 2011), Bulgarian (Grigorova, 2013), and Sanskrit (Gopal and Jha, 2017). Iida and Poesio (2011) proposed the first multilingual approach for AZP resolution. Current approaches suffer from one (or more) of the following. First, they assume AZPs are available; so they focus mainly on the resolution part. Second, they apply on a private or very small size corpus. Third, they rely on an extensive set of features or language-dependent rules to identify AZP. 3"
2020.crac-1.3,D13-1095,0,0.0220457,"laborative-filtering approach to resolve AZPs. Kong et al. (2019) identified AZPs using a learning-based classifier with semantic, lexical and syntactic features, and used coreferential chain information to improve AZP resolution. Other languages: There has been also a great deal of research on identification and resolution of AZPs, particularly in Japanese (Yoshimoto, 1988; Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2006; Iida et al., 2007; Sasano et al., 2008; Sasano et al., 2009; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Iida et al., 2015; Yoshino et al., 2013; Yamashiro et al., 2018), but also in other languages, including Korean (Han, 2004; Byron et al., 2006), Spanish (Ferrández and Peral, 2000; Rello and Ilisei, 2009), Portuguese (Rello et al., 2012), Romanian (Mihăilă et al., 2011), Bulgarian (Grigorova, 2013), and Sanskrit (Gopal and Jha, 2017). Iida and Poesio (2011) proposed the first multilingual approach for AZP resolution. Current approaches suffer from one (or more) of the following. First, they assume AZPs are available; so they focus mainly on the resolution part. Second, they apply on a privat"
2020.crac-1.3,P11-1081,1,0.860219,"e (Yoshimoto, 1988; Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2006; Iida et al., 2007; Sasano et al., 2008; Sasano et al., 2009; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Iida et al., 2015; Yoshino et al., 2013; Yamashiro et al., 2018), but also in other languages, including Korean (Han, 2004; Byron et al., 2006), Spanish (Ferrández and Peral, 2000; Rello and Ilisei, 2009), Portuguese (Rello et al., 2012), Romanian (Mihăilă et al., 2011), Bulgarian (Grigorova, 2013), and Sanskrit (Gopal and Jha, 2017). Iida and Poesio (2011) proposed the first multilingual approach for AZP resolution. Current approaches suffer from one (or more) of the following. First, they assume AZPs are available; so they focus mainly on the resolution part. Second, they apply on a private or very small size corpus. Third, they rely on an extensive set of features or language-dependent rules to identify AZP. 3 Model To identify AZPs, context understanding and semantic knowledge of entities are essential in Chinese (Huang, 1984) as well as in Arabic which requires, in addition, deep understanding of its complex morphology (Alnajadat, 2017). Re"
2020.crac-1.3,P06-1079,0,0.047053,"6) features to resolve AZPs. Yin et al. (2019) applied the same heuristics in (Chen and Ng, 2015) to identify AZPs and applied a collaborative-filtering approach to resolve AZPs. Kong et al. (2019) identified AZPs using a learning-based classifier with semantic, lexical and syntactic features, and used coreferential chain information to improve AZP resolution. Other languages: There has been also a great deal of research on identification and resolution of AZPs, particularly in Japanese (Yoshimoto, 1988; Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2006; Iida et al., 2007; Sasano et al., 2008; Sasano et al., 2009; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Iida et al., 2015; Yoshino et al., 2013; Yamashiro et al., 2018), but also in other languages, including Korean (Han, 2004; Byron et al., 2006), Spanish (Ferrández and Peral, 2000; Rello and Ilisei, 2009), Portuguese (Rello et al., 2012), Romanian (Mihăilă et al., 2011), Bulgarian (Grigorova, 2013), and Sanskrit (Gopal and Jha, 2017). Iida and Poesio (2011) proposed the first multilingual approach for AZP resolution. Current approaches suffer from one (or more"
2020.crac-1.3,D15-1260,0,0.0150202,"approach to resolve AZPs. Kong et al. (2019) identified AZPs using a learning-based classifier with semantic, lexical and syntactic features, and used coreferential chain information to improve AZP resolution. Other languages: There has been also a great deal of research on identification and resolution of AZPs, particularly in Japanese (Yoshimoto, 1988; Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2006; Iida et al., 2007; Sasano et al., 2008; Sasano et al., 2009; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Iida et al., 2015; Yoshino et al., 2013; Yamashiro et al., 2018), but also in other languages, including Korean (Han, 2004; Byron et al., 2006), Spanish (Ferrández and Peral, 2000; Rello and Ilisei, 2009), Portuguese (Rello et al., 2012), Romanian (Mihăilă et al., 2011), Bulgarian (Grigorova, 2013), and Sanskrit (Gopal and Jha, 2017). Iida and Poesio (2011) proposed the first multilingual approach for AZP resolution. Current approaches suffer from one (or more) of the following. First, they assume AZPs are available; so they focus mainly on the resolution part. Second, they apply on a private or very small siz"
2020.crac-1.3,W03-1024,0,0.0975315,"rk with (Chen and Ng, 2016) features to resolve AZPs. Yin et al. (2019) applied the same heuristics in (Chen and Ng, 2015) to identify AZPs and applied a collaborative-filtering approach to resolve AZPs. Kong et al. (2019) identified AZPs using a learning-based classifier with semantic, lexical and syntactic features, and used coreferential chain information to improve AZP resolution. Other languages: There has been also a great deal of research on identification and resolution of AZPs, particularly in Japanese (Yoshimoto, 1988; Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2006; Iida et al., 2007; Sasano et al., 2008; Sasano et al., 2009; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Iida et al., 2015; Yoshino et al., 2013; Yamashiro et al., 2018), but also in other languages, including Korean (Han, 2004; Byron et al., 2006), Spanish (Ferrández and Peral, 2000; Rello and Ilisei, 2009), Portuguese (Rello et al., 2012), Romanian (Mihăilă et al., 2011), Bulgarian (Grigorova, 2013), and Sanskrit (Gopal and Jha, 2017). Iida and Poesio (2011) proposed the first multilingual approach for AZP resolution. Current approaches suffe"
2020.crac-1.3,P19-1356,0,0.119276,"e; so they focus mainly on the resolution part. Second, they apply on a private or very small size corpus. Third, they rely on an extensive set of features or language-dependent rules to identify AZP. 3 Model To identify AZPs, context understanding and semantic knowledge of entities are essential in Chinese (Huang, 1984) as well as in Arabic which requires, in addition, deep understanding of its complex morphology (Alnajadat, 2017). Recently, it has been shown that BERT (Devlin et al., 2018) can capture structural properties of a language, such as its surface, semantic, and syntactic aspects (Jawahar et al., 2019) which seems suitable for the AZP identification task. Therefore, we use BERT to produce representations for ZP candidates. Our model is a binary classifier that takes an automatically predicted ZP candidate as input, and classifies it as an AZP or not. In this section, we first give an overview of BERT and its adaptation modes. We then describe how we generate AZP candidates, and how we represent them. Finally, we present the training objective and hyperparameter tuning settings. 3.1 BERT BERT is a language representation model consisting of multiple stacked Transformers (Vaswani et al., 2017"
2020.crac-1.3,D10-1086,0,0.176796,"ious work has considered Arabic AZP identification. Chinese: Converse (2006) studied AZP resolution and applied a rule-based approach that employed Hobbs algorithm (Hobbs, 1978) to resolve ZPs in the Chinese Treebank; however, did not attempt to automatically identify AZP. Yeh and Chen (2006) is another rule-based approach, for AZP resolution and also used a set of handengineered rules to identify AZPs. Zhao and Ng (2007), the first machine learning approach to Chinese AZPs identification and resolution, by applying decision trees incorporated with a set of syntactic and positional features. (Kong and Zhou, 2010) employed a tree kernel-based approach to AZP identification and resolution. Chen and Ng (2013) is an extension of (Zhao and Ng, 2007), they incorporated contextual features for AZP resolution and applied a combination of syntactic, lexical and other features for the identification. Chen and Ng (2014) proposed unsupervised techniques to resolve AZPs and applied a set of rules to identify AZP. Chen and Ng (2015) is another unsupervised approach on the AZP resolution. Recent approaches applying deep-learning neural networks include Chen and Ng (2016) trained a binary classifier to identify AZP a"
2020.crac-1.3,D19-1445,0,0.0257059,"d Ng, 2014) (Chen and Ng, 2016) (Chang et al., 2017) (Kong et al., 2019) Our model (r=10) Settings 1: Gold Parse R P F1 50.6 55.1 52.8 72.4 42.3 53.4 75.1 50.1 60.1 63.5 65.3 64.4 70.1 59.4 64.3 90.7 55.8 69.1 Settings 2: System Parse R P F1 30.8 34.4 32.5 42.3 26.8 32.8 43.7 30.7 36.1 57.2 55.7 56.4 60.2 40.2 48.2 81.9 59.2 68.7 Table 4: AZP identification results for Chinese. The highest score is in bold. 5.3 Discussion BERT representations work interestingly well on AZPs even though empty categories have not been considered during the BERT’s pretraining. Recent works (Jawahar et al., 2019; Kovaleva et al., 2019; Goldberg, 2019; Clark et al., 2019) have shown that BERT learns various linguistic information such as, syntactic roles, coreference resolution, semantic relations and others. Our experimental results suggest that these information might be encoded in AZP contexts which make them distinctive. Current approaches for AZP identification evaluate under two settings: gold and system annotations because the task depend highly on the annotation quality of parse trees. In our experiments, gold settings for both Arabic and Chinese achieve outstanding results. In system parse, Chinese achieves results"
2020.crac-1.3,P17-1010,0,0.0587729,"ic, we deduce the reference information from the context, especially the verb that precedes the AZP, in the example the verb is ”wanted/”ﯾﺮﯾﺪ. Since English is not a pro-drop language (White, 1985), the AZP gap position is translated into an overt pronoun (he). The AZP problem has inspired much research because it benefits many natural language processing tasks such as machine translation (Mitkov and Schmidt, 1998), and coreference resolution (Mitkov et al., 2000). Recently, there has been a great deal of research on AZPs for Chinese (Kong et al., 2019; Yin et al., 2018; Chang et al., 2017; Liu et al., 2017; Yin et al., 2017), Arabic (Aloraini and Poesio, 2020), Japanese (Shimazu et al., 2020), Korean (Jung and Lee, 2018), and other languages (Grigorova, 2016; Gopal and Jha, 2017). A major drawback of many existing studies is the assumption that AZP locations are given; hence, they focus primarily on resolving AZPs to their correct antecedent. However, such assumption does not reflect real-life applications. Another drawback is that current AZP identification systems rely on language-dependent features and fail to detect many AZP locations. In addition, some languages do not have an AZP identifi"
2020.crac-1.3,J93-2004,0,0.07044,"first neural network model of AZP identification for Arabic; and our approach outperforms the stateof-the-art for Chinese. Experiment results suggest that BERT implicitly encode information about AZPs through their surrounding context. 1 Introduction Empty categories provide an important source of syntactic information about the phonetically null arguments in prodrop languages such as Arabic (Eid, 1983), Chinese (Li and Thompson, 1979), Italian (Di Eugenio, 1990), Japanese (Kameyama, 1985), and others (Bever and Sanz, 1997; Kim, 2000). The use of empty categories started with Penn Treebanks (Marcus et al., 1993), followed by Arabic Treebank (Maamouri et al., 2004), Chinese Treebank (Xue et al., 2005) and other Penn-style series. Empty categories are used to represent traces, such as, movement operations in interrogative sentence, also to represent right node raising which is a shared argument in the rightmost constituent of a coordinate structure. Another usage of empty categories is zero-pronouns (ZP) which are omitted pronouns in places where they are expected to be, and function as overt pronouns. Anaphoric zero pronouns (AZP) are ZPs that corefer to one or more noun phrases in a preceding text. T"
2020.crac-1.3,W19-4302,0,0.0246945,"rs for words and contexts. BERT was pretrained on different settings, we use BERT-base Multilingual which was pretrained on many languages, including Chinese and Arabic, and is publicly available1 . BERT has two modes of adaptation: feature extraction and fine-tuning. Feature extraction (also called feature-based) is when BERT representations are used as they were originally pretrained, without any further training. Fine-tuning is the process of slightly adjusting BERT’s parameters for a target task. Feature extraction is computationally cheaper and might be more suitable for a specific task (Peters et al., 2019). Fine-tuning is more convenient to utilize, but restricted to several generalpurpose tasks. AZP identification task was not pretrained as part of BERT tasks and not directly applicable to fine tuning mode without any modifications to BERT’s architecture. We employ feature extraction mode to represent AZP candidate in our classifier. 3.2 Candidate Generation Although ZPs are annotated in OntoNotes, our model works off automatically predicted candidates. ZP locations differ in Chinese and Arabic. In Chinese, ZPs appear before a VP node while in Arabic they appear after the head of a VP node 2 ."
2020.crac-1.3,W12-4501,0,0.0386263,"depend on the target language structure. The second step is classification step which determines which of the extracted candidate are AZP. The classification step is more challenging because of the varieties and size of the extracted candidates. In this paper, we propose a multilingual approach to AZP identification based on BERT. We make three main contributions: • We propose a BERT-based multilingual model and evaluate on languages that differ completely in their morphological structure: Arabic and Chinese. (Arabic is morphologically rich, whereas Chinese’s morphology is relatively simple (Pradhan et al., 2012)) • Ours is the first neural network-based AZP identification model for Arabic, and it substantially surpasses the current state-of-the-art on Chinese. • Our experimental results suggest that BERT representations encode information about AZPs through their context. The rest of the paper is organized as follows. We review Arabic and Chinese ZP-related literature, and other languages as well in Section 2. We explain our proposed model in Section 3. We discuss the evaluation settings in Section 4. We show the results and discuss them in Section 5. We conclude in Section 6. Figure 1: Chinese ZPs a"
2020.crac-1.3,R09-2011,0,0.0401521,"n to improve AZP resolution. Other languages: There has been also a great deal of research on identification and resolution of AZPs, particularly in Japanese (Yoshimoto, 1988; Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2006; Iida et al., 2007; Sasano et al., 2008; Sasano et al., 2009; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Iida et al., 2015; Yoshino et al., 2013; Yamashiro et al., 2018), but also in other languages, including Korean (Han, 2004; Byron et al., 2006), Spanish (Ferrández and Peral, 2000; Rello and Ilisei, 2009), Portuguese (Rello et al., 2012), Romanian (Mihăilă et al., 2011), Bulgarian (Grigorova, 2013), and Sanskrit (Gopal and Jha, 2017). Iida and Poesio (2011) proposed the first multilingual approach for AZP resolution. Current approaches suffer from one (or more) of the following. First, they assume AZPs are available; so they focus mainly on the resolution part. Second, they apply on a private or very small size corpus. Third, they rely on an extensive set of features or language-dependent rules to identify AZP. 3 Model To identify AZPs, context understanding and semantic knowledge of entities"
2020.crac-1.3,I11-1085,0,0.0245959,"hen and Ng, 2015) to identify AZPs and applied a collaborative-filtering approach to resolve AZPs. Kong et al. (2019) identified AZPs using a learning-based classifier with semantic, lexical and syntactic features, and used coreferential chain information to improve AZP resolution. Other languages: There has been also a great deal of research on identification and resolution of AZPs, particularly in Japanese (Yoshimoto, 1988; Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2006; Iida et al., 2007; Sasano et al., 2008; Sasano et al., 2009; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Iida et al., 2015; Yoshino et al., 2013; Yamashiro et al., 2018), but also in other languages, including Korean (Han, 2004; Byron et al., 2006), Spanish (Ferrández and Peral, 2000; Rello and Ilisei, 2009), Portuguese (Rello et al., 2012), Romanian (Mihăilă et al., 2011), Bulgarian (Grigorova, 2013), and Sanskrit (Gopal and Jha, 2017). Iida and Poesio (2011) proposed the first multilingual approach for AZP resolution. Current approaches suffer from one (or more) of the following. First, they assume AZPs are available; so they focus mainly on the re"
2020.crac-1.3,C08-1097,0,0.0107064,". (2019) applied the same heuristics in (Chen and Ng, 2015) to identify AZPs and applied a collaborative-filtering approach to resolve AZPs. Kong et al. (2019) identified AZPs using a learning-based classifier with semantic, lexical and syntactic features, and used coreferential chain information to improve AZP resolution. Other languages: There has been also a great deal of research on identification and resolution of AZPs, particularly in Japanese (Yoshimoto, 1988; Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2006; Iida et al., 2007; Sasano et al., 2008; Sasano et al., 2009; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Iida et al., 2015; Yoshino et al., 2013; Yamashiro et al., 2018), but also in other languages, including Korean (Han, 2004; Byron et al., 2006), Spanish (Ferrández and Peral, 2000; Rello and Ilisei, 2009), Portuguese (Rello et al., 2012), Romanian (Mihăilă et al., 2011), Bulgarian (Grigorova, 2013), and Sanskrit (Gopal and Jha, 2017). Iida and Poesio (2011) proposed the first multilingual approach for AZP resolution. Current approaches suffer from one (or more) of the following. First, they assume A"
2020.crac-1.3,N09-1059,0,0.0208106,"same heuristics in (Chen and Ng, 2015) to identify AZPs and applied a collaborative-filtering approach to resolve AZPs. Kong et al. (2019) identified AZPs using a learning-based classifier with semantic, lexical and syntactic features, and used coreferential chain information to improve AZP resolution. Other languages: There has been also a great deal of research on identification and resolution of AZPs, particularly in Japanese (Yoshimoto, 1988; Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2006; Iida et al., 2007; Sasano et al., 2008; Sasano et al., 2009; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Iida et al., 2015; Yoshino et al., 2013; Yamashiro et al., 2018), but also in other languages, including Korean (Han, 2004; Byron et al., 2006), Spanish (Ferrández and Peral, 2000; Rello and Ilisei, 2009), Portuguese (Rello et al., 2012), Romanian (Mihăilă et al., 2011), Bulgarian (Grigorova, 2013), and Sanskrit (Gopal and Jha, 2017). Iida and Poesio (2011) proposed the first multilingual approach for AZP resolution. Current approaches suffer from one (or more) of the following. First, they assume AZPs are available; so"
2020.crac-1.3,C02-1078,0,0.215336,"ombined their network with (Chen and Ng, 2016) features to resolve AZPs. Yin et al. (2019) applied the same heuristics in (Chen and Ng, 2015) to identify AZPs and applied a collaborative-filtering approach to resolve AZPs. Kong et al. (2019) identified AZPs using a learning-based classifier with semantic, lexical and syntactic features, and used coreferential chain information to improve AZP resolution. Other languages: There has been also a great deal of research on identification and resolution of AZPs, particularly in Japanese (Yoshimoto, 1988; Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2006; Iida et al., 2007; Sasano et al., 2008; Sasano et al., 2009; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Iida et al., 2015; Yoshino et al., 2013; Yamashiro et al., 2018), but also in other languages, including Korean (Han, 2004; Byron et al., 2006), Spanish (Ferrández and Peral, 2000; Rello and Ilisei, 2009), Portuguese (Rello et al., 2012), Romanian (Mihăilă et al., 2011), Bulgarian (Grigorova, 2013), and Sanskrit (Gopal and Jha, 2017). Iida and Poesio (2011) proposed the first multilingual approach for AZP resolution."
2020.crac-1.3,2020.lrec-1.447,0,0.0268431,"precedes the AZP, in the example the verb is ”wanted/”ﯾﺮﯾﺪ. Since English is not a pro-drop language (White, 1985), the AZP gap position is translated into an overt pronoun (he). The AZP problem has inspired much research because it benefits many natural language processing tasks such as machine translation (Mitkov and Schmidt, 1998), and coreference resolution (Mitkov et al., 2000). Recently, there has been a great deal of research on AZPs for Chinese (Kong et al., 2019; Yin et al., 2018; Chang et al., 2017; Liu et al., 2017; Yin et al., 2017), Arabic (Aloraini and Poesio, 2020), Japanese (Shimazu et al., 2020), Korean (Jung and Lee, 2018), and other languages (Grigorova, 2016; Gopal and Jha, 2017). A major drawback of many existing studies is the assumption that AZP locations are given; hence, they focus primarily on resolving AZPs to their correct antecedent. However, such assumption does not reflect real-life applications. Another drawback is that current AZP identification systems rely on language-dependent features and fail to detect many AZP locations. In addition, some languages do not have an AZP identification system, one of which is Arabic. This work is licensed under a Creative Commons At"
2020.crac-1.3,Y18-1089,0,0.0115645,"19) identified AZPs using a learning-based classifier with semantic, lexical and syntactic features, and used coreferential chain information to improve AZP resolution. Other languages: There has been also a great deal of research on identification and resolution of AZPs, particularly in Japanese (Yoshimoto, 1988; Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2006; Iida et al., 2007; Sasano et al., 2008; Sasano et al., 2009; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Iida et al., 2015; Yoshino et al., 2013; Yamashiro et al., 2018), but also in other languages, including Korean (Han, 2004; Byron et al., 2006), Spanish (Ferrández and Peral, 2000; Rello and Ilisei, 2009), Portuguese (Rello et al., 2012), Romanian (Mihăilă et al., 2011), Bulgarian (Grigorova, 2013), and Sanskrit (Gopal and Jha, 2017). Iida and Poesio (2011) proposed the first multilingual approach for AZP resolution. Current approaches suffer from one (or more) of the following. First, they assume AZPs are available; so they focus mainly on the resolution part. Second, they apply on a private or very small size corpus. Third, they rely on an extensive set"
2020.crac-1.3,D17-1135,0,0.0571057,"reference information from the context, especially the verb that precedes the AZP, in the example the verb is ”wanted/”ﯾﺮﯾﺪ. Since English is not a pro-drop language (White, 1985), the AZP gap position is translated into an overt pronoun (he). The AZP problem has inspired much research because it benefits many natural language processing tasks such as machine translation (Mitkov and Schmidt, 1998), and coreference resolution (Mitkov et al., 2000). Recently, there has been a great deal of research on AZPs for Chinese (Kong et al., 2019; Yin et al., 2018; Chang et al., 2017; Liu et al., 2017; Yin et al., 2017), Arabic (Aloraini and Poesio, 2020), Japanese (Shimazu et al., 2020), Korean (Jung and Lee, 2018), and other languages (Grigorova, 2016; Gopal and Jha, 2017). A major drawback of many existing studies is the assumption that AZP locations are given; hence, they focus primarily on resolving AZPs to their correct antecedent. However, such assumption does not reflect real-life applications. Another drawback is that current AZP identification systems rely on language-dependent features and fail to detect many AZP locations. In addition, some languages do not have an AZP identification system, one"
2020.crac-1.3,C18-1002,0,0.0930532,", in the example ”Bush/”ﺑﻮش. In Arabic, we deduce the reference information from the context, especially the verb that precedes the AZP, in the example the verb is ”wanted/”ﯾﺮﯾﺪ. Since English is not a pro-drop language (White, 1985), the AZP gap position is translated into an overt pronoun (he). The AZP problem has inspired much research because it benefits many natural language processing tasks such as machine translation (Mitkov and Schmidt, 1998), and coreference resolution (Mitkov et al., 2000). Recently, there has been a great deal of research on AZPs for Chinese (Kong et al., 2019; Yin et al., 2018; Chang et al., 2017; Liu et al., 2017; Yin et al., 2017), Arabic (Aloraini and Poesio, 2020), Japanese (Shimazu et al., 2020), Korean (Jung and Lee, 2018), and other languages (Grigorova, 2016; Gopal and Jha, 2017). A major drawback of many existing studies is the assumption that AZP locations are given; hence, they focus primarily on resolving AZPs to their correct antecedent. However, such assumption does not reflect real-life applications. Another drawback is that current AZP identification systems rely on language-dependent features and fail to detect many AZP locations. In addition, some"
2020.crac-1.3,I11-1126,0,0.0141238,"y AZPs and applied a collaborative-filtering approach to resolve AZPs. Kong et al. (2019) identified AZPs using a learning-based classifier with semantic, lexical and syntactic features, and used coreferential chain information to improve AZP resolution. Other languages: There has been also a great deal of research on identification and resolution of AZPs, particularly in Japanese (Yoshimoto, 1988; Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2006; Iida et al., 2007; Sasano et al., 2008; Sasano et al., 2009; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Iida et al., 2015; Yoshino et al., 2013; Yamashiro et al., 2018), but also in other languages, including Korean (Han, 2004; Byron et al., 2006), Spanish (Ferrández and Peral, 2000; Rello and Ilisei, 2009), Portuguese (Rello et al., 2012), Romanian (Mihăilă et al., 2011), Bulgarian (Grigorova, 2013), and Sanskrit (Gopal and Jha, 2017). Iida and Poesio (2011) proposed the first multilingual approach for AZP resolution. Current approaches suffer from one (or more) of the following. First, they assume AZPs are available; so they focus mainly on the resolution part. Second, t"
2020.crac-1.3,C88-2159,0,0.718676,"n. Yin et al. (2018), also used an attention-based model, but combined their network with (Chen and Ng, 2016) features to resolve AZPs. Yin et al. (2019) applied the same heuristics in (Chen and Ng, 2015) to identify AZPs and applied a collaborative-filtering approach to resolve AZPs. Kong et al. (2019) identified AZPs using a learning-based classifier with semantic, lexical and syntactic features, and used coreferential chain information to improve AZP resolution. Other languages: There has been also a great deal of research on identification and resolution of AZPs, particularly in Japanese (Yoshimoto, 1988; Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2006; Iida et al., 2007; Sasano et al., 2008; Sasano et al., 2009; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Iida et al., 2015; Yoshino et al., 2013; Yamashiro et al., 2018), but also in other languages, including Korean (Han, 2004; Byron et al., 2006), Spanish (Ferrández and Peral, 2000; Rello and Ilisei, 2009), Portuguese (Rello et al., 2012), Romanian (Mihăilă et al., 2011), Bulgarian (Grigorova, 2013), and Sanskrit (Gopal and Jha, 2017). Iida and Poesio (2"
2020.crac-1.3,I13-1126,0,0.0112778,"AZPs. Kong et al. (2019) identified AZPs using a learning-based classifier with semantic, lexical and syntactic features, and used coreferential chain information to improve AZP resolution. Other languages: There has been also a great deal of research on identification and resolution of AZPs, particularly in Japanese (Yoshimoto, 1988; Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2006; Iida et al., 2007; Sasano et al., 2008; Sasano et al., 2009; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Iida et al., 2015; Yoshino et al., 2013; Yamashiro et al., 2018), but also in other languages, including Korean (Han, 2004; Byron et al., 2006), Spanish (Ferrández and Peral, 2000; Rello and Ilisei, 2009), Portuguese (Rello et al., 2012), Romanian (Mihăilă et al., 2011), Bulgarian (Grigorova, 2013), and Sanskrit (Gopal and Jha, 2017). Iida and Poesio (2011) proposed the first multilingual approach for AZP resolution. Current approaches suffer from one (or more) of the following. First, they assume AZPs are available; so they focus mainly on the resolution part. Second, they apply on a private or very small size corpus. Third, they"
2020.crac-1.3,D07-1057,0,0.126285,"e Arabic empty categories. Aloraini and Poesio (2020) proposed the first neural model for resolving Arabic AZP, but they did not consider the AZP identification step. As far as we know, no previous work has considered Arabic AZP identification. Chinese: Converse (2006) studied AZP resolution and applied a rule-based approach that employed Hobbs algorithm (Hobbs, 1978) to resolve ZPs in the Chinese Treebank; however, did not attempt to automatically identify AZP. Yeh and Chen (2006) is another rule-based approach, for AZP resolution and also used a set of handengineered rules to identify AZPs. Zhao and Ng (2007), the first machine learning approach to Chinese AZPs identification and resolution, by applying decision trees incorporated with a set of syntactic and positional features. (Kong and Zhou, 2010) employed a tree kernel-based approach to AZP identification and resolution. Chen and Ng (2013) is an extension of (Zhao and Ng, 2007), they incorporated contextual features for AZP resolution and applied a combination of syntactic, lexical and other features for the identification. Chen and Ng (2014) proposed unsupervised techniques to resolve AZPs and applied a set of rules to identify AZP. Chen and"
2020.gamnlp-1.11,W09-3309,1,0.776102,"Missing"
2020.gamnlp-1.11,D18-1218,1,0.817256,"A player who performs poorly in terms of accuracy can simply hoard points by playing longer and still reach the next player level. Therefore, when assessing the players’ competence for more advanced tasks, their annotation accuracy can be a better indicator rather than the points they managed to hoard. Comparing the players’ annotations to the gold or aggregated data yields the player accuracy. However, cases in Phrase Detectives show that higher numbers of players can agree on a wrong annotation while fewer number of skilled players might contrarily have given the correct answer for a label (Paun et al., 2018). Relying solely on the number of annotators can be misleading in such cases. Therefore, Mention Pair Annotations model (MPA) builds a confidence-based model. MPA generates confidence scores for annotations, and players, via Bayesian models with the players’ annotation accuracy taken into consideration. Players who have higher accuracy gain a higher confidence score from a range between 0 and 1. During data aggregation, the annotations of players with higher confidence scores are evaluated with higher weight. MPA also generates separate player confidence scores for each task, evaluating player"
2020.lrec-1.1,W07-1009,0,0.0671979,"al., 2016); thus, the quality of mention detection affects both the performance of models for such applications and the quality of annotated data used to train them (Chamberlain et al., 2016; Poesio et al., 2019). Much mention detection research for NER has concentrated on a simplified version of MD that focuses on proper names only (i.e., it doesn’t consider as mentions nominals such as the protein or pronouns such as it), and ignores the fact that mentions may nest (e.g., noun phrases such as [[CCITA] mRNA] in the GENIA corpus are mentions of two separate entities, CCITA and CCITA and mRNA (Alex et al., 2007)). However such simplified view of mentions is not sufficient for NER in domains such as biomedical, or for coreference, that requires full mention detection. Another limitation of typical mention detection systems is that they only predict mentions in a HIGH F 1 fashion, whereas in coreference, for instance, mentions are usually predicted in a HIGH RE CALL setting, since further pruning will be carried out at the coreference system (Clark and Manning, 2016b; Clark and Manning, 2016a; Lee et al., 2017; Lee et al., 2018; Kantor and Globerson, 2019). There are only very few recent studies that a"
2020.lrec-1.1,P14-1005,0,0.171029,"Missing"
2020.lrec-1.1,L16-1323,1,0.891442,"Missing"
2020.lrec-1.1,Q16-1026,0,0.073381,"Missing"
2020.lrec-1.1,P15-1136,0,0.351811,"ition, Deep Neural Networks 1. Introduction tract mentions for an annotation project, or by other coreference systems. Thus the only standalone mention detectors that can be used as preprocessing for a coreference annotation are ones that do not take advantage of these advances and still heavily rely on parsing to identify all NPs as candidate mentions (Bj¨orkelund and Kuhn, 2014; Wiseman et al., 2015; Wiseman et al., 2016) or ones that use the rulebased mention detector from the Stanford deterministic system (Lee et al., 2013) to extract mentions from NPs, named entity mentions and pronouns (Clark and Manning, 2015; Clark and Manning, 2016b). To the best of our knowledge, Poesio et al. (2018) introduced the only standalone neural mention detector. By using a modified version of the NER system of Lample et al. (2016), they showed substantial performance gains at mention detection on the benchmark CONLL 2012 data set and on the CRAC 2018 data set when compared with the Stanford deterministic system. In this paper, we compare three neural architectures for standalone MD. The first system is a slightly modified version of the mention detection part of the Lee et al. (2018) system. The second system employs"
2020.lrec-1.1,D16-1245,0,0.366373,"ks 1. Introduction tract mentions for an annotation project, or by other coreference systems. Thus the only standalone mention detectors that can be used as preprocessing for a coreference annotation are ones that do not take advantage of these advances and still heavily rely on parsing to identify all NPs as candidate mentions (Bj¨orkelund and Kuhn, 2014; Wiseman et al., 2015; Wiseman et al., 2016) or ones that use the rulebased mention detector from the Stanford deterministic system (Lee et al., 2013) to extract mentions from NPs, named entity mentions and pronouns (Clark and Manning, 2015; Clark and Manning, 2016b). To the best of our knowledge, Poesio et al. (2018) introduced the only standalone neural mention detector. By using a modified version of the NER system of Lample et al. (2016), they showed substantial performance gains at mention detection on the benchmark CONLL 2012 data set and on the CRAC 2018 data set when compared with the Stanford deterministic system. In this paper, we compare three neural architectures for standalone MD. The first system is a slightly modified version of the mention detection part of the Lee et al. (2018) system. The second system employs a bi-directional LSTM on"
2020.lrec-1.1,P16-1061,0,0.276917,"ks 1. Introduction tract mentions for an annotation project, or by other coreference systems. Thus the only standalone mention detectors that can be used as preprocessing for a coreference annotation are ones that do not take advantage of these advances and still heavily rely on parsing to identify all NPs as candidate mentions (Bj¨orkelund and Kuhn, 2014; Wiseman et al., 2015; Wiseman et al., 2016) or ones that use the rulebased mention detector from the Stanford deterministic system (Lee et al., 2013) to extract mentions from NPs, named entity mentions and pronouns (Clark and Manning, 2015; Clark and Manning, 2016b). To the best of our knowledge, Poesio et al. (2018) introduced the only standalone neural mention detector. By using a modified version of the NER system of Lample et al. (2016), they showed substantial performance gains at mention detection on the benchmark CONLL 2012 data set and on the CRAC 2018 data set when compared with the Stanford deterministic system. In this paper, we compare three neural architectures for standalone MD. The first system is a slightly modified version of the mention detection part of the Lee et al. (2018) system. The second system employs a bi-directional LSTM on"
2020.lrec-1.1,N19-1423,0,0.491022,"et al. (2016), they showed substantial performance gains at mention detection on the benchmark CONLL 2012 data set and on the CRAC 2018 data set when compared with the Stanford deterministic system. In this paper, we compare three neural architectures for standalone MD. The first system is a slightly modified version of the mention detection part of the Lee et al. (2018) system. The second system employs a bi-directional LSTM on the sentence level and uses biaffine attention (Dozat and Manning, 2017) over the LSTM outputs to predict the mentions. The third system takes the outputs from BERT (Devlin et al., 2019) and feeds them into a feed-forward neural network to classify candidates into mentions and non mentions. All three systems have the options to output mentions in HIGH RECALL or HIGH F 1 settings; the former is well suited for the coreference task, whereas the latter can be used as a standard mention detector for tasks like nested named entity recognition. We evaluate our models on the CONLL and the CRAC data sets for coreference mention detection, and on GENIA corpora for nested NER. The contributions of this paper are therefore as follows. First, we show that mention detection performance im"
2020.lrec-1.1,D09-1015,0,0.0189104,"ion (the PEAR stories). This corpus is more appropriate for studying mention detection as all mentions are annotated. As done in the CRAC shared task, we used the RST portion of the corpora, consisting of news texts (1/3 of the PENN Treebank). Since none of the state-of-the-art coreference systems predict singleton mentions, a version of the CRAC dataset with singleton mentions excluded was created for the coreference task evaluation. The GENIA corpora is one of the main resources for studying nested NER. We use the GENIA v3.0.2 corpus and preprocess the dataset following the same settings of Finkel and Manning (2009) and Lu and Roth (2015). Historically, the dataset has been split into two different ways: the first approach splits the data into two sets (train and test) by 90:10 (GENIA 90), whereas the second approach further creates a development set by splitting the data into 81:9:10 (GENIA 81). We evaluate our model on both approaches to make the fair comparisons with previous work. For evaluation on GENIA 90, since we do not have a development set, we train our model for 40K steps (20 epochs) and take evaluate on the final model. 4.2. Value L EE, B IA L EE, B IA L EE, B IA B ER B ER B ER L EE, B IA, B"
2020.lrec-1.1,N18-1131,0,0.332814,"not be directly applied in tasks that require nested mentions, such as NER in the biomedical domain or coreference. The first neural network based NER model was introduced by Collobert et al. (2011), who used a CNN to encode the tokens and applied a CRF layer on top. After that, many other network architectures for NER MD have also been proposed, such as LSTM - CRF (Lample et al., 2016; Chiu and Nichols, 2016), LSTM-CRF + ELMO (Peters et al., 2018) and BERT (Devlin et al., 2019). Recently, a number of NER systems based on neural network architectures have been introduced to solve nested NER . Ju et al. (2018) introduce a stacked LSTM - CRF approach to solve nested NER in multi-steps. Sohrab and Miwa (2018) use an exhaustive region classification model. Lin et al. (2019) solve the problem in two steps: they first detect the entity head, and then infer the entity boundaries and classes in the second step. Strakov´a et al. (2019) infer the nested NER by a sequence-to-sequence model. Zheng et al. (2019) introduce a boundary aware network to train the boundary detection and the entity classification models in a multi-task learning setting. However, none of those systems can be directly used for corefer"
2020.lrec-1.1,P19-1066,0,0.171584,"ons of two separate entities, CCITA and CCITA and mRNA (Alex et al., 2007)). However such simplified view of mentions is not sufficient for NER in domains such as biomedical, or for coreference, that requires full mention detection. Another limitation of typical mention detection systems is that they only predict mentions in a HIGH F 1 fashion, whereas in coreference, for instance, mentions are usually predicted in a HIGH RE CALL setting, since further pruning will be carried out at the coreference system (Clark and Manning, 2016b; Clark and Manning, 2016a; Lee et al., 2017; Lee et al., 2018; Kantor and Globerson, 2019). There are only very few recent studies that attempt to apply neural network approaches to develop a standalone mention detector. Neural network approaches using contextsensitive embeddings such as ELMO (Peters et al., 2018) and BERT (Devlin et al., 2019) have resulted in substantial improvements for mention detectors in the NER benchmark CONLL 2003 data set. However, most coreference systems that appeared after Lee et al., (2017; 2018) carry out mention detection as a part of their end-to-end coreference system. Such systems do not output intermediate mentions, hence the mention detector can"
2020.lrec-1.1,N16-1030,0,0.220661,"rence annotation are ones that do not take advantage of these advances and still heavily rely on parsing to identify all NPs as candidate mentions (Bj¨orkelund and Kuhn, 2014; Wiseman et al., 2015; Wiseman et al., 2016) or ones that use the rulebased mention detector from the Stanford deterministic system (Lee et al., 2013) to extract mentions from NPs, named entity mentions and pronouns (Clark and Manning, 2015; Clark and Manning, 2016b). To the best of our knowledge, Poesio et al. (2018) introduced the only standalone neural mention detector. By using a modified version of the NER system of Lample et al. (2016), they showed substantial performance gains at mention detection on the benchmark CONLL 2012 data set and on the CRAC 2018 data set when compared with the Stanford deterministic system. In this paper, we compare three neural architectures for standalone MD. The first system is a slightly modified version of the mention detection part of the Lee et al. (2018) system. The second system employs a bi-directional LSTM on the sentence level and uses biaffine attention (Dozat and Manning, 2017) over the LSTM outputs to predict the mentions. The third system takes the outputs from BERT (Devlin et al.,"
2020.lrec-1.1,J13-4004,0,0.272961,"ask. Keywords: Mention Detection, Coreference Resolution, Nested Named Entity Recognition, Deep Neural Networks 1. Introduction tract mentions for an annotation project, or by other coreference systems. Thus the only standalone mention detectors that can be used as preprocessing for a coreference annotation are ones that do not take advantage of these advances and still heavily rely on parsing to identify all NPs as candidate mentions (Bj¨orkelund and Kuhn, 2014; Wiseman et al., 2015; Wiseman et al., 2016) or ones that use the rulebased mention detector from the Stanford deterministic system (Lee et al., 2013) to extract mentions from NPs, named entity mentions and pronouns (Clark and Manning, 2015; Clark and Manning, 2016b). To the best of our knowledge, Poesio et al. (2018) introduced the only standalone neural mention detector. By using a modified version of the NER system of Lample et al. (2016), they showed substantial performance gains at mention detection on the benchmark CONLL 2012 data set and on the CRAC 2018 data set when compared with the Stanford deterministic system. In this paper, we compare three neural architectures for standalone MD. The first system is a slightly modified version"
2020.lrec-1.1,D17-1018,0,0.0677949,"mRNA] in the GENIA corpus are mentions of two separate entities, CCITA and CCITA and mRNA (Alex et al., 2007)). However such simplified view of mentions is not sufficient for NER in domains such as biomedical, or for coreference, that requires full mention detection. Another limitation of typical mention detection systems is that they only predict mentions in a HIGH F 1 fashion, whereas in coreference, for instance, mentions are usually predicted in a HIGH RE CALL setting, since further pruning will be carried out at the coreference system (Clark and Manning, 2016b; Clark and Manning, 2016a; Lee et al., 2017; Lee et al., 2018; Kantor and Globerson, 2019). There are only very few recent studies that attempt to apply neural network approaches to develop a standalone mention detector. Neural network approaches using contextsensitive embeddings such as ELMO (Peters et al., 2018) and BERT (Devlin et al., 2019) have resulted in substantial improvements for mention detectors in the NER benchmark CONLL 2003 data set. However, most coreference systems that appeared after Lee et al., (2017; 2018) carry out mention detection as a part of their end-to-end coreference system. Such systems do not output interm"
2020.lrec-1.1,N18-2108,0,0.126431,"ntity mentions and pronouns (Clark and Manning, 2015; Clark and Manning, 2016b). To the best of our knowledge, Poesio et al. (2018) introduced the only standalone neural mention detector. By using a modified version of the NER system of Lample et al. (2016), they showed substantial performance gains at mention detection on the benchmark CONLL 2012 data set and on the CRAC 2018 data set when compared with the Stanford deterministic system. In this paper, we compare three neural architectures for standalone MD. The first system is a slightly modified version of the mention detection part of the Lee et al. (2018) system. The second system employs a bi-directional LSTM on the sentence level and uses biaffine attention (Dozat and Manning, 2017) over the LSTM outputs to predict the mentions. The third system takes the outputs from BERT (Devlin et al., 2019) and feeds them into a feed-forward neural network to classify candidates into mentions and non mentions. All three systems have the options to output mentions in HIGH RECALL or HIGH F 1 settings; the former is well suited for the coreference task, whereas the latter can be used as a standard mention detector for tasks like nested named entity recognit"
2020.lrec-1.1,P19-1511,0,0.125014,"troduced by Collobert et al. (2011), who used a CNN to encode the tokens and applied a CRF layer on top. After that, many other network architectures for NER MD have also been proposed, such as LSTM - CRF (Lample et al., 2016; Chiu and Nichols, 2016), LSTM-CRF + ELMO (Peters et al., 2018) and BERT (Devlin et al., 2019). Recently, a number of NER systems based on neural network architectures have been introduced to solve nested NER . Ju et al. (2018) introduce a stacked LSTM - CRF approach to solve nested NER in multi-steps. Sohrab and Miwa (2018) use an exhaustive region classification model. Lin et al. (2019) solve the problem in two steps: they first detect the entity head, and then infer the entity boundaries and classes in the second step. Strakov´a et al. (2019) infer the nested NER by a sequence-to-sequence model. Zheng et al. (2019) introduce a boundary aware network to train the boundary detection and the entity classification models in a multi-task learning setting. However, none of those systems can be directly used for coreference, due to the large difference between the settings used in NER and in coreference (e.g. for coreference the mention need to be predicted in a HIGH RECALL fashio"
2020.lrec-1.1,D15-1102,0,0.0248292,"orpus is more appropriate for studying mention detection as all mentions are annotated. As done in the CRAC shared task, we used the RST portion of the corpora, consisting of news texts (1/3 of the PENN Treebank). Since none of the state-of-the-art coreference systems predict singleton mentions, a version of the CRAC dataset with singleton mentions excluded was created for the coreference task evaluation. The GENIA corpora is one of the main resources for studying nested NER. We use the GENIA v3.0.2 corpus and preprocess the dataset following the same settings of Finkel and Manning (2009) and Lu and Roth (2015). Historically, the dataset has been split into two different ways: the first approach splits the data into two sets (train and test) by 90:10 (GENIA 90), whereas the second approach further creates a development set by splitting the data into 81:9:10 (GENIA 81). We evaluate our model on both approaches to make the fair comparisons with previous work. For evaluation on GENIA 90, since we do not have a development set, we train our model for 40K steps (20 epochs) and take evaluate on the final model. 4.2. Value L EE, B IA L EE, B IA L EE, B IA B ER B ER B ER L EE, B IA, B ER L EE, B IA, B ER L"
2020.lrec-1.1,D14-1162,0,0.0829822,"raw candidate scores (rm ). The raw scores are then used to create the probabilities (pm ) by applying a sigmoid function to the rm : L EE MD Our first system is based on the mention detection part of the Lee et al. (2018) system. The system represents a candidate span with the outputs of a bi-directional LSTM. The sentences of a document are encoded bidirectional via the LSTM s to obtain forward/backward representations for each token in the sentence. The bi-directional LSTM takes as input the concatenated embeddings ((xt )Tt=1 ) of both word and character levels. For word embeddings, GloVe (Pennington et al., 2014) and ELMO (Peters et al., 2018) embeddings are used. Character embeddings are learned from convolution neural networks (CNN) during training. The tokens are represented by concatenated outputs from the forward and the backward LSTMs. The token representations (x∗t )Tt=1 are used together with head representations (h∗i ) to represent candidate spans (Ni∗ ). The h∗i of a span is obtained by applying an attention over its token representations ({x∗si , ..., x∗ei }), where si and ei are the indices of the start and the end of the span respectively. Formally, we compute h∗i , Ni∗ as follows: rm (i)"
2020.lrec-1.1,N18-1202,0,0.402252,"Another limitation of typical mention detection systems is that they only predict mentions in a HIGH F 1 fashion, whereas in coreference, for instance, mentions are usually predicted in a HIGH RE CALL setting, since further pruning will be carried out at the coreference system (Clark and Manning, 2016b; Clark and Manning, 2016a; Lee et al., 2017; Lee et al., 2018; Kantor and Globerson, 2019). There are only very few recent studies that attempt to apply neural network approaches to develop a standalone mention detector. Neural network approaches using contextsensitive embeddings such as ELMO (Peters et al., 2018) and BERT (Devlin et al., 2019) have resulted in substantial improvements for mention detectors in the NER benchmark CONLL 2003 data set. However, most coreference systems that appeared after Lee et al., (2017; 2018) carry out mention detection as a part of their end-to-end coreference system. Such systems do not output intermediate mentions, hence the mention detector cannot be directly used to ex1 1 This performance difference is measured on mention recall, by training the mention detector alone. Second, our best system achieves improvements of 5.3 and 6.2 percentage points when compared wit"
2020.lrec-1.1,W18-0702,1,0.93671,"ct, or by other coreference systems. Thus the only standalone mention detectors that can be used as preprocessing for a coreference annotation are ones that do not take advantage of these advances and still heavily rely on parsing to identify all NPs as candidate mentions (Bj¨orkelund and Kuhn, 2014; Wiseman et al., 2015; Wiseman et al., 2016) or ones that use the rulebased mention detector from the Stanford deterministic system (Lee et al., 2013) to extract mentions from NPs, named entity mentions and pronouns (Clark and Manning, 2015; Clark and Manning, 2016b). To the best of our knowledge, Poesio et al. (2018) introduced the only standalone neural mention detector. By using a modified version of the NER system of Lample et al. (2016), they showed substantial performance gains at mention detection on the benchmark CONLL 2012 data set and on the CRAC 2018 data set when compared with the Stanford deterministic system. In this paper, we compare three neural architectures for standalone MD. The first system is a slightly modified version of the mention detection part of the Lee et al. (2018) system. The second system employs a bi-directional LSTM on the sentence level and uses biaffine attention (Dozat"
2020.lrec-1.1,N19-1176,1,0.858045,"he contributions of this paper are therefore as follows. First, we show that mention detection performance improved by up to 1.5 percentage points1 can be achieved Mention detection (MD) is the task of identifying mentions of entities in text. It is an important preprocessing step for downstream applications such as nested named entity recognition (Zheng et al., 2019) or coreference resolution (Poesio et al., 2016); thus, the quality of mention detection affects both the performance of models for such applications and the quality of annotated data used to train them (Chamberlain et al., 2016; Poesio et al., 2019). Much mention detection research for NER has concentrated on a simplified version of MD that focuses on proper names only (i.e., it doesn’t consider as mentions nominals such as the protein or pronouns such as it), and ignores the fact that mentions may nest (e.g., noun phrases such as [[CCITA] mRNA] in the GENIA corpus are mentions of two separate entities, CCITA and CCITA and mRNA (Alex et al., 2007)). However such simplified view of mentions is not sufficient for NER in domains such as biomedical, or for coreference, that requires full mention detection. Another limitation of typical menti"
2020.lrec-1.1,W12-4501,0,0.135911,"effects of our model on coreference: i.e., we integrate the mentions extracted from our best system into state-of-the-art coreference systems (both end-to-end and the pipeline system). The third series of experiments focuses on the nested NER task. We evaluate our systems both on boundary detection and on the full NER tasks. The rest of this section introduces our experimental settings in detail. 4.1. Data Set We evaluate our models on two different corpora for both the mention detection and the coreference tasks and one additional corpora for nested NER task, the CONLL 2012 English corpora (Pradhan et al., 2012), the CRAC 2018 corpora (Poesio et al., 2018) and the GENIA (Kim et al., 2003) corpora. The CONLL data set is the standard reference corpora for coreference resolution. The English subset consists of 2802, 342, and 348 documents for the train, development and test sets respectively. The CONLL data set is not however ideal for mention detection, since not all mentions are annotated, but only mentions involved in coreference Ni∗ = [x∗si , x∗ei ] rm (i) = FFNNm (Ni∗ ) 1 pm (i) = 1 + e−rm (i) 4 chains of length &gt; 1. This has a negative impact on learning since singleton mentions will always receiv"
2020.lrec-1.1,D18-1309,0,0.227154,"domain or coreference. The first neural network based NER model was introduced by Collobert et al. (2011), who used a CNN to encode the tokens and applied a CRF layer on top. After that, many other network architectures for NER MD have also been proposed, such as LSTM - CRF (Lample et al., 2016; Chiu and Nichols, 2016), LSTM-CRF + ELMO (Peters et al., 2018) and BERT (Devlin et al., 2019). Recently, a number of NER systems based on neural network architectures have been introduced to solve nested NER . Ju et al. (2018) introduce a stacked LSTM - CRF approach to solve nested NER in multi-steps. Sohrab and Miwa (2018) use an exhaustive region classification model. Lin et al. (2019) solve the problem in two steps: they first detect the entity head, and then infer the entity boundaries and classes in the second step. Strakov´a et al. (2019) infer the nested NER by a sequence-to-sequence model. Zheng et al. (2019) introduce a boundary aware network to train the boundary detection and the entity classification models in a multi-task learning setting. However, none of those systems can be directly used for coreference, due to the large difference between the settings used in NER and in coreference (e.g. for cor"
2020.lrec-1.1,P19-1527,0,0.0322637,"Missing"
2020.lrec-1.1,I13-1012,0,0.0613488,"Missing"
2020.lrec-1.1,P15-1137,0,0.427361,"Missing"
2020.lrec-1.1,N16-1114,0,0.262682,"at our model matches or outperforms state-of-the-art models despite not being specifically designed for this task. Keywords: Mention Detection, Coreference Resolution, Nested Named Entity Recognition, Deep Neural Networks 1. Introduction tract mentions for an annotation project, or by other coreference systems. Thus the only standalone mention detectors that can be used as preprocessing for a coreference annotation are ones that do not take advantage of these advances and still heavily rely on parsing to identify all NPs as candidate mentions (Bj¨orkelund and Kuhn, 2014; Wiseman et al., 2015; Wiseman et al., 2016) or ones that use the rulebased mention detector from the Stanford deterministic system (Lee et al., 2013) to extract mentions from NPs, named entity mentions and pronouns (Clark and Manning, 2015; Clark and Manning, 2016b). To the best of our knowledge, Poesio et al. (2018) introduced the only standalone neural mention detector. By using a modified version of the NER system of Lample et al. (2016), they showed substantial performance gains at mention detection on the benchmark CONLL 2012 data set and on the CRAC 2018 data set when compared with the Stanford deterministic system. In this paper"
2020.lrec-1.1,P18-2017,0,0.0380079,"Missing"
2020.lrec-1.1,D19-1034,0,0.0334868,"Missing"
2020.lrec-1.11,N07-1030,0,0.0778927,"Bennett, 1995; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2006; Iida et al., 2007; Sasano et al., 2008; Sasano et al., 2009; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Iida et al., 2015; Yoshino et al., 2013; Yamashiro et al., 2018), but also in other languages, including Korean (Han, 2004; Byron et al., 2006), Spanish (Ferrández and Peral, 2000), Romanian (Mihăilă et al., 2011), Bulgarian (Grigorova, 2013), and Sanskrit (Gopal and Jha, 2017). Iida and Poesio (2011) proposed the first cross-lingual approach for this task. They used the ILP model of Denis and Baldridge (2007) and introduced a new set of constraints incorporating common features for Italian and Japanese. All current approaches suffer from a number of limitations, one of which is that most of them rely on an extensive set of features which, as we will see below, are languagedependent. The systems using more complex linguistic features also require larger training datasets than available for many languages, including, e.g., Arabic. 2.2. seems related to what we need for resolving ZPs. Therefore, we use BERT to produce a mention representation for AZPs and the candidates, and we also incorporate a few"
2020.lrec-1.11,C90-2047,0,0.776453,"Missing"
2020.lrec-1.11,J14-4004,0,0.0242394,"th additional task-related features to improve ZP resolution. In our model, we use BERT feature extraction mode to produce embeddings for AZPs and their antecedents, and add two features: same_sentence and find_distance. same_sentence feature finds whether an AZP and a candidate appear in the same sentence or not, and find_distance Arabic There have been several studies of Arabic coreference resolution task, but none specifically devoted to ZPs except as part of the overall coreference task. In particular, several of the systems involved in the CONLL 2012 shared task attempted Arabic as well. Fernandes et al. (2014) utilized latent tree to capture hidden structure and finding coreference chains. Björkelund and Kuhn (2014) stacked multiple pairwise coreference resolvers and combined decoders to cluster mentions together. Chen and Ng (2012) employed multiple sieves (Lee et al., 2011) for English and Chinese, but used only an exact match sieve for Arabic. Green et al. (2009) proposed CRF sequence classifier to detect Arabic noun phrases, and captured ZPs implicitly. Gabbard (2010) showed that Arabic ZPs can be identified and retrieved. As far as we know none of these proposals reported the results of ZP res"
2020.lrec-1.11,P00-1022,0,0.06682,"the art, also used an attention-based model, but combined their network with (Chen and Ng, 2016) features. Other languages: There has been also a great deal of research on ZPs particularly in Japanese (Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2006; Iida et al., 2007; Sasano et al., 2008; Sasano et al., 2009; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Iida et al., 2015; Yoshino et al., 2013; Yamashiro et al., 2018), but also in other languages, including Korean (Han, 2004; Byron et al., 2006), Spanish (Ferrández and Peral, 2000), Romanian (Mihăilă et al., 2011), Bulgarian (Grigorova, 2013), and Sanskrit (Gopal and Jha, 2017). Iida and Poesio (2011) proposed the first cross-lingual approach for this task. They used the ILP model of Denis and Baldridge (2007) and introduced a new set of constraints incorporating common features for Italian and Japanese. All current approaches suffer from a number of limitations, one of which is that most of them rely on an extensive set of features which, as we will see below, are languagedependent. The systems using more complex linguistic features also require larger training dataset"
2020.lrec-1.11,2009.mtsummit-caasl.4,0,0.0824553,"udies of Arabic coreference resolution task, but none specifically devoted to ZPs except as part of the overall coreference task. In particular, several of the systems involved in the CONLL 2012 shared task attempted Arabic as well. Fernandes et al. (2014) utilized latent tree to capture hidden structure and finding coreference chains. Björkelund and Kuhn (2014) stacked multiple pairwise coreference resolvers and combined decoders to cluster mentions together. Chen and Ng (2012) employed multiple sieves (Lee et al., 2011) for English and Chinese, but used only an exact match sieve for Arabic. Green et al. (2009) proposed CRF sequence classifier to detect Arabic noun phrases, and captured ZPs implicitly. Gabbard (2010) showed that Arabic ZPs can be identified and retrieved. As far as we know none of these proposals reported the results of ZP resolution. 3. BERT Our Model ZPs resolution involves complex, comprehensive language understanding skills. Resolving ZPs in Chinese requires reasoning, context, and background knowledge of real world entities (Huang, 1984), whereas Arabic, in addition to the previously mentioned skills, requires deep understanding of its rich morphology (Alnajadat, 2017). Recentl"
2020.lrec-1.11,J95-2003,0,0.689611,"oesio et al., 2010; Yoshino et al., 2013). However, it has proven challenging to combine the task with the resolution of overt mentions, so separating the task from coreference resolution may lead to more improvements (Iida and Poesio, 2011). Chinese: The release of OntoNotes has spurred a lot of research on zero pronoun resolution in Chinese, but earlier research exists as well. Converse (2006) proposed a rulebased approach that employed Hobbs algorithm (Hobbs, 1978) to resolve ZPs in the Chinese Treebank. Yeh and Chen (2006) is another rule-based approach, using rules from Centering Theory (Grosz et al., 1995). Zhao and Ng (2007), the first machine learning approach to Chinese ZPs, used decision trees and a set of syntactic and positional features. Chen and Ng (2013) extended (Zhao and Ng, 2007) by incorporating contextual features and ZP links. Chen and Ng (2014; Chen and Ng (2015) proposed unsupervised techniques to resolve the task. Kong and Zhou (2010) proposed a tree kernel-based unified framework for ZP detection and resolution. Recent approaches applying deep-learning neural networks include Chen and Ng (2016), the first to apply a forward neural network to the task; Yin et al. (2016), who e"
2020.lrec-1.11,W04-0205,0,0.110619,". Yin et al. (2018), the current state of the art, also used an attention-based model, but combined their network with (Chen and Ng, 2016) features. Other languages: There has been also a great deal of research on ZPs particularly in Japanese (Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2006; Iida et al., 2007; Sasano et al., 2008; Sasano et al., 2009; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Iida et al., 2015; Yoshino et al., 2013; Yamashiro et al., 2018), but also in other languages, including Korean (Han, 2004; Byron et al., 2006), Spanish (Ferrández and Peral, 2000), Romanian (Mihăilă et al., 2011), Bulgarian (Grigorova, 2013), and Sanskrit (Gopal and Jha, 2017). Iida and Poesio (2011) proposed the first cross-lingual approach for this task. They used the ILP model of Denis and Baldridge (2007) and introduced a new set of constraints incorporating common features for Italian and Japanese. All current approaches suffer from a number of limitations, one of which is that most of them rely on an extensive set of features which, as we will see below, are languagedependent. The systems using more comple"
2020.lrec-1.11,D13-1095,0,0.248666,"Missing"
2020.lrec-1.11,P14-1005,0,0.18391,"n mode to produce embeddings for AZPs and their antecedents, and add two features: same_sentence and find_distance. same_sentence feature finds whether an AZP and a candidate appear in the same sentence or not, and find_distance Arabic There have been several studies of Arabic coreference resolution task, but none specifically devoted to ZPs except as part of the overall coreference task. In particular, several of the systems involved in the CONLL 2012 shared task attempted Arabic as well. Fernandes et al. (2014) utilized latent tree to capture hidden structure and finding coreference chains. Björkelund and Kuhn (2014) stacked multiple pairwise coreference resolvers and combined decoders to cluster mentions together. Chen and Ng (2012) employed multiple sieves (Lee et al., 2011) for English and Chinese, but used only an exact match sieve for Arabic. Green et al. (2009) proposed CRF sequence classifier to detect Arabic noun phrases, and captured ZPs implicitly. Gabbard (2010) showed that Arabic ZPs can be identified and retrieved. As far as we know none of these proposals reported the results of ZP resolution. 3. BERT Our Model ZPs resolution involves complex, comprehensive language understanding skills. Res"
2020.lrec-1.11,W12-4504,0,0.577292,"ce feature finds whether an AZP and a candidate appear in the same sentence or not, and find_distance Arabic There have been several studies of Arabic coreference resolution task, but none specifically devoted to ZPs except as part of the overall coreference task. In particular, several of the systems involved in the CONLL 2012 shared task attempted Arabic as well. Fernandes et al. (2014) utilized latent tree to capture hidden structure and finding coreference chains. Björkelund and Kuhn (2014) stacked multiple pairwise coreference resolvers and combined decoders to cluster mentions together. Chen and Ng (2012) employed multiple sieves (Lee et al., 2011) for English and Chinese, but used only an exact match sieve for Arabic. Green et al. (2009) proposed CRF sequence classifier to detect Arabic noun phrases, and captured ZPs implicitly. Gabbard (2010) showed that Arabic ZPs can be identified and retrieved. As far as we know none of these proposals reported the results of ZP resolution. 3. BERT Our Model ZPs resolution involves complex, comprehensive language understanding skills. Resolving ZPs in Chinese requires reasoning, context, and background knowledge of real world entities (Huang, 1984), where"
2020.lrec-1.11,D13-1135,0,0.0978157,"rom coreference resolution may lead to more improvements (Iida and Poesio, 2011). Chinese: The release of OntoNotes has spurred a lot of research on zero pronoun resolution in Chinese, but earlier research exists as well. Converse (2006) proposed a rulebased approach that employed Hobbs algorithm (Hobbs, 1978) to resolve ZPs in the Chinese Treebank. Yeh and Chen (2006) is another rule-based approach, using rules from Centering Theory (Grosz et al., 1995). Zhao and Ng (2007), the first machine learning approach to Chinese ZPs, used decision trees and a set of syntactic and positional features. Chen and Ng (2013) extended (Zhao and Ng, 2007) by incorporating contextual features and ZP links. Chen and Ng (2014; Chen and Ng (2015) proposed unsupervised techniques to resolve the task. Kong and Zhou (2010) proposed a tree kernel-based unified framework for ZP detection and resolution. Recent approaches applying deep-learning neural networks include Chen and Ng (2016), the first to apply a forward neural network to the task; Yin et al. (2016), who employed an LSTM to represent AZP and two subnetworks (general encoder and local encoder) to capture Alhariri’s statement included more details ...in which (he)"
2020.lrec-1.11,D14-1084,0,0.0897788,"ase of OntoNotes has spurred a lot of research on zero pronoun resolution in Chinese, but earlier research exists as well. Converse (2006) proposed a rulebased approach that employed Hobbs algorithm (Hobbs, 1978) to resolve ZPs in the Chinese Treebank. Yeh and Chen (2006) is another rule-based approach, using rules from Centering Theory (Grosz et al., 1995). Zhao and Ng (2007), the first machine learning approach to Chinese ZPs, used decision trees and a set of syntactic and positional features. Chen and Ng (2013) extended (Zhao and Ng, 2007) by incorporating contextual features and ZP links. Chen and Ng (2014; Chen and Ng (2015) proposed unsupervised techniques to resolve the task. Kong and Zhou (2010) proposed a tree kernel-based unified framework for ZP detection and resolution. Recent approaches applying deep-learning neural networks include Chen and Ng (2016), the first to apply a forward neural network to the task; Yin et al. (2016), who employed an LSTM to represent AZP and two subnetworks (general encoder and local encoder) to capture Alhariri’s statement included more details ...in which (he) emphasized that the council of ministers of Lebanon is the only representative ... In the example,"
2020.lrec-1.11,P15-2053,0,0.0179943,"s spurred a lot of research on zero pronoun resolution in Chinese, but earlier research exists as well. Converse (2006) proposed a rulebased approach that employed Hobbs algorithm (Hobbs, 1978) to resolve ZPs in the Chinese Treebank. Yeh and Chen (2006) is another rule-based approach, using rules from Centering Theory (Grosz et al., 1995). Zhao and Ng (2007), the first machine learning approach to Chinese ZPs, used decision trees and a set of syntactic and positional features. Chen and Ng (2013) extended (Zhao and Ng, 2007) by incorporating contextual features and ZP links. Chen and Ng (2014; Chen and Ng (2015) proposed unsupervised techniques to resolve the task. Kong and Zhou (2010) proposed a tree kernel-based unified framework for ZP detection and resolution. Recent approaches applying deep-learning neural networks include Chen and Ng (2016), the first to apply a forward neural network to the task; Yin et al. (2016), who employed an LSTM to represent AZP and two subnetworks (general encoder and local encoder) to capture Alhariri’s statement included more details ...in which (he) emphasized that the council of ministers of Lebanon is the only representative ... In the example, the zero pronoun in"
2020.lrec-1.11,P16-1074,0,0.434422,"nd Chen (2006) is another rule-based approach, using rules from Centering Theory (Grosz et al., 1995). Zhao and Ng (2007), the first machine learning approach to Chinese ZPs, used decision trees and a set of syntactic and positional features. Chen and Ng (2013) extended (Zhao and Ng, 2007) by incorporating contextual features and ZP links. Chen and Ng (2014; Chen and Ng (2015) proposed unsupervised techniques to resolve the task. Kong and Zhou (2010) proposed a tree kernel-based unified framework for ZP detection and resolution. Recent approaches applying deep-learning neural networks include Chen and Ng (2016), the first to apply a forward neural network to the task; Yin et al. (2016), who employed an LSTM to represent AZP and two subnetworks (general encoder and local encoder) to capture Alhariri’s statement included more details ...in which (he) emphasized that the council of ministers of Lebanon is the only representative ... In the example, the zero pronoun indicated with ’*’ refers to an entity introduced with a masculine singular noun that was previously mentioned in the sentence. (In OntoNotes 5.0, zero pronouns are denoted as * in Arabic text, and *pro* in Chinese). AZP resolution usually c"
2020.lrec-1.11,P06-1079,0,0.0608733,"emory network capable to improve the semantic information of ZPs and its candidates; and Liu et al. (2017), using an attention-based neural network and enhanced the performance by training the model on automatically generated large-scale training data of resolved ZP. Yin et al. (2018), the current state of the art, also used an attention-based model, but combined their network with (Chen and Ng, 2016) features. Other languages: There has been also a great deal of research on ZPs particularly in Japanese (Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2006; Iida et al., 2007; Sasano et al., 2008; Sasano et al., 2009; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Iida et al., 2015; Yoshino et al., 2013; Yamashiro et al., 2018), but also in other languages, including Korean (Han, 2004; Byron et al., 2006), Spanish (Ferrández and Peral, 2000), Romanian (Mihăilă et al., 2011), Bulgarian (Grigorova, 2013), and Sanskrit (Gopal and Jha, 2017). Iida and Poesio (2011) proposed the first cross-lingual approach for this task. They used the ILP model of Denis and Baldridge (2007) and introduced a new set of constraints incorporat"
2020.lrec-1.11,D15-1260,0,0.0195048,"nhanced the performance by training the model on automatically generated large-scale training data of resolved ZP. Yin et al. (2018), the current state of the art, also used an attention-based model, but combined their network with (Chen and Ng, 2016) features. Other languages: There has been also a great deal of research on ZPs particularly in Japanese (Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2006; Iida et al., 2007; Sasano et al., 2008; Sasano et al., 2009; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Iida et al., 2015; Yoshino et al., 2013; Yamashiro et al., 2018), but also in other languages, including Korean (Han, 2004; Byron et al., 2006), Spanish (Ferrández and Peral, 2000), Romanian (Mihăilă et al., 2011), Bulgarian (Grigorova, 2013), and Sanskrit (Gopal and Jha, 2017). Iida and Poesio (2011) proposed the first cross-lingual approach for this task. They used the ILP model of Denis and Baldridge (2007) and introduced a new set of constraints incorporating common features for Italian and Japanese. All current approaches suffer from a number of limitations, one of which is that most of them rely on an ex"
2020.lrec-1.11,P09-2022,0,0.0728679,"es: • We carried out an extensive analysis on BERT layers, and discuss which settings can give the optimal performance. The rest of the paper is organized as follows. We discuss Arabic and Chinese ZP-related literature and in other languages in Section 2. We explain our proposed model in Section 3. We discuss the evaluation settings and results in Section 4. We conclude in Section 5. 2. 2.1.  ﺣﯿﺚ رﻛﺰ * ﻋﻠﻰ أن ﻣﺠﻠﺲ. . . ﺑﯿﺎن اﻟﺤﺮﯾﺮي ﺗﻤﯿﺰ ﺑﺘﻔﺎﺻﯿﻞ . . . وزراء ﻟﺒﻨﺎن وﺣﺪه اﻟﻤﺴﺆول ﻋﻦ Zero Pronoun Resolution AZP resolution is included in some coreference resolution systems (Taira et al., 2008; Imamura et al., 2009; Watanabe et al., 2010; Poesio et al., 2010; Yoshino et al., 2013). However, it has proven challenging to combine the task with the resolution of overt mentions, so separating the task from coreference resolution may lead to more improvements (Iida and Poesio, 2011). Chinese: The release of OntoNotes has spurred a lot of research on zero pronoun resolution in Chinese, but earlier research exists as well. Converse (2006) proposed a rulebased approach that employed Hobbs algorithm (Hobbs, 1978) to resolve ZPs in the Chinese Treebank. Yeh and Chen (2006) is another rule-based approach, using rul"
2020.lrec-1.11,W03-1024,0,0.14798,"7), who proposed a deep memory network capable to improve the semantic information of ZPs and its candidates; and Liu et al. (2017), using an attention-based neural network and enhanced the performance by training the model on automatically generated large-scale training data of resolved ZP. Yin et al. (2018), the current state of the art, also used an attention-based model, but combined their network with (Chen and Ng, 2016) features. Other languages: There has been also a great deal of research on ZPs particularly in Japanese (Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2006; Iida et al., 2007; Sasano et al., 2008; Sasano et al., 2009; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Iida et al., 2015; Yoshino et al., 2013; Yamashiro et al., 2018), but also in other languages, including Korean (Han, 2004; Byron et al., 2006), Spanish (Ferrández and Peral, 2000), Romanian (Mihăilă et al., 2011), Bulgarian (Grigorova, 2013), and Sanskrit (Gopal and Jha, 2017). Iida and Poesio (2011) proposed the first cross-lingual approach for this task. They used the ILP model of Denis and Baldridge (2007) and introduced a new set of con"
2020.lrec-1.11,P19-1356,0,0.0977235,"trieved. As far as we know none of these proposals reported the results of ZP resolution. 3. BERT Our Model ZPs resolution involves complex, comprehensive language understanding skills. Resolving ZPs in Chinese requires reasoning, context, and background knowledge of real world entities (Huang, 1984), whereas Arabic, in addition to the previously mentioned skills, requires deep understanding of its rich morphology (Alnajadat, 2017). Recently, it has been shown that BERT (Devlin et al., 2018) can capture structural properties of a language, such as its surface, semantic, and syntactic aspects (Jawahar et al., 2019) which 2 https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip 91 ##heart). In 6, 7, and 8 equations, the subscript of embeddings represents the word location in the sentence. µ is a function to compute the mean of a mention representation which can made of several subtoken embeddings 4 . computes the word distance between an AZP and a candidate. These two features are cross-lingual and highly related to the task because AZPs and their antecedents usually appear near each other (Chen and Ng, 2014). Consider a sentence consisting of n words and containing an AZP"
2020.lrec-1.11,D10-1086,0,0.11868,"rlier research exists as well. Converse (2006) proposed a rulebased approach that employed Hobbs algorithm (Hobbs, 1978) to resolve ZPs in the Chinese Treebank. Yeh and Chen (2006) is another rule-based approach, using rules from Centering Theory (Grosz et al., 1995). Zhao and Ng (2007), the first machine learning approach to Chinese ZPs, used decision trees and a set of syntactic and positional features. Chen and Ng (2013) extended (Zhao and Ng, 2007) by incorporating contextual features and ZP links. Chen and Ng (2014; Chen and Ng (2015) proposed unsupervised techniques to resolve the task. Kong and Zhou (2010) proposed a tree kernel-based unified framework for ZP detection and resolution. Recent approaches applying deep-learning neural networks include Chen and Ng (2016), the first to apply a forward neural network to the task; Yin et al. (2016), who employed an LSTM to represent AZP and two subnetworks (general encoder and local encoder) to capture Alhariri’s statement included more details ...in which (he) emphasized that the council of ministers of Lebanon is the only representative ... In the example, the zero pronoun indicated with ’*’ refers to an entity introduced with a masculine singular n"
2020.lrec-1.11,D19-1445,0,0.0258442,"might have hurt the performance rather than helped. Also, (Chen and Ng, 2016)’s model lacked morphological features because Chinese morphology is considered relatively simple. In contrast, Arabic morphology is highly derivational and inflectional, and very important for resolving ZPs. Arabic ZPs are preceded by verbs, and verbs encode information about gender, person, and number. The context of ZPs and their antecedents share similar morphological characteristics. 4.3.3. BERT Layers Numerous studies show that BERT layers encode rich information about language structure (Jawahar et al., 2019; Kovaleva et al., 2019; Aken et al., 2019; Goldberg, 2019; Hewitt and Manning, 2019). For a specific NLP task, some layers may carry more useful information than others. In fact, layers that contain indirect information may not lead 5 94 https://github.com/google-research/bert/blob/master/multilingual.md (Zhao and Ng, 2007) (Chen and Ng, 2015) (Chen and Ng, 2016) (Yin et al., 2016) (Yin et al., 2017) (Liu et al., 2017) (Yin et al., 2018) BERT (feature extraction) BERT (fine-tuning) Our model NW (84) 40.5 46.4 48.8 50.0 48.8 59.2 64.3 59.3 61.8 63.4 MZ (162) 28.4 39.0 41.5 45.0 46.3 51.3 52.5 48.7 51.8 54.4 WB (284)"
2020.lrec-1.11,W11-1902,0,0.0892397,"te appear in the same sentence or not, and find_distance Arabic There have been several studies of Arabic coreference resolution task, but none specifically devoted to ZPs except as part of the overall coreference task. In particular, several of the systems involved in the CONLL 2012 shared task attempted Arabic as well. Fernandes et al. (2014) utilized latent tree to capture hidden structure and finding coreference chains. Björkelund and Kuhn (2014) stacked multiple pairwise coreference resolvers and combined decoders to cluster mentions together. Chen and Ng (2012) employed multiple sieves (Lee et al., 2011) for English and Chinese, but used only an exact match sieve for Arabic. Green et al. (2009) proposed CRF sequence classifier to detect Arabic noun phrases, and captured ZPs implicitly. Gabbard (2010) showed that Arabic ZPs can be identified and retrieved. As far as we know none of these proposals reported the results of ZP resolution. 3. BERT Our Model ZPs resolution involves complex, comprehensive language understanding skills. Resolving ZPs in Chinese requires reasoning, context, and background knowledge of real world entities (Huang, 1984), whereas Arabic, in addition to the previously men"
2020.lrec-1.11,P17-1010,0,0.410931,", BERT-based model and test it on languages that differ completely in their morphological structure: Arabic and Chinese. (Arabic is morphologically rich, whereas Chinese’s morphology is relatively simple (Pradhan et al., 2012)) • As far as we know this is the first neural network-based ZP resolution model for Arabic, and outperforms the 1 Related work The terms null-subject or zero-subject are also used. 90 context-level and word-level information of the candidates; Yin et al. (2017), who proposed a deep memory network capable to improve the semantic information of ZPs and its candidates; and Liu et al. (2017), using an attention-based neural network and enhanced the performance by training the model on automatically generated large-scale training data of resolved ZP. Yin et al. (2018), the current state of the art, also used an attention-based model, but combined their network with (Chen and Ng, 2016) features. Other languages: There has been also a great deal of research on ZPs particularly in Japanese (Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2006; Iida et al., 2007; Sasano et al., 2008; Sasano et al., 2009; Sasano and Kurohashi, 2011"
2020.lrec-1.11,W19-4302,0,0.0252482,"st portion does not contain zero pronouns; therefore, the development portion is used for evaluation as done in prior works. P recision = Recall 8.1 47.9 50.3 51.8 Results We compare our results with other published results, and with the results using BERT’s two adaptation modes. BERT fine-tuning already has a built-in classification layer on top of the stacked Transformers. The feature extraction mode only produces the learned vectors and needs a framework to be trained on. To do so, we implement a bi-attentive neural network to train feature extraction embeddings and optimize it as done in (Peters et al., 2019) who empirically analyzed fine-tuning and feature extraction modes for a few pretrained models, including BERT. In both modes, we train AZPs and their antecedents without the proposed additional features. 4.3.1. Arabic We report our results for Arabic in Table 3. Given that there was no existing ZP resolver for Arabic, we implemented (Chen and Ng, 2016)’s model and used it as a baseline in our experiments, as it features an extensive range of syntactic, positional, and grammatical features which were then used in other systems as well (Yin et al., 2018). However, Table 3 shows that these featu"
2020.lrec-1.11,poesio-etal-2010-creating,1,0.738554,"BERT layers, and discuss which settings can give the optimal performance. The rest of the paper is organized as follows. We discuss Arabic and Chinese ZP-related literature and in other languages in Section 2. We explain our proposed model in Section 3. We discuss the evaluation settings and results in Section 4. We conclude in Section 5. 2. 2.1.  ﺣﯿﺚ رﻛﺰ * ﻋﻠﻰ أن ﻣﺠﻠﺲ. . . ﺑﯿﺎن اﻟﺤﺮﯾﺮي ﺗﻤﯿﺰ ﺑﺘﻔﺎﺻﯿﻞ . . . وزراء ﻟﺒﻨﺎن وﺣﺪه اﻟﻤﺴﺆول ﻋﻦ Zero Pronoun Resolution AZP resolution is included in some coreference resolution systems (Taira et al., 2008; Imamura et al., 2009; Watanabe et al., 2010; Poesio et al., 2010; Yoshino et al., 2013). However, it has proven challenging to combine the task with the resolution of overt mentions, so separating the task from coreference resolution may lead to more improvements (Iida and Poesio, 2011). Chinese: The release of OntoNotes has spurred a lot of research on zero pronoun resolution in Chinese, but earlier research exists as well. Converse (2006) proposed a rulebased approach that employed Hobbs algorithm (Hobbs, 1978) to resolve ZPs in the Chinese Treebank. Yeh and Chen (2006) is another rule-based approach, using rules from Centering Theory (Grosz et al., 1995"
2020.lrec-1.11,W12-4501,0,0.119259,"s * in Arabic text, and *pro* in Chinese). AZP resolution usually consists of two steps: extracting ZPs that are anaphoric, and identifying the correct antecedents for AZPs. Our focus is on the latter because there has been no proposal for Arabic. In this paper we propose a crosslingual, BERT based model of zero pronoun resolution. Our contributions include: • We propose a novel cross-lingual, BERT-based model and test it on languages that differ completely in their morphological structure: Arabic and Chinese. (Arabic is morphologically rich, whereas Chinese’s morphology is relatively simple (Pradhan et al., 2012)) • As far as we know this is the first neural network-based ZP resolution model for Arabic, and outperforms the 1 Related work The terms null-subject or zero-subject are also used. 90 context-level and word-level information of the candidates; Yin et al. (2017), who proposed a deep memory network capable to improve the semantic information of ZPs and its candidates; and Liu et al. (2017), using an attention-based neural network and enhanced the performance by training the model on automatically generated large-scale training data of resolved ZP. Yin et al. (2018), the current state of the art"
2020.lrec-1.11,I11-1085,0,0.0266866,"tes; and Liu et al. (2017), using an attention-based neural network and enhanced the performance by training the model on automatically generated large-scale training data of resolved ZP. Yin et al. (2018), the current state of the art, also used an attention-based model, but combined their network with (Chen and Ng, 2016) features. Other languages: There has been also a great deal of research on ZPs particularly in Japanese (Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2006; Iida et al., 2007; Sasano et al., 2008; Sasano et al., 2009; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Iida et al., 2015; Yoshino et al., 2013; Yamashiro et al., 2018), but also in other languages, including Korean (Han, 2004; Byron et al., 2006), Spanish (Ferrández and Peral, 2000), Romanian (Mihăilă et al., 2011), Bulgarian (Grigorova, 2013), and Sanskrit (Gopal and Jha, 2017). Iida and Poesio (2011) proposed the first cross-lingual approach for this task. They used the ILP model of Denis and Baldridge (2007) and introduced a new set of constraints incorporating common features for Italian and Japanese. All current approaches suffer from a number"
2020.lrec-1.11,C08-1097,0,0.0379533,"emantic information of ZPs and its candidates; and Liu et al. (2017), using an attention-based neural network and enhanced the performance by training the model on automatically generated large-scale training data of resolved ZP. Yin et al. (2018), the current state of the art, also used an attention-based model, but combined their network with (Chen and Ng, 2016) features. Other languages: There has been also a great deal of research on ZPs particularly in Japanese (Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2006; Iida et al., 2007; Sasano et al., 2008; Sasano et al., 2009; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Iida et al., 2015; Yoshino et al., 2013; Yamashiro et al., 2018), but also in other languages, including Korean (Han, 2004; Byron et al., 2006), Spanish (Ferrández and Peral, 2000), Romanian (Mihăilă et al., 2011), Bulgarian (Grigorova, 2013), and Sanskrit (Gopal and Jha, 2017). Iida and Poesio (2011) proposed the first cross-lingual approach for this task. They used the ILP model of Denis and Baldridge (2007) and introduced a new set of constraints incorporating common features for Italian and Japa"
2020.lrec-1.11,N09-1059,0,0.0236555,"f ZPs and its candidates; and Liu et al. (2017), using an attention-based neural network and enhanced the performance by training the model on automatically generated large-scale training data of resolved ZP. Yin et al. (2018), the current state of the art, also used an attention-based model, but combined their network with (Chen and Ng, 2016) features. Other languages: There has been also a great deal of research on ZPs particularly in Japanese (Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2006; Iida et al., 2007; Sasano et al., 2008; Sasano et al., 2009; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Iida et al., 2015; Yoshino et al., 2013; Yamashiro et al., 2018), but also in other languages, including Korean (Han, 2004; Byron et al., 2006), Spanish (Ferrández and Peral, 2000), Romanian (Mihăilă et al., 2011), Bulgarian (Grigorova, 2013), and Sanskrit (Gopal and Jha, 2017). Iida and Poesio (2011) proposed the first cross-lingual approach for this task. They used the ILP model of Denis and Baldridge (2007) and introduced a new set of constraints incorporating common features for Italian and Japanese. All current app"
2020.lrec-1.11,C02-1078,0,0.199014,"es; Yin et al. (2017), who proposed a deep memory network capable to improve the semantic information of ZPs and its candidates; and Liu et al. (2017), using an attention-based neural network and enhanced the performance by training the model on automatically generated large-scale training data of resolved ZP. Yin et al. (2018), the current state of the art, also used an attention-based model, but combined their network with (Chen and Ng, 2016) features. Other languages: There has been also a great deal of research on ZPs particularly in Japanese (Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2006; Iida et al., 2007; Sasano et al., 2008; Sasano et al., 2009; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Iida et al., 2015; Yoshino et al., 2013; Yamashiro et al., 2018), but also in other languages, including Korean (Han, 2004; Byron et al., 2006), Spanish (Ferrández and Peral, 2000), Romanian (Mihăilă et al., 2011), Bulgarian (Grigorova, 2013), and Sanskrit (Gopal and Jha, 2017). Iida and Poesio (2011) proposed the first cross-lingual approach for this task. They used the ILP model of Denis and Baldridge (2007) and in"
2020.lrec-1.11,D08-1055,0,0.275013,"c section of OntoNotes: • We carried out an extensive analysis on BERT layers, and discuss which settings can give the optimal performance. The rest of the paper is organized as follows. We discuss Arabic and Chinese ZP-related literature and in other languages in Section 2. We explain our proposed model in Section 3. We discuss the evaluation settings and results in Section 4. We conclude in Section 5. 2. 2.1.  ﺣﯿﺚ رﻛﺰ * ﻋﻠﻰ أن ﻣﺠﻠﺲ. . . ﺑﯿﺎن اﻟﺤﺮﯾﺮي ﺗﻤﯿﺰ ﺑﺘﻔﺎﺻﯿﻞ . . . وزراء ﻟﺒﻨﺎن وﺣﺪه اﻟﻤﺴﺆول ﻋﻦ Zero Pronoun Resolution AZP resolution is included in some coreference resolution systems (Taira et al., 2008; Imamura et al., 2009; Watanabe et al., 2010; Poesio et al., 2010; Yoshino et al., 2013). However, it has proven challenging to combine the task with the resolution of overt mentions, so separating the task from coreference resolution may lead to more improvements (Iida and Poesio, 2011). Chinese: The release of OntoNotes has spurred a lot of research on zero pronoun resolution in Chinese, but earlier research exists as well. Converse (2006) proposed a rulebased approach that employed Hobbs algorithm (Hobbs, 1978) to resolve ZPs in the Chinese Treebank. Yeh and Chen (2006) is another rule-bas"
2020.lrec-1.11,P10-2018,0,0.0415267,"n extensive analysis on BERT layers, and discuss which settings can give the optimal performance. The rest of the paper is organized as follows. We discuss Arabic and Chinese ZP-related literature and in other languages in Section 2. We explain our proposed model in Section 3. We discuss the evaluation settings and results in Section 4. We conclude in Section 5. 2. 2.1.  ﺣﯿﺚ رﻛﺰ * ﻋﻠﻰ أن ﻣﺠﻠﺲ. . . ﺑﯿﺎن اﻟﺤﺮﯾﺮي ﺗﻤﯿﺰ ﺑﺘﻔﺎﺻﯿﻞ . . . وزراء ﻟﺒﻨﺎن وﺣﺪه اﻟﻤﺴﺆول ﻋﻦ Zero Pronoun Resolution AZP resolution is included in some coreference resolution systems (Taira et al., 2008; Imamura et al., 2009; Watanabe et al., 2010; Poesio et al., 2010; Yoshino et al., 2013). However, it has proven challenging to combine the task with the resolution of overt mentions, so separating the task from coreference resolution may lead to more improvements (Iida and Poesio, 2011). Chinese: The release of OntoNotes has spurred a lot of research on zero pronoun resolution in Chinese, but earlier research exists as well. Converse (2006) proposed a rulebased approach that employed Hobbs algorithm (Hobbs, 1978) to resolve ZPs in the Chinese Treebank. Yeh and Chen (2006) is another rule-based approach, using rules from Centering Theor"
2020.lrec-1.11,Y18-1089,0,0.155355,"odel on automatically generated large-scale training data of resolved ZP. Yin et al. (2018), the current state of the art, also used an attention-based model, but combined their network with (Chen and Ng, 2016) features. Other languages: There has been also a great deal of research on ZPs particularly in Japanese (Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2006; Iida et al., 2007; Sasano et al., 2008; Sasano et al., 2009; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Iida et al., 2015; Yoshino et al., 2013; Yamashiro et al., 2018), but also in other languages, including Korean (Han, 2004; Byron et al., 2006), Spanish (Ferrández and Peral, 2000), Romanian (Mihăilă et al., 2011), Bulgarian (Grigorova, 2013), and Sanskrit (Gopal and Jha, 2017). Iida and Poesio (2011) proposed the first cross-lingual approach for this task. They used the ILP model of Denis and Baldridge (2007) and introduced a new set of constraints incorporating common features for Italian and Japanese. All current approaches suffer from a number of limitations, one of which is that most of them rely on an extensive set of features which, as we will see b"
2020.lrec-1.11,D17-1135,0,0.05807,"ropose a crosslingual, BERT based model of zero pronoun resolution. Our contributions include: • We propose a novel cross-lingual, BERT-based model and test it on languages that differ completely in their morphological structure: Arabic and Chinese. (Arabic is morphologically rich, whereas Chinese’s morphology is relatively simple (Pradhan et al., 2012)) • As far as we know this is the first neural network-based ZP resolution model for Arabic, and outperforms the 1 Related work The terms null-subject or zero-subject are also used. 90 context-level and word-level information of the candidates; Yin et al. (2017), who proposed a deep memory network capable to improve the semantic information of ZPs and its candidates; and Liu et al. (2017), using an attention-based neural network and enhanced the performance by training the model on automatically generated large-scale training data of resolved ZP. Yin et al. (2018), the current state of the art, also used an attention-based model, but combined their network with (Chen and Ng, 2016) features. Other languages: There has been also a great deal of research on ZPs particularly in Japanese (Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et al., 2002; Is"
2020.lrec-1.11,C18-1002,0,0.860139,"ogy is relatively simple (Pradhan et al., 2012)) • As far as we know this is the first neural network-based ZP resolution model for Arabic, and outperforms the 1 Related work The terms null-subject or zero-subject are also used. 90 context-level and word-level information of the candidates; Yin et al. (2017), who proposed a deep memory network capable to improve the semantic information of ZPs and its candidates; and Liu et al. (2017), using an attention-based neural network and enhanced the performance by training the model on automatically generated large-scale training data of resolved ZP. Yin et al. (2018), the current state of the art, also used an attention-based model, but combined their network with (Chen and Ng, 2016) features. Other languages: There has been also a great deal of research on ZPs particularly in Japanese (Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2006; Iida et al., 2007; Sasano et al., 2008; Sasano et al., 2009; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Iida et al., 2015; Yoshino et al., 2013; Yamashiro et al., 2018), but also in other languages, including Korean (Han, 2004; Byron et"
2020.lrec-1.11,I11-1126,0,0.0648939,"Missing"
2020.lrec-1.11,I13-1126,0,0.0396442,"cuss which settings can give the optimal performance. The rest of the paper is organized as follows. We discuss Arabic and Chinese ZP-related literature and in other languages in Section 2. We explain our proposed model in Section 3. We discuss the evaluation settings and results in Section 4. We conclude in Section 5. 2. 2.1.  ﺣﯿﺚ رﻛﺰ * ﻋﻠﻰ أن ﻣﺠﻠﺲ. . . ﺑﯿﺎن اﻟﺤﺮﯾﺮي ﺗﻤﯿﺰ ﺑﺘﻔﺎﺻﯿﻞ . . . وزراء ﻟﺒﻨﺎن وﺣﺪه اﻟﻤﺴﺆول ﻋﻦ Zero Pronoun Resolution AZP resolution is included in some coreference resolution systems (Taira et al., 2008; Imamura et al., 2009; Watanabe et al., 2010; Poesio et al., 2010; Yoshino et al., 2013). However, it has proven challenging to combine the task with the resolution of overt mentions, so separating the task from coreference resolution may lead to more improvements (Iida and Poesio, 2011). Chinese: The release of OntoNotes has spurred a lot of research on zero pronoun resolution in Chinese, but earlier research exists as well. Converse (2006) proposed a rulebased approach that employed Hobbs algorithm (Hobbs, 1978) to resolve ZPs in the Chinese Treebank. Yeh and Chen (2006) is another rule-based approach, using rules from Centering Theory (Grosz et al., 1995). Zhao and Ng (2007),"
2020.lrec-1.11,D07-1057,0,0.669643,"oshino et al., 2013). However, it has proven challenging to combine the task with the resolution of overt mentions, so separating the task from coreference resolution may lead to more improvements (Iida and Poesio, 2011). Chinese: The release of OntoNotes has spurred a lot of research on zero pronoun resolution in Chinese, but earlier research exists as well. Converse (2006) proposed a rulebased approach that employed Hobbs algorithm (Hobbs, 1978) to resolve ZPs in the Chinese Treebank. Yeh and Chen (2006) is another rule-based approach, using rules from Centering Theory (Grosz et al., 1995). Zhao and Ng (2007), the first machine learning approach to Chinese ZPs, used decision trees and a set of syntactic and positional features. Chen and Ng (2013) extended (Zhao and Ng, 2007) by incorporating contextual features and ZP links. Chen and Ng (2014; Chen and Ng (2015) proposed unsupervised techniques to resolve the task. Kong and Zhou (2010) proposed a tree kernel-based unified framework for ZP detection and resolution. Recent approaches applying deep-learning neural networks include Chen and Ng (2016), the first to apply a forward neural network to the task; Yin et al. (2016), who employed an LSTM to r"
2020.lrec-1.2,W15-2501,0,0.0420144,"Missing"
2020.lrec-1.2,D19-1588,0,0.648998,"Missing"
2020.lrec-1.2,P19-1066,0,0.138761,"ns are directly linked to entities / coreference chains (Luo et al., 2004; Rahman and Ng, 2011). The mention pair models are simpler in concept and easier to implement, so many SoTA systems are exclusively based on mention ranking (Wiseman et al., 2015; Clark and Manning, 2016a; Lee et al., 2017). But it has long been known that entity-level information is important for coreference (Luo et al., 2004; Poesio et al., 2016b) so many systems attempted to explore features beyond those of mention pairs (Bj¨orkelund and Kuhn, 2014; Clark and Manning, 2015; Clark and Manning, 2016b; Lee et al., 2018; Kantor and Globerson, 2019; Joshi et al., 2019b; Joshi et al., 2019a). However, those systems are usually much more complex Anaphora resolution is the task of identifying and resolving nominal anaphoric reference to discourse entities (Poesio et al., 2016b).1 It is an important aspect of natural language processing and has a substantial impact on downstream applications such as summarization (Steinberger et al., 2007; Steinberger et al., 2016). Since the CONLL 2012 shared task (Pradhan et al., 2012), the ONTONOTES corpus has been the dominant resource in research on identity anaphora resolution (coreference) (Fernandes"
2020.lrec-1.2,J94-4002,0,0.0916215,"Missing"
2020.lrec-1.2,J13-4004,0,0.118747,"Missing"
2020.lrec-1.2,D17-1018,0,0.238662,"Missing"
2020.lrec-1.2,N18-2108,0,0.0548424,"s, in which mentions are directly linked to entities / coreference chains (Luo et al., 2004; Rahman and Ng, 2011). The mention pair models are simpler in concept and easier to implement, so many SoTA systems are exclusively based on mention ranking (Wiseman et al., 2015; Clark and Manning, 2016a; Lee et al., 2017). But it has long been known that entity-level information is important for coreference (Luo et al., 2004; Poesio et al., 2016b) so many systems attempted to explore features beyond those of mention pairs (Bj¨orkelund and Kuhn, 2014; Clark and Manning, 2015; Clark and Manning, 2016b; Lee et al., 2018; Kantor and Globerson, 2019; Joshi et al., 2019b; Joshi et al., 2019a). However, those systems are usually much more complex Anaphora resolution is the task of identifying and resolving nominal anaphoric reference to discourse entities (Poesio et al., 2016b).1 It is an important aspect of natural language processing and has a substantial impact on downstream applications such as summarization (Steinberger et al., 2007; Steinberger et al., 2016). Since the CONLL 2012 shared task (Pradhan et al., 2012), the ONTONOTES corpus has been the dominant resource in research on identity anaphora resolut"
2020.lrec-1.2,D17-1137,0,0.0519433,"Missing"
2020.lrec-1.2,P04-1018,0,0.262308,"Missing"
2020.lrec-1.2,Q15-1029,0,0.114413,"Missing"
2020.lrec-1.2,P08-1002,0,0.115664,"Missing"
2020.lrec-1.2,P14-1005,0,0.300864,"Missing"
2020.lrec-1.2,W05-0406,0,0.197902,"Missing"
2020.lrec-1.2,D18-1016,0,0.133651,"Missing"
2020.lrec-1.2,P15-1136,0,0.348837,"tablished between mentions, or entity mention models, in which mentions are directly linked to entities / coreference chains (Luo et al., 2004; Rahman and Ng, 2011). The mention pair models are simpler in concept and easier to implement, so many SoTA systems are exclusively based on mention ranking (Wiseman et al., 2015; Clark and Manning, 2016a; Lee et al., 2017). But it has long been known that entity-level information is important for coreference (Luo et al., 2004; Poesio et al., 2016b) so many systems attempted to explore features beyond those of mention pairs (Bj¨orkelund and Kuhn, 2014; Clark and Manning, 2015; Clark and Manning, 2016b; Lee et al., 2018; Kantor and Globerson, 2019; Joshi et al., 2019b; Joshi et al., 2019a). However, those systems are usually much more complex Anaphora resolution is the task of identifying and resolving nominal anaphoric reference to discourse entities (Poesio et al., 2016b).1 It is an important aspect of natural language processing and has a substantial impact on downstream applications such as summarization (Steinberger et al., 2007; Steinberger et al., 2016). Since the CONLL 2012 shared task (Pradhan et al., 2012), the ONTONOTES corpus has been the dominant resou"
2020.lrec-1.2,D16-1245,0,0.619105,".., x∗ei }), where si and ei are the indices of the start and the end of System architecture Anaphora resolution is the task of identifying the referring mentions in a text and assigning those mentions to disjoint clusters such that mentions in the same cluster refer to the same entity. The first subtask of anaphora resolution is mention detection, i.e., extracting candidate mentions from the document. Until recently, most coreference systems selected mentions prior to coreference resolution via heuristic methods often based on parse trees (Bj¨orkelund and Kuhn, 2014; Clark and Manning, 2015; Clark and Manning, 2016a; Clark and Manning, 2016b; Wiseman et al., 3 The oracle clusters are created from system mention using gold cluster information. 12 the span respectively. Formally, we compute h∗i , Ni∗ as follows: αt = ai,t h∗i with consideration of mention importance. More precisely, we compute the scores as follows: ∗ FFNN α ([xt , φ(t)]) s (i) = exp(αt ) = Pei k=si exp(αk ) = ei X β(i) = ai,t · xt where φ(t), φ(i) are the cluster position and span width feature embeddings respectively. To make the task computationally tractable, our model only considers the spans up to a maximum length of l, i.e. ei − s"
2020.lrec-1.2,P16-1061,0,0.64845,".., x∗ei }), where si and ei are the indices of the start and the end of System architecture Anaphora resolution is the task of identifying the referring mentions in a text and assigning those mentions to disjoint clusters such that mentions in the same cluster refer to the same entity. The first subtask of anaphora resolution is mention detection, i.e., extracting candidate mentions from the document. Until recently, most coreference systems selected mentions prior to coreference resolution via heuristic methods often based on parse trees (Bj¨orkelund and Kuhn, 2014; Clark and Manning, 2015; Clark and Manning, 2016a; Clark and Manning, 2016b; Wiseman et al., 3 The oracle clusters are created from system mention using gold cluster information. 12 the span respectively. Formally, we compute h∗i , Ni∗ as follows: αt = ai,t h∗i with consideration of mention importance. More precisely, we compute the scores as follows: ∗ FFNN α ([xt , φ(t)]) s (i) = exp(αt ) = Pei k=si exp(αk ) = ei X β(i) = ai,t · xt where φ(t), φ(i) are the cluster position and span width feature embeddings respectively. To make the task computationally tractable, our model only considers the spans up to a maximum length of l, i.e. ei − s"
2020.lrec-1.2,N19-1423,0,0.0414548,"sed for three situations: a span is not a mention, or is a non-referring expression, or is the first mention of a cluster. 2.1. Mention Representation We use a mention representation based on those in (Lee et al., 2018; Kantor and Globerson, 2019). Our system represents a candidate span with the outputs of a BiLSTM, encoding the sentences in a document from both directions to obtain a representation for each token in the sentence. The BiLSTM takes as input the concatenated embeddings ((xt )Tt=1 ) of both word and character levels. For word embeddings, GloVe (Pennington et al., 2014) and BERT (Devlin et al., 2019) embeddings are used. Character embeddings are learned by a convolution neural networks (CNN) during training. The tokens are represented by concatenated outputs from the forward and the backward LSTMs. The token representations (x∗t )Tt=1 are used together with head representations (h∗i ) to represent candidate spans (Ni∗ ). The h∗i of a span is obtained by applying an attention over its token representations ({x∗si , ..., x∗ei }), where si and ei are the indices of the start and the end of System architecture Anaphora resolution is the task of identifying the referring mentions in a text and"
2020.lrec-1.2,J14-4004,0,0.0603458,"Missing"
2020.lrec-1.2,L16-1100,0,0.0479918,"Missing"
2020.lrec-1.2,D14-1162,0,0.0864198,"Missing"
2020.lrec-1.2,N18-1202,0,0.162041,"Missing"
2020.lrec-1.2,W18-0702,1,0.886236,"ty the system naturally resolves singletons (i.e. the clusters with a size of one). Non-referring expressions are usually filtered before building the coreference chains, e.g. in MARS (Mitkov et al., 2002); we will call this PREFILTERING approach. In the PREFILTERING approach, the system removes the markables identified as non-referring expressions from further processing once they have been identified. To be more specific, we replace line 8 of algorithm 1 with: if b == Parameter 3. Data and Hyperparameters For full anaphora resolution, our primary evaluation dataset was the CRAC 2018 corpus (Poesio et al., 2018). In addition, we evaluated our model on the PD corpus, also containing expletives. Finally, we evaluated our model on the CONLL 2012 English corpora (Pradhan et al., 2012) to compare its performance with the SoTA on the CONLL task. The CRAC Task 1 dataset is based on the RST portion of the ARRAU corpus (Uryupina et al., 2019). The annotation scheme specifies the annotation of referring expressions (including singletons) and non-referring expressions; split antecedent plurals, generic references, and discourse deixis are annotated, as well as bridging references. The RST portion of ARRAU consi"
2020.lrec-1.2,N19-1176,1,0.821276,"compare its performance with the SoTA on the CONLL task. The CRAC Task 1 dataset is based on the RST portion of the ARRAU corpus (Uryupina et al., 2019). The annotation scheme specifies the annotation of referring expressions (including singletons) and non-referring expressions; split antecedent plurals, generic references, and discourse deixis are annotated, as well as bridging references. The RST portion of ARRAU consists of news texts (1/3 of the PENN Treebank), with 228,000 tokens and 72,000 mentions. PD is a constantly growing corpus collected using the annotation game Phrase Detectives (Poesio et al., 2019). The corpus was annotated by players and then aggregated by Learning To train a cluster ranking model on system clusters is challenging, as we need to find a way to learn from the partially correct clusters. It is also slow, as the system processes one mention at a time, hence cannot benefit largely from parallel computing. The solution we adopted was training the model on oracle clusters. This is simpler and faster, since the clusters for one training document can be created before 6 We train both approaches on the CONLL data for 200K steps on a GTX 1080Ti GPU. It takes 16 and 80 hours to tr"
2020.lrec-1.2,W12-4501,0,0.756291,"e of mention pairs (Bj¨orkelund and Kuhn, 2014; Clark and Manning, 2015; Clark and Manning, 2016b; Lee et al., 2018; Kantor and Globerson, 2019; Joshi et al., 2019b; Joshi et al., 2019a). However, those systems are usually much more complex Anaphora resolution is the task of identifying and resolving nominal anaphoric reference to discourse entities (Poesio et al., 2016b).1 It is an important aspect of natural language processing and has a substantial impact on downstream applications such as summarization (Steinberger et al., 2007; Steinberger et al., 2016). Since the CONLL 2012 shared task (Pradhan et al., 2012), the ONTONOTES corpus has been the dominant resource in research on identity anaphora resolution (coreference) (Fernandes et al., 2014; Bj¨orkelund and Kuhn, 2014; Martschat and Strube, 2015; Clark and Manning, 2015; Clark and Manning, 2016a; Clark and Manning, 2016b; Lee et al., 2017; Lee et al., 2018; Kantor and Globerson, 2019; Joshi et al., 2019b; Joshi et al., 2019a). But ONTONOTES has a number of limitations. An often mentioned limitation is that singletons are not annotated (De Marneffe et al., 2015; Chen et al., 2018). A less discussed, but still crucial, limitation is that although s"
2020.lrec-1.2,J01-4004,0,0.81389,"Missing"
2020.lrec-1.2,taule-etal-2008-ancora,0,0.11793,"Missing"
2020.lrec-1.2,C08-1121,1,0.804256,"Missing"
2020.lrec-1.2,P15-1137,0,0.184791,"l allow us to explore rich cluster level features and advanced search algorithms (e.g. beam search) in future work. The Effect of Oracle Clusters on Training Time Training cluster ranking systems using system clusters is timeconsuming: Our model trained on system clusters takes 80 hours to train for 200K steps, which is much more than 5. Related Work Pure Mention Ranking Models Most recent coreference systems are highly reliant on mention ranking, which is effective and generally faster to train compared with the cluster ranking system. Systems based only on the mention ranking model include (Wiseman et al., 2015; Clark and Manning, 2016b; Lee et al., 2017). Wiseman et al. (2015) introduced a neural network based approach to solve the task in a non-linear way. In their system, the heuristic features commonly used in linear models are transformed by a tanh function to be used as the mention representations. Clark and Manning (2016b) integrated reinforcement learning to let the model optimize directly on the B3 scores. Lee 17 et al. (2017) first presented a neural joint approach for mention detection and coreference resolution. Their model does not rely on parse trees; instead, the system learns to dete"
2020.lrec-1.2,N16-1114,0,0.183406,"Missing"
2020.lrec-1.2,P18-2017,0,0.0244547,"Missing"
2020.nlp4call-1.3,rayner-etal-2010-multilingual,0,0.0352603,"a learning program that employs a CMC application for audio and text, highlighting the importance of such tools in the context of distance learning. These applications were dedicated to teaching a single language, but MagicWord (Hatier et al., 2019), offers a multilingual word game, initially developed for Italian, French and English. Revita (Katinskaia et al., 2017) is a system with automated fill-thegap exercises for stimulating active vocabulary. While it is proposed for endangered languages, it faces various challenges related to its multilingualism such as the lack of corpora. CALL-SLT (Rayner et al., 2010) is a system that uses a textual or pictorial representation of an interlingua to prompt users’ speech in four supported second languages. Despite facing challenges like limited 1 While Krashen makes a distinction, in this paper we use the terms learning and acquisition interchangeably. Proceedings of the 9th Workshop on Natural Language Processing for Computer Assisted Language Learning (NLP4CALL 2020) 22 language they would like to learn. Another important difference is that our experiment did not run on Amazon Mechanical Turk, but rather as a free downloadable mobile application for the And"
2020.pam-1.17,J91-4003,0,0.554806,"bedding systems by examining whether they encode differences in word sense similarity and iii) compare the word sense similarities of both methods to assess their correlation and gain some intuition as to how well contextualised word embeddings could be used as surrogate word sense similarity judgements in linguistic experiments. 1 (2) Specifically, we aim to investigate a recent model of polyseme sense clustering proposed by Ortega-Andr´es and Vicente (2019), suggesting that similarity differences in polysemic senses could lead to a grouping in their representation in the Generative Lexicon (Pustejovsky, 1991), addressing and attempting an explanation for processing differences observed within the seemingly homogeneous group of polysemes. Through a range of surveys we collect word sense similarity judgements for a set of polysemes to provide empirical data for an investigation of word sense clustering as proposed by OrtegaAndr´es and Vicente. We then aim to extend the linguistic evaluation of context-sensitive word embeddings by examining whether their contextualised encodings of polysemes show signs of word sense grouping, and whether these groupings correlate with the patterns observed in the hum"
2020.starsem-1.12,2020.pam-1.17,1,0.866582,"d-for-container] He took a sip and put his beer/coffee/juice/gin/soup... back on the kitchen table. The newspaper fired its editor in chief., The newspaper was sued for defamation. The newspaper lies on the kitchen table., The newspaper got wet from the rain. The newspaper wasn’t very interesting., The newspaper is rather satirical today. Besides the polyseme samples, we created an adWith growing evidence that irregular polysemes might be processed differently than their regular counterparts (e.g. Klepousniotou et al., 2012), we 3 As in “The school is an old building.” for sense building. See Haber and Poesio (2020) for more details. 116 ditional two samples sets. The first set is made up of 15 common homonyms, with two sentences invoking their two most dominant senses each. While our focus is on polysemes, comparing ratings for the homonym samples to ratings assigned to polyseme pairs, we will be able to test the different similarity measures’ performance in predicting whether an ambiguous target pair is polysemic or homonymic. The second set contains 15 pairs of synonyms meant to be used as quality control and to calibrate the rating scale. All sample sentences were rated to be acceptable by annotators"
2020.starsem-1.12,J12-3005,0,0.0324455,"e stored in 16dimensional multi-hot vectors indicating the selection of labels together with the worker’s ID. We kept the same worker qualification requirements and payment regime as before and collected 15 la118 2.5 Word Embedding Similarity Because the three previously described measures of word sense similarity are based on costly humanannotated labels, we were also interested in investigating how well sense similarity estimates derived from computational models would correlate with these metrics. Models of polysemy have previously been proposed in distributional semantics (see for example Boleda et al., 2012), but for the most part, such models found limited application in computational linguistics. With the recent development of context-sensitive models of word embeddings such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018), the field however obtained a new tool to capture polysemic sense alterations, leading to a demonstrated improvement in various NLP systems. While ELMo was developed explicitly to capture a target word’s context, BERT is a language model based on the encoder architecture of the Transformer model (Vaswani et al., 2017), an attention mechanism for learning the conte"
2020.starsem-1.12,D19-1006,0,0.0205661,"encoder architecture of the Transformer model (Vaswani et al., 2017), an attention mechanism for learning the contextual relations between words. While BERT’s output is usually fed to a downstream model, our aim is to see whether it is able to capture differences in word sense by using its outputs directly. To obtain ELMo embeddings we used a pretrained model available on TensorFlow Hub8 and extracted target word vectors from the LSTM’s second layer hidden state, which has previously been shown to encode more semantic information than the character-level first layer or the LSTM’s first layer (Ethayarajh, 2019; Haber and Poesio, 2020). For the investigation of BERT’s embeddings we used the output of a pretrained cased model as provided by Huggingface9 with 12 layers, a hidden 7 https://github.com/dali-ambiguity/ Word-Sense-Dataset-v1 8 https://tfhub.dev/google/ELMo/3 9 https://huggingface.co/transformers/ Density Density Distribution of Annotator Scores 6 4 2 0 3 2 1 0 0.0 0.0 0.2 0.4 0.6 0.8 Mean Word Sense Similarity 1.0 0.0 0.0 0.2 0.4 0.6 0.8 Mean Co-predication Acceptability 1.0 Density Density 7.5 5.0 2.5 0.0 0.0 0.2 0.4 0.6 0.8 BERT WE Cosine Similarity 0.2 0.4 0.6 0.8 Word2Vec SE Cosine Sim"
2020.starsem-1.12,N18-1202,0,0.0726825,"d 15 la118 2.5 Word Embedding Similarity Because the three previously described measures of word sense similarity are based on costly humanannotated labels, we were also interested in investigating how well sense similarity estimates derived from computational models would correlate with these metrics. Models of polysemy have previously been proposed in distributional semantics (see for example Boleda et al., 2012), but for the most part, such models found limited application in computational linguistics. With the recent development of context-sensitive models of word embeddings such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018), the field however obtained a new tool to capture polysemic sense alterations, leading to a demonstrated improvement in various NLP systems. While ELMo was developed explicitly to capture a target word’s context, BERT is a language model based on the encoder architecture of the Transformer model (Vaswani et al., 2017), an attention mechanism for learning the contextual relations between words. While BERT’s output is usually fed to a downstream model, our aim is to see whether it is able to capture differences in word sense by using its outputs directly. To obtai"
2020.starsem-1.12,J91-4003,0,0.339773,"8, 2012), the prevailing understanding of co-predication is that it is rendered felicitous if the different sense interpretations are activated simultaneously and can be shifted between without additional processing costs. Co-predication is thought to lead to infelicitous sentences if the different activations are not activated automatically, and cognitive effort is involved in updating the assumed meaning of a word. These hypotheses informed a number of linguistic models to define different mental representations of homonymic meaning and polysemic sense, respectively. The Generative Lexicon (Pustejovsky, 1991; Asher and Pustejovsky, 2006; Asher, 2011) for example postulates individual lexicon entries for different interpretations of a homonym, while all sense interpretations of a polyseme are represented by a single under-specified entry and therefore do not require any processing cost for sense switching. Recently, a growing body of work however came to challenge a unified, under-specified representation of polysemic sense (see Klepousniotou, 2002; Pylkk¨anen et al., 2006; Frisson, 2015). Klepousniotou et al. (2012) for example report that their experiments indicate that the processing of irregul"
2020.wanlp-1.31,2020.wanlp-1.9,0,0.0367487,"luding Egyptian, Levantine, and Gulf. Huang (2015) applied a word-level n-gram model to identify MSA and regional dialects, and also considered Facebook posts . Elaraby and Abdul-Mageed (2018) used the AOC dataset to evaluate several machine learning and deep learning models. Salameh et al. (2018) proposed a fine-grained dialect identification module for Corpus-25 by applying a multinomial Naive Bayes classifier with a large set of features. The Nuanced Arabic Dialect Identification (NADI) shared task aims to incentivise research on identifying different Arabic dialects in every Arab country (Abdul-Mageed et al., 2020). The NADI dataset was collected from Arabic twitter, and each tweet is labeled with two labels, country-label and provincelabel, for a total of 21 countries and 100 of their provinces. There are two subtasks in NADI: the first is targeting these country-level labels and the second one is targeting the provinces. In the rest of the paper, This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 295 Proceedings of the Fifth Arabic Natural Language Processing Workshop, pages 295–301 Barcelona, Spain (Onli"
2020.wanlp-1.31,althobaiti-etal-2014-aranlp,1,0.785374,"9,081 1,136 1,133 Total 86,541 10,821 10,812 Table 2: AOC portions of MSA and region dialects from (Elaraby and Abdul-Mageed, 2018) 3 3.1 System Tweet Text Prepossessing Arabic tweets are very noisy, so we removed URLS, emojis, Latin-characters, numbers, mentions, and any non-Arabic characters. Arabic hashtags were kept as they are because they might contain important 296 information such as (Lebanon revolts, )ﻟﺒﻨﺎن ﯾﻨﺘﻔﺾ. Text normalisation was carried out to normalise different forms of “Alif”, “Yaa”, removing punctuation, excessive character repetitions, Kashida “tatweel” and diacritics (Althobaiti et al., 2014). The class distribution is highly imbalanced, which could make a model biased towards certain classes. Therefore, random up-sampling for each data class was applied to match the size of the majority class, the Egyptian class. 3.2 Combined Features Model Our approach to classifying tweets involves three components: 1. BiLSTM-CNN model: to extract word and character representations, we build a BiLSTM-CNN model following the same settings in (Ma and Hovy, 2016). We pre-trained FastText on the 10m unlabeled tweets to represent words. We randomly initialize character embeddings of size 30 and trai"
2020.wanlp-1.31,cotterell-callison-burch-2014-multi,0,0.0250639,"del while the development set was used to optimize model parameters. In addition, NADI also provided 10 million unlabeled tweets which we used to train a word embedding model for Arabic tweets. Table 1 shows the number of the annotated Arabic tweets for each country in training and development sets. The data samples distribution over the dialectal classes or countries is unbalanced. Our model was also evaluated against another dataset, the Arabic Online Commentary (AOC). A subset of the data was annotated using crowd-sourcing and has been used in previous work (Zaidan and CallisonBurch, 2014; Cotterell and Callison-Burch, 2014). The AOC dataset classifies Arabic dialects into three dialects in addition to the MSA, so spoken dialects in different countries may be grouped together as one dialect. For example, the “Gulf” dialect label may includes all spoken dialects in Saudi Arabia, United Arab Emirates, Kuwait, Bahrain, Iraq, etc. (Elaraby and Abdul-Mageed, 2018) benchmarked a portion of AOC, and applied various machine learning algorithms to identify MSA and dialects based on three settings: • Binary: where they classify the data to MSA and or not MSA. • Three-way: where they classify three dialectal regions (Egypti"
2020.wanlp-1.31,D14-1154,0,0.0186747,"c news sites. Zaghouani and Charfi (2018) collected and annotated Twitter data from 11 regions and 16 countries in the Arab world. Abdul-Mageed et al. (2018) also collected Twitter data and annotated them at the city-level i.e 29 cities of 11 countries. Bouamor et al. (2018) translated the Basic Traveling Expression Corpus (BTEC) (Takezawa et al., 2007) into 25 city dialects of Arab countries; this corpus is referred to as Corpus-25. Anong the models, some proposals only aim to separating one dialect (e.g., Egyptian) from MSA on the AOC dataset (Elfardy and Diab, 2013; Tillmann et al., 2014). Darwish et al. (2014) also distinguished between Egyptian and MSA, but focusing on twitter data. Zaidan and Callison-Burch (2014) proposed a model to identify MSA and regional dialects including Egyptian, Levantine, and Gulf. Huang (2015) applied a word-level n-gram model to identify MSA and regional dialects, and also considered Facebook posts . Elaraby and Abdul-Mageed (2018) used the AOC dataset to evaluate several machine learning and deep learning models. Salameh et al. (2018) proposed a fine-grained dialect identification module for Corpus-25 by applying a multinomial Naive Bayes classifier with a large set"
2020.wanlp-1.31,W18-3930,0,0.168527,"2007) into 25 city dialects of Arab countries; this corpus is referred to as Corpus-25. Anong the models, some proposals only aim to separating one dialect (e.g., Egyptian) from MSA on the AOC dataset (Elfardy and Diab, 2013; Tillmann et al., 2014). Darwish et al. (2014) also distinguished between Egyptian and MSA, but focusing on twitter data. Zaidan and Callison-Burch (2014) proposed a model to identify MSA and regional dialects including Egyptian, Levantine, and Gulf. Huang (2015) applied a word-level n-gram model to identify MSA and regional dialects, and also considered Facebook posts . Elaraby and Abdul-Mageed (2018) used the AOC dataset to evaluate several machine learning and deep learning models. Salameh et al. (2018) proposed a fine-grained dialect identification module for Corpus-25 by applying a multinomial Naive Bayes classifier with a large set of features. The Nuanced Arabic Dialect Identification (NADI) shared task aims to incentivise research on identifying different Arabic dialects in every Arab country (Abdul-Mageed et al., 2020). The NADI dataset was collected from Arabic twitter, and each tweet is labeled with two labels, country-label and provincelabel, for a total of 21 countries and 100"
2020.wanlp-1.31,P13-2081,0,0.00884236,"nd Arabic regional dialects collected from Arabic news sites. Zaghouani and Charfi (2018) collected and annotated Twitter data from 11 regions and 16 countries in the Arab world. Abdul-Mageed et al. (2018) also collected Twitter data and annotated them at the city-level i.e 29 cities of 11 countries. Bouamor et al. (2018) translated the Basic Traveling Expression Corpus (BTEC) (Takezawa et al., 2007) into 25 city dialects of Arab countries; this corpus is referred to as Corpus-25. Anong the models, some proposals only aim to separating one dialect (e.g., Egyptian) from MSA on the AOC dataset (Elfardy and Diab, 2013; Tillmann et al., 2014). Darwish et al. (2014) also distinguished between Egyptian and MSA, but focusing on twitter data. Zaidan and Callison-Burch (2014) proposed a model to identify MSA and regional dialects including Egyptian, Levantine, and Gulf. Huang (2015) applied a word-level n-gram model to identify MSA and regional dialects, and also considered Facebook posts . Elaraby and Abdul-Mageed (2018) used the AOC dataset to evaluate several machine learning and deep learning models. Salameh et al. (2018) proposed a fine-grained dialect identification module for Corpus-25 by applying a multi"
2020.wanlp-1.31,D15-1254,0,0.0166917,"29 cities of 11 countries. Bouamor et al. (2018) translated the Basic Traveling Expression Corpus (BTEC) (Takezawa et al., 2007) into 25 city dialects of Arab countries; this corpus is referred to as Corpus-25. Anong the models, some proposals only aim to separating one dialect (e.g., Egyptian) from MSA on the AOC dataset (Elfardy and Diab, 2013; Tillmann et al., 2014). Darwish et al. (2014) also distinguished between Egyptian and MSA, but focusing on twitter data. Zaidan and Callison-Burch (2014) proposed a model to identify MSA and regional dialects including Egyptian, Levantine, and Gulf. Huang (2015) applied a word-level n-gram model to identify MSA and regional dialects, and also considered Facebook posts . Elaraby and Abdul-Mageed (2018) used the AOC dataset to evaluate several machine learning and deep learning models. Salameh et al. (2018) proposed a fine-grained dialect identification module for Corpus-25 by applying a multinomial Naive Bayes classifier with a large set of features. The Nuanced Arabic Dialect Identification (NADI) shared task aims to incentivise research on identifying different Arabic dialects in every Arab country (Abdul-Mageed et al., 2020). The NADI dataset was c"
2020.wanlp-1.31,P16-1101,0,0.0388962,"rmalise different forms of “Alif”, “Yaa”, removing punctuation, excessive character repetitions, Kashida “tatweel” and diacritics (Althobaiti et al., 2014). The class distribution is highly imbalanced, which could make a model biased towards certain classes. Therefore, random up-sampling for each data class was applied to match the size of the majority class, the Egyptian class. 3.2 Combined Features Model Our approach to classifying tweets involves three components: 1. BiLSTM-CNN model: to extract word and character representations, we build a BiLSTM-CNN model following the same settings in (Ma and Hovy, 2016). We pre-trained FastText on the 10m unlabeled tweets to represent words. We randomly initialize character embeddings of size 30 and train them into a CNN neural network to learn morphological information, for example, the prefix or suffix of a word (Dos Santos and Zadrozny, 2014). We concatenate each word embedding with its character embedding and feed them into a BiLSTM network to learn the sentence information. 2. Character-level TF-IDF: we applied (1-5) character grams of TF-IDF on the train set. We tried to expand gram range, but that did not improve the performance. The TF-IDF component"
2020.wanlp-1.31,C18-1113,0,0.0179391,"osals only aim to separating one dialect (e.g., Egyptian) from MSA on the AOC dataset (Elfardy and Diab, 2013; Tillmann et al., 2014). Darwish et al. (2014) also distinguished between Egyptian and MSA, but focusing on twitter data. Zaidan and Callison-Burch (2014) proposed a model to identify MSA and regional dialects including Egyptian, Levantine, and Gulf. Huang (2015) applied a word-level n-gram model to identify MSA and regional dialects, and also considered Facebook posts . Elaraby and Abdul-Mageed (2018) used the AOC dataset to evaluate several machine learning and deep learning models. Salameh et al. (2018) proposed a fine-grained dialect identification module for Corpus-25 by applying a multinomial Naive Bayes classifier with a large set of features. The Nuanced Arabic Dialect Identification (NADI) shared task aims to incentivise research on identifying different Arabic dialects in every Arab country (Abdul-Mageed et al., 2020). The NADI dataset was collected from Arabic twitter, and each tweet is labeled with two labels, country-label and provincelabel, for a total of 21 countries and 100 of their provinces. There are two subtasks in NADI: the first is targeting these country-level labels and"
2020.wanlp-1.31,W19-4639,0,0.0171105,"ayers of the classifier. The overall model is in Figure 1. Also, we find training Fasttext on the 10 million unlabeled tweets to yield better results than using existing pretrained Arabic word embeddings. We train the model using the train set, and we optimize the hyperparameters based on the evaluation of the development set, the hyperparameter settings in Table 3. We applied the early stopping technique based on the F1-macro score. In NADI shared task, we used the mentioned model for our first run. 3.3 Combined Features with Heuristic Model Next, we augmented our model with a heuristic from Samih et al. (2019). The heuristic is based on a list of all Arabic speaking countries and their major cities. If a tweet mentions any country/city in the list, the tweet would be classified to the mentioned country/city. We excluded the cities because they did not boost the overall performance. Our model combined with the heuristic was our second run to NADI challenge. Number of units in the first layer Number of units in the second layer Cell size of BiLSTM Learning rate Dropout rate Optimizer 1200 800 500 1e-4 0.5 Adam Table 3: Hyperparameter settings. We implemented the neural network using Tensorflow (Abadi"
2020.wanlp-1.31,O07-5005,0,0.0622319,"ted in computational models. Among the studies addressing this issue, Zaidan and Callison-Burch (2014) created a large dataset for this purpose, called Arabic Online Commentary (AOC). The data consists of texts in MSA and Arabic regional dialects collected from Arabic news sites. Zaghouani and Charfi (2018) collected and annotated Twitter data from 11 regions and 16 countries in the Arab world. Abdul-Mageed et al. (2018) also collected Twitter data and annotated them at the city-level i.e 29 cities of 11 countries. Bouamor et al. (2018) translated the Basic Traveling Expression Corpus (BTEC) (Takezawa et al., 2007) into 25 city dialects of Arab countries; this corpus is referred to as Corpus-25. Anong the models, some proposals only aim to separating one dialect (e.g., Egyptian) from MSA on the AOC dataset (Elfardy and Diab, 2013; Tillmann et al., 2014). Darwish et al. (2014) also distinguished between Egyptian and MSA, but focusing on twitter data. Zaidan and Callison-Burch (2014) proposed a model to identify MSA and regional dialects including Egyptian, Levantine, and Gulf. Huang (2015) applied a word-level n-gram model to identify MSA and regional dialects, and also considered Facebook posts . Elarab"
2020.wanlp-1.31,W14-5313,0,0.0175448,"cts collected from Arabic news sites. Zaghouani and Charfi (2018) collected and annotated Twitter data from 11 regions and 16 countries in the Arab world. Abdul-Mageed et al. (2018) also collected Twitter data and annotated them at the city-level i.e 29 cities of 11 countries. Bouamor et al. (2018) translated the Basic Traveling Expression Corpus (BTEC) (Takezawa et al., 2007) into 25 city dialects of Arab countries; this corpus is referred to as Corpus-25. Anong the models, some proposals only aim to separating one dialect (e.g., Egyptian) from MSA on the AOC dataset (Elfardy and Diab, 2013; Tillmann et al., 2014). Darwish et al. (2014) also distinguished between Egyptian and MSA, but focusing on twitter data. Zaidan and Callison-Burch (2014) proposed a model to identify MSA and regional dialects including Egyptian, Levantine, and Gulf. Huang (2015) applied a word-level n-gram model to identify MSA and regional dialects, and also considered Facebook posts . Elaraby and Abdul-Mageed (2018) used the AOC dataset to evaluate several machine learning and deep learning models. Salameh et al. (2018) proposed a fine-grained dialect identification module for Corpus-25 by applying a multinomial Naive Bayes class"
2020.wanlp-1.31,L18-1111,0,0.0235365,"quite a challenging task. Thus, many studies have been devoted to Arabic dialect identification, because it benefits automatic speech recognition, remote access, e-health, and other applications (Etman and Beex, 2015). The main challenge for Arabic dialect identification is the lack of large dataset that can be exploited in computational models. Among the studies addressing this issue, Zaidan and Callison-Burch (2014) created a large dataset for this purpose, called Arabic Online Commentary (AOC). The data consists of texts in MSA and Arabic regional dialects collected from Arabic news sites. Zaghouani and Charfi (2018) collected and annotated Twitter data from 11 regions and 16 countries in the Arab world. Abdul-Mageed et al. (2018) also collected Twitter data and annotated them at the city-level i.e 29 cities of 11 countries. Bouamor et al. (2018) translated the Basic Traveling Expression Corpus (BTEC) (Takezawa et al., 2007) into 25 city dialects of Arab countries; this corpus is referred to as Corpus-25. Anong the models, some proposals only aim to separating one dialect (e.g., Egyptian) from MSA on the AOC dataset (Elfardy and Diab, 2013; Tillmann et al., 2014). Darwish et al. (2014) also distinguished"
2020.wanlp-1.31,J14-1006,0,0.0328621,"l Arabic (CA), Modern Standard Arabic (MSA), or several Arabic dialects (ADs). Both classical Arabic and MSA are standardized, while Arabic dialects are not. So identifying different Arabic dialect varieties is quite a challenging task. Thus, many studies have been devoted to Arabic dialect identification, because it benefits automatic speech recognition, remote access, e-health, and other applications (Etman and Beex, 2015). The main challenge for Arabic dialect identification is the lack of large dataset that can be exploited in computational models. Among the studies addressing this issue, Zaidan and Callison-Burch (2014) created a large dataset for this purpose, called Arabic Online Commentary (AOC). The data consists of texts in MSA and Arabic regional dialects collected from Arabic news sites. Zaghouani and Charfi (2018) collected and annotated Twitter data from 11 regions and 16 countries in the Arab world. Abdul-Mageed et al. (2018) also collected Twitter data and annotated them at the city-level i.e 29 cities of 11 countries. Bouamor et al. (2018) translated the Basic Traveling Expression Corpus (BTEC) (Takezawa et al., 2007) into 25 city dialects of Arab countries; this corpus is referred to as Corpus-2"
2021.bppf-1.3,N13-1062,0,0.0223759,"it fundamentally hides the true nature of the task we are trying to solve. However, there is plenty of evidence that gold labels are an idealization, and that unreconcilable disagreement is abundant. Figure 1 shows two examples from CV and NLP. This is particularly true for tasks involving highly subjective judgments, such as hate speech detection (Akhtar et al., 2019, 2020) or sentiment analysis (Kenyon-Dean et al., 2018). However, it is not a trivial issue even in more linguistic tasks, such as part-of-speech tagging (Plank et al., 2014), word sense disambiguation (Passonneau et al., 2012; Jurgens, 2013), or coreference resolution (Poesio and Artstein, 2005; Recasens et al., 2011). Systematic disagreement also exists in image classification tasks, where labels may overlap (Rodrigues and Pereira, 2018; Peterson et al., 2019). Disagreement and task difficulty and subjectivity also challenge traditional agreement measures (Artstein and Poesio, 2008). High agreement is typically used as a proxy for data quality. However, it obscures possible sources of disagreement (Poesio and Artstein, 2005). We summarize some of the evidence on disagreement in Section 2. The need for metrics not based on the as"
2021.bppf-1.3,2021.naacl-main.204,1,0.779829,"s disagree. There is no consensus yet on this form of evaluation, but a few proposals have been used already. In fact, a way of performing soft evaluation exists which is a natural extension of current practice in NLP. This is to evaluate ambiguity-aware models by treating the probability distribution of labels they produce as a soft label, and comparing that to a full distribution of labels, instead of a ‘one-hot’ approach. This can be done using, for example, cross-entropy, although other options also exist. This approach was adopted in, inter alia, (Peterson et al., 2019; Uma et al., 2020; Fornaciari et al., 2021). Peterson et al. (2019) tested this approach on image classification tasks, generating the soft label by transforming the item annotation distribution using standard normalization. Uma et al. (2020) employed this form of soft metric evaluation for NLP , also comparing different ways to obtain a soft label from the raw data. They use soft metrics to compare the classifiers’ distribution to the human-derived label distributions, complementing traditional hard evaluation measures. Basile (2020) suggested a more extreme evaluation framework, where a model is required to produce different outputs"
2021.bppf-1.3,N18-1171,0,0.0210479,"unnecessary: while evaluation methods that include disagreement are not yet established, several methodologies already do exist. Removing the disagreement might lead to better evaluation scores, but it fundamentally hides the true nature of the task we are trying to solve. However, there is plenty of evidence that gold labels are an idealization, and that unreconcilable disagreement is abundant. Figure 1 shows two examples from CV and NLP. This is particularly true for tasks involving highly subjective judgments, such as hate speech detection (Akhtar et al., 2019, 2020) or sentiment analysis (Kenyon-Dean et al., 2018). However, it is not a trivial issue even in more linguistic tasks, such as part-of-speech tagging (Plank et al., 2014), word sense disambiguation (Passonneau et al., 2012; Jurgens, 2013), or coreference resolution (Poesio and Artstein, 2005; Recasens et al., 2011). Systematic disagreement also exists in image classification tasks, where labels may overlap (Rodrigues and Pereira, 2018; Peterson et al., 2019). Disagreement and task difficulty and subjectivity also challenge traditional agreement measures (Artstein and Poesio, 2008). High agreement is typically used as a proxy for data quality."
2021.bppf-1.3,P11-2008,0,0.163137,"Missing"
2021.bppf-1.3,W04-1013,0,0.0362296,"nd, therefore, the disagreement levels. Such individual differences can be partially explained by cultural and socio-demographic norms and variables, such as age, gender, instruction level, or cultural background. However, none of them is sufficient to capture the uniqueness of each subject and their evaluations. 2.2 Disagreement in ‘Objective’ Tasks The NLP community has long been aware that it makes no sense to evaluate natural language generation applications against a hypothetical ‘gold’ output. These areas have developed specialized training and evaluation methods (Papineni et al., 2002; Lin, 2004). More surprisingly, disagreements in interpretation have been found to be frequent in annotation projects concerned with apparently more ‘objective’ aspects of language, such as coreference (Poesio and Artstein, 2005; Recasens et al., 2011), part-of-speech tagging (Plank et al., 2014), word sense disambiguation (Passonneau et al., 2012) and semantic role labelling (Dumitrache et al., 2019), to name a few examples. Even if in these tasks individual instances can be found to be reasonably objective, these findings appear to reflect the existence of extensive and systematic disagreement on what"
2021.bppf-1.3,P02-1040,0,0.11206,"he annotation outcome and, therefore, the disagreement levels. Such individual differences can be partially explained by cultural and socio-demographic norms and variables, such as age, gender, instruction level, or cultural background. However, none of them is sufficient to capture the uniqueness of each subject and their evaluations. 2.2 Disagreement in ‘Objective’ Tasks The NLP community has long been aware that it makes no sense to evaluate natural language generation applications against a hypothetical ‘gold’ output. These areas have developed specialized training and evaluation methods (Papineni et al., 2002; Lin, 2004). More surprisingly, disagreements in interpretation have been found to be frequent in annotation projects concerned with apparently more ‘objective’ aspects of language, such as coreference (Poesio and Artstein, 2005; Recasens et al., 2011), part-of-speech tagging (Plank et al., 2014), word sense disambiguation (Passonneau et al., 2012) and semantic role labelling (Dumitrache et al., 2019), to name a few examples. Even if in these tasks individual instances can be found to be reasonably objective, these findings appear to reflect the existence of extensive and systematic disagreem"
2021.bppf-1.3,S13-2025,0,0.0191364,"based on the assumption that a gold standard exists has long been accepted for end-to-end tasks, particularly those involving an aspect of natural language generation, such as conversational agents, machine translation, surface realisation, image captioning, or summarization. Metrics such as BLEU for machine translation/generation, ROUGE for summarization, or NDCG for ranking Web searches all support more than one gold standard reference. Shared tasks in this areas (particularly on paraphrasing), have also considered the role of disagreement in their evaluation metrics (Butnariu et al., 2009; Hendrickx et al., 2013). Variability in the annotation is a feature of 2 Disagreement in NLP In this section, we outline three possible sources of disagreement. Afterward, we describe how disagreement has been studied in objective and arguably more subjective tasks in NLP. 2.1 Sources of Disagreement Annotation implies an interaction between the human judge, the instance which has to be evaluated, and the moment/context in which the process takes place. For each instance, the annotation outcome 16 depends on these three elements, assuming the task is properly defined, designed, and carried out, e.g., in terms of qua"
2021.bppf-1.3,W13-2323,0,0.0253399,"arking: Past, Present and Future, pages 15–21 August 5–6, 2021. ©2021 Association for Computational Linguistics best possible approximation of the truth about a given phenomenon, or at least a reasonable one. This ground truth is usually obtained by developing an annotation scheme for the task aiming to achieve the highest possible agreement between human annotators (Artstein and Poesio, 2008). Disagreements between annotators are either reconciled by hand or aggregated (particularly in the case of crowdsourced annotations) to extract the most likely or agreed-upon choices (Hovy et al., 2013; Passonneau and Carpenter, 2013; Paun et al., 2018). This aggregated data is referred to as “gold standard” (see Ide and Pustejovsky (2017) for an in-depth analysis of annotation methodology). many such tasks (see, e.g., van der Lee et al. (2019) for agreement issues in generated text evaluation) even though many corpora still may come with single references due to data collection costs. High agreement is disfavored, and even bears risks of non-natural, highly homogenized system outputs for generation tasks (Amidei et al., 2018). The main argument of this position paper is that we should recognize that the same issues, if p"
2021.bppf-1.3,N13-1132,1,0.771702,"Workshop on Benchmarking: Past, Present and Future, pages 15–21 August 5–6, 2021. ©2021 Association for Computational Linguistics best possible approximation of the truth about a given phenomenon, or at least a reasonable one. This ground truth is usually obtained by developing an annotation scheme for the task aiming to achieve the highest possible agreement between human annotators (Artstein and Poesio, 2008). Disagreements between annotators are either reconciled by hand or aggregated (particularly in the case of crowdsourced annotations) to extract the most likely or agreed-upon choices (Hovy et al., 2013; Passonneau and Carpenter, 2013; Paun et al., 2018). This aggregated data is referred to as “gold standard” (see Ide and Pustejovsky (2017) for an in-depth analysis of annotation methodology). many such tasks (see, e.g., van der Lee et al. (2019) for agreement issues in generated text evaluation) even though many corpora still may come with single references due to data collection costs. High agreement is disfavored, and even bears risks of non-natural, highly homogenized system outputs for generation tasks (Amidei et al., 2018). The main argument of this position paper is that we should reco"
2021.bppf-1.3,Q18-1040,1,0.826112,", pages 15–21 August 5–6, 2021. ©2021 Association for Computational Linguistics best possible approximation of the truth about a given phenomenon, or at least a reasonable one. This ground truth is usually obtained by developing an annotation scheme for the task aiming to achieve the highest possible agreement between human annotators (Artstein and Poesio, 2008). Disagreements between annotators are either reconciled by hand or aggregated (particularly in the case of crowdsourced annotations) to extract the most likely or agreed-upon choices (Hovy et al., 2013; Passonneau and Carpenter, 2013; Paun et al., 2018). This aggregated data is referred to as “gold standard” (see Ide and Pustejovsky (2017) for an in-depth analysis of annotation methodology). many such tasks (see, e.g., van der Lee et al. (2019) for agreement issues in generated text evaluation) even though many corpora still may come with single references due to data collection costs. High agreement is disfavored, and even bears risks of non-natural, highly homogenized system outputs for generation tasks (Amidei et al., 2018). The main argument of this position paper is that we should recognize that the same issues, if perhaps in less extre"
2021.bppf-1.3,Q19-1043,0,0.0234722,"have been found to be frequent in annotation projects concerned with apparently more ‘objective’ aspects of language, such as coreference (Poesio and Artstein, 2005; Recasens et al., 2011), part-of-speech tagging (Plank et al., 2014), word sense disambiguation (Passonneau et al., 2012) and semantic role labelling (Dumitrache et al., 2019), to name a few examples. Even if in these tasks individual instances can be found to be reasonably objective, these findings appear to reflect the existence of extensive and systematic disagreement on what can be concluded from a natural language statement (Pavlick and Kwiatkowski, 2019). Stimulus Characteristics. Instance characteristics have paramount importance for the annotation as well. Language meaning is often equivocal and carries ambiguities of several kinds: lexical, syntactical, semantic, and others. Humour, for example, often relies on lexical or syntactic ambiguity (Raskin, 1985; Poesio, 2020). Other genres using deliberate ambiguity as a rhetorical device include poetry (Su, 1994) or political discourse (Winkler, 2015). For some instances, more than one label is correct, and the relative annotation task would be better framed as multi-label multi-class, rather t"
2021.bppf-1.3,D15-1035,0,0.0223806,"though many corpora still may come with single references due to data collection costs. High agreement is disfavored, and even bears risks of non-natural, highly homogenized system outputs for generation tasks (Amidei et al., 2018). The main argument of this position paper is that we should recognize that the same issues, if perhaps in less extreme version, apply to the analysis tasks we discuss here. In recent years, proposals have been put forward to consider the disagreement as informative content that can be leveraged to improve task performance (Plank et al., 2014; Aroyo and Welty, 2015; Jamison and Gurevych, 2015). Uma et al. (2020) and Basile (2020) investigated the impact of disagreement-informed data on the quality of NLP evaluation, and found it to be beneficial and providing complementary information, as further discussed in Section 3. This led them to organize a first shared task on learning from disagreement and providing non-aggregated benchmarks for evaluation (Uma et al., 2021). In contrast with this trend, Bowman and Dahl (2021) recently proposed to study biases and artifacts in data to eliminate them. Beigman Klebanov and Beigman (2009) adopt a slightly softer stance, proposing to only eval"
2021.bppf-1.3,P14-2083,1,0.879095,"issues in generated text evaluation) even though many corpora still may come with single references due to data collection costs. High agreement is disfavored, and even bears risks of non-natural, highly homogenized system outputs for generation tasks (Amidei et al., 2018). The main argument of this position paper is that we should recognize that the same issues, if perhaps in less extreme version, apply to the analysis tasks we discuss here. In recent years, proposals have been put forward to consider the disagreement as informative content that can be leveraged to improve task performance (Plank et al., 2014; Aroyo and Welty, 2015; Jamison and Gurevych, 2015). Uma et al. (2020) and Basile (2020) investigated the impact of disagreement-informed data on the quality of NLP evaluation, and found it to be beneficial and providing complementary information, as further discussed in Section 3. This led them to organize a first shared task on learning from disagreement and providing non-aggregated benchmarks for evaluation (Uma et al., 2021). In contrast with this trend, Bowman and Dahl (2021) recently proposed to study biases and artifacts in data to eliminate them. Beigman Klebanov and Beigman (2009) ad"
2021.bppf-1.3,W05-0311,1,0.768942,"the task we are trying to solve. However, there is plenty of evidence that gold labels are an idealization, and that unreconcilable disagreement is abundant. Figure 1 shows two examples from CV and NLP. This is particularly true for tasks involving highly subjective judgments, such as hate speech detection (Akhtar et al., 2019, 2020) or sentiment analysis (Kenyon-Dean et al., 2018). However, it is not a trivial issue even in more linguistic tasks, such as part-of-speech tagging (Plank et al., 2014), word sense disambiguation (Passonneau et al., 2012; Jurgens, 2013), or coreference resolution (Poesio and Artstein, 2005; Recasens et al., 2011). Systematic disagreement also exists in image classification tasks, where labels may overlap (Rodrigues and Pereira, 2018; Peterson et al., 2019). Disagreement and task difficulty and subjectivity also challenge traditional agreement measures (Artstein and Poesio, 2008). High agreement is typically used as a proxy for data quality. However, it obscures possible sources of disagreement (Poesio and Artstein, 2005). We summarize some of the evidence on disagreement in Section 2. The need for metrics not based on the assumption that a gold standard exists has long been acc"
2021.bppf-1.3,P19-1572,0,0.0275709,"arning from disagreements in NLP and CV using datasets containing information about disagreements for interpreting language and classifying images. Five well-known datasets for very different NLP and CV tasks were identified, all characterized by a multiplicity of labels for each instance, by having a size sufficient to train state-of-the-art models, and by evincing different characteristics in terms of the crowd annotators and data collection procedure. These include: a dataset of Twitter posts annotated with POS tags collected by Gimpel et al. (2011), a datasets for humour identification by Simpson et al. (2019), and two CV datasets on object identification namely the LabelMe (Russell et al., 2008) and CIFAR -10 datasets (Peterson et al., 2019). Both hard evaluation metrics (F1) and soft evaluation metrics (cross-entropy, as discussed in Section 3) were used for evaluation (Uma et al., 2021). The results showed that in nearly all cases, models that account for noise and disagreement have the best (lowest) cross-entropy scores. These results are consistent with the findings of Uma et al. (2020) and Peterson et al. (2019). Evaluation in Light of Disagreement While the research mentioned in the previous"
2021.bppf-1.3,2021.semeval-1.41,1,0.865681,"discuss here. In recent years, proposals have been put forward to consider the disagreement as informative content that can be leveraged to improve task performance (Plank et al., 2014; Aroyo and Welty, 2015; Jamison and Gurevych, 2015). Uma et al. (2020) and Basile (2020) investigated the impact of disagreement-informed data on the quality of NLP evaluation, and found it to be beneficial and providing complementary information, as further discussed in Section 3. This led them to organize a first shared task on learning from disagreement and providing non-aggregated benchmarks for evaluation (Uma et al., 2021). In contrast with this trend, Bowman and Dahl (2021) recently proposed to study biases and artifacts in data to eliminate them. Beigman Klebanov and Beigman (2009) adopt a slightly softer stance, proposing to only evaluating on “easy” (as in, highly agreed upon) instances. Based on the evidence about the prevalence of disagreement in NLP judgments, we argue against this approach. First, it leads to information loss in the attempt to reducing noise in the data. Second, it is unnecessary: while evaluation methods that include disagreement are not yet established, several methodologies already d"
2021.bppf-1.3,W19-8643,0,0.0631318,"Missing"
2021.codi-sharedtask.1,D19-1422,0,0.0559867,"variant that is only trained on bridging annotations. We evaluated their best-performing model, which was trained on the RST sub-corpus of ARRAU, on CODI - CRAC 2021 data.19 The baseline for Task 3 leverages a simple heuristic that only considers demonstrative pronouns (this, that) as anaphors and considers the immediately preceding clause/utterance in the conversation to be their antecedent. Although simplistic, the algorithm achieves respectable scores on the CODI CRAC 2021 development corpus. The performance KU_NLP submitted results for tasks 1 and 2. For identity anaphora, they leveraged Cui and Zhang (2019)’s model with an ELECTRA-large backbone 18 20 The necessary scripts are available from https:// github.com/sopankhosla/codi2021_scripts 21 Participants were allowed to create teams. https://github.com/lxucs/coref-hoi/ https://github.com/juntaoy/ dali-bridging 19 8 9 Discourse Deixis Resolution Bridging Resolution Anaphora Resolution Track - Joshi et al. Leverage baseline’s architecture to find the correct (2019) bridging antecedent in the gold setting. KU_NLP INRIA - Yu and Poe- A multi-pass sieve approach which used the baseline sio (2020) as one of the sieves and consists of a set of learnin"
2021.codi-sharedtask.1,E89-1022,0,0.449685,"v., USA; 2 Univ. of Essex, UK; 3 Intel Labs, USA; 4 UT Dallas, USA; 5 Queen Mary Univ., UK; 6 HITS, Germany; 7 AWS AI, Amazon, USA sopankh@amazon.com; j.yu@essex.ac.uk; ramesh.manuvinakurike@intel.com; vince@hlt.utdallas.edu; m.poesio@qmul.ac.uk; Michael.Strube@h-its.org; cprose@cs.cmu.edu Abstract such as ARRAU (Poesio et al., 2018; Uryupina et al., 2020). These include, e.g., bridging reference (Clark, 1977; Hou et al., 2018; Hou, 2020; Yu and Poesio, 2020; Kobayashi and Ng, 2021), discourse deixis (Webber, 1991; Marasovi´c et al., 2017; Kolhatkar et al., 2018) or split-antecedent anaphora (Eschenbach et al., 1989; Vala et al., 2016; Zhou and Choi, 2018; Yu et al., 2020b, 2021). There has been interest in other genres apart from news. This includes substantial research on annotating and resolving coreference in biomedical and other scientific domains (Cohen et al., 2017; Lu and Poesio, 2021) as well as in literary documents (Bamman et al., 2020). There are, however, language genres still understudied in the literature on anaphoric reference. Arguably the most important among these is conversational language in dialogue. Anaphora resolution in dialogue requires systems to handle grammatically incorrect"
2021.codi-sharedtask.1,J18-3007,0,0.521559,"chael Strube6 , and Carolyn Rosé1 1 Canegie Mellon Univ., USA; 2 Univ. of Essex, UK; 3 Intel Labs, USA; 4 UT Dallas, USA; 5 Queen Mary Univ., UK; 6 HITS, Germany; 7 AWS AI, Amazon, USA sopankh@amazon.com; j.yu@essex.ac.uk; ramesh.manuvinakurike@intel.com; vince@hlt.utdallas.edu; m.poesio@qmul.ac.uk; Michael.Strube@h-its.org; cprose@cs.cmu.edu Abstract such as ARRAU (Poesio et al., 2018; Uryupina et al., 2020). These include, e.g., bridging reference (Clark, 1977; Hou et al., 2018; Hou, 2020; Yu and Poesio, 2020; Kobayashi and Ng, 2021), discourse deixis (Webber, 1991; Marasovi´c et al., 2017; Kolhatkar et al., 2018) or split-antecedent anaphora (Eschenbach et al., 1989; Vala et al., 2016; Zhou and Choi, 2018; Yu et al., 2020b, 2021). There has been interest in other genres apart from news. This includes substantial research on annotating and resolving coreference in biomedical and other scientific domains (Cohen et al., 2017; Lu and Poesio, 2021) as well as in literary documents (Bamman et al., 2020). There are, however, language genres still understudied in the literature on anaphoric reference. Arguably the most important among these is conversational language in dialogue. Anaphora resolution in dialog"
2021.codi-sharedtask.1,D17-1018,0,0.0879633,"iple tasks focusing individually on these key relations. We discuss the evaluation scripts used to assess the system performance on these tasks, and provide a brief summary of the participating systems and the results obtained across 115 runs from six teams, with most submissions achieving significantly better results than our baseline methods. 1 Introduction The performance of models for single-antecedent anaphora resolution on the aspects of anaphoric interpretation annotated in the standard ONTONOTES dataset (Pradhan et al., 2012) has greatly improved in recent years (Wiseman et al., 2015; Lee et al., 2017, 2018; Kantor and Globerson, 2019; Joshi et al., 2020). So the attention of the community has started to turn to more complex cases of anaphora not found or not properly tested in ONTONOTES. Well-known examples of this trend are work on the cases of anaphora whose interpretation requires some form of commonsense knowledge tested by benchmarks for the Winograd Schema Challenge (Rahman and Ng, 2012; Liu et al., 2017; Sakaguchi et al., 2020), or the pronominal anaphors that cannot be resolved purely using gender, for which benchmarks such as GAP have been developed (Webster et al., 2018). GAP, h"
2021.codi-sharedtask.1,N18-2108,0,0.205145,"Missing"
2021.codi-sharedtask.1,2020.acl-main.132,0,0.135367,"urse Deixis in Dialogue Sopan Khosla1,7∗, Juntao Yu2 , Ramesh Manuvinakurike3 , Vincent Ng4 , Massimo Poesio5 , Michael Strube6 , and Carolyn Rosé1 1 Canegie Mellon Univ., USA; 2 Univ. of Essex, UK; 3 Intel Labs, USA; 4 UT Dallas, USA; 5 Queen Mary Univ., UK; 6 HITS, Germany; 7 AWS AI, Amazon, USA sopankh@amazon.com; j.yu@essex.ac.uk; ramesh.manuvinakurike@intel.com; vince@hlt.utdallas.edu; m.poesio@qmul.ac.uk; Michael.Strube@h-its.org; cprose@cs.cmu.edu Abstract such as ARRAU (Poesio et al., 2018; Uryupina et al., 2020). These include, e.g., bridging reference (Clark, 1977; Hou et al., 2018; Hou, 2020; Yu and Poesio, 2020; Kobayashi and Ng, 2021), discourse deixis (Webber, 1991; Marasovi´c et al., 2017; Kolhatkar et al., 2018) or split-antecedent anaphora (Eschenbach et al., 1989; Vala et al., 2016; Zhou and Choi, 2018; Yu et al., 2020b, 2021). There has been interest in other genres apart from news. This includes substantial research on annotating and resolving coreference in biomedical and other scientific domains (Cohen et al., 2017; Lu and Poesio, 2021) as well as in literary documents (Bamman et al., 2020). There are, however, language genres still understudied in the literature on an"
2021.codi-sharedtask.1,J18-2002,1,0.943726,"ridging, and Discourse Deixis in Dialogue Sopan Khosla1,7∗, Juntao Yu2 , Ramesh Manuvinakurike3 , Vincent Ng4 , Massimo Poesio5 , Michael Strube6 , and Carolyn Rosé1 1 Canegie Mellon Univ., USA; 2 Univ. of Essex, UK; 3 Intel Labs, USA; 4 UT Dallas, USA; 5 Queen Mary Univ., UK; 6 HITS, Germany; 7 AWS AI, Amazon, USA sopankh@amazon.com; j.yu@essex.ac.uk; ramesh.manuvinakurike@intel.com; vince@hlt.utdallas.edu; m.poesio@qmul.ac.uk; Michael.Strube@h-its.org; cprose@cs.cmu.edu Abstract such as ARRAU (Poesio et al., 2018; Uryupina et al., 2020). These include, e.g., bridging reference (Clark, 1977; Hou et al., 2018; Hou, 2020; Yu and Poesio, 2020; Kobayashi and Ng, 2021), discourse deixis (Webber, 1991; Marasovi´c et al., 2017; Kolhatkar et al., 2018) or split-antecedent anaphora (Eschenbach et al., 1989; Vala et al., 2016; Zhou and Choi, 2018; Yu et al., 2020b, 2021). There has been interest in other genres apart from news. This includes substantial research on annotating and resolving coreference in biomedical and other scientific domains (Cohen et al., 2017; Lu and Poesio, 2021) as well as in literary documents (Bamman et al., 2020). There are, however, language genres still understudied in the liter"
2021.codi-sharedtask.1,L16-1145,0,0.382038,"Missing"
2021.codi-sharedtask.1,2020.tacl-1.5,0,0.344926,"ons. We discuss the evaluation scripts used to assess the system performance on these tasks, and provide a brief summary of the participating systems and the results obtained across 115 runs from six teams, with most submissions achieving significantly better results than our baseline methods. 1 Introduction The performance of models for single-antecedent anaphora resolution on the aspects of anaphoric interpretation annotated in the standard ONTONOTES dataset (Pradhan et al., 2012) has greatly improved in recent years (Wiseman et al., 2015; Lee et al., 2017, 2018; Kantor and Globerson, 2019; Joshi et al., 2020). So the attention of the community has started to turn to more complex cases of anaphora not found or not properly tested in ONTONOTES. Well-known examples of this trend are work on the cases of anaphora whose interpretation requires some form of commonsense knowledge tested by benchmarks for the Winograd Schema Challenge (Rahman and Ng, 2012; Liu et al., 2017; Sakaguchi et al., 2020), or the pronominal anaphors that cannot be resolved purely using gender, for which benchmarks such as GAP have been developed (Webster et al., 2018). GAP, however, still focused on identity coreference. In addit"
2021.codi-sharedtask.1,D19-1588,0,0.0733603,"om the end-to-end neural coreference resolution model of Joshi et al. (2020). They recognized singletons, encoded speakers for all turns, and leveraged other out-of-domain datasets during training. Eval DD (Pred) UTD_NLP DFKI 42.70 20.97 35.35 17.43 39.64 23.76 35.43 23.86 38.3 21.5 Baseline 12.12 15.75 18.27 13.55 14.9 Table 5: Performance on Task 3 (Evaluation Phase) – Discourse Deixis (CoNLL Avg. F1) INRIA submitted an end-to-end transformerbased model fine-tuned for the bridging resolution task. They formulated the bridging problem as antecedent selection, and leveraged Lee et al. (2018); Joshi et al. (2019)’s architecture to find the correct antecedent. for mention detection. The resulting mention representation, created from the constituent token representations, is then fed to a pointer-network (Vinyals et al., 2015) based coreference resolution model for clustering. They solved the bridging resolution problem using a machine reading comprehension framework, where they constructed a query for each entity of the form – ""What is related of ENTITY?"". The input of their model is the query and the document (i.e., all utterances of dialogue), and the output is the entity span that is the answer for"
2021.codi-sharedtask.1,2021.crac-1.2,1,0.739024,".edu Abstract such as ARRAU (Poesio et al., 2018; Uryupina et al., 2020). These include, e.g., bridging reference (Clark, 1977; Hou et al., 2018; Hou, 2020; Yu and Poesio, 2020; Kobayashi and Ng, 2021), discourse deixis (Webber, 1991; Marasovi´c et al., 2017; Kolhatkar et al., 2018) or split-antecedent anaphora (Eschenbach et al., 1989; Vala et al., 2016; Zhou and Choi, 2018; Yu et al., 2020b, 2021). There has been interest in other genres apart from news. This includes substantial research on annotating and resolving coreference in biomedical and other scientific domains (Cohen et al., 2017; Lu and Poesio, 2021) as well as in literary documents (Bamman et al., 2020). There are, however, language genres still understudied in the literature on anaphoric reference. Arguably the most important among these is conversational language in dialogue. Anaphora resolution in dialogue requires systems to handle grammatically incorrect language suffering from disfluencies. Dialogue involves much more deictic reference, vaguer anaphoric and discourse deictic reference, speaker grounding of pronouns and long-distance conversation structure. These are complexities that are often missing in news or Wikipedia articles,"
2021.codi-sharedtask.1,P19-1066,0,0.0336391,"vidually on these key relations. We discuss the evaluation scripts used to assess the system performance on these tasks, and provide a brief summary of the participating systems and the results obtained across 115 runs from six teams, with most submissions achieving significantly better results than our baseline methods. 1 Introduction The performance of models for single-antecedent anaphora resolution on the aspects of anaphoric interpretation annotated in the standard ONTONOTES dataset (Pradhan et al., 2012) has greatly improved in recent years (Wiseman et al., 2015; Lee et al., 2017, 2018; Kantor and Globerson, 2019; Joshi et al., 2020). So the attention of the community has started to turn to more complex cases of anaphora not found or not properly tested in ONTONOTES. Well-known examples of this trend are work on the cases of anaphora whose interpretation requires some form of commonsense knowledge tested by benchmarks for the Winograd Schema Challenge (Rahman and Ng, 2012; Liu et al., 2017; Sakaguchi et al., 2020), or the pronominal anaphors that cannot be resolved purely using gender, for which benchmarks such as GAP have been developed (Webster et al., 2018). GAP, however, still focused on identity"
2021.codi-sharedtask.1,D17-1021,0,0.24606,"Missing"
2021.codi-sharedtask.1,2020.coling-main.331,1,0.631559,"atasets: the AMI corpus (Carletta, 2006), the LIGHT corpus (Urbanek et al., 2019), the PERSUASION corpus (Wang et al., 2019) and SWITCHBOARD (Godfrey et al., 1992). For each of these datasets, documents for about 15K tokens were annotated for development, and about the same number of tokens were annotated for testing, according to the ARRAU annotation scheme. Bridging references Annotating — indeed, even identifying — bridging references in a reliable way is difficult, which is one of the reasons why so few large-scale corpora for anaphora include this type of annotation (Poesio et al., 2016; Kobayashi and Ng, 2020). The ARRAU guidelines for bridging anaphora are based on experiments that were started by Vieira and Poesio (Poesio and Vieira, 1998) and continued in the GNOME project (Poesio, 2004). The ARRAU Release 1 guidelines followed the GNOME guidelines, but with an extension and a simplification. Annotators were asked to mark a markable as related to a particular antecedent if it stood to that antecedent in one of the relations identified in GNOME (indeed, the same examples Switchboard SWITCHBOARD8 (Godfrey et al., 1992) is one of the best known dialogue corpora. It consists of 1,155 five-minute spo"
2021.codi-sharedtask.1,J93-2004,0,0.0756707,"ther markable and thus form a singleton coreference chain. Moreover, in ARRAU non-referring markables are manually sub-classified into expletives, predicative, and quantifiers. In addition, all generic references are marked, including premodifiers when the entity referred to is mentioned again, e.g., in the case of ARRAU: Corpus and Annotation Scheme Genres The ARRAU corpus4 (Poesio and Artstein, 2008; Uryupina et al., 2020) was designed to cover a variety of genres. It includes a substantial amount of news text in a sub-corpus called RST, consisting of the entire subset of the Penn Treebank (Marcus et al., 1993) that was annotated in the RST treebank (Carlson et al., 2003). In addition to the news data, ARRAU includes three more sub-corpora. The TRAINS sub-corpus includes all the task-oriented dialogues in the TRAINS-93 corpus5 as well as the pilot dialogues in the socalled TRAINS-91 corpus. The PEAR sub-corpus consists of the complete collection of spoken nar6 The original intention had been to use the soon-to-bereleased ARRAU 3, but as the work on this version was still under way by the time the training data had to be released, ARRAU 2 was used instead–i.e., the exact same version used for the CRA"
2021.codi-sharedtask.1,2021.naacl-main.131,1,0.669111,"osla1,7∗, Juntao Yu2 , Ramesh Manuvinakurike3 , Vincent Ng4 , Massimo Poesio5 , Michael Strube6 , and Carolyn Rosé1 1 Canegie Mellon Univ., USA; 2 Univ. of Essex, UK; 3 Intel Labs, USA; 4 UT Dallas, USA; 5 Queen Mary Univ., UK; 6 HITS, Germany; 7 AWS AI, Amazon, USA sopankh@amazon.com; j.yu@essex.ac.uk; ramesh.manuvinakurike@intel.com; vince@hlt.utdallas.edu; m.poesio@qmul.ac.uk; Michael.Strube@h-its.org; cprose@cs.cmu.edu Abstract such as ARRAU (Poesio et al., 2018; Uryupina et al., 2020). These include, e.g., bridging reference (Clark, 1977; Hou et al., 2018; Hou, 2020; Yu and Poesio, 2020; Kobayashi and Ng, 2021), discourse deixis (Webber, 1991; Marasovi´c et al., 2017; Kolhatkar et al., 2018) or split-antecedent anaphora (Eschenbach et al., 1989; Vala et al., 2016; Zhou and Choi, 2018; Yu et al., 2020b, 2021). There has been interest in other genres apart from news. This includes substantial research on annotating and resolving coreference in biomedical and other scientific domains (Cohen et al., 2017; Lu and Poesio, 2021) as well as in literary documents (Bamman et al., 2020). There are, however, language genres still understudied in the literature on anaphoric reference. Arguably the most important"
2021.codi-sharedtask.1,D14-1056,0,0.107024,"2018). That shared task was articulated around three tasks: identity coreference (including identification of nonreferring expressions), bridging references, and discourse deixis. The organization of the shared task resulted in the development of an extended version of the Coreference Reference Scorer (Pradhan et al., 2014), which also scores non-referring expressions. Separate scorers were developed for bridging reference resolution, carrying out both mention-based evaluation and entity-based evaluation of bridging references, as done by Hou et al. (2018), and for discourse deixis, based on Kolhatkar and Hirst (2014). The present shared task was modeled on that. (5) There was not a moment to be lost: away went Alice like the wind, and was just in time to hear it say, as it turned a corner, ’Oh my ears and whiskers, how late it’s getting!’ She was close behind it when she turned the corner, but the Rabbit was no longer to be seen: she found herself in [a long, low hall, which was lit up by a row of lamps hanging from [the roof]]. There were doors all round the hall, but they were all locked; and when Alice had been all the way down one side and up the other, trying every door, she walked sadly down [the mi"
2021.codi-sharedtask.1,P12-1084,1,0.830857,"es of identity anaphora exist, as well as other types of anaphoric relations that are not annotated in ONTONOTES but are annotated in other corpora. Split-antecedent anaphora In ONTONOTES, plural reference is only marked when the antecedent is mentioned by a single noun phrase. However, split-antecedent anaphors are also possible (Eschenbach et al., 1989; Kamp and Reyle, 1 https://competitions.codalab.org/ competitions/30312 2 just in time to see it pop down a large rabbithole under the hedge. and Martí, 2010), ARRAU (Uryupina et al., 2020), GNOME (Poesio, 2004), GUM (Zeldes, 2017), IS NOTES (Markert et al., 2012), the Prague Dependency Treebank (Nedoluzhko, 2013), and TÜBA DZ (Versley, 2008). (See Poesio et al. (2016) for a more detailed survey and Nedoluzhko et al. (2021) for a more recent, extensive update.) Bridging references There are other forms of anaphoric reference besides identity, and there are now a number of corpora annotating (a subset of) these forms. Possibly the most studied of nonidentity anaphora is bridging reference or associative anaphora (Clark, 1977; Hawkins, 1978; Prince, 1981) as in (5), where bridging reference / associative anaphora the roof refers to an object which is rel"
2021.codi-sharedtask.1,J98-2001,1,0.56338,"CHBOARD (Godfrey et al., 1992). For each of these datasets, documents for about 15K tokens were annotated for development, and about the same number of tokens were annotated for testing, according to the ARRAU annotation scheme. Bridging references Annotating — indeed, even identifying — bridging references in a reliable way is difficult, which is one of the reasons why so few large-scale corpora for anaphora include this type of annotation (Poesio et al., 2016; Kobayashi and Ng, 2020). The ARRAU guidelines for bridging anaphora are based on experiments that were started by Vieira and Poesio (Poesio and Vieira, 1998) and continued in the GNOME project (Poesio, 2004). The ARRAU Release 1 guidelines followed the GNOME guidelines, but with an extension and a simplification. Annotators were asked to mark a markable as related to a particular antecedent if it stood to that antecedent in one of the relations identified in GNOME (indeed, the same examples Switchboard SWITCHBOARD8 (Godfrey et al., 1992) is one of the best known dialogue corpora. It consists of 1,155 five-minute spontaneous telephone conversations between two participants not previously acquainted with each other. In these conversations, callers q"
2021.codi-sharedtask.1,W00-1007,0,0.805051,"nt. . . The statement was the [US]1 ’s government first acknowledgment of what other groups, such as the International Monetary Fund, have been predicting for months. • an undersp-rel relation for ‘obvious cases of bridging that didn’t fit any other category’. Discourse deixis Discourse deixis in its full form is a very complex form of reference, both to annotate (Kolhatkar et al., 2018) and to resolve. Very few anaphoric annotation projects have attempted to annotate discourse deixis in its entirety (Kolhatkar et al., 2018). More typical is a partial annotation, as in (Byron and Allen, 1998; Navarretta, 2000), who annotated pronominal reference to abstract objects; in O NTO N OTES, where event anaphora was marked (Pradhan et al., 2007); and in the work of Kolhatkar and Hirst (2014), which focused on so-called shell nouns. In ARRAU, A coder specifying that a referring expression is discourseold is asked whether its antecedent was introduced using a phrase (markable) or a segment (discourse segment). Coders who choose segment have to mark a sequence of predefined clauses. (9) The Treasury report, which is required annually by a provision of the 1988 trade act, again took South Korea to task for its"
2021.codi-sharedtask.1,P14-2006,1,0.93955,"association such as identity of sense anaphora, etc. (Poesio, 2016). Some of these resources are of a sufficient size to support shared tasks. In particular, the AR RAU corpus was used as the dataset for the Shared Task on Anaphora Resolution with ARRAU in the CRAC 2018 Workshop (Poesio et al., 2018). That shared task was articulated around three tasks: identity coreference (including identification of nonreferring expressions), bridging references, and discourse deixis. The organization of the shared task resulted in the development of an extended version of the Coreference Reference Scorer (Pradhan et al., 2014), which also scores non-referring expressions. Separate scorers were developed for bridging reference resolution, carrying out both mention-based evaluation and entity-based evaluation of bridging references, as done by Hou et al. (2018), and for discourse deixis, based on Kolhatkar and Hirst (2014). The present shared task was modeled on that. (5) There was not a moment to be lost: away went Alice like the wind, and was just in time to hear it say, as it turned a corner, ’Oh my ears and whiskers, how late it’s getting!’ She was close behind it when she turned the corner, but the Rabbit was no"
2021.codi-sharedtask.1,W13-2313,0,0.0303269,"f anaphoric relations that are not annotated in ONTONOTES but are annotated in other corpora. Split-antecedent anaphora In ONTONOTES, plural reference is only marked when the antecedent is mentioned by a single noun phrase. However, split-antecedent anaphors are also possible (Eschenbach et al., 1989; Kamp and Reyle, 1 https://competitions.codalab.org/ competitions/30312 2 just in time to see it pop down a large rabbithole under the hedge. and Martí, 2010), ARRAU (Uryupina et al., 2020), GNOME (Poesio, 2004), GUM (Zeldes, 2017), IS NOTES (Markert et al., 2012), the Prague Dependency Treebank (Nedoluzhko, 2013), and TÜBA DZ (Versley, 2008). (See Poesio et al. (2016) for a more detailed survey and Nedoluzhko et al. (2021) for a more recent, extensive update.) Bridging references There are other forms of anaphoric reference besides identity, and there are now a number of corpora annotating (a subset of) these forms. Possibly the most studied of nonidentity anaphora is bridging reference or associative anaphora (Clark, 1977; Hawkins, 1978; Prince, 1981) as in (5), where bridging reference / associative anaphora the roof refers to an object which is related to / associated with, but not identical to, th"
2021.codi-sharedtask.1,W12-4501,0,0.677797,"ric relations: identity, bridging references and discourse deixis, we defined multiple tasks focusing individually on these key relations. We discuss the evaluation scripts used to assess the system performance on these tasks, and provide a brief summary of the participating systems and the results obtained across 115 runs from six teams, with most submissions achieving significantly better results than our baseline methods. 1 Introduction The performance of models for single-antecedent anaphora resolution on the aspects of anaphoric interpretation annotated in the standard ONTONOTES dataset (Pradhan et al., 2012) has greatly improved in recent years (Wiseman et al., 2015; Lee et al., 2017, 2018; Kantor and Globerson, 2019; Joshi et al., 2020). So the attention of the community has started to turn to more complex cases of anaphora not found or not properly tested in ONTONOTES. Well-known examples of this trend are work on the cases of anaphora whose interpretation requires some form of commonsense knowledge tested by benchmarks for the Winograd Schema Challenge (Rahman and Ng, 2012; Liu et al., 2017; Sakaguchi et al., 2020), or the pronominal anaphors that cannot be resolved purely using gender, for wh"
2021.codi-sharedtask.1,nissim-etal-2004-annotation,0,0.152087,"2) is one of the best known dialogue corpora. It consists of 1,155 five-minute spontaneous telephone conversations between two participants not previously acquainted with each other. In these conversations, callers question receivers on provided topics, such as child care, recycling, and news media. 440 speakers participate in these 1,155 conversations, producing 221,616 utterances. It was 7 Identity anaphora also includes split antecedent plural anaphoric reference. 8 https://catalog.ldc.upenn.edu/ LDC97S62 5 annotated for dialogue acts by Stolcke et al. (1997)9 and for information status by Nissim et al. (2004). exactly the same MMAX style – and by the same two annotators from the DALI team at Queen Mary University and University of Essex, Dr. Maris Camilleri and Dr. Paloma Carretero Garcia, who annotated and checked ARRAU Release 3, which is currently being prepared for release. However, due to time constraints, each document was only annotated by a single annotator, with spot checks carried out by the other annotator and Massimo Poesio (in ARRAU 3 each document was looked at by both annotators, and most documents were also independently checked by Massimo Poesio). To prepare the data for the share"
2021.codi-sharedtask.1,W04-0210,1,0.761359,"j didn’t fit [her]i . However, many other types of identity anaphora exist, as well as other types of anaphoric relations that are not annotated in ONTONOTES but are annotated in other corpora. Split-antecedent anaphora In ONTONOTES, plural reference is only marked when the antecedent is mentioned by a single noun phrase. However, split-antecedent anaphors are also possible (Eschenbach et al., 1989; Kamp and Reyle, 1 https://competitions.codalab.org/ competitions/30312 2 just in time to see it pop down a large rabbithole under the hedge. and Martí, 2010), ARRAU (Uryupina et al., 2020), GNOME (Poesio, 2004), GUM (Zeldes, 2017), IS NOTES (Markert et al., 2012), the Prague Dependency Treebank (Nedoluzhko, 2013), and TÜBA DZ (Versley, 2008). (See Poesio et al. (2016) for a more detailed survey and Nedoluzhko et al. (2021) for a more recent, extensive update.) Bridging references There are other forms of anaphoric reference besides identity, and there are now a number of corpora annotating (a subset of) these forms. Possibly the most studied of nonidentity anaphora is bridging reference or associative anaphora (Clark, 1977; Hawkins, 1978; Prince, 1981) as in (5), where bridging reference / associati"
2021.codi-sharedtask.1,D12-1071,1,0.763158,"e-antecedent anaphora resolution on the aspects of anaphoric interpretation annotated in the standard ONTONOTES dataset (Pradhan et al., 2012) has greatly improved in recent years (Wiseman et al., 2015; Lee et al., 2017, 2018; Kantor and Globerson, 2019; Joshi et al., 2020). So the attention of the community has started to turn to more complex cases of anaphora not found or not properly tested in ONTONOTES. Well-known examples of this trend are work on the cases of anaphora whose interpretation requires some form of commonsense knowledge tested by benchmarks for the Winograd Schema Challenge (Rahman and Ng, 2012; Liu et al., 2017; Sakaguchi et al., 2020), or the pronominal anaphors that cannot be resolved purely using gender, for which benchmarks such as GAP have been developed (Webster et al., 2018). GAP, however, still focused on identity coreference. In addition, more research has been carried out on aspects of anaphoric interpretation that go beyond identity anaphora but are covered by datasets ∗ Work done when the author was a student at CMU 1 Proceedings of the CODI-CRAC 2021 Shared Task on Anaphora, Bridging, and Discourse Deixis in Dialogue, pages 1–15 Punta Cana, Dominican Republic, November"
2021.codi-sharedtask.1,poesio-artstein-2008-anaphoric,1,0.800941,"lopment in the trial phase, whereas all other datasets were used for training.6 A limitation of most resources annotated for anaphora is that they mostly focus on expository text. The one substantial dataset of anaphoric relations in dialogue is ANCOR for French (Muzerelle et al., 2014), in which identity and bridging anaphora are annotated. Among the small number of English corpora that cover dialogue include ONTONOTES (Pradhan et al., 2012), which contains a small number of conversations annotated for identity anaphora and a small subtype of discourse deixis (as discussed earlier). ARRAU’s (Poesio and Artstein, 2008; Uryupina et al., 2020) TRAINS sub-corpus consists of task-oriented dialogues for identity, bridging, and discourse deixis. We include TRAINS in CODI - CRAC 2021 training data. The more recently released ONTOGUM (Zhu et al., 2021) builds upon the ONTONOTES schema and adds several new genres (including more spoken data) to the ONTONOTES family. Both identity anaphora and bridging are annotated in the dataset. 3 Annotation scheme The original annotation scheme used for Release 1 (Poesio and Artstein, 2008) is distributed with the dataset and is also available from the ARRAU corpus page. For the"
2021.codi-sharedtask.1,N19-1176,1,0.849468,"tings, and evaluation metrics in Section 4, and submission details in Section 5. This is followed by details of the baselines in Section 6 and participating systems in Section 7. We present a discussion of the performance of the systems on different tasks and sub-corpora in Section 8, and finally conclude this paper in Section 9. 2 1993), as in (2). These are also cases of plural identity coreference, but to sets composed of two or more entities introduced by separate noun phrases. Such references are annotated in, e.g., ARRAU (Uryupina et al., 2020), GUM (Zeldes, 2017) and Phrase Detectives (Poesio et al., 2019). (2) [John]1 met [Mary]2 . [He]1 greeted [her]2 . [They]1,2 went to the movies. Discourse deixis In ONTONOTES, event anaphora, a subtype of discourse deixis (Webber, 1991; Kolhatkar et al., 2018) is marked, as in (3) (where [that] arguably refers to the event of a white rabbit with pink ears running past Alice) but not the whole range of abstract anaphora, illustrated by, e.g., (4), where again arguably [this] refers to the fact that the Rabbit was able to talk. (Both examples from the Phrase Detectives corpus (Poesio et al., 2019).) (3) So she was considering in her own mind (as well as she"
2021.codi-sharedtask.1,2020.emnlp-main.686,0,0.790191,"f 55 individual participants registered for the CODI - CRAC 2021 shared task on CodaLab.21 Among them, five teams submitted results for Task 1, three submitted results for Task 2, and two submitted results for Task 3. Teams UTD_NLP, KU_NLP, DFKI_TalkingRobots, Emory_NLP, and INRIA submitted system description papers. We summarize their approaches below (and in Table 2): Baselines UTD_NLP participated in all three tasks. For identity anaphora, they deployed a pipeline architecture consisting of a mention detection component and an entity coreference component. The coreference component extends Xu and Choi (2020)’s implementation of Lee et al. (2018) by modifying the objective so that it can output singleton clusters, and enforces dialogue-specific constraints. They setup a similar architecture for discourse deixis. However, they slightly modified the objective function in Xu and Choi (2020) by classifying each span as a candidate anaphor, a candidate antecedent, or a non-mention in the mention detection stage, and resolving only candidate anaphors to candidate antecedents later. The team used a multi-pass sieve approach for bridging resolution to target same-head bridging links, with Yu and Poesio (2"
2021.codi-sharedtask.1,2020.lrec-1.1,1,0.560581,"ities that are often missing in news or Wikipedia articles, which form a large chunk of current datasets for coreference resolution. There has been some research on coreference in dialogue (Byron, 2002; Eckert and Strube, 2001; Müller, 2008), but very limited in scope (primarily related to pronominal interpretation), due to the lack of suitable corpora. The one language for which substantial corpora of coreference in dialogue exist is French: the ANCOR corpus (Muzerelle et al., 2014) has enabled the development of an end-to-end neural model for coreference interpretation in dialogue by Grobol (2020). For English, the one resource we are aware of fully annotated for anaphoric reference is the TRAINS corpora included in the ARRAU corpus (Uryupina et al., 2020). The objective of the CODI - CRAC 2021 Shared In this paper, we provide an overview of the CODI - CRAC 2021 Shared Task. The shared task focuses on detecting anaphoric relations in different genres of conversations. Using five conversational datasets, four of which have been newly annotated with a wide range of anaphoric relations: identity, bridging references and discourse deixis, we defined multiple tasks focusing individually on"
2021.codi-sharedtask.1,2020.coling-main.538,1,0.858079,"A; 5 Queen Mary Univ., UK; 6 HITS, Germany; 7 AWS AI, Amazon, USA sopankh@amazon.com; j.yu@essex.ac.uk; ramesh.manuvinakurike@intel.com; vince@hlt.utdallas.edu; m.poesio@qmul.ac.uk; Michael.Strube@h-its.org; cprose@cs.cmu.edu Abstract such as ARRAU (Poesio et al., 2018; Uryupina et al., 2020). These include, e.g., bridging reference (Clark, 1977; Hou et al., 2018; Hou, 2020; Yu and Poesio, 2020; Kobayashi and Ng, 2021), discourse deixis (Webber, 1991; Marasovi´c et al., 2017; Kolhatkar et al., 2018) or split-antecedent anaphora (Eschenbach et al., 1989; Vala et al., 2016; Zhou and Choi, 2018; Yu et al., 2020b, 2021). There has been interest in other genres apart from news. This includes substantial research on annotating and resolving coreference in biomedical and other scientific domains (Cohen et al., 2017; Lu and Poesio, 2021) as well as in literary documents (Bamman et al., 2020). There are, however, language genres still understudied in the literature on anaphoric reference. Arguably the most important among these is conversational language in dialogue. Anaphora resolution in dialogue requires systems to handle grammatically incorrect language suffering from disfluencies. Dialogue involves m"
2021.codi-sharedtask.1,D19-1062,0,0.248851,"duced (antecedent). For discourse-old mentions, an antecedent can be identified, either of type phrase (if the antecedent was introduced using a nominal markable) or segment (not introduced by a nominal markable, for discourse deixis).7 In addition, referring NPs can be marked as related to a previously mentioned discourse entity to identify them as examples of associative (bridging) anaphora. 3.2 New Data The annotated corpus prepared for the CODI - CRAC 2021 shared task consists of conversations from four well-known conversational datasets: the AMI corpus (Carletta, 2006), the LIGHT corpus (Urbanek et al., 2019), the PERSUASION corpus (Wang et al., 2019) and SWITCHBOARD (Godfrey et al., 1992). For each of these datasets, documents for about 15K tokens were annotated for development, and about the same number of tokens were annotated for testing, according to the ARRAU annotation scheme. Bridging references Annotating — indeed, even identifying — bridging references in a reliable way is difficult, which is one of the reasons why so few large-scale corpora for anaphora include this type of annotation (Poesio et al., 2016; Kobayashi and Ng, 2020). The ARRAU guidelines for bridging anaphora are based on"
2021.codi-sharedtask.1,2021.naacl-main.329,1,0.738872,"Missing"
2021.codi-sharedtask.1,P16-1216,0,0.0628664,"Missing"
2021.codi-sharedtask.1,2020.coling-main.315,1,0.910842,"in Dialogue Sopan Khosla1,7∗, Juntao Yu2 , Ramesh Manuvinakurike3 , Vincent Ng4 , Massimo Poesio5 , Michael Strube6 , and Carolyn Rosé1 1 Canegie Mellon Univ., USA; 2 Univ. of Essex, UK; 3 Intel Labs, USA; 4 UT Dallas, USA; 5 Queen Mary Univ., UK; 6 HITS, Germany; 7 AWS AI, Amazon, USA sopankh@amazon.com; j.yu@essex.ac.uk; ramesh.manuvinakurike@intel.com; vince@hlt.utdallas.edu; m.poesio@qmul.ac.uk; Michael.Strube@h-its.org; cprose@cs.cmu.edu Abstract such as ARRAU (Poesio et al., 2018; Uryupina et al., 2020). These include, e.g., bridging reference (Clark, 1977; Hou et al., 2018; Hou, 2020; Yu and Poesio, 2020; Kobayashi and Ng, 2021), discourse deixis (Webber, 1991; Marasovi´c et al., 2017; Kolhatkar et al., 2018) or split-antecedent anaphora (Eschenbach et al., 1989; Vala et al., 2016; Zhou and Choi, 2018; Yu et al., 2020b, 2021). There has been interest in other genres apart from news. This includes substantial research on annotating and resolving coreference in biomedical and other scientific domains (Cohen et al., 2017; Lu and Poesio, 2021) as well as in literary documents (Bamman et al., 2020). There are, however, language genres still understudied in the literature on anaphoric reference. Ar"
2021.codi-sharedtask.1,C18-1003,0,0.0784215,"USA; 4 UT Dallas, USA; 5 Queen Mary Univ., UK; 6 HITS, Germany; 7 AWS AI, Amazon, USA sopankh@amazon.com; j.yu@essex.ac.uk; ramesh.manuvinakurike@intel.com; vince@hlt.utdallas.edu; m.poesio@qmul.ac.uk; Michael.Strube@h-its.org; cprose@cs.cmu.edu Abstract such as ARRAU (Poesio et al., 2018; Uryupina et al., 2020). These include, e.g., bridging reference (Clark, 1977; Hou et al., 2018; Hou, 2020; Yu and Poesio, 2020; Kobayashi and Ng, 2021), discourse deixis (Webber, 1991; Marasovi´c et al., 2017; Kolhatkar et al., 2018) or split-antecedent anaphora (Eschenbach et al., 1989; Vala et al., 2016; Zhou and Choi, 2018; Yu et al., 2020b, 2021). There has been interest in other genres apart from news. This includes substantial research on annotating and resolving coreference in biomedical and other scientific domains (Cohen et al., 2017; Lu and Poesio, 2021) as well as in literary documents (Bamman et al., 2020). There are, however, language genres still understudied in the literature on anaphoric reference. Arguably the most important among these is conversational language in dialogue. Anaphora resolution in dialogue requires systems to handle grammatically incorrect language suffering from disfluencies. Di"
2021.codi-sharedtask.1,P19-1566,0,0.250523,", an antecedent can be identified, either of type phrase (if the antecedent was introduced using a nominal markable) or segment (not introduced by a nominal markable, for discourse deixis).7 In addition, referring NPs can be marked as related to a previously mentioned discourse entity to identify them as examples of associative (bridging) anaphora. 3.2 New Data The annotated corpus prepared for the CODI - CRAC 2021 shared task consists of conversations from four well-known conversational datasets: the AMI corpus (Carletta, 2006), the LIGHT corpus (Urbanek et al., 2019), the PERSUASION corpus (Wang et al., 2019) and SWITCHBOARD (Godfrey et al., 1992). For each of these datasets, documents for about 15K tokens were annotated for development, and about the same number of tokens were annotated for testing, according to the ARRAU annotation scheme. Bridging references Annotating — indeed, even identifying — bridging references in a reliable way is difficult, which is one of the reasons why so few large-scale corpora for anaphora include this type of annotation (Poesio et al., 2016; Kobayashi and Ng, 2020). The ARRAU guidelines for bridging anaphora are based on experiments that were started by Vieira and"
2021.codi-sharedtask.1,2021.acl-short.59,0,0.0207201,"gue is ANCOR for French (Muzerelle et al., 2014), in which identity and bridging anaphora are annotated. Among the small number of English corpora that cover dialogue include ONTONOTES (Pradhan et al., 2012), which contains a small number of conversations annotated for identity anaphora and a small subtype of discourse deixis (as discussed earlier). ARRAU’s (Poesio and Artstein, 2008; Uryupina et al., 2020) TRAINS sub-corpus consists of task-oriented dialogues for identity, bridging, and discourse deixis. We include TRAINS in CODI - CRAC 2021 training data. The more recently released ONTOGUM (Zhu et al., 2021) builds upon the ONTONOTES schema and adds several new genres (including more spoken data) to the ONTONOTES family. Both identity anaphora and bridging are annotated in the dataset. 3 Annotation scheme The original annotation scheme used for Release 1 (Poesio and Artstein, 2008) is distributed with the dataset and is also available from the ARRAU corpus page. For the second release (Uryupina et al., 2020), the guidelines for bridging were extended and genericity was also annotated using the GNOME guidelines, but a complete new manual was not produced. However, a fairly extensive description ca"
2021.codi-sharedtask.1,Q18-1042,0,0.0216415,"t al., 2015; Lee et al., 2017, 2018; Kantor and Globerson, 2019; Joshi et al., 2020). So the attention of the community has started to turn to more complex cases of anaphora not found or not properly tested in ONTONOTES. Well-known examples of this trend are work on the cases of anaphora whose interpretation requires some form of commonsense knowledge tested by benchmarks for the Winograd Schema Challenge (Rahman and Ng, 2012; Liu et al., 2017; Sakaguchi et al., 2020), or the pronominal anaphors that cannot be resolved purely using gender, for which benchmarks such as GAP have been developed (Webster et al., 2018). GAP, however, still focused on identity coreference. In addition, more research has been carried out on aspects of anaphoric interpretation that go beyond identity anaphora but are covered by datasets ∗ Work done when the author was a student at CMU 1 Proceedings of the CODI-CRAC 2021 Shared Task on Anaphora, Bridging, and Discourse Deixis in Dialogue, pages 1–15 Punta Cana, Dominican Republic, November 10, 2021. ©2021 Association for Computational Linguistics Task in Anaphora Resolution in Dialogue1 was to provide participants with the opportunity to develop automated approaches for corefer"
2021.codi-sharedtask.1,P15-1137,0,0.0532448,"eixis, we defined multiple tasks focusing individually on these key relations. We discuss the evaluation scripts used to assess the system performance on these tasks, and provide a brief summary of the participating systems and the results obtained across 115 runs from six teams, with most submissions achieving significantly better results than our baseline methods. 1 Introduction The performance of models for single-antecedent anaphora resolution on the aspects of anaphoric interpretation annotated in the standard ONTONOTES dataset (Pradhan et al., 2012) has greatly improved in recent years (Wiseman et al., 2015; Lee et al., 2017, 2018; Kantor and Globerson, 2019; Joshi et al., 2020). So the attention of the community has started to turn to more complex cases of anaphora not found or not properly tested in ONTONOTES. Well-known examples of this trend are work on the cases of anaphora whose interpretation requires some form of commonsense knowledge tested by benchmarks for the Winograd Schema Challenge (Rahman and Ng, 2012; Liu et al., 2017; Sakaguchi et al., 2020), or the pronominal anaphors that cannot be resolved purely using gender, for which benchmarks such as GAP have been developed (Webster et"
2021.crac-1.2,N19-1423,0,0.0173507,"dical text can be linked to a Concept Unique Identifier (CUI) in UMLS, and each CUI has a synonym set. SAPBERT (Liu et al., 2021), UMLSBERT (Michalopoulos et al., 2021) and BIOSYN (Sung et al., 2020) further pre-trained PubMedBERT, Bio_Clinical BERT and BioBERT on UMLS synonyms, using multi-similarity loss, multi-label loss and synonym marginalization algorithm respectively. In addition to synonym knowledge, Clinical KBBERT (Hao et al., 2020) injects UMLS relation knowledge into BioBERT. Whereas CODER (Yuan et al., 2020) learns both synonym and relation knowledge based on PubMedBERT or mBERT (Devlin et al., 2019) via contrastive learning. Also, some research focus on fusing the UMLS entity embeddings with contextual embeddings to improve biomedical PLMs (He et al., 2020; Fei et al., 2021; Yuan et al., 2021). This paper selected some of the models above to evaluate the ability of biomedical-specific representation for biomedical coreference task, detailed in Section 6. Biomedical Language Representation Models The news domain and the biomedical domain are different in a number of respects, such as markable types. Some authors have argued that biomedical domain knowledge is the key to bridging the gap ("
2021.crac-1.2,W19-1909,0,0.0643986,"Missing"
2021.crac-1.2,W11-0210,0,0.0906035,"Missing"
2021.crac-1.2,D19-1371,0,0.0938729,") is the first transformerbased biomedical PLM, pre-trained on PubMed abstracts and PubMed Central full-text articles. ClinicalBERT and Bio_ClinicalBERT (Alsentzer et al., 2019) are pre-trained on MIMIC-III Clinical Notes, whereas BlueBERT (Peng et al., 2019) uses both PubMed and MIMIC-III for pre-training. All these models are pre-trained based on general BERT, except Bio_ClinicalBERT which is initialized from BioBERT. In addition to initializing from general BERT, some biomedical PLMs are directly pre-trained on biomedical text from scratch and use domainspecific custom vocabulary. SciBERT (Beltagy et al., 2019) is pre-trained on biomedical and computer science papers from scratch and achieved 5 5.1 Coreference Models for the Biomedical Domain Rule-based models Early approaches to biomedical coreference resolution are primarily rule-based. These models rely on syntactic parsers to extract hand-crafted features and rules. 15 Nguyen et al. (2012) implemented a protein coreference system that makes use of syntactic information from the parser output, and proteinindicated information. The results showed that domain-specific semantic information is important for coreference resolution. Miwa et al. (2012)"
2021.crac-1.2,2020.coling-main.57,0,0.018203,"biomedical knowledge bases, such as UMLS (Bodenreider, 2004). Several models enhance biomedical PLMs by integrating synonym knowledge from UMLS. Each mention in the biomedical text can be linked to a Concept Unique Identifier (CUI) in UMLS, and each CUI has a synonym set. SAPBERT (Liu et al., 2021), UMLSBERT (Michalopoulos et al., 2021) and BIOSYN (Sung et al., 2020) further pre-trained PubMedBERT, Bio_Clinical BERT and BioBERT on UMLS synonyms, using multi-similarity loss, multi-label loss and synonym marginalization algorithm respectively. In addition to synonym knowledge, Clinical KBBERT (Hao et al., 2020) injects UMLS relation knowledge into BioBERT. Whereas CODER (Yuan et al., 2020) learns both synonym and relation knowledge based on PubMedBERT or mBERT (Devlin et al., 2019) via contrastive learning. Also, some research focus on fusing the UMLS entity embeddings with contextual embeddings to improve biomedical PLMs (He et al., 2020; Fei et al., 2021; Yuan et al., 2021). This paper selected some of the models above to evaluate the ability of biomedical-specific representation for biomedical coreference task, detailed in Section 6. Biomedical Language Representation Models The news domain and t"
2021.crac-1.2,D17-1018,0,0.130442,"Missing"
2021.crac-1.2,2020.findings-emnlp.207,0,0.0316135,"Missing"
2021.crac-1.2,N18-2108,0,0.100793,"Missing"
2021.crac-1.2,N06-2015,0,0.2642,"Missing"
2021.crac-1.2,2020.tacl-1.5,0,0.123975,"e Biomedical Language Representation Models for Coreference In Section 4, we introduced a series of biomedical language representation models. To investigate the ability of these models for biomedical coreference task, we conduct experiments to evaluate these models on CRAFT-CR dataset. 6.1 6.2 We apply two types of PLM to replace LSTM encoder respectively: Biomedical PLMs: to enhance the baseline model with biomedical domain knowledge, several biomedical PLMs are selected, including models pre-training on biomedical corpora or integrating biomedical knowledge bases. SpanBERT: since SpanBERT (Joshi et al., 2020) is a state-of-the-art coreference resolution model in the general domain, we also evaluate SpanBERT and general BERT (Joshi et al., 2019) on biomedical coreference. Since CRAFT-CR is a more challenging biomedical coreference dataset consisting of full-text articles, we choose CRAFT-CR to fine-tune and evaluate these models. The details are introduced in Section 7.2. Baseline model We employ the higher-order coreference model (Lee et al., 2018) as the baseline model, but use different pre-trained language models with BERT architecture to replace LSTM encoder. The goal is to learn a distributio"
2021.crac-1.2,D19-1588,0,0.217353,"els. To investigate the ability of these models for biomedical coreference task, we conduct experiments to evaluate these models on CRAFT-CR dataset. 6.1 6.2 We apply two types of PLM to replace LSTM encoder respectively: Biomedical PLMs: to enhance the baseline model with biomedical domain knowledge, several biomedical PLMs are selected, including models pre-training on biomedical corpora or integrating biomedical knowledge bases. SpanBERT: since SpanBERT (Joshi et al., 2020) is a state-of-the-art coreference resolution model in the general domain, we also evaluate SpanBERT and general BERT (Joshi et al., 2019) on biomedical coreference. Since CRAFT-CR is a more challenging biomedical coreference dataset consisting of full-text articles, we choose CRAFT-CR to fine-tune and evaluate these models. The details are introduced in Section 7.2. Baseline model We employ the higher-order coreference model (Lee et al., 2018) as the baseline model, but use different pre-trained language models with BERT architecture to replace LSTM encoder. The goal is to learn a distribution P (yi ) over possible antecedents Y (i) for each span i: P (yi ) = P es(i,yi ) s(i,y 0 ) y 0 ∈Y (i) e (1) where s(i, j) is a pairwise sc"
2021.crac-1.2,O04-1011,0,0.152684,", the mention this protein refers to the antecedent the HSPB2 gene product. To understand the coreference relation, we need the background knowledge that proteins are fundamental encoded by genes. In example b), to correctly resolve the coreference relation between these different biomedical entities, the biomedical-domain knowledge that cholinergic starburst amacrine cell is a kind of interneuron and belongs to retinal neuron are required. A large number of coreference models for the biomedical domain have already been proposed, from rule-based models (Castano et al. 2002, Kim and Park 2004, Lin and Liang 2004, Nguyen et al. 2012, Miwa et al. 2012, Kilicoglu and DemnerFushman 2016, Li et al. 2018), machine learning12 Proceedings of the 4th Workshop on Computational Models of Reference, Anaphora and Coreference (CRAC 2021), pages 12–23 c Punta Cana, Dominican Republic, November 10–11, 2021. 2021 Association for Computational Linguistics based models (Yang et al. 2004, Torii and VijayShanker 2005, Su et al. 2008, Gasperin 2009, Kim et al. 2011) to recent deep learning-based models (Trieu et al. 2018, Trieu et al. 2019, Li et al. 2021). These models usually integrate biomedical specific information, t"
2021.crac-1.2,2021.naacl-main.334,0,0.0361537,"gatron (Shoeybi et al., 2019) architecture. It also investigated the effect of vocabulary and corpora domain on the performance of biomedical tasks. 4.2 Although the biomedical PLMs, such as BioBERT, have achieved good performance on many biomedical tasks, however, these models can be further enhanced by integrating biomedical knowledge bases, such as UMLS (Bodenreider, 2004). Several models enhance biomedical PLMs by integrating synonym knowledge from UMLS. Each mention in the biomedical text can be linked to a Concept Unique Identifier (CUI) in UMLS, and each CUI has a synonym set. SAPBERT (Liu et al., 2021), UMLSBERT (Michalopoulos et al., 2021) and BIOSYN (Sung et al., 2020) further pre-trained PubMedBERT, Bio_Clinical BERT and BioBERT on UMLS synonyms, using multi-similarity loss, multi-label loss and synonym marginalization algorithm respectively. In addition to synonym knowledge, Clinical KBBERT (Hao et al., 2020) injects UMLS relation knowledge into BioBERT. Whereas CODER (Yuan et al., 2020) learns both synonym and relation knowledge based on PubMedBERT or mBERT (Devlin et al., 2019) via contrastive learning. Also, some research focus on fusing the UMLS entity embeddings with contextual emb"
2021.crac-1.2,W04-0711,0,0.0600654,"017). In example a), the mention this protein refers to the antecedent the HSPB2 gene product. To understand the coreference relation, we need the background knowledge that proteins are fundamental encoded by genes. In example b), to correctly resolve the coreference relation between these different biomedical entities, the biomedical-domain knowledge that cholinergic starburst amacrine cell is a kind of interneuron and belongs to retinal neuron are required. A large number of coreference models for the biomedical domain have already been proposed, from rule-based models (Castano et al. 2002, Kim and Park 2004, Lin and Liang 2004, Nguyen et al. 2012, Miwa et al. 2012, Kilicoglu and DemnerFushman 2016, Li et al. 2018), machine learning12 Proceedings of the 4th Workshop on Computational Models of Reference, Anaphora and Coreference (CRAC 2021), pages 12–23 c Punta Cana, Dominican Republic, November 10–11, 2021. 2021 Association for Computational Linguistics based models (Yang et al. 2004, Torii and VijayShanker 2005, Su et al. 2008, Gasperin 2009, Kim et al. 2011) to recent deep learning-based models (Trieu et al. 2018, Trieu et al. 2019, Li et al. 2021). These models usually integrate biomedical spe"
2021.crac-1.2,2021.naacl-main.139,0,0.0269507,"architecture. It also investigated the effect of vocabulary and corpora domain on the performance of biomedical tasks. 4.2 Although the biomedical PLMs, such as BioBERT, have achieved good performance on many biomedical tasks, however, these models can be further enhanced by integrating biomedical knowledge bases, such as UMLS (Bodenreider, 2004). Several models enhance biomedical PLMs by integrating synonym knowledge from UMLS. Each mention in the biomedical text can be linked to a Concept Unique Identifier (CUI) in UMLS, and each CUI has a synonym set. SAPBERT (Liu et al., 2021), UMLSBERT (Michalopoulos et al., 2021) and BIOSYN (Sung et al., 2020) further pre-trained PubMedBERT, Bio_Clinical BERT and BioBERT on UMLS synonyms, using multi-similarity loss, multi-label loss and synonym marginalization algorithm respectively. In addition to synonym knowledge, Clinical KBBERT (Hao et al., 2020) injects UMLS relation knowledge into BioBERT. Whereas CODER (Yuan et al., 2020) learns both synonym and relation knowledge based on PubMedBERT or mBERT (Devlin et al., 2019) via contrastive learning. Also, some research focus on fusing the UMLS entity embeddings with contextual embeddings to improve biomedical PLMs (He"
2021.crac-1.2,W11-1813,0,0.0852652,"Missing"
2021.crac-1.2,W11-1811,0,0.135169,"2002, Miwa et al. 2012). Biomedical coreference resolution has become an essential task to support the discovery of complex information by identifying coreference links in biomedical texts. In recent years in particular, biomedical coreference resolution has attracted a great deal of attention due both to its great potential for application, and to its theoretical interest e.g., as an application of knowledge embeddings and entity linking. Several biomedical coreference corpora have been made available, especially for protein coreference which is a supporting task for BioNLP 2011 shared task (Nguyen et al., 2011). Biomedical coreference is quite different from the general domain coreference, such as different markable types. Therefore, domain-specific knowledge is important for bridging the gap. Figure 1 shows two examples of biomedical coreference in the CRAFT-CR dataset (Cohen et al., 2017). In example a), the mention this protein refers to the antecedent the HSPB2 gene product. To understand the coreference relation, we need the background knowledge that proteins are fundamental encoded by genes. In example b), to correctly resolve the coreference relation between these different biomedical entitie"
2021.crac-1.2,2020.acl-main.335,0,0.0171025,"effect of vocabulary and corpora domain on the performance of biomedical tasks. 4.2 Although the biomedical PLMs, such as BioBERT, have achieved good performance on many biomedical tasks, however, these models can be further enhanced by integrating biomedical knowledge bases, such as UMLS (Bodenreider, 2004). Several models enhance biomedical PLMs by integrating synonym knowledge from UMLS. Each mention in the biomedical text can be linked to a Concept Unique Identifier (CUI) in UMLS, and each CUI has a synonym set. SAPBERT (Liu et al., 2021), UMLSBERT (Michalopoulos et al., 2021) and BIOSYN (Sung et al., 2020) further pre-trained PubMedBERT, Bio_Clinical BERT and BioBERT on UMLS synonyms, using multi-similarity loss, multi-label loss and synonym marginalization algorithm respectively. In addition to synonym knowledge, Clinical KBBERT (Hao et al., 2020) injects UMLS relation knowledge into BioBERT. Whereas CODER (Yuan et al., 2020) learns both synonym and relation knowledge based on PubMedBERT or mBERT (Devlin et al., 2019) via contrastive learning. Also, some research focus on fusing the UMLS entity embeddings with contextual embeddings to improve biomedical PLMs (He et al., 2020; Fei et al., 2021;"
2021.crac-1.2,W19-5006,0,0.0244184,"ss of large-scale pre-training language models (PLMs) in the general domain, several biomedical-domain PLMs have been developed in recent years by pre-training on large-scale biomedical corpora. Most biomedical PLMs conduct continual pretraining of the general domain PLMs and still use vocabulary trained on the general domain text. BioBERT (Lee et al., 2020) is the first transformerbased biomedical PLM, pre-trained on PubMed abstracts and PubMed Central full-text articles. ClinicalBERT and Bio_ClinicalBERT (Alsentzer et al., 2019) are pre-trained on MIMIC-III Clinical Notes, whereas BlueBERT (Peng et al., 2019) uses both PubMed and MIMIC-III for pre-training. All these models are pre-trained based on general BERT, except Bio_ClinicalBERT which is initialized from BioBERT. In addition to initializing from general BERT, some biomedical PLMs are directly pre-trained on biomedical text from scratch and use domainspecific custom vocabulary. SciBERT (Beltagy et al., 2019) is pre-trained on biomedical and computer science papers from scratch and achieved 5 5.1 Coreference Models for the Biomedical Domain Rule-based models Early approaches to biomedical coreference resolution are primarily rule-based. These"
2021.crac-1.2,I05-2038,0,0.0500817,"Missing"
2021.crac-1.2,P14-2006,0,0.342181,"Missing"
2021.crac-1.2,W02-0312,0,0.207667,"Missing"
2021.crac-1.2,W18-2324,0,0.139167,"al domain have already been proposed, from rule-based models (Castano et al. 2002, Kim and Park 2004, Lin and Liang 2004, Nguyen et al. 2012, Miwa et al. 2012, Kilicoglu and DemnerFushman 2016, Li et al. 2018), machine learning12 Proceedings of the 4th Workshop on Computational Models of Reference, Anaphora and Coreference (CRAC 2021), pages 12–23 c Punta Cana, Dominican Republic, November 10–11, 2021. 2021 Association for Computational Linguistics based models (Yang et al. 2004, Torii and VijayShanker 2005, Su et al. 2008, Gasperin 2009, Kim et al. 2011) to recent deep learning-based models (Trieu et al. 2018, Trieu et al. 2019, Li et al. 2021). These models usually integrate biomedical specific information, typically specific rules, pre-trained embeddings and features. This paper reviews and analyses coreference datasets and models for the biomedical domain, as well as recent biomedical language representation models which can enhance coreference models with domain-specific knowledge. In addition, we conduct experiments to evaluate the ability of these language represetation models for biomedical coreference task. The structure of this paper is as follows. In Section 2 we briefly provide some bac"
2021.crac-1.2,2021.bionlp-1.16,0,0.0669032,"bbreviation/acronyms and numerical anaphora. CRAFT-CR (Cohen et al., 2017) consists of 97 full-text biomedical journal articles. Similar to the general domain, this corpus was annotated with coreferent chains in full-text articles, while most other biomedical coreference datasets focuse on annotating the pairwise coreference relation between an anaphor and its antecedent. In addition, all coreference expressions were annotated regardless of semantic type. These datasets are summarized in Table 1. 4 good performance on many scientific NLP tasks. PubMedBERT (Gu et al., 2020) and BioELECTRA (raj Kanakarajan et al., 2021) are both pre-trained on PubMed abstract and PubMed Central full text articles, but the latter adopts ELECTRA architecture (Clark et al., 2019). BioMegatron (Shin et al., 2020) is a large-scale model based on Megatron (Shoeybi et al., 2019) architecture. It also investigated the effect of vocabulary and corpora domain on the performance of biomedical tasks. 4.2 Although the biomedical PLMs, such as BioBERT, have achieved good performance on many biomedical tasks, however, these models can be further enhanced by integrating biomedical knowledge bases, such as UMLS (Bodenreider, 2004). Several m"
2021.crac-1.2,2020.acl-main.622,0,0.0471434,"Missing"
2021.crac-1.2,2020.emnlp-main.379,0,0.0341443,"ed with coreferent chains in full-text articles, while most other biomedical coreference datasets focuse on annotating the pairwise coreference relation between an anaphor and its antecedent. In addition, all coreference expressions were annotated regardless of semantic type. These datasets are summarized in Table 1. 4 good performance on many scientific NLP tasks. PubMedBERT (Gu et al., 2020) and BioELECTRA (raj Kanakarajan et al., 2021) are both pre-trained on PubMed abstract and PubMed Central full text articles, but the latter adopts ELECTRA architecture (Clark et al., 2019). BioMegatron (Shin et al., 2020) is a large-scale model based on Megatron (Shoeybi et al., 2019) architecture. It also investigated the effect of vocabulary and corpora domain on the performance of biomedical tasks. 4.2 Although the biomedical PLMs, such as BioBERT, have achieved good performance on many biomedical tasks, however, these models can be further enhanced by integrating biomedical knowledge bases, such as UMLS (Bodenreider, 2004). Several models enhance biomedical PLMs by integrating synonym knowledge from UMLS. Each mention in the biomedical text can be linked to a Concept Unique Identifier (CUI) in UMLS, and ea"
2021.crac-1.2,C04-1033,0,0.248074,"crine cell is a kind of interneuron and belongs to retinal neuron are required. A large number of coreference models for the biomedical domain have already been proposed, from rule-based models (Castano et al. 2002, Kim and Park 2004, Lin and Liang 2004, Nguyen et al. 2012, Miwa et al. 2012, Kilicoglu and DemnerFushman 2016, Li et al. 2018), machine learning12 Proceedings of the 4th Workshop on Computational Models of Reference, Anaphora and Coreference (CRAC 2021), pages 12–23 c Punta Cana, Dominican Republic, November 10–11, 2021. 2021 Association for Computational Linguistics based models (Yang et al. 2004, Torii and VijayShanker 2005, Su et al. 2008, Gasperin 2009, Kim et al. 2011) to recent deep learning-based models (Trieu et al. 2018, Trieu et al. 2019, Li et al. 2021). These models usually integrate biomedical specific information, typically specific rules, pre-trained embeddings and features. This paper reviews and analyses coreference datasets and models for the biomedical domain, as well as recent biomedical language representation models which can enhance coreference models with domain-specific knowledge. In addition, we conduct experiments to evaluate the ability of these language rep"
2021.crac-1.2,2020.emnlp-main.582,0,0.0401661,"Missing"
2021.crac-1.2,P10-2029,0,0.112385,"Missing"
2021.crac-1.2,2021.bionlp-1.20,0,0.0135786,"further pre-trained PubMedBERT, Bio_Clinical BERT and BioBERT on UMLS synonyms, using multi-similarity loss, multi-label loss and synonym marginalization algorithm respectively. In addition to synonym knowledge, Clinical KBBERT (Hao et al., 2020) injects UMLS relation knowledge into BioBERT. Whereas CODER (Yuan et al., 2020) learns both synonym and relation knowledge based on PubMedBERT or mBERT (Devlin et al., 2019) via contrastive learning. Also, some research focus on fusing the UMLS entity embeddings with contextual embeddings to improve biomedical PLMs (He et al., 2020; Fei et al., 2021; Yuan et al., 2021). This paper selected some of the models above to evaluate the ability of biomedical-specific representation for biomedical coreference task, detailed in Section 6. Biomedical Language Representation Models The news domain and the biomedical domain are different in a number of respects, such as markable types. Some authors have argued that biomedical domain knowledge is the key to bridging the gap (Choi et al., 2014), and that therefore, incorporating biomedical specific representation is beneficial for resolving coreferring expressions in the biomedical domain. In this section, we will give a"
2021.crac-1.2,P82-1020,0,0.78769,"Missing"
2021.crac-1.9,2020.deelio-1.4,0,0.0430334,"that share similar context to improve the training of n-gram and sequence-to-sequence language models. Guo et al. (2020) proposed a statistical approach called SeqMix to decide which token to use at each position of an input, and they also provided a framework that combines several data augmentation approaches for several NLP tasks. Chen et al. (2020) represented datasets as graphs and proposed methods to augment data based on graph theory for paraphrasing. Ding et al. (2020) trained a language model on the linearized version of the input to synthesize data for low-resource sequence-tagging. Feng et al. (2020) inserted character-level synthetic noise and word hypernyms to augment data for text generation. Louvan and Magnini (2020) used a set of augmentation methods that span words and modify sentences for slot filling and intent detection. (Ri et al., 2021) proposed a method to augment ZPs (not AZPs). They delete personal overt pronouns which results in extra sentences with no pronouns, and use them in Japanese-English translation. Other proposals investigated data augmentation for neural machine translation (Vaibhav et al., 2019; Gao et al., 2019; Nguyen et al., 2019), text classification (Kobayas"
2021.crac-1.9,D13-1135,0,0.0257059,"applied a rule-based approach that employed Hobbs algorithm (Hobbs, 1978) to resolve ZPs in the Chinese Treebank; however, did not attempt to automatically identify AZP. Yeh and Chen (2006) is another rulebased approach for Chinese which used a set of hand-engineered rules to identify and resolve AZPs. Zhao and Ng (2007) proposed the first machine learning approach to Chinese AZPs identification and resolution. They applied decision trees incorporated with a set of syntactic and positional features. Other proposals followed the machine learning approach targeting Chinese (Kong and Zhou, 2010; Chen and Ng, 2013, 2014, 2015, 2016; Yin et al., 2016, 2017; Liu et al., 2017; Chang et al., 2017; Yin et al., 2018; Kong et al., 2019). There has been also a great deal of research on identification and resolution of AZPs, particularly in Japanese (Yoshimoto, 1988; Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Sasano et al., 2008, 2009; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Iida et al., 2015; Yoshino et al., 2013; Yamashiro et al., 2.2 Data Augmentation Data augmentation is an active research topic and has"
2021.crac-1.9,D14-1084,0,0.0378057,"Missing"
2021.crac-1.9,D19-1272,0,0.0230657,"Missing"
2021.crac-1.9,P00-1022,0,0.114928,"rence, because since the beginning, (he) wanted to attend another conference ... • We propose a method to automatically find the true antecedent of AZPs. • The augmented data improve AZP identification and resolution for Arabic, and surpass the 82 Proceedings of the 4th Workshop on Computational Models of Reference, Anaphora and Coreference (CRAC 2021), pages 82–93 c Punta Cana, Dominican Republic, November 10–11, 2021. 2021 Association for Computational Linguistics current state-of-the-art results. 2018), but also in other languages, including Korean (Han, 2004; Byron et al., 2006), Spanish (Ferrández and Peral, 2000; Rello and Ilisei, 2009), Portuguese (Rello et al., 2012), Romanian (Mihăilă et al., 2011), Bulgarian (Grigorova, 2013), and Sanskrit (Gopal and Jha, 2017). Iida and Poesio (2011) proposed the first multilingual approach for AZP resolution. These proposals applied their AZP systems to labeled dataset. Even though data augmentation improved many NLP tasks, as discussed in Section 2.2, none of the above proposals considered augmenting AZPs datasets automatically. As far as we know, (Konno et al., 2020) is the only proposal to use data augmentation for AZP resolution. Konno et al applied a data"
2021.crac-1.9,P06-1079,0,0.100679,"g approach to Chinese AZPs identification and resolution. They applied decision trees incorporated with a set of syntactic and positional features. Other proposals followed the machine learning approach targeting Chinese (Kong and Zhou, 2010; Chen and Ng, 2013, 2014, 2015, 2016; Yin et al., 2016, 2017; Liu et al., 2017; Chang et al., 2017; Yin et al., 2018; Kong et al., 2019). There has been also a great deal of research on identification and resolution of AZPs, particularly in Japanese (Yoshimoto, 1988; Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Sasano et al., 2008, 2009; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Iida et al., 2015; Yoshino et al., 2013; Yamashiro et al., 2.2 Data Augmentation Data augmentation is an active research topic and has been applied in different areas of research of NLP (Feng et al., 2021; Chen et al., 2021). However, there has been very limited proposals for AZP data augmentation. Zhang et al. (2015) augmented data by replacing a word with its synonyms to improve text classification. Sennrich et al. (2015) augmented data by translating a sequence from one language to an"
2021.crac-1.9,P19-1555,0,0.0195423,"ynthesize data for low-resource sequence-tagging. Feng et al. (2020) inserted character-level synthetic noise and word hypernyms to augment data for text generation. Louvan and Magnini (2020) used a set of augmentation methods that span words and modify sentences for slot filling and intent detection. (Ri et al., 2021) proposed a method to augment ZPs (not AZPs). They delete personal overt pronouns which results in extra sentences with no pronouns, and use them in Japanese-English translation. Other proposals investigated data augmentation for neural machine translation (Vaibhav et al., 2019; Gao et al., 2019; Nguyen et al., 2019), text classification (Kobayashi, 2018; Wei and Zou, 2019; Anaby-Tavor et al., 2020; Jindal et al., 2020), and question-answering (Kafle et al., 2017; Yang et al., 2019). Methodology We applied the following five methods to generate AZPs: 1. OntoNotes Patterns (ONP): AZPs may occur more frequently in the company of certain verbs. To find the most frequent collocations, we apply the t-test on the Part-of-Speech sequences of AZP sentences. We tried a window of one, two, and three and we found empirically a window of two to detect many correct AZP collocations. 2. Removing S"
2021.crac-1.9,P11-1081,1,0.73954,"AZP identification and resolution for Arabic, and surpass the 82 Proceedings of the 4th Workshop on Computational Models of Reference, Anaphora and Coreference (CRAC 2021), pages 82–93 c Punta Cana, Dominican Republic, November 10–11, 2021. 2021 Association for Computational Linguistics current state-of-the-art results. 2018), but also in other languages, including Korean (Han, 2004; Byron et al., 2006), Spanish (Ferrández and Peral, 2000; Rello and Ilisei, 2009), Portuguese (Rello et al., 2012), Romanian (Mihăilă et al., 2011), Bulgarian (Grigorova, 2013), and Sanskrit (Gopal and Jha, 2017). Iida and Poesio (2011) proposed the first multilingual approach for AZP resolution. These proposals applied their AZP systems to labeled dataset. Even though data augmentation improved many NLP tasks, as discussed in Section 2.2, none of the above proposals considered augmenting AZPs datasets automatically. As far as we know, (Konno et al., 2020) is the only proposal to use data augmentation for AZP resolution. Konno et al applied a data augmentation method called contextual data augmentation (CDA) to Japanese AZPs. CDA method is based on language models to generate different variants of a labeled input by masking"
2021.crac-1.9,D15-1260,0,0.0146332,"d positional features. Other proposals followed the machine learning approach targeting Chinese (Kong and Zhou, 2010; Chen and Ng, 2013, 2014, 2015, 2016; Yin et al., 2016, 2017; Liu et al., 2017; Chang et al., 2017; Yin et al., 2018; Kong et al., 2019). There has been also a great deal of research on identification and resolution of AZPs, particularly in Japanese (Yoshimoto, 1988; Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Sasano et al., 2008, 2009; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Iida et al., 2015; Yoshino et al., 2013; Yamashiro et al., 2.2 Data Augmentation Data augmentation is an active research topic and has been applied in different areas of research of NLP (Feng et al., 2021; Chen et al., 2021). However, there has been very limited proposals for AZP data augmentation. Zhang et al. (2015) augmented data by replacing a word with its synonyms to improve text classification. Sennrich et al. (2015) augmented data by translating a sequence from one language to another, and then translating the sequence back into the original language. The new data were used to enhance the performance o"
2021.crac-1.9,2009.mtsummit-caasl.4,0,0.082205,"e, and resolve AZPs while they focus generating samples for AZP resolution. The generated AZP samples are of numerous types and they are automatically prepared for both AZP tasks. The rest of the paper is organized as follows. We discuss AZP and data augmentation related literature in Section 2. We explain our dataaugmentation methods in Section 3. We discuss the evaluation settings and results in Section 4 and Section 5 respectively. We conclude in Section 6. 2 2.1 Related Work Anaphoric Zero Pronouns Arabic: There have been a few proposals devoted to AZP tasks and null arguments in general. Green et al. (2009) proposed a conditional-random-field (CRF) sequence classifier to detect Arabic noun phrases, and captured ZPs implicitly. Bakr et al. (2009) applied a statistical approach to detect empty categories. Gabbard (2010) proposed a pipeline made of maximum entropy classifiers which jointly make a CRF to retrieve Arabic empty categories. Aloraini and Poesio (2020b) proposed the first neural model for resolving Arabic AZP, but they did not consider the AZP identification step. Aloraini and Poesio (2020a) showed a multilingual approach to detect AZPs for Arabic and Chinese using BERT (Devlin et al., 2"
2021.crac-1.9,W03-1024,0,0.185602,"the first machine learning approach to Chinese AZPs identification and resolution. They applied decision trees incorporated with a set of syntactic and positional features. Other proposals followed the machine learning approach targeting Chinese (Kong and Zhou, 2010; Chen and Ng, 2013, 2014, 2015, 2016; Yin et al., 2016, 2017; Liu et al., 2017; Chang et al., 2017; Yin et al., 2018; Kong et al., 2019). There has been also a great deal of research on identification and resolution of AZPs, particularly in Japanese (Yoshimoto, 1988; Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Sasano et al., 2008, 2009; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Iida et al., 2015; Yoshino et al., 2013; Yamashiro et al., 2.2 Data Augmentation Data augmentation is an active research topic and has been applied in different areas of research of NLP (Feng et al., 2021; Chen et al., 2021). However, there has been very limited proposals for AZP data augmentation. Zhang et al. (2015) augmented data by replacing a word with its synonyms to improve text classification. Sennrich et al. (2015) augmented data by translating a sequence from"
2021.crac-1.9,W19-4427,0,0.0205112,"proposed to randomly replace words in source and target languages to improve neural machine translation. Şahin and Steedman (2019) removed dependency links and modified tree nodes to create an augmented dataset for Part-of-Speech tagging. Gulordava et al. (2018) replaced words with other words that share the same Part-of-Speech, morphological features, and dependency labels, to improve subject-verb agreement models. Feng et al. (2019) introduced a pipeline called SMERTI which combines various data augmentation methods, such as, entity replacement, 83 3 similarity masking, and text in-filling. Grundkiewicz et al. (2019) used a spellchecker to augment training data which are then used to pre-train sequence-to-sequence models. Singh et al. (2019) introduced a cross-lingual data augmentation called XLDA, evaluated on 14 languages on a natural language inference (XNLI) benchmark and questionanswering task. XLDA replaces segments of an inputs text with its translation in other languages. Kumar et al. (2019) proposed DiPS, a model that generates various paraphrased sentences used to train conversational agents and in text summarization tasks. Andreas (2019) proposed rule-based data augmentation, which replaces seg"
2021.crac-1.9,2020.coling-main.611,0,0.0492771,"Missing"
2021.crac-1.9,N18-1108,0,0.0138244,"synonyms to improve text classification. Sennrich et al. (2015) augmented data by translating a sequence from one language to another, and then translating the sequence back into the original language. The new data were used to enhance the performance of neural machine translation models. Wang et al. (2018) examined various methods for data augmentation and proposed to randomly replace words in source and target languages to improve neural machine translation. Şahin and Steedman (2019) removed dependency links and modified tree nodes to create an augmented dataset for Part-of-Speech tagging. Gulordava et al. (2018) replaced words with other words that share the same Part-of-Speech, morphological features, and dependency labels, to improve subject-verb agreement models. Feng et al. (2019) introduced a pipeline called SMERTI which combines various data augmentation methods, such as, entity replacement, 83 3 similarity masking, and text in-filling. Grundkiewicz et al. (2019) used a spellchecker to augment training data which are then used to pre-train sequence-to-sequence models. Singh et al. (2019) introduced a cross-lingual data augmentation called XLDA, evaluated on 14 languages on a natural language in"
2021.crac-1.9,W17-3529,0,0.0140548,"van and Magnini (2020) used a set of augmentation methods that span words and modify sentences for slot filling and intent detection. (Ri et al., 2021) proposed a method to augment ZPs (not AZPs). They delete personal overt pronouns which results in extra sentences with no pronouns, and use them in Japanese-English translation. Other proposals investigated data augmentation for neural machine translation (Vaibhav et al., 2019; Gao et al., 2019; Nguyen et al., 2019), text classification (Kobayashi, 2018; Wei and Zou, 2019; Anaby-Tavor et al., 2020; Jindal et al., 2020), and question-answering (Kafle et al., 2017; Yang et al., 2019). Methodology We applied the following five methods to generate AZPs: 1. OntoNotes Patterns (ONP): AZPs may occur more frequently in the company of certain verbs. To find the most frequent collocations, we apply the t-test on the Part-of-Speech sequences of AZP sentences. We tried a window of one, two, and three and we found empirically a window of two to detect many correct AZP collocations. 2. Removing Subject Mention (RSM): AZPs are dropped subjects of verbs. When an (explicit) mention is the subject of a verb phrase, we remove the mention to obtain an AZP sentence. 3. M"
2021.crac-1.9,2020.emnlp-main.447,0,0.0106584,". (2019) introduced a cross-lingual data augmentation called XLDA, evaluated on 14 languages on a natural language inference (XNLI) benchmark and questionanswering task. XLDA replaces segments of an inputs text with its translation in other languages. Kumar et al. (2019) proposed DiPS, a model that generates various paraphrased sentences used to train conversational agents and in text summarization tasks. Andreas (2019) proposed rule-based data augmentation, which replaces segments of inputs that share similar context to improve the training of n-gram and sequence-to-sequence language models. Guo et al. (2020) proposed a statistical approach called SeqMix to decide which token to use at each position of an input, and they also provided a framework that combines several data augmentation approaches for several NLP tasks. Chen et al. (2020) represented datasets as graphs and proposed methods to augment data based on graph theory for paraphrasing. Ding et al. (2020) trained a language model on the linearized version of the input to synthesize data for low-resource sequence-tagging. Feng et al. (2020) inserted character-level synthetic noise and word hypernyms to augment data for text generation. Louva"
2021.crac-1.9,N06-2013,0,0.080502,"stems Data Preprocessing Arabic is a morphologically rich language with a large set of morphological properties. Arabic text can suffer from sparsity (different forms for the same word) and ambiguity (same form for numerous words) if the text is not pre-processed. There are two reasons for these problems. First, certain letters can have different forms which are usually misspelled, such as the various forms of the letter “alif”. Second, same word can have a fully diacritized, partially diacritized or undiacritized forms. Retaining diacritics can complex word representation and model training (Habash and Sadat, 2006). Pre-processing Arabic text improves the overall performance. For example, Aloraini et al. (2020) folFor AZP identification, we use the model by (Aloraini and Poesio, 2020a) which is a binary classifier that takes a candidate ZP location as input, and classifies whether it as an AZP or not. For AZP resolution, we use the model by (Aloraini and Poesio, 2020b) which combines BERT representations and additional task-related features of AZPS to learn their true antecedent. We adopt their systems and apply the same settings on Arabic AZPs. We then evaluate each method of our data augmentation for"
2021.crac-1.9,W04-0205,0,0.109083,"ny enthusiasm for the international conference, because since the beginning, (he) wanted to attend another conference ... • We propose a method to automatically find the true antecedent of AZPs. • The augmented data improve AZP identification and resolution for Arabic, and surpass the 82 Proceedings of the 4th Workshop on Computational Models of Reference, Anaphora and Coreference (CRAC 2021), pages 82–93 c Punta Cana, Dominican Republic, November 10–11, 2021. 2021 Association for Computational Linguistics current state-of-the-art results. 2018), but also in other languages, including Korean (Han, 2004; Byron et al., 2006), Spanish (Ferrández and Peral, 2000; Rello and Ilisei, 2009), Portuguese (Rello et al., 2012), Romanian (Mihăilă et al., 2011), Bulgarian (Grigorova, 2013), and Sanskrit (Gopal and Jha, 2017). Iida and Poesio (2011) proposed the first multilingual approach for AZP resolution. These proposals applied their AZP systems to labeled dataset. Even though data augmentation improved many NLP tasks, as discussed in Section 2.2, none of the above proposals considered augmenting AZPs datasets automatically. As far as we know, (Konno et al., 2020) is the only proposal to use data aug"
2021.crac-1.9,D13-1095,0,0.0168342,"a set of syntactic and positional features. Other proposals followed the machine learning approach targeting Chinese (Kong and Zhou, 2010; Chen and Ng, 2013, 2014, 2015, 2016; Yin et al., 2016, 2017; Liu et al., 2017; Chang et al., 2017; Yin et al., 2018; Kong et al., 2019). There has been also a great deal of research on identification and resolution of AZPs, particularly in Japanese (Yoshimoto, 1988; Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Sasano et al., 2008, 2009; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Iida et al., 2015; Yoshino et al., 2013; Yamashiro et al., 2.2 Data Augmentation Data augmentation is an active research topic and has been applied in different areas of research of NLP (Feng et al., 2021; Chen et al., 2021). However, there has been very limited proposals for AZP data augmentation. Zhang et al. (2015) augmented data by replacing a word with its synonyms to improve text classification. Sennrich et al. (2015) augmented data by translating a sequence from one language to another, and then translating the sequence back into the original language. The new data were used to enhanc"
2021.crac-1.9,N18-2072,0,0.0227992,"(2020) inserted character-level synthetic noise and word hypernyms to augment data for text generation. Louvan and Magnini (2020) used a set of augmentation methods that span words and modify sentences for slot filling and intent detection. (Ri et al., 2021) proposed a method to augment ZPs (not AZPs). They delete personal overt pronouns which results in extra sentences with no pronouns, and use them in Japanese-English translation. Other proposals investigated data augmentation for neural machine translation (Vaibhav et al., 2019; Gao et al., 2019; Nguyen et al., 2019), text classification (Kobayashi, 2018; Wei and Zou, 2019; Anaby-Tavor et al., 2020; Jindal et al., 2020), and question-answering (Kafle et al., 2017; Yang et al., 2019). Methodology We applied the following five methods to generate AZPs: 1. OntoNotes Patterns (ONP): AZPs may occur more frequently in the company of certain verbs. To find the most frequent collocations, we apply the t-test on the Part-of-Speech sequences of AZP sentences. We tried a window of one, two, and three and we found empirically a window of two to detect many correct AZP collocations. 2. Removing Subject Mention (RSM): AZPs are dropped subjects of verbs. Wh"
2021.crac-1.9,D10-1086,0,0.0400064,"d AZP resolution and applied a rule-based approach that employed Hobbs algorithm (Hobbs, 1978) to resolve ZPs in the Chinese Treebank; however, did not attempt to automatically identify AZP. Yeh and Chen (2006) is another rulebased approach for Chinese which used a set of hand-engineered rules to identify and resolve AZPs. Zhao and Ng (2007) proposed the first machine learning approach to Chinese AZPs identification and resolution. They applied decision trees incorporated with a set of syntactic and positional features. Other proposals followed the machine learning approach targeting Chinese (Kong and Zhou, 2010; Chen and Ng, 2013, 2014, 2015, 2016; Yin et al., 2016, 2017; Liu et al., 2017; Chang et al., 2017; Yin et al., 2018; Kong et al., 2019). There has been also a great deal of research on identification and resolution of AZPs, particularly in Japanese (Yoshimoto, 1988; Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Sasano et al., 2008, 2009; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Iida et al., 2015; Yoshino et al., 2013; Yamashiro et al., 2.2 Data Augmentation Data augmentation is an active rese"
2021.crac-1.9,R09-2011,0,0.0519301,"ginning, (he) wanted to attend another conference ... • We propose a method to automatically find the true antecedent of AZPs. • The augmented data improve AZP identification and resolution for Arabic, and surpass the 82 Proceedings of the 4th Workshop on Computational Models of Reference, Anaphora and Coreference (CRAC 2021), pages 82–93 c Punta Cana, Dominican Republic, November 10–11, 2021. 2021 Association for Computational Linguistics current state-of-the-art results. 2018), but also in other languages, including Korean (Han, 2004; Byron et al., 2006), Spanish (Ferrández and Peral, 2000; Rello and Ilisei, 2009), Portuguese (Rello et al., 2012), Romanian (Mihăilă et al., 2011), Bulgarian (Grigorova, 2013), and Sanskrit (Gopal and Jha, 2017). Iida and Poesio (2011) proposed the first multilingual approach for AZP resolution. These proposals applied their AZP systems to labeled dataset. Even though data augmentation improved many NLP tasks, as discussed in Section 2.2, none of the above proposals considered augmenting AZPs datasets automatically. As far as we know, (Konno et al., 2020) is the only proposal to use data augmentation for AZP resolution. Konno et al applied a data augmentation method calle"
2021.crac-1.9,N19-1363,0,0.0604232,"Missing"
2021.crac-1.9,2021.wat-1.11,0,0.0716805,"Missing"
2021.crac-1.9,I05-1052,0,0.126078,"ample). Contextual information is required for these two tasks, such as that provided by the verb which has the AZP as an argument (in the example the verb is ”wanted/”)ير يد. The AZP problem has inspired much research because AZP interpretation benefits many natural language processing (NLP) tasks such as machine translation (Mitkov and Schmidt, 1998). Although AZPs are common (Chen and Ng, 2016), they are not always annotated in NLP corpora. There are two reasons for this. First, AZPs have no surface realization and the focus in coreference is usually on arguments realized on the surface (Lee et al., 2005). Second, pro-drop languages have several types of ZPs, not all of which anaphoric, which can make it challenging to identify AZPs. Therefore, the number of datasets with annotated AZPs is small (Konno et al., 2020). We investigate therefore automatic methods for augmenting these existing data, applying them to Arabic. Our contributions are as follows:  المفارقة الأخرى عن بوش هي عدم حماسته..  ير يد * اجتماعا، اذ انه من البداية،للمؤتمر الدولي .... مختلفا • We apply various data augmentation methods to detect potential AZPs in unannotated sentences, and to generate sentences containing"
2021.crac-1.9,C08-1097,0,0.0296342,"s identification and resolution. They applied decision trees incorporated with a set of syntactic and positional features. Other proposals followed the machine learning approach targeting Chinese (Kong and Zhou, 2010; Chen and Ng, 2013, 2014, 2015, 2016; Yin et al., 2016, 2017; Liu et al., 2017; Chang et al., 2017; Yin et al., 2018; Kong et al., 2019). There has been also a great deal of research on identification and resolution of AZPs, particularly in Japanese (Yoshimoto, 1988; Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Sasano et al., 2008, 2009; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Iida et al., 2015; Yoshino et al., 2013; Yamashiro et al., 2.2 Data Augmentation Data augmentation is an active research topic and has been applied in different areas of research of NLP (Feng et al., 2021; Chen et al., 2021). However, there has been very limited proposals for AZP data augmentation. Zhang et al. (2015) augmented data by replacing a word with its synonyms to improve text classification. Sennrich et al. (2015) augmented data by translating a sequence from one language to another, and then translating"
2021.crac-1.9,N09-1059,0,0.074697,"Missing"
2021.crac-1.9,P17-1010,0,0.0139972,"Hobbs, 1978) to resolve ZPs in the Chinese Treebank; however, did not attempt to automatically identify AZP. Yeh and Chen (2006) is another rulebased approach for Chinese which used a set of hand-engineered rules to identify and resolve AZPs. Zhao and Ng (2007) proposed the first machine learning approach to Chinese AZPs identification and resolution. They applied decision trees incorporated with a set of syntactic and positional features. Other proposals followed the machine learning approach targeting Chinese (Kong and Zhou, 2010; Chen and Ng, 2013, 2014, 2015, 2016; Yin et al., 2016, 2017; Liu et al., 2017; Chang et al., 2017; Yin et al., 2018; Kong et al., 2019). There has been also a great deal of research on identification and resolution of AZPs, particularly in Japanese (Yoshimoto, 1988; Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Sasano et al., 2008, 2009; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Iida et al., 2015; Yoshino et al., 2013; Yamashiro et al., 2.2 Data Augmentation Data augmentation is an active research topic and has been applied in different areas of research of NLP (Feng et"
2021.crac-1.9,2020.paclic-1.20,0,0.0205524,"2020) proposed a statistical approach called SeqMix to decide which token to use at each position of an input, and they also provided a framework that combines several data augmentation approaches for several NLP tasks. Chen et al. (2020) represented datasets as graphs and proposed methods to augment data based on graph theory for paraphrasing. Ding et al. (2020) trained a language model on the linearized version of the input to synthesize data for low-resource sequence-tagging. Feng et al. (2020) inserted character-level synthetic noise and word hypernyms to augment data for text generation. Louvan and Magnini (2020) used a set of augmentation methods that span words and modify sentences for slot filling and intent detection. (Ri et al., 2021) proposed a method to augment ZPs (not AZPs). They delete personal overt pronouns which results in extra sentences with no pronouns, and use them in Japanese-English translation. Other proposals investigated data augmentation for neural machine translation (Vaibhav et al., 2019; Gao et al., 2019; Nguyen et al., 2019), text classification (Kobayashi, 2018; Wei and Zou, 2019; Anaby-Tavor et al., 2020; Jindal et al., 2020), and question-answering (Kafle et al., 2017; Ya"
2021.crac-1.9,I11-1085,0,0.014861,"tion. They applied decision trees incorporated with a set of syntactic and positional features. Other proposals followed the machine learning approach targeting Chinese (Kong and Zhou, 2010; Chen and Ng, 2013, 2014, 2015, 2016; Yin et al., 2016, 2017; Liu et al., 2017; Chang et al., 2017; Yin et al., 2018; Kong et al., 2019). There has been also a great deal of research on identification and resolution of AZPs, particularly in Japanese (Yoshimoto, 1988; Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Sasano et al., 2008, 2009; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Iida et al., 2015; Yoshino et al., 2013; Yamashiro et al., 2.2 Data Augmentation Data augmentation is an active research topic and has been applied in different areas of research of NLP (Feng et al., 2021; Chen et al., 2021). However, there has been very limited proposals for AZP data augmentation. Zhang et al. (2015) augmented data by replacing a word with its synonyms to improve text classification. Sennrich et al. (2015) augmented data by translating a sequence from one language to another, and then translating the sequence back into the origin"
2021.crac-1.9,C02-1078,0,0.153956,"Ng (2007) proposed the first machine learning approach to Chinese AZPs identification and resolution. They applied decision trees incorporated with a set of syntactic and positional features. Other proposals followed the machine learning approach targeting Chinese (Kong and Zhou, 2010; Chen and Ng, 2013, 2014, 2015, 2016; Yin et al., 2016, 2017; Liu et al., 2017; Chang et al., 2017; Yin et al., 2018; Kong et al., 2019). There has been also a great deal of research on identification and resolution of AZPs, particularly in Japanese (Yoshimoto, 1988; Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Sasano et al., 2008, 2009; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Iida et al., 2015; Yoshino et al., 2013; Yamashiro et al., 2.2 Data Augmentation Data augmentation is an active research topic and has been applied in different areas of research of NLP (Feng et al., 2021; Chen et al., 2021). However, there has been very limited proposals for AZP data augmentation. Zhang et al. (2015) augmented data by replacing a word with its synonyms to improve text classification. Sennrich et al. (2015) augmented data by tr"
2021.crac-1.9,P16-1009,0,0.073435,"Missing"
2021.crac-1.9,N19-1190,0,0.132307,"sion of the input to synthesize data for low-resource sequence-tagging. Feng et al. (2020) inserted character-level synthetic noise and word hypernyms to augment data for text generation. Louvan and Magnini (2020) used a set of augmentation methods that span words and modify sentences for slot filling and intent detection. (Ri et al., 2021) proposed a method to augment ZPs (not AZPs). They delete personal overt pronouns which results in extra sentences with no pronouns, and use them in Japanese-English translation. Other proposals investigated data augmentation for neural machine translation (Vaibhav et al., 2019; Gao et al., 2019; Nguyen et al., 2019), text classification (Kobayashi, 2018; Wei and Zou, 2019; Anaby-Tavor et al., 2020; Jindal et al., 2020), and question-answering (Kafle et al., 2017; Yang et al., 2019). Methodology We applied the following five methods to generate AZPs: 1. OntoNotes Patterns (ONP): AZPs may occur more frequently in the company of certain verbs. To find the most frequent collocations, we apply the t-test on the Part-of-Speech sequences of AZP sentences. We tried a window of one, two, and three and we found empirically a window of two to detect many correct AZP collocati"
2021.crac-1.9,C88-2159,0,0.176201,"f hand-engineered rules to identify and resolve AZPs. Zhao and Ng (2007) proposed the first machine learning approach to Chinese AZPs identification and resolution. They applied decision trees incorporated with a set of syntactic and positional features. Other proposals followed the machine learning approach targeting Chinese (Kong and Zhou, 2010; Chen and Ng, 2013, 2014, 2015, 2016; Yin et al., 2016, 2017; Liu et al., 2017; Chang et al., 2017; Yin et al., 2018; Kong et al., 2019). There has been also a great deal of research on identification and resolution of AZPs, particularly in Japanese (Yoshimoto, 1988; Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Sasano et al., 2008, 2009; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Iida et al., 2015; Yoshino et al., 2013; Yamashiro et al., 2.2 Data Augmentation Data augmentation is an active research topic and has been applied in different areas of research of NLP (Feng et al., 2021; Chen et al., 2021). However, there has been very limited proposals for AZP data augmentation. Zhang et al. (2015) augmented data by replacing a word with its synonyms to improve"
2021.crac-1.9,D18-1100,0,0.0248665,"al., 2.2 Data Augmentation Data augmentation is an active research topic and has been applied in different areas of research of NLP (Feng et al., 2021; Chen et al., 2021). However, there has been very limited proposals for AZP data augmentation. Zhang et al. (2015) augmented data by replacing a word with its synonyms to improve text classification. Sennrich et al. (2015) augmented data by translating a sequence from one language to another, and then translating the sequence back into the original language. The new data were used to enhance the performance of neural machine translation models. Wang et al. (2018) examined various methods for data augmentation and proposed to randomly replace words in source and target languages to improve neural machine translation. Şahin and Steedman (2019) removed dependency links and modified tree nodes to create an augmented dataset for Part-of-Speech tagging. Gulordava et al. (2018) replaced words with other words that share the same Part-of-Speech, morphological features, and dependency labels, to improve subject-verb agreement models. Feng et al. (2019) introduced a pipeline called SMERTI which combines various data augmentation methods, such as, entity replace"
2021.crac-1.9,I13-1126,0,0.0186721,"es. Other proposals followed the machine learning approach targeting Chinese (Kong and Zhou, 2010; Chen and Ng, 2013, 2014, 2015, 2016; Yin et al., 2016, 2017; Liu et al., 2017; Chang et al., 2017; Yin et al., 2018; Kong et al., 2019). There has been also a great deal of research on identification and resolution of AZPs, particularly in Japanese (Yoshimoto, 1988; Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Sasano et al., 2008, 2009; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Iida et al., 2015; Yoshino et al., 2013; Yamashiro et al., 2.2 Data Augmentation Data augmentation is an active research topic and has been applied in different areas of research of NLP (Feng et al., 2021; Chen et al., 2021). However, there has been very limited proposals for AZP data augmentation. Zhang et al. (2015) augmented data by replacing a word with its synonyms to improve text classification. Sennrich et al. (2015) augmented data by translating a sequence from one language to another, and then translating the sequence back into the original language. The new data were used to enhance the performance of neural machine trans"
2021.crac-1.9,D19-1670,0,0.0481078,"Missing"
2021.crac-1.9,N18-1057,0,0.0250954,"es a pattern, it suggests that the sentence might be an AZP. The AZP true antecedent is usually in the first sentence of the summary section. We join the first sentence and the detected sentence together to represent one AZP sample, as shown in Figure 3. In Section 3.6, we discuss in details why we follow this approach and the challenges of finding the true antecedent of an AZP. 3.2 Masking Candidate Mentions 3.4 Back Translation It has been shown that translating a sentence from one language to another, and then translating it back to the original language to be beneficial to some NLP tasks (Xie et al., 2018; Vaibhav et al., 2019; Removing Subject Mentions 2 https://github.com/google-research/bert [MASK] is a special token in BERT which is used for prediction. Subject nouns or pronouns of a verb phrase might be optional. Removing these subjects transforms a 3 85 Zhang et al., 2019). Back Translation generates a paraphrased version of the original input adding noise, such as, semantic and syntactical changes. Therefore, we translate the Arabic samples to English, and translate them back to Arabic. 3.5 Wikipedia articles. Every Wikipedia articles focuses on a single topic especially in the summary"
2021.crac-1.9,D07-1057,0,0.0469366,"ural model for resolving Arabic AZP, but they did not consider the AZP identification step. Aloraini and Poesio (2020a) showed a multilingual approach to detect AZPs for Arabic and Chinese using BERT (Devlin et al., 2018). Other languages: for Chinese, Converse (2006) studied AZP resolution and applied a rule-based approach that employed Hobbs algorithm (Hobbs, 1978) to resolve ZPs in the Chinese Treebank; however, did not attempt to automatically identify AZP. Yeh and Chen (2006) is another rulebased approach for Chinese which used a set of hand-engineered rules to identify and resolve AZPs. Zhao and Ng (2007) proposed the first machine learning approach to Chinese AZPs identification and resolution. They applied decision trees incorporated with a set of syntactic and positional features. Other proposals followed the machine learning approach targeting Chinese (Kong and Zhou, 2010; Chen and Ng, 2013, 2014, 2015, 2016; Yin et al., 2016, 2017; Liu et al., 2017; Chang et al., 2017; Yin et al., 2018; Kong et al., 2019). There has been also a great deal of research on identification and resolution of AZPs, particularly in Japanese (Yoshimoto, 1988; Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et a"
2021.crac-1.9,Y18-1089,0,0.0242311,"Missing"
2021.crac-1.9,D17-1135,0,0.0266604,"Missing"
2021.crac-1.9,C18-1002,0,0.0135933,"nese Treebank; however, did not attempt to automatically identify AZP. Yeh and Chen (2006) is another rulebased approach for Chinese which used a set of hand-engineered rules to identify and resolve AZPs. Zhao and Ng (2007) proposed the first machine learning approach to Chinese AZPs identification and resolution. They applied decision trees incorporated with a set of syntactic and positional features. Other proposals followed the machine learning approach targeting Chinese (Kong and Zhou, 2010; Chen and Ng, 2013, 2014, 2015, 2016; Yin et al., 2016, 2017; Liu et al., 2017; Chang et al., 2017; Yin et al., 2018; Kong et al., 2019). There has been also a great deal of research on identification and resolution of AZPs, particularly in Japanese (Yoshimoto, 1988; Kim and Ehara., 1995; Aone and Bennett, 1995; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Sasano et al., 2008, 2009; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Iida et al., 2015; Yoshino et al., 2013; Yamashiro et al., 2.2 Data Augmentation Data augmentation is an active research topic and has been applied in different areas of research of NLP (Feng et al., 2021; Chen et al., 2021). However"
2021.crac-1.9,I11-1126,0,0.0395177,"Missing"
2021.eacl-main.232,W12-0405,0,0.0220821,"implications on personal, economic, legal, and political levels. There has been a growing interest in automatic deception detection from academia and industry in recent years (see section 9). One of the main research lines tries to increase the collection of deception cues in terms of number and variety. For example, several successful studies show how to exploit multi-modal signals, jointly analyzing verbal, video, and audio data (P´erez-Rosas et al., 2015). For the same reason, several early studies tried to identify deception cues through manual feature annotation, like irony or ambiguity (Fitzpatrick and Bachenko, 2012). While these approaches offer a broad and interpretable description of the phenomenon, their main limitation lies in data collection and preprocessing difficulty. Surprisingly, so far, little attention has been paid to expanding the targets’ linguistic context, which is the easiest source of additional cues and data. Even in dialogues, which by definition are exchanges between different speakers/writers, the main focus is typically on the target text. None consider the preceding statements, be they issued by the same speaker of an interlocutor. We hypothesize that linguistic context can be us"
2021.eacl-main.232,fornaciari-poesio-2012-decour,1,0.749023,"2021. ©2021 Association for Computational Linguistics these methods on data collected from real, highstakes conditions for the subjects and not from a laboratory or game environment. Contributions The contributions of this paper are as follows: • We evaluate ways to incorporate contextual information for detecting deception on reallife data. • We significantly outperform the previous stateof-the-art results. • We show that language models are useful for the task, but they need the support of methods dedicated to detect deception’s stylometric features. 2 Dataset We use the D E C OUR dataset (Fornaciari and Poesio, 2012), which includes courtroom data transcripts of 35 hearings for criminal proceedings held in Italian courts. This provides a unique source of real deception data. The corpus is in Italian. It consists of dialogues between an interviewee and some interviewers (such as the judge, the prosecutor, the lawyer). Each dialogue contains a sequence of utterances of the different speakers. These utterances are called turns. By definition, adjacent turns come from different speakers. Each turn contains one or more utterances. Each utterance by the interviewee is labeled as True, False or Uncertain. The ut"
2021.eacl-main.232,W14-1601,1,0.712961,"Missing"
2021.eacl-main.232,N18-1176,0,0.0393176,"Missing"
2021.eacl-main.232,P11-1032,0,0.0484129,"Missing"
2021.eacl-main.232,2020.acl-main.353,0,0.0187805,"emember I pushed him away IG*100 21.858 10.831 09.257 08.674 07.789 07.627 06.843 06.677 06.674 06.674 Table 3: Information Gain (rescaled by 100 to avoid tiny values) of tri-grams indicative of truth (left) and deception (right) Figure 2: Output of the SOC algorithm. The red terms predict deception, the blue ones predict truthfulness. we speculate that BERT’s contextual knowledge works as a regularizer, which provides the Transformer with previously weighted inputs, according to the sentences’ meaning. Our results concerning BERT’s usefulness with context are different from those obtained by Peskov et al. (2020), who work on Diplomacy board-game deception data. Their study associated BERT to LSTM-based contextual models, and they did not find a BERT contribution in their model’s performance. They tried to fine-tune it, and they hypothesized that the lack of performance improvement was motivated by the “relatively small size” of the training data. This hypothesis could be correct, but our outcome allows us to formulate another hypothesis. Their data set concerns an online game, where the range of topics in the dialogues is presumably restricted and specific. This limitation would not allow BERT’s broa"
2021.eacl-main.232,2020.acl-main.468,1,0.713708,"Missing"
2021.findings-emnlp.226,J16-2003,0,0.0287322,"rns at all.9 These observations suggest that sense similarity patterns are best to be investigated within a given type of alternation, and further research will be needed to develop a more detailed account of polysemy types and sense similarity patterns. 3.5 Sense Clustering As BERT Large displayed a high correlation with the human judgements of word sense similarity, and some capability in replicating similarity patterns across target words, we next wanted to investigate how well BERT’s contextualised embeddings can be used to cluster our polysemous targets according to their interpretation (McCarthy et al., 2016; Garí Soler and Apidianaki, 2021). To provide a tentative analysis, we grouped BERT Large’s contextualised target encodings based on 9 See Figure 7 in the Appendix for the similarity maps of the animal/meat targets their similarity using the hierarchical Ward clustering method implemented in SciPy.10 We opted for hierarchical clustering as this method has to determine the optimal number of clusters itself, and does not take this number as an argument like most clustering methods do. We experimented with two different clustering criteria based only on a threshold parameter t. The quantitativel"
2021.findings-emnlp.226,H93-1061,0,0.290886,"certain types of alternations. While on the other hand collects graded similarity judge- more work on this matter will be needed before definite conclusions can be drawn, both of these obserments –but does so for different, related targets. vations can be taken as additional evidence against In parallel to our work, Nair et al. (2020) relinguistic models proposing a uniform treatment of cently conducted an investigation of 32 polysepolysemic senses. We also used the collected data mic and homonymic word types extracted from to test how well different ‘off-the-shelf’ contextuthe Semcor corpus (Miller et al., 1993) by comparalised language models can predict human word ing the distances between a selection of cross-sense sense similarity ratings. Among the tested models, samples as determined by participants arranging especially BERT Large seems to capture nuanced them in a 2D spatial arrangement task. In line with word sense distinctions in a similar way to human our results, they reported polysemic senses to be annotators, and to some degree is capable of grouprated significantly more similar to one another in ing sense interpretations by their contextualised both the human annotations and BERT Base e"
2021.findings-emnlp.226,2020.cogalex-1.16,0,0.185051,"and that some sense interpretations lead to zeugma:1 (3) # They took the door off its hinges and walked through it. Joining a range of recent work seeking to provide empirical data of graded word use similarity, in Haber and Poesio (2020a,b) we recently released an experimental small-scale dataset of graded word sense similarity judgements for a highly controlled set of polysemic targets to investigate the notion of structured sense representations. Analysing results for ten seminal English polysemes, we observed significant differences among polysemic sense interpretations (Erk et al., 2013; Nair et al., 2020; Trott and Bergen, 2021) and found first evidence of a distance-based grouping of word senses in some of the targets. In this paper we present a modified and extended version of this initial dataset to i) provide additional annotated data to validate previous observations, and ii) include new targets allowing for the same alternations as the initial set. This expansion enables us to carry out analyses not possible with the original dataset, including iii) investigating similarity patterns and polysemy types, iii) performing a detailed analysis the correlation between human judgements and sens"
2021.findings-emnlp.226,N18-1202,0,0.0554601,"while a homonym needs to be intermodel displays high confidence in discerning preted correctly in order to arrive at the correct homonyms and some types of polysemic alternations, but consistently fails for others. meaning of an utterance, polysemes refer to different aspects or facets of the same concept, and might 1 Introduction not even need to be completely specified to elicit a Capturing lexical ambiguity has been a driving fac- good-enough interpretation of what is meant (Kletor in the development of contextualised language pousniotou, 2002; Pylkkänen et al., 2006; Recasens models (e.g. Peters et al., 2018; Devlin et al., 2019). et al., 2011; Frisson, 2015; Poesio, 2020). Evidence Evaluating their performance, much of the focus from psycholinguistic studies supports this distinchas been on homonymy, a variety of multiplicity tion, indicating that polysemes are processed very of meaning exemplified by word forms such match differently than homonyms (Frazier and Rayner, in (1), whose different meanings are entirely unre- 1990; Rodd et al., 2002; Klepousniotou et al., 2008, lated. 2012). A growing body of work recently also has 2663 Findings of the Association for Computational Linguistics: EMNLP"
2021.findings-emnlp.226,N19-1128,0,0.0358008,"Missing"
2021.findings-emnlp.226,J91-4003,0,0.475739,"ity tion, indicating that polysemes are processed very of meaning exemplified by word forms such match differently than homonyms (Frazier and Rayner, in (1), whose different meanings are entirely unre- 1990; Rodd et al., 2002; Klepousniotou et al., 2008, lated. 2012). A growing body of work recently also has 2663 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2663–2676 November 7–11, 2021. ©2021 Association for Computational Linguistics started to challenge the uniform treatment of polysemic sense postulated by traditional theories such as the Generative Lexicon (Pustejovsky, 1991; Asher and Pustejovsky, 2006; Asher, 2011), and put forward proposals of a more structured mental representation of polysemic sense (Ortega-Andrés and Vicente, 2019). Using co-predication tests, studies such as Antunes and Chaves (2003); Traxler et al. (2005); Schumacher (2013) show that not all polysemic interpretations can be co-predicated, and that some sense interpretations lead to zeugma:1 (3) # They took the door off its hinges and walked through it. Joining a range of recent work seeking to provide empirical data of graded word use similarity, in Haber and Poesio (2020a,b) we recently"
2021.findings-emnlp.226,2021.acl-long.550,0,0.112173,"interpretations lead to zeugma:1 (3) # They took the door off its hinges and walked through it. Joining a range of recent work seeking to provide empirical data of graded word use similarity, in Haber and Poesio (2020a,b) we recently released an experimental small-scale dataset of graded word sense similarity judgements for a highly controlled set of polysemic targets to investigate the notion of structured sense representations. Analysing results for ten seminal English polysemes, we observed significant differences among polysemic sense interpretations (Erk et al., 2013; Nair et al., 2020; Trott and Bergen, 2021) and found first evidence of a distance-based grouping of word senses in some of the targets. In this paper we present a modified and extended version of this initial dataset to i) provide additional annotated data to validate previous observations, and ii) include new targets allowing for the same alternations as the initial set. This expansion enables us to carry out analyses not possible with the original dataset, including iii) investigating similarity patterns and polysemy types, iii) performing a detailed analysis the correlation between human judgements and sense similarities predicted"
2021.naacl-main.204,P14-2064,0,0.0628865,"Missing"
2021.naacl-main.204,D12-1091,0,0.0822475,"Missing"
2021.naacl-main.204,P13-1004,0,0.0147721,"; Han functions is not significant, we find that the inverse et al., 2018b,a) treat disagreement as a corruption version of KL gives the best results in all the experof a theoretical gold standard. Since the robustness imental conditions but one. This finding supports of machine learning models is affected by the data our idea of emphasizing the coders’ disagreement annotation quality, reducing noisy labels generally during training. We conjecture that predicting the improves the models’ performance. The closest to soft labels acts as a regularizer, reducing overfitour work are the studies of Cohn and Specia (2013) ting. That effect is especially likely for ambiguous and Rodrigues and Pereira (2018), who both use instances, where annotators’ label distributions difMTL. In contrast to our approach, though, each fer especially strongly from one-hot encoded gold of their tasks represents an annotator. We instead labels. propose to learn from both the gold labels and the Acknowledgements distribution over multiple annotators, which we treat as soft label distributions in a single auxil- DH and TF are members of the Data and Marketing iary task. Compared to treating each annotator as a Insights Unit at the B"
2021.naacl-main.204,N13-1132,1,0.807851,"ed labels. The 1 Introduction main impediment to the direct use of soft labels as targets, though, is the lack of universally accepted Usually, the labels used in NLP classification tasks are produced by sets of human annotators. As dis- performance metrics to evaluate the divergence between probability distributions. (Most metrics lack agreement between annotators is common, many an upper bound, making it difficult to assess predicmethods aggregate the different answers into a supposedly correct one (Dawid and Skene, 1979; tion quality). Usually, annotations are incorporated Carpenter, 2008; Hovy et al., 2013; Raykar et al., into the models without soft labels (Plank et al., 2010; Paun et al., 2018; Ruiz et al., 2019). How- 2014; Rodrigues and Pereira, 2018). Where soft labels are used, they are variously filtered according ever, the aggregated labels obtained in this way to their distance from the correct labels and then mask the world’s real complexity: instances can be intrinsically ambiguous (Poesio and Artstein, used to weight the training instances rather than as prediction targets. These models still predict only 2005; Zeman, 2010; Plank et al., 2014; Pavlick and true labels (Jamison and Gu"
2021.naacl-main.204,P14-2062,1,0.739372,"ging and morphological stemming. We use the respective data sets from Plank et al. (2014) and Jamison and Gurevych (2015) (where data sets are sufficiently large to train a neural model). In both cases, we use data sets where both one-hot (gold) and probabilistic (soft) labels (i.e., distributions over labels annotations) are available. The code for all models in this paper will be available on github.com/fornaciari. 3.1 POS tagging Data set For this task, we use the data set released by Gimpel et al. (2010) with the crowdThis measures the divergence from P to Q and sourced labels provided by Hovy et al. (2014). The encourages a narrow Q distribution because the same data set was used by Jamison and Gurevych model will try to allocate mass to Q in all the places (2015). Similarly, we use the CONLL Universal 2592 i POS tags (Petrov et al., 2012) and 5-fold crossvalidation. The soft labels come from the annotation of 177 annotators, with at least five annotations for each instance. Differently from Jamison and Gurevych (2015), however, we also test the model on a completely independent test set, released by Plank et al. (2014). This data set does not contain soft labels. However, they are not necessar"
2021.naacl-main.204,2021.naacl-main.49,1,0.714309,"ularization methods for neural networks. Among them, label smoothing (Pereyra et al., 2017) penalizes the cases of over-confident network predictions. Both label smoothing and soft labels reduce overfitting regulating the loss size. However, label smoothing relies on the gold labels’ distribution, not accounting for the instances’ inherent ambiguity, while soft labels selectively train the models to reduce the confidence when dealing with unclear cases, not affecting the prediction of clear cases. Disagreement also relates to the issue of annotator biases (Shah et al., 2020; Sap et al., 2019; Hovy and Yang, 2021), and our method can provide a possible way to address it. Several different lines of research use annotation disagreement. One line focuses on the aggregation of multiple annotations before model training. Seminal work includes the proposal by Dawid and Skene (1979), who proposed an ExpectationMaximization (EM) based aggregation model. This model has since influenced a large body of work on annotation aggregation, and modeling annotator competence (Carpenter et al., 2009; Hovy et al., 2013; Raykar et al., 2010; Paun et al., 2018; Ruiz et al., 2019). In our experiments on POS-tagging, we evalu"
2021.naacl-main.204,D15-1035,0,0.537521,"y et al., 2013; Raykar et al., into the models without soft labels (Plank et al., 2010; Paun et al., 2018; Ruiz et al., 2019). How- 2014; Rodrigues and Pereira, 2018). Where soft labels are used, they are variously filtered according ever, the aggregated labels obtained in this way to their distance from the correct labels and then mask the world’s real complexity: instances can be intrinsically ambiguous (Poesio and Artstein, used to weight the training instances rather than as prediction targets. These models still predict only 2005; Zeman, 2010; Plank et al., 2014; Pavlick and true labels (Jamison and Gurevych, 2015). Kwiatkowski, 2019), or so challenging to evaluate that considerable disagreement between different In contrast to previous approaches, we use Multiannotators is unavoidable. In those cases, it is rea- Task Learning (MTL) to predict a probability distrisonable to wonder whether the ambiguity is indeed bution over the soft labels as additional output. We harmful to the models or whether it carries valuable jointly model the main task of predicting standard information about the relative difficulty of each in- gold labels and the novel auxiliary task of predictstance (Aroyo and Welty, 2015). Se"
2021.naacl-main.204,Q18-1040,1,0.944915,"though, is the lack of universally accepted Usually, the labels used in NLP classification tasks are produced by sets of human annotators. As dis- performance metrics to evaluate the divergence between probability distributions. (Most metrics lack agreement between annotators is common, many an upper bound, making it difficult to assess predicmethods aggregate the different answers into a supposedly correct one (Dawid and Skene, 1979; tion quality). Usually, annotations are incorporated Carpenter, 2008; Hovy et al., 2013; Raykar et al., into the models without soft labels (Plank et al., 2010; Paun et al., 2018; Ruiz et al., 2019). How- 2014; Rodrigues and Pereira, 2018). Where soft labels are used, they are variously filtered according ever, the aggregated labels obtained in this way to their distance from the correct labels and then mask the world’s real complexity: instances can be intrinsically ambiguous (Poesio and Artstein, used to weight the training instances rather than as prediction targets. These models still predict only 2005; Zeman, 2010; Plank et al., 2014; Pavlick and true labels (Jamison and Gurevych, 2015). Kwiatkowski, 2019), or so challenging to evaluate that considerable disagree"
2021.naacl-main.204,Q19-1043,0,0.110921,"Missing"
2021.naacl-main.204,D14-1162,0,0.0838039,"Missing"
2021.naacl-main.204,petrov-etal-2012-universal,0,0.119656,"Missing"
2021.naacl-main.204,W14-1601,1,0.870017,"Missing"
2021.naacl-main.204,E14-1078,1,0.897989,"tations are incorporated Carpenter, 2008; Hovy et al., 2013; Raykar et al., into the models without soft labels (Plank et al., 2010; Paun et al., 2018; Ruiz et al., 2019). How- 2014; Rodrigues and Pereira, 2018). Where soft labels are used, they are variously filtered according ever, the aggregated labels obtained in this way to their distance from the correct labels and then mask the world’s real complexity: instances can be intrinsically ambiguous (Poesio and Artstein, used to weight the training instances rather than as prediction targets. These models still predict only 2005; Zeman, 2010; Plank et al., 2014; Pavlick and true labels (Jamison and Gurevych, 2015). Kwiatkowski, 2019), or so challenging to evaluate that considerable disagreement between different In contrast to previous approaches, we use Multiannotators is unavoidable. In those cases, it is rea- Task Learning (MTL) to predict a probability distrisonable to wonder whether the ambiguity is indeed bution over the soft labels as additional output. We harmful to the models or whether it carries valuable jointly model the main task of predicting standard information about the relative difficulty of each in- gold labels and the novel auxil"
2021.naacl-main.204,P16-2067,1,0.774899,"Q in all the places (2015). Similarly, we use the CONLL Universal 2592 i POS tags (Petrov et al., 2012) and 5-fold crossvalidation. The soft labels come from the annotation of 177 annotators, with at least five annotations for each instance. Differently from Jamison and Gurevych (2015), however, we also test the model on a completely independent test set, released by Plank et al. (2014). This data set does not contain soft labels. However, they are not necessary to test our models. Model We use a tagging model that takes two kinds of input representations, at the character and the word level (Plank et al., 2016). At the character level, we use character embeddings trained on the same data set; at the word level, we use Glove embeddings (Pennington et al., 2014). We feed the word representation into a ‘context bi-RNN’, selecting the hidden state of the RNN at the target word’s position in the sentence. The character representation is then fed into a ‘sequence bi-RNN’, whose output is its final state. The two outputs are concatenated and passed to an attention mechanism, as proposed by Vaswani et al. (2017). In the STL models, the attention mechanisms’ output is passed to a last attention mechanism and"
2021.naacl-main.204,W05-0311,1,0.842083,"Missing"
2021.naacl-main.204,P19-1163,0,0.0225307,"search area of regularization methods for neural networks. Among them, label smoothing (Pereyra et al., 2017) penalizes the cases of over-confident network predictions. Both label smoothing and soft labels reduce overfitting regulating the loss size. However, label smoothing relies on the gold labels’ distribution, not accounting for the instances’ inherent ambiguity, while soft labels selectively train the models to reduce the confidence when dealing with unclear cases, not affecting the prediction of clear cases. Disagreement also relates to the issue of annotator biases (Shah et al., 2020; Sap et al., 2019; Hovy and Yang, 2021), and our method can provide a possible way to address it. Several different lines of research use annotation disagreement. One line focuses on the aggregation of multiple annotations before model training. Seminal work includes the proposal by Dawid and Skene (1979), who proposed an ExpectationMaximization (EM) based aggregation model. This model has since influenced a large body of work on annotation aggregation, and modeling annotator competence (Carpenter et al., 2009; Hovy et al., 2013; Raykar et al., 2010; Paun et al., 2018; Ruiz et al., 2019). In our experiments on"
2021.naacl-main.204,2020.acl-main.468,1,0.71779,"y belongs to the research area of regularization methods for neural networks. Among them, label smoothing (Pereyra et al., 2017) penalizes the cases of over-confident network predictions. Both label smoothing and soft labels reduce overfitting regulating the loss size. However, label smoothing relies on the gold labels’ distribution, not accounting for the instances’ inherent ambiguity, while soft labels selectively train the models to reduce the confidence when dealing with unclear cases, not affecting the prediction of clear cases. Disagreement also relates to the issue of annotator biases (Shah et al., 2020; Sap et al., 2019; Hovy and Yang, 2021), and our method can provide a possible way to address it. Several different lines of research use annotation disagreement. One line focuses on the aggregation of multiple annotations before model training. Seminal work includes the proposal by Dawid and Skene (1979), who proposed an ExpectationMaximization (EM) based aggregation model. This model has since influenced a large body of work on annotation aggregation, and modeling annotator competence (Carpenter et al., 2009; Hovy et al., 2013; Raykar et al., 2010; Paun et al., 2018; Ruiz et al., 2019). In"
2021.naacl-main.329,P01-1006,0,0.103657,"Missing"
2021.naacl-main.329,D16-1245,0,0.0529308,"Missing"
2021.naacl-main.329,N19-1423,0,0.179946,"tasks, previous work is usually based on assuming that either gold anaphors (Hou, 2020; Yu et al., 2020a) or gold mentions (Zhou and Choi, 2018; Yu and Poesio, 2020) are provided. By contrast, in this work we introduce a system that 1 Introduction resolves both single and split-antecedent anaphors, Thanks in part to the latest developments in deep and is evaluated in a more realistic setting that does neural network architectures and contextual word not rely on gold anaphors/mentions. We evaluembeddings (e.g., ELMo (Peters et al., 2018) and ate our system on the ARRAU corpus (Poesio and BERT (Devlin et al., 2019)), the performance of Artstein, 2008; Uryupina et al., 2020), in which models for single-antecedent anaphora resolution both single and split-antecedent anaphors are anhas greatly improved (Wiseman et al., 2016; Clark notated, although the latter are much rarer than the and Manning, 2016b; Lee et al., 2017, 2018; Kan- former. We use the state-of-the-art coreference restor and Globerson, 2019; Joshi et al., 2020). So olution system on ARRAU (Yu et al., 2020b) as our recently, the attention has turned to more com- base system for single-antecedent anaphors. This plex cases of anaphora, such as a"
2021.naacl-main.329,2020.acl-main.132,0,0.138595,"gold split-antecedent anaphors are available. These systems also focus on split-antecedent anaphors only. In this work, we introduce a system that resolves both single and split-antecedent anaphors, and evaluate it in a more realistic setting that uses predicted mentions. We also start addressing the question of how to evaluate single and split-antecedent anaphors together using standard coreference evaluation metrics.1 2012; Peng et al., 2015; Liu et al., 2017; Sakaguchi et al., 2020); pronominal anaphors that cannot be resolved purely using gender (Webster et al., 2018), bridging reference (Hou, 2020; Yu and Poesio, 2020), discourse deixis (Kolhatkar and Hirst, 2014; Marasovi´c et al., 2017; Kolhatkar et al., 2018) and, finally, split-antecedent anaphora (Zhou and Choi, 2018; Yu et al., 2020a) - plural anaphoric reference in which the two antecedents are not part of a single noun phrase. However, a number of hurdles have to be tackled when trying to study these cases of anaphora, ranging from the lack of annotated resources to the rarity of some of these phenomena in the existing ones. Thus, most previous work on resolving these anaphoric relations focused on developing dedicated systems"
2021.naacl-main.329,N18-2108,0,0.123236,"Missing"
2021.naacl-main.329,2020.tacl-1.5,0,0.0350433,"ork architectures and contextual word not rely on gold anaphors/mentions. We evaluembeddings (e.g., ELMo (Peters et al., 2018) and ate our system on the ARRAU corpus (Poesio and BERT (Devlin et al., 2019)), the performance of Artstein, 2008; Uryupina et al., 2020), in which models for single-antecedent anaphora resolution both single and split-antecedent anaphors are anhas greatly improved (Wiseman et al., 2016; Clark notated, although the latter are much rarer than the and Manning, 2016b; Lee et al., 2017, 2018; Kan- former. We use the state-of-the-art coreference restor and Globerson, 2019; Joshi et al., 2020). So olution system on ARRAU (Yu et al., 2020b) as our recently, the attention has turned to more com- base system for single-antecedent anaphors. This plex cases of anaphora, such as anaphora requir- cluster-ranking system interprets single-antecedent ing some sort of commonsense knowledge as in anaphors, singletons and non-referring expressions the Winograd Schema Challenge (Rahman and Ng, jointly. In this work, we extend the system to 1 resolve split-antecedent anaphors. The extended The code is available at https://github.com/ juntaoy/dali-full-anaphora part of the system shares mention re"
2021.naacl-main.329,D17-1021,0,0.595788,"Missing"
2021.naacl-main.329,D19-1588,0,0.0978038,"ase system, and extend it to resolve split-antecedent anaphors. (2017) system has become the blueprint for most subsequent systems. Lee et al. (2018) and KanA few systems resolving split-antecedent tor and Globerson (2019) showed that employing anaphors have been proposed in recent years. Vala contextual ELMo (Peters et al., 2018) and BERT et al. (2016) introduced a system to resolve plural (Devlin et al., 2019) embeddings in the system by pronouns they and them in a fiction corpus they Lee et al. (2017) can significantly improve perfor- themselves annotated. Zhou and Choi (2018) intromance. (Joshi et al., 2019, 2020) fine-tuned BERT duced an entity-linking corpus based on the tranand SpanBERT to further improve performance. scripts of the Friends sitcom. The mentions (in4175 cluding plural mentions) are annotated if they are linked to the main characters. Coreference clusters are then created for mentions linked to the same entities. One issue with this corpus is that it is mainly created for entity-linking, so it is problematic as a coreference dataset, as many mentions are linked to general entities that are not annotated in the text. Zhou and Choi (2018) trained a CNN classifier to determine the"
2021.naacl-main.329,P16-1060,1,0.941051,"configure our system to learn the split-antecedent part and the base system in both JOINT and PRE - TRAINED fashion. The results show both versions work much better than naive baselines based on heuristics and random selection. The PRE - TRAINED version works equally well as the JOINT version on splitantecedent anaphors, but it is better for the other aspects of anaphoric interpretation. In the paper we also begin to address the question of how a system carrying out both single and splitantecedent anaphora resolution should be evaluated. Specifically, we introduce an extended version of LEA (Moosavi and Strube, 2016), a standard coreference metric which can be used to give partial credit for resolution, to evaluate single and splitantecedent anaphors together. Using this metric, we find that our best model achieves a better LEA score than the baselines. We further evaluate our best system in the gold setting to compare with the Yu et al. (2020a) system. The model achieved better performance when compared to their system that is designed solely for split-antecedent task. Recently, Wu et al. (2020) framed coreference resolution task as question answering and showed that the additional pre-training on a larg"
2021.naacl-main.329,P19-1066,0,0.149068,"these aspects of the system are not Anaphora Resolution evaluated. Carrying out such an evaluation requires Single-antecedent anaphora resolution is an ac- a corpus with richer anaphoric annotations, such as ARRAU (Uryupina et al., 2020). tive research topic. The first neural model was Yu et al. (2020b) is the only neural system that introduced by Wiseman et al. (2015) and later extended in (Wiseman et al., 2016). Clark and Man- targets singletons and non-referring expressions. ning (2016b) introduced a hybrid cluster/mention- The system uses the mention representation from Lee et al. (2018); Kantor and Globerson (2019) ranking approach, whereas Clark and Manning and applies a cluster-ranking algorithm to incre(2016a) adapted reinforcement learning to a mention-ranking model. Lee et al. (2017) intro- mentally attach mentions directly to their clusduced the first end-to-end system, performing men- ters. Yu et al. (2020b) showed that performance tion detection and coreference resolution jointly. on single-antecedent anaphors improves by up to The Lee et al. (2017) system was also simpler than 1.4 p.p. when jointly training the model with nonreferring expressions and singletons. We use Yu previous systems, usin"
2021.naacl-main.329,D14-1056,0,0.320223,"systems also focus on split-antecedent anaphors only. In this work, we introduce a system that resolves both single and split-antecedent anaphors, and evaluate it in a more realistic setting that uses predicted mentions. We also start addressing the question of how to evaluate single and split-antecedent anaphors together using standard coreference evaluation metrics.1 2012; Peng et al., 2015; Liu et al., 2017; Sakaguchi et al., 2020); pronominal anaphors that cannot be resolved purely using gender (Webster et al., 2018), bridging reference (Hou, 2020; Yu and Poesio, 2020), discourse deixis (Kolhatkar and Hirst, 2014; Marasovi´c et al., 2017; Kolhatkar et al., 2018) and, finally, split-antecedent anaphora (Zhou and Choi, 2018; Yu et al., 2020a) - plural anaphoric reference in which the two antecedents are not part of a single noun phrase. However, a number of hurdles have to be tackled when trying to study these cases of anaphora, ranging from the lack of annotated resources to the rarity of some of these phenomena in the existing ones. Thus, most previous work on resolving these anaphoric relations focused on developing dedicated systems for the specific task. The systems are usually enhanced by transfer"
2021.naacl-main.329,J18-3007,0,0.246071,"y. In this work, we introduce a system that resolves both single and split-antecedent anaphors, and evaluate it in a more realistic setting that uses predicted mentions. We also start addressing the question of how to evaluate single and split-antecedent anaphors together using standard coreference evaluation metrics.1 2012; Peng et al., 2015; Liu et al., 2017; Sakaguchi et al., 2020); pronominal anaphors that cannot be resolved purely using gender (Webster et al., 2018), bridging reference (Hou, 2020; Yu and Poesio, 2020), discourse deixis (Kolhatkar and Hirst, 2014; Marasovi´c et al., 2017; Kolhatkar et al., 2018) and, finally, split-antecedent anaphora (Zhou and Choi, 2018; Yu et al., 2020a) - plural anaphoric reference in which the two antecedents are not part of a single noun phrase. However, a number of hurdles have to be tackled when trying to study these cases of anaphora, ranging from the lack of annotated resources to the rarity of some of these phenomena in the existing ones. Thus, most previous work on resolving these anaphoric relations focused on developing dedicated systems for the specific task. The systems are usually enhanced by transfer-learning to utilise extra resources, as those ana"
2021.naacl-main.329,J13-4004,0,0.102026,"Missing"
2021.naacl-main.329,D17-1018,0,0.238712,"art to the latest developments in deep and is evaluated in a more realistic setting that does neural network architectures and contextual word not rely on gold anaphors/mentions. We evaluembeddings (e.g., ELMo (Peters et al., 2018) and ate our system on the ARRAU corpus (Poesio and BERT (Devlin et al., 2019)), the performance of Artstein, 2008; Uryupina et al., 2020), in which models for single-antecedent anaphora resolution both single and split-antecedent anaphors are anhas greatly improved (Wiseman et al., 2016; Clark notated, although the latter are much rarer than the and Manning, 2016b; Lee et al., 2017, 2018; Kan- former. We use the state-of-the-art coreference restor and Globerson, 2019; Joshi et al., 2020). So olution system on ARRAU (Yu et al., 2020b) as our recently, the attention has turned to more com- base system for single-antecedent anaphors. This plex cases of anaphora, such as anaphora requir- cluster-ranking system interprets single-antecedent ing some sort of commonsense knowledge as in anaphors, singletons and non-referring expressions the Winograd Schema Challenge (Rahman and Ng, jointly. In this work, we extend the system to 1 resolve split-antecedent anaphors. The extended"
2021.naacl-main.329,N15-1082,0,0.0649557,"Missing"
2021.naacl-main.329,D14-1162,0,0.0901547,"work, we use the system of Yu et al. (2020b) as starting point, and extend it to handle split-antecedent anaphora. Yu et al. (2020b) is a cluster-ranking system that jointly processes singleantecedent anaphors, singletons and non-referring expressions. The system uses the same mention representations as in Lee et al. (2018); Kantor and  0 j= Globerson (2019). The input to the system is a sp (i, j) = sm (i) + sc (j) + spmc (i, j) j ∈ Ci−1 concatenation of contextual BERT (Devlin et al., 2019) embeddings, context-independent GLOVE The extension for split-antecedents uses the same embeddings (Pennington et al., 2014) and learned mention/cluster representations as well as the can4176 didate mentions/clusters of the single-antecedent component. This benefits the split-antecedent anaphors part of the system, that can share the representations learned from more numerous singleantecedent anaphors. As a result, the extension shares the same sm (i) and sc (j) scores as the base system. spmc is calculated by applying a FFNN to the cluster-mention pairwise representations. At test time, we convert sp (i, j) into probabilities (pp (i, j)), and assign split-antecedents to plural mentions when the pp (i, j) between t"
2021.naacl-main.329,N18-1202,0,0.0545469,"extra resource is single-antecedent anaphors. Due to the complexity of these tasks, previous work is usually based on assuming that either gold anaphors (Hou, 2020; Yu et al., 2020a) or gold mentions (Zhou and Choi, 2018; Yu and Poesio, 2020) are provided. By contrast, in this work we introduce a system that 1 Introduction resolves both single and split-antecedent anaphors, Thanks in part to the latest developments in deep and is evaluated in a more realistic setting that does neural network architectures and contextual word not rely on gold anaphors/mentions. We evaluembeddings (e.g., ELMo (Peters et al., 2018) and ate our system on the ARRAU corpus (Poesio and BERT (Devlin et al., 2019)), the performance of Artstein, 2008; Uryupina et al., 2020), in which models for single-antecedent anaphora resolution both single and split-antecedent anaphors are anhas greatly improved (Wiseman et al., 2016; Clark notated, although the latter are much rarer than the and Manning, 2016b; Lee et al., 2017, 2018; Kan- former. We use the state-of-the-art coreference restor and Globerson, 2019; Joshi et al., 2020). So olution system on ARRAU (Yu et al., 2020b) as our recently, the attention has turned to more com- base"
2021.naacl-main.329,poesio-artstein-2008-anaphoric,1,0.84867,"Missing"
2021.naacl-main.329,W12-4501,0,0.0741867,"l anaphors are in fact non-referring, or singletons; other expressions refer to entities which have to be introduced in the discourse model via accomodation processes involving for instance the construction of a plural object out of other entities, as in the case of split-antecedent anaphors; other expressions again are related to existing entities by associative relations, as in one-anaphora or bridging reference. These other anaphoric interpretation processes are much less studied, primarily because the relevant information is not annotated in the dominant corpus for coreference, OntoNotes (Pradhan et al., 2012). Systems such as the Stanford Deterministic Coreference Resolver (Lee et al., 2013) do use linguistically-based heuristic rules to 2 Related Work recognize and filter singletons and non-referring 2.1 Neural Approaches to Single-antecedent expressions, but these aspects of the system are not Anaphora Resolution evaluated. Carrying out such an evaluation requires Single-antecedent anaphora resolution is an ac- a corpus with richer anaphoric annotations, such as ARRAU (Uryupina et al., 2020). tive research topic. The first neural model was Yu et al. (2020b) is the only neural system that introdu"
2021.naacl-main.329,D12-1071,0,0.318738,"Missing"
2021.naacl-main.329,P16-1216,0,0.19252,"Missing"
2021.naacl-main.329,N16-1114,0,0.0423537,"Missing"
2021.naacl-main.329,2020.acl-main.622,0,0.0148005,"edent anaphora resolution should be evaluated. Specifically, we introduce an extended version of LEA (Moosavi and Strube, 2016), a standard coreference metric which can be used to give partial credit for resolution, to evaluate single and splitantecedent anaphors together. Using this metric, we find that our best model achieves a better LEA score than the baselines. We further evaluate our best system in the gold setting to compare with the Yu et al. (2020a) system. The model achieved better performance when compared to their system that is designed solely for split-antecedent task. Recently, Wu et al. (2020) framed coreference resolution task as question answering and showed that the additional pre-training on a large question answering dataset can further improve performance. However, those systems are only focused on singleantecedent anaphors and do not consider the other anaphoric relations. 2.2 Other Aspects of Anaphoric Interpretation Interpreting nominal expressions with respect to a discourse model is not simply a matter of identifying identity links; it also involves recognizing that certain potential anaphors are in fact non-referring, or singletons; other expressions refer to entities w"
2021.naacl-main.329,2020.coling-main.538,1,0.141139,"anaphors, and evaluate it in a more realistic setting that uses predicted mentions. We also start addressing the question of how to evaluate single and split-antecedent anaphors together using standard coreference evaluation metrics.1 2012; Peng et al., 2015; Liu et al., 2017; Sakaguchi et al., 2020); pronominal anaphors that cannot be resolved purely using gender (Webster et al., 2018), bridging reference (Hou, 2020; Yu and Poesio, 2020), discourse deixis (Kolhatkar and Hirst, 2014; Marasovi´c et al., 2017; Kolhatkar et al., 2018) and, finally, split-antecedent anaphora (Zhou and Choi, 2018; Yu et al., 2020a) - plural anaphoric reference in which the two antecedents are not part of a single noun phrase. However, a number of hurdles have to be tackled when trying to study these cases of anaphora, ranging from the lack of annotated resources to the rarity of some of these phenomena in the existing ones. Thus, most previous work on resolving these anaphoric relations focused on developing dedicated systems for the specific task. The systems are usually enhanced by transfer-learning to utilise extra resources, as those anaphoric relations are sparsely annotated. The most frequently used extra resour"
2021.naacl-main.329,2020.coling-main.315,1,0.788524,"Missing"
2021.naacl-main.329,2020.lrec-1.2,1,0.0591745,"anaphors, and evaluate it in a more realistic setting that uses predicted mentions. We also start addressing the question of how to evaluate single and split-antecedent anaphors together using standard coreference evaluation metrics.1 2012; Peng et al., 2015; Liu et al., 2017; Sakaguchi et al., 2020); pronominal anaphors that cannot be resolved purely using gender (Webster et al., 2018), bridging reference (Hou, 2020; Yu and Poesio, 2020), discourse deixis (Kolhatkar and Hirst, 2014; Marasovi´c et al., 2017; Kolhatkar et al., 2018) and, finally, split-antecedent anaphora (Zhou and Choi, 2018; Yu et al., 2020a) - plural anaphoric reference in which the two antecedents are not part of a single noun phrase. However, a number of hurdles have to be tackled when trying to study these cases of anaphora, ranging from the lack of annotated resources to the rarity of some of these phenomena in the existing ones. Thus, most previous work on resolving these anaphoric relations focused on developing dedicated systems for the specific task. The systems are usually enhanced by transfer-learning to utilise extra resources, as those anaphoric relations are sparsely annotated. The most frequently used extra resour"
2021.naacl-main.329,C18-1003,0,0.44766,"Missing"
2021.naacl-main.329,Q18-1042,0,0.0871741,"onditions that assume gold mentions and/or gold split-antecedent anaphors are available. These systems also focus on split-antecedent anaphors only. In this work, we introduce a system that resolves both single and split-antecedent anaphors, and evaluate it in a more realistic setting that uses predicted mentions. We also start addressing the question of how to evaluate single and split-antecedent anaphors together using standard coreference evaluation metrics.1 2012; Peng et al., 2015; Liu et al., 2017; Sakaguchi et al., 2020); pronominal anaphors that cannot be resolved purely using gender (Webster et al., 2018), bridging reference (Hou, 2020; Yu and Poesio, 2020), discourse deixis (Kolhatkar and Hirst, 2014; Marasovi´c et al., 2017; Kolhatkar et al., 2018) and, finally, split-antecedent anaphora (Zhou and Choi, 2018; Yu et al., 2020a) - plural anaphoric reference in which the two antecedents are not part of a single noun phrase. However, a number of hurdles have to be tackled when trying to study these cases of anaphora, ranging from the lack of annotated resources to the rarity of some of these phenomena in the existing ones. Thus, most previous work on resolving these anaphoric relations focused o"
2021.naacl-main.329,P15-1137,0,0.13956,"ms such as the Stanford Deterministic Coreference Resolver (Lee et al., 2013) do use linguistically-based heuristic rules to 2 Related Work recognize and filter singletons and non-referring 2.1 Neural Approaches to Single-antecedent expressions, but these aspects of the system are not Anaphora Resolution evaluated. Carrying out such an evaluation requires Single-antecedent anaphora resolution is an ac- a corpus with richer anaphoric annotations, such as ARRAU (Uryupina et al., 2020). tive research topic. The first neural model was Yu et al. (2020b) is the only neural system that introduced by Wiseman et al. (2015) and later extended in (Wiseman et al., 2016). Clark and Man- targets singletons and non-referring expressions. ning (2016b) introduced a hybrid cluster/mention- The system uses the mention representation from Lee et al. (2018); Kantor and Globerson (2019) ranking approach, whereas Clark and Manning and applies a cluster-ranking algorithm to incre(2016a) adapted reinforcement learning to a mention-ranking model. Lee et al. (2017) intro- mentally attach mentions directly to their clusduced the first end-to-end system, performing men- ters. Yu et al. (2020b) showed that performance tion detectio"
2021.semeval-1.41,J09-4005,0,0.0549194,"s and Pereira, 2018; Peterson et al., 2019) Much recent research has explored the question of whether corpora of this type, besides being more 338 Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), pages 338–347 Bangkok, Thailand (online), August 5–6, 2021. ©2021 Association for Computational Linguistics accurate characterizations of the linguistic reality of language interpretation and image categorization, are also better resources for training nlp and computer vision models, and if so, what is the best way for exploiting disagreements in modeling. Beigman Klebanov and Beigman (2009) used information about disagreements to exclude items on which judgements are unclear (‘hard’ items). In the CrowdTruth project (Aroyo and Welty, 2015; Dumitrache et al., 2019) information about disagreement is used to weigh the items used for training. Plank et al. (2014a) proposed to use the information about disagreement to supplement the gold label during training. Finally, methods were proposed for training directly from the data with disagreements, without first obtaining an aggregated label (Sheng et al., 2008; Rodrigues and Pereira, 2018; Peterson et al., 2019; Uma et al., 2020). Only"
2021.semeval-1.41,N19-1423,0,0.00621773,"t model.10 4 Participating systems Unfortunately, we observed a dramatic difference in the number of participants that signed up to the competition (over 100 groups), the number of groups that participated in the trial phase, and the number of groups that submitted a run for official evaluation.11 Only one group, uor, submitted in the evaluation phase (Osei-Brefo et al., 2021). However, they did submit models for each of the tasks, and did adopt a learning from disagreements approach. pos tagging. For pos tagging, uor developed a novel pos tagging model by fine-tuning the bert language model (Devlin et al., 2019). The (tweet, token) pairs were encoded in the form [cls] Tweeted text [sep] Token [sep] where the ‘[cls]’ token was added for classification and the ‘[sep]’ token separated the tweet from the token under consideration. To learn the class for the token, the learned classification token was passed through a single feed-forward neural network layer with softmax activation. The output of this layer represented the probabilities of the token belonging to each of the 12 classes. To extend this model for crowd learning, uor added an adaptation of the crowd layer from Rodrigues and Pereira (2018). Ra"
2021.semeval-1.41,N19-1224,1,0.89232,"Missing"
2021.semeval-1.41,P11-2008,0,0.593029,"Missing"
2021.semeval-1.41,C16-1177,0,0.0200632,"341 The pos tagging model. The pos tagger is a bilstm (Plank et al., 2016) with additional use of attention over the input word and character embeddings, as used in Uma et al. (2020). The pdis classification model. The model for this task was developed by comparing architectures from two models: a state-of-the-art coreference model and a state-of-the-art is classification model. We combined the mention representation component of Lee et al.’s (2018) coreference resolution system with the mention sorting and non-syntactic feature extraction components of the is classification model proposed by Hou (2016)9 to create a novel is classification model that outperforms Hou (2016) on the pdis corpus. The training parameters were set following Lee et al. (2018). The humour preference learning model. We use as base model for this task Gaussian process preference learning (gppl) with stochastic variational inference, as described and implemented by Simpson and Gurevych (2020). As an input vector to gppl, we first take the mean word embedding of a text, using 300-dimensional word2vec embeddings trained on the Google News corpus (Mikolov et al., 2013). Then, we compute the frequency of each unigram in th"
2021.semeval-1.41,N13-1111,0,0.0272005,"y. Hence, it is a strong baseline for accounting for disagreement among annotators. This same gppl approach set the previous state of the art on the humour dataset (Simpson et al., 2019). The LabelMe image classification model. For this task, we replicated the model from Rodrigues and Pereira (2018). The images were encoded using pretrained cnn layers of the vgg-16 deep neural network (Simonyan et al., 2013). This encoding is passed into a feed-forward neural network layer 9This model was developed for fine-grained information status classification on the isnotes corpus (Markert et al., 2012; Hou et al., 2013). with a relu activated hidden layer with 128 units. A 0.2 dropout is applied to this learned representation which is then passed through a final layer with softmax activation to produce the model’s predictions. The cifar-10 image classification model. The trained model provided for this task is the ResNet34A model (He et al., 2016), a deep residual framework which is one of the best performing systems for the cifar-10 image classification. We made available to participants the publicly available Pytorch implementation of this ResNet model.10 4 Participating systems Unfortunately, we observed"
2021.semeval-1.41,N13-1132,0,0.326957,"Missing"
2021.semeval-1.41,D15-1035,0,0.722999,"ch judgements are unclear (‘hard’ items). In the CrowdTruth project (Aroyo and Welty, 2015; Dumitrache et al., 2019) information about disagreement is used to weigh the items used for training. Plank et al. (2014a) proposed to use the information about disagreement to supplement the gold label during training. Finally, methods were proposed for training directly from the data with disagreements, without first obtaining an aggregated label (Sheng et al., 2008; Rodrigues and Pereira, 2018; Peterson et al., 2019; Uma et al., 2020). Only limited comparisons of these methods have been carried out (Jamison and Gurevych, 2015), and the sparse research landscape remains fragmented; in particular, methods applied in cv have not yet been tested in nlp, and vice versa. The objective of SemEval-2021 Task 12, Learning with Disagreements (Le-wi-Di), was to provide a unified testing framework for learning from disagreements in nlp and cv using datasets containing information about disagreements for interpreting language and classifying images. The expectation being that unifying research on disagreement from different fields may lead to novel insights and impact ai widely. 2 that the crowd learning adaptations of the base"
2021.semeval-1.41,N18-2108,0,0.0415615,"Missing"
2021.semeval-1.41,P12-1084,0,0.036338,"confidence accordingly. Hence, it is a strong baseline for accounting for disagreement among annotators. This same gppl approach set the previous state of the art on the humour dataset (Simpson et al., 2019). The LabelMe image classification model. For this task, we replicated the model from Rodrigues and Pereira (2018). The images were encoded using pretrained cnn layers of the vgg-16 deep neural network (Simonyan et al., 2013). This encoding is passed into a feed-forward neural network layer 9This model was developed for fine-grained information status classification on the isnotes corpus (Markert et al., 2012; Hou et al., 2013). with a relu activated hidden layer with 128 units. A 0.2 dropout is applied to this learned representation which is then passed through a final layer with softmax activation to produce the model’s predictions. The cifar-10 image classification model. The trained model provided for this task is the ResNet34A model (He et al., 2016), a deep residual framework which is one of the best performing systems for the cifar-10 image classification. We made available to participants the publicly available Pytorch implementation of this ResNet model.10 4 Participating systems Unfortun"
2021.semeval-1.41,W16-1706,1,0.843985,"coder disagreement can be genuine—i.e., resulting from debatable, difficult, or linguistic ambiguity—has long been known for anaphora and coreference (Poesio and Artstein, 2005; Versley, 2008; Recasens et al., 2011).1 But in recent years, we have also seen evidence that disagreements among subjects/coders are common with virtually every aspect of language interpretation, from apparently simple aspects such as partof-speech tagging (Plank et al., 2014b), to more 1See also the analysis of disagreements in OntoNotes and word senses in Pradhan et al. (2012), Passonneau et al. (2012), and Martínez Alonso et al. (2016). complex ones like semantic role assignment (Dumitrache et al., 2019), to subjective tasks such as sentiment analysis (Kenyon-Dean et al., 2018), and to the inferences that can be drawn from sentences (Pavlick and Kwiatkowski, 2019). In computer vision, as well, the assumption that gold labels may be specified for items has proven an idealization (Rodrigues and Pereira, 2018)—in fact, possibly even more than for nlp. In many widely used crowdsourced datasets for computer vision, different coders assign equally plausible labels to the same items. The problem of disagreement among coders, inclu"
2021.semeval-1.41,N15-1152,1,0.827054,"ry. 2.2 Evaluation metrics While recent research questions the assumption that a single ‘hard’ label (a gold label) exists for every employed, paid in line with the federal minimum wage. 5http://labelme.csail.mit.edu/Release3.0 6https://github.com/jcpeterson/cifar-10h 3https://github.com/dali-ambiguity 4us-based workers from Amazon Mechanical Turk were 340 item in a dataset, the models proposed for learning from multiple interpretations are still largely evaluated under this assumption, using ‘hard’ measures like accuracy or class-weighted F1 (Sheng et al., 2008; Plank et al., 2014a; Martínez Alonso et al., 2015; Sharmanska et al., 2016; Rodrigues and Pereira, 2018). For reference and comparison reasons, we also evaluate the models produced for this shared task using F1 . However, a way of evaluating models as to their ability to capture disagreement is needed, especially for datasets with substantial extent of disagreement. The simplest ‘soft’ metric of this type is to evaluate ambiguity-aware models by treating the probability distribution of labels they produce as a soft label, and comparing that to the full distribution produced by annotators, using, for example, cross-entropy. This approach was"
2021.semeval-1.41,2021.semeval-1.186,0,0.0350661,"is the ResNet34A model (He et al., 2016), a deep residual framework which is one of the best performing systems for the cifar-10 image classification. We made available to participants the publicly available Pytorch implementation of this ResNet model.10 4 Participating systems Unfortunately, we observed a dramatic difference in the number of participants that signed up to the competition (over 100 groups), the number of groups that participated in the trial phase, and the number of groups that submitted a run for official evaluation.11 Only one group, uor, submitted in the evaluation phase (Osei-Brefo et al., 2021). However, they did submit models for each of the tasks, and did adopt a learning from disagreements approach. pos tagging. For pos tagging, uor developed a novel pos tagging model by fine-tuning the bert language model (Devlin et al., 2019). The (tweet, token) pairs were encoded in the form [cls] Tweeted text [sep] Token [sep] where the ‘[cls]’ token was added for classification and the ‘[sep]’ token separated the tweet from the token under consideration. To learn the class for the token, the learned classification token was passed through a single feed-forward neural network layer with softm"
2021.semeval-1.41,Q14-1025,0,0.0225794,"ld aim to collect all distinct interpretations of an expression (Smyth et al., 1994; Poesio and Artstein, 2005; Aroyo and Welty, 2015; Sharmanska et al., 2016; Plank, 2016; Kenyon-Dean et al., 2018; Firman et al., 2018; Pavlick and Kwiatkowski, 2019). Poesio and Artstein (2005) and Recasens et al. (2012) suggest that the best way to create resources capturing disagreements is by preserving implicit ambiguity—i.e., having multiple annotators label the items, and then keeping all these annotations, not just an aggregated ‘gold standard’. A number of corpora with these characteristics now exist (Passonneau and Carpenter, 2014; Plank et al., 2014a; Dumitrache et al., 2019; Poesio et al., 2019; Rodrigues and Pereira, 2018; Peterson et al., 2019) Much recent research has explored the question of whether corpora of this type, besides being more 338 Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), pages 338–347 Bangkok, Thailand (online), August 5–6, 2021. ©2021 Association for Computational Linguistics accurate characterizations of the linguistic reality of language interpretation and image categorization, are also better resources for training nlp and computer vision models, and i"
2021.semeval-1.41,Q19-1043,0,0.132281,"n recent years, we have also seen evidence that disagreements among subjects/coders are common with virtually every aspect of language interpretation, from apparently simple aspects such as partof-speech tagging (Plank et al., 2014b), to more 1See also the analysis of disagreements in OntoNotes and word senses in Pradhan et al. (2012), Passonneau et al. (2012), and Martínez Alonso et al. (2016). complex ones like semantic role assignment (Dumitrache et al., 2019), to subjective tasks such as sentiment analysis (Kenyon-Dean et al., 2018), and to the inferences that can be drawn from sentences (Pavlick and Kwiatkowski, 2019). In computer vision, as well, the assumption that gold labels may be specified for items has proven an idealization (Rodrigues and Pereira, 2018)—in fact, possibly even more than for nlp. In many widely used crowdsourced datasets for computer vision, different coders assign equally plausible labels to the same items. The problem of disagreement among coders, including experts, on the classification of noisy image data has arisen in many cv applications. This includes classification of astronomical images (Smyth et al., 1994), medical image classification (Raykar et al., 2010), and numerous ot"
2021.semeval-1.41,petrov-etal-2012-universal,0,0.151453,"Missing"
2021.semeval-1.41,E14-1078,1,0.856087,"st a convenient idealization; virtually every project devoted to large-scale annotation has found that genuine disagreements are widespread. In nlp, that annotator/coder disagreement can be genuine—i.e., resulting from debatable, difficult, or linguistic ambiguity—has long been known for anaphora and coreference (Poesio and Artstein, 2005; Versley, 2008; Recasens et al., 2011).1 But in recent years, we have also seen evidence that disagreements among subjects/coders are common with virtually every aspect of language interpretation, from apparently simple aspects such as partof-speech tagging (Plank et al., 2014b), to more 1See also the analysis of disagreements in OntoNotes and word senses in Pradhan et al. (2012), Passonneau et al. (2012), and Martínez Alonso et al. (2016). complex ones like semantic role assignment (Dumitrache et al., 2019), to subjective tasks such as sentiment analysis (Kenyon-Dean et al., 2018), and to the inferences that can be drawn from sentences (Pavlick and Kwiatkowski, 2019). In computer vision, as well, the assumption that gold labels may be specified for items has proven an idealization (Rodrigues and Pereira, 2018)—in fact, possibly even more than for nlp. In many wide"
2021.semeval-1.41,P14-2083,1,0.835772,"st a convenient idealization; virtually every project devoted to large-scale annotation has found that genuine disagreements are widespread. In nlp, that annotator/coder disagreement can be genuine—i.e., resulting from debatable, difficult, or linguistic ambiguity—has long been known for anaphora and coreference (Poesio and Artstein, 2005; Versley, 2008; Recasens et al., 2011).1 But in recent years, we have also seen evidence that disagreements among subjects/coders are common with virtually every aspect of language interpretation, from apparently simple aspects such as partof-speech tagging (Plank et al., 2014b), to more 1See also the analysis of disagreements in OntoNotes and word senses in Pradhan et al. (2012), Passonneau et al. (2012), and Martínez Alonso et al. (2016). complex ones like semantic role assignment (Dumitrache et al., 2019), to subjective tasks such as sentiment analysis (Kenyon-Dean et al., 2018), and to the inferences that can be drawn from sentences (Pavlick and Kwiatkowski, 2019). In computer vision, as well, the assumption that gold labels may be specified for items has proven an idealization (Rodrigues and Pereira, 2018)—in fact, possibly even more than for nlp. In many wide"
2021.semeval-1.41,P16-2067,1,0.810277,"we briefly discuss the baseline models for each task that we provided. In Section 5, we report the results using these base models and two crowd learning approaches: majority voting and the soft loss method (Peterson et al., 2019; Uma et al., 2020). 7Our competition can be found at https://competitions. codalab.org/competitions/25748. 8This proved unnecessary as the inherent difficulty of the shared task was enough of a deterrent. CodaLab was the designated site for hosting SemEval-2021 competitions.7 Le-wi-Di was run in two main phases: 341 The pos tagging model. The pos tagger is a bilstm (Plank et al., 2016) with additional use of attention over the input word and character embeddings, as used in Uma et al. (2020). The pdis classification model. The model for this task was developed by comparing architectures from two models: a state-of-the-art coreference model and a state-of-the-art is classification model. We combined the mention representation component of Lee et al.’s (2018) coreference resolution system with the mention sorting and non-syntactic feature extraction components of the is classification model proposed by Hou (2016)9 to create a novel is classification model that outperforms Hou"
2021.semeval-1.41,W05-0311,1,0.897441,"assumption that natural language (nl) expressions have a single and clearly identifiable interpretation in a given context, or that images have a preferred labels, still underlies most work in nlp and computer vision. However, there is now plenty of evidence that this assumption is just a convenient idealization; virtually every project devoted to large-scale annotation has found that genuine disagreements are widespread. In nlp, that annotator/coder disagreement can be genuine—i.e., resulting from debatable, difficult, or linguistic ambiguity—has long been known for anaphora and coreference (Poesio and Artstein, 2005; Versley, 2008; Recasens et al., 2011).1 But in recent years, we have also seen evidence that disagreements among subjects/coders are common with virtually every aspect of language interpretation, from apparently simple aspects such as partof-speech tagging (Plank et al., 2014b), to more 1See also the analysis of disagreements in OntoNotes and word senses in Pradhan et al. (2012), Passonneau et al. (2012), and Martínez Alonso et al. (2016). complex ones like semantic role assignment (Dumitrache et al., 2019), to subjective tasks such as sentiment analysis (Kenyon-Dean et al., 2018), and to th"
2021.semeval-1.41,N19-1176,1,0.889311,"Missing"
2021.semeval-1.41,W12-4501,0,0.044054,"genuine disagreements are widespread. In nlp, that annotator/coder disagreement can be genuine—i.e., resulting from debatable, difficult, or linguistic ambiguity—has long been known for anaphora and coreference (Poesio and Artstein, 2005; Versley, 2008; Recasens et al., 2011).1 But in recent years, we have also seen evidence that disagreements among subjects/coders are common with virtually every aspect of language interpretation, from apparently simple aspects such as partof-speech tagging (Plank et al., 2014b), to more 1See also the analysis of disagreements in OntoNotes and word senses in Pradhan et al. (2012), Passonneau et al. (2012), and Martínez Alonso et al. (2016). complex ones like semantic role assignment (Dumitrache et al., 2019), to subjective tasks such as sentiment analysis (Kenyon-Dean et al., 2018), and to the inferences that can be drawn from sentences (Pavlick and Kwiatkowski, 2019). In computer vision, as well, the assumption that gold labels may be specified for items has proven an idealization (Rodrigues and Pereira, 2018)—in fact, possibly even more than for nlp. In many widely used crowdsourced datasets for computer vision, different coders assign equally plausible labels to th"
2021.semeval-1.41,recasens-etal-2012-annotating,0,0.0365301,"tion (Raykar et al., 2010), and numerous others (Sharmanska et al., 2016; Rodrigues and Pereira, 2018; Firman et al., 2018). Many ai researchers have concluded that rather than attempting to eliminate disagreements from annotated corpora, we should preserve them—indeed, some researchers have argued that corpora should aim to collect all distinct interpretations of an expression (Smyth et al., 1994; Poesio and Artstein, 2005; Aroyo and Welty, 2015; Sharmanska et al., 2016; Plank, 2016; Kenyon-Dean et al., 2018; Firman et al., 2018; Pavlick and Kwiatkowski, 2019). Poesio and Artstein (2005) and Recasens et al. (2012) suggest that the best way to create resources capturing disagreements is by preserving implicit ambiguity—i.e., having multiple annotators label the items, and then keeping all these annotations, not just an aggregated ‘gold standard’. A number of corpora with these characteristics now exist (Passonneau and Carpenter, 2014; Plank et al., 2014a; Dumitrache et al., 2019; Poesio et al., 2019; Rodrigues and Pereira, 2018; Peterson et al., 2019) Much recent research has explored the question of whether corpora of this type, besides being more 338 Proceedings of the 15th International Workshop on S"
2021.semeval-1.41,P19-1572,1,0.473071,"d bigram frequencies with the mean word embedding vector to obtain the input vector representation for each short text. The gppl model is trained on pairwise labels from the training set to obtain a ranking function that can be used to score test instances or output pairwise label probabilities. As a Bayesian model, it takes into account sparsity and noise in the crowdsourced training labels, and moderates its confidence accordingly. Hence, it is a strong baseline for accounting for disagreement among annotators. This same gppl approach set the previous state of the art on the humour dataset (Simpson et al., 2019). The LabelMe image classification model. For this task, we replicated the model from Rodrigues and Pereira (2018). The images were encoded using pretrained cnn layers of the vgg-16 deep neural network (Simonyan et al., 2013). This encoding is passed into a feed-forward neural network layer 9This model was developed for fine-grained information status classification on the isnotes corpus (Markert et al., 2012; Hou et al., 2013). with a relu activated hidden layer with 128 units. A 0.2 dropout is applied to this learned representation which is then passed through a final layer with softmax acti"
A00-2001,P94-1001,1,0.806809,"trigger. The update rules for dialogue acts that we assume here are a simplified version of the formalisations proposed in (Poesio and Traum, 1998; Traum et al., 1999) (henceforth, PTT). The main aspects of PTT which have been implemented concern the way discourse obligations are handled and the manner in which dialogue participants interact to add information to the common ground. Obligations are essentially social in nature, and directly characterise spoken dialogue; a typical example of a discourse obligation concerns the relationship between questions and answers. Poesio and Traum follow (Traum and Allen, 1994) in suggesting that the utterance of a question imposes an obligation on the hearer to address the question (e.g., by providing an answer), irrespective of intentions. As for the process by which common ground is established, or GROUNDING(Clark and Schaefer, 1989; Traum, 1994), the assumption in PTT is that classical speech act theory is inherently too simplistic in that it ignores the fact that co-operative interaction is essential in discourse; thus, for instance, simply asserting something does not make it become mutually &apos;known&apos; (part of the common ground). It is actually necessary for the"
althobaiti-etal-2014-aranlp,W10-2417,0,\N,Missing
althobaiti-etal-2014-aranlp,P00-1026,0,\N,Missing
althobaiti-etal-2014-aranlp,N06-2013,0,\N,Missing
althobaiti-etal-2014-aranlp,P12-1016,0,\N,Missing
althobaiti-etal-2014-aranlp,R13-1005,1,\N,Missing
broscheit-etal-2010-extending,M95-1005,0,\N,Missing
broscheit-etal-2010-extending,W05-0406,0,\N,Missing
broscheit-etal-2010-extending,C04-1074,0,\N,Missing
broscheit-etal-2010-extending,W01-0717,0,\N,Missing
broscheit-etal-2010-extending,W05-0303,0,\N,Missing
broscheit-etal-2010-extending,J96-1002,0,\N,Missing
broscheit-etal-2010-extending,H05-1004,0,\N,Missing
broscheit-etal-2010-extending,P05-3002,0,\N,Missing
broscheit-etal-2010-extending,P04-1018,0,\N,Missing
broscheit-etal-2010-extending,P94-1019,0,\N,Missing
broscheit-etal-2010-extending,N06-1025,1,\N,Missing
broscheit-etal-2010-extending,P06-1041,0,\N,Missing
broscheit-etal-2010-extending,P02-1014,0,\N,Missing
broscheit-etal-2010-extending,W02-1040,0,\N,Missing
broscheit-etal-2010-extending,J01-4004,0,\N,Missing
broscheit-etal-2010-extending,P04-2010,0,\N,Missing
broscheit-etal-2010-extending,kunze-lemnitzer-2002-germanet,0,\N,Missing
broscheit-etal-2010-extending,poesio-kabadjov-2004-general,1,\N,Missing
broscheit-etal-2010-extending,finthammer-cramer-2008-exploring,0,\N,Missing
broscheit-etal-2010-extending,P03-1023,0,\N,Missing
C00-1045,J95-2003,0,0.70835,"Missing"
C00-1045,W99-0108,0,0.338526,"ing theory. Both accounts say that the determination of the focus depends on syntactic as well as pragmatic factors, but have not been able to pin those factors down. In this paper, we uncover the major factors which determine the focus set in descriptive texts. This new focus definition has been evaluated with respect to two corpora: museum exhibit labels, and newspaper articles. It provides an operationalizable basis for pronoun production, and has been implemented as the reusable module gnome-np. The algorithm behind gnome-np is compared with the most recent pronoun generation algorithm of McCoy and Strube (1999). 1 Introduction Besides the well established problem of pronoun resolution, pronoun generation is now attracting renewed attention. In the past, generation systems generated pronouns without attaching much importance to the problem, one notable exception being the classical algorithm of Dale (1990), loosely based on centering theory. With the emergence of corpus based studies in computational linguistics, the question arises whether it is possible to refine known standard algorithms, or whether an improvement is only to be achieved with the help of world knowledge reasoning – a matter too com"
C00-1045,J99-3001,0,0.103389,"ol. However, Dale’s center definition differs from standard centering theory in that it is defined semantically and not on the basis of a syntactic ranking.2 This approach has some appeal, especially for generation, because it supports the natural modularity between strate2 In particular, Dale adopts the result of the action denoted by the previous clause of a recipe as the center. gic generation – which would determine the semantic center for each utterance – and tactical generation – which decides about grammatical functions. Functional centering. Finally, the centering version suggested by Strube and Hahn (1999) appears to reveal an underlying discourse mechanism responsible for centering: the information structure of an utterance (roughly the givennew pattern) is the deeper reason for the ranking of the forward-looking centers. This permits a generalization of standard centering into a language-independent theory covering both free and fixed word-order languages. It is however then surprising that this result is not made maximal use of in the subsequent generationoriented work of McCoy and Strube (1999). Beyond centering. The questions which remain open with all three approaches - standard centering"
C00-1045,P98-2204,0,0.0919864,"Missing"
C00-2130,P99-1048,0,0.0316569,"s calls these ‘unexplanatory modifiers’; Loebner (1987) showed how these predicates may license the use of definite descriptions in an account of definite descriptions based on functionality); – a head noun taking a complement such as the fact that there is life on Earth (Hawkins calls this subclass ‘NP complements’);  the presence of restrictive modification, as in the inequities of the current land-ownership system. Our system attempts to recognize these syntactic patterns; in addition, it considers as unfamiliar some definites occurring in 4 This list was developed by hand; more recently, Bean and Riloff (1999) proposed methods for automatically extracting from a corpus such special predicates, i.e., heads that correlate well with discourse novelty.  appositive constructions (e.g., Glenn Cox, the president of Phillips Petroleum Co.);  copular constructions (e.g.,the man most likely to gain custody of all this is a career politician named David Dinkins). In our corpus study (Poesio and Vieira, 1998) we found that our subjects did better at identifying discourse-new descriptions all together (K=.68) than they did at distinguish ‘unfamiliar’ from ‘larger situation’ (Hawkins, 1978) cases (K = .63). Th"
C00-2130,J96-2004,0,0.0229313,"Missing"
C00-2130,J86-3001,0,0.270788,"Missing"
C00-2130,J97-1003,0,0.096954,"Missing"
C00-2130,M98-1007,0,0.0271505,"f the problems observed in our previous study concerning agreement between annotators, we evaluated the system both by measuring precision/recall against a ‘gold standard’ and by measuring the agreement between the annotation it produces and the annotators. 2 General Overview At the moment, the only systems engaged in semantic interpretation whose performance can be evaluated on fairly unrestricted text such as the Wall Street Journal articles are based on a shallowprocessing approach, i.e., that do not rely on extensive amounts of hand-coded commonsense knowledge (Carter, 1987; Appelt, 1995; Humphreys et al., 1998).1 Our system is of this type: it only relies on structural information, on the information provided by pre-existing lexical sources such as WordNet (Fellbaum, 1998), on minimal amounts of general hand-coded information, and on information that can be acquired automatically from a corpus. Although we believe that quantitative evaluations of the performance of a system on a large number of examples are the only true assessment of its performance, and therefore a shallow processing approach is virtually unavoidable for implemented systems until better sources of commonsense knowledge become avai"
C00-2130,P99-1047,0,0.127096,"ximate segments, generally by means of lexical density measures (Hearst, 1997) . In fact, the methods to limit the lifespan of discourse entity we considered for our system were even simpler. One type of heuristics we looked at are window-based techniques, i.e., considering as potential antecedents only the discourse entities within fixed-size windows of previous sentences, allowing however for some discourse entities to take a longer life span: we call this method LOOSE SEG MENTATION . More specifically, a discourse entity is considered as potential antecedent for a definite 2 See, however, (Marcu, 1999). description when the antecedent’s head is identical to the description’s head, and soning; for the moment, we only developed heuristic solutions to the problem, including:  the potential antecedent’s distance from the description is within the established window, or else  allowing an antecedent to match with a definite description if the premodifiers of the description are a subset of the premodifiers of the antecedent. This heuristic deals with definites which contain less information than the antecedent, such as an old Victorian house... the house, and prevents matches such as the busine"
C00-2130,E99-1001,0,0.0257662,"Missing"
C00-2130,J98-2001,1,0.832431,"tions. The system is based on the results of a corpus analysis previously reported, which showed how common discourse-new descriptions are in newspaper corpora, and identified several problems to be dealt with when developing computational methods for interpreting bridging descriptions. The annotated corpus produced in this earlier work was used to extensively evaluate the proposed techniques for matching definite descriptions with their antecedents, discourse segmentation, recognizing discourse-new descriptions, and suggesting anchors for bridging descriptions. 1 Motivation In previous work (Poesio and Vieira, 1998) we reported the results of corpus annotation experiments in which the subjects were asked to classify the uses of definite descriptions in Wall Stree Journal articles according to a scheme derived from work by Hawkins (1978) and Prince (1981) and including three classes: DIRECT ANAPHORA, DISCOURSE NEW , and BRIDGING DESCRIPTION (Clark, 1977). This study showed that about half of the time, definite descriptions are used to introduce a new entity in the discourse, rather than to refer to an object already mentioned. We also observed that our subjects didn’t always agree on the classification of"
C00-2130,M95-1017,0,\N,Missing
C08-1121,P95-1017,0,0.0873369,")))) 3.2 Binding Kernels The resolution of pronominal anaphora heavily relies on the syntactic information and relationships between the anaphor and the antecedent candidates, including binding and other constraints, but also context-induced preferences in sub-clauses. Some researchers (Lappin and Leass 1994; Kennedy and Boguraev 1996) use manually designed rules to take into account the grammatical role of the antecedent candidates as well as the governing relations between the candidate and the pronoun, while others use features determined over the parse tree in a machine-learning approach (Aone and Bennett 1995; Yang et al. 2004; Luo and Zitouni 2005). However, such a solution has limitations, since the syntactic features have to be selected and defined manually, and it is still partly an open question which syntactic properties should be considered in anaphora resolution. We follow (Yang et al. 2006; Iida et al. 2006) in using a tree kernel to represent structural information using the subtree that covers a pronoun and its antecedent candidate. Given a sentence like “The Figure 3 graphically shows such tree highlighted with dash lines. More in detail we operate the following tree transformation: (a"
C08-1121,P05-1018,0,0.0198924,",d) are expletives requires a combination of structural information and lexical information (Lappin and Leass 1994; Evans 2001). But some sort of structure also underlies our interpretation of other types of coreference: e.g., knowledge about the structure of names certainly plays a role in recognizing that BJ Habibie is a possible antecedent for Mr. Habibie. Information about coreference relations–i.e., which noun phrases are mentions of the same entity–has been shown to be beneficial in a great number of NLP tasks, including information extraction (McCarthy and Lehnert 1995), text planning (Barzilay and Lapata 2005) and summarization (Steinberger et al. 2007). However, the performance of coreference resolvers on (1) a. John thinks that Peter hates him. b. John hopes that Jane is speaking only to unrestricted text is still quite low. One reason himself. for this is that coreference resolution requires a c. It’s lonely here. great deal of information, ranging from string matching to syntactic constraints to semantic d. It had been raining all day. knowledge to discourse salience information to The need to capture such information suggests c 2008. Licensed under the Creative Commons a role for kernel method"
C08-1121,W05-0406,0,0.0127941,"Missing"
C08-1121,P02-1034,0,0.663339,"corefAttribution-Noncommercial-Share Alike 3.0 Unported lierence resolution. Kernel functions make it poscense (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. sible to capture the similarity between structures 961 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 961–968 Manchester, August 2008 without explicitly enumerating all the substructures, and have therefore been shown to be a viable approach to feature engineering for natural language processing for any task in which structural information plays a role, e.g. (Collins and Duffy 2002; Zelenko et al. 2003; Giuglea and Moschitti 2006; Zanzotto and Moschitti 2006; Moschitti et al. 2007). Indeed, they have already been used in NLP to encode the type of structural information that plays a role in binding constraints (Yang et al. 2006); however, the methods used in this previous work do not make it possible to exploit the full power of kernel functions. In this work, we extend the use of kernel functions for coreference by designing and testing kernels for three subtasks of the coreference task: • Binding constraints • Expletive detection • Aliasing and developing distinct clas"
C08-1121,P06-1117,1,0.56365,"0 Unported lierence resolution. Kernel functions make it poscense (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. sible to capture the similarity between structures 961 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 961–968 Manchester, August 2008 without explicitly enumerating all the substructures, and have therefore been shown to be a viable approach to feature engineering for natural language processing for any task in which structural information plays a role, e.g. (Collins and Duffy 2002; Zelenko et al. 2003; Giuglea and Moschitti 2006; Zanzotto and Moschitti 2006; Moschitti et al. 2007). Indeed, they have already been used in NLP to encode the type of structural information that plays a role in binding constraints (Yang et al. 2006); however, the methods used in this previous work do not make it possible to exploit the full power of kernel functions. In this work, we extend the use of kernel functions for coreference by designing and testing kernels for three subtasks of the coreference task: • Binding constraints • Expletive detection • Aliasing and developing distinct classifiers for each of these tasks. We show that our"
C08-1121,J95-2003,0,0.0306172,"lexical information (as in expletive detection). Kernel functions appear to be a promising candidate to capture structure-sensitive similarities and complex feature combinations, but care is required to ensure they are exploited in the best possible fashion. In this paper we propose kernel functions for three subtasks of coreference resolution - binding constraint detection, expletive identification, and aliasing - together with an architecture to integrate them within the standard framework for coreference resolution. 1 Introduction full common sense reasoning (Sidner 1979; Hobbs 1978, 1979; Grosz et al. 1995; Vieira and Poesio 2000; Mitkov 2002). Much of this information won’t be available to robust coreference resolvers until better methods are found to represent and encode common sense knowledge; but part of the problem is also the need for better methods to encode information that is in part structural, in part lexical. Enforcing binding constraints –e.g., ruling out Peter as antecedent of him in (1a) requires recognizing that the anaphor occurs in a particular type of construction (Chomsky 1981; Lappin and Leass 1994; Yang et al. 2006) whose exact definition however has not yet been agreed up"
C08-1121,P06-1079,0,0.016425,"and Boguraev 1996) use manually designed rules to take into account the grammatical role of the antecedent candidates as well as the governing relations between the candidate and the pronoun, while others use features determined over the parse tree in a machine-learning approach (Aone and Bennett 1995; Yang et al. 2004; Luo and Zitouni 2005). However, such a solution has limitations, since the syntactic features have to be selected and defined manually, and it is still partly an open question which syntactic properties should be considered in anaphora resolution. We follow (Yang et al. 2006; Iida et al. 2006) in using a tree kernel to represent structural information using the subtree that covers a pronoun and its antecedent candidate. Given a sentence like “The Figure 3 graphically shows such tree highlighted with dash lines. More in detail we operate the following tree transformation: (a) To distinguish from other words, we explicitly mark up in the structured feature the pronoun and the antecedent candidate under consideration, by appending a string tag “ANA” and “CANDI” in their respective nodes, i.e. “NN-CANDI” for “man” and “PRP-ANA” for “him”. (b) To reduce the data sparseness, the leaf nod"
C08-1121,C96-1021,0,0.0600413,"n in the path. (S-I (NP-I (PRP-I It)) (VP (VBX have) (NP)) (.)) or, in a similar fashion, (S (NP (PRP it)) (VP (VBZ ’s) (NP (NP (NN time)) (PP (IN for) (NP (PRP$ their) (JJ biannual) (NN powwow)))))) would just be represented as the ST: (S-I (NP-I (PRP-I it)) (VP (BE VBZ) (NP-PRD (NN time)))) 3.2 Binding Kernels The resolution of pronominal anaphora heavily relies on the syntactic information and relationships between the anaphor and the antecedent candidates, including binding and other constraints, but also context-induced preferences in sub-clauses. Some researchers (Lappin and Leass 1994; Kennedy and Boguraev 1996) use manually designed rules to take into account the grammatical role of the antecedent candidates as well as the governing relations between the candidate and the pronoun, while others use features determined over the parse tree in a machine-learning approach (Aone and Bennett 1995; Yang et al. 2004; Luo and Zitouni 2005). However, such a solution has limitations, since the syntactic features have to be selected and defined manually, and it is still partly an open question which syntactic properties should be considered in anaphora resolution. We follow (Yang et al. 2006; Iida et al. 2006) i"
C08-1121,J94-4002,0,0.871698,"tion. 1 Introduction full common sense reasoning (Sidner 1979; Hobbs 1978, 1979; Grosz et al. 1995; Vieira and Poesio 2000; Mitkov 2002). Much of this information won’t be available to robust coreference resolvers until better methods are found to represent and encode common sense knowledge; but part of the problem is also the need for better methods to encode information that is in part structural, in part lexical. Enforcing binding constraints –e.g., ruling out Peter as antecedent of him in (1a) requires recognizing that the anaphor occurs in a particular type of construction (Chomsky 1981; Lappin and Leass 1994; Yang et al. 2006) whose exact definition however has not yet been agreed upon by linguists (indeed, it may only be definable in a graded sense (Sturt 2003; Yang et al. 2006)), witness examples like (1b). Parallelism effects are a good example of structural information inducing preferences rather than constraints. Recognizing that It in examples such as (1c,d) are expletives requires a combination of structural information and lexical information (Lappin and Leass 1994; Evans 2001). But some sort of structure also underlies our interpretation of other types of coreference: e.g., knowledge abo"
C08-1121,H05-1083,0,0.0761939,"f pronominal anaphora heavily relies on the syntactic information and relationships between the anaphor and the antecedent candidates, including binding and other constraints, but also context-induced preferences in sub-clauses. Some researchers (Lappin and Leass 1994; Kennedy and Boguraev 1996) use manually designed rules to take into account the grammatical role of the antecedent candidates as well as the governing relations between the candidate and the pronoun, while others use features determined over the parse tree in a machine-learning approach (Aone and Bennett 1995; Yang et al. 2004; Luo and Zitouni 2005). However, such a solution has limitations, since the syntactic features have to be selected and defined manually, and it is still partly an open question which syntactic properties should be considered in anaphora resolution. We follow (Yang et al. 2006; Iida et al. 2006) in using a tree kernel to represent structural information using the subtree that covers a pronoun and its antecedent candidate. Given a sentence like “The Figure 3 graphically shows such tree highlighted with dash lines. More in detail we operate the following tree transformation: (a) To distinguish from other words, we exp"
C08-1121,W04-2403,1,0.845958,"to all possible child sequences of the tree nodes, i.e. a string kernel combined with a STK. 2.3 Kernel Engineering The Kernels of previous section are basic functions that can be applied to feature vectors, strings and 962 trees. In order to make them effective for a specific task, e.g. for coreference resolution: (a) we can combine them with additive or multiplicative operators and (b) we can design specific data objects (vectors, sequences and tree structures) for the target tasks. It is worth noting that a basic kernel applied to an innovative view of a structure yields a new kernel (e.g. Moschitti and Bejan (2004); Moschitti et al. (2006)), as we show below: Let K(t1 , t2 ) = φ(t1 ) · φ(t2 ) be a basic kernel, where t1 and t2 are two trees. If we map t1 and t2 into two new structures s1 and s2 with a mapping φM (·), we obtain: K(s1 , s2 ) = φ(s1 ) · φ(s2 ) = φ(φM (t1 )) · φ(φM (t2 )) = φ′ (t1 ) · φ′ (t2 )=K′ (t1 , t2 ), which is a noticeably different kernel induced by the mapping φ′ = φ ◦ φM . 3 Kernels for Coreference Resolution In this paper we follow the standard learning approach to coreference developed by Soon et al. (2001) and also used the few variants in Ng and Cardie (2002). In this framewor"
C08-1121,W06-2909,1,0.595127,"ces of the tree nodes, i.e. a string kernel combined with a STK. 2.3 Kernel Engineering The Kernels of previous section are basic functions that can be applied to feature vectors, strings and 962 trees. In order to make them effective for a specific task, e.g. for coreference resolution: (a) we can combine them with additive or multiplicative operators and (b) we can design specific data objects (vectors, sequences and tree structures) for the target tasks. It is worth noting that a basic kernel applied to an innovative view of a structure yields a new kernel (e.g. Moschitti and Bejan (2004); Moschitti et al. (2006)), as we show below: Let K(t1 , t2 ) = φ(t1 ) · φ(t2 ) be a basic kernel, where t1 and t2 are two trees. If we map t1 and t2 into two new structures s1 and s2 with a mapping φM (·), we obtain: K(s1 , s2 ) = φ(s1 ) · φ(s2 ) = φ(φM (t1 )) · φ(φM (t2 )) = φ′ (t1 ) · φ′ (t2 )=K′ (t1 , t2 ), which is a noticeably different kernel induced by the mapping φ′ = φ ◦ φM . 3 Kernels for Coreference Resolution In this paper we follow the standard learning approach to coreference developed by Soon et al. (2001) and also used the few variants in Ng and Cardie (2002). In this framework, training and testing i"
C08-1121,P07-1098,1,0.614863,"poscense (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. sible to capture the similarity between structures 961 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 961–968 Manchester, August 2008 without explicitly enumerating all the substructures, and have therefore been shown to be a viable approach to feature engineering for natural language processing for any task in which structural information plays a role, e.g. (Collins and Duffy 2002; Zelenko et al. 2003; Giuglea and Moschitti 2006; Zanzotto and Moschitti 2006; Moschitti et al. 2007). Indeed, they have already been used in NLP to encode the type of structural information that plays a role in binding constraints (Yang et al. 2006); however, the methods used in this previous work do not make it possible to exploit the full power of kernel functions. In this work, we extend the use of kernel functions for coreference by designing and testing kernels for three subtasks of the coreference task: • Binding constraints • Expletive detection • Aliasing and developing distinct classifiers for each of these tasks. We show that our developed kernels produce high accuracy for both dis"
C08-1121,P02-1014,0,0.478169,"ernel (e.g. Moschitti and Bejan (2004); Moschitti et al. (2006)), as we show below: Let K(t1 , t2 ) = φ(t1 ) · φ(t2 ) be a basic kernel, where t1 and t2 are two trees. If we map t1 and t2 into two new structures s1 and s2 with a mapping φM (·), we obtain: K(s1 , s2 ) = φ(s1 ) · φ(s2 ) = φ(φM (t1 )) · φ(φM (t2 )) = φ′ (t1 ) · φ′ (t2 )=K′ (t1 , t2 ), which is a noticeably different kernel induced by the mapping φ′ = φ ◦ φM . 3 Kernels for Coreference Resolution In this paper we follow the standard learning approach to coreference developed by Soon et al. (2001) and also used the few variants in Ng and Cardie (2002). In this framework, training and testing instances consist of a pair (anaphor, antecedent). During training, a positive instance is created for each anaphor encountered by pairing the anaphor with its closest antecedent; each of the non-coreferential mentions between anaphor and antecedent is used to produce a negative instance. During resolution, every mention to be resolved is paired with each preceding antecedent candidate to form a testing instance. This instance is presented to the classifier which then returns a class label with a confidence value indicating the likelihood that the cand"
C08-1121,J01-4004,0,0.917128,"d to an innovative view of a structure yields a new kernel (e.g. Moschitti and Bejan (2004); Moschitti et al. (2006)), as we show below: Let K(t1 , t2 ) = φ(t1 ) · φ(t2 ) be a basic kernel, where t1 and t2 are two trees. If we map t1 and t2 into two new structures s1 and s2 with a mapping φM (·), we obtain: K(s1 , s2 ) = φ(s1 ) · φ(s2 ) = φ(φM (t1 )) · φ(φM (t2 )) = φ′ (t1 ) · φ′ (t2 )=K′ (t1 , t2 ), which is a noticeably different kernel induced by the mapping φ′ = φ ◦ φM . 3 Kernels for Coreference Resolution In this paper we follow the standard learning approach to coreference developed by Soon et al. (2001) and also used the few variants in Ng and Cardie (2002). In this framework, training and testing instances consist of a pair (anaphor, antecedent). During training, a positive instance is created for each anaphor encountered by pairing the anaphor with its closest antecedent; each of the non-coreferential mentions between anaphor and antecedent is used to produce a negative instance. During resolution, every mention to be resolved is paired with each preceding antecedent candidate to form a testing instance. This instance is presented to the classifier which then returns a class label with a c"
C08-1121,J00-4003,1,0.915842,"(as in expletive detection). Kernel functions appear to be a promising candidate to capture structure-sensitive similarities and complex feature combinations, but care is required to ensure they are exploited in the best possible fashion. In this paper we propose kernel functions for three subtasks of coreference resolution - binding constraint detection, expletive identification, and aliasing - together with an architecture to integrate them within the standard framework for coreference resolution. 1 Introduction full common sense reasoning (Sidner 1979; Hobbs 1978, 1979; Grosz et al. 1995; Vieira and Poesio 2000; Mitkov 2002). Much of this information won’t be available to robust coreference resolvers until better methods are found to represent and encode common sense knowledge; but part of the problem is also the need for better methods to encode information that is in part structural, in part lexical. Enforcing binding constraints –e.g., ruling out Peter as antecedent of him in (1a) requires recognizing that the anaphor occurs in a particular type of construction (Chomsky 1981; Lappin and Leass 1994; Yang et al. 2006) whose exact definition however has not yet been agreed upon by linguists (indeed,"
C08-1121,P06-1006,1,0.913917,"ll common sense reasoning (Sidner 1979; Hobbs 1978, 1979; Grosz et al. 1995; Vieira and Poesio 2000; Mitkov 2002). Much of this information won’t be available to robust coreference resolvers until better methods are found to represent and encode common sense knowledge; but part of the problem is also the need for better methods to encode information that is in part structural, in part lexical. Enforcing binding constraints –e.g., ruling out Peter as antecedent of him in (1a) requires recognizing that the anaphor occurs in a particular type of construction (Chomsky 1981; Lappin and Leass 1994; Yang et al. 2006) whose exact definition however has not yet been agreed upon by linguists (indeed, it may only be definable in a graded sense (Sturt 2003; Yang et al. 2006)), witness examples like (1b). Parallelism effects are a good example of structural information inducing preferences rather than constraints. Recognizing that It in examples such as (1c,d) are expletives requires a combination of structural information and lexical information (Lappin and Leass 1994; Evans 2001). But some sort of structure also underlies our interpretation of other types of coreference: e.g., knowledge about the structure of"
C08-1121,P04-1017,1,0.839828,"s The resolution of pronominal anaphora heavily relies on the syntactic information and relationships between the anaphor and the antecedent candidates, including binding and other constraints, but also context-induced preferences in sub-clauses. Some researchers (Lappin and Leass 1994; Kennedy and Boguraev 1996) use manually designed rules to take into account the grammatical role of the antecedent candidates as well as the governing relations between the candidate and the pronoun, while others use features determined over the parse tree in a machine-learning approach (Aone and Bennett 1995; Yang et al. 2004; Luo and Zitouni 2005). However, such a solution has limitations, since the syntactic features have to be selected and defined manually, and it is still partly an open question which syntactic properties should be considered in anaphora resolution. We follow (Yang et al. 2006; Iida et al. 2006) in using a tree kernel to represent structural information using the subtree that covers a pronoun and its antecedent candidate. Given a sentence like “The Figure 3 graphically shows such tree highlighted with dash lines. More in detail we operate the following tree transformation: (a) To distinguish f"
C08-1121,P06-1051,1,0.82065,"on. Kernel functions make it poscense (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. sible to capture the similarity between structures 961 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 961–968 Manchester, August 2008 without explicitly enumerating all the substructures, and have therefore been shown to be a viable approach to feature engineering for natural language processing for any task in which structural information plays a role, e.g. (Collins and Duffy 2002; Zelenko et al. 2003; Giuglea and Moschitti 2006; Zanzotto and Moschitti 2006; Moschitti et al. 2007). Indeed, they have already been used in NLP to encode the type of structural information that plays a role in binding constraints (Yang et al. 2006); however, the methods used in this previous work do not make it possible to exploit the full power of kernel functions. In this work, we extend the use of kernel functions for coreference by designing and testing kernels for three subtasks of the coreference task: • Binding constraints • Expletive detection • Aliasing and developing distinct classifiers for each of these tasks. We show that our developed kernels produce hi"
C08-1121,J08-1001,0,\N,Missing
C12-2086,D08-1031,0,0.0195255,"els of coreference resolution (Rahman and Ng, 2009, 2011; Ng, 2010) has come from the adoption of richer models of this interpretive task that overcome the limitations and simplifications of earlier models (Soon et al., 2001; Ng and Cardie, 2002), such as the assumption that resolving coreference involves linking mentions. There has also been some progress towards taking advantage of richer forms of information in general and of semantic knowledge in particular. Lexical knowledge has been shown to be clearly useful (Ponzetto and Strube, 2006) and is exploited by most state-of-the-art systems (Bengtson and Roth, 2008; Lee et al., 2011); it has been shown that encyclopedic knowledge as contained e.g., in Wikipedia can help as well (Ponzetto and Strube, 2006; Uryupina et al., 2011). But the ultimate goal is to develop a statistic-based integrated model of semantic interpretation in which coreference interacts with other aspects of interpretation such as predicate-argument structure recognition or discourse structure resolution, as argued in particular by (Hobbs, 1979) and implemented on a small-scale basis in the early, pre-statistical systems (Wilks, 1975; Hobbs et al., 1993; Alshawi, 1992). Most work to t"
C12-2086,doddington-etal-2004-automatic,0,0.0165025,"Missing"
C12-2086,H05-1003,0,0.248782,"‘colleague’ relation with Mr. Smith, then most likely Jack and Mr. Smith are not coreferent. Such information could also be useful to increase recall: if Jack is related by a ‘works-for’ relation to an entity mentioned as ‘Foobar Inc.’ and by a ‘colleague’ relation with Mr. Smith, and Mr. Smith is related by a ‘works-for’ relation to an entity mentioned as ‘the international conglomerate’, then most likely ‘Foobar Inc.’ and ‘the international conglomerate’ are mentions of the same entity. Yet we are only aware of one study exploring the use of such information to improve coreference, namely (Ji et al., 2005), whose approach however was rule-based. In this paper we revisit the topic and compare rule-based methods with machine-learning approaches to integrating relational and coreference information. The structure of the paper is as follows. In Section 2 we discuss previous work on using relational information for coreference. In Section 3 we describe relational information in the ACE corpora. In Section 4 we propose three methods for integrating relational information in a coreference resolver; the experimental setting used to evaluate these methods and the results we obtained are discussed in Sec"
C12-2086,W11-1902,0,0.0171424,"tion (Rahman and Ng, 2009, 2011; Ng, 2010) has come from the adoption of richer models of this interpretive task that overcome the limitations and simplifications of earlier models (Soon et al., 2001; Ng and Cardie, 2002), such as the assumption that resolving coreference involves linking mentions. There has also been some progress towards taking advantage of richer forms of information in general and of semantic knowledge in particular. Lexical knowledge has been shown to be clearly useful (Ponzetto and Strube, 2006) and is exploited by most state-of-the-art systems (Bengtson and Roth, 2008; Lee et al., 2011); it has been shown that encyclopedic knowledge as contained e.g., in Wikipedia can help as well (Ponzetto and Strube, 2006; Uryupina et al., 2011). But the ultimate goal is to develop a statistic-based integrated model of semantic interpretation in which coreference interacts with other aspects of interpretation such as predicate-argument structure recognition or discourse structure resolution, as argued in particular by (Hobbs, 1979) and implemented on a small-scale basis in the early, pre-statistical systems (Wilks, 1975; Hobbs et al., 1993; Alshawi, 1992). Most work to this end has been co"
C12-2086,P10-1142,0,0.0176306,"al structures between mentions and their corresponding relationships. The second approach is to use a joint model enriched with a set of relational features derived from semantic relations of each mention. Both methods have shown to improve the performance of a learning-based state-of-the-art coreference resolver. Keywords: coreference resolution, relation extraction, machine learning. Proceedings of COLING 2012: Posters, pages 883–892, COLING 2012, Mumbai, December 2012. 883 1 Introduction Much of the recent progress in statistical models of coreference resolution (Rahman and Ng, 2009, 2011; Ng, 2010) has come from the adoption of richer models of this interpretive task that overcome the limitations and simplifications of earlier models (Soon et al., 2001; Ng and Cardie, 2002), such as the assumption that resolving coreference involves linking mentions. There has also been some progress towards taking advantage of richer forms of information in general and of semantic knowledge in particular. Lexical knowledge has been shown to be clearly useful (Ponzetto and Strube, 2006) and is exploited by most state-of-the-art systems (Bengtson and Roth, 2008; Lee et al., 2011); it has been shown that"
C12-2086,P02-1014,0,0.078791,"m semantic relations of each mention. Both methods have shown to improve the performance of a learning-based state-of-the-art coreference resolver. Keywords: coreference resolution, relation extraction, machine learning. Proceedings of COLING 2012: Posters, pages 883–892, COLING 2012, Mumbai, December 2012. 883 1 Introduction Much of the recent progress in statistical models of coreference resolution (Rahman and Ng, 2009, 2011; Ng, 2010) has come from the adoption of richer models of this interpretive task that overcome the limitations and simplifications of earlier models (Soon et al., 2001; Ng and Cardie, 2002), such as the assumption that resolving coreference involves linking mentions. There has also been some progress towards taking advantage of richer forms of information in general and of semantic knowledge in particular. Lexical knowledge has been shown to be clearly useful (Ponzetto and Strube, 2006) and is exploited by most state-of-the-art systems (Bengtson and Roth, 2008; Lee et al., 2011); it has been shown that encyclopedic knowledge as contained e.g., in Wikipedia can help as well (Ponzetto and Strube, 2006; Uryupina et al., 2011). But the ultimate goal is to develop a statistic-based i"
C12-2086,I11-1082,1,0.899237,"Missing"
C12-2086,C10-2104,1,0.874953,"Missing"
C12-2086,N06-1025,0,0.0281389,"cember 2012. 883 1 Introduction Much of the recent progress in statistical models of coreference resolution (Rahman and Ng, 2009, 2011; Ng, 2010) has come from the adoption of richer models of this interpretive task that overcome the limitations and simplifications of earlier models (Soon et al., 2001; Ng and Cardie, 2002), such as the assumption that resolving coreference involves linking mentions. There has also been some progress towards taking advantage of richer forms of information in general and of semantic knowledge in particular. Lexical knowledge has been shown to be clearly useful (Ponzetto and Strube, 2006) and is exploited by most state-of-the-art systems (Bengtson and Roth, 2008; Lee et al., 2011); it has been shown that encyclopedic knowledge as contained e.g., in Wikipedia can help as well (Ponzetto and Strube, 2006; Uryupina et al., 2011). But the ultimate goal is to develop a statistic-based integrated model of semantic interpretation in which coreference interacts with other aspects of interpretation such as predicate-argument structure recognition or discourse structure resolution, as argued in particular by (Hobbs, 1979) and implemented on a small-scale basis in the early, pre-statistic"
C12-2086,D09-1101,0,0.0964862,"er is based on the relational structures between mentions and their corresponding relationships. The second approach is to use a joint model enriched with a set of relational features derived from semantic relations of each mention. Both methods have shown to improve the performance of a learning-based state-of-the-art coreference resolver. Keywords: coreference resolution, relation extraction, machine learning. Proceedings of COLING 2012: Posters, pages 883–892, COLING 2012, Mumbai, December 2012. 883 1 Introduction Much of the recent progress in statistical models of coreference resolution (Rahman and Ng, 2009, 2011; Ng, 2010) has come from the adoption of richer models of this interpretive task that overcome the limitations and simplifications of earlier models (Soon et al., 2001; Ng and Cardie, 2002), such as the assumption that resolving coreference involves linking mentions. There has also been some progress towards taking advantage of richer forms of information in general and of semantic knowledge in particular. Lexical knowledge has been shown to be clearly useful (Ponzetto and Strube, 2006) and is exploited by most state-of-the-art systems (Bengtson and Roth, 2008; Lee et al., 2011); it has"
C12-2086,J01-4004,0,0.586408,"eatures derived from semantic relations of each mention. Both methods have shown to improve the performance of a learning-based state-of-the-art coreference resolver. Keywords: coreference resolution, relation extraction, machine learning. Proceedings of COLING 2012: Posters, pages 883–892, COLING 2012, Mumbai, December 2012. 883 1 Introduction Much of the recent progress in statistical models of coreference resolution (Rahman and Ng, 2009, 2011; Ng, 2010) has come from the adoption of richer models of this interpretive task that overcome the limitations and simplifications of earlier models (Soon et al., 2001; Ng and Cardie, 2002), such as the assumption that resolving coreference involves linking mentions. There has also been some progress towards taking advantage of richer forms of information in general and of semantic knowledge in particular. Lexical knowledge has been shown to be clearly useful (Ponzetto and Strube, 2006) and is exploited by most state-of-the-art systems (Bengtson and Roth, 2008; Lee et al., 2011); it has been shown that encyclopedic knowledge as contained e.g., in Wikipedia can help as well (Ponzetto and Strube, 2006; Uryupina et al., 2011). But the ultimate goal is to devel"
C12-2086,P07-1067,0,0.0165651,"tto and Strube, 2006; Uryupina et al., 2011). But the ultimate goal is to develop a statistic-based integrated model of semantic interpretation in which coreference interacts with other aspects of interpretation such as predicate-argument structure recognition or discourse structure resolution, as argued in particular by (Hobbs, 1979) and implemented on a small-scale basis in the early, pre-statistical systems (Wilks, 1975; Hobbs et al., 1993; Alshawi, 1992). Most work to this end has been concerned with the use of semantic role information to improve in particular the resolution of pronouns (Yang and Su, 2007; Ponzetto and Strube, 2006; Bean and Riloff, 2004). However, there has been much more limited investigation of the effect on coreference of the information provided by ACE-style relations. This is surprising given, first, that prima facie, such information should be very useful, and second, that annotated containing both coreference and relational information exist, most notably ACE-05. ACE-style relational information could be useful to increase precision, by ruling out coreference relations between entities already known to be related by other relations: if Jack is related by a ‘colleague’"
C12-2086,N04-1038,0,\N,Missing
C12-2086,D08-1067,0,\N,Missing
C98-1087,J86-3001,0,0.582395,"Missing"
C98-1087,P85-1027,0,0.0640316,"heir algorithm for identifying the cp is not identical to Sidner's. Their analysis of German texts shows a rather good perfornlance for their algorithm, but, as only MSEs are predicted to be accessible, none of the anaphors depending oll focus space information could be resolved. Their algorithm also appears to treat definite descriptions and pronouns uniformly as 'anaphors', which seems problematic in the light of psychological evidence showing that they behave differently, and examples like the following: (7) a. b, John/saw Mary. Hei greeted her. John/ saw Mary. '?'?The lllani greeted her. (Guindon, 1985) proposed an alternative model of the attentional state involving a cache instead of a stack, and Walker (1996) argues that the cache model can account for all of the data that originally motivated the stack model and, in addition, explains the use of informationally redundant utterances. The cache model isn't yet specifed in enough detail for all of its implications for the data discussed here to be clear, but it appears that some of the issues discussed in this paper would have to be addressed in a cache model as well, and that some of our conclusions would apply in a model of that type as w"
C98-1087,P97-1014,0,0.130477,"or to an MSE, or to a discourse topic; SThis would explain the difference in ,eading times observed by (Clark and Sengul, 1979). 555 6. Definite descriptions can refer back to any entity in the global focus, including discourse topics. Tim reason for using the term 'optional' in 2 is that whereas locus spaces can always be described as be~ ing about something, they are not always associated with a 'most salient entity': e.g., the first sentence in (6) introduces several topics (woodwind players, their need to be creative, etc.) but does not introduce an MSE. 5 Related Work In a recent paper, Hahn and Strube (1997) propose to extend centering theory with what is, essentially, Sidner's stack of discourse loci, although their algorithm for identifying the cp is not identical to Sidner's. Their analysis of German texts shows a rather good perfornlance for their algorithm, but, as only MSEs are predicted to be accessible, none of the anaphors depending oll focus space information could be resolved. Their algorithm also appears to treat definite descriptions and pronouns uniformly as 'anaphors', which seems problematic in the light of psychological evidence showing that they behave differently, and examples"
C98-1087,J93-4004,0,0.0300364,"l'erent ways. Nevertheless, we believe that the structure depicted in Figure 1 is a plausible analysis lot"" (1); an alternative analysis would be to take the 4th and 5th sentence as elaborations of and he lavished on those materials an incredibly painstaking technique . . . . but in this case, as well (and in all other rhetorical structures we could consider) sentences 4 and 5 are satellites of sentence 3. (We have employed the set of rhetorical relations currently used to analyse the ILEX data.) The relation between G & S ' s and RST's notion of structure has been analysed by, among others, (Moore and Paris, 1993; Moser and Moore, 1996). According to Moser and Moore, the relation can be characterised as follows: an RST nucleus expresses an intetation I,~; a satellite expresses an intention 1.~; and 1,, dominates [.~. Thus, in (1), the nucleus of the exemplification relation, sentence 3, would dominate the satellite, consisting of sentences 4 and 5. We will make the same assumption here. Hence we can assume that the third sentence in (1) will still be on the stack when processing the 4th and 5th sentences. This would also hold for the alternative rhetorical structures we have considered. 5 4Fox, as wel"
C98-1087,J96-3006,0,0.0767172,"ess, we believe that the structure depicted in Figure 1 is a plausible analysis lot"" (1); an alternative analysis would be to take the 4th and 5th sentence as elaborations of and he lavished on those materials an incredibly painstaking technique . . . . but in this case, as well (and in all other rhetorical structures we could consider) sentences 4 and 5 are satellites of sentence 3. (We have employed the set of rhetorical relations currently used to analyse the ILEX data.) The relation between G & S ' s and RST's notion of structure has been analysed by, among others, (Moore and Paris, 1993; Moser and Moore, 1996). According to Moser and Moore, the relation can be characterised as follows: an RST nucleus expresses an intetation I,~; a satellite expresses an intention 1.~; and 1,, dominates [.~. Thus, in (1), the nucleus of the exemplification relation, sentence 3, would dominate the satellite, consisting of sentences 4 and 5. We will make the same assumption here. Hence we can assume that the third sentence in (1) will still be on the stack when processing the 4th and 5th sentences. This would also hold for the alternative rhetorical structures we have considered. 5 4Fox, as well, used RST to analyse t"
C98-1087,J98-2001,1,0.859133,"nd that the prefen-ed antecedent of a bridging description is a previous MSE in 54 out of 203 cases. In the SOLE corpus, 8 out of 11 bridging descriptions relate to the MSE. Does this mean, then, that we can get rid of focus spaces, and assume that it's MSEs that go on the stack? Before looking at the data, we have to be cleat as to what would count as evidence one way or the other. Even an approach in which only previotis MSEs are on the stack would still allow access to entities which are part of what Grosz called the ~MPLICIT FOCUS o f these MSEs, i.e., the entities that ('As discussed in (Poesio and Vieira, 1998), in general there is more than one potential 'antecedent' for a bridging description in a text. 554 are 'strongly associated' with the MSEs. This notion of 'strong association' is difficult to define- in fact, it is likely to be a matter of d e g r e e - but nevertheless it is plausible to assume that the objects 'strongly associated' with a discourse entity A do not include every discourse entity B which is part of a situation described in the text in which A is also involved; and this can be tested with linguistic examples, up to a point. For example, whereas definite descriptions like the"
C98-1087,W97-1301,1,0.91363,"If, however, she becomes the 'main topic' of discussion, then later, whenever we talk about her again, we can use reduced forms of her proper name, such its King. Again, this difference is not easy to explain in terms of focus spaces if we assume that all objects in a locus space have the same status. A third class of expressions providing evidence relevant to this discussion are bridging descriptions, i.e., definite descriptions like the d o o r that refer to an object associated with a previously mentioned discourse entity such its the hottse, rather than to the entity itself (Clark, 1977). Poesio et al. (1997; 1998) report experiments in which different types of lexical knowledge sources are used to resolve bridging descriptions and other cases of definite descriptions that require more than simple string match for their resolution. Their results indicate that to resolve bridging descriptions it is n o t sufficient simply to find which of the entities in the current focus space is semantically closest to the bridging description: in about half of the cases o f bridging descriptions that could be resolved on the basis of the lexical knowledge used in these experiments, the focus spaces contained an"
C98-1087,J96-2005,0,0.312896,"s an entity explicitly introduced in the text, but can also be a more abstract DISCOURSE TOPIC, by which we 7Notice however that the claim that only MSEs go on the stack does not entail that everything else in the text is simply forgotten-the claim is simply that that in for,nation is not available for resolving ,eferences anymore; presumably it would be stored somewhere in 'long term memory'. Conversely, the claim that everything stays on the stack would have to be supplemented by some story concerning how information gets forgotten-e.g., by some caching mechanism such as the one proposed by Walker (1996). mean an issue / proposition that can be said to characterise the content of the focus space as a whole. In a corpus analysis done in connection with (Poesio et al., 1997; Poesio et al., 1998), we found that 7 out of 70 inferential descriptions were of this type; in the SOLE corpus, in which 3 out of i t bridging descriptions behave this way. An example of this use is the description the problem below, that refers to the problem introduced by the first sentence in the text: (6) Solo woodwind players have to be creative if they want to work a lot, because their repertoire and audience appeal a"
C98-1087,J95-2003,0,\N,Missing
D09-1065,W02-0908,0,0.0339623,"r via preposition-mediated paths (e.g., tagliare con forbici “to cut with scissors”), and where the paths were among the top 30 most frequent in the corpus. In the repubblica-depfilter model, we record co-occurrence with verbs that are linked to the nouns by one of the top 30 paths, but we do not preserve the paths themselves in the features. This is analogous to the model proposed by Pad´o and Lapata (2007). In the repubblica-deppath model, we preserve the paths as part of the features (so that subj-uccidere “subj-kill” and objuccidere count as different features), analogously to Lin (1998), Curran and Moens (2002) and others. For all models, following standard practice in computational linguistics (Evert, 2005), we transform raw co-occurrence counts into log-likelihood ratios. repubblica-window abbattere “demolish” afferrare “seize” impugnare “grasp” tagliare “cut” trovare “find” repubblica-depfilter abbattere “demolish” correre “run” parlare “speak” saltare “jump” tagliare “cut” repubblica-position X-ferire “X-wound” X-usare “X-use” dipingere-X “paint-X” munire-X “supply-X” tagliare-X “cut-X” repubblica-deppath con+tagliare “with+cut” obj+abbattere “obj+demolish” obj+uccidere “obj+kill” intr-subj+vive"
D09-1065,P98-2127,0,0.0179111,"nd object) or via preposition-mediated paths (e.g., tagliare con forbici “to cut with scissors”), and where the paths were among the top 30 most frequent in the corpus. In the repubblica-depfilter model, we record co-occurrence with verbs that are linked to the nouns by one of the top 30 paths, but we do not preserve the paths themselves in the features. This is analogous to the model proposed by Pad´o and Lapata (2007). In the repubblica-deppath model, we preserve the paths as part of the features (so that subj-uccidere “subj-kill” and objuccidere count as different features), analogously to Lin (1998), Curran and Moens (2002) and others. For all models, following standard practice in computational linguistics (Evert, 2005), we transform raw co-occurrence counts into log-likelihood ratios. repubblica-window abbattere “demolish” afferrare “seize” impugnare “grasp” tagliare “cut” trovare “find” repubblica-depfilter abbattere “demolish” correre “run” parlare “speak” saltare “jump” tagliare “cut” repubblica-position X-ferire “X-wound” X-usare “X-use” dipingere-X “paint-X” munire-X “supply-X” tagliare-X “cut-X” repubblica-deppath con+tagliare “with+cut” obj+abbattere “obj+demolish” obj+uccidere"
D09-1065,J07-2002,0,0.0268598,"Missing"
D09-1065,sahlgren-2006-towards,0,\N,Missing
D09-1065,C98-2122,0,\N,Missing
D13-1202,P13-4032,1,0.838757,"Missing"
D13-1202,J90-1003,0,0.540752,"Missing"
D13-1202,N10-1011,0,0.0328261,"y et al. (2012), who showed that purely corpus-based distributional models are at least as good at brain signal prediction tasks as earlier models that made use of manually-generated or controlled knowledge sources (Chang et al., 2011; Palatucci et al., 2009; Pereira et al., 2011), and we evaluate a very recent type of distributional model, namely one that is not extracted from textual data but from image collections through automated visual feature extraction techniques. It has been argued that this new generation of image-based distributional models (Bruni et al., 2011; Bruni et al., 2012b; Feng and Lapata, 2010; Leong and Mihalcea, 2011) provides a more realistic view of meaning, since humans obviously acquire a large proportion of their semantic knowledge from perceptual data. The first question that we ask, thus, is whether the more “grounded” image-based models can help us in interpreting conceptual representations in the brain. More specifically, we will compare the performance of different image-based representations, and we will test whether text- and image-based representations are complementary, so that when used together they can better account for patterns in neural data. Finally, we will"
D13-1202,I11-1162,0,0.0210728,"owed that purely corpus-based distributional models are at least as good at brain signal prediction tasks as earlier models that made use of manually-generated or controlled knowledge sources (Chang et al., 2011; Palatucci et al., 2009; Pereira et al., 2011), and we evaluate a very recent type of distributional model, namely one that is not extracted from textual data but from image collections through automated visual feature extraction techniques. It has been argued that this new generation of image-based distributional models (Bruni et al., 2011; Bruni et al., 2012b; Feng and Lapata, 2010; Leong and Mihalcea, 2011) provides a more realistic view of meaning, since humans obviously acquire a large proportion of their semantic knowledge from perceptual data. The first question that we ask, thus, is whether the more “grounded” image-based models can help us in interpreting conceptual representations in the brain. More specifically, we will compare the performance of different image-based representations, and we will test whether text- and image-based representations are complementary, so that when used together they can better account for patterns in neural data. Finally, we will check for differences betwe"
D13-1202,S12-1019,0,0.144058,"hmarks). If we found that a corpus-based model of meaning can make nontrivial predictions about the structure of the semantic space in the brain, that would make a pretty strong case for the intriguing idea that the model is approximating, in interesting ways, the way in which humans acquire and represent semantic knowledge. 1960 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1960–1970, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics We take as our starting point the extensive experiments reported in Murphy et al. (2012), who showed that purely corpus-based distributional models are at least as good at brain signal prediction tasks as earlier models that made use of manually-generated or controlled knowledge sources (Chang et al., 2011; Palatucci et al., 2009; Pereira et al., 2011), and we evaluate a very recent type of distributional model, namely one that is not extracted from textual data but from image collections through automated visual feature extraction techniques. It has been argued that this new generation of image-based distributional models (Bruni et al., 2011; Bruni et al., 2012b; Feng and Lapata"
D13-1202,P13-1056,0,0.0735555,"Missing"
D13-1202,P12-1015,1,\N,Missing
D13-1202,W11-2503,1,\N,Missing
D13-1202,R11-1055,0,\N,Missing
D18-1218,W98-1501,0,0.190952,"ins, and the viability of using the silver chains as an alternative to the expert-annotated chains in training a state of the art coreference system. The results suggest that our model can extract from crowdsourced annotations coreference chains of comparable quality to those obtained with expert annotation. 1 Introduction The task of identifying and resolving anaphoric reference to discourse entities, known in NLP as coreference resolution, has long been considered a core aspect of language interpretation (Poesio et al., 2016b), also because of its role in applications such as summarization (Baldwin and Morton, 1998; Steinberger et al., 2007), information extraction (Humphreys et al.) or question answering (Morton, 1999; Zheng, 2002). In the 1990s the field made a paradigmatic turn towards corpus based approaches initiated by campaigns such as MUC (Grishman and Sundheim, 1995; Chinchor, 1998) and since then we have seen the development of a range of data-driven approaches, spurred by the development of ever larger and richer datasets. Nowadays, a variety of datasets exist for several languages (Poesio et al., 2016a). These include medium-scale multilingual datasets such as ONTONOTES (Pradhan et al., 2007"
D18-1218,N13-1132,0,0.171783,"18. 2018 Association for Computational Linguistics Quiz Bowl dataset (Guha et al., 2015).2 However, such existing corpora are not widely used yet. One of the reasons for this is the lack of suitable aggregation methods for anaphora. Crowdsourced annotations require aggregation methods to select among the different interpretations produced by the crowd. Standard practice for crowdsourced data analysis has seen a shift in recent years from simple majority vote to much more effective aggregation methods (Smyth et al., 1994; Quoc Viet Hung et al., 2013; Sheshadri and Lease, 2013; Carpenter, 2008; Hovy et al., 2013; Passonneau and Carpenter, 2014). Probabilistic models of annotation, in particular, make it possible to characterize the accuracy of the annotators and correct for their bias (Dawid and Skene, 1979; Passonneau and Carpenter, 2014), to account for item-level effects (e.g.: difficulty) (Whitehill et al., 2009), and to employ different pooling strategies (Carpenter, 2008). However, existing models of annotation cannot be used for anaphora. Such methods assume that coders choose between a fixed set of general labels, the same labels across all annotated items. In anaphoric annotation, by contras"
D18-1218,L16-1323,1,0.85589,"Missing"
D18-1218,M98-1001,0,0.492388,"expert annotation. 1 Introduction The task of identifying and resolving anaphoric reference to discourse entities, known in NLP as coreference resolution, has long been considered a core aspect of language interpretation (Poesio et al., 2016b), also because of its role in applications such as summarization (Baldwin and Morton, 1998; Steinberger et al., 2007), information extraction (Humphreys et al.) or question answering (Morton, 1999; Zheng, 2002). In the 1990s the field made a paradigmatic turn towards corpus based approaches initiated by campaigns such as MUC (Grishman and Sundheim, 1995; Chinchor, 1998) and since then we have seen the development of a range of data-driven approaches, spurred by the development of ever larger and richer datasets. Nowadays, a variety of datasets exist for several languages (Poesio et al., 2016a). These include medium-scale multilingual datasets such as ONTONOTES (Pradhan et al., 2007; Weischedel et al., 2011), which led to the most recent evaluation campaigns, in particular CONLL 2012 (Pradhan et al., 2012), and are used in most current research (Bj¨orkelund and Kuhn, 2014; Martschat and Strube, 2015; Clark and Manning, 2016; Lee et al., 2017). However, there"
D18-1218,P16-1061,0,0.0536606,"s such as MUC (Grishman and Sundheim, 1995; Chinchor, 1998) and since then we have seen the development of a range of data-driven approaches, spurred by the development of ever larger and richer datasets. Nowadays, a variety of datasets exist for several languages (Poesio et al., 2016a). These include medium-scale multilingual datasets such as ONTONOTES (Pradhan et al., 2007; Weischedel et al., 2011), which led to the most recent evaluation campaigns, in particular CONLL 2012 (Pradhan et al., 2012), and are used in most current research (Bj¨orkelund and Kuhn, 2014; Martschat and Strube, 2015; Clark and Manning, 2016; Lee et al., 2017). However, there are still many languages and domains for which no such resources are available, and even for English much larger corpora than ONTONOTES will eventually be required. However, annotating data on the scale required to train state of the art systems using traditional expert annotation would be unaffordable. One alternative is to employ crowdsourcing, either via platforms like Amazon Mechanical Turk and Crowdflower, or using Games-With-A-Purpose (Poesio et al., 2017). Studies such as (Snow et al., 2008; Raykar et al., 2010) have shown that when a sufficiently lar"
D18-1218,felt-etal-2014-momresp,0,0.0188579,"i log ζi,m ∝ Eq [log(1 − πzi,m )]+ Datasets The largest coreference dataset with crowdsourced annotations is the Phrase Detectives corpus. A subset of this corpus is the Phrase Detectives 1.0 dataset (Chamberlain et al., 2016), which also includes gold annotations and can therefore be used to evaluate the accuracy of MPA at mention-pair and coreference chain inference, but is too small to train a state-of-the-art coreference system. To carry out this second type of evaluation we used the approach, common in the crowdsourcing literature (Carpenter, 2008; Raykar et al., 2010; Hovy et al., 2013; Felt et al., 2014), of generating simulated datasets by corrupting the gold standard 1929 Simulation Profile Type Error Distribution 1 2 3 4 Synthetic Synthetic PD-inspired PD-inspired Uniform Sparse Uniform Sparse Table 1: Simulation summary Data Method Accuracy avg. PD 1.0 Figure 2: Sensitivity profiles extracted from the PD corpus: DO (x-axis) vs. DN (y-axis) Synthetic Uniform Synthetic Sparse of an existing corpus. For this purpose, we use the CONLL-2012 dataset (Pradhan et al., 2012), at present the standard dataset for coreference resolution. 3.1.1 Crowdsourced Data The Phrase Detectives (PD) 1.0 dataset"
D18-1218,M95-1001,0,0.393093,"ality to those obtained with expert annotation. 1 Introduction The task of identifying and resolving anaphoric reference to discourse entities, known in NLP as coreference resolution, has long been considered a core aspect of language interpretation (Poesio et al., 2016b), also because of its role in applications such as summarization (Baldwin and Morton, 1998; Steinberger et al., 2007), information extraction (Humphreys et al.) or question answering (Morton, 1999; Zheng, 2002). In the 1990s the field made a paradigmatic turn towards corpus based approaches initiated by campaigns such as MUC (Grishman and Sundheim, 1995; Chinchor, 1998) and since then we have seen the development of a range of data-driven approaches, spurred by the development of ever larger and richer datasets. Nowadays, a variety of datasets exist for several languages (Poesio et al., 2016a). These include medium-scale multilingual datasets such as ONTONOTES (Pradhan et al., 2007; Weischedel et al., 2011), which led to the most recent evaluation campaigns, in particular CONLL 2012 (Pradhan et al., 2012), and are used in most current research (Bj¨orkelund and Kuhn, 2014; Martschat and Strube, 2015; Clark and Manning, 2016; Lee et al., 2017)"
D18-1218,N15-1117,0,0.141499,"ted.1 A second coreference corpus created using crowdsourcing (in the context of a trivia game) also exists, the 1 Note that although the Phrase Detectives corpus is slightly smaller in terms of tokens than the currently largest coreference corpus for English, the CONLL 2012 dataset (Pradhan et al., 2012), it has about twice the number of markables, 390,000 vs. 190,000. 1926 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1926–1937 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Quiz Bowl dataset (Guha et al., 2015).2 However, such existing corpora are not widely used yet. One of the reasons for this is the lack of suitable aggregation methods for anaphora. Crowdsourced annotations require aggregation methods to select among the different interpretations produced by the crowd. Standard practice for crowdsourced data analysis has seen a shift in recent years from simple majority vote to much more effective aggregation methods (Smyth et al., 1994; Quoc Viet Hung et al., 2013; Sheshadri and Lease, 2013; Carpenter, 2008; Hovy et al., 2013; Passonneau and Carpenter, 2014). Probabilistic models of annotation,"
D18-1218,W11-1902,0,0.327094,"which extends the official CONLL scorer to include in the evaluation system-predicted singletons and non referring expressions, both of which are annotated in Phrase Detectives; when singletons and nonreferring expressions are not considered, the Extended Scorer is identical to the official scorer. As in the previous experiment, the evaluation is conducted on the crowdsourced annotated PD 1.0 dataset and on simulated data generated from the CONLL-2012 test set. We compare silver chains produced using our MPA model, using MV, and using the Stanford deterministic coreference system (Stanford) (Lee et al., 2011). To run the latter on PD 1.0, we used the default annotators of the CoreNLP toolkit (Manning et al., 2014) to supply the information required by the coreference sys8 Our use of the term ’silver standard’ should not be confused with the other common use of standard generated out of automatic annotations. tem and switched off the post-processing to output singleton clusters; for the CONLL-2012 data we set the dcoref.replicate.conll = true to run exactly the same method as Lee et al. (2011). On both datasets we evaluated on gold mentions. Table 3 summarizes the results on the crowdsourced annota"
D18-1218,D17-1018,0,0.146626,"nd Sundheim, 1995; Chinchor, 1998) and since then we have seen the development of a range of data-driven approaches, spurred by the development of ever larger and richer datasets. Nowadays, a variety of datasets exist for several languages (Poesio et al., 2016a). These include medium-scale multilingual datasets such as ONTONOTES (Pradhan et al., 2007; Weischedel et al., 2011), which led to the most recent evaluation campaigns, in particular CONLL 2012 (Pradhan et al., 2012), and are used in most current research (Bj¨orkelund and Kuhn, 2014; Martschat and Strube, 2015; Clark and Manning, 2016; Lee et al., 2017). However, there are still many languages and domains for which no such resources are available, and even for English much larger corpora than ONTONOTES will eventually be required. However, annotating data on the scale required to train state of the art systems using traditional expert annotation would be unaffordable. One alternative is to employ crowdsourcing, either via platforms like Amazon Mechanical Turk and Crowdflower, or using Games-With-A-Purpose (Poesio et al., 2017). Studies such as (Snow et al., 2008; Raykar et al., 2010) have shown that when a sufficiently large number of worker"
D18-1218,P14-5010,0,0.00248713,"on referring expressions, both of which are annotated in Phrase Detectives; when singletons and nonreferring expressions are not considered, the Extended Scorer is identical to the official scorer. As in the previous experiment, the evaluation is conducted on the crowdsourced annotated PD 1.0 dataset and on simulated data generated from the CONLL-2012 test set. We compare silver chains produced using our MPA model, using MV, and using the Stanford deterministic coreference system (Stanford) (Lee et al., 2011). To run the latter on PD 1.0, we used the default annotators of the CoreNLP toolkit (Manning et al., 2014) to supply the information required by the coreference sys8 Our use of the term ’silver standard’ should not be confused with the other common use of standard generated out of automatic annotations. tem and switched off the post-processing to output singleton clusters; for the CONLL-2012 data we set the dcoref.replicate.conll = true to run exactly the same method as Lee et al. (2011). On both datasets we evaluated on gold mentions. Table 3 summarizes the results on the crowdsourced annotated PD 1.0 dataset. The silver chains obtained using our MPA model are of a far better quality than those o"
D18-1218,Q15-1029,0,0.0432173,"oaches initiated by campaigns such as MUC (Grishman and Sundheim, 1995; Chinchor, 1998) and since then we have seen the development of a range of data-driven approaches, spurred by the development of ever larger and richer datasets. Nowadays, a variety of datasets exist for several languages (Poesio et al., 2016a). These include medium-scale multilingual datasets such as ONTONOTES (Pradhan et al., 2007; Weischedel et al., 2011), which led to the most recent evaluation campaigns, in particular CONLL 2012 (Pradhan et al., 2012), and are used in most current research (Bj¨orkelund and Kuhn, 2014; Martschat and Strube, 2015; Clark and Manning, 2016; Lee et al., 2017). However, there are still many languages and domains for which no such resources are available, and even for English much larger corpora than ONTONOTES will eventually be required. However, annotating data on the scale required to train state of the art systems using traditional expert annotation would be unaffordable. One alternative is to employ crowdsourcing, either via platforms like Amazon Mechanical Turk and Crowdflower, or using Games-With-A-Purpose (Poesio et al., 2017). Studies such as (Snow et al., 2008; Raykar et al., 2010) have shown tha"
D18-1218,W99-0212,0,0.250984,"of the art coreference system. The results suggest that our model can extract from crowdsourced annotations coreference chains of comparable quality to those obtained with expert annotation. 1 Introduction The task of identifying and resolving anaphoric reference to discourse entities, known in NLP as coreference resolution, has long been considered a core aspect of language interpretation (Poesio et al., 2016b), also because of its role in applications such as summarization (Baldwin and Morton, 1998; Steinberger et al., 2007), information extraction (Humphreys et al.) or question answering (Morton, 1999; Zheng, 2002). In the 1990s the field made a paradigmatic turn towards corpus based approaches initiated by campaigns such as MUC (Grishman and Sundheim, 1995; Chinchor, 1998) and since then we have seen the development of a range of data-driven approaches, spurred by the development of ever larger and richer datasets. Nowadays, a variety of datasets exist for several languages (Poesio et al., 2016a). These include medium-scale multilingual datasets such as ONTONOTES (Pradhan et al., 2007; Weischedel et al., 2011), which led to the most recent evaluation campaigns, in particular CONLL 2012 (P"
D18-1218,nissim-etal-2004-annotation,0,0.0284103,"gated in the sense discussed below; rather, an expert adjudicates between the interpretations produced by players. in a standard annotation task. The implementation is available as supplementary material. 2 A Mention-Pair Model of Annotation Traditional models of annotation (Dawid and Skene, 1979; Smyth et al., 1994; Raykar et al., 2010; Hovy et al., 2013) are specified assuming the annotations are chosen among a general set of classes that is consistent across the annotated items. This is the case in a type of annotation closely related to anaphoric annotation, information status annotation (Nissim et al., 2004; Riester et al., 2010). In this type of annotation, an annotator marks a mention as either discourse old (DO) – referring to an existing entity (coreference chain) – or as discourse-new (DN) – introducing a new coreference chain, but without specifying which coreference chain the mention belongs to, if any. We will refer below to categories such as DN and DO as (general) classes. Traditional models of annotation can model this type of annotation, but not the task of anaphoric annotation proper. In standard annotation schemes for anaphora/coreference (Poesio et al., 2016a) the annotator may ma"
D18-1218,passonneau-2004-computing,0,0.0651684,"accuracy of the annotators and correct for their bias (Dawid and Skene, 1979; Passonneau and Carpenter, 2014), to account for item-level effects (e.g.: difficulty) (Whitehill et al., 2009), and to employ different pooling strategies (Carpenter, 2008). However, existing models of annotation cannot be used for anaphora. Such methods assume that coders choose between a fixed set of general labels, the same labels across all annotated items. In anaphoric annotation, by contrast, coders relate markables to coreference chains which depend on the markables that are annotated in that given document (Passonneau, 2004; Artstein and Poesio, 2008) Contributions In this paper we propose a mention pair-based approach to aggregating crowdsourced anaphoric annotations. Concretely, we introduce a new model of annotation capable of inferring the most likely mention pairs from crowdannotated anaphoric relations. We then use these pairs to build the most likely coreference chains. This approach to building chains is evaluated on both crowdsourced and synthetic (via simulation) coreference datasets. The evaluations include assessing the accuracy of the inferred mention pairs; the quality of the chains; and the viabil"
D18-1218,Q14-1025,0,0.55094,"n for Computational Linguistics Quiz Bowl dataset (Guha et al., 2015).2 However, such existing corpora are not widely used yet. One of the reasons for this is the lack of suitable aggregation methods for anaphora. Crowdsourced annotations require aggregation methods to select among the different interpretations produced by the crowd. Standard practice for crowdsourced data analysis has seen a shift in recent years from simple majority vote to much more effective aggregation methods (Smyth et al., 1994; Quoc Viet Hung et al., 2013; Sheshadri and Lease, 2013; Carpenter, 2008; Hovy et al., 2013; Passonneau and Carpenter, 2014). Probabilistic models of annotation, in particular, make it possible to characterize the accuracy of the annotators and correct for their bias (Dawid and Skene, 1979; Passonneau and Carpenter, 2014), to account for item-level effects (e.g.: difficulty) (Whitehill et al., 2009), and to employ different pooling strategies (Carpenter, 2008). However, existing models of annotation cannot be used for anaphora. Such methods assume that coders choose between a fixed set of general labels, the same labels across all annotated items. In anaphoric annotation, by contrast, coders relate markables to cor"
D18-1218,P14-2083,0,0.19083,"where each mention is paired with the most voted label.7 The evaluation is conducted on the crowdsourced annotated PD 1.0 dataset and on simulated data generated from the CONLL-2012 test set. The results, summarized in Table 2, indicate the mention pairs inferred by our model (MPA) obtain a much better level of agreement with the gold mention pairs, compared with the output of the majority vote (MV) baseline. MV implicitly assumes equal expertise among the annotators, which has repeatedly been shown to be false in annotation practice (Poesio and Artstein, 2005; Passonneau and Carpenter, 2014; Plank et al., 2014). 3.3 Evaluation 2: Silver Chain Quality • Distribute the errors uniformly random given the remaining mass (1 - sensitivity) After the mention pairs have been inferred using MPA , producing the coreference chains – we will henceforth refer to the coreference chains thus ob• Distribute the errors in a sparse manner, i.e., assume that some errors will be more likely 7 Throughout the paper we report the best majority vote result after 10 random rounds of splitting ties. 1931 CoNLL 2012 Test Dataset MUC Simulation Method P R F1 P R F1 P R F1 None Stanford 89.78 73.88 81.06 83.93 59.22 69.44 73.87"
D18-1218,W05-0311,1,0.878933,"ween generic classes and specific labels the MPA model is equivalent to training K binary Bayesian versions of the Dawid 3 Notation: jj[i,m,n] returns the index of the annotator who made the n-th decision on the m-th label of mention i. Figure 1: Plate diagram for MPA and Skene (1979) model (one for each general class) on data processed using the binary relevance method. Note also that whereas traditional models of annotation assume one true class per annotated item, an implicit benefit of our approach is allowing for potentially multiple true classes, which can be useful to detect ambiguity (Poesio and Artstein, 2005), but we don’t exploit that in this work. 2.1 Parameter Estimation We infer the parameters of the proposed model using Variational Inference (VI). Unlike Markov Chain Monte Carlo (MCMC) approaches (e.g.: Gibbs Sampling, Hamiltonian Monte Carlo), VI is deterministic, fast, and benefits from a clear convergence criterion (Blei et al., 2017). Specifically we approximate the intractable posterior p(θ|D) with a variational distribution q(θ) such that the Kullback-Leibler (KL) divergence between the two distributions is minimized. It can be shown this minimization is equivalent to maximizing the evi"
D18-1218,P79-1022,0,0.108072,"Missing"
D18-1218,W12-4501,0,0.642096,"9; Zheng, 2002). In the 1990s the field made a paradigmatic turn towards corpus based approaches initiated by campaigns such as MUC (Grishman and Sundheim, 1995; Chinchor, 1998) and since then we have seen the development of a range of data-driven approaches, spurred by the development of ever larger and richer datasets. Nowadays, a variety of datasets exist for several languages (Poesio et al., 2016a). These include medium-scale multilingual datasets such as ONTONOTES (Pradhan et al., 2007; Weischedel et al., 2011), which led to the most recent evaluation campaigns, in particular CONLL 2012 (Pradhan et al., 2012), and are used in most current research (Bj¨orkelund and Kuhn, 2014; Martschat and Strube, 2015; Clark and Manning, 2016; Lee et al., 2017). However, there are still many languages and domains for which no such resources are available, and even for English much larger corpora than ONTONOTES will eventually be required. However, annotating data on the scale required to train state of the art systems using traditional expert annotation would be unaffordable. One alternative is to employ crowdsourcing, either via platforms like Amazon Mechanical Turk and Crowdflower, or using Games-With-A-Purpose"
D18-1218,riester-etal-2010-recursive,0,0.0517513,"Missing"
D18-1218,D08-1027,0,0.38107,"Missing"
D18-1218,J01-4004,0,0.756582,"ing to a discourse new entity as above; but in case the mention is identified as discourse-old, this entity, or coreference chain–the set of coreferring mentions–is also specified. The available coreference chains differ from document to document. Our proposal for a probabilistic model of this type of annotation is based on one of the most widely used models of coreference resolution: the mention pair model. In the mention pair model, the task of linking the mention to a coreference chain/entity is split in two parts: classifying mention pairs as coreferring or not, and subsequent clustering (Soon et al., 2001; Hoste, 2016). The model we propose addresses the first part. More formally, the crowdsourced data to be modeled consists of I mentions (indexed by i) annotated by a total of J coders (indexed by j). Each mention i has Ni annotations (indexed by n), for a total of Mi distinct labels (indexed by m). Each label m of mention i belongs to a class zi,m . The label of a mention could be the ID of the antecedent, in case that mention is annotated as belonging to the discourse old (general) class; or could be discourse new or another general class (e.g.: property, non referring). In these latter case"
day-etal-2008-corpus,N07-1011,1,\N,Missing
day-etal-2008-corpus,S07-1027,0,\N,Missing
day-etal-2008-corpus,W03-0405,0,\N,Missing
E14-1030,P09-2078,0,0.422982,"ers writing glowing reviews of their own works / hotels (and occasionally also negative reviews of the competitors).1 The prevalence of this phenomenon has been revealed by campaigners such as crime writer Jeremy Duns, who exposed a number of fellow authors involved in such practices.2 A number of sites have also emerged offering Amazon reviews to authors for a fee.3 Several automatic techniques for exposing such deceptive reviews have been proposed in recent years (Feng et al., 2012; Ott et al., 2001). But like all work on deceptive language (computational or otherwise) (Newman et al., 2003; Strapparava and Mihalcea, 2009), such works suffer from a serious problem: the lack of a gold standard containing ‘real life’ examples of deceptive uses of language. This is because it is very difficult to find definite proof that an Amazon review is either deceptive or genuine. Thus most researchers recreate deceptive behavior in the lab, as done by Newman et al. (2003). For instance, Ott et al. (2001), Feng et al. (2012) and Strapparava and Mihalcea (2009) used crowdsourcing, asking turkers to produce instances of deceptive behavior. Finally, Li et al. (2011) classify reviews as deceptive or truthful by hand on the basis"
E14-1030,P12-2034,0,0.350545,"omer confidence in such reviews is often misplaced, due to the growth of the so-called sock puppetry phenomenon: authors / hoteliers writing glowing reviews of their own works / hotels (and occasionally also negative reviews of the competitors).1 The prevalence of this phenomenon has been revealed by campaigners such as crime writer Jeremy Duns, who exposed a number of fellow authors involved in such practices.2 A number of sites have also emerged offering Amazon reviews to authors for a fee.3 Several automatic techniques for exposing such deceptive reviews have been proposed in recent years (Feng et al., 2012; Ott et al., 2001). But like all work on deceptive language (computational or otherwise) (Newman et al., 2003; Strapparava and Mihalcea, 2009), such works suffer from a serious problem: the lack of a gold standard containing ‘real life’ examples of deceptive uses of language. This is because it is very difficult to find definite proof that an Amazon review is either deceptive or genuine. Thus most researchers recreate deceptive behavior in the lab, as done by Newman et al. (2003). For instance, Ott et al. (2001), Feng et al. (2012) and Strapparava and Mihalcea (2009) used crowdsourcing, askin"
E14-1030,P11-1032,0,\N,Missing
E14-3012,C12-2005,0,0.19046,"edia Articles into NE Categories Categorising Wikipedia articles is the initial step in producing NE training data. Therefore, all Wikipedia articles need to be classified into a specific set of named entity types. 3 The terms ‘type’, ‘class’ and ‘category’ are used interchangeably in this paper. 4 108 http://dumps.wikimedia.org/arwiki/ 5.1 The Dataset and Annotation In order to optimise features, we implemented a filtered version of the bag-of-words article representation (e.g., removing punctuation marks and symbols) to classify the Arabic Wikipedia documents instead of using a raw dataset (Alotaibi and Lee, 2012). In addition, the same study shows the high impact of applying tokenisation6 as opposed to the neutral effect of using stemming. We used the filtered features proposed in the study of Alotaibi and Lee (2012), which included removing punctuation marks, symbols, filtering stop words, and normalising digits. We extended the features, however, by utilising the tokenisation scheme that involves separating conjunctions, prepositions, and pronouns from each word. The feature set has been represented using Term Frequency-Inverse Document Frequency (T F − IDF ). This representation method is a numeric"
E14-3012,I13-1045,0,0.174081,"existing systems. The NEWS dataset is also a news-wire domain dataset collected by Darwish (2013) from the RSS feed of the Arabic version of news.google.com from October 2012. The RSS consists of the headline and the first 50 to 100 words in the news articles. This set contains approximately 15k tokens. The third test set was extracted randomly from Twitter and contains a set of 1,423 tweets authored in November 2011. It has approximately 26k tokens (Darwish, 2013). 8.2 Table 4: The results of Supervised Classifiers. 8.4 Results We compared a system trained on WDC with the systems trained by Alotaibi and Lee (2013) on two datasets, WikiFANE(whole) and WikiFANE(selective), which are also automatically collected from Arabic Wikipedia. The evaluation process was conducted by testing them on the ANERcorp set. The results shown in Table 5 prove that the methodology we proposed in this paper produces a dataset that outperforms the two other datasets in terms of recall and F-measure. Our Supervised Classifier All experiments to train and build a probabilistic classifier were conducted using Conditional Random Fields (CRF)12 . Regarding the features used in all our experiments, we selected the most successful f"
E14-3012,W03-0430,0,0.0311711,"a mechanism based on the high coverage of Wikipedia in order to address two challenges particular to tagging NEs in Arabic text: rich morphology and the absence of capitalisation. The corpus created with our new method (WDC) has been used to train an NE tagger which has been tested on different domains. Judging by the results, an NE tagger trained on WDC can compete with those trained on manually annotated corpora. 1 Introduction Supervised learning techniques are well known for their effectiveness to develop Named Entity Recognition (NER) taggers (Bikel et al., 1997; Sekine and others, 1998; McCallum and Li, 2003; Benajiba et al., 2008). The main disadvantage of supervised learning is that it requires a large annotated corpus. Although a substantial amount of annotated data is available for some languages, for other languages, including Arabic, more work is needed to enrich their linguistic resources. In fact, changing the domain or just expanding the set of classes always requires domain-specific experts and new annotated data, both of which cost time and effort. Therefore, current research focuses on approaches that require minimal human intervention to facilitate the process of moving the NE classi"
E14-3012,R13-1005,1,0.499362,"Missing"
E14-3012,P03-2031,0,0.42175,"nt learning, which depends on the recruitment of external knowledge to increase the performance of the classifier, or to automatically create new resources used in the learning stage. Kazama and Torisawa (2007) exploited Wikipedia-based features to improve their NE machine learning recogniser’s F-score by three percent. Their method retrieved the corresponding Wikipedia entry for each candidate word sequence in the CoNLL 2003 dataset and extracted a category label from the first sentence of the entry. The automatic creation of training data has also been investigated using external knowledge. An et al. (2003) extracted sentences containing listed entities from the web, and produced a 1.8 million Korean word dataset. Their corpus Wikipedia offers several ways to group articles. One method is to group articles by lists. The items on these lists include links to articles in a particular subject area, and may include additional information about the listed items. For example, ‘list of scientists’ contains links to articles of scientists and also links to more specific lists of scientists. 2.2 Links A link is a method used by Wikipedia to link pages within wiki environments. Links are enclosed in doubl"
E14-3012,U08-1016,0,0.575018,"lchester, UK {mjaltha, udo, poesio}@essex.ac.uk Abstract classes. Semi-supervised and unsupervised learning approaches, along with the automatic creation of tagged corpora, are alternatives that avoid manually annotated data (Richman and Schone, 2008; Althobaiti et al., 2013). The high coverage and rich informational structure of online encyclopedias can be exploited for the automatic creation of datasets. For example, many researchers have investigated the use of Wikipedia’s structure to classify Wikipedia articles and to transform links into NE annotations according to the link target type (Nothman et al., 2008; Ringland et al., 2009). In this paper we present our approach to automatically derive a large NE annotated corpora from Arabic Wikipedia. The key to our method lies in the exploitation of Wikipedia’s concepts, specifically anchor texts1 and redirects, to handle the rich morphology in Arabic, and thereby eliminate the need to perform any deep morphological analysis. In addition, a capitalisation probability measure has been introduced and incorporated into the approach in order to replace the capitalisation feature that does not exist in the Arabic script. This capitalisation measure has been"
E14-3012,D08-1030,0,0.216186,"e high coverage of Wikipedia in order to address two challenges particular to tagging NEs in Arabic text: rich morphology and the absence of capitalisation. The corpus created with our new method (WDC) has been used to train an NE tagger which has been tested on different domains. Judging by the results, an NE tagger trained on WDC can compete with those trained on manually annotated corpora. 1 Introduction Supervised learning techniques are well known for their effectiveness to develop Named Entity Recognition (NER) taggers (Bikel et al., 1997; Sekine and others, 1998; McCallum and Li, 2003; Benajiba et al., 2008). The main disadvantage of supervised learning is that it requires a large annotated corpus. Although a substantial amount of annotated data is available for some languages, for other languages, including Arabic, more work is needed to enrich their linguistic resources. In fact, changing the domain or just expanding the set of classes always requires domain-specific experts and new annotated data, both of which cost time and effort. Therefore, current research focuses on approaches that require minimal human intervention to facilitate the process of moving the NE classifiers to new domains and"
E14-3012,P08-1001,0,0.55283,"Missing"
E14-3012,A97-1029,0,0.105342,"ging candidate NEs. Herein we also introduce a mechanism based on the high coverage of Wikipedia in order to address two challenges particular to tagging NEs in Arabic text: rich morphology and the absence of capitalisation. The corpus created with our new method (WDC) has been used to train an NE tagger which has been tested on different domains. Judging by the results, an NE tagger trained on WDC can compete with those trained on manually annotated corpora. 1 Introduction Supervised learning techniques are well known for their effectiveness to develop Named Entity Recognition (NER) taggers (Bikel et al., 1997; Sekine and others, 1998; McCallum and Li, 2003; Benajiba et al., 2008). The main disadvantage of supervised learning is that it requires a large annotated corpus. Although a substantial amount of annotated data is available for some languages, for other languages, including Arabic, more work is needed to enrich their linguistic resources. In fact, changing the domain or just expanding the set of classes always requires domain-specific experts and new annotated data, both of which cost time and effort. Therefore, current research focuses on approaches that require minimal human intervention t"
E14-3012,U09-1004,0,0.350273,"udo, poesio}@essex.ac.uk Abstract classes. Semi-supervised and unsupervised learning approaches, along with the automatic creation of tagged corpora, are alternatives that avoid manually annotated data (Richman and Schone, 2008; Althobaiti et al., 2013). The high coverage and rich informational structure of online encyclopedias can be exploited for the automatic creation of datasets. For example, many researchers have investigated the use of Wikipedia’s structure to classify Wikipedia articles and to transform links into NE annotations according to the link target type (Nothman et al., 2008; Ringland et al., 2009). In this paper we present our approach to automatically derive a large NE annotated corpora from Arabic Wikipedia. The key to our method lies in the exploitation of Wikipedia’s concepts, specifically anchor texts1 and redirects, to handle the rich morphology in Arabic, and thereby eliminate the need to perform any deep morphological analysis. In addition, a capitalisation probability measure has been introduced and incorporated into the approach in order to replace the capitalisation feature that does not exist in the Arabic script. This capitalisation measure has been utilised in order to fi"
E14-3012,M98-1019,0,0.89202,"Missing"
E14-3012,U09-1015,0,0.628431,"Missing"
E14-3012,I08-1071,0,0.407065,"Missing"
E14-3012,P13-1153,0,0.314305,"while 11 112 https://www.dropbox.com/sh/27afkiqvlpwyfq0/1hwWGqAcTL 8 Experimental Evaluation 8.3 To evaluate the quality of the methodology, we used WDC as training data to build an NER model. Then we tested the resulting classifier on datasets from different domains. 8.1 The supervised classifier in Section 8.2 was trained on the ANERcorp training set. We refer to the resulting model as the ANERcorp-Model. Table 4 shows the results of the ANERcorp-Model on the ANERcorp test set. The table also shows the results of the state-of-the-art supervised classifier ‘ANERcorp-Model(SoA)’ developed by Darwish (2013) when trained and tested on the same datasets used for ANERcorp-Model. Datasets For the evaluation purposes, we used three datasets: ANERcorp, NEWS, and TWEETS. ANERcorp is a news-wire domain dataset built and tagged especially for the NER task by Benajiba et al. (2007). It contains around 150k tokens and is available for free. We tested our methodology on the ANERcorp test corpus because it is widely used in the literature for comparing with existing systems. The NEWS dataset is also a news-wire domain dataset collected by Darwish (2013) from the RSS feed of the Arabic version of news.google."
E14-3012,D07-1073,0,0.0483055,"d ‘Countries in Europe’. 2 Related Work Current NE research seeks out adequate alternatives to traditional techniques such that they require minimal human intervention and solve deficiencies of traditional methods. Specific deficiencies include the limited number of NE classes resulting from the high cost of setting up corpora, and the difficulty of adapting the system to new domains. One of these trends is distant learning, which depends on the recruitment of external knowledge to increase the performance of the classifier, or to automatically create new resources used in the learning stage. Kazama and Torisawa (2007) exploited Wikipedia-based features to improve their NE machine learning recogniser’s F-score by three percent. Their method retrieved the corresponding Wikipedia entry for each candidate word sequence in the CoNLL 2003 dataset and extracted a category label from the first sentence of the entry. The automatic creation of training data has also been investigated using external knowledge. An et al. (2003) extracted sentences containing listed entities from the web, and produced a 1.8 million Korean word dataset. Their corpus Wikipedia offers several ways to group articles. One method is to group"
E14-3012,W10-2417,0,\N,Missing
fornaciari-poesio-2012-decour,C08-1006,0,\N,Missing
fornaciari-poesio-2012-decour,P09-2078,0,\N,Missing
fornaciari-poesio-2012-decour,J08-4004,1,\N,Missing
fornaciari-poesio-2012-decour,J96-2004,0,\N,Missing
fornaciari-poesio-2012-decour,W12-0406,1,\N,Missing
H05-1001,W99-0211,0,0.0212143,"- BASED: they attempt to identify the main ‘topics,’ which generally are TERMS, and then to extract from the document the most important information about these terms (Hovy and Lin, 1997). These approaches can be divided again very broadly in ‘lexical’ approaches, among which we would include LSA -based approaches, and ‘coreference-based’ approaches . Lexical approaches to term-based summarization use lexical relations to identify central terms (Barzilay and Elhadad, 1997; Gong and Liu, 2002); coreference- (or anaphora-) based approaches (Baldwin and Morton, 1998; Boguraev and Kennedy, 1999; Azzam et al., 1999; Bergler et al., 2003; Stuckardt, 2003) identify these terms by running a coreference- or anaphoric resolver over the text.1 We are not aware, however, of any attempt to use both lexical and anaphoric information to identify the main terms. In addition, to our knowledge no authors have convincingly demonstrated that feeding anaphoric information to a summarizer significantly improves the performance of a summarizer using a standard evaluation procedure (a reference corpus and baseline, and widely accepted evaluation measures). In this paper we compare two sentence extractionbased summarizers."
H05-1001,W98-1501,0,0.0137632,"ummarization can be very broadly characterized as TERM - BASED: they attempt to identify the main ‘topics,’ which generally are TERMS, and then to extract from the document the most important information about these terms (Hovy and Lin, 1997). These approaches can be divided again very broadly in ‘lexical’ approaches, among which we would include LSA -based approaches, and ‘coreference-based’ approaches . Lexical approaches to term-based summarization use lexical relations to identify central terms (Barzilay and Elhadad, 1997; Gong and Liu, 2002); coreference- (or anaphora-) based approaches (Baldwin and Morton, 1998; Boguraev and Kennedy, 1999; Azzam et al., 1999; Bergler et al., 2003; Stuckardt, 2003) identify these terms by running a coreference- or anaphoric resolver over the text.1 We are not aware, however, of any attempt to use both lexical and anaphoric information to identify the main terms. In addition, to our knowledge no authors have convincingly demonstrated that feeding anaphoric information to a summarizer significantly improves the performance of a summarizer using a standard evaluation procedure (a reference corpus and baseline, and widely accepted evaluation measures). In this paper we c"
H05-1001,poesio-kabadjov-2004-general,1,0.889061,"d evaluation procedure (a reference corpus and baseline, and widely accepted evaluation measures). In this paper we compare two sentence extractionbased summarizers. Both use Latent Semantic Analysis (LSA) (Landauer, 1997) to identify the main terms of a text for summarization; however, the first system (Steinberger and Jezek, 2004), discussed in Section 2, only uses lexical information to identify the main topics, whereas the second system exploits both lexical and anaphoric information. This second system uses an existing anaphora resolution system to resolve anaphoric expressions, GUI TAR (Poesio and Kabadjov, 2004); but, crucially, two different ways of using this information for summarization were tested. (Section 3.) Both summarizers were tested over the CAST corpus (Orasan et al., 2003), as discussed in Section 4, and sig1 The terms ’anaphora resolution’ and ’coreference resolution’ have been variously defined (Stuckardt, 2003), but the latter term is generally used to refer to the coreference task as defined in MUC and ACE. We use the term ’anaphora resolution’ to refer to the task of identifying successive mentions of the same discourse entity, realized via any type of noun phrase (proper noun, def"
H05-1001,W97-0703,0,0.132462,"rformance, simple substitution makes the performance worse. 1 Introduction Many approaches to summarization can be very broadly characterized as TERM - BASED: they attempt to identify the main ‘topics,’ which generally are TERMS, and then to extract from the document the most important information about these terms (Hovy and Lin, 1997). These approaches can be divided again very broadly in ‘lexical’ approaches, among which we would include LSA -based approaches, and ‘coreference-based’ approaches . Lexical approaches to term-based summarization use lexical relations to identify central terms (Barzilay and Elhadad, 1997; Gong and Liu, 2002); coreference- (or anaphora-) based approaches (Baldwin and Morton, 1998; Boguraev and Kennedy, 1999; Azzam et al., 1999; Bergler et al., 2003; Stuckardt, 2003) identify these terms by running a coreference- or anaphoric resolver over the text.1 We are not aware, however, of any attempt to use both lexical and anaphoric information to identify the main terms. In addition, to our knowledge no authors have convincingly demonstrated that feeding anaphoric information to a summarizer significantly improves the performance of a summarizer using a standard evaluation procedure ("
H05-1001,J04-3003,1,0.870611,"Missing"
H05-1001,A00-2018,0,0.0599429,"Missing"
H05-1001,W01-0514,0,0.0319468,"uage c Processing (HLT/EMNLP), pages 1–8, Vancouver, October 2005. 2005 Association for Computational Linguistics nificant improvements were observed over both the baseline CAST system and our previous LSA-based summarizer. 2 An LSA-based Summarizer Using Lexical Information Only LSA (Landauer, 1997) is a technique for extracting the ‘hidden’ dimensions of the semantic representation of terms, sentences, or documents, on the basis of their contextual use. It is a very powerful technique already used for NLP applications such as information retrieval (Berry et al., 1995) and text segmentation (Choi et al., 2001) and, more recently, multi- and single-document summarization. The approach to using LSA in text summarization we followed in this paper was proposed in (Gong and Liu, 2002). Gong and Liu propose to start by creating a term by sentences matrix A = [A1 , A2 , . . . , An ], where each column vector Ai represents the weighted term-frequency vector of sentence i in the document under consideration. If there are a total of m terms and n sentences in the document, then we will have an m × n matrix A for the document. The next step is to apply Singular Value Decomposition (SVD) to matrix A. Given an"
H05-1001,J00-4003,1,0.761489,"ntered’ structure of the type hypothesized in Rhetorical Structures Theory (Knott et al., 2001; Poesio et al., 2004). 3 3.2 GUITAR: A General-Purpose Anaphoric Resolver The system we used in these experiments, GUITAR (Poesio and Kabadjov, 2004), is an anaphora resolution system designed to be high precision, modular, and usable as an off-the-shelf component of a NL processing pipeline. The current version of the system includes an implementation of the MARS pronoun resolution algorithm (Mitkov, 1998) and a partial implementation of the algorithm for resolving definite descriptions proposed by Vieira and Poesio (2000). The current version of GUITAR does not include methods for resolving proper nouns. 3.2.1 Pronoun Resolution Mitkov (1998) developed a robust approach to pronoun resolution which only requires input text to be part-of-speech tagged and noun phrases to be identified. Mitkov’s algorithm operates on the basis of antecedent-tracking preferences (referred to hereafter as ”antecedent indicators”). The approach works as follows: the system identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor, and then app"
H05-1001,W97-0704,0,0.0504259,"in significant performance improvements over a previously developed system, in which only lexical terms are used as the input to SVD. However, we also show that how anaphoric information is used is crucial: whereas using this information to add new terms does result in improved performance, simple substitution makes the performance worse. 1 Introduction Many approaches to summarization can be very broadly characterized as TERM - BASED: they attempt to identify the main ‘topics,’ which generally are TERMS, and then to extract from the document the most important information about these terms (Hovy and Lin, 1997). These approaches can be divided again very broadly in ‘lexical’ approaches, among which we would include LSA -based approaches, and ‘coreference-based’ approaches . Lexical approaches to term-based summarization use lexical relations to identify central terms (Barzilay and Elhadad, 1997; Gong and Liu, 2002); coreference- (or anaphora-) based approaches (Baldwin and Morton, 1998; Boguraev and Kennedy, 1999; Azzam et al., 1999; Bergler et al., 2003; Stuckardt, 2003) identify these terms by running a coreference- or anaphoric resolver over the text.1 We are not aware, however, of any attempt to"
H05-1001,P98-2143,0,0.0297252,"ducational texts, only a ‘entity-centered’ structure can be clearly identified, as opposed to a ‘relation-centered’ structure of the type hypothesized in Rhetorical Structures Theory (Knott et al., 2001; Poesio et al., 2004). 3 3.2 GUITAR: A General-Purpose Anaphoric Resolver The system we used in these experiments, GUITAR (Poesio and Kabadjov, 2004), is an anaphora resolution system designed to be high precision, modular, and usable as an off-the-shelf component of a NL processing pipeline. The current version of the system includes an implementation of the MARS pronoun resolution algorithm (Mitkov, 1998) and a partial implementation of the algorithm for resolving definite descriptions proposed by Vieira and Poesio (2000). The current version of GUITAR does not include methods for resolving proper nouns. 3.2.1 Pronoun Resolution Mitkov (1998) developed a robust approach to pronoun resolution which only requires input text to be part-of-speech tagged and noun phrases to be identified. Mitkov’s algorithm operates on the basis of antecedent-tracking preferences (referred to hereafter as ”antecedent indicators”). The approach works as follows: the system identifies the noun phrases which precede t"
H05-1001,X98-1026,0,\N,Missing
H05-1001,W00-0403,0,\N,Missing
H05-1001,C98-2138,0,\N,Missing
H05-1001,E03-1066,0,\N,Missing
I11-1011,D08-1031,0,0.0143919,"001100101000011011100101101101111001100 11111001100001000011110100101111101110001101 01000100100101011001000010111100101100001000 10100101101011100011111110010100100010010011 00101111101110101001100000010101001011001001 00011000101110100010000010011000100110000100 76.8 76.7 74.6 72.2 71.4 74.0† 71.4 71.7 64.6 63.6 66.5 66.2 63.8 63.4 71.5 71.8† 67.1 67.6 65.2 70.3† 72.3 72.1 59.7 59.6 59.7† 59.1 60.0 61.2 74.5 74.9† 70.1† 69.1 70.3 73.1† 73.6 74.4 58.4 58.8 54.7† 55.6† 58.1 58.4 gos (2008). The results clearly show that although even larger sets of features have been proposed (Uryupina, 2007; Bengtson and Roth, 2008), the set of features already included in BART is sufficient to achieve results well above the state of the art on the dataset we used. larger datasets and larger sets of features learning a model becomes slower and requires much more memory. This suggests that automatic feature selection may be essential not just to improve performance but also to be able to train a model—i.e., that an efficient coreference resolution system should combine rich linguistic feature sets with automatic feature selection mechanisms. The results in Table 2 also confirm the intuition that, contrary to what is sugge"
I11-1011,W09-2411,0,0.0934369,"Missing"
I11-1011,doddington-etal-2004-automatic,0,0.0770207,"Missing"
I11-1011,J01-4004,0,0.640849,"(MOO) for coreference, that suggests a family of systems, showing reliable performance according to all the desired metrics. A form of MOO was applied to coreference by Munson et al. (2005). Their general conclusion was negative, stating that “ensemble selection seems too unreliable for use in NLP”, but they did see some improvements for coreference. 2 Background: Optimizing for Anaphora Resolution A great number of statistical approaches to anaphora resolution have been proposed in the past ten years. These approaches differ with respect to their underlying models (e.g., mention pair model (Soon et al., 2001) vs. tournament model (Iida et al., 2003; Yang et al., 2005), vs. entity-model (Luo et al., 2004)), machine learners (e.g., decision trees vs. maximum entropy vs. SVMs vs. TiMBL) and their parameters, and with respect to feature sets used. There have been, however, only few attempts at explicit optimization of these aspects, and in those few cases, optimization tends to be done by hand. An early step in this direction was the work by Ng and Cardie (2002), who developed a rich feature set including 53 features, but reported no significant improvement over their baseline when all these features"
I11-1011,S10-1020,1,0.825903,"ts) as the optimization functions). Perhaps the most interesting result of this work is the finding that by working in such a multi-metric space it is possible to find solutions that are better with respect to an individual metric than when trying to optimize for that metric alone—which arguably suggests that indeed both families of metrics capture some fundamental intuition about anaphora, and taking into account both intuitions we avoid local optima. 1 Introduction In anaphora resolution,1 as in other HLT tasks, optimization to a metric is essential to achieve good performance (Hoste, 2005; Uryupina, 2010). However, many evaluation metrics have been proposed for anaphora resolution, each capturing what seems to be a key intuition about the task: from MUC (Vilain et al., 1995) to B3 (Bagga and 1 We use the term ’anaphora resolution’ to refer to the task perhaps most commonly referred to as ’coreference resolution,’ which many including us find a misnomer. For the purposes of the present paper the two terms could be seen as interchangeable. The structure of the paper is as follows. We first review the literature on using genetic algorithms for both single function and multi function opti93 Procee"
I11-1011,P08-4003,1,0.897342,"the system proposed by Soon et al. (2001), commonly used as baseline and relying only on very shallow information. Our reimplementation of the Soon et al. model uses only a subset of features: those marked with an asterisk in Table 1. We also provide in Table 2 typical state-of-the-art figures on the ACE-02 dataset, as presented in an overview by Poon and Domin5 Methods 2 5.1 The choice of the best model and the best machine learner, along with its parameters, is the main direction of our future work. 3 http://sourceforge.net/projects/ carafe The BART System For our experiments, we use BART (Versley et al., 2008), a modular toolkit for anaphora reso96 Table 1: Features used by BART: each feature describes a pair of mentions {Mi , Mj }, i &lt; j, where Mi is a candidate antecedent and Mj is a candidate anaphor MentionType* MentionType Ante Salient MentionType Ante Extra MentionType Ana MentionType2 MentionType Salience FirstSecondPerson PronounLeftRight PronounWordForm SemClassValue BothLocation GenderAgree* NumberAgree* AnimacyAgree* Alias* BetterNames Appositive* Appositive2 Coordination HeadPartOfSpeech SynPos Attributes Relations StringMatch* NonPro StringMatch Pro StringMatch NE StringMatch HeadMatch"
I11-1011,W03-2604,0,0.0420636,"amily of systems, showing reliable performance according to all the desired metrics. A form of MOO was applied to coreference by Munson et al. (2005). Their general conclusion was negative, stating that “ensemble selection seems too unreliable for use in NLP”, but they did see some improvements for coreference. 2 Background: Optimizing for Anaphora Resolution A great number of statistical approaches to anaphora resolution have been proposed in the past ten years. These approaches differ with respect to their underlying models (e.g., mention pair model (Soon et al., 2001) vs. tournament model (Iida et al., 2003; Yang et al., 2005), vs. entity-model (Luo et al., 2004)), machine learners (e.g., decision trees vs. maximum entropy vs. SVMs vs. TiMBL) and their parameters, and with respect to feature sets used. There have been, however, only few attempts at explicit optimization of these aspects, and in those few cases, optimization tends to be done by hand. An early step in this direction was the work by Ng and Cardie (2002), who developed a rich feature set including 53 features, but reported no significant improvement over their baseline when all these features were used with the MUC 6 and MUC 7 corpo"
I11-1011,P04-1018,0,0.0209417,"to all the desired metrics. A form of MOO was applied to coreference by Munson et al. (2005). Their general conclusion was negative, stating that “ensemble selection seems too unreliable for use in NLP”, but they did see some improvements for coreference. 2 Background: Optimizing for Anaphora Resolution A great number of statistical approaches to anaphora resolution have been proposed in the past ten years. These approaches differ with respect to their underlying models (e.g., mention pair model (Soon et al., 2001) vs. tournament model (Iida et al., 2003; Yang et al., 2005), vs. entity-model (Luo et al., 2004)), machine learners (e.g., decision trees vs. maximum entropy vs. SVMs vs. TiMBL) and their parameters, and with respect to feature sets used. There have been, however, only few attempts at explicit optimization of these aspects, and in those few cases, optimization tends to be done by hand. An early step in this direction was the work by Ng and Cardie (2002), who developed a rich feature set including 53 features, but reported no significant improvement over their baseline when all these features were used with the MUC 6 and MUC 7 corpora. They then proceeded to manually select a subset of fe"
I11-1011,H05-1004,0,0.0543821,"coreference resolution system (i.e., BART) with only these N features. 3. This coreference system is evaluated on the development data. The recall, precision and F-measure values of three metrics are calculated. In case of single objective optimization (SOO), the objective function corresponding to a particular chromosome is the F-measure value of a single metric. This objective function is optimized using the search capability of GA. For MOO, the objective functions corresponding to a particular chromosome are FM U C (for the MUC metric), Fφ3 (for CEAF using the φ3 entity alignment function (Luo, 2005)) and Fφ4 (for CEAF using the φ4 entity alignment function). These three objective functions are simultaneously optimized using the search capability of NSGA-II. 4.3 Genetic Operators In case of SOO, a single point crossover operation is used with a user defined crossover probability, µc . A mutation operator is applied to each entry of the chromosome with a mutation probability, µm , where the entry is randomly replaced by either 0 or 1. In this approach, the processes of fitness computation, selection, crossover, and mutation are executed for a maximum number of generations. The best string"
I11-1011,H05-1068,0,0.69205,"C, CEAF or BLANC ). The results on the SEMEVAL-10 dataset clearly show that existing metrics of coreference rely on different intuitions and therefore a system, optimized for a particular metric, might show inferior results for the other ones. For example, the reported BLANC difference between the runs optimized for BLANC and CEAF is around 10 percentage points. This highlights the importance of the multiobjective optimization (MOO) for coreference, that suggests a family of systems, showing reliable performance according to all the desired metrics. A form of MOO was applied to coreference by Munson et al. (2005). Their general conclusion was negative, stating that “ensemble selection seems too unreliable for use in NLP”, but they did see some improvements for coreference. 2 Background: Optimizing for Anaphora Resolution A great number of statistical approaches to anaphora resolution have been proposed in the past ten years. These approaches differ with respect to their underlying models (e.g., mention pair model (Soon et al., 2001) vs. tournament model (Iida et al., 2003; Yang et al., 2005), vs. entity-model (Luo et al., 2004)), machine learners (e.g., decision trees vs. maximum entropy vs. SVMs vs."
I11-1011,P02-1014,0,0.158918,"resolution have been proposed in the past ten years. These approaches differ with respect to their underlying models (e.g., mention pair model (Soon et al., 2001) vs. tournament model (Iida et al., 2003; Yang et al., 2005), vs. entity-model (Luo et al., 2004)), machine learners (e.g., decision trees vs. maximum entropy vs. SVMs vs. TiMBL) and their parameters, and with respect to feature sets used. There have been, however, only few attempts at explicit optimization of these aspects, and in those few cases, optimization tends to be done by hand. An early step in this direction was the work by Ng and Cardie (2002), who developed a rich feature set including 53 features, but reported no significant improvement over their baseline when all these features were used with the MUC 6 and MUC 7 corpora. They then proceeded to manually select a subset of features that did yield better results for the MUC-6/7 datasets. A much larger scale and very systematic effort of manual feature selection over the same dataset was carried out by Uryupina (2007), who evaluated over 600 features. The first systematic attempt at automatic optimization of anaphora resolution we are aware of was carried out by Hoste (2005), who i"
I11-1011,D08-1068,0,0.0358973,"Missing"
I11-1011,M95-1005,0,0.335391,"solutions that are better with respect to an individual metric than when trying to optimize for that metric alone—which arguably suggests that indeed both families of metrics capture some fundamental intuition about anaphora, and taking into account both intuitions we avoid local optima. 1 Introduction In anaphora resolution,1 as in other HLT tasks, optimization to a metric is essential to achieve good performance (Hoste, 2005; Uryupina, 2010). However, many evaluation metrics have been proposed for anaphora resolution, each capturing what seems to be a key intuition about the task: from MUC (Vilain et al., 1995) to B3 (Bagga and 1 We use the term ’anaphora resolution’ to refer to the task perhaps most commonly referred to as ’coreference resolution,’ which many including us find a misnomer. For the purposes of the present paper the two terms could be seen as interchangeable. The structure of the paper is as follows. We first review the literature on using genetic algorithms for both single function and multi function opti93 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 93–101, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP mization. Next, we di"
I11-1011,I05-1063,0,0.0178361,"howing reliable performance according to all the desired metrics. A form of MOO was applied to coreference by Munson et al. (2005). Their general conclusion was negative, stating that “ensemble selection seems too unreliable for use in NLP”, but they did see some improvements for coreference. 2 Background: Optimizing for Anaphora Resolution A great number of statistical approaches to anaphora resolution have been proposed in the past ten years. These approaches differ with respect to their underlying models (e.g., mention pair model (Soon et al., 2001) vs. tournament model (Iida et al., 2003; Yang et al., 2005), vs. entity-model (Luo et al., 2004)), machine learners (e.g., decision trees vs. maximum entropy vs. SVMs vs. TiMBL) and their parameters, and with respect to feature sets used. There have been, however, only few attempts at explicit optimization of these aspects, and in those few cases, optimization tends to be done by hand. An early step in this direction was the work by Ng and Cardie (2002), who developed a rich feature set including 53 features, but reported no significant improvement over their baseline when all these features were used with the MUC 6 and MUC 7 corpora. They then procee"
I11-1011,C10-1147,0,0.0142034,"imization of anaphora resolution we are aware of was carried out by Hoste (2005), who investigated the possibility of using genetic algorithms for automatic optimization of both feature selection and of learning parameters, also considering two different machine learners, TiMBL and Ripper. Her results suggest that such techniques yield improvements on the MUC-6/7 datasets. Recasens and Hovy (2009) carried out an investigation of feature selection for Spanish using the ANCORA corpus. These approaches focused on a single metric only; the one proposal simultaneously to consider multiple metrics, Zhao and Ng (2010) still optimized for each metric individually. The effect of optimization on anaphora resolution was dramatically demonstrated by Uryupina’s contribution to SEMEVAL 2010 Multilingual 3 Optimization with Genetic Algorithms In this section, we review optimization techniques using genetic algorithms (GAs) (Goldberg, 1989). We first discuss single objective optimization, that can optimize according to a single objective function, and then multi-objective optimization (MOO), that can optimize more than one objective function, in particular, a popular MOO technique named Non-dominated Sorting Geneti"
I11-1011,versley-etal-2008-bart-modular,1,\N,Missing
I11-1011,S10-1001,1,\N,Missing
I13-1099,S10-1021,1,0.847757,"ing mention detection models: 1. First Model: In our first model we consider each noun phrase(NP) as a possible candidate of mention. Results of this model are shown in Table 1. 2 Brief Description of BART System Architecture 2. Second Model: In our second model we consider each Named Entity (NE) or pronoun (PRP) as a mention and its results are shown in Table 1. Our starting point of anaphora resolution system is the toolkit from (Versley et al., 2008), originally conceived as a modularized version of previous efforts from (Ponzetto and Strube, 2006; Poesio and Kabadjov, 2004; Versley, 2006; Broscheit et al., 2010). BART’s final aim is to bring together stateof-the-art approaches, including syntax-based and semantic features. The state-of-the-art anaphora resolution system, BART has five main components: preprocessing pipeline, mention factory, feature extraction module, decoder and encoder. In addition, an independent language plugin module handles all the language specific information and is accessible from any component. Each module can be accessed independently and thus adjusted to leverage the system’s performance on a particular language or domain. The preprocessing pipeline converts an input docu"
I13-1099,P04-1018,0,0.0659193,"Missing"
I13-1099,H05-1004,0,0.0388236,"ve mentioned feature combinations. Instances are created following (Soon et al., 2001). We generate a positive training instance from each pair of adjacent coreferent markables. Negative instances are created by pairing the anaphor with any markable occurring between the anaphor and the antecedent. During testing, we perform a closest first clustering of instances deemed coreferent by the classifier. Each text is processed from left to right: each In order to evaluate the anaphora resolution system we use different scorers such as MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAF (Luo, 2005) and BLANC (Recasens and Hovy, October 2011). We experiment with the different mention detectors for anaphora resolution. Table 4 shows the MUC recall, precision and F-measure values of the system trained using the training data and evaluated using the development data. Experiments were carried out on a high performance computing facility with the following configuration: Dell machine, 216 cores, 2.66 GHZ Intel Xeon processors, 4 GB RAM/core, and 10 TB storage. 7 http://ltrc.iiit.ac.in/showfile.php? filename=downloads/shallow parser.php 819 Mentions NP NE/PRP PER/PRP CRF Classifier recall 52.5"
I13-1099,P02-1014,0,0.290061,"Missing"
I13-1099,P10-1142,0,0.0522234,"Missing"
I13-1099,poesio-kabadjov-2004-general,1,0.899434,"utpal.sikdar,asif,sriparna}@iitp.ac.in 2 University of Trento, Center for Mind/Brain Sciences, uryupina@unitn.it 3 University of Essex, Language and Computation Group, poesio@essex.ac.uk Abstract Most of these works on supervised machine learning co-reference resolution have been developed for English (Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2003; Luo et al., 2004), due to the availability of large corpora such as ACE (Walker et al., 2006) and OntoNotes (Weischedel et al., 2008). BART, the Beautiful Anaphora Resolution Toolkit (Versley et al., 2008), (Ponzetto and Strube, 2006), (Poesio and Kabadjov, 2004), is the resultant of the project titled ”Exploiting Lexical and Encyclopedic Resources For Entity Disambiguation” carried out at the Johns Hopkins Summer Workshop 2007. It can handle all the preprocessing tasks to perform automatic coreference resolution. A variety of machine learning approaches are used in BART; it mainly uses several machine learning toolkits, including WEKA, MaxEnt and Support Vector Machine (SVM). Literature shows the significant amount of works in the area of anaphora resolution. But these (Pradhan et al., 2012; Ng, 2010; Poesio et al., 2010) are mainy in non-Indian lang"
I13-1099,N06-1025,0,0.147589,"ineering, IIT Patna, India, {utpal.sikdar,asif,sriparna}@iitp.ac.in 2 University of Trento, Center for Mind/Brain Sciences, uryupina@unitn.it 3 University of Essex, Language and Computation Group, poesio@essex.ac.uk Abstract Most of these works on supervised machine learning co-reference resolution have been developed for English (Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2003; Luo et al., 2004), due to the availability of large corpora such as ACE (Walker et al., 2006) and OntoNotes (Weischedel et al., 2008). BART, the Beautiful Anaphora Resolution Toolkit (Versley et al., 2008), (Ponzetto and Strube, 2006), (Poesio and Kabadjov, 2004), is the resultant of the project titled ”Exploiting Lexical and Encyclopedic Resources For Entity Disambiguation” carried out at the Johns Hopkins Summer Workshop 2007. It can handle all the preprocessing tasks to perform automatic coreference resolution. A variety of machine learning approaches are used in BART; it mainly uses several machine learning toolkits, including WEKA, MaxEnt and Support Vector Machine (SVM). Literature shows the significant amount of works in the area of anaphora resolution. But these (Pradhan et al., 2012; Ng, 2010; Poesio et al., 2010)"
I13-1099,W12-4501,1,0.883955,"Missing"
I13-1099,J01-4004,0,0.89698,"ot first person then the corresponding feature is also set to high. The feature also behaves in a similar way if the pair (REj , REi ) appears outside the quotation. Preprocessing and Markable Extraction For the anaphora resolution system, mentions are identified from the datasets based on the gold annotations. These are treated as the markables. Thereafter we convert the markables to the data format used by BART, namely MMAX2s standoff XML format. 3.2 Features for anaphora resolution We view coreference resolution as a binary classification problem. We use the learning framework proposed by (Soon et al., 2001) as a baseline. Each classification instance consists of two markables, i.e. an anaphor and its potential antecedent. Instances are modelled as feature vectors and are used to train a binary classifier. The classifier has to decide, given the features, whether the anaphor and the candidate antecedent are coreferent or not. To improve the performance we define some features specific to the language. Given BART’s flexible architecture, we explore the contribution of some features implemented in BART for co-reference resolution in Bengali. Given a potential antecedent REi and a anaphor REj , we c"
I13-1099,P08-4003,1,0.925741,"Computer Science and Engineering, IIT Patna, India, {utpal.sikdar,asif,sriparna}@iitp.ac.in 2 University of Trento, Center for Mind/Brain Sciences, uryupina@unitn.it 3 University of Essex, Language and Computation Group, poesio@essex.ac.uk Abstract Most of these works on supervised machine learning co-reference resolution have been developed for English (Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2003; Luo et al., 2004), due to the availability of large corpora such as ACE (Walker et al., 2006) and OntoNotes (Weischedel et al., 2008). BART, the Beautiful Anaphora Resolution Toolkit (Versley et al., 2008), (Ponzetto and Strube, 2006), (Poesio and Kabadjov, 2004), is the resultant of the project titled ”Exploiting Lexical and Encyclopedic Resources For Entity Disambiguation” carried out at the Johns Hopkins Summer Workshop 2007. It can handle all the preprocessing tasks to perform automatic coreference resolution. A variety of machine learning approaches are used in BART; it mainly uses several machine learning toolkits, including WEKA, MaxEnt and Support Vector Machine (SVM). Literature shows the significant amount of works in the area of anaphora resolution. But these (Pradhan et al., 2012; N"
I13-1099,M95-1005,0,0.157159,"sion tree learning algorithm (Quinlan, 1993), with the above mentioned feature combinations. Instances are created following (Soon et al., 2001). We generate a positive training instance from each pair of adjacent coreferent markables. Negative instances are created by pairing the anaphor with any markable occurring between the anaphor and the antecedent. During testing, we perform a closest first clustering of instances deemed coreferent by the classifier. Each text is processed from left to right: each In order to evaluate the anaphora resolution system we use different scorers such as MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAF (Luo, 2005) and BLANC (Recasens and Hovy, October 2011). We experiment with the different mention detectors for anaphora resolution. Table 4 shows the MUC recall, precision and F-measure values of the system trained using the training data and evaluated using the development data. Experiments were carried out on a high performance computing facility with the following configuration: Dell machine, 216 cores, 2.66 GHZ Intel Xeon processors, 4 GB RAM/core, and 10 TB storage. 7 http://ltrc.iiit.ac.in/showfile.php? filename=downloads/shallow parser.php 819 Menti"
I13-1099,P03-1023,0,0.0856957,"Missing"
J00-4003,P95-1017,0,0.0946297,"Missing"
J00-4003,P98-1011,0,0.0199669,"Missing"
J00-4003,P99-1048,0,0.350022,"Missing"
J00-4003,A97-1029,0,0.0254698,"to entities introduced by proper names (such as Pinkerton Inc ... the company) are very common in newspaper articles. Processing such descriptions requires determining an entity type for each name in the text, that is, if we recognize Pinkerton Inc. as an entity of type company, we can then resolve the subsequent description the company, or even a description such as the firm by finding a synonymy relation between company and firm using WordNet. This so-called named entity recognition task has received considerable attention recently (Mani and MacMillan 1996; McDonald 1996; Paik et al. 1996; Bikel et al. 1997; Palmer and Day 1997; Wacholder and Ravin 1997; Mikheev, Moens, and Grover 1999) and was one of the tasks evaluated in the Sixth and Seventh Message Understanding Conferences. In MUC-6, 15 different systems participated in the competition (Sundheim 1995). For the version of the system discussed and evaluated here, we implemented a preliminary algorithm for named entity recognition that we developed ourselves; a more recent version of the system (Ishikawa 1998) uses the named entity recognition software developed by HCRC for the MUC-7 competition (Mikheev, Moens, and Grover 1999). WordNet cont"
J00-4003,C88-1021,0,0.0244603,"herefore either mainly theoretical or d o m a i n dependent), and systems that can be quantitatively evaluated, such as those competing on the coreference task in the Sixth and Seventh Message Understanding Conference (Sundheim 1995). We discuss these two types of work in turn. 7.1 Models Based On Commonsense Reasoning The crucial characteristic of these proposals is that they exploit hand-coded commonsense knowledge, and cannot therefore be tested on just any arbitrary text. Some of them are simply tested on texts that were especially built for the purpose of testing the system (Carter 1987; Carbonell and Brown 1988); systems like the Core Language Engine are more robust, but they have to be applied to a d o m a i n restricted enough that all relevant knowledge can be encoded by hand. 32 This p r o b l e m is also a central concern in the w o r k b y Bean a n d Riloff (1999). 584 Vieira a n d Poesio Processing Definite Descriptions Sidner's Theory of Definite Anaphora Comprehension. In her dissertation, Sidner (1979) proposed a complete theory of definite NP resolution, including detailed algorithms for resolving pronouns, anaphoric definite descriptions, and bridging descriptions. She also proposed metho"
J00-4003,J96-2004,0,0.0117352,"identity to an entity already introduced in the discourse; 7 • discourse-new: first-mention definite descriptions that denote objects not related b y shared associative k n o w l e d g e to entities already introduced in the discourse. In the second experiment w e treated all anaphoric definite descriptions as part of one class (direct anaphora + bridging (i)), and all inferrables as part of a different class (bridging (ii)), w i t h o u t significant changes in the agreement results. A g r e e m e n t a m o n g annotators was m e a s u r e d using the K statistic (Siegel and Castellan 1988; Carletta 1996). K measures agreement a m o n g k annotators over and above chance agreement (Siegel and Castellan 1988). The K coefficient of agreement is defined as: K -- P(a) - P(E) 1 - w h e r e P(A) is the p r o p o r t i o n of times the annotators agree, and P(E) the p r o p o r t i o n of times that we w o u l d expect t h e m to agree b y chance. The interpretation of K figures is an o p e n question, but in the field of content analysis, where reliability has long been an issue (Krippendorff 1980), K > 0.8 is generally taken to indicate good reliability, whereas 0.68 < K < 0.8 allows tentative conc"
J00-4003,J97-1002,0,0.0306696,"Missing"
J00-4003,M95-1004,0,0.0931765,"Missing"
J00-4003,M95-1017,0,0.0911069,"Missing"
J00-4003,P83-1007,0,0.0490437,"Missing"
J00-4003,J95-2003,0,0.660798,"Missing"
J00-4003,J97-4002,0,0.0138162,"e, the architecture of our system is motivated by the results concerning definite description use in our corpus, discussed in Poesio and Vieira (1998). In this section we briefly review the results presented in that paper. 2 In fact, it is precisely because we are interested in identifying the types of commonsense reasoning actually used in language processing that we focused on definite descriptions rather than on other types of anaphoric expressions (such as pronouns and ellipsis) that can be processed much more effectively on the basis of syntactic information alone (Lappin and Leass 1994; Hardt 1997). 540 Vieira and Poesio Processing Definite Descriptions 2.1 The Corpus We used a subset of the Penn Treebank I corpus (Marcus, Santorini, and Marcinkiewicz 1993) from the A C L / D C I CD-ROM, containing n e w s p a p e r articles from the Wall Street Journal. We divided the corpus into two parts: one, containing about 1,000 definite descriptions, was used as a source during the d e v e l o p m e n t of the system; we will refer to these texts as Corpus 1.3 The other part, containing about 400 definite descriptions, was kept aside during d e v e l o p m e n t and used for testing; we will ref"
J00-4003,P93-1023,0,0.00945209,"nding coreference chain is c h e c k e d - - t h a t is, the system's indexes and the annotated indexes do not need to be exactly the same as long as they belong to the same coreference chain. In this way, both (40a) and (40b) w o u l d be evaluated as correct answers if the corpus is annotated with the links s h o w n in (39). (39) A house1°6... The house135... The house154... coder: corer(135,106). coder: corer(154,135). (40) a. system: corer (154,135). b. system: corer(154,106). 26 An alternative method is to give fractional values to a classification depending on the number of agreements (Hatzivassiloglou and McKeown (1993). 562 Vieira and Poesio Processing Definite Descriptions In the end, we still need to check the results manually, because our annotated coreference chains are not complete: our annotators did not annotate all types of anaphoric expressions, so it may happen that the system indicates as antecedent an element outside an annotated coreference chain, such as a bare noun or possessive. In (41), for example, suppose that all references to the house are coreferential: A house1°6... The house135... His house14°... The house154... corer ( 154,140). (41) If NP 135 is indicated as the antecedent for NP 1"
J00-4003,J97-1003,0,0.0985138,"(Fox 1987; Grosz 1977; Grosz and Sidner 1986; Reichman 1985), whereas the antecedents introduced in a prior segment at the same level may be. Later in (8), for example, the housej in sentence 50 becomes inaccessible again, and in sentence 65, the text starts referring again to the house introduced in sentence 2. Automatically recognizing the hierarchical structure of texts is an unresolved problem, as it involves reasoning about intentions;14 better results have been achieved on the simpler task of ""chunking"" the text into sequences of segments, generally by means of lexical density measures (Hearst 1997; Richmond, Smith, and Amitay 1997). The methods for limiting the life span of discourse entities that we considered for our system are even simpler. One type of heuristic we looked at are windowbased techniques, i.e., considering only the antecedents within fixed-size windows of previous sentences, although we allow some discourse entities to have a longer life span: we call this method loose segmentation. More specifically, a discourse entity is considered a potential antecedent for a definite description when the antecedent's head is identical to the description's head, and • the potential"
J00-4003,M98-1007,0,0.0509034,"Missing"
J00-4003,W97-1307,0,0.0409179,"Missing"
J00-4003,J94-4002,0,0.354156,"Work As mentioned above, the architecture of our system is motivated by the results concerning definite description use in our corpus, discussed in Poesio and Vieira (1998). In this section we briefly review the results presented in that paper. 2 In fact, it is precisely because we are interested in identifying the types of commonsense reasoning actually used in language processing that we focused on definite descriptions rather than on other types of anaphoric expressions (such as pronouns and ellipsis) that can be processed much more effectively on the basis of syntactic information alone (Lappin and Leass 1994; Hardt 1997). 540 Vieira and Poesio Processing Definite Descriptions 2.1 The Corpus We used a subset of the Penn Treebank I corpus (Marcus, Santorini, and Marcinkiewicz 1993) from the A C L / D C I CD-ROM, containing n e w s p a p e r articles from the Wall Street Journal. We divided the corpus into two parts: one, containing about 1,000 definite descriptions, was used as a source during the d e v e l o p m e n t of the system; we will refer to these texts as Corpus 1.3 The other part, containing about 400 definite descriptions, was kept aside during d e v e l o p m e n t and used for testing"
J00-4003,P99-1047,0,0.0115776,"er, when matching a definite description with a potential antecedent the information provided by the prenominal and the postnominal part of the noun phrases also has to be taken into account. For example, a blue car cannot serve as the antecedent for the red car, or the house on the left for the house on the right. In our corpus, cases of antecedents that would incorrectly match by simply matching heads without regarding premodification include: (10) a. the business community ... the younger, more activist black political community; b. the population.., the voting population. 14 See, however, Marcu (1999). 550 Vieira and Poesio Processing Definite Descriptions Again, taking p r o p e r account of the semantic contribution of these premodifiers would, in general, require c o m m o n s e n s e reasoning. For the m o m e n t , w e only d e v e l o p e d heuristic solutions to the p r o b l e m , including: • allowing an antecedent to m a t c h with a definite description if the premodifiers of the description are a subset of the premodifiers of the antecedent. This heuristic deals with definites that contain less information than the antecedent, such as an old Victorian house ... the house, a n d"
J00-4003,J93-2004,0,0.0338035,"Missing"
J00-4003,E99-1001,0,0.0127753,"Missing"
J00-4003,mitkov-2000-towards,0,0.0263593,"Missing"
J00-4003,A97-1028,0,0.00783675,"uced by proper names (such as Pinkerton Inc ... the company) are very common in newspaper articles. Processing such descriptions requires determining an entity type for each name in the text, that is, if we recognize Pinkerton Inc. as an entity of type company, we can then resolve the subsequent description the company, or even a description such as the firm by finding a synonymy relation between company and firm using WordNet. This so-called named entity recognition task has received considerable attention recently (Mani and MacMillan 1996; McDonald 1996; Paik et al. 1996; Bikel et al. 1997; Palmer and Day 1997; Wacholder and Ravin 1997; Mikheev, Moens, and Grover 1999) and was one of the tasks evaluated in the Sixth and Seventh Message Understanding Conferences. In MUC-6, 15 different systems participated in the competition (Sundheim 1995). For the version of the system discussed and evaluated here, we implemented a preliminary algorithm for named entity recognition that we developed ourselves; a more recent version of the system (Ishikawa 1998) uses the named entity recognition software developed by HCRC for the MUC-7 competition (Mikheev, Moens, and Grover 1999). WordNet contains the types of a f"
J00-4003,J98-2001,1,0.0740713,"e-new descriptions in newspaper corpora. The annotated corpus was used to extensively evaluate the proposed techniquesfor matching definite descriptions with their antecedents, discourse segmentation, recognizing discourse-new descriptions, and suggesting anchorsfor bridging descriptions. 1. Introduction Most models of definite description processing p r o p o s e d in the literature tend to emphasise the anaphoric role of these elements. 1 (Heim [1982] is perhaps the best formalization of this type of theory). This approach is challenged b y the results of experiments we reported previously (Poesio and Vieira 1998), in which subjects were asked to classify the uses of definite descriptions in Wall Street Journal articles according to schemes derived from proposals b y Hawkins (1978) and Prince (1981). The results of these experiments indicated that definite descriptions are not primarily anaphoric; about half of the time they are used to introduce a new entity in the discourse. In this paper, we present an i m p l e m e n t e d system for processing definite descriptions based on the results of that earlier study. In our system, techniques for recognizing discourse-new descriptions play a role as import"
J00-4003,W97-1301,1,0.662839,"Missing"
J00-4003,W97-0305,0,0.0293658,"Missing"
J00-4003,M95-1002,0,0.11034,"ac.uk 1 We use the term definite description (Russell 1905) to indicate definite noun phrases with the definite article the, such as the car. We are not concerned with other types of definite noun phrases such as pronouns, demonstratives, or possessive descriptions. Anaphoric expressions are those linguistic expressions used to signal, evoke, or refer to previously mentioned entities. (~) 2001 Association for Computational Linguistics Computational Linguistics Volume 26, Number 4 interpretation as well, for example, at the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) (Sundheim 1995; Chinchor 1997), which also included evaluations of systems on the so-called coreference task, a subtask of which is the resolution of definite descriptions. The system we present was developed to be evaluated in a quantitative fashion, as well, but because of the problems concerning agreement between annotators observed in our previous study, we evaluated the system both by measuring precision/recall against a ""gold standard,"" as done in MUC, and by measuring agreement between the annotations produced by the system and those proposed by the annotators. The decision to develop a system that c"
J00-4003,J94-2006,0,0.0489412,"Missing"
J00-4003,P99-1079,0,0.0113123,"ith Sidner's theory of local focus, as well as others such as Centering Theory (Grosz, Joshi, and Weinstein 1995), is the lack of a precise characterization of how to deal with complex sentences. Revisions and extensions of Sidner's proposal related to these problems have been proposed in Suri and McCoy (1994), and include algorithms for updating focus in complex sentences containing adjunct clauses such as before- and after-clauses. We plan to incorporate simpler focus-tracking mechanisms in future versions of the system, possibly along the lines of Azzam, Humphreys, and Gaizauskas (1998) or Tetreault (1999). 8.3.3 Theoretical Developments. We defended the importance of developing methods for identifying discourse-new descriptions, and we believe that there is still need for research into the semantics of this class; that is, what, exactly, licenses the use of a definite description to refer to a discourse-new entity? The role of premodification and postmodification should also be further examined. Postmodification is one of the most frequent features of discourse-new descriptions; additional empirical studies considering a detailed subclassification of discourse-new descriptions would give us a"
J00-4003,P97-1072,1,0.907568,"icult to decide what the intended anchor and the intended link are (Poesio and Vieira 1998). For all these reasons, this class has been the most challenging problem we have dealt with in the development of our system, and the results we have obtained so far can only be considered very preliminary. Nevertheless, we feel that trying to process these definite descriptions is the only way to discover which types of commonsense knowledge are actually needed. 4.4 Types of Bridging Descriptions Our work on bridging descriptions began with the development of a classification of bridging descriptions (Vieira and Teufel 1997) according to the kind of information needed to resolve them, rather than on the basis of the possible relations between descriptions and their anchors as is typical in the literature. This allowed us to get an estimate of what types of bridging descriptions we might expect our system to resolve. The classification is as follows: • cases based on well-defined lexical relations, such as synonymy, hypernymy, and meronymy, that can be found in a lexical database such as WordNet (Fellbaum 1998), as in theyqat . . . the living room; • bridging descriptions in which the antecedent is a proper name a"
J00-4003,M95-1005,0,0.192359,"ed by the automatic evaluation, even though all of these NPs refer to the same entity. A second consequence of the fact that the coreference chains in our standard annotation are not complete is that in the evaluation of direct anaphora resolution, we only verify if the antecedents indicated are correct; we do not evaluate how complete the coreferential chains produced by the system are. By contrast, in the evaluation of the MUC coreference task, where all types of referring expressions are considered, the resulting co-reference chains are evaluated, rather than just the indicated antecedent (Vilain et al. 1995). Even our limited notion of coreference chain was, nevertheless, very helpful in the automatic evaluation, considerably reducing the number of cases to be checked manually. 5.1.3 Measuring the Agreement of the System with the Annotators. Because the agreement between our annotators in Poesio and Vieira (1998) was often only partial, in addition to precision and recall measures, we evaluated the system's performance by measuring its agreement with the annotators using the K statistic we used in Poesio and Vieira (1998) to measure agreement among annotators. Because the proper interpretation of"
J00-4003,A97-1030,0,0.0172891,"rton Inc ... the company) are very common in newspaper articles. Processing such descriptions requires determining an entity type for each name in the text, that is, if we recognize Pinkerton Inc. as an entity of type company, we can then resolve the subsequent description the company, or even a description such as the firm by finding a synonymy relation between company and firm using WordNet. This so-called named entity recognition task has received considerable attention recently (Mani and MacMillan 1996; McDonald 1996; Paik et al. 1996; Bikel et al. 1997; Palmer and Day 1997; Wacholder and Ravin 1997; Mikheev, Moens, and Grover 1999) and was one of the tasks evaluated in the Sixth and Seventh Message Understanding Conferences. In MUC-6, 15 different systems participated in the competition (Sundheim 1995). For the version of the system discussed and evaluated here, we implemented a preliminary algorithm for named entity recognition that we developed ourselves; a more recent version of the system (Ishikawa 1998) uses the named entity recognition software developed by HCRC for the MUC-7 competition (Mikheev, Moens, and Grover 1999). WordNet contains the types of a few names--typically, of fa"
J00-4003,J90-3001,0,\N,Missing
J00-4003,C96-1084,0,\N,Missing
J00-4003,J86-3001,0,\N,Missing
J00-4003,P98-1090,1,\N,Missing
J00-4003,C98-1087,1,\N,Missing
J00-4003,C98-1011,0,\N,Missing
J00-4003,M98-1001,0,\N,Missing
J04-3003,P98-1013,0,0.0128841,"Missing"
J04-3003,P87-1022,0,0.941844,"Missing"
J04-3003,P98-2241,0,0.104716,"setting the theory’s parameters, a systematic comparison can be made only by computational means. A corpus-based evaluation has other advantages, as well, among which is that it is perhaps the best way to identify the aspects of the theory that need to be further specified, and the factors such as temporal coherence or stylistic variation that may interact with the preferences expressed by centering. (Also, knowing the extent to which real texts conform to centering preferences is an important goal in its own right.) In previous corpus-based studies of centering (Walker 1989; Passonneau 1993; Byron and Stent 1998; Di Eugenio 1998; Kameyama 1998; Strube and Hahn 1999; Tetreault 2001), only a few instantiations of centering were compared. The present study is more systematic in that it considers a greater number of parameters, as well as more parameter instantiations, including “crossing” instantiations in which the parameters are set according to proposals due to different researchers. Only re310 Poesio et al. Centering: A Parametric Theory liable annotation techniques were used; we produced an annotation manual that can be used to extend our analysis to other data, as well as a companion Web site (htt"
J04-3003,J96-2004,0,0.0771215,", so the hypotheses about coherence formulated in centering are likely to play an important part in the way these texts are constructed. 3.3 Annotation The previous corpus-based investigations of centering theory we are aware of (Walker 1989; Passonneau 1993, 1998; Byron and Stent 1998; Di Eugenio 1998; Hurewitz 1998; Kameyama 1998; Strube and Hahn 1999) were all carried out by a single annotator annotating her or his corpus according to her or his own subjective judgment. One of our goals was to use for this study only information that could be annotated reliably (Passonneau and Litman 1993; Carletta 1996), as we believe this will make our results easier to replicate. The price we paid to achieve replicability is that we couldn’t test all proposals about the computation of centering parameters proposed in the literature, especially those about segmentation and about ranking, as discussed below. The annotation followed a detailed manual, available on the companion Web site. Eight paid annotators were involved in the reliability studies and the annotation. In the following we briefly discuss the information that we were able to annotate, what we didn’t annotate, and the problems we encountered; f"
J04-3003,P97-1011,1,0.751061,"Missing"
J04-3003,P83-1007,0,0.589485,"Missing"
J04-3003,J95-2003,0,0.981377,"Missing"
J04-3003,J86-3001,0,0.509906,", U.K. E-mail: poesio@essex.ac.uk. † Department of Psychology, University of Durham, U.K. ‡ Department of Computer Science, University of Illinois at Chicago, Chicago, IL 60607-7053, USA. E-mail: bdieugen@cs.uic.edu § MITRE Corporation, 202 Burlington Road, Bedford, MA 01730-1428, USA. E-mail:hitze@mitre.org. Submission received: 16 April 2002; Revised submission received: 3 September 2003; Accepted for publication: 11 December 2003 c 2004 Association for Computational Linguistics  Computational Linguistics Volume 30, Number 3 of attention and coherence in discourse (Grosz 1977; Sidner 1979; Grosz and Sidner 1986) concerned with local coherence and salience, that is, coherence and salience within a discourse segment. A fundamental characteristic of centering is that it is better viewed as a linguistic theory than a computational one. By this we mean that its primary aim is to make cross-linguistically valid claims about which discourses are easier to process, abstracting away from specific algorithms for anaphora resolution or anaphora generation (although many such algorithms are based on the theory). The result is a very different theory from those one usually finds in computational linguistics. In c"
J04-3003,C00-1045,1,0.906405,"Missing"
J04-3003,P98-1090,1,0.899002,"Missing"
J04-3003,P86-1031,0,0.651811,"d to the RET transition, which is preferred to the Smooth Shift transition, which is preferred to the Rough Shift transition. This formulation of Rule 2 depends on a further distinction between two types of SHIFT: Smooth Shift (SSH), when CB(Un ) = CP(Un ), and Rough-Shift (RSH), when CB(Un ) = CP(Un ). Transitions can then be classified along two dimensions, as in the following table: CB(Un ) = CB(Un−1 ) or CB(Un−1 ) = NIL CB(Un ) = CB(Un−1 ) CB(Un ) = CP(Un ) Continue Smooth Shift CB(Un ) = CP(Un ) Retain Rough Shift Further refinements of these classification schemes have been proposed. Kameyama (1986) proposed a fourth transition type, Center Establishment (EST), for utterances E.g., in Bruno was the bully of the neighborhood. Bruno / He often taunted Tommy, the second sentence would be read more slowly when Bruno was used than when He was used. 315 Computational Linguistics Volume 30, Number 3 that establish a CB after an utterance without one, such as the first utterance of a segment. Walker, Iida, and Cote (1994) argued that these utterances should be classified as Center Continuations, the idea being that even the first utterance of a segment does have a CB, but this CB is initially un"
J04-3003,P93-1010,1,0.772367,"Missing"
J04-3003,J01-4007,0,0.352925,"ecific definition, even for English. Similarly undefined is the notion of “pronominalization” governed by Rule 1. But without further specification of these concepts it is impossible to evaluate the claims above, just as it is not possible to evaluate the predictions of, say, “government and binding theory” without providing an explicit definition of “command” or “argument”. As a result, a considerable amount of research has been concerned with establishing the best specification for what are, essentially, parameters of the theory. We briefly review some of these proposals in this section.7 6 Kibble (2001) proposed a version of Rule 2 that further develops the “decompositional” view of Rule 2 introduced by Brennan et al., while simultaneously incorporating Strube and Hahn’s intuition that “cheap” transitions should be preferred. Kibble formulates his version of Rule 2 as a series of preferences: for transitions that preserve the CB—that is, those such that CB(Un ) = CB(Un−1 ) (he calls these transitions cohesive), that identify CB(Un ) with CP(Un ), and/or that are cheap. Code to test an earlier version of Kibble’s form of Rule 2 (Kibble 2000) has been incorporated in the scripts discussed late"
J04-3003,J94-4002,0,0.243633,"rred over (sequences of) shifts. 2.3.1 Constraint 1, Topic Uniqueness, and Entity Coherence. If we view the CB as a formalization of the idea of “topic” (Vallduvi 1990; Gundel 1998; Hurewitz 1988; Miltsakaki 1999; Beaver 2004), Constraint 1 expresses, first and foremost, the original claim from Joshi and Kuhn (1979) and Joshi and Weinstein (1981) that discourses with exactly one (or no more than one) “topic” at each point are easier to process. This view contrasts both with Sidner’s (1979) hypothesis that utterances may have two “topics” and with theories such as Givon (1983), Alshawi (1987), Lappin and Leass (1994) and Arnold (1998), which view “topichood” as a matter of degree and therefore allow for an arbitrary number of topics. In the strong form just presented, Constraint 1 is also a claim about local coherence. It expresses a preference for discourses to be entity coherent: to continue talking about the same entities. Each utterance in a segment should realize at least one of the discourse entities realized in the previous utterance. A weaker form of Constraint 1 has also been suggested (e.g., Walker, Joshi, and Prince 1998a, footnote 2, page 3); the preference for a unique CB is preserved, but no"
J04-3003,J02-3003,0,0.172029,"in German. Conversely, before taking the evidence for a slight advantage of STRUBE-HAHN ranking over grammatical-function ranking as conclusive, one needs to supplement our studies with psychological experiments reconciling these results with numerous results indicating the important role played by grammatical function, especially subjecthood (among others, Hudson, Tanenhaus, and Dell [1986]; Gordon, Grosz, and Gillion [1993]; Brennan [1995]). Information structure has also been found not to be appropriate for languages including Greek, Hindi, and Turkish (Turan 1998; Prasad and Strube 2000; Miltsakaki 2002). Similar considerations apply to the definition of previous utterance, since we saw that a considerable amount of psychological evidence supports treating adjuncts as embedded, at least when the syntactically embedded clause is at the end of the sentence (Cooreman and Sanford 1996; Pearson, Stevenson, and Poesio 2000). In the case of the definition of utterance, our results indicate that identifying utterances with sentences, rather than finite clauses, leads to results much more consistent with the claimed preference for discourses to be entity-coherent. While this result is likely to be use"
J04-3003,J96-3006,0,0.0194465,"values of agreement. 325 Computational Linguistics Volume 30, Number 3 matched by any other CF in the same sentence. We tested only heuristic methods as well, using the layout structure of the texts as a rough indicator of discourse structure. In this article we discuss only the results with the heuristic proposed by Walker. In the extended technical report available on the Web site, we discuss the results with other segmentation heuristics, as well as further results with the tutorial dialogues subdomain of the GNOME corpus, independently annotated according to relational discourse analysis (Moser and Moore 1996), a technique inspired by Grosz and Sidner’s proposals, from which a Grosz and Sidner–like segmentation was extracted as proposed in Poesio and Di Eugenio (2001). 3.4 Automatic Computation of Centering Information The annotated corpus is used by Perl scripts that automatically compute the centering data structures (utterances, CFs, and CBs) according to the particular parameter instantiation chosen, find violations of Constraint 1, Rule 1, and Rule 2 (according to several versions of Rule 1 and Rule 2), and evaluate the claims using the statistical tests. The behavior of the scripts is control"
J04-3003,poesio-2000-annotating,1,0.700552,"dner (1986). According to Kameyama, only a few types of clauses, such as the complements of certain verbs, are embedded. For example, Kameyama proposes to break up (4) into utterances as follows, and to treat each of these utterances, including subordinate clauses such as (U2) or (U5), as an update: (4) (u1) Her entrance in Scene 2 Act 1 brought some disconcerting applause (u2) even before she had sung a note. (u3) Thereafter the audience waxed applause happy (u4) but discriminating operagoers reserved judgment (u5) as her singing showed signs of strain. Experiments by Pearson, Stevenson, and Poesio (2000) confirmed that CFs introduced in main clauses are significantly more likely to be subsequently mentioned than CFs introduced in complement clauses. However, a semicontrolled study by Suri and McCoy (1994) suggested that other types of clauses—specifically, adjunct clauses headed by after and before–are also “embedded,” not “permanent updates” as suggested by Kameyama; these results were subsequently confirmed by Cooreman and Sanford (1996). The status of other types of clauses is less clear. Kameyama (1998) also proposed a tentative analysis of relative clauses, according to which they are te"
J04-3003,W99-0309,0,0.104221,"Missing"
J04-3003,J98-2001,1,0.829069,"h that it can be used to identify the intentional structure of texts—which, according to Grosz and Sidner, determines their segmentation. As a result, only preliminary attempts at annotating texts according to Grosz and Sidner’s theory have been made. For this reason, most previous corpus-based studies of centering either ignored segmentation or used heuristics such as those proposed by Walker (1989): Consider every paragraph as a separate discourse segment, except when its first sentence contains a pronoun in subject position or a pronoun whose agreement features are not 18 In previous work (Poesio and Vieira 1998) we came to the conclusion that kappa, while appropriate when the number of categories is fixed and relatively small, is problematic for anaphoric reference, when neither condition applies, and may result in inflated values of agreement. 325 Computational Linguistics Volume 30, Number 3 matched by any other CF in the same sentence. We tested only heuristic methods as well, using the layout structure of the texts as a rough indicator of discourse structure. In this article we discuss only the results with the heuristic proposed by Walker. In the extended technical report available on the Web si"
J04-3003,W98-1427,0,0.0785317,"Missing"
J04-3003,P98-2204,0,0.0291145,"clauses headed by after and before–are also “embedded,” not “permanent updates” as suggested by Kameyama; these results were subsequently confirmed by Cooreman and Sanford (1996). The status of other types of clauses is less clear. Kameyama (1998) also proposed a tentative analysis of relative clauses, according to which they are temporarily treated as utterances and update the local focus but are then merged with the embedding clause; she didn’t, however provide empirical support for this hypothesis. Other types of subordinate clauses and parentheticals are not discussed in this literature. Strube (1998) and Miltsakaki (1999) question Kameyama’s identification of utterances with (tensed) clauses. Miltsakaki (1999) argues, on the basis of data from English and Greek, that the local focus is updated only after every sentence and that only the CFs in the main clause are considered when establishing the CB. 2.4.2 Realization. Grosz, Joshi, and Weinstein (1995) simply say that what it means for utterance U to realize center C depends on the particular semantic theory one adopts. They consider two ways in which a discourse entity may be “realized” in an utterance as required by Constraint 2. Direct"
J04-3003,J99-3001,0,0.0635668,"ntering argue that while these concepts play a central role in any theory of discourse coherence and salience, their precise characterization is best left for subsequent research, indeed, that some of these concepts (e.g., ranking) might be defined in a different way for each language (Walker, Iida, and Cote 1994). In other words, these notions should be viewed as parameters of centering. This feature of the theory has inspired a great deal of research attempting to specify centering’s parameters for different languages (Kameyama 1985; Walker, Iida, and Cote 1994; Di Eugenio 1998; Turan 1998; Strube and Hahn 1999). Competing versions of the central definitions and claims of the theory have also been proposed: For example, different definitions of backward-looking center (CB) can be found in Grosz, Joshi, and Weinstein (1983, 1995) and Gordon, Grosz, and Gillion (1993). As a result, a researcher wishing to test the predictions of centering, or to use it for practical applications, is confronted with a large number of possible instantiations of the theory. The main goal of the work reported in this article was to explore the space of parameter configurations, measuring the impact of different ways of set"
J04-3003,J94-2006,0,0.146961,"Missing"
J04-3003,J01-4003,0,0.385979,"computational means. A corpus-based evaluation has other advantages, as well, among which is that it is perhaps the best way to identify the aspects of the theory that need to be further specified, and the factors such as temporal coherence or stylistic variation that may interact with the preferences expressed by centering. (Also, knowing the extent to which real texts conform to centering preferences is an important goal in its own right.) In previous corpus-based studies of centering (Walker 1989; Passonneau 1993; Byron and Stent 1998; Di Eugenio 1998; Kameyama 1998; Strube and Hahn 1999; Tetreault 2001), only a few instantiations of centering were compared. The present study is more systematic in that it considers a greater number of parameters, as well as more parameter instantiations, including “crossing” instantiations in which the parameters are set according to proposals due to different researchers. Only re310 Poesio et al. Centering: A Parametric Theory liable annotation techniques were used; we produced an annotation manual that can be used to extend our analysis to other data, as well as a companion Web site (http://cswww.essex.ac.uk/staff/poesio/cbc/) to allow readers to try out in"
J04-3003,J00-4005,0,0.0611502,"Missing"
J04-3003,P89-1031,0,0.341262,"us number of possible ways of setting the theory’s parameters, a systematic comparison can be made only by computational means. A corpus-based evaluation has other advantages, as well, among which is that it is perhaps the best way to identify the aspects of the theory that need to be further specified, and the factors such as temporal coherence or stylistic variation that may interact with the preferences expressed by centering. (Also, knowing the extent to which real texts conform to centering preferences is an important goal in its own right.) In previous corpus-based studies of centering (Walker 1989; Passonneau 1993; Byron and Stent 1998; Di Eugenio 1998; Kameyama 1998; Strube and Hahn 1999; Tetreault 2001), only a few instantiations of centering were compared. The present study is more systematic in that it considers a greater number of parameters, as well as more parameter instantiations, including “crossing” instantiations in which the parameters are set according to proposals due to different researchers. Only re310 Poesio et al. Centering: A Parametric Theory liable annotation techniques were used; we produced an annotation manual that can be used to extend our analysis to other dat"
J04-3003,J94-2003,0,0.0518391,"Missing"
J04-3003,P00-1051,0,\N,Missing
J04-3003,P05-1018,0,\N,Missing
J04-3003,C98-1013,0,\N,Missing
J04-3003,C98-2236,0,\N,Missing
J04-3003,P97-1014,0,\N,Missing
J04-3003,J88-2006,0,\N,Missing
J04-3003,J92-4007,0,\N,Missing
J04-3003,W00-1411,0,\N,Missing
J04-3003,P93-1020,0,\N,Missing
J04-3003,J97-3006,0,\N,Missing
J04-3003,C98-1087,1,\N,Missing
J04-3003,N01-1002,1,\N,Missing
J04-3003,C98-2199,0,\N,Missing
J04-3003,W04-2327,1,\N,Missing
J04-3003,W03-2605,1,\N,Missing
J04-3003,C69-7001,0,\N,Missing
J04-3003,C69-6902,0,\N,Missing
J04-3003,W02-2111,0,\N,Missing
J04-3003,J96-2005,0,\N,Missing
J08-4004,W98-1507,0,0.0780676,"Castellan. Submission received: 26 August 2005; revised submission received: 21 December 2007; accepted for publication: 28 January 2008. © 2008 Association for Computational Linguistics Computational Linguistics Volume 34, Number 4 were enormously inﬂuential, and K quickly became the de facto standard for measuring agreement in computational linguistics not only in work on discourse (Carletta et al. 1997; Core and Allen 1997; Hearst 1997; Poesio and Vieira 1998; Di Eugenio 2000; Stolcke et al. 2000; Carlson, Marcu, and Okurowski 2003) but also for other annotation tasks (e.g., V´eronis 1998; Bruce and Wiebe 1998; Stevenson and Gaizauskas 2000; Craggs and McGee Wood 2004; Mieskes and Strube 2006). During this period, however, a number of questions have also been raised about K and similar coefﬁcients—some already in Carletta’s own work (Carletta et al. 1997)—ranging from simple questions about the way the coefﬁcient is computed (e.g., whether it is really applicable when more than two coders are used), to debates about which levels of agreement can be considered ‘acceptable’ (Di Eugenio 2000; Craggs and McGee Wood 2005), to the realization that K is not appropriate for all types of agreement (Poesio a"
J08-4004,J96-2004,0,0.857273,"r areas of computational linguistics (CL). This soon led to worries about the subjectivity of the judgments required to create annotated resources, much greater for semantics and pragmatics than for the aspects of language interpretation of concern in the creation of early resources such as the Brown corpus (Francis and Kucera 1982), the British National Corpus (Leech, Garside, and Bryant 1994), or the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993). Problems with early proposals for assessing coders’ agreement on discourse segmentation tasks (such as Passonneau and Litman 1993) led Carletta (1996) to suggest the adoption of the K coefﬁcient of agreement, a variant of Cohen’s κ (Cohen 1960), as this had already been used for similar purposes in content analysis for a long time.1 Carletta’s proposals ∗ Now at the Institute for Creative Technologies, University of Southern California, 13274 Fiji Way, Marina Del Rey, CA 90292. ∗∗ At the University of Essex: Department of Computing and Electronic Systems, University of Essex, Wivenhoe Park, Colchester, CO4 3SQ, UK. E-mail: poesio@essex.ac.uk. At the University of Trento: CIMeC, Universit`a degli Studi di Trento, Palazzo Fedrigotti, Corso Be"
J08-4004,J97-1002,0,0.0265824,"Missing"
J08-4004,J05-3001,0,0.0471299,"Missing"
J08-4004,di-eugenio-2000-usage,0,0.0200079,"Missing"
J08-4004,P98-1052,0,0.0727297,"Missing"
J08-4004,W01-1607,0,0.0268042,"barczy, Carroll, and Sampson 2006), let alone for discourse coding tasks such as dialogue act coding. We concentrate here on this latter type of coding, but a discussion of issues raised for POS, named entity, and prosodic coding can be found in the extended version of the article. Dialogue act tagging is a type of linguistic annotation with which by now the CL community has had extensive experience: Several dialogue-act-annotated spoken language corpora now exist, such as MapTask (Carletta et al. 1997), Switchboard (Stolcke et al. 2000), Verbmobil (Jekat et al. 1995), and Communicator (e.g., Doran et al. 2001), among others. Historically, dialogue act annotation was also one of the types of annotation that motivated the introduction in CL of chance-corrected coefﬁcients of agreement (Carletta et al. 1997) and, as we will see, it has been the type of annotation that has generated the most discussion concerning annotation methodology and measuring agreement. A number of coding schemes for dialogue acts have achieved values of K over 0.8 and have therefore been assumed to be reliable: For example, K = 0.83 for the 577 Computational Linguistics Volume 34, Number 4 13-tag MapTask coding scheme (Carletta"
J08-4004,W06-1318,0,0.0328439,"n to be performed by the addressee. At least in principle, an organization of this type opens up the possibility for coders to mark an utterance with the superclass (IAFA) in case they do not feel conﬁdent that the utterance satisﬁes the additional requirements for Open-option or Directive. This, in turn, would do away with the need to make a choice between these two options. This possibility wasn’t pursued in the studies using the original DAMSL that we are aware of (Core and Allen 1997; Di Eugenio 2000; Stent 2001), but was tested by Shriberg et al. (2004) and subsequent work, in particular Geertzen and Bunt (2006), who were speciﬁcally interested in the idea of using hierarchical schemes to measure partial agreement, and in addition experimented with weighted coefﬁcients of agreement for their hierarchical tagging scheme, speciﬁcally κ w . Geertzen and Bunt tested intercoder agreement with Bunt’s DIT++ (Bunt 2005), a scheme with 11 dimensions that builds on ideas from DAMSL and from Dynamic Interpretation Theory (Bunt 2000). In DIT++, tags can be hierarchically related: For example, the class information-seeking is viewed as consisting of two classes, yesno question (ynq) and wh-question (whq). The hie"
J08-4004,J86-3001,0,0.179667,"Missing"
J08-4004,J97-1003,0,0.197477,"by K. In what follows, we use κ to indicate Cohen’s original coefﬁcient and its generalization to more than two coders, and K for the coefﬁcient discussed by Siegel and Castellan. Submission received: 26 August 2005; revised submission received: 21 December 2007; accepted for publication: 28 January 2008. © 2008 Association for Computational Linguistics Computational Linguistics Volume 34, Number 4 were enormously inﬂuential, and K quickly became the de facto standard for measuring agreement in computational linguistics not only in work on discourse (Carletta et al. 1997; Core and Allen 1997; Hearst 1997; Poesio and Vieira 1998; Di Eugenio 2000; Stolcke et al. 2000; Carlson, Marcu, and Okurowski 2003) but also for other annotation tasks (e.g., V´eronis 1998; Bruce and Wiebe 1998; Stevenson and Gaizauskas 2000; Craggs and McGee Wood 2004; Mieskes and Strube 2006). During this period, however, a number of questions have also been raised about K and similar coefﬁcients—some already in Carletta’s own work (Carletta et al. 1997)—ranging from simple questions about the way the coefﬁcient is computed (e.g., whether it is really applicable when more than two coders are used), to debates about which l"
J08-4004,N06-2015,0,0.177165,"main headings: methodology, choice of coefﬁcients, and interpretation of coefﬁcients. 589 Computational Linguistics Volume 34, Number 4 5.1 Methodology Our ﬁrst recommendation is that annotation efforts should perform and report rigorous reliability testing. The last decade has already seen considerable improvement, from the absence of any tests for the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993) or the British National Corpus (Leech, Garside, and Bryant 1994) to the central role played by reliability testing in the Penn Discourse Treebank (Miltsakaki et al. 2004) and OntoNotes (Hovy et al. 2006). But even the latter efforts only measure and report percent agreement. We believe that part of the reluctance to report chance-corrected measures is the difﬁculty in interpreting them. However, our experience is that chancecorrected coefﬁcients of agreement do provide a better indication of the quality of the resulting annotation than simple percent agreement, and moreover, the detailed calculations leading to the coefﬁcients can be very revealing as to where the disagreements are located and what their sources may be. A rigorous methodology for reliability testing does not, in our opinion,"
J08-4004,E99-1046,0,0.0728621,"enough to be useful (cf. Krippendorff 2004a, pages 213–214). In content analysis, conclusions are drawn directly from annotated corpora, so the emphasis is more on replicability; whereas in CL, corpora constitute a resource which is used by other processes, so the emphasis is more towards usefulness. There is also a tradeoff between the sophistication of judgments and the availability of coders who can make such judgments. Consequently, annotation by experts is often the only practical way to get useful corpora for CL. Current practice achieves high reliability either by using professionals (Kilgarriff 1999) or through intensive training (Hovy et al. 2006; Carlson, Marcu, and Okurowski 2003); this means that results are not replicable across sites, and are therefore less reliable than annotation by naive coders adhering to written instructions. We feel that inter-annotator agreement studies should still be carried out, as they serve as an assurance that the results are replicable when the annotators are chosen from the same population as the original annotators. An important additional assurance should be provided in the form of an independent evaluation of the task for which the corpus is used ("
J08-4004,C94-1103,0,0.107243,"Missing"
J08-4004,J93-2004,0,0.0623642,"Missing"
J08-4004,mieskes-strube-2006-part,0,0.0384854,"ecember 2007; accepted for publication: 28 January 2008. © 2008 Association for Computational Linguistics Computational Linguistics Volume 34, Number 4 were enormously inﬂuential, and K quickly became the de facto standard for measuring agreement in computational linguistics not only in work on discourse (Carletta et al. 1997; Core and Allen 1997; Hearst 1997; Poesio and Vieira 1998; Di Eugenio 2000; Stolcke et al. 2000; Carlson, Marcu, and Okurowski 2003) but also for other annotation tasks (e.g., V´eronis 1998; Bruce and Wiebe 1998; Stevenson and Gaizauskas 2000; Craggs and McGee Wood 2004; Mieskes and Strube 2006). During this period, however, a number of questions have also been raised about K and similar coefﬁcients—some already in Carletta’s own work (Carletta et al. 1997)—ranging from simple questions about the way the coefﬁcient is computed (e.g., whether it is really applicable when more than two coders are used), to debates about which levels of agreement can be considered ‘acceptable’ (Di Eugenio 2000; Craggs and McGee Wood 2005), to the realization that K is not appropriate for all types of agreement (Poesio and Vieira 1998; Marcu, Romera, and Amorrortu 1999; Di Eugenio 2000; Stevenson and Gai"
J08-4004,W04-0807,0,0.0167724,"Missing"
J08-4004,W04-2703,0,0.00940122,"ment. These can be grouped under three main headings: methodology, choice of coefﬁcients, and interpretation of coefﬁcients. 589 Computational Linguistics Volume 34, Number 4 5.1 Methodology Our ﬁrst recommendation is that annotation efforts should perform and report rigorous reliability testing. The last decade has already seen considerable improvement, from the absence of any tests for the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993) or the British National Corpus (Leech, Garside, and Bryant 1994) to the central role played by reliability testing in the Penn Discourse Treebank (Miltsakaki et al. 2004) and OntoNotes (Hovy et al. 2006). But even the latter efforts only measure and report percent agreement. We believe that part of the reluctance to report chance-corrected measures is the difﬁculty in interpreting them. However, our experience is that chancecorrected coefﬁcients of agreement do provide a better indication of the quality of the resulting annotation than simple percent agreement, and moreover, the detailed calculations leading to the coefﬁcients can be very revealing as to where the disagreements are located and what their sources may be. A rigorous methodology for reliability t"
J08-4004,W00-1007,0,0.0214899,"d next. 4.4.3 Discourse Deixis. A second annotation study we carried out (Artstein and Poesio 2006) shows even more clearly the possible side effects of using weighted coefﬁcients. This study was concerned with the annotation of the antecedents of references to abstract objects, such as the example of the pronoun that in utterance 7.6 (TRAINS 1991, dialogue d91-2.2). 7.3 7.4 7.5 7.6 : : : : so we ship one boxcar of oranges to Elmira and that takes another 2 hours Previous studies of discourse deixis annotation showed that these are extremely difﬁcult judgments to make (Eckert and Strube 2000; Navarretta 2000; Byron 2002), except perhaps for identifying the type of object (Poesio and Modjeska 2005), so we simpliﬁed the task by only requiring our participants to identify the boundaries of the area of text in which the antecedent was introduced. Even so, we found a great variety in how these boundaries were marked: Exactly as in the case of discourse segmentation discussed earlier, our participants broadly agreed on the area of text, but disagreed on 0.7 Chain K α None Partial Full 0.628 0.563 0.480 0.656 0.677 0.691 α K α α 0.6 K 0.5 0.4 K no chain partial chain full chain Figure 3 A comparison of"
J08-4004,N04-1019,0,0.00655755,"t by using individual coder marginals (κ) or pooled distributions (K) can lead to reliability values falling on different sides of the accepted 0.67 threshold, and recommended reporting both values. Craggs and McGee Wood argued, following Krippendorff (2004a,b), that measures like Cohen’s κ are inappropriate for measuring agreement. Finally, Passonneau has been advocating the use of Krippendorff’s α (Krippendorff 1980, 2004a) for coding tasks in CL which do not involve nominal and disjoint categories, including anaphoric annotation, wordsense tagging, and summarization (Passonneau 2004, 2006; Nenkova and Passonneau 2004; Passonneau, Habash, and Rambow 2006). Now that more than ten years have passed since Carletta’s original presentation at the workshop on Empirical Methods in Discourse, it is time to reconsider the use of coefﬁcients of agreement in CL in a systematic way. In this article, a survey of coefﬁcients of agreement and their use in CL, we have three main goals. First, we discuss in some detail the mathematics and underlying assumptions of the coefﬁcients used or mentioned in the CL and content analysis literatures. Second, we also cover in some detail Krippendorff’s α, often mentioned but never re"
J08-4004,passonneau-2004-computing,0,0.0606495,"ulating chance agreement by using individual coder marginals (κ) or pooled distributions (K) can lead to reliability values falling on different sides of the accepted 0.67 threshold, and recommended reporting both values. Craggs and McGee Wood argued, following Krippendorff (2004a,b), that measures like Cohen’s κ are inappropriate for measuring agreement. Finally, Passonneau has been advocating the use of Krippendorff’s α (Krippendorff 1980, 2004a) for coding tasks in CL which do not involve nominal and disjoint categories, including anaphoric annotation, wordsense tagging, and summarization (Passonneau 2004, 2006; Nenkova and Passonneau 2004; Passonneau, Habash, and Rambow 2006). Now that more than ten years have passed since Carletta’s original presentation at the workshop on Empirical Methods in Discourse, it is time to reconsider the use of coefﬁcients of agreement in CL in a systematic way. In this article, a survey of coefﬁcients of agreement and their use in CL, we have three main goals. First, we discuss in some detail the mathematics and underlying assumptions of the coefﬁcients used or mentioned in the CL and content analysis literatures. Second, we also cover in some detail Krippendorf"
J08-4004,passonneau-2006-measuring,0,0.431865,"et relation are closer (less distant) than ones that merely intersect. This leads to the following distance metric between two sets A and B.  0 if   1 /3 if dP = 2 / if    3 1 if A=B A ⊂ B or B ⊂ A A ∩ B = ∅, but A ⊂ B and B ⊂ A A∩B = ∅ Alternative distance metrics take the size of the anaphoric chain into account, based on measures used to compare sets in Information Retrieval, such as the coefﬁcient of community of Jaccard (1912) and the coincidence index of Dice (1945) (Manning and ¨ Schutze 1999). Jaccard: d J = 1 − |A ∩ B| |A ∪ B| Dice: d D = 1 − 2 |A ∩ B| |A |+ |B| In later work, Passonneau (2006) offers a reﬁned distance metric which she called MASI (Measuring Agreement on Set-valued Items), obtained by multiplying Passonneau’s original metric d P by the metric derived from Jaccard d J . d M = dP × d J 4.4.2 Experience with α for Anaphoric Annotation. In the experiment mentioned previously (Poesio and Artstein 2005) we used 18 coders to test α and K under a variety of conditions. We found that even though our coders by and large agreed on the interpretation of anaphoric expressions, virtually no coder ever identiﬁed all the mentions of a discourse entity. As a result, even though the"
J08-4004,passonneau-etal-2006-inter,0,0.142739,"Missing"
J08-4004,P93-1020,0,0.0570518,"e same empirical footing as other areas of computational linguistics (CL). This soon led to worries about the subjectivity of the judgments required to create annotated resources, much greater for semantics and pragmatics than for the aspects of language interpretation of concern in the creation of early resources such as the Brown corpus (Francis and Kucera 1982), the British National Corpus (Leech, Garside, and Bryant 1994), or the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993). Problems with early proposals for assessing coders’ agreement on discourse segmentation tasks (such as Passonneau and Litman 1993) led Carletta (1996) to suggest the adoption of the K coefﬁcient of agreement, a variant of Cohen’s κ (Cohen 1960), as this had already been used for similar purposes in content analysis for a long time.1 Carletta’s proposals ∗ Now at the Institute for Creative Technologies, University of Southern California, 13274 Fiji Way, Marina Del Rey, CA 90292. ∗∗ At the University of Essex: Department of Computing and Electronic Systems, University of Essex, Wivenhoe Park, Colchester, CO4 3SQ, UK. E-mail: poesio@essex.ac.uk. At the University of Trento: CIMeC, Universit`a degli Studi di Trento, Palazzo"
J08-4004,J97-1005,0,0.0215495,"Missing"
J08-4004,J02-1002,0,0.0336925,"ng several units, as done in the methods proposed to evaluate the performance of topic detection algorithms such as 581 Computational Linguistics Volume 34, Number 4 Table 8 Fewer boundaries, higher expected agreement. Case 1: Broad segments Ao = 0.96, Ae = 0.89, K = 0.65 C ODER A C ODER B B OUNDARY N O B OUNDARY T OTAL B OUNDARY N O B OUNDARY T OTAL 2 1 3 1 46 47 3 47 50 Case 2: Fine discourse units Ao = 0.88, Ae = 0.53, K = 0.75 C ODER A C ODER B B OUNDARY N O B OUNDARY T OTAL B OUNDARY N O B OUNDARY T OTAL 16 3 19 3 28 31 19 31 50 Pk (Beeferman, Berger, and Lafferty 1999) or W INDOW D IFF (Pevzner and Hearst 2002) (which are, however, raw agreement scores not corrected for chance). 4.3.2 Unitizing (Or, Agreement on Markable Identiﬁcation). It is often assumed in CL annotation practice that the units of analysis are “natural” linguistic objects, and therefore there is no need to check agreement on their identiﬁcation. As a result, agreement is usually measured on the labeling of units rather than on the process of identifying them (unitizing, Krippendorff 1995). We have just seen, however, two coding tasks for which the reliability of unit identiﬁcation is a crucial part of the overall reliability, and"
J08-4004,W04-0210,1,0.0907393,"ies and Unitizing Before labeling can take place, the units of annotation, or markables, need to be identiﬁed—a process Krippendorff (1995, 2004a) calls unitizing. The practice in CL for the forms of annotation discussed in the previous section is to assume that the units are linguistic constituents which can be easily identiﬁed, such as words, utterances, or noun phrases, and therefore there is no need to check the reliability of this process. We are aware of few exceptions to this assumption, such as Carletta et al. (1997) on unitization for move coding and our own work on the GNOME corpus (Poesio 2004b). In cases such as text segmentation, however, the identiﬁcation of units is as important as their labeling, if not more important, and therefore checking agreement on unit identiﬁcation is essential. In this section we discuss current CL practice with reliability testing of these types of annotation, before brieﬂy summarizing Krippendorff’s proposals concerning measuring reliability for unitizing. 4.3.1 Segmentation and Topic Marking. Discourse segments are portions of text that constitute a unit either because they are about the same “topic” (Hearst 1997; Reynar 1998) or because they have"
J08-4004,W04-2327,1,0.116185,"ies and Unitizing Before labeling can take place, the units of annotation, or markables, need to be identiﬁed—a process Krippendorff (1995, 2004a) calls unitizing. The practice in CL for the forms of annotation discussed in the previous section is to assume that the units are linguistic constituents which can be easily identiﬁed, such as words, utterances, or noun phrases, and therefore there is no need to check the reliability of this process. We are aware of few exceptions to this assumption, such as Carletta et al. (1997) on unitization for move coding and our own work on the GNOME corpus (Poesio 2004b). In cases such as text segmentation, however, the identiﬁcation of units is as important as their labeling, if not more important, and therefore checking agreement on unit identiﬁcation is essential. In this section we discuss current CL practice with reliability testing of these types of annotation, before brieﬂy summarizing Krippendorff’s proposals concerning measuring reliability for unitizing. 4.3.1 Segmentation and Topic Marking. Discourse segments are portions of text that constitute a unit either because they are about the same “topic” (Hearst 1997; Reynar 1998) or because they have"
J08-4004,W05-0311,1,0.212648,"ly that multiple coders increase reliability: The variance of the individual coders’ distributions can be just as large with many coders as with few coders, but its effect on the value of κ decreases as the number of coders grows, and becomes more similar to random noise. The same holds for weighted measures too; see the extended version of this article for deﬁnitions and proof. In an annotation study with 18 subjects, we compared α with a variant which uses individual coder distributions to calculate expected agreement, and found that the values never differed beyond the third decimal point (Poesio and Artstein 2005). We conclude with a summary of our views concerning the difference between πstyle and κ-style coefﬁcients. First of all, keep in mind that empirically the difference is small, and gets smaller as the number of annotators increases. Then instead of reporting two coefﬁcients, as suggested by Di Eugenio and Glass (2004), the appropriate coefﬁcient should be chosen based on the task (not on the observed differences between coder marginals). When the coefﬁcient is used to assess reliability, a single-distribution coefﬁcient like π or α should be used; this is indeed already the practice in CL, bec"
J08-4004,J98-2001,1,0.107964,"follows, we use κ to indicate Cohen’s original coefﬁcient and its generalization to more than two coders, and K for the coefﬁcient discussed by Siegel and Castellan. Submission received: 26 August 2005; revised submission received: 21 December 2007; accepted for publication: 28 January 2008. © 2008 Association for Computational Linguistics Computational Linguistics Volume 34, Number 4 were enormously inﬂuential, and K quickly became the de facto standard for measuring agreement in computational linguistics not only in work on discourse (Carletta et al. 1997; Core and Allen 1997; Hearst 1997; Poesio and Vieira 1998; Di Eugenio 2000; Stolcke et al. 2000; Carlson, Marcu, and Okurowski 2003) but also for other annotation tasks (e.g., V´eronis 1998; Bruce and Wiebe 1998; Stevenson and Gaizauskas 2000; Craggs and McGee Wood 2004; Mieskes and Strube 2006). During this period, however, a number of questions have also been raised about K and similar coefﬁcients—some already in Carletta’s own work (Carletta et al. 1997)—ranging from simple questions about the way the coefﬁcient is computed (e.g., whether it is really applicable when more than two coders are used), to debates about which levels of agreement can b"
J08-4004,J08-3001,0,0.233261,"re appropriate for many annotation tasks, make the issue of deciding when the value of a coefﬁcient indicates sufﬁcient agreement even K= 0.0 Poor 0.2 Slight 0.4 Fair 0.6 Moderate 0.8 Substantial Figure 1 Kappa values and strength of agreement according to Landis and Koch (1977). 576 1.0 Perfect Artstein and Poesio Inter-Coder Agreement for CL more complicated because of the problem of determining appropriate weights (see Section 4.4). We will return to the issue of interpreting the value of the coefﬁcients at the end of this article. 4.1.4 Agreement and Machine Learning. In a recent article, Reidsma and Carletta (2008) point out that the goals of annotation in CL differ from those of content analysis, where agreement coefﬁcients originate. A common use of an annotated corpus in CL is not to conﬁrm or reject a hypothesis, but to generalize the patterns using machine-learning algorithms. Through a series of simulations, Reidsma and Carletta demonstrate that agreement coefﬁcients are poor predictors of machine-learning success: Even highly reproducible annotations are difﬁcult to generalize when the disagreements contain patterns that can be learned, whereas highly noisy and unreliable data can be generalized"
J08-4004,N04-4020,0,0.0258176,"computing agreement proposed here could could also be used to allow coders to choose either a more speciﬁc label or one of Palmer, Dang, and Fellbaum’s superlabels. For example, suppose A sticks to WN1, but B decides to mark the use above using Palmer, Dang, and Fellbaum’s LABEL category, then we would still ﬁnd a distance d = 1/3. An alternative way of using α for word sense annotation was developed and tested by Passonneau, Habash, and Rambow (2006). Their approach is to allow coders to assign multiple labels (WordNet synsets) for wordsenses, as done by V´eronis (1998) and more recently by Rosenberg and Binkowski (2004) for text classiﬁcation labels and by Poesio and Artstein (2005) for anaphora. These multi-label sets can then be compared using the MASI distance metric for α (Passonneau 2006). 5. Conclusions The purpose of this article has been to expose the reader to the mathematics of chancecorrected coefﬁcients of agreement as well as the current state of the art of using these coefﬁcients in CL. Our hope is that readers come to view agreement studies not as an additional chore or hurdle for publication, but as a tool for analysis which offers new insights into the annotation process. We conclude by summ"
J08-4004,W04-2319,0,0.0265492,"erg, and Biasca (1997), which incorporates many ideas from the “multi-dimensional” theories of dialogue acts, but does not allow marking an utterance as both an acknowledgment and a statement; a choice has to be made. This tagset results in overall agreement of K = 0.80. Interestingly, subsequent developments of SWITCHBOARD-DAMSL backtracked on some of these decisions. For instance, the ICSI-MRDA tagset developed for the annotation of the ICSI Meeting Recorder corpus reintroduces some of the DAMSL ideas, in that annotators are allowed to assign multiple SWITCHBOARD-DAMSL labels to utterances (Shriberg et al. 2004). Shriberg et al. achieved a comparable reliability to that obtained with SWITCHBOARD-DAMSL, but only when using a tagset of just ﬁve “class-maps”. Shriberg et al. (2004) also introduced a hierarchical organization of tags to improve reliability. The dimensions of the DAMSL scheme can be viewed as “superclasses” of dialogue acts which share some aspect of their meaning. For instance, the dimension of Influencing-Addressee-Future-Action (IAFA) includes the two dialogue acts Open-option (used to mark suggestions) and Directive, both of which bring into 578 Artstein and Poesio Inter-Coder Agreeme"
J08-4004,A00-1012,0,0.00727323,"received: 26 August 2005; revised submission received: 21 December 2007; accepted for publication: 28 January 2008. © 2008 Association for Computational Linguistics Computational Linguistics Volume 34, Number 4 were enormously inﬂuential, and K quickly became the de facto standard for measuring agreement in computational linguistics not only in work on discourse (Carletta et al. 1997; Core and Allen 1997; Hearst 1997; Poesio and Vieira 1998; Di Eugenio 2000; Stolcke et al. 2000; Carlson, Marcu, and Okurowski 2003) but also for other annotation tasks (e.g., V´eronis 1998; Bruce and Wiebe 1998; Stevenson and Gaizauskas 2000; Craggs and McGee Wood 2004; Mieskes and Strube 2006). During this period, however, a number of questions have also been raised about K and similar coefﬁcients—some already in Carletta’s own work (Carletta et al. 1997)—ranging from simple questions about the way the coefﬁcient is computed (e.g., whether it is really applicable when more than two coders are used), to debates about which levels of agreement can be considered ‘acceptable’ (Di Eugenio 2000; Craggs and McGee Wood 2005), to the realization that K is not appropriate for all types of agreement (Poesio and Vieira 1998; Marcu, Romera,"
J08-4004,J00-3003,0,0.109726,"Missing"
J08-4004,E99-1015,0,0.0753719,"Missing"
J08-4004,J02-4002,0,0.0397052,"Missing"
J08-4004,M95-1005,0,0.0295577,"ent on such sets, and 8 ftp://ftp.cs.rochester.edu/pub/papers/ai/92.tn1.trains 91 dialogues.txt. 583 Computational Linguistics Volume 34, Number 4 consequently it raises serious questions about weighted measures—in particular, about the interpretability of the results, as we will see shortly. 4.4.1 Passonneau’s Proposal. Passonneau (2004) recommends measuring agreement on anaphoric annotation by using sets of mentions of discourse entities as labels, that is, the emerging anaphoric/coreference chains. This proposal is in line with the methods developed to evaluate anaphora resolution systems (Vilain et al. 1995). But using anaphoric chains as labels would not make unweighted measures such as K a good measure for agreement. Practical experience suggests that, except when a text is very short, few annotators will catch all mentions of a discourse entity: Most will forget to mark a few, with the result that the chains (that is, category labels) differ from coder to coder and agreement as measured with K is always very low. What is needed is a coefﬁcient that also allows for partial disagreement between judgments, when two annotators agree on part of the coreference chain but not on all of it. Passonneau"
J08-4004,J96-3006,0,\N,Missing
J08-4004,brants-plaehn-2000-interactive,0,\N,Missing
J08-4004,sekine-etal-2002-extended,0,\N,Missing
J08-4004,W03-1903,0,\N,Missing
J08-4004,W01-1605,0,\N,Missing
J08-4004,P02-1011,0,\N,Missing
J08-4004,P03-1048,0,\N,Missing
J08-4004,doddington-etal-2004-automatic,0,\N,Missing
J08-4004,buhmann-etal-2002-annotation,0,\N,Missing
J08-4004,wayne-2000-multilingual,0,\N,Missing
J08-4004,J09-4005,0,\N,Missing
J09-1003,P04-1051,1,0.592181,"Missing"
J09-1003,P05-1018,0,0.280506,"ple in Strube and Hahn’s model. In addition to the variability caused by the numerous deﬁnitions of transitions and the introduction of the various principles, parameters such as “utterance,” “ranking,” and “realization” can also be speciﬁed in several ways giving rise to different instantiations of centering (Poesio et al. 2004). The following section discusses how these parameters were deﬁned in the corpora we deploy. 4. Experimental Data We made use of the data of Dimitromanolaki and Androutsopoulos (2003), the GNOME corpus (Poesio et al. 2004), and the two corpora that Barzilay and Lapata (2005) experimented with. In this section, we discuss how the centering representations we utilize were derived from each corpus. 4.1 The MPIRO-CF Corpus Dimitromanolaki and Androutsopoulos (2003, henceforth D&A) derived facts from the database of the MPIRO concept-to-text generation system (Isard et al. 2003), realized them as sentences, and organized them in sets. Each set consisted of six facts which were ordered by a domain expert. The orderings produced by this expert were shown to be very close to those produced by two other archeologists (Karamanis and Mellish 2005b). Our ﬁrst corpus, MPIRO-C"
J09-1003,N04-1015,0,0.208113,"Missing"
J09-1003,P06-1049,0,0.0283821,"Missing"
J09-1003,P87-1022,0,0.502533,"sentences of a comprehensible text. This process very often gives rise to documents that do not make sense although the information content is the same before and after the reordering (Hovy 1988; Marcu 1997; Reiter and Dale 2000). Entity coherence, which is based on the way the referents of noun phrases (NPs) relate subsequent clauses in the text, is an important aspect of textual organization. Since the early 1980s, when it was ﬁrst introduced, centering theory has been an inﬂuential framework for modelling entity coherence. Seminal papers on centering such as Brennan, Friedman [Walker], and Pollard (1987, page 160) and Grosz, Joshi, and Weinstein (1995, page 215) suggest that centering may provide solutions for information ordering. Indeed, following the pioneering work of McKeown (1985), recent work on text generation exploits constraints on entity coherence to organize information (Mellish et al. 1998; Kibble and Power 2000, 2004; O’Donnell et al. 2001; Cheng 2002; Lapata ∗ Computer Laboratory, William Gates Building, Cambridge CB3 0FD, UK. Nikiforos.Karamanis@cl.cam.ac.uk. ∗∗ Department of Computing Science, King’s College, Aberdeen AB24 3UE, UK. † Department of Computer Science, Wivenhoe"
J09-1003,P97-1003,0,0.0149678,"... ... ... ... ... ... (e) [The government]S may ﬁle [a civil suit]O ruling that [conspiracy]S to curb [competition]O through [collusion]X is [a violation]O of [the Sherman Act]X . (f) [Microsoft]S continues to show [increased earnings]O despite [the trial]X . Barzilay and Lapata automatically annotated their corpora for the grammatical function of the NPs in each sentence (denoted in the example by the subscripts S, O, and X for subject, object, and other, respectively) as well as their coreferential relations (which do not include bridging references). More speciﬁcally, they used a parser (Collins 1997) to determine the constituent structure of the sentences from which the grammatical function for each NP was derived.6 Coreferential NPs such as Microsoft Corp. and the company in (3a) were identiﬁed using the system of Ng and Cardie (2002). The entity grid is a two-dimensional array that captures the distribution of NP referents across sentences in the text using the aforementioned symbols for their grammatical role and the symbol “−” for a referent that does not occur in a sentence. Table 4 illustrates a fragment of the grid for the sentences in Example (3).7 Barzilay and Lapata use the grid"
J09-1003,W03-2304,0,0.120101,"sults of our experiments and Section 8 discusses their implications. We conclude the paper with directions for future work and a summary of our main contributions.1 2. Information Ordering Information ordering has been investigated by substantial recent work in text-totext generation (Barzilay, Elhadad, and McKeown 2002; Lapata 2003; Barzilay and Lee 2004; Barzilay and Lapata 2005; Bollegala, Okazaki, and Ishizuka 2006; Ji and Pulman 2006; Siddharthan 2006; Soricut and Marcu 2006; Madnani et al. 2007, among others) as well as concept-to-text generation (particularly Kan and McKeown [2002] and Dimitromanolaki and Androutsopoulos 2003).2 We added to this work by presenting approaches to information ordering based on a genetic algorithm (Karamanis and Manurung 2002) and linear programming (Althaus, Karamanis, and Koller 2004) which can be applied to both concept-to-text and text-to-text generation. These approaches use a metric of coherence deﬁned using features derived from 1 Earlier versions of this work were presented in Karamanis et al. (2004) and Karamanis (2006). 2 Concept-to-text generation is concerned with the automatic generation of text from some underlying non-linguistic representation. By contrast, the input to"
J09-1003,J95-2003,0,0.839789,"Missing"
J09-1003,P88-1020,0,0.534593,"rzilay and Lee 2004), that is, deciding in which sequence to present a set of preselected information-bearing items, has received much attention in recent work in automatic text generation. This is because text generation systems need to organize the content in a way that makes the output text coherent, that is, easy to read and understand. The easiest way to exemplify coherence is by arbitrarily reordering the sentences of a comprehensible text. This process very often gives rise to documents that do not make sense although the information content is the same before and after the reordering (Hovy 1988; Marcu 1997; Reiter and Dale 2000). Entity coherence, which is based on the way the referents of noun phrases (NPs) relate subsequent clauses in the text, is an important aspect of textual organization. Since the early 1980s, when it was ﬁrst introduced, centering theory has been an inﬂuential framework for modelling entity coherence. Seminal papers on centering such as Brennan, Friedman [Walker], and Pollard (1987, page 160) and Grosz, Joshi, and Weinstein (1995, page 215) suggest that centering may provide solutions for information ordering. Indeed, following the pioneering work of McKeown"
J09-1003,W06-1662,0,0.0175532,"arious metrics of coherence suitable for information ordering. Then, Section 6 outlines a corpus-based methodology for choosing among these metrics. Section 7 reports on the results of our experiments and Section 8 discusses their implications. We conclude the paper with directions for future work and a summary of our main contributions.1 2. Information Ordering Information ordering has been investigated by substantial recent work in text-totext generation (Barzilay, Elhadad, and McKeown 2002; Lapata 2003; Barzilay and Lee 2004; Barzilay and Lapata 2005; Bollegala, Okazaki, and Ishizuka 2006; Ji and Pulman 2006; Siddharthan 2006; Soricut and Marcu 2006; Madnani et al. 2007, among others) as well as concept-to-text generation (particularly Kan and McKeown [2002] and Dimitromanolaki and Androutsopoulos 2003).2 We added to this work by presenting approaches to information ordering based on a genetic algorithm (Karamanis and Manurung 2002) and linear programming (Althaus, Karamanis, and Koller 2004) which can be applied to both concept-to-text and text-to-text generation. These approaches use a metric of coherence deﬁned using features derived from 1 Earlier versions of this work were presented in Karam"
J09-1003,W02-2101,0,0.0518339,"Missing"
J09-1003,N06-2017,1,0.782977,"icut and Marcu 2006; Madnani et al. 2007, among others) as well as concept-to-text generation (particularly Kan and McKeown [2002] and Dimitromanolaki and Androutsopoulos 2003).2 We added to this work by presenting approaches to information ordering based on a genetic algorithm (Karamanis and Manurung 2002) and linear programming (Althaus, Karamanis, and Koller 2004) which can be applied to both concept-to-text and text-to-text generation. These approaches use a metric of coherence deﬁned using features derived from 1 Earlier versions of this work were presented in Karamanis et al. (2004) and Karamanis (2006). 2 Concept-to-text generation is concerned with the automatic generation of text from some underlying non-linguistic representation. By contrast, the input to text-to-text generation applications is text. 30 Karamanis et al. Centering for Information Ordering centering and will serve as the premises of our investigation of centering in this article. Metrics of coherence are used in other work on text generation, too (Mellish et al. 1998; Kibble and Power 2000, 2004; Cheng 2002). With the exception of Kibble and Power’s work, the features of entity coherence used in these metrics are informall"
J09-1003,P04-1050,1,0.762185,"2006; Siddharthan 2006; Soricut and Marcu 2006; Madnani et al. 2007, among others) as well as concept-to-text generation (particularly Kan and McKeown [2002] and Dimitromanolaki and Androutsopoulos 2003).2 We added to this work by presenting approaches to information ordering based on a genetic algorithm (Karamanis and Manurung 2002) and linear programming (Althaus, Karamanis, and Koller 2004) which can be applied to both concept-to-text and text-to-text generation. These approaches use a metric of coherence deﬁned using features derived from 1 Earlier versions of this work were presented in Karamanis et al. (2004) and Karamanis (2006). 2 Concept-to-text generation is concerned with the automatic generation of text from some underlying non-linguistic representation. By contrast, the input to text-to-text generation applications is text. 30 Karamanis et al. Centering for Information Ordering centering and will serve as the premises of our investigation of centering in this article. Metrics of coherence are used in other work on text generation, too (Mellish et al. 1998; Kibble and Power 2000, 2004; Cheng 2002). With the exception of Kibble and Power’s work, the features of entity coherence used in these"
J09-1003,P13-2092,0,0.0541466,"Missing"
J09-1003,W02-2111,1,0.916787,"ain contributions.1 2. Information Ordering Information ordering has been investigated by substantial recent work in text-totext generation (Barzilay, Elhadad, and McKeown 2002; Lapata 2003; Barzilay and Lee 2004; Barzilay and Lapata 2005; Bollegala, Okazaki, and Ishizuka 2006; Ji and Pulman 2006; Siddharthan 2006; Soricut and Marcu 2006; Madnani et al. 2007, among others) as well as concept-to-text generation (particularly Kan and McKeown [2002] and Dimitromanolaki and Androutsopoulos 2003).2 We added to this work by presenting approaches to information ordering based on a genetic algorithm (Karamanis and Manurung 2002) and linear programming (Althaus, Karamanis, and Koller 2004) which can be applied to both concept-to-text and text-to-text generation. These approaches use a metric of coherence deﬁned using features derived from 1 Earlier versions of this work were presented in Karamanis et al. (2004) and Karamanis (2006). 2 Concept-to-text generation is concerned with the automatic generation of text from some underlying non-linguistic representation. By contrast, the input to text-to-text generation applications is text. 30 Karamanis et al. Centering for Information Ordering centering and will serve as the"
J09-1003,W05-1621,1,0.89359,"), and the two corpora that Barzilay and Lapata (2005) experimented with. In this section, we discuss how the centering representations we utilize were derived from each corpus. 4.1 The MPIRO-CF Corpus Dimitromanolaki and Androutsopoulos (2003, henceforth D&A) derived facts from the database of the MPIRO concept-to-text generation system (Isard et al. 2003), realized them as sentences, and organized them in sets. Each set consisted of six facts which were ordered by a domain expert. The orderings produced by this expert were shown to be very close to those produced by two other archeologists (Karamanis and Mellish 2005b). Our ﬁrst corpus, MPIRO-CF, consists of 122 orderings that were made available to us by D&A. We computed a CF list for each fact in each ordering by applying the instantiation of centering introduced by Kibble and Power (2000, 2004) for concept-totext generation. That is, we took each database fact to correspond to an “utterance” and speciﬁed the “realization” parameter using the arguments of each fact as the members of the corresponding CF list. Table 2 shows the CF lists, the CBs, the centering transitions, and the violations of CHEAPNESS for the following example from MPIRO-CF: (1) (a) T"
J09-1003,J01-4007,0,0.333116,"that CF(Un ) should contain at least one member of CF(Un−1 ). This became known as the principle of CONTINUITY (Karamanis and Manurung 2002). Although Grosz, Joshi, and Weinstein and Brennan, Friedman [Walker], and Pollard do not discuss the effect of violating CONTINUITY, Kibble and Power (2000, Figure 1) deﬁne the additional transition NOCB to account for this case. Different types of NOCB transitions are introduced by Passoneau (1998) and Poesio et al. (2004), among others. Other researchers, however, consider the NOCB transition to be a type of ROUGH - SHIFT (Miltsakaki and Kukich 2004). Kibble (2001) and Beaver (2004) introduced the principles of COHERENCE and SALIENCE , which correspond to the identity checks used to deﬁne the transitions (see Table 1). To improve the way centering resolves pronominal anaphora, Strube and Hahn (1999) introduced a fourth principle called CHEAPNESS and deﬁned it as CB(Un )=CP(Un−1 ). They also redeﬁned Rule 2 to favor transition pairs which satisfy 3 “CB(Un−1 ) undef.” in Table 1 stands for the cases where Un−1 does not have a CB. Instead of classifying the transition of Un as a CONTINUE or a RETAIN in such cases, the additional transition ESTABLISHMENT is"
J09-1003,W00-1411,0,0.0673401,"Missing"
J09-1003,J04-4001,0,0.642398,"to have the same number of CONTINUEs, the sum of RETAINs is examined, and so forth for the other two types of centering transitions.12 M.KP, the metric deployed by Kibble and Power (2000) in their text generation system, sums up the NOCBs as well as the violations of CHEAPNESS, COHERENCE, and SALIENCE, preferring the ordering with the lowest total cost. In addition to the violations of CONTINUITY and CHEAPNESS, the candidate ordering also violates SALIENCE once, so its score according to M.KP is 5. An alternative ordering with a lower score (if any) will be preferred by this metric. Although Kibble and Power (2004) introduced a weighted version of M.KP, the exact weighting of centering’s principles remains an open question, as argued by Kibble (2001). This is why we decided to experiment with M.KP instead of its weighted variant. In the remainder of the paper, we take forward the four metrics motivated in this section as the most appropriate starting point for experimentation. We would like to emphasize, however, that these are not the only possible options. Indeed, similarly to the various ways in which centering’s parameters can be speciﬁed, there exist many other ways of using centering to deﬁne metr"
J09-1003,P03-1069,0,0.187334,"ering data structures from existing corpora. Section 5 discusses how centering can be used to deﬁne various metrics of coherence suitable for information ordering. Then, Section 6 outlines a corpus-based methodology for choosing among these metrics. Section 7 reports on the results of our experiments and Section 8 discusses their implications. We conclude the paper with directions for future work and a summary of our main contributions.1 2. Information Ordering Information ordering has been investigated by substantial recent work in text-totext generation (Barzilay, Elhadad, and McKeown 2002; Lapata 2003; Barzilay and Lee 2004; Barzilay and Lapata 2005; Bollegala, Okazaki, and Ishizuka 2006; Ji and Pulman 2006; Siddharthan 2006; Soricut and Marcu 2006; Madnani et al. 2007, among others) as well as concept-to-text generation (particularly Kan and McKeown [2002] and Dimitromanolaki and Androutsopoulos 2003).2 We added to this work by presenting approaches to information ordering based on a genetic algorithm (Karamanis and Manurung 2002) and linear programming (Althaus, Karamanis, and Koller 2004) which can be applied to both concept-to-text and text-to-text generation. These approaches use a me"
J09-1003,J06-4002,0,0.112028,"easible, computational corpus-based experiments are 11 In order to estimate the effect of CHEAPNESS only, NOCBs are not counted as violations of CHEAPNESS. 12 Following Brennan, Friedman [Walker], and Pollard (1987), NOCBs are not taken into account for the deﬁnition of transitions in M.BFP. 38 Karamanis et al. Centering for Information Ordering often the most viable alternative (Poesio et al. 2004; Barzilay and Lee 2004). Corpusbased evaluation can be usefully employed during system development and may be later supplemented by less extended evaluation based on human judgments as suggested by Lapata (2006). The corpus-based methodology of Karamanis (2003) served as our experimental framework. This methodology is based on the premise that the original sentence order (OSO, Barzilay and Lee 2004) observed in a corpus text is more coherent than any other ordering. If a metric takes an alternative ordering to be more coherent than the OSO, it has to be penalized. Karamanis (2003) introduced a performance measure called the classiﬁcation error rate which is computed according to the formula: Better(M,OSO)+Equal(M,OSO)/2. Better(M,OSO) stands for the percentage of orderings that score better than the"
J09-1003,W07-2312,0,0.0225034,"Missing"
J09-1003,W98-1411,1,0.754978,"mplex interactions between different types of coherence which leave many other equally plausible combinations unexplored. Clearly, one would like to know what centering can achieve on its own before devising more complicated metrics. To address this question, we deﬁne metrics which are purely centering-based, placing any attempt to specify a more elaborate model of coherence beyond the scope of this article. This strategy is similar to most work on centering for text interpretation in which additional constraints on coherence are not taken into account (the papers in Walker, Joshi, and Prince [1998] are characteristic examples). This simpliﬁcation makes it possible to assess for the ﬁrst time how useful the employed centering features are for information ordering. Work on text generation which is solely based on rhetorical relations (Hovy 1988; Marcu 1997, among others) typically masks entity coherence under the ELABORATION relation. However, ELABORATION has been characterized as “the weakest of all rhetorical relations” (Scott and de Souza 1990, page 60). Knott et al. (2001) identiﬁed several theoretical problems all related to ELABORATION and suggested that this relation be replaced by"
J09-1003,P02-1014,0,0.0251601,"Missing"
J09-1003,J04-3003,1,0.272865,"Missing"
J09-1003,P06-2103,0,0.161845,"Missing"
J09-1003,J99-3001,0,0.154073,"discuss the effect of violating CONTINUITY, Kibble and Power (2000, Figure 1) deﬁne the additional transition NOCB to account for this case. Different types of NOCB transitions are introduced by Passoneau (1998) and Poesio et al. (2004), among others. Other researchers, however, consider the NOCB transition to be a type of ROUGH - SHIFT (Miltsakaki and Kukich 2004). Kibble (2001) and Beaver (2004) introduced the principles of COHERENCE and SALIENCE , which correspond to the identity checks used to deﬁne the transitions (see Table 1). To improve the way centering resolves pronominal anaphora, Strube and Hahn (1999) introduced a fourth principle called CHEAPNESS and deﬁned it as CB(Un )=CP(Un−1 ). They also redeﬁned Rule 2 to favor transition pairs which satisfy 3 “CB(Un−1 ) undef.” in Table 1 stands for the cases where Un−1 does not have a CB. Instead of classifying the transition of Un as a CONTINUE or a RETAIN in such cases, the additional transition ESTABLISHMENT is sometimes used (Kameyama 1998; Poesio et al. 2004). 32 Karamanis et al. Centering for Information Ordering CHEAPNESS over those which violate it. This means that CHEAPNESS is given priority over every other centering principle in Strube a"
J09-4002,C02-1140,0,0.0651193,"Missing"
J09-4002,C02-1059,0,0.0606955,"Missing"
J09-4002,1997.iwpt-1.16,0,0.101538,"y of Manchester, UK Hozumi Tanaka—or Tanaka-sensei as he was fondly known to his colleagues and students in Japanese—passed away at the age of 67 in the early morning of 27 July 2009. He is survived by his wife Reiko and two sons. Tanaka-sensei’s primary contributions to natural language processing (NLP) are in parsing and semantic analysis. In parsing, he extended the GLR parsing algorithm to incorporate probabilities, multiple connection tables, and simultaneously carry out morphological and syntactic analysis for non-segmenting languages such as Japanese (Tanaka, Tokunaga, and Aizawa 1993; Inui et al. 1997; Shirai et al. 2000). His research on semantic analysis covered a broad spectrum, encompassing word sense disambiguation (Fujii et al. 1998), spoken language understanding for virtual agent systems (Shinyama, Tokunaga, and Tanaka 2000), lexical semantic approaches to query expansion in information retrieval (Mandala, Tokunaga, and Tanaka 2000), and metaphor processing (Iwayama, Tokunaga, and Tanaka 1990). He also carried out research on machine translation (Tanaka, Isahara, and Yasuhara 1983; Tanaka 1999b; Baldwin and Tanaka 2000), computer-assisted language learning (Bilac, Baldwin, and Tana"
J09-4002,1999.mtsummit-1.1,0,0.0366081,"is for non-segmenting languages such as Japanese (Tanaka, Tokunaga, and Aizawa 1993; Inui et al. 1997; Shirai et al. 2000). His research on semantic analysis covered a broad spectrum, encompassing word sense disambiguation (Fujii et al. 1998), spoken language understanding for virtual agent systems (Shinyama, Tokunaga, and Tanaka 2000), lexical semantic approaches to query expansion in information retrieval (Mandala, Tokunaga, and Tanaka 2000), and metaphor processing (Iwayama, Tokunaga, and Tanaka 1990). He also carried out research on machine translation (Tanaka, Isahara, and Yasuhara 1983; Tanaka 1999b; Baldwin and Tanaka 2000), computer-assisted language learning (Bilac, Baldwin, and Tanaka 2002), speech recognition (Itou, Hayamizu, and Tanaka 1992; Li, Tanaka, and Tokunaga 1995), dialogue systems (Akiba and Tanaka 1994; Funakoshi, Tokunaga, and Tanaka 2002), and automatic music generation (Suzuki, Tokunaga, and Tanaka 1999). He was the author or editor of a number of popular introductory texts on NLP in Japanese (Tanaka 1989, 1999a). Tanaka-sensei was the technical lead on the Japanese government-funded CICC Machine Translation Project (1987–1995) between East and South-East Asian langua"
J09-4002,1993.iwpt-1.10,0,0.393005,"Missing"
J96-1007,P92-1005,0,0.0265629,"nkal also provides in his book a theory of ambiguity, a property of natural language utterances that is the source of endless trouble for computational linguists. Ambiguity--syntactic ambiguity and lexical ambiguity in particular--is a central concern both in computational linguistics and in psycholinguistics, but hasn&apos;t been a central area of research in linguistics, and in particular not in formal semantics. However, the interest in theories of semantic-ambiguity processing has been raised in recent years by the development of theories of semantic interpretation based on underspecification (Alshawi and Crouch 1992; Reyle 1993; Poesio 1995; van Deemter and Peters 1995) as well as by work on the lexicon (Copestake and Briscoe 1995 and Saint-Dizier and Viegas 1995 for example). Pinkal&apos;s proposals concerning ambiguity will be of great interest to those working in these areas. 140 Book Reviews 2. The content of the book Logic and Lexicon consists of three parts. The first part does not require much prior technical knowledge, and is recommended to those readers who have no formal propensities or only want to learn about Pinkal&apos;s main ideas. The range of phenomena to be discussed in the book is outlined in ch"
J98-2001,J96-2004,0,0.679644,"ch took into account our intention of having naive speakers p e r f o r m the classification. Our experiments were also designed to assess the feasibility of a system to process definite descriptions on unrestricted text and to collect data that could be used for this implementation. For both of these reasons, the classification schemes that we tried differ in several respects from those a d o p t e d in prior corpus-based studies such as Prince (1981) and Fraurud (1990). Our study is also different from these previous ones in that measuring the agreement a m o n g annotators became an issue (Carletta 1996). For the experiments, we used a set of r a n d o m l y selected articles from the Wall Street Journal contained in the A C L / D C I CD-ROM, rather than a corpus of transcripts of spoken language corpora such as the HCRC MapTask corpus (Anderson et al. 1991) or the TRAINS corpus ( H e e m a n and Allen 1995). The main reason for this choice was 2 We will not be concerned with other cases of definite noun phrases such as pronouns, or possessive descriptions; hence the term definite description rather than the more general term definite NP. 3 The word the is by far the most common word in the B"
J98-2001,J97-1002,0,0.0489454,"Missing"
J98-2001,J86-3001,0,0.144008,"of definite description interpretation should include methods for recognizing such definites. The architecture of our own classifier (see below) is also consistent with Fraurud's hypothesis that these methods are not just used when no suitable antecedent can be found, but more extensive investigations will be needed before we can conclude that this architecture significantly outperforms other ones. The presence of such a large number of discourse-new definite descriptions is also problematic for the idea that definite descriptions are interpreted with respect to the global focus (Grosz 1977; Grosz and Sidner 1986). A significant percentage of the larger situation definite descriptions encountered in our corpus cannot be said to be in the globai focus in any significant sense: as we observed above, in many of these cases the writer seems to rely on the reader's capability to add a new object such as the Illinois Commerce Commission to her or his model of the world, rather than expecting that object to be already present. 5.2 A (Semi)Automatic Classifier As already mentioned, we are in the course of implementing a system capable of performing the classification task semiautomatically (Vieira 1998). This"
J98-2001,T87-1035,0,0.134807,"scuss our two classification experiments in Sections 3 and 4. 2. Towards a Classification Scheme: Linguistic Theories of Definite Descriptions When looking for an annotation scheme for definite descriptions, one is faced with a wide range of options. At one end of the spectrum there are mostly descriptive lists of definite description uses, such as those in Christophersen (1939) and Hawkins (1978), whose only goal is to assign a classification to all uses of definite descriptions. At the other end, there are highly developed formal analyses, such as Russell (1905), Heim (1982), L6bner (1985), Kadmon (1987), Neale (1990), Barker (1991), and Kamp and Reyle (1993), in which the compositional contribution of definite descriptions to the meaning of an utterance, as well as their truth-conditional properties, are spelled out in detail. These more formal analyses are concerned with questions such as the quantificational or nonquantificational status of definite descriptions and the proper treatment of presuppositions, but tend to concentrate on a subset of the full range of definite description use. Among the more developed semantic analyses, some identify uniqueness as the defining property of defini"
J98-2001,J93-2004,0,0.0304668,"Missing"
J98-2001,W97-1301,1,0.503317,"Missing"
L16-1131,W97-0702,0,0.291402,"→ C2 : Patronising and cynical comment by the Government. I daresay we can ‘adapt’ to a certain extent but there are limits. In example 1, the first comment (C1 ) links to article sentence SA through ‘human adaptability’ and it expresses a view against the quote given in SA and then the second comment (C2 ) seconds the viewpoint of C1 (it is actually a reply to C1 ). Such clusters of linked sentences are not summaries in themselves, but can be seen as digests of the mass of comments and key points covered in news articles (to an extent resembling the idea of ‘capsule overview’ put forward in (Boguraev and Kennedy, 1997)). The argument labels are: in favour, against, neutral and not applicable. The choice of modelling argument structure with a closed set of labels is a rather pragmatic choice driven, firstly, by the need to capture both argument structure and sentiment whilst modelling these in an integrated manner12 and, secondly, by the objective to define a feasible shared task cast as a classification problem that can be tackled with standard machine learning algorithms. 3.3. Crowdsourcing Validation Adopting a more pragmatic view on argument structure also has the advantage that it is suitable for annota"
L16-1131,D09-1030,0,0.096477,"Missing"
L16-1131,P08-1081,0,0.0222901,"annotation breadth, we believe the corpus will prove a useful resource in stimulating and furthering research in the areas of Argumentation Mining, Summarisation, Sentiment, Coreference and the interlinks therein. Keywords: Online Forums, Summarization, Argument Structure, Sentiment Analysis 1. Introduction Internet or online forums are discussion websites where people can hold asynchronous conversations in the form of posted messages. Much work has been devoted in recent years on mining and analysing online forums – as in search (Bhatia and Mitra, 2010; Seo et al., 2009), question answering (Ding et al., 2008; Hong and Davison, 2009), classification of argumentative propositions (Park and Cardie, 2014) and automatic summarisation (Giannakopoulos et al., 2015) – and such work, to a large extent is fuelled by the creation and public release of online forums corpora of various kinds (e.g., (Wang et al., 2011) and the boards.ie Forums Dataset as part of the ICWSM 2012 conference1 ). One type of online forums which is increasingly popular is that of readers’ discussions taking place on news publishers sites, such as The Guardian2 or Le Monde3 . There is strong interest in such forums, their mining and"
L16-1131,W15-4638,1,0.845174,"ning, Summarisation, Sentiment, Coreference and the interlinks therein. Keywords: Online Forums, Summarization, Argument Structure, Sentiment Analysis 1. Introduction Internet or online forums are discussion websites where people can hold asynchronous conversations in the form of posted messages. Much work has been devoted in recent years on mining and analysing online forums – as in search (Bhatia and Mitra, 2010; Seo et al., 2009), question answering (Ding et al., 2008; Hong and Davison, 2009), classification of argumentative propositions (Park and Cardie, 2014) and automatic summarisation (Giannakopoulos et al., 2015) – and such work, to a large extent is fuelled by the creation and public release of online forums corpora of various kinds (e.g., (Wang et al., 2011) and the boards.ie Forums Dataset as part of the ICWSM 2012 conference1 ). One type of online forums which is increasingly popular is that of readers’ discussions taking place on news publishers sites, such as The Guardian2 or Le Monde3 . There is strong interest in such forums, their mining and analysis, by a broad range of information seekers, as are journalists, news editors and trend and media monitors. Yet, to our knowledge currently there i"
L16-1131,W14-2105,0,0.0282549,"furthering research in the areas of Argumentation Mining, Summarisation, Sentiment, Coreference and the interlinks therein. Keywords: Online Forums, Summarization, Argument Structure, Sentiment Analysis 1. Introduction Internet or online forums are discussion websites where people can hold asynchronous conversations in the form of posted messages. Much work has been devoted in recent years on mining and analysing online forums – as in search (Bhatia and Mitra, 2010; Seo et al., 2009), question answering (Ding et al., 2008; Hong and Davison, 2009), classification of argumentative propositions (Park and Cardie, 2014) and automatic summarisation (Giannakopoulos et al., 2015) – and such work, to a large extent is fuelled by the creation and public release of online forums corpora of various kinds (e.g., (Wang et al., 2011) and the boards.ie Forums Dataset as part of the ICWSM 2012 conference1 ). One type of online forums which is increasingly popular is that of readers’ discussions taking place on news publishers sites, such as The Guardian2 or Le Monde3 . There is strong interest in such forums, their mining and analysis, by a broad range of information seekers, as are journalists, news editors and trend a"
L16-1131,W13-2323,0,0.0150836,". Links validated (via crowdsourcing) All Links Unique Links and Labels Unique Links only Type d Links Type c Links Type b Links Type a Links English 2311 9635 6576 5789 3517 2975 63 21 Italian 1087 6193 4138 4016 2083 2024 20 11 Table 3: OnForumS corpus: coreference statistics (TBA: to be annotated). Number of markables Number of coreference chains English 14378 1463 Italian TBA TBA Figure 1: Validation HIT on CrowdFlower. tion or validation of automatic output using crowdsourcing13 , which is a commonly used method for evaluating Human Language Technology (HLT) systems (CallisonBurch, 2009; Passonneau and Carpenter, 2013). Thus, the crowdsourcing Human Intelligence Task (HIT) was designed as a validation task (as opposed to annotation), where each system-proposed link and labels are presented to a human contributor for their validation with both article sentence and comment sentence placed within context (see Fig. 1). Both the HIT and the instructions for contributors were translated to English and Italian, thus targeting two distinct groups of native speakers. Participation in the crowdsourcing HIT varied between 20−40 contributors approximately. A sample snapshot of a finished project from CrowdFlower can be"
L16-1131,poesio-artstein-2008-anaphoric,1,0.788734,"dations as the gold labels for those links. And the other way is by exclusion, if all possible labels for a given link except for one have a ‘no’ validation then this makes the remaining label a gold label (e.g., if it is not “against”, nor “impartial”, then it is “in favour”). With these criteria in mind we created a small gold standard set. 4. 4.1. Other Annotations Coreference Annotation The annotation scheme used to annotate for coreference the OnForumS corpus is a variant of the LiveMemories annotation scheme (Rodriguez et al., 2010) which in turn is based on the ARRAU annotation scheme (Poesio and Artstein, 2008). In this corpus all noun phrases are taken as mentions, and the whole noun phrase is considered (with all its embedded NPs). All anaphoric relations of identity between any pairs of mentions are annotated. Coordinations are also treated as mentions, and annotated. Key coreference statistics are shown in Table 3. 4.2. Sentiment Annotation As mentioned earlier, the sentiment annotation parallels that of the argument structure annotation and for each comment-article link systems participating in the OnForumS task were supposed to produce a closed set of sentiment labels. These set of labels are:"
L16-1131,rodriguez-etal-2010-anaphoric,1,0.822473,"ks as gold links and then all labels for argument and sentiment with ‘yes’ validations as the gold labels for those links. And the other way is by exclusion, if all possible labels for a given link except for one have a ‘no’ validation then this makes the remaining label a gold label (e.g., if it is not “against”, nor “impartial”, then it is “in favour”). With these criteria in mind we created a small gold standard set. 4. 4.1. Other Annotations Coreference Annotation The annotation scheme used to annotate for coreference the OnForumS corpus is a variant of the LiveMemories annotation scheme (Rodriguez et al., 2010) which in turn is based on the ARRAU annotation scheme (Poesio and Artstein, 2008). In this corpus all noun phrases are taken as mentions, and the whole noun phrase is considered (with all its embedded NPs). All anaphoric relations of identity between any pairs of mentions are annotated. Coordinations are also treated as mentions, and annotated. Key coreference statistics are shown in Table 3. 4.2. Sentiment Annotation As mentioned earlier, the sentiment annotation parallels that of the argument structure annotation and for each comment-article link systems participating in the OnForumS task w"
L16-1131,D14-1006,0,0.0296316,"ce-split (see http: //multiling.iit.demokritos.gr/pages/view/ 1531/task-onforums-data-and-information). associated comments and are expected to link each comment sentence to article sentences (which, for simplification, are assumed to be the appropriate units here) or to preceding comments and then to label each link for argument structure in f avour, against, impartial and sentiment positive, negative, neutral (for more details see (Kabadjov et al., 2015)). 3.2. OnForumS’ view on Argument Structure Identifying argument structure is currently an active area of research (Palau and Moens, 2011; Stab and Gurevych, 2014). In the context of the OnForumS task, the view of argument structure we adopted was that of articulating a closed set of argument labels for the linking of sentence pairs from readers comments and news articles. On one hand, linking comment sentences to article sentences is a useful step towards summarising the mass of comments. For instance, comment sentences linked to the same article sentence can be seen as forming a “cluster” of sentences on a specific point or topic. On the other hand, having labels capturing argument structure enables computing statistics within such topic clusters on h"
L16-1323,J08-4004,1,0.811899,"Missing"
L16-1323,W09-3309,1,0.877574,"Missing"
L16-1323,W10-1837,0,0.0175888,"2006). Since then GWAPs have been developed for numerous tasks, including image and video annotation, natural language processing, biomedical research and search refinement (Chamberlain et al., 2013). Several GWAPs have attempted anaphoric coreference including PlayCoref, a two-player game in which players mark coreferential pairs between words in a text (Hladk´a et al., 2009), and PhraTris, a GWAP for syntactic annotation using a general-purpose development platform called GALOAP (Attardi and the Galoap Team, 2010).1 PackPlay was another attempt to build semantically-rich annotated corpora (Green et al., 2010). The two game variants Entity Discovery and Name That Entity use slightly different approaches in multi-player games to elicit annotations from players. A more unified attempt at creating a gaming platform, named Wordrobe2 , targeted different linguistic tasks including part-of-speech tagging, named entity tagging, coreference resolution, word sense disambiguation and compound relations (Venhuizen et al., 2013). More recently, GWAPs integrated into social networking sites such as Sentiment Quiz (Rafelsberger and Scharl, 2009) and TypeAttack (Jovian and Amprimo, 2011) on Facebook show that soc"
L16-1323,P09-2053,0,0.0496594,"Missing"
L16-1323,P06-1055,0,0.149188,"Missing"
L16-1323,poesio-artstein-2008-anaphoric,1,0.698043,"correlation). 5. 5.1. Descriptive analysis of the corpora The corpora were analysed for syntactic and structural differences (see Table 2): 11 total syllables total words 206.835 − 1.015 total sentences − 84.6 total words http://opennlp.apache.org No markables were actually deleted, to ensure database integrity they were instead flagged to be ignored by system outputs. Annotations Coding scheme The corpus was annotated according to the linguisticallyoriented approach to anaphoric annotation that is currently prevalent, having been adopted in OntoNotes (Pradhan et al., 2007), the ARRAU corpus (Poesio and Artstein, 2008) 12 2041 13 https://readability-score.com and in all the corpora used in the 2010 SEMEVAL anaphora evaluation (Recasens et al., 2010). Markables can be assigned four types of interpretation: • DN (discourse-new): this markable refers to a newly introduced entity; • DO (discourse-old): this markable refers to an entity already mentioned in the text; • NR (non-referring): this markable does not refer to anything (e.g. pleonastic it); • PR (property attribute): this markable represents a property of a previously mentioned entity (e.g. a teacher in ‘He is a teacher’). Annotations can be examined a"
L16-1323,W04-0210,1,0.622038,"8 9 • the annotations produced by the players including the user ID, the user rating, the time it took to make the decision, whether the decision is an agreement, in what mode the decision occurred (annotation or validation), timestamp, and the interface that was used; • any player comments about the markable; Export format • and any time a player skipped the markable. The PD-MAS-XML format used to export Phrase Detectives data is a modified version of the Minimum Anaphoric Syntax (MAS-XML) format, a form of inline XML in which the basic information required to carry out resolution is marked (Poesio, 2004b). As an example, the representation in MAS-XML of the noun phrase four little rabbits is as follows: 1 Figure 3: Screenshot showing the expert annotation administration interface. &lt;ne id=&quot;ne4&quot; AAcat=&quot;num-np&quot; AAgen=&quot;neut&quot; AAnum=&quot;plur&quot; AAper=&quot;per3&quot;&gt; &lt;mod id=&quot;AAm2&quot; AAcat=&quot;AApre&quot;&gt; &lt;W Lpos=&quot;CD&quot;&gt;four&lt;/W&gt; &lt;W Lpos=&quot;JJ&quot;&gt;little&lt;/W&gt; &lt;/mod&gt; &lt;nphead id=&quot;AAh4&quot;&gt; &lt;W Lpos=&quot;NNS&quot;&gt;rabbits&lt;/W&gt; &lt;/nphead&gt; &lt;/ne&gt; The following is a simplified example of how player annotations are appended to the MAS-XML to create PD-MASXML: 1 2 3 4 5 6 7 PD-MAS-XML allows all interpretations for the markables to be stored, leaving i"
L16-1323,W09-2411,0,0.0638734,"Missing"
L16-1323,D08-1027,0,0.723495,"Missing"
L16-1323,W13-0215,0,0.0201816,"actic annotation using a general-purpose development platform called GALOAP (Attardi and the Galoap Team, 2010).1 PackPlay was another attempt to build semantically-rich annotated corpora (Green et al., 2010). The two game variants Entity Discovery and Name That Entity use slightly different approaches in multi-player games to elicit annotations from players. A more unified attempt at creating a gaming platform, named Wordrobe2 , targeted different linguistic tasks including part-of-speech tagging, named entity tagging, coreference resolution, word sense disambiguation and compound relations (Venhuizen et al., 2013). More recently, GWAPs integrated into social networking sites such as Sentiment Quiz (Rafelsberger and Scharl, 2009) and TypeAttack (Jovian and Amprimo, 2011) on Facebook show that social interaction within a game environment also motivates players to participate in corpus an2039 1 2 http://galoap.codeplex.com http://www.wordrobe.org ID GN W2 G2 W1 G1 Gold Standard Consensus+1 2 experts 2 experts 1 expert 1 expert D 5 5 1 30 4 45 W 874 495 180 12,106 6,231 19,886 M 274 185 69 3,953 1,971 6,452 Table 1: Summary of corpora from Phrase Detectives Corpus 1.0 showing total documents (D), total wor"
L16-1326,doddington-etal-2004-automatic,0,0.117394,"e can not participate in a coreference chain. In a few cases, these constraints revealed intriguing cases of anaphoric expressions. Mostly, however, they have helped us identify and eliminate clear annotation errors. We will provide more details on our approach in Section 4. below. 3. ARRAU and other coreferentially annotated corpora The ARRAU guidelines focus on more detailed representation of linguistic phenomena related to anaphora and coreference. In this section, we highlight the main differences between ARRAU and two other commonly used corpora annotated for coreference in English, ACE (Doddington et al., 2004) and OntoNotes (Pradhan et al., 2011; Pradhan et al., 2012). Table 2 provides a summary of the most distinctive features of ARRAU as opposed to ACE and OntoNotes. The most prominent feature of ARRAU is its rich linguistically motivated annotation of markables. To start with, each nominal markables is shown with its minimal and maximal span. This solution is in line with the ACE annotation guidelines and has unfortunately been discarded for the OntoNotes dataset in order to decrease the annotation price and thus augment the corpus size. The maximal span corresponds to the full noun phrase, wher"
L16-1326,W11-1916,0,0.0174322,"nts with head-finding rules, one might expect to extract the minimal span for each NP rather reliably. It has been shown, however, that naive parsing-based heuristics do not lead to the best performance and a coreference resolver might benefit considerably from explicit or latent identification of minimal spans or heads (Zhekova and K¨ubler, 2013; Peng et al., 2015). Moreover, explicitly annotated minimal spans allow for better lenient matching that has been shown to improve the training procedure of coreference resolvers through better alignment of automatically extracted and gold markables (Kummerfeld et al., 2011). We believe therefore that the combination of minimal and maximal spans is the most reliable way of annotating markable boundaries for coreference. In the second release of ARRAU, we provide minimal and maximal spans for all the domains. In ARRAU, we focus on different types of noun phrases. In particular, we label markables that do not participate in coreference chains: singletons and non-referentials. The ACE guidelines restrict the annotation scope to referentials1 , whereas OntoNotes only marks co-referential (no singletons) markables. As Table 3 shows, non-referentials and singletons acc"
L16-1326,K15-1002,0,0.0525315,"rresponds to the head noun or to the bare named entity for complex NE-nominals. With the latest development in the parsing technology, it might seem redundant to include minimal spans in the manual annotation directly: using dependencies or constituents with head-finding rules, one might expect to extract the minimal span for each NP rather reliably. It has been shown, however, that naive parsing-based heuristics do not lead to the best performance and a coreference resolver might benefit considerably from explicit or latent identification of minimal spans or heads (Zhekova and K¨ubler, 2013; Peng et al., 2015). Moreover, explicitly annotated minimal spans allow for better lenient matching that has been shown to improve the training procedure of coreference resolvers through better alignment of automatically extracted and gold markables (Kummerfeld et al., 2011). We believe therefore that the combination of minimal and maximal spans is the most reliable way of annotating markable boundaries for coreference. In the second release of ARRAU, we provide minimal and maximal spans for all the domains. In ARRAU, we focus on different types of noun phrases. In particular, we label markables that do not part"
L16-1326,poesio-artstein-2008-anaphoric,1,0.83521,"task achieve robust performance on relatively easy cases of coreference, especially since the vast model optimization efforts have been undertaken by various research groups for the recent SemEval and CoNLL coreference resolution tracks (Recasens et al., 2010; Pradhan et al., 2011; Pradhan et al., 2012). More complex cases have been identified and investigated since the first years of research on coreference resolution, however, they have been out of the scope of the mainstream community till very recently. One of the main reasons is the lack of appropriate datasets. Since its first release (Poesio and Artstein, 2008), the ARRAU corpus has been used, on one hand, for research on more complex coreference phenomena, and, on the other hand, as a reference point for annotating coreference corpora in other languages. The current paper presents the second release of ARRAU. For the second release we have not only focused on augmenting the number of covered documents, but also invested a considerable effort into improving the data quality. This involved annotating more attributes and designing a methodology for cleaning up the annotations. The former allows to use ARRAU for a variety of coreference-related problem"
L16-1326,W11-1901,0,0.116866,"ywords: Discourse, Anaphora, Coreference 1. Introduction Coreference resolution is a crucial step in deep text understanding and as such is a vital prerequisite for a variety of high-level natural processing tasks, ranging from information extraction to summarization or machine translation. State-of-the-art statistical approaches to the task achieve robust performance on relatively easy cases of coreference, especially since the vast model optimization efforts have been undertaken by various research groups for the recent SemEval and CoNLL coreference resolution tracks (Recasens et al., 2010; Pradhan et al., 2011; Pradhan et al., 2012). More complex cases have been identified and investigated since the first years of research on coreference resolution, however, they have been out of the scope of the mainstream community till very recently. One of the main reasons is the lack of appropriate datasets. Since its first release (Poesio and Artstein, 2008), the ARRAU corpus has been used, on one hand, for research on more complex coreference phenomena, and, on the other hand, as a reference point for annotating coreference corpora in other languages. The current paper presents the second release of ARRAU. F"
L16-1326,W12-4501,1,0.90767,"phora, Coreference 1. Introduction Coreference resolution is a crucial step in deep text understanding and as such is a vital prerequisite for a variety of high-level natural processing tasks, ranging from information extraction to summarization or machine translation. State-of-the-art statistical approaches to the task achieve robust performance on relatively easy cases of coreference, especially since the vast model optimization efforts have been undertaken by various research groups for the recent SemEval and CoNLL coreference resolution tracks (Recasens et al., 2010; Pradhan et al., 2011; Pradhan et al., 2012). More complex cases have been identified and investigated since the first years of research on coreference resolution, however, they have been out of the scope of the mainstream community till very recently. One of the main reasons is the lack of appropriate datasets. Since its first release (Poesio and Artstein, 2008), the ARRAU corpus has been used, on one hand, for research on more complex coreference phenomena, and, on the other hand, as a reference point for annotating coreference corpora in other languages. The current paper presents the second release of ARRAU. For the second release w"
L16-1326,S10-1001,1,0.894033,"Missing"
L16-1326,R13-1097,0,0.0359572,"Missing"
N01-1002,W98-0607,1,\N,Missing
N01-1002,P00-1019,0,\N,Missing
N01-1002,W00-1425,1,\N,Missing
N01-1002,W00-1415,1,\N,Missing
N01-1002,J86-3001,0,\N,Missing
N01-1002,poesio-2000-annotating,1,\N,Missing
N19-1176,P14-1005,0,0.251147,"Missing"
N19-1176,D18-1016,0,0.278263,"ncluding training on singletons and non-referring expressions. The annotation model can also result in more than one label, or no label, being proposed for a markable, thus serving as a baseline method for automatically identifying ambiguous markables. A preliminary analysis of the results is presented. 1 Introduction A number of datasets for anaphora resolution / coreference now exist (Poesio et al., 2016), including ONTONOTES that has been the de facto standard since the CONLL shared tasks in 2011 and 2012 (Pradhan et al., 2012), and the just introduced and very substantial P RE C O corpus (Chen et al., 2018). None of these datasets however take into account the research challenging the idea that a ‘gold standard’ interpretation can be obtained through adjudication, in particular for anaphora (Poesio and Artstein, 2005b; Wong and Lee, 2013; Aroyo and Welty, 2015). Virtually every project devoted to large-scale annotation of discourse or semantic phenomena has reached the conclusion that genuine disagreements are widespread. This has long been known for anaphora (Poesio and Artstein, 2005b; Versley, 2008; Recasens et al., 2011) (see also the analysis of disagreements in ONTONOTES in (Pradhan et al."
N19-1176,J00-4005,0,0.445401,"Missing"
N19-1176,doddington-etal-2004-automatic,0,0.0420616,"markables, half of which are singletons. However, the corpus is not intended as a general purpose dataset as only the 3000 most common English words appear in the documents (the majority - 2/3 - of the documents are from Chinese high-school English tests). The corpus’s annotation scheme mainly follows the ONTONOTES guidelines, with a few important differences: singleton mentions and generic coreference are annotated, event anaphora is not, and predicative NPs are annotated as co-referring with their argument, as previously done in the MUC (Grishman and Sundheim, 1995; Chinchor, 1998) and ACE (Doddington et al., 2004) corpora.1 As one could expect, the corpus is relatively easy for coreference systems. The Peters et al. (2018) system trained and tested on P RE C O achieves an av1 An example of predicative NP is 24 degrees in The temperature is 24 degrees. As discussed by van Deemter and Kibble (2000), annotating The temperature and 24 degrees as coreferent would result in nonsensical coreference chains for sentences like The temperature was 24 degrees but it is 27 degrees now. As a result, such markables were annotated as predicative in recent corpora. It’s not clear why we find a return to the old practic"
N19-1176,J14-4004,0,0.163729,"Missing"
N19-1176,M95-1001,0,0.0567654,"35,000 documents for a total of 12.5M tokens and 3.8M markables, half of which are singletons. However, the corpus is not intended as a general purpose dataset as only the 3000 most common English words appear in the documents (the majority - 2/3 - of the documents are from Chinese high-school English tests). The corpus’s annotation scheme mainly follows the ONTONOTES guidelines, with a few important differences: singleton mentions and generic coreference are annotated, event anaphora is not, and predicative NPs are annotated as co-referring with their argument, as previously done in the MUC (Grishman and Sundheim, 1995; Chinchor, 1998) and ACE (Doddington et al., 2004) corpora.1 As one could expect, the corpus is relatively easy for coreference systems. The Peters et al. (2018) system trained and tested on P RE C O achieves an av1 An example of predicative NP is 24 degrees in The temperature is 24 degrees. As discussed by van Deemter and Kibble (2000), annotating The temperature and 24 degrees as coreferent would result in nonsensical coreference chains for sentences like The temperature was 24 degrees but it is 27 degrees now. As a result, such markables were annotated as predicative in recent corpora. It’"
N19-1176,M98-1001,0,0.113102,"of 12.5M tokens and 3.8M markables, half of which are singletons. However, the corpus is not intended as a general purpose dataset as only the 3000 most common English words appear in the documents (the majority - 2/3 - of the documents are from Chinese high-school English tests). The corpus’s annotation scheme mainly follows the ONTONOTES guidelines, with a few important differences: singleton mentions and generic coreference are annotated, event anaphora is not, and predicative NPs are annotated as co-referring with their argument, as previously done in the MUC (Grishman and Sundheim, 1995; Chinchor, 1998) and ACE (Doddington et al., 2004) corpora.1 As one could expect, the corpus is relatively easy for coreference systems. The Peters et al. (2018) system trained and tested on P RE C O achieves an av1 An example of predicative NP is 24 degrees in The temperature is 24 degrees. As discussed by van Deemter and Kibble (2000), annotating The temperature and 24 degrees as coreferent would result in nonsensical coreference chains for sentences like The temperature was 24 degrees but it is 27 degrees now. As a result, such markables were annotated as predicative in recent corpora. It’s not clear why w"
N19-1176,N15-1117,0,0.0347087,"ve been used in NLP to collect data on specific linguistic features; broader platforms such as Wordrobe (Venhuizen et al., 2013) to gamify the entire text annotation pipeline. Crowdsourcing is the most realistic approach to collect a large number of judgments about phenomena such as anaphora. Games in particular are the one type of crowdsourcing scalable to the goal of, for example, a 100M word corpus. So far, however, only small and medium scale resources for NLP have been created via crowdsourcing. For coreference we are only aware of two, both around 50K tokens in size (Chamberlain et al.; Guha et al., 2015). The Groningen Meaning Bank being collected through the Wordrobe platform (Bos et al., 2017) includes many more documents, but so far only very few interpretations have been obtained through the games (e.g., only around 4K judgments have been collected for anaphora). 2.3 Collecting Multiple Judgments In most of the best known efforts at creating anaphoric corpora for English and other languages substantial disagreements between the coders were observed, but none of the resulting resources contains multiple anaphoric interpretations. Systematic analyses of the disagreements among coders observ"
N19-1176,P15-1136,0,0.119573,"ple labels in cases of ambiguity. As far as we know, ours is the first use of MPA to create a large-scale dataset. We show in the paper that MPA can be used to extract from the judgments a high quality coreference dataset that can be used to develop standard coreference resolvers, as well as to investigate disagreements on anaphora. 2 2.1 Background Datasets for Anaphora/Coreference Since the two CONLL shared tasks (Pradhan et al., 2012), ONTONOTES has become the dominant resource for anaphora resolution research (Fernandes et al., 2014; Bj¨orkelund and Kuhn, 2014; Martschat and Strube, 2015; Clark and Manning, 2015, 2016a,b; Lee et al., 2017, 2018). ONTONOTES contains documents in three languages, Arabic (300K tokens), Chinese (950K) and English (1.6M), from several genres but predominantly news. One frequently discussed limitation of ONTONOTES is the absence of singletons (De Marneffe et al., 2015; Chen et al., 2018), which makes it harder to train models for mention detection (Poesio et al., 2018). Another limitation is that expletives are not annotated. As a consequence, downstream applications such as machine translation (Guillou and Hardmeier, 2016) that require pronoun interpretation have to adopt"
N19-1176,L16-1100,0,0.0395008,"j¨orkelund and Kuhn, 2014; Martschat and Strube, 2015; Clark and Manning, 2015, 2016a,b; Lee et al., 2017, 2018). ONTONOTES contains documents in three languages, Arabic (300K tokens), Chinese (950K) and English (1.6M), from several genres but predominantly news. One frequently discussed limitation of ONTONOTES is the absence of singletons (De Marneffe et al., 2015; Chen et al., 2018), which makes it harder to train models for mention detection (Poesio et al., 2018). Another limitation is that expletives are not annotated. As a consequence, downstream applications such as machine translation (Guillou and Hardmeier, 2016) that require pronoun interpretation have to adopt various workarounds. Because of these two restrictions, ONTONOTES only has 195K markables, and a low markable density (0.12 markable/token). A number of smaller corpora provide linguistically richer information (Poesio et al., 2016). Examples include ANCORA for Spanish (Recasens and Mart´ı, 2010), TUBA - D / Z for German (Hinrichs et al., 2005), the Prague Dependency Treebank for Czech and English (Nedoluzhko et al., 2009), and ARRAU for English (Uryupina et al., To Appear). In ARRAU, for example, singletons and expletives are annotated as wel"
N19-1176,D16-1245,0,0.115257,"Missing"
N19-1176,W05-0303,0,0.0733178,"harder to train models for mention detection (Poesio et al., 2018). Another limitation is that expletives are not annotated. As a consequence, downstream applications such as machine translation (Guillou and Hardmeier, 2016) that require pronoun interpretation have to adopt various workarounds. Because of these two restrictions, ONTONOTES only has 195K markables, and a low markable density (0.12 markable/token). A number of smaller corpora provide linguistically richer information (Poesio et al., 2016). Examples include ANCORA for Spanish (Recasens and Mart´ı, 2010), TUBA - D / Z for German (Hinrichs et al., 2005), the Prague Dependency Treebank for Czech and English (Nedoluzhko et al., 2009), and ARRAU for English (Uryupina et al., To Appear). In ARRAU, for example, singletons and expletives are annotated as well, as are split antecedent plurals, generic coreference, discourse deixis, and bridging references. The AR RAU corpus is relatively small in terms of tokens (350K), but has a higher markable density than ONTONOTES (0.29 markable/token), so it has around 100K markables, half the number of ONTONOTES. ARRAU was recently used in the CRAC 2018 shared task (Poesio et al., 2018) to evaluate a number o"
N19-1176,P16-1061,0,0.135835,"Missing"
N19-1176,N13-1132,0,0.320381,"lso annotated: both expletives (not annotated either in ONTONOTES or P RE C O) and predicative NPs. Finally, all types of plurals were annotated, including also split-antecedent plurals as in John met with Mary, and they went to dinner, which again are not annotated either in ONTONOTES or P RE C O . Turning a crowdsourced corpus into a highquality dataset suitable to train and evaluate NLP systems requires, however, an aggregation method appropriate to the data and capable of achieving sufficient quality, something that simple majority voting typically cannot guarantee (Dawid and Skene, 1979; Hovy et al., 2013). What made it possible to extract such a dataset from the collected judgments was the recent development of a probabilistic method for aggregating coreference annotations called MPA (Paun et al., 2018b). MPA extracts silver labels from a coreference annotation and associates them with a probability, allowing for multiple labels in cases of ambiguity. As far as we know, ours is the first use of MPA to create a large-scale dataset. We show in the paper that MPA can be used to extract from the judgments a high quality coreference dataset that can be used to develop standard coreference resolvers"
N19-1176,W07-1525,0,0.117946,"Missing"
N19-1176,W11-1902,0,0.0688459,"e use PDsilver to train a coreference system able to simultaneously identify non-referring expression and build coreference chains (including singletons). As no other system of this type exists at the moment, we developed one ourselves. 6.1 After inferring the mention pairs, coreference chains can be extracted and their quality assessed using standard coreference metrics. Table 5 presents the evaluation against gold chains in PD gold . We compare the chains produced from the mention pairs inferred by MPA and by MAJVOTE, and the chains produced by the STANFORD deterministic coreference system (Lee et al., 2011) (for which we switched off post-processing to output singleton clusters). The results indicate a far better quality of the chains produced using MPA over the alternative methods. Another interesting result is that even a simple MAJVOTE baseline based on crowdsourced annotations performed far better than the STANFORD system, underlining the advantage of crowdsourced annotations for coreference over automatically produced annotations. Using the corpus for coreference resolution Our system The system trained and tested on the corpus is a cluster ranking system that does mention detection and cor"
N19-1176,D17-1018,0,0.0480015,"As far as we know, ours is the first use of MPA to create a large-scale dataset. We show in the paper that MPA can be used to extract from the judgments a high quality coreference dataset that can be used to develop standard coreference resolvers, as well as to investigate disagreements on anaphora. 2 2.1 Background Datasets for Anaphora/Coreference Since the two CONLL shared tasks (Pradhan et al., 2012), ONTONOTES has become the dominant resource for anaphora resolution research (Fernandes et al., 2014; Bj¨orkelund and Kuhn, 2014; Martschat and Strube, 2015; Clark and Manning, 2015, 2016a,b; Lee et al., 2017, 2018). ONTONOTES contains documents in three languages, Arabic (300K tokens), Chinese (950K) and English (1.6M), from several genres but predominantly news. One frequently discussed limitation of ONTONOTES is the absence of singletons (De Marneffe et al., 2015; Chen et al., 2018), which makes it harder to train models for mention detection (Poesio et al., 2018). Another limitation is that expletives are not annotated. As a consequence, downstream applications such as machine translation (Guillou and Hardmeier, 2016) that require pronoun interpretation have to adopt various workarounds. Becau"
N19-1176,N18-2108,0,0.100297,"a far better quality of the chains produced using MPA over the alternative methods. Another interesting result is that even a simple MAJVOTE baseline based on crowdsourced annotations performed far better than the STANFORD system, underlining the advantage of crowdsourced annotations for coreference over automatically produced annotations. Using the corpus for coreference resolution Our system The system trained and tested on the corpus is a cluster ranking system that does mention detection and corefence resolution jointly. The system uses the mention representation from the stateof-the-art (Lee et al., 2018) system, but replaces their mention-ranking model with a cluster ranking model. Our cluster ranking model forms clusters by going through the candidate mentions in their text order and adding them to the clusters, which take into consideration the relative importance of the mentions. An attention mechanism is used to assign mentions within the clusters salience scores, and the clusters are represented as the weighted sums of the mention representations. Separate classifiers are used to identify nonreferring markables and singletons. 1784 Singletons MUC Method P BCUB R F1 P CEAFE R F1 P R F1 Av"
N19-1176,Q15-1029,0,0.0214192,"bability, allowing for multiple labels in cases of ambiguity. As far as we know, ours is the first use of MPA to create a large-scale dataset. We show in the paper that MPA can be used to extract from the judgments a high quality coreference dataset that can be used to develop standard coreference resolvers, as well as to investigate disagreements on anaphora. 2 2.1 Background Datasets for Anaphora/Coreference Since the two CONLL shared tasks (Pradhan et al., 2012), ONTONOTES has become the dominant resource for anaphora resolution research (Fernandes et al., 2014; Bj¨orkelund and Kuhn, 2014; Martschat and Strube, 2015; Clark and Manning, 2015, 2016a,b; Lee et al., 2017, 2018). ONTONOTES contains documents in three languages, Arabic (300K tokens), Chinese (950K) and English (1.6M), from several genres but predominantly news. One frequently discussed limitation of ONTONOTES is the absence of singletons (De Marneffe et al., 2015; Chen et al., 2018), which makes it harder to train models for mention detection (Poesio et al., 2018). Another limitation is that expletives are not annotated. As a consequence, downstream applications such as machine translation (Guillou and Hardmeier, 2016) that require pronoun int"
N19-1176,W09-3017,0,0.0733196,"Missing"
N19-1176,Q14-1025,0,0.0425079,"y identifying plausible interpretations, as well as the results of a preliminary handanalysis of the disagreements in a few documents in our corpus, in Section 7. 5 5.1 Aggregation Probabilistic Aggregation Methods The data collected via Phrase Detectives require an aggregation method to help choose between the different interpretations provided by the players. Simple heuristics such as majority voting are known to underperform compared to probabilistic models of annotation (Whitehill et al., 2009; Raykar et al., 2010; Quoc Viet Hung et al., 2013; Sheshadri and Lease, 2013; Hovy et al., 2013; Passonneau and Carpenter, 2014; Paun et al., 2018a). MPA In MPA, the term label is used to refer to a specific interpretation provided by a player, and the term class to refer to general interpretation categories such as discourse old, discourse new, expletive, or predicative NP. Please note that under this formalism each label belongs to a class: the antecedents belong to the discourse old category, while the other possible labels (e.g., discourse new) coincide with the classes they belong to. The model assumes a preprocessing step in which the markablelevel annotations are transformed into a series of binary decisions wi"
N19-1176,Q18-1040,1,0.918342,"ith Mary, and they went to dinner, which again are not annotated either in ONTONOTES or P RE C O . Turning a crowdsourced corpus into a highquality dataset suitable to train and evaluate NLP systems requires, however, an aggregation method appropriate to the data and capable of achieving sufficient quality, something that simple majority voting typically cannot guarantee (Dawid and Skene, 1979; Hovy et al., 2013). What made it possible to extract such a dataset from the collected judgments was the recent development of a probabilistic method for aggregating coreference annotations called MPA (Paun et al., 2018b). MPA extracts silver labels from a coreference annotation and associates them with a probability, allowing for multiple labels in cases of ambiguity. As far as we know, ours is the first use of MPA to create a large-scale dataset. We show in the paper that MPA can be used to extract from the judgments a high quality coreference dataset that can be used to develop standard coreference resolvers, as well as to investigate disagreements on anaphora. 2 2.1 Background Datasets for Anaphora/Coreference Since the two CONLL shared tasks (Pradhan et al., 2012), ONTONOTES has become the dominant reso"
N19-1176,D18-1218,1,0.930036,"ith Mary, and they went to dinner, which again are not annotated either in ONTONOTES or P RE C O . Turning a crowdsourced corpus into a highquality dataset suitable to train and evaluate NLP systems requires, however, an aggregation method appropriate to the data and capable of achieving sufficient quality, something that simple majority voting typically cannot guarantee (Dawid and Skene, 1979; Hovy et al., 2013). What made it possible to extract such a dataset from the collected judgments was the recent development of a probabilistic method for aggregating coreference annotations called MPA (Paun et al., 2018b). MPA extracts silver labels from a coreference annotation and associates them with a probability, allowing for multiple labels in cases of ambiguity. As far as we know, ours is the first use of MPA to create a large-scale dataset. We show in the paper that MPA can be used to extract from the judgments a high quality coreference dataset that can be used to develop standard coreference resolvers, as well as to investigate disagreements on anaphora. 2 2.1 Background Datasets for Anaphora/Coreference Since the two CONLL shared tasks (Pradhan et al., 2012), ONTONOTES has become the dominant reso"
N19-1176,N18-1202,0,0.0334728,"Missing"
N19-1176,N07-1051,0,0.0291735,"2005b)) is marked, but implicitly, i.e., by asking the judgment of at least 8 players per markable, as opposed to explicitly, as attempted in ARRAU (with little success). 3.3 Markable identification Following standard practice in anaphoric annotation and GWAPs, the markables to be annotated were not identified by the participants themselves; instead, markable identification was carried out semi-automatically. Each document would first be processed by a pipeline combining off-the-shelf tools (sentence splitting and tokenization using the OpenNLP pipeline3 and parsing using the Berkeley Parser (Petrov and Klein, 2007)) and custom preprocessing and post-processing heuristic steps to correct the output. (See (Poesio et al., 2013) for more details about the pipeline and its performance.) Then one of the administrators would carry out a quick check of the document removing the most obvious mistakes before uploading it. After the document was uploaded, participants could report markable errors, which would then be corrected by hand.4 4 The corpus 4.1 Basic statistics This second release of the Phrase Detectives corpus consists of a total of 542 documents contain3 http://opennlp.apache.org As participants report"
N19-1176,P14-2083,0,0.134297,"Missing"
N19-1176,W05-0311,1,0.837792,"utomatically identifying ambiguous markables. A preliminary analysis of the results is presented. 1 Introduction A number of datasets for anaphora resolution / coreference now exist (Poesio et al., 2016), including ONTONOTES that has been the de facto standard since the CONLL shared tasks in 2011 and 2012 (Pradhan et al., 2012), and the just introduced and very substantial P RE C O corpus (Chen et al., 2018). None of these datasets however take into account the research challenging the idea that a ‘gold standard’ interpretation can be obtained through adjudication, in particular for anaphora (Poesio and Artstein, 2005b; Wong and Lee, 2013; Aroyo and Welty, 2015). Virtually every project devoted to large-scale annotation of discourse or semantic phenomena has reached the conclusion that genuine disagreements are widespread. This has long been known for anaphora (Poesio and Artstein, 2005b; Versley, 2008; Recasens et al., 2011) (see also the analysis of disagreements in ONTONOTES in (Pradhan et al., 2012)) and wordsenses (Passonneau et al., 2012), but more recent work has provided evidence that disagreements are frequent for virtually every aspect of language interpretation, not just in subjective tasks such"
N19-1176,W12-4501,0,0.649925,"and make it possible to successfully train a state of the art coreference resolver, including training on singletons and non-referring expressions. The annotation model can also result in more than one label, or no label, being proposed for a markable, thus serving as a baseline method for automatically identifying ambiguous markables. A preliminary analysis of the results is presented. 1 Introduction A number of datasets for anaphora resolution / coreference now exist (Poesio et al., 2016), including ONTONOTES that has been the de facto standard since the CONLL shared tasks in 2011 and 2012 (Pradhan et al., 2012), and the just introduced and very substantial P RE C O corpus (Chen et al., 2018). None of these datasets however take into account the research challenging the idea that a ‘gold standard’ interpretation can be obtained through adjudication, in particular for anaphora (Poesio and Artstein, 2005b; Wong and Lee, 2013; Aroyo and Welty, 2015). Virtually every project devoted to large-scale annotation of discourse or semantic phenomena has reached the conclusion that genuine disagreements are widespread. This has long been known for anaphora (Poesio and Artstein, 2005b; Versley, 2008; Recasens et"
N19-1176,recasens-etal-2012-annotating,0,0.157254,"Missing"
N19-1176,D08-1027,0,0.45181,"Missing"
N19-1176,W13-0215,0,0.028126,"a game-with-a-purpose (GWAP) to aggregate data from non-expert players for collective decisions similar to those from an expert (von Ahn, 2006). The game-based approach to collecting language data is initially costly, but once a game is deployed it can continue to collect data with very little financial support, especially if there is an active community. GWAPs such as Phrase Detectives (Poesio et al., 2013), JeuxDesMots (Joubert and Lafourcade, 2008) and Zombie Lingo (Fort et al., 2014) have been used in NLP to collect data on specific linguistic features; broader platforms such as Wordrobe (Venhuizen et al., 2013) to gamify the entire text annotation pipeline. Crowdsourcing is the most realistic approach to collect a large number of judgments about phenomena such as anaphora. Games in particular are the one type of crowdsourcing scalable to the goal of, for example, a 100M word corpus. So far, however, only small and medium scale resources for NLP have been created via crowdsourcing. For coreference we are only aware of two, both around 50K tokens in size (Chamberlain et al.; Guha et al., 2015). The Groningen Meaning Bank being collected through the Wordrobe platform (Bos et al., 2017) includes many mo"
N19-1176,W13-4307,0,0.121979,"Missing"
P04-1019,J86-3001,0,0.315606,"Missing"
P04-1019,J03-3005,0,0.0189819,"Missing"
P04-1019,W03-2606,0,0.179561,"l., 1997; Vieira and Poesio, 2000) WordNet 1.6 was used as a lexical resource, with poor or mediocre results. These results were due in part to missing entries and / or relations; in part to the fact that because of the monotonic organization of information in WordNet, complex searches are required even to find apparently close associations (like that between wheel and car). Similar results using WordNet 1.6 were reported at around the same time by other groups - e.g., (Humphreys et al., 1997; Harabagiu and Moldovan, 1998) and have been confirmed by more recent studies studying both hyponymy (Markert et al., 2003) and more specifically mereological BDs. Poesio (2003) found that none of the 58 mereological references in the GNOME corpus (discussed below) had a direct mereological link to their anchor: for example, table is not listed as a possible holonym of drawer, nor is house listed as a possible holonym for furniture. Garcia-Almanza (2003) found that only 16 of these 58 mereological references could be resolved by means of more complex searches in WordNet, including following the hypernymy hierarchy for both the anchor and the bridging reference, and a ’spreading activation’ search. Poesio et al. (1"
P04-1019,P02-1014,0,0.259962,"Missing"
P04-1019,J98-2001,1,0.764181,"and a page fell out (Prince, 1981)). Of the 153 mereological references, 58 mereological references are realized by definite descriptions. 6 In (Poesio, 2003), bridging descriptions based on set relations (element, subset) were also considered, but we found that this class of BDs required completely different methods. 7 A serious problem when working with bridging references is the fact that subjects, when asked for judgments about bridging references in general, have a great deal of difficulty in agreeing on which expressions in the corpus are bridging references, and what their anchors are (Poesio and Vieira, 1998). This finding raises a number of interesting theoretical questions concerning the extent of agreement on semantic judgments, but also the practical question of whether it is possible to evaluate the performance of a system on this task. Subsequent work found, however, that restricting the type of bridging inferences required does make it possible for annotators to agree among themselves (Poesio et al., 2004). In the GNOME corpus only a few types of associative relations are marked, but these can be marked reliably, and do include part-of relations like that between the top and the cabinet tha"
P04-1019,W97-1301,1,0.873221,"he Bridging Descriptions (BDs) contained in the corpus used by Vieira and Poesio 3 We make use of the classification of bridging references proposed by Vieira and Poesio (2000). ‘Mereological’ bridging references are one of the the ‘WordNet’ bridging classes, which cover cases where the information required to bridge the gap may be found in a resource such as WordNet (Fellbaum, 1998): synonymy, hyponymy, and meronymy. (2000). In these studies, the lexical distance between a BD and its antecedent was used to choose the anchor for the BD among the antecedents in the previous five sentences. In (Poesio et al., 1997; Vieira and Poesio, 2000) WordNet 1.6 was used as a lexical resource, with poor or mediocre results. These results were due in part to missing entries and / or relations; in part to the fact that because of the monotonic organization of information in WordNet, complex searches are required even to find apparently close associations (like that between wheel and car). Similar results using WordNet 1.6 were reported at around the same time by other groups - e.g., (Humphreys et al., 1997; Harabagiu and Moldovan, 1998) and have been confirmed by more recent studies studying both hyponymy (Markert"
P04-1019,poesio-etal-2002-acquiring,1,0.917293,"Missing"
P04-1019,J04-3003,1,0.755776,"Missing"
P04-1019,P03-2012,0,0.0352566,"Missing"
P04-1019,J00-4003,1,0.514026,"s, whose copper-filled flutes give an added rich color and contrast to the giltbronze mounts, flank the panels. Yellow jasper, a semiprecious stone, rather than the usual marble, forms the top. 2 2.1 Two sources of information for bridging reference resolution Lexical information The use of different sources of lexical knowledge for resolving bridging references has been investigated in a series of papers by Poesio et al. all using as dataset the Bridging Descriptions (BDs) contained in the corpus used by Vieira and Poesio 3 We make use of the classification of bridging references proposed by Vieira and Poesio (2000). ‘Mereological’ bridging references are one of the the ‘WordNet’ bridging classes, which cover cases where the information required to bridge the gap may be found in a resource such as WordNet (Fellbaum, 1998): synonymy, hyponymy, and meronymy. (2000). In these studies, the lexical distance between a BD and its antecedent was used to choose the anchor for the BD among the antecedents in the previous five sentences. In (Poesio et al., 1997; Vieira and Poesio, 2000) WordNet 1.6 was used as a lexical resource, with poor or mediocre results. These results were due in part to missing entries and /"
P04-1019,W03-1023,0,\N,Missing
P04-1019,J95-2003,0,\N,Missing
P04-1019,P99-1008,0,\N,Missing
P04-1019,poesio-kabadjov-2004-general,1,\N,Missing
P04-1019,W03-2605,1,\N,Missing
P04-1050,P87-1022,0,0.851057,"Missing"
P04-1050,W03-2304,0,0.140846,"fication rate of each metric. The gnome corpus contains texts from different genres, not all of which are of interest to us. In order to restrict the scope of the experiment to the text-type most relevant to our study, we selected 20 “museum labels”, i.e., short texts that describe a concrete artefact, which served as the input to seec together with the metrics in section 3.10 5.1 Permutation and search strategy In specifying the performance of the metrics we made use of a simple permutation heuristic exploiting a piece of domain-specific communication knowledge (Kittredge et al., 1991). Like Dimitromanolaki and Androutsopoulos (2003), we noticed that utterances like (a) in example (1), should always appear at the beginning of a felicitous museum label. Hence, we restricted the orderings considered by the seec 9 The Sign Test was chosen over its parametric alternatives to test significance because it does not carry specific assumptions about population distributions and variance. It is also more appropriate for small samples like the one used in this study. 10 Note that example (1) is characteristic of the genre, not the length, of the texts in our subcorpus. The number of CF lists that the BfCs consist of ranges from 4 to"
P04-1050,J95-2003,0,0.982899,"t structuring perspective. 1 Motivation Our research area is descriptive text generation (O’Donnell et al., 2001; Isard et al., 2003), i.e. the generation of descriptions of objects, typically museum artefacts, depicted in a picture. Text (1), from the gnome corpus (Poesio et al., 2004), is an example of short human-authored text from this genre: (1) (a) 144 is a torc. (b) Its present arrangement, twisted into three rings, may be a modern alteration; (c) it should probably be a single ring, worn around the neck. (d) The terminals are in the form of goats’ heads. According to Centering Theory (Grosz et al., 1995; Walker et al., 1998a), an important factor for the felicity of (1) is its entity coherence: the way centers (discourse entities), such as the referent of the NPs “144” in clause (a) and “its” in clause (b), are introduced and discussed in subsequent clauses. It is often claimed in current work on in natural language generation that the constraints on felicitous text proposed by the theory are useful to guide text structuring, in combination with other factors (see (Karamanis, 2003) for an overview). However, how successful Centering’s constraints are on their own in generating a felicitous t"
P04-1050,W02-2111,1,0.941744,"deliberately ignored. In accordance with recent work in the emerging field of text-to-text generation (Barzilay et al., 2002; Lapata, 2003), we assume that the input to text structuring is a set of clauses. The output of text structuring is merely an ordering of these clauses, rather than the tree-like structure of database facts often used in traditional deep generation (Reiter and Dale, 2000). Our approach is further characterized by two key insights. The first distinguishing feature is that we assume a search-based approach to text structuring (Mellish et al., 1998; Kibble and Power, 2000; Karamanis and Manurung, 2002) in which many candidate orderings of clauses are evaluated according to scores assigned by a given metric, and the best-scoring ordering among the candidate solutions is chosen. The second novel aspect is that our approach is based on the position that the most straightforward way of using Centering for text structuring is by defining a Centering-based metric of coherence Karamanis (2003). Together, these two assumptions lead to a view of text planning in which the constraints of Centering act not as filters, but as ranking factors, and the text planner may be forced to choose a sub-optimal s"
P04-1050,W00-1411,0,0.722635,"ole of other factors is deliberately ignored. In accordance with recent work in the emerging field of text-to-text generation (Barzilay et al., 2002; Lapata, 2003), we assume that the input to text structuring is a set of clauses. The output of text structuring is merely an ordering of these clauses, rather than the tree-like structure of database facts often used in traditional deep generation (Reiter and Dale, 2000). Our approach is further characterized by two key insights. The first distinguishing feature is that we assume a search-based approach to text structuring (Mellish et al., 1998; Kibble and Power, 2000; Karamanis and Manurung, 2002) in which many candidate orderings of clauses are evaluated according to scores assigned by a given metric, and the best-scoring ordering among the candidate solutions is chosen. The second novel aspect is that our approach is based on the position that the most straightforward way of using Centering for text structuring is by defining a Centering-based metric of coherence Karamanis (2003). Together, these two assumptions lead to a view of text planning in which the constraints of Centering act not as filters, but as ranking factors, and the text planner may be f"
P04-1050,J01-4007,0,0.339699,"Missing"
P04-1050,P03-1069,0,0.412162,"guide text structuring, in combination with other factors (see (Karamanis, 2003) for an overview). However, how successful Centering’s constraints are on their own in generating a felicitous text structure is an open question, already raised by the seminal papers of the theory (Brennan et al., 1987; Grosz et al., 1995). In this work, we explored this question by developing an approach to text structuring purely based on Centering, in which the role of other factors is deliberately ignored. In accordance with recent work in the emerging field of text-to-text generation (Barzilay et al., 2002; Lapata, 2003), we assume that the input to text structuring is a set of clauses. The output of text structuring is merely an ordering of these clauses, rather than the tree-like structure of database facts often used in traditional deep generation (Reiter and Dale, 2000). Our approach is further characterized by two key insights. The first distinguishing feature is that we assume a search-based approach to text structuring (Mellish et al., 1998; Kibble and Power, 2000; Karamanis and Manurung, 2002) in which many candidate orderings of clauses are evaluated according to scores assigned by a given metric, an"
P04-1050,W98-1411,1,0.776343,"Missing"
P04-1050,J02-3003,0,0.0184596,"ite=’finite-yes’ id=’u210’> <ne id=&quot;ne410&quot; gf=&quot;subj&quot;>144</ne> is <ne id=&quot;ne411&quot; gf=&quot;predicate&quot;> a torc</ne> </unit>. The ranking of the CFs other than the CP is defined according to the following preference on their gf (Brennan et al., 1987): obj>iobj>other. CFs with the same gf are ranked according to the linear order of the corresponding NPs in the utterance. The second column of Table 1 shows how the utterances in example (1) are automatically translated by the scripts developed by Poesio et al. (2004) into a 1 For example, one could equate “utterance” with sentence (Strube and Hahn, 1999; Miltsakaki, 2002), use indirect realisation for the computation of the CF list (Grosz et al., 1995), rank the CFs according to their information status (Strube and Hahn, 1999), etc. 2 Our definition includes titles which are not always finite units, but excludes finite relative clauses, the second element of coordinated VPs and clause complements which are often taken as not having their own CF lists in the literature. 3 Or as a post-copular subject in a there-clause. U (a) (b) (c) (d) CF list: {CP, {de374, {de376, {de374, {de380, other CFs} de375} de374, de377} de379} de381, de382} CB n.a. de374 de374 - Trans"
P04-1050,J04-3003,1,0.846124,"Missing"
P04-1050,J99-3001,0,0.352512,"10 “144”. (2) <unit finite=’finite-yes’ id=’u210’> <ne id=&quot;ne410&quot; gf=&quot;subj&quot;>144</ne> is <ne id=&quot;ne411&quot; gf=&quot;predicate&quot;> a torc</ne> </unit>. The ranking of the CFs other than the CP is defined according to the following preference on their gf (Brennan et al., 1987): obj>iobj>other. CFs with the same gf are ranked according to the linear order of the corresponding NPs in the utterance. The second column of Table 1 shows how the utterances in example (1) are automatically translated by the scripts developed by Poesio et al. (2004) into a 1 For example, one could equate “utterance” with sentence (Strube and Hahn, 1999; Miltsakaki, 2002), use indirect realisation for the computation of the CF list (Grosz et al., 1995), rank the CFs according to their information status (Strube and Hahn, 1999), etc. 2 Our definition includes titles which are not always finite units, but excludes finite relative clauses, the second element of coordinated VPs and clause complements which are often taken as not having their own CF lists in the literature. 3 Or as a post-copular subject in a there-clause. U (a) (b) (c) (d) CF list: {CP, {de374, {de376, {de374, {de380, other CFs} de375} de374, de377} de379} de381, de382} CB n.a."
P08-4003,P05-1022,0,0.0193699,"as additional information such as part-of-speech tags and merging these information into markables that are the starting point for the mentions used by the coreference resolution proper. Starting out with a chunking pipeline, which uses a classical combination of tagger and chunker, with the Stanford POS tagger (Toutanova et al., 2003), the YamCha chunker (Kudoh and Matsumoto, 2000) and the Stanford Named Entity Recognizer (Finkel et al., 2005), the desire to use richer syntactic representations led to the development of a parsing pipeline, which uses Charniak and Johnson’s reranking parser (Charniak and Johnson, 2005) to assign POS tags and uses base NPs as chunk equivalents, while also providing syntactic trees that can be used by feature extractors. BART also supports using the Berkeley parser (Petrov et al., 2006), yielding an easy-to-use Java-only solution. To provide a better starting point for mention detection on the ACE corpora, the Carafe pipeline uses an ACE mention tagger provided by MITRE (Wellner and Vilain, 2006). A specialized merger then discards any base NP that was not detected to be an ACE mention. To perform coreference resolution proper, the mention-building module uses the markables c"
P08-4003,N07-1011,0,0.0838311,"Missing"
P08-4003,P05-1045,0,0.0128475,"RT is available from http://www.sfs.uni-tuebingen.de/˜versley/BART/. 10 diff, visual display). Preprocessing consists in marking up noun chunks and named entities, as well as additional information such as part-of-speech tags and merging these information into markables that are the starting point for the mentions used by the coreference resolution proper. Starting out with a chunking pipeline, which uses a classical combination of tagger and chunker, with the Stanford POS tagger (Toutanova et al., 2003), the YamCha chunker (Kudoh and Matsumoto, 2000) and the Stanford Named Entity Recognizer (Finkel et al., 2005), the desire to use richer syntactic representations led to the development of a parsing pipeline, which uses Charniak and Johnson’s reranking parser (Charniak and Johnson, 2005) to assign POS tags and uses base NPs as chunk equivalents, while also providing syntactic trees that can be used by feature extractors. BART also supports using the Berkeley parser (Petrov et al., 2006), yielding an easy-to-use Java-only solution. To provide a better starting point for mention detection on the ACE corpora, the Carafe pipeline uses an ACE mention tagger provided by MITRE (Wellner and Vilain, 2006). A s"
P08-4003,W00-0730,0,0.105306,"grated MMAX2 functionality (annotation 1 An open source version of BART is available from http://www.sfs.uni-tuebingen.de/˜versley/BART/. 10 diff, visual display). Preprocessing consists in marking up noun chunks and named entities, as well as additional information such as part-of-speech tags and merging these information into markables that are the starting point for the mentions used by the coreference resolution proper. Starting out with a chunking pipeline, which uses a classical combination of tagger and chunker, with the Stanford POS tagger (Toutanova et al., 2003), the YamCha chunker (Kudoh and Matsumoto, 2000) and the Stanford Named Entity Recognizer (Finkel et al., 2005), the desire to use richer syntactic representations led to the development of a parsing pipeline, which uses Charniak and Johnson’s reranking parser (Charniak and Johnson, 2005) to assign POS tags and uses base NPs as chunk equivalents, while also providing syntactic trees that can be used by feature extractors. BART also supports using the Berkeley parser (Petrov et al., 2006), yielding an easy-to-use Java-only solution. To provide a better starting point for mention detection on the ACE corpora, the Carafe pipeline uses an ACE m"
P08-4003,P00-1023,0,0.0981402,"Missing"
P08-4003,E06-1015,1,0.765228,"pFigure 2: Example system configuration ment. The set of feature extractors that the system uses is set in an XML description file, which allows for straightforward prototyping and experimentation with different feature sets. Learning BART provides a generic abstraction layer that maps application-internal representations to a suitable format for several machine learning toolkits: One module exposes the functionality of the the WEKA machine learning toolkit (Witten and Frank, 2005), while others interface to specialized state-of-the art learners. SVMLight (Joachims, 1999), in the SVMLight/TK (Moschitti, 2006) variant, allows to use tree-valued features. SVM Classification uses a Java Native Interface-based wrapper replacing SVMLight/TK’s svm classify program to improve the classification speed. Also included is a Maximum entropy classifier that is based upon Robert Dodier’s translation of Liu and Nocedal’s (1989) L-BFGS optimization code, with a function for programmatic feature combination.2 Training/Testing The training and testing phases slightly differ from each other. In the training phase, the pairs that are to be used as training examples have to be selected in a process of sample selection"
P08-4003,P06-1055,0,0.0140385,"unking pipeline, which uses a classical combination of tagger and chunker, with the Stanford POS tagger (Toutanova et al., 2003), the YamCha chunker (Kudoh and Matsumoto, 2000) and the Stanford Named Entity Recognizer (Finkel et al., 2005), the desire to use richer syntactic representations led to the development of a parsing pipeline, which uses Charniak and Johnson’s reranking parser (Charniak and Johnson, 2005) to assign POS tags and uses base NPs as chunk equivalents, while also providing syntactic trees that can be used by feature extractors. BART also supports using the Berkeley parser (Petrov et al., 2006), yielding an easy-to-use Java-only solution. To provide a better starting point for mention detection on the ACE corpora, the Carafe pipeline uses an ACE mention tagger provided by MITRE (Wellner and Vilain, 2006). A specialized merger then discards any base NP that was not detected to be an ACE mention. To perform coreference resolution proper, the mention-building module uses the markables created by the pipeline to create mention objects, which provide an interface more appropriate for coreference resolution than the MiniDiscourse markables. These objects are grouped into equivalence class"
P08-4003,N06-1025,1,0.924334,"Missing"
P08-4003,qiu-etal-2004-public,0,0.168384,"Missing"
P08-4003,J01-4004,0,0.934827,"er NLP applications. Developing a full coreference system able to run all the way from raw text to semantic interpretation is a considerable engineering effort, yet there is very limited availability of off-the shelf tools for researchers whose interests are not in coreference, or for researchers who want to concentrate on a specific aspect of the problem. We present BART, a highly modular toolkit for developing coreference applications. In the Johns Hopkins workshop on using lexical and encyclopedic knowledge for entity disambiguation, the toolkit was used to extend a reimplementation of the Soon et al. (2001) proposal with a variety of additional syntactic and knowledge-based features, and experiment with alternative resolution processes, preprocessing tools, and classifiers. 1 A number of systems that perform coreference resolution are publicly available, such as G UITAR (Steinberger et al., 2007), which handles the full coreference task, and JAVA RAP (Qiu et al., 2004), which only resolves pronouns. However, literature on coreference resolution, if providing a baseline, usually uses the algorithm and feature set of Soon et al. (2001) for this purpose. Introduction Coreference resolution refers t"
P08-4003,N03-1033,0,0.00525991,"forming qualitative error analysis using integrated MMAX2 functionality (annotation 1 An open source version of BART is available from http://www.sfs.uni-tuebingen.de/˜versley/BART/. 10 diff, visual display). Preprocessing consists in marking up noun chunks and named entities, as well as additional information such as part-of-speech tags and merging these information into markables that are the starting point for the mentions used by the coreference resolution proper. Starting out with a chunking pipeline, which uses a classical combination of tagger and chunker, with the Stanford POS tagger (Toutanova et al., 2003), the YamCha chunker (Kudoh and Matsumoto, 2000) and the Stanford Named Entity Recognizer (Finkel et al., 2005), the desire to use richer syntactic representations led to the development of a parsing pipeline, which uses Charniak and Johnson’s reranking parser (Charniak and Johnson, 2005) to assign POS tags and uses base NPs as chunk equivalents, while also providing syntactic trees that can be used by feature extractors. BART also supports using the Berkeley parser (Petrov et al., 2006), yielding an easy-to-use Java-only solution. To provide a better starting point for mention detection on th"
P08-4003,uryupina-2006-coreference,0,0.0608766,"Missing"
P08-4003,wellner-vilain-2006-leveraging,0,0.0159334,"cognizer (Finkel et al., 2005), the desire to use richer syntactic representations led to the development of a parsing pipeline, which uses Charniak and Johnson’s reranking parser (Charniak and Johnson, 2005) to assign POS tags and uses base NPs as chunk equivalents, while also providing syntactic trees that can be used by feature extractors. BART also supports using the Berkeley parser (Petrov et al., 2006), yielding an easy-to-use Java-only solution. To provide a better starting point for mention detection on the ACE corpora, the Carafe pipeline uses an ACE mention tagger provided by MITRE (Wellner and Vilain, 2006). A specialized merger then discards any base NP that was not detected to be an ACE mention. To perform coreference resolution proper, the mention-building module uses the markables created by the pipeline to create mention objects, which provide an interface more appropriate for coreference resolution than the MiniDiscourse markables. These objects are grouped into equivalence classes by the resolution process and a coreference layer is written into the document, which can be used for detailed error analysis. Feature Extraction BART’s default resolver goes through all mentions and looks for p"
P08-4003,P06-1006,1,0.890114,"Missing"
P08-4003,P04-1018,0,\N,Missing
P08-4003,I05-1063,1,\N,Missing
P09-5006,poesio-etal-2002-acquiring,1,\N,Missing
P09-5006,N01-1008,0,\N,Missing
P09-5006,H05-1013,0,\N,Missing
P09-5006,J05-3004,0,\N,Missing
P09-5006,P04-1018,0,\N,Missing
P09-5006,N06-1025,1,\N,Missing
P09-5006,P02-1014,0,\N,Missing
P09-5006,J01-4004,0,\N,Missing
P09-5006,P07-1067,0,\N,Missing
P09-5006,P03-1023,0,\N,Missing
P11-1081,D07-1119,0,0.0554162,"Missing"
P11-1081,bosco-etal-2010-comparing,0,0.0188446,"Missing"
P11-1081,N07-1030,0,0.260518,"crease in performance. We conclude and discuss future work in Section 7. 2 Using ILP for joint anaphoricity and coreference determination Integer Linear Programming (ILP) is a method for constraint-based inference aimed at finding the values for a set of variables that maximize a (linear) objective function while satisfying a number of constraints. Roth and Yih (2004) advocated ILP as a general solution for a number of NLP tasks that require combining multiple classifiers and which the traditional pipeline architecture is not appropriate, such as entity disambiguation and relation extraction. Denis and Baldridge (2007) defined the following object function for the joint anaphoricity and coreference determination problem. 805  min i,j∈P −C cC i,j · xi,j + ci,j · (1 − xi,j ) +  −A cA · (1 − yj ) (2) j · yj + cj j∈M subject to xi,j ∈ {0, 1} ∀i, j ∈ P yj ∈ {0, 1} ∀j ∈ M M stands for the set of mentions in the document, and P the set of possible coreference links over these mentions. xi,j is an indicator variable that is set to 1 if mentions i and j are coreferent, and 0 otherwise. yj is an indicator variable that is set to 1 if mention j is anaphoric, and 0 otherwise. The costs cC i,j = −log"
P11-1081,J95-2003,0,0.459765,"n Section 3: the direct reimplementation of the Denis and Baldridge proposal (i.e., using the same constrains), a version replacing Do-Not-Resolve-Not-Anaphors with BestFirst, and a version with Subject Detection as well. As discussed by Iida et al. (2007a) and Imamura et al. (2009), useful features in intra-sentential zeroanaphora are different from ones in inter-sentential zero-anaphora because in the former problem syntactic information between a zero pronoun and its candidate antecedent is essential, while the latter needs to capture the significance of saliency based on Centering Theory (Grosz et al., 1995). To directly reflect this difference, we created two antecedent identification models; one for intrasentential zero-anaphora, induced using the training instances which a zero pronoun and its candidate antecedent appear in the same sentences, the other for 6 7 807 Models http://chasen-legacy.sourceforge.jp/ http://sourceforge.jp/projects/naist-jdic/ inter-sentential cases, induced from the remaining training instances. To estimate the feature weights of each classifier, we used MEGAM8 , an implementation of the Maximum Entropy model, with default parameter settings. The ILP-based models were"
P11-1081,W03-2604,1,0.842834,"baselines, that precision is still low. One of the major source of the errors is that zero pronouns are frequently used in Italian and Japanese in contexts in which in English as so-called generic they would be used: “I walked into the hotel and (they) said ..”. In such case, the zero pronoun detection model is often incorrect. We are considering adding a generic they detection component. We also intend to experiment with introducing more sophisticated antecedent identification models in the ILP framework. In this paper, we used a very basic pairwise classifier; however Yang et al. (2008) and Iida et al. (2003) showed that the relative comparison of two candidate antecedents leads to obtaining better accuracy than the pairwise model. However, these approaches do not output absolute probabilities, but relative significance between two candidates, and therefore cannot be directly integrated with the ILP-framework. We plan to examine ways of appropriately estimating an absolute score from a set of relative scores for further refinement. Finally, we would like to test our model with English constructions which closely resemble zero anaphora. One example were studied in the Semeval 2010 ‘Linking Events a"
P11-1081,W07-1522,1,0.353433,"anaphoric reference depends on the referred entity being sufficiently salient, hence this type of data–particularly in Japanese and Italian–played a key role in early work in coreference resolution, e.g., in the development of Centering (Kameyama, 1985; Walker et al., 1994; Di Eugenio, 1998). This research highlighted both commonalities and differences between the phenomenon in such languages. Zero anaphora resolution has remained a very active area of study for researchers working on Japanese, because of the prevalence of zeros in such languages1 (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Imamura et al., 2009; Sasano et al., 2009; Taira et al., 2010). But now the availability of corpora annotated to study anaphora, including zero anaphora, in languages such as Italian (e.g., Rodriguez et al. (2010)), and their use in competitions such as SEMEVAL 2010 Task 1 on Multilingual Coreference (Recasens et al., 2010), is leading to a renewed interest in zero anaphora resolution, particularly at the light of the mediocre results obtained on zero anaphors by most systems participating in SEMEVAL. Resolving zero anaphora requires the simultaneous decision that one of"
P11-1081,P09-2022,0,0.688302,"rred entity being sufficiently salient, hence this type of data–particularly in Japanese and Italian–played a key role in early work in coreference resolution, e.g., in the development of Centering (Kameyama, 1985; Walker et al., 1994; Di Eugenio, 1998). This research highlighted both commonalities and differences between the phenomenon in such languages. Zero anaphora resolution has remained a very active area of study for researchers working on Japanese, because of the prevalence of zeros in such languages1 (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Imamura et al., 2009; Sasano et al., 2009; Taira et al., 2010). But now the availability of corpora annotated to study anaphora, including zero anaphora, in languages such as Italian (e.g., Rodriguez et al. (2010)), and their use in competitions such as SEMEVAL 2010 Task 1 on Multilingual Coreference (Recasens et al., 2010), is leading to a renewed interest in zero anaphora resolution, particularly at the light of the mediocre results obtained on zero anaphors by most systems participating in SEMEVAL. Resolving zero anaphora requires the simultaneous decision that one of the arguments of a verb is phonetically un"
P11-1081,W03-1024,0,0.707512,"he felicitousness of zero anaphoric reference depends on the referred entity being sufficiently salient, hence this type of data–particularly in Japanese and Italian–played a key role in early work in coreference resolution, e.g., in the development of Centering (Kameyama, 1985; Walker et al., 1994; Di Eugenio, 1998). This research highlighted both commonalities and differences between the phenomenon in such languages. Zero anaphora resolution has remained a very active area of study for researchers working on Japanese, because of the prevalence of zeros in such languages1 (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Imamura et al., 2009; Sasano et al., 2009; Taira et al., 2010). But now the availability of corpora annotated to study anaphora, including zero anaphora, in languages such as Italian (e.g., Rodriguez et al. (2010)), and their use in competitions such as SEMEVAL 2010 Task 1 on Multilingual Coreference (Recasens et al., 2010), is leading to a renewed interest in zero anaphora resolution, particularly at the light of the mediocre results obtained on zero anaphors by most systems participating in SEMEVAL. Resolving zero anaphora requires the simultaneous d"
P11-1081,S10-1018,0,0.0269548,"Missing"
P11-1081,W04-3239,0,0.026554,"and transformation-based learning respectively in order to manually analyze which clues are important for each argument assignment. Imamura et al. (2009) also tackled to the same problem setting by applying a pairwise classifier for each argument. In their approach, a ‘null’ argument is explicitly added into the set of candidate argument to learn the situation where an argument of a predicate is ‘exophoric’. They reported their model achieved better performance than the work by Taira et al. (2008). Iida et al. (2007a) also used the NAIST text corpus. They adopted the BACT learning algorithm (Kudo and Matsumoto, 2004) to effectively learn subtrees useful for both antecedent identification and zero pronoun detection. Their model drastically outperformed a simple pairwise model, but it is still performed as a cascaded process. Incorporating Italian system mentions gold mentions combined separated combined model R P F R P F R P PAIRWISE 0.508 0.208 0.295 0.472 0.241 0.319 0.582 0.261 DS-CASCADE 0.225 0.553 0.320 0.217 0.574 0.315 0.245 0.609 I-BART 0.324 0.294 0.308 – – – 0.532 0.441 ILP 0.539 0.321 0.403 0.535 0.316 0.397 0.614 0.369 0.471 0.404 0.435 0.483 0.409 0.443 0.545 0.517 ILP +BF 0.537 0.325 0.405 0"
P11-1081,P02-1014,0,0.0429582,"coreference resolution for all anaphors In a second series of experiments we evaluated the performance of our models together with a full coreference system resolving all anaphors, not just zeros. 5.1 Separating vs combining classifiers Different types of nominal expressions display very different anaphoric behavior: e.g., pronoun resolution involves very different types of information from nominal expression resolution, depending more on syntactic information and on the local context and less on commonsense knowledge. But the most common approach to coreference resolution (Soon et al., 2001; Ng and Cardie, 2002, etc.) is to use a single classifier to identify antecedents of all anaphoric expressions, relying on the ability of the machine learning algorithm to learn these differences. These models, however, often fail to capture the differences in anaphoric behavior between different types of expressions–one of the reasons being that the amount of training instances is often too small to learn such differences.11 Using different models would appear to be key in the case of zeroanaphora resolution, which differs even more from the rest of anaphora resolution, e.g., in being particularly sensitive to l"
P11-1081,pianta-etal-2008-textpro,0,0.141876,"6 / 29,544 10,206 / 161,124 28,732 / 190,668 test 696 9,287 250,901 7,877 / 11,205 4,396 / 61,652 12,273 / 72,857 In the 6th column we use the term ‘anaphoric’ to indicate the number of zero anaphors that have an antecedent in the text, whereas the total figure is the sum of anaphoric and exophoric zero-anaphors - zeros with a vague / generic reference. language Italian Table 1: Italian and Japanese Data Sets coreference are annotated. This dataset consists of articles from Italian Wikipedia, tokenized, POStagged and morphologically analyzed using TextPro, a freely available Italian pipeline (Pianta et al., 2008). We parsed the corpus using the Italian version of the DESR dependency parser (Attardi et al., 2007). In Italian, zero pronouns may only occur as omitted subjects of verbs. Therefore, in the task of zero-anaphora resolution all verbs appearing in a text are considered candidates for zero pronouns, and all gold mentions or system mentions preceding a candidate zero pronoun are considered as candidate antecedents. (In contrast, in the experiments on coreference resolution discussed in the following section, all mentions are considered as both candidate anaphors and candidate antecedents. To com"
P11-1081,poesio-etal-2010-creating,1,0.909451,"sk of zero-anaphora resolution all verbs appearing in a text are considered candidates for zero pronouns, and all gold mentions or system mentions preceding a candidate zero pronoun are considered as candidate antecedents. (In contrast, in the experiments on coreference resolution discussed in the following section, all mentions are considered as both candidate anaphors and candidate antecedents. To compare the results with gold mentions and with system detected mentions, we carried out an evaluation using the mentions automatically detected by the Italian version of the BART system (I-BART) (Poesio et al., 2010), which is freely downloadable.3 Japanese For Japanese coreference we used the NAIST Text Corpus (Iida et al., 2007b) version 1.4β, which contains the annotated data about NP coreference and zero-anaphoric relations. We also used the Kyoto University Text Corpus4 that provides dependency relations information for the same articles as the NAIST Text Corpus. In addition, we also used a Japanese named entity tagger, CaboCha5 for automatically tagging named entity labels. In the NAIST Text Corpus mention boundaries are not annotated, only the heads. Thus, we considered 3 http://www.bart-coref.org/"
P11-1081,W09-2411,0,0.0693318,"Missing"
P11-1081,rodriguez-etal-2010-anaphoric,1,0.747253,"ntering (Kameyama, 1985; Walker et al., 1994; Di Eugenio, 1998). This research highlighted both commonalities and differences between the phenomenon in such languages. Zero anaphora resolution has remained a very active area of study for researchers working on Japanese, because of the prevalence of zeros in such languages1 (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Imamura et al., 2009; Sasano et al., 2009; Taira et al., 2010). But now the availability of corpora annotated to study anaphora, including zero anaphora, in languages such as Italian (e.g., Rodriguez et al. (2010)), and their use in competitions such as SEMEVAL 2010 Task 1 on Multilingual Coreference (Recasens et al., 2010), is leading to a renewed interest in zero anaphora resolution, particularly at the light of the mediocre results obtained on zero anaphors by most systems participating in SEMEVAL. Resolving zero anaphora requires the simultaneous decision that one of the arguments of a verb is phonetically unrealized (and which argument exactly–in this paper, we will only be concerned with subject zeros as these are the only type to occur in Italian) and that a particular entity is its antecedent."
P11-1081,W04-2401,0,0.0164358,"ormulation in Section 3. In Section 4 we show the experimental results with zero anaphora only. In Section 5 we discuss experiments testing that adding our zero anaphora detector and resolver to a full coreference resolver would result in overall increase in performance. We conclude and discuss future work in Section 7. 2 Using ILP for joint anaphoricity and coreference determination Integer Linear Programming (ILP) is a method for constraint-based inference aimed at finding the values for a set of variables that maximize a (linear) objective function while satisfying a number of constraints. Roth and Yih (2004) advocated ILP as a general solution for a number of NLP tasks that require combining multiple classifiers and which the traditional pipeline architecture is not appropriate, such as entity disambiguation and relation extraction. Denis and Baldridge (2007) defined the following object function for the joint anaphoricity and coreference determination problem. 805  min i,j∈P −C cC i,j · xi,j + ci,j · (1 − xi,j ) +  −A cA · (1 − yj ) (2) j · yj + cj j∈M subject to xi,j ∈ {0, 1} ∀i, j ∈ P yj ∈ {0, 1} ∀j ∈ M M stands for the set of mentions in the document, and P the set of possible"
P11-1081,N09-1059,0,0.199712,"iciently salient, hence this type of data–particularly in Japanese and Italian–played a key role in early work in coreference resolution, e.g., in the development of Centering (Kameyama, 1985; Walker et al., 1994; Di Eugenio, 1998). This research highlighted both commonalities and differences between the phenomenon in such languages. Zero anaphora resolution has remained a very active area of study for researchers working on Japanese, because of the prevalence of zeros in such languages1 (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Imamura et al., 2009; Sasano et al., 2009; Taira et al., 2010). But now the availability of corpora annotated to study anaphora, including zero anaphora, in languages such as Italian (e.g., Rodriguez et al. (2010)), and their use in competitions such as SEMEVAL 2010 Task 1 on Multilingual Coreference (Recasens et al., 2010), is leading to a renewed interest in zero anaphora resolution, particularly at the light of the mediocre results obtained on zero anaphors by most systems participating in SEMEVAL. Resolving zero anaphora requires the simultaneous decision that one of the arguments of a verb is phonetically unrealized (and which a"
P11-1081,C02-1078,0,0.431839,"o.poesio@unitn.it The felicitousness of zero anaphoric reference depends on the referred entity being sufficiently salient, hence this type of data–particularly in Japanese and Italian–played a key role in early work in coreference resolution, e.g., in the development of Centering (Kameyama, 1985; Walker et al., 1994; Di Eugenio, 1998). This research highlighted both commonalities and differences between the phenomenon in such languages. Zero anaphora resolution has remained a very active area of study for researchers working on Japanese, because of the prevalence of zeros in such languages1 (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Imamura et al., 2009; Sasano et al., 2009; Taira et al., 2010). But now the availability of corpora annotated to study anaphora, including zero anaphora, in languages such as Italian (e.g., Rodriguez et al. (2010)), and their use in competitions such as SEMEVAL 2010 Task 1 on Multilingual Coreference (Recasens et al., 2010), is leading to a renewed interest in zero anaphora resolution, particularly at the light of the mediocre results obtained on zero anaphors by most systems participating in SEMEVAL. Resolving zero anaphora re"
P11-1081,J01-4004,0,0.504846,"entification models; one for intrasentential zero-anaphora, induced using the training instances which a zero pronoun and its candidate antecedent appear in the same sentences, the other for 6 7 807 Models http://chasen-legacy.sourceforge.jp/ http://sourceforge.jp/projects/naist-jdic/ inter-sentential cases, induced from the remaining training instances. To estimate the feature weights of each classifier, we used MEGAM8 , an implementation of the Maximum Entropy model, with default parameter settings. The ILP-based models were compared with the following baselines. PAIRWISE: as in the work by Soon et al. (2001), antecedent identification and anaphoricity determination are simultaneously executed by a single classifier. DS-CASCADE: the model first filters out nonanaphoric candidate anaphors using an anaphoricity determination model, then selects an antecedent from a set of candidate antecedents of anaphoric candidate anaphors using an antecedent identification model. 4.3 SUBJ PRE TOPIC PRE * NUM PRE (GEN PRE ) FIRST SENT FIRST WORD POS / / DEP LEMMA LABEL D POS D LEMMA / / D DEP LABEL PATH * description 1 if subject is included in the preceding words of ZERO in a sentence; otherwise 0. 1 if topic cas"
P11-1081,D08-1055,0,0.616277,"depends on the referred entity being sufficiently salient, hence this type of data–particularly in Japanese and Italian–played a key role in early work in coreference resolution, e.g., in the development of Centering (Kameyama, 1985; Walker et al., 1994; Di Eugenio, 1998). This research highlighted both commonalities and differences between the phenomenon in such languages. Zero anaphora resolution has remained a very active area of study for researchers working on Japanese, because of the prevalence of zeros in such languages1 (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Imamura et al., 2009; Sasano et al., 2009; Taira et al., 2010). But now the availability of corpora annotated to study anaphora, including zero anaphora, in languages such as Italian (e.g., Rodriguez et al. (2010)), and their use in competitions such as SEMEVAL 2010 Task 1 on Multilingual Coreference (Recasens et al., 2010), is leading to a renewed interest in zero anaphora resolution, particularly at the light of the mediocre results obtained on zero anaphors by most systems participating in SEMEVAL. Resolving zero anaphora requires the simultaneous decision that one of the arguments of a v"
P11-1081,P10-2030,0,0.0400395,"ce this type of data–particularly in Japanese and Italian–played a key role in early work in coreference resolution, e.g., in the development of Centering (Kameyama, 1985; Walker et al., 1994; Di Eugenio, 1998). This research highlighted both commonalities and differences between the phenomenon in such languages. Zero anaphora resolution has remained a very active area of study for researchers working on Japanese, because of the prevalence of zeros in such languages1 (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Imamura et al., 2009; Sasano et al., 2009; Taira et al., 2010). But now the availability of corpora annotated to study anaphora, including zero anaphora, in languages such as Italian (e.g., Rodriguez et al. (2010)), and their use in competitions such as SEMEVAL 2010 Task 1 on Multilingual Coreference (Recasens et al., 2010), is leading to a renewed interest in zero anaphora resolution, particularly at the light of the mediocre results obtained on zero anaphors by most systems participating in SEMEVAL. Resolving zero anaphora requires the simultaneous decision that one of the arguments of a verb is phonetically unrealized (and which argument exactly–in th"
P11-1081,J94-2003,0,0.164301,": e.g., the subjects of Italian and Japanese translations of buy in (1b) and (1c) are not explicitly realized. We call these nonrealized mandatory arguments zero anaphors. Massimo Poesio Universit`a di Trento, Center for Mind / Brain Sciences University of Essex, Language and Computation Group massimo.poesio@unitn.it The felicitousness of zero anaphoric reference depends on the referred entity being sufficiently salient, hence this type of data–particularly in Japanese and Italian–played a key role in early work in coreference resolution, e.g., in the development of Centering (Kameyama, 1985; Walker et al., 1994; Di Eugenio, 1998). This research highlighted both commonalities and differences between the phenomenon in such languages. Zero anaphora resolution has remained a very active area of study for researchers working on Japanese, because of the prevalence of zeros in such languages1 (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Imamura et al., 2009; Sasano et al., 2009; Taira et al., 2010). But now the availability of corpora annotated to study anaphora, including zero anaphora, in languages such as Italian (e.g., Rodriguez et al. (2010)), and their use in c"
P11-1081,J08-3002,0,0.0155502,"recall compared to the baselines, that precision is still low. One of the major source of the errors is that zero pronouns are frequently used in Italian and Japanese in contexts in which in English as so-called generic they would be used: “I walked into the hotel and (they) said ..”. In such case, the zero pronoun detection model is often incorrect. We are considering adding a generic they detection component. We also intend to experiment with introducing more sophisticated antecedent identification models in the ILP framework. In this paper, we used a very basic pairwise classifier; however Yang et al. (2008) and Iida et al. (2003) showed that the relative comparison of two candidate antecedents leads to obtaining better accuracy than the pairwise model. However, these approaches do not output absolute probabilities, but relative significance between two candidates, and therefore cannot be directly integrated with the ILP-framework. We plan to examine ways of appropriately estimating an absolute score from a set of relative scores for further refinement. Finally, we would like to test our model with English constructions which closely resemble zero anaphora. One example were studied in the Semeval"
P11-1081,S10-1001,1,\N,Missing
P19-1077,D12-1133,0,0.0191367,"ropy loss. During prediction, mentions with a score above the threshold (t) are returned. The threshold can be adjusted to create models for different purposes. In particular, in this paper we experimented with two models: one optimized for high recall, the other for high F1. We use the same network parameters as Lample et al. (2016) except the two parameters introduced by our system. We set maximum mention width to 30 i.e. l = 30, and set t = 0.5/0.95 for our highrecall and high-F1 versions respectively. pipeline Our DEP pipeline first parses input sentences using the Mate dependency parser (Bohnet and Nivre, 2012), then applies a rule based mention extractor. Our extractor follows a three steps approach. It first extracts mention heads using heuristic patterns based on part-of-speech tags and dependency relations. The patterns are automatically extracted from the gold annotation of the Phrase Detectives 1.0 corpus (Chamberlain et al., 2016). We extract all the part-of-speech tags and dependency relations pairs of the mentions’ head in the corpus, and use the most frequent patterns. The second step finds the maximum span related to a given mention head; for this we use the left/right-most direct or indi"
P19-1077,W11-1916,0,0.0241353,"2004), such as ONTONOTES (Pradhan et al., 2012) ARRAU (Uryupina et al., 2019) and Phrase Detectives 1.0 (Chamberlain et al., 2016). According to this definition, candidate mentions include all noun phrases (NPs) and all possessive pronouns. Non-referring NPs (like It in It is sunny 2 One difference between the mention detectors used for coreference resolvers and those used to preprocess data for coreference annotation is relevant for subsequent discussion. The former usually aim for high recall and compromise on precision, placing more confidence/importance on the coreference resolution step (Kummerfeld et al., 2011) and being satisfied that incorrectly identified mentions will simply remain singletons which can be removed in post processing (Lee et al., 2011). The latter tend to go for high F. This difference played a role in our experiments, as discussed later. 798 Configuration OntoNotes Stanford or a policeman in John is a policeman) and singletons are considered candidate mentions as well, possibly to be filtered during coreference annotation proper. The maximal projection of the NP is marked; i.e., the full extent of the NP including premodifiers, post-modifiers and appositions. In the case of a coo"
P19-1077,N16-1030,0,0.0811371,": DEP High F1 NN High Recall News Stanford NN DEP High F1 High Recall Other Domains Stanford NN NN [[Alice]i and [Bob]j ]k went to the shops. [They]k had a coffee. 3 Two automated mention detectors DEP NN The first ingredient of our proposal is two strong mention detectorsto serve both as baselines and as AI opponents for TileAttack.3 The first pipeline first parses the input sentences using a dependency parser and then extracts mentions from the dependency parse; we call this the DEP pipeline. The second pipeline is a modified version of the neural named entity recognition system proposed by Lample et al. (2016); we call it NN pipeline. Both pipelines are trained on the Penn Treebank (PTB). 3.1 DEP NN NN R F1 40.38 36.60 73.53 51.53 89.46 83.79 74.01 87.53 55.65 50.95 73.77 64.87 71.55 86.03 79.33 71.65 67.28 72.33 86.16 91.29 69.35 78.59 82.60 80.29 77.52 84.72 79.92 73.35 80.11 81.78 87.48 93.04 78.79 83.22 83.53 82.03 Table 1: Mention detectors comparison. Lample et al. (2016). This takes a sentence as the input and outputs a sequence of IOB style NER labels. The system uses a bidirectional LSTM to encode sentences and applies a sequential conditional random layer (CRF) over the output of the LSTM"
P19-1077,L16-1323,1,0.931151,"er is a novel method to use aggregation with potentially nested markables. We show that using this method to aggregate mention detector labels and TileAttack labels results in improved markable boundary quality. 2 Markables for coreference Different coreference corpora adopt different definitions of markable (Poesio et al., 2016; Uryupina and Zanoli, 2016). The definition of (candidate) mention used in this paper is broadly speaking that adopted in corpora based on the MATE scheme (Poesio, 2004), such as ONTONOTES (Pradhan et al., 2012) ARRAU (Uryupina et al., 2019) and Phrase Detectives 1.0 (Chamberlain et al., 2016). According to this definition, candidate mentions include all noun phrases (NPs) and all possessive pronouns. Non-referring NPs (like It in It is sunny 2 One difference between the mention detectors used for coreference resolvers and those used to preprocess data for coreference annotation is relevant for subsequent discussion. The former usually aim for high recall and compromise on precision, placing more confidence/importance on the coreference resolution step (Kummerfeld et al., 2011) and being satisfied that incorrectly identified mentions will simply remain singletons which can be remov"
P19-1077,W10-0712,0,0.0345275,"7–807 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics domain: the difference in performance between running coreference resolvers on gold mentions and running them on system mentions can be of up to 20 percentage points, and the results are even poorer when running such systems out-of-domain, for domains like biomedicine (Kim et al., 2011) or for under-resourced languages (Soraluze et al., 2012). So a manual checking step is still required to obtain high-quality results.2 Markable checking is increasingly done using crowdsourcing (Snow et al., 2008; Lawson et al., 2010; Bontcheva et al., 2017). But crowdsourcing, using microtask platforms such as Amazon Mechanical Turk can be too expensive for large scale annotation. For these cases, gamification tends to be a cheaper alternative (Poesio et al., 2013), also providing more accurate results and better contributor engagement (Lee et al., 2013). The second contribution of this paper is an approach to mention detection for large-scale coreference annotation projects in which the output of mention detectors is corrected using a Gamewith-a-Purpose (GWAP) (Von Ahn and Dabbish, 2008). A Game-With-A-Purpose is a game"
P19-1077,W11-1902,0,0.0488902,"inition, candidate mentions include all noun phrases (NPs) and all possessive pronouns. Non-referring NPs (like It in It is sunny 2 One difference between the mention detectors used for coreference resolvers and those used to preprocess data for coreference annotation is relevant for subsequent discussion. The former usually aim for high recall and compromise on precision, placing more confidence/importance on the coreference resolution step (Kummerfeld et al., 2011) and being satisfied that incorrectly identified mentions will simply remain singletons which can be removed in post processing (Lee et al., 2011). The latter tend to go for high F. This difference played a role in our experiments, as discussed later. 798 Configuration OntoNotes Stanford or a policeman in John is a policeman) and singletons are considered candidate mentions as well, possibly to be filtered during coreference annotation proper. The maximal projection of the NP is marked; i.e., the full extent of the NP including premodifiers, post-modifiers and appositions. In the case of a coordinated NP such as Alice and Bob, each conjunct and the coordinated NP are treated as candidate mentions: DEP High F1 NN High Recall News Stanfor"
P19-1077,felt-etal-2014-momresp,0,0.0208937,"put. The game supports any text segmentation task, whether markables are nested or non-nested, aligned or not aligned, and is therefore applicable at least in principle to a variety of text annotation tasks besides coreference, including e.g., Named Entity Resolution (NER). Key to this result is the use of a novel aggregation method to combine the labels produced by the mention detector with the labels collected using the game. A number of aggregation methods applicable to text segmentation labelling have been proposed (Dawid and Skene, 1979; Hovy et al., 2013; Passonneau and Carpenter, 2014; Felt et al., 2014; Rodrigues et al., 2014; Nguyen et al., 2017; Paun et al., 2018), but they are not directly applicable when markables can be nested. The third contribution of this paper is a novel method to use aggregation with potentially nested markables. We show that using this method to aggregate mention detector labels and TileAttack labels results in improved markable boundary quality. 2 Markables for coreference Different coreference corpora adopt different definitions of markable (Poesio et al., 2016; Uryupina and Zanoli, 2016). The definition of (candidate) mention used in this paper is broadly spea"
P19-1077,N18-2108,0,0.031995,"task of identifying the text segments to be annotated–the markables–from the annotation task proper. In our specific case, the markables of interest are the mentions used in coreference resolution, to be labelled as belonging to a coreference chain or as singletons; typical examples of mentions are pronouns, named entities, and other nominal phrases (Poesio et al., 2016). 1 Note that in many of the most recent systems mention detection is carried out as a joint inference task with coreference resolution (Peng et al., 2015)–e.g., by the current top performing system on the CONLL 2012 dataset, (Lee et al., 2018). These approaches generally result in better performance at coreference resolution, but not necessarily at mention detection. And even end-to-end systems require mention-annotated corpora for training and testing of course. 797 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 797–807 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics domain: the difference in performance between running coreference resolvers on gold mentions and running them on system mentions can be of up to 20 percentage points, and the r"
P19-1077,H05-1048,0,0.0769144,"berlain University of Essex jchamb@essex.ac.uk [A wolf]i had been gorging on [an animal [he]i had killed]j The methods proposed in this paper are also applicable when markables are nested. Mention identification for annotation is typically done semi-automatically, using first an automatic mention detector (or extractor) (Uryupina and Zanoli, 2016) and then checking its output by hand. Automatic mention detectors developed for coreference systems are generally used in the first step. Mention detection was recognized early on as a key step for overall coreference quality (Stoyanov et al., 2009; Hacioglu et al., 2005; Zhekova and K¨ubler, 2010; Uryupina and Zanoli, 2016), so a number of good quality mention detectors were developed, such as the mention detector included in the Stanford CORE pipeline (Manning et al., 2014), used by many of the top-performing systems in the 2012 CONLL Shared Task (Pradhan et al., 2012).1 But this performance can be improved. The first contribution of this paper are new fully-trainable, language-independent mention detectors that outperform the Stanford CORE mention detector in a variety of genres. But even the best automatic mention detectors do not achieve the accuracy req"
P19-1077,N13-1132,0,0.0247454,"n substantial improvement to the quality of the output. The game supports any text segmentation task, whether markables are nested or non-nested, aligned or not aligned, and is therefore applicable at least in principle to a variety of text annotation tasks besides coreference, including e.g., Named Entity Resolution (NER). Key to this result is the use of a novel aggregation method to combine the labels produced by the mention detector with the labels collected using the game. A number of aggregation methods applicable to text segmentation labelling have been proposed (Dawid and Skene, 1979; Hovy et al., 2013; Passonneau and Carpenter, 2014; Felt et al., 2014; Rodrigues et al., 2014; Nguyen et al., 2017; Paun et al., 2018), but they are not directly applicable when markables can be nested. The third contribution of this paper is a novel method to use aggregation with potentially nested markables. We show that using this method to aggregate mention detector labels and TileAttack labels results in improved markable boundary quality. 2 Markables for coreference Different coreference corpora adopt different definitions of markable (Poesio et al., 2016; Uryupina and Zanoli, 2016). The definition of (ca"
P19-1077,Q14-1035,0,0.0230968,"ms (Cooper et al., 2010). However, so far there have not been any truly successful GWAPs for NLP. It has proven difficult to go from simple gamification of a labelling task to developing a proper game: e.g., in one of the best-known GWAPs for NLP, Phrase Detectives (Poesio et al., 2013), the labelling remains the core of the game dynamics. Yet, games such as Puzzle Racer have shown that engaging GWAPs producing annotations for text are possible. Furthermore, that the annotations thus collected are of a quality comparable to that obtainable using microtask crowdsourcing, and at a reduced cost (Jurgens and Navigli, 2014). However, such games have yet to achieve the player uptake or number of judgements comparable to GWAPs in other domains. Furthermore, it is not clear yet whether using GWAPs can result in better performance for tasks such as mention detection, for which good-performance systems exist. In this work, automatically extracted mentions are checked using a two-player GWAP, TileAttack. Our previous analysis of the performance of TileAttack using player satisfaction metrics derived from the Free 2 Play literature suggests that we are succeeding in developing an engaging game (Madge et al., 2017). In"
P19-1077,P14-5010,0,0.0108755,"on for annotation is typically done semi-automatically, using first an automatic mention detector (or extractor) (Uryupina and Zanoli, 2016) and then checking its output by hand. Automatic mention detectors developed for coreference systems are generally used in the first step. Mention detection was recognized early on as a key step for overall coreference quality (Stoyanov et al., 2009; Hacioglu et al., 2005; Zhekova and K¨ubler, 2010; Uryupina and Zanoli, 2016), so a number of good quality mention detectors were developed, such as the mention detector included in the Stanford CORE pipeline (Manning et al., 2014), used by many of the top-performing systems in the 2012 CONLL Shared Task (Pradhan et al., 2012).1 But this performance can be improved. The first contribution of this paper are new fully-trainable, language-independent mention detectors that outperform the Stanford CORE mention detector in a variety of genres. But even the best automatic mention detectors do not achieve the accuracy required for high-quality corpus annotation, even when run inIntroduction Developing Natural Language Processing (NLP) systems still requires large amounts of annotated text to train models, or as a gold standard"
P19-1077,J93-2004,0,0.0687638,"t this performance can be improved. The first contribution of this paper are new fully-trainable, language-independent mention detectors that outperform the Stanford CORE mention detector in a variety of genres. But even the best automatic mention detectors do not achieve the accuracy required for high-quality corpus annotation, even when run inIntroduction Developing Natural Language Processing (NLP) systems still requires large amounts of annotated text to train models, or as a gold standard to test the effectiveness of such models. The approach followed to create the most widely used data (Marcus et al., 1993; Palmer et al., 2005; Pradhan et al., 2012) is to separate the task of identifying the text segments to be annotated–the markables–from the annotation task proper. In our specific case, the markables of interest are the mentions used in coreference resolution, to be labelled as belonging to a coreference chain or as singletons; typical examples of mentions are pronouns, named entities, and other nominal phrases (Poesio et al., 2016). 1 Note that in many of the most recent systems mention detection is carried out as a joint inference task with coreference resolution (Peng et al., 2015)–e.g., b"
P19-1077,D08-1027,0,0.357367,"Missing"
P19-1077,P17-1028,0,0.0960143,"n task, whether markables are nested or non-nested, aligned or not aligned, and is therefore applicable at least in principle to a variety of text annotation tasks besides coreference, including e.g., Named Entity Resolution (NER). Key to this result is the use of a novel aggregation method to combine the labels produced by the mention detector with the labels collected using the game. A number of aggregation methods applicable to text segmentation labelling have been proposed (Dawid and Skene, 1979; Hovy et al., 2013; Passonneau and Carpenter, 2014; Felt et al., 2014; Rodrigues et al., 2014; Nguyen et al., 2017; Paun et al., 2018), but they are not directly applicable when markables can be nested. The third contribution of this paper is a novel method to use aggregation with potentially nested markables. We show that using this method to aggregate mention detector labels and TileAttack labels results in improved markable boundary quality. 2 Markables for coreference Different coreference corpora adopt different definitions of markable (Poesio et al., 2016; Uryupina and Zanoli, 2016). The definition of (candidate) mention used in this paper is broadly speaking that adopted in corpora based on the MAT"
P19-1077,J05-1004,0,0.0249051,"n be improved. The first contribution of this paper are new fully-trainable, language-independent mention detectors that outperform the Stanford CORE mention detector in a variety of genres. But even the best automatic mention detectors do not achieve the accuracy required for high-quality corpus annotation, even when run inIntroduction Developing Natural Language Processing (NLP) systems still requires large amounts of annotated text to train models, or as a gold standard to test the effectiveness of such models. The approach followed to create the most widely used data (Marcus et al., 1993; Palmer et al., 2005; Pradhan et al., 2012) is to separate the task of identifying the text segments to be annotated–the markables–from the annotation task proper. In our specific case, the markables of interest are the mentions used in coreference resolution, to be labelled as belonging to a coreference chain or as singletons; typical examples of mentions are pronouns, named entities, and other nominal phrases (Poesio et al., 2016). 1 Note that in many of the most recent systems mention detection is carried out as a joint inference task with coreference resolution (Peng et al., 2015)–e.g., by the current top per"
P19-1077,Q14-1025,0,0.143896,"vement to the quality of the output. The game supports any text segmentation task, whether markables are nested or non-nested, aligned or not aligned, and is therefore applicable at least in principle to a variety of text annotation tasks besides coreference, including e.g., Named Entity Resolution (NER). Key to this result is the use of a novel aggregation method to combine the labels produced by the mention detector with the labels collected using the game. A number of aggregation methods applicable to text segmentation labelling have been proposed (Dawid and Skene, 1979; Hovy et al., 2013; Passonneau and Carpenter, 2014; Felt et al., 2014; Rodrigues et al., 2014; Nguyen et al., 2017; Paun et al., 2018), but they are not directly applicable when markables can be nested. The third contribution of this paper is a novel method to use aggregation with potentially nested markables. We show that using this method to aggregate mention detector labels and TileAttack labels results in improved markable boundary quality. 2 Markables for coreference Different coreference corpora adopt different definitions of markable (Poesio et al., 2016; Uryupina and Zanoli, 2016). The definition of (candidate) mention used in this pa"
P19-1077,P09-1074,0,0.0481044,"n scenarios. 1 Jon Chamberlain University of Essex jchamb@essex.ac.uk [A wolf]i had been gorging on [an animal [he]i had killed]j The methods proposed in this paper are also applicable when markables are nested. Mention identification for annotation is typically done semi-automatically, using first an automatic mention detector (or extractor) (Uryupina and Zanoli, 2016) and then checking its output by hand. Automatic mention detectors developed for coreference systems are generally used in the first step. Mention detection was recognized early on as a key step for overall coreference quality (Stoyanov et al., 2009; Hacioglu et al., 2005; Zhekova and K¨ubler, 2010; Uryupina and Zanoli, 2016), so a number of good quality mention detectors were developed, such as the mention detector included in the Stanford CORE pipeline (Manning et al., 2014), used by many of the top-performing systems in the 2012 CONLL Shared Task (Pradhan et al., 2012).1 But this performance can be improved. The first contribution of this paper are new fully-trainable, language-independent mention detectors that outperform the Stanford CORE mention detector in a variety of genres. But even the best automatic mention detectors do not a"
P19-1077,Q18-1040,1,0.899888,"bles are nested or non-nested, aligned or not aligned, and is therefore applicable at least in principle to a variety of text annotation tasks besides coreference, including e.g., Named Entity Resolution (NER). Key to this result is the use of a novel aggregation method to combine the labels produced by the mention detector with the labels collected using the game. A number of aggregation methods applicable to text segmentation labelling have been proposed (Dawid and Skene, 1979; Hovy et al., 2013; Passonneau and Carpenter, 2014; Felt et al., 2014; Rodrigues et al., 2014; Nguyen et al., 2017; Paun et al., 2018), but they are not directly applicable when markables can be nested. The third contribution of this paper is a novel method to use aggregation with potentially nested markables. We show that using this method to aggregate mention detector labels and TileAttack labels results in improved markable boundary quality. 2 Markables for coreference Different coreference corpora adopt different definitions of markable (Poesio et al., 2016; Uryupina and Zanoli, 2016). The definition of (candidate) mention used in this paper is broadly speaking that adopted in corpora based on the MATE scheme (Poesio, 20"
P19-1077,K15-1002,0,0.0279283,"data (Marcus et al., 1993; Palmer et al., 2005; Pradhan et al., 2012) is to separate the task of identifying the text segments to be annotated–the markables–from the annotation task proper. In our specific case, the markables of interest are the mentions used in coreference resolution, to be labelled as belonging to a coreference chain or as singletons; typical examples of mentions are pronouns, named entities, and other nominal phrases (Poesio et al., 2016). 1 Note that in many of the most recent systems mention detection is carried out as a joint inference task with coreference resolution (Peng et al., 2015)–e.g., by the current top performing system on the CONLL 2012 dataset, (Lee et al., 2018). These approaches generally result in better performance at coreference resolution, but not necessarily at mention detection. And even end-to-end systems require mention-annotated corpora for training and testing of course. 797 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 797–807 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics domain: the difference in performance between running coreference resolvers on gold men"
P19-1077,N19-1176,1,0.835523,"82.523 87.344 84.865 Table 4: Results on the ‘Other Domains’ dataset (rounded to 3 dp) ing platforms such as Amazon Mechanical Turk is best to label small to medium size amounts of data in a short time, and for labelling data of no intrinsic interest. Whereas crowdsourcing with gameswith-a-purpose is best in cases when the objective is to collect very large amounts of labels, so that the initial costs for setting up the game can be offset by the reduced costs of labelling (Poesio et al., 2013). One example in point is the Phrase Detectives annotation effort. The latest release of these data (Poesio et al., 2019) contains 2.2M judgments, around 4 times the number of judgments collected for ONTONOTES. The approach to mention detection proposed in this paper was developed in support of games such as Phrase Detectives, thus a GWAP or at least gamified approach as exemplified by TileAttack was deemed more appropriate even if the judgments used in this paper were collected using Amazon Mechanical Turk for speed. About 5,000 sentences were annotated by regular (i.e., not paid) players in this initial development phase, but we expect the game will be able to collect a comparable amount of judgments as for Ph"
P19-1077,S10-1019,0,0.0718575,"Missing"
P19-1408,P16-1061,0,0.0772022,"broadcast news, broadcast conversation, telephone conversation, magazine, weblogs, and Bible genres while the annotated documents in WikiCoref are selected from Wikipedia. 6.2 Results Table 4 shows the maximum vs. minimum span evaluations of several recent coreference resolvers on the CoNLL-2012 test set and the WikiCoref dataset. The examined coreference resolvers are as follows: the Stanford rule-based system (Lee et al., 2013), the coreference resolver of Peng et al. (2015), the ranking model of cort (Martschat and Strube, 2015), the ranking and reinforcement learning models of deep-coref (Clark and Manning, 2016a,b), the single and ensemble models of Lee et al. (2017), and the current stateof-the-art system by Lee et al. (2018). We make the following observations based on the results of Table 4: Using minimum spans in coreference evaluation strongly affects the comparisons in the cross-dataset setting. The results on the WikiCoref dataset show that mention boundary detection errors specifically affect coreference scores in cross-dataset evaluations. The ranking of systems is very different by using maximum vs. minimum spans. The reinforcement learning model of deep-coref, i.e., deep-coref RL, has the"
P19-1408,D16-1245,0,0.0305313,"Missing"
P19-1408,L16-1021,0,0.0159792,"the CoNLL score (Pradhan et al., 2014), i.e. the average F1 value of MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAF e (Luo, 2005), and the LEA F1 (Moosavi and Strube, 2016) score.10 Minimum spans are detected using both MINA and Collins’ head finding rules. All examined coreference resolvers are trained on the CoNLL-2012 training data. For indomain evaluations, models are evaluated on the CoNLL-2012 test data and minimum spans are extracted using gold parse trees, which are provided in CoNLL-2012.11 For cross-dataset evaluations, models are tested on the WikiCoref dataset (Ghaddar and Langlais, 2016). For extracting minimum spans, we parse WikiCoref by the Stanford neural parser. This dataset is annotated using the same annotation guidelines as that of CoNLL-2012, however, it contains documents from a different domain. 10 We use the python implementation that is available at https://github.com/ns-moosavi/coval. 11 We also examined the in-domain results of Table 4 based on the system parse trees of CoNLL-2012 instead of gold parse trees. The differences between scores based on MINA spans that are extracted from gold vs. those that are extracted from system parse trees were only about 0.2 p"
P19-1408,D17-1018,0,0.169234,"annotated using the same annotation guidelines as that of CoNLL-2012, however, it contains documents from a different domain. 10 We use the python implementation that is available at https://github.com/ns-moosavi/coval. 11 We also examined the in-domain results of Table 4 based on the system parse trees of CoNLL-2012 instead of gold parse trees. The differences between scores based on MINA spans that are extracted from gold vs. those that are extracted from system parse trees were only about 0.2 points. 4173 max CoNLL MINA Stanford rule-based cort Peng et al. deep-coref ranking deep-coref RL Lee et al. 2017 single Lee et al. 2017 ensemble Lee et al. 2018 55.60 (8) 63.03 (7) 63.05 (6) 65.59 (5) 65.81 (4) 67.23 (3) 68.87 (2) 72.96 (1) 57.55 (8) 64.60 (6) 63.50 (7) 67.29 (5) 67.50 (4) 68.55 (3) 70.12 (2) 74.26 (1) Stanford rule-based deep-coref ranking deep-coref RL Lee et al. 2017 single Lee et al. 2017 ensemble Lee et al. 2018 51.78 (4) 52.90 (3) 50.73 (5) 50.38 (6) 53.63 (2) 57.89 (1) 53.79 (5) 55.16 (2) 54.26 (4) 52.16 (6) 55.03 (3) 59.90 (1) head max CoNLL-2012 test set 57.38 (8) 47.31 (8) 64.51 (6) 56.10 (6) 63.54 (7) 55.22 (7) 67.09 (5) 59.58 (5) 67.36 (4) 59.76 (4) 68.53 (3) 61.24 (3) 70.05"
P19-1408,N18-2108,0,0.0860512,"Missing"
P19-1408,H05-1004,0,0.0716784,"l parser, makes MINA spans, as well as detected heads, more consistent compared to MIN spans. In addition to the above two properties, i.e. the length of minimum spans and their consistency with MIN annotations, we also check that MINA Experimental Setup In this section, we investigate how the use of minimum spans instead of maximum spans in coreference evaluation affects the results in in-domain as well as cross-dataset evaluations. For comparisons, we use the CoNLL score (Pradhan et al., 2014), i.e. the average F1 value of MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAF e (Luo, 2005), and the LEA F1 (Moosavi and Strube, 2016) score.10 Minimum spans are detected using both MINA and Collins’ head finding rules. All examined coreference resolvers are trained on the CoNLL-2012 training data. For indomain evaluations, models are evaluated on the CoNLL-2012 test data and minimum spans are extracted using gold parse trees, which are provided in CoNLL-2012.11 For cross-dataset evaluations, models are tested on the WikiCoref dataset (Ghaddar and Langlais, 2016). For extracting minimum spans, we parse WikiCoref by the Stanford neural parser. This dataset is annotated using the same"
P19-1408,Q15-1029,1,0.833242,"ferent based on maximum vs. MINA spans are highlighted. CoNLL-2012 contains the newswire, broadcast news, broadcast conversation, telephone conversation, magazine, weblogs, and Bible genres while the annotated documents in WikiCoref are selected from Wikipedia. 6.2 Results Table 4 shows the maximum vs. minimum span evaluations of several recent coreference resolvers on the CoNLL-2012 test set and the WikiCoref dataset. The examined coreference resolvers are as follows: the Stanford rule-based system (Lee et al., 2013), the coreference resolver of Peng et al. (2015), the ranking model of cort (Martschat and Strube, 2015), the ranking and reinforcement learning models of deep-coref (Clark and Manning, 2016a,b), the single and ensemble models of Lee et al. (2017), and the current stateof-the-art system by Lee et al. (2018). We make the following observations based on the results of Table 4: Using minimum spans in coreference evaluation strongly affects the comparisons in the cross-dataset setting. The results on the WikiCoref dataset show that mention boundary detection errors specifically affect coreference scores in cross-dataset evaluations. The ranking of systems is very different by using maximum vs. minim"
P19-1408,P16-1060,1,0.892913,"as well as detected heads, more consistent compared to MIN spans. In addition to the above two properties, i.e. the length of minimum spans and their consistency with MIN annotations, we also check that MINA Experimental Setup In this section, we investigate how the use of minimum spans instead of maximum spans in coreference evaluation affects the results in in-domain as well as cross-dataset evaluations. For comparisons, we use the CoNLL score (Pradhan et al., 2014), i.e. the average F1 value of MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAF e (Luo, 2005), and the LEA F1 (Moosavi and Strube, 2016) score.10 Minimum spans are detected using both MINA and Collins’ head finding rules. All examined coreference resolvers are trained on the CoNLL-2012 training data. For indomain evaluations, models are evaluated on the CoNLL-2012 test data and minimum spans are extracted using gold parse trees, which are provided in CoNLL-2012.11 For cross-dataset evaluations, models are tested on the WikiCoref dataset (Ghaddar and Langlais, 2016). For extracting minimum spans, we parse WikiCoref by the Stanford neural parser. This dataset is annotated using the same annotation guidelines as that of CoNLL-201"
P19-1408,P17-2003,1,0.701581,"ed on our analyses, MINA spans are compatible with those that are manually annotated by experts. By using MINA, we can benefit from minimum span evaluation for all corpora without introducing additional annotation costs. While the use of MINA spans already benefits in-domain evaluation, by reducing the gap between the performance on gold vs. system mentions, it has a more significant impact on crossdataset evaluation, in which detected maximum mention boundaries are noisier due to domain shift. Cross-dataset coreference evaluation is used to assess the generalization of coreference resolvers (Moosavi and Strube, 2017, 2018). Coreference resolution is a mid-step for text understanding in downstream tasks, e.g., question answering, text summarization, and information retrieval. Therefore, generalization is an important property for coreference resolvers because downstream datasets are not necessarily from the same domain as those of coreference-annotated corpora. When coreference resolvers are applied to a new domain, detected maximum boundaries become noisier, e.g., gold and system mentions differ by 4168 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4168–41"
P19-1408,P03-1054,0,0.0953729,"contain their corresponding MIN annotations in MUC and ARRAU. MINA and head words are detected using the parse trees of the Stanford PCFG parser. flight attendants more than 10000 NP MUC-7 92.4 90.0 NP QP QP MUC-6 95.6 92.9 and QP NP 329 million in NP cash marketable securities Figure 12: The system parse trees of two mentions from ARRAU. MINA spans are boldfaced. “securities” and “cash” are annotated as MIN for the left and right mentions, respectively. In order to investigate the effect of using a different parser, we perform the experiment of Table 2 using the Stanford English PCFG parser (Klein and Manning, 2003). The results are reported in Table 3. As we see, the use of a better parser, i.e., the Stanford neural parser, makes MINA spans, as well as detected heads, more consistent compared to MIN spans. In addition to the above two properties, i.e. the length of minimum spans and their consistency with MIN annotations, we also check that MINA Experimental Setup In this section, we investigate how the use of minimum spans instead of maximum spans in coreference evaluation affects the results in in-domain as well as cross-dataset evaluations. For comparisons, we use the CoNLL score (Pradhan et al., 201"
P19-1408,D18-1018,1,0.829277,"Missing"
P19-1408,J13-4004,0,0.144027,"Coref. The ranking of corresponding scores is specified in parentheses. Rankings which are different based on maximum vs. MINA spans are highlighted. CoNLL-2012 contains the newswire, broadcast news, broadcast conversation, telephone conversation, magazine, weblogs, and Bible genres while the annotated documents in WikiCoref are selected from Wikipedia. 6.2 Results Table 4 shows the maximum vs. minimum span evaluations of several recent coreference resolvers on the CoNLL-2012 test set and the WikiCoref dataset. The examined coreference resolvers are as follows: the Stanford rule-based system (Lee et al., 2013), the coreference resolver of Peng et al. (2015), the ranking model of cort (Martschat and Strube, 2015), the ranking and reinforcement learning models of deep-coref (Clark and Manning, 2016a,b), the single and ensemble models of Lee et al. (2017), and the current stateof-the-art system by Lee et al. (2018). We make the following observations based on the results of Table 4: Using minimum spans in coreference evaluation strongly affects the comparisons in the cross-dataset setting. The results on the WikiCoref dataset show that mention boundary detection errors specifically affect coreference"
P19-1408,K15-1002,0,0.286272,"Example 1. S VP NP NP This News Corp. has PP NP NP an extensive presence in PP , of NP Background NP this country course Figure 2: System parse tree of Example 1. A system that uses the system parse tree for mention detection links “that presence” to “an extensive presence, of course in this country” and gets penalized based on recall and precision. This penalty is the same as that of a system that links “that presence” to “this News Corp.”. Recall drops because of not recognizing “an extensive presence” and precision drops because of detecting a spurious mention. The coreference resolver of Peng et al. (2015) is developed around the idea that working with mention heads is more robust compared to working with maximum mention boundaries. In this regard, they develop a system that resolves coreference relations based on mention heads. The resolved mention heads are then expanded to full mention boundaries using a separate classifier that is trained to do so. Peng et al. (2015) also report the evaluation scores using both maximum mention boundaries and mention heads. Peng et al. (2015) extract mention heads using Collins’ head finder rules (Collins, 1999). They use gold 1 http://www-nlpir.nist.gov/rel"
P19-1408,W12-4501,0,0.0382966,"ngle mention. In order to decouple coreference evaluation from maximum boundary detection complexities, smaller corpora like MUC (Hirschman and Chinchor, 1997), ACE (Mitchell et al., 2002), and ARRAU (Uryupina et al., 2016) explicitly annotate the minimum span as well as the maximum logical span of each mention. The annotated minimum spans indicate the minimum strings that a coreference resolver must identify for the corresponding mentions. This solution comes with an additional annotation cost. As a result, the annotation of minimum spans has been discarded in larger corpora like CoNLL-2012 (Pradhan et al., 2012). In this paper, we propose MINA, a MINimum span extraction Algorithm that automatically determines minimum spans from constituency-based parse trees. Based on our analyses, MINA spans are compatible with those that are manually annotated by experts. By using MINA, we can benefit from minimum span evaluation for all corpora without introducing additional annotation costs. While the use of MINA spans already benefits in-domain evaluation, by reducing the gap between the performance on gold vs. system mentions, it has a more significant impact on crossdataset evaluation, in which detected maximu"
P19-1408,P13-1045,0,0.0156155,"ns, we use the provided parse trees in the key file.7 7 If the key file does not include parse information, we parse it with the Stanford parser. For the experiments of this section, we use the MUC-6, MUC-7, ARRAU, and CoNLL-2012 8 If the boundary of a mention is not recognized as a single phrase in the parse tree, as it is the case for the system mention, we add a dummy root (“X” in the right subtree of Figure 7) to include the whole span into a single phrase. 4171 ·104 corpora, from which MUC and ARRAU contain manually annotated minimum spans. We use the Stanford neural constituency parser (Socher et al., 2013) for getting system parse trees, unless otherwise stated. For the ARRAU corpus, we use mentions of the training split of the RST Discourse Treebank subpart. As a baseline, we also evaluate the syntactic head of mentions, based on Collins’ rules, as the minimum span.9 max-span min-span Count 1 0.5 0 How does the length of evaluated spans change by using MINA? Table 1 shows the average length of maximum spans vs. that of MINA spans on the training splits of the MUC-6, MUC-7 and ARRAU corpora as well as the development set of the CoNLL-2012 dataset. For the CoNLL-2012 dataset, we use the provided"
P19-1408,L16-1326,1,0.86508,"s is to specify the largest span of each mention. The problem with using maximum spans in coreference evaluation is that a single mention may have different maximum boundaries based on gold vs. automatically detected syntactic structures. For instance, variations in prepositional phrase attachment, which is a known challenge in syntactic parsing, will lead to different maximum boundaries for a single mention. In order to decouple coreference evaluation from maximum boundary detection complexities, smaller corpora like MUC (Hirschman and Chinchor, 1997), ACE (Mitchell et al., 2002), and ARRAU (Uryupina et al., 2016) explicitly annotate the minimum span as well as the maximum logical span of each mention. The annotated minimum spans indicate the minimum strings that a coreference resolver must identify for the corresponding mentions. This solution comes with an additional annotation cost. As a result, the annotation of minimum spans has been discarded in larger corpora like CoNLL-2012 (Pradhan et al., 2012). In this paper, we propose MINA, a MINimum span extraction Algorithm that automatically determines minimum spans from constituency-based parse trees. Based on our analyses, MINA spans are compatible wi"
P19-1408,M95-1005,0,0.661987,"As we see, the use of a better parser, i.e., the Stanford neural parser, makes MINA spans, as well as detected heads, more consistent compared to MIN spans. In addition to the above two properties, i.e. the length of minimum spans and their consistency with MIN annotations, we also check that MINA Experimental Setup In this section, we investigate how the use of minimum spans instead of maximum spans in coreference evaluation affects the results in in-domain as well as cross-dataset evaluations. For comparisons, we use the CoNLL score (Pradhan et al., 2014), i.e. the average F1 value of MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAF e (Luo, 2005), and the LEA F1 (Moosavi and Strube, 2016) score.10 Minimum spans are detected using both MINA and Collins’ head finding rules. All examined coreference resolvers are trained on the CoNLL-2012 training data. For indomain evaluations, models are evaluated on the CoNLL-2012 test data and minimum spans are extracted using gold parse trees, which are provided in CoNLL-2012.11 For cross-dataset evaluations, models are tested on the WikiCoref dataset (Ghaddar and Langlais, 2016). For extracting minimum spans, we parse WikiCoref by the Stanford n"
P19-1408,J03-4003,0,\N,Missing
P19-1408,P14-2006,1,\N,Missing
P93-1010,P86-1031,1,0.957403,"he choice of the discourse antecedent of a temporal operator is subject to centering effects. We assume that each temporal operator in a sentence introduces a discourse reference time into the discourse context. We claim that this set of times constitutes a list of potential discourse reference times for the next sentence, which we'll later refer to as the temporal forwardlooking center (TCf), and that the position of a temporal operator in the logical form of the sentence affects the choice of the antecedent through structural parallelism (as a case of the propertysharing effect in centering [16]). We formalize the effect of surface structure on the choice of temporal antecedent by means of defeasible axioms. These axioms must be less specific than axioms encoding causal reasoning. We argue that the choice of discourse reference time is an instance of a general principle in defeasible reasoning, namely, the Penguin Principle [19] that chooses the most specific axiom applicable. We support our claims with data from the Brown corpus. In the next section, we review the three existing proposals most related to ours - - Webber [27], Lascarides and Oberlander [19], and Hwang and Schubert [1"
P93-1010,J88-2005,1,0.852327,"o Poesio D e p t . of C o m p u t e r S c i e n c e U n i v e r s i t y of R o c h e s t e r R o c h e s t e r , N Y 14627-0226 poesio©cs.rochester.edu (2)a. John went over (el) to Mary's house. b. On the way, he had (t2) stopped (t3) by the flower shop for some roses. (t3 -~ t2 ( = t l ) ) c. Unfortunately, they failed (t4) to cheer her up. (t3 -~ t l -~ t4) c'. He picked out (t4') 5 red ones, 3 white ones and 1 pale pink. (t3 -&lt; t4' -&lt; tl) (2c) and (2c') are alternative third sentences. Although both are in the simple past, and both evoke events of the same aspectual type (transition event [23]), they are interpreted differently. We refer to the contextually established time that a past tense is resolved against as the &quot;discourse reference time.&quot; A discourse reference time (tl) is introduced in (2a) with the event of John going to Mary's house at t l ) The past perfect in (2b) introduces two times: John's stopping at the flower shop (t3) precedes the time t2 (t3 -~ t2), and t2 is typically inferred to be equal to the time of going over to Mary's house (tl); hence t3 ~ tl. In (2c), the time of failing to cheer Mary (t4) is inferred to occur just after tl, whereas in the parallel vers"
P93-1010,J90-3001,0,0.0234575,"have been distinguished in centering. In this paper, we will use the following four types: Cb-retention, Cb-establishment, Cb-resumption, and NULL-transition. z We assume the following general picture of discourse processing. A discourse consists of a sequence of utterances uttl,..., uttn. The sentence grammar translates the content of each utterance utti into a (set of) surface logical form(s) containing unresolved anaphoric expressions and operators. We call it here a &quot;surface&quot; formula ¢i. This logical form is similar, in spirit, to Hwang and Schubert's [14] indexical formula and Alshawi's [2] quasi logicalform, whose main motivations are to represent that part of the sentence meaning independent from the particular discourse context. This &quot;baseline&quot; meaning representation acts as a clean interface to the pragmatic processing needed to resolve context-dependent expressions. Utterance interpretation takes place in a context, and outputs an updated context. Part of this dynamic context is the attentional state that represents the currently salient entities partially ordered by relative salience. In Cb-retention, the same entity is retained as the Cb: Cbi-1 = Cbi y£ NULL. In Cbestabli"
P93-1010,J88-2006,0,0.454969,"P a r k , C A 94025 megumi©ai.sri.com CENTERING Rebecca Passonneau D e p t . of C o m p u t e r S c i e n c e Columbia University N e w York, N Y 10027 becky¢cs.columbia.edu ically related, and if they are, what the relative order of the associated events is. The determinant factors have been argued to be discourse structure ([27] [14]), aspectual type ([61 [12] [17]), surface structure ([7] [14]), and commonsense knowledge ([19] [271 [13]). However, no account has adequately addressed all four factors. The problem in tense interpretation that we address is illustrated with Example (2) (from [27]). Abstract We present a semantic and pragmatic account of the anaphoric properties of past and perfect that improves on previous work by integrating discourse structure, aspectual type, surface structure and commonsense knowledge. A novel aspect of our account is that we distinguish between two kinds of temporal intervals in the interpretation of temporal operators - - discourse reference intervals and event intervals. This distinction makes it possible to develop an analogy between centering and temporal centering, which operates on discourse reference intervals. Our temporal propertysharing"
P93-1010,P87-1022,0,0.336244,"ered set of forward-looking centers Cfi comprising the entities realized in ¢i. A member of Cfi might (but need not) be the backward-looking center Cbi, the currently most salient entity. Centering has mainly been used to constrain how discourse anaphoric pronouns are processed; e.g., the centering rule [9] predicts that Cbl will be realized with a pronoun if Cbi=Cbi_l. 2 Also, when Cbi=Cbi-1 and both are realized by definite pronouns, it is predicted that both will be real3Cb-retention and Cb-establishment are due to Kameyama [15] [16]. These two roughly correspond to the three [10] and four [5] transition types proposed elsewhere. Cb-resumption captures Sidner's [26] use of a discourse focus stack in the potential focus list, and can be analogously formalized as a Cb stack within the Cf. NULL-transition has been implicit in Kameyama's work but has not been made an explicit transition type. 2Here we avoid the complication acknowledged in [11] that the two relevant utterances need not literally be adjacent. 72 course sequences from the Brown corpus [8], a heterogeneous corpus that should yield unbiased data. Each multi-sentence sequence contained one of two types of trigger sentences"
P93-1010,J88-2003,0,\N,Missing
P93-1010,P88-1012,0,\N,Missing
P93-1010,P83-1007,0,\N,Missing
P93-1010,J86-3001,0,\N,Missing
P93-1010,C69-7001,0,\N,Missing
P93-1010,C69-6902,0,\N,Missing
P93-1010,P92-1030,0,\N,Missing
P93-1011,P92-1005,0,0.452036,"r (in which the indefinite takes wide scope). The claim is that (3) is interpreted in the vaguest possible way, and the strongest reading, if at all, is derived by pragmatic 'strengthening' [25]. A difficulty with this approach is that a vaguest reading doesn't always exist. The two readings of (4), for example, are distinct. Because new ways of obtaining semantically distinct interpretations for sentences are continuously discovered, coming to grips with ambiguity is becoming more and more of a necessity for developers of natural language processing systems, linguists and psychologists alike [9, 31, 7, 2]. In this paper, I am concerned with the scopal ambiguity of operators I [31, 33]. The attention of both psycholinguists and computational linguists interested in ambiguity has concentrated on the problem of combinatorial explosion. If the number of readings of an utterance were to actually grow with the factorial of the number of operators, even a simple sentence like (1), with 4 operators (the modal 'should', tense, an indefinite and a definite), would have 4I = 24 scopally different readings. Two distinct questions thus must be answered: how can listeners (and how should machines) deal with"
P93-1011,P83-1009,0,0.0407445,"dure I present builds an event structure by identifying the situations associated with the operators in the sentence and their mutual dependency relations, as well as the relations between these situations and other situations in the context. The procedure takes into account lexical semantics and the result of various discourse interpretation procedures such as definite description interpretation, and does not require a complete disambiguation to take place. (2) You can fool most people on most of the issues most of the time, but you can't fool everybody on every single issue all of the time. [15] Another position is that sentences like (1) are not semantically ambiguous, but vague. Consider for example (3): (3) THE PROBLEM Here, one of the readings (the one in which the indefinite takes narrow scope) is entailed by the other (in which the indefinite takes wide scope). The claim is that (3) is interpreted in the vaguest possible way, and the strongest reading, if at all, is derived by pragmatic 'strengthening' [25]. A difficulty with this approach is that a vaguest reading doesn't always exist. The two readings of (4), for example, are distinct. Because new ways of obtaining semantical"
P93-1011,J87-1005,0,0.2432,"use the phrase structure system largely adopted in the Government and Binding literature, according to which the sentence is the maximal projection of an Infl node and is therefore labeled IP [34]. I also assume the existence of a maximal projection of complementizer CP above IP. Because I don't discuss relatives here, I use the following simplified notation for NPs with determiners, such as ""every school"": [NP '~- Q (V x [Sl ~ SCHOOL(x)]Q(x))] LFs like (21) are usually treated in the natural language processing literature as uninterpreted data structures from which to 'extract' the readings [16, 17]. However, it has been recently proposed [31, 2, 33] that it is possible (and indeed desirable) to assign a denotation to expressions like (21). The reason is that in this way one can define a notion of sound inference --that is, one can specify what can and cannot properly be inferred from an expression like (21) prior to disambiguation; and therefore, a notion of 'monotone disambiguation.' I do not assume disambiguation to work monotonically, but I want to be able to treat expressions like (21) as full-fledged conditions so that a DRS containing a condition of this kind can be interpreted, a"
P98-1090,J86-3001,0,0.457062,"look at, for instance, the bangle at the bottom- that's the blue and red o n e - what looks as though it's painted decoration is in fact inlaid; it's bits of cut-off razor-blade, biro, knitting needles, inlaid into layer after layer of resin, which is done in emulation of Japanese lacquer technique. 5. And that particular bangle took hhn something like 120 hours of work. All 7 long-distance pronouns in the ILEX dialogues we have studied refer to discourse entities introduced in background text in this way. Unlike Sidner's theory of focus (Sidner, 1979), the theory of the attentional state in (Grosz and Sidner, 1986) (henceforth: G&S) does not include explicit provision for long-distance pronominalisations, although some of the necessary tools are potentially already there, as we will see. The component of the theory that deals with pronominal reference, centering theory (Grosz et al., 1995), only accounts for cases in which the antecedent of a pronoun is introduced by the previous sentence; cases such as (1) have to be handled by different mechanisms. In this paper we look the phenomenon of long-distance pronominalisation in some detail, examining data from different domains, and consider 3 its implicati"
P98-1090,J95-2003,0,0.482295,"cquer technique. 5. And that particular bangle took hhn something like 120 hours of work. All 7 long-distance pronouns in the ILEX dialogues we have studied refer to discourse entities introduced in background text in this way. Unlike Sidner's theory of focus (Sidner, 1979), the theory of the attentional state in (Grosz and Sidner, 1986) (henceforth: G&S) does not include explicit provision for long-distance pronominalisations, although some of the necessary tools are potentially already there, as we will see. The component of the theory that deals with pronominal reference, centering theory (Grosz et al., 1995), only accounts for cases in which the antecedent of a pronoun is introduced by the previous sentence; cases such as (1) have to be handled by different mechanisms. In this paper we look the phenomenon of long-distance pronominalisation in some detail, examining data from different domains, and consider 3 its implications for G & S ' s theory. 2 Theories of focus Space unfortunately prevents a full discussion of Grosz's (1977), Sidner's (1979), and G & S ' s (1986) theories of focus and the attentional state in this abstract. The crucial aspects of these theories, for the purpose of the discus"
P98-1090,P85-1027,0,0.136646,"Missing"
P98-1090,P97-1014,0,0.262471,"e, or to an MSE, or to a discourse topic; rThis would explain the difference in reading times observed by (Clark and Sengul, 1979). 555 6. Definite descriptions can refer back to any entity in the global focus, including discourse topics. The reason for using the term 'optional' in 2 is that whereas focus spaces can always be described as being about something, they are not always associated with a 'most salient entity': e.g., the first sentence in (6) introduces several topics (woodwind players, their need to be creative, etc.) but does not introduce an MSE. 5 Related Work In a recent paper, Hahn and Strube (1997) propose to extend centering theory with what is, essentially, Sidner's stack of discourse foci, although their algorithm for identifying the ce is not identical to Sidner's. Their analysis of German texts shows a rather good performance for their algorithm, but, as only MSEs are predicted to be accessible, none of the anaphors depending on focus space information could be resolved. Their algorithm also appears to treat definite descriptions and pronouns uniformly as 'anaphors', which seems problematic in the light of psychological evidence showing that they behave differently, and examples li"
P98-1090,J93-4004,0,0.0797718,"Missing"
P98-1090,J96-3006,0,0.0470344,"theless, we believe that the structure depicted in Figure 1 is a plausible analysis for (1); an alternative analysis would be to take the 4th and 5th sentence as elaborations of and he lavished on those materials an incredibly painstaking technique . . . . but in this case, as well (and in all other rhetorical structures we could consider) sentences 4 and 5 are satellites of sentence 3. (We have employed the set of rhetorical relations currently used to analyse the ILEX data.) The relation between G&S's and RST's notion of structure has been analysed by, among others, (Moore and Paris, i 993; Moser and Moore, 1996). According to Moser and Moore, the relation can be characterised as follows: an RST nucleus expresses an intention I~; a satellite expresses an intention 18; and I,~ dominates Is. Thus, in (1), the nucleus of the exemplification relation, sentence 3, would dominate the satellite, consisting of sentences 4 and 5. We will make the same assumption here. Hence we can assume that the third sentence in (1) will still be on the stack when processing the 4th and 5th sentences. This would also hold for the alternative rhetorical structures we have considered. 5 4Fox, as well, used RST to analyse the s"
P98-1090,J98-2001,1,0.855632,"found that the preferred antecedent of a bridging description is a previous MSE in 54 out of 203 cases. In the SOLE COrpUS, 8 OUt of 11 bridging descriptions relate to the MSE. Does this mean, then, that we can get rid of focus spaces, and assume that it's MSEs that go on the stack? Before looking at the data, we have to be clear as to what would count as evidence one way or the other. Even an approach in which only previous MSES are on the stack would still allow access to entities which are part of what Grosz called the IMPLICIT FOCUS of these MSEs, i.e., the entities that 6As discussed in (Poesio and Vieira, 1998), in general there is more than one potential 'antecedent' for a bridging description in a text. 554 are 'strongly associated' with the MSES. This notion of 'strong association' is difficult to define- in fact, it is likely to be a matter of degree- but nevertheless it is plausible to assume that the objects 'strongly associated' with a discourse entity A do not include every discourse entity B which is part of a situation described in the text in which A is also involved; and this can be tested with linguistic examples, up to a point. For example, whereas definite descriptions like the radiat"
P98-1090,W97-1301,1,0.920711,"King. If, however, she becomes the 'main topic' of discussion, then later, whenever we talk about her again, we can use reduced forms of her proper name, such as King. Again, this difference is not easy to explain in terms of focus spaces if we assume that all objects in a focus space have the same status. A third class of expressions providing evidence relevant to this discussion are bridging descriptions, i.e., definite descriptions like the door that refer to an object associated with a previously mentioned discourse entity such as the house, rather than to the entity itself (Clark, 1977). Poesio et al. (1997; 1998) report experiments in which different types of lexical knowledge sources are used to resolve bridging descriptions and other cases of definite descriptions that require more than simple string match for their resolution. Their results indicate that to resolve bridging descriptions it is not sufficient simply to find which of the entities in the current focus space is semantically closest to the bridging description: in about half of the cases of bridging descriptions that could be resolved on the basis of the lexical knowledge used in these experiments, the focus spaces contained an en"
P98-1090,J96-2005,0,0.197179,"ys an entity explicitly introduced in the text, but can also be a more abstract DISCOURSE TOPIC, by which we 7Notice however that the claim that only MSES go on the stack does not entail that everything else in the text is simply forgotten- the claim is simply that that intbrmation is not available for resolving references anymore; presumably it would be stored somewhere in 'long term memory'. Conversely, the claim that everything stays on the stack would have to be supplemented by some story concerning how information gets forgotten-e.g., by some caching mechanism such as the one proposed by Walker (1996). mean an issue / proposition that can be said to characterise the content of the focus space as a whole. In a corpus analysis done in connection with (Poesio et al., 1997; Poesio et al., 1998), we found that 7 out of 70 inferential descriptions were of this type; in the SOLE corpus, in which 3 out of 11 bridging descriptions behave this way. An example of this use is the description the problem below, that refers to the problem introduced by the first sentence in the text: (6) Solo woodwind players have to be creative if they want to work a lot, because their repertoire and audience appeal ar"
poesio-2000-annotating,W98-1427,0,\N,Missing
poesio-2000-annotating,J98-2001,1,\N,Missing
poesio-2000-annotating,J96-1002,0,\N,Missing
poesio-2000-annotating,J95-2003,0,\N,Missing
poesio-2000-annotating,J96-2004,0,\N,Missing
poesio-2000-annotating,W99-0309,0,\N,Missing
poesio-2000-annotating,M98-1001,0,\N,Missing
poesio-artstein-2008-anaphoric,J98-2001,1,\N,Missing
poesio-artstein-2008-anaphoric,A00-2018,0,\N,Missing
poesio-artstein-2008-anaphoric,J93-2004,0,\N,Missing
poesio-artstein-2008-anaphoric,W01-1605,0,\N,Missing
poesio-artstein-2008-anaphoric,W04-0210,1,\N,Missing
poesio-artstein-2008-anaphoric,W05-0311,1,\N,Missing
poesio-artstein-2008-anaphoric,N06-2015,0,\N,Missing
poesio-artstein-2008-anaphoric,W04-2327,1,\N,Missing
poesio-artstein-2008-anaphoric,M98-1029,0,\N,Missing
poesio-artstein-2008-anaphoric,J06-4012,0,\N,Missing
poesio-etal-2002-acquiring,J98-2001,1,\N,Missing
poesio-etal-2002-acquiring,W97-1301,1,\N,Missing
poesio-etal-2002-acquiring,E99-1001,0,\N,Missing
poesio-etal-2002-acquiring,P00-1051,0,\N,Missing
poesio-etal-2002-acquiring,P00-1023,0,\N,Missing
poesio-etal-2002-acquiring,J93-2002,0,\N,Missing
poesio-etal-2002-acquiring,J92-4003,0,\N,Missing
poesio-etal-2002-acquiring,P93-1032,0,\N,Missing
poesio-etal-2002-acquiring,J94-4002,0,\N,Missing
poesio-etal-2002-acquiring,P97-1072,1,\N,Missing
poesio-etal-2002-acquiring,J01-4003,0,\N,Missing
poesio-etal-2002-acquiring,P99-1008,0,\N,Missing
poesio-etal-2002-acquiring,J00-4003,1,\N,Missing
poesio-etal-2002-acquiring,P98-2143,0,\N,Missing
poesio-etal-2002-acquiring,C98-2138,0,\N,Missing
poesio-etal-2008-anawiki,W05-0311,1,\N,Missing
poesio-etal-2008-anawiki,N06-2015,0,\N,Missing
poesio-etal-2008-anawiki,P02-1033,0,\N,Missing
poesio-etal-2008-anawiki,J05-1004,0,\N,Missing
poesio-etal-2008-anawiki,W04-2327,1,\N,Missing
poesio-etal-2008-anawiki,J06-4012,0,\N,Missing
poesio-etal-2010-babyexp,J07-2002,0,\N,Missing
poesio-etal-2010-babyexp,P98-2127,0,\N,Missing
poesio-etal-2010-babyexp,C98-2122,0,\N,Missing
poesio-etal-2010-creating,H05-1083,0,\N,Missing
poesio-etal-2010-creating,A00-1020,0,\N,Missing
poesio-etal-2010-creating,P95-1017,0,\N,Missing
poesio-etal-2010-creating,J01-4004,0,\N,Missing
poesio-etal-2010-creating,bosco-lombardo-2006-comparing,0,\N,Missing
poesio-etal-2010-creating,W98-1502,0,\N,Missing
poesio-etal-2010-creating,pianta-etal-2008-textpro,0,\N,Missing
poesio-kabadjov-2004-general,poesio-etal-2002-acquiring,1,\N,Missing
poesio-kabadjov-2004-general,J98-2001,1,\N,Missing
poesio-kabadjov-2004-general,C96-1021,0,\N,Missing
poesio-kabadjov-2004-general,A97-1034,0,\N,Missing
poesio-kabadjov-2004-general,E99-1031,0,\N,Missing
poesio-kabadjov-2004-general,P00-1023,0,\N,Missing
poesio-kabadjov-2004-general,J95-2003,0,\N,Missing
poesio-kabadjov-2004-general,P02-1014,0,\N,Missing
poesio-kabadjov-2004-general,W99-0309,0,\N,Missing
poesio-kabadjov-2004-general,P02-1011,0,\N,Missing
poesio-kabadjov-2004-general,J00-4003,1,\N,Missing
Q15-1018,W10-2417,0,0.0717008,"Missing"
Q15-1018,C12-2005,0,0.0695603,"Missing"
Q15-1018,I13-1045,0,0.0346587,"Missing"
Q15-1018,R13-1005,1,0.939341,"e Bayesian Classifier Combination (BCC) procedure recently proposed for sentiment analysis. According to our results, the BCC model leads to an increase in performance of 8 percentage points over the best base classifiers. 1 Semi-supervised (Abney, 2010) and distant learning approaches (Mintz et al., 2009; Nothman et al., 2013) are alternatives to supervised methods that do not require manually annotated data. These approaches have proved to be effective and easily adaptable to new NE types. However, the performance of such methods tends to be lower than that achieved with supervised methods (Althobaiti et al., 2013; Nadeau, 2007; Nothman et al., 2013). Introduction Supervised learning techniques are very effective and widely used to solve many NLP problems, including NER (Sekine and others, 1998; Benajiba et al., 2007a; Darwish, 2013). The main disadvantage We propose combining these two minimally supervised methods in order to exploit their respective strengths and thereby obtain better results. Semisupervised learning tends to be more precise than distant learning, which in turn leads to higher recall than semi-supervised learning. In this work, we use various classifier combination schemes to combine"
Q15-1018,E14-3012,1,0.807942,"s iteration, plus one. These m examples will be used in the next iteration, and so on. For example, if we start the algorithm with 20 seed instances, the following iteration will start with 21, and the next one will start with 22, and so on. This procedure is necessary in order to carefully include examples from one iteration to another and to ensure that bad instances are not passed on to the next iteration. The same procedure was applied by (Althobaiti et al., 2013). 3.2 Distant Learning For distant learning we follow the state of the art approach to exploit Wikipedia for Arabic NER, as in (Althobaiti et al., 2014). Our distant learning system exploits many of Wikipedia’s features, such as anchor texts, redirects, and inter-language links, in order to automatically develop an Arabic NE annotated corpus, which is used later to train a state-ofthe-art supervised classifier. The three steps of this approach are: 1. Classify Wikipedia articles into a set of NE types. 2. Annotate the Wikipedia text as follows: • Identify and label matching text in the title and the first sentence of each article. • Label linked phrases in the text according to the NE type of the target article. • Compile a list of alternativ"
Q15-1018,P13-1004,0,0.0183325,"model combination that explicitly models the relationship between each classifier’s output and the unknown true label. As such, multiclass Bayesian Classifier Combination (BCC) models are developed to combine predictions of multiple classifiers. Their proposed method for BCC in the machine learning context is derived directly from the method proposed in (Haitovsky et al., 2002) for modelling disagreement between human assessors, which in turn is an extension of (Dawid and Skene, 1979). Similar studies for modelling data annotation using a variety of methods are presented in (Carpenter, 2008; Cohn and Specia, 2013). Simpson et al. (2013) present a variant of BCC in which they consider the use of a principled approximate Bayesian method, variational Bayes (VB), as an inference technique instead of using Gibbs Sampling. They also alter the model so as to use point values for hyper-parameters, instead of placing exponential hyper-priors over them. The following sections detail the combination methods used in this paper to combine the minimally supervised classifiers for Arabic NER. 4.2.1 Voting Voting is the most common method in classifier combination because of its simplicity and acceptable results (Van"
Q15-1018,P13-1153,0,0.0156809,"ervised (Abney, 2010) and distant learning approaches (Mintz et al., 2009; Nothman et al., 2013) are alternatives to supervised methods that do not require manually annotated data. These approaches have proved to be effective and easily adaptable to new NE types. However, the performance of such methods tends to be lower than that achieved with supervised methods (Althobaiti et al., 2013; Nadeau, 2007; Nothman et al., 2013). Introduction Supervised learning techniques are very effective and widely used to solve many NLP problems, including NER (Sekine and others, 1998; Benajiba et al., 2007a; Darwish, 2013). The main disadvantage We propose combining these two minimally supervised methods in order to exploit their respective strengths and thereby obtain better results. Semisupervised learning tends to be more precise than distant learning, which in turn leads to higher recall than semi-supervised learning. In this work, we use various classifier combination schemes to combine the minimal supervision methods. Most previous studies have examined classifier combination schemes to combine multiple supervisedlearning systems (Florian et al., 2003; Saha and Ekbal, 2013), but this research is the first"
Q15-1018,W03-0425,0,0.0697992,"luding NER (Sekine and others, 1998; Benajiba et al., 2007a; Darwish, 2013). The main disadvantage We propose combining these two minimally supervised methods in order to exploit their respective strengths and thereby obtain better results. Semisupervised learning tends to be more precise than distant learning, which in turn leads to higher recall than semi-supervised learning. In this work, we use various classifier combination schemes to combine the minimal supervision methods. Most previous studies have examined classifier combination schemes to combine multiple supervisedlearning systems (Florian et al., 2003; Saha and Ekbal, 2013), but this research is the first to combine minimal supervision approaches. In addition, 243 Transactions of the Association for Computational Linguistics, vol. 3, pp. 243–255, 2015. Action Editor: Ryan McDonald. Submission batch: 1/2015; Revision batch 4/2015; Published 5/2015. c 2015 Association for Computational Linguistics. Distributed under a CC-BY-NC-SA 4.0 license. we report our results from testing the recently proposed Independent Bayesian Classifier Combination (IBCC) scheme (Kim and Ghahramani, 2012; Levenberg et al., 2014) and comparing it with traditional vo"
Q15-1018,P09-1113,0,0.273621,"s. In this paper we present a novel approach to Arabic NER using a combination of semi-supervised and distant learning techniques. We trained a semi-supervised NER classifier and another one using distant learning techniques, and then combined them using a variety of classifier combination schemes, including the Bayesian Classifier Combination (BCC) procedure recently proposed for sentiment analysis. According to our results, the BCC model leads to an increase in performance of 8 percentage points over the best base classifiers. 1 Semi-supervised (Abney, 2010) and distant learning approaches (Mintz et al., 2009; Nothman et al., 2013) are alternatives to supervised methods that do not require manually annotated data. These approaches have proved to be effective and easily adaptable to new NE types. However, the performance of such methods tends to be lower than that achieved with supervised methods (Althobaiti et al., 2013; Nadeau, 2007; Nothman et al., 2013). Introduction Supervised learning techniques are very effective and widely used to solve many NLP problems, including NER (Sekine and others, 1998; Benajiba et al., 2007a; Darwish, 2013). The main disadvantage We propose combining these two mini"
Q15-1018,P11-2048,0,0.0633466,"Missing"
Q15-1018,C12-1132,0,0.0572781,"Missing"
Q15-1018,P08-1001,0,0.0526041,"Missing"
Q15-1018,M98-1019,0,0.213731,"Missing"
Q15-1018,U09-1015,0,0.0731969,"Missing"
Q15-1018,J01-2002,0,0.197422,"Missing"
Q15-1018,W14-1601,0,\N,Missing
Q17-1002,D13-1202,1,0.763078,"onal modelling there has been increasing importance attributed to grounding semantic models in sensory modalities, e.g., Bruni et al. (2014), Kiela and Bottou (2014). Andrews et al. (2009) demonstrated that multi-modal models formed by combining text-based distributional information with behaviourally generated conceptual properties (as a surrogate for perceptual experience) provide a better proxy for human-like intelligence. However, both the text-based and behaviourallybased components of their model were ultimately derived from linguistic information. Since then, in analyses of brain data, Anderson et al. (2013) have applied multi-modal models incorporating features that are truly grounded in natural image statistics to further support this claim. In addition, Anderson et al. (2015) have demonstrated that visually grounded models describe brain activity associated with internally induced visual features of objects as the ob17 Transactions of the Association for Computational Linguistics, vol. 5, pp. 17–30, 2017. Action Editor: Daichi Mochihashi. Submission batch: 2/2016; Revision batch: 7/2016; Published 1/2017. c 2017 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license."
Q17-1002,W04-2214,0,0.0431589,"domains (columns), and taxonomic categories (groups of 5 rows). The most concrete half of the words are indicated in bold font. Strike-throughs indicate words for which we did not have semantic model coverage. words in the norms of Barca et al. (2002). They then linked these to WordNet to identify the taxonomic category of the dominant sense of each word. Six taxonomic categories that were heavily populated with abstract words, as well as one unambiguously concrete category, were chosen. All categories supported ample coverage of Law and Music domains (determined according to WordNet Domains (Bentivogli et al., 2004)). Five law words and five music words were selected from each taxonomic category. Taxonomic categories and example stimulus words (translated into English) are as below: Ur-abstract: Anderson et al.’s term for concepts that are classified as abstract in WordNet but do not belong to a clear subcategory, e.g., law or music. At19 tribute: A construct whereby objects or individuals can be distinguished, e.g., legality, tonality. Communication: Something that is communicated by, to or between groups, e.g., accusation, symphony. Event/action: Something that happens at a given place and time, e.g.,"
Q17-1002,P13-1153,0,0.0342749,"on native Italians, and because approximately comparable text corpora in content were available in English and Italian (English and Italian Wikipedia), we were able to compare how well English and Italian text-based semantic models can decode neural activity patterns. Whilst Italian Wikipedia could reasonably be expected to be advantaged by supporting culturally appropriate nuances of semantic structure, it is disadvantaged by being considerably smaller than English Wikipedia. Taking inspiration from previous work exploiting cross-lingual resources (Richman and Schone, 2008; Shi et al., 2010; Darwish, 2013) we combined Italian and English text-based models in our decoding analyses in an attempt to leverage the benefits of both. Although combined language and English models tended to yield marginally better decoding accuracies, there were no significant differences between the different language models. Whilst we expect semantic structure on a grand scale to broadly straddle language boundaries for most concrete and abstract concepts (albeit with cultural specificities), this is proof of principle that cross linguistic commonalities are reflected in neural activity patterns measurable with curren"
Q17-1002,W10-0609,0,0.432764,"sity of Rochester aander41@ur.rochester.edu Douwe Kiela Computer Laboratory University of Cambridge dk427@cam.ac.uk Stephen Clark Massimo Poesio Computer Laboratory School of Computer Science and Electronic Engineering University of Cambridge University of Essex sc609@cam.ac.uk poesio@essex.ac.uk Abstract scanned as participants engage in conceptual tasks. This research has almost exclusively focused on brain activity elicited as participants comprehend concrete nouns as experimental stimuli. Different modelling approaches — predominantly distributional semantic models (Mitchell et al., 2008; Devereux et al., 2010; Murphy et al., 2012; Pereira et al., 2013; Carlson et al., 2014) and semantic models based on human behavioural estimation of conceptual features (Palatucci et al., 2009; Sudre et al., 2012; Chang et al., 2010; Bruffaerts et al., 2013; Fernandino et al., 2015) — have elucidated how different brain regions contribute to semantic representation of concrete nouns; however, how these results extend to non-concrete nouns is unknown. Important advances have recently been made using computational semantic models to decode brain activity patterns associated with concepts; however, this work has almo"
Q17-1002,P14-1046,0,0.010655,"computational representations derived from these images in the analysis. Secondary results are that we have exploited rep28 resentational similarity space to build group-level neural representations which better match our inherently group-level computational semantic models. In so doing, this exposes group-level commonalities in neural representation for both concrete and abstract words. Such group-level representations may prove both a useful test-bed for evaluating computational semantic models, as well as a potentially useful information source to incorporate into computational models (see Fyshe et al. (2014) for related work). Finally we have demonstrated that English and Italian text-based models are roughly interchangeable in our neural decoding task. That the English text-based model tended to return marginally higher results on our Italian brain data than the Italian model provides a cautionary note for future studies wishing to use semantic models from different languages to identify culturally specific aspects of neural semantic representation e.g., as a follow up to Zinszer et al. (2016). However we also note that the English Wikipedia data was larger than the corresponding Italian corpus."
Q17-1002,D14-1005,1,0.772322,"er, accuracy is significantly greater using the text-based models for the most abstract nouns. More generally this confirms that current computational models are sufficiently advanced to assist in investigating the representational structure of abstract concepts in the brain. 1 Introduction Since the work of Mitchell et al. (2008), there has been increasing interest in using computational semantic models to interpret neural activity patterns In computational modelling there has been increasing importance attributed to grounding semantic models in sensory modalities, e.g., Bruni et al. (2014), Kiela and Bottou (2014). Andrews et al. (2009) demonstrated that multi-modal models formed by combining text-based distributional information with behaviourally generated conceptual properties (as a surrogate for perceptual experience) provide a better proxy for human-like intelligence. However, both the text-based and behaviourallybased components of their model were ultimately derived from linguistic information. Since then, in analyses of brain data, Anderson et al. (2013) have applied multi-modal models incorporating features that are truly grounded in natural image statistics to further support this claim. In a"
Q17-1002,D15-1293,1,0.410712,"owing two factors. First, the dataset analysed was for a small sample of 67 words, and it is reasonable to conjecture that some of these words are also encoded in modalities other than vision and language. For example, musical words may be encoded in acoustic and motor features (see also Fernandino et al. (2015)). Future work will be necessary to verify that the findings generalise more broadly to words from domains beyond law and music. In work in progress the authors are undertaking more focused analyses on the current dataset, using textual, visual and newly developed audio semantic modes (Kiela and Clark, 2015) to tease apart linguistic, visual and acoustic contributions to semantic representation and how these vary throughout different regions of the brain. A second limitation of the current approach, as pointed out by a reviewer, is that the Google image search algorithm (the workings of which are unknown to the authors) may not perform as well for abstract words as it does for concrete words. Consequently, the visual model may have been handicapped compared to the textual model when decoding neural representations associated with more abstract words. We have no current measure of the degree of th"
Q17-1002,P14-2135,1,0.318578,"-wordout decoding procedure detailed later in Section 4 using the same method as Mitchell et al. (2008): Pearson’s correlation of each voxel’s activity between matched word lists in all scanning run pairs (10 unique run pairs giving 10 correlation coefficients of 68/70 words, where the other 2 words were test words to be decoded) was computed. The mean coefficient was used as stability measure. Voxels associated with the 500 largest stability measures were selected. 3 Semantic Models 3.1 Image-based semantic models Following previous work in multi-modal semantics (Bergsma and Van Durme, 2011; Kiela et al., 2014), we obtain a total of 20 images for each of the stimulus words from Google Images1 . Images from Google have been shown to yield representations that are competitive in quality compared to alternative resources (Bergsma and Van Durme, 2011; Fergus et al., 2005). Image representations are obtained by extracting the pre-softmax layer from a forward pass in a convolutional neural network (CNN) that has been trained on the ImageNet classification task using Caffe (Jia et al., 2014). This approach is similar to e.g., Kriegeskorte (2015), except that we only use the pre-softmax layer, which has bee"
Q17-1002,S12-1019,0,0.0487973,"r41@ur.rochester.edu Douwe Kiela Computer Laboratory University of Cambridge dk427@cam.ac.uk Stephen Clark Massimo Poesio Computer Laboratory School of Computer Science and Electronic Engineering University of Cambridge University of Essex sc609@cam.ac.uk poesio@essex.ac.uk Abstract scanned as participants engage in conceptual tasks. This research has almost exclusively focused on brain activity elicited as participants comprehend concrete nouns as experimental stimuli. Different modelling approaches — predominantly distributional semantic models (Mitchell et al., 2008; Devereux et al., 2010; Murphy et al., 2012; Pereira et al., 2013; Carlson et al., 2014) and semantic models based on human behavioural estimation of conceptual features (Palatucci et al., 2009; Sudre et al., 2012; Chang et al., 2010; Bruffaerts et al., 2013; Fernandino et al., 2015) — have elucidated how different brain regions contribute to semantic representation of concrete nouns; however, how these results extend to non-concrete nouns is unknown. Important advances have recently been made using computational semantic models to decode brain activity patterns associated with concepts; however, this work has almost exclusively focuse"
Q17-1002,P08-1001,0,0.0512656,"fMRI experiments were performed in Italian on native Italians, and because approximately comparable text corpora in content were available in English and Italian (English and Italian Wikipedia), we were able to compare how well English and Italian text-based semantic models can decode neural activity patterns. Whilst Italian Wikipedia could reasonably be expected to be advantaged by supporting culturally appropriate nuances of semantic structure, it is disadvantaged by being considerably smaller than English Wikipedia. Taking inspiration from previous work exploiting cross-lingual resources (Richman and Schone, 2008; Shi et al., 2010; Darwish, 2013) we combined Italian and English text-based models in our decoding analyses in an attempt to leverage the benefits of both. Although combined language and English models tended to yield marginally better decoding accuracies, there were no significant differences between the different language models. Whilst we expect semantic structure on a grand scale to broadly straddle language boundaries for most concrete and abstract concepts (albeit with cultural specificities), this is proof of principle that cross linguistic commonalities are reflected in neural activi"
Q17-1002,D10-1103,0,0.0189407,"formed in Italian on native Italians, and because approximately comparable text corpora in content were available in English and Italian (English and Italian Wikipedia), we were able to compare how well English and Italian text-based semantic models can decode neural activity patterns. Whilst Italian Wikipedia could reasonably be expected to be advantaged by supporting culturally appropriate nuances of semantic structure, it is disadvantaged by being considerably smaller than English Wikipedia. Taking inspiration from previous work exploiting cross-lingual resources (Richman and Schone, 2008; Shi et al., 2010; Darwish, 2013) we combined Italian and English text-based models in our decoding analyses in an attempt to leverage the benefits of both. Although combined language and English models tended to yield marginally better decoding accuracies, there were no significant differences between the different language models. Whilst we expect semantic structure on a grand scale to broadly straddle language boundaries for most concrete and abstract concepts (albeit with cultural specificities), this is proof of principle that cross linguistic commonalities are reflected in neural activity patterns measur"
Q18-1040,L16-1323,1,0.878666,"le, Median, Mean, 3rd Quartile, and Max). 4.1 Datasets We evaluate on a collection of datasets reflecting a variety of use-cases and conditions: binary vs. multi-class classification; small vs. large number of annotators; sparse vs. abundant number of items per annotator / annotators per item; and varying degrees of annotator quality (statistics presented in Table 1). Three of the datasets— WSD, RTE, and TEMP, created by Snow et al. (2008)—are widely used in the literature on annotation models (Carpenter, 2008; Hovy et al., 2013). In addition, we include the Phrase Detectives 1.0 (PD) corpus (Chamberlain et al., 2016), which differs in a number of key ways from the Snow et al. (2008) datasets: It has a much larger number of items and annotations, greater sparsity, and a much greater likelihood of spamming due to its collection via a game-with-a-purpose setting. This dataset is also less artificial than the datasets in Snow et al. (2008), which were created with the express purpose of testing crowdsourcing. The data consist of anaphoric annotations, which we reduce to four general classes (DN/DO = discourse new/old, PR = property, and NR = non-referring). To ensure similarity with the Snow et al. (2008) dat"
Q18-1040,D16-1129,0,0.182711,"s prevalence π ∼ Dirichlet(1K ) • For every item i ∈ {1, 2, ..., I}: – Draw true class ci ∼ Categorical(π) – For every position n ∈ {1, 2, ..., Ni }: ∗ Draw annotation yi,n ∼ Categorical(βjj[i,n],ci )4 • For every class k ∈ {1, 2, ..., K}: Multi-Annotator Competence Estimation (MACE) This model, introduced by Hovy et al. (2013), takes into account the credibility of the annotators and their spamming preference and strategy5 (see Figure 3). This is another example of an unpooled model, and possibly the model most widely applied to linguistic data (e.g., Plank et al., 2014a; Sabou et al., 2014; Habernal and Gurevych, 2016, inter alia). Its generative process is: • For every annotator j ∈ {1, 2, ..., J}: • For every annotator j ∈ {1, 2, ..., J}: – For every class k ∈ {1, 2, ..., K}: ∗ Draw class annotator abilities βj,k,k0 ∼ Normal(ζk,k0 , Ωk,k0 ), ∀k 0 • Draw class prevalence π ∼ Dirichlet(1K ) – Draw spamming behavior j ∼ Dirichlet(10K ) – Draw credibility θj ∼ Beta(0.5, 0.5) • For every item i ∈ {1, 2, ..., I}: – Draw true class ci ∼ Categorical(π) – For every position n ∈ {1, 2, ..., Ni }: ∗ Draw annotation yi,n ∼ Categorical(softmax(βjj[i,n],ci ))7 • For every item i ∈ {1, 2, ..., I}: – Draw true class ci"
Q18-1040,N13-1132,1,0.243271,"ough, has been repeatedly shown to be false in annotation practice (Poesio and Artstein, 2005; Passonneau and Carpenter, 2014; Plank et al., 2014b). Chanceadjusted coefficients of agreement also have many shortcomings—for example, agreements in mistake, overly large chance-agreement in datasets with skewed classes, or no annotator bias correction (Feinstein and Cicchetti, 1990; Passonneau and Carpenter, 2014). Research suggests that models of annotation can solve these problems of standard practices when applied to crowdsourcing (Dawid and Skene, 1979; Smyth et al., 1995; Raykar et al., 2010; Hovy et al., 2013; Passonneau and Carpenter, 2014). Such probabilistic approaches allow us to characterize the accuracy of the annotators and correct for their bias, as well as accounting for item-level effects. They have been shown to perform better than non-probabilistic alternatives based on heuristic analysis or adjudication (Quoc Viet Hung et al., 2013). But even though a large number of such models has been proposed (Carpenter, 2008; Whitehill et al., 2009; Raykar et al., 2010; Hovy et al., 2013; Simpson et al., 2013; Passonneau and Carpenter, 2014; Felt et al., 2015a; Kamar et al., 2015; Moreno et al.,"
Q18-1040,N15-1089,0,0.133747,"th et al., 1995; Raykar et al., 2010; Hovy et al., 2013; Passonneau and Carpenter, 2014). Such probabilistic approaches allow us to characterize the accuracy of the annotators and correct for their bias, as well as accounting for item-level effects. They have been shown to perform better than non-probabilistic alternatives based on heuristic analysis or adjudication (Quoc Viet Hung et al., 2013). But even though a large number of such models has been proposed (Carpenter, 2008; Whitehill et al., 2009; Raykar et al., 2010; Hovy et al., 2013; Simpson et al., 2013; Passonneau and Carpenter, 2014; Felt et al., 2015a; Kamar et al., 2015; Moreno et al., 2015, inter alia), it is not immediately obvious to potential users how these models differ or, in fact, how they should be applied at all. To our knowledge, the literature comparing models of annotation is limited, focused exclusively on synthetic data (Quoc Viet Hung et al., 2013) or using publicly available implementations that constrain the experiments almost exclusively to binary annotations (Sheshadri and Lease, 2013). Contributions • Our selection of six widely used models (Dawid and Skene, 1979; Carpenter, 2008; Hovy et al., 2013) covers models wit"
Q18-1040,felt-etal-2014-momresp,0,0.0823473,"Hill (2007). Item-response theory has also been recently applied to NLP applications (Lalor et al., 2016; Martınez-Plumed et al., 2016; Lalor et al., 2017). The models considered so far take into account only the annotations. There is work, however, that further exploits the features that can accompany items. A popular example is the model introduced by Raykar et al. (2010), where the true class of an item is made to depend both on the annotations and on a logistic regression model that are jointly fit; essentially, the logistic regression replaces the simple categorical model of prevalence. Felt et al. (2014, 2015b) introduced similar models that also modeled the predictors (features) and compared them to other approaches (Felt et al., 2015a). Kamar et al. (2015) account for task-specific feature effects on the annotations. In §6.2, we discussed the label switching problem (Stephens, 2000) that many models of annotation suffer from. Other solutions proposed in the literature include utilizing class-informative priors, imposing ordering constraints (obvious for univariate parameters; less so in multivariate cases) (Gelman et al., 2013), or applying different post-inference relabeling techniques (F"
Q18-1040,K15-1020,0,0.131074,"th et al., 1995; Raykar et al., 2010; Hovy et al., 2013; Passonneau and Carpenter, 2014). Such probabilistic approaches allow us to characterize the accuracy of the annotators and correct for their bias, as well as accounting for item-level effects. They have been shown to perform better than non-probabilistic alternatives based on heuristic analysis or adjudication (Quoc Viet Hung et al., 2013). But even though a large number of such models has been proposed (Carpenter, 2008; Whitehill et al., 2009; Raykar et al., 2010; Hovy et al., 2013; Simpson et al., 2013; Passonneau and Carpenter, 2014; Felt et al., 2015a; Kamar et al., 2015; Moreno et al., 2015, inter alia), it is not immediately obvious to potential users how these models differ or, in fact, how they should be applied at all. To our knowledge, the literature comparing models of annotation is limited, focused exclusively on synthetic data (Quoc Viet Hung et al., 2013) or using publicly available implementations that constrain the experiments almost exclusively to binary annotations (Sheshadri and Lease, 2013). Contributions • Our selection of six widely used models (Dawid and Skene, 1979; Carpenter, 2008; Hovy et al., 2013) covers models wit"
Q18-1040,D16-1062,0,0.0409278,"jointly estimating the ability of the individuals and the difficulty of the test items based on the correctness of their responses. The models of annotation we discussed in this paper are completely unsupervised and infer, in addition to annotator ability and/or item difficulty, the correct labels. More details on item-response models are given in Skrondal and Rabe-Hesketh (2004) and Gelman Technical Notes Posterior Curvature. In hierarchical models, a complicated posterior curvature increases the dif581 and Hill (2007). Item-response theory has also been recently applied to NLP applications (Lalor et al., 2016; Martınez-Plumed et al., 2016; Lalor et al., 2017). The models considered so far take into account only the annotations. There is work, however, that further exploits the features that can accompany items. A popular example is the model introduced by Raykar et al. (2010), where the true class of an item is made to depend both on the annotations and on a logistic regression model that are jointly fit; essentially, the logistic regression replaces the simple categorical model of prevalence. Felt et al. (2014, 2015b) introduced similar models that also modeled the predictors (features) and compa"
Q18-1040,H93-1012,0,0.0277539,"Missing"
Q18-1040,Q14-1025,1,0.848921,"of noise in the form of random (spammy) annotators. We conclude with guidelines for model selection, application, and implementation. 1 Introduction The standard methodology for analyzing crowdsourced data in NLP is based on majority voting (selecting the label chosen by the majority of coders) and inter-annotator coefficients of agreement, such as Cohen’s κ (Artstein and Poesio, 2008). However, aggregation by majority vote implicitly assumes equal expertise among the annotators. This assumption, though, has been repeatedly shown to be false in annotation practice (Poesio and Artstein, 2005; Passonneau and Carpenter, 2014; Plank et al., 2014b). Chanceadjusted coefficients of agreement also have many shortcomings—for example, agreements in mistake, overly large chance-agreement in datasets with skewed classes, or no annotator bias correction (Feinstein and Cicchetti, 1990; Passonneau and Carpenter, 2014). Research suggests that models of annotation can solve these problems of standard practices when applied to crowdsourcing (Dawid and Skene, 1979; Smyth et al., 1995; Raykar et al., 2010; Hovy et al., 2013; Passonneau and Carpenter, 2014). Such probabilistic approaches allow us to characterize the accuracy of th"
Q18-1040,C14-1168,1,0.740603,"(spammy) annotators. We conclude with guidelines for model selection, application, and implementation. 1 Introduction The standard methodology for analyzing crowdsourced data in NLP is based on majority voting (selecting the label chosen by the majority of coders) and inter-annotator coefficients of agreement, such as Cohen’s κ (Artstein and Poesio, 2008). However, aggregation by majority vote implicitly assumes equal expertise among the annotators. This assumption, though, has been repeatedly shown to be false in annotation practice (Poesio and Artstein, 2005; Passonneau and Carpenter, 2014; Plank et al., 2014b). Chanceadjusted coefficients of agreement also have many shortcomings—for example, agreements in mistake, overly large chance-agreement in datasets with skewed classes, or no annotator bias correction (Feinstein and Cicchetti, 1990; Passonneau and Carpenter, 2014). Research suggests that models of annotation can solve these problems of standard practices when applied to crowdsourcing (Dawid and Skene, 1979; Smyth et al., 1995; Raykar et al., 2010; Hovy et al., 2013; Passonneau and Carpenter, 2014). Such probabilistic approaches allow us to characterize the accuracy of the annotators and cor"
Q18-1040,P14-2083,1,0.89224,"(spammy) annotators. We conclude with guidelines for model selection, application, and implementation. 1 Introduction The standard methodology for analyzing crowdsourced data in NLP is based on majority voting (selecting the label chosen by the majority of coders) and inter-annotator coefficients of agreement, such as Cohen’s κ (Artstein and Poesio, 2008). However, aggregation by majority vote implicitly assumes equal expertise among the annotators. This assumption, though, has been repeatedly shown to be false in annotation practice (Poesio and Artstein, 2005; Passonneau and Carpenter, 2014; Plank et al., 2014b). Chanceadjusted coefficients of agreement also have many shortcomings—for example, agreements in mistake, overly large chance-agreement in datasets with skewed classes, or no annotator bias correction (Feinstein and Cicchetti, 1990; Passonneau and Carpenter, 2014). Research suggests that models of annotation can solve these problems of standard practices when applied to crowdsourcing (Dawid and Skene, 1979; Smyth et al., 1995; Raykar et al., 2010; Hovy et al., 2013; Passonneau and Carpenter, 2014). Such probabilistic approaches allow us to characterize the accuracy of the annotators and cor"
Q18-1040,W05-0311,1,0.910634,"tasets with varying degrees of noise in the form of random (spammy) annotators. We conclude with guidelines for model selection, application, and implementation. 1 Introduction The standard methodology for analyzing crowdsourced data in NLP is based on majority voting (selecting the label chosen by the majority of coders) and inter-annotator coefficients of agreement, such as Cohen’s κ (Artstein and Poesio, 2008). However, aggregation by majority vote implicitly assumes equal expertise among the annotators. This assumption, though, has been repeatedly shown to be false in annotation practice (Poesio and Artstein, 2005; Passonneau and Carpenter, 2014; Plank et al., 2014b). Chanceadjusted coefficients of agreement also have many shortcomings—for example, agreements in mistake, overly large chance-agreement in datasets with skewed classes, or no annotator bias correction (Feinstein and Cicchetti, 1990; Passonneau and Carpenter, 2014). Research suggests that models of annotation can solve these problems of standard practices when applied to crowdsourcing (Dawid and Skene, 1979; Smyth et al., 1995; Raykar et al., 2010; Hovy et al., 2013; Passonneau and Carpenter, 2014). Such probabilistic approaches allow us to"
Q18-1040,D08-1027,0,0.578026,"Missing"
Q18-1040,J08-4004,1,\N,Missing
R09-1006,P08-1003,0,0.0326585,"concept greyhound or builds a large nest for the concept bald eagle). For tools a common generated property type is the function (e.g. used by barbers for razor or used in druidic ritual for golden sickle). Interestingly enough, some extracted knowledge are rules, like: most birds build nests or most helicopters have a single main rotor. 4 Related Work With the advent of new information sources many teams are developing methods for large-scale information extraction taking advantage of the huge amounts of unstructured text currently available. In this framework relevant is the work of Pasca ([4]) who exploits both query logs and Web documents to acquire instances and knowledge for open domain classes. Recently the potential of Wikipedia for information extraction in general and knowledge extraction in particular was acknowledged by many research groups. The methods that use Wikipedia for knowledge extraction can be grouped in two major classes. The first class of methods takes profit of the internal link structure and the structured information in Wikipedia (e.g. infoboxes or templates), while the second class of methods use Wikipedias raw text. Representative for the second class of"
R09-1006,S07-1016,0,0.0189434,"kipedia articles we follow the next steps: • First, we pick a concept of interest representing the higher level node of the taxonomy to be extracted and map it onto a WordNet synset. For example, if you have chosen the concept dog and you want to get the sense corresponding to the animal, you map the concept to the sense number 1 in WordNet. 28 International Conference RANLP 2009 - Borovets, Bulgaria, pages 28–32 • Second, the hyponymy (sub)tree having as root the concept chosen in the previous step is produced and the concepts in the tree are mapped onto Wikipedia pages. As others have shown [5] the best mapping heuristic is to choose that member of a synset which has the sense number 1 . Even so, the ambiguity problem is not completely solved. For it is possible that concepts having low or no ambiguity in WordNet to be highly ambiguous in Wikipedia. Fortunately, in this case the Wikipedia server returns a page having a standard structure and allows us to reject the ambiguous concept or to guess the right mapping. The disambiguation is performed concatenating the ambiguous concept with each of its WordNet hyperonyms and searching again in Wikipedia until an unambiguos entry is found."
R13-1005,W10-2417,0,0.407043,"Missing"
R13-1005,C96-1079,0,0.300517,"systems. We evaluate this algorithm by way of experiments to extract the three standard named-entity types. Ultimately, our algorithm outperforms simple supervised systems and also performs well when we evaluate its performance in order to extract three new, specialised types of NEs (Politicians, Sportspersons, and Artists). 1 Introduction Named Entities (NEs) are textual references via proper names, such as first and last names, locations, and companies. Detecting NEs within unstructured text and classifying them into predefined categories of names is known as Named Entity Recognition (NER) (Grishman and Sundheim, 1996). Arabic NER has been given great amount of attention over the past fifteen years. A number of Arabic NER systems have been developed using three approaches, which have been investigated thoroughly in the literature of NER. These approaches are rule-based (Shaalan and Raza, 2007; Shaalan and Raza, 2009), Machine Learning (ML) 32 Proceedings of Recent Advances in Natural Language Processing, pages 32–40, Hissar, Bulgaria, 7-13 September 2013. 2 Background 2.1 of seeds in order to initiate the learning process (Nadeau and Sekine, 2007). An early study that influenced later works (Riloff and Jone"
R13-1005,N06-2013,0,0.160479,"Missing"
R13-1005,C12-1132,0,0.111084,"Missing"
R13-1005,W09-2208,0,0.0634375,"Missing"
R13-1005,P11-1037,0,0.0344159,"niversity of Essex Colchester, UK {mjaltha, udo, poesio}@essex.ac.uk Abstract (Benajiba et al., 2007; Benajiba and Rosso, 2007; Benajiba and Rosso, 2008; Abdul-Hamid and Darwish, 2010) and hybrid (Abdallah et al., 2012; Oudah and Shaalan, 2012). Over the past decade, some studies have explored the possibility of solving the problem of NER with a reduced level of supervision. These studies proposed semi-supervised and unsupervised systems, which no longer require annotated datasets and can be easily adapted to new types (Nadeau et al., 2006; Etzioni et al., 2005; Liao and Veeramachaneni, 2009; Liu et al., 2011). This paper introduces ASemiNER, an Arabic semi-supervised NER system built under minimal supervision. Gazetteers (predefined lists of NEs) and annotated corpora are not required by ASemiNER. That is, ASemiNER is a bootstrapping algorithm that takes a few examples of a particular NE type as input and iteratively induces and learns patterns, which are used to extract more examples. Extraction patterns are induced and generalised automatically from data using very general criteria that require no human intervention, and no prior knowledge of the language or the corpus domain. In addition to the"
R13-1005,W07-0803,0,0.0327967,"Politicians, Sportspersons, and Artists). 1 Introduction Named Entities (NEs) are textual references via proper names, such as first and last names, locations, and companies. Detecting NEs within unstructured text and classifying them into predefined categories of names is known as Named Entity Recognition (NER) (Grishman and Sundheim, 1996). Arabic NER has been given great amount of attention over the past fifteen years. A number of Arabic NER systems have been developed using three approaches, which have been investigated thoroughly in the literature of NER. These approaches are rule-based (Shaalan and Raza, 2007; Shaalan and Raza, 2009), Machine Learning (ML) 32 Proceedings of Recent Advances in Natural Language Processing, pages 32–40, Hissar, Bulgaria, 7-13 September 2013. 2 Background 2.1 of seeds in order to initiate the learning process (Nadeau and Sekine, 2007). An early study that influenced later works (Riloff and Jones, 1999) propounds that the algorithm begins with a set of seed examples of a particular entity type (e.g., London is entity of type city). Then, all contexts (e.g., “State of <X &gt;”, “seminars in <X &gt;”) found around these seeds in a large corpus will be gathered, ranked, and use"
R13-1005,E12-1017,0,0.0568981,"Missing"
rodriguez-etal-2010-anaphoric,poesio-artstein-2008-anaphoric,1,\N,Missing
rodriguez-etal-2010-anaphoric,N07-1025,0,\N,Missing
rodriguez-etal-2010-anaphoric,bosco-etal-2000-building,0,\N,Missing
rodriguez-etal-2010-anaphoric,P09-2040,0,\N,Missing
rodriguez-etal-2010-anaphoric,J96-2004,0,\N,Missing
rodriguez-etal-2010-anaphoric,magnini-etal-2006-cab,0,\N,Missing
rodriguez-etal-2010-anaphoric,pianta-etal-2008-textpro,0,\N,Missing
rodriguez-etal-2010-anaphoric,W09-4304,0,\N,Missing
S10-1001,W99-0212,0,0.0429338,"cation of the expressions in a text that refer to the same discourse entity (1), has attracted considerable attention within the NLP community. (1) Major League Baseball sent its head of security to Chicago to review the second incident of an on-field fan attack in the last seven months. The league is reviewing security at all ballparks to crack down on spectator violence. Using coreference information has been shown to be beneficial in a number of NLP applications including Information Extraction (McCarthy and Lehnert, 1995), Text Summarization (Steinberger et al., 2007), Question Answering (Morton, 1999), and Machine Translation. There have been a few evaluation campaigns on coreference resolution in the past, namely MUC (Hirschman and Chinchor, 1997), ACE (Doddington et al., 2004), and ARE (Orasan et al., 2008), yet many questions remain open: 2 Linguistic Resources In this section, we first present the sources of the data used in the task. We then describe the automatic tools that predicted input annotations for the coreference resolution systems. 1 http://stel.ub.edu/semeval2010-coref 1 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 1–8, c Uppsala, Sw"
S10-1001,S10-1022,0,0.0889833,"Missing"
S10-1001,orasan-etal-2008-anaphora,0,0.0341483,"o to review the second incident of an on-field fan attack in the last seven months. The league is reviewing security at all ballparks to crack down on spectator violence. Using coreference information has been shown to be beneficial in a number of NLP applications including Information Extraction (McCarthy and Lehnert, 1995), Text Summarization (Steinberger et al., 2007), Question Answering (Morton, 1999), and Machine Translation. There have been a few evaluation campaigns on coreference resolution in the past, namely MUC (Hirschman and Chinchor, 1997), ACE (Doddington et al., 2004), and ARE (Orasan et al., 2008), yet many questions remain open: 2 Linguistic Resources In this section, we first present the sources of the data used in the task. We then describe the automatic tools that predicted input annotations for the coreference resolution systems. 1 http://stel.ub.edu/semeval2010-coref 1 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 1–8, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics Catalan Dutch English German Italian Spanish #docs Training #sents #tokens #docs 829 145 229 900 80 875 8,709 2,544 3,648 19,233 2,951 9,022 2"
S10-1001,S10-1021,1,0.673549,"Missing"
S10-1001,W99-0707,0,0.0438335,"6 English The OntoNotes Release 2.0 corpus (Pradhan et al., 2007) covers newswire and broadcast news data: 300k words from The Wall Street Journal, and 200k words from the TDT-4 collection, respectively. OntoNotes builds on the Penn Treebank for syntactic annotation and on the Penn PropBank for predicate argument structures. Semantic annotations include NEs, words senses (linked to an ontology), and coreference information. The OntoNotes corpus is distributed by the Linguistic Data Consortium.2 Dutch Lemmas, PoS and NEs were automatically provided by the memory-based shallow parser for Dutch (Daelemans et al., 1999), and dependency information by the Alpino parser (van Noord et al., 2006). German Lemmas were predicted by TreeTagger (Schmid, 1995), PoS and morphology by RFTagger (Schmid and Laws, 2008), and dependency information by MaltParser (Hall and Nivre, 2008). German The T¨uBa-D/Z corpus (Hinrichs et al., 2005) is a newspaper treebank based on data taken from the daily issues of “die tageszeitung” (taz). It currently comprises 794k words manually annotated with semantic and coreference information. Due to licensing restrictions of the original texts, a taz-DVD must be purchased to obtain a license."
S10-1001,doddington-etal-2004-automatic,0,0.0804398,"sent its head of security to Chicago to review the second incident of an on-field fan attack in the last seven months. The league is reviewing security at all ballparks to crack down on spectator violence. Using coreference information has been shown to be beneficial in a number of NLP applications including Information Extraction (McCarthy and Lehnert, 1995), Text Summarization (Steinberger et al., 2007), Question Answering (Morton, 1999), and Machine Translation. There have been a few evaluation campaigns on coreference resolution in the past, namely MUC (Hirschman and Chinchor, 1997), ACE (Doddington et al., 2004), and ARE (Orasan et al., 2008), yet many questions remain open: 2 Linguistic Resources In this section, we first present the sources of the data used in the task. We then describe the automatic tools that predicted input annotations for the coreference resolution systems. 1 http://stel.ub.edu/semeval2010-coref 1 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 1–8, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics Catalan Dutch English German Italian Spanish #docs Training #sents #tokens #docs 829 145 229 900 80 875 8,709 2"
S10-1001,rodriguez-etal-2010-anaphoric,1,0.142463,"Missing"
S10-1001,S10-1017,1,0.853056,"Missing"
S10-1001,W08-1007,0,0.0168985,"annotation and on the Penn PropBank for predicate argument structures. Semantic annotations include NEs, words senses (linked to an ontology), and coreference information. The OntoNotes corpus is distributed by the Linguistic Data Consortium.2 Dutch Lemmas, PoS and NEs were automatically provided by the memory-based shallow parser for Dutch (Daelemans et al., 1999), and dependency information by the Alpino parser (van Noord et al., 2006). German Lemmas were predicted by TreeTagger (Schmid, 1995), PoS and morphology by RFTagger (Schmid and Laws, 2008), and dependency information by MaltParser (Hall and Nivre, 2008). German The T¨uBa-D/Z corpus (Hinrichs et al., 2005) is a newspaper treebank based on data taken from the daily issues of “die tageszeitung” (taz). It currently comprises 794k words manually annotated with semantic and coreference information. Due to licensing restrictions of the original texts, a taz-DVD must be purchased to obtain a license.2 Italian Lemmas and PoS were provided by TextPro,7 and dependency information by MaltParser.8 3 The German and Dutch training datasets were not completely stable during the competition period due to a few errors. Revised versions were released on March"
S10-1001,C08-1098,0,0.00615627,"respectively. OntoNotes builds on the Penn Treebank for syntactic annotation and on the Penn PropBank for predicate argument structures. Semantic annotations include NEs, words senses (linked to an ontology), and coreference information. The OntoNotes corpus is distributed by the Linguistic Data Consortium.2 Dutch Lemmas, PoS and NEs were automatically provided by the memory-based shallow parser for Dutch (Daelemans et al., 1999), and dependency information by the Alpino parser (van Noord et al., 2006). German Lemmas were predicted by TreeTagger (Schmid, 1995), PoS and morphology by RFTagger (Schmid and Laws, 2008), and dependency information by MaltParser (Hall and Nivre, 2008). German The T¨uBa-D/Z corpus (Hinrichs et al., 2005) is a newspaper treebank based on data taken from the daily issues of “die tageszeitung” (taz). It currently comprises 794k words manually annotated with semantic and coreference information. Due to licensing restrictions of the original texts, a taz-DVD must be purchased to obtain a license.2 Italian Lemmas and PoS were provided by TextPro,7 and dependency information by MaltParser.8 3 The German and Dutch training datasets were not completely stable during the competition per"
S10-1001,W05-0303,0,0.0585819,"Missing"
S10-1001,hoste-de-pauw-2006-knack,1,0.856981,"Missing"
S10-1001,S10-1020,0,0.0536547,"Missing"
S10-1001,van-noord-etal-2006-syntactic,0,0.0179959,"Missing"
S10-1001,S10-1018,0,0.147949,"Missing"
S10-1001,M95-1005,0,0.744599,"d). Results are presented sequentially by language and setting, and participating systems are ordered alphabetically. The participation of systems across languages and settings is rather irregular,11 thus making it difficult to draw firm concluEvaluation Metrics Since there is no agreement at present on a standard measure for coreference resolution evaluation, one of our goals was to compare the rankings produced by four different measures. The task scorer provides results in the two mentionbased metrics B3 (Bagga and Baldwin, 1998) and CEAF-φ3 (Luo, 2005), and the two link-based metrics MUC (Vilain et al., 1995) and BLANC (Recasens and Hovy, in prep). The first three measures have been widely used, while BLANC is a proposal of a new measure interesting to test. The mention detection subtask is measured with recall, precision, and F1 . Mentions are rewarded with 1 point if their boundaries coincide with those 11 4 Only 45 entries in Table 5 from 192 potential cases. BART (Broscheit et al., 2010) Corry (Uryupina, 2010) RelaxCor (Sapena et al., 2010) SUCRE (Kobdani and Sch¨utze, 2010) TANL-1 (Attardi et al., 2010) UBIU (Zhekova and K¨ubler, 2010) System Architecture ML Methods External Resources Closest"
S10-1001,H05-1004,0,0.516482,"st scores in each setting are highlighted in bold). Results are presented sequentially by language and setting, and participating systems are ordered alphabetically. The participation of systems across languages and settings is rather irregular,11 thus making it difficult to draw firm concluEvaluation Metrics Since there is no agreement at present on a standard measure for coreference resolution evaluation, one of our goals was to compare the rankings produced by four different measures. The task scorer provides results in the two mentionbased metrics B3 (Bagga and Baldwin, 1998) and CEAF-φ3 (Luo, 2005), and the two link-based metrics MUC (Vilain et al., 1995) and BLANC (Recasens and Hovy, in prep). The first three measures have been widely used, while BLANC is a proposal of a new measure interesting to test. The mention detection subtask is measured with recall, precision, and F1 . Mentions are rewarded with 1 point if their boundaries coincide with those 11 4 Only 45 entries in Table 5 from 192 potential cases. BART (Broscheit et al., 2010) Corry (Uryupina, 2010) RelaxCor (Sapena et al., 2010) SUCRE (Kobdani and Sch¨utze, 2010) TANL-1 (Attardi et al., 2010) UBIU (Zhekova and K¨ubler, 2010)"
S10-1001,S10-1019,0,0.0843106,"Missing"
S10-1001,C10-2125,1,\N,Missing
S10-1001,M98-1029,0,\N,Missing
S10-1021,broscheit-etal-2010-extending,1,0.899254,"sign their basic properties (number, gender etc). The feature extraction module describes pairs of mentions {Mi , Mj }, i &lt; j as a set of features. The decoder generates training examples through a process of sample selection and learns a pairwise classifier. Finally, the encoder generates testing examples through a (possibly distinct) process of sample selection, runs the classifier and partitions the mentions into coreference chains. 3 Language-specific issues Below we briefly describe our language-specific extensions to BART. These issues are addressed in more details in our recent papers (Broscheit et al., 2010; Poesio et al., 2010). 3.1 Mention Detection Robust mention detection is an essential component of any coreference resolution system. BART supports different pipelines for mention detection. The 104 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 104–107, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics Basic features Syntactic features LanguagePlugin Mention (with basic properties): Parser Unannotated Text Knowledge-based features - Number - Gender - Mention Type - Modifiers Dep-to-Const Converter Coreference Chains Morp"
S10-1021,finthammer-cramer-2008-exploring,0,0.0135445,"et al., 2010). The NodeDistance feature measures the number of clause nodes (SIMPX, R - SIMPX) and prepositional phrase nodes (PX) along the path between Mj and Mi in the parse tree. The PartialMorphMatch feature is a substring match with a morphological extension for common nouns. In German the frequent use of noun composition makes a simple string match for common nouns unfeasible. The feature checks for a match between the noun stems of Mi and Mj . We extract the morphology with SMOR/Morphisto (Schmid et al., 2004). The GermanetRelatedness feature uses the Pathfinder library for GermaNet (Finthammer and Cramer, 2008) that computes and discretizes raw scores into three categories of semantic relatedness. In our experiments we use the measure from Wu and Palmer (1994), which has been found to be the best performing on our development data. Italian. We have designed a feature to cover Italian aliasing patterns. A list of company/person designators (e.g., “S.p.a” or “D.ssa”) has been manually crafted. We have collected patterns of name variants for locations. Finally, we have relaxed abbreviation constraints, allowing for lower-case characters in the abbreviations. Our pilot experiments suggest that, although"
S10-1021,P06-1055,0,0.00843219,"Number - Gender - Mention Type - Modifiers Dep-to-Const Converter Coreference Chains Morphology Decoder Mention Factory Preprocessing MaxEnt Classifier Figure 1: BART architecture choice of a pipeline depends crucially on the availability of linguistic resources for a given language. For English and German, we use the Parsing Pipeline and Mention Factory to extract mentions. The parse trees are used to identify minimal and maximal noun projections, as well as additional features such as number, gender, and semantic class. For English, we use parses from a state-of-the-art constituent parser (Petrov et al., 2006) and extract all base noun phrases as mentions. For German, the SemEval dependency tree is transformed to a constituent representation and minimal and maximal phrases are extracted for all nominal elements (pronouns, common nouns, names), except when the noun phrase is in a non-referring syntactic position (for example, expletive “es”, predicates in copula constructions). For Italian, we use the EMD Pipeline and Mention Factory. The Typhoon (Zanoli et al., 2009) and DEMention (Biggio et al., 2009) systems were used to recognize mentions in the test set. For each mention, its head and extension"
S10-1021,poesio-etal-2010-creating,1,0.825324,"ies (number, gender etc). The feature extraction module describes pairs of mentions {Mi , Mj }, i &lt; j as a set of features. The decoder generates training examples through a process of sample selection and learns a pairwise classifier. Finally, the encoder generates testing examples through a (possibly distinct) process of sample selection, runs the classifier and partitions the mentions into coreference chains. 3 Language-specific issues Below we briefly describe our language-specific extensions to BART. These issues are addressed in more details in our recent papers (Broscheit et al., 2010; Poesio et al., 2010). 3.1 Mention Detection Robust mention detection is an essential component of any coreference resolution system. BART supports different pipelines for mention detection. The 104 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 104–107, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics Basic features Syntactic features LanguagePlugin Mention (with basic properties): Parser Unannotated Text Knowledge-based features - Number - Gender - Mention Type - Modifiers Dep-to-Const Converter Coreference Chains Morphology Decoder Mention"
S10-1021,schmid-etal-2004-smor,0,0.007832,"tween the two. German. We have tested extra features for German in our previous study (Broscheit et al., 2010). The NodeDistance feature measures the number of clause nodes (SIMPX, R - SIMPX) and prepositional phrase nodes (PX) along the path between Mj and Mi in the parse tree. The PartialMorphMatch feature is a substring match with a morphological extension for common nouns. In German the frequent use of noun composition makes a simple string match for common nouns unfeasible. The feature checks for a match between the noun stems of Mi and Mj . We extract the morphology with SMOR/Morphisto (Schmid et al., 2004). The GermanetRelatedness feature uses the Pathfinder library for GermaNet (Finthammer and Cramer, 2008) that computes and discretizes raw scores into three categories of semantic relatedness. In our experiments we use the measure from Wu and Palmer (1994), which has been found to be the best performing on our development data. Italian. We have designed a feature to cover Italian aliasing patterns. A list of company/person designators (e.g., “S.p.a” or “D.ssa”) has been manually crafted. We have collected patterns of name variants for locations. Finally, we have relaxed abbreviation constraint"
S10-1021,J01-4004,0,0.426265,"tances are modeled as feature vectors (cf. Table 1) and are handed over to a binary classifier that decides, given the features, whether the anaphor and the candidate are coreferent or not. All the feature values are computed automatically, without any manual intervention. Basic feature set. We use the same set of relatively language-independent features as a backbone of our system, extending it with a few languagespecific features for each subtask. Most of them are used by virtually all the state-of-the-art coreference resolution systems. A detailed description can be found, for example, in (Soon et al., 2001). English. Our English system is based on a novel model of coreference. The key concept of our model is a Semantic Tree – a filecard associated with each discourse entity containing the following fields: • Types: the list of types for mentions of a given entity. For example, if an entity contains the mention “software from India”, the shallow predicate “software” is added to the types. • Attributes: this field collects the premodifiers. For instance, if one of the mentions is “the expensive software” the shallow attribute “expensive” is added to the list of attributes. • Relations: this field"
S10-1021,P08-4003,1,0.868153,"i@fbk.eu Abstract 2 BART Architecture BART (Versley et al., 2008) is a highly modular toolkit for coreference resolution that supports state-of-the-art statistical approaches and enables efficient feature engineering. For the SemEval task 1 on Coreference Resolution, BART runs have been submitted for German, English, and Italian. BART relies on a maximum entropy-based classifier for pairs of mentions. A novel entitymention approach based on Semantic Trees is at the moment only supported for English. 1 Introduction This paper presents a multilingual coreference resolution system based on BART (Versley et al., 2008). BART is a modular toolkit for coreference resolution that supports state-of-the-art statistical approaches to the task and enables efficient feature engineering. BART has originally been created and tested for English, but its flexible modular architecture ensures its portability to other languages and domains. In SemEval-2010 task 1 on Coreference Resolution, BART has shown reliable performance for English, German and Italian. In our SemEval experiments, we mainly focus on extending BART to cover multiple languages. Given a corpus in a new language, one can re-train BART to obtain baseline"
S10-1021,P94-1019,0,0.0122284,"d Mi in the parse tree. The PartialMorphMatch feature is a substring match with a morphological extension for common nouns. In German the frequent use of noun composition makes a simple string match for common nouns unfeasible. The feature checks for a match between the noun stems of Mi and Mj . We extract the morphology with SMOR/Morphisto (Schmid et al., 2004). The GermanetRelatedness feature uses the Pathfinder library for GermaNet (Finthammer and Cramer, 2008) that computes and discretizes raw scores into three categories of semantic relatedness. In our experiments we use the measure from Wu and Palmer (1994), which has been found to be the best performing on our development data. Italian. We have designed a feature to cover Italian aliasing patterns. A list of company/person designators (e.g., “S.p.a” or “D.ssa”) has been manually crafted. We have collected patterns of name variants for locations. Finally, we have relaxed abbreviation constraints, allowing for lower-case characters in the abbreviations. Our pilot experiments suggest that, although a universal aliasing algorithm is able to resolve some coreference links between NEs, creating a language-specific module boosts the system’s performan"
sanchez-graillet-poesio-2004-acquiring,P99-1001,0,\N,Missing
sanchez-graillet-poesio-2004-acquiring,W03-1210,0,\N,Missing
sanchez-graillet-poesio-2004-acquiring,P00-1043,0,\N,Missing
uryupina-poesio-2012-domain,poesio-artstein-2008-anaphoric,1,\N,Missing
uryupina-poesio-2012-domain,J00-4005,0,\N,Missing
uryupina-poesio-2012-domain,W11-1901,0,\N,Missing
uryupina-poesio-2012-domain,P09-1074,0,\N,Missing
uryupina-poesio-2012-domain,W11-1908,1,\N,Missing
uryupina-poesio-2012-domain,P11-1157,0,\N,Missing
uryupina-poesio-2012-domain,P02-1014,0,\N,Missing
uryupina-poesio-2012-domain,J01-4004,0,\N,Missing
uryupina-poesio-2012-domain,doddington-etal-2004-automatic,0,\N,Missing
uryupina-poesio-2012-domain,uryupina-2006-coreference,1,\N,Missing
uryupina-poesio-2012-domain,N10-1004,0,\N,Missing
uryupina-poesio-2012-domain,P07-1033,0,\N,Missing
versley-etal-2008-bart-modular,N07-1011,0,\N,Missing
versley-etal-2008-bart-modular,qiu-etal-2004-public,0,\N,Missing
versley-etal-2008-bart-modular,N03-1033,0,\N,Missing
versley-etal-2008-bart-modular,P00-1023,0,\N,Missing
versley-etal-2008-bart-modular,P06-1006,1,\N,Missing
versley-etal-2008-bart-modular,P05-1022,0,\N,Missing
versley-etal-2008-bart-modular,P04-1018,0,\N,Missing
versley-etal-2008-bart-modular,I05-1063,1,\N,Missing
versley-etal-2008-bart-modular,P06-1055,0,\N,Missing
versley-etal-2008-bart-modular,N06-1025,1,\N,Missing
versley-etal-2008-bart-modular,J01-4004,0,\N,Missing
versley-etal-2008-bart-modular,E06-1015,1,\N,Missing
versley-etal-2008-bart-modular,wellner-vilain-2006-leveraging,0,\N,Missing
versley-etal-2008-bart-modular,uryupina-2006-coreference,0,\N,Missing
versley-etal-2008-bart-modular,W00-0730,0,\N,Missing
versley-etal-2008-bart-modular,P05-1045,0,\N,Missing
W00-1705,J96-1002,0,0.0106303,"Missing"
W00-1705,J96-2004,0,0.0122225,"he implementation is going to perform, especially if only approximations are implemented. In GNOME we have been studying these questions by means of corpus annotation studies. We have been trying to identify which of the queries used by systems such as KPML for NP realization can be generally understood by asking subjects to annotate the NPs in our corpus with the information needed to answer these queries, and we have then used the resulting annotation to train statistical models to evaluate the completeness of a given set of features. We use to measure agreement the K statistic discussed by Carletta (1996). A value of K between .8 and 1 indicates good agreement; a value between .6 and .8 indicates some agreement. 4 SEMANTIC AND DISCOURSE FEATURES THAT MAY AFFECT NP TYPE DETERMINATION Even if in this first phase we focused on realizing discourse entities only, we still need to know for each NP in the corpus its semantic type. Noun phrases appear in a text as the realization of at least three different types of logical form constituents: terms, which include referring expressions, as in Jessie M. King or the hour pieces here , but also non-referring terms such as jewelry or different types of cre"
W00-1705,J95-2003,0,0.344808,"l, whereas for an already mentioned one the definite description the jewel would be used. This simple notion of familiarity was refined by Prince herself as well by Gundel et al. (Gundel et al., 1993). Whether it’s hearer-new or hearer-old (Prince, 1992). Whether it is referring to an object in the visual situation or not: if so, a demonstrative NP may be used, as in this jewel. Whether it’s currently highly salient or not, which may prompt the use of a pronoun. Properties that have been claimed to affect the salience of a discourse entity include: whether it’s the current CENTER (CB) or not (Grosz et al., 1995), or more generally whether that entity is the TOPIC of the current discourse (Reinhart, 1981; Garrod and Sanford, 1983); its grammatical function; whether it’s animated or not; its role; its proximity. (For a discussion of the effect of these and other factors on salience see (Poesio and Stevenson, To appear)). According to Loebner (Loebner, 1987), the distinguishing property of definites is not familiarity (a discourse notion), but whether or not the predicate denoted by the head noun is functional or, more generally, UNIQUE. This seems to be the closest formal specification of the notion of"
W00-1705,J98-2001,1,0.884335,"Missing"
W00-1705,W99-0309,0,0.164296,", pronoun, etc.), and the task of organizing the additional information to be expressed with that discourse entity. We are using the annotated corpus to extract information useful to the development of hand-coded algorithms for the subtasks of NP realization we are focusing on, to develop statistical models of these subtasks, and to evaluate both types of algorithms. Conversely, we have been using the results of this evaluation to verify the completeness of our annotation scheme and to identify modifications. The annotation scheme used in our first corpus annotation exercise was discussed in (Poesio et al., 1999b); in this paper we present the modified annotation scheme that we developed as a result of that preliminary work, and discuss the problems we encountered when trying to annotate semantic and discourse information. 2 APPLICATIONS AND DATA The systems we are working with are the ILEX system developed at HCRC, University of Edinburgh (Oberlander et al., 1998),1 and the ICONOCLAST system (Scott et al., 1998), developed at ITRI, University of Brighton. The ILEX system generates Web pages describing museum objects on the basis of the perceived status of its user’s knowledge and of the objects she"
W00-1705,W98-1427,0,0.0282016,"lts of this evaluation to verify the completeness of our annotation scheme and to identify modifications. The annotation scheme used in our first corpus annotation exercise was discussed in (Poesio et al., 1999b); in this paper we present the modified annotation scheme that we developed as a result of that preliminary work, and discuss the problems we encountered when trying to annotate semantic and discourse information. 2 APPLICATIONS AND DATA The systems we are working with are the ILEX system developed at HCRC, University of Edinburgh (Oberlander et al., 1998),1 and the ICONOCLAST system (Scott et al., 1998), developed at ITRI, University of Brighton. The ILEX system generates Web pages describing museum objects on the basis of the perceived status of its user’s knowledge and of the objects she previously looked at; ICON OCLAST supports the creation of pharmaceutical leaflets by means of the WYSIWYM technique in which text generation and user input are interleaved. The corpus we have collected for GNOME includes texts from both the domains we are studying. It contains texts in the museum domain, extending the corpus collected by the SOLE project (Hitzeman et al., 1998); and texts from the corpus"
W00-1705,P98-2140,0,\N,Missing
W00-1705,C98-2135,0,\N,Missing
W03-2605,W98-1427,0,\N,Missing
W03-2605,poesio-etal-2002-acquiring,1,\N,Missing
W03-2605,J98-2001,1,\N,Missing
W03-2605,J99-3001,0,\N,Missing
W03-2605,W97-1301,1,\N,Missing
W03-2605,P00-1051,0,\N,Missing
W03-2605,J95-2003,0,\N,Missing
W03-2605,M95-1017,0,\N,Missing
W03-2605,W99-0309,0,\N,Missing
W03-2605,J00-4003,1,\N,Missing
W03-2605,poesio-2000-annotating,1,\N,Missing
W04-0210,P98-1013,0,0.350546,"maker in the inventory gives neither the name of the maker nor the location. This problem was solved by introducing a special undersp-gen value; indeed, underspecified values were provided for all attributes. The agreement values for these features were: GEN: κ = .89; NUM: κ = .84; PER: κ = .9. GF This attribute was used to annotate the grammatical function of the NP, a property generally taken to play an important role in determining the salience of the discourse entity it realizes (Grosz et al., 1995). Our instructions for this attribute are derived from those used in the FRAMENET project ((Baker et al., 1998); see also http://www.icsi.berkeley.edu/˜framenet/). The values are subj, obj, predicate (used for post-verbal objects in copular sentences, such as This is (a production watch)), there-obj (for post-verbal objects in there-sentences), comp (for indirect objects), adjunct (for the argument of PPs modifying VPs), gen (for NPs in determiner position in possessive NPs), np-compl, np-part, np-mod, adj-mod, and no-gf (for NP s occurring by themselves - eg., in titles). The agreement values for GF is κ = .85. LF TYPE Not all NPs realize discourse entities: some of them realize quantifiers (e.g., eac"
W04-0210,P87-1022,0,0.0610864,"al Focus The reader will have noticed that no attempt was done to directly mark up properties of the local focus - e.g., which discourse entity is the CB of a particular utterance. We found that it is much easier to annotate the ‘building blocks’ of a theory of the local focus, and then use scripts to automatically compute the CB. There are two advantages to this approach: first of all, agreement on the ‘building blocks’ is much easier to reach than agreement on the CB–in our preliminary experiments we didn’t go beyond κ = .6 when trying to directly identify the CB using the definitions from (Brennan et al., 1987). And secondly, this approach makes it possible to compute the CB according to different ways of instantiating what we call the ‘parameters of Centering’ –e.g., ranking. We developed such scripts for the work discussed in (Poesio et al., 2004b); they can be tested on the web site associated with that paper, http://cswww.essex.ac.uk/staff/poesio/ cbc/. These scripts have been subsequently used to compute the CB in, e.g., (Poesio and Nissim, 2001; Poesio and Nygren-Modjeska, To appear). 7 Discussions and Conclusion Corpus consistency The main lesson learned from this effort is that actually usin"
W04-0210,J96-2004,0,0.0386198,"parenmain, subject, complement, adjunct, coord-vp,preposed-pp, listitem, cleft, title, disc-marker. • VERBED: whether the unit contains a verb. • FINITE: for verbed units, whether the verb is finite or not. • SUBJECT: for verbed units, whether they have a full subject, an empty subject (expletive, as in there sentences), or no subject (e.g., for infinitival clauses). Annotation Issues Marking up sentences proved to be quite easy; marking up units, on the other hand, required extensive annotator training. The agreement on identifying the boundaries of units, using the κ statistic discussed in (Carletta, 1996), was κ = .9 (for two annotators and 500 units); the agreement on features (2 annotators and at least 200 units) was as follows: UTYPE: κ=.76; VERBED: κ=.9; FINITE: κ=.81. The main problems when marking units were to identify complements, to distinguish clausal adjuncts from prepositional phrases, and how to mark up coordinated units. The main problem with complements was to distinguish non-finite complements of verbs such as want from the non-finite part of verbal complexes containing modal auxiliaries such as get, let, make, and have: (2) a. (I would like (to be able to travel)) b. (I let hi"
W04-0210,N01-1002,1,0.947228,"generation, especially salience (Pearson et al., 2000; Poesio and Di Eugenio, 2001; Poesio and Nissim, 2001; Poesio et al., 2004b). Particular attention was paid to the factors affecting the generation of pronouns (Pearson et al., 2000; Henschel et al., 2000), demonstratives (Poesio and Nygren-Modjeska, To appear) possessives (Poesio and Nissim, 2001) and definites in general (Poesio, 2004a). These results, and the annotated corpus, were used in the development of both symbolic and statistical natural language generation algorithms for sentence planning (Poesio, 2000a; Henschel et al., 2000; Cheng et al., 2001), aggregation (Cheng, 2001) and text planning (Karamanis, 2003). The empirical side of the project involved both psychological experiments and corpus annotation, based on a scheme based on the MATE proposals, as well as on a detailed annotation manual (Poesio, 2000b), the reliability of whose instructions was tested by extensive experiments (Poesio, 2000a). More recently, the corpus has also been used to develop and evaluate anaphora resolution systems, with a special focus on the resolution of bridging references (Poesio, 2003; Poesio and Alexandrov-Kabadjov, 2004; Poesio et al., 2004a) Altho"
W04-0210,J95-2003,0,0.0975681,"was very difficult because of the presence of many references to individual of unspecified gender, such as the maker in the inventory gives neither the name of the maker nor the location. This problem was solved by introducing a special undersp-gen value; indeed, underspecified values were provided for all attributes. The agreement values for these features were: GEN: κ = .89; NUM: κ = .84; PER: κ = .9. GF This attribute was used to annotate the grammatical function of the NP, a property generally taken to play an important role in determining the salience of the discourse entity it realizes (Grosz et al., 1995). Our instructions for this attribute are derived from those used in the FRAMENET project ((Baker et al., 1998); see also http://www.icsi.berkeley.edu/˜framenet/). The values are subj, obj, predicate (used for post-verbal objects in copular sentences, such as This is (a production watch)), there-obj (for post-verbal objects in there-sentences), comp (for indirect objects), adjunct (for the argument of PPs modifying VPs), gen (for NPs in determiner position in possessive NPs), np-compl, np-part, np-mod, adj-mod, and no-gf (for NP s occurring by themselves - eg., in titles). The agreement values"
W04-0210,C00-1045,1,0.873406,"Kingdom Abstract The GNOME corpus was created to study the discourse and semantic properties of discourse entities that affect their realization and interpretation, and particularly salience. We discuss what information was annotated and the methods we followed. 1 Introduction The GNOME corpus was created to study the aspects of discourse that appear to affect generation, especially salience (Pearson et al., 2000; Poesio and Di Eugenio, 2001; Poesio and Nissim, 2001; Poesio et al., 2004b). Particular attention was paid to the factors affecting the generation of pronouns (Pearson et al., 2000; Henschel et al., 2000), demonstratives (Poesio and Nygren-Modjeska, To appear) possessives (Poesio and Nissim, 2001) and definites in general (Poesio, 2004a). These results, and the annotated corpus, were used in the development of both symbolic and statistical natural language generation algorithms for sentence planning (Poesio, 2000a; Henschel et al., 2000; Cheng et al., 2001), aggregation (Cheng, 2001) and text planning (Karamanis, 2003). The empirical side of the project involved both psychological experiments and corpus annotation, based on a scheme based on the MATE proposals, as well as on a detailed annotat"
W04-0210,M98-1029,0,0.0429884,"ype=""term"" onto=""person"" ani=""animate"" deix=""deix-no"" count=""count-yes"" structure=""set"" generic=""generic-yes"" loeb=""sort""> scholars </ne> <unit finite=’finite-no’ id=’u4’ utype=’complement’ verbed=’verbed-yes’> to link <ne id=""ne5"" cat=""pers-pro"" per=""per3"" num=""sing"" gen=""neut"" gf=""obj"" lftype=""term"" onto=""concrete"" ani=""inanimate"" deix=""deix-yes"" count=""count-yes"" structure=""atom"" generic=""generic-no"" loeb=""disc-function""> it </ne> ... The GNOME instructions for identifying NPs derive from those proposed in MATE (Poesio et al., 1999), in turn derived from DRAMA (Passonneau, 1997) and MUC-7 (Hirschman, 1998). An important difference between the instructions used for GNOME and those developed for MATE is that instead of attempting to get the annotators to recognize the NP that realize discourse entities and only mark those, in GNOME all NPs were marked with hnei elements; the separate LF TYPE attribute was used to distinguish between NPs with different types of denotations (see below). This change made the process of identifying nominal entities easier and potentially automatic (even though the identification of markables was still done by hand). As in the case of units, the main problem with mark"
W04-0210,kingsbury-palmer-2002-treebank,0,0.0117313,"onsistency The main lesson learned from this effort is that actually using a corpus is the best way both to ensure its correctness and to learn which types of information are most useful. Thematic Roles One attribute on which we weren’t able to reach acceptable agreement was the thematic role of an NP, which has been argued to be a better indicator of salience than grammatical function (Sidner, 1979; Stevenson et al., 1994); the agreement value in this case was κ = .35. Other groups however have shown that this can be done, e.g., in Framenet (Baker et al., 1998) and more recently in PropBank (Kingsbury and Palmer, 2002). Planned Revisions of the Scheme A number of aspects of the annotation scheme used for the corpus could be improved. An obvious improvement would be to directly annotate predicates with their WordNet senses instead of annotating ONTO and animacy. We started doing this for the annotation of modifiers (Cheng et al., 2001), and developed an interface to WordNet, but too late to redo the whole corpus. Of the attributes, COUNT and GENERIC were the most difficult to annotate; further tests with these attributes could be useful. Automatic annotation A substantial part of the annotation work required"
W04-0210,J02-3003,0,0.0207755,"r1 The museum subcorpus extends the corpus collected to support the ILEX and SOLE projects at the University of Edinburgh (Oberlander et al., 1998). 2 The leaflets in the pharmaceutical subcorpus are a subset of the collection of all patient leaflets in the UK which was digitized to support the ICONOCLAST project at the University of Brighton (Scott et al., 1998). ing theory (Grosz et al., 1995) are called UTTER ANCES , i.e., the units of text after which the local focus is updated. In most annotations concerned with salience, a predefined notion of utterance was adopted, typically sentences (Miltsakaki, 2002) or (finite) clauses (Kameyama, 1998). This approach, however, precludes using the corpus to compare possible definitions of utterance, one of the goals of the GNOME annotation (Poesio et al., 2004b). In order to do this, we marked all spans of text that might be claimed to update the local focus, including sentences (defined as all units of text ending with a full stop, a question mark, or an exclamation point) as well as what we called ( DISCOURSE ) UNITS . Units include clauses (defined as sequences of text containing a verbal complex, all its obligatory arguments, and all postverbal adjunc"
W04-0210,J98-2001,1,0.458975,"Missing"
W04-0210,W99-0309,0,0.0204559,"allow <ne id=""ne4"" cat=""bare-np"" per=""per3"" num=""plur"" gen=""neut"" gf=""obj"" lftype=""term"" onto=""person"" ani=""animate"" deix=""deix-no"" count=""count-yes"" structure=""set"" generic=""generic-yes"" loeb=""sort""> scholars </ne> <unit finite=’finite-no’ id=’u4’ utype=’complement’ verbed=’verbed-yes’> to link <ne id=""ne5"" cat=""pers-pro"" per=""per3"" num=""sing"" gen=""neut"" gf=""obj"" lftype=""term"" onto=""concrete"" ani=""inanimate"" deix=""deix-yes"" count=""count-yes"" structure=""atom"" generic=""generic-no"" loeb=""disc-function""> it </ne> ... The GNOME instructions for identifying NPs derive from those proposed in MATE (Poesio et al., 1999), in turn derived from DRAMA (Passonneau, 1997) and MUC-7 (Hirschman, 1998). An important difference between the instructions used for GNOME and those developed for MATE is that instead of attempting to get the annotators to recognize the NP that realize discourse entities and only mark those, in GNOME all NPs were marked with hnei elements; the separate LF TYPE attribute was used to distinguish between NPs with different types of denotations (see below). This change made the process of identifying nominal entities easier and potentially automatic (even though the identification of markables w"
W04-0210,P04-1019,1,0.747133,"in the GNOME Corpus Massimo Poesio University of Essex, Department of Computer Science and Centre for Cognitive Science, United Kingdom Abstract The GNOME corpus was created to study the discourse and semantic properties of discourse entities that affect their realization and interpretation, and particularly salience. We discuss what information was annotated and the methods we followed. 1 Introduction The GNOME corpus was created to study the aspects of discourse that appear to affect generation, especially salience (Pearson et al., 2000; Poesio and Di Eugenio, 2001; Poesio and Nissim, 2001; Poesio et al., 2004b). Particular attention was paid to the factors affecting the generation of pronouns (Pearson et al., 2000; Henschel et al., 2000), demonstratives (Poesio and Nygren-Modjeska, To appear) possessives (Poesio and Nissim, 2001) and definites in general (Poesio, 2004a). These results, and the annotated corpus, were used in the development of both symbolic and statistical natural language generation algorithms for sentence planning (Poesio, 2000a; Henschel et al., 2000; Cheng et al., 2001), aggregation (Cheng, 2001) and text planning (Karamanis, 2003). The empirical side of the project involved bo"
W04-0210,J04-3003,1,0.827951,"Missing"
W04-0210,poesio-2000-annotating,1,0.878915,"cts of discourse that appear to affect generation, especially salience (Pearson et al., 2000; Poesio and Di Eugenio, 2001; Poesio and Nissim, 2001; Poesio et al., 2004b). Particular attention was paid to the factors affecting the generation of pronouns (Pearson et al., 2000; Henschel et al., 2000), demonstratives (Poesio and Nygren-Modjeska, To appear) possessives (Poesio and Nissim, 2001) and definites in general (Poesio, 2004a). These results, and the annotated corpus, were used in the development of both symbolic and statistical natural language generation algorithms for sentence planning (Poesio, 2000a; Henschel et al., 2000; Cheng et al., 2001), aggregation (Cheng, 2001) and text planning (Karamanis, 2003). The empirical side of the project involved both psychological experiments and corpus annotation, based on a scheme based on the MATE proposals, as well as on a detailed annotation manual (Poesio, 2000b), the reliability of whose instructions was tested by extensive experiments (Poesio, 2000a). More recently, the corpus has also been used to develop and evaluate anaphora resolution systems, with a special focus on the resolution of bridging references (Poesio, 2003; Poesio and Alexandro"
W04-0210,W98-1427,0,0.0407985,"Missing"
W04-0210,J99-3001,0,0.00884676,"as carried out, highlighting a number of problems with such schemes, ranging from issues with the annotation methodology to semantic issues. Proposals for annotating ‘coreference’ such as (Hirschman, 1998) have <unit finite=’finite-yes’ id=’u227’> <ne id=’ne546’ gf=’subj’> The drawing of <ne id=’ne547’ gf=’np-compl’>the corner cupboard </ne></ne> <unit finite=’no-finite’ id=’u228’>,or more probably <ne id=’ne548’ gf=’no-gf’> an engraving of <ne id=’ne549’ gf=’np-compl’>it </ne></ne> </unit>, ... </unit> <ante current=""ne549"" rel=""ident""> <anchor ID=""ne547""> </ante> Work such as (Sidner, 1979; Strube and Hahn, 1999), as well as our own preliminary analysis, suggested that indirect realization can play a crucial role in maintaining the CB. However, previous attempts at marking anaphoric information, particularly in the context of the MUC initiative, suggested that while agreement on identity relations is 6 The presence of more than one hanchori element indicates that the anaphoric expression is ambiguous. fairly easy to achieve, marking bridging references is hard; this was confirmed by Poesio and Vieira (1998). For these reasons, and to reduce the annotators’ work, we did not mark all relations. Besides"
W04-0210,J00-4005,0,0.021357,"Missing"
W04-0210,C98-1013,0,\N,Missing
W04-0210,poesio-kabadjov-2004-general,1,\N,Missing
W04-0707,P99-1048,0,0.632695,"Missing"
W04-0707,J94-4002,0,0.487605,"role, but we feel that evaluating DN detectors in conjunction with highperforming systems would give a better idea of the improvements that one may hope to achieve. 3 Do Discourse-New Detectors Help? Preliminary Evaluations Vieira and Poesio did not test their system without DN-detection, but Ng and Cardie’s results indicate that DN detection does improve results, if not dramatically, provided that the same_head test is run first–although their DN detector does not appear to improve results for pronouns, the one category for which detection of non-anaphoricity has been shown to be essential (Lappin and Leass, 1994). In order to evaluate how much improvement can we expect by just improving the DN detector, we did a few preliminary evaluations both with a reimplementation of Vieira and Poesio’s algorithm which does not include a discourse-new detector, running over treebank text as the original algorithm, and with a simple statistical coreference resolver attempting to resolve all anaphoric expressions and running over unparsed text, using Uryupina’s features for discourse-new detection, and over the same corpus used by Ng and Cardie (MUC-7). 3.1 How much does DN-detection help the Vieira / Poesio algorit"
W04-0707,J93-2004,0,0.0243492,"Missing"
W04-0707,P98-2143,0,0.162133,"detector, running over treebank text as the original algorithm, and with a simple statistical coreference resolver attempting to resolve all anaphoric expressions and running over unparsed text, using Uryupina’s features for discourse-new detection, and over the same corpus used by Ng and Cardie (MUC-7). 3.1 How much does DN-detection help the Vieira / Poesio algorithm? GUITAR (Poesio and Alexandrov-Kabadjov, 2004) is a general-purpose anaphoric resolver that includes an implementation of the Vieira / Poesio algorithm for definite descriptions and of Mitkov’s algorithm for pronoun resolution (Mitkov, 1998). It is implemented in Java, takes its input in XML format and returns as output its input augmented with the anaphoric relations it has discovered. GUITAR has been implemented in such a way as to be fully modular, making it possible, for example, to replace the DD resolution method with alternative implementations. It includes a pre-processor incorporating a chunker so that it can run over both hand-parsed and raw text. A version of GUITAR without the DN detection aspects of the Vieira / Poesio algorithm was evaluated on the GNOME corpus (Poesio, 2000; Poesio et al., 2004), which contains 554"
W04-0707,mitkov-2000-towards,0,0.0225374,"systems do not achieve very good results on pronoun and definite description resolution in comparison with specialized algorithms: e.g., although Ng and Cardie’s best version achieves F=65.8 on all anaphoric expressions, it only achieves F=29.6 for definite descriptions (cfr. Vieira and Poesio’s best result of F=77), and F=28.2 for pronouns (as opposed to results as high as F=80 obtained by the pronoun resolution algorithms evaluated in (Tetreault, 2001)). Clearly these systems can only be properly compared by evaluating them all on the same corpora and the same data, and discussion such as (Mitkov, 2000) suggest caution in interpreting some of the results discussed in the literature as pre- and postprocessing often plays a crucial role, but we feel that evaluating DN detectors in conjunction with highperforming systems would give a better idea of the improvements that one may hope to achieve. 3 Do Discourse-New Detectors Help? Preliminary Evaluations Vieira and Poesio did not test their system without DN-detection, but Ng and Cardie’s results indicate that DN detection does improve results, if not dramatically, provided that the same_head test is run first–although their DN detector does not"
W04-0707,C02-1139,0,0.645786,"Missing"
W04-0707,P02-1014,0,0.337227,"Missing"
W04-0707,J98-2001,1,0.872377,"Missing"
W04-0707,J04-3003,1,0.833733,"Missing"
W04-0707,poesio-2000-annotating,1,0.842899,"tkov’s algorithm for pronoun resolution (Mitkov, 1998). It is implemented in Java, takes its input in XML format and returns as output its input augmented with the anaphoric relations it has discovered. GUITAR has been implemented in such a way as to be fully modular, making it possible, for example, to replace the DD resolution method with alternative implementations. It includes a pre-processor incorporating a chunker so that it can run over both hand-parsed and raw text. A version of GUITAR without the DN detection aspects of the Vieira / Poesio algorithm was evaluated on the GNOME corpus (Poesio, 2000; Poesio et al., 2004), which contains 554 definite descriptions, of which 180 anaphoric, and 305 third-person pronouns, of which 217 anaphoric. The results for definite descriptions over hand-parsed text are shown in Table 6. Total 180 Res 182 Corr 121 NM 43 WM 16 SM 45 R 67.2 P 66.5 F 66.8 Table 6: Evaluation of the GUITAR system without DN detection over a hand-annotated treebank GUITAR without a DN recognizer takes 182 DD s (Res) as anaphoric, resolving 121 of them correctly (Corr); of the 182 DDs it attempts to resolve, only 16 are incorrectly resolved (WM); almost three times that number"
W04-0707,J01-4003,0,0.0318996,"isolated from anaphoric resolution (witness the Ng and Cardie results). One problem with some of the machine learning approaches to coreference is that these systems do not achieve very good results on pronoun and definite description resolution in comparison with specialized algorithms: e.g., although Ng and Cardie’s best version achieves F=65.8 on all anaphoric expressions, it only achieves F=29.6 for definite descriptions (cfr. Vieira and Poesio’s best result of F=77), and F=28.2 for pronouns (as opposed to results as high as F=80 obtained by the pronoun resolution algorithms evaluated in (Tetreault, 2001)). Clearly these systems can only be properly compared by evaluating them all on the same corpora and the same data, and discussion such as (Mitkov, 2000) suggest caution in interpreting some of the results discussed in the literature as pre- and postprocessing often plays a crucial role, but we feel that evaluating DN detectors in conjunction with highperforming systems would give a better idea of the improvements that one may hope to achieve. 3 Do Discourse-New Detectors Help? Preliminary Evaluations Vieira and Poesio did not test their system without DN-detection, but Ng and Cardie’s result"
W04-0707,P03-2012,1,0.896322,"98), but whereas the inclusion of detectors for non-anaphoric pronouns in algorithms such as Lappin and Leass’ (1994) leads to clear improvements in precision, the improvements in anaphoric DD resolution (as opposed to classification) brought about by the detectors were rather small. In fact, Ng and Cardie (2002a) challenged the motivation for the inclusion of such detectors, reporting no improvements, or even worse performance. We re-examine the literature on the topic in detail, and propose a revised algorithm, taking advantage of the improved discourse-new detection techniques developed by Uryupina (2003). Vieira and Poesio (2000) proposed an algorithm for definite description resolution that incorporates a number of heuristics for detecting discourse-new (henceforth: DN) descriptions. But whereas the inclusion of detectors for non-anaphoric pronouns (e.g., It in It’s raining) in algorithms such as Lappin and Leass’ (1994) leads to clear improvements in precision, the improvements in anaphoric DD resolution (as opposed to classification) brought about by the detectors were rather small. In fact, Ng and Cardie (2002a) challenged the motivation for the inclusion of such detectors, reporting no i"
W04-0707,J00-4003,1,0.933692,"Missing"
W04-0707,C98-2138,0,\N,Missing
W04-2327,N01-1002,1,0.937257,"roject (McKelvie et al., 2001), whose goal was to develop annotation tools suitable for different types of dialogue annotation. The scheme has served as the basis for a number of annotation projects, such as the development of the GNOME corpus (Poesio, 2000a) and, more recently, of the VENEX corpus of anaphora in Italian spoken dialogue and text (Poesio et al., 2004a). The GNOME corpus has been used to study salience, particularly as formalized in Centering theory (Poesio et al., 2004c), to develop statistical models of natural language generation (e.g., (Poesio, 2000a; Henschel et al., 2000; Cheng et al., 2001; Cheng, 2001; Karamanis, 2003)) and to evaluate anaphora resolution systems, with a special focus on the resolution of bridging references (Poesio, 2003; Poesio and Alexandrov-Kabadjov, 2004; Poesio et al., 2004b). Aspects of the scheme have been implemented in annotation tools including MMAX (M¨uller and Strube, 2003) and the Annotator tool developed by ILSP. As a result of this work, many aspects of the proposals concerning anaphoric annotation made in MATE and GNOME have been subjected to a thorough test. In this paper we discuss some of the lessons learned through this work, some issues t"
W04-2327,J95-2003,0,0.0391449,"ed corpus for a number of studies, but did resulted in a number of problems, the main among which were that the annotators had to be very careful not to damage other annotations; that annotators working on one level were occasionally confused by annotations for other levels; and that the annotation work had to be organized in a careful sequential way even for levels that could have been annotated independently. The main new aspect of the markup scheme, especially as far as our studies of salience were concerned, are the elements used to annotate potential utterances in the sense of Centering (Grosz et al., 1995). In order not to prejudge the answer to the question of which text constituents are best viewed as utterances, we used a ‘generic’ element called huniti to mark up finite and non-finite clauses, but also parentheticals and appositions, elements of bulleted lists, etc. The following example illustrates both the use of huniti elements and of the elements hnei and hantei replacing hdei and hlinki: (5) &lt;unit finite=’finite-yes’ id=’u227’&gt; &lt;ne id=’ne546’ gf=’subj’&gt; The drawing of &lt;ne id=’ne547’ gf=’np-compl’&gt;the corner cupboard &lt;/ne&gt; &lt;/ne&gt; &lt;unit finite=’no-finite’ id=’u228’&gt;, or more probably &lt;ne"
W04-2327,C00-1045,1,0.926495,"d as part of the MATE project (McKelvie et al., 2001), whose goal was to develop annotation tools suitable for different types of dialogue annotation. The scheme has served as the basis for a number of annotation projects, such as the development of the GNOME corpus (Poesio, 2000a) and, more recently, of the VENEX corpus of anaphora in Italian spoken dialogue and text (Poesio et al., 2004a). The GNOME corpus has been used to study salience, particularly as formalized in Centering theory (Poesio et al., 2004c), to develop statistical models of natural language generation (e.g., (Poesio, 2000a; Henschel et al., 2000; Cheng et al., 2001; Cheng, 2001; Karamanis, 2003)) and to evaluate anaphora resolution systems, with a special focus on the resolution of bridging references (Poesio, 2003; Poesio and Alexandrov-Kabadjov, 2004; Poesio et al., 2004b). Aspects of the scheme have been implemented in annotation tools including MMAX (M¨uller and Strube, 2003) and the Annotator tool developed by ILSP. As a result of this work, many aspects of the proposals concerning anaphoric annotation made in MATE and GNOME have been subjected to a thorough test. In this paper we discuss some of the lessons learned through this"
W04-2327,M98-1029,0,0.432251,"ations, there can be no such thing as a general-purpose anaphoric annotation instructions. On the other hand, we also believed that it is possible to design a general purpose markup scheme (and therefore, general-purpose tools) that could then be used in different ways for different projects. The approach taken in MATE was then to design a general markup scheme (the ‘meta-scheme’) and then to show its basic building blocks could be used to implement different types of anaphoric annotation, including some of the most popular schemes for ’coreference annotation,’ such as the MUC scheme (MUCCS) (Hirschman, 1998), Passonneau’s DRAMA scheme (1997) , and the scheme used for annotation of references to landmarks in the MapTask corpus. In this section we summarize the most distinctive features of the proposals resulting from this basic assumption. The full description of the MATE scheme is available from the MATE project pages at http://mate.nis.sdu.dk/. 2.1 Coreference, Anaphora and Discourse Modeling The MATE scheme differs from the best-known scheme for annotating ‘coreference,’ MUCCS (Hirschman, 1998) both in the conceptualization underlying the annotation (i.e., what type of information should be ann"
W04-2327,J98-2001,1,0.871404,"sur un classement /hein classer LES FUSEES QUI ONT BIEN VOLE‘ ou QUI ONT MOINS BIEN VOLE‘ F: Alors donc / vous avez / ici / &lt;de ID=&quot;de_88&quot;&gt; les mode‘les de fuse’es &lt;/de&gt; M: Oui F: Et vous allez essayer de vous mettre d’accord sur un classement /hein classer &lt;de ID=&quot;de_89&quot;&gt; les fuse’es qui ont bien vole’ &lt;/de&gt; ou &lt;de ID=&quot;de_90&quot;&gt; qui ont moins bien vole’ &lt;/de&gt; &lt;link href=&quot;coref.xml#id(de_89)&quot;&gt; &lt;anchor href=&quot;coref.xml#id(de_88)&quot; type=&quot;subset &quot; /&gt; &lt;/link&gt; &lt;link href=&quot;coref.xml#id(de_90)&quot; type=&quot;subset &quot; &gt; &lt;anchor href=&quot;coref.xml#id(de_88)&quot;/&gt; &lt;/link&gt; It was pointed out, however, that the results of Poesio and Vieira (1998) indicated that this type of annotation could be highly unreliable. References to the Visual Situation A special huniversei element was suggested for MapTaskstyle annotations of references to visible objects. The huniversei element containing one huei element for each object in the visual scene; including such elements in an annotation makes it possible to use hlinki elements to annotate references to such objects.4 Cases in which the participants to a conversation have different visual situations, as in the MapTask dialogues, can be handled by having separate universes, one for each participa"
W04-2327,W99-0309,0,0.231458,"Department of Computer Science and Centre for Cognitive Science United Kingdom Abstract In the five years since it was proposed, the MATE scheme for anaphoric annotation has been used in a variety of annotation projects, and the resulting corpora have been used to study both anaphora resolution and NL generation. Annotation tools inspired by the proposals have been used in some of these projects. In this paper we discuss these first experiences with the scheme, some lessons that have been learned, and suggest a few modifications. 1 Introduction The MATE ‘meta-scheme’ for anaphora annotation (Poesio et al., 1999) is one of the annotation schemes developed as part of the MATE project (McKelvie et al., 2001), whose goal was to develop annotation tools suitable for different types of dialogue annotation. The scheme has served as the basis for a number of annotation projects, such as the development of the GNOME corpus (Poesio, 2000a) and, more recently, of the VENEX corpus of anaphora in Italian spoken dialogue and text (Poesio et al., 2004a). The GNOME corpus has been used to study salience, particularly as formalized in Centering theory (Poesio et al., 2004c), to develop statistical models of natural l"
W04-2327,P00-1051,0,0.0421278,"clusions concerning advantages and disadvantages of the MATE scheme that can be drawn from them. 5 A second range of issues considered in the MATE scheme had to do with dialogue phenomena, such as non-contiguous elements; we will not consider these issues here. 3.1 Annotation work related to the GNOME project The most direct application of the ideas discussed above was found in the annotation work undertaken as part of the GNOME project. GNOME was concerned with the empirical investigation of the aspects of discourse that appear to affect generation, especially salience (Pearson et al., 2000; Poesio et al., 2000; Poesio and Di Eugenio, 2001; Poesio and Nissim, 2001; Poesio et al., 2004c). Particular attention was paid to the factors affecting the generation of pronouns (Pearson et al., 2000; Henschel et al., 2000), demonstratives (Poesio and Nygren-Modjeska, To appear) possessives (Poesio and Nissim, 2001) and definites in general (Poesio, 2004). These results, and the annotated corpus, were applied to the development of both symbolic and statistical natural language generation algorithms with the application of these empirical results to natural language generation, from sentence planning (Poesio, 2"
W04-2327,P04-1019,1,0.851586,"rst experiences with the scheme, some lessons that have been learned, and suggest a few modifications. 1 Introduction The MATE ‘meta-scheme’ for anaphora annotation (Poesio et al., 1999) is one of the annotation schemes developed as part of the MATE project (McKelvie et al., 2001), whose goal was to develop annotation tools suitable for different types of dialogue annotation. The scheme has served as the basis for a number of annotation projects, such as the development of the GNOME corpus (Poesio, 2000a) and, more recently, of the VENEX corpus of anaphora in Italian spoken dialogue and text (Poesio et al., 2004a). The GNOME corpus has been used to study salience, particularly as formalized in Centering theory (Poesio et al., 2004c), to develop statistical models of natural language generation (e.g., (Poesio, 2000a; Henschel et al., 2000; Cheng et al., 2001; Cheng, 2001; Karamanis, 2003)) and to evaluate anaphora resolution systems, with a special focus on the resolution of bridging references (Poesio, 2003; Poesio and Alexandrov-Kabadjov, 2004; Poesio et al., 2004b). Aspects of the scheme have been implemented in annotation tools including MMAX (M¨uller and Strube, 2003) and the Annotator tool devel"
W04-2327,J04-3003,1,0.531481,"Missing"
W04-2327,poesio-2000-annotating,1,0.954314,"ls inspired by the proposals have been used in some of these projects. In this paper we discuss these first experiences with the scheme, some lessons that have been learned, and suggest a few modifications. 1 Introduction The MATE ‘meta-scheme’ for anaphora annotation (Poesio et al., 1999) is one of the annotation schemes developed as part of the MATE project (McKelvie et al., 2001), whose goal was to develop annotation tools suitable for different types of dialogue annotation. The scheme has served as the basis for a number of annotation projects, such as the development of the GNOME corpus (Poesio, 2000a) and, more recently, of the VENEX corpus of anaphora in Italian spoken dialogue and text (Poesio et al., 2004a). The GNOME corpus has been used to study salience, particularly as formalized in Centering theory (Poesio et al., 2004c), to develop statistical models of natural language generation (e.g., (Poesio, 2000a; Henschel et al., 2000; Cheng et al., 2001; Cheng, 2001; Karamanis, 2003)) and to evaluate anaphora resolution systems, with a special focus on the resolution of bridging references (Poesio, 2003; Poesio and Alexandrov-Kabadjov, 2004; Poesio et al., 2004b). Aspects of the scheme h"
W04-2327,W03-2117,0,0.0801504,"Missing"
W04-2327,J99-3001,0,0.0584954,"Missing"
W04-2327,J00-4005,0,0.243721,"Missing"
W04-3221,P99-1008,0,0.292295,"owledge, however, no attempt has been made by computational linguists to use the attributes themselves in such vectors: i.e., to learn that the description of the concept dog includes elements such as (dog color) or (dog size). This is surprising when considering that most models of concepts in the AI literature are based on such attributes (Brachman and Levesque, 1985). Two problems need to be addressed when trying to identify concept attributes. The first problem is that values are easier to extract. We found, however, that patterns like the X of the dog, poesio@essex.ac.uk already used in (Berland and Charniak, 1999; Poesio et al, 2002) to find part-of relations (using techniques derived from those used in (Hearst, 1998; Caraballo, 1999) to find hyponymy relations) are quite effective at finding attributes. A second problem might be that instances of such patterns are less frequent than those used to extract values, even in large corpora such as the British National Corpus (BNC). But this problem, as well, is less serious when using the Web as a corpus (Kilgarriff and Schuetze, 2003; Keller and Lapata, 2003; Markert et al, submitted). We report on two experiments whose goal was to test whether identifyin"
W04-3221,P99-1016,0,0.0170678,"that the description of the concept dog includes elements such as (dog color) or (dog size). This is surprising when considering that most models of concepts in the AI literature are based on such attributes (Brachman and Levesque, 1985). Two problems need to be addressed when trying to identify concept attributes. The first problem is that values are easier to extract. We found, however, that patterns like the X of the dog, poesio@essex.ac.uk already used in (Berland and Charniak, 1999; Poesio et al, 2002) to find part-of relations (using techniques derived from those used in (Hearst, 1998; Caraballo, 1999) to find hyponymy relations) are quite effective at finding attributes. A second problem might be that instances of such patterns are less frequent than those used to extract values, even in large corpora such as the British National Corpus (BNC). But this problem, as well, is less serious when using the Web as a corpus (Kilgarriff and Schuetze, 2003; Keller and Lapata, 2003; Markert et al, submitted). We report on two experiments whose goal was to test whether identifying attributes leads to better lexical descriptions of concepts. We do this by comparing the results obtained by using attribu"
W04-3221,W02-0908,0,0.0256681,"attributes (e.g., color). We compared models of concepts based on values with models based on attributes, using lexical clustering as the basis for comparison. We find that attribute-based models work better than value-based ones, and result in shorter descriptions; but that mixed models including both the best attributes and the best values work best of all. 1 Introduction In most recent research on concept acquisition from corpora (e.g., for lexicon construction), concepts are viewed as vectors of relations, or properties, extracted from syntactic structures (Grefenstette, 1993; Lin, 1998; Curran and Moens, 2002; Kilgarriff, 2003, and many others). These properties often specify values of attributes such as color, shape, or size: for example, the vector used by Lin (1998) for the concept dog includes the property (dog adj-mod brown). (We will use the term values here to refer to any modifier.) To our knowledge, however, no attempt has been made by computational linguists to use the attributes themselves in such vectors: i.e., to learn that the description of the concept dog includes elements such as (dog color) or (dog size). This is surprising when considering that most models of concepts in the AI"
W04-3221,P93-1023,0,0.183481,"r, we use extended Jaccard, which was found to produce more accurate results than the cosine function in similar tasks (Karypis, 2002; Curran and Moens, 2003). In CLUTO, the extended Jaccard function works only with the graph partitioning algorithm. 2.4 each system cluster, finding the class of each concept in the model clusters, and determining the majority class. The cluster is then labeled with this class; the concepts belonging to it are taken to be correctly clustered, whereas the remaining concepts are judged to be incorrectly clustered. In the contingency table evaluation (Swets, 1969; Hatzivassiloglou and McKeown, 1993), the clusters are converted into two lists (one for the system clusters and one for the model clusters) of yes-no answers to the question &quot;Does the pair of concepts occur in the same cluster?&quot; for each pair of concepts. A contingency table is then built, from which recall (R), precision (P), fallout, and F measures can be computed. For example, if the model clusters are: (A, B, C) and (D), and the system clusters are: (A, B) and (C, D), the yes-no lists are as in Table 2, and the contingency table is as in Table 3. Question Does the pair (A, the same cluster? Does the pair (A, the same cluste"
W04-3221,J03-3005,0,0.0165239,"ct. We found, however, that patterns like the X of the dog, poesio@essex.ac.uk already used in (Berland and Charniak, 1999; Poesio et al, 2002) to find part-of relations (using techniques derived from those used in (Hearst, 1998; Caraballo, 1999) to find hyponymy relations) are quite effective at finding attributes. A second problem might be that instances of such patterns are less frequent than those used to extract values, even in large corpora such as the British National Corpus (BNC). But this problem, as well, is less serious when using the Web as a corpus (Kilgarriff and Schuetze, 2003; Keller and Lapata, 2003; Markert et al, submitted). We report on two experiments whose goal was to test whether identifying attributes leads to better lexical descriptions of concepts. We do this by comparing the results obtained by using attributes or more general modifiers – that we will simply call values – as elements of concept vectors used to identify concept similarities via clustering. In Section 2, we discuss how Web data were used to build attribute- and value- based concept vectors, and our clustering and evaluation methods. In Section 3, we discuss a first experiment using the set of concepts used in (Lu"
W04-3221,P98-2127,0,0.0750994,"ms of their attributes (e.g., color). We compared models of concepts based on values with models based on attributes, using lexical clustering as the basis for comparison. We find that attribute-based models work better than value-based ones, and result in shorter descriptions; but that mixed models including both the best attributes and the best values work best of all. 1 Introduction In most recent research on concept acquisition from corpora (e.g., for lexicon construction), concepts are viewed as vectors of relations, or properties, extracted from syntactic structures (Grefenstette, 1993; Lin, 1998; Curran and Moens, 2002; Kilgarriff, 2003, and many others). These properties often specify values of attributes such as color, shape, or size: for example, the vector used by Lin (1998) for the concept dog includes the property (dog adj-mod brown). (We will use the term values here to refer to any modifier.) To our knowledge, however, no attempt has been made by computational linguists to use the attributes themselves in such vectors: i.e., to learn that the description of the concept dog includes elements such as (dog color) or (dog size). This is surprising when considering that most model"
W04-3221,P93-1024,0,0.0360206,"about the furniture concepts. First, at least two concepts (seat and lounge) have more than one sense in WordNet. Seat was clustered with body part concepts, which is acceptable if we think of seat as &quot;the fleshy part of the human body that you sit on&quot; (WordNet, sense 2). The same for lounge, which was clustered with buildings, which is consistent with its second sense in WordNet: &quot;a public room (as in a hotel or airport) with seating where people can wait&quot;. This indicates that techniques for differentiating between different senses are needed – e.g., using a soft clustering technique as in (Pereira et al, 1993) instead of a hard clustering technique. Second, furniture concepts may not have a common prototype that is shared by all of the member concepts. This is a well known problem in the prototype theory of concepts (Laurence and Margolis, 1999). The greater compactness of attribute-based representations vs. value-based ones was more evident in this second experiment. We collected 51,045 distinct values and 8,934 distinct attributes; the total number of value-concept relations is 1,026,335, compared to 422,621 attribute-concept relations. 5 Attributes and Values: A discussion Although our results s"
W04-3221,poesio-etal-2002-acquiring,1,0.873812,"has been made by computational linguists to use the attributes themselves in such vectors: i.e., to learn that the description of the concept dog includes elements such as (dog color) or (dog size). This is surprising when considering that most models of concepts in the AI literature are based on such attributes (Brachman and Levesque, 1985). Two problems need to be addressed when trying to identify concept attributes. The first problem is that values are easier to extract. We found, however, that patterns like the X of the dog, poesio@essex.ac.uk already used in (Berland and Charniak, 1999; Poesio et al, 2002) to find part-of relations (using techniques derived from those used in (Hearst, 1998; Caraballo, 1999) to find hyponymy relations) are quite effective at finding attributes. A second problem might be that instances of such patterns are less frequent than those used to extract values, even in large corpora such as the British National Corpus (BNC). But this problem, as well, is less serious when using the Web as a corpus (Kilgarriff and Schuetze, 2003; Keller and Lapata, 2003; Markert et al, submitted). We report on two experiments whose goal was to test whether identifying attributes leads to"
W04-3221,J91-4003,0,0.0674771,"Values: A discussion Although our results suggest that trying to identify attributes is beneficial, the notion of 'attribute' is not completely clear, and has been used in widely different ways in Knowledge Representation literature. An attempt of defining the notion has been made by Guarino (1992), who classifies attributes into relational and nonrelational attributes. Relational attributes include qualities such as color and position, and relational roles such as son and spouse. Non-relational attributes include parts such as wheel and engine. The Qualia Structure of the Generative Lexicon (Pustejovsky, 1991) is another attempt at identifying &quot;the essential attributes of an object as defined by the lexical item&quot;. Pustejovsky identifies four roles: Constitutive Role (Guarino's parts), Formal Role (Guarino's qualities), Agentive Role (Guarino's relational roles), and Telic Role (not included in Guarino's classification). Our analysis of the attribute data shows that the attributes we found can be mapped in the four roles of the Qualia structure. Table 8 shows how we manually mapped the top 50 attributes of the concept car to the Qualia roles and the Guarino's classes. This mapping is not trivial (e."
W04-3221,J05-3004,0,\N,Missing
W04-3221,C98-2122,0,\N,Missing
W04-3232,W98-1001,0,0.0607234,"( ) qlWb( ktab( , “book”) f3l ( ) ktb( &apos; ( , “hearts”) , “books”) The complexity of Arabic morphology has motivated a great deal of studies. Some of which especially concerned with broken plurals (McCarthy and Prince, 1990b; Kiraz, 1996a; Idrissi, 1997). These are successful to varying degrees, but have a main practical drawback in the context of information retrieval: they assume that words are fully vowelised. Unfortunately, short vowels are usually not written in published Arabic text, with the exception of the religious texts (e.g., the Holy Quran), poetry, and books for school children (Abuleil and Evens, 1998). 3. Different Approaches to BP Identification Figure 1: The process of mapping the root ktb ( pattern yf3l ( ). ) to the The Arabic number system has singular, dual, and plural. Plurals are traditionally distinguished into two categories: the regular (so-called sound) plurals, and the irregular (so-called broken) plurals. Sound Plurals are formed by appropriate suffixation (like English: hand hands). The sound masculine plural is formed by adding the suffix oun (!&quot;) in the nominative case and the suffix een (#) in the accusative & genitive cases. The sound feminine plural is formed by attachi"
W04-3232,C96-1017,0,0.0393368,"s. We give a brief overview of Arabic in Section 2. Several approaches to BP detection are discussed in Section 3, and their evaluation in Section 4. In Section 5, we present an improved light stemmer and its evaluation. Finally in Section 6, our conclusions are summarised. 2. Arabic Morphology and its Number System Arabic is a heavily inflected language. Its grammatical system is traditionally described in terms of a root-and-pattern structure, with about 10,000 roots (Ali, 1988). Roots such as drs ( ) and ktb ( ) are listed alphabetically in standard Arabic dictionaries like the Wehr-Cowan (Beesley, 1996). The root is the most basic verb form. Roots are categorized into: triliteral, quadriliteral, or rarely pentaliteral. Most words are derived from a finite set of roots formed by adding diacritics1 or affixes (prefixes, suffixes, and infixes) through an application of fixed patterns which are templates to help in deriving inflectional and derivational forms of a word. Theoretically, several hundreds of Arabic words can be derived from a single root. Traditional Arab grammarians describe Arabic morphology in terms of patterns associated with the basic root f3l ( , “to do”)- where f, 3, and l ar"
W05-0302,P98-1013,0,0.0768075,"Missing"
W05-0302,W99-0302,0,0.0733028,"Missing"
W05-0302,W01-1514,0,0.0273335,"k, NomBank and Coreference, the core predicate argument structures and referents for the arguments. One possible representation format would be to convert each annotation into features and values to be added to a larger feature structure. 1 The resulting feature structure would combine stand alone and offset annotation – it would include actual words and features from the text as well as special features that point to the actual text (character offsets) and, perhaps, syntactic trees (offsets along the lines of PropBank/NomBank). Alternative global annotation schemes include annotation graphs (Cieri & Bird, 2001), and MATE (Carletta, et. al., 1999). There are many areas in which the boundaries between these annotations have not been clearly defined, such as the treatment of support constructions and light verbs, as discussed below. Determining the most suitable format for the merged representation should be a top priority. NomBank would add arguments for report, trial, launch and beginning as follows: According to [Rel_report.01 reports], [Arg1 [ArgM-LOC sea [Rel_trial.01 trials] [Arg1 for [Arg1-CF_launch.01 a patrol boat] developed by Kazakhstan] are being conducted and the [ArgM-MNR formal] [Rel_lau"
W05-0302,hajicova-kucerova-2002-argument,0,0.0418506,"Missing"
W05-0302,W04-2705,1,0.895192,"Missing"
W05-0302,W04-0413,1,0.854761,"Missing"
W05-0302,miltsakaki-etal-2004-penn,0,0.0711906,"Missing"
W05-0302,J05-1004,1,0.327965,"fforts. This level could provide the foundation for a major advance in our ability to automatically extract salient relationships from text. This will in turn facilitate breakthroughs in message understanding, machine translation, fact retrieval, and information retrieval. Many recent annotation efforts for English have focused on pieces of the larger problem of semantic annotation, rather than producing a single unified representation like Head-driven Phrase Structure Grammar (Pollard and Sag 1994) or the Prague Dependency Tectogramatical Representation (Hajicova & Kucerova, 2002). PropBank (Palmer et al, 2005) annotates predicate argument structure anchored by verbs. NomBank (Meyers, et. al., 2004a) annotates predicate argument structure anchored by nouns. TimeBank (Pustejovsky et al, 2003) 2. The Component Annotation Schemata We describe below existing independent annotation efforts, each one of which is focused on a specific aspect of the semantic representation task: semantic role labeling, 5 Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 5–12, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics tuned to a hiring scenario (MUC-6, 1995),"
W05-0302,W99-0309,0,0.0605805,"esolved in order to bring these different layers together seamlessly. Most of these approaches have annotated the same type of data, Wall Street Journal text, so it is also important to demonstrate that the annotation can be extended to other genres such as spoken language. The demonstration of success for the extensions would be the training of accurate statistical semantic taggers. Coreference: Coreference involves the detection of subsequent mentions of invoked entities, as in George Bush,… he…. Researchers at Essex (UK) were responsible for the coreference markup scheme developed in MATE (Poesio et al, 1999; Poesio, 2004a), partially implemented in the annotation tool MMAX and now proposed as an ISO standard; and have been responsible for the creation of two small, but commonly used anaphorically annotated corpora – the Vieira / Poesio subset of the Penn Treebank (Poesio and Vieira, 1998), and the GNOME corpus (Poesio, 2004a). Parallel coreference annotation efforts funded by ACE have resulted in similar guidelines, exemplified by BBN’s recent annotation of Named Entities, common nouns and pronouns. These two approaches provide a suitable springboard for an attempt at achieving a community conse"
W05-0302,J98-2001,1,0.810969,"monstration of success for the extensions would be the training of accurate statistical semantic taggers. Coreference: Coreference involves the detection of subsequent mentions of invoked entities, as in George Bush,… he…. Researchers at Essex (UK) were responsible for the coreference markup scheme developed in MATE (Poesio et al, 1999; Poesio, 2004a), partially implemented in the annotation tool MMAX and now proposed as an ISO standard; and have been responsible for the creation of two small, but commonly used anaphorically annotated corpora – the Vieira / Poesio subset of the Penn Treebank (Poesio and Vieira, 1998), and the GNOME corpus (Poesio, 2004a). Parallel coreference annotation efforts funded by ACE have resulted in similar guidelines, exemplified by BBN’s recent annotation of Named Entities, common nouns and pronouns. These two approaches provide a suitable springboard for an attempt at achieving a community consensus on coreference. PropBank: The Penn Proposition Bank focuses on the argument structure of verbs, and provides a corpus annotated with semantic roles, including participants traditionally viewed as arguments and adjuncts. An important goal is to provide consistent semantic role label"
W05-0302,W04-2709,0,\N,Missing
W05-0302,W04-2704,1,\N,Missing
W05-0302,J93-2004,0,\N,Missing
W05-0302,W04-0210,1,\N,Missing
W05-0302,C98-1013,0,\N,Missing
W05-0302,poesio-kabadjov-2004-general,1,\N,Missing
W05-0302,W04-2327,1,\N,Missing
W05-0311,M98-1029,0,0.0956869,"United Kingdom Abstract We report the results of a study of the reliability of anaphoric annotation which (i) involved a substantial number of naive subjects, (ii) used Krippendorff’s α instead of K to measure agreement, as recently proposed by Passonneau, and (iii) allowed annotators to mark anaphoric expressions as ambiguous. 1 INTRODUCTION We tackle three limitations with the current state of the art in the annotation of anaphoric relations. The first problem is the lack of a truly systematic study of agreement on anaphoric annotation in the literature: none of the studies we are aware of (Hirschman, 1998; Poesio and Vieira, 1998; Byron, 2003; Poesio, 2004) is completely satisfactory, either because only a small number of coders was involved, or because agreement beyond chance couldn’t be assessed for lack of an appropriate statistic, a situation recently corrected by Passonneau (2004). The second limitation, which is particularly serious when working on dialogue, is our still limited understanding of the degree of agreement on references to abstract objects, as in discourse deixis (Webber, 1991; Eckert and Strube, 2001). The third shortcoming is a problem that affects all types of semantic an"
W05-0311,W03-2117,0,0.0588724,"Missing"
W05-0311,passonneau-2004-computing,0,0.578984,"s to mark anaphoric expressions as ambiguous. 1 INTRODUCTION We tackle three limitations with the current state of the art in the annotation of anaphoric relations. The first problem is the lack of a truly systematic study of agreement on anaphoric annotation in the literature: none of the studies we are aware of (Hirschman, 1998; Poesio and Vieira, 1998; Byron, 2003; Poesio, 2004) is completely satisfactory, either because only a small number of coders was involved, or because agreement beyond chance couldn’t be assessed for lack of an appropriate statistic, a situation recently corrected by Passonneau (2004). The second limitation, which is particularly serious when working on dialogue, is our still limited understanding of the degree of agreement on references to abstract objects, as in discourse deixis (Webber, 1991; Eckert and Strube, 2001). The third shortcoming is a problem that affects all types of semantic annotation. In all annotation studies we are aware of,1 the fact that an expression may not have a unique interpretation in the context of its 1 The one exception is Rosenberg and Binkowski (2004). occurrence is viewed as a problem with the annotation scheme, to be fixed by, e.g., develo"
W05-0311,J98-2001,1,0.54707,"stract We report the results of a study of the reliability of anaphoric annotation which (i) involved a substantial number of naive subjects, (ii) used Krippendorff’s α instead of K to measure agreement, as recently proposed by Passonneau, and (iii) allowed annotators to mark anaphoric expressions as ambiguous. 1 INTRODUCTION We tackle three limitations with the current state of the art in the annotation of anaphoric relations. The first problem is the lack of a truly systematic study of agreement on anaphoric annotation in the literature: none of the studies we are aware of (Hirschman, 1998; Poesio and Vieira, 1998; Byron, 2003; Poesio, 2004) is completely satisfactory, either because only a small number of coders was involved, or because agreement beyond chance couldn’t be assessed for lack of an appropriate statistic, a situation recently corrected by Passonneau (2004). The second limitation, which is particularly serious when working on dialogue, is our still limited understanding of the degree of agreement on references to abstract objects, as in discourse deixis (Webber, 1991; Eckert and Strube, 2001). The third shortcoming is a problem that affects all types of semantic annotation. In all annotati"
W05-0311,W99-0309,0,0.0165983,"r, even better, to develop methods to identify genuinely ambiguous expressions–the ultimate goal of this work. The paper is organized as follows. We first briefly review previous work on anaphoric annotation and on reliability indices. We then discuss our experiment with anaphoric annotation, and its results. Finally, we discuss the implications of this work. 2 ANNOTATING ANAPHORA It is not our goal at this stage to propose a new scheme for annotating anaphora. For this study we simply developed a coding manual for the purposes of our experiment, broadly based on the approach adopted in MATE (Poesio et al., 1999) and GNOME (Poesio, 2004), but introducing new types of annotation (ambiguous anaphora, and a simple form of discourse deixis) while simplifying other aspects (e.g., by not annotating bridging references). The task of ‘anaphoric annotation’ discussed here is related, although different from, the task of annotating ‘coreference’ in the sense of the so-called MUCSS scheme for the MUC -7 initiative (Hirschman, 1998). This scheme, while often criticized, is still widely used, and has been the basis of coreference annotation for the ACE initiative in the past two years. It suffers however from a nu"
W05-0311,N04-4020,0,0.0305547,"yond chance couldn’t be assessed for lack of an appropriate statistic, a situation recently corrected by Passonneau (2004). The second limitation, which is particularly serious when working on dialogue, is our still limited understanding of the degree of agreement on references to abstract objects, as in discourse deixis (Webber, 1991; Eckert and Strube, 2001). The third shortcoming is a problem that affects all types of semantic annotation. In all annotation studies we are aware of,1 the fact that an expression may not have a unique interpretation in the context of its 1 The one exception is Rosenberg and Binkowski (2004). occurrence is viewed as a problem with the annotation scheme, to be fixed by, e.g., developing suitably underspecified representations, as done particularly in work on wordsense annotation (Buitelaar, 1998; Palmer et al., 2005), but also on dialogue act tagging. Unfortunately, the underspecification solution only genuinely applies to cases of polysemy, not homonymy (Poesio, 1996), and anaphoric ambiguity is not a case of polysemy. Consider the dialogue excerpt in (1):2 it’s not clear to us (nor was to our annotators, as we’ll see below) whether the demonstrative that in utterance unit 18.1 r"
W05-0311,J00-4005,0,0.0736232,"Missing"
W05-0311,W04-2327,1,\N,Missing
W05-1003,W04-3221,1,0.783381,"uch relations. The earliest work of this type we are aware of is the work by Hearst (1998) on acquiring information about hyponymy (= IS-A links) by searching for instances of patterns such as NP {, NP}* or other NP (as in, e.g., bruises …. broken bones and other INJURIES). A similar approach was used by Berland and Charniak (1999) and Poesio et al (2002) to extract information about part-of relations using patterns such as the N of the N is …. (as in the wheel of the CAR is) and by Girju and Moldovan (2002) and Sanchez-Graillet and Poesio (2004) to extract causal relations. In previous work (Almuhareb and Poesio, 2004) we used this same approach to extract attributes, using the pattern “the * of the C [is|was]” (suggested by, e.g., (Woods, 1975) as a test for ‘attributehood’) to search for attributes of concept C in the Web, using the Google API. Although the information extracted this way proved a useful addition to our lexical representations from a clustering perspective, from the point of view of lexicon building this approach results in too many false positives, as very few syntactic constructions are used to express exclusively one type of semantic relation. For example, the ‘attributes’ of deer extra"
W05-1003,P99-1008,0,0.505611,"sis of the sets of features used in work such as (Vinson et al, 2003) (see Discussion). our disposal, WordNet (Fellbaum, 1998) contains very little information that would be considered as being about ‘attributes’—only information about parts, not about qualities such as height, or even to the values of such attributes in the adjective network—and this information is still very sparse. On the other hand, the only work on the extraction of lexical semantic relations we are aware of has concentrated on the type of relations found in WordNet: hyponymy (Hearst, 1998; Caraballo, 1999) and meronymy (Berland and Charniak, 1999; Poesio et al, 2002).2 The work discussed here could be perhaps best described as an example of empirical ontology: using linguistics and philosophical ideas to improve the results of empirical work on lexical / ontology acquisition, and vice versa, using findings from empirical analysis to question some of the assumptions of theoretical work on ontology and the lexicon. Specifically, we discuss work on the acquisition of (nominal) concept attributes whose goal is twofold: on the one hand, to clarify the notion of ‘attribute’ and its role in lexical semantics, if any; on the other, to develop"
W05-1003,J95-4004,0,0.0178556,"e usage familiarity of the word, which can also be found in WordNet. If the word is used more as a verb and the verbal usage is not rare, then again the system treats the noun as derived from the verb. To find nouns that are derived from adjectives we used simple heuristics based on suffixchecking. (This was also done by Berland and Charniak (1999).) All words that end with “ity” or “ness” are considered to be derived from adjectives. A noun not found to be derived from a verb or an adjective is assumed to be a basic noun root. In addition to derivational morphology, we used the Brill tagger (Brill, 1995) to filter out adjectives and other types of words that can occasionally be used as nouns such as better, first, and whole before training. Only nouns, base form verbs, and gerund form verbs were kept in the candidate attribute list. 4.2 Clustering Attributes Attributes are themselves concepts, at least in the sense that they have their own attributes: for example, a part of a car, such as a wheel, has its own parts (the tyre) its qualities (weight, diameter) etc. This observation suggests that it should be possible to find similar attributes in an unsupervised fashion by looking at their attr"
W05-1003,P99-1016,0,0.00767068,"carrying out a systematic analysis of the sets of features used in work such as (Vinson et al, 2003) (see Discussion). our disposal, WordNet (Fellbaum, 1998) contains very little information that would be considered as being about ‘attributes’—only information about parts, not about qualities such as height, or even to the values of such attributes in the adjective network—and this information is still very sparse. On the other hand, the only work on the extraction of lexical semantic relations we are aware of has concentrated on the type of relations found in WordNet: hyponymy (Hearst, 1998; Caraballo, 1999) and meronymy (Berland and Charniak, 1999; Poesio et al, 2002).2 The work discussed here could be perhaps best described as an example of empirical ontology: using linguistics and philosophical ideas to improve the results of empirical work on lexical / ontology acquisition, and vice versa, using findings from empirical analysis to question some of the assumptions of theoretical work on ontology and the lexicon. Specifically, we discuss work on the acquisition of (nominal) concept attributes whose goal is twofold: on the one hand, to clarify the notion of ‘attribute’ and its role in lexical se"
W05-1003,poesio-etal-2002-acquiring,1,0.887949,"used in work such as (Vinson et al, 2003) (see Discussion). our disposal, WordNet (Fellbaum, 1998) contains very little information that would be considered as being about ‘attributes’—only information about parts, not about qualities such as height, or even to the values of such attributes in the adjective network—and this information is still very sparse. On the other hand, the only work on the extraction of lexical semantic relations we are aware of has concentrated on the type of relations found in WordNet: hyponymy (Hearst, 1998; Caraballo, 1999) and meronymy (Berland and Charniak, 1999; Poesio et al, 2002).2 The work discussed here could be perhaps best described as an example of empirical ontology: using linguistics and philosophical ideas to improve the results of empirical work on lexical / ontology acquisition, and vice versa, using findings from empirical analysis to question some of the assumptions of theoretical work on ontology and the lexicon. Specifically, we discuss work on the acquisition of (nominal) concept attributes whose goal is twofold: on the one hand, to clarify the notion of ‘attribute’ and its role in lexical semantics, if any; on the other, to develop methods to acquire s"
W05-1003,J01-3003,0,\N,Missing
W05-1003,sanchez-graillet-poesio-2004-acquiring,1,\N,Missing
W07-1029,W07-1000,0,\N,Missing
W07-1524,N06-2015,0,0.0181765,"tion at one level may make other levels of annotation unusable as well, and that it is not possible for two annotators to work on different types of annotation for the same file at the same time. Most current annotation efforts, therefore, tend to adopt the ’multilevel’ approach pioneered during the development of the MAPTASK corpus and then developed as part of work on the EU-funded MATE project (McKelvie et al., 2001), in which each aspect of interpretation is annotated in a separate level, independently maintained. This approach is being followed, for instance, in the O NTO N OTES project (Hovy et al., 2006) and the SAMMIE project (Kruijff-Korbayova et al., 2006a). For the annotation of the LUNA corpus, we decided to follow the multilevel approach as well. That allows us to achieve more granularity in the annotation of each of the levels and to investigate more easily dependencies between features that belong to different levels. Furthermore, we can use different specialized off-the-shelf annotation tools, splitting up the annotation task and thus facilitating consistent annotation. 2.3 Annotation levels The LUNA corpus will contain different types of information. The first levels are necessary t"
W07-1524,W03-0804,0,0.179794,"cular levels: examples include tools for segmentation and transcription of the speech signal like PRAAT (Boersma and Weenink, 2005) and T RANSCRIBER (Barras et al., 1998), the SALSA tools for FrameNetstyle annotation (Burchardt et al., 2006), and MMAX (M¨uller and Strube, 2003) for coreference annotation. Even in these cases, however, it may still be useful, or even necessary, to be able to visualize more than one level at once, or to ‘knit’ together2 multiple levels to create a file that can be used to train a model for a particular type of annotation. The Linguistic Annotation Framework by (Ide et al., 2003) was proposed as a unifying markup format to be used to synchronize heterogeneous markup formats for such purposes. In this paper, we discuss how the PAULA representation format, a standoff format inspired by the Linguistic Annotation Framework, is being used to synchronize multiple levels of annotation in the LUNA corpus, a corpus of spoken dialogues in multiple languages and multiple domains that is being created to support the development of robust spoken language understanding models for multilingual dialogue services. The corpus is richly annotated with linguistic information that is cons"
W07-1524,W06-2711,0,0.0335333,"Missing"
W07-1524,brugman-russel-2004-annotating,0,0.0337614,"nd employs a rich meta specification, which determines—based upon the individual corpus characteristics— the concrete linearization of the respective XML representation. Furthermore, it is accompanied by a JAVA API and a query tool, forming a valuable toolkit for corpus engineers who can adapt available resources to their specific needs. The ELAN format is used by a family of tools developed primarily for language documentation, of which the most advanced one is ELAN, a robust, ready-to-use tool for multi-level annotation of video. Its underlying data model is the Abstract Corpus Model (ACM) (Brugman and Russel, 2004). PAULA aims at an application scenario different from both of these formats. First, it builds upon the usage of specialized off-the-shelf annotation tools for the variety of annotation tasks. Both the NITE XML and ELAN approaches require additional effort and skills from the user, to add the required functionality, which PAULA aims to avoid. Second, PAULA takes care of merging the annotations from different sources, which is not in focus of ELAN or NITE. We presented the LUNA dialogue corpus and its representation format, the standoff exchange format PAULA . In contrast to other formats, PAUL"
W07-1524,burchardt-etal-2006-salto,0,0.0222042,"Infso, Unit E1 and in the Collaborative Research Center 632 “Information Structure”, funded by the German Science Foundation, http://www.sfb632.uni-potsdam.de. tate as well as maintain all annotation levels (cf. the SAMMIE annotation effort (Kruijff-Korbayov´a et al., 2006b)). However, it is often the case that specialized tools are developed to facilitate the annotation of particular levels: examples include tools for segmentation and transcription of the speech signal like PRAAT (Boersma and Weenink, 2005) and T RANSCRIBER (Barras et al., 1998), the SALSA tools for FrameNetstyle annotation (Burchardt et al., 2006), and MMAX (M¨uller and Strube, 2003) for coreference annotation. Even in these cases, however, it may still be useful, or even necessary, to be able to visualize more than one level at once, or to ‘knit’ together2 multiple levels to create a file that can be used to train a model for a particular type of annotation. The Linguistic Annotation Framework by (Ide et al., 2003) was proposed as a unifying markup format to be used to synchronize heterogeneous markup formats for such purposes. In this paper, we discuss how the PAULA representation format, a standoff format inspired by the Linguistic"
W07-1524,N03-4009,0,0.0254061,"rmation; specialized annotation tools will be used for the annotation at each of these levels. In order to synchronize these multiple layers of annotation, the PAULA standoff exchange format will be used. In this paper, we present the corpus and its PAULA -based architecture.1 1 Introduction XML standoff markup (Thompson and McKelvie, 1997; Dybkjær et al., 1998) is emerging as the cleanest way to organize multi-level annotations of corpora. In many of the current annotation efforts based on standoff a single multi-purpose tool such as the NITE XML Toolkit (Carletta et al., 2003) or WordFreak (Morton and LaCivita, 2003) is used to anno1 The members of the LUNA project consortium are: Piedmont Consortium for Information Systems (IT), University of Trento (IT), Loquendo SpA (IT), RWTH-Aachen (DE), University of Avignon (FR), France Telecom R&D Division S.A. (FR), Polish-Japanese Institute of Information Technology (PL) and the Institute for Computer Science of the Polish Academy of Sciences (PL), http://www.ist-luna.eu. This research was performed in the LUNA project funded by the EC, DG Infso, Unit E1 and in the Collaborative Research Center 632 “Information Structure”, funded by the German Science Foundation"
W07-1524,W03-2117,0,0.0404835,"Missing"
W07-1524,J93-2004,0,\N,Missing
W07-1524,W00-1003,0,\N,Missing
W07-1524,P98-1013,0,\N,Missing
W07-1524,C98-1013,0,\N,Missing
W08-2230,N06-2015,0,0.0325368,"antic information from text is the lack of semantically annotated corpora large enough to be used to train and evaluate semantic interpretation methods. Recent efforts to create resources to support large evaluation initiatives in the USA such as Automatic Context Extraction (ACE), Translingual Information Detection, Extraction and Summarization (TIDES), and GALE are beginning to change this, but just at a point when the community is beginning to realize that even the 1M word annotated corpora created in substantial efforts such as Prop-Bank (Palmer et al., 2005) and the OntoNotes initiative (Hovy et al., 2006) are likely to be too small. Unfortunately, the creation of 100M-plus corpora via hand annotation is likely to be prohibitively expensive. Such a large hand-annotation effort would be even less sensible in the case of semantic annotation tasks such as coreference or wordsense disambiguation, given on the one side the greater difficulty of agreeing on a “neutral” theoretical framework, on the other the difficulty of achieving more than moderate agreement on semantic judgments (Poesio and Artstein, 2005). The ANAWIKI project1 presents an effort to create high-quality, large-scale anaphorically a"
W08-2230,J05-1004,0,0.0351836,"cle to progress towards systems able to extract semantic information from text is the lack of semantically annotated corpora large enough to be used to train and evaluate semantic interpretation methods. Recent efforts to create resources to support large evaluation initiatives in the USA such as Automatic Context Extraction (ACE), Translingual Information Detection, Extraction and Summarization (TIDES), and GALE are beginning to change this, but just at a point when the community is beginning to realize that even the 1M word annotated corpora created in substantial efforts such as Prop-Bank (Palmer et al., 2005) and the OntoNotes initiative (Hovy et al., 2006) are likely to be too small. Unfortunately, the creation of 100M-plus corpora via hand annotation is likely to be prohibitively expensive. Such a large hand-annotation effort would be even less sensible in the case of semantic annotation tasks such as coreference or wordsense disambiguation, given on the one side the greater difficulty of agreeing on a “neutral” theoretical framework, on the other the difficulty of achieving more than moderate agreement on semantic judgments (Poesio and Artstein, 2005). The ANAWIKI project1 presents an effort to"
W08-2230,W05-0311,1,0.92178,"created in substantial efforts such as Prop-Bank (Palmer et al., 2005) and the OntoNotes initiative (Hovy et al., 2006) are likely to be too small. Unfortunately, the creation of 100M-plus corpora via hand annotation is likely to be prohibitively expensive. Such a large hand-annotation effort would be even less sensible in the case of semantic annotation tasks such as coreference or wordsense disambiguation, given on the one side the greater difficulty of agreeing on a “neutral” theoretical framework, on the other the difficulty of achieving more than moderate agreement on semantic judgments (Poesio and Artstein, 2005). The ANAWIKI project1 presents an effort to create high-quality, large-scale anaphorically annotated resources (Poesio et al., 2008) by taking advantage of the collaboration of the Web community, both through co-operative annotation efforts using traditional annotation tools and through the use of game-like interfaces. This makes ANAWIKI a very ambitious project. It is not clear to what extend expert annotations can in fact be substituted by those judgements submitted by the general public as part of a game. If successful, ANAWIKI will actually be more than just an anaphora annotation tool. W"
W08-2230,poesio-etal-2008-anawiki,1,0.742846,"o small. Unfortunately, the creation of 100M-plus corpora via hand annotation is likely to be prohibitively expensive. Such a large hand-annotation effort would be even less sensible in the case of semantic annotation tasks such as coreference or wordsense disambiguation, given on the one side the greater difficulty of agreeing on a “neutral” theoretical framework, on the other the difficulty of achieving more than moderate agreement on semantic judgments (Poesio and Artstein, 2005). The ANAWIKI project1 presents an effort to create high-quality, large-scale anaphorically annotated resources (Poesio et al., 2008) by taking advantage of the collaboration of the Web community, both through co-operative annotation efforts using traditional annotation tools and through the use of game-like interfaces. This makes ANAWIKI a very ambitious project. It is not clear to what extend expert annotations can in fact be substituted by those judgements submitted by the general public as part of a game. If successful, ANAWIKI will actually be more than just an anaphora annotation tool. We see it as a framework aimed at creating large-scale annotated corpora in general. 2 Creating Resources through Web Collaboration La"
W08-2230,W07-1523,0,0.0282827,"rior quality. The ANAWIKI project bridges this gap by combining both approaches to annotate the data: an expert annotation tool and a game interface. Both Chamberlain, Poesio, and Kruschwitz 378 Figure 2: A screenshot of the Game Interface (Annotation Mode). tools are essential parts of ANAWIKI. We briefly describe both, with a particular focus on the game interface. 3.1 Expert Annotation Tool An expert annotation tool is used to obtain Gold Standard annotations from computational linguists. In the case of anaphora annotation we use the Serengeti tool developed at the University of Bielefeld (Stührenberg et al., 2007). The anaphoric annotation of markables within this environment will be very detailed and will serve as a training corpus as well as quality check for the second tool (see below). Figure 1 is a screenshot of this interface. 3.2 Game Interface A game interface is used to collect annotations from the general Web population. The game interface integrates with the database of the expert annotation tool but aims to collect large-scale (rather than detailed) anaphoric relations. Users are simply asked to assign an anaphoric link but are not asked to specify what type (or what features) are present."
W09-3309,N06-2015,0,0.0467533,"thodology was developed using preliminary annotation with automatic methods followed by partial hand-correction (Burnard, 2000). Medium and large-scale semantic annotation projects (for wordsense or coreference) are a recent innovation in Computational Linguistics. The semi-automatic annotation methodology cannot yet be used for this type of annotation, as the quality of, for instance, coreference resolvers is not yet high enough on general text. Nevertheless the semantic annotation methodology has made great progress with the development, on the one end, of effective quality control methods (Hovy et al., 2006) and on the other, of sophisticated annotation tools such as Serengeti (St¨uhrenberg et al., 2007). These developments have made it possible to move from the small-scale semantic annotation projects, the aim of which was to create resources of around 100K words in size (Poesio, 2004b), to the efforts made as part of US initiatives such as Automatic Context Extraction (ACE), Translingual Information Detection, Extraction and Summarization (TIDES), and GALE to create 1 million word corpora. Such techniques could not be expected to annotate data on the scale of the BNC. Introduction The statistic"
W09-3309,W07-1523,0,0.0564958,"Missing"
W09-3309,W04-0210,1,\N,Missing
W09-3309,W04-2327,1,\N,Missing
W10-0605,D09-1065,1,0.880091,"Missing"
W11-1508,councill-etal-2008-parscit,0,0.0319878,"he categories in C; the alphabet is hand built and tailored for the task and the probabilities in the probability matrix are derived empirically. The system obtains an average F1 measure of 93 for the Cora dataset. A better performance for sequence labeling is obtained if CRF replaces the traditional HMM. The reason for this is that CRF systems better tolerate errors and they have good performance even when richer features are not available. A system which uses CRF and a series of post-processing rules for both document logical structure identification and reference string parsing is ParsCit (Councill et al., 2008). ParsCit comprises three sub-modules: SectLabel and ParseHead for document logical structure identification and ParsCit for reference string parsing. The system is built on top of the well known CRF++ package. 56 The linguistic surface level, i.e. the linear order of words, sentences, and paragraphs, and the hierarchical, tree-like, logical structure also lends itself to parsing-like methods for the structure analysis. However, the complexity of fostering, maintaining, and augmenting document structure grammars is challenging, and the notorious uncertainty of the input demands for the whole s"
W11-1508,pianta-etal-2008-textpro,1,0.936467,"ore (logical) document structure or remove it. We argue that identifying the structure of documents is essential in digital library and other types of applications, and show that it is relatively straightforward to extend existing pipelines to achieve ones in which the structure of a document is preserved. 1 • techniques for extracting content from plain text do not work on, say, bibliographic references, or lists; Introduction Many off-the-shelf Human Language Technology (HLT) pipelines are now freely available (examples include LingPipe,1 OpenNLP,2 GATE3 (Cunningham et al., 2002), TextPro4 (Pianta et al., 2008)), and although they support a variety of document formats as input, actual processing (mostly) takes no advantage of structural information, i.e. structural information is not used, or stripped off during preprocessing. Such processing can be considered safe, e.g. in case of news wire snippets, when processing does not need to be aware of sentence or paragraph boundaries, or of text being part of a table or a figure caption. However, when processing large documents, section or chapter boundaries may be considered an important segmentation to use, and when working with the type of data typical"
W11-1508,W95-0107,0,0.388462,"fferent from that in other sections of text. In this paper we summarize several years of work on developing structure-preserving pipelines for different applications. We discuss the incorporation of 2 http://incubator.apache.org/opennlp/ 3 http://http://gate.ac.uk/ 4 http://textpro.fbk.eu/ 54 Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 54–62, c Portland, OR, USA, 24 June 2011. 2011 Association for Computational Linguistics document structure parsers both in pipelines which the information is passed in BOI format (Ramshaw and Marcus, 1995), such as the TEXTPRO pipeline (Pianta et al., 2008), and in pipelines based on a standoff XML (Ide, 1998). We also present several distinct applications that require preserving document structure. The structure of the paper is as follows. We first discuss the notion of document structure and previous work in extracting it. We then introduce our architecture for a structure-preserving pipeline. Next, we discuss two pipelines based on this general architecture. A discussion follows. 2 The Logical Structure of a Document Documents have at least two types of structure5 . The term geometrical, or"
W11-1908,P05-1045,0,0.0191921,"Missing"
W11-1908,H05-1004,0,0.507946,"asif@iitp.ac.in, massimo.poesio@unitn.it Abstract Because there is no generally accepted metric for measuring the performance of anaphora resolution systems, a combination of metrics was proposed to evaluate submissions to the 2011 CONLL Shared Task (Pradhan et al., 2011). We investigate therefore Multiobjective function Optimization (MOO) techniques based on Genetic Algorithms to optimize models according to multiple metrics simultaneously. 1 Introduction Many evaluation metrics have been proposed for anaphora resolution (Vilain et al., 1995; Bagga and Baldwin, 1998; Doddington et al., 2000; Luo, 2005; Recasens and Hovy, 2011). Each of these metrics seems to capture some genuine intuition about the the task, so that, unlike in other areas of HLT, none has really taken over. This makes it difficult to compare systems, as dramatically demonstrated by the results of the Coreference Task at SEMEVAL 2010 (Recasens et al., 2010). It was therefore wise of the CONLL organizers to use a basket of metrics to assess performance instead of a single one. This situation suggests using methods to optimize systems according to more than one metric at once. And as it happens, techniques for doing just that"
W11-1908,H05-1068,0,0.0196597,"tomatic optimization of both feature selection and of learning parameters, also considering 61 Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 61–65, c Portland, Oregon, 23-24 June 2011. 2011 Association for Computational Linguistics two different machine learners, TimBL and Ripper. Her results suggest that such techniques yield improvements on the MUC-6/7 datasets. Recasens and Hovy (2009) carried out an investigation of feature selection for Spanish using the ANCORA corpus. A form of multi-objective optimization was applied to coreference by Munson et al. (2005). Munson et al. (2005) did not propose to train models so as to simultaneously optimize according to multiple metrics; instead, they used ensemble selection to learn to choose among previously trained models the best model for each example. Their general conclusion was negative, stating that “ensemble selection seems too unreliable for use in NLP”, but they did see some improvements for coreference. Genetic algorithms are known to be more effective for solving MOO than classical methods such as weighted metrics, goal programming (Deb, 2001), because of their population-based nature. A particul"
W11-1908,P02-1014,0,0.480729,"inguistically-rich system for coreference resolution might benefit a lot from feature selection. In particular, we have investigated Non-Dominated Sorting Genetic Algorithm II (Deb et al., 2002) for multiobjective optimization. In subsequent work, we plan to expand the optimization technique to consider also learning parameters optimization, classifier selection, and learning model selection. 4 Results 4.1 Acknowledgments Development set Table 1 compares the performance level obtained using all the features with that of loose reimplementations of the systems proposed by Soon et al. (2001) and Ng and Cardie (2002), commonly used as baselines. Our reimplementation of the Ng & Cardie model uses only a subset of features. The results in Table 1 show that our system with a rich feature set does not outperform simpler baselines (and, in fact, yields poorer results). A similar trend has been observed by Ng and Cardie (2002), where the improvement was only possible after manual feature selection. The last line of Table 1 shows the performance level of the best chromosome found through the MOO technique. As it can be seen, it outperforms all the baselines according to all the measures, leading to an improvemen"
W11-1908,W11-1901,0,0.150736,"Missing"
W11-1908,W09-2411,0,0.0655299,"Missing"
W11-1908,I11-1011,1,0.674013,"one. This situation suggests using methods to optimize systems according to more than one metric at once. And as it happens, techniques for doing just that have been developed in the area of Genetic Algorithms—so-called multi-objective optimization techniques (MOO) (Deb, 2001). The key idea of our submission is to use MOO techniques to optimize our anaphora resolution system according to three metrics simultaneously: the MUC scorer (a member of what one might call the ’link-based’ cluster of metrics) and the two CEAF metrics (representative of the ’entity-based’ cluster). In a previous study (Saha et al., 2011), we show that our MOO -based approach yields more robust results than single-objective optimization. We test two types of optimization: feature selection and architecture–whether to learn a single model for all types of anaphors, or to learn separate models for pronouns and for other nominals. We also discuss how the default mention extraction techniques of the system we used for this submission, BART (Versley et al., 2008), were modified to handle the all-mention annotation in the OntoNotes corpus. In this paper, we first briefly provide some background on optimization for anaphora resolutio"
W11-1908,J01-4004,0,0.885984,"d split classifiers. We considered 42 features, including 7 classifying mention type, 8 for string matching of different subparts and different levels of exactness, 2 for aliasing, 4 for agreement, 12 for syntactic information including also binding constraints, 3 encoding salience, 1 encoding patterns extracted from the Web, 3 for proximity, and 2 for 1st and 2nd person pronouns. Again because of time considerations, we used decision trees as implemented in Weka as our classification model instead of maximum-entropy or SVMs. Finally, we used a simple mention-pair model without ranking as in (Soon et al., 2001). 3.2 Mention detection BART supports several solutions to the mention detection (MD) task. The users can input precomputed mentions, thus, experimenting with gold boundaries or system boundaries computed by external modules (e.g., CARAFE). BART also has a built-in mention extraction module, computing boundaries heuristically from the output of a parser. For the CoNLL shared task, we use the BART internal MD module, as it corresponds better to the mention detection guidelines of the OntoNotes dataset. We have further adjusted this module to improve the MD accuracy. The process of mention detec"
W11-1908,P08-4003,1,0.950061,"MUC scorer (a member of what one might call the ’link-based’ cluster of metrics) and the two CEAF metrics (representative of the ’entity-based’ cluster). In a previous study (Saha et al., 2011), we show that our MOO -based approach yields more robust results than single-objective optimization. We test two types of optimization: feature selection and architecture–whether to learn a single model for all types of anaphors, or to learn separate models for pronouns and for other nominals. We also discuss how the default mention extraction techniques of the system we used for this submission, BART (Versley et al., 2008), were modified to handle the all-mention annotation in the OntoNotes corpus. In this paper, we first briefly provide some background on optimization for anaphora resolution, on genetic algorithms, and on the method for multiobjective optimization we used, Non-Dominated Sorting Genetic Algorithm II (Deb et al., 2002). After that we discuss our experiments, and present our results. 2 Background 2.1 Optimization for Anaphora Resolution There have only been few attempts at optimization for anaphora resolution, and with a few exceptions, this was done by hand. The first systematic attempt at autom"
W11-1908,M95-1005,0,0.476534,"y Patna ∗ University of Essex uryupina@gmail.com, sriparna@iitp.ac.in, asif@iitp.ac.in, massimo.poesio@unitn.it Abstract Because there is no generally accepted metric for measuring the performance of anaphora resolution systems, a combination of metrics was proposed to evaluate submissions to the 2011 CONLL Shared Task (Pradhan et al., 2011). We investigate therefore Multiobjective function Optimization (MOO) techniques based on Genetic Algorithms to optimize models according to multiple metrics simultaneously. 1 Introduction Many evaluation metrics have been proposed for anaphora resolution (Vilain et al., 1995; Bagga and Baldwin, 1998; Doddington et al., 2000; Luo, 2005; Recasens and Hovy, 2011). Each of these metrics seems to capture some genuine intuition about the the task, so that, unlike in other areas of HLT, none has really taken over. This makes it difficult to compare systems, as dramatically demonstrated by the results of the Coreference Task at SEMEVAL 2010 (Recasens et al., 2010). It was therefore wise of the CONLL organizers to use a basket of metrics to assess performance instead of a single one. This situation suggests using methods to optimize systems according to more than one metr"
W11-1908,S10-1001,1,\N,Missing
W11-1908,doddington-etal-2004-automatic,0,\N,Missing
W12-0406,fornaciari-poesio-2012-decour,1,0.869149,"e data set is described. In Section 4 we discuss our machine learning and experimental methods. Finally, the results are presented in Section 5 and discussed in Section 6. 2 Background 2.1 Deceptive language analysis From a methodological point of view, to investigate deceptive language gives rise to some tricky issues: first of all, the strategy chosen to collect data. The literature can be divided in two main families of studies: 3 3.1 • Field studies; False testimonies in Court In order to study deceptive language, we created the D E C OUR - DEception in COURt - corpus, better described in Fornaciari and Poesio (2012). D E C OUR is a corpus constituted by the transcripts of 35 hearings held in four Italian Courts: Bologna, Bolzano, Prato and Trento. These transcripts report verbatim the statements issued by a total of 31 different subjects - four of which have been heard twice. All the hearings come from criminal proceedings for calumny and false testimony (artt. 368 and 372 of the Italian Criminal Code). In particular, the hearings of D E C OUR come mainly from two situations: • Laboratory studies. The first ones are usually interesting in forensic applications but in such studies verifying the sincerity"
W12-0406,C08-1006,0,0.213969,"rij, 2005). Laboratory studies, instead, are characterized by the artificiality of participants’ psychological conditions: therefore their findings may not be generalized to deception encountered in real life. Due to practical difficulties in collection and annotation of suitable data, in literature finding papers in which real life linguistic data are employed, where truthfulness is surely known, is less common and Zhou et al. (2008) complain about the lack of “data set for evaluating deception detection models”. Just recently some studies tried to fill this gap, concerning both the English (Bachenko et al., 2008; Fitzpatrick and Bachenko, 2009) and Italian language (Fornaciari and Poesio, 2011a,b). Just the studies on Italian language come from data which have constituted the first nucleus of the corpus analysed here. 2.2 Data set • the defendant for any criminal proceeding tries to use calumny against someone; • a witness in any criminal proceeding lies for some reason. In both cases, a new criminal proceeding arises, in which the subjects can issue new statements or not, and having as a body of evidence the transcript of the hearing held in the previous proceeding. The crucial point is that D E C O"
W12-0406,C08-1065,0,0.0319427,"ery similar to that of stylometry. Stylometry is a discipline which studies texts on the basis of their stylistic features, usually in order to attribute them to an author - giving rise to the branch of author attribution - or to get information about the author himself - this is the field of author profiling. Stylometric analyses, which relies mainly on machine learning algorithms, turned out to be effective in several forensic tasks: not only the classical field of author profiling (Coulthard, 2004; Koppel et al., 2006; Peersman et al., 2011; Solan and Tiersma, 2004) and author attribution (Luyckx and Daelemans, 2008; Mosteller and Wallace, 1964), but also emotion detection (Vaassen and Daelemans, 2011) and plagiarism analysis (Stein et al., 2007). Therefore, from a methodological point of view, Deceptive Language Analysis is a particular application of stylometry, exactly like other branches of Forensic Linguistics. of experiments in which our methods were trained either over the whole corpus or over smaller subsets consisting of the utterances produced by more homogenous subsets of subjects. These subsets were identified either automatically, by clustering subjects according to their language profile, o"
W12-0406,P09-2078,0,0.263924,"s Frank et al. (2008) write “We find that there is no clue or clue pattern that is specific to deception, although there are clues specific to emotion and cognition”, and they wish for “real-world databases, identifying base rates for malfeasant behavior in security settings, optimizing training, and identifying preexisting excellence within security organizations”. Jensen et al. (2010) exploited cues coming from audio, video and textual data. One solution is to let statistical and machine learning methods discover the clues. Work such as Fornaciari and Poesio (2011a,b); Newman et al. (2003); Strapparava and Mihalcea (2009) suggests that these techniques can perform reasonably well at the task of discovering deception even just from linguistic data, provided that corpora containing examples of deceptive and truthful texts are available. The availability of such corpora is not a trivial problem, and indeed, the creation of a realistic such corpus is one of the problems in which we invested substantial effort in our own previous work, as discussed in Section 3. In the work discussed in this paper, we tackle an issue which to our knowledge has not been addressed before, due to the limitations of the datasets previo"
W12-0406,W11-1713,0,0.0217087,"he basis of their stylistic features, usually in order to attribute them to an author - giving rise to the branch of author attribution - or to get information about the author himself - this is the field of author profiling. Stylometric analyses, which relies mainly on machine learning algorithms, turned out to be effective in several forensic tasks: not only the classical field of author profiling (Coulthard, 2004; Koppel et al., 2006; Peersman et al., 2011; Solan and Tiersma, 2004) and author attribution (Luyckx and Daelemans, 2008; Mosteller and Wallace, 1964), but also emotion detection (Vaassen and Daelemans, 2011) and plagiarism analysis (Stein et al., 2007). Therefore, from a methodological point of view, Deceptive Language Analysis is a particular application of stylometry, exactly like other branches of Forensic Linguistics. of experiments in which our methods were trained either over the whole corpus or over smaller subsets consisting of the utterances produced by more homogenous subsets of subjects. These subsets were identified either automatically, by clustering subjects according to their language profile, or by using meta-information about the subjects included in the corpus, such as their gen"
W12-3618,pianta-etal-2008-textpro,0,0.0276654,"a, together with temporal, spatial, and entity references (Poesio et al., 2011b). It provides access to the Archaeological articles in the APSAT / ALPINET repository, and therefore, dedicated content extraction resources needed to be created, tuned on the specificities of the domain. The corpus of articles in the repository consists of a complete collection of the journal Preistoria Alpina published by the Museo Tridentino di Scienze Naturali. In order to make those articles accessible through the portal, they are tokenized, PoS tagged and Named Entity (NE) annotated by the TEXTPRO1 pipeline (Pianta et al., 2008). The first version of the pipeline included the default TEXTPRO NE tagger, EntityPro, trained to recognize the standard ACE entity types. However, the final version of the portal is based on an improved version of the NEtagger capable of recognising all relevant entities in the APSAT/ALPINET collection (Poesio et al., 2011b; Ekbal et al., 2012) 3 Annotation Schema for the Archaeological Domain A close collaboration with the University of Trento’s “B. Bagolini” Laboratory, resulted in the development of an annotation schema, particularly suited for the Archaeological domain, (Table 1). Differe"
W12-3618,W11-0118,0,0.0473737,"Missing"
W12-3618,W11-1508,1,0.923427,"in which the annotation has been developed; in Section 3, we describe a first annotation schema, analysing its performance and its weaknesses; in Section 4 we propose a revised version of the annotation schema, building upon the first experience and, in Section 5, we evaluate the performance of the new schema, describing a pilot annotation test and the results of the inter-annotator agreement evaluation. 2 Framework and Corpus Description The annotation process at hand takes place in the framework of the development of the Portale della Ricerca Umanistica / Humanities Research Portal (PRU), (Poesio et al., 2011a), a one-stop search facility for repositories of research articles and other types of publications in the Humanities. The portal uses content extraction techniques for extract134 Proceedings of the 6th Linguistic Annotation Workshop, pages 134–138, c Jeju, Republic of Korea, 12-13 July 2012. 2012 Association for Computational Linguistics ing, from the uploaded publications, citations and metadata, together with temporal, spatial, and entity references (Poesio et al., 2011b). It provides access to the Archaeological articles in the APSAT / ALPINET repository, and therefore, dedicated content"
W12-3618,W06-2209,0,0.0472301,"uman being discussed in the text (Otzi the Iceman, Pliny the Elder, Caesar) Author in bibliographic references Publication location Publisher Publication year Table 1: Annotation schema for Named Entities in the Archaeology Domain were decided to be marked as underspecified. 3.1 Annotation with the First Annotation Schema and Error Analysis A manual annotation, using the described schema, was carried out on a small subset of 11 articles of Preistoria Alpina (in English and Italian) and was used as training set for the NE tagger; the latter was trained with a novel active annotation technique (Vlachos, 2006), (Settles, 2009). Quality of the initial manual annotation was estimated using qualitative analyses for assessing the representativeness of the annotation schema, and quantitative analyses for measuring the inter-annotator agreement. Qualitative analyses revealed lack of specificity of the entity TIME and of the entity PERSON. In fact, the annotation schema only provided a general TIME entity used for marking historical periods (as Mesolitic, Neolithic) as well as specific dates (as 1200 A.D.) and proposed dates(as from 50-100 B.C.), although all these instances need to be clearly distinguish"
W12-3618,W09-2418,0,\N,Missing
W12-3618,magnini-etal-2006-cab,0,\N,Missing
W12-4515,2011.mtsummit-papers.22,0,0.018256,"erence on EMNLP and CoNLL: Shared Task, pages 122–128, c Jeju Island, Korea, July 13, 2012. 2012 Association for Computational Linguistics from knowledge-lean approaches of the late nineties (Harabagiu and Maiorano, 1999). In fact, modern systems try to account for complex coreference links by incorporating lexicographic and world knowledge, for example, using WordNet (Harabagiu et al., 2001; Huang et al., 2009) or Wikipedia (Ponzetto and Strube, 2006). For languages other than English, however, even the most basic properties of mentions can be intrinsically difficult to extract. For example, Baran and Xue (2011) have shown that a complex algorithm is needed to identify the number property of Chinese nouns. Both Arabic and Chinese have long linguistic traditions and therefore most grammar studies rely on terminology that can be very confusing for an outsider. For example, several works on Arabic (Hoyt, 2008) mention that nouns can be made definite with the suffix “Al-”, but this is not a semantic, but syntactic definiteness. Without any experience in Arabic, one can hardly decide how such “syntactic definiteness” might affect coreference. In the present study, we have used the information provided by"
W12-4515,W99-0104,0,0.0553225,"ost commonly pronominal anaphors (cf., for example, (Iida and Poesio, 2011; Arregi et al., 2010) and many others). 1 Statistical EMD approaches have been proved useful for Two new languages, Arabic and Chinese, have ACE-style coreference resolution, where mentions are basic been proposed for the CoNLL-2012 shared task units belonging to a restricted set of semantic types. 122 Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 122–128, c Jeju Island, Korea, July 13, 2012. 2012 Association for Computational Linguistics from knowledge-lean approaches of the late nineties (Harabagiu and Maiorano, 1999). In fact, modern systems try to account for complex coreference links by incorporating lexicographic and world knowledge, for example, using WordNet (Harabagiu et al., 2001; Huang et al., 2009) or Wikipedia (Ponzetto and Strube, 2006). For languages other than English, however, even the most basic properties of mentions can be intrinsically difficult to extract. For example, Baran and Xue (2011) have shown that a complex algorithm is needed to identify the number property of Chinese nouns. Both Arabic and Chinese have long linguistic traditions and therefore most grammar studies rely on termi"
W12-4515,N01-1008,0,0.015388,"languages, Arabic and Chinese, have ACE-style coreference resolution, where mentions are basic been proposed for the CoNLL-2012 shared task units belonging to a restricted set of semantic types. 122 Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 122–128, c Jeju Island, Korea, July 13, 2012. 2012 Association for Computational Linguistics from knowledge-lean approaches of the late nineties (Harabagiu and Maiorano, 1999). In fact, modern systems try to account for complex coreference links by incorporating lexicographic and world knowledge, for example, using WordNet (Harabagiu et al., 2001; Huang et al., 2009) or Wikipedia (Ponzetto and Strube, 2006). For languages other than English, however, even the most basic properties of mentions can be intrinsically difficult to extract. For example, Baran and Xue (2011) have shown that a complex algorithm is needed to identify the number property of Chinese nouns. Both Arabic and Chinese have long linguistic traditions and therefore most grammar studies rely on terminology that can be very confusing for an outsider. For example, several works on Arabic (Hoyt, 2008) mention that nouns can be made definite with the suffix “Al-”, but this"
W12-4515,P11-1081,1,0.835891,"ble shift A number of high-performance coreference resolution (CR) systems have been created for English in the past decades, implementing both rule-based and statistical approaches. For other languages, however, the situation is far less optimistic. For Romance and German languages, several systems have been developed and evaluated, in particular, at the SemEval-2010 track 1 on Multilingual Coreference Resolution (Recasens et al., 2010). For other languages, individual approaches have been proposed, covering specific subparts of the task, most commonly pronominal anaphors (cf., for example, (Iida and Poesio, 2011; Arregi et al., 2010) and many others). 1 Statistical EMD approaches have been proved useful for Two new languages, Arabic and Chinese, have ACE-style coreference resolution, where mentions are basic been proposed for the CoNLL-2012 shared task units belonging to a restricted set of semantic types. 122 Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 122–128, c Jeju Island, Korea, July 13, 2012. 2012 Association for Computational Linguistics from knowledge-lean approaches of the late nineties (Harabagiu and Maiorano, 1999). In fact, modern systems try to account for"
W12-4515,N06-1025,0,0.0481714,"resolution, where mentions are basic been proposed for the CoNLL-2012 shared task units belonging to a restricted set of semantic types. 122 Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 122–128, c Jeju Island, Korea, July 13, 2012. 2012 Association for Computational Linguistics from knowledge-lean approaches of the late nineties (Harabagiu and Maiorano, 1999). In fact, modern systems try to account for complex coreference links by incorporating lexicographic and world knowledge, for example, using WordNet (Harabagiu et al., 2001; Huang et al., 2009) or Wikipedia (Ponzetto and Strube, 2006). For languages other than English, however, even the most basic properties of mentions can be intrinsically difficult to extract. For example, Baran and Xue (2011) have shown that a complex algorithm is needed to identify the number property of Chinese nouns. Both Arabic and Chinese have long linguistic traditions and therefore most grammar studies rely on terminology that can be very confusing for an outsider. For example, several works on Arabic (Hoyt, 2008) mention that nouns can be made definite with the suffix “Al-”, but this is not a semantic, but syntactic definiteness. Without any exp"
W12-4515,W12-4501,1,0.856641,"ssion to the CoNLL-2012 Shared Task on the Multilingual Coreference Resolution. We have extended our CoNLL-2011 submission, based on BART, to cover two additional languages, Arabic and Chinese. This paper focuses on adapting BART to new languages, discussing the problems we have encountered and the solutions adopted. In particular, we propose a novel entity-mention detection algorithm that might help identify nominal mentions in an unknown language. We also discuss the impact of basic linguistic information on the overall performance level of our coreference resolution system. 1 Introduction (Pradhan et al., 2012). They present a challenging problem: the systems are required to provide entity mention detection (EMD) and design a proper coreference resolver for both languages. At UniTN/Essex, we have focused on these parts of the task, relying on a modified version of our last-year submission for English. Most state-of-the-art full-scale coreference resolution systems rely on hand-written rules for the mention detection subtask.1 For English, such rules may vary from corpus to corpus, reflecting specifics of particular guidelines (e.g. whether nominal premodifiers can be mentions, as in MUC, or not, as"
W12-4515,W09-2411,0,0.0853079,"Missing"
W12-4515,I11-1011,1,0.835106,"xt Parser Mention Mention Tagger Factory Encoder/ Decoder Coreference chains (entities) Merger Preprocessing Machine Learner Figure 1: BART architecture ing pipeline to operate on the OntoNotes NE-types, mapping them into MUC types required by BART. This allows us to participate in the closed track, as no external material is used any longer. Since last year, we have continued with our experiments on multi-objective optimization, proposed in our CoNLL-2011 paper (Uryupina et al., 2011). We have extended the scope of our work to cover different machine learning algorithms and their parameters (Saha et al., 2011). For CoNLL-2012, we have re-tested all the solutions of our optimization experiments, picking the one with the highest score on the current development set. Finally, our recent experiments on domain selection (Uryupina and Poesio, 2012) suggest that, at least for some subparts of OntoNotes, a system might benefit from training a domain-specific model. We have tested this hypothesis on the CoNLL-2012 data and have consequently trained domain-specific classifiers for the nw and bc domains. 3 Coreference resolution in Arabic and Chinese 3.1 Mention detection Mention detection is rarely considere"
W12-4515,J01-4004,0,0.769784,"Missing"
W12-4515,uryupina-poesio-2012-domain,1,0.827356,"es required by BART. This allows us to participate in the closed track, as no external material is used any longer. Since last year, we have continued with our experiments on multi-objective optimization, proposed in our CoNLL-2011 paper (Uryupina et al., 2011). We have extended the scope of our work to cover different machine learning algorithms and their parameters (Saha et al., 2011). For CoNLL-2012, we have re-tested all the solutions of our optimization experiments, picking the one with the highest score on the current development set. Finally, our recent experiments on domain selection (Uryupina and Poesio, 2012) suggest that, at least for some subparts of OntoNotes, a system might benefit from training a domain-specific model. We have tested this hypothesis on the CoNLL-2012 data and have consequently trained domain-specific classifiers for the nw and bc domains. 3 Coreference resolution in Arabic and Chinese 3.1 Mention detection Mention detection is rarely considered to be a separate task. Only very few studies on coreference resolution report on their EMD techniques. Existing corpora of coreference follow different approaches to mention annotation: this includes defining mention boundaries (basic"
W12-4515,W11-1908,1,0.748386,"orts dual number). The encoder generates training examples through a process of sample selection and learns a pairwise classifier. Finally, the decoder generates testing examples through a (possibly distinct) process of sample selection, runs the classifier and partitions the mentions into coreference chains. 2 The English track at CoNLL-2012 can be considered an extension of the last year’s CoNLL task. New data have been added to the corpus, including two additional domains, but the annotation guidelines remain the same. We have therefore mainly relied on the CoNLL2011 version of our system (Uryupina et al., 2011) for the current submission, providing only minor adjustments. Thus, we have modified our preprocessBART Our CoNLL submission is based on BART (Versley et al., 2008). BART is a modular toolkit for coreference resolution that supports state-of-the-art statistical approaches to the task and enables efficient feature engineering. BART has originally been created and tested for English, but its flexible modular architecture ensures its portability to other languages and 123 2.1 Coreference resolution in English Language Plugin Feature Extractor POS Tagger Unannotated Text Parser Mention Mention Ta"
W12-4515,P08-4003,1,0.889157,"ing examples through a (possibly distinct) process of sample selection, runs the classifier and partitions the mentions into coreference chains. 2 The English track at CoNLL-2012 can be considered an extension of the last year’s CoNLL task. New data have been added to the corpus, including two additional domains, but the annotation guidelines remain the same. We have therefore mainly relied on the CoNLL2011 version of our system (Uryupina et al., 2011) for the current submission, providing only minor adjustments. Thus, we have modified our preprocessBART Our CoNLL submission is based on BART (Versley et al., 2008). BART is a modular toolkit for coreference resolution that supports state-of-the-art statistical approaches to the task and enables efficient feature engineering. BART has originally been created and tested for English, but its flexible modular architecture ensures its portability to other languages and 123 2.1 Coreference resolution in English Language Plugin Feature Extractor POS Tagger Unannotated Text Parser Mention Mention Tagger Factory Encoder/ Decoder Coreference chains (entities) Merger Preprocessing Machine Learner Figure 1: BART architecture ing pipeline to operate on the OntoNotes"
W12-4515,D09-1128,0,\N,Missing
W12-4515,J03-4003,0,\N,Missing
W12-4515,S10-1001,1,\N,Missing
W12-5103,W04-2214,0,0.0591419,"Missing"
W12-5103,D09-1065,1,\N,Missing
W15-4638,D09-1030,0,0.0138446,".g. polytope model optimization, genetic algorithms) on rich feature spaces to either maximize coverage of the output summaries, or train models for sentence scoring. The feature spaces went beyond words 4.2 Participation, Evaluation and Results Four research groups participated in the OnForumS, each submitting two runs. In addition, two baseline system runs were included making a total of ten different system runs. 1 272 http://www.sensei-conversation.eu/ Submissions were evaluated via crowdsourcing on Crowd Flowerwhich is a commonly used method for evaluating HLT systems (Snow et al., 2008; Callison-Burch, 2009). The crowdsourcing HIT was designed as a validation task (as opposed to annotation), where each system proposed link and labels are presented to a contributor for their validation. The approach used for the OnForumS evaluation is IR-inspired and based on the concept of pooling used in TREC (Soboroff, 2010), where the assumption is that possible links that were not proposed by any system are deemed irrelevant. Then from those links proposed by systems, four categories are formed as follows: (a) (b) (c) (d) phone. This task is different from news summarization in that dialogues need to be analy"
W15-4638,W13-3101,1,0.876356,"ional Linguistics independent summarization algorithms. Each system participating in the task was called upon to provide summaries for a range of different languages, based on corresponding language-specific corpora. Systems were to summarize texts in at least two of the ten different languages: Arabic, Chinese, Czech, English, French, Greek, Hebrew, Hindi, Romanian, Spanish. The task aims at the real problem of summarizing news topics, parts of which may be described or may happen in different moments in time. We consider, similarly to previous MultiLing efforts (Giannakopoulos et al., 2011; Li et al., 2013) that news topics can be seen as event sequences: style. Such articles make an excellent source of test data for single document summarization methods since they each have a well written summary (one of the style criterion), cover many languages, and have a diverse range of topics. 2.2 Participation, Evaluation, and Results Participation in the 2015 MSS task was excellent, 23 summarization systems were submitted by seven teams. Four of the teams submitted summaries for all 38 languages and the remaining three submitted summaries covering four languages. English was the only language for which"
W15-4638,W04-1013,0,0.148645,"summary score that participating systems should be able to exceed. An oracle summary was computed for each article using a covering algorithm (Davis et al., 2012) that selected sentences from the body text that covers the words in the summary using a minimal number of sentences until their aggregate size exceeds the summary. The oracle summary scores provide an approximate upper bound on the achievable summary scores and were, as expected, much higher than any submitted systems score. The baseline, oracle, and submitted summaries were scored against the human summaries using ROUGE-2, -3, -4 (Lin, 2004) and MeMoG (Giannakopoulos et al., 2008). Details of the preprocessing applied to the text and the performance of each submitted system are in (Kubina and Conroy, 2015b), but overall 14 of the 23 systems did better than the baseline summary for at least half of the languages they partook in. The ROUGE and MeMog scoring methods provide an automatic measure of summaries, which are good predictors of human judgements. A human evaluation of the summaries, that is currently underway, will measure the responsiveness and readability of each teams best performing system. 3 3.1 Definition 1. An event s"
W15-4638,W13-3102,1,0.925116,"Missing"
W15-4638,W15-4633,1,0.853959,"Missing"
W15-4638,W13-2323,0,0.0244471,"xcellent with 23 participants submitting two or more system runs across the four tasks that the campaign comprises. The next steps for the classical tasks MSS and MMS is to continue expanding the corpora in size and across languages, whereas for the pilot tasks is to further precise the boundaries of the new tasks and bridge the gaps in the evaluation methodologies by overcoming the limitations of ROUGE in order to assess abstractiveness and minimizing the effect of ‘cheating’ workers in crowdsourcing (e.g., by incorporating a probabilistic model of annotation, such as the one put forward by (Passonneau and Carpenter, 2013) to filter better noisy crowdsourcing data). The next MultiLing is planned for 2017. CCCS Task 5.1 Participation, evaluation and results Task description The call-center conversation summarization pilot task consists in automatically generating abstractive summaries of spoken conversations between a customer and an agent solving a problem over the 2 The popular links (a and b) were not that many, hence, we chose to include all. 3 Based on CrowdFlower’s aggregated judgements. 273 Acknowledgements G. Giannakopoulos, J. Kubina, J. Conroy, J. Steinberger, B. Favre, M. Kabadjov, U. Kruschwitz, and"
W15-4638,D08-1027,0,0.0276498,"Missing"
W15-4638,Q14-1025,0,\N,Missing
W16-0710,W04-0407,0,0.0286027,"d the Basque Language Plugin and added new features for coreference resolution specifically geared towards Basque. 4.1 Preprocessing and Mention Detection The preprocessing pipeline takes raw texts and applies a series of Basque linguistic processors to analyse the texts: i) A morphological analyser that performs word segmentation and PoS tagging (Alegria et al., 1996), ii) A lemmatiser that resolves the ambiguity caused at the previous phase (Alegria et al., 2002), iii) A multi-word item identifier that determines which groups of two or more words are to be considered multi-word expressions (Alegria et al., 2004), iv) A named-entity recogniser that identifies and classifies named entities (person, organisation, location) in the text (Alegria et al., 2003), v) A chunker, an analyser that identifies verbal and nominal chunks based on rule-based grammars (Aduriz and D´ıaz de Ilarraza, 2003), vi) A clause tagger, that is, an analyser that identifies clauses, combining rulebased-grammars and machine learning techniques (Alegria et al., 2008). After the preprocessing step, mentions that are potential candidates to be part of coreference chains are identified using the mention detector explained in Section 3"
W16-0710,broscheit-etal-2010-extending,1,0.838608,"on §4 describes the extension of BART to Basque, Section §5 presents results and provides a discussion on the challenges for coreference in Basque, and towards the end we draw conclusions and pointers to future work. 2 Related Work Preliminary work on Coreference for Basque was done by (Soraluze et al., 2015) where they adapt the Stanford coreference resolution system (Lee et al., 2013) to Basque. And there has been a lot of work on extending the BART coreference toolkit to languages other than English. (Poesio et al., 2010) extend it to Italian using the Evalita corpus of Wikipedia articles (Broscheit et al., 2010) work on German using the T¨uBa-D/Z coreference corpus, (Kope´c and Ogrodniczuk, 2012) develop the Polish plug-in using a subset of the National Corpus of Polish, and finally (Uryupina et al., 2012) run experiments on Arabic and Chinese. 3 Annotated Corpus of Basque EPEC (Reference Corpus for the Processing of Basque) (Aduriz et al., 2006) is a 300,000 word sample collection of standard written Basque that has been manually annotated at different levels (morphology, surface syntax, phrases, etc.). The corpus is composed by news published in Euskaldunon Egunkaria, a Basque language newspaper. I"
W16-0710,M98-1029,0,0.155235,"automatically processing it becoming increasingly available (Alegria et al., 1996; Alegria et al., 2002; Alegria et al., 2003; Aduriz and D´ıaz de Ilarraza, 2003; Alegria et al., 2008). However, as it is 67 the case with most less-resourced languages, there are tools for the core processing levels, such as tokenisation, sentence splitting, morphological analysis, syntactic parsing/chunking, but much less so for higher semantic levels required in end goal applications such as Question Answering (Morton, 2000), Text Summarisation (Steinberger et al., 2007) or Information Extraction (Def, 1995; Hirschman, 1998). One such intermediate problem which has been underresearched for Basque, and hence, no readily usable tools are publicly available yet, is that of Coreference Resolution (Poesio et al., 2016). However, preliminary work on Coreference for Basque is starting to emerge (Soraluze et al., 2015), and in this paper we describe our work on extending the coreference resolution toolkit, BART1 (Versley et al., 2008) to the Basque language. BART benefits from an open architecture and provides a mechanism through language plugins which makes it particularly suitable for adaptations to new languages, and"
W16-0710,kopec-ogrodniczuk-2012-creating,0,0.0439127,"Missing"
W16-0710,J13-4004,0,0.0252127,"s suggest viable solutions to model it with machine learning techniques. The remainder of this paper is organised as follows: Section §2 briefly surveys related work, Section §3 gives details of EPEC, a coreference corpus, Section §4 describes the extension of BART to Basque, Section §5 presents results and provides a discussion on the challenges for coreference in Basque, and towards the end we draw conclusions and pointers to future work. 2 Related Work Preliminary work on Coreference for Basque was done by (Soraluze et al., 2015) where they adapt the Stanford coreference resolution system (Lee et al., 2013) to Basque. And there has been a lot of work on extending the BART coreference toolkit to languages other than English. (Poesio et al., 2010) extend it to Italian using the Evalita corpus of Wikipedia articles (Broscheit et al., 2010) work on German using the T¨uBa-D/Z coreference corpus, (Kope´c and Ogrodniczuk, 2012) develop the Polish plug-in using a subset of the National Corpus of Polish, and finally (Uryupina et al., 2012) run experiments on Arabic and Chinese. 3 Annotated Corpus of Basque EPEC (Reference Corpus for the Processing of Basque) (Aduriz et al., 2006) is a 300,000 word sample"
W16-0710,H05-1004,0,0.0621071,"nder agreement does not cause any improvement in the scores, as Basque is genderless. 3 At this point the proposed new features to handle the specificity of Basque are not new and have also been used for other languages (see (Poesio et al., 2016) for details). 5 Experimental Results We have tested the two models presented in Subsection 4.3 in two different environments. In the first one automatically detected mentions are provided to the models and in the second one the mentions are gold.4 The metrics used in our evaluations are MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), CEAFm (Luo, 2005), and BLANC (Recasens and Hovy, 2011). The scores have been calculated using the reference implementation of the CoNLL scorer (Pradhan et al., 2014). Table 3 presents the results obtained by the two models when automatic mentions are used. Mention Detection Soon MUC Basque Soon B3 Basque Soon CEAFm Basque Soon CEAFe Basque Soon BLANC Basque Soon CONLL Basque R 72.91 18.37 35.44 53.96 58.10 57.50 58.67 67.42 61.63 32.29 38.70 P 74.69 67.23 45.53 72.85 65.27 58.90 60.10 52.93 58.15 62.47 48.81 - - F1 73.79 28.86 39.86 62.00 61.48 58.19 59.38 59.31 59.84 36.46 42.41 50.05 53.72"
W16-0710,P00-1023,0,0.0308321,"ly, the Basque language has also inspired a lot of work in Computational Linguistics with tools for automatically processing it becoming increasingly available (Alegria et al., 1996; Alegria et al., 2002; Alegria et al., 2003; Aduriz and D´ıaz de Ilarraza, 2003; Alegria et al., 2008). However, as it is 67 the case with most less-resourced languages, there are tools for the core processing levels, such as tokenisation, sentence splitting, morphological analysis, syntactic parsing/chunking, but much less so for higher semantic levels required in end goal applications such as Question Answering (Morton, 2000), Text Summarisation (Steinberger et al., 2007) or Information Extraction (Def, 1995; Hirschman, 1998). One such intermediate problem which has been underresearched for Basque, and hence, no readily usable tools are publicly available yet, is that of Coreference Resolution (Poesio et al., 2016). However, preliminary work on Coreference for Basque is starting to emerge (Soraluze et al., 2015), and in this paper we describe our work on extending the coreference resolution toolkit, BART1 (Versley et al., 2008) to the Basque language. BART benefits from an open architecture and provides a mechanis"
W16-0710,poesio-etal-2010-creating,1,0.725011,"riefly surveys related work, Section §3 gives details of EPEC, a coreference corpus, Section §4 describes the extension of BART to Basque, Section §5 presents results and provides a discussion on the challenges for coreference in Basque, and towards the end we draw conclusions and pointers to future work. 2 Related Work Preliminary work on Coreference for Basque was done by (Soraluze et al., 2015) where they adapt the Stanford coreference resolution system (Lee et al., 2013) to Basque. And there has been a lot of work on extending the BART coreference toolkit to languages other than English. (Poesio et al., 2010) extend it to Italian using the Evalita corpus of Wikipedia articles (Broscheit et al., 2010) work on German using the T¨uBa-D/Z coreference corpus, (Kope´c and Ogrodniczuk, 2012) develop the Polish plug-in using a subset of the National Corpus of Polish, and finally (Uryupina et al., 2012) run experiments on Arabic and Chinese. 3 Annotated Corpus of Basque EPEC (Reference Corpus for the Processing of Basque) (Aduriz et al., 2006) is a 300,000 word sample collection of standard written Basque that has been manually annotated at different levels (morphology, surface syntax, phrases, etc.). The"
W16-0710,P14-2006,0,0.0218894,"Basque are not new and have also been used for other languages (see (Poesio et al., 2016) for details). 5 Experimental Results We have tested the two models presented in Subsection 4.3 in two different environments. In the first one automatically detected mentions are provided to the models and in the second one the mentions are gold.4 The metrics used in our evaluations are MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), CEAFm (Luo, 2005), and BLANC (Recasens and Hovy, 2011). The scores have been calculated using the reference implementation of the CoNLL scorer (Pradhan et al., 2014). Table 3 presents the results obtained by the two models when automatic mentions are used. Mention Detection Soon MUC Basque Soon B3 Basque Soon CEAFm Basque Soon CEAFe Basque Soon BLANC Basque Soon CONLL Basque R 72.91 18.37 35.44 53.96 58.10 57.50 58.67 67.42 61.63 32.29 38.70 P 74.69 67.23 45.53 72.85 65.27 58.90 60.10 52.93 58.15 62.47 48.81 - - F1 73.79 28.86 39.86 62.00 61.48 58.19 59.38 59.31 59.84 36.46 42.41 50.05 53.72 Table 3: Scores with automatic mentions. In the case of automatically detected mentions, Basque model outperforms the Soon baseline model Basque with BART, at this st"
W16-0710,J01-4004,0,0.769101,"and in this paper we describe our work on extending the coreference resolution toolkit, BART1 (Versley et al., 2008) to the Basque language. BART benefits from an open architecture and provides a mechanism through language plugins which makes it particularly suitable for adaptations to new languages, and it attained good performance in the shared task on Multilingual Coreference at CoNLL 2012 (Uryupina et al., 2012). For our experiments we use the EPEC corpus annotated for coreference (Aduriz et al., 2006) and we run experiments across two dimensions. First, we use a baseline model based on (Soon et al., 2001) vs. a model that includes extra features reliably extracted for Basque with the tools at hand. Second, we measure performance on hand-parsed mentions vs. performance on automatically parsed mentions which illustrates the effect of pre-processing quality on the end results. 1 http://www.bart-coref.eu/ Proceedings of the Workshop on Coreference Resolution Beyond OntoNotes (CORBON 2016), co-located with NAACL 2016, pages 67–73, c San Diego, California, June 16, 2016. 2016 Association for Computational Linguistics One of the key challenges that the Basque language introduces for Coreference is th"
W16-0710,W12-4515,1,0.898558,"adily usable tools are publicly available yet, is that of Coreference Resolution (Poesio et al., 2016). However, preliminary work on Coreference for Basque is starting to emerge (Soraluze et al., 2015), and in this paper we describe our work on extending the coreference resolution toolkit, BART1 (Versley et al., 2008) to the Basque language. BART benefits from an open architecture and provides a mechanism through language plugins which makes it particularly suitable for adaptations to new languages, and it attained good performance in the shared task on Multilingual Coreference at CoNLL 2012 (Uryupina et al., 2012). For our experiments we use the EPEC corpus annotated for coreference (Aduriz et al., 2006) and we run experiments across two dimensions. First, we use a baseline model based on (Soon et al., 2001) vs. a model that includes extra features reliably extracted for Basque with the tools at hand. Second, we measure performance on hand-parsed mentions vs. performance on automatically parsed mentions which illustrates the effect of pre-processing quality on the end results. 1 http://www.bart-coref.eu/ Proceedings of the Workshop on Coreference Resolution Beyond OntoNotes (CORBON 2016), co-located wi"
W16-0710,P08-4003,1,0.77796,"ess so for higher semantic levels required in end goal applications such as Question Answering (Morton, 2000), Text Summarisation (Steinberger et al., 2007) or Information Extraction (Def, 1995; Hirschman, 1998). One such intermediate problem which has been underresearched for Basque, and hence, no readily usable tools are publicly available yet, is that of Coreference Resolution (Poesio et al., 2016). However, preliminary work on Coreference for Basque is starting to emerge (Soraluze et al., 2015), and in this paper we describe our work on extending the coreference resolution toolkit, BART1 (Versley et al., 2008) to the Basque language. BART benefits from an open architecture and provides a mechanism through language plugins which makes it particularly suitable for adaptations to new languages, and it attained good performance in the shared task on Multilingual Coreference at CoNLL 2012 (Uryupina et al., 2012). For our experiments we use the EPEC corpus annotated for coreference (Aduriz et al., 2006) and we run experiments across two dimensions. First, we use a baseline model based on (Soon et al., 2001) vs. a model that includes extra features reliably extracted for Basque with the tools at hand. Sec"
W16-0710,M95-1005,0,0.453995,"n each model are presented in Table 2. In the two models, gender agreement does not cause any improvement in the scores, as Basque is genderless. 3 At this point the proposed new features to handle the specificity of Basque are not new and have also been used for other languages (see (Poesio et al., 2016) for details). 5 Experimental Results We have tested the two models presented in Subsection 4.3 in two different environments. In the first one automatically detected mentions are provided to the models and in the second one the mentions are gold.4 The metrics used in our evaluations are MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), CEAFm (Luo, 2005), and BLANC (Recasens and Hovy, 2011). The scores have been calculated using the reference implementation of the CoNLL scorer (Pradhan et al., 2014). Table 3 presents the results obtained by the two models when automatic mentions are used. Mention Detection Soon MUC Basque Soon B3 Basque Soon CEAFm Basque Soon CEAFe Basque Soon BLANC Basque Soon CONLL Basque R 72.91 18.37 35.44 53.96 58.10 57.50 58.67 67.42 61.63 32.29 38.70 P 74.69 67.23 45.53 72.85 65.27 58.90 60.10 52.93 58.15 62.47 48.81 - - F1 73.79 28.86 39.86 62.00 61."
W16-4312,P04-1035,0,0.0223008,"Missing"
W16-4312,W14-2617,0,0.120914,"mmercial polling has taken another serious blow. By contrast, predictions using Natural Language Processing (NLP) and Computational Linguistics (CL) techniques, such as opinion mining, proved to be much more reliable. In what follows we will refer to opinion mining as the automatic task of assigning a polarity to a topic in context (Wiebe et al., 2005); to polarity classification and sentiment analysis as the tasks for the extraction of emotive polarity or scores from text; to agreement/disagreement classification as the task of recognizing the opinion of a message towards others in a thread (Wang and Cardie, 2014) or pairs of replying posts (Celli et al., 2016); and to stance classification as the task of recognizing the overall opinion of an author from text. In this paper we discuss the methods used by traditional pollsters and compare them to the predictions based on different opinion mining techniques, in particular polarity classification and agreement/disagreement classification. We describe a system that predicted the outcome of the This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 110 Proceedings of"
W17-4210,P13-2080,0,0.0273234,"ment or emotion), and so on. It may also be interesting to explore existing work on argument analysis: for example, Stab and Gurevych (2017) explore methods for the identification of arguments supported by insufficient evidence. This could be viewed as very close to the task of the detection of incongruent headlines, where the headline represents an argument which is not supported by claims in the text. Further, we could approach incongruence as a semantic issue and look to existing work on contradiction (De Marneffe et al., 2008), contrast (Harabagiu et al., 2006) and entailment recognition (Levy et al., 2013). In doing so, we may well discover several sub-types of incongruence which may fall into different semantic categories. (13) A sausage a day could lead to cancer: Pancreatic cancer warning over processed meat (14) Rise of the hugger mugger: Sociable thieves who cuddle while they rob (15) £100 to play truant! Schools accused of bribing worst pupils to stay away when Ofsted inspectors call Molek-Kozakowska (2013) views sensationalism as a discourse strategy used to repackage information in a more exciting, extraordinary or interesting way, via the presence of several discourse illocutions (e.g."
W17-4210,P00-1041,0,0.0467958,"of headline incongruence detection is best approached in parts: to analyse complex relationships between a headline and an entire news article is likely to be extremely difficult, not least because of their very different lengths and levels of linguistic complexity. This could therefore be facilitated with the extraction of key quotes (Pouliquen et al., 2007) or claims (Vlachos and Riedel, 2015; Thorne and Vlachos, 2017). Alternatively, one could automatically generate the statistically ‘best’ headline for an article using existing title and headline generation and summarisation methods (e.g. Banko et al. (2000); Zajic et al. (2002); Dorr et al. (2003)), and evaluate how far away the existing headline is from this in terms of a number of criteria, such as lexical choices, syntactic structure, length, tonality (sentiment or emotion), and so on. It may also be interesting to explore existing work on argument analysis: for example, Stab and Gurevych (2017) explore methods for the identification of arguments supported by insufficient evidence. This could be viewed as very close to the task of the detection of incongruent headlines, where the headline represents an argument which is not supported by claim"
W17-4210,S16-1003,0,0.0362797,"eadlines this paper aims to highlight: incongruent headlines do not necessarily adhere to an identifiable style in their surface form, but rather must be identified in relation to the text they represent. This presents significant problems for the NLP approaches so far discussed. 13 As Molek-Kozakowska (2013) used only one news source (the Daily Mail), this list may be specific to this particular newspaper’s voice and/or the knowledge, subjectivity and demographic range of the annotators. 14 See Hoffman and Justicz (2016, Appendices 1-4). 59 Finally, stance detection (Augenstein et al., 2016; Mohammad et al., 2016) has been applied in the Fake News Challenge (FNC-1)15 as a means of exploring whether different articles agree or disagree with a given headline or claim, to aid in the task of fact checking. Stance is certainly relevant to task of incongruence detection, but we argue that it is not sufficient for our task, as the headlinearticle relation may be incongruent in ways separate from (dis)agreement. Beyond the headlinearticle pair itself, however, stance detection could be used to analyse engagement and interaction with an article on social media, given that early indications suggest that users ar"
W17-4210,E17-1092,0,0.0219404,"quen et al., 2007) or claims (Vlachos and Riedel, 2015; Thorne and Vlachos, 2017). Alternatively, one could automatically generate the statistically ‘best’ headline for an article using existing title and headline generation and summarisation methods (e.g. Banko et al. (2000); Zajic et al. (2002); Dorr et al. (2003)), and evaluate how far away the existing headline is from this in terms of a number of criteria, such as lexical choices, syntactic structure, length, tonality (sentiment or emotion), and so on. It may also be interesting to explore existing work on argument analysis: for example, Stab and Gurevych (2017) explore methods for the identification of arguments supported by insufficient evidence. This could be viewed as very close to the task of the detection of incongruent headlines, where the headline represents an argument which is not supported by claims in the text. Further, we could approach incongruence as a semantic issue and look to existing work on contradiction (De Marneffe et al., 2008), contrast (Harabagiu et al., 2006) and entailment recognition (Levy et al., 2013). In doing so, we may well discover several sub-types of incongruence which may fall into different semantic categories. ("
W17-4210,E17-3010,0,0.094443,"ethodology, the source of the headline-article pair may well prove to be a useful feature in the broader classification process, which we will explore experimentally in future work. Arguably, the task of headline incongruence detection is best approached in parts: to analyse complex relationships between a headline and an entire news article is likely to be extremely difficult, not least because of their very different lengths and levels of linguistic complexity. This could therefore be facilitated with the extraction of key quotes (Pouliquen et al., 2007) or claims (Vlachos and Riedel, 2015; Thorne and Vlachos, 2017). Alternatively, one could automatically generate the statistically ‘best’ headline for an article using existing title and headline generation and summarisation methods (e.g. Banko et al. (2000); Zajic et al. (2002); Dorr et al. (2003)), and evaluate how far away the existing headline is from this in terms of a number of criteria, such as lexical choices, syntactic structure, length, tonality (sentiment or emotion), and so on. It may also be interesting to explore existing work on argument analysis: for example, Stab and Gurevych (2017) explore methods for the identification of arguments supp"
W17-4210,P08-1118,0,0.0662678,"Missing"
W17-4210,W03-0501,0,0.299577,"approached in parts: to analyse complex relationships between a headline and an entire news article is likely to be extremely difficult, not least because of their very different lengths and levels of linguistic complexity. This could therefore be facilitated with the extraction of key quotes (Pouliquen et al., 2007) or claims (Vlachos and Riedel, 2015; Thorne and Vlachos, 2017). Alternatively, one could automatically generate the statistically ‘best’ headline for an article using existing title and headline generation and summarisation methods (e.g. Banko et al. (2000); Zajic et al. (2002); Dorr et al. (2003)), and evaluate how far away the existing headline is from this in terms of a number of criteria, such as lexical choices, syntactic structure, length, tonality (sentiment or emotion), and so on. It may also be interesting to explore existing work on argument analysis: for example, Stab and Gurevych (2017) explore methods for the identification of arguments supported by insufficient evidence. This could be viewed as very close to the task of the detection of incongruent headlines, where the headline represents an argument which is not supported by claims in the text. Further, we could approach"
W17-4210,D15-1312,0,0.129872,"n conjunction with other methodology, the source of the headline-article pair may well prove to be a useful feature in the broader classification process, which we will explore experimentally in future work. Arguably, the task of headline incongruence detection is best approached in parts: to analyse complex relationships between a headline and an entire news article is likely to be extremely difficult, not least because of their very different lengths and levels of linguistic complexity. This could therefore be facilitated with the extraction of key quotes (Pouliquen et al., 2007) or claims (Vlachos and Riedel, 2015; Thorne and Vlachos, 2017). Alternatively, one could automatically generate the statistically ‘best’ headline for an article using existing title and headline generation and summarisation methods (e.g. Banko et al. (2000); Zajic et al. (2002); Dorr et al. (2003)), and evaluate how far away the existing headline is from this in terms of a number of criteria, such as lexical choices, syntactic structure, length, tonality (sentiment or emotion), and so on. It may also be interesting to explore existing work on argument analysis: for example, Stab and Gurevych (2017) explore methods for the ident"
W17-4210,N16-1138,0,0.0443085,"not sufficient for our task, as the headlinearticle relation may be incongruent in ways separate from (dis)agreement. Beyond the headlinearticle pair itself, however, stance detection could be used to analyse engagement and interaction with an article on social media, given that early indications suggest that users are compelled to alert others when they notice that a headline is misleading. 4 Discusses: The body text discuss the same topic as the headline, but does not take a position. Unrelated: The body text discusses a different topic than the headline. Built on the data set described in Ferreira and Vlachos (2016), which is collected from rumour tracking website, Emergent17 , the corpus contains approximately 50,000 annotated headlinebody pairs. A manual analysis of the first 50 body IDs led to a number of observations on the applicability of this data set to the problem of headline incongruence. Firstly, the ‘headline’ in a pair is the claim from the original post on the website, and is as such not necessarily a gold-standard headline. In addition, a single ‘headline’ can occur with multiple article bodies, and vice versa, which means that the original relation between the two is not captured. In our"
W17-4210,E17-4007,0,\N,Missing
W18-0702,J93-2004,0,0.0629773,"e field for many years. However, O NTO N OTES also has a number of frequently mentioned limitations, including: 2 2.1 • Not all NPs of relevance to anaphora resolution are treated as markables. For instance, expletives are not annotated. The ARRAU Corpus Genres The ARRAU corpus includes a substantial amount of news text in the sub-corpus called RST, con11 Proceedings of the Workshop on Computational Models of Reference, Anaphora and Coreference, pages 11–22 c New Orleans, Louisiana, June 6, 2018. 2018 Association for Computational Linguistics sisting of the entire subset of the Penn Treebank (Marcus et al., 1993) that was annotated in the RST treebank (Carlson et al., 2003). News data were annotated so that researchers could compare results on ARRAU with results on other news datasets; and these documents were chosen because they had already been annotated in a number of ways—not only syntactically (e.g., through the Penn Treebank (Marcus et al., 1993)) and for their argument structure (e.g., through Propbank (Palmer et al., 2005)) but also for rhetorical structure (Carlson et al., 2003). But one of the objectives of the ARRAU annotation was to cover genres other than news, so, in addition to RST, ARR"
W18-0702,P12-1084,0,0.235953,"references resolution, and discourse deixis; the evaluation scripts assessing system performance on those datasets; and preliminary results on these three tasks that may serve as baseline for subsequent research in these phenomena. 1 Furthermore, anaphora resolution involves a number of phenomena besides ‘coreference’, such as bridging reference (Clark, 1975) and discourse deixis (Webber, 1991). Only a simple form of discourse deixis, event anaphora, is annotated in O NTO N OTES; bridging reference was not annotated, although a subset of the corpus has been annotated with this information by Markert et al. (2012). A number of these limitations are overcome in the ARRAU corpus (Uryupina et al., In press). In ARRAU, all NPs are considered markables, including expletives and singletons. Both discourse deixis and bridging reference have been annotated. The corpus however, hasn’t been widely used for anaphora resolution research yet, with a few exceptions (Rodriguez, 2010; Uryupina and Poesio, 2012; Marasovi´c et al., 2017). There are a number of reasons for this, ranging from the fact that research in both bridging reference and discourse deixis is still limited, to the unusual markup format. The objectiv"
W18-0702,P16-1061,0,0.145025,"wledge is the only comparison between the two corpora in terms of system performance. Table 4 summarizes the results. 3.2 In this task, systems have to decide • whether a markable is referring or not; • if referring, whether it introduces a new entity/coreference chain (discourse new) or refers to an entity already introduced (discourse old); Discourse Deixis Marasovi´c et al. (2017) developed an approach to abstract anaphora resolution based on bidirectional LSTMs to produce representations of the anaphor and the candidate sentence, and a mention ranking component adapted from the systems by Clark and Manning (2016) and Wiseman et al. (2015). The system was tested using both the dataset by Kolhatkar et al. (2013) (for shell nouns) and the discourse deixis cases in ARRAU. 4 • in case it is classified as discourse old, the systems have to identify the antecedent (entity, or coreference chain). Data format For this task, the documents were exported in the format used for EVALITA-2011 (Uryupina and Poesio, 2013), derived from the tabular CONLL-style format used in the SEMEVAL 2010 shared task on multilingual anaphora (Recasens et al., 2010). The format used involves three tab-separated columns, with one line"
W18-0702,W00-1007,0,0.19518,"ricity is much more extensive. Another key annotation effort was the annotation of minimal spans of markables (MINs). Last but not least, extensive checks were run on the annotation of identity anaphora. This is the release used for the CRAC 2018 Shared Task. Discourse deixis is a very complex form of reference, both to annotate (Artstein and Poesio, 2006) and to resolve. Very few anaphoric annotation projects have attempted annotating discourse deixis in its entirety (Artstein and Poesio, 2006; Dipper and Zinsmeister, 2012). More typical is a partial annotation, as in (Byron and Allen, 1998; Navarretta, 2000), who annotated pronominal reference to abstract objects; in O NTO N OTES, where event anaphora was marked (Pradhan et al., 2007b); and in the work of Kolhatkar (2014), that focused on so-called shell nouns. In ARRAU, 1. A coder specifying that a referring expression is discourse old is asked whether its antecedent was introduced using a phrase (markable) or segment (discourse segment). 3 2. Coders choosing segment have to mark a sequence of predefined clauses. 3.1 Previous work on anaphora resolution with ARRAU Identity anaphora Rodriguez (2010) used BART (Versley et al., 2008) to compare the"
W18-0702,J05-1004,0,0.247159,"aphora and Coreference, pages 11–22 c New Orleans, Louisiana, June 6, 2018. 2018 Association for Computational Linguistics sisting of the entire subset of the Penn Treebank (Marcus et al., 1993) that was annotated in the RST treebank (Carlson et al., 2003). News data were annotated so that researchers could compare results on ARRAU with results on other news datasets; and these documents were chosen because they had already been annotated in a number of ways—not only syntactically (e.g., through the Penn Treebank (Marcus et al., 1993)) and for their argument structure (e.g., through Propbank (Palmer et al., 2005)) but also for rhetorical structure (Carlson et al., 2003). But one of the objectives of the ARRAU annotation was to cover genres other than news, so, in addition to RST, ARRAU includes three more sub-corpora. The TRAINS sub-corpus includes all the task-oriented dialogues in the TRAINS-93 corpus;1 the PEAR sub-corpus consists of the complete collection of spoken narratives in the Pear Stories that provided some of the early evidence on salience and anaphoric reference (Chafe, 1980); and the GNOME sub-corpus covers documents from the medical and art history genres covered by the GNOME corpus (P"
W18-0702,poesio-2000-annotating,1,0.745997,")) but also for rhetorical structure (Carlson et al., 2003). But one of the objectives of the ARRAU annotation was to cover genres other than news, so, in addition to RST, ARRAU includes three more sub-corpora. The TRAINS sub-corpus includes all the task-oriented dialogues in the TRAINS-93 corpus;1 the PEAR sub-corpus consists of the complete collection of spoken narratives in the Pear Stories that provided some of the early evidence on salience and anaphoric reference (Chafe, 1980); and the GNOME sub-corpus covers documents from the medical and art history genres covered by the GNOME corpus (Poesio, 2000a, 2004b) used to study both local and global salience (Poesio et al., 2004, 2006). The same coding scheme was used for all sub-corpora, but separate guidelines were written for the textual and the spoken dialogue sub-corpora. Table 1 provides basic statistics about the four ARRAU sub-corpora. Note in particular the large number of non-referring markables. RST, TRAINS and PEAR were used for the CRAC 2018 shared task. 2.2 pronouns are marked as well, and all premodifiers are marked when the entity referred to is mentioned again, e.g., in the case of the proper name US in (2), and when the premo"
W18-0702,N13-1111,0,0.675008,"e deixis: stating that markable 311 has been identified as belonging to entity set 148 as well as being an associative reference to entity set 3 through the undersp-rel relation. this I-markable_566 B-markable_322 This line states that token this belongs to unit markable 5665 , and it is the beginning of a discourse deixis, B-markable 322. The systems’ task is to identify which unit the discourse deixis refers to. The gold interpretation, using the =unit:<markable ID&gt; format would be as follows:6 Evaluation script The evaluation script for Task 2 is based on the evaluation method proposed in (Hou et al., 2013). The script separately measures precision and recall at anchor entity recognition (e.g., whether set 3 is the right coreference chain) and at anchor markable detection (i.e., whether markable 308 is the appropriate markable of set 3). Note that whereas the identification of the anchoring entity is considered correct whenever the right coreference chain is identified, irrespective of the particular anchor markable chosen, the identification of the anchor markable is strict, i.e., it is only considered correct if the same markable as annotated is found. 4.4 UNIT B-markable_565 I-markable_565 I-"
W18-0702,D14-1056,1,0.922068,"Missing"
W18-0702,W04-0210,1,0.724918,"s identified in the RST domain. In the table, we write P+S+E+O+U as category for the bridging references in the other domains, currently not classified. Annotating—indeed, even identifying—bridging references in a reliable way is difficult (Vieira, 1998; Poesio and Vieira, 1998), which is one of the reasons why so few large-scale corpora for anaphora include this type of annotation (Poesio et al., 2016). The ARRAU guidelines for bridging anaphora are based on experiments that started with the work of Vieira and Poesio (Vieira, 1998; Poesio and Vieira, 1998) and continued in the GNOME project (Poesio, 2004a). In GNOME, a subset of relations that could be annotated reliably was found (Poesio, 2004a), including three types of relations: element-of; Discourse deixis The term discourse deixis was introduced by Webber (1991) to indicate the reference to abstract entities which have not been introduced in the discourse through a nominal markable, as in the following example from the TRAINS corpus, where that in utterance 7.6 refers to the plan of shipping boxcars to oranges to Elmira. 13 all poss poss-inv subset subset-inv element element-inv other other-inv undersp-rel P+S+E+O+U RST 3777 87 25 1092"
W18-0702,D13-1030,1,0.897166,"izes the results. 3.2 In this task, systems have to decide • whether a markable is referring or not; • if referring, whether it introduces a new entity/coreference chain (discourse new) or refers to an entity already introduced (discourse old); Discourse Deixis Marasovi´c et al. (2017) developed an approach to abstract anaphora resolution based on bidirectional LSTMs to produce representations of the anaphor and the candidate sentence, and a mention ranking component adapted from the systems by Clark and Manning (2016) and Wiseman et al. (2015). The system was tested using both the dataset by Kolhatkar et al. (2013) (for shell nouns) and the discourse deixis cases in ARRAU. 4 • in case it is classified as discourse old, the systems have to identify the antecedent (entity, or coreference chain). Data format For this task, the documents were exported in the format used for EVALITA-2011 (Uryupina and Poesio, 2013), derived from the tabular CONLL-style format used in the SEMEVAL 2010 shared task on multilingual anaphora (Recasens et al., 2010). The format used involves three tab-separated columns, with one line per token: The Three Tasks of CRAC 2018 The CRAC 2018 Shared Task was the evaluation campaign asso"
W18-0702,N16-1030,0,0.0360409,"Missing"
W18-0702,J13-4004,0,0.284739,"Missing"
W18-0702,W05-0311,1,0.67427,"able 2: Distribution of bridging references in ARRAU. (7) 7.3 7.4 7.5 7.6 : : : : so we ship one boxcar of oranges to Elmira and that takes another 2 hours 2.5 There have been two releases of the corpus. The first release, in 2008, is discussed in (Poesio and Artstein, 2008). This first release was relatively small (about 100K words in total), and focused primarily on identity anaphora and on the annotation of ambiguity, but its development involved extensive experiments with the annotation of discourse deixis and of ambiguity that led to the annotation guidelines used throughout the project (Poesio and Artstein, 2005b,a; Artstein and Poesio, 2006). The second release, via LDC in 2013, is substantially larger than the first (350K) and the annotation of bridging reference, discourse deixis and genericity is much more extensive. Another key annotation effort was the annotation of minimal spans of markables (MINs). Last but not least, extensive checks were run on the annotation of identity anaphora. This is the release used for the CRAC 2018 Shared Task. Discourse deixis is a very complex form of reference, both to annotate (Artstein and Poesio, 2006) and to resolve. Very few anaphoric annotation projects hav"
W18-0702,D17-1021,0,0.251009,"Missing"
W18-0702,poesio-artstein-2008-anaphoric,1,0.872725,"corpus, where that in utterance 7.6 refers to the plan of shipping boxcars to oranges to Elmira. 13 all poss poss-inv subset subset-inv element element-inv other other-inv undersp-rel P+S+E+O+U RST 3777 87 25 1092 368 1126 152 332 7 588 N/A TRAINS 710 GNOME 692 PEAR 333 710 692 333 TOTAL 5512 ≥ 87 ≥ 25 ≥ 1092 ≥ 368 ≥ 1126 ≥ 152 ≥ 332 ≥7 ≥ 588 1735 Table 2: Distribution of bridging references in ARRAU. (7) 7.3 7.4 7.5 7.6 : : : : so we ship one boxcar of oranges to Elmira and that takes another 2 hours 2.5 There have been two releases of the corpus. The first release, in 2008, is discussed in (Poesio and Artstein, 2008). This first release was relatively small (about 100K words in total), and focused primarily on identity anaphora and on the annotation of ambiguity, but its development involved extensive experiments with the annotation of discourse deixis and of ambiguity that led to the annotation guidelines used throughout the project (Poesio and Artstein, 2005b,a; Artstein and Poesio, 2006). The second release, via LDC in 2013, is substantially larger than the first (350K) and the annotation of bridging reference, discourse deixis and genericity is much more extensive. Another key annotation effort was th"
W18-0702,uryupina-poesio-2012-domain,1,0.936445,"(Webber, 1991). Only a simple form of discourse deixis, event anaphora, is annotated in O NTO N OTES; bridging reference was not annotated, although a subset of the corpus has been annotated with this information by Markert et al. (2012). A number of these limitations are overcome in the ARRAU corpus (Uryupina et al., In press). In ARRAU, all NPs are considered markables, including expletives and singletons. Both discourse deixis and bridging reference have been annotated. The corpus however, hasn’t been widely used for anaphora resolution research yet, with a few exceptions (Rodriguez, 2010; Uryupina and Poesio, 2012; Marasovi´c et al., 2017). There are a number of reasons for this, ranging from the fact that research in both bridging reference and discourse deixis is still limited, to the unusual markup format. The objective of this paper is to introduce the community to the three datasets extracted from the ARRAU corpus to support this year’s CRAC18 Shared task, the first evaluation campaign based on ARRAU. Our hope is that making such datasets available may, on the one hand, facilitate the use of ARRAU ; on the other, increase the community of researchers working on these aspects of anaphora resolution"
W18-0702,J04-3003,1,0.731646,"Missing"
W18-0702,P08-4003,1,0.831518,"Missing"
W18-0702,J98-2001,1,0.785611,"Missing"
W18-0702,P14-2006,0,0.0382116,"e markables should output for every markable its coreference chain and information status (non referring, discourse new, or discourse old). Evaluation script The coreference evaluation script developed by Moosavi and Strube was modified to produce the scorer for Task 1. We will refer to this script as ’the extended coreference scorer’ below.3 The extended scorer, when run excluding non-referring expressions and singletons and ignoring MIN information, evaluates a system’s response using the same metrics (indeed, a reimplementation of the same code) as the standard CONLL evaluation script, v8 (Pradhan et al., 2014).4 When required to use MIN information, the extended scorer follows the MUC convention, and considers a mention boundary correct if it contains the MIN and doesn’t go beyond the annotated maximum boundary. When singletons are to be considered, singletons are also included in the scores (all metrics apart from MUC can deal with singletons). Finally, when run in allmarkables mode, the script scores referring and non-referring expressions separately. Referring expressions are scored using the CONLL metrics; for non-referring expressions, the script evaluates P, R and F1 at non-referring expressi"
W18-0702,P15-1137,0,0.115905,"between the two corpora in terms of system performance. Table 4 summarizes the results. 3.2 In this task, systems have to decide • whether a markable is referring or not; • if referring, whether it introduces a new entity/coreference chain (discourse new) or refers to an entity already introduced (discourse old); Discourse Deixis Marasovi´c et al. (2017) developed an approach to abstract anaphora resolution based on bidirectional LSTMs to produce representations of the anaphor and the candidate sentence, and a mention ranking component adapted from the systems by Clark and Manning (2016) and Wiseman et al. (2015). The system was tested using both the dataset by Kolhatkar et al. (2013) (for shell nouns) and the discourse deixis cases in ARRAU. 4 • in case it is classified as discourse old, the systems have to identify the antecedent (entity, or coreference chain). Data format For this task, the documents were exported in the format used for EVALITA-2011 (Uryupina and Poesio, 2013), derived from the tabular CONLL-style format used in the SEMEVAL 2010 shared task on multilingual anaphora (Recasens et al., 2010). The format used involves three tab-separated columns, with one line per token: The Three Task"
W18-0702,W12-4501,1,0.95417,"l limited, to the unusual markup format. The objective of this paper is to introduce the community to the three datasets extracted from the ARRAU corpus to support this year’s CRAC18 Shared task, the first evaluation campaign based on ARRAU. Our hope is that making such datasets available may, on the one hand, facilitate the use of ARRAU ; on the other, increase the community of researchers working on these aspects of anaphora resolution. Introduction The release of the O NTO N OTES coreference corpus (Pradhan et al., 2007a) and the organization of two CONLL shared tasks based on the dataset (Pradhan et al., 2012) have resulted in a substantial increase in coreference research, both in terms of quantity and in terms of quality. We expect O NTO N OTES to remain a key resource for the field for many years. However, O NTO N OTES also has a number of frequently mentioned limitations, including: 2 2.1 • Not all NPs of relevance to anaphora resolution are treated as markables. For instance, expletives are not annotated. The ARRAU Corpus Genres The ARRAU corpus includes a substantial amount of news text in the sub-corpus called RST, con11 Proceedings of the Workshop on Computational Models of Reference, Anaph"
W18-0702,S10-1001,1,0.840834,"Missing"
W18-0702,W18-0703,1,0.787799,"utputs the results on those separately. (The Stanford deterministic coreference resolver does not attempt to identify non-referring markables, hence all values are 0.) The first conclusion that can be obtained from this Table is that the results achieved by the Stan18 Configuration P R F1 Exclude singletons and non-referring MUC 58.65 42.33 49.17 3 B 53.20 32.40 40.27 CEAF e 42.77 37.88 40.18 CONLL score 43.21 LEA 27.61 46.17 34.55 CoNLL official scorer MUC 58.47 42.44 49.18 3 B 53.00 32.53 40.32 CEAF e 42.64 37.98 40.18 CONLL score 51.37 37.65 43.23 of Stuttgart participated in this subtask (Roesiger, 2018). We summarize here the results; for further detail, see the paper. Roesiger developed two systems, one rulebased, one ML-based. The results obtained by these systems on all three subdomains are summarized in Table 9 in the Appendix. The three columns present the result of the two systems at the tasks of (i) attempting to resolve all gold bridging references; (ii) only producing results when the system is reasonably convinced; and (iii) identifying and resolving bridging references. These results appear broadly comparable to those obtained by Hou et al. (2013) over the ISNotes corpus as far as"
W18-0702,J01-4004,0,0.690967,"Missing"
W97-1301,J86-3001,0,0.127727,"Missing"
W97-1301,P97-1072,1,0.3529,"Missing"
