2021.acl-long.164,Q17-1010,0,0.0790021,"andom weights while j ≤ t do k←1 while k ≤ b do Sample mini-batch of n examples {(ei ,mi )}n i=1 Generate word embeddings {(ei ,mi )}n i=1 of the examples. Update D and R by ascending their stochastic gradient: n ( ) 1∑ ∇θD ,θR [log D(R(ei )) + log 1 − D(F (mi )) ] n i=1 Update F by descending its stochastic gradient: n ) 1∑ ( log 1 − D(F (mi )) ∇θF n i=1 k ←k+1 end j ←j+1 end For each pair of an entity mention (ei ) and an entity-masked sentence (mi ) in the training data, we first generate two matrices of word embeddings ei and mi using word embeddings pretrained on Wikipedia with fastText (Bojanowski et al., 2017). Then, R and F generate, respectively, a 2106 real entity representation from ei and a fake entity representation from mi . Finally, they are given to D, which is a feed-forward network that judges whether F or R generated the representations, i.e., whether the representations are real or fake, using sigmoid outputs by the final logistic regression layer. The pseudo code of the training scheme is given in Algorithm 1. The training proceeds as follows: R and D as a team try to avoid the possibility that D misjudges F’s output (i.e., a fake entity representation) as a real entity representation"
2021.acl-long.164,P17-1171,0,0.0704455,"91.9 92.6 92.0 91.7 69.1 71.7 74.4 75.3 74.8 71.5 72.3 92.0 92.5 92.6 92.7 92.6 92.6 92.8 91.8 91.8 94.5 94.5 94.5 94.5 94.5 89.4 90.4 90.6 90.7 90.8 90.3 Table 3: GLUE test set results. Our model for test set results incorporates task-specific modification for CoLA and WNLI to improve scores (see Appendix A for details). All results are from the GLUE leaderboard. that only the proposed method with our GANstyle CNNs showed a higher average score than ALBERT. This suggests the effectiveness of our GAN-style pretraining scheme of CNNs. 5.2 Open-domain QA We also tested BERTAC on open-domain QA (Chen et al., 2017) with the publicly available datasets Quasar-T (Dhingra et al., 2017) and SearchQA (Dunn et al., 2017). We used the pre-processed version4 of the datasets provided by Lin et al. (2018), which contains passages retrieved for all questions, and followed their data split as described in Table 5. 5.2.1 BERTAC for open-domain QA We implemented our QA model following the approach of Lin et al. (2018), which combines a passage selector to choose relevant passages from retrieved passages and an answer span selector to identify the answer span in the selected passages. For the given question q and the"
2021.acl-long.164,D14-1179,0,0.0433062,"Missing"
2021.acl-long.164,N19-1423,0,0.0775044,"Missing"
2021.acl-long.164,2020.findings-emnlp.207,0,0.0675184,"Missing"
2021.acl-long.164,P18-1161,0,0.0223494,". Our model for test set results incorporates task-specific modification for CoLA and WNLI to improve scores (see Appendix A for details). All results are from the GLUE leaderboard. that only the proposed method with our GANstyle CNNs showed a higher average score than ALBERT. This suggests the effectiveness of our GAN-style pretraining scheme of CNNs. 5.2 Open-domain QA We also tested BERTAC on open-domain QA (Chen et al., 2017) with the publicly available datasets Quasar-T (Dhingra et al., 2017) and SearchQA (Dunn et al., 2017). We used the pre-processed version4 of the datasets provided by Lin et al. (2018), which contains passages retrieved for all questions, and followed their data split as described in Table 5. 5.2.1 BERTAC for open-domain QA We implemented our QA model following the approach of Lin et al. (2018), which combines a passage selector to choose relevant passages from retrieved passages and an answer span selector to identify the answer span in the selected passages. For the given question q and the set of retrieved passages P = {pi }, we computed the probability P r(a|q, P ) of extracting answer span a to question q from P in the following way, and then we extracted the answer sp"
2021.acl-long.164,2021.ccl-1.108,0,0.0782656,"Missing"
2021.acl-long.164,P19-1414,1,0.928508,"asks, possibly as subcomponents of their methods, and/or they have focused on scaling up TLMs or improving their pretraining schemes. As a result, other architectures like Recurrent Neural Networks (RNN) (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) and Convolutional Neural Networks (CNN) (LeCun et al., 1999) are fading away. In this work, we propose a method for improving TLMs by integrating a simple conventional CNN to them. We pretrained this CNN on Wikipedia using a Generative Adversarial Network (GAN) style training scheme (Goodfellow et al., 2014), and then combined it with TLMs. Oh et al. (2019) similarly used GAN-style training to improve a QA model using a CNN, but their training scheme was applicable only to QAspecific datasets. On the other hand, similarly to TLM, our proposed method for training the CNN is independent of specific tasks. We show that the combination of this CNN with TLMs can achieve higher performance than that of the original TLMs on publicly available datasets for several distinct tasks. We hope that this gives an insight into how to develop novel strong network architectures and training schemes. We call our combination of a TLM and a CNN BERTAC (BERT-style TL"
2021.acl-long.164,D19-1005,0,0.0486941,"Missing"
2021.acl-long.164,2020.semeval-1.271,0,0.0329287,"tary alternative to these existing methods in the sense that entity representations are integrated into TLMs via CNNs and not directly produced by the TLMs. Fine-tuning TLMs with external resources or other NNs: Yang et al. (2019a) and Liu et al. (2020) have used knowledge graphs for augmenting TLMs with entity representations during finetuning. Unlike these approaches, BERTAC uses unstructured texts rather than clean structured knowledge, such as knowledge graphs, to adversarially train a CNN. Other previous works have proposed combining CNNs or RNNs with BERT for NLP tasks (Lu et al., 2020; Safaya et al., 2020; Shao et al., 2019; Zhang et al., 2020), but their use of CNNs/RNNs was task-specific, so their models were not directly applicable to other tasks. Adversarial learning for improving TLMs: Oh et al. (2019) proposed a CNN-based answer representation generator for QA that can guess the vector representation of answers from given whytype questions and answer passages. The generator was trained in a GAN-style manner using QA datasets. We took inspiration from their adversarial training scheme to train task-independent representation generators from unsupervised texts (i.e., Wikipedia sentences in"
2021.acl-long.164,D19-1626,0,0.0212062,"hese existing methods in the sense that entity representations are integrated into TLMs via CNNs and not directly produced by the TLMs. Fine-tuning TLMs with external resources or other NNs: Yang et al. (2019a) and Liu et al. (2020) have used knowledge graphs for augmenting TLMs with entity representations during finetuning. Unlike these approaches, BERTAC uses unstructured texts rather than clean structured knowledge, such as knowledge graphs, to adversarially train a CNN. Other previous works have proposed combining CNNs or RNNs with BERT for NLP tasks (Lu et al., 2020; Safaya et al., 2020; Shao et al., 2019; Zhang et al., 2020), but their use of CNNs/RNNs was task-specific, so their models were not directly applicable to other tasks. Adversarial learning for improving TLMs: Oh et al. (2019) proposed a CNN-based answer representation generator for QA that can guess the vector representation of answers from given whytype questions and answer passages. The generator was trained in a GAN-style manner using QA datasets. We took inspiration from their adversarial training scheme to train task-independent representation generators from unsupervised texts (i.e., Wikipedia sentences in which an entity wa"
2021.acl-long.164,2020.findings-emnlp.278,0,0.366983,"lable at https://github.com/nict-wisdom/bertac. 2 Related Work Pretraining TLMs with entity information: There have been attempts to explicitly learn entity representation from text corpora using TLMs (He et al., 2020; Peters et al., 2019; Sun et al., 2020; Wang et al., 2020a; Xiong et al., 2020; Zhang et al., 2019). Our proposed method is a complementary alternative to these existing methods in the sense that entity representations are integrated into TLMs via CNNs and not directly produced by the TLMs. Fine-tuning TLMs with external resources or other NNs: Yang et al. (2019a) and Liu et al. (2020) have used knowledge graphs for augmenting TLMs with entity representations during finetuning. Unlike these approaches, BERTAC uses unstructured texts rather than clean structured knowledge, such as knowledge graphs, to adversarially train a CNN. Other previous works have proposed combining CNNs or RNNs with BERT for NLP tasks (Lu et al., 2020; Safaya et al., 2020; Shao et al., 2019; Zhang et al., 2020), but their use of CNNs/RNNs was task-specific, so their models were not directly applicable to other tasks. Adversarial learning for improving TLMs: Oh et al. (2019) proposed a CNN-based answer"
2021.acl-long.164,W18-5446,0,0.0763319,"Missing"
2021.acl-long.164,D19-1599,0,0.0172576,"CALBERT-xxlarge Table 5: Number of questions in each dataset. #p is the number of retrieved passages for each question. Non-TLM-based methods O PEN QA (Lin et al., 2018): An RNN-based method that jointly learns passage-selection and answer extraction. O PEN QA+ARG (Oh et al., 2019): An extension of O PEN QA that additionally uses an answer representation generator (ARG) trained by adversarial learning. TLM-based methods WKLM (Xiong et al., 2020): This uses a TLM pretrained with a weakly supervised objective for learning Wikipedia entity information. BERT-base was used for the training. MBERT (Wang et al., 2019): A BERT-based method that extracts answers using globally normalized answer scores across all the passages retrieved by the same question. BERT-large was used for the training. CF ORMER (Wang et al., 2020b): It uses a clusteringbased sparse transformer for long-range dependency encoding. The method was trained using RoBERTa-large. Quasar-T EM F1 42.2 49.3 43.2 49.7 45.8 52.2 51.1 59.1 54.0 63.9 55.8 63.7 58.0 65.8 SearchQA EM F1 58.8 64.5 59.6 65.3 61.7 66.7 65.1 70.7 68.0 75.1 71.9 77.1 74.0 79.2 Table 7: QA test set results. Figures of the previous works were taken from their original paper"
2021.acl-long.164,P19-1226,0,0.0244545,"code and models of BERTAC are available at https://github.com/nict-wisdom/bertac. 2 Related Work Pretraining TLMs with entity information: There have been attempts to explicitly learn entity representation from text corpora using TLMs (He et al., 2020; Peters et al., 2019; Sun et al., 2020; Wang et al., 2020a; Xiong et al., 2020; Zhang et al., 2019). Our proposed method is a complementary alternative to these existing methods in the sense that entity representations are integrated into TLMs via CNNs and not directly produced by the TLMs. Fine-tuning TLMs with external resources or other NNs: Yang et al. (2019a) and Liu et al. (2020) have used knowledge graphs for augmenting TLMs with entity representations during finetuning. Unlike these approaches, BERTAC uses unstructured texts rather than clean structured knowledge, such as knowledge graphs, to adversarially train a CNN. Other previous works have proposed combining CNNs or RNNs with BERT for NLP tasks (Lu et al., 2020; Safaya et al., 2020; Shao et al., 2019; Zhang et al., 2020), but their use of CNNs/RNNs was task-specific, so their models were not directly applicable to other tasks. Adversarial learning for improving TLMs: Oh et al. (2019) pro"
2021.acl-long.164,2020.acl-main.82,0,0.0183674,"ds in the sense that entity representations are integrated into TLMs via CNNs and not directly produced by the TLMs. Fine-tuning TLMs with external resources or other NNs: Yang et al. (2019a) and Liu et al. (2020) have used knowledge graphs for augmenting TLMs with entity representations during finetuning. Unlike these approaches, BERTAC uses unstructured texts rather than clean structured knowledge, such as knowledge graphs, to adversarially train a CNN. Other previous works have proposed combining CNNs or RNNs with BERT for NLP tasks (Lu et al., 2020; Safaya et al., 2020; Shao et al., 2019; Zhang et al., 2020), but their use of CNNs/RNNs was task-specific, so their models were not directly applicable to other tasks. Adversarial learning for improving TLMs: Oh et al. (2019) proposed a CNN-based answer representation generator for QA that can guess the vector representation of answers from given whytype questions and answer passages. The generator was trained in a GAN-style manner using QA datasets. We took inspiration from their adversarial training scheme to train task-independent representation generators from unsupervised texts (i.e., Wikipedia sentences in which an entity was masked in a cloze-t"
2021.acl-long.164,P19-1139,0,0.0533291,"Missing"
C12-2048,P05-1018,0,0.023057,"discourse entity appearing in the current utterance and was realised as most salient in the previous utterance. On the other hand, Miltsakaki and Kukich (2000) focused on investigating the relationship of the coherence of a text and the transition of centers and revealed that the rough-shift transition of centers correlates to incoherence of a text. In these studies, one of the most important work was to represent the relationship of discourse entities and their occurrences in a text based on the transition of discourse entities, which was done in a series of studies (Barzilay and Lee, 2004; Barzilay and Lapata, 2005; Lapata and Barzilay, 2005; Barzilay and Lapata, 2008). In Barzilay and Lapata (2008), the transition of discourse entities in adjacent discourse units (e.g. sentences) is formalised as an entity grid, which is a matrix of discourse entities and their realised grammatical roles, because a grammatical role of a discourse entity is a good indicator of its salience. For example, a given input text shown in Figure 1, consisting of the three sentences, each discourse entity is represented in the entity grid shown in Table 1. In the entity grid, each column is filled with the corresponding label (e"
C12-2048,J08-1001,0,0.140604,"u Tokunag a1 (1) Tokyo Institute of Technology, W8-73, 2-12-1 Ohokayama Meguro Tokyo, 152-8552 Japan {ryu-i,take} l. s.tite h.a .jp ABSTRACT We propose a simple and effective metric for automatically evaluating discourse coherence of a text using the outputs of a coreference resolution model. According to the idea that a writer tends to appropriately utilise coreference relations when writing a coherent text, we introduce a metric of discourse coherence based on automatically identified coreference relations. We empirically evaluated our metric by comparing it to the entity grid modelling by Barzilay and Lapata (2008) using Japanese newspaper articles as a target data set. The results indicate that our metric better reflects discourse coherence of texts than the existing model. KEYWORDS: discourse coherence, coreference resolution, evaluation metric. Proceedings of COLING 2012: Posters, pages 483–494, COLING 2012, Mumbai, December 2012. 483 1 Introduction The task of automatically evaluating discourse coherence has recently received much attention (Karamanis et al., 2004; Barzilay and Lapata, 2008; Lin et al., 2011, etc.) because it is essential for several NLP applications such as generation (Soricut and"
C12-2048,N04-1015,0,0.037678,"ters, each of which is a discourse entity appearing in the current utterance and was realised as most salient in the previous utterance. On the other hand, Miltsakaki and Kukich (2000) focused on investigating the relationship of the coherence of a text and the transition of centers and revealed that the rough-shift transition of centers correlates to incoherence of a text. In these studies, one of the most important work was to represent the relationship of discourse entities and their occurrences in a text based on the transition of discourse entities, which was done in a series of studies (Barzilay and Lee, 2004; Barzilay and Lapata, 2005; Lapata and Barzilay, 2005; Barzilay and Lapata, 2008). In Barzilay and Lapata (2008), the transition of discourse entities in adjacent discourse units (e.g. sentences) is formalised as an entity grid, which is a matrix of discourse entities and their realised grammatical roles, because a grammatical role of a discourse entity is a good indicator of its salience. For example, a given input text shown in Figure 1, consisting of the three sentences, each discourse entity is represented in the entity grid shown in Table 1. In the entity grid, each column is filled with"
C12-2048,P06-1049,0,0.0568112,"Missing"
C12-2048,C10-1017,0,0.0155792,"mploying our metric as a feature. 4 Coreference resolution model for a coherence metric The proposed metric introduced in Section 3 is designed for the use of any anaphora (or coreference) resolution model. In this work, we employ an NP coreference resolution model. According to formula (1) in Section 3, calculating our metric needs a reliability score of each anaphor and candidate antecedent pair. Recent sophisticated approaches to NP coreference range from considering the transitivity of discourse entities (Denis and Baldridge, 2007) to clustering-based approaches (Cardie and Wagstaf, 1999; Cai and Strube, 2010), but these approaches aim at obtaining globally optimised scores for a set of mentions. Therefore, it is generally difficult using such models to get a reliability score for a pair of two mentions though they typically achieved better performance than simple pairwise coreference resolution models such as Soon et al. (2001) and Ng and Cardie (2002), In the work on Japanese anaphora resolution by Iida and Poesio (2011), they employed an ILP-based approach to optimise final outputs of NP coreference resolution in Japanese and reported it achieved better performance than simple pairwise baselines"
C12-2048,W99-0611,0,0.0669354,"of the entity grid model employing our metric as a feature. 4 Coreference resolution model for a coherence metric The proposed metric introduced in Section 3 is designed for the use of any anaphora (or coreference) resolution model. In this work, we employ an NP coreference resolution model. According to formula (1) in Section 3, calculating our metric needs a reliability score of each anaphor and candidate antecedent pair. Recent sophisticated approaches to NP coreference range from considering the transitivity of discourse entities (Denis and Baldridge, 2007) to clustering-based approaches (Cardie and Wagstaf, 1999; Cai and Strube, 2010), but these approaches aim at obtaining globally optimised scores for a set of mentions. Therefore, it is generally difficult using such models to get a reliability score for a pair of two mentions though they typically achieved better performance than simple pairwise coreference resolution models such as Soon et al. (2001) and Ng and Cardie (2002), In the work on Japanese anaphora resolution by Iida and Poesio (2011), they employed an ILP-based approach to optimise final outputs of NP coreference resolution in Japanese and reported it achieved better performance than si"
C12-2048,N09-1042,0,0.0533691,"Missing"
C12-2048,N07-1030,0,0.018005,"tion). In Section 5.3 we will also demonstrate the results of the entity grid model employing our metric as a feature. 4 Coreference resolution model for a coherence metric The proposed metric introduced in Section 3 is designed for the use of any anaphora (or coreference) resolution model. In this work, we employ an NP coreference resolution model. According to formula (1) in Section 3, calculating our metric needs a reliability score of each anaphor and candidate antecedent pair. Recent sophisticated approaches to NP coreference range from considering the transitivity of discourse entities (Denis and Baldridge, 2007) to clustering-based approaches (Cardie and Wagstaf, 1999; Cai and Strube, 2010), but these approaches aim at obtaining globally optimised scores for a set of mentions. Therefore, it is generally difficult using such models to get a reliability score for a pair of two mentions though they typically achieved better performance than simple pairwise coreference resolution models such as Soon et al. (2001) and Ng and Cardie (2002), In the work on Japanese anaphora resolution by Iida and Poesio (2011), they employed an ILP-based approach to optimise final outputs of NP coreference resolution in Jap"
C12-2048,J95-2003,0,0.939254,"task of automatically evaluating discourse coherence has recently received much attention (Karamanis et al., 2004; Barzilay and Lapata, 2008; Lin et al., 2011, etc.) because it is essential for several NLP applications such as generation (Soricut and Marcu, 2006), summarisation (Lapata, 2003; Okazaki et al., 2004; Bollegala et al., 2006) and automated essay scoring (Miltsakaki and Kukich, 2000; Higgins et al., 2004). Researchers in these areas have mainly been concerned with introducing the linguistic notions of cohesion or coherence addressed in discourse theories, such as Centering Theory (Grosz et al., 1995) and Rhetorical Structure Theory (Mann and Thompson, 1988), into computational models for each task, ranging from heuristic rule-based to sophisticated machine learning-based approaches. Some of this research has relied on the occurrence of discourse entities (e.g. NPs and pronouns) to capture cohesion of a text for indirectly estimating discourse coherence. Barzilay and Lapata (2008)’s approach, for instance, models the transition of discourse entities appearing in adjacent sentences for capturing local discourse coherence, which is derived from the notion of Centering Theory. In their approa"
C12-2048,N04-1024,0,0.234087,"existing model. KEYWORDS: discourse coherence, coreference resolution, evaluation metric. Proceedings of COLING 2012: Posters, pages 483–494, COLING 2012, Mumbai, December 2012. 483 1 Introduction The task of automatically evaluating discourse coherence has recently received much attention (Karamanis et al., 2004; Barzilay and Lapata, 2008; Lin et al., 2011, etc.) because it is essential for several NLP applications such as generation (Soricut and Marcu, 2006), summarisation (Lapata, 2003; Okazaki et al., 2004; Bollegala et al., 2006) and automated essay scoring (Miltsakaki and Kukich, 2000; Higgins et al., 2004). Researchers in these areas have mainly been concerned with introducing the linguistic notions of cohesion or coherence addressed in discourse theories, such as Centering Theory (Grosz et al., 1995) and Rhetorical Structure Theory (Mann and Thompson, 1988), into computational models for each task, ranging from heuristic rule-based to sophisticated machine learning-based approaches. Some of this research has relied on the occurrence of discourse entities (e.g. NPs and pronouns) to capture cohesion of a text for indirectly estimating discourse coherence. Barzilay and Lapata (2008)’s approach, f"
C12-2048,P11-1081,1,0.909168,"ed approaches to NP coreference range from considering the transitivity of discourse entities (Denis and Baldridge, 2007) to clustering-based approaches (Cardie and Wagstaf, 1999; Cai and Strube, 2010), but these approaches aim at obtaining globally optimised scores for a set of mentions. Therefore, it is generally difficult using such models to get a reliability score for a pair of two mentions though they typically achieved better performance than simple pairwise coreference resolution models such as Soon et al. (2001) and Ng and Cardie (2002), In the work on Japanese anaphora resolution by Iida and Poesio (2011), they employed an ILP-based approach to optimise final outputs of NP coreference resolution in Japanese and reported it achieved better performance than simple pairwise baselines. In spite of the global optimisation by ILP, their formulation can be easily reinterpreted as follows due to the best-first constraint used in their ILP formula, which is for avoiding the redundant choice of more than one candidate antecedent: coref(i, j) = P(cor e f |i, j) + P(anaph |j) 2 (2) where j is a candidate anaphor and i is the most likely candidate antecedent of j. P(cor e f |i, j) is calculated by a simple"
C12-2048,P09-2022,0,0.0138399,"on ranking a pair of coherent and incoherent texts by comparing our metric with the entity grid model. 5.1 Data set For our evaluation, we used the NAIST text corpus, which consists of Japanese newspaper articles containing manually annotated NP coreference relations. Because the corpus has no explicit boundary between training and test sets, articles published from January 1st to January 11th and the editorials from January to August were used for training and articles dated January 14th to 17th and editorials dated October to December are used for testing as done by Taira et al. (2008) and Imamura et al. (2009). Table 2 summarises the statistics of annotated coreference relations in the corpus. Because the data set contains some texts consisting of only a sentence2 , we excluded them for our evaluation of information ordering. In line with the experiments done by Barzilay and Lapata (2008), we created 20 different texts by randomly scrambling the order of the sentences in an original text, each of which is henceforth called an incoherent text, while the original text is called a coherent text. In this evaluation, we followed Barzilay and Lapata (2008)’s experimental setting, that is, the task of pai"
C12-2048,P03-1069,0,0.102416,"Missing"
C12-2048,P11-1100,0,0.22703,"e empirically evaluated our metric by comparing it to the entity grid modelling by Barzilay and Lapata (2008) using Japanese newspaper articles as a target data set. The results indicate that our metric better reflects discourse coherence of texts than the existing model. KEYWORDS: discourse coherence, coreference resolution, evaluation metric. Proceedings of COLING 2012: Posters, pages 483–494, COLING 2012, Mumbai, December 2012. 483 1 Introduction The task of automatically evaluating discourse coherence has recently received much attention (Karamanis et al., 2004; Barzilay and Lapata, 2008; Lin et al., 2011, etc.) because it is essential for several NLP applications such as generation (Soricut and Marcu, 2006), summarisation (Lapata, 2003; Okazaki et al., 2004; Bollegala et al., 2006) and automated essay scoring (Miltsakaki and Kukich, 2000; Higgins et al., 2004). Researchers in these areas have mainly been concerned with introducing the linguistic notions of cohesion or coherence addressed in discourse theories, such as Centering Theory (Grosz et al., 1995) and Rhetorical Structure Theory (Mann and Thompson, 1988), into computational models for each task, ranging from heuristic rule-based to so"
C12-2048,P02-1014,0,0.2621,"ore of each anaphor and candidate antecedent pair. Recent sophisticated approaches to NP coreference range from considering the transitivity of discourse entities (Denis and Baldridge, 2007) to clustering-based approaches (Cardie and Wagstaf, 1999; Cai and Strube, 2010), but these approaches aim at obtaining globally optimised scores for a set of mentions. Therefore, it is generally difficult using such models to get a reliability score for a pair of two mentions though they typically achieved better performance than simple pairwise coreference resolution models such as Soon et al. (2001) and Ng and Cardie (2002), In the work on Japanese anaphora resolution by Iida and Poesio (2011), they employed an ILP-based approach to optimise final outputs of NP coreference resolution in Japanese and reported it achieved better performance than simple pairwise baselines. In spite of the global optimisation by ILP, their formulation can be easily reinterpreted as follows due to the best-first constraint used in their ILP formula, which is for avoiding the redundant choice of more than one candidate antecedent: coref(i, j) = P(cor e f |i, j) + P(anaph |j) 2 (2) where j is a candidate anaphor and i is the most likel"
C12-2048,C02-1139,0,0.186282,"ore of each anaphor and candidate antecedent pair. Recent sophisticated approaches to NP coreference range from considering the transitivity of discourse entities (Denis and Baldridge, 2007) to clustering-based approaches (Cardie and Wagstaf, 1999; Cai and Strube, 2010), but these approaches aim at obtaining globally optimised scores for a set of mentions. Therefore, it is generally difficult using such models to get a reliability score for a pair of two mentions though they typically achieved better performance than simple pairwise coreference resolution models such as Soon et al. (2001) and Ng and Cardie (2002), In the work on Japanese anaphora resolution by Iida and Poesio (2011), they employed an ILP-based approach to optimise final outputs of NP coreference resolution in Japanese and reported it achieved better performance than simple pairwise baselines. In spite of the global optimisation by ILP, their formulation can be easily reinterpreted as follows due to the best-first constraint used in their ILP formula, which is for avoiding the redundant choice of more than one candidate antecedent: coref(i, j) = P(cor e f |i, j) + P(anaph |j) 2 (2) where j is a candidate anaphor and i is the most likel"
C12-2048,C04-1108,0,0.0444743,"Missing"
C12-2048,J04-3003,0,0.0368159,"has been an increase in recent work for evaluating discourse (local) coherence of a text (Barzilay and Lapata, 2008; Karamanis et al., 2004; Lin et al., 2011; Miltsakaki and Kukich, 2000; Higgins et al., 2004, etc.), which strongly relates to the cohesion of discourse entities appearing in the text from the theoretical perspective mainly based on Centering Theory (Grosz et al., 1995). For example, Karamanis et al. (2004) and Miltsakaki and Kukich (2000) proposed a metric of coherence directly utilising the transition of centers in a text, as Centering Theory does. According to the analysis by Poesio et al. (2004), Karamanis et al. (2004) define a metric based on the numbers of missing backward-looking centers, each of which is a discourse entity appearing in the current utterance and was realised as most salient in the previous utterance. On the other hand, Miltsakaki and Kukich (2000) focused on investigating the relationship of the coherence of a text and the transition of centers and revealed that the rough-shift transition of centers correlates to incoherence of a text. In these studies, one of the most important work was to represent the relationship of discourse entities and their occurrences in"
C12-2048,J01-4004,0,0.115366,"needs a reliability score of each anaphor and candidate antecedent pair. Recent sophisticated approaches to NP coreference range from considering the transitivity of discourse entities (Denis and Baldridge, 2007) to clustering-based approaches (Cardie and Wagstaf, 1999; Cai and Strube, 2010), but these approaches aim at obtaining globally optimised scores for a set of mentions. Therefore, it is generally difficult using such models to get a reliability score for a pair of two mentions though they typically achieved better performance than simple pairwise coreference resolution models such as Soon et al. (2001) and Ng and Cardie (2002), In the work on Japanese anaphora resolution by Iida and Poesio (2011), they employed an ILP-based approach to optimise final outputs of NP coreference resolution in Japanese and reported it achieved better performance than simple pairwise baselines. In spite of the global optimisation by ILP, their formulation can be easily reinterpreted as follows due to the best-first constraint used in their ILP formula, which is for avoiding the redundant choice of more than one candidate antecedent: coref(i, j) = P(cor e f |i, j) + P(anaph |j) 2 (2) where j is a candidate anapho"
C12-2048,P06-2103,0,0.0485206,"Missing"
C12-2048,D08-1055,0,0.017004,"an empirical evaluation on ranking a pair of coherent and incoherent texts by comparing our metric with the entity grid model. 5.1 Data set For our evaluation, we used the NAIST text corpus, which consists of Japanese newspaper articles containing manually annotated NP coreference relations. Because the corpus has no explicit boundary between training and test sets, articles published from January 1st to January 11th and the editorials from January to August were used for training and articles dated January 14th to 17th and editorials dated October to December are used for testing as done by Taira et al. (2008) and Imamura et al. (2009). Table 2 summarises the statistics of annotated coreference relations in the corpus. Because the data set contains some texts consisting of only a sentence2 , we excluded them for our evaluation of information ordering. In line with the experiments done by Barzilay and Lapata (2008), we created 20 different texts by randomly scrambling the order of the sentences in an original text, each of which is henceforth called an incoherent text, while the original text is called a coherent text. In this evaluation, we followed Barzilay and Lapata (2008)’s experimental setting"
C12-2134,D08-1073,0,0.106911,"logy, Japan katsumasay@gmail.com, masayu-a@ninjal.ac.jp, ryu-i@cl.cs.titech.ac.jp Abstract This paper presents a temporal relation identification method optimizing relations at sentence and document levels. Temporal relation identification is to identify temporal orders between events and time expressions. Various approaches of this task have been studied through the shared tasks TempEval (Verhagen et al., 2007, 2010). Not only identifying each temporal relation independently, some works also try to find multiple temporal relations jointly by logical constraints in Integer Linear Programming (Chambers and Jurafsky, 2008; Do et al., 2012) or Markov Logic Networks (Yoshikawa et al., 2009; Ling and Weld, 2010; Ha et al., 2010). Though previous joint approaches optimize temporal relations in an entire document, we first optimize our model at sentence level and then extend it to document level. We consider that different types of temporal relations require different types of optimizations. By evaluating our sentence and document optimized model on the TempEval-2 data, we show that our approaches can achieve competitive performance in comparison to other state-of-the-art systems. We find that the sentence and docu"
C12-2134,D12-1062,0,0.0480689,"com, masayu-a@ninjal.ac.jp, ryu-i@cl.cs.titech.ac.jp Abstract This paper presents a temporal relation identification method optimizing relations at sentence and document levels. Temporal relation identification is to identify temporal orders between events and time expressions. Various approaches of this task have been studied through the shared tasks TempEval (Verhagen et al., 2007, 2010). Not only identifying each temporal relation independently, some works also try to find multiple temporal relations jointly by logical constraints in Integer Linear Programming (Chambers and Jurafsky, 2008; Do et al., 2012) or Markov Logic Networks (Yoshikawa et al., 2009; Ling and Weld, 2010; Ha et al., 2010). Though previous joint approaches optimize temporal relations in an entire document, we first optimize our model at sentence level and then extend it to document level. We consider that different types of temporal relations require different types of optimizations. By evaluating our sentence and document optimized model on the TempEval-2 data, we show that our approaches can achieve competitive performance in comparison to other state-of-the-art systems. We find that the sentence and document optimized mod"
C12-2134,S10-1076,0,0.230641,"oral relation identification method optimizing relations at sentence and document levels. Temporal relation identification is to identify temporal orders between events and time expressions. Various approaches of this task have been studied through the shared tasks TempEval (Verhagen et al., 2007, 2010). Not only identifying each temporal relation independently, some works also try to find multiple temporal relations jointly by logical constraints in Integer Linear Programming (Chambers and Jurafsky, 2008; Do et al., 2012) or Markov Logic Networks (Yoshikawa et al., 2009; Ling and Weld, 2010; Ha et al., 2010). Though previous joint approaches optimize temporal relations in an entire document, we first optimize our model at sentence level and then extend it to document level. We consider that different types of temporal relations require different types of optimizations. By evaluating our sentence and document optimized model on the TempEval-2 data, we show that our approaches can achieve competitive performance in comparison to other state-of-the-art systems. We find that the sentence and document optimized model has strong tasks in TempEval-2, respectively. Keywords: temporal relation identificat"
C12-2134,S10-1063,0,0.334019,"n which contains all the local and global features. 2.1 Local Model Our local model utilizes only local features and solves each task independently as a local classification problem. In the Markov Logic framework, local features are represented as local formulae. We say that a formula is local if it only considers the hidden temporal relation of a single event-event, event-time or event-DCT pair. The formulae in the second class are global: they involve two or more temporal relations at the same time. The local features are based on features employed in previous work (UzZaman and Allen, 2010; Llorens et al., 2010) and are listed in Table 3. In order to illustrate how we implement each feature as a formula, we show a simple example. Consider the tense-feature for Task F. For this feature we introduce a predicate tense(e, te) that denotes the tense te for an event e. In Table 3, this feature corresponds to the second row “EVENT-tense”. For Task F, we employ the tense combinations of two events (e1 x e2). Then we add a formula such as tense(e1, +te1) ∧ tense(e2, +te2) ⇒ e2s(e1, e2, +R) (1) which represents the properties of combinations between tense and event-event relations. Note, “+” sign means that th"
C12-2134,P06-1095,0,0.03644,"pEval-2. The temporal relations (TLINKs) are annotated as shown in Table 1 and we have to estimate these TLINK labels such as BEFORE, OVERLAP, and AFTER. task relation Task C Task D Task D Task D Task E Task F Task F Table e53 (change) OVERLAP t10 (a couple of years) e50 (think) OVERLAP t0 (DCT) e52 (think) OVERLAP t0 (DCT) e53 (change) AFTER t0 (DCT) e50 (think) OVERLAP e57 (reposition) e50 (think) OVERLAP e51 (gloomy) e52 (think) BEFORE e53 (change) 1: Temporal Relations (TLINKs) in Figure 1 While the first studies handled this task as local classification problems (Boguraev and Ando, 2005; Mani et al., 2006), some recent works regard temporal relation identification as a global optimization problem in an entire document. Global optimization approaches take into account several relations and jointly identify all relations within a document. In order to ensure the consistencies among relations, previous work exploited global approaches with transitivity constraints in Integer Linear Programming (Chambers and Jurafsky, 2008; Do et al., 2012) or Markov Logic (Yoshikawa et al., 2009; Ling and Weld, 2010). In this paper, we propose a new approach to temporal relation identification by optimizing tempor"
C12-2134,N09-1018,0,0.0249243,". With event-argument relations (semantic roles), we construct some more global formulae. For e2d, we assume that the relations sharing the same time expression have the same relations. Such properties can be expressed as, srl(e1, t, AM-TMP) ∧ srl(e2, t, AM-TMP) ⇒ e2d(e1, R1) ∧ e2d(e2, R2) ∧ R1 = R2. (10) Likewise, for e2t, we assume that the relations sharing the same time expression affect each other: srl(e1, t, AM-TMP) ∧ srl(e2, t, AM-TMP) ∧ e2t(e1, t, +R1) ⇒ e2t(e2, t, +R2). (11) It is easy for the sentence-optimized model to implement much more features and constraints as in other tasks (Meza-Ruiz and Riedel, 2009; Yoshikawa et al., 2011). 2.3 Document-optimized Model The last model is the method which optimizes problems at document level. We add another hidden predicate e2e which handles Task E of TempEval-2. Note, in order to pursue computational efficiency, we should deal with e2t, e2d, and e2s as observed predicates and solve only e2e in this phase. However, we only add a few global formulae and can construct 2 Formula (5b) is instantiated by the relations in Figure 1 1376 a global model which jointly optimizes four tasks. We no longer change the formulae we constructed for the sentence-optimized m"
C12-2134,D08-1068,0,0.069323,"Missing"
C12-2134,S10-1062,0,0.0904227,"ed model is a full version which contains all the local and global features. 2.1 Local Model Our local model utilizes only local features and solves each task independently as a local classification problem. In the Markov Logic framework, local features are represented as local formulae. We say that a formula is local if it only considers the hidden temporal relation of a single event-event, event-time or event-DCT pair. The formulae in the second class are global: they involve two or more temporal relations at the same time. The local features are based on features employed in previous work (UzZaman and Allen, 2010; Llorens et al., 2010) and are listed in Table 3. In order to illustrate how we implement each feature as a formula, we show a simple example. Consider the tense-feature for Task F. For this feature we introduce a predicate tense(e, te) that denotes the tense te for an event e. In Table 3, this feature corresponds to the second row “EVENT-tense”. For Task F, we employ the tense combinations of two events (e1 x e2). Then we add a formula such as tense(e1, +te1) ∧ tense(e2, +te2) ⇒ e2s(e1, e2, +R) (1) which represents the properties of combinations between tense and event-event relations. Note,"
C12-2134,S07-1014,0,0.171442,"and Document Optimizations Katsumasa Y oshikawa1 M asayuki Asahara2 Ryu Iida3 (1) IBM Research, Tokyo, Japan (2) National Institute for Japanese Language and Linguistics, Japan (3) Tokyo Institute of Technology, Japan katsumasay@gmail.com, masayu-a@ninjal.ac.jp, ryu-i@cl.cs.titech.ac.jp Abstract This paper presents a temporal relation identification method optimizing relations at sentence and document levels. Temporal relation identification is to identify temporal orders between events and time expressions. Various approaches of this task have been studied through the shared tasks TempEval (Verhagen et al., 2007, 2010). Not only identifying each temporal relation independently, some works also try to find multiple temporal relations jointly by logical constraints in Integer Linear Programming (Chambers and Jurafsky, 2008; Do et al., 2012) or Markov Logic Networks (Yoshikawa et al., 2009; Ling and Weld, 2010; Ha et al., 2010). Though previous joint approaches optimize temporal relations in an entire document, we first optimize our model at sentence level and then extend it to document level. We consider that different types of temporal relations require different types of optimizations. By evaluating"
C12-2134,I11-1126,1,0.806752,"ons (semantic roles), we construct some more global formulae. For e2d, we assume that the relations sharing the same time expression have the same relations. Such properties can be expressed as, srl(e1, t, AM-TMP) ∧ srl(e2, t, AM-TMP) ⇒ e2d(e1, R1) ∧ e2d(e2, R2) ∧ R1 = R2. (10) Likewise, for e2t, we assume that the relations sharing the same time expression affect each other: srl(e1, t, AM-TMP) ∧ srl(e2, t, AM-TMP) ∧ e2t(e1, t, +R1) ⇒ e2t(e2, t, +R2). (11) It is easy for the sentence-optimized model to implement much more features and constraints as in other tasks (Meza-Ruiz and Riedel, 2009; Yoshikawa et al., 2011). 2.3 Document-optimized Model The last model is the method which optimizes problems at document level. We add another hidden predicate e2e which handles Task E of TempEval-2. Note, in order to pursue computational efficiency, we should deal with e2t, e2d, and e2s as observed predicates and solve only e2e in this phase. However, we only add a few global formulae and can construct 2 Formula (5b) is instantiated by the relations in Figure 1 1376 a global model which jointly optimizes four tasks. We no longer change the formulae we constructed for the sentence-optimized model. So, what we have to"
C12-2134,P09-1046,1,0.892116,"ech.ac.jp Abstract This paper presents a temporal relation identification method optimizing relations at sentence and document levels. Temporal relation identification is to identify temporal orders between events and time expressions. Various approaches of this task have been studied through the shared tasks TempEval (Verhagen et al., 2007, 2010). Not only identifying each temporal relation independently, some works also try to find multiple temporal relations jointly by logical constraints in Integer Linear Programming (Chambers and Jurafsky, 2008; Do et al., 2012) or Markov Logic Networks (Yoshikawa et al., 2009; Ling and Weld, 2010; Ha et al., 2010). Though previous joint approaches optimize temporal relations in an entire document, we first optimize our model at sentence level and then extend it to document level. We consider that different types of temporal relations require different types of optimizations. By evaluating our sentence and document optimized model on the TempEval-2 data, we show that our approaches can achieve competitive performance in comparison to other state-of-the-art systems. We find that the sentence and document optimized model has strong tasks in TempEval-2, respectively."
C12-2134,S10-1010,0,\N,Missing
D15-1260,D13-1135,0,0.228295,"tic rules (Kameyama, 1986; Walker et al., 1994; Okumura and Tamura, 1996; Nakaiwa and Shirai, 1996), which are mainly motivated by the rules and preferences introduced in Centering Theory (Grosz et al., 1995). However, the research trend of zero anaphora resolution has shifted from such rule-based approaches to machine learningbased approaches because in machine learning we can easily integrate many different types of information, such as morpho-syntactic, semantic and discourse-related information. Researchers have developed methods of zero anaphora resolution for Chinese (Zhao and Ng, 2007; Chen and Ng, 2013), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Sasano et al., 2008; Sasano et al., 2009; Imamura 2180 et al., 2009; Watanabe et al., 2010; Hayashibe et al., 2011; Iida and Poesio, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Yoshino et al., 2013) and Italian (Iida and Poesio, 2011). One critical issue in zero anaphora resolution is optimizing the outputs of sub-problems (e.g., zero anaphor detection and antecedent identification). Recent works by Watanabe et al. (2010), Iida and Poesio (2011) and Yoshikawa et al. (2011) revealed that joint"
D15-1260,J95-2003,0,0.789369,"Missing"
D15-1260,D13-1095,0,0.205399,"from such rule-based approaches to machine learningbased approaches because in machine learning we can easily integrate many different types of information, such as morpho-syntactic, semantic and discourse-related information. Researchers have developed methods of zero anaphora resolution for Chinese (Zhao and Ng, 2007; Chen and Ng, 2013), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Sasano et al., 2008; Sasano et al., 2009; Imamura 2180 et al., 2009; Watanabe et al., 2010; Hayashibe et al., 2011; Iida and Poesio, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Yoshino et al., 2013) and Italian (Iida and Poesio, 2011). One critical issue in zero anaphora resolution is optimizing the outputs of sub-problems (e.g., zero anaphor detection and antecedent identification). Recent works by Watanabe et al. (2010), Iida and Poesio (2011) and Yoshikawa et al. (2011) revealed that joint inference improves the overall performance of zero anaphora resolution. We employed one of these works as a baseline in Section 6. Concerning subject sharing recognition, related methods have been explored for pronominal anaphora (Yang et al., 2005) or coreference resolution ("
D15-1260,P14-1093,1,0.808665,"Missing"
D15-1260,I11-1023,0,0.55312,"5). However, the research trend of zero anaphora resolution has shifted from such rule-based approaches to machine learningbased approaches because in machine learning we can easily integrate many different types of information, such as morpho-syntactic, semantic and discourse-related information. Researchers have developed methods of zero anaphora resolution for Chinese (Zhao and Ng, 2007; Chen and Ng, 2013), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Sasano et al., 2008; Sasano et al., 2009; Imamura 2180 et al., 2009; Watanabe et al., 2010; Hayashibe et al., 2011; Iida and Poesio, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Yoshino et al., 2013) and Italian (Iida and Poesio, 2011). One critical issue in zero anaphora resolution is optimizing the outputs of sub-problems (e.g., zero anaphor detection and antecedent identification). Recent works by Watanabe et al. (2010), Iida and Poesio (2011) and Yoshikawa et al. (2011) revealed that joint inference improves the overall performance of zero anaphora resolution. We employed one of these works as a baseline in Section 6. Concerning subject sharing recognition, related methods have been explored for"
D15-1260,P11-1081,1,0.910037,"h trend of zero anaphora resolution has shifted from such rule-based approaches to machine learningbased approaches because in machine learning we can easily integrate many different types of information, such as morpho-syntactic, semantic and discourse-related information. Researchers have developed methods of zero anaphora resolution for Chinese (Zhao and Ng, 2007; Chen and Ng, 2013), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Sasano et al., 2008; Sasano et al., 2009; Imamura 2180 et al., 2009; Watanabe et al., 2010; Hayashibe et al., 2011; Iida and Poesio, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Yoshino et al., 2013) and Italian (Iida and Poesio, 2011). One critical issue in zero anaphora resolution is optimizing the outputs of sub-problems (e.g., zero anaphor detection and antecedent identification). Recent works by Watanabe et al. (2010), Iida and Poesio (2011) and Yoshikawa et al. (2011) revealed that joint inference improves the overall performance of zero anaphora resolution. We employed one of these works as a baseline in Section 6. Concerning subject sharing recognition, related methods have been explored for pronominal anaphora (Y"
D15-1260,W07-1522,1,0.837638,"Nakaiwa and Shirai, 1996), which are mainly motivated by the rules and preferences introduced in Centering Theory (Grosz et al., 1995). However, the research trend of zero anaphora resolution has shifted from such rule-based approaches to machine learningbased approaches because in machine learning we can easily integrate many different types of information, such as morpho-syntactic, semantic and discourse-related information. Researchers have developed methods of zero anaphora resolution for Chinese (Zhao and Ng, 2007; Chen and Ng, 2013), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Sasano et al., 2008; Sasano et al., 2009; Imamura 2180 et al., 2009; Watanabe et al., 2010; Hayashibe et al., 2011; Iida and Poesio, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Yoshino et al., 2013) and Italian (Iida and Poesio, 2011). One critical issue in zero anaphora resolution is optimizing the outputs of sub-problems (e.g., zero anaphor detection and antecedent identification). Recent works by Watanabe et al. (2010), Iida and Poesio (2011) and Yoshikawa et al. (2011) revealed that joint inference improves the overall performance of zero anaphora resolution."
D15-1260,P09-2022,0,0.734906,"Missing"
D15-1260,W03-1024,0,0.669377,"kumura and Tamura, 1996; Nakaiwa and Shirai, 1996), which are mainly motivated by the rules and preferences introduced in Centering Theory (Grosz et al., 1995). However, the research trend of zero anaphora resolution has shifted from such rule-based approaches to machine learningbased approaches because in machine learning we can easily integrate many different types of information, such as morpho-syntactic, semantic and discourse-related information. Researchers have developed methods of zero anaphora resolution for Chinese (Zhao and Ng, 2007; Chen and Ng, 2013), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Sasano et al., 2008; Sasano et al., 2009; Imamura 2180 et al., 2009; Watanabe et al., 2010; Hayashibe et al., 2011; Iida and Poesio, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Yoshino et al., 2013) and Italian (Iida and Poesio, 2011). One critical issue in zero anaphora resolution is optimizing the outputs of sub-problems (e.g., zero anaphor detection and antecedent identification). Recent works by Watanabe et al. (2010), Iida and Poesio (2011) and Yoshikawa et al. (2011) revealed that joint inference improves the overall performance of zero an"
D15-1260,P86-1031,0,0.769488,"sharing recognition for the three types in Section 4. We evaluate how effectively our method recognizes subject sharing relations for these types in Section 5. After that, we investigate the impact of explicitly introducing SSPNs in Section 6 and compare our zero anaphora resolution method with a state-of-the-art ILP-based method on the task of intra-sentential subject zero anaphora resolution in Section 7. Finally, in Section 8 we summarize this work and discuss future directions. 2 Related work Traditional approaches to zero anaphora resolution are based on manually created heuristic rules (Kameyama, 1986; Walker et al., 1994; Okumura and Tamura, 1996; Nakaiwa and Shirai, 1996), which are mainly motivated by the rules and preferences introduced in Centering Theory (Grosz et al., 1995). However, the research trend of zero anaphora resolution has shifted from such rule-based approaches to machine learningbased approaches because in machine learning we can easily integrate many different types of information, such as morpho-syntactic, semantic and discourse-related information. Researchers have developed methods of zero anaphora resolution for Chinese (Zhao and Ng, 2007; Chen and Ng, 2013), Japan"
D15-1260,P08-1047,1,0.795694,"pe of subject sharing relations, whether certain nouns appear between two predicates is an important clue, e.g., koto (complementizer) in example (6) and nouryoku (ability) in example (7). 2183 Name Description PoSi (PoSj ) PoS of pi (pj ) lemmai (lemmaj ) lemma of pi (pj ) func wi (func wj ) function words following pi (pj ) casei (casej ) case marker of arguments of pi (pj ) btw case case marker of arguments that appeared between pi and pj NpPoS* PoS of np Np lemma* lemma of np func wnp * function words following np casenp * case marker of dependents of np n class* noun class of np based on Kazama and Torisawa (2008) pi and pj stand for the left and right predicates in predicate pairs. np is the noun phrase between pi and pj . bi (bj ) stands for the bunsetsu-unit4 including pi (pj ). The features marked with * are only used for PNP type. Table 1: Features of subject sharing recognition (6) seifui -wa (ϕi -ga) sono isetsu-o government-TOP iti -SUBJ the relocation-OBJ mitomeru koto-o kime-ta . admit decide-PAST period COMP - OBJ by assigning noun n to class c when the model parameter p(c|n) > θ (θ = 0.2). 5 Experiment 1: pairwise subject sharing recognition The governmenti decided that (iti ) admits the re"
D15-1260,C96-2137,0,0.0578603,"te how effectively our method recognizes subject sharing relations for these types in Section 5. After that, we investigate the impact of explicitly introducing SSPNs in Section 6 and compare our zero anaphora resolution method with a state-of-the-art ILP-based method on the task of intra-sentential subject zero anaphora resolution in Section 7. Finally, in Section 8 we summarize this work and discuss future directions. 2 Related work Traditional approaches to zero anaphora resolution are based on manually created heuristic rules (Kameyama, 1986; Walker et al., 1994; Okumura and Tamura, 1996; Nakaiwa and Shirai, 1996), which are mainly motivated by the rules and preferences introduced in Centering Theory (Grosz et al., 1995). However, the research trend of zero anaphora resolution has shifted from such rule-based approaches to machine learningbased approaches because in machine learning we can easily integrate many different types of information, such as morpho-syntactic, semantic and discourse-related information. Researchers have developed methods of zero anaphora resolution for Chinese (Zhao and Ng, 2007; Chen and Ng, 2013), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira"
D15-1260,C96-2147,0,0.195854,"es in Section 4. We evaluate how effectively our method recognizes subject sharing relations for these types in Section 5. After that, we investigate the impact of explicitly introducing SSPNs in Section 6 and compare our zero anaphora resolution method with a state-of-the-art ILP-based method on the task of intra-sentential subject zero anaphora resolution in Section 7. Finally, in Section 8 we summarize this work and discuss future directions. 2 Related work Traditional approaches to zero anaphora resolution are based on manually created heuristic rules (Kameyama, 1986; Walker et al., 1994; Okumura and Tamura, 1996; Nakaiwa and Shirai, 1996), which are mainly motivated by the rules and preferences introduced in Centering Theory (Grosz et al., 1995). However, the research trend of zero anaphora resolution has shifted from such rule-based approaches to machine learningbased approaches because in machine learning we can easily integrate many different types of information, such as morpho-syntactic, semantic and discourse-related information. Researchers have developed methods of zero anaphora resolution for Chinese (Zhao and Ng, 2007; Chen and Ng, 2013), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003"
D15-1260,C14-1135,1,0.876749,"Missing"
D15-1260,C08-1097,0,0.499313,"nly motivated by the rules and preferences introduced in Centering Theory (Grosz et al., 1995). However, the research trend of zero anaphora resolution has shifted from such rule-based approaches to machine learningbased approaches because in machine learning we can easily integrate many different types of information, such as morpho-syntactic, semantic and discourse-related information. Researchers have developed methods of zero anaphora resolution for Chinese (Zhao and Ng, 2007; Chen and Ng, 2013), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Sasano et al., 2008; Sasano et al., 2009; Imamura 2180 et al., 2009; Watanabe et al., 2010; Hayashibe et al., 2011; Iida and Poesio, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Yoshino et al., 2013) and Italian (Iida and Poesio, 2011). One critical issue in zero anaphora resolution is optimizing the outputs of sub-problems (e.g., zero anaphor detection and antecedent identification). Recent works by Watanabe et al. (2010), Iida and Poesio (2011) and Yoshikawa et al. (2011) revealed that joint inference improves the overall performance of zero anaphora resolution. We employed one of these works as a baseli"
D15-1260,N09-1059,0,0.389782,"rules and preferences introduced in Centering Theory (Grosz et al., 1995). However, the research trend of zero anaphora resolution has shifted from such rule-based approaches to machine learningbased approaches because in machine learning we can easily integrate many different types of information, such as morpho-syntactic, semantic and discourse-related information. Researchers have developed methods of zero anaphora resolution for Chinese (Zhao and Ng, 2007; Chen and Ng, 2013), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Sasano et al., 2008; Sasano et al., 2009; Imamura 2180 et al., 2009; Watanabe et al., 2010; Hayashibe et al., 2011; Iida and Poesio, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Yoshino et al., 2013) and Italian (Iida and Poesio, 2011). One critical issue in zero anaphora resolution is optimizing the outputs of sub-problems (e.g., zero anaphor detection and antecedent identification). Recent works by Watanabe et al. (2010), Iida and Poesio (2011) and Yoshikawa et al. (2011) revealed that joint inference improves the overall performance of zero anaphora resolution. We employed one of these works as a baseline in Section 6. Conc"
D15-1260,C02-1078,0,0.85003,"ker et al., 1994; Okumura and Tamura, 1996; Nakaiwa and Shirai, 1996), which are mainly motivated by the rules and preferences introduced in Centering Theory (Grosz et al., 1995). However, the research trend of zero anaphora resolution has shifted from such rule-based approaches to machine learningbased approaches because in machine learning we can easily integrate many different types of information, such as morpho-syntactic, semantic and discourse-related information. Researchers have developed methods of zero anaphora resolution for Chinese (Zhao and Ng, 2007; Chen and Ng, 2013), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Sasano et al., 2008; Sasano et al., 2009; Imamura 2180 et al., 2009; Watanabe et al., 2010; Hayashibe et al., 2011; Iida and Poesio, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Yoshino et al., 2013) and Italian (Iida and Poesio, 2011). One critical issue in zero anaphora resolution is optimizing the outputs of sub-problems (e.g., zero anaphor detection and antecedent identification). Recent works by Watanabe et al. (2010), Iida and Poesio (2011) and Yoshikawa et al. (2011) revealed that joint inference improves the overa"
D15-1260,D08-1055,0,0.447225,"1996), which are mainly motivated by the rules and preferences introduced in Centering Theory (Grosz et al., 1995). However, the research trend of zero anaphora resolution has shifted from such rule-based approaches to machine learningbased approaches because in machine learning we can easily integrate many different types of information, such as morpho-syntactic, semantic and discourse-related information. Researchers have developed methods of zero anaphora resolution for Chinese (Zhao and Ng, 2007; Chen and Ng, 2013), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Sasano et al., 2008; Sasano et al., 2009; Imamura 2180 et al., 2009; Watanabe et al., 2010; Hayashibe et al., 2011; Iida and Poesio, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Yoshino et al., 2013) and Italian (Iida and Poesio, 2011). One critical issue in zero anaphora resolution is optimizing the outputs of sub-problems (e.g., zero anaphor detection and antecedent identification). Recent works by Watanabe et al. (2010), Iida and Poesio (2011) and Yoshikawa et al. (2011) revealed that joint inference improves the overall performance of zero anaphora resolution. We employed one of th"
D15-1260,J94-2003,0,0.105941,"ion for the three types in Section 4. We evaluate how effectively our method recognizes subject sharing relations for these types in Section 5. After that, we investigate the impact of explicitly introducing SSPNs in Section 6 and compare our zero anaphora resolution method with a state-of-the-art ILP-based method on the task of intra-sentential subject zero anaphora resolution in Section 7. Finally, in Section 8 we summarize this work and discuss future directions. 2 Related work Traditional approaches to zero anaphora resolution are based on manually created heuristic rules (Kameyama, 1986; Walker et al., 1994; Okumura and Tamura, 1996; Nakaiwa and Shirai, 1996), which are mainly motivated by the rules and preferences introduced in Centering Theory (Grosz et al., 1995). However, the research trend of zero anaphora resolution has shifted from such rule-based approaches to machine learningbased approaches because in machine learning we can easily integrate many different types of information, such as morpho-syntactic, semantic and discourse-related information. Researchers have developed methods of zero anaphora resolution for Chinese (Zhao and Ng, 2007; Chen and Ng, 2013), Japanese (Seki et al., 200"
D15-1260,P10-2018,0,0.0738626,"eory (Grosz et al., 1995). However, the research trend of zero anaphora resolution has shifted from such rule-based approaches to machine learningbased approaches because in machine learning we can easily integrate many different types of information, such as morpho-syntactic, semantic and discourse-related information. Researchers have developed methods of zero anaphora resolution for Chinese (Zhao and Ng, 2007; Chen and Ng, 2013), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Sasano et al., 2008; Sasano et al., 2009; Imamura 2180 et al., 2009; Watanabe et al., 2010; Hayashibe et al., 2011; Iida and Poesio, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Yoshino et al., 2013) and Italian (Iida and Poesio, 2011). One critical issue in zero anaphora resolution is optimizing the outputs of sub-problems (e.g., zero anaphor detection and antecedent identification). Recent works by Watanabe et al. (2010), Iida and Poesio (2011) and Yoshikawa et al. (2011) revealed that joint inference improves the overall performance of zero anaphora resolution. We employed one of these works as a baseline in Section 6. Concerning subject sharing recognition, related method"
D15-1260,P05-1021,0,0.0131313,"1; Yoshikawa et al., 2011; Hangyo et al., 2013; Yoshino et al., 2013) and Italian (Iida and Poesio, 2011). One critical issue in zero anaphora resolution is optimizing the outputs of sub-problems (e.g., zero anaphor detection and antecedent identification). Recent works by Watanabe et al. (2010), Iida and Poesio (2011) and Yoshikawa et al. (2011) revealed that joint inference improves the overall performance of zero anaphora resolution. We employed one of these works as a baseline in Section 6. Concerning subject sharing recognition, related methods have been explored for pronominal anaphora (Yang et al., 2005) or coreference resolution (Bean and Riloff, 2004; Bansal and Klein, 2012). In these methods, the semantic compatibility between the contexts surrounding an anaphor and its antecedent (e.g., the compatibility of verbs kidnap and release given some arguments) was automatically extracted from raw texts in an unsupervised manner and used as features in a machine learning-based approach. However, because the automatically acquired semantic compatibility is not always true or applicable in the context of any pair of an anaphor and its antecedent, the effectiveness of the compatibility features migh"
D15-1260,I11-1126,0,0.37852,"a resolution has shifted from such rule-based approaches to machine learningbased approaches because in machine learning we can easily integrate many different types of information, such as morpho-syntactic, semantic and discourse-related information. Researchers have developed methods of zero anaphora resolution for Chinese (Zhao and Ng, 2007; Chen and Ng, 2013), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Sasano et al., 2008; Sasano et al., 2009; Imamura 2180 et al., 2009; Watanabe et al., 2010; Hayashibe et al., 2011; Iida and Poesio, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Yoshino et al., 2013) and Italian (Iida and Poesio, 2011). One critical issue in zero anaphora resolution is optimizing the outputs of sub-problems (e.g., zero anaphor detection and antecedent identification). Recent works by Watanabe et al. (2010), Iida and Poesio (2011) and Yoshikawa et al. (2011) revealed that joint inference improves the overall performance of zero anaphora resolution. We employed one of these works as a baseline in Section 6. Concerning subject sharing recognition, related methods have been explored for pronominal anaphora (Yang et al., 2005) or cor"
D15-1260,D09-1160,0,0.196514,"Missing"
D15-1260,I13-1126,0,0.395716,"approaches to machine learningbased approaches because in machine learning we can easily integrate many different types of information, such as morpho-syntactic, semantic and discourse-related information. Researchers have developed methods of zero anaphora resolution for Chinese (Zhao and Ng, 2007; Chen and Ng, 2013), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Sasano et al., 2008; Sasano et al., 2009; Imamura 2180 et al., 2009; Watanabe et al., 2010; Hayashibe et al., 2011; Iida and Poesio, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Yoshino et al., 2013) and Italian (Iida and Poesio, 2011). One critical issue in zero anaphora resolution is optimizing the outputs of sub-problems (e.g., zero anaphor detection and antecedent identification). Recent works by Watanabe et al. (2010), Iida and Poesio (2011) and Yoshikawa et al. (2011) revealed that joint inference improves the overall performance of zero anaphora resolution. We employed one of these works as a baseline in Section 6. Concerning subject sharing recognition, related methods have been explored for pronominal anaphora (Yang et al., 2005) or coreference resolution (Bean and Riloff, 2004;"
D15-1260,D07-1057,0,0.640123,"ally created heuristic rules (Kameyama, 1986; Walker et al., 1994; Okumura and Tamura, 1996; Nakaiwa and Shirai, 1996), which are mainly motivated by the rules and preferences introduced in Centering Theory (Grosz et al., 1995). However, the research trend of zero anaphora resolution has shifted from such rule-based approaches to machine learningbased approaches because in machine learning we can easily integrate many different types of information, such as morpho-syntactic, semantic and discourse-related information. Researchers have developed methods of zero anaphora resolution for Chinese (Zhao and Ng, 2007; Chen and Ng, 2013), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Sasano et al., 2008; Sasano et al., 2009; Imamura 2180 et al., 2009; Watanabe et al., 2010; Hayashibe et al., 2011; Iida and Poesio, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Yoshino et al., 2013) and Italian (Iida and Poesio, 2011). One critical issue in zero anaphora resolution is optimizing the outputs of sub-problems (e.g., zero anaphor detection and antecedent identification). Recent works by Watanabe et al. (2010), Iida and Poesio (2011) and Yoshikawa et al. (2011)"
D15-1260,N04-1038,0,\N,Missing
D15-1260,P12-1041,0,\N,Missing
D16-1132,P15-1026,0,0.0321947,"cture that can be applied to various NLP tasks, such as PoS tagging, chunking, named entity recognition 1246 and semantic role labeling. Following this work, CNNs have been utilized in such NLP tasks as document classification (Kalchbrenner et al., 2014; Kim, 2014; Johnson and Zhang, 2015), paraphrase (Hu et al., 2014; Yin and Sch¨utze, 2015) and relation extraction (Liu et al., 2013; Zeng et al., 2014; dos Santos et al., 2015; Nguyen and Grishman, 2015). MCNNs were first introduced for image classification (Cires¸an et al., 2012). In NLP tasks, they have been utilized for question-answering (Dong et al., 2015) and relation extraction (Zeng et al., 2015). Our MCNN architecture was inspired by a Siamese architecture (Chopra et al., 2005), which we extend to a multi-column network and replace its similarity measure with a softmax function at its top. 3 Proposed method Our proposed method consists of the following four steps: Step 1 Extract every pair of a predicate and a candidate antecedent, ⟨predi , candi ⟩, that appears in a target sentence. Step 2 Predict the probability of each pair using our MCNN. Step 3 Rank in descending order all the pairs by their probabilities obtained in Step 2. Step 4 Cho"
D16-1132,P15-1061,0,0.0128412,". (2015)’s method as a baseline in Section 4 because it achieved the state-of-the-art performance for intra-sentential zero anaphora resolution. Collobert et al. (2011) proposed CNN architecture that can be applied to various NLP tasks, such as PoS tagging, chunking, named entity recognition 1246 and semantic role labeling. Following this work, CNNs have been utilized in such NLP tasks as document classification (Kalchbrenner et al., 2014; Kim, 2014; Johnson and Zhang, 2015), paraphrase (Hu et al., 2014; Yin and Sch¨utze, 2015) and relation extraction (Liu et al., 2013; Zeng et al., 2014; dos Santos et al., 2015; Nguyen and Grishman, 2015). MCNNs were first introduced for image classification (Cires¸an et al., 2012). In NLP tasks, they have been utilized for question-answering (Dong et al., 2015) and relation extraction (Zeng et al., 2015). Our MCNN architecture was inspired by a Siamese architecture (Chopra et al., 2005), which we extend to a multi-column network and replace its similarity measure with a softmax function at its top. 3 Proposed method Our proposed method consists of the following four steps: Step 1 Extract every pair of a predicate and a candidate antecedent, ⟨predi , candi ⟩, that a"
D16-1132,J95-2003,0,0.781794,"eve such high precision is crucial to realworld applications, even though the recall remains low, and thus our method is preferable to Ouchi et al.’s method in that sense. In our proposed method, we use a Multi-column Convolutional Neural Network (MCNN) (Cires¸an et al., 2012), which is a variant of a Convolutional Neural Network (CNN) (LeCun et al., 1998). An MCNN has several independent columns, each of which has its own convolutional and pooling layers. The outputs of all the columns are combined in the final layer to provide a final prediction. In this work, motivated by Centering Theory (Grosz et al., 1995) and other previous works, we exploit as distinct columns the word sequences obtained from the surface word 1245 sequence and the dependency tree of a target sentence in our MCNN. Although the existing works also exploited such word sequences, they used only particular types of information from them as features based on the researchers’ linguistic insights. In contrast, we minimized such feature engineering due to using an MCNN. The rest of this paper is organized as follows. In Section 2, we briefly overview previous work on zero anaphora resolution. In Section 3, we present the procedure of"
D16-1132,I11-1023,0,0.150923,"tion method and explain the column sets used in our MCNN architecture. We evaluate how effectively our method recognizes intra-sentential subject zero anaphora in Section 4 and summarize this work and discuss future directions in Section 5. 2 Related work The typical zero anaphora resolution algorithms proposed so far have exploited the information of a predicate that potentially has a zero anaphor and its candidate antecedent in a supervised manner (Seki et al., 2002; Iida et al., 2003; Isozaki and Hirao, 2003; Iida et al., 2006; Taira et al., 2008; Sasano et al., 2008; Imamura et al., 2009; Hayashibe et al., 2011; Iida and Poesio, 2011; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011). In addition, existing works have exploited the dependency path between a predicate and a candidate antecedent either by encoding such paths to the set of binary features of the words that appear in the path (Iida and Poesio, 2011) or by mining from the paths the sub-trees that effectively discriminate zero anaphoric relations (Iida et al., 2006). However, both methods just focus on the dependency paths between a predicate and a candidate antecedent without exploiting other structural fragments in the dependency tree"
D16-1132,P11-1081,1,0.949442,"tial) zero anaphoric relations in the same sentence. The final determination of zero anaphoric relations for each zero anaphor in a given sentence is done in a greedy way; only the most likely candidate antecedent for each zero anaphor is selected as its antecedent as far as the likelihood score exceeds a 1244 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1244–1254, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics given threshold. This approach contrasts with global optimization methods (Yoshikawa et al., 2011; Iida and Poesio, 2011; Ouchi et al., 2015), which have recently become popular. These methods use the constraints among possible zero anaphoric relations, such as “if a candidate antecedent is identified as the antecedent of a subject zero anaphor of a predicate, the candidate cannot be referred to by the object zero anaphor of the same predicate”, and determine an optimal set of zero anaphoric relations in an entire sentence while satisfying such constraints, using such optimization techniques as sentence-wise global learning (Ouchi et al., 2015) and integer linear programming (Iida and Poesio, 2011). Although th"
D16-1132,W03-2604,1,0.730012,"briefly overview previous work on zero anaphora resolution. In Section 3, we present the procedure of our zero anaphora resolution method and explain the column sets used in our MCNN architecture. We evaluate how effectively our method recognizes intra-sentential subject zero anaphora in Section 4 and summarize this work and discuss future directions in Section 5. 2 Related work The typical zero anaphora resolution algorithms proposed so far have exploited the information of a predicate that potentially has a zero anaphor and its candidate antecedent in a supervised manner (Seki et al., 2002; Iida et al., 2003; Isozaki and Hirao, 2003; Iida et al., 2006; Taira et al., 2008; Sasano et al., 2008; Imamura et al., 2009; Hayashibe et al., 2011; Iida and Poesio, 2011; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011). In addition, existing works have exploited the dependency path between a predicate and a candidate antecedent either by encoding such paths to the set of binary features of the words that appear in the path (Iida and Poesio, 2011) or by mining from the paths the sub-trees that effectively discriminate zero anaphoric relations (Iida et al., 2006). However, both methods just focus on the de"
D16-1132,P06-1079,1,0.915957,"ora resolution. In Section 3, we present the procedure of our zero anaphora resolution method and explain the column sets used in our MCNN architecture. We evaluate how effectively our method recognizes intra-sentential subject zero anaphora in Section 4 and summarize this work and discuss future directions in Section 5. 2 Related work The typical zero anaphora resolution algorithms proposed so far have exploited the information of a predicate that potentially has a zero anaphor and its candidate antecedent in a supervised manner (Seki et al., 2002; Iida et al., 2003; Isozaki and Hirao, 2003; Iida et al., 2006; Taira et al., 2008; Sasano et al., 2008; Imamura et al., 2009; Hayashibe et al., 2011; Iida and Poesio, 2011; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011). In addition, existing works have exploited the dependency path between a predicate and a candidate antecedent either by encoding such paths to the set of binary features of the words that appear in the path (Iida and Poesio, 2011) or by mining from the paths the sub-trees that effectively discriminate zero anaphoric relations (Iida et al., 2006). However, both methods just focus on the dependency paths between a predicate and a can"
D16-1132,W07-1522,1,0.889983,"tated in the NAIST Text Corpus. In this revision phase, both the subject sharing and zero anaphora relations for such suspicious instances were independently re-annotated by three annotators, and their final labels of both relations were determined by a majority of the their decisions.3 As a result, 2,120 zero anaphoric instances were newly added to the corpus and 1,184 instances were removed from it for a total of 19,049 instances of intra-sentential subject zero anaphoric relations.4 3 In our preliminary investigation of the intrasentential zero anaphoric relations in the NAIST Text Corpus (Iida et al., 2007), since we found more annotation errors than we expected, we decided to 2 We use zero padding for dealing with text fragments of variable length (Kim, 2014). 1249 We are planning to release the annotated results and information on the data separation used in our evaluation from https://alaginrc.nict.go.jp/. 4 After this revision, a small number of inconsistent annotated results have both a syntactically dependent subject and a subject zero anaphor because the revision was performed locally. There were 30 inconsistent instances in the testing set and 100 in the training and development sets. We"
D16-1132,D15-1260,1,0.911685,"he rules and principles regarding the recency and saliency of candidate antecedents. Okumura and Tamura (1996) developed a rule-based method based on the idea of Centering Theory. Iida et al. (2003) and Imamura et al. (2009) used as features for machine learning the results of rule-based antecedent identification based on a variant of Centering Theory (Nariyama, 2002). However, we observed that actual anaphoric phenomena often do not obey Centering Theory. To robustly resolve zero anaphora, we need to explore additional clues that are represented in a target sentence (or text). Recent work by Iida et al. (2015) newly introduced a sub-problem of zero anaphora resolution, subject sharing recognition, which is the task that judges whether two predicates have the same subject. In their method, a network of subject sharing predicates is created by their subject sharing recognizer, and then zero anaphora resolution is performed by propagating a subject to the unrealized subject positions through the path in the network. Even though the accuracy of subject sharing recognition exceeds that of zero anaphora resolution, the zero anaphoric relations identified using the results of subject sharing recognition a"
D16-1132,P09-2022,0,0.464996,"r zero anaphora resolution method and explain the column sets used in our MCNN architecture. We evaluate how effectively our method recognizes intra-sentential subject zero anaphora in Section 4 and summarize this work and discuss future directions in Section 5. 2 Related work The typical zero anaphora resolution algorithms proposed so far have exploited the information of a predicate that potentially has a zero anaphor and its candidate antecedent in a supervised manner (Seki et al., 2002; Iida et al., 2003; Isozaki and Hirao, 2003; Iida et al., 2006; Taira et al., 2008; Sasano et al., 2008; Imamura et al., 2009; Hayashibe et al., 2011; Iida and Poesio, 2011; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011). In addition, existing works have exploited the dependency path between a predicate and a candidate antecedent either by encoding such paths to the set of binary features of the words that appear in the path (Iida and Poesio, 2011) or by mining from the paths the sub-trees that effectively discriminate zero anaphoric relations (Iida et al., 2006). However, both methods just focus on the dependency paths between a predicate and a candidate antecedent without exploiting other structural fragments"
D16-1132,W03-1024,0,0.698287,"evious work on zero anaphora resolution. In Section 3, we present the procedure of our zero anaphora resolution method and explain the column sets used in our MCNN architecture. We evaluate how effectively our method recognizes intra-sentential subject zero anaphora in Section 4 and summarize this work and discuss future directions in Section 5. 2 Related work The typical zero anaphora resolution algorithms proposed so far have exploited the information of a predicate that potentially has a zero anaphor and its candidate antecedent in a supervised manner (Seki et al., 2002; Iida et al., 2003; Isozaki and Hirao, 2003; Iida et al., 2006; Taira et al., 2008; Sasano et al., 2008; Imamura et al., 2009; Hayashibe et al., 2011; Iida and Poesio, 2011; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011). In addition, existing works have exploited the dependency path between a predicate and a candidate antecedent either by encoding such paths to the set of binary features of the words that appear in the path (Iida and Poesio, 2011) or by mining from the paths the sub-trees that effectively discriminate zero anaphoric relations (Iida et al., 2006). However, both methods just focus on the dependency paths between a"
D16-1132,N15-1011,0,0.0101775,"tential zero anaphoric relations) as features to directly decide more than one predicate-argument relation simultaneously. We adopted Ouchi et al. (2015)’s method as a baseline in Section 4 because it achieved the state-of-the-art performance for intra-sentential zero anaphora resolution. Collobert et al. (2011) proposed CNN architecture that can be applied to various NLP tasks, such as PoS tagging, chunking, named entity recognition 1246 and semantic role labeling. Following this work, CNNs have been utilized in such NLP tasks as document classification (Kalchbrenner et al., 2014; Kim, 2014; Johnson and Zhang, 2015), paraphrase (Hu et al., 2014; Yin and Sch¨utze, 2015) and relation extraction (Liu et al., 2013; Zeng et al., 2014; dos Santos et al., 2015; Nguyen and Grishman, 2015). MCNNs were first introduced for image classification (Cires¸an et al., 2012). In NLP tasks, they have been utilized for question-answering (Dong et al., 2015) and relation extraction (Zeng et al., 2015). Our MCNN architecture was inspired by a Siamese architecture (Chopra et al., 2005), which we extend to a multi-column network and replace its similarity measure with a softmax function at its top. 3 Proposed method Our propose"
D16-1132,P14-1062,0,0.0138566,"tions (e.g., the combination of two potential zero anaphoric relations) as features to directly decide more than one predicate-argument relation simultaneously. We adopted Ouchi et al. (2015)’s method as a baseline in Section 4 because it achieved the state-of-the-art performance for intra-sentential zero anaphora resolution. Collobert et al. (2011) proposed CNN architecture that can be applied to various NLP tasks, such as PoS tagging, chunking, named entity recognition 1246 and semantic role labeling. Following this work, CNNs have been utilized in such NLP tasks as document classification (Kalchbrenner et al., 2014; Kim, 2014; Johnson and Zhang, 2015), paraphrase (Hu et al., 2014; Yin and Sch¨utze, 2015) and relation extraction (Liu et al., 2013; Zeng et al., 2014; dos Santos et al., 2015; Nguyen and Grishman, 2015). MCNNs were first introduced for image classification (Cires¸an et al., 2012). In NLP tasks, they have been utilized for question-answering (Dong et al., 2015) and relation extraction (Zeng et al., 2015). Our MCNN architecture was inspired by a Siamese architecture (Chopra et al., 2005), which we extend to a multi-column network and replace its similarity measure with a softmax function at i"
D16-1132,D14-1181,0,0.0411336,"n of two potential zero anaphoric relations) as features to directly decide more than one predicate-argument relation simultaneously. We adopted Ouchi et al. (2015)’s method as a baseline in Section 4 because it achieved the state-of-the-art performance for intra-sentential zero anaphora resolution. Collobert et al. (2011) proposed CNN architecture that can be applied to various NLP tasks, such as PoS tagging, chunking, named entity recognition 1246 and semantic role labeling. Following this work, CNNs have been utilized in such NLP tasks as document classification (Kalchbrenner et al., 2014; Kim, 2014; Johnson and Zhang, 2015), paraphrase (Hu et al., 2014; Yin and Sch¨utze, 2015) and relation extraction (Liu et al., 2013; Zeng et al., 2014; dos Santos et al., 2015; Nguyen and Grishman, 2015). MCNNs were first introduced for image classification (Cires¸an et al., 2012). In NLP tasks, they have been utilized for question-answering (Dong et al., 2015) and relation extraction (Zeng et al., 2015). Our MCNN architecture was inspired by a Siamese architecture (Chopra et al., 2005), which we extend to a multi-column network and replace its similarity measure with a softmax function at its top. 3 P"
D16-1132,W04-3230,0,0.0574198,"of 0.5 to the final layer. We used an SGD with mini-batches of 100 and a learning rate decay of 0.95. We ran ten epochs through all of the training data, where each epoch consisted of many mini-batch updates. We utilized 3-, 4- and 5-grams with 100 filters each and used the F-score of positive instances as our evaluation metric. The total number of the nodes in the final layers of our MCNN was 3,300: 11 columns × 3 N -gram × 100 filters. Word segmentation, PoS tagging and dependency parsing of the sentences in the NAIST Text Corpus were performed by a Japanese morphological analyzer, MeCab8 (Kudo et al., 2004), and a depentwo sets. 5 Words occurring less than five times in all the sentences were ignored to train the word embedding vectors. 6 We set the skip distance to 5 and the number of negative samples to 10. 7 https://archive.org/details/jawiki-20150118 8 http://taku910.github.io/mecab/ 1250 We compared our method with three baseline methods. The first baseline is a single-column convolutional neural network in which the column includes the entire surface word sequence of a sentence. To give the positions of predi and candi to the network, we concatenated to each word vector an additional 2-dim"
D16-1132,2002.tmi-papers.15,0,0.0682717,"method uses the text fragments that cover the entire dependency tree. Another important clue was derived from discourse theories, such as Centering Theory (Grosz et al., 1995). In this theory, (zero) anaphoric phenomenon is explained based on the rules and principles regarding the recency and saliency of candidate antecedents. Okumura and Tamura (1996) developed a rule-based method based on the idea of Centering Theory. Iida et al. (2003) and Imamura et al. (2009) used as features for machine learning the results of rule-based antecedent identification based on a variant of Centering Theory (Nariyama, 2002). However, we observed that actual anaphoric phenomena often do not obey Centering Theory. To robustly resolve zero anaphora, we need to explore additional clues that are represented in a target sentence (or text). Recent work by Iida et al. (2015) newly introduced a sub-problem of zero anaphora resolution, subject sharing recognition, which is the task that judges whether two predicates have the same subject. In their method, a network of subject sharing predicates is created by their subject sharing recognizer, and then zero anaphora resolution is performed by propagating a subject to the un"
D16-1132,P04-1020,0,0.0182292,"ished as a subtask of coreference recency can be estimated by consulting the surface resolution. This problem was basically solved by word sequence between France and increase: no exploring the possible candidate antecedents for a other salient candidates are included in the word se- given anaphor candidate in its search space, and the quence. Also, the other two types of word sequences results were used for improving the overall perfor(i.e., the sequence that spans from the beginning of mance of coreference resolution, especially in Enthe sentence to candi and that spans from predi to glish (Ng, 2004; Wiseman et al., 2015). Inspired its end) are important for confirming whether a more by such previous works, we designed the P RED salient candidate than candi appears in each word C ONTEXT set to determine the anaphoricity of zero sequence. If such a more salient candidate is found, anaphors, i.e., to judge whether a zero anaphor canit should be a stronger candidate of the antecedent. didate has its antecedent in a sentence, by consulting The D EP T REE column set is introduced for cap- the surface word sequences before and after predi . turing a different aspect of intra-sentential zero an"
D16-1132,W15-1506,0,0.0142481,"a baseline in Section 4 because it achieved the state-of-the-art performance for intra-sentential zero anaphora resolution. Collobert et al. (2011) proposed CNN architecture that can be applied to various NLP tasks, such as PoS tagging, chunking, named entity recognition 1246 and semantic role labeling. Following this work, CNNs have been utilized in such NLP tasks as document classification (Kalchbrenner et al., 2014; Kim, 2014; Johnson and Zhang, 2015), paraphrase (Hu et al., 2014; Yin and Sch¨utze, 2015) and relation extraction (Liu et al., 2013; Zeng et al., 2014; dos Santos et al., 2015; Nguyen and Grishman, 2015). MCNNs were first introduced for image classification (Cires¸an et al., 2012). In NLP tasks, they have been utilized for question-answering (Dong et al., 2015) and relation extraction (Zeng et al., 2015). Our MCNN architecture was inspired by a Siamese architecture (Chopra et al., 2005), which we extend to a multi-column network and replace its similarity measure with a softmax function at its top. 3 Proposed method Our proposed method consists of the following four steps: Step 1 Extract every pair of a predicate and a candidate antecedent, ⟨predi , candi ⟩, that appears in a target sentence."
D16-1132,C96-2147,0,0.531054,"iminate zero anaphoric relations (Iida et al., 2006). However, both methods just focus on the dependency paths between a predicate and a candidate antecedent without exploiting other structural fragments in the dependency tree representing a target sentence, whereas our method uses the text fragments that cover the entire dependency tree. Another important clue was derived from discourse theories, such as Centering Theory (Grosz et al., 1995). In this theory, (zero) anaphoric phenomenon is explained based on the rules and principles regarding the recency and saliency of candidate antecedents. Okumura and Tamura (1996) developed a rule-based method based on the idea of Centering Theory. Iida et al. (2003) and Imamura et al. (2009) used as features for machine learning the results of rule-based antecedent identification based on a variant of Centering Theory (Nariyama, 2002). However, we observed that actual anaphoric phenomena often do not obey Centering Theory. To robustly resolve zero anaphora, we need to explore additional clues that are represented in a target sentence (or text). Recent work by Iida et al. (2015) newly introduced a sub-problem of zero anaphora resolution, subject sharing recognition, wh"
D16-1132,P15-1093,0,0.461032,"lations in the same sentence. The final determination of zero anaphoric relations for each zero anaphor in a given sentence is done in a greedy way; only the most likely candidate antecedent for each zero anaphor is selected as its antecedent as far as the likelihood score exceeds a 1244 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1244–1254, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics given threshold. This approach contrasts with global optimization methods (Yoshikawa et al., 2011; Iida and Poesio, 2011; Ouchi et al., 2015), which have recently become popular. These methods use the constraints among possible zero anaphoric relations, such as “if a candidate antecedent is identified as the antecedent of a subject zero anaphor of a predicate, the candidate cannot be referred to by the object zero anaphor of the same predicate”, and determine an optimal set of zero anaphoric relations in an entire sentence while satisfying such constraints, using such optimization techniques as sentence-wise global learning (Ouchi et al., 2015) and integer linear programming (Iida and Poesio, 2011). Although the global optimization"
D16-1132,I11-1085,0,0.700523,"our MCNN architecture. We evaluate how effectively our method recognizes intra-sentential subject zero anaphora in Section 4 and summarize this work and discuss future directions in Section 5. 2 Related work The typical zero anaphora resolution algorithms proposed so far have exploited the information of a predicate that potentially has a zero anaphor and its candidate antecedent in a supervised manner (Seki et al., 2002; Iida et al., 2003; Isozaki and Hirao, 2003; Iida et al., 2006; Taira et al., 2008; Sasano et al., 2008; Imamura et al., 2009; Hayashibe et al., 2011; Iida and Poesio, 2011; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011). In addition, existing works have exploited the dependency path between a predicate and a candidate antecedent either by encoding such paths to the set of binary features of the words that appear in the path (Iida and Poesio, 2011) or by mining from the paths the sub-trees that effectively discriminate zero anaphoric relations (Iida et al., 2006). However, both methods just focus on the dependency paths between a predicate and a candidate antecedent without exploiting other structural fragments in the dependency tree representing a target sentence, whereas our method"
D16-1132,C08-1097,0,0.267866,"t the procedure of our zero anaphora resolution method and explain the column sets used in our MCNN architecture. We evaluate how effectively our method recognizes intra-sentential subject zero anaphora in Section 4 and summarize this work and discuss future directions in Section 5. 2 Related work The typical zero anaphora resolution algorithms proposed so far have exploited the information of a predicate that potentially has a zero anaphor and its candidate antecedent in a supervised manner (Seki et al., 2002; Iida et al., 2003; Isozaki and Hirao, 2003; Iida et al., 2006; Taira et al., 2008; Sasano et al., 2008; Imamura et al., 2009; Hayashibe et al., 2011; Iida and Poesio, 2011; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011). In addition, existing works have exploited the dependency path between a predicate and a candidate antecedent either by encoding such paths to the set of binary features of the words that appear in the path (Iida and Poesio, 2011) or by mining from the paths the sub-trees that effectively discriminate zero anaphoric relations (Iida et al., 2006). However, both methods just focus on the dependency paths between a predicate and a candidate antecedent without exploiting othe"
D16-1132,C02-1078,0,0.332527,". In Section 2, we briefly overview previous work on zero anaphora resolution. In Section 3, we present the procedure of our zero anaphora resolution method and explain the column sets used in our MCNN architecture. We evaluate how effectively our method recognizes intra-sentential subject zero anaphora in Section 4 and summarize this work and discuss future directions in Section 5. 2 Related work The typical zero anaphora resolution algorithms proposed so far have exploited the information of a predicate that potentially has a zero anaphor and its candidate antecedent in a supervised manner (Seki et al., 2002; Iida et al., 2003; Isozaki and Hirao, 2003; Iida et al., 2006; Taira et al., 2008; Sasano et al., 2008; Imamura et al., 2009; Hayashibe et al., 2011; Iida and Poesio, 2011; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011). In addition, existing works have exploited the dependency path between a predicate and a candidate antecedent either by encoding such paths to the set of binary features of the words that appear in the path (Iida and Poesio, 2011) or by mining from the paths the sub-trees that effectively discriminate zero anaphoric relations (Iida et al., 2006). However, both methods j"
D16-1132,D08-1055,0,0.761888,"Missing"
D16-1132,P15-1137,0,0.0245863,"subtask of coreference recency can be estimated by consulting the surface resolution. This problem was basically solved by word sequence between France and increase: no exploring the possible candidate antecedents for a other salient candidates are included in the word se- given anaphor candidate in its search space, and the quence. Also, the other two types of word sequences results were used for improving the overall perfor(i.e., the sequence that spans from the beginning of mance of coreference resolution, especially in Enthe sentence to candi and that spans from predi to glish (Ng, 2004; Wiseman et al., 2015). Inspired its end) are important for confirming whether a more by such previous works, we designed the P RED salient candidate than candi appears in each word C ONTEXT set to determine the anaphoricity of zero sequence. If such a more salient candidate is found, anaphors, i.e., to judge whether a zero anaphor canit should be a stronger candidate of the antecedent. didate has its antecedent in a sentence, by consulting The D EP T REE column set is introduced for cap- the surface word sequences before and after predi . turing a different aspect of intra-sentential zero anaphora. In the explanat"
D16-1132,N15-1091,0,0.0611928,"Missing"
D16-1132,I11-1126,0,0.522837,"idering the other (potential) zero anaphoric relations in the same sentence. The final determination of zero anaphoric relations for each zero anaphor in a given sentence is done in a greedy way; only the most likely candidate antecedent for each zero anaphor is selected as its antecedent as far as the likelihood score exceeds a 1244 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1244–1254, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics given threshold. This approach contrasts with global optimization methods (Yoshikawa et al., 2011; Iida and Poesio, 2011; Ouchi et al., 2015), which have recently become popular. These methods use the constraints among possible zero anaphoric relations, such as “if a candidate antecedent is identified as the antecedent of a subject zero anaphor of a predicate, the candidate cannot be referred to by the object zero anaphor of the same predicate”, and determine an optimal set of zero anaphoric relations in an entire sentence while satisfying such constraints, using such optimization techniques as sentence-wise global learning (Ouchi et al., 2015) and integer linear programming (Iida and Poe"
D16-1132,D09-1160,0,0.094722,"tion on the data separation used in our evaluation from https://alaginrc.nict.go.jp/. 4 After this revision, a small number of inconsistent annotated results have both a syntactically dependent subject and a subject zero anaphor because the revision was performed locally. There were 30 inconsistent instances in the testing set and 100 in the training and development sets. We only removed such instances from the testing set without changing the other Type #docs #sentences train dev test 1,757 586 586 23,152 7,526 7,705 #zero anaphors (intra-sentential) 11,453 3,691 3,875 dency parser, J.DepP9 (Yoshinaga and Kitsuregawa, 2009). 4.3 Baselines Table 1: Statistics of our data set 4.2 Experimental settings The documents in the corpus were divided into five subsets, three of which were used as a training data set, one as a development data set, and one as a testing data set. The statistics of our data set are summarized in Table 1. We evaluated the performance of our intra-sentential subject zero anaphora resolution method and three baseline methods described below using the revised annotated results in our data set. We implemented our MCNN using Theano (Bastien et al., 2012). We pre-trained 300dimensional word embeddin"
D16-1132,C14-1220,0,0.0304154,"We adopted Ouchi et al. (2015)’s method as a baseline in Section 4 because it achieved the state-of-the-art performance for intra-sentential zero anaphora resolution. Collobert et al. (2011) proposed CNN architecture that can be applied to various NLP tasks, such as PoS tagging, chunking, named entity recognition 1246 and semantic role labeling. Following this work, CNNs have been utilized in such NLP tasks as document classification (Kalchbrenner et al., 2014; Kim, 2014; Johnson and Zhang, 2015), paraphrase (Hu et al., 2014; Yin and Sch¨utze, 2015) and relation extraction (Liu et al., 2013; Zeng et al., 2014; dos Santos et al., 2015; Nguyen and Grishman, 2015). MCNNs were first introduced for image classification (Cires¸an et al., 2012). In NLP tasks, they have been utilized for question-answering (Dong et al., 2015) and relation extraction (Zeng et al., 2015). Our MCNN architecture was inspired by a Siamese architecture (Chopra et al., 2005), which we extend to a multi-column network and replace its similarity measure with a softmax function at its top. 3 Proposed method Our proposed method consists of the following four steps: Step 1 Extract every pair of a predicate and a candidate antecedent,"
D16-1132,D15-1203,0,0.0287412,"ks, such as PoS tagging, chunking, named entity recognition 1246 and semantic role labeling. Following this work, CNNs have been utilized in such NLP tasks as document classification (Kalchbrenner et al., 2014; Kim, 2014; Johnson and Zhang, 2015), paraphrase (Hu et al., 2014; Yin and Sch¨utze, 2015) and relation extraction (Liu et al., 2013; Zeng et al., 2014; dos Santos et al., 2015; Nguyen and Grishman, 2015). MCNNs were first introduced for image classification (Cires¸an et al., 2012). In NLP tasks, they have been utilized for question-answering (Dong et al., 2015) and relation extraction (Zeng et al., 2015). Our MCNN architecture was inspired by a Siamese architecture (Chopra et al., 2005), which we extend to a multi-column network and replace its similarity measure with a softmax function at its top. 3 Proposed method Our proposed method consists of the following four steps: Step 1 Extract every pair of a predicate and a candidate antecedent, ⟨predi , candi ⟩, that appears in a target sentence. Step 2 Predict the probability of each pair using our MCNN. Step 3 Rank in descending order all the pairs by their probabilities obtained in Step 2. Step 4 Choose the top pair ⟨predi , candi ⟩ in the ran"
D19-1590,N19-1423,0,0.189337,"rds (Torisawa, 5816 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5816–5822, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2006; Riaz and Girju, 2010; Do et al., 2011), semantic polarities (Hashimoto et al., 2012), answers obtained from a web-based open-domain why-QA system and other causality related texts (Kruengkrai et al., 2017), and causality-related word embeddings (Xie and Mu, 2019). In this work, we investigate whether BERT (Devlin et al., 2019) (especially its pre-training) enables novel ways to exploit background knowledge. Our assumption is that if a BERT model is pre-trained using a large amount of causalityrich texts, it can learn some sort of background knowledge from the text. If the pre-training is adequately performed, background knowledge in the form of text fragments and a special mechanism for dealing with them might become obsolete. Our experimental results show that a BERT model pre-trained with causality-rich texts achieved significantly better performance than models using Wikipedia articles or randomly sampled web te"
D19-1590,D11-1027,0,0.272643,"Missing"
D19-1590,C16-1168,0,0.0440712,"Missing"
D19-1590,D12-1057,1,0.874603,"Missing"
D19-1590,P14-1093,1,0.907121,"rmance by simply adding texts related to an input causality candidate as background knowledge to the input of the BERT models. We believe these findings indicate a promising future research direction. 1 Introduction Event causality, such as “smoke cigarettes” → “die of lung cancer,” is critical knowledge for NLP applications such as machine reading (Rajpurkar et al., 2016). For the task of recognizing event causality written in web texts, we propose new BERT-based methods that exploit independent labels in a gold dataset provided by multiple annotators. In the creation of the dataset we used (Hashimoto et al., 2014), three annotators independently labeled the data and the final labels were determined by majority vote. In the previous work, each annotator’s independent judgments were ignored, but in our proposed method, we exploit each annotator’s judgments in predicting the majority vote labels. The dataset we used had a reasonable degree of inter-annotator agreement (Fleiss’ Kappa value was 0.67), but a discrepancy remained among the annotators. Despite this discrepancy, we assume that their judgments are more or less consistent and that we can improve performance by training multiple classifiers, each"
D19-1590,D15-1035,0,0.0128885,"a discrepancy remained among the annotators. Despite this discrepancy, we assume that their judgments are more or less consistent and that we can improve performance by training multiple classifiers, each from the labels provided by an individual annotator to grasp her/his policy, and by combining the resulting outputs of these classifiers. Researchers have studied how to exploit the differences between the behaviors of annotators and crowd workers to improve the quality of gold datasets (Snow et al., 2008; Zaidan and CallisonBurch, 2011; Zhou et al., 2012; Jurgens, 2013; Plank et al., 2014; Jamison and Gurevych, 2015; Felt et al., 2016; Li et al., 2017). In an attempt that resembles ours, one study (Jamison and Gurevych, 2015) successfully improved the performance of several NLP tasks by computing the agreement ratio of each training instance and using only those instances with high agreement. Another work (Plank et al., 2014) improved part-of-speech tagging by measuring the inter-annotator agreement on a small number of sampled data and incorporating this value during training via a modified loss function. However, neither of them directly used each annotator’s judgments, as we did in this work. As anoth"
D19-1590,N13-1062,0,0.0233135,"(Fleiss’ Kappa value was 0.67), but a discrepancy remained among the annotators. Despite this discrepancy, we assume that their judgments are more or less consistent and that we can improve performance by training multiple classifiers, each from the labels provided by an individual annotator to grasp her/his policy, and by combining the resulting outputs of these classifiers. Researchers have studied how to exploit the differences between the behaviors of annotators and crowd workers to improve the quality of gold datasets (Snow et al., 2008; Zaidan and CallisonBurch, 2011; Zhou et al., 2012; Jurgens, 2013; Plank et al., 2014; Jamison and Gurevych, 2015; Felt et al., 2016; Li et al., 2017). In an attempt that resembles ours, one study (Jamison and Gurevych, 2015) successfully improved the performance of several NLP tasks by computing the agreement ratio of each training instance and using only those instances with high agreement. Another work (Plank et al., 2014) improved part-of-speech tagging by measuring the inter-annotator agreement on a small number of sampled data and incorporating this value during training via a modified loss function. However, neither of them directly used each annotat"
D19-1590,P19-1414,1,0.849315,"Missing"
D19-1590,P13-1170,1,0.946064,"Missing"
D19-1590,E14-1078,0,0.0316585,"value was 0.67), but a discrepancy remained among the annotators. Despite this discrepancy, we assume that their judgments are more or less consistent and that we can improve performance by training multiple classifiers, each from the labels provided by an individual annotator to grasp her/his policy, and by combining the resulting outputs of these classifiers. Researchers have studied how to exploit the differences between the behaviors of annotators and crowd workers to improve the quality of gold datasets (Snow et al., 2008; Zaidan and CallisonBurch, 2011; Zhou et al., 2012; Jurgens, 2013; Plank et al., 2014; Jamison and Gurevych, 2015; Felt et al., 2016; Li et al., 2017). In an attempt that resembles ours, one study (Jamison and Gurevych, 2015) successfully improved the performance of several NLP tasks by computing the agreement ratio of each training instance and using only those instances with high agreement. Another work (Plank et al., 2014) improved part-of-speech tagging by measuring the inter-annotator agreement on a small number of sampled data and incorporating this value during training via a modified loss function. However, neither of them directly used each annotator’s judgments, as w"
D19-1590,D16-1264,0,0.0256425,"erformance improved when we pretrained the BERT models with web texts containing a large number of event causalities instead of Wikipedia articles or randomly sampled web texts. However, this effect was limited. Therefore, we further improved performance by simply adding texts related to an input causality candidate as background knowledge to the input of the BERT models. We believe these findings indicate a promising future research direction. 1 Introduction Event causality, such as “smoke cigarettes” → “die of lung cancer,” is critical knowledge for NLP applications such as machine reading (Rajpurkar et al., 2016). For the task of recognizing event causality written in web texts, we propose new BERT-based methods that exploit independent labels in a gold dataset provided by multiple annotators. In the creation of the dataset we used (Hashimoto et al., 2014), three annotators independently labeled the data and the final labels were determined by majority vote. In the previous work, each annotator’s independent judgments were ignored, but in our proposed method, we exploit each annotator’s judgments in predicting the majority vote labels. The dataset we used had a reasonable degree of inter-annotator agr"
D19-1590,D08-1027,0,0.0785427,"Missing"
D19-1590,N06-1008,1,0.756024,"Missing"
D19-1590,P11-1122,0,0.0740377,"Missing"
I05-2030,J95-2003,0,0.00340212,"004), is cost effective, while achieving a better performance than the best-performing rule-based systems for the test sets of MUC-6 and MUC-7 2 . As suggested by Figure 1, anaphora resolution can be decomposed into two subtasks: anaphoricity determination and antecedent identification. Anaphoricity determination is the task of judging whether a given NP is anaphoric or nonanaphoric. Recent research advances have provided several important findings as follows: • Learning-based methods for antecedent identification can also benefit from the use of linguistic clues inspired by Centering Theory (Grosz et al., 1995). • One useful clue for anaphoricity determination is the availability of a plausible candidate for the antecedent. If an appropriate candidate for the antecedent is found in the preceding discourse context, the NP is likely to be anaphoric. For these reasons, an anaphora resolution model performs best if it carries out the following pro2 The 7th Message Understanding Conference (1998): www.itl.nist.gov/iaui/894.02/related projects/muc/ 175 target value Select the best candidate attribute candidates Taro-wa shisetsu-wo（φ-ga）shirabe-te Onaka-ga hetta-node anaphor hungry Tarō-NOM attendance-ACC"
I05-2030,W03-2604,1,0.908937,"indefinite. While the figure shows Japanese examples, the similarity between anaphora resolution and opinion extraction is language independent. This analogy naturally leads us to think of applying existing techniques for anaphora resolution to our opinion extraction task since anaphora resolution has been studied for a considerably longer period in a wider range of disciplines as we briefly review below. 3.2 Existing techniques for anaphora resolution Corpus-based empirical approaches to anaphora resolution have been reasonably successful. This approach, as exemplified by (Soon et al., 2001; Iida et al., 2003; Ng, 2004), is cost effective, while achieving a better performance than the best-performing rule-based systems for the test sets of MUC-6 and MUC-7 2 . As suggested by Figure 1, anaphora resolution can be decomposed into two subtasks: anaphoricity determination and antecedent identification. Anaphoricity determination is the task of judging whether a given NP is anaphoric or nonanaphoric. Recent research advances have provided several important findings as follows: • Learning-based methods for antecedent identification can also benefit from the use of linguistic clues inspired by Centering T"
I05-2030,C04-1071,0,0.0255642,"nd recommendations. Previous approaches to the task of mining a large-scale document collection for opinions can be classified into two groups: the document classification approach and the information extraction approach. In the document classification approach, researchers have been exploring techniques for classifying documents according to semantic/sentiment orientation such as positive vs. negative (e.g. (Dave et al., 2003; Pang and Lee, 2004; Turney, 2002)). The information extraction approach, on the other hand, focuses on the task of extracting elements which constitute opinions (e.g. (Kanayama and Nasukawa, 2004; Hu and Liu, 2004; Tateishi et al., 2001)). The aim of this paper is to extract opinions that represent an evaluation of a products together with the evidence. To achieve this, we consider our task from the information extraction view173 point. We term the above task opinion extraction in this paper. While they can be linguistically realized in many ways, opinions on a product are in fact often expressed in the form of an attribute-value pair. An attribute represents one aspect of a subject and the value is a specific language expression that qualifies or quantifies the aspect. Given this obs"
I05-2030,P04-1020,0,0.0110102,"he figure shows Japanese examples, the similarity between anaphora resolution and opinion extraction is language independent. This analogy naturally leads us to think of applying existing techniques for anaphora resolution to our opinion extraction task since anaphora resolution has been studied for a considerably longer period in a wider range of disciplines as we briefly review below. 3.2 Existing techniques for anaphora resolution Corpus-based empirical approaches to anaphora resolution have been reasonably successful. This approach, as exemplified by (Soon et al., 2001; Iida et al., 2003; Ng, 2004), is cost effective, while achieving a better performance than the best-performing rule-based systems for the test sets of MUC-6 and MUC-7 2 . As suggested by Figure 1, anaphora resolution can be decomposed into two subtasks: anaphoricity determination and antecedent identification. Anaphoricity determination is the task of judging whether a given NP is anaphoric or nonanaphoric. Recent research advances have provided several important findings as follows: • Learning-based methods for antecedent identification can also benefit from the use of linguistic clues inspired by Centering Theory (Gros"
I05-2030,P04-1035,0,0.0333573,"ead of communication on the Web has attracted increasing interest in technologies for automatically mining large numbers of message boards and blog pages for opinions and recommendations. Previous approaches to the task of mining a large-scale document collection for opinions can be classified into two groups: the document classification approach and the information extraction approach. In the document classification approach, researchers have been exploring techniques for classifying documents according to semantic/sentiment orientation such as positive vs. negative (e.g. (Dave et al., 2003; Pang and Lee, 2004; Turney, 2002)). The information extraction approach, on the other hand, focuses on the task of extracting elements which constitute opinions (e.g. (Kanayama and Nasukawa, 2004; Hu and Liu, 2004; Tateishi et al., 2001)). The aim of this paper is to extract opinions that represent an evaluation of a products together with the evidence. To achieve this, we consider our task from the information extraction view173 point. We term the above task opinion extraction in this paper. While they can be linguistically realized in many ways, opinions on a product are in fact often expressed in the form of"
I05-2030,J01-4004,0,0.372641,"licit referent) or indefinite. While the figure shows Japanese examples, the similarity between anaphora resolution and opinion extraction is language independent. This analogy naturally leads us to think of applying existing techniques for anaphora resolution to our opinion extraction task since anaphora resolution has been studied for a considerably longer period in a wider range of disciplines as we briefly review below. 3.2 Existing techniques for anaphora resolution Corpus-based empirical approaches to anaphora resolution have been reasonably successful. This approach, as exemplified by (Soon et al., 2001; Iida et al., 2003; Ng, 2004), is cost effective, while achieving a better performance than the best-performing rule-based systems for the test sets of MUC-6 and MUC-7 2 . As suggested by Figure 1, anaphora resolution can be decomposed into two subtasks: anaphoricity determination and antecedent identification. Anaphoricity determination is the task of judging whether a given NP is anaphoric or nonanaphoric. Recent research advances have provided several important findings as follows: • Learning-based methods for antecedent identification can also benefit from the use of linguistic clues insp"
I05-2030,P02-1053,0,0.0547136,"on the Web has attracted increasing interest in technologies for automatically mining large numbers of message boards and blog pages for opinions and recommendations. Previous approaches to the task of mining a large-scale document collection for opinions can be classified into two groups: the document classification approach and the information extraction approach. In the document classification approach, researchers have been exploring techniques for classifying documents according to semantic/sentiment orientation such as positive vs. negative (e.g. (Dave et al., 2003; Pang and Lee, 2004; Turney, 2002)). The information extraction approach, on the other hand, focuses on the task of extracting elements which constitute opinions (e.g. (Kanayama and Nasukawa, 2004; Hu and Liu, 2004; Tateishi et al., 2001)). The aim of this paper is to extract opinions that represent an evaluation of a products together with the evidence. To achieve this, we consider our task from the information extraction view173 point. We term the above task opinion extraction in this paper. While they can be linguistically realized in many ways, opinions on a product are in fact often expressed in the form of an attribute-v"
I08-1073,D07-1007,0,0.019204,"ce and Technology University of Sussex 8916-5 Takayama, Ikoma, Nara, 630-0192, Japan Falmer, East Sussex ryu-i@is.naist.jp BN1 9QH, UK {dianam,robk}@sussex.ac.uk Abstract many researches believe it will be important for applications which require, or would beneﬁt from, some degree of semantic interpretation. There has been considerable skepticism over whether WSD will actually improve performance of applications, but we are now starting to see improvement in performance due to WSD in cross-lingual information retrieval (Clough and Stevenson, 2004; Vossen et al., 2006) and machine translation (Carpuat and Wu, 2007; Chan et al., 2007) and we hope that other applications such as question-answering, text simpliﬁcation and summarisation might also beneﬁt as WSD methods improve. In recent years there have been various approaches aimed at automatic acquisition of predominant senses of words. This information can be exploited as a powerful backoff strategy for word sense disambiguation given the zipﬁan distribution of word senses. Approaches which do not require manually sense-tagged data have been proposed for English exploiting lexical resources available, notably WordNet. In these approaches distributional"
I08-1073,P07-1007,0,0.0569397,"Missing"
I08-1073,P07-1005,0,0.0377788,"ersity of Sussex 8916-5 Takayama, Ikoma, Nara, 630-0192, Japan Falmer, East Sussex ryu-i@is.naist.jp BN1 9QH, UK {dianam,robk}@sussex.ac.uk Abstract many researches believe it will be important for applications which require, or would beneﬁt from, some degree of semantic interpretation. There has been considerable skepticism over whether WSD will actually improve performance of applications, but we are now starting to see improvement in performance due to WSD in cross-lingual information retrieval (Clough and Stevenson, 2004; Vossen et al., 2006) and machine translation (Carpuat and Wu, 2007; Chan et al., 2007) and we hope that other applications such as question-answering, text simpliﬁcation and summarisation might also beneﬁt as WSD methods improve. In recent years there have been various approaches aimed at automatic acquisition of predominant senses of words. This information can be exploited as a powerful backoff strategy for word sense disambiguation given the zipﬁan distribution of word senses. Approaches which do not require manually sense-tagged data have been proposed for English exploiting lexical resources available, notably WordNet. In these approaches distributional similarity is coupl"
I08-1073,W07-1428,0,0.0219632,"which are not always available. There are several issues for future directions of automatic detection of a ﬁrst sense heuristic. In this paper, we proposed an adaptation of the lesk measure of gloss-based similarity, by using the average similarity between nouns in the two glosses under comparison in a bag-of-words approach without recourse to other information. However, it would be worthwhile exploring other information in the glosses, such as words of other PoS and predicate argument relations. We also hope to investigate applying alignment techniques introduced for entailment recognition (Hickl and Bensley, 2007). Another important issue in WSD is to group ﬁnegrained word senses into clusters, making the task suitable for NLP applications (Ide and Wilks, 2006). We believe that our gloss-based similarity DSlesk might be very suitable for this task and we plan to investigate the possibility. There are other approaches we would like to explore in future. Mihalcea (2005) uses dictionary definitions alongside graphical algorithms for unsupervised WSD. Whilst the results are not directly comparable to ours because we have not included contextual evidence in our models, it would be worthwhile exploring if un"
I08-1073,P06-1013,0,0.0384264,"Missing"
I08-1073,O97-1002,0,0.819347,"g computed. In this paper we refer to the original method (Lesk, 1986) as lesk and the extended measure proposed by Banerjee and Pedersen as Elesk. This paper investigates the potential of using the overlap of dictionary deﬁnitions with the McCarthy et al.’s method. We test the method for obtaining a ﬁrst sense heuristic using two publicly available datasets of sense-tagged data in Japanese, EDR (NICT, 2002) and the S ENSEVAL-2 Japanese dictionary task (Shirai, 2001). We contrast an implementation of lesk (Lesk, 1986) which uses only dictionary deﬁnitions with the Jiang-Conrath measure (jcn) (Jiang and Conrath, 1997) which uses man562 ually produced hyponym links and was used previously for this purpose on English datasets (McCarthy et al., 2004). The jcn measure is only applicable to the EDR dataset because the dictionary has hyponymy links which are not available in the S ENSEVAL-2 Japanese dictionary task. We also propose a new extension to lesk which does not require hand-crafted hyponym links but instead uses distributional similarity to increase the possibilities for overlap of the word deﬁnitions. We refer to this new measure as DSlesk. We compare this to the original lesk on both datasets and show"
I08-1073,H05-1053,1,0.893981,"we select one word sense at random for each word token and average the precision over 100 trials. For contrast with a supervised approach we show the performance if we use handlabelled training data for obtaining the predominant sense of the test words. This method usually outperforms an automatic approach, but crucially relies on there being hand-labelled data which is expensive to produce. The method cannot be applied where there is no hand-labelled training data, it will be unreliable for low frequency data and a general dataset may not be applicable when one moves to domain speciﬁc text (Koeling et al., 2005). Since we are not using context for disambiguation, but just a ﬁrst sense heuristic, we also give the upper-bound which is the ﬁrst sense heuristic calculated from the test data itself. 4.1 EDR We conduct empirical evaluation using 3,836 polysemous nouns in the sense-tagged corpus provided with EDR (183,502 instances) where the glosses are deﬁned in the EDR dictionary. We evaluated on this dataset using WSD precision and recall of this corpus using only our ﬁrst-sense heuristic (no context). The results are shown in Table 1. The WSD performance of all the automatic methods is much lower than"
I08-1073,W02-2016,0,0.0228183,"ties, jcn, lesk and DSlesk3 , for use in the method to 3 Elesk can be used when several semantic relations such as hypnoymy and meronomy are available. However, we cannot directly apply Elesk as it was used in (McCarthy et al., 2004) to 565 ﬁnd the most likely sense in the set of word senses deﬁned in each inventory following the approach of McCarthy et al. (2004). For the thesaurus construction we used <verb, case, noun&gt; triplets extracted from Japanese newspaper articles (9 years of the Mainichi Shinbun (1991-1999) and 10 years of the Nihon Keizai Shinbun (1991-2000)) and parsed by CaboCha (Kudo and Matsumoto, 2002). This resulted in 53 million triplet instances for acquiring the distributional thesaurus. We adopt the similarity score proposed by Lin (1998) as the distributional similarity score and use 50 nearest neighbours in line with McCarthy et al. For the random baseline we select one word sense at random for each word token and average the precision over 100 trials. For contrast with a supervised approach we show the performance if we use handlabelled training data for obtaining the predominant sense of the test words. This method usually outperforms an automatic approach, but crucially relies on"
I08-1073,P98-2127,0,0.902401,"en the candidate paraphrases. For each word in each of the two texts they obtain the maximum similarity between the word and any of the words from the putative paraphrase. The similarity scores for each word of both phrases contribute to an overall semantic similarity between 0 and 1 and a threshold of 0.5 is used to decide if the candidate phrases are paraphrases. In our work, we compare glosses of words senses (senses of the target word and senses of the nearest neighbour) rather than paraphrases. In this approach we extend the deﬁnition overlap by considering the distributional similarity (Lin, 1998) rather than identify of the words in the two deﬁnitions. In addition to McCarthy et al. (2004) there are other approaches to ﬁnding predominant senses. Chan and Ng (2005) use parallel data to provide estimates for sense frequency distributions to feed into a supervised WSD system. Mohammad and Hirst (2006) propose an approach to acquiring predominant senses from corpora which makes use of the category information in the Macquarie Thesaurus (Barnard, 1986). Lexical chains (Galley and McKeown, 2003) may also provide a useful ﬁrst sense heuristic (Brody et al., 2006) but are produced 563 using W"
I08-1073,P04-1036,1,0.934538,"incorporates information from the sense inventory. It is this semantic similarity measure which is the focus of our paper in the context of the method for acquiring predominant senses. Whilst the McCarthy et al.’s method works well for English, other inventories do not always have WordNet style resources to tie the nearest neighbours to the sense inventory. WordNet has many semantic relations as well as glosses associated with its synsets (near synonym sets). While traditional dictionaries do not organise senses into synsets, they do typically have sense deﬁnitions associated with the senses. McCarthy et al. (2004) suggest that dictionary deﬁnitions can be used with their method, however in the implementation of the measure based on dictionary deﬁnitions that they use, the dictionary deﬁnitions are extended to those of related words using the hierarchical structure of WordNet (Banerjee and Pedersen, 2002). This extension to the original method (Lesk, 1986) was proposed because there is not always sufﬁcient overlap of the individual words for which semantic similarity is being computed. In this paper we refer to the original method (Lesk, 1986) as lesk and the extended measure proposed by Banerjee and Pe"
I08-1073,H05-1052,0,0.0519522,"Missing"
I08-1073,H93-1061,0,0.606995,"Missing"
I08-1073,E06-1016,0,0.205552,"Missing"
I08-1073,S07-1006,0,0.0881096,"Missing"
I08-1073,S01-1008,0,0.189448,"l method (Lesk, 1986) was proposed because there is not always sufﬁcient overlap of the individual words for which semantic similarity is being computed. In this paper we refer to the original method (Lesk, 1986) as lesk and the extended measure proposed by Banerjee and Pedersen as Elesk. This paper investigates the potential of using the overlap of dictionary deﬁnitions with the McCarthy et al.’s method. We test the method for obtaining a ﬁrst sense heuristic using two publicly available datasets of sense-tagged data in Japanese, EDR (NICT, 2002) and the S ENSEVAL-2 Japanese dictionary task (Shirai, 2001). We contrast an implementation of lesk (Lesk, 1986) which uses only dictionary deﬁnitions with the Jiang-Conrath measure (jcn) (Jiang and Conrath, 1997) which uses man562 ually produced hyponym links and was used previously for this purpose on English datasets (McCarthy et al., 2004). The jcn measure is only applicable to the EDR dataset because the dictionary has hyponymy links which are not available in the S ENSEVAL-2 Japanese dictionary task. We also propose a new extension to lesk which does not require hand-crafted hyponym links but instead uses distributional similarity to increase the"
I08-1073,W04-0811,0,0.069796,"Missing"
I08-1073,C98-2122,0,\N,Missing
I11-1010,stoia-etal-2008-scare,0,\N,Missing
I11-1010,D10-1046,0,\N,Missing
I11-1010,W98-1119,0,\N,Missing
I11-1010,D08-1069,0,\N,Missing
I11-1010,P10-1128,1,\N,Missing
I11-1010,E09-1032,0,\N,Missing
I11-1010,J95-2003,0,\N,Missing
I11-1010,J94-4002,0,\N,Missing
I11-1010,P87-1022,0,\N,Missing
I11-1010,P02-1014,0,\N,Missing
I11-1010,J01-4004,0,\N,Missing
I11-1010,J86-3001,0,\N,Missing
I11-1010,P96-1036,0,\N,Missing
I11-1010,P03-1023,0,\N,Missing
iida-tokunaga-2014-building,W09-2110,0,\N,Missing
iida-tokunaga-2014-building,W07-1522,1,\N,Missing
iida-tokunaga-2014-building,D10-1023,0,\N,Missing
iida-tokunaga-2014-building,P06-1032,0,\N,Missing
iida-tokunaga-2014-building,J95-2003,0,\N,Missing
iida-tokunaga-2014-building,J08-1001,0,\N,Missing
iida-tokunaga-2014-building,P11-1094,0,\N,Missing
iida-tokunaga-2014-building,C12-2048,1,\N,Missing
iida-tokunaga-2014-building,maekawa-etal-2010-design,0,\N,Missing
iida-tokunaga-2014-building,W10-4236,0,\N,Missing
kaplan-etal-2010-annotation,kingsbury-palmer-2002-treebank,0,\N,Missing
kaplan-etal-2010-annotation,miltsakaki-etal-2004-penn,0,\N,Missing
kaplan-etal-2010-annotation,J93-2004,0,\N,Missing
kaplan-etal-2010-annotation,bird-etal-2000-atlas,0,\N,Missing
kaplan-etal-2010-annotation,W07-1522,1,\N,Missing
kaplan-etal-2010-annotation,W07-1523,0,\N,Missing
kaplan-etal-2010-annotation,W03-2120,0,\N,Missing
L18-1556,doddington-etal-2004-automatic,0,0.0457969,"] while drinking coffee at a i caf´e. Table 1: Examples of -de zero anaphora in our data our first annotation scheme alone, while the Kyoto University Text Corpus (Kawahara et al., 2002), the largest existing resource that we are aware of, has only 333 instances of -de zero anaphora of the equivalent type. Table 1 shows a few illustrative examples of zero anaphora successfully collected in our work.1 2. Related work Anaphora or coreference has been annotated in several projects including Message Understanding Conference (MUC) (Hirschman and Chinchor, 1997), Automatic Context Extraction (ACE) (Doddington et al., 2004) and OntoNotes (Hovy et al., 2006), but zero anaphora is not annotated in their English corpora. The OntoNotes corpora for pro-drop languages like Chinese and Arabic contain coreference annotations for certain types of zero pronouns. They do not, however, include adjunct zero pronouns, which we deal with in this paper. Another kind of resources that are relevant to our work is annotated corpora of semantic roles or frame elements such as PropBank (Palmer et al., 2005) and FrameNet 3523 1 We only deal with intra-sentential anaphora in this paper. (Baker et al., 1998). In FrameNet, for example,"
L18-1556,N06-2015,0,0.0670484,"ble 1: Examples of -de zero anaphora in our data our first annotation scheme alone, while the Kyoto University Text Corpus (Kawahara et al., 2002), the largest existing resource that we are aware of, has only 333 instances of -de zero anaphora of the equivalent type. Table 1 shows a few illustrative examples of zero anaphora successfully collected in our work.1 2. Related work Anaphora or coreference has been annotated in several projects including Message Understanding Conference (MUC) (Hirschman and Chinchor, 1997), Automatic Context Extraction (ACE) (Doddington et al., 2004) and OntoNotes (Hovy et al., 2006), but zero anaphora is not annotated in their English corpora. The OntoNotes corpora for pro-drop languages like Chinese and Arabic contain coreference annotations for certain types of zero pronouns. They do not, however, include adjunct zero pronouns, which we deal with in this paper. Another kind of resources that are relevant to our work is annotated corpora of semantic roles or frame elements such as PropBank (Palmer et al., 2005) and FrameNet 3523 1 We only deal with intra-sentential anaphora in this paper. (Baker et al., 1998). In FrameNet, for example, frame elements that are not overtl"
L18-1556,W07-1522,1,0.614857,"ra-sentential anaphora in this paper. (Baker et al., 1998). In FrameNet, for example, frame elements that are not overtly encoded are annotated as Null Instantiation, some of which can be regarded as adjunct zero anaphors, although their antecedents are not annotated. As for Japanese resources, zero anaphora was annotated in 5,000 sentences of the Kyoto University Text Corpus (Kawahara et al., 2002), including -de zero anaphora, although its size is small; it has only 333 instances of intra-sentential -de zero anaphora. Zero anaphora is also annotated for 20,000 sentences in the NAIST corpus (Iida et al., 2007), but only for -ga (nominative), -o (accusative) and -ni (dative). While zero anaphora resolution has been recognized as an important task in pro-drop languages such as Chinese and Japanese, the task has been less prominent in languages like English, in which core arguments are usually realized as overt forms. However, adjuncts can be omitted in any language, and in such cases, they must be inferred from the context. For example, consider the following English sentence: Shortly after her arrival in Tokyo, she began her career as a journalist. Given this sentence, we can infer that she began he"
L18-1556,D16-1132,1,0.842636,"notate zero anaphora in a corpus to achieve our goal, such an approach might be inefficient when we are interested only in instances that are useful in a specific application. Given this, we propose two different annotation schemes in this paper. In the first scheme, annotators annotate QA instances that potentially involve zero anaphora. In the second scheme, annotators directly annotate noun-predicate pairs with regard to whether they are in a zero anaphora relationship. We evaluated the performance of these two schemes using an existing neural network-based zero anaphora resolution method (Iida et al., 2016). Our experimental results show that the first scheme achieved better performance when it is used to train a module for a QA system, suggesting that the effectiveness of an annotation scheme depends on applications even in a relatively well studied task like zero anaphora resolution. Conversely, the model trained with annotation results of the second scheme achieved better performance in identifying zero anaphora for sentences randomly sampled from a corpus. We collected 20,830 instances of -de zero anaphora with レンタルバイクを借りて島をまわる。I rent a motorcyclei and travel the island [by ∅ ]. i ソーラー発電が拡大す"
L18-1556,kawahara-etal-2002-construction,0,0.238288,"Missing"
L18-1556,W04-3230,0,0.0936705,"dicate. We also created a small third dataset for evaluation, which we refer to as General. For this dataset, sentences were randomly sampled from the four-billion-page web corpus. For each noun-predicate pair that was identified as not being in a dependency relationship, annotators annotated whether it was in a -de anaphora relationship or not. In order to restrict our data to Japanese body texts, we only used sentences that (i) have at least two postpositions and (ii) end with the Japanese full stop (。). To identify nouns and predicates to annotate, we used the morphological analyzer MeCab (Kudo et al., 2004), as well as the dependency parser J.DepP (Yoshinaga and Kitsuregawa, 2009). 5. Annotation results In this section, we describe annotation results obtained from the annotation tasks described above. Table 2 summarizes the sizes of our datasets as well as our annotation results. For each dataset, three annotators independently evaluate each instance, and the final judgment was determined by a majority vote. An annotator was sometimes replaced by another person after completing a set of 1,000 instances. The total numbers of annotators participated in our work, as well as the time required to cre"
L18-1556,C16-2055,1,0.845435,"ectly modify improve, but we can clearly see that it is the means of the action denoted by the predicate. Solving -de zero anaphora is useful in a variety of applications including question answering. For example, given sentence (4), to build a QA system that can correctly answer to the question ‘What can we improve medicine with?’, the system must resolve -de zero anaphora. More generally, -de anaphora resolution plays a crucial role when we would like to extract information like location and means that must be inferred from context.2 4. To generate datasets for annotation, we used WISDOM X (Mizuno et al., 2016), a question-answering system that our team has been developing.3 The factoid QA module of WISDOM X accepts a question in Japanese and returns nouns as answers, as well as original sentences from the web corpus that support the answers. An example is below. (5) Question AI-de nani-ga jitsugensuru AI-INSTR what-NOM be.realized ‘What will be realized by AI?’ Answer kaji-robotto ‘housekeeping robot’ Sentence AI-ga sarani hattensureba, AI-NOM further develop.if kaji-robotto-ga jitsugensuru daroo. housekeeping-robot-NOM be.realized will ‘If AI develops further, housekeeping robots will be brought i"
L18-1556,P17-1146,0,0.0327632,"Missing"
L18-1556,J05-1004,0,0.0726498,"ojects including Message Understanding Conference (MUC) (Hirschman and Chinchor, 1997), Automatic Context Extraction (ACE) (Doddington et al., 2004) and OntoNotes (Hovy et al., 2006), but zero anaphora is not annotated in their English corpora. The OntoNotes corpora for pro-drop languages like Chinese and Arabic contain coreference annotations for certain types of zero pronouns. They do not, however, include adjunct zero pronouns, which we deal with in this paper. Another kind of resources that are relevant to our work is annotated corpora of semantic roles or frame elements such as PropBank (Palmer et al., 2005) and FrameNet 3523 1 We only deal with intra-sentential anaphora in this paper. (Baker et al., 1998). In FrameNet, for example, frame elements that are not overtly encoded are annotated as Null Instantiation, some of which can be regarded as adjunct zero anaphors, although their antecedents are not annotated. As for Japanese resources, zero anaphora was annotated in 5,000 sentences of the Kyoto University Text Corpus (Kawahara et al., 2002), including -de zero anaphora, although its size is small; it has only 333 instances of intra-sentential -de zero anaphora. Zero anaphora is also annotated"
L18-1556,I08-1025,0,0.0334482,"ce, our target is limited to intrasentential anaphora. We created two datasets for annotation, QAAnnot and AllNouns, based on the QA instances generated by the procedure above. QAAnnot Annotators directly evaluate QA instances that potentially involve zero anaphora. This task can be simultaneously interpreted as both a QA evaluation task and a zero anaphora annotation task. For this task, we obtained 100,000 QA instances in the following manner. First, we generated questions for 10,000 nouns randomly sampled from the nouns that most frequently appear in the -de position in the TSUBAKI corpus (Shinzato et al., 2008) of 600 million web pages. Next, we randomly sampled questions according to the frequency distribution of the predicates; these questions were then input into WISDOM X until we obtained 100,000 QA instances. Finally, human annotators judge each QA instance for its correctness. AllNouns While QAAnnot may be optimized for QA, its special annotation scheme may have a negative impact on performance when it is used to train a model for identifying -de zero anaphora in general. To investigate this, we created the second dataset called AllNouns; for this dataset, we obtained 10,000 QA instances using"
L18-1556,D09-1160,0,0.071958,"Missing"
N15-1031,N12-1058,0,0.0590805,"Missing"
N15-1031,W12-1633,1,0.512713,"e of RR, ignoring pronouns or deixis. In this paper, we opted to use the model presented in Kennington et al. (2013), the simple incremental update model (SIUM). It has been tested extensively against data from a puzzle-playing human/computer interaction domain (the PENTO data, (Kousidis et al., 2013)); it can incorporate multi-modal information, works in real-time, and can resolve definite, exophoric, and deictic references in a single framework, all of which makes it a potential candidate for working in an interactive, multi-modal dialogue system. The model is similar to the one proposed in Funakoshi et al. (2012), which could resolve descriptions, anaphora, and deixis in a unified manner, but that model does not work incrementally.1 The main contributions of this paper are the more thorough exposition of the model (in Section 3) and its application and evaluation on much less constrained, more interactive (and hence realistic) data than what it has previously been tested on (Section 4). Moreover, the data set used here is also from a typologically very different language (Japanese) than what the model has been previously tested on (German), and so the robustness of the model against these differences"
N15-1031,J95-3003,0,0.412762,"Missing"
N15-1031,P10-1128,1,0.618145,"rence resolution, which has been tested in a simpler setup, on more natural data coming from a corpus of human/human interactions. The model is incremental in that it does not wait until the end of an utterance to process, rather it updates its interpretation at each word increment. The model can also incorporate other modalities, such as gaze or pointing cues (deixis) incrementally. We also model the saliency of the context, and show that the model can easily take such contextual information into account. The model improves over previous work on reference resolution applied to the same data (Iida et al., 2010; Iida et al., 2011). The paper is structured as follows: in the following section we discuss related work on incremental resolution of referring expressions. We explain the model that we use in Section 3 and the data we apply it to in Section 4. We then describe the experiments and the results and provide a discussion. 2 Related Work Reference resolution (RR), which is the task of resolving referring expressions (REs) to what they are 272 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 272–282, c Denver, Colorado, May 31 – June 5, 2015."
N15-1031,I11-1010,1,0.539537,"hich has been tested in a simpler setup, on more natural data coming from a corpus of human/human interactions. The model is incremental in that it does not wait until the end of an utterance to process, rather it updates its interpretation at each word increment. The model can also incorporate other modalities, such as gaze or pointing cues (deixis) incrementally. We also model the saliency of the context, and show that the model can easily take such contextual information into account. The model improves over previous work on reference resolution applied to the same data (Iida et al., 2010; Iida et al., 2011). The paper is structured as follows: in the following section we discuss related work on incremental resolution of referring expressions. We explain the model that we use in Section 3 and the data we apply it to in Section 4. We then describe the experiments and the results and provide a discussion. 2 Related Work Reference resolution (RR), which is the task of resolving referring expressions (REs) to what they are 272 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 272–282, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for"
N15-1031,W13-4030,1,0.881205,"(Schlangen et al., 2009), a model that used Markov Logic Networks to resolve objects on a screen (Kennington and Schlangen, 2013), a model of RR and incremental feedback (Traum et al., 2012), and an approach that used a semantic representation to refer to objects (Peldszus et al., 2012; 273 Kennington et al., 2014). However, the approaches reported there did not incorporate multi-modal information, were too slow to work in real-time, were evaluated on constrained data, or only focused on a specific type of RR, ignoring pronouns or deixis. In this paper, we opted to use the model presented in Kennington et al. (2013), the simple incremental update model (SIUM). It has been tested extensively against data from a puzzle-playing human/computer interaction domain (the PENTO data, (Kousidis et al., 2013)); it can incorporate multi-modal information, works in real-time, and can resolve definite, exophoric, and deictic references in a single framework, all of which makes it a potential candidate for working in an interactive, multi-modal dialogue system. The model is similar to the one proposed in Funakoshi et al. (2012), which could resolve descriptions, anaphora, and deixis in a unified manner, but that model"
N15-1031,C14-1170,1,0.724457,"s deixis. It has been shown that incorporating gaze improves RR in a situated setting because speakers need to look at and distinguish from distractors the objects they are describing: this has been shown in a static scene on a computer screen (Prasov and Chai, 2008), in human-human interactive puzzle tasks (Iida et al., 2010; Iida et al., 2011), in web browsing (Hakkani-t¨ur et al., 2014), and in a moving car where speakers look at objects in their vicinity (Misu et al., 2014). Incorporating pointing (deictic) gestures is also potentially useful in situated RR; as for example Matuszek et al. (2014) have shown in work on resolving objects processed by computer vision techniques. Chen and Eugenio (2012) looked into reference in multi-modal settings, with focus on co-referential pronouns and pointing gestures. However, these approaches were applied in settings in which communication between the two interlocutors was constrained, or the developed systems did not process incrementally. Kehler (2000) presented approach that focused more on interaction in a map task, though the model was not incremental, nor did grounding occur between language and world, as we do here. Incremental RR has also"
N15-1031,W14-4314,0,0.0217786,"f the corpus we used. 3 The Simple Incremental Update Model Following Kennington et al. (2013) and Kennington et al. (2014), we model the task at hand as one of recovering I, the intention of the speaker making the RE, where I ranges over the possible alternatives (the objects in the domain). This recovery proceeds incrementally (word by word), for RE of arbitrary length. That is, if U denotes the current word, we are interested in P (I|U ), the current hypothesis about 1 It can be argued that any non-incremental model could be made into an incremental one by applying that model at each word (Khouzaimi et al., 2014), but we would argue that more modeling effort is required in order for the model to work in an interactive dialogue system, see (Schlangen and Skantze, 2009; Aist et al., 2007; Skantze and Schlangen, 2009; Skantze and Hjalmarsson, 1991). the intended referent, given the observed word. We assume the presence of an unobserved, latent variable R, which models properties of the candidate objects such as colour or shape; explained further below), and so the computation formally is: P (I|U ) = X P (I, U, R) r∈R P (U ) (1) Which, after making some independence assumptions, can be factored into: P (I"
N15-1031,W13-4048,1,0.882779,"definite descriptions makes up a large part of human communication (Poesio and Vieira, 1997). In task-oriented situations, these references are often to entities that are visible in the shared environment. This kind of reference has attracted attention in recent computational research, but the kinds of interactions studied are often fairly restricted in controlled lab situations (Tanenhaus and Spivey-Knowlton, 1995) or simulated human/computer interactions, (Schlangen David Schlangen Bielefeld University Universit¨atsstraße 25 Bielefeld Germany david.schlangen@ uni-bielefeld.de et al., 2009; Kousidis et al., 2013; Chai et al., 2014). In such task-oriented, co-located settings, interlocutors can make use of extra-linguistic cues such as gaze or pointing gestures. Furthermore, listeners resolve references as they unfold, often identifying the referred entity before the end of the reference (Tanenhaus and Spivey-Knowlton, 1995; Spivey et al., 2002), however research in reference resolution has mostly focused on full, completed referring expressions. In this paper we make a first move towards addressing somewhat more complex domains. We apply a model of reference resolution, which has been tested in a sim"
N15-1031,W13-4010,0,0.019311,"intended to refer to, has been well-studied in various fields such as psychology (Isaacs and Clark, 1987; Tanenhaus and Spivey-Knowlton, 1995), linguistics (Pineda and Garza, 2000), as well as human/human (Iida et al., 2010) and human/machine interaction (Prasov and Chai, 2010; Siebert and Schlangen, 2008; Schlangen et al., 2009). In recent years, multi-modal corpora have emerged which provide RR with important contextual information: collecting dialogue between two humans (Tokunaga et al., 2012; Spanger et al., 2012), between a human and a (simulated) dialogue system (Kousidis et al., 2013; Liu et al., 2013), with gaze, information about the shared environment, and in some cases deixis. It has been shown that incorporating gaze improves RR in a situated setting because speakers need to look at and distinguish from distractors the objects they are describing: this has been shown in a static scene on a computer screen (Prasov and Chai, 2008), in human-human interactive puzzle tasks (Iida et al., 2010; Iida et al., 2011), in web browsing (Hakkani-t¨ur et al., 2014), and in a moving car where speakers look at objects in their vicinity (Misu et al., 2014). Incorporating pointing (deictic) gestures is"
N15-1031,W14-4304,0,0.0126023,"ulated) dialogue system (Kousidis et al., 2013; Liu et al., 2013), with gaze, information about the shared environment, and in some cases deixis. It has been shown that incorporating gaze improves RR in a situated setting because speakers need to look at and distinguish from distractors the objects they are describing: this has been shown in a static scene on a computer screen (Prasov and Chai, 2008), in human-human interactive puzzle tasks (Iida et al., 2010; Iida et al., 2011), in web browsing (Hakkani-t¨ur et al., 2014), and in a moving car where speakers look at objects in their vicinity (Misu et al., 2014). Incorporating pointing (deictic) gestures is also potentially useful in situated RR; as for example Matuszek et al. (2014) have shown in work on resolving objects processed by computer vision techniques. Chen and Eugenio (2012) looked into reference in multi-modal settings, with focus on co-referential pronouns and pointing gestures. However, these approaches were applied in settings in which communication between the two interlocutors was constrained, or the developed systems did not process incrementally. Kehler (2000) presented approach that focused more on interaction in a map task, thou"
N15-1031,E12-1052,1,0.853341,"n in a map task, though the model was not incremental, nor did grounding occur between language and world, as we do here. Incremental RR has also been studied in a number of papers, including a framework for fast incremental interpretation (Schuler et al., 2009), a Bayesian filtering model approach that was sensitive to disfluencies (Schlangen et al., 2009), a model that used Markov Logic Networks to resolve objects on a screen (Kennington and Schlangen, 2013), a model of RR and incremental feedback (Traum et al., 2012), and an approach that used a semantic representation to refer to objects (Peldszus et al., 2012; 273 Kennington et al., 2014). However, the approaches reported there did not incorporate multi-modal information, were too slow to work in real-time, were evaluated on constrained data, or only focused on a specific type of RR, ignoring pronouns or deixis. In this paper, we opted to use the model presented in Kennington et al. (2013), the simple incremental update model (SIUM). It has been tested extensively against data from a puzzle-playing human/computer interaction domain (the PENTO data, (Kousidis et al., 2013)); it can incorporate multi-modal information, works in real-time, and can re"
N15-1031,D10-1046,0,0.029976,"vide a discussion. 2 Related Work Reference resolution (RR), which is the task of resolving referring expressions (REs) to what they are 272 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 272–282, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics intended to refer to, has been well-studied in various fields such as psychology (Isaacs and Clark, 1987; Tanenhaus and Spivey-Knowlton, 1995), linguistics (Pineda and Garza, 2000), as well as human/human (Iida et al., 2010) and human/machine interaction (Prasov and Chai, 2010; Siebert and Schlangen, 2008; Schlangen et al., 2009). In recent years, multi-modal corpora have emerged which provide RR with important contextual information: collecting dialogue between two humans (Tokunaga et al., 2012; Spanger et al., 2012), between a human and a (simulated) dialogue system (Kousidis et al., 2013; Liu et al., 2013), with gaze, information about the shared environment, and in some cases deixis. It has been shown that incorporating gaze improves RR in a situated setting because speakers need to look at and distinguish from distractors the objects they are describing: this"
N15-1031,E09-1081,1,0.837559,"one of recovering I, the intention of the speaker making the RE, where I ranges over the possible alternatives (the objects in the domain). This recovery proceeds incrementally (word by word), for RE of arbitrary length. That is, if U denotes the current word, we are interested in P (I|U ), the current hypothesis about 1 It can be argued that any non-incremental model could be made into an incremental one by applying that model at each word (Khouzaimi et al., 2014), but we would argue that more modeling effort is required in order for the model to work in an interactive dialogue system, see (Schlangen and Skantze, 2009; Aist et al., 2007; Skantze and Schlangen, 2009; Skantze and Hjalmarsson, 1991). the intended referent, given the observed word. We assume the presence of an unobserved, latent variable R, which models properties of the candidate objects such as colour or shape; explained further below), and so the computation formally is: P (I|U ) = X P (I, U, R) r∈R P (U ) (1) Which, after making some independence assumptions, can be factored into: P (I|U ) = X 1 P (I) P (U |R)P (R|I) P (U ) (2) r∈R This is an update model in the usual sense that the posterior P (I|U ) at one step becomes the prior P (I) at"
N15-1031,W09-3905,1,0.960556,"on (RR), which is the task of resolving referring expressions (REs) to what they are 272 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 272–282, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics intended to refer to, has been well-studied in various fields such as psychology (Isaacs and Clark, 1987; Tanenhaus and Spivey-Knowlton, 1995), linguistics (Pineda and Garza, 2000), as well as human/human (Iida et al., 2010) and human/machine interaction (Prasov and Chai, 2010; Siebert and Schlangen, 2008; Schlangen et al., 2009). In recent years, multi-modal corpora have emerged which provide RR with important contextual information: collecting dialogue between two humans (Tokunaga et al., 2012; Spanger et al., 2012), between a human and a (simulated) dialogue system (Kousidis et al., 2013; Liu et al., 2013), with gaze, information about the shared environment, and in some cases deixis. It has been shown that incorporating gaze improves RR in a situated setting because speakers need to look at and distinguish from distractors the objects they are describing: this has been shown in a static scene on a computer screen"
N15-1031,J09-3001,0,0.0280009,"io (2012) looked into reference in multi-modal settings, with focus on co-referential pronouns and pointing gestures. However, these approaches were applied in settings in which communication between the two interlocutors was constrained, or the developed systems did not process incrementally. Kehler (2000) presented approach that focused more on interaction in a map task, though the model was not incremental, nor did grounding occur between language and world, as we do here. Incremental RR has also been studied in a number of papers, including a framework for fast incremental interpretation (Schuler et al., 2009), a Bayesian filtering model approach that was sensitive to disfluencies (Schlangen et al., 2009), a model that used Markov Logic Networks to resolve objects on a screen (Kennington and Schlangen, 2013), a model of RR and incremental feedback (Traum et al., 2012), and an approach that used a semantic representation to refer to objects (Peldszus et al., 2012; 273 Kennington et al., 2014). However, the approaches reported there did not incorporate multi-modal information, were too slow to work in real-time, were evaluated on constrained data, or only focused on a specific type of RR, ignoring pr"
N15-1031,W08-0113,1,0.80871,"lated Work Reference resolution (RR), which is the task of resolving referring expressions (REs) to what they are 272 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 272–282, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics intended to refer to, has been well-studied in various fields such as psychology (Isaacs and Clark, 1987; Tanenhaus and Spivey-Knowlton, 1995), linguistics (Pineda and Garza, 2000), as well as human/human (Iida et al., 2010) and human/machine interaction (Prasov and Chai, 2010; Siebert and Schlangen, 2008; Schlangen et al., 2009). In recent years, multi-modal corpora have emerged which provide RR with important contextual information: collecting dialogue between two humans (Tokunaga et al., 2012; Spanger et al., 2012), between a human and a (simulated) dialogue system (Kousidis et al., 2013; Liu et al., 2013), with gaze, information about the shared environment, and in some cases deixis. It has been shown that incorporating gaze improves RR in a situated setting because speakers need to look at and distinguish from distractors the objects they are describing: this has been shown in a static sc"
N15-1031,E09-1085,1,0.886272,"er making the RE, where I ranges over the possible alternatives (the objects in the domain). This recovery proceeds incrementally (word by word), for RE of arbitrary length. That is, if U denotes the current word, we are interested in P (I|U ), the current hypothesis about 1 It can be argued that any non-incremental model could be made into an incremental one by applying that model at each word (Khouzaimi et al., 2014), but we would argue that more modeling effort is required in order for the model to work in an interactive dialogue system, see (Schlangen and Skantze, 2009; Aist et al., 2007; Skantze and Schlangen, 2009; Skantze and Hjalmarsson, 1991). the intended referent, given the observed word. We assume the presence of an unobserved, latent variable R, which models properties of the candidate objects such as colour or shape; explained further below), and so the computation formally is: P (I|U ) = X P (I, U, R) r∈R P (U ) (1) Which, after making some independence assumptions, can be factored into: P (I|U ) = X 1 P (I) P (U |R)P (R|I) P (U ) (2) r∈R This is an update model in the usual sense that the posterior P (I|U ) at one step becomes the prior P (I) at the next. P (R|I) provides the link between the"
N15-1031,tokunaga-etal-2012-rex,1,0.782204,"ter of the ACL, pages 272–282, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics intended to refer to, has been well-studied in various fields such as psychology (Isaacs and Clark, 1987; Tanenhaus and Spivey-Knowlton, 1995), linguistics (Pineda and Garza, 2000), as well as human/human (Iida et al., 2010) and human/machine interaction (Prasov and Chai, 2010; Siebert and Schlangen, 2008; Schlangen et al., 2009). In recent years, multi-modal corpora have emerged which provide RR with important contextual information: collecting dialogue between two humans (Tokunaga et al., 2012; Spanger et al., 2012), between a human and a (simulated) dialogue system (Kousidis et al., 2013; Liu et al., 2013), with gaze, information about the shared environment, and in some cases deixis. It has been shown that incorporating gaze improves RR in a situated setting because speakers need to look at and distinguish from distractors the objects they are describing: this has been shown in a static scene on a computer screen (Prasov and Chai, 2008), in human-human interactive puzzle tasks (Iida et al., 2010; Iida et al., 2011), in web browsing (Hakkani-t¨ur et al., 2014), and in a moving car"
N15-1031,J98-2001,0,\N,Missing
N15-1031,J00-2002,0,\N,Missing
P06-1079,W04-3239,1,0.887184,"Missing"
P06-1079,J94-4002,0,0.200142,"logy 8916-5 Takayama, Ikoma, Nara, 630-0192, Japan {ryu-i,inui,matsu}@is.naist.jp Abstract can not be interpreted only by shallow syntactic parsing, a model specialized for zero-anaphora resolution needs to be devised on the top of shallow syntactic and semantic processing. Recent work on zero-anaphora resolution can be located in two different research contexts. First, zero-anaphora resolution is studied in the context of anaphora resolution (AR), in which zeroanaphora is regarded as a subclass of anaphora. In AR, the research trend has been shifting from rulebased approaches (Baldwin, 1995; Lappin and Leass, 1994; Mitkov, 1997, etc.) to empirical, or corpus-based, approaches (McCarthy and Lehnert, 1995; Ng and Cardie, 2002a; Soon et al., 2001; Strube and M¨uller, 2003; Yang et al., 2003) because the latter are shown to be a cost-efficient solution achieving a performance that is comparable to best performing rule-based systems (see the Coreference task in MUC1 and the Entity Detection and Tracking task in the ACE program2 ). The same trend is observed also in Japanese zeroanaphora resolution, where the findings made in rule-based or theory-oriented work (Kameyama, 1986; Nakaiwa and Shirai, 1996; Okumu"
P06-1079,W97-1303,0,0.0566686,"oma, Nara, 630-0192, Japan {ryu-i,inui,matsu}@is.naist.jp Abstract can not be interpreted only by shallow syntactic parsing, a model specialized for zero-anaphora resolution needs to be devised on the top of shallow syntactic and semantic processing. Recent work on zero-anaphora resolution can be located in two different research contexts. First, zero-anaphora resolution is studied in the context of anaphora resolution (AR), in which zeroanaphora is regarded as a subclass of anaphora. In AR, the research trend has been shifting from rulebased approaches (Baldwin, 1995; Lappin and Leass, 1994; Mitkov, 1997, etc.) to empirical, or corpus-based, approaches (McCarthy and Lehnert, 1995; Ng and Cardie, 2002a; Soon et al., 2001; Strube and M¨uller, 2003; Yang et al., 2003) because the latter are shown to be a cost-efficient solution achieving a performance that is comparable to best performing rule-based systems (see the Coreference task in MUC1 and the Entity Detection and Tracking task in the ACE program2 ). The same trend is observed also in Japanese zeroanaphora resolution, where the findings made in rule-based or theory-oriented work (Kameyama, 1986; Nakaiwa and Shirai, 1996; Okumura and Tamura,"
P06-1079,C96-2137,0,0.0779105,"n, 1995; Lappin and Leass, 1994; Mitkov, 1997, etc.) to empirical, or corpus-based, approaches (McCarthy and Lehnert, 1995; Ng and Cardie, 2002a; Soon et al., 2001; Strube and M¨uller, 2003; Yang et al., 2003) because the latter are shown to be a cost-efficient solution achieving a performance that is comparable to best performing rule-based systems (see the Coreference task in MUC1 and the Entity Detection and Tracking task in the ACE program2 ). The same trend is observed also in Japanese zeroanaphora resolution, where the findings made in rule-based or theory-oriented work (Kameyama, 1986; Nakaiwa and Shirai, 1996; Okumura and Tamura, 1996, etc.) have been successfully incorporated in machine learning-based frameworks (Seki et al., 2002; Iida et al., 2003). Second, the task of zero-anaphora resolution has some overlap with Propbank3 -style semantic role labeling (SRL), which has been intensively studied, for example, in the context of the CoNLL SRL task4 . In this task, given a sentence “To attract younger listeners, Radio Free Europe intersperses the latest in Western rock groups”, an SRL We approach the zero-anaphora resolution problem by decomposing it into intra-sentential and inter-sentential zero"
P06-1079,P04-1020,0,0.119987,"e are two alternative ways for anaphoricity determination: the single-step model and the two-step model. The single-step model (Soon et al., 2001; Ng and Cardie, 2002a) determines the anaphoricity of a given anaphor indirectly as a by-product of the search for its antecedent. If an appropriate candidate antecedent is found, the anaphor is classified as anaphoric; otherwise, it is classified as non-anaphoric. One disadvantage of this model is that it cannot employ the preferencebased model because the preference-based model is not capable of identifying non-anaphoric cases. The two-step model (Ng, 2004; Poesio et al., 2004; Iida et al., 2005), on the other hand, carries out anaphoricity determination in a separate step from antecedent identification. Poesio et al. (2004) and Iida et al. (2005) claim that the latter subtask should be done before the former. For example, given a target anaphor (TA), Iida et al.’s selection-then-classification model: 1. selects the most likely candidate antecedent (CA) of TA using the tournament model, 2. classifies TA paired with CA as either anaphoric or non-anaphoric using an anaphoricity determination model. If the CA-TA pair is classified as anaphoric, CA"
P06-1079,P02-1014,0,0.493031,"nly by shallow syntactic parsing, a model specialized for zero-anaphora resolution needs to be devised on the top of shallow syntactic and semantic processing. Recent work on zero-anaphora resolution can be located in two different research contexts. First, zero-anaphora resolution is studied in the context of anaphora resolution (AR), in which zeroanaphora is regarded as a subclass of anaphora. In AR, the research trend has been shifting from rulebased approaches (Baldwin, 1995; Lappin and Leass, 1994; Mitkov, 1997, etc.) to empirical, or corpus-based, approaches (McCarthy and Lehnert, 1995; Ng and Cardie, 2002a; Soon et al., 2001; Strube and M¨uller, 2003; Yang et al., 2003) because the latter are shown to be a cost-efficient solution achieving a performance that is comparable to best performing rule-based systems (see the Coreference task in MUC1 and the Entity Detection and Tracking task in the ACE program2 ). The same trend is observed also in Japanese zeroanaphora resolution, where the findings made in rule-based or theory-oriented work (Kameyama, 1986; Nakaiwa and Shirai, 1996; Okumura and Tamura, 1996, etc.) have been successfully incorporated in machine learning-based frameworks (Seki et al."
P06-1079,C96-2147,0,0.0563355,"1994; Mitkov, 1997, etc.) to empirical, or corpus-based, approaches (McCarthy and Lehnert, 1995; Ng and Cardie, 2002a; Soon et al., 2001; Strube and M¨uller, 2003; Yang et al., 2003) because the latter are shown to be a cost-efficient solution achieving a performance that is comparable to best performing rule-based systems (see the Coreference task in MUC1 and the Entity Detection and Tracking task in the ACE program2 ). The same trend is observed also in Japanese zeroanaphora resolution, where the findings made in rule-based or theory-oriented work (Kameyama, 1986; Nakaiwa and Shirai, 1996; Okumura and Tamura, 1996, etc.) have been successfully incorporated in machine learning-based frameworks (Seki et al., 2002; Iida et al., 2003). Second, the task of zero-anaphora resolution has some overlap with Propbank3 -style semantic role labeling (SRL), which has been intensively studied, for example, in the context of the CoNLL SRL task4 . In this task, given a sentence “To attract younger listeners, Radio Free Europe intersperses the latest in Western rock groups”, an SRL We approach the zero-anaphora resolution problem by decomposing it into intra-sentential and inter-sentential zeroanaphora resolution. For t"
P06-1079,W04-0707,0,0.0690539,"unity has recently made two important findings: • A model that identifies the antecedent of an anaphor by a series of comparisons between candidate antecedents has a remarkable advantage over a model that estimates the absolute likelihood of each candidate independently of other candidates (Iida et al., 2003; Yang et al., 2003). • An AR model that carries out antecedent identification before anaphoricity determination, the decision whether a given NP is anaphoric or not (i.e. discourse-new), significantly outperforms a model that executes those subtasks in the reverse order or simultaneously (Poesio et al., 2004; Iida et al., 2005). 2 Zero-anaphora resolution In this paper, we consider only zero-pronouns that function as an obligatory argument of a predicate for two reasons: • Providing a clear definition of zero-pronouns appearing in adjunctive argument positions involves awkward problems, which we believe should be postponed until obligatory zero-anaphora is well studied. • Resolving obligatory zero-anaphora tends to be more important than adjunctive zeropronouns in actual applications. A zero-pronoun may have its antecedent in the discourse; in this case, we say the zero-pronoun is anaphoric. On t"
P06-1079,C02-1078,0,0.701082,"ardie, 2002a; Soon et al., 2001; Strube and M¨uller, 2003; Yang et al., 2003) because the latter are shown to be a cost-efficient solution achieving a performance that is comparable to best performing rule-based systems (see the Coreference task in MUC1 and the Entity Detection and Tracking task in the ACE program2 ). The same trend is observed also in Japanese zeroanaphora resolution, where the findings made in rule-based or theory-oriented work (Kameyama, 1986; Nakaiwa and Shirai, 1996; Okumura and Tamura, 1996, etc.) have been successfully incorporated in machine learning-based frameworks (Seki et al., 2002; Iida et al., 2003). Second, the task of zero-anaphora resolution has some overlap with Propbank3 -style semantic role labeling (SRL), which has been intensively studied, for example, in the context of the CoNLL SRL task4 . In this task, given a sentence “To attract younger listeners, Radio Free Europe intersperses the latest in Western rock groups”, an SRL We approach the zero-anaphora resolution problem by decomposing it into intra-sentential and inter-sentential zeroanaphora resolution. For the former problem, syntactic patterns of the appearance of zero-pronouns and their antecedents are"
P06-1079,J01-4004,0,0.746746,"ic parsing, a model specialized for zero-anaphora resolution needs to be devised on the top of shallow syntactic and semantic processing. Recent work on zero-anaphora resolution can be located in two different research contexts. First, zero-anaphora resolution is studied in the context of anaphora resolution (AR), in which zeroanaphora is regarded as a subclass of anaphora. In AR, the research trend has been shifting from rulebased approaches (Baldwin, 1995; Lappin and Leass, 1994; Mitkov, 1997, etc.) to empirical, or corpus-based, approaches (McCarthy and Lehnert, 1995; Ng and Cardie, 2002a; Soon et al., 2001; Strube and M¨uller, 2003; Yang et al., 2003) because the latter are shown to be a cost-efficient solution achieving a performance that is comparable to best performing rule-based systems (see the Coreference task in MUC1 and the Entity Detection and Tracking task in the ACE program2 ). The same trend is observed also in Japanese zeroanaphora resolution, where the findings made in rule-based or theory-oriented work (Kameyama, 1986; Nakaiwa and Shirai, 1996; Okumura and Tamura, 1996, etc.) have been successfully incorporated in machine learning-based frameworks (Seki et al., 2002; Iida et al.,"
P06-1079,P03-1022,0,0.0257275,"Missing"
P06-1079,P03-1005,0,0.0206942,"earning algorithm As noted in Section 1, the use of zero-pronouns in Japanese is relatively less constrained by syntax compared, for example, with English. This forces the above way of encoding path information to produce an explosive number of different paths, which inevitably leads to serious data sparseness. This issue can be addressed in several ways. The SRL community has devised a range of variants of the standard path representation to reduce the complexity (Carreras and Marquez, 2005). Applying Kernel methods such as Tree kernels (Collins and Duffy, 2001) and Hierarchical DAG kernels (Suzuki et al., 2003) is another strong option. The Boosting-based algorithm pro• A path is represented by a subtree consisting of backbone nodes: φ (zero-pronoun), Ant (antecedent), Node (the lowest common ancestor), LeftNode (left-branch node) and RightNode. • Each backbone node has daughter nodes, each corresponding to a function word associated with it. • Content words are deleted. This way of encoding syntactic patterns is used in intra-sentential anaphoricity determination. In antecedent identification, on the other hand, the tournament model allows us to incorporate three paths, a path for each pair of a ze"
P06-1079,W05-0620,0,0.0407031,"s anaphoric as its antecedent, ‘shusho (prime minister)’, appears in the same sentence. In sentence (2), on the other hand, φj is considered non-anaphoric if its referent (i.e. the first person) does not appear in the discourse. To our best knowledge, however, existing SRL models do not exploit these advantages. In SRL, on the other hand, it is common to use syntactic features derived from the parse tree of a given input sentence for argument identification. A typical syntactic feature is the path on a parse tree from a target predicate to a noun phrase in question (Gildea and Jurafsky, 2002; Carreras and Marquez, 2005). However, existing AR models deal with intra- and inter-sentential anaphoric relations in a uniform manner; that is, they do not use as rich syntactic features as state-of-the-art SRL models do, even in finding intra-sentential anaphoric relations. We believe that the AR and SRL communities can learn more from each other. Given this background, in this paper, we show that combining the aforementioned techniques derived from each research trend makes significant impact on zero-anaphora resolution, taking Japanese as a target language. More specifically, we demonstrate the following: • Incorpor"
P06-1079,J02-3001,0,0.0236012,"ence (1), zero-pronoun φi is anaphoric as its antecedent, ‘shusho (prime minister)’, appears in the same sentence. In sentence (2), on the other hand, φj is considered non-anaphoric if its referent (i.e. the first person) does not appear in the discourse. To our best knowledge, however, existing SRL models do not exploit these advantages. In SRL, on the other hand, it is common to use syntactic features derived from the parse tree of a given input sentence for argument identification. A typical syntactic feature is the path on a parse tree from a target predicate to a noun phrase in question (Gildea and Jurafsky, 2002; Carreras and Marquez, 2005). However, existing AR models deal with intra- and inter-sentential anaphoric relations in a uniform manner; that is, they do not use as rich syntactic features as state-of-the-art SRL models do, even in finding intra-sentential anaphoric relations. We believe that the AR and SRL communities can learn more from each other. Given this background, in this paper, we show that combining the aforementioned techniques derived from each research trend makes significant impact on zero-anaphora resolution, taking Japanese as a target language. More specifically, we demonstr"
P06-1079,J95-2003,0,0.0943109,"Missing"
P06-1079,W03-2604,1,0.954084,"et al., 2001; Strube and M¨uller, 2003; Yang et al., 2003) because the latter are shown to be a cost-efficient solution achieving a performance that is comparable to best performing rule-based systems (see the Coreference task in MUC1 and the Entity Detection and Tracking task in the ACE program2 ). The same trend is observed also in Japanese zeroanaphora resolution, where the findings made in rule-based or theory-oriented work (Kameyama, 1986; Nakaiwa and Shirai, 1996; Okumura and Tamura, 1996, etc.) have been successfully incorporated in machine learning-based frameworks (Seki et al., 2002; Iida et al., 2003). Second, the task of zero-anaphora resolution has some overlap with Propbank3 -style semantic role labeling (SRL), which has been intensively studied, for example, in the context of the CoNLL SRL task4 . In this task, given a sentence “To attract younger listeners, Radio Free Europe intersperses the latest in Western rock groups”, an SRL We approach the zero-anaphora resolution problem by decomposing it into intra-sentential and inter-sentential zeroanaphora resolution. For the former problem, syntactic patterns of the appearance of zero-pronouns and their antecedents are useful clues. Taking"
P06-1079,P86-1031,0,0.0835096,"proaches (Baldwin, 1995; Lappin and Leass, 1994; Mitkov, 1997, etc.) to empirical, or corpus-based, approaches (McCarthy and Lehnert, 1995; Ng and Cardie, 2002a; Soon et al., 2001; Strube and M¨uller, 2003; Yang et al., 2003) because the latter are shown to be a cost-efficient solution achieving a performance that is comparable to best performing rule-based systems (see the Coreference task in MUC1 and the Entity Detection and Tracking task in the ACE program2 ). The same trend is observed also in Japanese zeroanaphora resolution, where the findings made in rule-based or theory-oriented work (Kameyama, 1986; Nakaiwa and Shirai, 1996; Okumura and Tamura, 1996, etc.) have been successfully incorporated in machine learning-based frameworks (Seki et al., 2002; Iida et al., 2003). Second, the task of zero-anaphora resolution has some overlap with Propbank3 -style semantic role labeling (SRL), which has been intensively studied, for example, in the context of the CoNLL SRL task4 . In this task, given a sentence “To attract younger listeners, Radio Free Europe intersperses the latest in Western rock groups”, an SRL We approach the zero-anaphora resolution problem by decomposing it into intra-sentential"
P06-1079,P03-1023,0,\N,Missing
P09-1073,D08-1069,0,0.336351,"didate in sentence Si . In this situation, for example, candidate c12 is labeled as retained when creating training instances for sentence S1 , but labeled as discarded from S2 onwards, because of the appearance of its zeropronoun. Another candidate c13 which is never referred to in the text is labeled as discarded for all training instances. Second, we need to capture the ‘relative’ salience of candidates appearing in the current discourse for each cache update, as also exploited in the tournament-based or ranking-based approaches to anaphora resolution (Iida et al., 2003; Yang et al., 2003; Denis and Baldridge, 2008). To solve it, we use a ranker trained on the instances created as described above. In order to train the ranker, we adopt the Ranking SVM algorithm (Joachims, 2002), which learns a weight vector to rank candidates for a given partial ranking of each discourse entity. Each training instance is created from the set of retained candidates, Ri , paired with the set of discarded candidates, Di , in each sentence. To Figure 2: Pseudo-code for creating training instances training instances S1 c11 c12 c13 c14 S 2 c21 c22 c23 φ φ i j S3 c31 c32 c33 φk φ retained c11 c12 discarded c13 c14 retained c11"
P09-1073,J86-3001,0,0.408154,"entity. Each training instance is created from the set of retained candidates, Ri , paired with the set of discarded candidates, Di , in each sentence. To Figure 2: Pseudo-code for creating training instances training instances S1 c11 c12 c13 c14 S 2 c21 c22 c23 φ φ i j S3 c31 c32 c33 φk φ retained c11 c12 discarded c13 c14 retained c11 c22 discarded c12 c13 c14 c21 c23 l Figure 3: Creating training instnaces define the partial ranking of candidates, we simply rank candidates in Ri as first place and candidates in Di as second place. 4.2 Static cache model Other research on discourse such as Grosz and Sidner (1986) has studied global focus, which generally refers to the entity or set of entities that are salient throughout the entire discourse. Since global focus may not be captured by Centeringbased models, we also propose another cache model which directly captures the global salience of a text. To train the model, all the candidates in a text which have an inter-sentential anaphoric relation with zero-pronouns are used as positive instances and the others used as negative ones. Unlike the 650 Table 1: Feature set used in the cache models Feature Description POS Part-of-speech of C followed by IPADIC4"
P09-1073,J95-2003,0,0.832228,"machine learning-based methods using considerable amounts of annotated data provided by, for example, the Message Understanding Conference and Automatic Context Extraction programs (Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2008; McCallum and Wellner, 2003, etc.). These methods reach a level comparable to or better than the state-of-the-art rule-based systems (e.g. Baldwin (1995)) by recasting the task of anaphora resolution into classification or clustering problems. However, such approaches tend to disregard theoretical findings from discourse theories, such as Centering Theory (Grosz et al., 1995). Therefore, one of the challenging issues in this area is to incorporate such findings from linguistic theories into machine learning-based approaches. A typical machine learning-based approach to zero-anaphora resolution searches for an antecedent in the set of candidates appearing in all the preceding contexts. However, computational time makes this approach largely infeasible for long texts. An alternative approach is to heuristically limit the search space (e.g. the system deals with candidates only occurring in the N previous sentences). Various research such as Yang et al. (2008) has ad"
P09-1073,P97-1014,0,0.20415,"to be excluded from target candidate antecedents. On the other hand, rule-based methods derived from theoretical background such as Centering Theory (Grosz et al., 1995) only deal with the salient discourse entities at each point of the discourse status. By incrementally updating the discourse status, the set of candidates in question is automatically limited. Although these methods have a theoretical advantage, they have a serious drawback in that Centering Theory only retains information about the previous sentence. A few methods have attempted to overcome this fault (Suri and McCoy, 1994; Hahn and Strube, 1997), but they are overly dependent upon the restrictions fundamental to the notion of centering. We hope that by relaxing such restrictions it will be possible for an anaphora resolution system to achieve a good balance between accuracy and computational cost. From this background, we focus on the issue of reducing candidate antecedents (discourse entities) for a given anaphor. Inspired by Walker’s argument (Walker, 1996), we propose a machine learning-based caching mechanism that captures the most salient candidates at each point of the discourse for efficient anaphora resolution. More specifica"
P09-1073,W03-2604,1,0.850397,"in Figure 3, where cij is the j-th candidate in sentence Si . In this situation, for example, candidate c12 is labeled as retained when creating training instances for sentence S1 , but labeled as discarded from S2 onwards, because of the appearance of its zeropronoun. Another candidate c13 which is never referred to in the text is labeled as discarded for all training instances. Second, we need to capture the ‘relative’ salience of candidates appearing in the current discourse for each cache update, as also exploited in the tournament-based or ranking-based approaches to anaphora resolution (Iida et al., 2003; Yang et al., 2003; Denis and Baldridge, 2008). To solve it, we use a ranker trained on the instances created as described above. In order to train the ranker, we adopt the Ranking SVM algorithm (Joachims, 2002), which learns a weight vector to rank candidates for a given partial ranking of each discourse entity. Each training instance is created from the set of retained candidates, Ri , paired with the set of discarded candidates, Di , in each sentence. To Figure 2: Pseudo-code for creating training instances training instances S1 c11 c12 c13 c14 S 2 c21 c22 c23 φ φ i j S3 c31 c32 c33 φk φ r"
P09-1073,W07-1522,1,0.905738,"irical evaluation with Japanese newspaper articles shows that the number of candidate antecedents for each zero-pronoun can be dramatically reduced while preserving the accuracy of resolving it. 1 Introduction There have been recently increasing concerns with the need for anaphora resolution to make NLP applications such as IE and MT more reliable. In particular, for languages such as Japanese, anaphora resolution is crucial for resolving a phrase in a text to its referent since phrases, especially nominative arguments of predicates, are frequently omitted by anaphoric functions in discourse (Iida et al., 2007b). Many researchers have recently explored machine learning-based methods using considerable amounts of annotated data provided by, for example, the Message Understanding Conference and Automatic Context Extraction programs (Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2008; McCallum and Wellner, 2003, etc.). These methods reach a level comparable to or better than the state-of-the-art rule-based systems (e.g. Baldwin (1995)) by recasting the task of anaphora resolution into classification or clustering problems. However, such approaches tend to disregard theoretical findings from dis"
P09-1073,W03-1024,0,0.0170854,"nd to the most salient discourse entities within the local transition and within the global focus of a text. Hahn and Strube (1997) estimate hierarchical discourse segments of a text by taking into account a series of Cp and then the resolution model searches for an antecedent in the estimated segment. Although these methods remedy the drawback of Centering, they still overly depend on the notion of Centering such as Cp. On the other hand, the existing machine learning-based methods (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Soon et al., 2001; Ng and Cardie, 2002; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2005; Iida et al., 2007a, etc.) have been developed with less attention given to such a problem. These methods exhaustively search for an antecedent within the list of all candidate antecedents until the beginning of the text. Otherwise, the process to search for antecedents is heuristically carried out in a limited search space (e.g. the previous N sentences of an anaphor) (Yang et al., 2008). In this paper, we consider only zero-pronouns that function as an obligatory argument of a predicate. A zero-pronoun may or may not have its antecedent in the discourse; in the case it do"
P09-1073,P86-1031,0,0.568605,"ion and then Section 3 gives an overview of previous work. Next, in Section 4 we propose a machine learning-based cache model. Section 5 presents the antecedent identification and anaphoricity determination models used in the experiments. To evaluate the model, we conduct several empirical evaluations and report their results in Section 6. Finally, we conclude and discuss the future direction of this research in Section 7. 2 Zero-anaphora resolution 3 Previous work Early methods for zero-anaphora resolution were developed with rule-based approaches in mind. Theory-oriented rule-based methods (Kameyama, 1986; Walker et al., 1994), for example, focus on the Centering Theory (Grosz et al., 1995) and are designed to collect the salient candidate antecedents in the forward-looking center (Cf ) list, and then choose the most salient candidate, Cp, as an antecedent of a zero-pronoun according to heuristic rules (e.g. topic > subject > indirect object > direct object > others1 ). Although these methods have a theoretical advantage, they have a serious drawback in that the original Centering Theory is restricted to keeping information about the previous sentence only. In order to loosen this restriction,"
P09-1073,W04-3239,1,0.836643,"r or not the model using a small number of discourse entities in the cache achieves performance comparable to the original one in a practical setting. For intra-sentential zero-anaphora resolution, we adopt the model proposed by Iida et al. (2007a), which exploits syntactic patterns as features that appear in the dependency path of a zero-pronoun and its candidate antecedent. Note that for simplicity we use bag-of-functional words and their part-of-speech intervening between a zero-pronoun and its candidate antecedent as features instead of learning syntactic patterns with the Bact algorithm (Kudo and Matsumoto, 2004). We illustrated the recall-precision curve of each model by altering the threshold parameter of intrasentential anaphoricity determination, which is shown in Figure 5. The results show that all models achieved almost the same performance when decreasing the cache size. It indicates that it is enough to cache a small number of the most salient References candidates in the current zero-anaphora resolution C. Aone and S. W. Bennett. 1995. Evaluating automated and manual acquisition of anaphora resolution strategies. model, while coverage decreases when the cache In Proceedings of 33th Annual Mee"
P09-1073,2002.tmi-papers.15,0,0.0271594,"antecedents when appearing zero-pronouns refer to an antecedent in a preceding sentence, i.e. we evaluate the cases of inter-sentential anaphora resolution. 6.2 Evaluation of the caching mechanism In this experiment, we directly compare the proAs a baseline, we adopt the following two cache posed static and dynamic cache models with the models. One is the Centering-derived model which heuristic methods presented in Section 2. Note that only stores the preceding ‘wa’ (topic)-marked or 652 ‘ga’ (subject)-marked candidate antecedents in the cache. It is an approximation of the model proposed by Nariyama (2002) for extending the local focus transition defined by Centering Theory. We henceforth call this model the centering-based cache model. The other baseline model stores candidates appearing in the N previous sentences of a zero-pronoun to simulate a heuristic approach used in works like Soon et al. (2001). We call this model the sentence-based cache model. By comparing these baselines with our cache models, we can see whether our models contribute to more efficiently storing salient candidates or not. The above dynamic cache model retains the salient candidates independently of the results of ant"
P09-1073,P02-1014,0,0.667329,"e need for anaphora resolution to make NLP applications such as IE and MT more reliable. In particular, for languages such as Japanese, anaphora resolution is crucial for resolving a phrase in a text to its referent since phrases, especially nominative arguments of predicates, are frequently omitted by anaphoric functions in discourse (Iida et al., 2007b). Many researchers have recently explored machine learning-based methods using considerable amounts of annotated data provided by, for example, the Message Understanding Conference and Automatic Context Extraction programs (Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2008; McCallum and Wellner, 2003, etc.). These methods reach a level comparable to or better than the state-of-the-art rule-based systems (e.g. Baldwin (1995)) by recasting the task of anaphora resolution into classification or clustering problems. However, such approaches tend to disregard theoretical findings from discourse theories, such as Centering Theory (Grosz et al., 1995). Therefore, one of the challenging issues in this area is to incorporate such findings from linguistic theories into machine learning-based approaches. A typical machine learning-based approach to zero"
P09-1073,D08-1068,0,0.0470475,"nt candidates. Our empirical evaluation on Japanese zero-anaphora resolution shows that our learningbased cache model drastically reduces the search space while preserving accuracy. The procedure for zero-anaphora resolution adopted in our model assumes that resolution is carried out linearly, i.e. an antecedent is independently selected without taking into account any other zero-pronouns. However, trends in anaphora resolution have shifted from such linear approaches to more sophisticated ones which globally optimize the interpretation of all the referring expressions in a text. For example, Poon and Domingos (2008) has empirically reported that such global approaches achieve performance better than the ones based on incrementally processing a text. Because their work basically builds on inductive logic programing, we can naturally extend this to incorporate our caching mechanism into the global optimization by expressing cache constraints as predicate logic, which is one of our next challenges in this research area. 6.4 Overall zero-anaphora resolution We finally investigate the effects of introducing the proposed model on overall zero-anaphora resolution including intra-sentential cases. The resolution"
P09-1073,C02-1078,0,0.0197281,"f Cp, that correspond to the most salient discourse entities within the local transition and within the global focus of a text. Hahn and Strube (1997) estimate hierarchical discourse segments of a text by taking into account a series of Cp and then the resolution model searches for an antecedent in the estimated segment. Although these methods remedy the drawback of Centering, they still overly depend on the notion of Centering such as Cp. On the other hand, the existing machine learning-based methods (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Soon et al., 2001; Ng and Cardie, 2002; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2005; Iida et al., 2007a, etc.) have been developed with less attention given to such a problem. These methods exhaustively search for an antecedent within the list of all candidate antecedents until the beginning of the text. Otherwise, the process to search for antecedents is heuristically carried out in a limited search space (e.g. the previous N sentences of an anaphor) (Yang et al., 2008). In this paper, we consider only zero-pronouns that function as an obligatory argument of a predicate. A zero-pronoun may or may not have its antecedent in the dis"
P09-1073,J01-4004,0,0.924987,"ng concerns with the need for anaphora resolution to make NLP applications such as IE and MT more reliable. In particular, for languages such as Japanese, anaphora resolution is crucial for resolving a phrase in a text to its referent since phrases, especially nominative arguments of predicates, are frequently omitted by anaphoric functions in discourse (Iida et al., 2007b). Many researchers have recently explored machine learning-based methods using considerable amounts of annotated data provided by, for example, the Message Understanding Conference and Automatic Context Extraction programs (Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2008; McCallum and Wellner, 2003, etc.). These methods reach a level comparable to or better than the state-of-the-art rule-based systems (e.g. Baldwin (1995)) by recasting the task of anaphora resolution into classification or clustering problems. However, such approaches tend to disregard theoretical findings from discourse theories, such as Centering Theory (Grosz et al., 1995). Therefore, one of the challenging issues in this area is to incorporate such findings from linguistic theories into machine learning-based approaches. A typical machine learning-b"
P09-1073,J94-2006,0,0.330217,"ts anaphor, causing it to be excluded from target candidate antecedents. On the other hand, rule-based methods derived from theoretical background such as Centering Theory (Grosz et al., 1995) only deal with the salient discourse entities at each point of the discourse status. By incrementally updating the discourse status, the set of candidates in question is automatically limited. Although these methods have a theoretical advantage, they have a serious drawback in that Centering Theory only retains information about the previous sentence. A few methods have attempted to overcome this fault (Suri and McCoy, 1994; Hahn and Strube, 1997), but they are overly dependent upon the restrictions fundamental to the notion of centering. We hope that by relaxing such restrictions it will be possible for an anaphora resolution system to achieve a good balance between accuracy and computational cost. From this background, we focus on the issue of reducing candidate antecedents (discourse entities) for a given anaphor. Inspired by Walker’s argument (Walker, 1996), we propose a machine learning-based caching mechanism that captures the most salient candidates at each point of the discourse for efficient anaphora re"
P09-1073,J94-2003,0,0.121523,"tion 3 gives an overview of previous work. Next, in Section 4 we propose a machine learning-based cache model. Section 5 presents the antecedent identification and anaphoricity determination models used in the experiments. To evaluate the model, we conduct several empirical evaluations and report their results in Section 6. Finally, we conclude and discuss the future direction of this research in Section 7. 2 Zero-anaphora resolution 3 Previous work Early methods for zero-anaphora resolution were developed with rule-based approaches in mind. Theory-oriented rule-based methods (Kameyama, 1986; Walker et al., 1994), for example, focus on the Centering Theory (Grosz et al., 1995) and are designed to collect the salient candidate antecedents in the forward-looking center (Cf ) list, and then choose the most salient candidate, Cp, as an antecedent of a zero-pronoun according to heuristic rules (e.g. topic > subject > indirect object > direct object > others1 ). Although these methods have a theoretical advantage, they have a serious drawback in that the original Centering Theory is restricted to keeping information about the previous sentence only. In order to loosen this restriction, the Centering-based m"
P09-1073,J96-2005,0,0.412033,"rious drawback in that Centering Theory only retains information about the previous sentence. A few methods have attempted to overcome this fault (Suri and McCoy, 1994; Hahn and Strube, 1997), but they are overly dependent upon the restrictions fundamental to the notion of centering. We hope that by relaxing such restrictions it will be possible for an anaphora resolution system to achieve a good balance between accuracy and computational cost. From this background, we focus on the issue of reducing candidate antecedents (discourse entities) for a given anaphor. Inspired by Walker’s argument (Walker, 1996), we propose a machine learning-based caching mechanism that captures the most salient candidates at each point of the discourse for efficient anaphora resolution. More specifically, we choose salient candidates for each sentence from the set of candidates appearing in that sentence and the candidates which are already 647 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 647–655, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP in the cache. Searching only through the set of salient candidates, the computational cost of zeroanaphora resolution is"
P09-1073,P03-1023,0,0.080586,"cij is the j-th candidate in sentence Si . In this situation, for example, candidate c12 is labeled as retained when creating training instances for sentence S1 , but labeled as discarded from S2 onwards, because of the appearance of its zeropronoun. Another candidate c13 which is never referred to in the text is labeled as discarded for all training instances. Second, we need to capture the ‘relative’ salience of candidates appearing in the current discourse for each cache update, as also exploited in the tournament-based or ranking-based approaches to anaphora resolution (Iida et al., 2003; Yang et al., 2003; Denis and Baldridge, 2008). To solve it, we use a ranker trained on the instances created as described above. In order to train the ranker, we adopt the Ranking SVM algorithm (Joachims, 2002), which learns a weight vector to rank candidates for a given partial ranking of each discourse entity. Each training instance is created from the set of retained candidates, Ri , paired with the set of discarded candidates, Di , in each sentence. To Figure 2: Pseudo-code for creating training instances training instances S1 c11 c12 c13 c14 S 2 c21 c22 c23 φ φ i j S3 c31 c32 c33 φk φ retained c11 c12 dis"
P09-1073,P08-1096,0,0.399047,"esolution to make NLP applications such as IE and MT more reliable. In particular, for languages such as Japanese, anaphora resolution is crucial for resolving a phrase in a text to its referent since phrases, especially nominative arguments of predicates, are frequently omitted by anaphoric functions in discourse (Iida et al., 2007b). Many researchers have recently explored machine learning-based methods using considerable amounts of annotated data provided by, for example, the Message Understanding Conference and Automatic Context Extraction programs (Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2008; McCallum and Wellner, 2003, etc.). These methods reach a level comparable to or better than the state-of-the-art rule-based systems (e.g. Baldwin (1995)) by recasting the task of anaphora resolution into classification or clustering problems. However, such approaches tend to disregard theoretical findings from discourse theories, such as Centering Theory (Grosz et al., 1995). Therefore, one of the challenging issues in this area is to incorporate such findings from linguistic theories into machine learning-based approaches. A typical machine learning-based approach to zero-anaphora resolutio"
P09-1073,P95-1017,0,\N,Missing
P10-1128,W98-1119,0,0.100768,"duction The task of identifying reference relations including anaphora and coreferences within texts has received a great deal of attention in natural language processing, from both theoretical and empirical perspectives. Recently, research trends for reference resolution have drastically shifted from handcrafted rule-based approaches to corpus-based approaches, due predominately to the growing success of machine learning algorithms (such as Support Vector Machines (Vapnik, 1998)); many researchers have examined ways for introducing various linguistic clues into machine learning-based models (Ge et al., 1998; Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2003; Iida et al., 2005; Yang et al., 2005; Yang et al., 2008; Poon and Domingos, 2008, etc.). Research has continued to progress each year, focusing on tackling the problem as it is represented in the annotated data sets provided by the Message Understanding Conference (MUC)1 and the Automatic Content Extraction (ACE)2 . In these data sets, coreference relations are deﬁned as a limited version of a typical coreference; this generally means that only the relations where expressions refer to the same named entities are addressed, because it"
P10-1128,J95-2003,0,0.240397,"pieces illustrated on the computer display. As a basis for our reference resolution model, we adopt an existing model for reference resolution. Recently, machine learning-based approaches to reference resolution (Soon et al., 2001; Ng and Cardie, 2002, etc.) have been developed, particularly focussing on identifying anaphoric relations in texts, and have achieved better performance than hand-crafted rule-based approaches. These models for reference resolution take into account linguistic factors, such as relative salience of candidate antecedents, which have been modeled in Centering Theory (Grosz et al., 1995) by ranking candidate antecedents appearing in the preceding discourse (Iida et al., 2003; Yang et al., 2003; Denis and Baldridge, 2008). In order to take advantage of existing models, we adopt the rankingbased approach as a basis for our reference resolution model. In conventional ranking-based models, Yang et al. (2003) and Iida et al. (2003) decompose the ranking process into a set of pairwise comparisons of two candidate antecedents. However, recent work by Denis and Baldridge (2008) reports that appropriately constructing a model for ranking all candidates yields improved performance over"
P10-1128,W03-2604,1,0.710746,"we adopt an existing model for reference resolution. Recently, machine learning-based approaches to reference resolution (Soon et al., 2001; Ng and Cardie, 2002, etc.) have been developed, particularly focussing on identifying anaphoric relations in texts, and have achieved better performance than hand-crafted rule-based approaches. These models for reference resolution take into account linguistic factors, such as relative salience of candidate antecedents, which have been modeled in Centering Theory (Grosz et al., 1995) by ranking candidate antecedents appearing in the preceding discourse (Iida et al., 2003; Yang et al., 2003; Denis and Baldridge, 2008). In order to take advantage of existing models, we adopt the rankingbased approach as a basis for our reference resolution model. In conventional ranking-based models, Yang et al. (2003) and Iida et al. (2003) decompose the ranking process into a set of pairwise comparisons of two candidate antecedents. However, recent work by Denis and Baldridge (2008) reports that appropriately constructing a model for ranking all candidates yields improved performance over those utilising pairwise ranking. Similarly we adopt a ranking-based model, in which all"
P10-1128,E06-1007,0,0.0355446,"Missing"
P10-1128,P07-1103,0,0.072211,"Missing"
P10-1128,P02-1014,0,0.472713,"referent of the current referring expression. 3.2 Use of extra-linguistic information 3.1 Ranking model to identify referents To investigate the impact of extra-linguistic information on reference resolution, we conduct an em4 pirical evaluation in which a reference resolution model chooses a referent (i.e. a piece) for a given referring expression from the set of pieces illustrated on the computer display. As a basis for our reference resolution model, we adopt an existing model for reference resolution. Recently, machine learning-based approaches to reference resolution (Soon et al., 2001; Ng and Cardie, 2002, etc.) have been developed, particularly focussing on identifying anaphoric relations in texts, and have achieved better performance than hand-crafted rule-based approaches. These models for reference resolution take into account linguistic factors, such as relative salience of candidate antecedents, which have been modeled in Centering Theory (Grosz et al., 1995) by ranking candidate antecedents appearing in the preceding discourse (Iida et al., 2003; Yang et al., 2003; Denis and Baldridge, 2008). In order to take advantage of existing models, we adopt the rankingbased approach as a basis fo"
P10-1128,D08-1068,0,0.0189595,"ention in natural language processing, from both theoretical and empirical perspectives. Recently, research trends for reference resolution have drastically shifted from handcrafted rule-based approaches to corpus-based approaches, due predominately to the growing success of machine learning algorithms (such as Support Vector Machines (Vapnik, 1998)); many researchers have examined ways for introducing various linguistic clues into machine learning-based models (Ge et al., 1998; Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2003; Iida et al., 2005; Yang et al., 2005; Yang et al., 2008; Poon and Domingos, 2008, etc.). Research has continued to progress each year, focusing on tackling the problem as it is represented in the annotated data sets provided by the Message Understanding Conference (MUC)1 and the Automatic Content Extraction (ACE)2 . In these data sets, coreference relations are deﬁned as a limited version of a typical coreference; this generally means that only the relations where expressions refer to the same named entities are addressed, because it makes the coreference resolution task more information extraction-oriented. In other words, the coreference task as deﬁned by MUC and ACE is"
P10-1128,J01-4004,0,0.61212,"f seven pieces as a referent of the current referring expression. 3.2 Use of extra-linguistic information 3.1 Ranking model to identify referents To investigate the impact of extra-linguistic information on reference resolution, we conduct an em4 pirical evaluation in which a reference resolution model chooses a referent (i.e. a piece) for a given referring expression from the set of pieces illustrated on the computer display. As a basis for our reference resolution model, we adopt an existing model for reference resolution. Recently, machine learning-based approaches to reference resolution (Soon et al., 2001; Ng and Cardie, 2002, etc.) have been developed, particularly focussing on identifying anaphoric relations in texts, and have achieved better performance than hand-crafted rule-based approaches. These models for reference resolution take into account linguistic factors, such as relative salience of candidate antecedents, which have been modeled in Centering Theory (Grosz et al., 1995) by ranking candidate antecedents appearing in the preceding discourse (Iida et al., 2003; Yang et al., 2003; Denis and Baldridge, 2008). In order to take advantage of existing models, we adopt the rankingbased a"
P10-1128,P03-1022,0,0.0604902,"Missing"
P10-1128,P03-1023,0,0.314027,"ding anaphora and coreferences within texts has received a great deal of attention in natural language processing, from both theoretical and empirical perspectives. Recently, research trends for reference resolution have drastically shifted from handcrafted rule-based approaches to corpus-based approaches, due predominately to the growing success of machine learning algorithms (such as Support Vector Machines (Vapnik, 1998)); many researchers have examined ways for introducing various linguistic clues into machine learning-based models (Ge et al., 1998; Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2003; Iida et al., 2005; Yang et al., 2005; Yang et al., 2008; Poon and Domingos, 2008, etc.). Research has continued to progress each year, focusing on tackling the problem as it is represented in the annotated data sets provided by the Message Understanding Conference (MUC)1 and the Automatic Content Extraction (ACE)2 . In these data sets, coreference relations are deﬁned as a limited version of a typical coreference; this generally means that only the relations where expressions refer to the same named entities are addressed, because it makes the coreference resolution task more information ext"
P10-1128,P05-1021,0,0.0182173,"texts has received a great deal of attention in natural language processing, from both theoretical and empirical perspectives. Recently, research trends for reference resolution have drastically shifted from handcrafted rule-based approaches to corpus-based approaches, due predominately to the growing success of machine learning algorithms (such as Support Vector Machines (Vapnik, 1998)); many researchers have examined ways for introducing various linguistic clues into machine learning-based models (Ge et al., 1998; Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2003; Iida et al., 2005; Yang et al., 2005; Yang et al., 2008; Poon and Domingos, 2008, etc.). Research has continued to progress each year, focusing on tackling the problem as it is represented in the annotated data sets provided by the Message Understanding Conference (MUC)1 and the Automatic Content Extraction (ACE)2 . In these data sets, coreference relations are deﬁned as a limited version of a typical coreference; this generally means that only the relations where expressions refer to the same named entities are addressed, because it makes the coreference resolution task more information extraction-oriented. In other words, the"
P10-1128,P08-1096,0,0.0196794,"a great deal of attention in natural language processing, from both theoretical and empirical perspectives. Recently, research trends for reference resolution have drastically shifted from handcrafted rule-based approaches to corpus-based approaches, due predominately to the growing success of machine learning algorithms (such as Support Vector Machines (Vapnik, 1998)); many researchers have examined ways for introducing various linguistic clues into machine learning-based models (Ge et al., 1998; Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2003; Iida et al., 2005; Yang et al., 2005; Yang et al., 2008; Poon and Domingos, 2008, etc.). Research has continued to progress each year, focusing on tackling the problem as it is represented in the annotated data sets provided by the Message Understanding Conference (MUC)1 and the Automatic Content Extraction (ACE)2 . In these data sets, coreference relations are deﬁned as a limited version of a typical coreference; this generally means that only the relations where expressions refer to the same named entities are addressed, because it makes the coreference resolution task more information extraction-oriented. In other words, the coreference task as"
P10-1128,D08-1069,0,\N,Missing
P11-1081,D07-1119,0,0.0554162,"Missing"
P11-1081,bosco-etal-2010-comparing,0,0.0188446,"Missing"
P11-1081,N07-1030,0,0.260518,"crease in performance. We conclude and discuss future work in Section 7. 2 Using ILP for joint anaphoricity and coreference determination Integer Linear Programming (ILP) is a method for constraint-based inference aimed at finding the values for a set of variables that maximize a (linear) objective function while satisfying a number of constraints. Roth and Yih (2004) advocated ILP as a general solution for a number of NLP tasks that require combining multiple classifiers and which the traditional pipeline architecture is not appropriate, such as entity disambiguation and relation extraction. Denis and Baldridge (2007) defined the following object function for the joint anaphoricity and coreference determination problem. 805  min i,j∈P −C cC i,j · xi,j + ci,j · (1 − xi,j ) +  −A cA · (1 − yj ) (2) j · yj + cj j∈M subject to xi,j ∈ {0, 1} ∀i, j ∈ P yj ∈ {0, 1} ∀j ∈ M M stands for the set of mentions in the document, and P the set of possible coreference links over these mentions. xi,j is an indicator variable that is set to 1 if mentions i and j are coreferent, and 0 otherwise. yj is an indicator variable that is set to 1 if mention j is anaphoric, and 0 otherwise. The costs cC i,j = −log"
P11-1081,J95-2003,0,0.459765,"n Section 3: the direct reimplementation of the Denis and Baldridge proposal (i.e., using the same constrains), a version replacing Do-Not-Resolve-Not-Anaphors with BestFirst, and a version with Subject Detection as well. As discussed by Iida et al. (2007a) and Imamura et al. (2009), useful features in intra-sentential zeroanaphora are different from ones in inter-sentential zero-anaphora because in the former problem syntactic information between a zero pronoun and its candidate antecedent is essential, while the latter needs to capture the significance of saliency based on Centering Theory (Grosz et al., 1995). To directly reflect this difference, we created two antecedent identification models; one for intrasentential zero-anaphora, induced using the training instances which a zero pronoun and its candidate antecedent appear in the same sentences, the other for 6 7 807 Models http://chasen-legacy.sourceforge.jp/ http://sourceforge.jp/projects/naist-jdic/ inter-sentential cases, induced from the remaining training instances. To estimate the feature weights of each classifier, we used MEGAM8 , an implementation of the Maximum Entropy model, with default parameter settings. The ILP-based models were"
P11-1081,W03-2604,1,0.842834,"baselines, that precision is still low. One of the major source of the errors is that zero pronouns are frequently used in Italian and Japanese in contexts in which in English as so-called generic they would be used: “I walked into the hotel and (they) said ..”. In such case, the zero pronoun detection model is often incorrect. We are considering adding a generic they detection component. We also intend to experiment with introducing more sophisticated antecedent identification models in the ILP framework. In this paper, we used a very basic pairwise classifier; however Yang et al. (2008) and Iida et al. (2003) showed that the relative comparison of two candidate antecedents leads to obtaining better accuracy than the pairwise model. However, these approaches do not output absolute probabilities, but relative significance between two candidates, and therefore cannot be directly integrated with the ILP-framework. We plan to examine ways of appropriately estimating an absolute score from a set of relative scores for further refinement. Finally, we would like to test our model with English constructions which closely resemble zero anaphora. One example were studied in the Semeval 2010 ‘Linking Events a"
P11-1081,W07-1522,1,0.353433,"anaphoric reference depends on the referred entity being sufficiently salient, hence this type of data–particularly in Japanese and Italian–played a key role in early work in coreference resolution, e.g., in the development of Centering (Kameyama, 1985; Walker et al., 1994; Di Eugenio, 1998). This research highlighted both commonalities and differences between the phenomenon in such languages. Zero anaphora resolution has remained a very active area of study for researchers working on Japanese, because of the prevalence of zeros in such languages1 (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Imamura et al., 2009; Sasano et al., 2009; Taira et al., 2010). But now the availability of corpora annotated to study anaphora, including zero anaphora, in languages such as Italian (e.g., Rodriguez et al. (2010)), and their use in competitions such as SEMEVAL 2010 Task 1 on Multilingual Coreference (Recasens et al., 2010), is leading to a renewed interest in zero anaphora resolution, particularly at the light of the mediocre results obtained on zero anaphors by most systems participating in SEMEVAL. Resolving zero anaphora requires the simultaneous decision that one of"
P11-1081,P09-2022,0,0.688302,"rred entity being sufficiently salient, hence this type of data–particularly in Japanese and Italian–played a key role in early work in coreference resolution, e.g., in the development of Centering (Kameyama, 1985; Walker et al., 1994; Di Eugenio, 1998). This research highlighted both commonalities and differences between the phenomenon in such languages. Zero anaphora resolution has remained a very active area of study for researchers working on Japanese, because of the prevalence of zeros in such languages1 (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Imamura et al., 2009; Sasano et al., 2009; Taira et al., 2010). But now the availability of corpora annotated to study anaphora, including zero anaphora, in languages such as Italian (e.g., Rodriguez et al. (2010)), and their use in competitions such as SEMEVAL 2010 Task 1 on Multilingual Coreference (Recasens et al., 2010), is leading to a renewed interest in zero anaphora resolution, particularly at the light of the mediocre results obtained on zero anaphors by most systems participating in SEMEVAL. Resolving zero anaphora requires the simultaneous decision that one of the arguments of a verb is phonetically un"
P11-1081,W03-1024,0,0.707512,"he felicitousness of zero anaphoric reference depends on the referred entity being sufficiently salient, hence this type of data–particularly in Japanese and Italian–played a key role in early work in coreference resolution, e.g., in the development of Centering (Kameyama, 1985; Walker et al., 1994; Di Eugenio, 1998). This research highlighted both commonalities and differences between the phenomenon in such languages. Zero anaphora resolution has remained a very active area of study for researchers working on Japanese, because of the prevalence of zeros in such languages1 (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Imamura et al., 2009; Sasano et al., 2009; Taira et al., 2010). But now the availability of corpora annotated to study anaphora, including zero anaphora, in languages such as Italian (e.g., Rodriguez et al. (2010)), and their use in competitions such as SEMEVAL 2010 Task 1 on Multilingual Coreference (Recasens et al., 2010), is leading to a renewed interest in zero anaphora resolution, particularly at the light of the mediocre results obtained on zero anaphors by most systems participating in SEMEVAL. Resolving zero anaphora requires the simultaneous d"
P11-1081,S10-1018,0,0.0269548,"Missing"
P11-1081,W04-3239,0,0.026554,"and transformation-based learning respectively in order to manually analyze which clues are important for each argument assignment. Imamura et al. (2009) also tackled to the same problem setting by applying a pairwise classifier for each argument. In their approach, a ‘null’ argument is explicitly added into the set of candidate argument to learn the situation where an argument of a predicate is ‘exophoric’. They reported their model achieved better performance than the work by Taira et al. (2008). Iida et al. (2007a) also used the NAIST text corpus. They adopted the BACT learning algorithm (Kudo and Matsumoto, 2004) to effectively learn subtrees useful for both antecedent identification and zero pronoun detection. Their model drastically outperformed a simple pairwise model, but it is still performed as a cascaded process. Incorporating Italian system mentions gold mentions combined separated combined model R P F R P F R P PAIRWISE 0.508 0.208 0.295 0.472 0.241 0.319 0.582 0.261 DS-CASCADE 0.225 0.553 0.320 0.217 0.574 0.315 0.245 0.609 I-BART 0.324 0.294 0.308 – – – 0.532 0.441 ILP 0.539 0.321 0.403 0.535 0.316 0.397 0.614 0.369 0.471 0.404 0.435 0.483 0.409 0.443 0.545 0.517 ILP +BF 0.537 0.325 0.405 0"
P11-1081,P02-1014,0,0.0429582,"coreference resolution for all anaphors In a second series of experiments we evaluated the performance of our models together with a full coreference system resolving all anaphors, not just zeros. 5.1 Separating vs combining classifiers Different types of nominal expressions display very different anaphoric behavior: e.g., pronoun resolution involves very different types of information from nominal expression resolution, depending more on syntactic information and on the local context and less on commonsense knowledge. But the most common approach to coreference resolution (Soon et al., 2001; Ng and Cardie, 2002, etc.) is to use a single classifier to identify antecedents of all anaphoric expressions, relying on the ability of the machine learning algorithm to learn these differences. These models, however, often fail to capture the differences in anaphoric behavior between different types of expressions–one of the reasons being that the amount of training instances is often too small to learn such differences.11 Using different models would appear to be key in the case of zeroanaphora resolution, which differs even more from the rest of anaphora resolution, e.g., in being particularly sensitive to l"
P11-1081,pianta-etal-2008-textpro,0,0.141876,"6 / 29,544 10,206 / 161,124 28,732 / 190,668 test 696 9,287 250,901 7,877 / 11,205 4,396 / 61,652 12,273 / 72,857 In the 6th column we use the term ‘anaphoric’ to indicate the number of zero anaphors that have an antecedent in the text, whereas the total figure is the sum of anaphoric and exophoric zero-anaphors - zeros with a vague / generic reference. language Italian Table 1: Italian and Japanese Data Sets coreference are annotated. This dataset consists of articles from Italian Wikipedia, tokenized, POStagged and morphologically analyzed using TextPro, a freely available Italian pipeline (Pianta et al., 2008). We parsed the corpus using the Italian version of the DESR dependency parser (Attardi et al., 2007). In Italian, zero pronouns may only occur as omitted subjects of verbs. Therefore, in the task of zero-anaphora resolution all verbs appearing in a text are considered candidates for zero pronouns, and all gold mentions or system mentions preceding a candidate zero pronoun are considered as candidate antecedents. (In contrast, in the experiments on coreference resolution discussed in the following section, all mentions are considered as both candidate anaphors and candidate antecedents. To com"
P11-1081,poesio-etal-2010-creating,1,0.909451,"sk of zero-anaphora resolution all verbs appearing in a text are considered candidates for zero pronouns, and all gold mentions or system mentions preceding a candidate zero pronoun are considered as candidate antecedents. (In contrast, in the experiments on coreference resolution discussed in the following section, all mentions are considered as both candidate anaphors and candidate antecedents. To compare the results with gold mentions and with system detected mentions, we carried out an evaluation using the mentions automatically detected by the Italian version of the BART system (I-BART) (Poesio et al., 2010), which is freely downloadable.3 Japanese For Japanese coreference we used the NAIST Text Corpus (Iida et al., 2007b) version 1.4β, which contains the annotated data about NP coreference and zero-anaphoric relations. We also used the Kyoto University Text Corpus4 that provides dependency relations information for the same articles as the NAIST Text Corpus. In addition, we also used a Japanese named entity tagger, CaboCha5 for automatically tagging named entity labels. In the NAIST Text Corpus mention boundaries are not annotated, only the heads. Thus, we considered 3 http://www.bart-coref.org/"
P11-1081,W09-2411,0,0.0693318,"Missing"
P11-1081,rodriguez-etal-2010-anaphoric,1,0.747253,"ntering (Kameyama, 1985; Walker et al., 1994; Di Eugenio, 1998). This research highlighted both commonalities and differences between the phenomenon in such languages. Zero anaphora resolution has remained a very active area of study for researchers working on Japanese, because of the prevalence of zeros in such languages1 (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Imamura et al., 2009; Sasano et al., 2009; Taira et al., 2010). But now the availability of corpora annotated to study anaphora, including zero anaphora, in languages such as Italian (e.g., Rodriguez et al. (2010)), and their use in competitions such as SEMEVAL 2010 Task 1 on Multilingual Coreference (Recasens et al., 2010), is leading to a renewed interest in zero anaphora resolution, particularly at the light of the mediocre results obtained on zero anaphors by most systems participating in SEMEVAL. Resolving zero anaphora requires the simultaneous decision that one of the arguments of a verb is phonetically unrealized (and which argument exactly–in this paper, we will only be concerned with subject zeros as these are the only type to occur in Italian) and that a particular entity is its antecedent."
P11-1081,W04-2401,0,0.0164358,"ormulation in Section 3. In Section 4 we show the experimental results with zero anaphora only. In Section 5 we discuss experiments testing that adding our zero anaphora detector and resolver to a full coreference resolver would result in overall increase in performance. We conclude and discuss future work in Section 7. 2 Using ILP for joint anaphoricity and coreference determination Integer Linear Programming (ILP) is a method for constraint-based inference aimed at finding the values for a set of variables that maximize a (linear) objective function while satisfying a number of constraints. Roth and Yih (2004) advocated ILP as a general solution for a number of NLP tasks that require combining multiple classifiers and which the traditional pipeline architecture is not appropriate, such as entity disambiguation and relation extraction. Denis and Baldridge (2007) defined the following object function for the joint anaphoricity and coreference determination problem. 805  min i,j∈P −C cC i,j · xi,j + ci,j · (1 − xi,j ) +  −A cA · (1 − yj ) (2) j · yj + cj j∈M subject to xi,j ∈ {0, 1} ∀i, j ∈ P yj ∈ {0, 1} ∀j ∈ M M stands for the set of mentions in the document, and P the set of possible"
P11-1081,N09-1059,0,0.199712,"iciently salient, hence this type of data–particularly in Japanese and Italian–played a key role in early work in coreference resolution, e.g., in the development of Centering (Kameyama, 1985; Walker et al., 1994; Di Eugenio, 1998). This research highlighted both commonalities and differences between the phenomenon in such languages. Zero anaphora resolution has remained a very active area of study for researchers working on Japanese, because of the prevalence of zeros in such languages1 (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Imamura et al., 2009; Sasano et al., 2009; Taira et al., 2010). But now the availability of corpora annotated to study anaphora, including zero anaphora, in languages such as Italian (e.g., Rodriguez et al. (2010)), and their use in competitions such as SEMEVAL 2010 Task 1 on Multilingual Coreference (Recasens et al., 2010), is leading to a renewed interest in zero anaphora resolution, particularly at the light of the mediocre results obtained on zero anaphors by most systems participating in SEMEVAL. Resolving zero anaphora requires the simultaneous decision that one of the arguments of a verb is phonetically unrealized (and which a"
P11-1081,C02-1078,0,0.431839,"o.poesio@unitn.it The felicitousness of zero anaphoric reference depends on the referred entity being sufficiently salient, hence this type of data–particularly in Japanese and Italian–played a key role in early work in coreference resolution, e.g., in the development of Centering (Kameyama, 1985; Walker et al., 1994; Di Eugenio, 1998). This research highlighted both commonalities and differences between the phenomenon in such languages. Zero anaphora resolution has remained a very active area of study for researchers working on Japanese, because of the prevalence of zeros in such languages1 (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Imamura et al., 2009; Sasano et al., 2009; Taira et al., 2010). But now the availability of corpora annotated to study anaphora, including zero anaphora, in languages such as Italian (e.g., Rodriguez et al. (2010)), and their use in competitions such as SEMEVAL 2010 Task 1 on Multilingual Coreference (Recasens et al., 2010), is leading to a renewed interest in zero anaphora resolution, particularly at the light of the mediocre results obtained on zero anaphors by most systems participating in SEMEVAL. Resolving zero anaphora re"
P11-1081,J01-4004,0,0.504846,"entification models; one for intrasentential zero-anaphora, induced using the training instances which a zero pronoun and its candidate antecedent appear in the same sentences, the other for 6 7 807 Models http://chasen-legacy.sourceforge.jp/ http://sourceforge.jp/projects/naist-jdic/ inter-sentential cases, induced from the remaining training instances. To estimate the feature weights of each classifier, we used MEGAM8 , an implementation of the Maximum Entropy model, with default parameter settings. The ILP-based models were compared with the following baselines. PAIRWISE: as in the work by Soon et al. (2001), antecedent identification and anaphoricity determination are simultaneously executed by a single classifier. DS-CASCADE: the model first filters out nonanaphoric candidate anaphors using an anaphoricity determination model, then selects an antecedent from a set of candidate antecedents of anaphoric candidate anaphors using an antecedent identification model. 4.3 SUBJ PRE TOPIC PRE * NUM PRE (GEN PRE ) FIRST SENT FIRST WORD POS / / DEP LEMMA LABEL D POS D LEMMA / / D DEP LABEL PATH * description 1 if subject is included in the preceding words of ZERO in a sentence; otherwise 0. 1 if topic cas"
P11-1081,D08-1055,0,0.616277,"depends on the referred entity being sufficiently salient, hence this type of data–particularly in Japanese and Italian–played a key role in early work in coreference resolution, e.g., in the development of Centering (Kameyama, 1985; Walker et al., 1994; Di Eugenio, 1998). This research highlighted both commonalities and differences between the phenomenon in such languages. Zero anaphora resolution has remained a very active area of study for researchers working on Japanese, because of the prevalence of zeros in such languages1 (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Imamura et al., 2009; Sasano et al., 2009; Taira et al., 2010). But now the availability of corpora annotated to study anaphora, including zero anaphora, in languages such as Italian (e.g., Rodriguez et al. (2010)), and their use in competitions such as SEMEVAL 2010 Task 1 on Multilingual Coreference (Recasens et al., 2010), is leading to a renewed interest in zero anaphora resolution, particularly at the light of the mediocre results obtained on zero anaphors by most systems participating in SEMEVAL. Resolving zero anaphora requires the simultaneous decision that one of the arguments of a v"
P11-1081,P10-2030,0,0.0400395,"ce this type of data–particularly in Japanese and Italian–played a key role in early work in coreference resolution, e.g., in the development of Centering (Kameyama, 1985; Walker et al., 1994; Di Eugenio, 1998). This research highlighted both commonalities and differences between the phenomenon in such languages. Zero anaphora resolution has remained a very active area of study for researchers working on Japanese, because of the prevalence of zeros in such languages1 (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Imamura et al., 2009; Sasano et al., 2009; Taira et al., 2010). But now the availability of corpora annotated to study anaphora, including zero anaphora, in languages such as Italian (e.g., Rodriguez et al. (2010)), and their use in competitions such as SEMEVAL 2010 Task 1 on Multilingual Coreference (Recasens et al., 2010), is leading to a renewed interest in zero anaphora resolution, particularly at the light of the mediocre results obtained on zero anaphors by most systems participating in SEMEVAL. Resolving zero anaphora requires the simultaneous decision that one of the arguments of a verb is phonetically unrealized (and which argument exactly–in th"
P11-1081,J94-2003,0,0.164301,": e.g., the subjects of Italian and Japanese translations of buy in (1b) and (1c) are not explicitly realized. We call these nonrealized mandatory arguments zero anaphors. Massimo Poesio Universit`a di Trento, Center for Mind / Brain Sciences University of Essex, Language and Computation Group massimo.poesio@unitn.it The felicitousness of zero anaphoric reference depends on the referred entity being sufficiently salient, hence this type of data–particularly in Japanese and Italian–played a key role in early work in coreference resolution, e.g., in the development of Centering (Kameyama, 1985; Walker et al., 1994; Di Eugenio, 1998). This research highlighted both commonalities and differences between the phenomenon in such languages. Zero anaphora resolution has remained a very active area of study for researchers working on Japanese, because of the prevalence of zeros in such languages1 (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Imamura et al., 2009; Sasano et al., 2009; Taira et al., 2010). But now the availability of corpora annotated to study anaphora, including zero anaphora, in languages such as Italian (e.g., Rodriguez et al. (2010)), and their use in c"
P11-1081,J08-3002,0,0.0155502,"recall compared to the baselines, that precision is still low. One of the major source of the errors is that zero pronouns are frequently used in Italian and Japanese in contexts in which in English as so-called generic they would be used: “I walked into the hotel and (they) said ..”. In such case, the zero pronoun detection model is often incorrect. We are considering adding a generic they detection component. We also intend to experiment with introducing more sophisticated antecedent identification models in the ILP framework. In this paper, we used a very basic pairwise classifier; however Yang et al. (2008) and Iida et al. (2003) showed that the relative comparison of two candidate antecedents leads to obtaining better accuracy than the pairwise model. However, these approaches do not output absolute probabilities, but relative significance between two candidates, and therefore cannot be directly integrated with the ILP-framework. We plan to examine ways of appropriately estimating an absolute score from a set of relative scores for further refinement. Finally, we would like to test our model with English constructions which closely resemble zero anaphora. One example were studied in the Semeval"
P11-1081,S10-1001,1,\N,Missing
P12-2068,J10-3005,0,0.0355848,"p), and SRL-based F1 (F1-SRL). In driven”. In addition, “say” is the main verb in this our experiment, we have three models. McDonald sentence and hard to be deleted due to the syntactic is a re-implementation of McDonald (2006). Clarke significance. and Lapata (2008) also re-implemented McDonald’s The second example in Table 4 requires to idenmodel with an ILP solver and experimented it on the tify a coreference relation between artificial lake and WNC Corpus. 9 MLN with SRL and MLN w/o Roadford Reservour. We consider that discourse SRL are our Markov Logic models with and with- constraints (Clarke and Lapata, 2010) help our model out SR Constraints, respectively. handle these cases. Discourse and coreference inforNote our three models have no constraint for the mation enable our model to select important argulength of compression. Therefore, we think the com- ments and their predicates. pression rate of the better system should get closer to 5 Conclusion that of human compression. In comparison between In this paper, we proposed new semantic conMLN models and McDonald, the former models outstraints for sentence compression. Our model with perform the latter model on both F1-Dep and F1global constraints"
P12-2068,C08-1018,0,0.168832,"tion. We expect that SR features also help our system to improve readability in sentence compression and summarization. 1 Introduction Recent work in document summarization do not only extract sentences but also compress sentences. Sentence compression enables summarizers to reduce the redundancy in sentences and generate informative summaries beyond the extractive summarization systems (Knight and Marcu, 2002). Conventional approaches to sentence compression exploit various linguistic properties based on lexical information and syntactic dependencies (McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2008; Galanis and Androutsopoulos, 2010). In contrast, our approach utilizes another property based on semantic roles (SRs) which improves weaknesses of syntactic dependencies. Syntactic dependencies are not sufficient to compress some complex sentences with coordination, with passive voice, and with an auxiliary verb. Figure 1 shows an example 2 Why are Semantic Roles Useful for Compressing Sentences? with a coordination structure. 1 1 Before describing our system, we show the statisThis example is from Written News Compression Cortics in terms of predicates, arguments and their relapus (http://j"
P12-2068,N10-1131,0,0.343879,"features also help our system to improve readability in sentence compression and summarization. 1 Introduction Recent work in document summarization do not only extract sentences but also compress sentences. Sentence compression enables summarizers to reduce the redundancy in sentences and generate informative summaries beyond the extractive summarization systems (Knight and Marcu, 2002). Conventional approaches to sentence compression exploit various linguistic properties based on lexical information and syntactic dependencies (McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2008; Galanis and Androutsopoulos, 2010). In contrast, our approach utilizes another property based on semantic roles (SRs) which improves weaknesses of syntactic dependencies. Syntactic dependencies are not sufficient to compress some complex sentences with coordination, with passive voice, and with an auxiliary verb. Figure 1 shows an example 2 Why are Semantic Roles Useful for Compressing Sentences? with a coordination structure. 1 1 Before describing our system, we show the statisThis example is from Written News Compression Cortics in terms of predicates, arguments and their relapus (http://jamesclarke.net/research/resources)."
P12-2068,W08-2123,0,0.0528298,"Missing"
P12-2068,C10-1081,0,0.021202,"dependency relation between Harari and became directly. SRs allow us to model the relations between a predicate and its arguments in a direct fashion. SR constraints are also advantageous in that we can compress sentences with semantic information. In Figure 1, became has three arguments, Harari as A1, businessman as A2, and shortly afterward as AM-TMP. As shown in this example, shortly afterword can be omitted (shaded boxes). In general, modifier arguments like AM-TMP or AM-LOC are more likely to be reduced than complement cases like A0-A4. We can implement such properties by SR constraints. Liu and Gildea (2010) suggests that SR features contribute to generating more readable sentence in machine translation. We expect that SR features also help our system to improve readability in sentence compression and summarization. 1 Introduction Recent work in document summarization do not only extract sentences but also compress sentences. Sentence compression enables summarizers to reduce the redundancy in sentences and generate informative summaries beyond the extractive summarization systems (Knight and Marcu, 2002). Conventional approaches to sentence compression exploit various linguistic properties based"
P12-2068,H05-1066,0,0.0795211,"Missing"
P12-2068,E06-1038,0,0.77525,"more readable sentence in machine translation. We expect that SR features also help our system to improve readability in sentence compression and summarization. 1 Introduction Recent work in document summarization do not only extract sentences but also compress sentences. Sentence compression enables summarizers to reduce the redundancy in sentences and generate informative summaries beyond the extractive summarization systems (Knight and Marcu, 2002). Conventional approaches to sentence compression exploit various linguistic properties based on lexical information and syntactic dependencies (McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2008; Galanis and Androutsopoulos, 2010). In contrast, our approach utilizes another property based on semantic roles (SRs) which improves weaknesses of syntactic dependencies. Syntactic dependencies are not sufficient to compress some complex sentences with coordination, with passive voice, and with an auxiliary verb. Figure 1 shows an example 2 Why are Semantic Roles Useful for Compressing Sentences? with a coordination structure. 1 1 Before describing our system, we show the statisThis example is from Written News Compression Cortics in terms of p"
P12-2068,N03-1026,0,0.0363577,"d) by role(i, j, +r). 4 Experiment and Result 4.1 Experimental Setup Our experimental setting follows previous work (Clarke and Lapata, 2008). As stated in Section 2, we employed the WNC Corpus. For preprocessing, we performed POS tagging by stanford-tagger. 5 and dependency parsing by MST-parser (McDonald et al., 2005). In addition, LTH 6 was exploited to perform both dependency parsing and SR labeling. We implemented our model by Markov Thebeast with Gurobi optimizer. 7 Our evaluation consists of two types of automatic evaluations. The first evaluation is dependency based evaluation same as Riezler et al. (2003). We performed dependency parsing on gold data and system outputs by RASP. 8 Then we calculated precision, recall, and F1 for the set of label(head, modi f ier). In order to demonstrate how well our SR constraints keep correct predicate-argument structures in compression, we propose SRL based evaluation. We performed SR labeling on gold data pos(i, +p1 ) ∧ pos(i + 1, +p2 ) ⇒ inComp(i). (9) POS features are often more reasonable than word 5 http://nlp.stanford.edu/software/tagger.shtml 6 form features to combine with the other properties. http://nlp.cs.lth.se/software/semantic_ parsing:_propban"
P19-1414,P17-1171,0,0.151559,"posed of a generator and a discriminator: the generator network is trained for generating (from answer passages) fake representations to make it hard for the discriminator network to distinguish these fake representations from the true representations derived from manually created compact-answers. Logistic regression +softmax+dropout ? R: Realrepresentation generator ?(?|?) Real D: Discriminator ? F: Fake? Realrepresentation representation generator Fake Fakerepresentation ?(?|?) (b) AGR and its three subnetworks F , R, and D Figure 1: System architecture pervised open-domain QA (DS-QA) task (Chen et al., 2017), which is an extension of a machinereading task, to check whether it is applicable to other datasets. We combined our generator network with a state-of-the-art DS-QA method, OpenQA (Lin et al., 2018), and used a generated compact-answer representation from a given passage as evidence to 1) select relevant passages from the retrieved ones and 2) find an answer from the selected passages. Although the task was not our initial target (why-QA) and the answers in the DS-QA task were considerably shorter than those in the why-QA, experiments using three publicly available datasets (Quasar-T (Dhingr"
P19-1414,N19-1423,0,0.198974,"were disappointed by the small performance improvement, as shown in our experimental results. We combined the generator network in the AGR with an extension of the state-of-the-art why-QA method (Oh et al., 2017). Our evaluation against a Japanese open-domain why-QA dataset, which was created using general web texts as a source of answer passages, revealed that the generator network significantly improved the accuracy of the top-ranked answer passages and that the combination significantly outperformed several strong baselines, including a combination of a generator network and a BERT model (Devlin et al., 2019). This combination also outperformed a vanilla BERT model, suggesting that the generator network in our AGR may be effective even if it is combined with many types of NN architectures. Another interesting point is that the performance improved even when we replaced, as the inputs to AGR, the word embedding vectors that represent an answer passage, with a random vector. This observation warrants further exploration in our future work. Finally, we applied our AGR to a distantly suCompact-answer representation Passage representation Question representation rc rp rq F in AGR (Pretrained) Passage e"
P19-1414,W03-1210,0,0.154125,"is thought to help prevent the growth of microbes in honey. Despite these properties, honey can be contaminated under certain circumstances. Because its acidity, low water activity, and hydrogen peroxide together hinder the growth of microbes. Table 1: Answer passage A to why-question Q and its compact answer C Introduction Why-question answering (why-QA) tasks retrieve from a text archive answers to such why-questions as “Why does honey last such a long time?” Previous why-QA methods retrieve from a text archive answer passages, each of which consists of several sentences, like A in Table 1 (Girju, 2003; Higashinaka and Isozaki, 2008; Oh et al., 2012, 2013, 2016, 2017; dos Santos et al., 2016; Sharp et al., 2016; Tan et al., 2016; Verberne et al., 2011), and then determine whether the passages answer the question. A proper answer passage must contain (1) a paraphrase of the why-question (e.g., the underlined texts in Table 1) and (2) the reasons or the causes (e.g., the bold texts in Table 1) of the events described in the why-question, both of which are often written in multiple non-adjacent sentences. This multi-sentenceness implies that the answer passages often contain redundant parts th"
P19-1414,D12-1057,1,0.854796,"Missing"
P19-1414,I08-1055,0,0.367783,"help prevent the growth of microbes in honey. Despite these properties, honey can be contaminated under certain circumstances. Because its acidity, low water activity, and hydrogen peroxide together hinder the growth of microbes. Table 1: Answer passage A to why-question Q and its compact answer C Introduction Why-question answering (why-QA) tasks retrieve from a text archive answers to such why-questions as “Why does honey last such a long time?” Previous why-QA methods retrieve from a text archive answer passages, each of which consists of several sentences, like A in Table 1 (Girju, 2003; Higashinaka and Isozaki, 2008; Oh et al., 2012, 2013, 2016, 2017; dos Santos et al., 2016; Sharp et al., 2016; Tan et al., 2016; Verberne et al., 2011), and then determine whether the passages answer the question. A proper answer passage must contain (1) a paraphrase of the why-question (e.g., the underlined texts in Table 1) and (2) the reasons or the causes (e.g., the bold texts in Table 1) of the events described in the why-question, both of which are often written in multiple non-adjacent sentences. This multi-sentenceness implies that the answer passages often contain redundant parts that are not directly related to"
P19-1414,P17-1147,0,0.0252233,"ether it is applicable to other datasets. We combined our generator network with a state-of-the-art DS-QA method, OpenQA (Lin et al., 2018), and used a generated compact-answer representation from a given passage as evidence to 1) select relevant passages from the retrieved ones and 2) find an answer from the selected passages. Although the task was not our initial target (why-QA) and the answers in the DS-QA task were considerably shorter than those in the why-QA, experiments using three publicly available datasets (Quasar-T (Dhingra et al., 2017), SearchQA (Dunn et al., 2017), and TriviaQA (Joshi et al., 2017)) revealed that the generator network improved the performance in most cases. This suggests that AGR may be applicable to many QA-like tasks. 2 Why-QA Model Figure 1 illustrates the architecture of our why-QA model and the AGR. Our why-QA model computes the probability that a given answer passage describes a proper answer to a given why-question 4228 using the representations of a question, an answer passage, and a compact answer. The probability (the why-QA model’s final output) is computed from these representations by our answer selection module, which is a logistic regression layer with dr"
P19-1414,D14-1181,0,0.00671335,"or word tj . Finally, we form two attention feature vectors, s a = [as1 , · · · , as|t |] and ac = [ac1 , · · · , ac|t |], concatenate them into a = [as ; ac ] ∈ R2×|t |, and produce attention-weighted word embedding tatt of given text t, which is either an answer passage or a compact answer: tatt = ReLU(Wt t + Wa a) where Wt ∈ R2d×2d and Wa ∈ R2d×2 are trainable parameters, t is the representation of text t, and ReLU represents the rectified linear units. 3.3.3 CNNs tatt is given to CNNs to generate final representation rt of a given passage/compact-answer t. 4230 The CNNs resembles those in Kim (2014). Convolutions are performed over the word embeddings using both multiple filters and multiple filter windows (e.g., sliding over 1, 2, or 3 word windows at a time and 100 filters for each window). An average pooling operation is applied to the convolution results to generate representation rt , which is the output/value of Encoder(t; θ, q); rt = Encoder(t; θ, q). In our experiments, we set the dimension of representation rt to 300. 4 4.1 Why-QA Experiments Datasets We used three datasets, W hySet, CmpAns, and AddT r, for our why-QA experiments. W hySet and AddT r were used for training and ev"
P19-1414,P14-1093,1,0.860546,"Missing"
P19-1414,P18-1161,0,0.436382,"fake representations from the true representations derived from manually created compact-answers. Logistic regression +softmax+dropout ? R: Realrepresentation generator ?(?|?) Real D: Discriminator ? F: Fake? Realrepresentation representation generator Fake Fakerepresentation ?(?|?) (b) AGR and its three subnetworks F , R, and D Figure 1: System architecture pervised open-domain QA (DS-QA) task (Chen et al., 2017), which is an extension of a machinereading task, to check whether it is applicable to other datasets. We combined our generator network with a state-of-the-art DS-QA method, OpenQA (Lin et al., 2018), and used a generated compact-answer representation from a given passage as evidence to 1) select relevant passages from the retrieved ones and 2) find an answer from the selected passages. Although the task was not our initial target (why-QA) and the answers in the DS-QA task were considerably shorter than those in the why-QA, experiments using three publicly available datasets (Quasar-T (Dhingra et al., 2017), SearchQA (Dunn et al., 2017), and TriviaQA (Joshi et al., 2017)) revealed that the generator network improved the performance in most cases. This suggests that AGR may be applicable t"
P19-1414,C16-2055,1,0.851719,"e AGR, we used CmpAns, the training data set created in Ishida et al. (2018) for compact-answer generation; CmpAns consists of 15,130 triples of a why-question, an answer passage, and a manually-created compact answer. These cover 2,060 unique why-questions. Note that there was no overlap between the questions in CmpAns and those in W hySet. CmpAns was created in the following manner: 1) human annotators manually came up with open-domain whyquestions, 2) retrieved the top-20 passages for each why-question using the open-domain whyQA module of a publicly available web-based QA system WISDOM X (Mizuno et al., 2016; Oh et al., 2016), and 3) three annotators created (when possible) a compact answer for each of the retrieved passages. The passages for which no annotator could create a compact answer were discarded, and were not included in the 15,130 triples mentioned previously. The average lengths of questions, passages, and compact answers in CmpAns were 10.5 words, 184.4 words, and 8.3 words, respectively. Finally, we created additional training data AddT r for training the why-QA models. If an annotator could write a compact answer for a question and an answer passage, she/he probably recognized the"
P19-1414,D12-1034,1,0.888127,"Missing"
P19-1414,P13-1170,1,0.940027,"Missing"
P19-1414,D14-1162,0,0.0821098,"ose majority are noun phrases, unlike the compact answers for our why-QA experiment, i.e., sentences or phrases (8.3 words on average). We trained our AGR with all the triples of 4234 a question, an answer, and a paragraph in the training data of SQuAD-v1.1 under the same settings for the AGR’s hyperparameters as in our why-QA experiment except that we use neither causal word embeddings nor causality-attention. In this experiment, we used the AGR training schemes for Ours(OP) and Ours(RV). We used the 300-dimensional GloVe word embeddings learned from 840 billion tokens in the web crawl data (Pennington et al., 2014), as general word embeddings. Then we combined the resulting fake-representation generator F in the AGR with the state-of-the-art DS-QA method, OpenQA (Lin et al., 2018)7 . We also used the hyperparameters presented in Lin et al. (2018). OpenQA is composed of two components: a paragraph selector to choose relevant paragraphs (or answer passages in our terms) from a set of paragraphs and a paragraph reader to extract answers from the selected paragraphs. For identifying answer a to given question q from set of paragraphs P = {pi }, the paragraph selector and the paragraph reader respectively co"
P19-1414,D16-1264,0,0.0608345,"of a compact answer than on other words. We also in out computed {rorg i }, {ri }, and {ri } with fakerepresentation generator FOP in the same way and observed the same tendency. 5 DS-QA Experiments We tested our framework on another task, the distantly supervised open-domain question answering (DS-QA) task (Chen et al., 2017), to check its generalizability. Table 4 shows the statistics for the datasets used in this experiment. The first three, Quasar-T, SearchQA, and TriviaQA provided by Lin et al. (2018), were used for training and evaluating DS-QA methods. The training data of SQuAD v1.1 (Rajpurkar et al., 2016) was used for training our AGR. The SQuAD dataset consisted of the triples of a question, an answer, and a paragraph that includes the answer. We assume that the answers are our compact answers, although the answers in the dataset are consecutive short word sequences (2.8 words on average), whose majority are noun phrases, unlike the compact answers for our why-QA experiment, i.e., sentences or phrases (8.3 words on average). We trained our AGR with all the triples of 4234 a question, an answer, and a paragraph in the training data of SQuAD-v1.1 under the same settings for the AGR’s hyperparam"
P19-1414,D16-1014,0,0.398581,"minated under certain circumstances. Because its acidity, low water activity, and hydrogen peroxide together hinder the growth of microbes. Table 1: Answer passage A to why-question Q and its compact answer C Introduction Why-question answering (why-QA) tasks retrieve from a text archive answers to such why-questions as “Why does honey last such a long time?” Previous why-QA methods retrieve from a text archive answer passages, each of which consists of several sentences, like A in Table 1 (Girju, 2003; Higashinaka and Isozaki, 2008; Oh et al., 2012, 2013, 2016, 2017; dos Santos et al., 2016; Sharp et al., 2016; Tan et al., 2016; Verberne et al., 2011), and then determine whether the passages answer the question. A proper answer passage must contain (1) a paraphrase of the why-question (e.g., the underlined texts in Table 1) and (2) the reasons or the causes (e.g., the bold texts in Table 1) of the events described in the why-question, both of which are often written in multiple non-adjacent sentences. This multi-sentenceness implies that the answer passages often contain redundant parts that are not directly related to a why-question or its reason/cause and whose presence complicates the why-QA tas"
P19-1414,P16-1044,0,0.415181,"n circumstances. Because its acidity, low water activity, and hydrogen peroxide together hinder the growth of microbes. Table 1: Answer passage A to why-question Q and its compact answer C Introduction Why-question answering (why-QA) tasks retrieve from a text archive answers to such why-questions as “Why does honey last such a long time?” Previous why-QA methods retrieve from a text archive answer passages, each of which consists of several sentences, like A in Table 1 (Girju, 2003; Higashinaka and Isozaki, 2008; Oh et al., 2012, 2013, 2016, 2017; dos Santos et al., 2016; Sharp et al., 2016; Tan et al., 2016; Verberne et al., 2011), and then determine whether the passages answer the question. A proper answer passage must contain (1) a paraphrase of the why-question (e.g., the underlined texts in Table 1) and (2) the reasons or the causes (e.g., the bold texts in Table 1) of the events described in the why-question, both of which are often written in multiple non-adjacent sentences. This multi-sentenceness implies that the answer passages often contain redundant parts that are not directly related to a why-question or its reason/cause and whose presence complicates the why-QA task. Highly accurate"
tokunaga-etal-2012-rex,stoia-etal-2008-scare,0,\N,Missing
tokunaga-etal-2012-rex,W10-3206,1,\N,Missing
tokunaga-etal-2012-rex,W10-4214,1,\N,Missing
tokunaga-etal-2012-rex,W09-0618,1,\N,Missing
tokunaga-etal-2012-rex,P10-1128,1,\N,Missing
tokunaga-etal-2012-rex,J96-2004,0,\N,Missing
tokunaga-etal-2012-rex,byron-fosler-lussier-2006-osu,0,\N,Missing
W03-1602,P01-1008,0,0.0389931,"To create such a system, one needs to feed it with a large collection of paraphrase patterns. Very timely, the acquisition of paraphrase patterns has been actively studied in recent years:  Manual collection of paraphrases in the context of language generation, e.g. (Robin and McKeown, 1996),  Derivation of paraphrases through existing lexical resources, e.g. (Kurohashi et al., 1999),  Corpus-based statistical methods inspired by the work on information extraction, e.g. (Jacquemin, 1999; Lin and Pantel, 2001), and  Alignment-based acquisition of paraphrases from comparable corpora, e.g. (Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003). One remaining issue is how effectively these methods contribute to the generation of paraphrases in our application-oriented context. 2.3 Paraphrase representation One of the findings obtained in the previous studies for paraphrase acquisition is that the automatic acquisition of candidates of paraphrases is quite realizable for various types of source data but acquired collections tend to be rather noisy and need manual cleaning as reported in, for example, (Lin and Pantel, 2001). Given that, it turns out to be important to devise an effective"
W03-1602,N03-1003,0,0.0148725,"a large collection of paraphrase patterns. Very timely, the acquisition of paraphrase patterns has been actively studied in recent years:  Manual collection of paraphrases in the context of language generation, e.g. (Robin and McKeown, 1996),  Derivation of paraphrases through existing lexical resources, e.g. (Kurohashi et al., 1999),  Corpus-based statistical methods inspired by the work on information extraction, e.g. (Jacquemin, 1999; Lin and Pantel, 2001), and  Alignment-based acquisition of paraphrases from comparable corpora, e.g. (Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003). One remaining issue is how effectively these methods contribute to the generation of paraphrases in our application-oriented context. 2.3 Paraphrase representation One of the findings obtained in the previous studies for paraphrase acquisition is that the automatic acquisition of candidates of paraphrases is quite realizable for various types of source data but acquired collections tend to be rather noisy and need manual cleaning as reported in, for example, (Lin and Pantel, 2001). Given that, it turns out to be important to devise an effective way of facilitating manual correction and a sta"
W03-1602,P98-1056,0,0.15982,"lly expressible formalism for representing paraphrases at the level of tree-to-tree transformation and (b) devise an additional layer of representation on its top that is designed to facilitate handcoding transformation rules. 2.4 Post-transfer text revision In paraphrasing, the morpho-syntactic information of a source sentence should be accessible throughout the transfer process since a morphosyntactic transformation in itself can often be a motivation or goal of paraphrasing. Therefore, such an approach as semantic transfer, where morphosyntactic information is highly abstracted away as in (Dorna et al., 1998; Richardson et al., 2001), does not suit this task. Provided that the morphosyntactic stratum be an optimal level of abstraction for representing paraphrasing/transfer patterns, one must recall that semantic-transfer approaches such as those cited above were motivated mainly by the need for reducing the complexity of transfer knowledge, which could be unmanageable in morpho-syntactic transfer. Our approach to this problem is to (a) leave the description of each transfer pattern underspecified and (b) implement the knowledge about linguistic constraints that are independent of a particular tra"
W03-1602,W01-0814,1,0.810902,"Missing"
W03-1602,P99-1044,0,0.0104458,"ding assistance system is, therefore, hoped to be able to generate sufficient varieties of paraphrases of a given input. To create such a system, one needs to feed it with a large collection of paraphrase patterns. Very timely, the acquisition of paraphrase patterns has been actively studied in recent years:  Manual collection of paraphrases in the context of language generation, e.g. (Robin and McKeown, 1996),  Derivation of paraphrases through existing lexical resources, e.g. (Kurohashi et al., 1999),  Corpus-based statistical methods inspired by the work on information extraction, e.g. (Jacquemin, 1999; Lin and Pantel, 2001), and  Alignment-based acquisition of paraphrases from comparable corpora, e.g. (Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003). One remaining issue is how effectively these methods contribute to the generation of paraphrases in our application-oriented context. 2.3 Paraphrase representation One of the findings obtained in the previous studies for paraphrase acquisition is that the automatic acquisition of candidates of paraphrases is quite realizable for various types of source data but acquired collections tend to be rather noisy and need m"
W03-1602,P99-1062,0,0.025313,"Missing"
W03-1602,A00-1009,0,0.0218102,"Meaning-preserving vs. reference-preserving paraphrases It is also useful to distinguish reference-preserving paraphrases from meaningpreserving ones. The above example in (3) is of the reference-preserving type. This types of paraphrasing requires the computation of reference to objects outside discourse and thus should be excluded from our scope for the present purpose. 4.2 Dependency trees (MDSs) Previous work on transfer-based machine translation (MT) suggests that the dependency-based representation has the advantage of facilitating syntactic transforming operations (Meyers et al., 1996; Lavoie et al., 2000). Following this, we adopt dependency trees as the internal representations of target texts. We suppose that a dependency tree consists of a set of nodes each of which corresponds to a lexeme or compound and a set of edges each of which represents the dependency relation between its ends. We call such a dependency tree a morpheme-based dependency structure (MDS). Each node in an MDS is supposed to be annotated with an open set of typed features that indicate morpho-syntactic and semantic information. We also assume a type hierarchy in dependency relations that consists of an open set of depend"
W03-1602,W97-0508,0,0.0659332,"Missing"
W03-1602,C96-1078,0,0.0432722,"cope our discussion. Meaning-preserving vs. reference-preserving paraphrases It is also useful to distinguish reference-preserving paraphrases from meaningpreserving ones. The above example in (3) is of the reference-preserving type. This types of paraphrasing requires the computation of reference to objects outside discourse and thus should be excluded from our scope for the present purpose. 4.2 Dependency trees (MDSs) Previous work on transfer-based machine translation (MT) suggests that the dependency-based representation has the advantage of facilitating syntactic transforming operations (Meyers et al., 1996; Lavoie et al., 2000). Following this, we adopt dependency trees as the internal representations of target texts. We suppose that a dependency tree consists of a set of nodes each of which corresponds to a lexeme or compound and a set of edges each of which represents the dependency relation between its ends. We call such a dependency tree a morpheme-based dependency structure (MDS). Each node in an MDS is supposed to be annotated with an open set of typed features that indicate morpho-syntactic and semantic information. We also assume a type hierarchy in dependency relations that consists of"
W03-1602,P01-1051,0,0.0130024,"evision would make the system tolerant to flows in paraphrasing rules. While many researchers have addressed the issue of paraphrase acquisition reporting promising results as cited above, the other three issues have been left relatively unexplored in spite of their significance in the above sense. Motivated by this context, in the rest of this paper, we address these remaining three. 3 Readability assessment To the best of our knowledge, there have never been no reports on research to build a computational model of the language proficiency of deaf people, except for the remarkable reports by Michaud and McCoy (2001). As a subpart of their research aimed at developing the ICICLE system (McCoy and Masterman, 1997), a language-tutoring application for deaf learners of written English, Michaud and McCoy developed an architecture for modeling the writing proficiency of a user called SLALOM. SLALOM is designed to capture the stereotypic linear order of acquisition within certain categories of morphological and/or syntactic features of language. Unfortunately, the modeling method used in SLALOM cannot be directly applied to our domain for three reasons.  Unlike writing tutoring, in reading assistance, target s"
W03-1602,W03-2314,0,0.0708175,"Missing"
W03-1602,W03-2317,0,0.0406903,"ree subprocesses: a. Problem identification: identify which portions of a given text will be difficult for a given user to read, b. Paraphrase generation: generate possible candidate paraphrases from the identified portions, and c. Evaluation: re-assess the resultant texts to choose the one in which the problems have been resolved. Given this decomposition, it is clear that one of the key issues in reading assistance is the problem of assessing the readability or comprehensibility1 of text because it is involved in subprocesses (a) and (c). Readability assessment is doubtlessly a tough issue (Williams et al., 2003). In this project, however, we argue that, if one targets only a particular population segment and if an adequate collection of data is available, then corpus-based empirical approaches may well be feasible. We have already proven that one can collect such readability assessment data by conducting survey questionnaires targeting teachers at schools for the deaf. 1 In this paper, we use the terms readability and comprehensibility interchangeably, while strictly distinguishing them from legibility of each fragment (typically, a sentence or paragraph) of a given text. 2.2 Paraphrase acquisition O"
W03-1602,W01-1402,0,\N,Missing
W03-1602,C98-1054,0,\N,Missing
W07-1522,P06-1079,1,0.472904,"rpus version 4.0 (Kawahara et al., 2002) and the GDATagged Corpus (Hasida, 2005). However, there is still much room for reﬁning their speciﬁcations. For this reason, we discuss issues in annotating these two types of relations, and propose a new speciﬁcation for each. In accordance with the speciﬁcation, we built a large-scaled annotated corpus, and examined its reliability. As a result of our current work, we have released an annotated corpus named the NAIST Text Corpus1 , which is used as the evaluation data set in the coreference and zero-anaphora resolution tasks in Iida et al. (2005) and Iida et al. (2006). 1 Introduction Coreference resolution and predicate-argument structure analysis has recently been a growing ﬁeld of research due to the demands from NLP application such as information extraction and machine translation. With the research focus placed on these tasks, the speciﬁcation of annotating corpora and the 1 The NAIST Text Corpus is downloadable from http://cl.naist.jp/nldata/corpus/, and it has already been downloaded by 102 unique users. data sets used in supervised techniques (Soon et al., 2001; Ng and Cardie, 2002, etc.) have also grown in sophistication. For English, several anno"
W07-1522,kawahara-etal-2002-construction,0,0.390972,"Japanese Text Corpus with Predicate-Argument and Coreference Relations Ryu Iida, Mamoru Komachi, Kentaro Inui and Yuji Matsumoto Graduate School of Information Science, Nara Institute of Science and Technology 8916-5 Takayama, Ikoma, Nara, 630-0192, Japan {ryu-i,mamoru-k,inui,matsu}@is.naist.jp Abstract In this paper, we discuss how to annotate coreference and predicate-argument relations in Japanese written text. There have been research activities for building Japanese text corpora annotated with coreference and predicate-argument relations as are done in the Kyoto Text Corpus version 4.0 (Kawahara et al., 2002) and the GDATagged Corpus (Hasida, 2005). However, there is still much room for reﬁning their speciﬁcations. For this reason, we discuss issues in annotating these two types of relations, and propose a new speciﬁcation for each. In accordance with the speciﬁcation, we built a large-scaled annotated corpus, and examined its reliability. As a result of our current work, we have released an annotated corpus named the NAIST Text Corpus1 , which is used as the evaluation data set in the coreference and zero-anaphora resolution tasks in Iida et al. (2005) and Iida et al. (2006). 1 Introduction Coref"
W07-1522,P02-1014,0,0.136204,"ference and zero-anaphora resolution tasks in Iida et al. (2005) and Iida et al. (2006). 1 Introduction Coreference resolution and predicate-argument structure analysis has recently been a growing ﬁeld of research due to the demands from NLP application such as information extraction and machine translation. With the research focus placed on these tasks, the speciﬁcation of annotating corpora and the 1 The NAIST Text Corpus is downloadable from http://cl.naist.jp/nldata/corpus/, and it has already been downloaded by 102 unique users. data sets used in supervised techniques (Soon et al., 2001; Ng and Cardie, 2002, etc.) have also grown in sophistication. For English, several annotation schemes have already been proposed for both coreference relation and argument structure, and annotated corpora have been developed accordingly (Hirschman, 1997; Poesio et al., 2004; Doddington et al., 2004). For instance, in the Coreference task on Message Understanding Conference (MUC) and the Entity Detection and Tracking (EDT) task in the Automatic Content Extraction (ACE) program, which is the successor of MUC, the details of speciﬁcation of annotating coreference relation have been discussed for several years. On t"
W07-1522,J05-1004,0,0.129176,"notated corpora have been developed accordingly (Hirschman, 1997; Poesio et al., 2004; Doddington et al., 2004). For instance, in the Coreference task on Message Understanding Conference (MUC) and the Entity Detection and Tracking (EDT) task in the Automatic Content Extraction (ACE) program, which is the successor of MUC, the details of speciﬁcation of annotating coreference relation have been discussed for several years. On the other hand, the speciﬁcation of predicate-argument structure analysis has mainly been discussed in the context of the CoNLL shared task2 on the basis of the PropBank (Palmer et al., 2005). In parallel with these efforts, there have also been research activities for building Japanese text corpora annotated with coreference and predicate-argument relations such as the Kyoto Text Corpus version 4.0 (Kawahara et al., 2002) and the GDA3 -Tagged Corpus (Hasida, 2005). However, as we discuss in this paper, there is still much room for arguing and reﬁning the speciﬁcation of such sorts of semantic annotation. In fact, for neither of the above two corpora, the adequacy and reliability of the annotation scheme has been deeply examined. In this paper, we discuss how to annotate coreferen"
W07-1522,P04-1019,0,0.0132674,"uch as information extraction and machine translation. With the research focus placed on these tasks, the speciﬁcation of annotating corpora and the 1 The NAIST Text Corpus is downloadable from http://cl.naist.jp/nldata/corpus/, and it has already been downloaded by 102 unique users. data sets used in supervised techniques (Soon et al., 2001; Ng and Cardie, 2002, etc.) have also grown in sophistication. For English, several annotation schemes have already been proposed for both coreference relation and argument structure, and annotated corpora have been developed accordingly (Hirschman, 1997; Poesio et al., 2004; Doddington et al., 2004). For instance, in the Coreference task on Message Understanding Conference (MUC) and the Entity Detection and Tracking (EDT) task in the Automatic Content Extraction (ACE) program, which is the successor of MUC, the details of speciﬁcation of annotating coreference relation have been discussed for several years. On the other hand, the speciﬁcation of predicate-argument structure analysis has mainly been discussed in the context of the CoNLL shared task2 on the basis of the PropBank (Palmer et al., 2005). In parallel with these efforts, there have also been research a"
W07-1522,J01-4004,0,0.294742,"ata set in the coreference and zero-anaphora resolution tasks in Iida et al. (2005) and Iida et al. (2006). 1 Introduction Coreference resolution and predicate-argument structure analysis has recently been a growing ﬁeld of research due to the demands from NLP application such as information extraction and machine translation. With the research focus placed on these tasks, the speciﬁcation of annotating corpora and the 1 The NAIST Text Corpus is downloadable from http://cl.naist.jp/nldata/corpus/, and it has already been downloaded by 102 unique users. data sets used in supervised techniques (Soon et al., 2001; Ng and Cardie, 2002, etc.) have also grown in sophistication. For English, several annotation schemes have already been proposed for both coreference relation and argument structure, and annotated corpora have been developed accordingly (Hirschman, 1997; Poesio et al., 2004; Doddington et al., 2004). For instance, in the Coreference task on Message Understanding Conference (MUC) and the Entity Detection and Tracking (EDT) task in the Automatic Content Extraction (ACE) program, which is the successor of MUC, the details of speciﬁcation of annotating coreference relation have been discussed fo"
W07-1522,P06-2105,0,0.00720716,"ikely to be more cost-efﬁcient than semantic roles 134 because they are often explicitly marked by case markers. This fact also allows us to avoid the difﬁculties in deﬁning a label set. • In Japanese, the mapping from syntactic cases to semantic roles tends to be reasonably straightforward if a semantically rich lexicon of verbs like the VerbNet (Kipper et al., 2000) is available. • Furthermore, we have not yet found many NLP applications for which the utility of semantic roles is actually demonstrated. One may think of using semantic roles in textual inference as exempliﬁed by, for example, Tatu and Moldovan (2006). However, similar sort of inference may well be realized with syntactic cases as demonstrated in the information extraction and question answering literature. Taking these respects into account, we choose to label predicate-argument relations in terms of syntactic cases, which follows the annotation scheme adopted in the Kyoto Corpus. 3.2 Syntactic case alternation Once the level of syntactic cases is chosen for our annotation, another issue immediately arises, alteration of syntactic cases by syntactic transformations such as passivization and causativization. For example, sentence (5) is an"
W07-1522,W99-0213,0,0.0702454,"Missing"
W07-1522,M95-1005,0,0.107471,"Missing"
W07-1522,W04-2705,0,\N,Missing
W07-1522,M98-1029,0,\N,Missing
W09-0618,stoia-etal-2008-scare,0,0.363745,"Missing"
W09-0618,byron-fosler-lussier-2006-osu,0,\N,Missing
W09-3611,N04-1037,0,0.0150312,"Missing"
W09-3611,J00-3005,0,0.0982094,"Missing"
W09-3611,P02-1014,0,0.278408,"6.27 80.49 50.00 48.35 88.00 62.41 46.70 86.73 60.71 salient for increased performance. We also extended this list by adding a cosine-similarity metric between two noun phrases; it uses bag-ofwords to create a vector for each noun phrase (where each word is a term in the vector) to compute their similarity. The intuition behind this is that noun phrases with more similar surface forms should be more likely to corefer. We further optimized string recognition and plurality detection for handling citation-strings. See Table 3 for the full list of our features. While both (Soon et al., 2001) and (Ng and Cardie, 2002) induced decision trees (C5 and C4.5, respectively) we opted for using an SVM-based approach instead (Vapnik, 1998; Joachims, 1999). SVMs are known for being reliable and having good performance. Training the Coreference Resolver To create and train our coreference resolver, we used a combination of techniques as outlined originally by (Soon et al., 2001) and subsequently extended by (Ng and Cardie, 2002). Mimicking their approaches, we used the corpora provided for the MUC-7 coreference resolution task (LDC2001T02, 2001), which includes sets of newspaper articles, annotated with coreference r"
W09-3611,C08-1087,0,0.354515,"Missing"
W09-3611,W06-0804,0,0.15775,"Missing"
W09-3611,J01-4004,0,0.510177,"1 Sentence Eval. R P F 36.27 80.49 50.00 48.35 88.00 62.41 46.70 86.73 60.71 salient for increased performance. We also extended this list by adding a cosine-similarity metric between two noun phrases; it uses bag-ofwords to create a vector for each noun phrase (where each word is a term in the vector) to compute their similarity. The intuition behind this is that noun phrases with more similar surface forms should be more likely to corefer. We further optimized string recognition and plurality detection for handling citation-strings. See Table 3 for the full list of our features. While both (Soon et al., 2001) and (Ng and Cardie, 2002) induced decision trees (C5 and C4.5, respectively) we opted for using an SVM-based approach instead (Vapnik, 1998; Joachims, 1999). SVMs are known for being reliable and having good performance. Training the Coreference Resolver To create and train our coreference resolver, we used a combination of techniques as outlined originally by (Soon et al., 2001) and subsequently extended by (Ng and Cardie, 2002). Mimicking their approaches, we used the corpora provided for the MUC-7 coreference resolution task (LDC2001T02, 2001), which includes sets of newspaper articles, an"
W09-3611,W06-1613,0,0.407422,"m of information overload, a form of knowledge reduction may be necessary. Past research (Garfield et al., 1964; Small, 1973) has shown that citations contain a plethora of latent information available and that much can be gained by exploiting it. Indeed, there is a wealth of literature on topic-clustering, e.g. bibliographic coupling (Kessler, 1963), or cocitation analysis (Small, 1973). Subsequent research demonstrated that citations could be clustered on their quality, using keywords that appeared in the running-text of the citation (Weinstock, 1971; Nanba et al., 2000; Nanba et al., 2004; Teufel et al., 2006). Similarly, other work has shown the utility in the IR domain of ranking the relevance of cited papers by using supplementary index terms extracted from the content of citations in citing papers, including methods that search through a fixed character-length window (O’Connor, 1982; Bradshaw, 2003), or that focus solely on the sentence containing the citation (Ritchie et al., 2008) for It is clear that merit exists behind extraction of citations in running text. This paper proposes a new method for performing this task based on coreference-chains. To evaluate our method we created a corpus of"
W10-3206,gargett-etal-2010-give,0,0.0165687,"o participants, who worked together on a simple 2-D design task, buying and arranging furniture for two rooms. The COCONUT corpus is limited in annotations which describe symbolic object information such as object intrinsic attributes and location in discrete co-ordinates. As an initial work of constructing a corpus for collaborative tasks, the COCONUT corpus can be characterised as having a rather simple domain as well As for a multilingual aspect, all the above corpora are English. There have been several recent attempts at collecting multilingual corpora in situated domains. For instance, (Gargett et al., 2010) collected German and English corpora in the same setting. Their domain is similar to the QUAKE corpus. Van der Sluis et al. (2009) aim at a comparative study of referring expressions between English and Japanese. Their domain is still static at the moment. Our corpora aim at dealing with the dynamic nature of situated dialogues between very different languages, English and Japanese. 6 We called such expressions as action-mentioning expressions (AME) in our previous work. 44 eration, and evaluation in different tasks (puzzles) as well. We are planning to distribute the REX-J corpus family thro"
W10-3206,W09-0618,1,0.828626,"notations which describe symbolic object information such as object intrinsic attributes and location in discrete co-ordinates. As an initial work of constructing a corpus for collaborative tasks, the COCONUT corpus can be characterised as having a rather simple domain as well As for a multilingual aspect, all the above corpora are English. There have been several recent attempts at collecting multilingual corpora in situated domains. For instance, (Gargett et al., 2010) collected German and English corpora in the same setting. Their domain is similar to the QUAKE corpus. Van der Sluis et al. (2009) aim at a comparative study of referring expressions between English and Japanese. Their domain is still static at the moment. Our corpora aim at dealing with the dynamic nature of situated dialogues between very different languages, English and Japanese. 6 We called such expressions as action-mentioning expressions (AME) in our previous work. 44 eration, and evaluation in different tasks (puzzles) as well. We are planning to distribute the REX-J corpus family through GSK (Language Resources Association in Japan)8 , and the REX-E corpus from both University of Brighton and GSK. Table 7: The RE"
W10-3206,J95-3003,0,0.0876286,"he QUAKE and SCARE corpora, we allowed a comparatively large flexibility in the actions necessary for achieving the goal shape (i.e. flipping, turning and moving of puzzle pieces at different degrees), relative to the complexity of the domain. Providing this relatively larger freedom of actions to the participants together with the recording of detailed information allows for research into new aspects of referring expressions. Related work Over the last decade, with a growing recognition that referring expressions frequently appear in collaborative task dialogues (Clark and WilkesGibbs, 1986; Heeman and Hirst, 1995), a number of corpora have been constructed to study the nature of their use. This tendency also reflects the recognition that this area yields both challenging research topics as well as promising applications such as human-robot interaction (Foster et al., 2008; Kruijff et al., 2010). The COCONUT corpus (Di Eugenio et al., 2000) was collected from keyboard-dialogs between two participants, who worked together on a simple 2-D design task, buying and arranging furniture for two rooms. The COCONUT corpus is limited in annotations which describe symbolic object information such as object intrins"
W10-3206,W10-4214,1,0.485114,"Missing"
W10-3206,P10-1128,1,0.781753,"Missing"
W10-3206,stoia-etal-2008-scare,0,0.0932985,"erest (e.g. used in describing where the object is located in space). REs have attracted a great deal of attention in both language analysis and language generation research. In language analysis research, 1 2 http://www.nlpir.nist.gov/related projects/muc/ http://www.itl.nist.gov/iad/tests/ace/ 38 Proceedings of the 8th Workshop on Asian Language Resources, pages 38–46, c Beijing, China, 21-22 August 2010. 2010 Asian Federation for Natural Language Processing have been developed (Di Eugenio et al., 2000; Byron, 2005; van Deemter et al., 2006; Foster and Oberlander, 2007; Foster et al., 2008; Stoia et al., 2008; Spanger et al., 2009a; Belz et al., 2010). Unlike the corpora of MUC and ACE, many are collected from situated dialogues, and therefore include multimodal information (e.g. gestures and eye-gaze) other than just transcribed text (Martin et al., 2007). Foster and Oberlander (2007) emphasised that any corpus for language generation should include all possible contextual information at the appropriate granularity. Since constructing a dialogue corpus generally requires experiments for data collection, this kind of corpus tends to be small-scale compared with corpora for reference resolution. Ag"
W10-3206,W06-1420,0,0.0651162,"Missing"
W10-3206,P93-1007,0,0.0262941,"y the sequence of possible referents as its referent, if any are present. • All expressions appearing in muttering to oneself are excluded. Table 2 shows a list of attributes of referring expressions used in annotating the corpus. The rest of the 20 Japanese dialogues were annotated by two of the authors and discrepancies were resolved by discussion. Four English dialogues have been annotated so far by one of the authors. • The minimum span of a noun phrase including necessary information to identify a referent is annotated. The span might include repairs with their reparandum and disfluency (Nakatani and Hirschberg, 1993) if needed. 4 Preliminary corpus analysis We have already completed the Japanese corpus, which is named REX-J (2008-08), but only 4 out of 24 dialogues have been annotated for the English counterpart (REX-E (2010-03)). Table 3 shows a summary of the trials. The horizontal • Demonstrative adjectives are included in expressions. 42 lines divide the trials by pairs, “o” in the “success” column denotes that the trial was successfully completed in the time limit (15 minutes), and the “OP-REX” and “SV-REX” columns show the number of referring expressions used by the operator and the solver respectiv"
W10-3206,byron-fosler-lussier-2006-osu,0,\N,Missing
W10-4214,P08-2050,0,0.114127,"been a number of criticisms against this type of evaluation. (Reiter and Sripada, 2002) argue, for example, that generated text might be very different from a corpus but still achieve the specific communicative goal. An additional problem is that corpus-similarity metrics measure how well a system reproduces what speakers (or writers) do, while for most NLG systems ultimately the most important consideration is its effect on the human user (i.e. listener or reader). Thus (Khan et al., 2009) argues that “measuring human-likeness disregards effectiveness of these expressions”. Furthermore, as (Belz and Gatt, 2008) state “there are no significant correlations between intrinsic and extrinsic evaluation measures”, concluding that “similarity to human-produced reference texts is not necessarily indicative of quality as measured by human task performance”. From early on in the NLG community, taskbased extrinsic evaluations have been considered as the most meaningful evaluation, especially when having to convince people in other communities of the usefulness of a system (Reiter and Belz, 2009). Task performance evaluation is recognized as the “only known way to measure the effectiveness of NLG systems with r"
W10-4214,W09-0628,0,0.0117979,"generation of referring expressions has moved to dynamic domains such as situated dialog, e.g. (Jordan and Walker, 2005) and (Stoia et al., 2006). However, both of them carried out an intrinsic evaluation measuring corpus-similarity or asking evaluators to compare system output to expressions used by human (the right bottom quadrant in Figure 1). The construction of effective generation systems in the dynamic domain requires the implementation of an extrinsic task performance evaluation. There has been work on extrinsic evaluation of instructions in the dynamic domain on the GIVE-2 challenge (Byron et al., 2009), which is a task to generate instructions in a virtual world. It is based on the GIVE-corpus (Gargett et al., 2010), which is collected through keyboard interaction. The evaluation measures used are e.g. the number of successfully completed trials, completion time as well as the numbers of instructions the system sent to the user. As part of the JAST project, a Joint Construction Task (JCT) puzzle construction corpus (Foster et al., 2008) was created which is similar in some ways in its set-up to the REXJ corpus which we use in the current research. There has been some work on evaluating gene"
W10-4214,P06-1130,0,0.0309125,"Missing"
W10-4214,W08-1113,0,0.0328025,"Missing"
W10-4214,gargett-etal-2010-give,0,0.0630249,"2005) and (Stoia et al., 2006). However, both of them carried out an intrinsic evaluation measuring corpus-similarity or asking evaluators to compare system output to expressions used by human (the right bottom quadrant in Figure 1). The construction of effective generation systems in the dynamic domain requires the implementation of an extrinsic task performance evaluation. There has been work on extrinsic evaluation of instructions in the dynamic domain on the GIVE-2 challenge (Byron et al., 2009), which is a task to generate instructions in a virtual world. It is based on the GIVE-corpus (Gargett et al., 2010), which is collected through keyboard interaction. The evaluation measures used are e.g. the number of successfully completed trials, completion time as well as the numbers of instructions the system sent to the user. As part of the JAST project, a Joint Construction Task (JCT) puzzle construction corpus (Foster et al., 2008) was created which is similar in some ways in its set-up to the REXJ corpus which we use in the current research. There has been some work on evaluating generation strategies of instructions for a collaborative construction task on this corpus, both considering intrinsic a"
W10-4214,W09-0629,0,0.0313814,"Missing"
W10-4214,W09-0615,0,0.0520131,"Missing"
W10-4214,P09-2076,0,0.0136793,"luation measures”, concluding that “similarity to human-produced reference texts is not necessarily indicative of quality as measured by human task performance”. From early on in the NLG community, taskbased extrinsic evaluations have been considered as the most meaningful evaluation, especially when having to convince people in other communities of the usefulness of a system (Reiter and Belz, 2009). Task performance evaluation is recognized as the “only known way to measure the effectiveness of NLG systems with real users” (Reiter et al., 2003). Following this direction, the GIVE-Challenges (Koller et al., 2009) at INLG 2010 (instruction generation) also include a taskperformance evaluation. In contrast to the vertical axis of Figure 1, there is the horizontal axis of the domain in which referring expressions are used. Referring expressions can thus be distinguished according to whether they are used in a static or a dynamic domain, corresponding to the left and right of the horizontal axis of Figure 1. A static domain is one such as the TUNA corpus (van Deemter, 2007), which collects referring expressions based on a motionless image. In contrast, a dynamic domain comprises a constantly changing situ"
W10-4214,P02-1040,0,0.0850087,"verview of recent work on evaluation of referring expressions Figure 1 shows a schematic overview of recent work on evaluation of referring expressions along the two axes of evaluation method and domain in which referring expressions are used. There are two different evaluation methods corresponding to the bottom and the top of the vertical axis in Figure 1: intrinsic and extrinsic evaluations (Sparck Jones and Galliers, 1996). Intrinsic methods often measure similarity between the system output and the gold standard corpora using metrics such as tree similarity, string-editdistance and BLEU (Papineni et al., 2002). Intrinsic methods have recently become popular in the NLG community. In contrast, extrinsic methods evaluate generated expressions based on an external metric, such as its impact on human task performance. While intrinsic evaluations have been widely used in NLG, e.g. (Reiter et al., 2005), (Cahill and van Genabith, 2006) and the competitive 2009 TUNA-Challenge, there have been a number of criticisms against this type of evaluation. (Reiter and Sripada, 2002) argue, for example, that generated text might be very different from a corpus but still achieve the specific communicative goal. An ad"
W10-4214,W06-1409,0,0.0635106,"Missing"
W10-4214,J09-4008,0,0.0146143,"n et al., 2009) argues that “measuring human-likeness disregards effectiveness of these expressions”. Furthermore, as (Belz and Gatt, 2008) state “there are no significant correlations between intrinsic and extrinsic evaluation measures”, concluding that “similarity to human-produced reference texts is not necessarily indicative of quality as measured by human task performance”. From early on in the NLG community, taskbased extrinsic evaluations have been considered as the most meaningful evaluation, especially when having to convince people in other communities of the usefulness of a system (Reiter and Belz, 2009). Task performance evaluation is recognized as the “only known way to measure the effectiveness of NLG systems with real users” (Reiter et al., 2003). Following this direction, the GIVE-Challenges (Koller et al., 2009) at INLG 2010 (instruction generation) also include a taskperformance evaluation. In contrast to the vertical axis of Figure 1, there is the horizontal axis of the domain in which referring expressions are used. Referring expressions can thus be distinguished according to whether they are used in a static or a dynamic domain, corresponding to the left and right of the horizontal"
W10-4214,W02-2113,0,0.014679,"e similarity between the system output and the gold standard corpora using metrics such as tree similarity, string-editdistance and BLEU (Papineni et al., 2002). Intrinsic methods have recently become popular in the NLG community. In contrast, extrinsic methods evaluate generated expressions based on an external metric, such as its impact on human task performance. While intrinsic evaluations have been widely used in NLG, e.g. (Reiter et al., 2005), (Cahill and van Genabith, 2006) and the competitive 2009 TUNA-Challenge, there have been a number of criticisms against this type of evaluation. (Reiter and Sripada, 2002) argue, for example, that generated text might be very different from a corpus but still achieve the specific communicative goal. An additional problem is that corpus-similarity metrics measure how well a system reproduces what speakers (or writers) do, while for most NLG systems ultimately the most important consideration is its effect on the human user (i.e. listener or reader). Thus (Khan et al., 2009) argues that “measuring human-likeness disregards effectiveness of these expressions”. Furthermore, as (Belz and Gatt, 2008) state “there are no significant correlations between intrinsic and"
W10-4214,W09-0618,1,0.262804,"next referring expression (go to (1)). Figure 3 shows a screenshot of the interface prepared for this experiment. The test data consists of three types of referring expressions: DPs (demonstrative pronouns), AMEs (action-mentioning expressions), and OTHERs (any other expression that is neither a DP nor AME, e.g intrinsic attributes and spatial relations). DPs are the most frequent type of referring expression in the corpus. AMEs are expressions that utilize an action on the referent such as “the triangle you put away to the top right” (see Table 1)1 . As we pointed out in our previous paper (Spanger et al., 2009a), they are also a fundamental type of referring expression in this domain. The basic question in investigating a suitable context is what information to consider about the preceding interaction; i.e. over what parameters to vary the context. In previous work on the generation of demonstrative pronouns in a situated domain (Spanger et al., 2009b), we investigated the role of linguistic and extra-linguistic information, and found that time distance from the last action (LA) on the referent as well as the last mention (LM) to the referent had a significant influence on the usage of referring ex"
W10-4214,W06-1412,0,0.0301277,"ntrinsic evaluation is (van der Sluis et al., 2007), who employed the Dice-coefficient measuring corpus-similarity. There have been a number of extrinsic evaluations as well, such as (Paraboni et al., 2006) and (Khan et al., 2009), respectively measuring the effect of overspecification on task performance and the impact of generated text on accuracy as well as processing speed. They belong thus in the top-left quadrant of Figure 1. Over a recent period, research in the generation of referring expressions has moved to dynamic domains such as situated dialog, e.g. (Jordan and Walker, 2005) and (Stoia et al., 2006). However, both of them carried out an intrinsic evaluation measuring corpus-similarity or asking evaluators to compare system output to expressions used by human (the right bottom quadrant in Figure 1). The construction of effective generation systems in the dynamic domain requires the implementation of an extrinsic task performance evaluation. There has been work on extrinsic evaluation of instructions in the dynamic domain on the GIVE-2 challenge (Byron et al., 2009), which is a task to generate instructions in a virtual world. It is based on the GIVE-corpus (Gargett et al., 2010), which is"
W12-1633,P02-1011,0,0.100083,"Missing"
W12-1633,W09-0609,0,0.059174,"he REX-J corpus shows promising results. 1 Introduction Referring expressions (REs) are expressions intended by speakers to identify entities to hearers. REs can be classified into three categories: descriptions, anaphora, and deixis; and, in most cases, have been studied within each category and with a narrowly focused interest. Descriptive expressions (such as “the blue glass on the table”) exploit attributes of entities and relations between them to distinguish an entity from the rest. They are well studied in natural language generation, e.g., (Dale and Reiter, 1995; Krahmer et al., 2003; Dale and Viethen, 2009). Anaphoric expressions (such as “it”) refer to entities or concepts introduced in the preceding discourse and are studied mostly on textual monologues, e.g., (Kamp and Reyle, 1993; Mitkov, 2002; Ng, 2010). Deictic (exophoric) expressions (such as “this one”) refer to entities outside the preceding discourse. They are often studied focusing on pronouns accompanied with pointing gestures in physical spaces, e.g., (Gieselmann, 2004). Dialogue systems (DSs) as natural humanmachine (HM) interfaces are expected to handle all the three categories of referring expressions (Salmon-Alt and Romary, 2001"
W12-1633,W10-4203,0,0.263673,"ndent attributes. In addition, by treating a reference domain as a referent, REs referring to sets of entities are handled, too. As far as the authors know, this work is the first that takes a probabilistic approach to reference domains. 237 Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 237–246, c Seoul, South Korea, 5-6 July 2012. 2012 Association for Computational Linguistics 1.1 Reference domains First, we explain reference domains concretely. Reference domains (RDs) (Salmon-Alt and Romary, 2000; Salmon-Alt and Romary, 2001; Denis, 2010) are theoretical constructs, which are basically sets of entities presupposed at each use of REs. RDs in the original literature are not mere sets of entities but mental objects equipped with properties such as type, focus, or saliency and internally structured with partitions. In this paper, while we do not explicitly handle partitions, reference domains can be nested as an approximation of partitioning, that is, an entity included in a RD is either an individual entity or another RD. Each RD d has its focus and degree of saliency (a non-negative real number). Hereafter, two of them are denot"
W12-1633,P10-1128,1,0.172335,"We employ a Bayesian network (BN) to model a RE. Dealing with continuous information and vague situations is critical to handle real world problems. Probabilistic approaches enable this for reference resolvers. Each BN is dynamically constructed based on the structural analysis result of a RE and contextual information available at that moment. The BN is used to estimate the probability with which the corresponding RE refers to an entity. One of the two major contributions of this paper is our probabilistic formulation that handles the above three kinds of REs in a unified manner. Previously Iida et al. (2010) proposed a quantitative approach that handles anaphoric and deictic expressions in a unified manner. However it lacks handling of descriptive expressions. Our formulation subsumes and extends it to handle descriptive REs. So far, no previously proposed method for reference resolution handles all three types of REs. The other contribution is bringing reference domains into that formulation. Reference domains (Salmon-Alt and Romary, 2000) are sets of referents implicitly presupposed at each use of REs. By considering them, our approach can appropriately interpret context-dependent attributes. I"
W12-1633,J03-1003,0,0.11197,"Missing"
W12-1633,P07-1103,0,0.0586855,"Missing"
W12-1633,P10-1142,0,0.0213797,"ra, and deixis; and, in most cases, have been studied within each category and with a narrowly focused interest. Descriptive expressions (such as “the blue glass on the table”) exploit attributes of entities and relations between them to distinguish an entity from the rest. They are well studied in natural language generation, e.g., (Dale and Reiter, 1995; Krahmer et al., 2003; Dale and Viethen, 2009). Anaphoric expressions (such as “it”) refer to entities or concepts introduced in the preceding discourse and are studied mostly on textual monologues, e.g., (Kamp and Reyle, 1993; Mitkov, 2002; Ng, 2010). Deictic (exophoric) expressions (such as “this one”) refer to entities outside the preceding discourse. They are often studied focusing on pronouns accompanied with pointing gestures in physical spaces, e.g., (Gieselmann, 2004). Dialogue systems (DSs) as natural humanmachine (HM) interfaces are expected to handle all the three categories of referring expressions (Salmon-Alt and Romary, 2001). In fact, the three categories are not mutually exclusive. To be concrete, a descriptive expression in conversation is either deictic or anaphoric. It is, however, not easy to tell whether a RE is deicti"
W13-0508,P10-1118,0,0.116178,"dates of predicates and arguments were marked beforehand in the annotation tool, the annotators were instructed to add links between correct predicateargument pairs by using the keyboard and mouse. We distinguished three types of links based on the case marker of arguments, i.e. ga (nominative), o (accusative) and ni (dative). For elliptical arguments 81 Number of instances 50 fin and Bock, 2000; Richardson et al., 2007). Compared to the studies on language and eye-gaze, the role of gaze in general problem solving settings has been less studied (Bednarik and Tukiainen, 2008; Rosengrant, 2010; Tomanek et al., 2010). Since our current interest, corpus annotation, can be considered as a problem solving as well as language comprehension, various existing metrics derived from eye-tracking data would be useful. Rosengrant (2010) proposed an analysis method named gaze scribing where eye-tracking data is combined with subjects thought process derived by the TAP, underlining the importance of applying gaze scribing to various problem solving. Tomanek et al. (2010) utilised eye-tracking data to evaluate difficulties of named entities for selecting training instances for active learning techniques. Our analysis i"
W13-0508,J96-2004,0,0.36999,"as 1, 280 × 1, 024 pixels and the distance between the display and the annotator’s eye was maintained at about 50 cm. In order to minimise the head movement, we used a chin rest. The following are potential uses of the collected data. Finding useful information for NLP Given a certain task, the collected data would give some hints to understand human decision processes. Therefore, the information can be useful for replicating human decisions by using ML-based approaches. Evaluating annotation quality The quality of corpora is often evaluated based on the agreement ratio and the κ coefficient (Carletta, 1996) between multiple annotators. Analysing the collected data would help to estimate the realiability of each annotation instance. For instance, the long annotation time for an instance is an indication of its difficulty, therefore the annotation on such an instance might be less reliable. Evaluating and training annotators Unlike the quality of corpora, the quality of annotators is rarely discussed. In addition to the extent to which annotators can replicate the gold standard annotation (result-based metric), the collected data can be used for evaluating annotators by comparing their behaviours"
W13-0508,N10-1061,0,0.0149646,"collect the annotator’s rationales behind the annotation process. 79 From an engineering viewpoint, a machine does not need to perform a task in the same manner as a human does. The currently used clues might be sufficient for doing the job even though a human uses different information. For instance, POS tagging and parsing are successful instances of the CCML approach. However, this approach does not always work well on other tasks such as semantic and discourse processing. For instance, the performance of the state-of-the-art coreference resolution model still stays around 0.7 in F-score (Haghighi and Klein, 2010). Furthermore, the performance of zero anaphora resolution in Japanese is much worse, around 0.4 in F-score (Iida and Poesio, 2011). Such relatively low performance suggests that some crucial information should have been utilised for ML techniques. Against this background, we propose annotating each annotation with the annotator’s rationale behind her/his decision. Since the rationale explains the validity of the annotation instance, it can be considered as a kind of meta-level annotation against the object-level annotation rather than a mere attribute of the annotation. We expect that analysi"
W13-0508,P11-1081,1,0.79637,"m a task in the same manner as a human does. The currently used clues might be sufficient for doing the job even though a human uses different information. For instance, POS tagging and parsing are successful instances of the CCML approach. However, this approach does not always work well on other tasks such as semantic and discourse processing. For instance, the performance of the state-of-the-art coreference resolution model still stays around 0.7 in F-score (Haghighi and Klein, 2010). Furthermore, the performance of zero anaphora resolution in Japanese is much worse, around 0.4 in F-score (Iida and Poesio, 2011). Such relatively low performance suggests that some crucial information should have been utilised for ML techniques. Against this background, we propose annotating each annotation with the annotator’s rationale behind her/his decision. Since the rationale explains the validity of the annotation instance, it can be considered as a kind of meta-level annotation against the object-level annotation rather than a mere attribute of the annotation. We expect that analysing these rationales behind human decisions reveals more effective information for a given task that has never been used by existing"
W13-0508,J10-2007,0,0.013271,"ing the annotation and discusses their potential uses. Finally a preliminary experiment for data collection is described with the data analysis. 1 The primacy of the CC-ML approach over the rule-based approach has been shown in fundamental NLP tasks (e.g. POS tagging, syntactic parsing and word sense disambiguation) as well as in various applications (e.g. information extraction, machine translation and summarisation) through prevalent competition-type conferences. However, too much dominance of the revived empiricism has recently worried a number of researchers (Reiter, 2007; Steedman, 2008; Krahmer, 2010; Church, 2011). For instance, Church (2011), who is one of the initiators of the revived empiricism, warned us that we should follow the CC-ML approach with an awareness of the limitations of the underlying ML techniques. Introduction The last two decades witnessed a great success of revived empiricism in NLP research. Namely, the corpus construction and machine learning (CC-ML) approach has been the main stream of NLP research, where corpora are annotated for a specific task and then they are used as training data for machine learning (ML) techniques to build a system for the task. The CC-ML"
W13-0508,maekawa-etal-2010-design,0,\N,Missing
W13-0508,J07-2013,0,\N,Missing
W13-2118,W11-2826,0,0.0220204,"k, passivisation is performed by taking into account both linguistic and extra-linguistic information. The linguistic information explains passivisation in an incremental generation process; realising the most salient discourse entity in short term memory as a subject eventually leads to passivisation. In contrast, extra-linguistic information is used to move a less salient entity to a subject position when an explicit agent is missing in the text. Although these two kinds of information seem adequate for explaining passivisation, their applicability was not examined in empirical evaluations. Sheikha and Inkpen (2011) focused attention on voice selection in the generation task distinguishing formal and informal sentences. In their work, passivisation is considered as a rhetorical technique for conveying formal intentions. However, they did not discuss passivisation in terms of discourse coherence. 3 where vi is a verb in question1 , f reqall (vi ) is the frequency of vi appearing in corpora, and f reqpas (vi ) is the frequency of vi with the passive marker, (ra)reru. The logarithm of f reqall (vi ) is multiplied due to avoiding the overestimation of the score for less frequent instances. In the evaluation,"
W13-2118,E93-1002,0,0.495819,"se Japanese journalists tend to write their opinions objectively by omitting the agent role. To take into account this preference of verb passivisation, we define a preference score by the following formula: f reqpas (vi ) scorepas (vi ) = ·log f reqall (vi ) (1) f reqall (vi ) Related work The task of automatic voice selection has been mainly developed in the NLG community. However, it has attracted less attention compared with other major NLG problems, such as generating referring expressions. There is less work focusing singly on voice selection, but not entirely without exception, such as Abb et al. (1993). In their work, passivisation is performed by taking into account both linguistic and extra-linguistic information. The linguistic information explains passivisation in an incremental generation process; realising the most salient discourse entity in short term memory as a subject eventually leads to passivisation. In contrast, extra-linguistic information is used to move a less salient entity to a subject position when an explicit agent is missing in the text. Although these two kinds of information seem adequate for explaining passivisation, their applicability was not examined in empirical"
W13-2118,J95-2003,0,0.0289711,") in the preceding context. P stands for the predicate in question. The four feature types (PRED, SYN, ARG and COREF) correspond to each information described in Section 3. Table 1: Feature set for voice selection named entity labels provided by CaboCha, such as Person and Organisation, and the ontological information defined in a Japanese ontology, nihongo goi taikei (Ikehara et al., 1997)) as features. (in this case, the agent filler) of a predicate is exophoric, the passive voice is selected. Coreference and anaphora of arguments As discussed in discourse theories such as Centering Theory (Grosz et al., 1995), arguments which have been already most salient in the preceding context tend to be placed at the beginning of a sentence for reducing the cognitive cost of reading, as argued in Functional Grammar (Halliday and Matthiessen, 2004). In order to consider the characteristic, we employ an extension of Centering Theory (Grosz et al., 1995), proposed by Nariyama (2002) for implementing the COREF type features in Table 1. She proposed a generalised version of the forward looking-center list, called the Salient Reference List (SRL), which stores all salient discourse entities (e.g. NP) in the precedi"
W13-2118,W07-1522,1,0.751044,"take into account the following four information as features The details of the feature set are shown in Table 1. research tackles the problem of voice selection considering a wide range of linguistic information that is assumed to be already decided in the preceding stages of a generation process. The paper is organised as follows. We first overview the related work in Section 2, and then propose a voice selection model based on the four kinds of information that impact voice selection in Section 3. Section 4 then demonstrates the results of empirical evaluation using the NAIST Text Corpus (Iida et al., 2007) as training and evaluation data sets. Finally, Section 5 concludes and discusses our future directions. 2 Passivisation preference of each verb An important factor of voice selection is the preference for how frequently a verb is used in passive sentences. This means each verb has a potential tendency of being used in passive sentences in a domain. For example, the verb ‘yosou-suru (to expect)’ tends to be realised in the passive in the newspaper domain because Japanese journalists tend to write their opinions objectively by omitting the agent role. To take into account this preference of ver"
W13-2118,2002.tmi-papers.15,0,0.0378611,"(Ikehara et al., 1997)) as features. (in this case, the agent filler) of a predicate is exophoric, the passive voice is selected. Coreference and anaphora of arguments As discussed in discourse theories such as Centering Theory (Grosz et al., 1995), arguments which have been already most salient in the preceding context tend to be placed at the beginning of a sentence for reducing the cognitive cost of reading, as argued in Functional Grammar (Halliday and Matthiessen, 2004). In order to consider the characteristic, we employ an extension of Centering Theory (Grosz et al., 1995), proposed by Nariyama (2002) for implementing the COREF type features in Table 1. She proposed a generalised version of the forward looking-center list, called the Salient Reference List (SRL), which stores all salient discourse entities (e.g. NP) in the preceding contexts in the order of their saliency. A highly ranked argument’s entity in the SRL tends to be placed in the subject position, resulting in a passive sentence if that salient entity has a THEME role in the predicateargument structure. To capture this characteristic, the order and rank of discourse entities in the SRL are used as features5 . In addition, as d"
W13-2118,J98-3005,0,0.036923,"paper, we propose an automatic voice selection model based on various linguistic information, ranging from lexical to discourse information. Our empirical evaluation using a manually annotated corpus in Japanese demonstrates that the proposed model achieved 0.758 in F-score, outperforming the two baseline models. 1 The realisation from a semantic representation (e.g. predicate argument structures) to an actual text has been mainly developed in the area of natural language generation (Reiter and Dale, 2000), and has been applied to various NLP applications such as multi-document summarisation (Radev and McKeown, 1998) and tutoring systems (Di Eugenio et al., 2005). During the course of a text generation process, various kinds of decisions should be made, including decisions on textual content, clustering the content of each clause, discourse structure of the clauses, lexical choices, types of referring expressions and syntactic structures. Since these different kinds of decisions are interrelated to each other, it is not a trivial problem to find an optimal order among these decisions. This issue has been much discussed in terms of architecture of generation systems. Although a variety of architectures has"
W13-2118,W94-0319,0,0.0239267,"choices, types of referring expressions and syntactic structures. Since these different kinds of decisions are interrelated to each other, it is not a trivial problem to find an optimal order among these decisions. This issue has been much discussed in terms of architecture of generation systems. Although a variety of architectures has been proposed in the past, e.g. an integrated architecture (Appelt, 1985) and a revision-based architecture (Inui et al., 1994; Robin, 1994), a pipeline architecture is considered as a consensus architecture in which decisions are made in a predetermined order (Reiter, 1994). Voice selection is a syntactic decision that tends to be made in a later stage of the pipeline architecture, even though it influences various decisions, such as discourse structure and lexical choice. Unlike referring expression generation, voice selection has received less attention and been less discussed in the past. Against this background, this Introduction Generating a readable text is the primary goal in natural language generation (NLG). To realise such text, we need to arrange discourse entities (e.g. NPs) in appropriate positions in a sentence according to their discourse salience"
W13-2326,P09-1075,0,0.0200399,"contrast, in semantic and discourse processing, such as coreference resolution and discourse structure analysis, it is not trivial to employ as features deeper linguistic knowledge and human linguistic intuition that are indispensable for these tasks. In order to improve system performance, past attempts have integrated deeper linguistic knowledge through manually constructed linguistic resources such as WordNet (Miller, 1995) and linguistic theories such as Centering Theory (Grosz et al., 1995). They partially succeed in improving performance, but there is still room for further improvement (duVerle and Prendinger, 2009; Ng, 2010; Lin et al., 2010; Pradhan et al., 2012). This paper presents an analysis of an annotator’s behaviour during her/his annotation process for eliciting useful information for natural language processing (NLP) tasks. Text annotation is essential for machine learning-based NLP where annotated texts are used for both training and evaluating supervised systems. Since an annotator’s behaviour during annotation can be seen as reflecting her/his cognitive process during her/his attempt to understand the text for annotation, analysing the process of text annotation has potential to reveal use"
W13-2326,P10-1142,0,0.02903,"ourse processing, such as coreference resolution and discourse structure analysis, it is not trivial to employ as features deeper linguistic knowledge and human linguistic intuition that are indispensable for these tasks. In order to improve system performance, past attempts have integrated deeper linguistic knowledge through manually constructed linguistic resources such as WordNet (Miller, 1995) and linguistic theories such as Centering Theory (Grosz et al., 1995). They partially succeed in improving performance, but there is still room for further improvement (duVerle and Prendinger, 2009; Ng, 2010; Lin et al., 2010; Pradhan et al., 2012). This paper presents an analysis of an annotator’s behaviour during her/his annotation process for eliciting useful information for natural language processing (NLP) tasks. Text annotation is essential for machine learning-based NLP where annotated texts are used for both training and evaluating supervised systems. Since an annotator’s behaviour during annotation can be seen as reflecting her/his cognitive process during her/his attempt to understand the text for annotation, analysing the process of text annotation has potential to reveal useful inform"
W13-2326,W12-4501,0,0.0170225,"reference resolution and discourse structure analysis, it is not trivial to employ as features deeper linguistic knowledge and human linguistic intuition that are indispensable for these tasks. In order to improve system performance, past attempts have integrated deeper linguistic knowledge through manually constructed linguistic resources such as WordNet (Miller, 1995) and linguistic theories such as Centering Theory (Grosz et al., 1995). They partially succeed in improving performance, but there is still room for further improvement (duVerle and Prendinger, 2009; Ng, 2010; Lin et al., 2010; Pradhan et al., 2012). This paper presents an analysis of an annotator’s behaviour during her/his annotation process for eliciting useful information for natural language processing (NLP) tasks. Text annotation is essential for machine learning-based NLP where annotated texts are used for both training and evaluating supervised systems. Since an annotator’s behaviour during annotation can be seen as reflecting her/his cognitive process during her/his attempt to understand the text for annotation, analysing the process of text annotation has potential to reveal useful information for NLP tasks, in particular semant"
W13-2326,J95-2003,0,0.158349,"e information like words and their POS within a window of a certain size can be easily employed as useful features. In contrast, in semantic and discourse processing, such as coreference resolution and discourse structure analysis, it is not trivial to employ as features deeper linguistic knowledge and human linguistic intuition that are indispensable for these tasks. In order to improve system performance, past attempts have integrated deeper linguistic knowledge through manually constructed linguistic resources such as WordNet (Miller, 1995) and linguistic theories such as Centering Theory (Grosz et al., 1995). They partially succeed in improving performance, but there is still room for further improvement (duVerle and Prendinger, 2009; Ng, 2010; Lin et al., 2010; Pradhan et al., 2012). This paper presents an analysis of an annotator’s behaviour during her/his annotation process for eliciting useful information for natural language processing (NLP) tasks. Text annotation is essential for machine learning-based NLP where annotated texts are used for both training and evaluating supervised systems. Since an annotator’s behaviour during annotation can be seen as reflecting her/his cognitive process du"
W13-2326,W07-1522,1,0.830245,"Missing"
W13-2326,N09-1059,0,0.0271494,"of arguments and dispersal of eye gaze Existing Japanese corpora annotated with predicate-argument relations (Iida et al., 2007; Kawahara et al., 2002) have had syntactic heads (nouns) of their projected NPs related their predicates. Since Japanese is a head-final language, a head noun is always placed in the last position of an NP. This scheme has the advantage that predicate-argument relations can be annotated without identifying the starting boundary of the argument NP under consideration. The scheme is also reflected in the structure of automatically constructed Japanese case frames, e.g. Sasano et al. (2009), which consist of triplets in the form of hNoun, Case, Verbi. Noun is a head noun extracted from its projected NP in the original text. We followed this scheme in our annotation experiments. However, a head noun of an argument does not always have enough information. A nominaliser which often appears in the head position in an NP does not have any semantic meaning by itself. For instance, in the NP “benkyˆo suru koto (to study/studying)”, the head noun “koto” has no specific semantic meaning, corresponding to an English morpheme “to” or “-ing”. In such cases, inspecting a whole NP including i"
W13-2326,kawahara-etal-2002-construction,0,0.111046,"Missing"
W13-2326,W13-0508,1,0.832708,"ntity annotation task, our annotation task, annotating predicate-argument relations, is more complex. In addition, our experimental setting is more natural, meaning that all possible relations in a text were annotated in a single session, while each session targeted a single named entity (NE) in a limited context in the setting of Tomanek et al. (2010). Finally, our fixation target is more precise, i.e. words, rather than a coarse area around the target NE. We have also discussed evaluating annotation difficulty for predicate-argument relations by using the same data introduced in this paper (Tokunaga et al., 2013). Through manual analysis of the collected data, we suggested that an annotation time necessary for annotating a single predicateargument relation was correlated with the agreement ratio among multiple human annotators. tions in Japanese texts. The collected data were analysed from three aspects: (i) the relationship of predicate-argument distances and argument’s cases, (ii) the effect of already-existing links and (iii) specificity of arguments and dispersal of eye gaze. The analysis on these aspects suggested that obtained insight into human annotation behaviour could be useful for exploring"
W13-2326,P10-1118,0,0.391111,"tic knowledge. Particularly we focus on annotator eye gaze during annotation. Because of recent developments in eye-tracking technology, eye gaze data has been widely used in various research fields, including psycholinguistics and problem solving (Duchowski, 2002). There have been a number of studies on the relations between eye gaze and language comprehension/production (Griffin and Bock, 2000; Richardson et al., 2007). Compared to the studies on language and eye gaze, the role of gaze in general problem solving settings has been less studied (Bednarik and Tukiainen, 2008; Rosengrant, 2010; Tomanek et al., 2010). Since our current interest, text annotation, can be considered a problem solving as well as language comprehension task, we refer to them when defining our probIntroduction Text annotation is essential for machine learning (ML)-based natural language processing (NLP) where annotated texts are used for both training and evaluating supervised systems. This annotation-then-learning approach has been broadly applied to various NLP tasks, ranging from shallow processing tasks, such as POS tagging and NP chunking, to tasks requiring deeper linguistic information, such as coreference resolution and"
W13-2326,maekawa-etal-2010-design,0,\N,Missing
W13-4303,J08-4004,0,0.114752,"nt using Eye Gaze Information Koh Mitsuda Ryu Iida Takenobu Tokunaga Department of Computer Science, Tokyo Institute of Technology {mitsudak,ryu-i,take}@cl.cs.titech.ac.jp Abstract ing better annotation tools (Kaplan et al., 2012; Lenzi et al., 2012; Marci´nczuk et al., 2012). The assessment of annotation quality is also an important issue in corpus building. The annotation quality is often evaluated with the agreement ratio among annotation results by multiple independent annotators. Various metrics for measuring reliability of annotation have been proposed (Carletta, 1996; Passonneau, 2006; Artstein and Poesio, 2008; Fort et al., 2012), which are based on inter-annotator agreement. Unlike these past studies, we look at annotation processes rather than annotation results, and aim at eliciting useful information for NLP through the analysis of annotation processes. This is in line with Behaviour mining (Chen, 2006) instead of data mining. There is few work looking at the annotation process for assessing annotation quality with a few exceptions like Tomanek et al. (2010), which estimated difficulty of annotating named entities by analysing annotator eye gaze during her annotation process. They concluded tha"
W13-4303,J96-2004,0,0.0311936,"ting Missing Annotation Disagreement using Eye Gaze Information Koh Mitsuda Ryu Iida Takenobu Tokunaga Department of Computer Science, Tokyo Institute of Technology {mitsudak,ryu-i,take}@cl.cs.titech.ac.jp Abstract ing better annotation tools (Kaplan et al., 2012; Lenzi et al., 2012; Marci´nczuk et al., 2012). The assessment of annotation quality is also an important issue in corpus building. The annotation quality is often evaluated with the agreement ratio among annotation results by multiple independent annotators. Various metrics for measuring reliability of annotation have been proposed (Carletta, 1996; Passonneau, 2006; Artstein and Poesio, 2008; Fort et al., 2012), which are based on inter-annotator agreement. Unlike these past studies, we look at annotation processes rather than annotation results, and aim at eliciting useful information for NLP through the analysis of annotation processes. This is in line with Behaviour mining (Chen, 2006) instead of data mining. There is few work looking at the annotation process for assessing annotation quality with a few exceptions like Tomanek et al. (2010), which estimated difficulty of annotating named entities by analysing annotator eye gaze duri"
W13-4303,W06-0602,0,0.0269414,"tor for detecting the MADs. 1 Introduction Over the last two decades, with the development of supervised machine learning techniques, annotating texts has become an essential task in natural language processing (NLP) (Stede and Huang, 2012). Since the annotation quality directly impacts on performance of ML-based NLP systems, many researchers have been concerned with building high-quality annotated corpora at a lower cost. Several different approaches have been taken for this purpose, such as semi-automating annotation by combining human annotation and existing NLP tools (Marcus et al., 1993; Chou et al., 2006; Rehbein et al., 2012; Voutilainen, 2012), implement19 International Joint Conference on Natural Language Processing, pages 19–26, Nagoya, Japan, 14-18 October 2013. while her counterpart correctly annotates it. We call this type of disagreement missing annotation disagreement (MAD). MADs were excluded from our previous analysis. Estimating MADs from the behaviour of a single annotator would be useful in a situation where only a single annotator is available. Against this background, we tackle a problem of detecting MADs based on both linguistic information of annotation targets and annotator"
W13-4303,fort-etal-2012-analyzing,0,0.0231814,"Missing"
W13-4303,bartalesi-lenzi-etal-2012-cat,0,0.0522053,"Missing"
W13-4303,voutilainen-2012-improving,0,0.0291368,"n Over the last two decades, with the development of supervised machine learning techniques, annotating texts has become an essential task in natural language processing (NLP) (Stede and Huang, 2012). Since the annotation quality directly impacts on performance of ML-based NLP systems, many researchers have been concerned with building high-quality annotated corpora at a lower cost. Several different approaches have been taken for this purpose, such as semi-automating annotation by combining human annotation and existing NLP tools (Marcus et al., 1993; Chou et al., 2006; Rehbein et al., 2012; Voutilainen, 2012), implement19 International Joint Conference on Natural Language Processing, pages 19–26, Nagoya, Japan, 14-18 October 2013. while her counterpart correctly annotates it. We call this type of disagreement missing annotation disagreement (MAD). MADs were excluded from our previous analysis. Estimating MADs from the behaviour of a single annotator would be useful in a situation where only a single annotator is available. Against this background, we tackle a problem of detecting MADs based on both linguistic information of annotation targets and annotator eye gaze. In our approach, the eye gaze d"
W13-4303,marcinczuk-etal-2012-inforex,0,0.0429848,"Missing"
W13-4303,J93-2004,0,0.0416852,"ould be a good indicator for detecting the MADs. 1 Introduction Over the last two decades, with the development of supervised machine learning techniques, annotating texts has become an essential task in natural language processing (NLP) (Stede and Huang, 2012). Since the annotation quality directly impacts on performance of ML-based NLP systems, many researchers have been concerned with building high-quality annotated corpora at a lower cost. Several different approaches have been taken for this purpose, such as semi-automating annotation by combining human annotation and existing NLP tools (Marcus et al., 1993; Chou et al., 2006; Rehbein et al., 2012; Voutilainen, 2012), implement19 International Joint Conference on Natural Language Processing, pages 19–26, Nagoya, Japan, 14-18 October 2013. while her counterpart correctly annotates it. We call this type of disagreement missing annotation disagreement (MAD). MADs were excluded from our previous analysis. Estimating MADs from the behaviour of a single annotator would be useful in a situation where only a single annotator is available. Against this background, we tackle a problem of detecting MADs based on both linguistic information of annotation ta"
W13-4303,passonneau-2006-measuring,0,0.0324465,"otation Disagreement using Eye Gaze Information Koh Mitsuda Ryu Iida Takenobu Tokunaga Department of Computer Science, Tokyo Institute of Technology {mitsudak,ryu-i,take}@cl.cs.titech.ac.jp Abstract ing better annotation tools (Kaplan et al., 2012; Lenzi et al., 2012; Marci´nczuk et al., 2012). The assessment of annotation quality is also an important issue in corpus building. The annotation quality is often evaluated with the agreement ratio among annotation results by multiple independent annotators. Various metrics for measuring reliability of annotation have been proposed (Carletta, 1996; Passonneau, 2006; Artstein and Poesio, 2008; Fort et al., 2012), which are based on inter-annotator agreement. Unlike these past studies, we look at annotation processes rather than annotation results, and aim at eliciting useful information for NLP through the analysis of annotation processes. This is in line with Behaviour mining (Chen, 2006) instead of data mining. There is few work looking at the annotation process for assessing annotation quality with a few exceptions like Tomanek et al. (2010), which estimated difficulty of annotating named entities by analysing annotator eye gaze during her annotation"
W13-4303,W13-0508,1,0.748029,"with a few exceptions like Tomanek et al. (2010), which estimated difficulty of annotating named entities by analysing annotator eye gaze during her annotation process. They concluded that the annotation difficulty depended on the semantic and syntactic complexity of the annotation targets, and the estimated difficulty would be useful for selecting training data for active learning techniques. We also reported an analysis of relations between a necessary time for annotating a single predicate-argument relation in Japanese text and the agreement ratio of the annotation among three annotators (Tokunaga et al., 2013). The annotation time was defined based on annotator actions and eye gaze. The analysis revealed that a longer annotation time suggested difficult annotation. Thus, we could estimate annotation quality based on the eye gaze and actions of a single annotator instead of the annotation results of multiple annotators. Following up our previous work (Tokunaga et al., 2013), this paper particularly focuses on a certain type of disagreement in which an annotator misses annotating a predicate-argument relation This paper discusses the detection of missing annotation disagreements (MADs), in which an a"
W13-4303,P10-1118,0,0.281943,"independent annotators. Various metrics for measuring reliability of annotation have been proposed (Carletta, 1996; Passonneau, 2006; Artstein and Poesio, 2008; Fort et al., 2012), which are based on inter-annotator agreement. Unlike these past studies, we look at annotation processes rather than annotation results, and aim at eliciting useful information for NLP through the analysis of annotation processes. This is in line with Behaviour mining (Chen, 2006) instead of data mining. There is few work looking at the annotation process for assessing annotation quality with a few exceptions like Tomanek et al. (2010), which estimated difficulty of annotating named entities by analysing annotator eye gaze during her annotation process. They concluded that the annotation difficulty depended on the semantic and syntactic complexity of the annotation targets, and the estimated difficulty would be useful for selecting training data for active learning techniques. We also reported an analysis of relations between a necessary time for annotating a single predicate-argument relation in Japanese text and the agreement ratio of the annotation among three annotators (Tokunaga et al., 2013). The annotation time was d"
W13-4303,maekawa-etal-2010-design,0,\N,Missing
