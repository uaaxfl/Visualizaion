2011.iwslt-evaluation.8,P07-2045,0,0.00419654,"ng with interpolation. We interpolated a LM trained on the TED training data (47k sentences) with a LM trained on Europarl, News, UN and 68 News-mono (24M sentences in total). After a perplexity test to optimize the interpolation weight (on Dev2010), we chose an interpolation weight equal to 0.5. 2.4. Translation modeling and tuning For the translation model training, the uncased (but punctuated) corpus was word aligned and then, the pairs of source and corresponding target phrases were extracted from the word-aligned bilingual training corpus using the scripts provided with the Moses decoder [3]. The result is a phrasetable containing all the aligned phrases. This phrase-table, produced by the translation modeling, is used to extract several translations models. In the experiments reported here, only 8 features were used in the phrase-based models: 5 translation model scores, 1 distance-based reordering score, 1 LM score and 1 word penalty score. We used the Minimum Error Rate Training (MERT) method to tune the weights. MERT was applied on the TED Dev2010 corpus (934 sentences). Moreover, it is important to note that, during tuning, punctuation was systematically removed from the Nbe"
2011.iwslt-evaluation.8,D07-1103,0,0.0175508,"ut translation task for which decoding (and tuning) is also done without punctuation. +News+UN+Newsmono: 24M sentences in total). The punctuation was restored after translation using this LM and the hidden-ngram command from SRILM toolkit. After repunctuation, we used the SMT-based recaser presented earlier. For the SLT task, the final system submitted by LIG in 2010 was ranked among the best sites that participated to the TALK task last year. 3. Improvements of MT and SLT systems done for 2011 3.1. Iterative improvement of the MT system -apply phrase-table pruning with a technique similar to [4] (retuning with MERT needed after pruning). Table 2 summarizes the iterative improvements done this year over the LIG 2010 system. First, we evaluated the performance of a phrase-table trained on the TED 2011 bilingual data (107268 sentences in total) only with and without tuning (2,3). The target language model was also updated using the TED 2011 mono (111431 sentences) data (4), which slightly increased the performance. The results obtained show a reasonable performance of the PT trained on TED 2011 only, so we experimented multiple phrasetable decoding where translation options are collecte"
2011.iwslt-evaluation.8,P02-1040,0,0.0851348,"Missing"
2011.iwslt-evaluation.8,W11-2154,1,0.875821,"Missing"
2011.iwslt-evaluation.8,2010.iwslt-evaluation.1,0,\N,Missing
2012.iwslt-evaluation.13,2005.mtsummit-papers.11,0,0.008448,"as well as the LIG ofﬁcial results obtained this year. 2. Resources used in 2012 The following sections describe the resources used to build the translation models as well as the language models. We built three translation models for our machine translation systems (see table 1). • An in-domain translation model trained on TED Talks collection (TED) corpus. • A (bigger) out-of-domain translation model trained on six different (freely available) corpora in which three of them are part of the WMT 2012 shared task training data: – the latest version of the Europarl (version 7) corpus (EUROPARL1 [1]) – the latest version of the News-Commentary (version 7) corpus (NEWS-C) – the United Nations corpus (UN 2 [2]) • We also used the Corpus of Parallel Patent Applications (PCT3 ), the DGT Multilingual Translation Memory of the Acquis Communautaire (DGT-TM [3]), and the EUconst corpus (EU-CONST [4]). These three corpora are all freely available. • An additional out-of-domain translation model was trained on a subset of the French-English Gigaword corpus (GIGA-5M). After cleaning, the whole Gigaword corpus was sorted at sentence level according to the sum of perplexities of the source (English)"
2012.iwslt-evaluation.13,eisele-chen-2010-multiun,0,0.0138842,"be the resources used to build the translation models as well as the language models. We built three translation models for our machine translation systems (see table 1). • An in-domain translation model trained on TED Talks collection (TED) corpus. • A (bigger) out-of-domain translation model trained on six different (freely available) corpora in which three of them are part of the WMT 2012 shared task training data: – the latest version of the Europarl (version 7) corpus (EUROPARL1 [1]) – the latest version of the News-Commentary (version 7) corpus (NEWS-C) – the United Nations corpus (UN 2 [2]) • We also used the Corpus of Parallel Patent Applications (PCT3 ), the DGT Multilingual Translation Memory of the Acquis Communautaire (DGT-TM [3]), and the EUconst corpus (EU-CONST [4]). These three corpora are all freely available. • An additional out-of-domain translation model was trained on a subset of the French-English Gigaword corpus (GIGA-5M). After cleaning, the whole Gigaword corpus was sorted at sentence level according to the sum of perplexities of the source (English) and the target (French) based on two French and English pretrained language models. For this, LMs were trained"
2012.iwslt-evaluation.13,steinberger-etal-2012-dgt,0,0.0128047,"systems (see table 1). • An in-domain translation model trained on TED Talks collection (TED) corpus. • A (bigger) out-of-domain translation model trained on six different (freely available) corpora in which three of them are part of the WMT 2012 shared task training data: – the latest version of the Europarl (version 7) corpus (EUROPARL1 [1]) – the latest version of the News-Commentary (version 7) corpus (NEWS-C) – the United Nations corpus (UN 2 [2]) • We also used the Corpus of Parallel Patent Applications (PCT3 ), the DGT Multilingual Translation Memory of the Acquis Communautaire (DGT-TM [3]), and the EUconst corpus (EU-CONST [4]). These three corpora are all freely available. • An additional out-of-domain translation model was trained on a subset of the French-English Gigaword corpus (GIGA-5M). After cleaning, the whole Gigaword corpus was sorted at sentence level according to the sum of perplexities of the source (English) and the target (French) based on two French and English pretrained language models. For this, LMs were trained separately on all the data listed in table 2 except the Gigaword corpus itself (the News Shufﬂe corpus was also available on the source English side"
2012.iwslt-evaluation.13,P07-1040,0,0.0999912,"Missing"
2012.iwslt-evaluation.13,P09-1066,0,0.0718746,"ature weights are modiﬁed using Minimum Error Rate Training (MERT). Experiments are performed to ﬁnd the optimal size for N-Best list combination. Four systems are used and analysed on combination of two best systems and all the systems. 50-best list was found to be optimal size for both cases. The authors showed that the impact of gradually introducing a new system for combination becomes lower as the number of systems increases. Anyway the best result is obtained when all of the systems are combined. -Co-decoding Recently, the concept of collaborative decoding (codecoding) was introduced by [11] to improve machine translation accuracy by leveraging translation consensus between multiple machine translation decoders. Different from what we described earlier (postprocess the n-best lists or word graphs), this method uses multiple machine translation decoders that collaborate by exchanging partial translation results. Using an iterative decoding approach, n-gram agreement statistics between translations of multiple decoders are employed to re-rank full and partial hypotheses explored in decoding. 3.2. Overview of the Driven Decoding Concept 3.2.1. Driven Decoding Es = arg minE∈Ei Ns  T"
2012.iwslt-evaluation.13,P02-1040,0,0.0936693,"ance between the current hypothesis decoded (called H) and the auxiliary translation available (T) : d(T,H). Let’s say that 2 auxiliary translations are available (from system 1 and system 2) and that 4 distance metrics are available (BLEU, TER, TERp-A and PER); in that case, 8 scores are added to each line of the N-Best list. The distance metrics used in our experiments are described in the next section and then N-Best reordering process is detailed. 3.2.2. Distance Metrics used The distance metrics used are Translation Error Rate (TER), Position independent Error Rate (PER), TERp-A and BLEU [12]. The TER score reﬂects the number of edit operations (insertions, deletions, words substitutions and blocks shifts) needed to transform a hypothesis translation into the reference translation, while the BLEU score is the geometric mean of n-gram precision. Lower TER and higher BLEU score suggest better translation quality. In addition, we use PER score (position independent error rate) which can be seen as a bag-of-words metric potentially interesting in the context of the driven decoding proposed. In addition we use TERp [13] which is an extension of TER eliminating its 105 The 9th Internati"
2012.iwslt-evaluation.13,2009.eamt-1.5,0,0.0170064,"task with and without the use of conﬁdence measure will be shown in Table 3. 5. Experimental Results of LIG Systems 4. Use of Conﬁdence Measures for SMT Besides driven decoding (DD) scores, a sentence conﬁdence score can be added as an additional feature in the N-best list to improve the re-ordering performance. To obtain such a conﬁdence score, a classiﬁer must be constructed. We concatenate two data sets dev2010 + tst2010 to form the training data. Features used to train our model come from the baseline features of the WMT2012 quality estimation shared task (features originally presented in [14]), which can We recall that our systems were systematically tuned on dev2010 corpus. Our baseline system, trained as described in section 2, lead to a BLEU score of 30.28 on tst2010 using 2 translation and re-ordering models (no GIGAword) while it improves to 30.80 using 3 translation and reordering models (using GIGAword). This result has to be compared with 27.58 obtained on tst2010 with our system last year. As far as the driven decoding is concerned, the results show that using the Google 1best hypothesis to guide the 106 The 9th International Workshop on Spoken Language Translation Hong K"
2012.iwslt-evaluation.13,P10-1052,0,0.0330874,"each word in the source corpus. The core element needed for the classiﬁer construction process is the training label for each sentence. The TERp-A metric [13], which we select to perform this task, provides the linguistic and semantic matching between each sentence in training set and its reference (available for dev2010 and tst2010 corpora), then yields the minimum cost for matching normalized by its number of tokens as its score. We then categorize them in a binary set: sentences with score higher than 0.3 is assigned with ”Good” (G) label, otherwise, ”Bad” (B). A CRF-based toolkit, WAPITI [15], is then called to build the classiﬁer. The training phase is conducted using stochastic gradient descent (SGD-L1) algorithm, with values for maximum number of iterations done by the algorithm (-maxiter), stop window size (–stopwin) and stop epsilon (–stopeps) to 200, 6, and 0.00005 respectively. Applying this classiﬁer in both test sets (test2011 + test2012, with WAPITI’s default threshold = 0.5) gives us the result ﬁles detailing hypothesized label along with its probability at the sentence level. Then, the conﬁdence score used is the probability of sentence to be regarded as a “Good” sente"
2012.iwslt-evaluation.13,N07-1029,0,\N,Missing
2012.iwslt-evaluation.13,2008.amta-srw.3,0,\N,Missing
2014.eamt-1.23,P07-2045,0,0.00382925,"tences, we applied a Moses-based SMT system to generate their English hypotheses. Next, human translators were invited to correct MT outputs, giving us the post editions. The set of triples (source, hypothesis, post edition) was then divided into the training set (10000 first triples) and test set (881 remaining ones). The WCE model was trained over all 1-best hypotheses of the training set. More details on our WCE system can be found in next section. The N-best list (N = 1000) with involved alignment information is also obtained on the test set (1000 * 881 = 881000 sentences) by using Moses (Koehn et al., 2007) options “-n-best-list” and “-print-alignment-info-in-n-best”. Besides, the SGs are extracted by some parameter settings: “output-search-graph”, “-search-algorithm 1” (using cube pruning) and “-cube-pruning-pop-limit 5000” (adds 5000 hypotheses to each stack). They are compactly encoded under a plain formatted text file that is convenient to transform into userdefined structures for further processing. We then store the SG for each source sentence in a separated file, and the average size is 43.8 MB. 4.2 WCE scores and Oracle Labels We employ the Conditional Random Fields (Lafferty et al., 200"
2014.eamt-1.23,P11-1022,0,0.409726,"Missing"
2014.eamt-1.23,P11-2031,0,0.0326608,"amatically the baseline quality. BL+OR(1a) augments 7.87 points in BLEU, and diminishes 0.0607 (0.0794) point in TER(TERp-A), compared to BL. Meanwhile, with BL+OR(2a), these gains are 7.67, 0.0565 and 0.0514 (in that order). Besides, the contribution of our real WCE system scores seems less prominent, yet positive: the best performing BL+WCE(1a) 123 increases 1.49 BLEU points of BL (0.0029 and 0.0136 gained for TER and TERp-A). More remarkable, tiny p-values (in the range [0.00; 0.02], seen on Table 2) estimated between BLEU of each BL+WCE system and that of BL relying on Approximate Method (Clark et al., 2011) indicate that these performance improvements are significant. Results also reveal that the use of WCE labels are slightly more beneficial than that of confidence probabilities: BL+WCE(1a) and BL+WCE(2a) outperform BL+WCE(1b) and BL+WCE(2b). In both scenarios, we observe that the global update score (Definition 1) performs more fruitfully compared to the local one (Definition 2). For more insightful understanding about WCE scores’ acuteness, we make a comparison with the best achievable hypotheses in the SG (oracles), based on the “LM Oracle” approximation approach presented in (Sokolov et al."
2014.eamt-1.23,P08-2010,0,0.137938,"chievements, the conventional one-pass SMT decoders are still not sufficient yet in yielding human-acceptable translations (Zhang et al., 2006; Venugopal et al., 2007). Therefore, a number of methods to enhance them are proposed, such as: post-editing, re-ranking or re-decoding, etc. Post-editing (Parton et al., c 2014 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 117 2012) is in fact known to be a human-inspired task where the machine post edits translations in a second automatic pass. In re-ranking (Zhang et al., 2006; Duh and Kirchhoff, 2008; Bach et al., 2011), more features are integrated with the existing multiple model scores for re-selecting the best candidate among N-best list. Meanwhile, the redecoding process intervenes directly into the decoder’s search graph (SG), driving it to the optimal path (cheapest hypothesis). The two-pass decoder has been built by several discrepant ways in the past. Kirchhoff and Yang (2005); Zhang et al. (2006) train additional Language Models (LM) and combine LM scores with existing model scores to re-rank the N-best list. Also focusing on the idea of re-ranking, yet Bach et al. (2011); Luong"
2014.eamt-1.23,W05-0821,0,0.0340837,"ence, no derivative works, attribution, CCBY-ND. 117 2012) is in fact known to be a human-inspired task where the machine post edits translations in a second automatic pass. In re-ranking (Zhang et al., 2006; Duh and Kirchhoff, 2008; Bach et al., 2011), more features are integrated with the existing multiple model scores for re-selecting the best candidate among N-best list. Meanwhile, the redecoding process intervenes directly into the decoder’s search graph (SG), driving it to the optimal path (cheapest hypothesis). The two-pass decoder has been built by several discrepant ways in the past. Kirchhoff and Yang (2005); Zhang et al. (2006) train additional Language Models (LM) and combine LM scores with existing model scores to re-rank the N-best list. Also focusing on the idea of re-ranking, yet Bach et al. (2011); Luong et al. (2014) employ sentence and word confidence scores in the second pass. Meanwhile, Venugopal et al. (2007) do a first pass translation without LM, but use it to score the pruned search hyper-graph in the second pass. This work concentrates on a second automatic pass where the costs of all hypotheses in the decoder’s SG containing words of the N-best list will be updated regarding the"
2014.eamt-1.23,W14-0301,1,0.75748,"2008; Bach et al., 2011), more features are integrated with the existing multiple model scores for re-selecting the best candidate among N-best list. Meanwhile, the redecoding process intervenes directly into the decoder’s search graph (SG), driving it to the optimal path (cheapest hypothesis). The two-pass decoder has been built by several discrepant ways in the past. Kirchhoff and Yang (2005); Zhang et al. (2006) train additional Language Models (LM) and combine LM scores with existing model scores to re-rank the N-best list. Also focusing on the idea of re-ranking, yet Bach et al. (2011); Luong et al. (2014) employ sentence and word confidence scores in the second pass. Meanwhile, Venugopal et al. (2007) do a first pass translation without LM, but use it to score the pruned search hyper-graph in the second pass. This work concentrates on a second automatic pass where the costs of all hypotheses in the decoder’s SG containing words of the N-best list will be updated regarding the word quality predicted by Word Confidence Estimation (Ueffing and Ney, 2005) (WCE) system. In single-pass decoding, the decoder searches among complete paths (i.e. those cover all source words) for obtaining the optimalco"
2014.eamt-1.23,2012.eamt-1.34,0,0.270744,"Missing"
2014.eamt-1.23,E12-1013,0,0.0235204,"et al., 2011) indicate that these performance improvements are significant. Results also reveal that the use of WCE labels are slightly more beneficial than that of confidence probabilities: BL+WCE(1a) and BL+WCE(2a) outperform BL+WCE(1b) and BL+WCE(2b). In both scenarios, we observe that the global update score (Definition 1) performs more fruitfully compared to the local one (Definition 2). For more insightful understanding about WCE scores’ acuteness, we make a comparison with the best achievable hypotheses in the SG (oracles), based on the “LM Oracle” approximation approach presented in (Sokolov et al., 2012). This method allows to simplify the oracle decoding to the problem of searching for the cheapest path on a SG where all transition costs are replaced by the n-gram LM scores of the corresponding words. The LM is built for each source sentence using uniquely its target post-edition. We update the SG by assigning all edges with the LM back-off score of the word it contains (instead of using the current transition cost). Finally, we combine the oracles of all sentences yielding BLEU oracle of 66.48. To better understand the benefit of SG redecoding, we compare the obtained performances with thos"
2014.eamt-1.23,H05-1096,0,0.0347679,"(LM) and combine LM scores with existing model scores to re-rank the N-best list. Also focusing on the idea of re-ranking, yet Bach et al. (2011); Luong et al. (2014) employ sentence and word confidence scores in the second pass. Meanwhile, Venugopal et al. (2007) do a first pass translation without LM, but use it to score the pruned search hyper-graph in the second pass. This work concentrates on a second automatic pass where the costs of all hypotheses in the decoder’s SG containing words of the N-best list will be updated regarding the word quality predicted by Word Confidence Estimation (Ueffing and Ney, 2005) (WCE) system. In single-pass decoding, the decoder searches among complete paths (i.e. those cover all source words) for obtaining the optimalcost ones. Essentially, the hypothesis cost is a composite score, synthesized from various SMT models (reordering, translation, LMs etc.). Although the N-bests beat other SG hypotheses in term of model scores, there is no certain clue that they will be the closest to the human references. As the reference closeness is the users’ most pivotal concern on SMT decoder, this work establishes one second pass where model-independent scores related to word conf"
2014.eamt-1.23,2003.mtsummit-papers.52,0,0.296113,"earning method, with WAPITI toolkit (Lavergne et al., 2010), to train the WCE model. A number of knowledge resources are employed for extracting the system-based, lexical, syntactic and semantic characteristics of word, resulting in the total of 25 major feature types as follows: • Target Side: target word; bigram (trigram) backward sequences; number of occurrences • Source Side: source word(s) aligned to the target word 122 • Alignment Context (Bach et al., 2011): the combinations of the target (source) word and all aligned source (target) words in the window ±2 • Word posterior probability (Ueffing et al., 2003) • Pseudo-reference (Google Translate): Does the word appear in the pseudo reference? • Graph topology (Luong et al., 2013): number of alternative paths in the confusion set, maximum and minimum values of posterior probability distribution • Language model (LM) based: length of the longest sequence of the current word and its previous ones in the target (resp. source) LM. For example, with the target word wi : if the sequence wi−2 wi−1 wi appears in the target LM but the sequence wi−3 wi−2 wi−1 wi does not, the n-gram value for wi will be 3. • Lexical Features: word’s Part-Of-Speech (POS); seq"
2014.eamt-1.23,N07-1063,0,0.091305,"to update the cost of SG hypotheses containing it, we hope to “reinforce” or “weaken” them relied on word quality. After the update, new best translations are re-determined using updated costs. In the experiments on our real WCE scores and ideal (oracle) ones, the latter significantly boosts one-pass decoder by 7.87 BLEU points, meanwhile the former yields an improvement of 1.49 points for the same metric. 1 Introduction Beside plenty of commendable achievements, the conventional one-pass SMT decoders are still not sufficient yet in yielding human-acceptable translations (Zhang et al., 2006; Venugopal et al., 2007). Therefore, a number of methods to enhance them are proposed, such as: post-editing, re-ranking or re-decoding, etc. Post-editing (Parton et al., c 2014 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 117 2012) is in fact known to be a human-inspired task where the machine post edits translations in a second automatic pass. In re-ranking (Zhang et al., 2006; Duh and Kirchhoff, 2008; Bach et al., 2011), more features are integrated with the existing multiple model scores for re-selecting the best candidate among N-best list"
2014.eamt-1.23,P10-1062,0,0.269189,"the confusion set, maximum and minimum values of posterior probability distribution • Language model (LM) based: length of the longest sequence of the current word and its previous ones in the target (resp. source) LM. For example, with the target word wi : if the sequence wi−2 wi−1 wi appears in the target LM but the sequence wi−3 wi−2 wi−1 wi does not, the n-gram value for wi will be 3. • Lexical Features: word’s Part-Of-Speech (POS); sequence of POS of all its aligned source words; POS bigram (trigram) backward sequences; punctuation; proper name; numerical • Syntactic Features: null link (Xiong et al., 2010); constituent label; depth in the constituent tree • Semantic Features: number of word senses in WordNet. In the next step, the word’s reference labels (or so-called oracle labels) are initially set by using TERp-A toolkit (Snover et al., 2008) in one of the following classes: “I’ (insertions), “S” (substitutions), “T” (stem matches), “Y” (synonym matches), “P” (phrasal substitutions), “E” (exact matches) and are then regrouped into binary class: “G” (good word) or “B” (bad word). Once having the prediction model, we apply it on the test set (881 x 1000 best = 881000 sentences) and get needed"
2014.eamt-1.23,W06-1626,0,0.398676,"d in the N-best list to update the cost of SG hypotheses containing it, we hope to “reinforce” or “weaken” them relied on word quality. After the update, new best translations are re-determined using updated costs. In the experiments on our real WCE scores and ideal (oracle) ones, the latter significantly boosts one-pass decoder by 7.87 BLEU points, meanwhile the former yields an improvement of 1.49 points for the same metric. 1 Introduction Beside plenty of commendable achievements, the conventional one-pass SMT decoders are still not sufficient yet in yielding human-acceptable translations (Zhang et al., 2006; Venugopal et al., 2007). Therefore, a number of methods to enhance them are proposed, such as: post-editing, re-ranking or re-decoding, etc. Post-editing (Parton et al., c 2014 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 117 2012) is in fact known to be a human-inspired task where the machine post edits translations in a second automatic pass. In re-ranking (Zhang et al., 2006; Duh and Kirchhoff, 2008; Bach et al., 2011), more features are integrated with the existing multiple model scores for re-selecting the best ca"
2015.iwslt-papers.11,N07-1051,0,\N,Missing
2015.iwslt-papers.11,C10-2013,0,\N,Missing
2015.iwslt-papers.11,2003.mtsummit-papers.52,0,\N,Missing
2015.iwslt-papers.11,W10-1723,1,\N,Missing
2015.iwslt-papers.11,W12-3102,0,\N,Missing
2015.iwslt-papers.11,P07-2045,0,\N,Missing
2015.iwslt-papers.11,P11-1022,0,\N,Missing
2015.iwslt-papers.11,W13-2242,0,\N,Missing
2015.iwslt-papers.11,W14-3344,0,\N,Missing
2015.iwslt-papers.11,C04-1046,0,\N,Missing
2015.iwslt-papers.11,P10-1062,0,\N,Missing
2015.iwslt-papers.11,J03-1002,0,\N,Missing
2015.iwslt-papers.11,W12-3110,0,\N,Missing
2015.iwslt-papers.11,W14-3340,0,\N,Missing
2015.iwslt-papers.11,P15-4020,0,\N,Missing
2015.iwslt-papers.11,potet-etal-2012-collection,1,\N,Missing
2015.iwslt-papers.11,W14-3302,0,\N,Missing
2015.iwslt-papers.11,W13-2201,0,\N,Missing
2015.iwslt-papers.11,P10-1052,0,\N,Missing
2015.iwslt-papers.11,W12-3113,0,\N,Missing
2015.iwslt-papers.11,W13-2245,0,\N,Missing
2015.iwslt-papers.11,W14-3342,1,\N,Missing
2015.jeptalnrecital-long.21,2014.iwslt-papers.3,0,0.0848215,"Missing"
2015.jeptalnrecital-long.21,W13-2242,0,0.0550652,"Missing"
2015.jeptalnrecital-long.21,P08-2010,0,0.0603965,"Missing"
2015.jeptalnrecital-long.21,galliano-etal-2006-corpus,0,0.0805766,"Missing"
2015.jeptalnrecital-long.21,W13-2245,0,0.0628553,"Missing"
2015.jeptalnrecital-long.21,P07-2045,0,0.00382776,"Missing"
2015.jeptalnrecital-long.21,P10-1052,0,0.0639667,"Missing"
2015.jeptalnrecital-long.21,2014.eamt-1.23,1,0.818099,"Missing"
2015.jeptalnrecital-long.21,W14-0301,1,0.844331,"Missing"
2015.jeptalnrecital-long.21,W13-2248,1,0.897933,"Missing"
2015.jeptalnrecital-long.21,2012.eamt-1.34,0,0.0553249,"Missing"
2015.jeptalnrecital-long.21,W10-1723,1,0.896895,"Missing"
2015.jeptalnrecital-long.21,potet-etal-2012-collection,1,0.874018,"Missing"
2015.jeptalnrecital-long.21,D08-1065,0,0.0612848,"Missing"
2015.jeptalnrecital-long.21,2003.mtsummit-papers.52,0,0.128287,"Missing"
2015.jeptalnrecital-long.21,N07-1063,0,0.0742563,"Missing"
2015.jeptalnrecital-long.21,P10-1062,0,0.0483527,"Missing"
2015.jeptalnrecital-long.21,W06-3110,0,0.087503,"Missing"
2015.jeptalnrecital-long.21,W06-1626,0,0.0720658,"Missing"
2016.jeptalnrecital-jep.4,L16-1221,1,0.846918,"Missing"
2016.jeptalnrecital-jep.4,W15-5121,1,0.855143,"Missing"
2016.jeptalnrecital-jep.4,vacher-etal-2014-sweet,1,0.887403,"Missing"
2017.jeptalnrecital-court.18,P14-1023,0,0.0537753,"Missing"
2017.jeptalnrecital-court.18,D14-1110,0,0.0676671,"Missing"
2017.jeptalnrecital-court.18,S17-2012,1,0.860558,"Missing"
2017.jeptalnrecital-court.18,P14-2050,0,0.0874036,"Missing"
2017.jeptalnrecital-court.18,D14-1162,0,0.0778873,"Missing"
2017.jeptalnrecital-court.18,C16-1130,0,0.0376626,"Missing"
2017.jeptalnrecital-demo.9,P00-1064,0,0.0558796,"Missing"
2017.jeptalnrecital-demo.9,H93-1061,0,0.60827,"Missing"
2017.jeptalnrecital-demo.9,S15-2049,0,0.0681988,"Missing"
2017.jeptalnrecital-demo.9,ide-etal-2008-masc,0,0.0509677,"Missing"
2017.jeptalnrecital-demo.9,P10-1023,0,0.0605544,"Missing"
2017.jeptalnrecital-demo.9,P96-1006,0,0.620944,"Missing"
2017.jeptalnrecital-demo.9,E17-1010,0,0.0204335,"Missing"
2017.jeptalnrecital-demo.9,K15-1037,0,0.0366829,"Missing"
2017.jeptalnrecital-long.5,W16-4015,0,0.0357791,"Missing"
2017.jeptalnrecital-long.5,D10-1115,0,0.0298498,"Missing"
2017.jeptalnrecital-long.5,S15-2048,0,0.0566084,"Missing"
2017.jeptalnrecital-long.5,L16-1662,1,0.871404,"Missing"
2017.jeptalnrecital-long.5,2010.iwslt-evaluation.12,1,0.868413,"Missing"
2017.jeptalnrecital-long.5,D12-1050,0,0.0203048,"Missing"
2017.jeptalnrecital-long.5,D14-1179,0,0.0166687,"Missing"
2017.jeptalnrecital-long.5,P08-1115,0,0.0924009,"Missing"
2017.jeptalnrecital-long.5,P08-2015,0,0.0741144,"Missing"
2017.jeptalnrecital-long.5,P12-1092,0,0.0586573,"Missing"
2017.jeptalnrecital-long.5,P07-2045,0,0.0106168,"Missing"
2017.jeptalnrecital-long.5,D09-1040,0,0.076908,"Missing"
2017.jeptalnrecital-long.5,2005.iwslt-1.19,0,0.0745007,"Missing"
2017.jeptalnrecital-long.5,2014.amta-researchers.23,1,0.862839,"Missing"
2017.jeptalnrecital-long.5,J97-2003,0,0.286516,"Missing"
2017.jeptalnrecital-long.5,P03-1021,0,0.0206422,"Missing"
2017.jeptalnrecital-long.5,P02-1040,0,0.0970493,"Missing"
2017.jeptalnrecital-long.5,P10-1040,0,0.0364167,"Missing"
2018.jeptalnrecital-long.12,W14-4012,0,0.0651643,"Missing"
2018.jeptalnrecital-long.12,P16-1085,0,0.0321342,"Missing"
2018.jeptalnrecital-long.12,ide-etal-2008-masc,0,0.0194786,"Missing"
2018.jeptalnrecital-long.12,W16-5307,0,0.0542954,"Missing"
2018.jeptalnrecital-long.12,H93-1061,0,0.0974287,"Missing"
2018.jeptalnrecital-long.12,S15-2049,0,0.026349,"Missing"
2018.jeptalnrecital-long.12,D14-1162,0,0.0803252,"Missing"
2018.jeptalnrecital-long.12,S07-1016,0,0.11434,"Missing"
2018.jeptalnrecital-long.12,E17-1010,0,0.0286327,"Missing"
2018.jeptalnrecital-long.12,D17-1120,0,0.0241459,"Missing"
2018.jeptalnrecital-long.12,W04-0811,0,0.217008,"Missing"
2018.jeptalnrecital-long.12,K15-1037,0,0.0308682,"Missing"
2018.jeptalnrecital-long.12,C16-1130,0,0.0491544,"Missing"
2018.jeptalnrecital-long.12,P10-4014,0,0.0850606,"Missing"
2019.gwc-1.14,S01-1001,0,0.887612,"Missing"
2019.gwc-1.14,P16-1085,0,0.252731,"Missing"
2019.gwc-1.14,W02-0808,0,0.155709,"Missing"
2019.gwc-1.14,W16-5307,0,0.24442,"Missing"
2019.gwc-1.14,C18-1030,0,0.0216755,"Missing"
2019.gwc-1.14,D18-1170,0,0.185445,"Missing"
2019.gwc-1.14,P18-1230,0,0.12078,"Missing"
2019.gwc-1.14,H93-1061,0,0.920144,"Missing"
2019.gwc-1.14,S15-2049,0,0.748046,"Missing"
2019.gwc-1.14,Q14-1019,0,0.186664,"Missing"
2019.gwc-1.14,S07-1006,0,0.0918902,"Missing"
2019.gwc-1.14,S13-2040,0,0.809637,"Missing"
2019.gwc-1.14,D17-1008,0,0.0244557,"Missing"
2019.gwc-1.14,D14-1162,0,0.0819786,"Missing"
2019.gwc-1.14,N18-1202,0,0.0627995,"Missing"
2019.gwc-1.14,S07-1016,0,0.80523,"Missing"
2019.gwc-1.14,E17-1010,0,0.795791,"Missing"
2019.gwc-1.14,D17-1120,0,0.44339,"Missing"
2019.gwc-1.14,W04-0811,0,0.572166,"Missing"
2019.gwc-1.14,K15-1037,0,0.124404,"Missing"
2019.gwc-1.14,L18-1166,1,0.892281,"Missing"
2019.gwc-1.14,P10-4014,0,0.316001,"Missing"
2019.gwc-1.14,W06-1670,0,\N,Missing
2019.gwc-1.14,C16-1130,0,\N,Missing
2019.jeptalnrecital-long.4,P16-1085,0,0.0365024,"Missing"
2019.jeptalnrecital-long.4,W16-5307,0,0.0486658,"Missing"
2019.jeptalnrecital-long.4,C18-1030,0,0.0209002,"Missing"
2019.jeptalnrecital-long.4,D18-1170,0,0.0304913,"Missing"
2019.jeptalnrecital-long.4,P18-1230,0,0.0230245,"Missing"
2019.jeptalnrecital-long.4,H93-1061,0,0.452761,"Missing"
2019.jeptalnrecital-long.4,S15-2049,0,0.0369978,"Missing"
2019.jeptalnrecital-long.4,Q14-1019,0,0.069963,"Missing"
2019.jeptalnrecital-long.4,W18-6301,0,0.0422646,"Missing"
2019.jeptalnrecital-long.4,D17-1008,0,0.0229993,"Missing"
2019.jeptalnrecital-long.4,D14-1162,0,0.0816368,"Missing"
2019.jeptalnrecital-long.4,N18-1202,0,0.0267707,"Missing"
2019.jeptalnrecital-long.4,S07-1016,0,0.0667002,"Missing"
2019.jeptalnrecital-long.4,D17-1120,0,0.0232364,"Missing"
2019.jeptalnrecital-long.4,W04-0811,0,0.0788343,"Missing"
2019.jeptalnrecital-long.4,K15-1037,0,0.0267548,"Missing"
2019.jeptalnrecital-long.4,L18-1166,1,0.872858,"Missing"
2019.jeptalnrecital-long.4,C16-1130,0,0.0277121,"Missing"
2019.jeptalnrecital-long.4,P10-4014,0,0.0943769,"Missing"
2020.iwslt-1.2,P19-1126,0,0.076722,"Missing"
2020.iwslt-1.2,2005.mtsummit-papers.11,0,0.172794,".e. reading the full source before writing the target. We evaluate four wait-k systems each trained with a value of ktrain in {5, 7, 9, ∞} and decoded with keval ranging from 2 to 11. We then ensemble the aforementioned wait-k models and evaluate a multipath model that jointly optimizes a large set of wait-k paths. The results demonstrate that multipath is competetive with wait-k without the need to select which path to optimize (some values of k, e.g. 5, underperform in comparison). Ensembling the wait-k models gives a boost of 1.43 BLEU points on average. and WMT’19 data,6 namely, Europarl (Koehn, 2005), News Commentary (Tiedemann, 2012) and Common Crawl (Smith et al., 2013). We remove pairs with a length-ratio exceeding 1.3 from Common Crawl and pairs exceeding a length-ratio of 1.5 from the rest. We develop on MuST-C dev and report results on MuST-C tst-COMMON. For open-vocabulary translation, we use SentencePiece (Kudo and Richardson, 2018) to segment the bitexts with byte pair encoding (Sennrich et al., 2016). This results in a joint vocabulary of 32K types. Details of the training data are provided in Table 5. We train Transformer big architectures and tie the embeddings of the encoder"
2020.iwslt-1.2,D18-2012,0,0.0233125,"nstrate that multipath is competetive with wait-k without the need to select which path to optimize (some values of k, e.g. 5, underperform in comparison). Ensembling the wait-k models gives a boost of 1.43 BLEU points on average. and WMT’19 data,6 namely, Europarl (Koehn, 2005), News Commentary (Tiedemann, 2012) and Common Crawl (Smith et al., 2013). We remove pairs with a length-ratio exceeding 1.3 from Common Crawl and pairs exceeding a length-ratio of 1.5 from the rest. We develop on MuST-C dev and report results on MuST-C tst-COMMON. For open-vocabulary translation, we use SentencePiece (Kudo and Richardson, 2018) to segment the bitexts with byte pair encoding (Sennrich et al., 2016). This results in a joint vocabulary of 32K types. Details of the training data are provided in Table 5. We train Transformer big architectures and tie the embeddings of the encoder with the decoder’s input and output embeddings. We optimize our models with label-smoothed maximum likelihood (Szegedy et al., 2016) with a smoothing rate  = 0.1. The parameters are updated using 6 27 7 https://github.com/pytorch/fairseq/ blob/simulastsharedtask/examples/ simultaneous_translation http://www.statmt.org/wmt19/ 40 TED-LIUM 3 How2"
2020.iwslt-1.2,N18-2079,0,0.0241677,"eous speech translation track, we build on Transformer-based wait-k models for the text-to-text subtask. For speech-to-text simultaneous translation, we attach a wait-k MT system to a hybrid ASR system. We propose an algorithm to control the latency of the ASR+MT cascade and achieve a good latency-quality trade-off on both subtasks. 1 • IWSLT 2020 offline translation track with end-to-end models for the English-German language pair, • IWSLT 2020 simultaneous translation track with a cascade of an ASR system trained using Kaldi (Povey et al., 2011) and an online MT system with wait-k policies (Dalvi et al., 2018; Ma et al., 2019). Introduction While cascaded speech-to-text translation (AST) systems (combining source language speech recognition (ASR) and source-to-target text translation (MT)) remain state-of-the-art, recent works have attempted to build end-to-end AST with very encouraging results (Bérard et al., 2016; Weiss et al., 2017; Bérard et al., 2018; Jia et al., 2019; Sperber et al., 2019). This year, IWSLT 2020 offline translation track attempts to evaluate if endto-end AST will close the gap with cascaded AST for the English-to-German language pair. Another increasingly popular topic is si"
2020.iwslt-1.2,N19-1202,0,0.0478636,"Missing"
2020.iwslt-1.2,N19-4009,0,0.0278244,"multi-path Ensemble 25 23 21 19 1 3 5 7 9 Average Lagging (AL) in detokenized tokens Figure 2: [Text-to-Text] Latency-quality trade-offs evaluated on MuST-C tst-COMMON with greedy decoding. Offline systems have an AL of 18.55 words. The red vertical bars correspond to the AL evaluation thresholds. Adam (Kingma and Ba, 2015) (β1 , β2 = 0.9, 0.98) with a learning rate that follows an inverse squareroot schedule. We train for a total of 50K updates and evaluate with the check-pointed weights corresponding to the lowest (best) loss on the development set. Our models are implemented with Fairseq (Ott et al., 2019). We generate translation hypotheses with greedy decoding and evaluate the latency-quality trade-off by measuring casesensitive detokenized BLEU (Papineni et al., 2002) and word-level Average Lagging (AL) (Ma et al., 2019). Table 5: Parallel training data for the MT systems. Results. We show in Figure 2 the performance of our systems on the test set (MuST tst-COMMON) measured with the provided evaluation server.7 We denote with ktrain =∞ a unidirectional model trained for wait-until-end decoding i.e. reading the full source before writing the target. We evaluate four wait-k systems each traine"
2020.iwslt-1.2,P02-1040,0,0.106509,"OMMON with greedy decoding. Offline systems have an AL of 18.55 words. The red vertical bars correspond to the AL evaluation thresholds. Adam (Kingma and Ba, 2015) (β1 , β2 = 0.9, 0.98) with a learning rate that follows an inverse squareroot schedule. We train for a total of 50K updates and evaluate with the check-pointed weights corresponding to the lowest (best) loss on the development set. Our models are implemented with Fairseq (Ott et al., 2019). We generate translation hypotheses with greedy decoding and evaluate the latency-quality trade-off by measuring casesensitive detokenized BLEU (Papineni et al., 2002) and word-level Average Lagging (AL) (Ma et al., 2019). Table 5: Parallel training data for the MT systems. Results. We show in Figure 2 the performance of our systems on the test set (MuST tst-COMMON) measured with the provided evaluation server.7 We denote with ktrain =∞ a unidirectional model trained for wait-until-end decoding i.e. reading the full source before writing the target. We evaluate four wait-k systems each trained with a value of ktrain in {5, 7, 9, ∞} and decoded with keval ranging from 2 to 11. We then ensemble the aforementioned wait-k models and evaluate a multipath model t"
2020.iwslt-1.2,P16-1162,0,0.0399967,"t which path to optimize (some values of k, e.g. 5, underperform in comparison). Ensembling the wait-k models gives a boost of 1.43 BLEU points on average. and WMT’19 data,6 namely, Europarl (Koehn, 2005), News Commentary (Tiedemann, 2012) and Common Crawl (Smith et al., 2013). We remove pairs with a length-ratio exceeding 1.3 from Common Crawl and pairs exceeding a length-ratio of 1.5 from the rest. We develop on MuST-C dev and report results on MuST-C tst-COMMON. For open-vocabulary translation, we use SentencePiece (Kudo and Richardson, 2018) to segment the bitexts with byte pair encoding (Sennrich et al., 2016). This results in a joint vocabulary of 32K types. Details of the training data are provided in Table 5. We train Transformer big architectures and tie the embeddings of the encoder with the decoder’s input and output embeddings. We optimize our models with label-smoothed maximum likelihood (Szegedy et al., 2016) with a smoothing rate  = 0.1. The parameters are updated using 6 27 7 https://github.com/pytorch/fairseq/ blob/simulastsharedtask/examples/ simultaneous_translation http://www.statmt.org/wmt19/ 40 TED-LIUM 3 How2 Europarl #hours #words #speakers 452 365 94 5.05M 3.31M 0.75M 2,028 13,"
2020.iwslt-1.2,P13-1135,0,0.0524922,"Missing"
2020.iwslt-1.2,Q19-1020,0,0.0316591,"models for the English-German language pair, • IWSLT 2020 simultaneous translation track with a cascade of an ASR system trained using Kaldi (Povey et al., 2011) and an online MT system with wait-k policies (Dalvi et al., 2018; Ma et al., 2019). Introduction While cascaded speech-to-text translation (AST) systems (combining source language speech recognition (ASR) and source-to-target text translation (MT)) remain state-of-the-art, recent works have attempted to build end-to-end AST with very encouraging results (Bérard et al., 2016; Weiss et al., 2017; Bérard et al., 2018; Jia et al., 2019; Sperber et al., 2019). This year, IWSLT 2020 offline translation track attempts to evaluate if endto-end AST will close the gap with cascaded AST for the English-to-German language pair. Another increasingly popular topic is simultaneous (online) machine translation which consists in generating an output hypothesis before the entire ∗ This paper goes as follows: we review the systems built for the offline speech translation track in §2. Then, we present our approaches to the simultaneous track for both text-to-text and speech-to-text subtasks in §3. We ultimately conclude this work in §4. 2 Offline Speech translat"
2020.iwslt-1.2,tiedemann-2012-parallel,0,0.0284139,"ore writing the target. We evaluate four wait-k systems each trained with a value of ktrain in {5, 7, 9, ∞} and decoded with keval ranging from 2 to 11. We then ensemble the aforementioned wait-k models and evaluate a multipath model that jointly optimizes a large set of wait-k paths. The results demonstrate that multipath is competetive with wait-k without the need to select which path to optimize (some values of k, e.g. 5, underperform in comparison). Ensembling the wait-k models gives a boost of 1.43 BLEU points on average. and WMT’19 data,6 namely, Europarl (Koehn, 2005), News Commentary (Tiedemann, 2012) and Common Crawl (Smith et al., 2013). We remove pairs with a length-ratio exceeding 1.3 from Common Crawl and pairs exceeding a length-ratio of 1.5 from the rest. We develop on MuST-C dev and report results on MuST-C tst-COMMON. For open-vocabulary translation, we use SentencePiece (Kudo and Richardson, 2018) to segment the bitexts with byte pair encoding (Sennrich et al., 2016). This results in a joint vocabulary of 32K types. Details of the training data are provided in Table 5. We train Transformer big architectures and tie the embeddings of the encoder with the decoder’s input and output"
2020.jeptalnrecital-taln.26,D19-1572,0,0.0323304,"Missing"
2020.jeptalnrecital-taln.26,P18-1031,0,0.0541897,"Missing"
2020.jeptalnrecital-taln.26,P19-1340,0,0.0367789,"Missing"
2020.jeptalnrecital-taln.26,P18-1249,0,0.0395933,"Missing"
2020.jeptalnrecital-taln.26,P07-2045,0,0.0108736,"Missing"
2020.jeptalnrecital-taln.26,P10-1023,0,0.119362,"Missing"
2020.jeptalnrecital-taln.26,D14-1162,0,0.0892081,"Missing"
2020.jeptalnrecital-taln.26,N18-1202,0,0.0990203,"Missing"
2020.jeptalnrecital-taln.26,P10-1114,0,0.0829177,"Missing"
2020.jeptalnrecital-taln.26,D17-1039,0,0.0479498,"Missing"
2020.jeptalnrecital-taln.26,W13-4917,0,0.0975746,"112 500 paires de phrases annotées avec les étiquettes entailment, contradiction ou neutre. FLUE intègre la partie française de ce corpus. Analyse syntaxique et étiquetage morphosyntaxique Nous considérons deux tâches d’analyse syntaxique : analyse en constituants et en dépendances, ainsi que l’étiquetage morphosyntaxique. Pour cela, nous utilisons le French Treebank (Abeillé et al., 2003), une collection de phrases du Monde annotées manuellement en constituants et dépendances syntaxiques. Nous utilisons la version de ce corpus de la campagne d’évaluation SPMRL 2014 décrite par Seddah et al. (2013), qui contient 14759, 1235 et 2541 phrases pour respectivement l’entraînement, le développement et l’évaluation. 271 Désambiguïsation lexicale des verbes et des noms La désambiguïsation lexicale consiste à assigner un sens, parmi un inventaire donné, à des mots d’une phrase. Pour la désambiguïsation lexicale de verbes, nous utilisons les données de FrenchSemEval (Segonne et al., 2019). Il s’agit d’un corpus d’évaluation dont les occurrences de verbes ont été annotées manuellement avec les sens de Wiktionary. 10 Pour la désambiguïsation lexicale des noms, nous utilisons la partie française de l"
2020.jeptalnrecital-taln.26,W19-0422,1,0.888597,"Missing"
2020.jeptalnrecital-taln.26,P16-1162,0,0.146858,"Missing"
2020.jeptalnrecital-taln.26,tiedemann-2012-parallel,0,0.0825572,"Missing"
2020.jeptalnrecital-taln.26,2019.gwc-1.14,1,0.893471,"Missing"
2020.jeptalnrecital-taln.26,W18-5446,0,0.0474061,"Missing"
2020.jeptalnrecital-taln.26,N18-1101,0,0.0610743,"Missing"
2020.jeptalnrecital-taln.26,D19-1382,0,0.0395373,"Missing"
2020.jeptalnrecital-taln.26,N19-1131,0,0.0313441,"Missing"
2020.lrec-1.21,H93-1061,0,0.763148,"then relied on the state-of-the-art WSD system proposed by (Vial et al., 2019), which is implemented in the open-source tool disambiguate2 . In their work, the authors only provide a model able to disambiguate English text, but their tool can be used to train a new disambiguation model in any language, given a set of sense annotated data used for training. We used the state of the art English to French Machine Translation system of the tool fairseq3 which also provide alignment between the source and the target words, and we translated the two corpora used by (Vial et al., 2019): the SemCor (Miller et al., 1993) and the WordNet Gloss Corpus4 . We then used these two corpora as training data for 2 https://github.com/getalp/disambiguate https://github.com/pytorch/fairseq 4 https://wordnetcode.princeton.edu/ glosstag-files/glosstag.shtml 3 the WSD system. Finally, once the WSD system trained, we used its predictions to facilitate the work of the mapping pictogramto-WordNet. For instance, if the annotator wanted to map a pictogram for “take a shower” (in French “prendre une douche”), the sense predicted by the WSD system on the word “take” could help to take a decision. We present in (Vaschalde et al., 2"
2020.lrec-1.21,2019.gwc-1.14,1,0.829306,"ive language, which gives the most probable WordNet sense to each word in a given French sentence. For the construction of this system, we exploited the method proposed by (Hadj Salah et al., 2018), which consists in automatically translating and aligning English sense annotated corpora into another language (in our case French), in order to have French sense annotated corpora. Indeed, manually sense annotated data are rare and almost nonexistent in non-English languages, but they are useful for building a good-quality WSD system. We then relied on the state-of-the-art WSD system proposed by (Vial et al., 2019), which is implemented in the open-source tool disambiguate2 . In their work, the authors only provide a model able to disambiguate English text, but their tool can be used to train a new disambiguation model in any language, given a set of sense annotated data used for training. We used the state of the art English to French Machine Translation system of the tool fairseq3 which also provide alignment between the source and the target words, and we translated the two corpora used by (Vial et al., 2019): the SemCor (Miller et al., 1993) and the WordNet Gloss Corpus4 . We then used these two cor"
2020.lrec-1.302,2020.osact-1.2,0,0.0214996,"QuAD (Rajpurkar et al., 2018), surpassing previous methods by a large margin. 2.2. Pre-trained Language Models Beyond English Given the impact of pre-trained language models on NLP downstream tasks in English, several works have recently released pre-trained models for other languages. For instance, ELMo exists for Portuguese, Japanese, German and Basque,5 while BERT and variants were specifically trained for simplified and traditional Chinese8 and German.6 A Portuguese version of MultiFiT is also available.7 Recently, more monolingual BERT-based models have been released, such as for Arabic (Antoun et al., 2020), Dutch (de Vries et al., 2019; Delobelle et al., 2020), Finnish (Virtanen et al., 2019), Italian (Polignano et al., 2019), Portuguese (Souza et al., 2019), Russian (Kuratov and Arkhipov, 2019), Spanish (Ca˜nete et al., 2020), and Vietnamese (Nguyen and Nguyen, 2020). For French, besides pre-trained language models using ULMFiT and MultiFiT configurations,7 CamemBERT (Martin et al., 2019) is a French BERT model concurrent to our work. Another trend considers one model estimated for several languages with a shared vocabulary. The release of multilingual BERT for 104 languages pioneered this app"
2020.lrec-1.302,Q19-1038,0,0.0214599,"19), Portuguese (Souza et al., 2019), Russian (Kuratov and Arkhipov, 2019), Spanish (Ca˜nete et al., 2020), and Vietnamese (Nguyen and Nguyen, 2020). For French, besides pre-trained language models using ULMFiT and MultiFiT configurations,7 CamemBERT (Martin et al., 2019) is a French BERT model concurrent to our work. Another trend considers one model estimated for several languages with a shared vocabulary. The release of multilingual BERT for 104 languages pioneered this approach.8 A recent extension of this work leverages parallel data to build a cross-lingual pre-trained version of LASER (Artetxe and Schwenk, 2019) for 93 languages, XLM (Lample and Conneau, 2019) and XLM-R (Conneau et al., 2019) for 100 languages. 2.3. Evaluation Protocol for French NLP Tasks The existence of a multi-task evaluation benchmark such as GLUE (Wang et al., 2018) for English is highly beneficial to facilitate research in the language of interest. The GLUE benchmark has become a prominent framework to evaluate the performance of NLP models in English. The recent contributions based on pre-trained language models have led to remarkable performance across a wide range of Natural Language Understanding (NLU) tasks. The authors o"
2020.lrec-1.302,W06-1615,0,0.0880109,"2 1 985 XNLI-FR Diverse genres 392 702 2 490 5 010 French Treebank Daily newspaper 14 759 1 235 2 541 FrenchSemEval Diverse genres 55 206 - 3 199 Noun Sense Disambiguation Diverse genres 818 262 - 1 445 Table 2: Descriptions of the datasets included in our FLUE benchmark. 4.1. Text Classification CLS The Cross Lingual Sentiment CLS (Prettenhofer and Stein, 2010) dataset consists of Amazon reviews for three product categories: books, DVD, and music in four languages: English, French, German, and Japanese. Each sample contains a review text and the associated rating from 1 to 5 stars. Following Blitzer et al. (2006) and Prettenhofer and Stein (2010), ratings with 3 stars are removed. Positive reviews have ratings higher than 3 and negative reviews are those rated lower than 3. There is one train and test set for each product category. The train and test sets are balanced, including around 1 000 positive and 1 000 negative reviews for a total of 2 000 reviews in each dataset. We take the French portion to create the binary text classification task in FLUE and report the accuracy on the test set. 4.2. Paraphrasing PAWS-X The Cross-lingual Adversarial Dataset for Paraphrase Identification PAWS-X (Yang et al"
2020.lrec-1.302,P17-1152,0,0.0617991,"Missing"
2020.lrec-1.302,D18-1269,0,0.0239485,"gement. The paraphrasing task is to identify whether the sentences in these pairs are semantically equivalent or not. Similar to previous approaches to create multilingual corpora, Yang et al. (2019a) used machine translation to create the training set for each target language in PAWS-X from the English training set in PAWS. The development and test sets for each language are translated by human translators. We take the related datasets for French to perform the paraphrasing task and report the accuracy on the test set. 4.3. Natural Language Inference XNLI The Cross-lingual NLI (XNLI) corpus (Conneau et al., 2018) extends the development and test sets of the Multi-Genre Natural Language Inference corpus (Williams et al., 2018, MultiNLI) to 15 languages. The development and test sets for each language consist of 7 500 humanannotated examples, making up a total of 112 500 sentence pairs annotated with the labels entailment, contradiction, or neutral. Each sentence pair includes a premise (p) and a hypothesis (h). The Natural Language Inference (NLI) task, also known as recognizing textual entailment (RTE), is to determine whether p entails, contradicts or neither entails nor contradicts h. We take the Fr"
2020.lrec-1.302,P19-4007,0,0.0544476,"Missing"
2020.lrec-1.302,W13-4905,0,0.054327,"shared task organizers. Our word representations are a concatenation of word embeddings and tag embeddings learned together with the model parameters on the French Treebank data itself, and at most one of (fastText, CamemBERT, FlauBERTBASE , FlauBERTBASE , mBERT) word vector. As Dozat and Manning (2016), we use word and tag dropout (d = 0.5) on word and tag embeddings but without dropout on BERT representations. We performed a fairly comprehensive grid search on hyperparameters for each model tested. Results The results are reported in Table 7. The best published results in this shared task (Constant et al., 2013) were involving an ensemble of parsers with additional resources for modelling multi word expressions (MWE), typical of the French treebank annotations. The monolingual French BERT models (CamemBERT, FlauBERT) perform better and set the new state of the art on this dataset with a single parser and without specific modelling for MWEs. One can observe that both FlauBERT models perform marginally better than CamemBERT, while all of them outperform mBERT by a large margin. 2484 Model UAS LAS Best published (Constant et al., 2013) 89.19 85.86 No pre-training fastText pre-training mBERT CamemBERT Fl"
2020.lrec-1.302,2020.findings-emnlp.292,0,0.0289963,"ethods by a large margin. 2.2. Pre-trained Language Models Beyond English Given the impact of pre-trained language models on NLP downstream tasks in English, several works have recently released pre-trained models for other languages. For instance, ELMo exists for Portuguese, Japanese, German and Basque,5 while BERT and variants were specifically trained for simplified and traditional Chinese8 and German.6 A Portuguese version of MultiFiT is also available.7 Recently, more monolingual BERT-based models have been released, such as for Arabic (Antoun et al., 2020), Dutch (de Vries et al., 2019; Delobelle et al., 2020), Finnish (Virtanen et al., 2019), Italian (Polignano et al., 2019), Portuguese (Souza et al., 2019), Russian (Kuratov and Arkhipov, 2019), Spanish (Ca˜nete et al., 2020), and Vietnamese (Nguyen and Nguyen, 2020). For French, besides pre-trained language models using ULMFiT and MultiFiT configurations,7 CamemBERT (Martin et al., 2019) is a French BERT model concurrent to our work. Another trend considers one model estimated for several languages with a shared vocabulary. The release of multilingual BERT for 104 languages pioneered this approach.8 A recent extension of this work leverages paral"
2020.lrec-1.302,N19-1423,0,0.620298,"is-diderot.fr, alexandre.allauzen@espci.fr Abstract Language models have become a key step to achieve state-of-the art results in many different Natural Language Processing (NLP) tasks. Leveraging the huge amount of unlabeled texts nowadays available, they provide an efficient way to pre-train continuous word representations that can be fine-tuned for a downstream task, along with their contextualization at the sentence level. This has been widely demonstrated for English using contextualized representations (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019b). In this paper, we introduce and share FlauBERT, a model learned on a very large and heterogeneous French corpus. Models of different sizes are trained using the new CNRS (French National Centre for Scientific Research) Jean Zay supercomputer. We apply our French language models to diverse NLP tasks (text classification, paraphrasing, natural language inference, parsing, word sense disambiguation) and show that most of the time they outperform other pre-training approaches. Different versions of FlauBERT as well as a unified evaluation protocol for the downstream tasks, c"
2020.lrec-1.302,eisele-chen-2010-multiun,0,0.103871,"Missing"
2020.lrec-1.302,D19-1572,0,0.0171919,"architecture, while ELMo adopts a bidirectional LSTM to build the final embedding for each input token from the concatenation of the left-to-right and rightto-left representations. Another fundamental difference lies in how each model can be tuned to different downstream tasks: ELMo delivers different word vectors that can be interpolated, whereas ULMFiT enables robust fine-tuning of the whole network w.r.t. the downstream tasks. The ability of fine-tuning was shown to significantly boost the performance, and thus this approach has been further developed in the recent works such as MultiFiT (Eisenschlos et al., 2019) or most prominently Transformer-based (Vaswani et al., 2017) architectures: GPT (Radford et al., 2018), BERT (Devlin et al., 2019), XLNet (Yang et al., 2019b), XLM (Lample and Conneau, 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2019), T5 (Raffel et al., 2019). These methods have one after the other established new state-ofthe-art results on various NLP benchmarks, such as GLUE (Wang et al., 2018) or SQuAD (Rajpurkar et al., 2018), surpassing previous methods by a large margin. 2.2. Pre-trained Language Models Beyond English Given the impact of pre-trained language models on NLP do"
2020.lrec-1.302,P18-1031,0,0.289639,"incent.segonne@etu, bcrabbe@linguist}.univ-paris-diderot.fr, alexandre.allauzen@espci.fr Abstract Language models have become a key step to achieve state-of-the art results in many different Natural Language Processing (NLP) tasks. Leveraging the huge amount of unlabeled texts nowadays available, they provide an efficient way to pre-train continuous word representations that can be fine-tuned for a downstream task, along with their contextualization at the sentence level. This has been widely demonstrated for English using contextualized representations (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019b). In this paper, we introduce and share FlauBERT, a model learned on a very large and heterogeneous French corpus. Models of different sizes are trained using the new CNRS (French National Centre for Scientific Research) Jean Zay supercomputer. We apply our French language models to diverse NLP tasks (text classification, paraphrasing, natural language inference, parsing, word sense disambiguation) and show that most of the time they outperform other pre-training approaches. Different versions of FlauBERT as well as a unified eval"
2020.lrec-1.302,P18-1249,0,0.141589,"Table 5. The results confirm the superiority of the French models compared to the multilingual model mBERT on this task. FlauBERTLARGE performs moderately better than CamemBERT. Both of them clearly outperform XLM-RBASE , while cannot surpass XLM-RLARGE . Model XLM-RLARGE † XLM-RBASE † mBERT‡ CamemBERT ‡ FlauBERTBASE FlauBERTLARGE † ‡ Accuracy 85.2 80.1 76.9 81.2 80.6 83.4 Results reported in (Conneau et al., 2019). Results reported in (Martin et al., 2019). Table 5: Results on the French XNLI dataset. 5.4. Constituency Parsing and POS Tagging Model description We use the parser described by Kitaev and Klein (2018) and Kitaev et al. (2019). It is an openly available19 chart parser based on a self-attentive encoder. We compare (i) a model without any pre-trained parameters, (ii) a model that additionally uses and fine-tunes fastText20 pre-trained embeddings, (iii) models based on pre-trained language models: mBERT, CamemBERT, and FlauBERT. We use the default hyperparameters from Kitaev and Klein (2018) for the first two settings and the hyperparameters from Kitaev et al. (2019) when using pretrained language models, except for FlauBERTLARGE . For this last model, we use a different learning rate (0.00001"
2020.lrec-1.302,P19-1340,0,0.0189308,"m the superiority of the French models compared to the multilingual model mBERT on this task. FlauBERTLARGE performs moderately better than CamemBERT. Both of them clearly outperform XLM-RBASE , while cannot surpass XLM-RLARGE . Model XLM-RLARGE † XLM-RBASE † mBERT‡ CamemBERT ‡ FlauBERTBASE FlauBERTLARGE † ‡ Accuracy 85.2 80.1 76.9 81.2 80.6 83.4 Results reported in (Conneau et al., 2019). Results reported in (Martin et al., 2019). Table 5: Results on the French XNLI dataset. 5.4. Constituency Parsing and POS Tagging Model description We use the parser described by Kitaev and Klein (2018) and Kitaev et al. (2019). It is an openly available19 chart parser based on a self-attentive encoder. We compare (i) a model without any pre-trained parameters, (ii) a model that additionally uses and fine-tunes fastText20 pre-trained embeddings, (iii) models based on pre-trained language models: mBERT, CamemBERT, and FlauBERT. We use the default hyperparameters from Kitaev and Klein (2018) for the first two settings and the hyperparameters from Kitaev et al. (2019) when using pretrained language models, except for FlauBERTLARGE . For this last model, we use a different learning rate (0.00001), batch size (8) and ign"
2020.lrec-1.302,P07-2045,0,0.0134554,"to extract the text or download them directly from their websites. The total size of the uncompressed text before preprocessing is 270 GB. More details can be found in Appendix A.1. Data preprocessing For all sub-corpora, we filtered out very short sentences as well as repetitive and nonmeaningful content such as telephone/fax numbers, email addresses, etc. For Common Crawl, which is our largest sub-corpus with 215 GB of raw text, we applied aggressive cleaning to reduce its size to 43.4 GB. All the data were Unicode-normalized in a consistent way before being tokenized using Moses tokenizer (Koehn et al., 2007). The resulting training corpus is 71 GB in size. Our code for downloading and preprocessing data is made publicly available.13 3.2. Models and Training Configurations Model architecture FlauBERT has the same model architecture as BERT (Devlin et al., 2019), which consists of a multi-layer bidirectional Transformer (Vaswani et al., 2017). Following Devlin et al. (2019), we propose two model sizes: • FlauBERTBASE : L = 12, H = 768, A = 12, • FlauBERTLARGE : L = 24, H = 1024, A = 16, where L, H and A respectively denote the number of Transformer blocks, the hidden size, and the number of selfatt"
2020.lrec-1.302,2005.mtsummit-papers.11,0,0.015061,"Missing"
2020.lrec-1.302,W19-5303,0,0.226301,"ding FlauBERT In this section, we describe the training corpus, the text preprocessing pipeline, the model architecture and training configurations to build FlauBERTBASE and FlauBERTLARGE . 3.1. Training Data Data collection Our French text corpus consists of 24 sub-corpora gathered from different sources, covering diverse topics and writing styles, ranging from formal and well-written text (e.g. Wikipedia and books)10 to random text crawled from the Internet (e.g. Common Crawl).11 The data were collected from three main sources: (1) monolingual data for French provided in WMT19 shared tasks (Li et al., 2019, 4 sub-corpora); (2) French text corpora offered in the OPUS collection (Tiedemann, 2012, 8 sub-corpora); and (3) datasets available in the Wikimedia projects (Meta, 2019, 8 sub-corpora). We used the WikiExtractor tool12 to extract the text from Wikipedia. For the other sub-corpora, we either used our 7 https://github.com/piegu/language-models https://github.com/google-research/bert 9 https://github.com/chineseGLUE/chineseGLUE 10 http://www.gutenberg.org 11 http://data.statmt.org/ngrams/deduped2017 12 https://github.com/attardi/wikiextractor 8 4 It should be noted that learning contextual emb"
2020.lrec-1.302,L16-1147,0,0.0701355,"Missing"
2020.lrec-1.302,H93-1061,0,0.0518853,"rt of the Multilingual WSD task of SemEval 2013 (Navigli et al., 2013), which targets nouns only. We adapted the task to use the WordNet 3.0 sense inventory (Miller, 1995) instead of BabelNet (Navigli and Ponzetto, 2010), by converting the sense keys to WordNet 3.0 if a mapping exists in BabelNet, and removing them otherwise. The result of the conversion process is an evaluation corpus composed of 306 sentences and 1 445 French nouns annotated with WordNet sense keys, and manually verified. For the training data, we followed the method proposed by Hadj Salah (2018), and translated the SemCor (Miller et al., 1993) and the WordNet Gloss Corpus16 into French, using the best English-French Machine Translation system of the fairseq toolkit17 (Ott et al., 2019). Finally, we aligned the WordNet sense annotation from the source English words to the the translated French words, using the alignment provided by the MT system. We rely on WordNet sense keys instead of the original BabelNet annotations for the following two reasons. First, WordNet is a resource that is entirely manually verified, and widely used in WSD research (Navigli, 2009). Second, there is already a large quantity of sense annotated data based"
2020.lrec-1.302,P10-1023,0,0.0572014,"(04-20-2018) openly available via Dbnary (S´erasset, 2012). For a given sense of a target key, the sense inventory offers a definition along with one or more examples. For this task, we considered the examples of the sense inventory as training examples and tested our model on the evaluation dataset. Noun Sense Disambiguation We propose a new challenging task for the WSD of French, based on the French part of the Multilingual WSD task of SemEval 2013 (Navigli et al., 2013), which targets nouns only. We adapted the task to use the WordNet 3.0 sense inventory (Miller, 1995) instead of BabelNet (Navigli and Ponzetto, 2010), by converting the sense keys to WordNet 3.0 if a mapping exists in BabelNet, and removing them otherwise. The result of the conversion process is an evaluation corpus composed of 306 sentences and 1 445 French nouns annotated with WordNet sense keys, and manually verified. For the training data, we followed the method proposed by Hadj Salah (2018), and translated the SemCor (Miller et al., 1993) and the WordNet Gloss Corpus16 into French, using the best English-French Machine Translation system of the fairseq toolkit17 (Ott et al., 2019). Finally, we aligned the WordNet sense annotation from"
2020.lrec-1.302,S13-2040,0,0.0327036,"ated version to French provided in XNLI. Following Conneau et al. (2018), we report the test accuracy. 4.4. Parsing and Part-of-Speech Tagging Syntactic parsing consists in assigning a tree structure to a sentence in natural language. We perform parsing on the French Treebank (Abeill´e et al., 2003), a collection of sentences extracted from French daily newspaper Le Monde, and manually annotated with both constituency and dependency syntactic trees and part-of-speech tags. Specifically, we use the version of the corpus instantiated for the SPMRL 2013 shared task and described by Seddah et al. (2013). This version is provided with a standard split representing 14 759 sentences for the training corpus, and respectively 1 235 and 2 541 sentences for the development and evaluation sets. 4.5. Word Sense Disambiguation Tasks Word Sense Disambiguation (WSD) is a classification task which aims to predict the sense of words in a given context according to a specific sense inventory. We used two French WSD tasks: the FrenchSemEval task (Segonne et al., 2019), which targets verbs only, and a modified version of the French part of the Multilingual WSD task of SemEval 2013 (Navigli et al., 2013), whi"
2020.lrec-1.302,2020.findings-emnlp.92,0,0.0305509,"ls for other languages. For instance, ELMo exists for Portuguese, Japanese, German and Basque,5 while BERT and variants were specifically trained for simplified and traditional Chinese8 and German.6 A Portuguese version of MultiFiT is also available.7 Recently, more monolingual BERT-based models have been released, such as for Arabic (Antoun et al., 2020), Dutch (de Vries et al., 2019; Delobelle et al., 2020), Finnish (Virtanen et al., 2019), Italian (Polignano et al., 2019), Portuguese (Souza et al., 2019), Russian (Kuratov and Arkhipov, 2019), Spanish (Ca˜nete et al., 2020), and Vietnamese (Nguyen and Nguyen, 2020). For French, besides pre-trained language models using ULMFiT and MultiFiT configurations,7 CamemBERT (Martin et al., 2019) is a French BERT model concurrent to our work. Another trend considers one model estimated for several languages with a shared vocabulary. The release of multilingual BERT for 104 languages pioneered this approach.8 A recent extension of this work leverages parallel data to build a cross-lingual pre-trained version of LASER (Artetxe and Schwenk, 2019) for 93 languages, XLM (Lample and Conneau, 2019) and XLM-R (Conneau et al., 2019) for 100 languages. 2.3. Evaluation Prot"
2020.lrec-1.302,N19-4009,0,0.0205028,"nventory (Miller, 1995) instead of BabelNet (Navigli and Ponzetto, 2010), by converting the sense keys to WordNet 3.0 if a mapping exists in BabelNet, and removing them otherwise. The result of the conversion process is an evaluation corpus composed of 306 sentences and 1 445 French nouns annotated with WordNet sense keys, and manually verified. For the training data, we followed the method proposed by Hadj Salah (2018), and translated the SemCor (Miller et al., 1993) and the WordNet Gloss Corpus16 into French, using the best English-French Machine Translation system of the fairseq toolkit17 (Ott et al., 2019). Finally, we aligned the WordNet sense annotation from the source English words to the the translated French words, using the alignment provided by the MT system. We rely on WordNet sense keys instead of the original BabelNet annotations for the following two reasons. First, WordNet is a resource that is entirely manually verified, and widely used in WSD research (Navigli, 2009). Second, there is already a large quantity of sense annotated data based on the sense inventory of WordNet (Vial et al., 2018) that we can use for the training of our system. We publicly release18 both our training da"
2020.lrec-1.302,D14-1162,0,0.0923659,"ding Evaluation), are shared to the research community for further reproducible experiments in French NLP. Keywords: FlauBERT, FLUE, BERT, Transformer, French, language model, pre-training, NLP benchmark, text classification, parsing, word sense disambiguation, natural language inference, paraphrase. 1. Introduction A recent game-changing contribution in Natural Language Processing (NLP) was the introduction of deep unsupervised language representations pre-trained using only plain text corpora. Previous word embedding pre-training approaches, such as word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014), learn a single vector for each wordform. By contrast, these new models are trained to produce contextual embeddings: the output representation depends on the entire input sequence (e.g. each token instance has a vector representation that depends on its left and right context). Initially based on recurrent neural networks (Dai and Le, 2015; Ramachandran et al., 2017; Howard and Ruder, 2018; Peters et al., 2018), these models quickly converged towards the use of the Transformer (Vaswani et al., 2017), such as GPT (Radford et al., 2018), BERT (Devlin et al., 2019), XLNet (Yang et al., 2019b),"
2020.lrec-1.302,N18-1202,0,0.806323,"-grenoble-alpes.fr {vincent.segonne@etu, bcrabbe@linguist}.univ-paris-diderot.fr, alexandre.allauzen@espci.fr Abstract Language models have become a key step to achieve state-of-the art results in many different Natural Language Processing (NLP) tasks. Leveraging the huge amount of unlabeled texts nowadays available, they provide an efficient way to pre-train continuous word representations that can be fine-tuned for a downstream task, along with their contextualization at the sentence level. This has been widely demonstrated for English using contextualized representations (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019b). In this paper, we introduce and share FlauBERT, a model learned on a very large and heterogeneous French corpus. Models of different sizes are trained using the new CNRS (French National Centre for Scientific Research) Jean Zay supercomputer. We apply our French language models to diverse NLP tasks (text classification, paraphrasing, natural language inference, parsing, word sense disambiguation) and show that most of the time they outperform other pre-training approaches. Different versions of FlauBERT a"
2020.lrec-1.302,P10-1114,0,0.0398144,"Missing"
2020.lrec-1.302,P18-2124,0,0.0233078,"f fine-tuning was shown to significantly boost the performance, and thus this approach has been further developed in the recent works such as MultiFiT (Eisenschlos et al., 2019) or most prominently Transformer-based (Vaswani et al., 2017) architectures: GPT (Radford et al., 2018), BERT (Devlin et al., 2019), XLNet (Yang et al., 2019b), XLM (Lample and Conneau, 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2019), T5 (Raffel et al., 2019). These methods have one after the other established new state-ofthe-art results on various NLP benchmarks, such as GLUE (Wang et al., 2018) or SQuAD (Rajpurkar et al., 2018), surpassing previous methods by a large margin. 2.2. Pre-trained Language Models Beyond English Given the impact of pre-trained language models on NLP downstream tasks in English, several works have recently released pre-trained models for other languages. For instance, ELMo exists for Portuguese, Japanese, German and Basque,5 while BERT and variants were specifically trained for simplified and traditional Chinese8 and German.6 A Portuguese version of MultiFiT is also available.7 Recently, more monolingual BERT-based models have been released, such as for Arabic (Antoun et al., 2020), Dutch ("
2020.lrec-1.302,D17-1039,0,0.213586,"uage Processing (NLP) was the introduction of deep unsupervised language representations pre-trained using only plain text corpora. Previous word embedding pre-training approaches, such as word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014), learn a single vector for each wordform. By contrast, these new models are trained to produce contextual embeddings: the output representation depends on the entire input sequence (e.g. each token instance has a vector representation that depends on its left and right context). Initially based on recurrent neural networks (Dai and Le, 2015; Ramachandran et al., 2017; Howard and Ruder, 2018; Peters et al., 2018), these models quickly converged towards the use of the Transformer (Vaswani et al., 2017), such as GPT (Radford et al., 2018), BERT (Devlin et al., 2019), XLNet (Yang et al., 2019b), RoBERTa (Liu et al., 2019). Using these pre-trained models in a transfer learning fashion has shown to yield striking improvements across a wide range of NLP tasks. One can easily build state-of-the-art NLP systems thanks to the publicly available pre-trained weights, saving time, energy, and resources. As a consequence, unsupervised language model pre-training has be"
2020.lrec-1.302,W13-4917,0,0.0460081,"Missing"
2020.lrec-1.302,W19-0422,1,0.895578,"Missing"
2020.lrec-1.302,P16-1162,0,0.0200296,"(attention) layers at each training step. Other techniques are also available such as progressive training (Gong et al., 2019), or improving initialization (Zhang et al., 2019a; Xu et al., 2019) and normalization (Nguyen and Salazar, 2019). For training FlauBERTLARGE , we employed pre-norm attention and stochastic depths for their simplicity. We found that these two techniques were sufficient for successful training. We set the rate of layer dropping to 0.2 in all the experiments. Other training details A vocabulary of 50K sub-word units is built using the Byte Pair Encoding (BPE) algorithm (Sennrich et al., 2016). The only difference between our work and RoBERTa is that the training data are preprocessed and tokenized using a basic tokenizer for French (Koehn et al., 2007, Moses), as in XLM (Lample and Conneau, 2019), before the application of BPE. We use fastBPE,15 a very efficient implementation to extract the BPE units and encode the corpora. 15 2481 https://github.com/glample/fastBPE FlauBERTBASE is trained on 32 GPUs Nvidia V100 in 410 hours and FlauBERTLARGE is trained on 128 GPUs in 390 hours, both with the effective batch size of 8192 sequences. Finally, we summarize the differences between Fl"
2020.lrec-1.302,serasset-2012-dbnary,0,0.036519,"Missing"
2020.lrec-1.302,skadins-etal-2014-billions,0,0.0456176,"Missing"
2020.lrec-1.302,tiedemann-2012-parallel,0,0.180846,"peline, the model architecture and training configurations to build FlauBERTBASE and FlauBERTLARGE . 3.1. Training Data Data collection Our French text corpus consists of 24 sub-corpora gathered from different sources, covering diverse topics and writing styles, ranging from formal and well-written text (e.g. Wikipedia and books)10 to random text crawled from the Internet (e.g. Common Crawl).11 The data were collected from three main sources: (1) monolingual data for French provided in WMT19 shared tasks (Li et al., 2019, 4 sub-corpora); (2) French text corpora offered in the OPUS collection (Tiedemann, 2012, 8 sub-corpora); and (3) datasets available in the Wikimedia projects (Meta, 2019, 8 sub-corpora). We used the WikiExtractor tool12 to extract the text from Wikipedia. For the other sub-corpora, we either used our 7 https://github.com/piegu/language-models https://github.com/google-research/bert 9 https://github.com/chineseGLUE/chineseGLUE 10 http://www.gutenberg.org 11 http://data.statmt.org/ngrams/deduped2017 12 https://github.com/attardi/wikiextractor 8 4 It should be noted that learning contextual embeddings was also proposed in (McCann et al., 2017), but in a supervised fashion as they u"
2020.lrec-1.302,W18-1819,0,0.025239,"RTLARGE : warmup steps of 30k, peak learning rate of 3e−4, β1 = 0.9, β2 = 0.98,  = 1e−6 and weight decay of 0.01. Training FlauBERTLARGE Training very deep Transformers is known to be susceptible to instability (Wang et al., 2019b; Nguyen and Salazar, 2019; Xu et al., 2019; Fan et al., 2019). Not surprisingly, we also observed this difficulty when training FlauBERTLARGE using the same configurations as BERTLARGE and RoBERTaLARGE , where divergence happened at an early stage. Several methods have been proposed to tackle this issue. For example, in an updated implementation of the Transformer (Vaswani et al., 2018), layer normalization is applied before each attention layer by default, rather than after each residual block as in the original implementation (Vaswani et al., 2017). These configurations are called pre-norm and post-norm, respectively. It was observed by Vaswani et al. (2018), and again confirmed by later works e.g. (Wang et al., 2019b; Xu et al., 2019; Nguyen and Salazar, 2019), that pre-norm helps stabilize training. Recently, a regularization technique called stochastic depths (Huang et al., 2016) has been demonstrated to be very effective for training deep Transformers, by e.g. Pham et"
2020.lrec-1.302,L18-1166,1,0.830909,"French, using the best English-French Machine Translation system of the fairseq toolkit17 (Ott et al., 2019). Finally, we aligned the WordNet sense annotation from the source English words to the the translated French words, using the alignment provided by the MT system. We rely on WordNet sense keys instead of the original BabelNet annotations for the following two reasons. First, WordNet is a resource that is entirely manually verified, and widely used in WSD research (Navigli, 2009). Second, there is already a large quantity of sense annotated data based on the sense inventory of WordNet (Vial et al., 2018) that we can use for the training of our system. We publicly release18 both our training data and the evaluation data in the UFSAC format (Vial et al., 2018). 5. Experiments and Results In this section, we present FlauBERT fine-tuning results on the FLUE benchmark. We compare the performance of FlauBERT with Multilingual BERT (Devlin et al., 2019, mBERT) and CamemBERT (Martin et al., 2019) on all tasks. In addition, for each task we also include the best non-BERT model for comparison. We made use of the open source libraries (Lample and Conneau, 2019, XLM) and (Wolf et al., 2019, Transformers)"
2020.lrec-1.302,2019.gwc-1.14,1,0.892947,"Missing"
2020.lrec-1.302,W18-5446,0,0.470157,"escribe our methodology to build FlauBERT – French Language Understanding via Bidirectional Encoder Representations from Transformers, a French BERT1 model that outperforms multilingual/cross-lingual models in several downstream NLP tasks, under similar configurations. FlauBERT relies on freely available datasets and is made publicly available in different versions.2 For further reproducible experiments, we also provide the complete processing and training pipeline as well as a general benchmark for evaluating French NLP systems. This evaluation setup is similar to the popular GLUE benchmark (Wang et al., 2018), and is named FLUE (French Language Understanding Evaluation). 2. 2.1. Related Work Pre-trained Language Models Self-supervised3 pre-training on unlabeled text data was first proposed in the task of neural language modeling (Bengio et al., 2003; Collobert and Weston, 2008), where it was shown that a neural network trained to predict next word from prior words can learn useful embedding representations, called word embeddings (each word is represented by a fixed vector). These representations were shown to play an important role in NLP, yielding state-of-the-art performance on multiple tasks ("
2020.lrec-1.302,P19-1176,0,0.104394,"2019) and XLM-R (Conneau et al., 2019) for 100 languages. 2.3. Evaluation Protocol for French NLP Tasks The existence of a multi-task evaluation benchmark such as GLUE (Wang et al., 2018) for English is highly beneficial to facilitate research in the language of interest. The GLUE benchmark has become a prominent framework to evaluate the performance of NLP models in English. The recent contributions based on pre-trained language models have led to remarkable performance across a wide range of Natural Language Understanding (NLU) tasks. The authors of GLUE have therefore introduced SuperGLUE (Wang et al., 2019a): a new benchmark built on the principles of GLUE, including more challenging and diverse set of tasks. A Chinese version of GLUE9 is also developed to evaluate model performance in Chinese NLP tasks. As of now, we have not learned of any such benchmark for French. 3. Building FlauBERT In this section, we describe the training corpus, the text preprocessing pipeline, the model architecture and training configurations to build FlauBERTBASE and FlauBERTLARGE . 3.1. Training Data Data collection Our French text corpus consists of 24 sub-corpora gathered from different sources, covering diverse"
2020.lrec-1.302,N18-1101,0,0.0161936,"ot. Similar to previous approaches to create multilingual corpora, Yang et al. (2019a) used machine translation to create the training set for each target language in PAWS-X from the English training set in PAWS. The development and test sets for each language are translated by human translators. We take the related datasets for French to perform the paraphrasing task and report the accuracy on the test set. 4.3. Natural Language Inference XNLI The Cross-lingual NLI (XNLI) corpus (Conneau et al., 2018) extends the development and test sets of the Multi-Genre Natural Language Inference corpus (Williams et al., 2018, MultiNLI) to 15 languages. The development and test sets for each language consist of 7 500 humanannotated examples, making up a total of 112 500 sentence pairs annotated with the labels entailment, contradiction, or neutral. Each sentence pair includes a premise (p) and a hypothesis (h). The Natural Language Inference (NLI) task, also known as recognizing textual entailment (RTE), is to determine whether p entails, contradicts or neither entails nor contradicts h. We take the French part of the XNLI corpus to form the development and test sets for the NLI task in FLUE. The train set is obta"
2020.lrec-1.302,D19-1382,0,0.0391611,"Missing"
2020.lrec-1.302,N19-1131,0,0.100143,"firmed by later works e.g. (Wang et al., 2019b; Xu et al., 2019; Nguyen and Salazar, 2019), that pre-norm helps stabilize training. Recently, a regularization technique called stochastic depths (Huang et al., 2016) has been demonstrated to be very effective for training deep Transformers, by e.g. Pham et al. (2019) and Fan et al. (2019) who successfully trained architectures of more than 40 layers. The idea is to randomly drop a number of (attention) layers at each training step. Other techniques are also available such as progressive training (Gong et al., 2019), or improving initialization (Zhang et al., 2019a; Xu et al., 2019) and normalization (Nguyen and Salazar, 2019). For training FlauBERTLARGE , we employed pre-norm attention and stochastic depths for their simplicity. We found that these two techniques were sufficient for successful training. We set the rate of layer dropping to 0.2 in all the experiments. Other training details A vocabulary of 50K sub-word units is built using the Byte Pair Encoding (BPE) algorithm (Sennrich et al., 2016). The only difference between our work and RoBERTa is that the training data are preprocessed and tokenized using a basic tokenizer for French (Koehn et a"
2021.iwslt-1.20,2020.emnlp-main.480,0,0.0339641,"Missing"
2021.iwslt-1.20,2020.iwslt-1.2,1,0.756555,"7 hours (it) to 189 hours (es). Translation data is part of the ASR talks for a given source language. Our experiments were performed in the constrained setting where only the provided data for the task is used. Model architecture Our system is based on the Dual-decoder Transformer (Le et al., 2020) which consists of an encoder and two decoders. This architecture jointly transcribes and translates an input speech. Each of the decoders is responsible for one task (ASR or ST) while interacting with each other. We refer the reader to the paper for further details. We initially followed Le et al. (2020) and used 12 encoder layers, 6 decoder layers, and a hidden dimension of d = 256. However, this model produced poor results. We hypothesize that with this configuration, the model capacity is too large for the dataset described in the previous section. In the end, we ended up using only 6 encoder layers and 3 decoder layers (with the same d = 256). In addition, we also trained a Transformer model having the same encoder of 6 layers but with only one decoder as the baseline (hereafter called singledecoder model). 3.3 Speech-to-text translation (ST) consists in translating a speech utterance in"
2021.iwslt-1.20,P82-1020,0,0.744682,"Missing"
2021.iwslt-1.20,D18-2012,0,0.0335627,"Missing"
2021.iwslt-1.20,2020.coling-main.314,1,0.844969,") on the test datasets of the low-resource task for the submitted system. et al., 2021), in which there are four source languages (Spanish (es), French (fr), Portuguese (pt), and Italian (it)) and five target languages (the aforementioned source languages plus English (en)). The sizes of the ASR talks range from 107 hours (it) to 189 hours (es). Translation data is part of the ASR talks for a given source language. Our experiments were performed in the constrained setting where only the provided data for the task is used. Model architecture Our system is based on the Dual-decoder Transformer (Le et al., 2020) which consists of an encoder and two decoders. This architecture jointly transcribes and translates an input speech. Each of the decoders is responsible for one task (ASR or ST) while interacting with each other. We refer the reader to the paper for further details. We initially followed Le et al. (2020) and used 12 encoder layers, 6 decoder layers, and a hidden dimension of d = 256. However, this model produced poor results. We hypothesize that with this configuration, the model capacity is too large for the dataset described in the previous section. In the end, we ended up using only 6 enco"
2021.iwslt-1.20,P12-3005,0,0.0177994,"Missing"
2021.iwslt-1.20,D15-1166,0,0.188486,"Missing"
2021.iwslt-1.20,P02-1040,0,0.109262,"Missing"
2021.iwslt-1.20,2020.aacl-demo.6,0,0.0637687,"Missing"
F12-1088,N04-1017,0,0.0246606,"Missing"
F13-2004,2012.iwslt-evaluation.13,1,0.876458,"Missing"
F13-2004,W08-0509,0,0.0378081,"Missing"
F13-2004,2008.amta-srw.3,0,0.0438155,"Missing"
F13-2004,J10-4005,0,0.0591795,"Missing"
F13-2004,P07-2045,0,0.00959479,"Missing"
F13-2004,P09-1066,0,0.0350692,"Missing"
F13-2004,C04-1072,0,0.0295495,"Missing"
F13-2004,P03-1021,0,0.087008,"Missing"
F13-2004,J03-1002,0,0.0103436,"Missing"
F13-2004,W11-2154,1,0.838748,"Missing"
F13-2004,N07-1029,0,0.0571615,"Missing"
F13-2004,P07-1040,0,0.0504359,"Missing"
L16-1221,vacher-etal-2014-sweet,1,0.336331,"Missing"
L16-1221,W15-5121,1,0.818255,"Missing"
L16-1313,W13-3906,0,0.0637923,"Missing"
L16-1313,cristoforetti-etal-2014-dirha,0,0.0736114,"Missing"
L16-1313,W13-3916,1,0.816748,"Missing"
L16-1313,vacher-etal-2014-sweet,1,0.895806,"Missing"
L16-1313,W15-5121,1,0.833563,"Missing"
L16-1313,L16-1221,1,0.78768,"Missing"
L18-1166,S07-1054,0,0.0144781,"rns from examples contained in the annotated corpus. For instance, the OMSTI (Taghipour and Ng, 2015) was created for this purpose. • Evaluate a WSD system by comparing its output to the annotations in the corpus, as it is the case for instance with corpora created as part of the evaluation campaigns SensEval-SemEval. After their distribution, there is no scientific reason not to use indistinctly these corpora either for building a WSD system, for estimating the distribution of senses or for evaluating a WSD system. Indeed, the SemCor is used since a long time for the learning of WSD systems (Chan et al., 2007; Navigli et al., 2007) or more recently for the evaluation of different methods (Yuan et al., 2016). This last usage is still very rare, since it is one of the first experiment that we found in the literature, along with (M`arquez et al., 2002). However, the format of the resources differs greatly depending on their original purpose. For the SemCor, a single file groups all the information, whereas in the case of the evaluation corpora, there are two files: one that contains the unannotated corpus, and the other that contains the sense annotations. In some corpora, like in the DSO and the OMS"
L18-1166,D14-1110,0,0.0548277,"Missing"
L18-1166,H92-1046,0,0.427313,"Missing"
L18-1166,P00-1064,0,0.0451383,"Missing"
L18-1166,H93-1061,0,0.906383,"with an identifier of sense from 1027 a specific lexical database. For example, all words in the corpus of the 7th task of the SemEval 2007 semantic evaluation campaign (Navigli et al., 2007) are annotated with sense identifiers from WordNet 2.1, whereas in the English corpus of the 13th task of SemEval 2015 (Moro and Navigli, 2015), all words are annotated with sense identifiers from WordNet 3.0, BabelNet 2.5 and Wikipedia pages. There are at least three reasons to create a sense annotated corpus: • Estimate the distribution of senses in the language. It is for this purpose that the SemCor (Miller et al., 1993) was annotated. Consequently, the senses in WordNet are, since version 1.7, sorted by this distribution of senses estimated on the SemCor. • Build a Word Sense Disambiguation system which learns from examples contained in the annotated corpus. For instance, the OMSTI (Taghipour and Ng, 2015) was created for this purpose. • Evaluate a WSD system by comparing its output to the annotations in the corpus, as it is the case for instance with corpora created as part of the evaluation campaigns SensEval-SemEval. After their distribution, there is no scientific reason not to use indistinctly these cor"
L18-1166,C12-1109,0,0.0622556,"Missing"
L18-1166,S15-2049,0,0.493012,"the learning process of a Word Sense Disambiguation (WSD) system, the importance of sense annotated corpora in Natural Language Processing (NLP) is considerable. On one hand, the evaluation in vivo, i.e. the evaluation of a WSD system as part of a larger task, has never been really exploited. On the other hand, the evaluation in vitro, which uses directly sense annotated corpora by comparing the output of a system to manual annotations, is predominant. Moreover, WSD systems exploiting examples from sense annotated corpora are generally far better than those which do not (Navigli et al., 2007; Moro and Navigli, 2015). At the time of its creation, WordNet (Miller, 1995) was undoubtedly the only lexical database freely available for English. Since the beginning of the 2000s, it has become the de facto standard for WSD in this language. Indeed, most of sense annotated corpora are either directly annotated with WordNet sense keys or they are annotated with a sense inventory linked to the senses of WordNet, such as BabelNet (Navigli and Ponzetto, 2010). However, it is not trivial to use these corpora, because most of them differ in their format and on the version of WordNet they use. As a consequence, very few"
L18-1166,P10-1023,0,0.0205213,"ions, is predominant. Moreover, WSD systems exploiting examples from sense annotated corpora are generally far better than those which do not (Navigli et al., 2007; Moro and Navigli, 2015). At the time of its creation, WordNet (Miller, 1995) was undoubtedly the only lexical database freely available for English. Since the beginning of the 2000s, it has become the de facto standard for WSD in this language. Indeed, most of sense annotated corpora are either directly annotated with WordNet sense keys or they are annotated with a sense inventory linked to the senses of WordNet, such as BabelNet (Navigli and Ponzetto, 2010). However, it is not trivial to use these corpora, because most of them differ in their format and on the version of WordNet they use. As a consequence, very few works in the literature of WSD are trained or evaluated on more than two annotated corpora. Also, WSD systems are systematically evaluated on corpora that have been initially created for the purpose of evaluation, and never on corpora that have been created for another purpose, such as training or for sense distribution estimation, whereas there is no scientific reason for that. This paper presents a work of unification of all existin"
L18-1166,S07-1006,0,0.081942,"Missing"
L18-1166,E17-1010,0,0.421311,"ing one is used for the evaluation, then switch the corpora and do this for every existing corpus. The language resource that we provide contains all English sense annotated corpora in UFSAC (Unified Format for Sense Annotated Corpora), the format that we propose, with sense annotations converted to the last version of WordNet (3.0), along with Java code to easily read, write and modify any corpus in this format, and scripts for converting a corpus from its original format to UFSAC. Our work is the continuity of the demonstration of (Vial et al., 2017), and it differs from the recent work of (Raganato et al., 2017) in several points. Their work is focused on the evaluation of WSD systems, whereas we provide a complete API for manipulating corpora in a new unified format (UFSAC), and conversion scripts allowing the full reconstruction of the corpora from the original data. We also propose five additional corpora in our resource among the most difficult to parse. In our resource, we provide a script for converting a corpus from our format to theirs, so existing WSD systems that rely on their format can be trained or evaluated on any of the corpus that we produced. We also provide a script for converting t"
L18-1166,C12-1146,1,0.883405,"Missing"
L18-1166,K15-1037,0,0.117918,"task of SemEval 2015 (Moro and Navigli, 2015), all words are annotated with sense identifiers from WordNet 3.0, BabelNet 2.5 and Wikipedia pages. There are at least three reasons to create a sense annotated corpus: • Estimate the distribution of senses in the language. It is for this purpose that the SemCor (Miller et al., 1993) was annotated. Consequently, the senses in WordNet are, since version 1.7, sorted by this distribution of senses estimated on the SemCor. • Build a Word Sense Disambiguation system which learns from examples contained in the annotated corpus. For instance, the OMSTI (Taghipour and Ng, 2015) was created for this purpose. • Evaluate a WSD system by comparing its output to the annotations in the corpus, as it is the case for instance with corpora created as part of the evaluation campaigns SensEval-SemEval. After their distribution, there is no scientific reason not to use indistinctly these corpora either for building a WSD system, for estimating the distribution of senses or for evaluating a WSD system. Indeed, the SemCor is used since a long time for the learning of WSD systems (Chan et al., 2007; Navigli et al., 2007) or more recently for the evaluation of different methods (Yu"
L18-1166,N03-1033,0,0.00904342,"ith annotations on different words, and creates a single sentence containing all annotations. Thus, this steps adds a crucial information for some WSD systems. For instance, a similarity-based WSD system can now “learn” that two word senses are often located in the same sentence. 3 http://wordnet.princeton.edu/glosstag.shtml 4.1.3. Lemma and POS tagging For the corpora that do not already contain these informations, we added the lemma for every word, when existing, using the WordNet’s morphy tool, and the part-of-speech tag from the Penn Treebank tag set using Stanford’s Loglinear POS tagger (Toutanova et al., 2003). 4.1.4. Cleaning Finally, this last step consists of trimming words, removing invisible characters and removing inconsistent annotations, for instance when the part of speech annotation differs from the part of speech of the sense annotation. 4.2. UFSAC File format Our approach for the unification of the different annotated corpora begins with a file format that is descriptive, easily understandable and readable by a human, and at the same time, efficient for a program to parse and create. Finally, it should be able to contain all the information contained in the original resources. These inf"
L18-1166,2016.jeptalnrecital-long.13,1,0.784695,"Missing"
L18-1166,C16-1130,0,0.251677,"5) was created for this purpose. • Evaluate a WSD system by comparing its output to the annotations in the corpus, as it is the case for instance with corpora created as part of the evaluation campaigns SensEval-SemEval. After their distribution, there is no scientific reason not to use indistinctly these corpora either for building a WSD system, for estimating the distribution of senses or for evaluating a WSD system. Indeed, the SemCor is used since a long time for the learning of WSD systems (Chan et al., 2007; Navigli et al., 2007) or more recently for the evaluation of different methods (Yuan et al., 2016). This last usage is still very rare, since it is one of the first experiment that we found in the literature, along with (M`arquez et al., 2002). However, the format of the resources differs greatly depending on their original purpose. For the SemCor, a single file groups all the information, whereas in the case of the evaluation corpora, there are two files: one that contains the unannotated corpus, and the other that contains the sense annotations. In some corpora, like in the DSO and the OMSTI, there is one file for every lemma in the dictionary, and each file contains thousands of example"
senay-etal-2010-transcriber,bazillon-etal-2008-manual,0,\N,Missing
W11-2154,W08-0509,0,0.0344349,"of the two seed systems (LIG and LIA), whereas test08 and testcomb08 were used to tune the weights for system combination. test10 was finally put aside to compare internally our methods. 2.2 LIG and LIA system characteristics Both LIG and LIA systems are phrase-based translation models. All the data were first tokenized with the tokenizer provided for the workshop. KneserNey discounted LMs were built from monolingual corpora using the SRILM toolkit (Stolcke, 2002), while bilingual corpora were aligned at the wordlevel using G IZA ++ (Och and Ney, 2003) or its multi-threaded version MG IZA ++ (Gao and Vogel, 2008) for the large corpora UN and giga. Phrase table and lexicalized reordering models were built with M OSES (Koehn et al., 2007). Finally, 14 features were used in the phrase-based models: 1 When not specified otherwise “our” system refers to the LIGA system. 440 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 440–446, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics C ORPORA News Commentary v6 Europarl v6 United Nation corpus 109 corpus D ESIGNATION S IZE ( SENTENCES ) English-French Bilingual training news-c euro UN giga Engl"
W11-2154,2005.eamt-1.19,0,0.0365934,"l when citations included in sentences have to be translated. Two configurations were tested: zone markups inclusion around quotes and wall markups inclusion within zone markups. However, the measured gains were finally too marginal to include the method in the final system. 4.2 Parallel corpus subsampling As the only news parallel corpus provided for the workshop contains 116 k sentence pairs, we must resort to parallel out-of-domain corpora in order to build reliable translation models. Information retrieval (IR) methods have been used in the past to subsample parallel corpora. For example, Hildebrand et al. (2005) used sentences belonging to the development and test corpora as queries to select the k most similar source sentences in an indexed parallel corpus. The retrieved sentence pairs constituted a training corpus for the translation models. The RALI submission for WMT10 proposed a similar approach that builds queries from the monolingual news corpus in order to select sentence pairs stylistically close to the news domain (Huet et al., 2010). This method has the major interest that it does not require to build a new training parallel corpus for each news data set to translate. Following the best co"
W11-2154,W10-1713,1,0.771338,"in order to build reliable translation models. Information retrieval (IR) methods have been used in the past to subsample parallel corpora. For example, Hildebrand et al. (2005) used sentences belonging to the development and test corpora as queries to select the k most similar source sentences in an indexed parallel corpus. The retrieved sentence pairs constituted a training corpus for the translation models. The RALI submission for WMT10 proposed a similar approach that builds queries from the monolingual news corpus in order to select sentence pairs stylistically close to the news domain (Huet et al., 2010). This method has the major interest that it does not require to build a new training parallel corpus for each news data set to translate. Following the best configuration tested in (Huet et al., 2010), we index the three out-of-domain corpora using L EMUR3 , and build queries from English news-s sentences where stop words are removed. The 10 top sentence pairs retrieved per query are selected and added to the new training corpus if they are not redundant with a sentence pair already collected. The process is repeated until the training parallel corpus reaches a threshold over the number of re"
W11-2154,D07-1103,0,0.0637009,"ning on mono-news-c and news-s Training on news-c, euro and UN 5-gram models Training on 10 M sentence pairs selected in news-c, euro, UN and giga Translation model Phrase table filtering Use of -monotone-at-punctuation option Table 2: Distinct features between final configurations retained for the LIG and LIA systems 3.3 Translation model training Translation models were trained from the parallel corpora news-c, euro and UN. Data were aligned at the word-level and then used to build standard phrase-based translation models. We filtered the obtained phrase table using the method described in (Johnson et al., 2007). Since this technique drastically reduces the size of the phrase table, while not degrading (and even slightly improving) the results on the development and test corpora (System 6), we decided to employ filtered phrase tables in the final configuration of the LIG system. 3.4 Tuning For decoding, the system uses a log-linear combination of translation model scores with the LM log-probability. We prevent phrase reordering over punctuation using the M OSES option -monotone-atpunctuation. As the system can be beforehand tuned by adjusting the log-linear combination weights on a development corpus"
W11-2154,P07-2045,0,0.0138313,"10 was finally put aside to compare internally our methods. 2.2 LIG and LIA system characteristics Both LIG and LIA systems are phrase-based translation models. All the data were first tokenized with the tokenizer provided for the workshop. KneserNey discounted LMs were built from monolingual corpora using the SRILM toolkit (Stolcke, 2002), while bilingual corpora were aligned at the wordlevel using G IZA ++ (Och and Ney, 2003) or its multi-threaded version MG IZA ++ (Gao and Vogel, 2008) for the large corpora UN and giga. Phrase table and lexicalized reordering models were built with M OSES (Koehn et al., 2007). Finally, 14 features were used in the phrase-based models: 1 When not specified otherwise “our” system refers to the LIGA system. 440 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 440–446, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics C ORPORA News Commentary v6 Europarl v6 United Nation corpus 109 corpus D ESIGNATION S IZE ( SENTENCES ) English-French Bilingual training news-c euro UN giga English Monolingual training News Commentary v6 mono-news-c Shuffled News Crawl corpus (from 2007 to 2011) news-s Europarl v6 mono"
W11-2154,J03-1002,0,0.00334518,"evoted to model tuning: test09 was used for the development of the two seed systems (LIG and LIA), whereas test08 and testcomb08 were used to tune the weights for system combination. test10 was finally put aside to compare internally our methods. 2.2 LIG and LIA system characteristics Both LIG and LIA systems are phrase-based translation models. All the data were first tokenized with the tokenizer provided for the workshop. KneserNey discounted LMs were built from monolingual corpora using the SRILM toolkit (Stolcke, 2002), while bilingual corpora were aligned at the wordlevel using G IZA ++ (Och and Ney, 2003) or its multi-threaded version MG IZA ++ (Gao and Vogel, 2008) for the large corpora UN and giga. Phrase table and lexicalized reordering models were built with M OSES (Koehn et al., 2007). Finally, 14 features were used in the phrase-based models: 1 When not specified otherwise “our” system refers to the LIGA system. 440 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 440–446, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics C ORPORA News Commentary v6 Europarl v6 United Nation corpus 109 corpus D ESIGNATION S IZE ( SENTENCE"
W11-2154,P03-1021,0,0.0208191,"us (from 2007 to 2011) news-s Europarl v6 mono-euro 116 k 1.8 M 12 M 23 M 181 k 25 M 1.8 M Development newstest2008 newssyscomb2009 newstest2009 test08 testcomb09 test09 2,051 502 2,525 test10 2,489 Test newstest2010 Table 1: Used corpora processed in order to normalize a special French form (named euphonious “t”) as described in (Potet et al., 2010). • 5 translation model scores, • 1 distance-based reordering score, • 6 lexicalized reordering score, • 1 LM score and • 1 word penalty score. The score weights were optimized on the test09 corpus according to the BLEU score with the MERT method (Och, 2003). The experiments led specifically with either LIG or LIA system are respectively described in Sections 3 and 4. Unless otherwise indicated, all the evaluations were performed using case-insensitive BLEU and were computed with the mteval-v13a.pl script provided by NIST. Table 2 summarizes the differences between the final configuration of the systems. 3 The LIG machine translation system LIG participated for the second time to the WMT shared news translation task for the French-English language pair. 3.1 Pre-processing Training data were first lowercased with the P ERL script provided for the"
W13-2248,W12-3110,0,0.032269,"IBM-model based features (Blatz et al., 2004). Ueffing and Ney (2005) propose an approach for phrase-based translation models: a phrase is a sequence of contiguous words and is extracted from word-aligned bilingual training corpus. The confidence value of each word is then computed by summing over all phrase pairs in which the target part contains this word. Xiong et al. (2010) integrate target word’s Part-Of-Speech (POS) and train them by Maximum Entropy Model, allowing significative gains compared to WPP features. Other approaches are based on external features (Soricut and Echihabi, 2010; Felice and Specia, 2012) allowing to deal with various MT systems (e.g. statistical, rule based etc.). In this paper, we propose to use both internal and external features into a conditionnal random fields (CRF) model to predict the label for each word in the MT hypothesis. We organize the article as follows: section 2 explains all the used features. Section 3 presents our experimental settings and the preliminary experiments. Section 4 explores a feature selection refinement and the section 5 presents work using several classifiers associated with a boosting decision. Finally we present Introduction Recently Statist"
W13-2248,P10-1052,0,0.187852,"Missing"
W13-2248,F12-3004,0,0.02544,"Missing"
W13-2248,N07-1051,0,0.0121351,"rce word features: all the source words that align to the target one, represented in BIO1 format. • Source alignment context features: the combinations of the target word and one word before (left source context) or after (right source context) the source word aligned to it. • Target alignment context features: the combinations of the source word and each word in the window ±2 (two before, two after) of the target word. • The word’s constituent label and its depth in the tree (or the distance between it and the tree root) obtained from the constituent tree as an output of the Berkeley parser (Petrov and Klein, 2007) (trained over a Spanish treebank: AnCora3 ). • Target Word’s Posterior Probability (WPP). • Backoff behaviour: a score assigned to the word according to how many times the target Language Model has to back-off in order to assign a probability to the word sequence, as described in (Raybaud et al., 2011). 1 The LIG features • Occurrence in Google Translate hypothesis: we check whether this target word appears in the sentence generated by Google Translate engine for the same source. 2 3 http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/ 387 http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTa"
W13-2248,P10-1063,0,0.0242751,"t al. (2003) associated with IBM-model based features (Blatz et al., 2004). Ueffing and Ney (2005) propose an approach for phrase-based translation models: a phrase is a sequence of contiguous words and is extracted from word-aligned bilingual training corpus. The confidence value of each word is then computed by summing over all phrase pairs in which the target part contains this word. Xiong et al. (2010) integrate target word’s Part-Of-Speech (POS) and train them by Maximum Entropy Model, allowing significative gains compared to WPP features. Other approaches are based on external features (Soricut and Echihabi, 2010; Felice and Specia, 2012) allowing to deal with various MT systems (e.g. statistical, rule based etc.). In this paper, we propose to use both internal and external features into a conditionnal random fields (CRF) model to predict the label for each word in the MT hypothesis. We organize the article as follows: section 2 explains all the used features. Section 3 presents our experimental settings and the preliminary experiments. Section 4 explores a feature selection refinement and the section 5 presents work using several classifiers associated with a boosting decision. Finally we present Int"
W13-2248,H05-1096,0,0.0833261,"the best systems for submission and present the official results obtained. 1 • a binary classification: good (keep) or bad (change) label • a multi-class classification: the label refers to the edit action needed for the token (i.e. keep, delete or substitute). Various approaches have been proposed for WCE: Blatz et al. (2003) combine several features using neural network and naive Bayes learning algorithms. One of the most effective feature combinations is the Word Posterior Probability (WPP) as proposed by Ueffing et al. (2003) associated with IBM-model based features (Blatz et al., 2004). Ueffing and Ney (2005) propose an approach for phrase-based translation models: a phrase is a sequence of contiguous words and is extracted from word-aligned bilingual training corpus. The confidence value of each word is then computed by summing over all phrase pairs in which the target part contains this word. Xiong et al. (2010) integrate target word’s Part-Of-Speech (POS) and train them by Maximum Entropy Model, allowing significative gains compared to WPP features. Other approaches are based on external features (Soricut and Echihabi, 2010; Felice and Specia, 2012) allowing to deal with various MT systems (e.g"
W13-2248,2003.mtsummit-papers.52,0,0.507011,"king advantage of their complementarity is presented and experimented. We then select the best systems for submission and present the official results obtained. 1 • a binary classification: good (keep) or bad (change) label • a multi-class classification: the label refers to the edit action needed for the token (i.e. keep, delete or substitute). Various approaches have been proposed for WCE: Blatz et al. (2003) combine several features using neural network and naive Bayes learning algorithms. One of the most effective feature combinations is the Word Posterior Probability (WPP) as proposed by Ueffing et al. (2003) associated with IBM-model based features (Blatz et al., 2004). Ueffing and Ney (2005) propose an approach for phrase-based translation models: a phrase is a sequence of contiguous words and is extracted from word-aligned bilingual training corpus. The confidence value of each word is then computed by summing over all phrase pairs in which the target part contains this word. Xiong et al. (2010) integrate target word’s Part-Of-Speech (POS) and train them by Maximum Entropy Model, allowing significative gains compared to WPP features. Other approaches are based on external features (Soricut and"
W13-2248,C04-1046,0,\N,Missing
W13-2248,P10-1062,0,\N,Missing
W14-0301,P03-1021,0,0.112235,"Missing"
W14-0301,P02-1040,0,0.0929368,"hypotheses. This latter work is the one that is the most related to our paper. However, the major differences are: (1) our proposed sen3 Our Approach Our approach can be expressed in three steps: investigate the potential of using word-level score in N-best list re-ranking, build the WCE system and 2 extract additional features to integrate with the existing log-linear model. 3.1 Investigating the correlation between “word quality” scores and other metrics Firstly, we investigate the correlation between sentence-level scores (obtained from WCE labels) and conventional evaluation scores (BLEU (Papineni et al., 2002), TER and TERp-A (Snover et al., 2008)). For each sentence, a word quality score (WQS) is calculated by: W QS = #00 G00 (good) words #words (1) In other words, we are trying to answer the following question: can the high percentage of “G” (good) words (predicted by WCE system) in a MT output ensure its possibility of having a better BLEU and low TER (TERp-A) value ? This investigation is a strong prerequisite for further experiments in order to check that WCE scores do not bring additional “noise” to the re-ranking process. In this experiment, we compute WQS over our entire French - English da"
W14-0301,2012.eamt-1.34,0,0.257157,"Missing"
W14-0301,potet-etal-2012-collection,1,0.81717,"avergne et al., 2010). Basically, CRF computes the probability of the output sequence Y = (y1 , y2 , ..., yN ) given the input sequence X = (x1 , x2 , ..., xN ) by: Essentially, a WCE system construction consists of two pivotal elements: the features (the SMT system dependent or independent information extracted for each word to represent its characteristics) and the machine learning method (to train the prediction model). Motivated 3 rank in WMT 2013 Quality Estimation Shared Task (Luong et al., 2013b). For building the WCE training and test sets, we use a dataset of 10,881 French sentences (Potet et al., 2012) , and apply a baseline SMT system to generate hypotheses (1000-best list). Our baseline SMT system (presented for WMT 2010 evaluation) keeps the Moses’s default setting (Koehn et al., 2007): log-linear model with 14 weighted feature functions. The translation model is trained on the Europarl and News parallel corpora of WMT102 evaluation campaign (1,638,440 sentences). The target language model is trained by the SRI language modeling toolkit (Stolcke, 2002) on the news monolingual corpus (48,653,884 sentences). Translators were then invited to correct MT outputs, giving us the same amount of"
W14-0301,C04-1046,0,0.0796473,"The interesting uses of WCE include: pointing out the words that need to be corrected by the post-editor, telling readers about the reliability of a specific portion, and selecting the best segments among options from multiple translation systems for combination. Dealing with this problem, various approaches have been proposed: Blatz et al. (2003) combine several features using neural network and naive Bayes learning algorithms. One of the most effective feature combinations is the Word Posterior Probability (WPP) as suggested by Ueffing et al. (2003) associated with IBM-model based features (Blatz et al., 2004). Ueffing and Ney (2005) propose an approach for phrase-based translation models: a phrase is a sequence of contiguous words and is extracted from the word-aligned bilingual training corpus. The confidence value of each word is then computed by summing over all phrase pairs in which the target part contains this word. Xiong et al. (2010) integrate target word’s Part-Of-Speech (POS) and train them by Maximum Entropy Model, allowing significative gains in comparison to WPP features. The novel features from source side, alignment context, and dependency structure (Bach et al., 2011) help to augme"
W14-0301,2012.amta-papers.17,0,0.0901014,"Missing"
W14-0301,P11-2031,0,0.0946635,"Missing"
W14-0301,P10-1063,0,0.0260328,"s extracted from the word-aligned bilingual training corpus. The confidence value of each word is then computed by summing over all phrase pairs in which the target part contains this word. Xiong et al. (2010) integrate target word’s Part-Of-Speech (POS) and train them by Maximum Entropy Model, allowing significative gains in comparison to WPP features. The novel features from source side, alignment context, and dependency structure (Bach et al., 2011) help to augment marginally in F-score as well as the Pearson correlation with human judgment. Other approaches are based on external features (Soricut and Echihabi, 2010; Felice and Specia, 2012) allowing to cope with various MT systems (e.g. statistical, rule based etc.). Among the numerous WCE applications, we consider its contribution in a specific step of SMT pipeline: N-best list reranking. Our WCE system and the proposed reranking features are presented in the next section. 2.1 N-best List Re-ranking Walking through various related work concerning this issue, we observe some prominent ideas. The first attempt focuses on proposing additional Language Models. Kirchhoff and Yang (2005) train one word-based 4-gram model (with modified Kneser-Ney smoothing)"
W14-0301,W12-3110,0,0.0151351,"igned bilingual training corpus. The confidence value of each word is then computed by summing over all phrase pairs in which the target part contains this word. Xiong et al. (2010) integrate target word’s Part-Of-Speech (POS) and train them by Maximum Entropy Model, allowing significative gains in comparison to WPP features. The novel features from source side, alignment context, and dependency structure (Bach et al., 2011) help to augment marginally in F-score as well as the Pearson correlation with human judgment. Other approaches are based on external features (Soricut and Echihabi, 2010; Felice and Specia, 2012) allowing to cope with various MT systems (e.g. statistical, rule based etc.). Among the numerous WCE applications, we consider its contribution in a specific step of SMT pipeline: N-best list reranking. Our WCE system and the proposed reranking features are presented in the next section. 2.1 N-best List Re-ranking Walking through various related work concerning this issue, we observe some prominent ideas. The first attempt focuses on proposing additional Language Models. Kirchhoff and Yang (2005) train one word-based 4-gram model (with modified Kneser-Ney smoothing) and one factored trigram o"
W14-0301,H05-1096,0,0.119399,"of WCE include: pointing out the words that need to be corrected by the post-editor, telling readers about the reliability of a specific portion, and selecting the best segments among options from multiple translation systems for combination. Dealing with this problem, various approaches have been proposed: Blatz et al. (2003) combine several features using neural network and naive Bayes learning algorithms. One of the most effective feature combinations is the Word Posterior Probability (WPP) as suggested by Ueffing et al. (2003) associated with IBM-model based features (Blatz et al., 2004). Ueffing and Ney (2005) propose an approach for phrase-based translation models: a phrase is a sequence of contiguous words and is extracted from the word-aligned bilingual training corpus. The confidence value of each word is then computed by summing over all phrase pairs in which the target part contains this word. Xiong et al. (2010) integrate target word’s Part-Of-Speech (POS) and train them by Maximum Entropy Model, allowing significative gains in comparison to WPP features. The novel features from source side, alignment context, and dependency structure (Bach et al., 2011) help to augment marginally in F-score"
W14-0301,W05-0821,0,0.36147,"Missing"
W14-0301,P07-2045,0,0.00603561,"construction consists of two pivotal elements: the features (the SMT system dependent or independent information extracted for each word to represent its characteristics) and the machine learning method (to train the prediction model). Motivated 3 rank in WMT 2013 Quality Estimation Shared Task (Luong et al., 2013b). For building the WCE training and test sets, we use a dataset of 10,881 French sentences (Potet et al., 2012) , and apply a baseline SMT system to generate hypotheses (1000-best list). Our baseline SMT system (presented for WMT 2010 evaluation) keeps the Moses’s default setting (Koehn et al., 2007): log-linear model with 14 weighted feature functions. The translation model is trained on the Europarl and News parallel corpora of WMT102 evaluation campaign (1,638,440 sentences). The target language model is trained by the SRI language modeling toolkit (Stolcke, 2002) on the news monolingual corpus (48,653,884 sentences). Translators were then invited to correct MT outputs, giving us the same amount of post editions (Potet et al., 2012). The set of triples (source, hypothesis, post edition) is then divided into the training set (10000 first triples) and test set (881 remaining). To train t"
W14-0301,2003.mtsummit-papers.52,0,0.614644,"put. If the error is predicted for each word, this becomes WCE. The interesting uses of WCE include: pointing out the words that need to be corrected by the post-editor, telling readers about the reliability of a specific portion, and selecting the best segments among options from multiple translation systems for combination. Dealing with this problem, various approaches have been proposed: Blatz et al. (2003) combine several features using neural network and naive Bayes learning algorithms. One of the most effective feature combinations is the Word Posterior Probability (WPP) as suggested by Ueffing et al. (2003) associated with IBM-model based features (Blatz et al., 2004). Ueffing and Ney (2005) propose an approach for phrase-based translation models: a phrase is a sequence of contiguous words and is extracted from the word-aligned bilingual training corpus. The confidence value of each word is then computed by summing over all phrase pairs in which the target part contains this word. Xiong et al. (2010) integrate target word’s Part-Of-Speech (POS) and train them by Maximum Entropy Model, allowing significative gains in comparison to WPP features. The novel features from source side, alignment conte"
W14-0301,N07-1063,0,0.541731,"Missing"
W14-0301,P10-1052,0,0.148325,"Missing"
W14-0301,D07-1080,0,0.0823775,"Missing"
W14-0301,W13-2248,1,0.925434,"labeling process, we employ the Conditional Random Fields (CRFs) for our model training, with WAPITI toolkit (Lavergne et al., 2010). Basically, CRF computes the probability of the output sequence Y = (y1 , y2 , ..., yN ) given the input sequence X = (x1 , x2 , ..., xN ) by: Essentially, a WCE system construction consists of two pivotal elements: the features (the SMT system dependent or independent information extracted for each word to represent its characteristics) and the machine learning method (to train the prediction model). Motivated 3 rank in WMT 2013 Quality Estimation Shared Task (Luong et al., 2013b). For building the WCE training and test sets, we use a dataset of 10,881 French sentences (Potet et al., 2012) , and apply a baseline SMT system to generate hypotheses (1000-best list). Our baseline SMT system (presented for WMT 2010 evaluation) keeps the Moses’s default setting (Koehn et al., 2007): log-linear model with 14 weighted feature functions. The translation model is trained on the Europarl and News parallel corpora of WMT102 evaluation campaign (1,638,440 sentences). The target language model is trained by the SRI language modeling toolkit (Stolcke, 2002) on the news monolingual"
W14-0301,C12-1121,0,0.0354352,"Missing"
W14-0301,W06-1626,0,\N,Missing
W14-0301,P11-1022,0,\N,Missing
W14-0301,P10-1062,0,\N,Missing
W14-0301,aziz-etal-2012-pet,0,\N,Missing
W14-0301,2012.eamt-1.31,0,\N,Missing
W14-0301,P08-2010,0,\N,Missing
W14-0301,2012.tc-1.5,0,\N,Missing
W14-3342,N07-1051,0,0.0208934,"re for POS tag. • Language Model (LM) features: the “longest target n-gram length” and “longest source n-gram length”(length of the longest sequence created by the current target (source aligned) word and its previous ones in the target (source) LM). For example, with the target word wi : if the sequence wi−2 wi−1 wi appears in the target LM but the sequence wi−3 wi−2 wi−1 wi does not, the n-gram value for wi will be 3. • The word’s constituent label and its depth in the tree (or the distance between it and the tree root) obtained from the constituent tree as an output of the Berkeley parser (Petrov and Klein, 2007) (trained over a Spanish treebank: AnCora4 ). • Occurrence in Google Translate hypothesis: we check whether this target word appears in • Word Occurrence in multiple translations: one novel point in this year’s shared task is that the targets come from multiple MT 3 5 4 http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/ http://clic.ub.edu/corpus/en/ancora 337 6 http://search.cpan.org/dist/Lingua-Wordnet/Wordnet.pm http://babelnet.org System BL(bin) outputs (from systems or from humans) for the same source sentences. Obviously, one would have a “natural” intuition that: the occurrence"
W14-3342,W13-2245,0,0.0545799,"election strategy on our development set, before dealing with the test set for submission. 1 1.1 • Level 1: the Good class is kept intact, whereas Bad one is further divided into subcategories: Accuracy issue (the word does not accurately reflect the source text) and Fluency issue (the word does not relate to the form or content of the target text). • Multi-class: more detailed judgement, where the translation errors are further decomposed into 16 labels based on MQM1 metric. 1.2 Related work WMT 2013 witnessed several attempts dealing with this evaluation type in its first launch. Han et al. (2013); Luong et al. (2013) employed the Conditional Random Fields (CRF) (Lafferty et al., 2001) model as their Machine Learning method to address the problem as a sequence labeling task. Meanwhile, Bicici (2013) extended the global learning model by dynamic training with adaptive weight updates in the perceptron training algorithm. As far as prediction indicators are concerned, Bicici (2013) proposed seven word feature types and found among them the “common cover links” (the links that point from the leaf node containing this word to other leaf nodes in the same subtree of the syntactic tree) the m"
W14-3342,P07-2045,0,0.00888298,"ts the most outstanding system for submission. The last section summarizes the approach and opens new outlook. 2 • Spanish - English 2013 MT system On the contrary, no specific MT setting is provided (e.g. the code to re-run Moses system like WMT 2013), leading to the unavailability of some crucial resources, such as the N-best list and alignment information. Coping with this, we firstly thought of using the Moses “Constrained Decoding” option as a method to tie our (already available) decoder’s output to the given target translations (this feature is supported by the latest version of Moses (Koehn et al., 2007) in 2013). Our hope was that, by doing so, both N-best list and alignment information would be generated during decoding. But the decoder failed to output all translations (only 1/4 was obtained) when the number of allowed unknown words (-max-unknowns) was set as 0. Switching to non zero value for this option did not help either since, even if more outputs were generated, alignment information was biased in that case due to additional/missing words in the obtained MT output. Ultimately, we decided to employ GIZA++ toolkit (Och and Ney, 2003) to obtain at least the alignment information (and as"
W14-3342,P10-1052,0,0.0614962,"Missing"
W14-3342,W13-2242,0,\N,Missing
W14-3342,J03-1002,0,\N,Missing
W15-5121,W13-3912,0,0.0500209,"Missing"
W15-5121,vacher-etal-2014-sweet,1,0.357146,"ta features and energy (40 features). Acoustic models were composed of 11,000 contextdependent states and 150,000 Gaussians. The state tying is performed using a decision tree based on a tree-clustering of the phones. In addition, off-line fMLLR linear transformation acoustic adaptation was performed. The acoustic models were trained on 500 hours of transcribed French speech composed of the ESTER 1&2 (broadcast news and conversational speech recorded on the radio) and REPERE (TV news and talk-shows) challenges as well as from 7 hours of transcribed French speech of the SH corpus (S WEETH OME) [22] which consists of records of 60 speakers interacting in the smart home and from 28 minutes of the Voix-détresse corpus [23] which is made of records of speakers eliciting a distress emotion. 2.3. Recognition of distress calls The recognition of distress calls consists in computing the phonetic distance of an hypothesis to a list of predefined distress calls. Each ASR hypothesis Hi is phonetized, every voice commands Tj is aligned to Hi using Levenshtein distance. The deletion, insertion and substitution costs were computed empirically while the cumulative distance γ(i, j) between Hj and Ti is"
W15-5121,W13-3906,0,\N,Missing
W17-6940,P14-1023,0,0.0654902,"cabulary size is about 3 million words and phrases, and the vectors have a dimension of 300. 2. The pre-trained Pennington et al. (2014)’s GloVe 3 , trained on 42 billion words from Common 2 3 https://code.google.com/archive/p/word2vec/ https://nlp.stanford.edu/projects/glove/ Crawl. The vocabulary size is about 2 million words, and the vectors have a dimension of 300. 3. The pre-trained Levy and Goldberg (2014)’s dependency-based word embeddings 4 . The training was done on Wikipedia. The vocabulary size is about 175,000 words, and the dimension is 300. 4. The best predict vectors created in Baroni et al. (2014) 5 . The vectors have a dimension of 400 and the vocabulary size is about 300,000. 5. Finally, the best reduced count vectors also created in Baroni et al. (2014) 5 , of dimension 500, and with the same vocabulary than the previous model. In the experiments section, we compare the performance of each of the embeddings model in our WSD system extension. All five sense embeddings models are publicly released on our GitHub6 . 3 Evaluation in Knowledge-Based WSD The generated vector representation of senses will be evaluated on a WSD task, as a supplementary resource for a knowledge-based method."
W17-6940,D14-1110,0,0.0237029,"reason, this paper will focus on a knowledge-based method, and on a novel approach that improves its performance in an unsupervised manner, by using word embeddings. Word embeddings are a set of methods which aim to represent words as vectors. Several recent state of the art methods such as Mikolov et al. (2013)’s Word2Vec, Pennington et al. (2014)’s GloVe and Levy and Goldberg (2014)’s dependency-based vectors have proven to be very useful in many NLP tasks, like Machine Translation, Word Similarity tasks, and even in WSD, where word embeddings are also parts of some recent methods, such as Chen et al. (2014), Iacobacci et al. (2016) or Yuan et al. (2016). In this paper, we are going to produce sense embeddings, i.e. vectors which represent senses present in the lexical database Princeton WordNet (Miller, 1995). The methodology used to create these vectors is described in section 2. Other methods exist for computing sense embeddings, such as in Iacobacci et al. 1 Less than ten different languages have at least one corpus sense annotated with WordNet, as listed in http:// globalwordnet.org/wordnet-annotated-corpora/ (2015) for example, who learn a distributional representation of senses through sen"
W17-6940,S17-2012,1,0.861005,"Missing"
W17-6940,P15-1010,0,0.0476432,"Missing"
W17-6940,P16-1085,0,0.0207521,"will focus on a knowledge-based method, and on a novel approach that improves its performance in an unsupervised manner, by using word embeddings. Word embeddings are a set of methods which aim to represent words as vectors. Several recent state of the art methods such as Mikolov et al. (2013)’s Word2Vec, Pennington et al. (2014)’s GloVe and Levy and Goldberg (2014)’s dependency-based vectors have proven to be very useful in many NLP tasks, like Machine Translation, Word Similarity tasks, and even in WSD, where word embeddings are also parts of some recent methods, such as Chen et al. (2014), Iacobacci et al. (2016) or Yuan et al. (2016). In this paper, we are going to produce sense embeddings, i.e. vectors which represent senses present in the lexical database Princeton WordNet (Miller, 1995). The methodology used to create these vectors is described in section 2. Other methods exist for computing sense embeddings, such as in Iacobacci et al. 1 Less than ten different languages have at least one corpus sense annotated with WordNet, as listed in http:// globalwordnet.org/wordnet-annotated-corpora/ (2015) for example, who learn a distributional representation of senses through sense-annotated corpora and"
W17-6940,P14-2050,0,0.413651,"enerally the only usable systems to work for disambiguating another language than English. Indeed, sense-annotated corpora are very expensive resources to produce, and almost practically inexisting for every other languages 1 . For this reason, this paper will focus on a knowledge-based method, and on a novel approach that improves its performance in an unsupervised manner, by using word embeddings. Word embeddings are a set of methods which aim to represent words as vectors. Several recent state of the art methods such as Mikolov et al. (2013)’s Word2Vec, Pennington et al. (2014)’s GloVe and Levy and Goldberg (2014)’s dependency-based vectors have proven to be very useful in many NLP tasks, like Machine Translation, Word Similarity tasks, and even in WSD, where word embeddings are also parts of some recent methods, such as Chen et al. (2014), Iacobacci et al. (2016) or Yuan et al. (2016). In this paper, we are going to produce sense embeddings, i.e. vectors which represent senses present in the lexical database Princeton WordNet (Miller, 1995). The methodology used to create these vectors is described in section 2. Other methods exist for computing sense embeddings, such as in Iacobacci et al. 1 Less tha"
W17-6940,S15-2049,0,0.0220209,"to see how our sense embeddings model performs as a semantic network, we use it for a lexical expansion of the dictionary’s glosses, for improving the Lesk local algorithm of our WSD system. Our expansion considers the related senses regarding two cosine similarity threshold: δ1 , filtering out senses based on their similarity with the target sense’s lemma vector, and δ2 , filtering out senses based on their similarity with the target sense vector. These two parameters have to be set in some way, so we chose two WSD tasks: SemEval 2007 task 7 (Navigli et al., 2007), and SemEval 2015 task 13 (Moro and Navigli, 2015), and we estimated the best set of parameters on a task, then tested this set of parameters on the other. The reason why we chose these two tasks is that they are of the same nature, i.e. both all-words WSD tasks, and that the task 7 of SemEval 2007 is largely used in most WSD articles, so it is easier to put the results in perspective. All five word embeddings models mentioned in section 2 are evaluated separately. And the parameters δ1 and δ2 have been estimated by testing every values in the range [0.5, 0.9] with steps of 0.1. The results of the best parameters estimation on SemEval 2007 ta"
W17-6940,D14-1162,0,0.0759212,"sed and knowledge-based methods are generally the only usable systems to work for disambiguating another language than English. Indeed, sense-annotated corpora are very expensive resources to produce, and almost practically inexisting for every other languages 1 . For this reason, this paper will focus on a knowledge-based method, and on a novel approach that improves its performance in an unsupervised manner, by using word embeddings. Word embeddings are a set of methods which aim to represent words as vectors. Several recent state of the art methods such as Mikolov et al. (2013)’s Word2Vec, Pennington et al. (2014)’s GloVe and Levy and Goldberg (2014)’s dependency-based vectors have proven to be very useful in many NLP tasks, like Machine Translation, Word Similarity tasks, and even in WSD, where word embeddings are also parts of some recent methods, such as Chen et al. (2014), Iacobacci et al. (2016) or Yuan et al. (2016). In this paper, we are going to produce sense embeddings, i.e. vectors which represent senses present in the lexical database Princeton WordNet (Miller, 1995). The methodology used to create these vectors is described in section 2. Other methods exist for computing sense embeddings, s"
W17-6940,C16-1130,0,0.0182188,"ased method, and on a novel approach that improves its performance in an unsupervised manner, by using word embeddings. Word embeddings are a set of methods which aim to represent words as vectors. Several recent state of the art methods such as Mikolov et al. (2013)’s Word2Vec, Pennington et al. (2014)’s GloVe and Levy and Goldberg (2014)’s dependency-based vectors have proven to be very useful in many NLP tasks, like Machine Translation, Word Similarity tasks, and even in WSD, where word embeddings are also parts of some recent methods, such as Chen et al. (2014), Iacobacci et al. (2016) or Yuan et al. (2016). In this paper, we are going to produce sense embeddings, i.e. vectors which represent senses present in the lexical database Princeton WordNet (Miller, 1995). The methodology used to create these vectors is described in section 2. Other methods exist for computing sense embeddings, such as in Iacobacci et al. 1 Less than ten different languages have at least one corpus sense annotated with WordNet, as listed in http:// globalwordnet.org/wordnet-annotated-corpora/ (2015) for example, who learn a distributional representation of senses through sense-annotated corpora and Word2Vec, but it presu"
W17-6940,S07-1006,0,\N,Missing
W18-5402,I17-1001,0,0.11056,"Missing"
W18-5402,gravier-etal-2012-etape,1,0.819103,"Missing"
W18-5402,D14-1181,0,0.00777471,"Missing"
W18-5402,D16-1159,0,0.0307087,"Analyzing Learned Representations of a Deep ASR Performance Prediction Model Zied Elloumi1,2 Laurent Besacier2 Olivier Galibert1 Benjamin Lecouteux2 1 2 Laboratoire national de m´etrologie et d’essais (LNE) , France Univ. Grenoble Alpes, CNRS, Grenoble INP, LIG, F-38000 Grenoble, France firstname.name@lne.fr firstname.name@univ-grenoble-alpes.fr Abstract general problem in AI. Recent papers started to address this issue and analyzed hidden representations learned during training of different natural language processing models (Mohamed et al., 2012; Wu and King, 2016; Belinkov and Glass, 2017; Shi et al., 2016; Belinkov et al., 2017; Wang et al., 2017). Contribution. This work is dedicated to the analysis of speech signal embeddings and text embeddings learnt by the CNN during training of our ASR performance prediction model. Our goal is to better understand which information is captured by the deep model and its relation with conditioning factors such as speech style, accent or broadcast program type. For this, we use a data set presented in (Elloumi et al., 2018) which contains a large amount of speech utterances taken from various collections of French broadcast programs. Following a methodology"
