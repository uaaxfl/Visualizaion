2008.amta-papers.19,J93-2003,0,0.0116609,"e ME model for estimating possible domains of a given source term. Section 4 presents an algorithm to mine domain specific MPTPs from a bilingual lexicon. In Section 5, we describe the decoding algorithm for the translation model. Section 6 reports our experimental setup and results. After reviewing the related work briefly in Section 7, we conclude this paper in Section 8. 3 Cascaded translation model Our cascaded translation model is a direct translation model using a LM for ranking. It can be regarded as a reversal of the order of the source and target languages in the noisy-channel model (Brown et al., 1993). Let E and C denote terms in the source and target languages. The translation model produces the optimal term C ∗ given E, C ∗ = arg max p(C|E)p(C). C (1) In order to choose a target term C by making use of the domain information of the source term E, we integrate the domain D in the channel model, X p(C|E) = p(C|E, D)p(D|E). (2) D We estimate D of E using a log-linear ME model: ( ) X 1 exp λk hk (D, E) , (3) pΛ (D|E) = ZΛ (E) k ( ) X X ZΛ (E) = exp λk hk (D, E) . D k Here, pΛ (D|E) is computed by the feature vector {hk }, and the feature weight vector Λ = {λk }. We use the words and the morp"
2008.amta-papers.19,2007.mtsummit-papers.9,0,0.245396,"Missing"
2008.amta-papers.19,J07-2003,0,0.101549,"Missing"
2008.amta-papers.19,H05-1085,0,0.0742789,"Missing"
2008.amta-papers.19,E03-1076,0,0.156072,"24 |20 |14 transliteration 17 |19 |16 １ － 羟基 － ２ － 丙酮/１ － 羟基 － ２ － 丙酮 减压 舱/减压 室; rose/玫瑰/蔷薇 current - mode logic/电流 型 逻辑/电流 － 型 逻辑 light antiaircraft artillery/小高 炮/轻 高射 炮兵 specific gravity cell /比 重 电池/比 重 细胞 autohemorrhage/自出血/autohemorrhage grc/盖尔研究公司/grc kallman syndrome/卡尔曼 综合征/kallman 综合征 1 - hydroxy - 2 - propanone/ decompression chamber/ Table 7: Error analysis on translating the MD test set. M and P denote Moses 4 and Pharaoh respectively. main specific n-gram character/word LMs. In addition, integrating abbreviation lexicons and transliteration models are also needed. 7 Related work Koehn and Knight (2003a) investigated several empirical methods for splitting German compounds. Even though they did not apply morphology analysis to German compounds, their work integrated the idea of word-splitting into a phrase-based translation system. They reported significant results in translating German base noun phrases into English. We built our cascaded translation model inspired by their work. The differences are that we further employed morphological analysis to technical terms, grouped the morpheme correspondences, and proposed using domain information for morphemelevel translation disambiguation. Vir"
2008.amta-papers.19,P03-1040,0,0.123589,"24 |20 |14 transliteration 17 |19 |16 １ － 羟基 － ２ － 丙酮/１ － 羟基 － ２ － 丙酮 减压 舱/减压 室; rose/玫瑰/蔷薇 current - mode logic/电流 型 逻辑/电流 － 型 逻辑 light antiaircraft artillery/小高 炮/轻 高射 炮兵 specific gravity cell /比 重 电池/比 重 细胞 autohemorrhage/自出血/autohemorrhage grc/盖尔研究公司/grc kallman syndrome/卡尔曼 综合征/kallman 综合征 1 - hydroxy - 2 - propanone/ decompression chamber/ Table 7: Error analysis on translating the MD test set. M and P denote Moses 4 and Pharaoh respectively. main specific n-gram character/word LMs. In addition, integrating abbreviation lexicons and transliteration models are also needed. 7 Related work Koehn and Knight (2003a) investigated several empirical methods for splitting German compounds. Even though they did not apply morphology analysis to German compounds, their work integrated the idea of word-splitting into a phrase-based translation system. They reported significant results in translating German base noun phrases into English. We built our cascaded translation model inspired by their work. The differences are that we further employed morphological analysis to technical terms, grouped the morpheme correspondences, and proposed using domain information for morphemelevel translation disambiguation. Vir"
2008.amta-papers.19,koen-2004-pharaoh,0,0.0553447,"Missing"
2008.amta-papers.19,2005.mtsummit-papers.11,0,0.0154416,"der to find correspondences of morpheme phrases between the source and target languages, we propose an algorithm to mine morpheme phrase translation pairs from a bilingual lexicon. We also build a cascaded translation model that dynamically shifts translation units from phrase level to word and morpheme phrase levels. The experimental results show the significant improvements over the current phrase-based SMT systems. 1 Introduction Statistical machine translation (SMT) provides an impressive framework in which a machine translation system can be built, only if a parallel corpus is available. Koehn (2005) collected a corpus of parallel text in 11 languages and trained SMT systems for 110 language pairs within three weeks. However, the experimental results also revealed several language-specific issues: morphologically-rich languages (e.g., German) were more difficult to translate into than from, and two distant languages (e.g., 202 Finnish and English) that have discrepant morphologies (word structures) were difficult to deal with. These issues arise because SMT systems usually employ words as the minimum units of translation, even when some elements represented by individual words in one lang"
2008.amta-papers.19,P08-1113,0,0.103495,"Missing"
2008.amta-papers.19,J04-4002,0,0.0711605,"Missing"
2008.amta-papers.19,W07-0704,0,0.402782,"., German) were more difficult to translate into than from, and two distant languages (e.g., 202 Finnish and English) that have discrepant morphologies (word structures) were difficult to deal with. These issues arise because SMT systems usually employ words as the minimum units of translation, even when some elements represented by individual words in one language are included in the morphology of another language. Numerous researchers have proposed a variety of approaches that make use of morphological information in machine translation (Popovi´c and Ney, 2004; Goldwater and McClosky, 2005; Oflazer and El-Kahlout, 2007; Virpioja et al., 2007; Oflazer, 2008). Most studies assume that the input language (e.g., Arabic, Catalan, Czech, and Spanish) is morphologically richer than the output language (English) because translating from an information-rich language into an information-poor language is easier than the other way around (Koehn 2005). Some recent studies (Oflazer and El-Kahlout, 2007; Oflazer, 2008) explored the opposite direction (e.g., English to Turkish), but more case studies are necessary. Moreover, a number of new technical terms are emerging daily in English, which is the dominant international"
2008.amta-papers.19,P02-1040,0,0.074818,"Missing"
2008.amta-papers.19,popovic-ney-2004-towards,0,0.155206,"Missing"
2008.amta-papers.19,2007.mtsummit-papers.65,0,0.0855577,"Missing"
2009.iwslt-evaluation.15,P08-1023,0,0.0260161,"a2 are word alignments between a source language phrase f1m and a target language phrase en1 ; p(·) and l(·) are phrasal translation probabilities and lexical weights, respectively; wp stands for the word penalty. In order to be used in CKY-style decoding [10], a rule in the form of (1) can be easily transformed into an end-to-end Hiero-style [10] translation rule: (2) And the corresponding synchronous PCFG production takes the form of: X →∑i wi ∗log(pi ) f1m , en1 . (3) It has been proved that the end-to-end phrase table significantly influence the translation result of syntax-based systems [11, 12]. Note that the phrase table mentioned here 2 http://www.statmt.org/moses/ is not constrained to be linguistic phrases, i.e., a phrase inside the table is not necessarily covered by a subtree. This makes the phrase table more flexible to be used than in the tree/forest-to-string systems [11, 12] where additional approaches have to be employed in order to make use of nonlinguistic phrases [13, 14]. Return back to Figure 1, HPSG trees attached with PASs are generated by parsing the target language sentences using Enju. Then, we extract HPSG and PAS based translation rules from the word-aligned a"
2009.iwslt-evaluation.15,P07-1089,0,0.0155645,"hronous PCFG production takes the form of: X →∑i wi ∗log(pi ) f1m , en1 . (3) It has been proved that the end-to-end phrase table significantly influence the translation result of syntax-based systems [11, 12]. Note that the phrase table mentioned here 2 http://www.statmt.org/moses/ is not constrained to be linguistic phrases, i.e., a phrase inside the table is not necessarily covered by a subtree. This makes the phrase table more flexible to be used than in the tree/forest-to-string systems [11, 12] where additional approaches have to be employed in order to make use of nonlinguistic phrases [13, 14]. Return back to Figure 1, HPSG trees attached with PASs are generated by parsing the target language sentences using Enju. Then, we extract HPSG and PAS based translation rules from the word-aligned and target-language-parsed parallel corpora. The HPSG-based xRs (tree-to-string, [15]) translation rules (binarized inversely [16]) are extracted using the GHKM minimal-rule extraction algorithm of [1]. In order to trace the lexical level translation information (similar to PASbased rules (Section 3 and Figure 2)), we remember the endto-end alignments in an HPSG-based translation rule. The ideas a"
2009.iwslt-evaluation.15,N04-1035,0,0.47118,"by Head-driven Phrase Structure Grammar and Predicate-Argument Structures. We report the results of our system on both the development and test sets. 1. Introduction How can we integrate deep syntactic information into current statistical machine translation (SMT) systems to further improve the accuracy and fluency? How to deal with global reordering problem for a language pair that do not share isomorphic syntactic structures? These remain to be essential issues faced by current SMT research community. In this paper, we manage to answer these questions in terms of string-to-tree translation [1, 2, 3]. English, the target language in our case study, is the most popularly researched language with plenty resources and syntactic parsers. In contrast to commit to Probabilistic Context-Free Grammar (PCFG) which only generates shallow trees of English [1, 2, 3], we propose the use of deep parse trees and semantic dependencies described respectively by Head-driven Phrase Structure Grammar (HPSG) [4, 5] and PredicateArgument Structures (PASs). We illustrate two major characteristics that an HPSG tree (used by us) differs from a PCFG tree. First, a node in an HPSG tree is represented by a typed fea"
2009.iwslt-evaluation.15,P06-1121,0,0.0665645,"by Head-driven Phrase Structure Grammar and Predicate-Argument Structures. We report the results of our system on both the development and test sets. 1. Introduction How can we integrate deep syntactic information into current statistical machine translation (SMT) systems to further improve the accuracy and fluency? How to deal with global reordering problem for a language pair that do not share isomorphic syntactic structures? These remain to be essential issues faced by current SMT research community. In this paper, we manage to answer these questions in terms of string-to-tree translation [1, 2, 3]. English, the target language in our case study, is the most popularly researched language with plenty resources and syntactic parsers. In contrast to commit to Probabilistic Context-Free Grammar (PCFG) which only generates shallow trees of English [1, 2, 3], we propose the use of deep parse trees and semantic dependencies described respectively by Head-driven Phrase Structure Grammar (HPSG) [4, 5] and PredicateArgument Structures (PASs). We illustrate two major characteristics that an HPSG tree (used by us) differs from a PCFG tree. First, a node in an HPSG tree is represented by a typed fea"
2009.iwslt-evaluation.15,N09-1025,0,0.211268,"by Head-driven Phrase Structure Grammar and Predicate-Argument Structures. We report the results of our system on both the development and test sets. 1. Introduction How can we integrate deep syntactic information into current statistical machine translation (SMT) systems to further improve the accuracy and fluency? How to deal with global reordering problem for a language pair that do not share isomorphic syntactic structures? These remain to be essential issues faced by current SMT research community. In this paper, we manage to answer these questions in terms of string-to-tree translation [1, 2, 3]. English, the target language in our case study, is the most popularly researched language with plenty resources and syntactic parsers. In contrast to commit to Probabilistic Context-Free Grammar (PCFG) which only generates shallow trees of English [1, 2, 3], we propose the use of deep parse trees and semantic dependencies described respectively by Head-driven Phrase Structure Grammar (HPSG) [4, 5] and PredicateArgument Structures (PASs). We illustrate two major characteristics that an HPSG tree (used by us) differs from a PCFG tree. First, a node in an HPSG tree is represented by a typed fea"
2009.iwslt-evaluation.15,J08-1002,1,0.797575,"e pointer to semantic arguments Table 1: Examples of syntactic/semantic features extracted from HPSG signs that are included in the output of Enju (top and bottom stands for features of phrasal and lexical nodes, respectively). ing translation. The idea proposed in this paper can be considered as a natural integration of syntactic information and semantic dependency information for assisting string-to-tree translation. We call the integration natural here, because the HPSG tree and PAS of an English sentence are generated synchronously by using a state-of-the-art HPSG parser on English, Enju1 [6]. Note that the information available in the output of Enju is a fairly crude approximation of the TFS used in the full HPSG grammar [4, 5] due to practicable considerations [6]. Although the information taken from Enju’s output is much more than the commonly used PCFG parser, the HPSG-based translation rule is still extracted from an approximation of the full HPSG grammar. 2. System Outline 2.1. Parameter Estimation The diagram of parameter estimation in our system is shown in Figure 1, which is similar to most syntax-based SMT sys1 http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html Proceed"
2009.iwslt-evaluation.15,J03-1002,0,0.00225715,"ction and estimation, GHKM [1] Phrase-training (Moses [9]) PAS-based translation rules Phrase translation table HPSG-based translation rules Binarizing (Section 4) To Hiero-style (Section 2.1) α1 SRILM [17] 5-gram LM α2 α3 Binarizing (Section 4) α4 Minimum Error Rate Training on the development sets (Z-mert [18]) Figure 1: The parameter estimation and rule combination diagram of our system. tems [1, 7]. Given bilingual parallel corpora, we first tokenize the source and target sentences (e.g., word segmentation of Chinese; punctuation segmentation and lowercase of English). Then, we use GIZA++ [8] and grow-diag-finaland balancing strategy (dealing with unaligned source/target words) [9] on the tokenized parallel corpora to obtain a phrase-aligned parallel corpora. A phrase translation table (PTT) is estimated from the phrase-aligned parallel corpora. We implement the step of phrase table extraction employing the Moses toolkit2 [9]. Recall that the Moses-style phrase translation rule takes the following form: Here, a1 and a2 are word alignments between a source language phrase f1m and a target language phrase en1 ; p(·) and l(·) are phrasal translation probabilities and lexical weights,"
2009.iwslt-evaluation.15,P07-2045,0,0.00823132,"The diagram of parameter estimation in our system is shown in Figure 1, which is similar to most syntax-based SMT sys1 http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html Proceedings of IWSLT 2009, Tokyo - Japan Original parallel corpora Target language parsing (Enju [6]) Lexical analyzing Phrase-aligned and target-language-parsed parallel corpora Tokenized parallel corpora Predicate-Argument Structure based translation rule extraction and estimation (Section 3) GIZA++ & balancing Phrase-aligned parallel corpora HPSG translation rule extraction and estimation, GHKM [1] Phrase-training (Moses [9]) PAS-based translation rules Phrase translation table HPSG-based translation rules Binarizing (Section 4) To Hiero-style (Section 2.1) α1 SRILM [17] 5-gram LM α2 α3 Binarizing (Section 4) α4 Minimum Error Rate Training on the development sets (Z-mert [18]) Figure 1: The parameter estimation and rule combination diagram of our system. tems [1, 7]. Given bilingual parallel corpora, we first tokenize the source and target sentences (e.g., word segmentation of Chinese; punctuation segmentation and lowercase of English). Then, we use GIZA++ [8] and grow-diag-finaland balancing strategy (dealing wi"
2009.iwslt-evaluation.15,J07-2003,0,0.634756,"9] on the tokenized parallel corpora to obtain a phrase-aligned parallel corpora. A phrase translation table (PTT) is estimated from the phrase-aligned parallel corpora. We implement the step of phrase table extraction employing the Moses toolkit2 [9]. Recall that the Moses-style phrase translation rule takes the following form: Here, a1 and a2 are word alignments between a source language phrase f1m and a target language phrase en1 ; p(·) and l(·) are phrasal translation probabilities and lexical weights, respectively; wp stands for the word penalty. In order to be used in CKY-style decoding [10], a rule in the form of (1) can be easily transformed into an end-to-end Hiero-style [10] translation rule: (2) And the corresponding synchronous PCFG production takes the form of: X →∑i wi ∗log(pi ) f1m , en1 . (3) It has been proved that the end-to-end phrase table significantly influence the translation result of syntax-based systems [11, 12]. Note that the phrase table mentioned here 2 http://www.statmt.org/moses/ is not constrained to be linguistic phrases, i.e., a phrase inside the table is not necessarily covered by a subtree. This makes the phrase table more flexible to be used than in"
2009.iwslt-evaluation.15,P09-1020,0,0.0220831,"hronous PCFG production takes the form of: X →∑i wi ∗log(pi ) f1m , en1 . (3) It has been proved that the end-to-end phrase table significantly influence the translation result of syntax-based systems [11, 12]. Note that the phrase table mentioned here 2 http://www.statmt.org/moses/ is not constrained to be linguistic phrases, i.e., a phrase inside the table is not necessarily covered by a subtree. This makes the phrase table more flexible to be used than in the tree/forest-to-string systems [11, 12] where additional approaches have to be employed in order to make use of nonlinguistic phrases [13, 14]. Return back to Figure 1, HPSG trees attached with PASs are generated by parsing the target language sentences using Enju. Then, we extract HPSG and PAS based translation rules from the word-aligned and target-language-parsed parallel corpora. The HPSG-based xRs (tree-to-string, [15]) translation rules (binarized inversely [16]) are extracted using the GHKM minimal-rule extraction algorithm of [1]. In order to trace the lexical level translation information (similar to PASbased rules (Section 3 and Figure 2)), we remember the endto-end alignments in an HPSG-based translation rule. The ideas a"
2009.iwslt-evaluation.15,N04-1014,0,0.0309107,"t constrained to be linguistic phrases, i.e., a phrase inside the table is not necessarily covered by a subtree. This makes the phrase table more flexible to be used than in the tree/forest-to-string systems [11, 12] where additional approaches have to be employed in order to make use of nonlinguistic phrases [13, 14]. Return back to Figure 1, HPSG trees attached with PASs are generated by parsing the target language sentences using Enju. Then, we extract HPSG and PAS based translation rules from the word-aligned and target-language-parsed parallel corpora. The HPSG-based xRs (tree-to-string, [15]) translation rules (binarized inversely [16]) are extracted using the GHKM minimal-rule extraction algorithm of [1]. In order to trace the lexical level translation information (similar to PASbased rules (Section 3 and Figure 2)), we remember the endto-end alignments in an HPSG-based translation rule. The ideas are described in [1, 11] in detail. In order to use the dependency structure, we describe a linear-time algorithm based on minimum covering trees to extract PAS-based translation rules (Section 3). SRI Language Modeling (SRILM) toolkit3 [17] is employed to train a 5-gram language model"
2009.iwslt-evaluation.15,N06-1033,0,0.449598,"a phrase inside the table is not necessarily covered by a subtree. This makes the phrase table more flexible to be used than in the tree/forest-to-string systems [11, 12] where additional approaches have to be employed in order to make use of nonlinguistic phrases [13, 14]. Return back to Figure 1, HPSG trees attached with PASs are generated by parsing the target language sentences using Enju. Then, we extract HPSG and PAS based translation rules from the word-aligned and target-language-parsed parallel corpora. The HPSG-based xRs (tree-to-string, [15]) translation rules (binarized inversely [16]) are extracted using the GHKM minimal-rule extraction algorithm of [1]. In order to trace the lexical level translation information (similar to PASbased rules (Section 3 and Figure 2)), we remember the endto-end alignments in an HPSG-based translation rule. The ideas are described in [1, 11] in detail. In order to use the dependency structure, we describe a linear-time algorithm based on minimum covering trees to extract PAS-based translation rules (Section 3). SRI Language Modeling (SRILM) toolkit3 [17] is employed to train a 5-gram language model (LM) on the tokenized target language side w"
2009.iwslt-evaluation.15,P03-1021,0,0.00667254,"lexical level translation information (similar to PASbased rules (Section 3 and Figure 2)), we remember the endto-end alignments in an HPSG-based translation rule. The ideas are described in [1, 11] in detail. In order to use the dependency structure, we describe a linear-time algorithm based on minimum covering trees to extract PAS-based translation rules (Section 3). SRI Language Modeling (SRILM) toolkit3 [17] is employed to train a 5-gram language model (LM) on the tokenized target language side with Kneser-Ney smoothing. Toolkit Z-mert4 [18] is used for Minimum-Error Rate Training (MERT) [19]. 3 http://www.speech.sri.com/projects/srilm/ 4 http://www.cs.jhu.edu/ - 100 - ozaidan/zmert/ Proceedings of IWSLT 2009, Tokyo - Japan 2.2. Rule Combination Since the PTT, the HPSG-based rules, and the PAS-based rules are independently extracted and estimated, the distribution overlapping among them is inevitable. As shown in Figure 1, we use Z-mert to tune their weights on the development sets. The optimal derivation is computed by:   3 ∑  ∑ αi wj 4 d∗ = arg max log pji i (d) + log pα . LM (d)  d∈D  i=1 ji Here, pj1 , pj2 , and pj3 represent the feature subsets from PPT, binarized PAS-b"
2009.iwslt-evaluation.15,W05-1506,0,0.0324405,"3], we define a PASR to be an xRs rule and binarize it in an inverse way [16]. 3.2.1. Definition of PASR 2.3. Decoding We use a CKY-style algorithm with beam-pruning and cubepruning [10] to decode Chinese sentences. For efficient decoding with integrated N-gram LMs, we binarize all translation rules into rules that contain at most two variables and can be incrementally scored by LM [16]. For each source language sentence f , the output of the chart-parsing algorithm is expressed as a hyper-graph representing a set of derivations. Given a hyper-graph for f , we use the Algorithm 3 described in [20] to extract its k-best derivations. Since different derivations may lead to the same target language string e, we further adopt Algorithm 3’s modification (i.e., keep a hash-table to maintain the unique target sentences [21]) to efficiently generate the unique k-best translations. 3. PAS-based Translation Rule Extraction In this section, we first express an example of an HPSG tree attached with PASs, and then describe the data structure and an extraction algorithm of PAS-based translation rules (short as PASR, hereafter). A PASR is a 4-tuple ⟨S, T, A, n⟩, which describes the alignment A betwee"
2009.iwslt-evaluation.15,2006.amta-papers.8,0,0.0203907,"e included in the three subsets. In addition, number of phrases and words are contained in pj1 , and number of rules are included in pj2 and pj3 . three elements in the source and target language sides. In particular, we observe that the “head” of this rule is ignored whose arguments can be generalized into variables. Note that PASs are not only attached to verbs in a sentence, but also to all other words in the sentence. The corresponding PASRs are illustrated in Figure 2 as well. Even apparently similar in data structure, we argue our PASRs are still different from the traditional xRs rules [1, 11, 21], since the knowledge of semantic dependencies are further explicitly employed. We give the formal definitions and a lineartime rule extraction algorithm in the following subsections. 3.2. Definitions Using a strategy similar to most string-to-tree systems [3], we define a PASR to be an xRs rule and binarize it in an inverse way [16]. 3.2.1. Definition of PASR 2.3. Decoding We use a CKY-style algorithm with beam-pruning and cubepruning [10] to decode Chinese sentences. For efficient decoding with integrated N-gram LMs, we binarize all translation rules into rules that contain at most two varia"
2009.iwslt-evaluation.15,P02-1040,0,0.0763328,"Missing"
2020.aacl-main.89,C10-1021,0,0.209234,"Lake Kawaguchi.” Furthermore, this study does not accept natural language requests as inputs, which is a major bottleneck for building dialog-based interactive systems. In this study, we propose a novel and practical task to identify and explain evidences that satisfy a given vague request expressed freely in natural language. Speciﬁcally, assuming a practical situation of recommendation, we address a hotel booking service. When choosing a hotel on an interactive service, users make a wide range of vague requests, which differ from predeﬁned aspects (Wang et al., 2010), emotional expressions (Chen et al., 2010) and questions (Rajani et al., 2019). In order to satisfy vague requests by recommending hotels with evidences, the system must understand a given request, associate the request to a hotel with speExplainable recommendation is a good way to improve user satisfaction. However, explainable recommendation in dialogue is challenging since it has to handle natural language as both input and output. To tackle the challenge, this paper proposes a novel and practical task to explain evidences in recommending hotels given vague requests expressed freely in natural language. We decompose the process int"
2020.aacl-main.89,N19-1423,0,0.10738,"n dataset, which is the largest dataset for explaining evidences in recommending hotels for vague requests. Assuming that titles of hotel reviews often correspond to vague requests, the dataset includes 37,280 hotel reviews with annotations for vague requests, evidence sentences for the requests, recommendation sentences based on the evidence sentences. The key feature of the dataset is the variety of requests: it includes 15,767 unique types of requests written in natural language. This dataset is publicly available2 . We report experiments for the two subtasks in Section 3. We build a BERT (Devlin et al., 2019) model for the ﬁrst subtask, which predicts whether a sentence contains evidence for a request. Experimental results show that the model can detect evidence sentence for various requests with a high (79.94) F1-score, and that the score does not drop so much even for requests unseen in the training data. We present encoderdecoder models for the second subtask, which rewrite an evidence sentence into a recommendation sentence. The experiments demonstrate that an LSTM (Luong et al., 2015) based model achieves the BLEU score (Papineni et al., 2002) of 56.09 with a gold evidence sentence given and"
2020.aacl-main.89,D16-1170,0,0.0123865,"ated output is no-evidence. 897 texts with the overall ratings. Zhao et al. (2019) formulated a problem called personalized reason generation and generated a recommendation sentence given a song name, author, and user tag as input. The inputs of those studies were user vectors created from the user’s action history or limited aspects. However, our study deals with a wide range of natural language requests for a dialog system in the hotel booking domain. In the ﬁeld of sentiment analysis, research that extracts evidence based on sentiment expressions has attracted attention (Chen et al., 2010; Gui et al., 2016; Kim and Klinger, 2018). Chen et al. (2010) extracted the cause of a target emotional expression based on a rule. Gui et al. (2016) annotated an emotional expression and its cause. These studies aimed to gather useful information to extract emotional expressions and provide evidence simultaneously by examining the reputations for speciﬁc products. Although our study also aims to collect useful information, the requests are not limited to emotional expressions. In addition, we generate recommendation sentences. Our study can be viewed as a special application of argument mining in the domain o"
2020.aacl-main.89,C18-1114,0,0.0127686,"evidence. 897 texts with the overall ratings. Zhao et al. (2019) formulated a problem called personalized reason generation and generated a recommendation sentence given a song name, author, and user tag as input. The inputs of those studies were user vectors created from the user’s action history or limited aspects. However, our study deals with a wide range of natural language requests for a dialog system in the hotel booking domain. In the ﬁeld of sentiment analysis, research that extracts evidence based on sentiment expressions has attracted attention (Chen et al., 2010; Gui et al., 2016; Kim and Klinger, 2018). Chen et al. (2010) extracted the cause of a target emotional expression based on a rule. Gui et al. (2016) annotated an emotional expression and its cause. These studies aimed to gather useful information to extract emotional expressions and provide evidence simultaneously by examining the reputations for speciﬁc products. Although our study also aims to collect useful information, the requests are not limited to emotional expressions. In addition, we generate recommendation sentences. Our study can be viewed as a special application of argument mining in the domain of hotel review. Liu et a"
2020.aacl-main.89,D15-1166,0,0.0993361,"s dataset is publicly available2 . We report experiments for the two subtasks in Section 3. We build a BERT (Devlin et al., 2019) model for the ﬁrst subtask, which predicts whether a sentence contains evidence for a request. Experimental results show that the model can detect evidence sentence for various requests with a high (79.94) F1-score, and that the score does not drop so much even for requests unseen in the training data. We present encoderdecoder models for the second subtask, which rewrite an evidence sentence into a recommendation sentence. The experiments demonstrate that an LSTM (Luong et al., 2015) based model achieves the BLEU score (Papineni et al., 2002) of 56.09 with a gold evidence sentence given and that of 45.38 without a gold sentence (only a re2 view and a request is given). We also report experiments when the two subtasks are combined to generate a recommendation sentence for a given review. The contributions of this paper are as follows: 1. We propose a novel and practical task to explain evidences given vague requests expressed freely in natural language. 2. We create a new dataset by annotating review sentences with evidences and rewriting each evidence into a recommendatio"
2020.aacl-main.89,N19-4009,0,0.0174508,"ecommendation sentence by focusing on the root node in the parse tree of the evidence sentence. The rules include: if the root node is a verb, adjective, or auxiliary verb, add “because” at the beginning; if the root node is a noun, add “because of” at the beginning; and if the root node in an adverb, add “because you can do” at the beginning. For neural network models, we employed an LSTM model with attention (Luong et al., 2015) and a Transformer model (Vaswani et al., 2017), assuming that the task is translation from an evidence sentence into a recommendation sentence. We used the FAIRSEQ (Ott et al., 2019) to implement the models. We tokenized it using Juman++ and BPE. The input to the model was in the following order: request sentence, [SEP], and evidence sentence. Hyper-parameters of the models were tuned by the BLEU score on the development set. For the evaluation, we used the BLEU score on sentences tokenized by Juman++ (not by BPE). Since the number of references for each evidence sentence was not constant, we randomly selected one. 3.3 End-to-end Experiment In this section, we present an experiment to generate a recommendation sentence given review data (a request and review sentences) as"
2020.aacl-main.89,P02-1040,0,0.10771,"for the two subtasks in Section 3. We build a BERT (Devlin et al., 2019) model for the ﬁrst subtask, which predicts whether a sentence contains evidence for a request. Experimental results show that the model can detect evidence sentence for various requests with a high (79.94) F1-score, and that the score does not drop so much even for requests unseen in the training data. We present encoderdecoder models for the second subtask, which rewrite an evidence sentence into a recommendation sentence. The experiments demonstrate that an LSTM (Luong et al., 2015) based model achieves the BLEU score (Papineni et al., 2002) of 56.09 with a gold evidence sentence given and that of 45.38 without a gold sentence (only a re2 view and a request is given). We also report experiments when the two subtasks are combined to generate a recommendation sentence for a given review. The contributions of this paper are as follows: 1. We propose a novel and practical task to explain evidences given vague requests expressed freely in natural language. 2. We create a new dataset by annotating review sentences with evidences and rewriting each evidence into a recommendation sentence. This is the largest dataset for explaining evide"
2020.aacl-main.89,P19-1487,0,0.0185816,"study does not accept natural language requests as inputs, which is a major bottleneck for building dialog-based interactive systems. In this study, we propose a novel and practical task to identify and explain evidences that satisfy a given vague request expressed freely in natural language. Speciﬁcally, assuming a practical situation of recommendation, we address a hotel booking service. When choosing a hotel on an interactive service, users make a wide range of vague requests, which differ from predeﬁned aspects (Wang et al., 2010), emotional expressions (Chen et al., 2010) and questions (Rajani et al., 2019). In order to satisfy vague requests by recommending hotels with evidences, the system must understand a given request, associate the request to a hotel with speExplainable recommendation is a good way to improve user satisfaction. However, explainable recommendation in dialogue is challenging since it has to handle natural language as both input and output. To tackle the challenge, this paper proposes a novel and practical task to explain evidences in recommending hotels given vague requests expressed freely in natural language. We decompose the process into two subtasks on hotel reviews: evi"
2020.aacl-main.89,D16-1264,0,0.0490441,"tences. If the BERT model predicts no sentence in the review as evidence, the method generates [no-evidence]. The end-to-end method is an encoder-decoder LSTM model that directly generates a recommendation sentence given a review title and text. An input to the model is request and [SEP], followed by multiple sentences of the review. When a review did not contain an evidence for the request, the model is trained to generate [no-evidence]. Table 10 shows the BLEU scores and the macro-average F1-scores of the methods. The macro-average F1-score is deﬁned similarly to the evaluation conducted by Rajpurkar et al. (2016)10 . The pipeline method outperformed the end-to-end method, achiving a BLEU score of 45.38, 29.11 points higher than the end2end model. This is probably because the pipeline model could utilize the pre-trained BERT model and because training the end-to-end method was difﬁcult with very long sequences of tokens given as inputs. In addition, the end-to-end method tends to output too many [no-evidence] and the total number of output words is low, so the BLEU score is also low due to brevity penalty. Table 11 presents examples of the generated sentences. In the ﬁrst example, the both models succe"
2020.aacl-main.89,P16-1162,0,0.0145345,"Missing"
2020.aacl-main.89,L18-1355,0,0.0206773,"ce, we extracted vague requests from review titles and annotated evidence sentences for requests in review texts. Finally, we rewrote the evidence sentences into recommendation sentences. Figure 1 illustrates the overall pipeline to construct the dataset. It consists of three steps. that included words representing accommodations such as “inn” or “hotel.” In addition, we applied ﬁltering rules to remove other unuseful titles4 . Considering the possibility of data imbalance, we performed a categorical analysis. First, we applied morphological analysis of the collected requests using SudachiPy (Takaoka et al., 2018) to normalize surface variations in the requests. We manually checked and categorized all ﬁltered titles appearing more than twenty times in the corpus, which resulted in ten categories of vague requests. The distribution of categories in the dataset was skewed; the numbers of instances for some categories were small. For example, “Good hotel” is common but not “Hotel with delicious food.” This is because a small percentage of requests appear with the expression “inn” or “hotel.” Titles such as “Delicious dinner” are more frequent than “Hotel with delicious food.” Therefore, we extracted addit"
2020.aacl-main.89,D18-2010,0,0.0305315,"Missing"
2020.acl-main.123,P18-1015,0,0.0837668,"et al., 2000; Erkan and Radev, 2004; Mihalcea, 2004; Lin and Bilmes, 2011). In contrast, the latter involves more complex linguistic operations (e.g., abstraction, paraphrasing, and compression) to generate a new text (Knight and Marcu, 2000; Clarke and Lapata, 2008). Until 2014, abstractive summarization had been less popular than extractive one because of the difficulty of generating a natural text. However, research on abstractive summarization has attracted a lot of attentions recently with the advances on encoder-decoder models (Rush et al., 2015; Takase et al., 2016; Zhou et al., 2017; Cao et al., 2018a; Song et al., 2019; Wang et al., 2019). English Gigaword (Graff and Cieri, 2003; Napoles et al., 2012) is a representative dataset for abstractive summarization. Rush et al. (2015) regarded Gigaword as a corpus containing a large number of article-headline pairs for training an encoder-decoder model. Their work assumed a task setting where the first sentence of an article is a source text and its corresponding headline is a target text (summary). Since then, it has been a common practice to use the Gigaword dataset with this task setting and to measure the quality of generated headlines with"
2020.acl-main.123,N19-1423,0,0.0160289,": the use of full-text articles increases the cost for training, and may decrease the quality of headlines because of longer inputs to encoder-decoder models. Furthermore, JNC does not provide full-text articles but only lead three sentences. Therefore, we take the latter strategy, removing non-entailment pairs from the supervision data for headline generation. 4.1 Recognizing textual entailment In order to find non-entailment pairs in the dataset, we build a binary classifier that judges whether a source document entails its headline or not. Recently, pretrained language models such as BERT (Devlin et al., 2019) show remarkable advances in the task of recognizing textual entailment (RTE)8 . Thus, we fine-tune pretrained models on the supervision data for entailment relation between source documents and their headlines. For English Gigaword dataset, we use the pretrained RoBERTa large (Liu et al., 2019) finetuned on Multi-Genre Natural Language Inference (MultiNLI) (Williams et al., 2018). We further fine1339 8 https://gluebenchmark.com/leaderboard tuned the model on the supervision data of the leadheadline pairs with entailment labels (acquired in Section 3). Here, the supervision data include leadhe"
2020.acl-main.123,P19-1213,0,0.0686982,"Missing"
2020.acl-main.123,W00-0405,0,0.0265473,"ision data. Experimental results demonstrate that the headline generation model trained on filtered supervision data shows no clear difference in ROUGE scores but remarkable improvements in automatic and manual evaluations of the generated headlines. 1 Introduction Automatic text summarization aims at condensing a text into a shorter version while maintaining the essential information (Mani, 2001). Methods on summarization are broadly categorized into two approaches: extractive and abstractive. The former extracts important words, phrases, or sentences from a source text to compile a summary (Goldstein et al., 2000; Erkan and Radev, 2004; Mihalcea, 2004; Lin and Bilmes, 2011). In contrast, the latter involves more complex linguistic operations (e.g., abstraction, paraphrasing, and compression) to generate a new text (Knight and Marcu, 2000; Clarke and Lapata, 2008). Until 2014, abstractive summarization had been less popular than extractive one because of the difficulty of generating a natural text. However, research on abstractive summarization has attracted a lot of attentions recently with the advances on encoder-decoder models (Rush et al., 2015; Takase et al., 2016; Zhou et al., 2017; Cao et al., 2"
2020.acl-main.123,P18-1064,0,0.0515929,"Missing"
2020.acl-main.123,W19-8641,1,0.869355,"Missing"
2020.acl-main.123,Y18-1034,1,0.89785,"Missing"
2020.acl-main.123,D19-1051,0,0.0376765,"Missing"
2020.acl-main.123,D18-2012,0,0.013493,"it of training (3.8M instances), development (390k instances), and test (380k instances) sets to Rush et al. (2015). In this study, we used 10,000 instances for evaluation that were sampled from the test set and unused in the analysis in Section 3. We do not apply any replace operations for the English Gigaword dataset: digit masking, rare word to UNK, and lower-casing. The dataset is tokenized by WordPiece (Wu et al., 2016) with the same vocabulary used in UniLM. Splitting JNC into 1.7M training and 3k development instances, we evaluate the model on the JAMUL dataset. We use SentencePiece10 (Kudo and Richardson, 2018) for tokenization. 5.3 Evaluation protocol We evaluate the quality of generated headlines by using full-length F1 ROUGE scores11 , following the previous work. However, Kry´sci´nski et al. (2019) reported that ROUGE scores between system and reference summaries had only a weak correlation with human judgments. Furthermore, we would like to confirm whether the filtering strategy can improve the truthfulness of the model. Therefore, we also report the support score, the ratio of entailment relation between source documents and generated headlines measured by the entailment classifiers (explained"
2020.acl-main.123,W04-3230,0,0.0634113,"Missing"
2020.acl-main.123,C18-1121,0,0.0828837,"Missing"
2020.acl-main.123,W12-3018,0,0.0993584,"Missing"
2020.acl-main.123,N19-4009,0,0.0382972,"Missing"
2020.acl-main.123,N18-2102,0,0.0388118,"Missing"
2020.acl-main.123,D15-1044,0,0.74215,"entences from a source text to compile a summary (Goldstein et al., 2000; Erkan and Radev, 2004; Mihalcea, 2004; Lin and Bilmes, 2011). In contrast, the latter involves more complex linguistic operations (e.g., abstraction, paraphrasing, and compression) to generate a new text (Knight and Marcu, 2000; Clarke and Lapata, 2008). Until 2014, abstractive summarization had been less popular than extractive one because of the difficulty of generating a natural text. However, research on abstractive summarization has attracted a lot of attentions recently with the advances on encoder-decoder models (Rush et al., 2015; Takase et al., 2016; Zhou et al., 2017; Cao et al., 2018a; Song et al., 2019; Wang et al., 2019). English Gigaword (Graff and Cieri, 2003; Napoles et al., 2012) is a representative dataset for abstractive summarization. Rush et al. (2015) regarded Gigaword as a corpus containing a large number of article-headline pairs for training an encoder-decoder model. Their work assumed a task setting where the first sentence of an article is a source text and its corresponding headline is a target text (summary). Since then, it has been a common practice to use the Gigaword dataset with this task sett"
2020.acl-main.123,N03-1020,0,0.436063,"Missing"
2020.acl-main.123,P11-1052,0,0.0293733,"generation model trained on filtered supervision data shows no clear difference in ROUGE scores but remarkable improvements in automatic and manual evaluations of the generated headlines. 1 Introduction Automatic text summarization aims at condensing a text into a shorter version while maintaining the essential information (Mani, 2001). Methods on summarization are broadly categorized into two approaches: extractive and abstractive. The former extracts important words, phrases, or sentences from a source text to compile a summary (Goldstein et al., 2000; Erkan and Radev, 2004; Mihalcea, 2004; Lin and Bilmes, 2011). In contrast, the latter involves more complex linguistic operations (e.g., abstraction, paraphrasing, and compression) to generate a new text (Knight and Marcu, 2000; Clarke and Lapata, 2008). Until 2014, abstractive summarization had been less popular than extractive one because of the difficulty of generating a natural text. However, research on abstractive summarization has attracted a lot of attentions recently with the advances on encoder-decoder models (Rush et al., 2015; Takase et al., 2016; Zhou et al., 2017; Cao et al., 2018a; Song et al., 2019; Wang et al., 2019). English Gigaword"
2020.acl-main.123,D16-1112,1,0.858095,"rce text to compile a summary (Goldstein et al., 2000; Erkan and Radev, 2004; Mihalcea, 2004; Lin and Bilmes, 2011). In contrast, the latter involves more complex linguistic operations (e.g., abstraction, paraphrasing, and compression) to generate a new text (Knight and Marcu, 2000; Clarke and Lapata, 2008). Until 2014, abstractive summarization had been less popular than extractive one because of the difficulty of generating a natural text. However, research on abstractive summarization has attracted a lot of attentions recently with the advances on encoder-decoder models (Rush et al., 2015; Takase et al., 2016; Zhou et al., 2017; Cao et al., 2018a; Song et al., 2019; Wang et al., 2019). English Gigaword (Graff and Cieri, 2003; Napoles et al., 2012) is a representative dataset for abstractive summarization. Rush et al. (2015) regarded Gigaword as a corpus containing a large number of article-headline pairs for training an encoder-decoder model. Their work assumed a task setting where the first sentence of an article is a source text and its corresponding headline is a target text (summary). Since then, it has been a common practice to use the Gigaword dataset with this task setting and to measure th"
2020.acl-main.123,2021.ccl-1.108,0,0.0462017,"Missing"
2020.acl-main.123,W01-0100,0,0.632582,"edy the problem of the untruthful behaviors of the model. Building a binary classifier that predicts an entailment relation between an article and its headline, we filter out untruthful instances from the supervision data. Experimental results demonstrate that the headline generation model trained on filtered supervision data shows no clear difference in ROUGE scores but remarkable improvements in automatic and manual evaluations of the generated headlines. 1 Introduction Automatic text summarization aims at condensing a text into a shorter version while maintaining the essential information (Mani, 2001). Methods on summarization are broadly categorized into two approaches: extractive and abstractive. The former extracts important words, phrases, or sentences from a source text to compile a summary (Goldstein et al., 2000; Erkan and Radev, 2004; Mihalcea, 2004; Lin and Bilmes, 2011). In contrast, the latter involves more complex linguistic operations (e.g., abstraction, paraphrasing, and compression) to generate a new text (Knight and Marcu, 2000; Clarke and Lapata, 2008). Until 2014, abstractive summarization had been less popular than extractive one because of the difficulty of generating a"
2020.acl-main.123,P04-3020,0,0.11064,"at the headline generation model trained on filtered supervision data shows no clear difference in ROUGE scores but remarkable improvements in automatic and manual evaluations of the generated headlines. 1 Introduction Automatic text summarization aims at condensing a text into a shorter version while maintaining the essential information (Mani, 2001). Methods on summarization are broadly categorized into two approaches: extractive and abstractive. The former extracts important words, phrases, or sentences from a source text to compile a summary (Goldstein et al., 2000; Erkan and Radev, 2004; Mihalcea, 2004; Lin and Bilmes, 2011). In contrast, the latter involves more complex linguistic operations (e.g., abstraction, paraphrasing, and compression) to generate a new text (Knight and Marcu, 2000; Clarke and Lapata, 2008). Until 2014, abstractive summarization had been less popular than extractive one because of the difficulty of generating a natural text. However, research on abstractive summarization has attracted a lot of attentions recently with the advances on encoder-decoder models (Rush et al., 2015; Takase et al., 2016; Zhou et al., 2017; Cao et al., 2018a; Song et al., 2019; Wang et al., 2"
2020.acl-main.123,P19-1207,0,0.0278084,"Missing"
2020.acl-main.123,N18-1101,0,0.117281,"Missing"
2020.acl-main.123,D18-1089,0,0.0136082,"is unconventional, the intention here is to measure how many words in a generated headline originate from the input document. In other words, if all words in a generated headline are covered by its source document (truthful), the score is 100; if none of the words in a generated headline originate from its source document (untruthful), the score is 0. We call this ROUGE score support score hereafter to avoid naming conflicts with conventional ROUGE scores between system and reference summaries. We mention that we can find a similar method to the support score in several studies; for example, Zhang et al. (2018) measured the abstractiveness of an output. Our support score is roughly a reverse 4 We ignore instances whose source documents are less than ten characters long. The total number of instances after this treatment is 1,936. 0 20 40 60 80 100 ROUGE-1 (F-value) between system and reference outputs Figure 2: Scatter plots of ROUGE scores and support scores: X-axis presents ROUGE-1 score between system and reference headlines; and Y-axis presents support score (the same to Figure 1). version of abstractiveness because the abstractiveness measures the number of words in an output that do not appear"
2020.acl-main.123,P17-1101,0,0.0252781,"summary (Goldstein et al., 2000; Erkan and Radev, 2004; Mihalcea, 2004; Lin and Bilmes, 2011). In contrast, the latter involves more complex linguistic operations (e.g., abstraction, paraphrasing, and compression) to generate a new text (Knight and Marcu, 2000; Clarke and Lapata, 2008). Until 2014, abstractive summarization had been less popular than extractive one because of the difficulty of generating a natural text. However, research on abstractive summarization has attracted a lot of attentions recently with the advances on encoder-decoder models (Rush et al., 2015; Takase et al., 2016; Zhou et al., 2017; Cao et al., 2018a; Song et al., 2019; Wang et al., 2019). English Gigaword (Graff and Cieri, 2003; Napoles et al., 2012) is a representative dataset for abstractive summarization. Rush et al. (2015) regarded Gigaword as a corpus containing a large number of article-headline pairs for training an encoder-decoder model. Their work assumed a task setting where the first sentence of an article is a source text and its corresponding headline is a target text (summary). Since then, it has been a common practice to use the Gigaword dataset with this task setting and to measure the quality of genera"
2020.acl-main.147,W19-5203,0,0.111334,"al. (2019) integrate source-side syntax by concatenating the intermediate representations of a dependency parser to word embeddings. ∗ Work done while at Tokyo Institute of Technology. In contrast to ours, this approach does not allow to learn sub-word units at the source side, requiring a larger vocabulary to minimize out-of-vocabulary words. Saunders et al. (2018) interleave words with syntax representations which results in longer sequences – requiring gradient accumulation for effective training – while only leading to +0.5 B LEU on WAT Ja-En when using ensembles of Transformers. Finally, Currey and Heafield (2019) propose two simple data augmentation techniques to incorporate source-side syntax: one that works well on low-resource data, and one that achieves a high score on a large-scale task. Our approach, on the other hand, performs equally well in both settings. While these studies improve the translation quality of the Transformer, they do not exploit its properties. In response, we propose to explicitly enhance the its self-attention mechanism (a core component of this architecture) to include syntactic information without compromising its flexibility. Recent studies have, in fact, shown that self"
2020.acl-main.147,P16-1078,0,0.101241,"Missing"
2020.acl-main.147,P17-2012,0,0.0127271,"y, especially for long sentences and in low-resource scenarios. We show the efficacy of each approach on WMT English↔German and English→Turkish, and WAT English→Japanese translation tasks. 1 Introduction Research in neural machine translation (NMT) has mostly exploited corpora consisting of pairs of parallel sentences, with the assumption that a model can automatically learn prior linguistic knowledge via an attention mechanism (Luong et al., 2015). However, Shi et al. (2006) found that these models still fail to capture deep structural details, and several studies (Sennrich and Haddow, 2016; Eriguchi et al., 2017; Chen et al., 2017, 2018) have shown that syntactic information has the potential to improve these models. Nevertheless, the majority of syntax-aware NMT models are based on recurrent neural networks (RNNs; Elman 1990), with only a few recent studies that have investigated methods for the Transformer model (Vaswani et al., 2017). Wu et al. (2018) evaluated an approach to incorporate syntax in NMT with a Transformer model, which not only required three encoders and two decoders, but also target-side dependency relations (precluding its use to low-resource target languages). Zhang et al. (2019)"
2020.acl-main.147,D17-1012,0,0.170767,"Missing"
2020.acl-main.147,D10-1092,0,0.0793436,"Missing"
2020.acl-main.147,W04-3250,0,0.34598,"Missing"
2020.acl-main.147,P07-2045,0,0.00881105,"Missing"
2020.acl-main.147,D15-1166,0,0.508266,"e syntactic knowledge in the Transformer model and also propose a novel, parameter-free, dependency-aware self-attention mechanism that improves its translation quality, especially for long sentences and in low-resource scenarios. We show the efficacy of each approach on WMT English↔German and English→Turkish, and WAT English→Japanese translation tasks. 1 Introduction Research in neural machine translation (NMT) has mostly exploited corpora consisting of pairs of parallel sentences, with the assumption that a model can automatically learn prior linguistic knowledge via an attention mechanism (Luong et al., 2015). However, Shi et al. (2006) found that these models still fail to capture deep structural details, and several studies (Sennrich and Haddow, 2016; Eriguchi et al., 2017; Chen et al., 2017, 2018) have shown that syntactic information has the potential to improve these models. Nevertheless, the majority of syntax-aware NMT models are based on recurrent neural networks (RNNs; Elman 1990), with only a few recent studies that have investigated methods for the Transformer model (Vaswani et al., 2017). Wu et al. (2018) evaluated an approach to incorporate syntax in NMT with a Transformer model, whic"
2020.acl-main.147,P14-5010,0,0.0035037,"approach on standard, large-scale benchmarks and on lowresource scenarios, where the Transformer was shown to induce poorer syntax. Following Bastings et al. (2017), we use News Commentary v11 (NC11) with En-De and De-En tasks to simulate low resources and test multiple source languages. To compare with previous work, we train our models on WMT16 En-De and WAT En-Ja tasks, removing sentences in incorrect languages from WMT16 data sets. For a thorough comparison with concurrent work, we also evaluate on the largescale WMT17 En-De and low-resource WMT18 En-Tr tasks. We rely on Stanford CoreNLP (Manning et al., 2014) to parse source sentences.1 Training We implement our models in PyTorch on top of the Fairseq toolkit.2 Hyperparameters, including the number of PASCAL heads, that achieved the highest validation B LEU (Papineni et al., 2002) score were selected via a small grid search. We report previous results in syntax-aware NMT for completeness, and train a Transformer model as a strong, standard baseline. We also investigate the following syntax-aware Transformer approaches:1 • +PASCAL: The model presented in §2. The variance of the normal distribution was set to 1 (i.e., an effective window size of 3)"
2020.acl-main.147,P02-1040,0,0.108432,"Missing"
2020.acl-main.147,W18-6319,0,0.0254166,"Missing"
2020.acl-main.147,W18-5431,0,0.0212495,"y translated to the past tense instead of present. PASCAL layer When we introduced our model, we motivated our design choice of placing PASCAL heads in the first layer in order to enrich the representations of words from their isolated embeddings by introducing contextualization from their parents. We ran an ablation study on the NC11 data in order to verify our hypothesis. As shown in Table 3a, the performance of our model on the validation sets is lower when placing Pascal heads in upper layers; a trend that we also observed with the LISA mechanism. These results corroborate the findings of Raganato and Tiedemann (2018) who noticed that, in the first layer, more attention heads solely focus on the word to be translated itself rather than its context. We can then deduce that enforcing syntactic dependencies in the first layer effectively leads to better word representations, which further enhance the translation accuracy of the Transformer model. Investigating the performance of multiple syntax-aware layers is left as future work. Gaussian variance Another design choice we made was the variance of the Gaussian weighing function. We set it to 1 in our experiments motivated by the statistics of our datasets, wh"
2020.acl-main.147,P18-2051,0,0.0303435,"Missing"
2020.acl-main.147,W16-2209,0,0.0402704,"oves its translation quality, especially for long sentences and in low-resource scenarios. We show the efficacy of each approach on WMT English↔German and English→Turkish, and WAT English→Japanese translation tasks. 1 Introduction Research in neural machine translation (NMT) has mostly exploited corpora consisting of pairs of parallel sentences, with the assumption that a model can automatically learn prior linguistic knowledge via an attention mechanism (Luong et al., 2015). However, Shi et al. (2006) found that these models still fail to capture deep structural details, and several studies (Sennrich and Haddow, 2016; Eriguchi et al., 2017; Chen et al., 2017, 2018) have shown that syntactic information has the potential to improve these models. Nevertheless, the majority of syntax-aware NMT models are based on recurrent neural networks (RNNs; Elman 1990), with only a few recent studies that have investigated methods for the Transformer model (Vaswani et al., 2017). Wu et al. (2018) evaluated an approach to incorporate syntax in NMT with a Transformer model, which not only required three encoders and two decoders, but also target-side dependency relations (precluding its use to low-resource target language"
2020.acl-main.147,N18-2074,0,0.0241988,"urce data, and one that achieves a high score on a large-scale task. Our approach, on the other hand, performs equally well in both settings. While these studies improve the translation quality of the Transformer, they do not exploit its properties. In response, we propose to explicitly enhance the its self-attention mechanism (a core component of this architecture) to include syntactic information without compromising its flexibility. Recent studies have, in fact, shown that self-attention networks benefit from modeling local contexts by reducing the dispersion of the attention distribution (Shaw et al., 2018; Yang et al., 2018, 2019), and that they might not capture the inherent syntactic structure of languages as well as recurrent models, especially in low-resource settings (Tran et al., 2018; Tang et al., 2018). Here, we present parentscaled self-attention (PASCAL): a novel, parameterfree local attention mechanism that lets the model focus on the dependency parent of each token when encoding the source sentence. Our method is simple yet effective, improving translation quality with no additional parameter or computational overhead. Our main contributions are: • introducing PASCAL: an effective"
2020.acl-main.147,P06-1062,0,0.0260192,"ransformer model and also propose a novel, parameter-free, dependency-aware self-attention mechanism that improves its translation quality, especially for long sentences and in low-resource scenarios. We show the efficacy of each approach on WMT English↔German and English→Turkish, and WAT English→Japanese translation tasks. 1 Introduction Research in neural machine translation (NMT) has mostly exploited corpora consisting of pairs of parallel sentences, with the assumption that a model can automatically learn prior linguistic knowledge via an attention mechanism (Luong et al., 2015). However, Shi et al. (2006) found that these models still fail to capture deep structural details, and several studies (Sennrich and Haddow, 2016; Eriguchi et al., 2017; Chen et al., 2017, 2018) have shown that syntactic information has the potential to improve these models. Nevertheless, the majority of syntax-aware NMT models are based on recurrent neural networks (RNNs; Elman 1990), with only a few recent studies that have investigated methods for the Transformer model (Vaswani et al., 2017). Wu et al. (2018) evaluated an approach to incorporate syntax in NMT with a Transformer model, which not only required three en"
2020.acl-main.147,D18-1503,0,0.0635315,"Missing"
2020.acl-main.147,D18-1548,0,0.078813,"Missing"
2020.acl-main.147,D18-1475,0,0.157682,"that achieves a high score on a large-scale task. Our approach, on the other hand, performs equally well in both settings. While these studies improve the translation quality of the Transformer, they do not exploit its properties. In response, we propose to explicitly enhance the its self-attention mechanism (a core component of this architecture) to include syntactic information without compromising its flexibility. Recent studies have, in fact, shown that self-attention networks benefit from modeling local contexts by reducing the dispersion of the attention distribution (Shaw et al., 2018; Yang et al., 2018, 2019), and that they might not capture the inherent syntactic structure of languages as well as recurrent models, especially in low-resource settings (Tran et al., 2018; Tang et al., 2018). Here, we present parentscaled self-attention (PASCAL): a novel, parameterfree local attention mechanism that lets the model focus on the dependency parent of each token when encoding the source sentence. Our method is simple yet effective, improving translation quality with no additional parameter or computational overhead. Our main contributions are: • introducing PASCAL: an effective parameterfree local"
2020.acl-main.147,N19-1118,0,0.0869429,"iguchi et al., 2017; Chen et al., 2017, 2018) have shown that syntactic information has the potential to improve these models. Nevertheless, the majority of syntax-aware NMT models are based on recurrent neural networks (RNNs; Elman 1990), with only a few recent studies that have investigated methods for the Transformer model (Vaswani et al., 2017). Wu et al. (2018) evaluated an approach to incorporate syntax in NMT with a Transformer model, which not only required three encoders and two decoders, but also target-side dependency relations (precluding its use to low-resource target languages). Zhang et al. (2019) integrate source-side syntax by concatenating the intermediate representations of a dependency parser to word embeddings. ∗ Work done while at Tokyo Institute of Technology. In contrast to ours, this approach does not allow to learn sub-word units at the source side, requiring a larger vocabulary to minimize out-of-vocabulary words. Saunders et al. (2018) interleave words with syntax representations which results in longer sequences – requiring gradient accumulation for effective training – while only leading to +0.5 B LEU on WAT Ja-En when using ensembles of Transformers. Finally, Currey and"
2020.acl-main.147,D18-1458,0,0.0338676,"Missing"
2020.acl-main.149,N18-2085,1,0.921528,"of the candidate translation our model actually produces, e.g. under approximate decoding. However, an information-theoretic evaluation 1641 is much more suitable for measuring the more abstract notion of which language pairs are hardest to translate to and from, which is our purpose here. 3 Disentangling Translation Difficulty and Monolingual Complexity We contend that simply reporting cross-entropies is not enough. A second issue in performing a controlled, cross-lingual MT comparison is that the language generation component (without translation) is not equally difficult across languages (Cotterell et al., 2018). We claim that the difficulty of translation corresponds more closely to the mutual information MI(S; T ) between the source and target language, which tells us how much easier it becomes to predict T when S is given (see Figure 1). But what is the appropriate analogue of mutual information for cross-entropy? One such natural generalization is a novel quantity that we term cross-mutual information, defined as: XMI(S → T ) = HqLM (T ) − HqMT (T |S) (3) where HqLM (T ) denotes the cross-entropy of the target sentence T under the model qLM . As in §2, this can, analogously, be approximated by th"
2020.acl-main.149,D18-1312,0,0.025153,"Missing"
2020.acl-main.149,D10-1092,0,0.0433027,"rich language is harder than translating into a morphologically impoverished one. In fact, the only significant correlate of MT difficulty we find is source-side type–token ratio. 2 Cross-Linguistic Comparability through Likelihoods, not BLEU Human evaluation will always be the gold standard of MT evaluation. However, it is both timeconsuming and expensive to perform. To help researchers and practitioners quickly deploy and evaluate new systems, automatic metrics that correlate fairly well with human evaluations have been proposed over the years (Banerjee and Lavie, 2005; Snover et al., 2006; Isozaki et al., 2010; Lo, 2019). BLEU (Papineni et al., 2002), however, has remained the most common metric to report the performance of MT systems. BLEU is a precisionbased metric: a BLEU score is proportional to the geometric average of the number of n-grams in the candidate translation that also appear in the reference translation for 1 ≤ n ≤ 4.1 In the context of our study, we take issue with two shortcomings of BLEU scores that prevent a cross-linguistically comparable study. First, it is not possible to directly compare BLEU scores across languages because different languages might express the same meaning"
2020.acl-main.149,W04-3250,0,0.537126,"Missing"
2020.acl-main.149,2005.mtsummit-papers.11,0,0.165253,"tely estimate the difficulty of translation for a given architecture in a controlled way. In summary, by looking at XMI, we can effectively decouple the language generation component, whose difficulties have been investigated by Cotterell et al. 2018 and Mielke et al. 2019, from the translation component. This gives us a measure of how rich and useful the information extracted from the source language is for the target-language generation component. 4 Experiments In order to measure which pairs of languages are harder to translate to and from, we make use of the latest release v7 of Europarl (Koehn, 2005): a corpus of the proceedings of the European Parliament containing parallel sentences between English (en) and 20 other European languages: Bulgarian (bg), Czech (cs), Danish (da), German (de), Greek (el), Spanish (es), Estonian (et), Finnish (fi), French (fr), Hungarian (hu), Italian (it), Lithuanian (lt), Latvian (lv), Dutch (nl), Polish (pl), Portuguese (pt), Romanian (ro), Slovak (sk), Slovene (sl) and Swedish (sv). Pre-processing steps In order to precisely effect a fully controlled experiment, we enforce a fair comparison by selecting the set of parallel sentences available across all 2"
2020.acl-main.149,P09-5002,0,0.0271803,"but the relation between H(S) and H(T ) can be arbitrary. Right: estimating cross-entropies using models qMT and qLM invalidates relations between bars, except that Hq· (·) ≥ H(·). XMI, our proposed metric, is no longer purely a symmetric measure of language, but now an asymmetric measure that mostly highlights models’ shortcomings. Introduction Machine translation (MT) is one of the core research areas in natural language processing. Current state-of-the-art MT systems are based on neural networks (Sutskever et al., 2014; Bahdanau et al., 2015), which generally surpass phrase-based systems (Koehn, 2009) in a variety of domains and languages (Bentivogli et al., 2016; Toral and S´anchez-Cartagena, 2017; Castilho et al., 2017; Bojar et al., 2018; Barrault et al., 2019). Using phrase-based MT systems, various controlled studies to understand where the translation difficulties lie for different language pairs were conducted (Birch et al., 2008; Koehn et al., 2009). However, comparable studies have yet to be performed for neural machine translation (NMT). As a result, it is still unclear whether all translation directions are equally easy (or hard) to model for NMT. This paper hence aims at fillin"
2020.acl-main.149,2009.mtsummit-papers.7,0,0.0241981,"translation (MT) is one of the core research areas in natural language processing. Current state-of-the-art MT systems are based on neural networks (Sutskever et al., 2014; Bahdanau et al., 2015), which generally surpass phrase-based systems (Koehn, 2009) in a variety of domains and languages (Bentivogli et al., 2016; Toral and S´anchez-Cartagena, 2017; Castilho et al., 2017; Bojar et al., 2018; Barrault et al., 2019). Using phrase-based MT systems, various controlled studies to understand where the translation difficulties lie for different language pairs were conducted (Birch et al., 2008; Koehn et al., 2009). However, comparable studies have yet to be performed for neural machine translation (NMT). As a result, it is still unclear whether all translation directions are equally easy (or hard) to model for NMT. This paper hence aims at filling this gap: Ceteris paribus, is it easier to translate from English into Finnish or into Hungarian? And how much easier is it? Conversely, is it equally hard to translate Finnish and Hungarian into another language? Based on BLEU (Papineni et al., 2002) scores, previous work (Belinkov et al., 2017) suggests that translating into morphologically rich languages,"
2020.acl-main.149,W18-1819,0,0.0425158,"Missing"
2020.acl-main.149,P02-1040,0,\N,Missing
2020.acl-main.149,W05-0909,0,\N,Missing
2020.acl-main.149,D08-1078,0,\N,Missing
2020.acl-main.149,W17-0230,0,\N,Missing
2020.acl-main.149,E17-2002,0,\N,Missing
2020.acl-main.149,W18-6401,0,\N,Missing
2020.acl-main.149,P16-1162,0,\N,Missing
2020.acl-main.149,W19-5358,0,\N,Missing
2020.acl-main.149,2020.acl-main.615,1,\N,Missing
2020.coling-main.176,L18-1273,0,0.0718806,"The news-image captioning task is different from the conventional image captioning, which receives only an image as input. In other words, news-image captioning requires a mutual understanding of image and text. Early work proposed a two-stage approach for news-image captioning (Feng and Lapata, 2013; Tariq and Foroosh, 2017). The first stage annotates keywords to a given image and text, and the second stage realizes a description based on the extracted keywords. Recent work presented an end-to-end approach that integrates image and text features in deep neural networks (Ramisa et al., 2018; Batra et al., 2018; Biten et al., 2019). However, the previous studies did not focus on the usefulness of text in the news-image captioning task, extending the conventional models for image captioning to incorporate text features. Figure 1 shows an example of a news-image caption. It may be difficult to recognize which is the central object in the image, for example, people, violins, and stick (bow). Also, the caption includes much information (e.g., Juilliard Orchestra, Vladimir Jurowsky, and Alice Tully Hall) that may be hard to tell only from the image. This kind of example is rather common in news articles,"
2020.coling-main.176,W14-3348,0,0.0115738,"of 0.3 after the layer normalization and before the residual connection. In both the encoder and decoder, we also applied dropout with the rate of 0.3 after taking the sums of token embeddings and positional encoding. We also applied dropout with the rate of 0.1 to the attention weights. Training all models for 50 epochs, we stored model parameters that yielded the minimum loss. It took about 1.2 days to train a Multimodal Transformer model on four NVIDIA Tesla V100 for NVLink (16GiB HBM2). 3.5 Evaluation Metrics We used five automatic evaluation metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), ROUGE (Lin and Och, 2004), CIDEr (Vedantam et al., 2015), and SPICE (Anderson et al., 2016). Because news-image captions contain a fair amount of named entities, we considered CIDEr as the most appropriate and informative metric among these five metrics. We used the MS-COCO caption evaluation tool6 after lower-casing and removing the punctuation in captions. Both news articles and news-image captions contain a certain amount of named entities, and these entities often carry relevant contextual knowledge. Therefore we introduced CoverageNE to measure the coverage of named entities in generate"
2020.coling-main.176,D19-1301,0,0.0209878,"l (Biten et al., 2019). We report the results of human evaluation and discuss challenges in news-image captioning. The dataset and code used in this work are publicly available1 . 2 Multimodal Transformer Model Given an image i and a text as a sequence of n tokens (x1 , x2 , . . . xn ), the task of news-image captioning generates a caption as a sequence of m tokens (y1 , y2 , . . . ym ). As the base architecture for realizing the task, we use the Transformer model (Vaswani et al., 2017), which has been a popular architecture for headline generation for news articles (Takase and Okazaki, 2019; Duan et al., 2019; Dong et al., 2019). Because headline generation is a sequence-to-sequence task from (x1 , . . . xn ) to (y1 , . . . ym ), we consider incorporating features from an image i to the architecture. Figure 2 illustrates the proposed model. The model consists of image encoder, image-article encoder, and decoder. The image encoder converts an image i into a feature vector pi ∈ Rd , pi = CNN(i). (1) Here, d presents the number of dimensions of hidden feature vectors, and CNN(.) is a Convolutional Neural Network to convert an image into a feature vector. In this study, we compare two CNN models train"
2020.coling-main.176,D18-2012,0,0.0374275,"245K articles for the training set, 10K for the validation set, and 13K for the test set. 3.2 Data Preprocessing Some articles are too long to store them in the GPU memory, and a headline and leading paragraphs of an article provide the most useful information, we truncated each news article in the dataset to keep the first 416 words at most (including the headline). In addition to truncating articles, we also removed punctuation and non-ASCII characters from text. We left a dot ‘.’ as a delimiter of sentences. Applying the algorithm of Byte-Pair-Encoding (BPE) implemented in SentencePiece2 (Kudo and Richardson, 2018) to the preprocessed news articles of the training set, we built a vocabulary of 32,000 subwords. 3.3 Baselines and Model Variants Biten et al. (2019) presented a model that achieved the state-of-art performance on the GoodNews dataset. Using the publicly-available implementation3 , we trained their models on our dataset with six different settings: Avg + AttIns, Avg + CtxIns, TBB + AttIns, TBB + CtxIns, Wavg + AttIns, and Wavg + CtxIns. These settings are described in detail in Biten et al. (2019). For the sake of clarity, we briefly explain some keywords: Avg: article embeddings computed by"
2020.coling-main.176,P04-1077,0,0.0222657,"n and before the residual connection. In both the encoder and decoder, we also applied dropout with the rate of 0.3 after taking the sums of token embeddings and positional encoding. We also applied dropout with the rate of 0.1 to the attention weights. Training all models for 50 epochs, we stored model parameters that yielded the minimum loss. It took about 1.2 days to train a Multimodal Transformer model on four NVIDIA Tesla V100 for NVLink (16GiB HBM2). 3.5 Evaluation Metrics We used five automatic evaluation metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), ROUGE (Lin and Och, 2004), CIDEr (Vedantam et al., 2015), and SPICE (Anderson et al., 2016). Because news-image captions contain a fair amount of named entities, we considered CIDEr as the most appropriate and informative metric among these five metrics. We used the MS-COCO caption evaluation tool6 after lower-casing and removing the punctuation in captions. Both news articles and news-image captions contain a certain amount of named entities, and these entities often carry relevant contextual knowledge. Therefore we introduced CoverageNE to measure the coverage of named entities in generated captions. The coverage fo"
2020.coling-main.176,N19-4009,0,0.0181161,"ual features trained for two different tasks: object recognition (ImageNet) and scene recognition (Places 365). We used ResNet-18 (He et al., 2016) pre-trained on ImageNet and Places365 to obtain visual features from an image. In addition to these variants, we also included two simple baselines: Lead and Headline assume lead sentences and headlines as image captions. In other words, these baselines reveal the similarity between image captions and lead/headline sentences. 3.4 Implementation and Training We implemented all Transformer models using PyTorch (Paszke et al., 2019) based on Fairseq (Ott et al., 2019). Hyper-parameters All Transformer-based models in our experiments used the same hyperparameters: the number of dimensions of hidden vectors d = 512; the number of attention heads H = 8; the number of encoder blocks N = 3; the number of decoder blocks M = 6. Table 1 shows the number of parameters trained for each Transformer-based model. Model Transformer (Text) Transformer (Image) Multimodal Transformer # Parameters 93M 57M 93M Table 1: Number of parameters trained in the Transformer-based models. Training We used Adam (Kingma and Ba, 2015) with β1 = 0.9, β2 = 0.98,  = 10−8 for parameter est"
2020.coling-main.176,P02-1040,0,0.107188,"ava et al., 2014) with the rate of 0.3 after the layer normalization and before the residual connection. In both the encoder and decoder, we also applied dropout with the rate of 0.3 after taking the sums of token embeddings and positional encoding. We also applied dropout with the rate of 0.1 to the attention weights. Training all models for 50 epochs, we stored model parameters that yielded the minimum loss. It took about 1.2 days to train a Multimodal Transformer model on four NVIDIA Tesla V100 for NVLink (16GiB HBM2). 3.5 Evaluation Metrics We used five automatic evaluation metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), ROUGE (Lin and Och, 2004), CIDEr (Vedantam et al., 2015), and SPICE (Anderson et al., 2016). Because news-image captions contain a fair amount of named entities, we considered CIDEr as the most appropriate and informative metric among these five metrics. We used the MS-COCO caption evaluation tool6 after lower-casing and removing the punctuation in captions. Both news articles and news-image captions contain a certain amount of named entities, and these entities often carry relevant contextual knowledge. Therefore we introduced CoverageNE to measure the co"
2020.coling-main.176,N19-1401,1,0.816751,"s the state-ofthe-art model (Biten et al., 2019). We report the results of human evaluation and discuss challenges in news-image captioning. The dataset and code used in this work are publicly available1 . 2 Multimodal Transformer Model Given an image i and a text as a sequence of n tokens (x1 , x2 , . . . xn ), the task of news-image captioning generates a caption as a sequence of m tokens (y1 , y2 , . . . ym ). As the base architecture for realizing the task, we use the Transformer model (Vaswani et al., 2017), which has been a popular architecture for headline generation for news articles (Takase and Okazaki, 2019; Duan et al., 2019; Dong et al., 2019). Because headline generation is a sequence-to-sequence task from (x1 , . . . xn ) to (y1 , . . . ym ), we consider incorporating features from an image i to the architecture. Figure 2 illustrates the proposed model. The model consists of image encoder, image-article encoder, and decoder. The image encoder converts an image i into a feature vector pi ∈ Rd , pi = CNN(i). (1) Here, d presents the number of dimensions of hidden feature vectors, and CNN(.) is a Convolutional Neural Network to convert an image into a feature vector. In this study, we compare t"
2020.emnlp-main.631,N19-1423,0,0.178758,"yze the OOV rate on downstream tasks, how it introduces information loss, and as a side-effect, obstructs the potential of the underlying model. We then propose multiple approaches for mitigation and demonstrate that it improves performance with the same parameter count when combined with finetuning. 1 Background The advent of large scale language models pretrained with large unannotated corpora has shown significant advancements in the domain of natural language processing, especially by demonstrating their effectiveness through transfer learning for downstream tasks (Howard and Ruder, 2018; Devlin et al., 2019; Conneau and Lample, 2019; Radford et al.), analogous to ImageNet (Deng et al., 2009) pre-trained backbones in the domain of computer vision. In the domain of natural language processing, new methods have made it possible to use internet corpora as a nearly free source for increasing the amount of data at an unprecedented scale during pre-training. Additionally, new tokenization methods such as Byte-Pair Encoding (BPE, Sennrich et al. (2016)), WordPiece (Wu et al., 2016), SentencePiece (Kudo and Richardson, 2018), which break the lexicons into smaller subwords, have shown to be effective when"
2020.emnlp-main.631,N16-1155,0,0.0305195,"hods such as Byte-Pair Encoding (BPE, Sennrich et al. (2016)), WordPiece (Wu et al., 2016), SentencePiece (Kudo and Richardson, 2018), which break the lexicons into smaller subwords, have shown to be effective when applied to alphabetic languages to reduce the size of the vocabulary while increasing the robustness against out-of-vocabulary (OOV) in downstream tasks. This is especially powerful when combined with transfer learning. However, these tokenizers still operate at Unicode character levels - contrary to the names suggesting bytelevel (which would completely mitigate OOV, as studied in Gillick et al. (2016)). Hence, the minimum size of the vocabulary is twice the size of all unique characters in the corpus, as subword tokenizers store each character in prefix and suffix form in the vocabulary. As OOV was a much more prevalent problem in the context of lexiconbased methods, there have been many methods, such as dictionary-based postprocessing (Luong et al., 2015) and distributional representation based substitution (Kolachina et al., 2017). Recently this has not been as actively studied in the context of subword tokenization as Latin languages are no longer affected. For these reasons, when train"
2020.emnlp-main.631,P18-1031,0,0.0289391,"gual BERT model and analyze the OOV rate on downstream tasks, how it introduces information loss, and as a side-effect, obstructs the potential of the underlying model. We then propose multiple approaches for mitigation and demonstrate that it improves performance with the same parameter count when combined with finetuning. 1 Background The advent of large scale language models pretrained with large unannotated corpora has shown significant advancements in the domain of natural language processing, especially by demonstrating their effectiveness through transfer learning for downstream tasks (Howard and Ruder, 2018; Devlin et al., 2019; Conneau and Lample, 2019; Radford et al.), analogous to ImageNet (Deng et al., 2009) pre-trained backbones in the domain of computer vision. In the domain of natural language processing, new methods have made it possible to use internet corpora as a nearly free source for increasing the amount of data at an unprecedented scale during pre-training. Additionally, new tokenization methods such as Byte-Pair Encoding (BPE, Sennrich et al. (2016)), WordPiece (Wu et al., 2016), SentencePiece (Kudo and Richardson, 2018), which break the lexicons into smaller subwords, have shown"
2020.emnlp-main.631,W17-0202,0,0.0757539,"ever, these tokenizers still operate at Unicode character levels - contrary to the names suggesting bytelevel (which would completely mitigate OOV, as studied in Gillick et al. (2016)). Hence, the minimum size of the vocabulary is twice the size of all unique characters in the corpus, as subword tokenizers store each character in prefix and suffix form in the vocabulary. As OOV was a much more prevalent problem in the context of lexiconbased methods, there have been many methods, such as dictionary-based postprocessing (Luong et al., 2015) and distributional representation based substitution (Kolachina et al., 2017). Recently this has not been as actively studied in the context of subword tokenization as Latin languages are no longer affected. For these reasons, when trained against a diverse set of languages, the vocabulary size increases proportionally to the number of languages supported. Existing models have sampled portions of entire corpora or relaxed constraints on character level coverage for these languages, to prevent the vocabulary from growing to an unmanageable scale. As of today, this is an unavoidable trade-off when training multilingual models. This introduces a bottleneck for downstream"
2020.emnlp-main.631,D16-1264,0,0.0297282,"Missing"
2020.emnlp-main.631,P16-1162,0,0.0263146,"main of natural language processing, especially by demonstrating their effectiveness through transfer learning for downstream tasks (Howard and Ruder, 2018; Devlin et al., 2019; Conneau and Lample, 2019; Radford et al.), analogous to ImageNet (Deng et al., 2009) pre-trained backbones in the domain of computer vision. In the domain of natural language processing, new methods have made it possible to use internet corpora as a nearly free source for increasing the amount of data at an unprecedented scale during pre-training. Additionally, new tokenization methods such as Byte-Pair Encoding (BPE, Sennrich et al. (2016)), WordPiece (Wu et al., 2016), SentencePiece (Kudo and Richardson, 2018), which break the lexicons into smaller subwords, have shown to be effective when applied to alphabetic languages to reduce the size of the vocabulary while increasing the robustness against out-of-vocabulary (OOV) in downstream tasks. This is especially powerful when combined with transfer learning. However, these tokenizers still operate at Unicode character levels - contrary to the names suggesting bytelevel (which would completely mitigate OOV, as studied in Gillick et al. (2016)). Hence, the minimum size of the vocab"
2020.emnlp-main.631,W18-5446,0,0.0591674,"Missing"
2020.emnlp-main.631,K19-1030,0,0.0437833,"Missing"
2020.findings-emnlp.120,2020.acl-main.275,0,0.149321,"rocess to vary tokenization, and Hiraoka et al. (2019) by updating the language model during training. Optimization of the tokenization has attracted attention mainly in the field of machine translation. Some studies have attempted to optimize the tokenization using simple criteria for a machine translation (Xu et al., 2008; Chung and Gildea, 2009; Nguyen et al., 2010; Mermer et al., 2013). Recent studies also tackled this issue for generation tasks. Salesky et al. (2020) developed Incremental BPE, which automatically defines the number of BPE’s merge operation for neural machine translation. He et al. (2020) proposed a neural architecture to find a better subword sequence from both the source and target corpora by enhancing the study in Chan et al. (2016). Our work differs from the existing research in that the proposed method is appliable to various neural encoders, and we can optimize the tokenization directly using only backpropagation from the training loss of the downstream tasks without any hand-crafted criteria. 6 Conclusion In this paper, we propose OpTok which explores an appropriate tokenization to the target downstream task. We combine OpTok with the downstream model and train them sim"
2020.findings-emnlp.120,P19-1158,1,0.85056,"2016; Sennrich et al., 2016; He and Sun, 2017; A and Augenstein, 2020; Bollegala et al., 2020). In traditional NLP, we tokenize a given sentence as a preprocessing. Thus, as shown in Figure 1(a), we apply an existing tokenizer to the given sentence, and then input the tokenized sentence into a model for a target downstream task. In the conventional approach, we obtain the most plausible tokenized sentence based on the tokenizer; however, some studies have varied the tokenization using a sampling during the training to enable the downstream model to adapt to various tokenizations (Kudo, 2018; Hiraoka et al., 2019; Provilkov et al., 2019). Although such a strategy makes the downstream model robust, little attention has been paid to optimizing the tokenizers for a downstream task. Thus, if we acquire an appropriate tokenization to a downstream task, we might improve the task performance. By contrast, some studies have used multiple tokenized sentences to prevent the damage depending on the tokenization (Chen et al., 2017; Zhang and Yang, 2018; Yang et al., 2018). Their methods compute various tokenizations for a given sentence, and then encode the tokenizations using an architecture based on the LSTM (H"
2020.findings-emnlp.120,P17-1137,0,0.012306,"y by taking the top-|V 0 |tokens from V based on the updated token probabilities obtained by Eq. (2). Such sampling of the vocabulary results in the diversity of tokenization in the N -best candidates during training. Setting the lower α mentioned in Section 2.2, the distribution of the tokens becomes flatter, and the model can sample various tokens for V 0 . In addition, through the sampling process, we can reduce the importance of words that are unuseful in V for the downstream task. This procedure is related to a vocabulary restriction with a continuous cache technique (Grave et al., 2016; Kawakami et al., 2017). 2.5 (6) Maintaining Nature of Language Model Since the optimization of OpTok only depends on the loss function for the downstream task, the language model of OpTok might be much different from the unigram language model (i.e., frequency 2 an hs0n . Restricting Vocabulary We tried an alternative approach to sampling a plausible tokenization using Gumbel softmax (Jang et al., 2016), but found that it causes instability in the learning. 1343 Positive Weibo(Zh) 407,057 Twitter(Ja) 10,319 Twitter(En) 56,462 Negative 263,995 16,035 43,538 Neutral 135,830 - Total 671,052 162,184 100,000 Table 1: Da"
2020.findings-emnlp.120,P18-1007,0,0.23081,"redze, 2015, 2016; Sennrich et al., 2016; He and Sun, 2017; A and Augenstein, 2020; Bollegala et al., 2020). In traditional NLP, we tokenize a given sentence as a preprocessing. Thus, as shown in Figure 1(a), we apply an existing tokenizer to the given sentence, and then input the tokenized sentence into a model for a target downstream task. In the conventional approach, we obtain the most plausible tokenized sentence based on the tokenizer; however, some studies have varied the tokenization using a sampling during the training to enable the downstream model to adapt to various tokenizations (Kudo, 2018; Hiraoka et al., 2019; Provilkov et al., 2019). Although such a strategy makes the downstream model robust, little attention has been paid to optimizing the tokenizers for a downstream task. Thus, if we acquire an appropriate tokenization to a downstream task, we might improve the task performance. By contrast, some studies have used multiple tokenized sentences to prevent the damage depending on the tokenization (Chen et al., 2017; Zhang and Yang, 2018; Yang et al., 2018). Their methods compute various tokenizations for a given sentence, and then encode the tokenizations using an architectur"
2020.findings-emnlp.120,D18-2012,0,0.424759,"α probability as p∗ (w) = P p(w)p(w) ˆ α . We convert a w∈V ˆ sentence into a sequence of tokens depending on a probability of a tokenized sentence: Y p∗ (w). (3) p(s0 ) = w∈s0 We initialize vocabulary V with a reasonable number of tokens. To choose the initial vocabulary, both supervised and unsupervised word segmentation methods are available, e.g., publicly available pre-trained tokenizers (Kudo, 2006; Yang et al., 2017) and vocabulary acquired using unsupervised word segmentation (Goldwater et al., 2006; Mochihashi et al., 2009; Sennrich et al., 2016). In this study, we use SentencePiece (Kudo and Richardson, 2018) for initialization. 2.3 Module for Selecting Tokenization OpTok generates multiple tokenized sentences as candidates and converts them into a single vector using their probabilities during the training phase. First, we obtain the N -best tokenization of the sentence s01 , ..., sn , ..., s0N . We obtain the N -best tokenization using the Forward-DP Backward-A* algorithm (Nagata, 1994) for the probabilities produced using the language model mentioned in Section 2.2. Second, we convert the tokenized sequences into the vectors hs0n severally as follows: hs0n = g(s0n ), (4) where g(·) is a neural"
2020.findings-emnlp.120,2020.acl-main.611,0,0.0139243,"ling tokenization. This result indicates that OpTok contributes to an improvement in the popular NLP architecture in terms of optimizing the tokenization. 7 https://github.com/huggingface/ transformers Related Work Numerous studies have aimed to improve the NLP tasks from the perspective of tokenization. Some studies have proposed an approach to prevent segmentation errors by encoding multiple tokenizations jointly. Recent studies investigated Lattice LSTM, which expands LSTM to allow multiple segmentations to be taken as a lattice (Chen et al., 2017; Zhang and Yang, 2018; Yang et al., 2018). Li et al. (2020) followed them to utilize a transformer. Subword regularization is a famous solution to this problem (Kudo, 2018; Kudo and Richardson, 2018). The authors demonstrated that training models with various tokenizations contribute to an improved performance for machine translations. Provilkov et al. (2019) followed this approach by dropping tokens during the BPE process to vary tokenization, and Hiraoka et al. (2019) by updating the language model during training. Optimization of the tokenization has attracted attention mainly in the field of machine translation. Some studies have attempted to opti"
2020.findings-emnlp.120,N09-1069,0,0.3024,"pling a plausible tokenization using Gumbel softmax (Jang et al., 2016), but found that it causes instability in the learning. 1343 Positive Weibo(Zh) 407,057 Twitter(Ja) 10,319 Twitter(En) 56,462 Negative 263,995 16,035 43,538 Neutral 135,830 - Total 671,052 162,184 100,000 Table 1: Dataset components on sentiment analysis. of words) obtained from the training corpus. Meanwhile, we have to keep the corpus-based language model in some cases. To address such cases, we can use the following loss for the sentence s to update the language model using neural EM algorithm (Deligne and Bimbot, 1995; Liang and Klein, 2009; Tran et al., 2016): Llm s =− N X n=1 an X logp∗ (w). (7) w∈s0n We then optimize the weighted sum of the downstream task loss and Llm s . Consider text classification as an example. We use cross-entropy loss for the ground-truth label of the sentence Lcl s . Thus, we optimize the following equation: lm Ls = Lcl s + µLs , (8) where µ is the hyperparameter. Note that we set µ = 0 to confirm the effect of the proposed method in this study. 3 Experiments The goal of this study is to improve the performance of downstream tasks by optimizing the tokenization. Therefore, we evaluate OpTok on various"
2020.findings-emnlp.120,P09-1012,0,0.409652,"probability (Kudo, 2018) with a hyperparameter α. Concretely, we obtain the smoothed α probability as p∗ (w) = P p(w)p(w) ˆ α . We convert a w∈V ˆ sentence into a sequence of tokens depending on a probability of a tokenized sentence: Y p∗ (w). (3) p(s0 ) = w∈s0 We initialize vocabulary V with a reasonable number of tokens. To choose the initial vocabulary, both supervised and unsupervised word segmentation methods are available, e.g., publicly available pre-trained tokenizers (Kudo, 2006; Yang et al., 2017) and vocabulary acquired using unsupervised word segmentation (Goldwater et al., 2006; Mochihashi et al., 2009; Sennrich et al., 2016). In this study, we use SentencePiece (Kudo and Richardson, 2018) for initialization. 2.3 Module for Selecting Tokenization OpTok generates multiple tokenized sentences as candidates and converts them into a single vector using their probabilities during the training phase. First, we obtain the N -best tokenization of the sentence s01 , ..., sn , ..., s0N . We obtain the N -best tokenization using the Forward-DP Backward-A* algorithm (Nagata, 1994) for the probabilities produced using the language model mentioned in Section 2.2. Second, we convert the tokenized sequence"
2020.findings-emnlp.120,C94-1032,0,0.847634,"o, 2006; Yang et al., 2017) and vocabulary acquired using unsupervised word segmentation (Goldwater et al., 2006; Mochihashi et al., 2009; Sennrich et al., 2016). In this study, we use SentencePiece (Kudo and Richardson, 2018) for initialization. 2.3 Module for Selecting Tokenization OpTok generates multiple tokenized sentences as candidates and converts them into a single vector using their probabilities during the training phase. First, we obtain the N -best tokenization of the sentence s01 , ..., sn , ..., s0N . We obtain the N -best tokenization using the Forward-DP Backward-A* algorithm (Nagata, 1994) for the probabilities produced using the language model mentioned in Section 2.2. Second, we convert the tokenized sequences into the vectors hs0n severally as follows: hs0n = g(s0n ), (4) where g(·) is a neural encoder, which encodes the sequence of tokens, such as those using a CNN and BiLSTM. We found that the learning is stabilized by sharing word embeddings between the encoder and the neural unigram language model. Finally, we calculate the final vector of the sentence by weighting the vectors of the candidates using their probabilities calculated through Eq. (3) as follows: p(s0 ) an ="
2020.findings-emnlp.120,C10-1092,0,0.6916,"; Kudo and Richardson, 2018). The authors demonstrated that training models with various tokenizations contribute to an improved performance for machine translations. Provilkov et al. (2019) followed this approach by dropping tokens during the BPE process to vary tokenization, and Hiraoka et al. (2019) by updating the language model during training. Optimization of the tokenization has attracted attention mainly in the field of machine translation. Some studies have attempted to optimize the tokenization using simple criteria for a machine translation (Xu et al., 2008; Chung and Gildea, 2009; Nguyen et al., 2010; Mermer et al., 2013). Recent studies also tackled this issue for generation tasks. Salesky et al. (2020) developed Incremental BPE, which automatically defines the number of BPE’s merge operation for neural machine translation. He et al. (2020) proposed a neural architecture to find a better subword sequence from both the source and target corpora by enhancing the study in Chan et al. (2016). Our work differs from the existing research in that the proposed method is appliable to various neural encoders, and we can optimize the tokenization directly using only backpropagation from the trainin"
2020.findings-emnlp.120,C08-1128,0,0.621112,"mous solution to this problem (Kudo, 2018; Kudo and Richardson, 2018). The authors demonstrated that training models with various tokenizations contribute to an improved performance for machine translations. Provilkov et al. (2019) followed this approach by dropping tokens during the BPE process to vary tokenization, and Hiraoka et al. (2019) by updating the language model during training. Optimization of the tokenization has attracted attention mainly in the field of machine translation. Some studies have attempted to optimize the tokenization using simple criteria for a machine translation (Xu et al., 2008; Chung and Gildea, 2009; Nguyen et al., 2010; Mermer et al., 2013). Recent studies also tackled this issue for generation tasks. Salesky et al. (2020) developed Incremental BPE, which automatically defines the number of BPE’s merge operation for neural machine translation. He et al. (2020) proposed a neural architecture to find a better subword sequence from both the source and target corpora by enhancing the study in Chan et al. (2016). Our work differs from the existing research in that the proposed method is appliable to various neural encoders, and we can optimize the tokenization directl"
2020.findings-emnlp.120,P17-1078,0,0.110889,"w. To stabilize the learning, as explained in Section 2.5, we employ the smoothed distribution of unigram probability (Kudo, 2018) with a hyperparameter α. Concretely, we obtain the smoothed α probability as p∗ (w) = P p(w)p(w) ˆ α . We convert a w∈V ˆ sentence into a sequence of tokens depending on a probability of a tokenized sentence: Y p∗ (w). (3) p(s0 ) = w∈s0 We initialize vocabulary V with a reasonable number of tokens. To choose the initial vocabulary, both supervised and unsupervised word segmentation methods are available, e.g., publicly available pre-trained tokenizers (Kudo, 2006; Yang et al., 2017) and vocabulary acquired using unsupervised word segmentation (Goldwater et al., 2006; Mochihashi et al., 2009; Sennrich et al., 2016). In this study, we use SentencePiece (Kudo and Richardson, 2018) for initialization. 2.3 Module for Selecting Tokenization OpTok generates multiple tokenized sentences as candidates and converts them into a single vector using their probabilities during the training phase. First, we obtain the N -best tokenization of the sentence s01 , ..., sn , ..., s0N . We obtain the N -best tokenization using the Forward-DP Backward-A* algorithm (Nagata, 1994) for the proba"
2020.findings-emnlp.120,P18-1144,0,0.104411,"wever, some studies have varied the tokenization using a sampling during the training to enable the downstream model to adapt to various tokenizations (Kudo, 2018; Hiraoka et al., 2019; Provilkov et al., 2019). Although such a strategy makes the downstream model robust, little attention has been paid to optimizing the tokenizers for a downstream task. Thus, if we acquire an appropriate tokenization to a downstream task, we might improve the task performance. By contrast, some studies have used multiple tokenized sentences to prevent the damage depending on the tokenization (Chen et al., 2017; Zhang and Yang, 2018; Yang et al., 2018). Their methods compute various tokenizations for a given sentence, and then encode the tokenizations using an architecture based on the LSTM (Hochreiter and Schmidhuber, 1997). Although their methods prevent the error propagation from the tokenizer, they are intractable when handling all possible tokenizations owing to the computational costs required. This paper describes an exploration into an appropriate tokenization to the downstream tasks. We propose a novel method to optimize a tokenizer based on the downstream task, as shown in Figure 1341 Findings of the Associatio"
2020.findings-emnlp.120,D15-1064,0,0.0308892,"w of (a) conventional tokenization and (b) optimizing tokenization proposed herein. We directly optimize the tokenizer to improve the performance of the model for a downstream task using the loss of the target task. Introduction Tokenization is a fundamental problem in natural language processing (NLP). We must split a given sequence into a sequence of words for languages that do not contain obvious boundaries, such as Chinese and Japanese. In addition, it is also better to explore appropriate segmentations for languages containing obvious boundaries indicated by whitespaces, such as English (Peng and Dredze, 2015, 2016; Sennrich et al., 2016; He and Sun, 2017; A and Augenstein, 2020; Bollegala et al., 2020). In traditional NLP, we tokenize a given sentence as a preprocessing. Thus, as shown in Figure 1(a), we apply an existing tokenizer to the given sentence, and then input the tokenized sentence into a model for a target downstream task. In the conventional approach, we obtain the most plausible tokenized sentence based on the tokenizer; however, some studies have varied the tokenization using a sampling during the training to enable the downstream model to adapt to various tokenizations (Kudo, 2018;"
2020.findings-emnlp.120,P16-2025,0,0.0605705,"Missing"
2020.findings-emnlp.120,P16-1162,0,0.313295,"ation and (b) optimizing tokenization proposed herein. We directly optimize the tokenizer to improve the performance of the model for a downstream task using the loss of the target task. Introduction Tokenization is a fundamental problem in natural language processing (NLP). We must split a given sequence into a sequence of words for languages that do not contain obvious boundaries, such as Chinese and Japanese. In addition, it is also better to explore appropriate segmentations for languages containing obvious boundaries indicated by whitespaces, such as English (Peng and Dredze, 2015, 2016; Sennrich et al., 2016; He and Sun, 2017; A and Augenstein, 2020; Bollegala et al., 2020). In traditional NLP, we tokenize a given sentence as a preprocessing. Thus, as shown in Figure 1(a), we apply an existing tokenizer to the given sentence, and then input the tokenized sentence into a model for a target downstream task. In the conventional approach, we obtain the most plausible tokenized sentence based on the tokenizer; however, some studies have varied the tokenization using a sampling during the training to enable the downstream model to adapt to various tokenizations (Kudo, 2018; Hiraoka et al., 2019; Provil"
2020.findings-emnlp.120,W16-5907,0,0.026932,"ization using Gumbel softmax (Jang et al., 2016), but found that it causes instability in the learning. 1343 Positive Weibo(Zh) 407,057 Twitter(Ja) 10,319 Twitter(En) 56,462 Negative 263,995 16,035 43,538 Neutral 135,830 - Total 671,052 162,184 100,000 Table 1: Dataset components on sentiment analysis. of words) obtained from the training corpus. Meanwhile, we have to keep the corpus-based language model in some cases. To address such cases, we can use the following loss for the sentence s to update the language model using neural EM algorithm (Deligne and Bimbot, 1995; Liang and Klein, 2009; Tran et al., 2016): Llm s =− N X n=1 an X logp∗ (w). (7) w∈s0n We then optimize the weighted sum of the downstream task loss and Llm s . Consider text classification as an example. We use cross-entropy loss for the ground-truth label of the sentence Lcl s . Thus, we optimize the following equation: lm Ls = Lcl s + µLs , (8) where µ is the hyperparameter. Note that we set µ = 0 to confirm the effect of the proposed method in this study. 3 Experiments The goal of this study is to improve the performance of downstream tasks by optimizing the tokenization. Therefore, we evaluate OpTok on various text classification"
2020.lrec-1.429,P18-2078,0,0.0282073,"is challenging to do an inverse transform algorithmically. This requires comprehension of the context to select the most likely candidate, which would be analogous to a quasi-masked language modeling task. 3. Related Work and Background The fundamental idea of characters is not new; in the past, many character-level approaches have been proposed in the form of task-specific architectures. There are also subcharacter level methods analogous to our method, all of which we discuss in the language-specific sections below. 3.1. Non-Korean Languages A study on a limited subset of Brahmic languages (Ding et al., 2018) proposes a method which can be used to reduce the vocabulary budget needed for all languages by generalizing, simplifying, then aligning multiple language alphabets together. This is applicable when the writing systems have genealogical relations that allow this form of 3490 江 宮 강 Phonetic Phonetic 궁 Jamo Jamo ㄱ[choseong] ㅏ[joongseong] ㅇ[jongseong] ㄱ[choseong] ㅜ[joongseong] ㅇ[jongseong] We attempt to address this limitation in our work. This is analogous to how subword tokenization methods have brought to the field guarantees of lossless encoding and decoding, which was not possible with conv"
2020.lrec-1.429,N16-1155,0,0.0260191,"viance of the token length for Korean subwords in each model’s vocabulary. For our methods, we have scaled it down by 2.5 for a fair comparison. SP is SentencePiece, and what comes after the @ is the size of the vocabulary. +Al denotes aligned, and +Au denotes automaton (unaligned). 6. Limitations and Future Work Both methods suffer from one severe limitation; the reconstruction is only possible given that the model output is sane. This is analogous to the Unicode pair reconstruction problem which can be observed in the tokenizer used by GPT-2 (Radford et al., ) or other byte-level approaches(Gillick et al., 2016), as unless the output is as sane byte sequence which can be reconstructed into a Unicode character, the output is not only unusable, it also can break parsing the remainder of the sequence. While hypothetically a mitigation for these cases can be put in by encoding checksums for cases where the postprocessor cannot reconstruct into the subword tokens, which in turn can be used as hints to discard erroneous output from the model in a generative task, we have not explored this direction within the scope of this work and leave it as a point for future exploration. 7. Conclusion To summarize the"
2020.lrec-1.429,P18-1031,0,0.0272689,"ny unsupervised multilingual pre-training task, increasing the elasticity of budget required for building the vocabulary in Byte-Pair Encoding inspired tokenizers, significantly reducing the cost of supporting Korean in a multilingual model. Keywords: tokenization, vocabulary compaction, sub-character representations, out-of-vocabulary mitigation 1. Background With the introduction of large-scale language model pretraining in the domain of natural language processing, the domain has seen significant advances in the performance of downstream tasks using transfer learning on pre-trained models (Howard and Ruder, 2018; Devlin et al., 2018) when compared to conventional per-task models. As a part of this trend, it has also become common to perform this form of pre-training against multiple languages when training a single model. For these multilingual pre-training cases, stateof-the-art methods have relied on subword based tokenizers, such as Byte-Pair Encoding (BPE) (Sennrich et al., 2016) or SentencePiece (Kudo and Richardson, 2018) as a robust mechanism to mitigate the out-of-vocabulary (OOV) problem at a tokenizer level, by having a fallback to a character level vocabulary. Not only have these methods s"
2020.lrec-1.429,D18-2012,0,0.107147,"aining in the domain of natural language processing, the domain has seen significant advances in the performance of downstream tasks using transfer learning on pre-trained models (Howard and Ruder, 2018; Devlin et al., 2018) when compared to conventional per-task models. As a part of this trend, it has also become common to perform this form of pre-training against multiple languages when training a single model. For these multilingual pre-training cases, stateof-the-art methods have relied on subword based tokenizers, such as Byte-Pair Encoding (BPE) (Sennrich et al., 2016) or SentencePiece (Kudo and Richardson, 2018) as a robust mechanism to mitigate the out-of-vocabulary (OOV) problem at a tokenizer level, by having a fallback to a character level vocabulary. Not only have these methods shown to be robust against OOV compared to standard lexicon-based tokenization methods, but they also have benefited from a computational cost perspective as it reduces the size of the input and output layer. While these methods have shown significant improvements in alphabetic languages such as English and other Western European languages that use a Latin alphabet, these methods have limitations when applied to languages"
2020.lrec-1.429,P18-1226,0,0.0183949,"ample illustrated in figure 1 can be explained as ㄱ∈ Jh , ㅏ∈ Jv , and ㅇ∈ Jt . Note that Jt can be omitted, in which case it corresponds to &lt;nil> ∈ Jt . Level Jh , Choseong Jv , Jungseong Jt , Jongseong Jamo (Subcharacters) ㄱㄲㄴㄷㄸㄹㅁㅂㅃ ㅈㅉㅊㅋㅌㅍㅎ ㅏㅐㅑㅒㅓㅔㅕㅖㅗ ㅛㅜㅝㅞㅟㅠㅡㅢㅣ &lt;nil> ㄱ ㄲ ㄳ ㄴ ㄵ ㄶ ㄷ ㄼㄽㄾㄿㅀㅁㅂㅄㅅ ㅊㅋㅌㅍㅎ ㅅㅆㅇ ㅘㅙㅚ ㄹㄺㄻ ㅆㅇㅈ Table 1: Hangul Jamo sub-characters, along with their respective positional roles for composition. Exploiting these characteristics is not a new idea, and have been explored in the context of an end-to-end architecture in (Stratos, 2017), as a method for learning subword embeddings in (Park et al., 2018), and for classification in (Cho et al., 2019). One significant contribution of our method is the guarantees of round trip consistency. Previous work, which we discussed above, also discuss sub-character (Jamo) based methods, but the evaluation was limited to tasks that do not require generation, and reconstruction was not discussed. ㅎㅏㄴㄷㅏ Declarative Present Formal High 합니다 ㅎㅏㅂㄴㅣㄷㅏ Figure 2: In this example, the standard form 하다 (to do) can be conjugated into many forms. The first two Jamo correspond to a common morpheme, which through agglutination becomes different conjugations. 4. Method T"
2020.lrec-1.429,D16-1264,0,0.0849153,"Missing"
2020.lrec-1.429,P18-2124,0,0.0566829,"Missing"
2020.lrec-1.429,P16-1162,0,0.0121656,"ction of large-scale language model pretraining in the domain of natural language processing, the domain has seen significant advances in the performance of downstream tasks using transfer learning on pre-trained models (Howard and Ruder, 2018; Devlin et al., 2018) when compared to conventional per-task models. As a part of this trend, it has also become common to perform this form of pre-training against multiple languages when training a single model. For these multilingual pre-training cases, stateof-the-art methods have relied on subword based tokenizers, such as Byte-Pair Encoding (BPE) (Sennrich et al., 2016) or SentencePiece (Kudo and Richardson, 2018) as a robust mechanism to mitigate the out-of-vocabulary (OOV) problem at a tokenizer level, by having a fallback to a character level vocabulary. Not only have these methods shown to be robust against OOV compared to standard lexicon-based tokenization methods, but they also have benefited from a computational cost perspective as it reduces the size of the input and output layer. While these methods have shown significant improvements in alphabetic languages such as English and other Western European languages that use a Latin alphabet, these metho"
2020.lrec-1.429,P15-2098,0,0.0285485,"ng] We attempt to address this limitation in our work. This is analogous to how subword tokenization methods have brought to the field guarantees of lossless encoding and decoding, which was not possible with conventional lossy encoding methods such as lemmatization, stemming, and other normalization methods. Standard Form 하다 ㅎㅏㄷㅏ Declarative Present Formal Low 한다 Figure 1: Transformation process and Hangul Jamo subcharacter composition. In the real world, Hangul to Chinese almost always has a 1:n mapping. Declarative Future Informal Low 할거야 ㅎㅏㄹㄱㅓㅇㅑ alignment. Previous works (He et al., 2018; Shi et al., 2015; Sun et al., 2014; Yin et al., 2016) demonstrate the potential of sub-character based methods in the context Chinese and Japanese of CJK languages through radical based decomposition. 3.2. Korean Korean, as shown in figure 1 builds on a small phonetic alphabet, and uses a combination of the consonants and vowels called Jamo as a building block, and use a combination of those when composing each character. Following the notation in (Stratos, 2017), given this Jamo alphabet J , where the following table can explain |J |= 51, and each possible role when forming a complete character. The Jamo alp"
2020.lrec-1.429,D17-1075,0,0.0126084,"world, Hangul to Chinese almost always has a 1:n mapping. Declarative Future Informal Low 할거야 ㅎㅏㄹㄱㅓㅇㅑ alignment. Previous works (He et al., 2018; Shi et al., 2015; Sun et al., 2014; Yin et al., 2016) demonstrate the potential of sub-character based methods in the context Chinese and Japanese of CJK languages through radical based decomposition. 3.2. Korean Korean, as shown in figure 1 builds on a small phonetic alphabet, and uses a combination of the consonants and vowels called Jamo as a building block, and use a combination of those when composing each character. Following the notation in (Stratos, 2017), given this Jamo alphabet J , where the following table can explain |J |= 51, and each possible role when forming a complete character. The Jamo alphabet J is a union defined as J = Jh ∪ Jv ∪ Jt , where Jh is a Choseong (head consonant), Jh is Jungseong (vowel), and Jh is Jongseong (tail consonant). Therefore, the first example illustrated in figure 1 can be explained as ㄱ∈ Jh , ㅏ∈ Jv , and ㅇ∈ Jt . Note that Jt can be omitted, in which case it corresponds to &lt;nil> ∈ Jt . Level Jh , Choseong Jv , Jungseong Jt , Jongseong Jamo (Subcharacters) ㄱㄲㄴㄷㄸㄹㅁㅂㅃ ㅈㅉㅊㅋㅌㅍㅎ ㅏㅐㅑㅒㅓㅔㅕㅖㅗ ㅛㅜㅝㅞㅟㅠㅡㅢㅣ &lt;nil> ㄱ ㄲ ㄳ ㄴ"
2020.lrec-1.429,D16-1100,0,0.0457455,"Missing"
2020.lrec-1.447,N18-1118,0,0.353724,"ce that contains the antecedent from the current sentence as the previous sentence. Figure 1: Example of the zero pronoun problem. These sentences represent a conversation between a reporter and a paramedic. tence. The current sentence contains a word for which we need the context sentence to translate it correctly, such as ambiguous anaphoric pronouns la and le in French. Thus, a translation model needs to pay attention to the context sentence to output the correct translation. They evaluated whether each model selects the correct translation based on a score computed by the model. Following Bawden et al. (2018), we construct a test set that consists of four components: a current sentence, context, a correct translation, and an incorrect translation. In our test set, since the current sentence contains zero pronoun, translation models need to detect the correct antecedent from the context sentences. 2.2. Test Set Construction To avoid contaminating the test set with noisy instances, we construct our test set with fully hand-crafted. Thus, the construction of our test set consisted of three steps: translation, zero pronoun detection, and incorrect instance construction. To easily detect zero pronouns"
2020.lrec-1.447,E12-3001,0,0.064363,"Missing"
2020.lrec-1.447,P14-2091,0,0.0356914,"Missing"
2020.lrec-1.447,P18-1044,0,0.196074,"Missing"
2020.lrec-1.447,W10-1737,0,0.0896794,"Missing"
2020.lrec-1.447,L16-1147,0,0.0180745,"only one sentence as the context, they prepared only one additional encoder. In contrast, we prepared more encoders because we used at most three sentences as contexts. For this reason, we applied 3-TO-1 and 4-TO-1 in addition to 2-TO1 in Bawden et al. (2018). We trained each method with the same hyper-parameters as Bawden et al. (2018). 3.1. Training Data We have two requirements for the training corpus: the training corpus consists of conversation such as our constructed test set and contains context sentences for each JapaneseEnglish sentence pair. In this paper, we used OpenSubtitles2016 (Lison and Tiedemann, 2016)5 as a corpus that satisfies these requirements. We extracted Japanese-English sentence pairs from OpenSubtitles2016 with applying the 2 We also applied Skip-gram but CBOW achieved better performance in our experiments. 3 https://github.com/OpenNMT/OpenNMT-py 4 They used GRU but we applied LSTM for comparison with the SINGLE - ENCODER. 5 https://www.opensubtitles.org/ja 3632 Model M AJORITY C O - OCCUR ( X , TARGET ) C O - OCCUR ( X , S OURCE +TARGET ) S INGLE 1-TO-1 S INGLE 2-TO-1 S INGLE 3-TO-1 S INGLE 4-TO-1 M ULTI 2-TO-1 M ULTI 3-TO-1 M ULTI 4-TO-1 H UMAN W / O CONTEXT H UMAN W / CONTEXT d"
2020.lrec-1.447,D15-1166,0,0.0560902,"eted as factorized results of a co-occurrence matrix (Levy and Goldberg, 2014). We define the cooccurrence score as follows: C O - OCCUR ( X , W) = X vx · vw , (1) w∈W x where W is a set of words in the sentence, x is a target pronoun in the correct (or incorrect) sentence, and vw is an embedding of a word w. We used two configurations for W : one was from the target side only (TARGET), i.e., W contains words in the correct (or incorrect) sentence and the other was from both of the target and the source (S OURCE +TARGET). S INGLE - ENCODER We applied a widely-used LSTM encoder-decoder model (Luong et al., 2015) as the basic NMT model. We used the implementation of OpenNMTpy3 with the same hyper-parameters as Bawden et al. (2018). In addition to translations from only the current sentence (1-TO-1), we evaluated the performance of the model with contexts. We concatenated context sentences with the current sentence to encode multiple sentences in this method. Figure 3: Example of our test set. These sentences represent a conversation between a reporter and a paramedic. The context sentences are spoken by the reporter and the current sentence is spoken by the paramedic. they answered “undecidable”. More"
2020.lrec-1.447,P14-5010,0,0.00472763,"Missing"
2020.lrec-1.447,C18-1009,0,0.0443329,"Missing"
2020.lrec-1.447,P02-1040,0,0.110051,"Missing"
2020.lrec-1.447,P16-1162,0,0.0997833,"Missing"
2020.lrec-1.447,P18-1054,0,0.17657,"Missing"
2020.lrec-1.447,W12-4213,0,0.0706452,"Missing"
2020.lrec-1.447,W17-4811,0,0.0668043,"Missing"
2020.lrec-1.447,P18-1117,0,0.0319429,"Missing"
2020.lrec-1.447,P19-1116,0,0.103041,"Missing"
2020.semeval-1.221,C18-1139,0,0.129043,"ies from ground truth (x) (x) set forms Sm , while Sˆm consists of words with top m predicted probabilities from the prediction set. We used this metric as the only evaluation criterion in our experiments. 3.2 Embedding Layer Selection This experiment is to select the best performing pre-trained conventional word embeddings or pre-trained language models as the embedding layer in the first stage. For pre-trained conventional word embeddings, we selected GloVe (Pennington et al., 2014) pre-trained on Common Crawl (840B tokens). For the pre-trained language models, we selected Flair embeddings (Akbik et al., 2018) pre-trained with 1-billion word corpus (Chelba et al., 2013) and RoBERTa (large model) (Liu et al., 2019) finetuned on the Multi-Genre Natural Language Inference (MultiNLI) corpus (Williams et al., 2018) (RoBERTa-large-mnli). Since each layer in RoBERTa preserves different linguistic information (Tenney et al., 2019), we experimented with concatenating the features extracted from 17th-24th layer and concatenating 21st-24th layer to get richer contextual representation. We trained the base ranking models with each candidate as embedding layer and compared their performances based on match4 sco"
2020.semeval-1.221,N19-4010,0,0.0240971,"Missing"
2020.semeval-1.221,D14-1162,0,0.0933004,")) matchm := x∈Dtest s, m ∈ {1...4} |Dtest | 1693 In the test set Dtest , for each sentence x, the words with top m emphasis probabilities from ground truth (x) (x) set forms Sm , while Sˆm consists of words with top m predicted probabilities from the prediction set. We used this metric as the only evaluation criterion in our experiments. 3.2 Embedding Layer Selection This experiment is to select the best performing pre-trained conventional word embeddings or pre-trained language models as the embedding layer in the first stage. For pre-trained conventional word embeddings, we selected GloVe (Pennington et al., 2014) pre-trained on Common Crawl (840B tokens). For the pre-trained language models, we selected Flair embeddings (Akbik et al., 2018) pre-trained with 1-billion word corpus (Chelba et al., 2013) and RoBERTa (large model) (Liu et al., 2019) finetuned on the Multi-Genre Natural Language Inference (MultiNLI) corpus (Williams et al., 2018) (RoBERTa-large-mnli). Since each layer in RoBERTa preserves different linguistic information (Tenney et al., 2019), we experimented with concatenating the features extracted from 17th-24th layer and concatenating 21st-24th layer to get richer contextual representat"
2020.semeval-1.221,P19-1112,0,0.139289,"Missing"
2020.semeval-1.221,2020.semeval-1.184,0,0.0731643,"Missing"
2020.semeval-1.221,P19-1452,0,0.0265149,"eddings or pre-trained language models as the embedding layer in the first stage. For pre-trained conventional word embeddings, we selected GloVe (Pennington et al., 2014) pre-trained on Common Crawl (840B tokens). For the pre-trained language models, we selected Flair embeddings (Akbik et al., 2018) pre-trained with 1-billion word corpus (Chelba et al., 2013) and RoBERTa (large model) (Liu et al., 2019) finetuned on the Multi-Genre Natural Language Inference (MultiNLI) corpus (Williams et al., 2018) (RoBERTa-large-mnli). Since each layer in RoBERTa preserves different linguistic information (Tenney et al., 2019), we experimented with concatenating the features extracted from 17th-24th layer and concatenating 21st-24th layer to get richer contextual representation. We trained the base ranking models with each candidate as embedding layer and compared their performances based on match4 scores, since we will select at least 4 words for the second stage. From Table 1, the base ranking model using GloVe pre-trained on Common Crawl performs the worst across matchm and matchaverage scores. For the base ranking model using RoBERTa-large-mnli, both concatenated 17th-24th and concatenated 21st-24th layer outpe"
2020.semeval-1.221,N18-1101,0,0.0222835,"nts. 3.2 Embedding Layer Selection This experiment is to select the best performing pre-trained conventional word embeddings or pre-trained language models as the embedding layer in the first stage. For pre-trained conventional word embeddings, we selected GloVe (Pennington et al., 2014) pre-trained on Common Crawl (840B tokens). For the pre-trained language models, we selected Flair embeddings (Akbik et al., 2018) pre-trained with 1-billion word corpus (Chelba et al., 2013) and RoBERTa (large model) (Liu et al., 2019) finetuned on the Multi-Genre Natural Language Inference (MultiNLI) corpus (Williams et al., 2018) (RoBERTa-large-mnli). Since each layer in RoBERTa preserves different linguistic information (Tenney et al., 2019), we experimented with concatenating the features extracted from 17th-24th layer and concatenating 21st-24th layer to get richer contextual representation. We trained the base ranking models with each candidate as embedding layer and compared their performances based on match4 scores, since we will select at least 4 words for the second stage. From Table 1, the base ranking model using GloVe pre-trained on Common Crawl performs the worst across matchm and matchaverage scores. For"
2020.semeval-1.51,D19-1109,0,0.0245645,"answer the multiple-choice question in subtask A, it requires knowledge about the world, including time, positional, and size reasoning, among others. Concretely, in order for the system to justify why He put an elephant in the fridge is against commonsense, it would need to have some understanding of the respective sizes of an elephant and a standard refrigerator and deduce that the statement is incorrect. Moreover, it would need to make that connection between the reasoning and the incorrect sentence. The extent of such knowledge that BERT contains remains unclear in the research community (Davison et al., 2019). Figure 4: BERT model modified for Commonsense Explanation Task 3.3 Subtask C: Explanation Text Generation For subtask C, we used the pre-trained GPT-2 model and fine-tuned it on the correct answers for subtask B. We limited the text generated to 20 words and process the resulting generated text by removing any extra words. 425 Figure 5: Input to the model consisting of a concatenation of the sentence and each explanation. The model chooses the explanation with the highest internal probability score extracted from the softmax layer. 4 4.1 Experiments Baselines As a baseline, we fine-tuned BER"
2020.semeval-1.51,2020.semeval-1.39,0,0.486246,"orld that can follow the given sentence even when it is not strictly entailed. Making such inference necessitates a rich understanding of everyday physical situations (Zellers et al., 2018). Figure 1: SWAG dataset example. The startphrase offers one sentence as context, and a beginning part of the second sentence. The task is to predict its appropriate ending. In this paper, we explore the ability of SWAG to infer commonsense explanation to tackle Semeval 2020 Task 4: Commonsesense Validation and Explanation (ComVE). To this end, we followed the directions given by the organizers of the task (Wang et al., 2020) and attempted a contribution to their two main tasks. The first main task is concerned with the commonsense validation of a sentence, which is also called Senmaking. It consists of providing two sentences that have similar wordings, one being against commonsense This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/. 422 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 422–429 Barcelona, Spain (Online), December 12, 2020. and the other not. The sentences given do not require"
2020.semeval-1.51,D18-1009,0,0.0537177,"uction Commonsense reasoning has long been a challenge in the field of Natural Language Processing (NLP). Recently, the emergence of large pre-trained language models achieving state-of-the-art results in several NLP tasks, such as BERT and GPT (Devlin et al., 2018; Radford et al., 2019) has also influenced commonsense tasks to regain popularity. Natural Language Inference (NLI) is one of the most popular tasks in commonsense reasoning research. It consists of determining if a hypothesis is correct, given a premise. Situations With Adversarial Generation (SWAG) task is closely related to NLI (Zellers et al., 2018). It consists of determining the most appropriate ending given a start phase as a context. The start phrases and endings in SWAG dataset are video captions. We show an example in Figure 1. The task is to find the ending that describes a possible (future) world that can follow the given sentence even when it is not strictly entailed. Making such inference necessitates a rich understanding of everyday physical situations (Zellers et al., 2018). Figure 1: SWAG dataset example. The startphrase offers one sentence as context, and a beginning part of the second sentence. The task is to predict its a"
2021.emnlp-main.335,D17-1098,0,0.0515645,"Missing"
2021.emnlp-main.335,N16-1012,0,0.0238994,"roWe also show that our generation strategy perduced the Generative Adversarial Network (GAN) forms better than previous strategies. to the model of Mou et al. (2015) to resolve the exposure bias problem (Bengio et al., 2015) caused 1 Introduction by generating sequences individually, and used the Following the initial work of Rush et al. (2015), attention mechanism (Bahdanau et al., 2015) to abstractive headline generation using the encoder- improve the consistency between both sequences. However, their model does not support the seq2seq decoder model has been studied extensively framework. (Chopra et al., 2016; Nallapati et al., 2016; Paulus Recently, He et al. (2020) used a Transformeret al., 2018). In the automatic headline generation based model (Vaswani et al., 2017), which is refor advertising articles, there are requests to include ported to achieve high performance, to generate a a given phrase such as a company or product name headline containing a given phrase. They proposed in the headline. providing an encoder with additional information Generating a headline that includes a given phrase has been considered one of the lexically con- related to the given phrase. However, their method stra"
2021.emnlp-main.335,W19-8641,1,0.842409,"specify the areas where input tokens disallow the attention in the backward and forward directions, depending on each generation strategy (see Figure 2). 3 Experiment We conducted the experiment to verify the performance of our methods in the headline generation task. The objective of our experiment is to compare our method with previous Transformer-based methods that generate tokens from left to right. We also compare Seq-B/F, the generation orders proposed by Mou et al. (2016), with Tok-B/F, our new generation orders. 3.1 Setting We used the 2019 version of the Japanese News Corpus (JNC)1 (Hitomi et al., 2019) as the dataset. The JNC contains 1,932,399 article-headline pairs, and we split them randomly at a ratio of 98:1:1 for use as training, validation, and test sets, respectively.2 We utilized MeCab (Kudo et al., 2004) with the IPAdic3 and then applied the Byte Pair Encoding (BPE) algorithm4 (Gage, 1994) for tokenization. We trained BPE with 10,000 merge operations and obtained the most frequent 32,000 1 https://cl.asahi.com/api_data/ jnc-jamul-en.html 2 We applied the preprocessing script https://github.com/asahi-research/ script-for-transformer-based-seq2bf the original JNC to obtain the split"
2021.emnlp-main.335,P17-1141,0,0.0210451,"Okazaki Kentaro Inui Koichi Takeda1 1 2 3 Nagoya University Insight Edge, Inc. The Asahi Shimbun Company 4 5 6 Tokyo Institute of Technology Tohoku University RIKEN AIP yamada.kosuke@c.mbox.nagoya-u.ac.jp, yuta.hitomi@insightedge.jp, tamori-h@asahi.com, {sasano,takedasu}@i.nagoya-u.ac.jp, okazaki@c.titech.ac.jp, inui@ecei.tohoku.ac.jp Abstract there are two major approaches. One approach is to select a plausible sentence including the given This paper explores a variant of automatic phrase from several candidate sentences generated headline generation methods, where a generfrom left to right (Hokamp and Liu, 2017; Anderated headline is required to include a given son et al., 2017; Post and Vilar, 2018). Although phrase such as a company or a product name. these methods can include multiple phrases in a Previous methods using Transformer-based generated sentence, they are computationally exmodels generate a headline including a given pensive due to the large search space of the decodphrase by providing the encoder with additional information corresponding to the given ing process. In addition, since they try to force phrase. However, these methods cannot algiven phrases into sentences at every step of"
2021.emnlp-main.335,W04-3230,0,0.0630665,"Missing"
2021.emnlp-main.335,2020.acl-main.703,0,0.0296085,"2.32 tokens. We evaluated our methods using precision, recall, and F-score of ROUGE-1/2/L (Lin, 2004) and success rate (SR), which is the percentage of the headline that includes the given phrase. We also calculated the Average Length Difference (ALD) to analyze the length of the generated headlines, as n 1X ALD = li − leni , n (2) i=1 where n, li , and leni are the number of samples, the length of the generated headline, and the length of the reference headline, respectively. As a comparison method, we adopted the method proposed by He et al. (2020) with vanilla Transformer instead of BART (Lewis et al., 2020). This method controls the output by inserting the given phrase and the special token ‘|’ in front of the input articles and randomly drops the given phrase from the input articles during training to improve the performance. The hyperparameters of both the comparison and our models are determined as described in Vaswani et al. (2017). The training was terminated when the perplexity computed on the validation set did not update three times in a row, and we used the model with the minimum perplexity on the validation set. The beam size during the inference was set to three. 5 6 4087 https://gith"
2021.emnlp-main.335,D15-1044,0,0.0453908,"ou et al. (2015), generating the backward seguaranteed to include the phrase in the generquence from the phrase and then generating the ated headline, achieve ROUGE scores comparable to previous Transformer-based methods. remaining forward sequence. Liu et al. (2019) introWe also show that our generation strategy perduced the Generative Adversarial Network (GAN) forms better than previous strategies. to the model of Mou et al. (2015) to resolve the exposure bias problem (Bengio et al., 2015) caused 1 Introduction by generating sequences individually, and used the Following the initial work of Rush et al. (2015), attention mechanism (Bahdanau et al., 2015) to abstractive headline generation using the encoder- improve the consistency between both sequences. However, their model does not support the seq2seq decoder model has been studied extensively framework. (Chopra et al., 2016; Nallapati et al., 2016; Paulus Recently, He et al. (2020) used a Transformeret al., 2018). In the automatic headline generation based model (Vaswani et al., 2017), which is refor advertising articles, there are requests to include ported to achieve high performance, to generate a a given phrase such as a company or product n"
2021.emnlp-main.335,W04-1013,0,0.0196194,"e preprocessing script https://github.com/asahi-research/ script-for-transformer-based-seq2bf the original JNC to obtain the split dataset. 3 https://taku910.github.io/mecab/ 4 https://github.com/rsennrich/ subword-nmt at to tokens from the articles and the headlines, respectively. We used context word sequences extracted from the reference headlines by GiNZA5 as the ‘given’ phrase.6 An average of 4.99 phrases was extracted from the reference headlines, and the ‘given’ phrases consisted of an average of 2.32 tokens. We evaluated our methods using precision, recall, and F-score of ROUGE-1/2/L (Lin, 2004) and success rate (SR), which is the percentage of the headline that includes the given phrase. We also calculated the Average Length Difference (ALD) to analyze the length of the generated headlines, as n 1X ALD = li − leni , n (2) i=1 where n, li , and leni are the number of samples, the length of the generated headline, and the length of the reference headline, respectively. As a comparison method, we adopted the method proposed by He et al. (2020) with vanilla Transformer instead of BART (Lewis et al., 2020). This method controls the output by inserting the given phrase and the special tok"
2021.emnlp-main.335,C16-1316,0,0.138499,"ir method strained sentence generation tasks. For these tasks, may not always include the given phrases in the generated headline. ∗ Work done during an internship at The Asahi Shimbun In this study, we work on generating lexically Company † Work done at The Asahi Shimbun Company constrained headlines using Transformer-based 4085 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4085–4090 c November 7–11, 2021. 2021 Association for Computational Linguistics 2 Proposed Method We propose a Transformer-based Seq2BF model that applies Seq2BF proposed by Mou et al. (2016) to the Transformer model to generate headlines including a given phrase. The Seq2BF takes W (= w1 , ..., wL ; w1:L ) as the given phrase consisting of L tokens and generates the headline y−M :−1 of M tokens backward from W , and the headline y1:N of N tokens forward from W . The Transformerbased Seq2BF is the Transformer model with two generation components, consisting of a linear and a softmax layer (see Figure 1). In Transformer-based Seq2BF unlike Transformer generating tokens from left to right, the token position changes relatively depending on already generated tokens. We determine the"
2021.emnlp-main.335,K16-1028,0,0.0193208,"ur generation strategy perduced the Generative Adversarial Network (GAN) forms better than previous strategies. to the model of Mou et al. (2015) to resolve the exposure bias problem (Bengio et al., 2015) caused 1 Introduction by generating sequences individually, and used the Following the initial work of Rush et al. (2015), attention mechanism (Bahdanau et al., 2015) to abstractive headline generation using the encoder- improve the consistency between both sequences. However, their model does not support the seq2seq decoder model has been studied extensively framework. (Chopra et al., 2016; Nallapati et al., 2016; Paulus Recently, He et al. (2020) used a Transformeret al., 2018). In the automatic headline generation based model (Vaswani et al., 2017), which is refor advertising articles, there are requests to include ported to achieve high performance, to generate a a given phrase such as a company or product name headline containing a given phrase. They proposed in the headline. providing an encoder with additional information Generating a headline that includes a given phrase has been considered one of the lexically con- related to the given phrase. However, their method strained sentence generation"
2021.emnlp-main.335,N18-1119,0,0.0158763,"himbun Company 4 5 6 Tokyo Institute of Technology Tohoku University RIKEN AIP yamada.kosuke@c.mbox.nagoya-u.ac.jp, yuta.hitomi@insightedge.jp, tamori-h@asahi.com, {sasano,takedasu}@i.nagoya-u.ac.jp, okazaki@c.titech.ac.jp, inui@ecei.tohoku.ac.jp Abstract there are two major approaches. One approach is to select a plausible sentence including the given This paper explores a variant of automatic phrase from several candidate sentences generated headline generation methods, where a generfrom left to right (Hokamp and Liu, 2017; Anderated headline is required to include a given son et al., 2017; Post and Vilar, 2018). Although phrase such as a company or a product name. these methods can include multiple phrases in a Previous methods using Transformer-based generated sentence, they are computationally exmodels generate a headline including a given pensive due to the large search space of the decodphrase by providing the encoder with additional information corresponding to the given ing process. In addition, since they try to force phrase. However, these methods cannot algiven phrases into sentences at every step of the ways include the phrase in the generated generation process, these methods may harm the"
2021.findings-acl.21,D15-1075,0,0.0360134,"(2020). et al., 2020) for text classification and DPE (He et al., 2020) for machine translation. 3.1 Text Classification Settings We utilized ten datasets of text classification tasks in three languages. Weibo(Zh)2 , Twitter(Ja)3 , and Twitter(En)4 are sentiment analyses on SNS corpora in Chinese, Japanese, and English, respectively. Genre and Rating are genre prediction and rating predictions from reviews posted on Ecommerce corpora, respectively, in Chinese (Zhang et al., 2015)5 , Japanese (Rakuten, Inc., 2014), and English (He and McAuley, 2016)6 . In addition, we employed the SNLI corpus (Bowman et al., 2015) to evaluate our method on the setting requiring two sentences as the input. We focus on SentencePiece (SP) (Kudo and Richardson, 2018) and OpTok (Hiraoka et al., 2020) as other tokenizers for comparison with the proposed method. OpTok is a method to optimize tokenization for text classification by weighting a sentence vector with N -best tokenization. In addition, we trained each model with subword regularization (SP+R) (Kudo, 2018) for fair comparisons. In subword regularization, we used a sampled tokenization for the training phase and a 1-best tokenization for the inference phase. For the"
2021.findings-acl.21,P17-2096,0,0.0277918,"ne translation in eight language pairs. Experimental results show that our proposed method improves the performance by determining appropriate tokenizations. 1 Introduction Tokenization, which converts a raw sentence into a sequence of tokens, is a crucial process that affects the performance of NLP tasks. Existing studies have proposed various tokenization methods including rule-based tokenization (Koehn et al., 2007), dictionary-based tokenization (Kudo, 2006; Morita et al., 2015; Tolmachev et al., 2018; Takaoka et al., 2018), supervised tokenization with neural networks (Yang et al., 2017; Cai et al., 2017; Yang et al., 2019), and unsupervised tokenization (Goldwater et al., 2006, 2009; Mochihashi et al., 2009; Sennrich et al., 2016; Kudo and Richardson, 2018). Much of prior research has reported that an appropriate tokenization depends on each downstream task (Xu et al., 2008; Chang et al., 2008; Nguyen et al., 2010; Domingo et al., 2018; Hiraoka et al., 2019; Gowda and May, 2020). Moreover, Hiraoka et al. (2020) implies that we have to consider a downstream model to determine an appropriate tokenization. In other words, we can improve the performance of a downstream model by determining an ap"
2021.findings-acl.21,W08-0336,0,0.19455,"Missing"
2021.findings-acl.21,D09-1075,0,0.048638,"onsider that optimization of the tokenization of both sides with a large N becomes unstable because tokenization on the source side varies vastly during training. 6 Related Work Many researchers have tackled the problem of optimizing tokenization, especially in the machine translation field. For statistical machine translation, Nießen and Ney (2004) and Goldwater and McClosky (2005) attempted to obtain good tokenization using hand-crafted linguistic information. Some studies explored appropriate tokenization using alignment information between the source and target languages (Xu et al., 2008; Chung and Gildea, 2009; Nguyen et al., 2010). Recent studies have attempted to obtain appropriate tokenization for the downstream task using neural networks. Gowda and May (2020) analysed the optimal granularity of tokenization on NMT. Salesky et al. (2020) proposed Incremental-BPE, which automatically explores the appropriate granularity of BPE tokenization. They stopped the merge operation of the BPE depending on the loss on a validation split. He et al. (2020) proposesd DPE, which obtains the appropriate tokenization of a target language depending on the tokenization of the source side on the NMT. Our method is"
2021.findings-acl.21,P06-1085,0,0.114352,"t our proposed method improves the performance by determining appropriate tokenizations. 1 Introduction Tokenization, which converts a raw sentence into a sequence of tokens, is a crucial process that affects the performance of NLP tasks. Existing studies have proposed various tokenization methods including rule-based tokenization (Koehn et al., 2007), dictionary-based tokenization (Kudo, 2006; Morita et al., 2015; Tolmachev et al., 2018; Takaoka et al., 2018), supervised tokenization with neural networks (Yang et al., 2017; Cai et al., 2017; Yang et al., 2019), and unsupervised tokenization (Goldwater et al., 2006, 2009; Mochihashi et al., 2009; Sennrich et al., 2016; Kudo and Richardson, 2018). Much of prior research has reported that an appropriate tokenization depends on each downstream task (Xu et al., 2008; Chang et al., 2008; Nguyen et al., 2010; Domingo et al., 2018; Hiraoka et al., 2019; Gowda and May, 2020). Moreover, Hiraoka et al. (2020) implies that we have to consider a downstream model to determine an appropriate tokenization. In other words, we can improve the performance of a downstream model by determining an appropriate tokenization for the downstream model. However, since traditional"
2021.findings-acl.21,H05-1085,0,0.0689222,"space when we set a large number as N because the neural encoder of NMT allows various tokenizations for its input. When we use our method for both the encoder and the decoder (OURS-OURS), the performance decreases slightly with higher N . We consider that optimization of the tokenization of both sides with a large N becomes unstable because tokenization on the source side varies vastly during training. 6 Related Work Many researchers have tackled the problem of optimizing tokenization, especially in the machine translation field. For statistical machine translation, Nießen and Ney (2004) and Goldwater and McClosky (2005) attempted to obtain good tokenization using hand-crafted linguistic information. Some studies explored appropriate tokenization using alignment information between the source and target languages (Xu et al., 2008; Chung and Gildea, 2009; Nguyen et al., 2010). Recent studies have attempted to obtain appropriate tokenization for the downstream task using neural networks. Gowda and May (2020) analysed the optimal granularity of tokenization on NMT. Salesky et al. (2020) proposed Incremental-BPE, which automatically explores the appropriate granularity of BPE tokenization. They stopped the merge"
2021.findings-acl.21,2020.acl-main.275,0,0.228594,"l to determine an appropriate tokenization. In other words, we can improve the performance of a downstream model by determining an appropriate tokenization for the downstream model. However, since traditional tokenizers are isolated from a downstream model, we need to train a given downstream model with each possible tokenization and evaluate its performance to determine the appropriate tokenization. Performing such an exploration whenever we construct a new downstream model is impractical. Several studies have addressed the optimization of a tokenizer based on a downstream task or/and model (He et al., 2020; Hiraoka et al., 2020), but existing methods are restricted to specific tasks. He et al. (2020) proposed DPE as a tokenization method for a sequence-to-sequence problem such as machine translation. Their method trains a tokenizer with a given training corpus, but it is isolated from a downstream model such as a neural encoder–decoder for machine translation. Hiraoka et al. (2020) proposed OpTok, which jointly trains a tokenizer and a downstream model. However, its architecture is specific to classification problems based on sentence representations, and thus, it cannot be applied for various"
2021.findings-acl.21,P19-1158,1,0.913073,"on methods including rule-based tokenization (Koehn et al., 2007), dictionary-based tokenization (Kudo, 2006; Morita et al., 2015; Tolmachev et al., 2018; Takaoka et al., 2018), supervised tokenization with neural networks (Yang et al., 2017; Cai et al., 2017; Yang et al., 2019), and unsupervised tokenization (Goldwater et al., 2006, 2009; Mochihashi et al., 2009; Sennrich et al., 2016; Kudo and Richardson, 2018). Much of prior research has reported that an appropriate tokenization depends on each downstream task (Xu et al., 2008; Chang et al., 2008; Nguyen et al., 2010; Domingo et al., 2018; Hiraoka et al., 2019; Gowda and May, 2020). Moreover, Hiraoka et al. (2020) implies that we have to consider a downstream model to determine an appropriate tokenization. In other words, we can improve the performance of a downstream model by determining an appropriate tokenization for the downstream model. However, since traditional tokenizers are isolated from a downstream model, we need to train a given downstream model with each possible tokenization and evaluate its performance to determine the appropriate tokenization. Performing such an exploration whenever we construct a new downstream model is impractical"
2021.findings-acl.21,2020.findings-emnlp.120,1,0.83297,"al., 2007), dictionary-based tokenization (Kudo, 2006; Morita et al., 2015; Tolmachev et al., 2018; Takaoka et al., 2018), supervised tokenization with neural networks (Yang et al., 2017; Cai et al., 2017; Yang et al., 2019), and unsupervised tokenization (Goldwater et al., 2006, 2009; Mochihashi et al., 2009; Sennrich et al., 2016; Kudo and Richardson, 2018). Much of prior research has reported that an appropriate tokenization depends on each downstream task (Xu et al., 2008; Chang et al., 2008; Nguyen et al., 2010; Domingo et al., 2018; Hiraoka et al., 2019; Gowda and May, 2020). Moreover, Hiraoka et al. (2020) implies that we have to consider a downstream model to determine an appropriate tokenization. In other words, we can improve the performance of a downstream model by determining an appropriate tokenization for the downstream model. However, since traditional tokenizers are isolated from a downstream model, we need to train a given downstream model with each possible tokenization and evaluate its performance to determine the appropriate tokenization. Performing such an exploration whenever we construct a new downstream model is impractical. Several studies have addressed the optimization of a"
2021.findings-acl.21,P07-2045,0,0.0270846,"cessing. Therefore, the proposed method is applicable to various situations. We evaluated whether our method contributes to improving performance on text classification in three languages and machine translation in eight language pairs. Experimental results show that our proposed method improves the performance by determining appropriate tokenizations. 1 Introduction Tokenization, which converts a raw sentence into a sequence of tokens, is a crucial process that affects the performance of NLP tasks. Existing studies have proposed various tokenization methods including rule-based tokenization (Koehn et al., 2007), dictionary-based tokenization (Kudo, 2006; Morita et al., 2015; Tolmachev et al., 2018; Takaoka et al., 2018), supervised tokenization with neural networks (Yang et al., 2017; Cai et al., 2017; Yang et al., 2019), and unsupervised tokenization (Goldwater et al., 2006, 2009; Mochihashi et al., 2009; Sennrich et al., 2016; Kudo and Richardson, 2018). Much of prior research has reported that an appropriate tokenization depends on each downstream task (Xu et al., 2008; Chang et al., 2008; Nguyen et al., 2010; Domingo et al., 2018; Hiraoka et al., 2019; Gowda and May, 2020). Moreover, Hiraoka et"
2021.findings-acl.21,P18-1007,0,0.260746,"refining tokenization. We call this refinement of tokenization post-processing. Thus, we can easily use the proposed method in various situations including the case where we have a sufficiently trained downstream model. We conducted experiments on text classification and machine translation tasks in various languages. Experimental results indicate that the proposed method outperformed existing tokenization methods in both the tasks. We also showed that our method can enhance the performance by refining tokenization as post-processing for downstream models trained with subword regularization (Kudo, 2018; Provilkov et al., 2020). 2 Optimizing Tokenization with Loss Proposed Method The proposed method comprises a tokenizer and a downstream model. We optimize the two modules simultaneously. First, we present the training outline of the case where we use one sentence as an input in Section 2.1. Second, we introduce the training of the tokenizer (Section 2.2) and the downstream model (Section 2.3). Finally, we explain the training strategy for a task that requires multiple inputs such as machine translation (Section 2.4). (1) where f (s0 ) is a downstream model that outputs a prediction of the do"
2021.findings-acl.21,D18-2012,0,0.111633,"nizations. 1 Introduction Tokenization, which converts a raw sentence into a sequence of tokens, is a crucial process that affects the performance of NLP tasks. Existing studies have proposed various tokenization methods including rule-based tokenization (Koehn et al., 2007), dictionary-based tokenization (Kudo, 2006; Morita et al., 2015; Tolmachev et al., 2018; Takaoka et al., 2018), supervised tokenization with neural networks (Yang et al., 2017; Cai et al., 2017; Yang et al., 2019), and unsupervised tokenization (Goldwater et al., 2006, 2009; Mochihashi et al., 2009; Sennrich et al., 2016; Kudo and Richardson, 2018). Much of prior research has reported that an appropriate tokenization depends on each downstream task (Xu et al., 2008; Chang et al., 2008; Nguyen et al., 2010; Domingo et al., 2018; Hiraoka et al., 2019; Gowda and May, 2020). Moreover, Hiraoka et al. (2020) implies that we have to consider a downstream model to determine an appropriate tokenization. In other words, we can improve the performance of a downstream model by determining an appropriate tokenization for the downstream model. However, since traditional tokenizers are isolated from a downstream model, we need to train a given downstr"
2021.findings-acl.21,P09-1012,0,0.0572597,"the performance by determining appropriate tokenizations. 1 Introduction Tokenization, which converts a raw sentence into a sequence of tokens, is a crucial process that affects the performance of NLP tasks. Existing studies have proposed various tokenization methods including rule-based tokenization (Koehn et al., 2007), dictionary-based tokenization (Kudo, 2006; Morita et al., 2015; Tolmachev et al., 2018; Takaoka et al., 2018), supervised tokenization with neural networks (Yang et al., 2017; Cai et al., 2017; Yang et al., 2019), and unsupervised tokenization (Goldwater et al., 2006, 2009; Mochihashi et al., 2009; Sennrich et al., 2016; Kudo and Richardson, 2018). Much of prior research has reported that an appropriate tokenization depends on each downstream task (Xu et al., 2008; Chang et al., 2008; Nguyen et al., 2010; Domingo et al., 2018; Hiraoka et al., 2019; Gowda and May, 2020). Moreover, Hiraoka et al. (2020) implies that we have to consider a downstream model to determine an appropriate tokenization. In other words, we can improve the performance of a downstream model by determining an appropriate tokenization for the downstream model. However, since traditional tokenizers are isolated from a"
2021.findings-acl.21,D15-1276,0,0.0259387,"situations. We evaluated whether our method contributes to improving performance on text classification in three languages and machine translation in eight language pairs. Experimental results show that our proposed method improves the performance by determining appropriate tokenizations. 1 Introduction Tokenization, which converts a raw sentence into a sequence of tokens, is a crucial process that affects the performance of NLP tasks. Existing studies have proposed various tokenization methods including rule-based tokenization (Koehn et al., 2007), dictionary-based tokenization (Kudo, 2006; Morita et al., 2015; Tolmachev et al., 2018; Takaoka et al., 2018), supervised tokenization with neural networks (Yang et al., 2017; Cai et al., 2017; Yang et al., 2019), and unsupervised tokenization (Goldwater et al., 2006, 2009; Mochihashi et al., 2009; Sennrich et al., 2016; Kudo and Richardson, 2018). Much of prior research has reported that an appropriate tokenization depends on each downstream task (Xu et al., 2008; Chang et al., 2008; Nguyen et al., 2010; Domingo et al., 2018; Hiraoka et al., 2019; Gowda and May, 2020). Moreover, Hiraoka et al. (2020) implies that we have to consider a downstream model t"
2021.findings-acl.21,C94-1032,0,0.776926,"Missing"
2021.findings-acl.21,C10-1092,0,0.139571,"ng studies have proposed various tokenization methods including rule-based tokenization (Koehn et al., 2007), dictionary-based tokenization (Kudo, 2006; Morita et al., 2015; Tolmachev et al., 2018; Takaoka et al., 2018), supervised tokenization with neural networks (Yang et al., 2017; Cai et al., 2017; Yang et al., 2019), and unsupervised tokenization (Goldwater et al., 2006, 2009; Mochihashi et al., 2009; Sennrich et al., 2016; Kudo and Richardson, 2018). Much of prior research has reported that an appropriate tokenization depends on each downstream task (Xu et al., 2008; Chang et al., 2008; Nguyen et al., 2010; Domingo et al., 2018; Hiraoka et al., 2019; Gowda and May, 2020). Moreover, Hiraoka et al. (2020) implies that we have to consider a downstream model to determine an appropriate tokenization. In other words, we can improve the performance of a downstream model by determining an appropriate tokenization for the downstream model. However, since traditional tokenizers are isolated from a downstream model, we need to train a given downstream model with each possible tokenization and evaluate its performance to determine the appropriate tokenization. Performing such an exploration whenever we con"
2021.findings-acl.21,J04-2003,0,0.148708,"ion from the large search space when we set a large number as N because the neural encoder of NMT allows various tokenizations for its input. When we use our method for both the encoder and the decoder (OURS-OURS), the performance decreases slightly with higher N . We consider that optimization of the tokenization of both sides with a large N becomes unstable because tokenization on the source side varies vastly during training. 6 Related Work Many researchers have tackled the problem of optimizing tokenization, especially in the machine translation field. For statistical machine translation, Nießen and Ney (2004) and Goldwater and McClosky (2005) attempted to obtain good tokenization using hand-crafted linguistic information. Some studies explored appropriate tokenization using alignment information between the source and target languages (Xu et al., 2008; Chung and Gildea, 2009; Nguyen et al., 2010). Recent studies have attempted to obtain appropriate tokenization for the downstream task using neural networks. Gowda and May (2020) analysed the optimal granularity of tokenization on NMT. Salesky et al. (2020) proposed Incremental-BPE, which automatically explores the appropriate granularity of BPE tok"
2021.findings-acl.21,N19-4009,0,0.020997,"15.59 30.04 18.18 38.68 40.08 15.74 12.37 31.98 27.52 31.83 24.53 Ours Ours 35.13 29.30 29.44 31.70 21.89 15.31 29.78 18.21 38.58 39.68 15.60 12.33 31.90 27.44 31.72 24.03 Table 2: Results of experiments on machine translation task using IWSLT and WMT corpus (BLEU). We show the tokenization method for Encoder and Decoder. SP and R mean SentencePiece and subword regularization, respectively. The highest scores are highlighted in bold. classification, we used subword regularization as a strong baseline. For the downstream model, we used Transformer (Vaswani et al., 2017) implemented in Fairseq (Ott et al., 2019). For the IWSLT dataset, we used the small Transformer, and we created the initial vocabulary using SentencePiece with a 16K size of the vocabulary for each language. For the WMT dataset, we employed Transformer (base), and the size of the vocabulary is 32K. Similar to the case of text classification tasks, we initialized our NULM with the result of SentencePiece. The hyperparameters for subword regularization are α = 0.2 for IWSLT, α = 0.5 for WMT, and k = ∞ for both datasets. The number of tokenizations for the training of the proposed method is N = 8 for ISWLT and N = 3 for WMT. In the trai"
2021.findings-acl.21,W18-6319,0,0.013119,". In contrast, since our method trains the downstream model with only one sampled tokenization, the downstream model receives one tokenization in both training and inference consistently. We consider that this consistency improves the performance. 3.2 Machine Translation Settings For experiments on the machine translation task, we employ IWSLT and WMT corpora on eight language pairs. We pre-tokenized all the datasets except for the Chinese corpus with Moses Tokenizer9 , and we used jieba10 for the Chinese corpus. We evaluate the performance of each method with detokenized BLEU with SacreBLEU (Post, 2018) after detokenization. As a recent tokenizer for machine translation, we compare the proposed method with DPE (He et al., 2020), which tokenizes a target sentence, considering the source tokenization, in addition to SentencePiece. We employed the official implementation of DPE11 and train the DPE model using SentencePiece tokenization. In the same as text 247 8 We limit the size of the vocabulary to the half size of the initial vocabulary as well as the training of OpTok. 9 https://github.com/moses-smt/ mosesdecoder 10 https://github.com/fxsjy/jieba 11 https://github.com/xlhex/dpe IWSLT14 IWSL"
2021.findings-acl.21,2020.acl-main.170,0,0.176213,"kenization. We call this refinement of tokenization post-processing. Thus, we can easily use the proposed method in various situations including the case where we have a sufficiently trained downstream model. We conducted experiments on text classification and machine translation tasks in various languages. Experimental results indicate that the proposed method outperformed existing tokenization methods in both the tasks. We also showed that our method can enhance the performance by refining tokenization as post-processing for downstream models trained with subword regularization (Kudo, 2018; Provilkov et al., 2020). 2 Optimizing Tokenization with Loss Proposed Method The proposed method comprises a tokenizer and a downstream model. We optimize the two modules simultaneously. First, we present the training outline of the case where we use one sentence as an input in Section 2.1. Second, we introduce the training of the tokenizer (Section 2.2) and the downstream model (Section 2.3). Finally, we explain the training strategy for a task that requires multiple inputs such as machine translation (Section 2.4). (1) where f (s0 ) is a downstream model that outputs a prediction of the downstream task from a toke"
2021.findings-acl.21,P16-1162,0,0.254471,"mining appropriate tokenizations. 1 Introduction Tokenization, which converts a raw sentence into a sequence of tokens, is a crucial process that affects the performance of NLP tasks. Existing studies have proposed various tokenization methods including rule-based tokenization (Koehn et al., 2007), dictionary-based tokenization (Kudo, 2006; Morita et al., 2015; Tolmachev et al., 2018; Takaoka et al., 2018), supervised tokenization with neural networks (Yang et al., 2017; Cai et al., 2017; Yang et al., 2019), and unsupervised tokenization (Goldwater et al., 2006, 2009; Mochihashi et al., 2009; Sennrich et al., 2016; Kudo and Richardson, 2018). Much of prior research has reported that an appropriate tokenization depends on each downstream task (Xu et al., 2008; Chang et al., 2008; Nguyen et al., 2010; Domingo et al., 2018; Hiraoka et al., 2019; Gowda and May, 2020). Moreover, Hiraoka et al. (2020) implies that we have to consider a downstream model to determine an appropriate tokenization. In other words, we can improve the performance of a downstream model by determining an appropriate tokenization for the downstream model. However, since traditional tokenizers are isolated from a downstream model, we n"
2021.findings-acl.21,L18-1355,0,0.0145325,"contributes to improving performance on text classification in three languages and machine translation in eight language pairs. Experimental results show that our proposed method improves the performance by determining appropriate tokenizations. 1 Introduction Tokenization, which converts a raw sentence into a sequence of tokens, is a crucial process that affects the performance of NLP tasks. Existing studies have proposed various tokenization methods including rule-based tokenization (Koehn et al., 2007), dictionary-based tokenization (Kudo, 2006; Morita et al., 2015; Tolmachev et al., 2018; Takaoka et al., 2018), supervised tokenization with neural networks (Yang et al., 2017; Cai et al., 2017; Yang et al., 2019), and unsupervised tokenization (Goldwater et al., 2006, 2009; Mochihashi et al., 2009; Sennrich et al., 2016; Kudo and Richardson, 2018). Much of prior research has reported that an appropriate tokenization depends on each downstream task (Xu et al., 2008; Chang et al., 2008; Nguyen et al., 2010; Domingo et al., 2018; Hiraoka et al., 2019; Gowda and May, 2020). Moreover, Hiraoka et al. (2020) implies that we have to consider a downstream model to determine an appropriate tokenization. In oth"
2021.findings-acl.21,D18-2010,0,0.0188839,"ated whether our method contributes to improving performance on text classification in three languages and machine translation in eight language pairs. Experimental results show that our proposed method improves the performance by determining appropriate tokenizations. 1 Introduction Tokenization, which converts a raw sentence into a sequence of tokens, is a crucial process that affects the performance of NLP tasks. Existing studies have proposed various tokenization methods including rule-based tokenization (Koehn et al., 2007), dictionary-based tokenization (Kudo, 2006; Morita et al., 2015; Tolmachev et al., 2018; Takaoka et al., 2018), supervised tokenization with neural networks (Yang et al., 2017; Cai et al., 2017; Yang et al., 2019), and unsupervised tokenization (Goldwater et al., 2006, 2009; Mochihashi et al., 2009; Sennrich et al., 2016; Kudo and Richardson, 2018). Much of prior research has reported that an appropriate tokenization depends on each downstream task (Xu et al., 2008; Chang et al., 2008; Nguyen et al., 2010; Domingo et al., 2018; Hiraoka et al., 2019; Gowda and May, 2020). Moreover, Hiraoka et al. (2020) implies that we have to consider a downstream model to determine an appropria"
2021.findings-acl.21,C08-1128,0,0.305748,"the performance of NLP tasks. Existing studies have proposed various tokenization methods including rule-based tokenization (Koehn et al., 2007), dictionary-based tokenization (Kudo, 2006; Morita et al., 2015; Tolmachev et al., 2018; Takaoka et al., 2018), supervised tokenization with neural networks (Yang et al., 2017; Cai et al., 2017; Yang et al., 2019), and unsupervised tokenization (Goldwater et al., 2006, 2009; Mochihashi et al., 2009; Sennrich et al., 2016; Kudo and Richardson, 2018). Much of prior research has reported that an appropriate tokenization depends on each downstream task (Xu et al., 2008; Chang et al., 2008; Nguyen et al., 2010; Domingo et al., 2018; Hiraoka et al., 2019; Gowda and May, 2020). Moreover, Hiraoka et al. (2020) implies that we have to consider a downstream model to determine an appropriate tokenization. In other words, we can improve the performance of a downstream model by determining an appropriate tokenization for the downstream model. However, since traditional tokenizers are isolated from a downstream model, we need to train a given downstream model with each possible tokenization and evaluate its performance to determine the appropriate tokenization. Perfo"
2021.findings-acl.21,P17-1078,0,0.025549,"languages and machine translation in eight language pairs. Experimental results show that our proposed method improves the performance by determining appropriate tokenizations. 1 Introduction Tokenization, which converts a raw sentence into a sequence of tokens, is a crucial process that affects the performance of NLP tasks. Existing studies have proposed various tokenization methods including rule-based tokenization (Koehn et al., 2007), dictionary-based tokenization (Kudo, 2006; Morita et al., 2015; Tolmachev et al., 2018; Takaoka et al., 2018), supervised tokenization with neural networks (Yang et al., 2017; Cai et al., 2017; Yang et al., 2019), and unsupervised tokenization (Goldwater et al., 2006, 2009; Mochihashi et al., 2009; Sennrich et al., 2016; Kudo and Richardson, 2018). Much of prior research has reported that an appropriate tokenization depends on each downstream task (Xu et al., 2008; Chang et al., 2008; Nguyen et al., 2010; Domingo et al., 2018; Hiraoka et al., 2019; Gowda and May, 2020). Moreover, Hiraoka et al. (2020) implies that we have to consider a downstream model to determine an appropriate tokenization. In other words, we can improve the performance of a downstream model by"
2021.findings-acl.21,N19-1278,0,0.019179,"eight language pairs. Experimental results show that our proposed method improves the performance by determining appropriate tokenizations. 1 Introduction Tokenization, which converts a raw sentence into a sequence of tokens, is a crucial process that affects the performance of NLP tasks. Existing studies have proposed various tokenization methods including rule-based tokenization (Koehn et al., 2007), dictionary-based tokenization (Kudo, 2006; Morita et al., 2015; Tolmachev et al., 2018; Takaoka et al., 2018), supervised tokenization with neural networks (Yang et al., 2017; Cai et al., 2017; Yang et al., 2019), and unsupervised tokenization (Goldwater et al., 2006, 2009; Mochihashi et al., 2009; Sennrich et al., 2016; Kudo and Richardson, 2018). Much of prior research has reported that an appropriate tokenization depends on each downstream task (Xu et al., 2008; Chang et al., 2008; Nguyen et al., 2010; Domingo et al., 2018; Hiraoka et al., 2019; Gowda and May, 2020). Moreover, Hiraoka et al. (2020) implies that we have to consider a downstream model to determine an appropriate tokenization. In other words, we can improve the performance of a downstream model by determining an appropriate tokenizati"
2021.inlg-1.6,2020.emnlp-main.284,0,0.0113297,"s such as sentiment transfer (Li et al., 2018) and claim generation (Hidey and McKeown, 2019). Further, semantically contrasting expressions with antonyms are utilized in advertising slogans (Katrandjiev et al., 2016), political speeches (Heritage and Greatbatch, 1986), and Chinese poetry (Yan et al., 2016). As antonymy is one of the relations of lexical semantics, such as synonymy and hyponymy, antonymy can be modeled using a similar approach to lexical knowledge acquisition. Most of the published studies on this topic have focused on the prediction of the relation between a given word pair (Barkan et al., 2020; Shwartz and Dagan, 2016), or a target (tail) for a given word (head) and its relation (Camacho-Collados et al., 2018; Rimell et al., 2017). However, predicting an antonym is challenging because multiple types of words are plausible as antonyms for a word. This is because a word can have semantic contrastiveness to the other as long as the word contains at least one feature contrasting to the other (Leech, 1976). For example, dual, double, and multiple can be antonyms for 48 Proceedings of the 14th International Conference on Natural Language Generation (INLG), pages 48–54, Aberdeen, Scotland"
2021.inlg-1.6,E17-2012,0,0.0150219,"s with antonyms are utilized in advertising slogans (Katrandjiev et al., 2016), political speeches (Heritage and Greatbatch, 1986), and Chinese poetry (Yan et al., 2016). As antonymy is one of the relations of lexical semantics, such as synonymy and hyponymy, antonymy can be modeled using a similar approach to lexical knowledge acquisition. Most of the published studies on this topic have focused on the prediction of the relation between a given word pair (Barkan et al., 2020; Shwartz and Dagan, 2016), or a target (tail) for a given word (head) and its relation (Camacho-Collados et al., 2018; Rimell et al., 2017). However, predicting an antonym is challenging because multiple types of words are plausible as antonyms for a word. This is because a word can have semantic contrastiveness to the other as long as the word contains at least one feature contrasting to the other (Leech, 1976). For example, dual, double, and multiple can be antonyms for 48 Proceedings of the 14th International Conference on Natural Language Generation (INLG), pages 48–54, Aberdeen, Scotland, UK, 20-24 September 2021. ©2021 Association for Computational Linguistics sitional encodings to capture antithesis structures, and (4) pse"
2021.inlg-1.6,N19-1423,0,0.171333,"ill-in-the-blanks problem for antonyms in context. For example, in the sentence, “A bed is better than single for me,” we expect to fill the blank with the words “double” or “king-sized.” The fill-in-theblanks setting requires the prediction of contextaware antonyms by capturing the contrasting features between the word pair. The task also requires a consideration of the naturalness of a text when filling the blank, which is necessary for applications of generating text with antonyms. In recent years, pre-training and fine-tuning approaches have achieved high performance in various NLP tasks (Devlin et al., 2019; Yang et al., 2019). Therefore, we use Bidirectional Encoder Representations from Transformers (BERT) as a pre-trained model to predict antonyms in a context. However, it is not easy to collect training data for fine-tuning the model, that is, text containing antonym pairs with a contrastive context. Therefore, we focus on the rhetorical device that effectively employs antonymy, that is, antithesis, which juxtaposes words or phrases in a similar structure with contrasting meanings. An antithesis is suitable for data creation because it ensures that a text has one or more antonym pairs in a co"
2021.inlg-1.6,W16-5304,0,0.0359906,"Missing"
2021.inlg-1.6,N19-1174,0,0.042664,"Missing"
2021.inlg-1.6,W04-3230,0,0.0895199,"Missing"
2021.inlg-1.6,P16-1222,0,0.033587,"Missing"
2021.inlg-1.6,N18-1169,0,0.0577829,"Missing"
2021.mtsummit-up.17,2020.acl-main.123,1,0.853561,"Missing"
2021.mtsummit-up.17,2020.lrec-1.445,1,0.817562,"Missing"
2021.mtsummit-up.17,P02-1040,0,0.120766,"Missing"
2021.mtsummit-up.17,P16-1009,0,0.0408644,"tes news headline. These technologies are outlined below. 2.1. Machine Translation The MT system (Mino, 2020) used for this research is a transformer-based encoder-decoder model (Vaswani, 2017). We constructed different types of parallel news corpora to develop our MT system. The primary corpus was built by manually translating Japanese news articles. The remaining corpora were respectively constructed by different approaches: an automatic sentence alignment method between Japanese and English news articles; post-editing of the aligned news articles manually; and a back-translation technique (Sennrich et al., 2016) to leverage monolingual news articles. To exploit multiple corpora with different features, we extend a domain-adaptation method by using multiple tags to train an NMT model effectively. This improves the translation quality of the MT system. 2.2. Headline Generation using Text Summarization Technology 2.2.1. Text Summarization Technology The text summarization method used for our research is a transformer-based abstractive text summarization method (Matsumaru, 2020), which is trained to output headlines from the leading sentences of news articles. Using this method, the text summarization sy"
C04-1108,W03-0507,0,0.0608861,"Missing"
C04-1108,P03-1069,0,0.254128,"ordering techniques such as majority ordering (examines most frequent orders in the original documents) and chronological ordering (orders sentence by the publication date). Showing that using naive ordering algorithms does not produce satisfactory orderings, Barzilay et al. also investigates through experiments with humans, how to identify patterns of orderings that can improve the algorithm. Based on the experiments, they propose another algorithm that utilizes chronological ordering with topical segmentation to separate sentences referring to a topic from ones referring to another. Lapata (Lapata, 2003) proposes another approach to information ordering based on a probabilistic model that assumes the probability of any given sentence is determined by its adjacent sentence and learns constraints on sentence order from a corpus of domain specific texts. Lapata estimates transitional probability between sentences by some attributes such as verbs (precedence relationships of verbs in the corpus), nouns (entity-based coherence by keeping track of the nouns) and dependencies (structure of sentences). 2.2 by chronological ordering. When we read these sentences in this order, we find sentence b to be"
C04-1108,P00-1010,0,0.0180013,"na while we do not have conclusive evidence whether a pair of sentences gathered arbitrarily from multiple documents has some relation. A newspaper usually deals with novel events that have occurred since the last publication. Hence, publication date (time) of each article turns out to be a good estimator of resemblance relation (i.e., we observe a trend or series of relevant events in a time period), contiguity in time, and cause-effect relation (i.e., an event occurs as a result of previous events). Although resolving temporal expressions in sentences (e.g., yesterday, the next year, etc.) (Mani and Wilson, 2000; Mani et al., 2003) may give a more precise estimation of these relations, it is not an easy task. For this reason we order sentences of each segment (cluster) by the chronological order, assigning a time stamp for each sentence by its publication date (i.e., the date when the article was written). When there are sentences having the same time stamp, we elaborate the order on the basis of sentence position and sentence connectivity. We restore an original ordering if two sentences have the same time stamp and belong to the same article. If sentences have the same time stamp and are not from t"
C04-1108,N03-2019,0,0.0200762,"conclusive evidence whether a pair of sentences gathered arbitrarily from multiple documents has some relation. A newspaper usually deals with novel events that have occurred since the last publication. Hence, publication date (time) of each article turns out to be a good estimator of resemblance relation (i.e., we observe a trend or series of relevant events in a time period), contiguity in time, and cause-effect relation (i.e., an event occurs as a result of previous events). Although resolving temporal expressions in sentences (e.g., yesterday, the next year, etc.) (Mani and Wilson, 2000; Mani et al., 2003) may give a more precise estimation of these relations, it is not an easy task. For this reason we order sentences of each segment (cluster) by the chronological order, assigning a time stamp for each sentence by its publication date (i.e., the date when the article was written). When there are sentences having the same time stamp, we elaborate the order on the basis of sentence position and sentence connectivity. We restore an original ordering if two sentences have the same time stamp and belong to the same article. If sentences have the same time stamp and are not from the same article, we"
C08-1083,J96-1002,0,0.130705,"t to design effective features for abbreviation recognition and to reuse the knowledge obtained from the training processes. In this paper, we formalize the task of abbreviation recognition as a sequential alignment problem, which finds the optimal alignment (origins of abbreviation letters) between two strings (abbreviation and full form). We design a large amount of features that directly express the events where letters produce or do not produce abbreviations. Preparing an aligned abbreviation corpus, we obtain the optimal combination of the features by using the maximum entropy framework (Berger et al., 1996). We report the remarkable improvements and conclude this paper. 2 2.1 Proposed method Abbreviation alignment model We express a sentence x as a sequence of letters (x1 , ..., xL ), and an abbreviation candidate y in the sentence as a sequence of letters (y1 , ..., yM ). We define a letter mapping a = (i, j) to indicate that the abbreviation letter yj is produced by the letter in the full form xi . A null mapping a = (i, 0) indicates that the letter in the sentence xi is unused to form the abbreviation. Similarly, a null mapping a = (0, j) indicates that the abbreviation letter yj does not ori"
C08-1083,P06-1009,0,0.022573,"2 displays the complete list of generation rules for unigram and bigram features4 , unigram(t) and bigram(s, t). For each generation rule in unigram(t) and bigram(s, t), we define boolean functions that test the possible values yielded by the corresponding atomic function(s). 2.3 Alignment candidates Formula 1 requires a sum over the possible alignments, which amounts to 2LM for a sentence (L letters) with an abbreviation (M letters). It is unrealistic to compute the partition factor of the formula directly; therefore, the factor has been computed by dynamic programing (McCallum et al., 2005; Blunsom and Cohn, 2006; Shimbo and Hara, 2007) or approximated by the n-best list of highly probable alignments (Och and Ney, 2002; Liu et al., 2005). Fortunately, we can prune alignments that are unlikely to present full forms, by introducing the natural assumptions for abbreviation definitions: 4 In Table 2, a set of curly brackets {} denotes a list (array) rather than a mathematical set. Operators ⊕ and ⊗ present concatenation and Cartesian product of lists. For instance, when A = {a, b} and B = {c, d}, A ⊕ B = {a, b, c, d} and A ⊗ B = {ac, ad, bc, bd}. 660 min(|y|+5, 2|y|) = 8 words, (|y |= 4; y = &quot;TTF-1&quot;) ai="
C08-1083,P05-1057,0,0.0188745,"rule in unigram(t) and bigram(s, t), we define boolean functions that test the possible values yielded by the corresponding atomic function(s). 2.3 Alignment candidates Formula 1 requires a sum over the possible alignments, which amounts to 2LM for a sentence (L letters) with an abbreviation (M letters). It is unrealistic to compute the partition factor of the formula directly; therefore, the factor has been computed by dynamic programing (McCallum et al., 2005; Blunsom and Cohn, 2006; Shimbo and Hara, 2007) or approximated by the n-best list of highly probable alignments (Och and Ney, 2002; Liu et al., 2005). Fortunately, we can prune alignments that are unlikely to present full forms, by introducing the natural assumptions for abbreviation definitions: 4 In Table 2, a set of curly brackets {} denotes a list (array) rather than a mathematical set. Operators ⊕ and ⊗ present concatenation and Cartesian product of lists. For instance, when A = {a, b} and B = {c, d}, A ⊕ B = {a, b, c, d} and A ⊗ B = {ac, ad, bc, bd}. 660 min(|y|+5, 2|y|) = 8 words, (|y |= 4; y = &quot;TTF-1&quot;) ai= 4 9 13 x: investigate ~ ~ 0 0 #0 0 0 #1 0 0 #2 0 0 #3 0 #4 Shffle 0 0 #5 Shffle 0 0 #6 Shffle 0 0 #7 Shffle 0 0 #8 Shffle 0 . ."
C08-1083,P02-1038,0,0.013388,"For each generation rule in unigram(t) and bigram(s, t), we define boolean functions that test the possible values yielded by the corresponding atomic function(s). 2.3 Alignment candidates Formula 1 requires a sum over the possible alignments, which amounts to 2LM for a sentence (L letters) with an abbreviation (M letters). It is unrealistic to compute the partition factor of the formula directly; therefore, the factor has been computed by dynamic programing (McCallum et al., 2005; Blunsom and Cohn, 2006; Shimbo and Hara, 2007) or approximated by the n-best list of highly probable alignments (Och and Ney, 2002; Liu et al., 2005). Fortunately, we can prune alignments that are unlikely to present full forms, by introducing the natural assumptions for abbreviation definitions: 4 In Table 2, a set of curly brackets {} denotes a list (array) rather than a mathematical set. Operators ⊕ and ⊗ present concatenation and Cartesian product of lists. For instance, when A = {a, b} and B = {c, d}, A ⊕ B = {a, b, c, d} and A ⊗ B = {ac, ad, bc, bd}. 660 min(|y|+5, 2|y|) = 8 words, (|y |= 4; y = &quot;TTF-1&quot;) ai= 4 9 13 x: investigate ~ ~ 0 0 #0 0 0 #1 0 0 #2 0 0 #3 0 #4 Shffle 0 0 #5 Shffle 0 0 #6 Shffle 0 0 #7 Shffle"
C08-1083,P02-1021,0,0.515377,"tions including named entity recognition, information retrieval, and question answering. c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. The task of abbreviation recognition, in which abbreviations and their expanded forms appearing in actual text are extracted, addresses the term variation problem caused by the increase in the number of abbreviations (Chang and Sch¨utze, 2006). Furthermore, abbreviation recognition is also crucial for disambiguating abbreviations (Pakhomov, 2002; Gaudan et al., 2005; Yu et al., 2006), providing sense inventories (lists of abbreviation definitions), training corpora (context information of full forms), and local definitions of abbreviations. Hence, abbreviation recognition plays a key role in abbreviation management. Numerous researchers have proposed a variety of heuristics for recognizing abbreviation definitions, e.g., the use of initials, capitalizations, syllable boundaries, stop words, lengths of abbreviations, and co-occurrence statistics (Park and Byrd, 2001; Wren and Garner, 2002; Liu and Friedman, 2003; Okazaki and Ananiadou"
C08-1083,W01-0516,0,0.670892,"abbreviation recognition is also crucial for disambiguating abbreviations (Pakhomov, 2002; Gaudan et al., 2005; Yu et al., 2006), providing sense inventories (lists of abbreviation definitions), training corpora (context information of full forms), and local definitions of abbreviations. Hence, abbreviation recognition plays a key role in abbreviation management. Numerous researchers have proposed a variety of heuristics for recognizing abbreviation definitions, e.g., the use of initials, capitalizations, syllable boundaries, stop words, lengths of abbreviations, and co-occurrence statistics (Park and Byrd, 2001; Wren and Garner, 2002; Liu and Friedman, 2003; Okazaki and Ananiadou, 2006; Zhou et al., 2006; Jain et al., 2007). Schwartz and Hearst (2003) implemented a simple algorithm that finds the shortest expression containing all alphanumerical letters of an abbreviation. Adar (2004) presented four scoring rules to choose the most likely expanded form in multiple candidates. Ao and Takagi (2005) designed more detailed conditions for accepting or discarding candidates of abbreviation definitions. However, these studies have limitations in discovering an optimal combination of heuristic rules from ma"
C08-1083,W02-0312,0,0.115089,"Missing"
C08-1083,D07-1064,0,0.0289905,"list of generation rules for unigram and bigram features4 , unigram(t) and bigram(s, t). For each generation rule in unigram(t) and bigram(s, t), we define boolean functions that test the possible values yielded by the corresponding atomic function(s). 2.3 Alignment candidates Formula 1 requires a sum over the possible alignments, which amounts to 2LM for a sentence (L letters) with an abbreviation (M letters). It is unrealistic to compute the partition factor of the formula directly; therefore, the factor has been computed by dynamic programing (McCallum et al., 2005; Blunsom and Cohn, 2006; Shimbo and Hara, 2007) or approximated by the n-best list of highly probable alignments (Och and Ney, 2002; Liu et al., 2005). Fortunately, we can prune alignments that are unlikely to present full forms, by introducing the natural assumptions for abbreviation definitions: 4 In Table 2, a set of curly brackets {} denotes a list (array) rather than a mathematical set. Operators ⊕ and ⊗ present concatenation and Cartesian product of lists. For instance, when A = {a, b} and B = {c, d}, A ⊕ B = {a, b, c, d} and A ⊗ B = {ac, ad, bc, bd}. 660 min(|y|+5, 2|y|) = 8 words, (|y |= 4; y = &quot;TTF-1&quot;) ai= 4 9 13 x: investigate ~"
C08-2032,J90-2002,0,0.22229,"Missing"
C08-2032,N03-1017,0,0.00647893,"f by a polysemous pivot word wp 1 . Previous work addressed the polysemy problem in pivot-based methods (Tanaka and Umemura, 1994; Schafer and Yarowsky, 2002). Pivot-based methods also suffer from a mismatch problem, in which a pivot word wp from a source word wf does not exist in the bilingual lexicon Lp – Le 2 . Moreover, a bilingual lexicon for technical terms is prone to include a number of pivot terms that are not included in another lexicon. This paper proposes a method for building a bilingual lexicon through a pivot language by using phrase-based statistical machine translation (SMT) (Koehn et al., 2003). We build a translation model between Lf and Le by assuming two lexicons Lf –Lp and Lp –Le as parallel corpora, in order to increase the obtained lexicon size by handling multi-word expressions appropriately. The main advantage of this method is its ability to incorporate various translation models that associate languages Lf –Le ; for example, we can further improve the translation model by integrating a small bilingual lexicon Lf –Le . 1 A Japanese term “ख”: dote, embankment, may be associated with a Chinese term “ၿߦ,” y´ıngh´ang: banking institution, using the pivot word bank in English."
C08-2032,P07-2045,0,0.00974321,", in order to improve both the merged lexicon size and its accuracy. Recently, several researchers proposed the use of the pivot language for phrase-based SMT (Utiyama and Isahara, 2007; Wu and Wang, 2007). We employ a similar approach for obtaining phrase translations with the translation probabilities by assuming the bilingual lexicons as parallel corpora. Figure 1 illustrates the framework of our approach. Let us suppose that we have two bilingual lexicons Lf –Lp and Lp –Le . We obtain word alignments of these lexicons by applying GIZA++ (Och and Ney, 2003), and grow-diag-final heuristics (Koehn et al., 2007). Let w ¯x be a phrase that represents a sequence of words in the language ¯p , w ¯f ) and (w ¯e , w ¯p ), the Lx . For phrase pairs (w ¯f ) and p(w ¯e |w ¯p ) translation probabilities p(w ¯p |w are computed using the maximum likelihood estimation from the co-occurrence frequencies, consistent with the word alignment in the bilingual lexicons. We calculate the direct translation probabilities between source and target phrases, ¯f ) =  p(w ¯e |w w ¯e w ¯p  p(w ¯e |w ¯p )p(w ¯p |w ¯f ) w ¯p p(w ¯e |w ¯p )p(w ¯p |w ¯f ) . (1) We employ the log-linear model of phrase-based SMT (Och and Ney, 2"
C08-2032,P02-1038,0,0.0126593,"et al., 2007). Let w ¯x be a phrase that represents a sequence of words in the language ¯p , w ¯f ) and (w ¯e , w ¯p ), the Lx . For phrase pairs (w ¯f ) and p(w ¯e |w ¯p ) translation probabilities p(w ¯p |w are computed using the maximum likelihood estimation from the co-occurrence frequencies, consistent with the word alignment in the bilingual lexicons. We calculate the direct translation probabilities between source and target phrases, ¯f ) =  p(w ¯e |w w ¯e w ¯p  p(w ¯e |w ¯p )p(w ¯p |w ¯f ) w ¯p p(w ¯e |w ¯p )p(w ¯p |w ¯f ) . (1) We employ the log-linear model of phrase-based SMT (Och and Ney, 2002) for translating the source term w ¯f in the lexicon Lf –Lp into the tarˆ¯ e that maximizes get language by finding a term w the translation probability, ˆ¯ e = argmax w¯e Pr(w w ¯e |w ¯f ) = argmax w¯e M  m=1  i Merging two bilingual lexicons  =1− ¯f ) ED(w ¯e , w , max(w ¯e , w ¯f ) (3) where ED(x, y) represents a Levenshtein distance of characters between the two terms x and y 3 . We also define an additional bilingual lexicon feature, Figure 1: Framework of our approach 2 ¯e , w ¯f ) sim (w λm hm (w ¯e , w ¯f ), (2) (i) (i) log p (w ¯e(i) |w ¯f ), (4) (i) ¯f represent an i-th translate"
C08-2032,J03-1002,0,0.00224552,"introduce phrase-based SMT for merging the lexicons, in order to improve both the merged lexicon size and its accuracy. Recently, several researchers proposed the use of the pivot language for phrase-based SMT (Utiyama and Isahara, 2007; Wu and Wang, 2007). We employ a similar approach for obtaining phrase translations with the translation probabilities by assuming the bilingual lexicons as parallel corpora. Figure 1 illustrates the framework of our approach. Let us suppose that we have two bilingual lexicons Lf –Lp and Lp –Le . We obtain word alignments of these lexicons by applying GIZA++ (Och and Ney, 2003), and grow-diag-final heuristics (Koehn et al., 2007). Let w ¯x be a phrase that represents a sequence of words in the language ¯p , w ¯f ) and (w ¯e , w ¯p ), the Lx . For phrase pairs (w ¯f ) and p(w ¯e |w ¯p ) translation probabilities p(w ¯p |w are computed using the maximum likelihood estimation from the co-occurrence frequencies, consistent with the word alignment in the bilingual lexicons. We calculate the direct translation probabilities between source and target phrases, ¯f ) =  p(w ¯e |w w ¯e w ¯p  p(w ¯e |w ¯p )p(w ¯p |w ¯f ) w ¯p p(w ¯e |w ¯p )p(w ¯p |w ¯f ) . (1) We employ the"
C08-2032,W02-2026,0,0.0955309,"ilingual lexicon for every language pair; thus, comprehensible bilingual lexicons are available only for a limited number of language pairs. One of the solutions is to build a bilingual lexicon of the source language Lf and the target Le through a pivot language Lp , when large bilingual c 2008.  Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. lexicons Lf –Lp and Lp –Le are available. Numerous researchers have explored the use of pivot languages (Tanaka and Umemura, 1994; Schafer and Yarowsky, 2002; Zhang et al., 2005). This approach is advantageous because we can obtain a bilingual lexicon between Le and Lf , even if no bilingual lexicon exists between these languages. Pivot-based methods for dictionary construction may produce incorrect translations when the word we is translated from a word wf by a polysemous pivot word wp 1 . Previous work addressed the polysemy problem in pivot-based methods (Tanaka and Umemura, 1994; Schafer and Yarowsky, 2002). Pivot-based methods also suffer from a mismatch problem, in which a pivot word wp from a source word wf does not exist in the bilingual l"
C08-2032,N07-1061,0,0.0383126,"d additional bilingual lexicon. We define a character-based similarity feature, Lf-Lp lexicon Le-Lp lexicon Word alignment & grow-diag-final method Lf-Lp translation Le-Lp translation phrase table phrase table Merging phrase tables Lf-Le translation phrase table Additional features hchar Phrase-based Le: translations of SMT system Lf-Lp lexicon INPUT OUTPUT hadd ¯e , w ¯f ) lex (w = We introduce phrase-based SMT for merging the lexicons, in order to improve both the merged lexicon size and its accuracy. Recently, several researchers proposed the use of the pivot language for phrase-based SMT (Utiyama and Isahara, 2007; Wu and Wang, 2007). We employ a similar approach for obtaining phrase translations with the translation probabilities by assuming the bilingual lexicons as parallel corpora. Figure 1 illustrates the framework of our approach. Let us suppose that we have two bilingual lexicons Lf –Lp and Lp –Le . We obtain word alignments of these lexicons by applying GIZA++ (Och and Ney, 2003), and grow-diag-final heuristics (Koehn et al., 2007). Let w ¯x be a phrase that represents a sequence of words in the language ¯p , w ¯f ) and (w ¯e , w ¯p ), the Lx . For phrase pairs (w ¯f ) and p(w ¯e |w ¯p ) transl"
C08-2032,P07-1108,0,0.118778,"con. We define a character-based similarity feature, Lf-Lp lexicon Le-Lp lexicon Word alignment & grow-diag-final method Lf-Lp translation Le-Lp translation phrase table phrase table Merging phrase tables Lf-Le translation phrase table Additional features hchar Phrase-based Le: translations of SMT system Lf-Lp lexicon INPUT OUTPUT hadd ¯e , w ¯f ) lex (w = We introduce phrase-based SMT for merging the lexicons, in order to improve both the merged lexicon size and its accuracy. Recently, several researchers proposed the use of the pivot language for phrase-based SMT (Utiyama and Isahara, 2007; Wu and Wang, 2007). We employ a similar approach for obtaining phrase translations with the translation probabilities by assuming the bilingual lexicons as parallel corpora. Figure 1 illustrates the framework of our approach. Let us suppose that we have two bilingual lexicons Lf –Lp and Lp –Le . We obtain word alignments of these lexicons by applying GIZA++ (Och and Ney, 2003), and grow-diag-final heuristics (Koehn et al., 2007). Let w ¯x be a phrase that represents a sequence of words in the language ¯p , w ¯f ) and (w ¯e , w ¯p ), the Lx . For phrase pairs (w ¯f ) and p(w ¯e |w ¯p ) translation probabilities"
C08-2032,C94-1048,0,\N,Missing
C08-2032,P07-2055,0,\N,Missing
C10-1096,P07-1083,0,0.0097517,"ity Sensitive Hash (LSH) function (Andoni and Indyk, 2008), which preserves the property of cosine similarity. The essence of this function is to map strings into N -bit hash values where the bitwise hamming distance between the hash values of two strings approximately corresponds to the angle of the two strings. Ravichandran et al. (2005) applied LSH to the task of noun clustering. Adapting this algorithm to approximate dictionary matching, we discussed its performance in Section 3. Several researchers have presented refined similarity measures for strings (Winkler, 1999; Cohen et al., 2003; Bergsma and Kondrak, 2007; Davis et al., 2007). Although these studies are sometimes regarded as a research topic of approximate dictionary matching, they assume that two strings for the target of similarity computation are given; in other words, it is out of their scope to find strings in a large collection that are similar to a given string. Thus, it is a reasonable approach for an approximate dictionary matching to quickly collect candidate strings with a loose similarity threshold, and for a refined similarity measure to scrutinize each candidate string for the target application. 5 Conclusions We present a simple"
C10-1096,P05-1077,0,0.464443,"exactly by a τ -overlap join (Sarawagi and Kirpal, 2004) of inverted lists. Then we present CPMerge, which is a simple and efficient algorithm for the τ -overlap join. In addition, the algorithm is easily implemented. 2. We demonstrate the efficiency of the algorithm on three large-scale datasets with person names, biomedical concept names, 851 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 851–859, Beijing, August 2010 and general English words. We compare the algorithm with state-of-the-art algorithms, including Locality Sensitive Hashing (Ravichandran et al., 2005; Andoni and Indyk, 2008) and DivideSkip (Li et al., 2008). The proposed algorithm retrieves strings the most rapidly, e.g., in 1.1 ms from Google Web1T unigrams (with cosine similarity and threshold 0.7). 2 Proposed Method 2.1 In this paper, we assume that the features of a string are represented arbitrarily by a set. Although it is important to design a string representation for an accurate similarity measure, we do not address this problem: our emphasis is not on designing a better representation for string similarity but on establishing an efficient algorithm. The most popular representati"
C10-2141,P00-1038,0,0.0953371,"Missing"
C10-2141,P08-1092,0,0.0158856,"query patterns to achieve the F’score of 21.4. Therefore, the proposed method achieves a high F’-score by using only a small number of patterns. This implies that the method achieves high performance in a short query processing time. 4 Related Work Many studies have addressed the problem of pattern extraction from Wikipedia (or other large corpora). Filatova et al. (2006) presented an approach for automatically extracting important word patterns from a large corpus. They analyzed the BBC corpus to extract word patterns containing verbs that are supposed to be important for a specific domain. Biadsy et al. (2008) described a system for producing biographies for a given target name. They used Wikipedia to learn the document structures of a biography. Ye et al. (2009) 1235 explored a method for generating a series of summaries of various lengths by using information from Wikipedia. Sauper and Barzilay (2009) proposed an approach for creating a summary of many chunks of text that are related to an entity and retrieved from the Web. They used Wikipedia not only for producing the template, but also for improving the summaries. Although the target of their work is very close to that of our study, the focus"
C10-2141,H05-1071,0,0.0627441,"Missing"
C10-2141,P06-1039,0,0.0739258,"Missing"
C10-2141,P06-2027,0,0.0262112,"merican tennis players. We observe that the templates generated by the proposed method achieve the highest F’-score at every value of N. The maximum F’-score is 24.7, which is achieved when N is 30. Moreover, the proposed method requires only five query patterns to achieve the F’score of 21.4. Therefore, the proposed method achieves a high F’-score by using only a small number of patterns. This implies that the method achieves high performance in a short query processing time. 4 Related Work Many studies have addressed the problem of pattern extraction from Wikipedia (or other large corpora). Filatova et al. (2006) presented an approach for automatically extracting important word patterns from a large corpus. They analyzed the BBC corpus to extract word patterns containing verbs that are supposed to be important for a specific domain. Biadsy et al. (2008) described a system for producing biographies for a given target name. They used Wikipedia to learn the document structures of a biography. Ye et al. (2009) 1235 explored a method for generating a series of summaries of various lengths by using information from Wikipedia. Sauper and Barzilay (2009) proposed an approach for creating a summary of many chu"
C10-2141,P07-2015,0,0.0605836,"Missing"
C10-2141,P09-1024,0,0.095066,"o verify the performance of the proposed method, we compare the precision, coverage, and F’-score of the information retrieval process by using the template obtained by the proposed method with that by three other baseline methods. 3.1 Experimental Settings 3.1.1 Data We use articles of five categories in Wikipedia as the data for evaluation: American actors, Genetic disorders, American tennis players, Software companies, and Operas. Among these categories, the first two (American actors, Genetic disorders) have been commonly used as evaluation data in previous research on text summarization (Sauper and Barzilay, 2009). The other three (American tennis players, Software companies, Operas) are categories about three distinct topics (sport, business, entertainment).Table 2 shows information about these categories. We divide the article set of a given category into six subsets. We use one subset as the development set for tuning the parameters α and β in the proposed method. The remaining five subsets are the training set and the test set, which are used for the 5-fold cross-validation. We create the template by using the training set and evaluate it with the test set. For evaluation of the baseline methods, w"
C10-2141,C04-1093,0,0.0240581,"used Wikipedia not only for producing the template, but also for improving the summaries. Although the target of their work is very close to that of our study, the focus of each study is different. They address the method for selecting appropriate sentences for summarization, whereas we consider the method for selecting query patterns that can generate a comprehensive summary of an entity. Various studies have addressed Web page summarization and query-focused summarization, from search result summarization (Kanungo et al., 2009) to query biased summarization (Wang et al., 2007). Furthermore, Fujii and Ishikawa (2004) presented a method to automatically compile encyclopedic knowledge from the Web. Similar to relation extraction, the proposed method retrieves information concerning an entity by using query patterns. This is because query patterns for relation extraction are also appropriate in sentence extraction for multi-document summarization (Hachey, 2009). However, the relation extraction task primarily obtains query patterns that retrieve instances of a specific relation. This is different from the goal of this study, which is obtaining a set of patterns that are able to retrieve a large range of topi"
C10-2141,P07-2049,0,0.0318993,"nd concept of interest to a user. The ultimate goal of this study is to generate articles about an entity of a specified category from the Web by using Wikipedia articles in the same entity category as exemplars. This study follows previous work of the other authors on query-biased/focused summarization (Tombros and Sanderson, 1998; Berger 1 http://en.wikipedia.org/ A great number of researchers have addressed the problem of query-focused summarization (Carbonell and Goldstein, 1998; White et al., 2003; Dang, 2005; Daum´e and Marcu, 2006; Varadarajan and Hristidis, 2006; Fuentes et al., 2007; Gupta et al., 2007; Wang et al., 2007; Kanungo et al., 2009). However, these studies assume that a document collection is provided for the summarization systems. In other words, collecting source documents that include important concepts for the target entity is not in the scope of these studies. For example, queries such as “(actor) was born in,” “(actor) born on,” “(actor) plays,” and “(actor) won” may be more suitable than the simple query “(actor)” for obtaining concepts concerning the actor. Source documents can be collected by a similar idea in relation extraction, which extracts entities having specific"
C10-2141,D09-1044,0,0.0244663,"prehensive summary of an entity. Various studies have addressed Web page summarization and query-focused summarization, from search result summarization (Kanungo et al., 2009) to query biased summarization (Wang et al., 2007). Furthermore, Fujii and Ishikawa (2004) presented a method to automatically compile encyclopedic knowledge from the Web. Similar to relation extraction, the proposed method retrieves information concerning an entity by using query patterns. This is because query patterns for relation extraction are also appropriate in sentence extraction for multi-document summarization (Hachey, 2009). However, the relation extraction task primarily obtains query patterns that retrieve instances of a specific relation. This is different from the goal of this study, which is obtaining a set of patterns that are able to retrieve a large range of topics related to an entity. 5 Conclusion We present a novel method to acquire a set of query patterns for retrieving documents that contain important information regarding an entity. Especially, we concentrate on the method for selecting query patterns that are able to comprehensively and precisely retrieve important concepts concerning an entity. T"
C10-2141,C92-2082,0,0.0527863,"., 2009). However, these studies assume that a document collection is provided for the summarization systems. In other words, collecting source documents that include important concepts for the target entity is not in the scope of these studies. For example, queries such as “(actor) was born in,” “(actor) born on,” “(actor) plays,” and “(actor) won” may be more suitable than the simple query “(actor)” for obtaining concepts concerning the actor. Source documents can be collected by a similar idea in relation extraction, which extracts entities having specific relations with the target entity (Hearst, 1992; Brin, 1999; Agichtein and Gravano, 2000; Turney, 2001; Pantel and Pennac1229 Coling 2010: Poster Volume, pages 1229–1237, Beijing, August 2010 1. Triplet extraction identifies, for each Wikipedia article, entity mentions, concepts, and phrases that form a bridge between the entity mentions and concepts. In the context of learning query patterns from Wikipedia, we assume that a Wikipedia article is written for an entity. By identifying entity mentions and concepts in the article, we obtain bridging phrases between entity mentions and concepts as candidates for query patterns. chiotti, 2006; B"
C10-2141,P09-1023,0,0.0185607,"hat the method achieves high performance in a short query processing time. 4 Related Work Many studies have addressed the problem of pattern extraction from Wikipedia (or other large corpora). Filatova et al. (2006) presented an approach for automatically extracting important word patterns from a large corpus. They analyzed the BBC corpus to extract word patterns containing verbs that are supposed to be important for a specific domain. Biadsy et al. (2008) described a system for producing biographies for a given target name. They used Wikipedia to learn the document structures of a biography. Ye et al. (2009) 1235 explored a method for generating a series of summaries of various lengths by using information from Wikipedia. Sauper and Barzilay (2009) proposed an approach for creating a summary of many chunks of text that are related to an entity and retrieved from the Web. They used Wikipedia not only for producing the template, but also for improving the summaries. Although the target of their work is very close to that of our study, the focus of each study is different. They address the method for selecting appropriate sentences for summarization, whereas we consider the method for selecting quer"
C10-2141,P06-1015,0,0.0898001,"Missing"
C12-1171,bentivogli-etal-2010-building,0,0.0699236,"Missing"
C12-1171,W09-3401,0,0.0239442,"Missing"
C12-1171,N10-1066,0,0.143081,"Supervised on Forward-Entailment and outperforms all other methods on Alternation/Negation), it performs drastically worse on sentence-level semantic relation recognition, sometimes with an f-score that is more than 20 points lower than the best performing method. These results suggest that it is important to jointly model alignment prediction and sentence-level semantic relation recognition so that globally optimal alignments are promoted. 2817 5 Related Work There are a number of existing works which explore the use of latent variable or structure models for recognizing textual entailment. Chang et al. (2010) proposed a discriminative linear model where alignments are treated as hidden structures, and the sentence-level semantic relation is derived based on the best latent alignment structure. They formulated the problem of predicting the best hidden structure as an Integer Linear Programming problem, where domain knowledge is encoded as constraints. Wang and Manning (2010) proposed a latent variable model where the model provides a conditional distribution of a sequence of edits, which can be seen as a transformation-based approach. In the model, edits are treated as hidden variables that populat"
C12-1171,D09-1122,0,0.0292576,"nt and its lemmas are the same. 1 if e has an argument of some predicates and the other sentence also contains an argument and its cases are the same. 1 if the head word of the bunsetsu in e is also contained in the other sentence. POS sequence in e head POS of e edit type of e the number of bunsetsus in e the number of shared arguments if both t and h in e are predicates 1 if t and t have the same particle Set relation type if e matches an entry in Japanese WordNet (Bond et al., 2009). 1 if e matches an entry in (Sumida et al., 2008). 1 if an entry in the verb entailment relation dictionary (Hashimoto et al., 2009) matches e 1 if an entry in the verb relation dictionary (Matsuyoshi et al., 2008) matches e the number of shared arguments of the parent of t and h if t and h are arguments 1 if each bunsetsu in e is an argument of a predicate. 1 if each bunsetsu in e has the same case. 1 if the POSs of the heads in e are the same 1 if the POS sequences of chunks in e are the same 1 if t and h are the same return unigram cosine value if the cosine similarity of two chunks in e is greater than the pre-defined threshold SAME_CASE_IN_{T,H} {T,H}_CONTAINS_{H,T}_LEMMA ΨA POS_SEQ HEADPOS TYPE SIZE NUM_SHARED_ARGS S"
C12-1171,N10-1145,0,0.0286773,"way to capture the affects of diverse linguistic phenomena and their interactions, where a set of linguistic phenomena are decomposed into units. By doing so it becomes possible to consider their effects on entailment independently. A number of previous works explores transformation-based entailment relation recognition. The approach of Stern et al. (2011) recognizes a sentence-level semantic relation through a proof which represents a sequence of edits from T to H produced by applying various entailment rules and the operations such as insertion, deletion, moving subtrees, etc. In addition, Heilman and Smith (2010) proposed a tree edit model which selects a sequence of edits using Tree Kernels, and Wang and Manning (2010) proposed a latent variable model which consider possible alignments as hidden structures. However, these model do not sufficiently represent interactions between linguistic phenomena such as factuality reversals caused by negation and flipping of entailment direction under downward-monotone contexts. In order to realize precise entailment relation recognition, we need to appropriately deal with semantic relations resulting from the interaction between linguistic phenomena. One of the m"
C12-1171,P10-1026,0,0.0164499,"f the cosine similarity of two chunks in e is greater than the pre-defined threshold SAME_CASE_IN_{T,H} {T,H}_CONTAINS_{H,T}_LEMMA ΨA POS_SEQ HEADPOS TYPE SIZE NUM_SHARED_ARGS SUB PARTICLE_SAME JAPANESE_WORDNET WIKIPEDIA_HYPERNYM-HYPONYM VERB_ENTAILMENT_REL VERB_RELATION_REL PARENT_NUM_SHARED_ARGS BOTH_HAVE_A_ROLE BOTH_HAVE_THE_SAME_ROLE HEAD_POS_SAME POS_SEQ_SAME EXACT_MATCH UNIGRAM_COSINE SIZE HEAD_LEMMA HEAD_WORD_CLASS number of bunsetsus in e lemma of the head of bunsetsu in e Word class of the head of bunsetsu in e. The word class information is extracted from the dictionary provided by (Kazama et al., 2010) 1 if the bunsetsu contains a negation Pair of the number of bunsetsus in e POS pair of the heads of bunsetsus in e 1 if the lemmas of the heads in e are the same 1 if the POS sequences of chunks in e are the same same as in ΨA same as in ΨA same as in ΨA same as in ΨA ΨS DEL, INS ΨS SUB ΨP – MONOTONE_{UP/DOWN} the context of e is upward-monotone or downwardmonotone. ΨC – COMPOSITION_RULE 1 if the tuple of semantic relations is included in a set of defined compositional rules. NEGATION SIZE HEAD_POS_PAIR HEAD_LEMMA_SAME POS_SEQ_SAME JAPANESE_WORDNET WIKIPEDIA_HYPERNYM-HYPONYM VERB_ENTAILMENT_R"
C12-1171,W02-2016,0,0.0460378,"68.8 68.4 54.9 100.0 100.0 100.0 100.0 reachable examples only 45.6 65.5 53.8 41.7 48.4 66.7 56.1 42.6 72.4 73.1 72.7 59.2 74.2 75.0 74.6 61.9 Sem. Rel. Acc. 54.9 53.5 51.9 55.5 100.0 44.5 44.9 51.6 55.2 100.0 31.8 35.0 47.5 43.7 55.2 59.9 58.8 59.8 62.6 49.2 49.4 59.5 62.3 38.5 37.7 60.2 46.4 Table 4: Performance of alignment prediction and sentence-level semantic relation recognition. 4.4 Preprocessing For each sentence, we conducted various forms of linguistic analysis: morphological analysis using MeCab (Kudo et al., 2004), syntactic parsing using the Japanese dependency parser, CaboCha (Kudo and Matsumoto, 2002) and predicate-argument structure analysis (Watanabe et al., 2010) to provide a basis for alignment and semantic relation classification. 4.5 Results Table 4 shows the experimental results of 10-fold cross validation for alignment prediction and sentence level semantic relation recognition. We can see that while the proposed method is less successful at reproducing gold standard alignments, it greatly outperforms Supervised Learning for sentence-level semantic relation recognition8 . We expected Supervised Learning to perform best on reachable examples, which should have the most straightforwa"
C12-1171,W04-3230,0,0.00953329,"beled) Prec. Rec. F1 42.6 62.5 50.6 37.5 45.6 63.3 53.0 38.6 67.1 67.8 67.4 51.3 68.0 68.8 68.4 54.9 100.0 100.0 100.0 100.0 reachable examples only 45.6 65.5 53.8 41.7 48.4 66.7 56.1 42.6 72.4 73.1 72.7 59.2 74.2 75.0 74.6 61.9 Sem. Rel. Acc. 54.9 53.5 51.9 55.5 100.0 44.5 44.9 51.6 55.2 100.0 31.8 35.0 47.5 43.7 55.2 59.9 58.8 59.8 62.6 49.2 49.4 59.5 62.3 38.5 37.7 60.2 46.4 Table 4: Performance of alignment prediction and sentence-level semantic relation recognition. 4.4 Preprocessing For each sentence, we conducted various forms of linguistic analysis: morphological analysis using MeCab (Kudo et al., 2004), syntactic parsing using the Japanese dependency parser, CaboCha (Kudo and Matsumoto, 2002) and predicate-argument structure analysis (Watanabe et al., 2010) to provide a basis for alignment and semantic relation classification. 4.5 Results Table 4 shows the experimental results of 10-fold cross validation for alignment prediction and sentence level semantic relation recognition. We can see that while the proposed method is less successful at reproducing gold standard alignments, it greatly outperforms Supervised Learning for sentence-level semantic relation recognition8 . We expected Supervi"
C12-1171,D08-1084,0,0.443103,"ent and semantic relation 2806 recognition between sentences is needed that learns the alignments which will generate the correct semantic relation by considering the interaction between diverse linguistic phenomena. In this paper, we propose a novel latent discriminative model that jointly handles predicting alignment edits, classification of their semantic relations and entailment relation recognition by providing a joint distribution of variables including alignment edits, their local semantic relations and sentence-level semantic relations. Inspired by the Natural Logic-based approach of (MacCartney et al., 2008), we incorporate the set of semantic relations and their composition rules from Natural Logic into our proposed model. In addition, our model can be trained from only sentence-level semantic relations to predict alignments and semantic relations that are consistent with Natural Logic composition. To the best of our knowledge, our study is the first work to propose a latent model for training a Natural Logic-based semantic relation recognition system that does not require alignment annotations and that jointly predicts plausible alignments and semantic relations between sentences, modeling a va"
C12-1171,C08-1066,0,0.274111,"Tree Kernels, and Wang and Manning (2010) proposed a latent variable model which consider possible alignments as hidden structures. However, these model do not sufficiently represent interactions between linguistic phenomena such as factuality reversals caused by negation and flipping of entailment direction under downward-monotone contexts. In order to realize precise entailment relation recognition, we need to appropriately deal with semantic relations resulting from the interaction between linguistic phenomena. One of the most promising approaches to RTE is Natural Logic-based recognition (MacCartney and Manning, 2008; MacCartney, 2009). This approach represents transformations from T to H with a set of three types of alignment edits (substitution, insertion and deletion), and assigns one of a set-theoretically defined semantic relations to each alignment edit. This approach is based on the principle of compositionality, i.e. the sentence-level semantic relation is derived by combining semantic relations of edits using pre-defined composition rules. By doing so, this approach makes progress toward precise sentence-level entailment relation recognition that considers linguistic phenomena and their interacti"
C12-1171,P10-1122,0,0.0720479,"Missing"
C12-1171,sumida-etal-2008-boosting,0,0.0188101,"s a nominative argument and the other sentence also contains a nominative argument and its lemmas are the same. 1 if e has an argument of some predicates and the other sentence also contains an argument and its cases are the same. 1 if the head word of the bunsetsu in e is also contained in the other sentence. POS sequence in e head POS of e edit type of e the number of bunsetsus in e the number of shared arguments if both t and h in e are predicates 1 if t and t have the same particle Set relation type if e matches an entry in Japanese WordNet (Bond et al., 2009). 1 if e matches an entry in (Sumida et al., 2008). 1 if an entry in the verb entailment relation dictionary (Hashimoto et al., 2009) matches e 1 if an entry in the verb relation dictionary (Matsuyoshi et al., 2008) matches e the number of shared arguments of the parent of t and h if t and h are arguments 1 if each bunsetsu in e is an argument of a predicate. 1 if each bunsetsu in e has the same case. 1 if the POSs of the heads in e are the same 1 if the POS sequences of chunks in e are the same 1 if t and h are the same return unigram cosine value if the cosine similarity of two chunks in e is greater than the pre-defined threshold SAME_CASE"
C12-1171,C10-1131,0,0.405266,"phenomena are decomposed into units. By doing so it becomes possible to consider their effects on entailment independently. A number of previous works explores transformation-based entailment relation recognition. The approach of Stern et al. (2011) recognizes a sentence-level semantic relation through a proof which represents a sequence of edits from T to H produced by applying various entailment rules and the operations such as insertion, deletion, moving subtrees, etc. In addition, Heilman and Smith (2010) proposed a tree edit model which selects a sequence of edits using Tree Kernels, and Wang and Manning (2010) proposed a latent variable model which consider possible alignments as hidden structures. However, these model do not sufficiently represent interactions between linguistic phenomena such as factuality reversals caused by negation and flipping of entailment direction under downward-monotone contexts. In order to realize precise entailment relation recognition, we need to appropriately deal with semantic relations resulting from the interaction between linguistic phenomena. One of the most promising approaches to RTE is Natural Logic-based recognition (MacCartney and Manning, 2008; MacCartney,"
C12-1171,D09-1082,0,0.0220178,"Text T and Hypothesis H. RTE is useful for many information access tasks that depend on natural language processing technologies, and a breakthrough would lead to significant progress in information retrieval, document summarization, and question answering, among other tasks. The majority of approaches proposed in previous work recognize entailment relations between a pair of texts by capturing lexical or structural correspondences. Methods include simple word overlap-based measures (Jijkoun and de Rijke, 2005) as well as alignment of syntactic and semantic dependencies (Sammons et al., 2009; Wang and Zhang, 2009). However, sentencelevel semantic relations are affected by various linguistic phenomena: not only lexical semantic relations (synonyms, antonyms) but also monotonicity (e.g. downward-monotone caused by scope of negation), implicative/factive expressions, quantifiers, etc. Thus similarity measures are insufficient to capture these phenomena and their interactions. Transformation-based approaches are one way to capture the affects of diverse linguistic phenomena and their interactions, where a set of linguistic phenomena are decomposed into units. By doing so it becomes possible to consider the"
C12-1171,P10-2018,1,0.83968,"65.5 53.8 41.7 48.4 66.7 56.1 42.6 72.4 73.1 72.7 59.2 74.2 75.0 74.6 61.9 Sem. Rel. Acc. 54.9 53.5 51.9 55.5 100.0 44.5 44.9 51.6 55.2 100.0 31.8 35.0 47.5 43.7 55.2 59.9 58.8 59.8 62.6 49.2 49.4 59.5 62.3 38.5 37.7 60.2 46.4 Table 4: Performance of alignment prediction and sentence-level semantic relation recognition. 4.4 Preprocessing For each sentence, we conducted various forms of linguistic analysis: morphological analysis using MeCab (Kudo et al., 2004), syntactic parsing using the Japanese dependency parser, CaboCha (Kudo and Matsumoto, 2002) and predicate-argument structure analysis (Watanabe et al., 2010) to provide a basis for alignment and semantic relation classification. 4.5 Results Table 4 shows the experimental results of 10-fold cross validation for alignment prediction and sentence level semantic relation recognition. We can see that while the proposed method is less successful at reproducing gold standard alignments, it greatly outperforms Supervised Learning for sentence-level semantic relation recognition8 . We expected Supervised Learning to perform best on reachable examples, which should have the most straightforward connection between alignment semantic relation labels and sente"
C12-2118,I08-1065,0,0.0191451,"cause we trained the constraint selector for the proposed method. We observed that the half of rules extracted by the proposed method were judged incorrect. Analyzing these false cases, we found that these errors appeared in the phase of selecting constraints. 4 Related Work The previous work on automatic acquisition of causal knowledge can be categorized into three groups in terms of types of inference rules: noun-noun causality (Girju, 2003; Chang and Choi, 2006; Saeger et al., 2011), verb-verb causality (Lin and Pantel, 2001; Chklovski and Pantel, 2004; Torisawa, 2006; Pantel et al., 2007; Abe et al., 2008; Beamer and Girju, 2009; Do et al., 2011; Hashimoto et al., 2012), and inference rules of other types (e.g., entailment) (Pekar, 2006; Szpektor and Dagan, 2008; Aharon et al., 2010; Schoenmackers et al., 2010; Berant et al., 2010, 2011; Gordon and Schubert, 2011; Berant et al., 2012). However, causal rules extracted by the previous work were limited to those without variables (e.g., lean ⇒ kiss) or those with the same set of variables (e.g., ❳ leaves for ❨ ⇒ ❳ gets to ❨) in the head and body of a rule. In contrast, our work is the first approach that leverages deverbal nouns that directly exp"
C12-2118,P10-2045,0,0.0158127,"cases, we found that these errors appeared in the phase of selecting constraints. 4 Related Work The previous work on automatic acquisition of causal knowledge can be categorized into three groups in terms of types of inference rules: noun-noun causality (Girju, 2003; Chang and Choi, 2006; Saeger et al., 2011), verb-verb causality (Lin and Pantel, 2001; Chklovski and Pantel, 2004; Torisawa, 2006; Pantel et al., 2007; Abe et al., 2008; Beamer and Girju, 2009; Do et al., 2011; Hashimoto et al., 2012), and inference rules of other types (e.g., entailment) (Pekar, 2006; Szpektor and Dagan, 2008; Aharon et al., 2010; Schoenmackers et al., 2010; Berant et al., 2010, 2011; Gordon and Schubert, 2011; Berant et al., 2012). However, causal rules extracted by the previous work were limited to those without variables (e.g., lean ⇒ kiss) or those with the same set of variables (e.g., ❳ leaves for ❨ ⇒ ❳ gets to ❨) in the head and body of a rule. In contrast, our work is the first approach that leverages deverbal nouns that directly express causal relations, and generalizes causal relations into causal rules with multiple variables. 5 Conclusion In this paper, we presented a novel approach for inducing causal rule"
C12-2118,P98-1013,0,0.022155,"esent a method for generalizing and constraining causal relations by making use of relation instances acquired automatically from a large corpus. Previous work replaced the same mention (string) in a pattern with a variable to induce an inference rule (template). In contrast, this work unveils hidden predicates and variables that are not stated explicitly in text, but are crucial for explaining causal relations. This part is very challenging because we need to combine pieces of predicates obtained from different texts. 1210 2 Proposed Method The proposed system uses FrameNet1 (Fillmore, 1976; Baker et al., 1998) for obtaining a list of verbs and their deverbal nouns (e.g., acquisition and purchase as deverbal nouns of verbs buy and acquire). Finding documents containing both verbs and their deverbal nouns in the corpus, the system extracts text fragments in which causal relations are expressed by pairs of sentences, “A verb B ...” and “Deverbal-noun ...”. Here, A and B present named entities2 , and “Deverbal-noun ...” denotes a sentence starting with the deverbal noun of verb. We call the former sentence “A verb B ...” a head sentence and the latter sentence “Deverbal-noun ...” a body sentence. We ap"
C12-2118,P12-1013,0,0.0113056,"revious work on automatic acquisition of causal knowledge can be categorized into three groups in terms of types of inference rules: noun-noun causality (Girju, 2003; Chang and Choi, 2006; Saeger et al., 2011), verb-verb causality (Lin and Pantel, 2001; Chklovski and Pantel, 2004; Torisawa, 2006; Pantel et al., 2007; Abe et al., 2008; Beamer and Girju, 2009; Do et al., 2011; Hashimoto et al., 2012), and inference rules of other types (e.g., entailment) (Pekar, 2006; Szpektor and Dagan, 2008; Aharon et al., 2010; Schoenmackers et al., 2010; Berant et al., 2010, 2011; Gordon and Schubert, 2011; Berant et al., 2012). However, causal rules extracted by the previous work were limited to those without variables (e.g., lean ⇒ kiss) or those with the same set of variables (e.g., ❳ leaves for ❨ ⇒ ❳ gets to ❨) in the head and body of a rule. In contrast, our work is the first approach that leverages deverbal nouns that directly express causal relations, and generalizes causal relations into causal rules with multiple variables. 5 Conclusion In this paper, we presented a novel approach for inducing causal rules from the sentences with deverbal nouns. We conducted two experiments, and demonstrated that deverbal n"
C12-2118,P10-1124,0,0.0139676,"e phase of selecting constraints. 4 Related Work The previous work on automatic acquisition of causal knowledge can be categorized into three groups in terms of types of inference rules: noun-noun causality (Girju, 2003; Chang and Choi, 2006; Saeger et al., 2011), verb-verb causality (Lin and Pantel, 2001; Chklovski and Pantel, 2004; Torisawa, 2006; Pantel et al., 2007; Abe et al., 2008; Beamer and Girju, 2009; Do et al., 2011; Hashimoto et al., 2012), and inference rules of other types (e.g., entailment) (Pekar, 2006; Szpektor and Dagan, 2008; Aharon et al., 2010; Schoenmackers et al., 2010; Berant et al., 2010, 2011; Gordon and Schubert, 2011; Berant et al., 2012). However, causal rules extracted by the previous work were limited to those without variables (e.g., lean ⇒ kiss) or those with the same set of variables (e.g., ❳ leaves for ❨ ⇒ ❳ gets to ❨) in the head and body of a rule. In contrast, our work is the first approach that leverages deverbal nouns that directly express causal relations, and generalizes causal relations into causal rules with multiple variables. 5 Conclusion In this paper, we presented a novel approach for inducing causal rules from the sentences with deverbal nouns. We cond"
C12-2118,P11-1062,0,0.0238016,"Missing"
C12-2118,W04-3205,0,0.0439605,"or did not contribute to the baseline method. This is probably because we trained the constraint selector for the proposed method. We observed that the half of rules extracted by the proposed method were judged incorrect. Analyzing these false cases, we found that these errors appeared in the phase of selecting constraints. 4 Related Work The previous work on automatic acquisition of causal knowledge can be categorized into three groups in terms of types of inference rules: noun-noun causality (Girju, 2003; Chang and Choi, 2006; Saeger et al., 2011), verb-verb causality (Lin and Pantel, 2001; Chklovski and Pantel, 2004; Torisawa, 2006; Pantel et al., 2007; Abe et al., 2008; Beamer and Girju, 2009; Do et al., 2011; Hashimoto et al., 2012), and inference rules of other types (e.g., entailment) (Pekar, 2006; Szpektor and Dagan, 2008; Aharon et al., 2010; Schoenmackers et al., 2010; Berant et al., 2010, 2011; Gordon and Schubert, 2011; Berant et al., 2012). However, causal rules extracted by the previous work were limited to those without variables (e.g., lean ⇒ kiss) or those with the same set of variables (e.g., ❳ leaves for ❨ ⇒ ❳ gets to ❨) in the head and body of a rule. In contrast, our work is the first a"
C12-2118,D11-1027,0,0.0577014,"or the proposed method. We observed that the half of rules extracted by the proposed method were judged incorrect. Analyzing these false cases, we found that these errors appeared in the phase of selecting constraints. 4 Related Work The previous work on automatic acquisition of causal knowledge can be categorized into three groups in terms of types of inference rules: noun-noun causality (Girju, 2003; Chang and Choi, 2006; Saeger et al., 2011), verb-verb causality (Lin and Pantel, 2001; Chklovski and Pantel, 2004; Torisawa, 2006; Pantel et al., 2007; Abe et al., 2008; Beamer and Girju, 2009; Do et al., 2011; Hashimoto et al., 2012), and inference rules of other types (e.g., entailment) (Pekar, 2006; Szpektor and Dagan, 2008; Aharon et al., 2010; Schoenmackers et al., 2010; Berant et al., 2010, 2011; Gordon and Schubert, 2011; Berant et al., 2012). However, causal rules extracted by the previous work were limited to those without variables (e.g., lean ⇒ kiss) or those with the same set of variables (e.g., ❳ leaves for ❨ ⇒ ❳ gets to ❨) in the head and body of a rule. In contrast, our work is the first approach that leverages deverbal nouns that directly express causal relations, and generalizes ca"
C12-2118,D11-1142,0,0.0302518,"and “Deverbal-noun ...”. Here, A and B present named entities2 , and “Deverbal-noun ...” denotes a sentence starting with the deverbal noun of verb. We call the former sentence “A verb B ...” a head sentence and the latter sentence “Deverbal-noun ...” a body sentence. We apply several NLP analyses including part-of-speech tagging, dependency parsing, named entity recognition, and coreference resolution for obtaining causal relation instances as dependency trees with variables (Section 2.1). Independently of this process, the system extracts relation instances from the corpus by using ReVerb3 (Fader et al., 2011) (Section 2.2). Searching for candidates of relation instances that can be inserted to a causal rule as a constraint, the system chooses the best relation instance as a constraint (Section 2.3). 2.1 Extracting causal relations using deverbal nouns Using the list of verbs and their deverbal nouns extracted from FrameNet, the proposed system finds documents that contain both verbs and their corresponding deverbal nouns. For example, we extract a document when it contains a verb buy and its deverbal nouns (purchase, acquisition, procurement, etc). We employ Stanford Core NLP4 for fundamental NLP"
C12-2118,W03-1210,0,0.202977,"COLING 2012: Posters, pages 1209–1218, COLING 2012, Mumbai, December 2012. 1209 1 Introduction Performing semantic inference is important for natural language applications such as Question Answering (QA), Information Extraction, and Discourse Analysis. One of the missing links for semantic inference is the availability of commonsense knowledge in computers. In this paper, we focus on acquiring knowledge about causal relations between events. Previous work on causal rule acquisition targeted at simple rules each of whose head is represented by a single literal or n-ary predicate: for example, Girju (2003) collected causal rules between nouns (e.g., hunger ⇒ headache); and Pantel et al. (2007) acquired causal rules between verbs (e.g., ❨ announced the arrest of ❳ ⇒ ❳ is charged by ❨). However, humans perform more complicated inferences to predict outcomes of an event. Let us consider the following example: Google acquires Android Inc. The acquisition will enhance Google’s competition in mobile phones. The first sentence mentions an acquisition event with the verb acquire. Starting with its deverbal noun acquisition, the second sentence describes the possible outcome of the acquisition event. Re"
C12-2118,W11-2408,0,0.0162952,"aints. 4 Related Work The previous work on automatic acquisition of causal knowledge can be categorized into three groups in terms of types of inference rules: noun-noun causality (Girju, 2003; Chang and Choi, 2006; Saeger et al., 2011), verb-verb causality (Lin and Pantel, 2001; Chklovski and Pantel, 2004; Torisawa, 2006; Pantel et al., 2007; Abe et al., 2008; Beamer and Girju, 2009; Do et al., 2011; Hashimoto et al., 2012), and inference rules of other types (e.g., entailment) (Pekar, 2006; Szpektor and Dagan, 2008; Aharon et al., 2010; Schoenmackers et al., 2010; Berant et al., 2010, 2011; Gordon and Schubert, 2011; Berant et al., 2012). However, causal rules extracted by the previous work were limited to those without variables (e.g., lean ⇒ kiss) or those with the same set of variables (e.g., ❳ leaves for ❨ ⇒ ❳ gets to ❨) in the head and body of a rule. In contrast, our work is the first approach that leverages deverbal nouns that directly express causal relations, and generalizes causal relations into causal rules with multiple variables. 5 Conclusion In this paper, we presented a novel approach for inducing causal rules from the sentences with deverbal nouns. We conducted two experiments, and demons"
C12-2118,D12-1057,0,0.130851,"Missing"
C12-2118,N07-1071,0,0.128676,"1 Introduction Performing semantic inference is important for natural language applications such as Question Answering (QA), Information Extraction, and Discourse Analysis. One of the missing links for semantic inference is the availability of commonsense knowledge in computers. In this paper, we focus on acquiring knowledge about causal relations between events. Previous work on causal rule acquisition targeted at simple rules each of whose head is represented by a single literal or n-ary predicate: for example, Girju (2003) collected causal rules between nouns (e.g., hunger ⇒ headache); and Pantel et al. (2007) acquired causal rules between verbs (e.g., ❨ announced the arrest of ❳ ⇒ ❳ is charged by ❨). However, humans perform more complicated inferences to predict outcomes of an event. Let us consider the following example: Google acquires Android Inc. The acquisition will enhance Google’s competition in mobile phones. The first sentence mentions an acquisition event with the verb acquire. Starting with its deverbal noun acquisition, the second sentence describes the possible outcome of the acquisition event. Referring to events explained in the preceding sentences, deverbal nouns often provide good"
C12-2118,N06-1007,0,0.0188848,"judged incorrect. Analyzing these false cases, we found that these errors appeared in the phase of selecting constraints. 4 Related Work The previous work on automatic acquisition of causal knowledge can be categorized into three groups in terms of types of inference rules: noun-noun causality (Girju, 2003; Chang and Choi, 2006; Saeger et al., 2011), verb-verb causality (Lin and Pantel, 2001; Chklovski and Pantel, 2004; Torisawa, 2006; Pantel et al., 2007; Abe et al., 2008; Beamer and Girju, 2009; Do et al., 2011; Hashimoto et al., 2012), and inference rules of other types (e.g., entailment) (Pekar, 2006; Szpektor and Dagan, 2008; Aharon et al., 2010; Schoenmackers et al., 2010; Berant et al., 2010, 2011; Gordon and Schubert, 2011; Berant et al., 2012). However, causal rules extracted by the previous work were limited to those without variables (e.g., lean ⇒ kiss) or those with the same set of variables (e.g., ❳ leaves for ❨ ⇒ ❳ gets to ❨) in the head and body of a rule. In contrast, our work is the first approach that leverages deverbal nouns that directly express causal relations, and generalizes causal relations into causal rules with multiple variables. 5 Conclusion In this paper, we pres"
C12-2118,D11-1076,0,0.0971799,"to produce causal rules (0.1667). The SVM-based constraint selector did not contribute to the baseline method. This is probably because we trained the constraint selector for the proposed method. We observed that the half of rules extracted by the proposed method were judged incorrect. Analyzing these false cases, we found that these errors appeared in the phase of selecting constraints. 4 Related Work The previous work on automatic acquisition of causal knowledge can be categorized into three groups in terms of types of inference rules: noun-noun causality (Girju, 2003; Chang and Choi, 2006; Saeger et al., 2011), verb-verb causality (Lin and Pantel, 2001; Chklovski and Pantel, 2004; Torisawa, 2006; Pantel et al., 2007; Abe et al., 2008; Beamer and Girju, 2009; Do et al., 2011; Hashimoto et al., 2012), and inference rules of other types (e.g., entailment) (Pekar, 2006; Szpektor and Dagan, 2008; Aharon et al., 2010; Schoenmackers et al., 2010; Berant et al., 2010, 2011; Gordon and Schubert, 2011; Berant et al., 2012). However, causal rules extracted by the previous work were limited to those without variables (e.g., lean ⇒ kiss) or those with the same set of variables (e.g., ❳ leaves for ❨ ⇒ ❳ gets to"
C12-2118,D10-1106,0,0.0272693,"these errors appeared in the phase of selecting constraints. 4 Related Work The previous work on automatic acquisition of causal knowledge can be categorized into three groups in terms of types of inference rules: noun-noun causality (Girju, 2003; Chang and Choi, 2006; Saeger et al., 2011), verb-verb causality (Lin and Pantel, 2001; Chklovski and Pantel, 2004; Torisawa, 2006; Pantel et al., 2007; Abe et al., 2008; Beamer and Girju, 2009; Do et al., 2011; Hashimoto et al., 2012), and inference rules of other types (e.g., entailment) (Pekar, 2006; Szpektor and Dagan, 2008; Aharon et al., 2010; Schoenmackers et al., 2010; Berant et al., 2010, 2011; Gordon and Schubert, 2011; Berant et al., 2012). However, causal rules extracted by the previous work were limited to those without variables (e.g., lean ⇒ kiss) or those with the same set of variables (e.g., ❳ leaves for ❨ ⇒ ❳ gets to ❨) in the head and body of a rule. In contrast, our work is the first approach that leverages deverbal nouns that directly express causal relations, and generalizes causal relations into causal rules with multiple variables. 5 Conclusion In this paper, we presented a novel approach for inducing causal rules from the sentences with de"
C12-2118,C08-1107,0,0.0286329,"ect. Analyzing these false cases, we found that these errors appeared in the phase of selecting constraints. 4 Related Work The previous work on automatic acquisition of causal knowledge can be categorized into three groups in terms of types of inference rules: noun-noun causality (Girju, 2003; Chang and Choi, 2006; Saeger et al., 2011), verb-verb causality (Lin and Pantel, 2001; Chklovski and Pantel, 2004; Torisawa, 2006; Pantel et al., 2007; Abe et al., 2008; Beamer and Girju, 2009; Do et al., 2011; Hashimoto et al., 2012), and inference rules of other types (e.g., entailment) (Pekar, 2006; Szpektor and Dagan, 2008; Aharon et al., 2010; Schoenmackers et al., 2010; Berant et al., 2010, 2011; Gordon and Schubert, 2011; Berant et al., 2012). However, causal rules extracted by the previous work were limited to those without variables (e.g., lean ⇒ kiss) or those with the same set of variables (e.g., ❳ leaves for ❨ ⇒ ❳ gets to ❨) in the head and body of a rule. In contrast, our work is the first approach that leverages deverbal nouns that directly express causal relations, and generalizes causal relations into causal rules with multiple variables. 5 Conclusion In this paper, we presented a novel approach for"
C12-2118,N06-1008,0,0.302364,"baseline method. This is probably because we trained the constraint selector for the proposed method. We observed that the half of rules extracted by the proposed method were judged incorrect. Analyzing these false cases, we found that these errors appeared in the phase of selecting constraints. 4 Related Work The previous work on automatic acquisition of causal knowledge can be categorized into three groups in terms of types of inference rules: noun-noun causality (Girju, 2003; Chang and Choi, 2006; Saeger et al., 2011), verb-verb causality (Lin and Pantel, 2001; Chklovski and Pantel, 2004; Torisawa, 2006; Pantel et al., 2007; Abe et al., 2008; Beamer and Girju, 2009; Do et al., 2011; Hashimoto et al., 2012), and inference rules of other types (e.g., entailment) (Pekar, 2006; Szpektor and Dagan, 2008; Aharon et al., 2010; Schoenmackers et al., 2010; Berant et al., 2010, 2011; Gordon and Schubert, 2011; Berant et al., 2012). However, causal rules extracted by the previous work were limited to those without variables (e.g., lean ⇒ kiss) or those with the same set of variables (e.g., ❳ leaves for ❨ ⇒ ❳ gets to ❨) in the head and body of a rule. In contrast, our work is the first approach that lev"
C12-2118,C98-1013,0,\N,Missing
C16-1005,P11-1020,0,0.284446,"Missing"
C16-1005,W14-3348,0,0.0282314,"Missing"
C16-1005,W04-1013,0,0.0294467,"Missing"
C16-1005,D15-1166,0,0.0855569,"Missing"
C16-1005,P02-1040,0,0.095693,"Missing"
C16-1005,N15-1173,0,0.224094,"Missing"
C16-1184,A00-2004,0,0.0251382,"throughout this study, we use the term segment to refer to a discourse segment in lyrics. 2 Related work This section reviews related work into the discourse structure of lyrics, with particular focus on the segmentation of lyrics using repeated patterns. Text segmentation is a classic text retrieval problem, and there exists a rich body of research into text segmentation in natural language processing. Various linguistic cues have been suggested to identify text boundaries such as expressions that frequently appear at the end of segments (Beeferman et al., 1999), contextual/topical changes (Choi, 2000; Malioutov and Barzilay, 2006; Riedl and Biemann, 2012), 1960 and word/entity repetition (Kan et al., 1998; Reynar, 1999). Although we share the same motivation as these studies, these text segmentation methods do not consider repeated patterns of phrasal segments because this type of repetition is nearly always absent in prose text. On the other hand, segments in lyrics often have repetitions (Austin et al., 2010) as shown in Section 3.1. We aim to capture the segment structure of lyrics using repeated patterns. Previous computational work into lyrics segmentation has focused on identifying"
C16-1184,W98-1123,0,0.213989,"work This section reviews related work into the discourse structure of lyrics, with particular focus on the segmentation of lyrics using repeated patterns. Text segmentation is a classic text retrieval problem, and there exists a rich body of research into text segmentation in natural language processing. Various linguistic cues have been suggested to identify text boundaries such as expressions that frequently appear at the end of segments (Beeferman et al., 1999), contextual/topical changes (Choi, 2000; Malioutov and Barzilay, 2006; Riedl and Biemann, 2012), 1960 and word/entity repetition (Kan et al., 1998; Reynar, 1999). Although we share the same motivation as these studies, these text segmentation methods do not consider repeated patterns of phrasal segments because this type of repetition is nearly always absent in prose text. On the other hand, segments in lyrics often have repetitions (Austin et al., 2010) as shown in Section 3.1. We aim to capture the segment structure of lyrics using repeated patterns. Previous computational work into lyrics segmentation has focused on identifying the segment labels of lyrics that are already segmented. For example, the structure of lyrics can be repres"
C16-1184,P06-1004,0,0.0386521,"this study, we use the term segment to refer to a discourse segment in lyrics. 2 Related work This section reviews related work into the discourse structure of lyrics, with particular focus on the segmentation of lyrics using repeated patterns. Text segmentation is a classic text retrieval problem, and there exists a rich body of research into text segmentation in natural language processing. Various linguistic cues have been suggested to identify text boundaries such as expressions that frequently appear at the end of segments (Beeferman et al., 1999), contextual/topical changes (Choi, 2000; Malioutov and Barzilay, 2006; Riedl and Biemann, 2012), 1960 and word/entity repetition (Kan et al., 1998; Reynar, 1999). Although we share the same motivation as these studies, these text segmentation methods do not consider repeated patterns of phrasal segments because this type of repetition is nearly always absent in prose text. On the other hand, segments in lyrics often have repetitions (Austin et al., 2010) as shown in Section 3.1. We aim to capture the segment structure of lyrics using repeated patterns. Previous computational work into lyrics segmentation has focused on identifying the segment labels of lyrics t"
C16-1184,J02-1002,0,0.0380659,"ther. 5.1 Performance evaluation metrics We used two sets of metrics to evaluate the performance of each model for the task. One was standardly used in audio music segmentation, i.e., the precision, recall, and F-measure of identifying segment boundaries. Precision is the ratio of correctly predicted boundaries over all predicted boundaries, recall is the ratio of correctly predicted boundaries over all true boundaries, and F-measure is the harmonic mean of precision and recall. The other set was standardly used in text segmentation literature: Pk (Beeferman et al., 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002). Pk is the probability of segmentation error that evaluates whether two lines li and lj in lyrics fewer than k lines apart are incorrectly concatenated or divided by a segmentation model. Pk is considered a more suitable measure than F-measure in text segmentation because it assigns partial credit to nearly correct estimations. WD is a variant of Pk that resolves a problem of Pk by penalizing false positives. We set the window size k of Pk and WD to 1965 1 2 1: I think of you 2: in the spring when gentle rains turn to showers 3: I think of you 4: when the summer breezes dance through the flow"
C16-1184,P99-1046,0,0.120921,"reviews related work into the discourse structure of lyrics, with particular focus on the segmentation of lyrics using repeated patterns. Text segmentation is a classic text retrieval problem, and there exists a rich body of research into text segmentation in natural language processing. Various linguistic cues have been suggested to identify text boundaries such as expressions that frequently appear at the end of segments (Beeferman et al., 1999), contextual/topical changes (Choi, 2000; Malioutov and Barzilay, 2006; Riedl and Biemann, 2012), 1960 and word/entity repetition (Kan et al., 1998; Reynar, 1999). Although we share the same motivation as these studies, these text segmentation methods do not consider repeated patterns of phrasal segments because this type of repetition is nearly always absent in prose text. On the other hand, segments in lyrics often have repetitions (Austin et al., 2010) as shown in Section 3.1. We aim to capture the segment structure of lyrics using repeated patterns. Previous computational work into lyrics segmentation has focused on identifying the segment labels of lyrics that are already segmented. For example, the structure of lyrics can be represented Figure 2:"
C16-1184,W12-3307,0,0.0779148,"gment to refer to a discourse segment in lyrics. 2 Related work This section reviews related work into the discourse structure of lyrics, with particular focus on the segmentation of lyrics using repeated patterns. Text segmentation is a classic text retrieval problem, and there exists a rich body of research into text segmentation in natural language processing. Various linguistic cues have been suggested to identify text boundaries such as expressions that frequently appear at the end of segments (Beeferman et al., 1999), contextual/topical changes (Choi, 2000; Malioutov and Barzilay, 2006; Riedl and Biemann, 2012), 1960 and word/entity repetition (Kan et al., 1998; Reynar, 1999). Although we share the same motivation as these studies, these text segmentation methods do not consider repeated patterns of phrasal segments because this type of repetition is nearly always absent in prose text. On the other hand, segments in lyrics often have repetitions (Austin et al., 2010) as shown in Section 3.1. We aim to capture the segment structure of lyrics using repeated patterns. Previous computational work into lyrics segmentation has focused on identifying the segment labels of lyrics that are already segmented."
C16-1184,N03-1033,0,0.0409146,"Missing"
C16-1184,Y14-1049,1,0.770092,"of discourse segments in lyrics. 1 Introduction Lyrics are an important element of popular music. They provide an effective means to express the message and emotion of music. Similar to prose text, lyrics are a discourse: i.e., they are, typically, a sequence of related lines, rather than an unconnected stream of lines of arbitrary order. Thus, like texts, lyrics also have a discourse structure consisting discourse segments. Each discourse segment exhibits an individual topic in discourse, and the transition of topics over successive discourse segments constitutes a flow (Austin et al., 2010; Watanabe et al., 2014). Unlike prose text, lyrics have their own peculiar properties, such as frequent repetition of identical or similar phrases and extensive use of rhyme and refrain (Austin et al., 2010), as illustrated in Figure 1. Analogous to prose text, the lyrics in Figure 1 can be viewed as a sequence of discourse segments, wherein each segment is depicted by a colored box. 1 and  3 appear repeatedly (e.g., segments  4 and  8 are identical to segment ), 1 Segments  which is not typically observed in prose text. Our goal is to reveal the discourse structure of lyrics in popular music by quantitatively"
C16-1266,P09-1068,0,0.0361461,"ra et al., 2014) estimate the plausibility of a subject–verb–object (SVO) tuple. These studies model a type of CSP: a subject or an object can be regarded as an additional context to restrict a set of possible fillers of a query predicate. However, the context captured in our study is not a local context of a query predicate but that of a query argument, working as a validator of the narrative consistency between a query predicate and events in which a query argument participates (see Section 4). In addition, the modeling of a narrative consistency between events has been studied extensively (Chambers and Jurafsky, 2009; Modi and Titov, 2014; Granroth-wilding and Clark, 2016, etc.). Chambers and Jurafsky (2009) acquired sets of narratively related events sharing at least one entity (e.g., {X commit a crime, police arrest X, X convict, ...}) by collecting a set of verbal mentions sharing coreferring arguments in a large corpus. The relatedness between two events was then estimated statistically through pointwise mutual information (Church and Hanks, 1990). To address the data sparseness problem of Chambers and Jurafsky (2009), Granroth-wilding and Clark (2016) proposed an architecture based on distributed rep"
C16-1266,J81-4005,0,0.754299,"Missing"
C16-1266,P07-1028,0,0.244953,"fundamental approach to modeling SP is to count the co-occurrences of predicates and their arguments on a large corpus. As simply counting a predicate-argument pair causes data sparseness problem, previous SP models adopted methods for smoothing co-occurrence counts. Earlier efforts combined a manually crafted thesaurus with the acquired distribution (Resnik, 1996; Li and Abe, 1998). Another approach used a latent probabilistic model to obtain a semantically smoothed probability distribution (Rooth et al., 1999; S´eaghdha and Korhonen, 2014). Other directions include example-based approaches (Erk, 2007). However, these studies differ from ours in that they do not consider the context-sensitivity. Some previous studies (Ritter et al., 2010; Van de Cruys, 2014; Kawahara et al., 2014) estimate the plausibility of a subject–verb–object (SVO) tuple. These studies model a type of CSP: a subject or an object can be regarded as an additional context to restrict a set of possible fillers of a query predicate. However, the context captured in our study is not a local context of a query predicate but that of a query argument, working as a validator of the narrative consistency between a query predicate"
C16-1266,D15-1038,0,0.0148855,"s candidates for antecedents. As he(?) is a subject of the predicate made a statement, sc(⟨cs , v, o⟩) gives the preference of cs as an antecedent of the pronoun, where v = made and o = statement. In the example, cs can be a preceding noun with its context attached (if any) or without its context: ⟨Chen, declare, determination⟩subj , ⟨Chen, declare, determination⟩obj , speech, or people. We expect that an SP model prefers the correct antecedent ⟨Chen, declare, determination⟩subj over the others. We measure the ability of a model for selecting the correct cluster by using a mean quantile (MQ) (Guu et al., 2015). Let p be the target pronoun, Cp+ the correct cluster for the pronoun, and Np the set of negative (incorrect) clusters. MQ for the pronoun p is defined as: MQ(p) = |{C − ∈ Np |sp(C − , p) &lt; sp(Cp+ , p)}| . |Np | (10) Intuitively, MQ(p) represents the ratio where a correct cluster Cp+ is preferred to incorrect clusters C − ∈ Np by the model. In general, a cluster contains multiple mentions; thus, we simply consider the maximal of the scores in a cluster C: for example, sp(C, p) = maxm∈C sc(m, v, o). 7 To ensure that the two subsets are strictly disjoint, we prohibited a test instance from bein"
C16-1266,D14-1163,0,0.248412,"y not. The challenges in modeling a CSP are as follows: (i) data sparseness caused by the incorporation of context words, and (ii) an effective means of incorporating context-sensitivity into SP. To address these issues, we propose to extend the state-of-the-art SP model by using a distributed representation (Van de Cruys, 2014). The distributed framework alleviates the data sparseness problem and naturally injects the contextual information of a query argument into its word vector based on compositional distributional semantics (Socher et al., 2012; Socher et al., 2013; Muraoka et al., 2014; Hashimoto et al., 2014, etc.). We empirically evaluate the impacts of incorporating context-sensitivity into SP for two tasks: (i) context-sensitive pseudo-disambiguation, a novel benchmark tailored for evaluating CSP models, and (ii) coreference cluster ranking for pronominal anaphora resolution. The results demonstrate that our approach achieves considerable improvements. Moreover, the results suggest that CSP is a meaningful problem setting and that our model captures the context-sensitivity of SP. 2 Related Work A fundamental approach to modeling SP is to count the co-occurrences of predicates and their argumen"
C16-1266,N06-2015,0,0.23662,"uples; B hereafter) and 4,824,394 T YPE A positive instances (1,500,990 unique tuples; A hereafter). 6 Evaluation To check whether the CSP model can properly learn the conventional SP and narrative consistency, we first evaluated the CSP model against Van de Cruys’ model by using a pseudo-disambiguation test, a binary classification task of discriminating a positive SVO tuple from its pseudo-negative counterpart. We then evaluated the effectiveness of the CSP model in a realistic problem setting, in which the disambiguation test is created from coreference annotations of the OntoNotes corpus (Hovy et al., 2006). 6.1 Parameters We set the dimension of word embedding d = 50 and the dimension of hidden layer h = 50. The word embeddings are initialized with the publicly available word vectors trained through GloVe (Pennington et al., 2014)5 and updated through back propagation. We updated weights by using Adam (Kingma and Ba, 2014) with a mini-batch size of 1,000 and 30 epochs6 . To evaluate the effectiveness of the CSP model, we replicated the SVO model of Van de Cruys (2014) by training the CSP model only with T YPE A instances (henceforth, SP). 6.2 Pseudo-disambiguation test Inspired by the conventio"
C16-1266,C12-1079,1,0.824571,"ord vector, which is explained in the rest of this section. Similarly, for ⟨cs , v, o⟩ and ⟨cs , v, co ⟩, we extend Equation (3) as follows: gc′ s ,v,o = f (ϕc (cs ) ⊕ ϕ(v) ⊕ ϕ(o)), and gc′ s ,v,co = f (ϕc (cs ) ⊕ ϕ(v) ⊕ ϕc (co )). As a context of w, we can potentially consider various types of modifiers, such as predicates, adverbs, appositives, and genitives, that affects the preference score of a query argument. In this study, as a first step, we restrict the context information to PASs along the lines of the previous studies that utilize event-to-event relations in an anaphora resolution (Inoue et al., 2012; Peng et al., 2015). Specifically, we first assume a context of a query argument be a single PAS which takes the query argument as its argument (referred to as a context-PAS). Then we represent w with its context-PAS as cw = ⟨sw , pw , ow ⟩r , where sw and ow are respectively the subject and object of a predicate pw that syntactically governs the word w (i.e., either sw or ow is w). r indicates the grammatical role of the query argument w in the context-PAS, whose value is either subj (when w = sw ) or obj (when w = ow ). For example, for text (4), the context for the query object Bob, modifi"
C16-1266,P14-1097,0,0.0213887,"s data sparseness problem, previous SP models adopted methods for smoothing co-occurrence counts. Earlier efforts combined a manually crafted thesaurus with the acquired distribution (Resnik, 1996; Li and Abe, 1998). Another approach used a latent probabilistic model to obtain a semantically smoothed probability distribution (Rooth et al., 1999; S´eaghdha and Korhonen, 2014). Other directions include example-based approaches (Erk, 2007). However, these studies differ from ours in that they do not consider the context-sensitivity. Some previous studies (Ritter et al., 2010; Van de Cruys, 2014; Kawahara et al., 2014) estimate the plausibility of a subject–verb–object (SVO) tuple. These studies model a type of CSP: a subject or an object can be regarded as an additional context to restrict a set of possible fillers of a query predicate. However, the context captured in our study is not a local context of a query predicate but that of a query argument, working as a validator of the narrative consistency between a query predicate and events in which a query argument participates (see Section 4). In addition, the modeling of a narrative consistency between events has been studied extensively (Chambers and Jur"
C16-1266,J98-2002,0,0.0307096,"ution. The results demonstrate that our approach achieves considerable improvements. Moreover, the results suggest that CSP is a meaningful problem setting and that our model captures the context-sensitivity of SP. 2 Related Work A fundamental approach to modeling SP is to count the co-occurrences of predicates and their arguments on a large corpus. As simply counting a predicate-argument pair causes data sparseness problem, previous SP models adopted methods for smoothing co-occurrence counts. Earlier efforts combined a manually crafted thesaurus with the acquired distribution (Resnik, 1996; Li and Abe, 1998). Another approach used a latent probabilistic model to obtain a semantically smoothed probability distribution (Rooth et al., 1999; S´eaghdha and Korhonen, 2014). Other directions include example-based approaches (Erk, 2007). However, these studies differ from ours in that they do not consider the context-sensitivity. Some previous studies (Ritter et al., 2010; Van de Cruys, 2014; Kawahara et al., 2014) estimate the plausibility of a subject–verb–object (SVO) tuple. These studies model a type of CSP: a subject or an object can be regarded as an additional context to restrict a set of possible"
C16-1266,P14-5010,0,0.00333912,"secret, hurt, someone⟩ from B1 and B2 , respectively. To generate T YPE A negative instance, we follow the same procedure described in Section 3 but use probabilistic negative sampling instead. 2 Although Van de Cruys (2014) used random sampling to generate negative instances, we used probabilistic sampling because our preliminary experiment shows a better performance. 2833 5.3 Dataset We identified syntactic dependency relations and coreference relations in 4.5 billion sentences extracted from the ClueWeb12 corpus3 , that is, a large collection of Web documents, by applying Stanford CoreNLP (Manning et al., 2014). To reduce noises from the obtained coreference relations, we applied the following heuristics to skim only highly plausible coreference relations off from the pool: (i) the coreference relation must be intrasentential, (ii) the head words of the coreferent mentions must be identical and nonpronominal (e.g. John–John but not John–boy)4 . Furthermore, we discarded T YPE A and T YPE B instances containing low-frequency words so that all our training instances include only the top 50k frequent verbs, 50k frequent nouns, and 50k frequent adjectives. We replaced all the rare words (occurring less"
C16-1266,W14-1606,0,0.11654,"plausibility of a subject–verb–object (SVO) tuple. These studies model a type of CSP: a subject or an object can be regarded as an additional context to restrict a set of possible fillers of a query predicate. However, the context captured in our study is not a local context of a query predicate but that of a query argument, working as a validator of the narrative consistency between a query predicate and events in which a query argument participates (see Section 4). In addition, the modeling of a narrative consistency between events has been studied extensively (Chambers and Jurafsky, 2009; Modi and Titov, 2014; Granroth-wilding and Clark, 2016, etc.). Chambers and Jurafsky (2009) acquired sets of narratively related events sharing at least one entity (e.g., {X commit a crime, police arrest X, X convict, ...}) by collecting a set of verbal mentions sharing coreferring arguments in a large corpus. The relatedness between two events was then estimated statistically through pointwise mutual information (Church and Hanks, 1990). To address the data sparseness problem of Chambers and Jurafsky (2009), Granroth-wilding and Clark (2016) proposed an architecture based on distributed representation to judge t"
C16-1266,Y14-1010,1,0.929498,"ut other candidates may not. The challenges in modeling a CSP are as follows: (i) data sparseness caused by the incorporation of context words, and (ii) an effective means of incorporating context-sensitivity into SP. To address these issues, we propose to extend the state-of-the-art SP model by using a distributed representation (Van de Cruys, 2014). The distributed framework alleviates the data sparseness problem and naturally injects the contextual information of a query argument into its word vector based on compositional distributional semantics (Socher et al., 2012; Socher et al., 2013; Muraoka et al., 2014; Hashimoto et al., 2014, etc.). We empirically evaluate the impacts of incorporating context-sensitivity into SP for two tasks: (i) context-sensitive pseudo-disambiguation, a novel benchmark tailored for evaluating CSP models, and (ii) coreference cluster ranking for pronominal anaphora resolution. The results demonstrate that our approach achieves considerable improvements. Moreover, the results suggest that CSP is a meaningful problem setting and that our model captures the context-sensitivity of SP. 2 Related Work A fundamental approach to modeling SP is to count the co-occurrences of pred"
C16-1266,N15-1082,0,0.0834339,"explained in the rest of this section. Similarly, for ⟨cs , v, o⟩ and ⟨cs , v, co ⟩, we extend Equation (3) as follows: gc′ s ,v,o = f (ϕc (cs ) ⊕ ϕ(v) ⊕ ϕ(o)), and gc′ s ,v,co = f (ϕc (cs ) ⊕ ϕ(v) ⊕ ϕc (co )). As a context of w, we can potentially consider various types of modifiers, such as predicates, adverbs, appositives, and genitives, that affects the preference score of a query argument. In this study, as a first step, we restrict the context information to PASs along the lines of the previous studies that utilize event-to-event relations in an anaphora resolution (Inoue et al., 2012; Peng et al., 2015). Specifically, we first assume a context of a query argument be a single PAS which takes the query argument as its argument (referred to as a context-PAS). Then we represent w with its context-PAS as cw = ⟨sw , pw , ow ⟩r , where sw and ow are respectively the subject and object of a predicate pw that syntactically governs the word w (i.e., either sw or ow is w). r indicates the grammatical role of the query argument w in the context-PAS, whose value is either subj (when w = sw ) or obj (when w = ow ). For example, for text (4), the context for the query object Bob, modified by the PAS of the"
C16-1266,D14-1162,0,0.0798895,"luated the CSP model against Van de Cruys’ model by using a pseudo-disambiguation test, a binary classification task of discriminating a positive SVO tuple from its pseudo-negative counterpart. We then evaluated the effectiveness of the CSP model in a realistic problem setting, in which the disambiguation test is created from coreference annotations of the OntoNotes corpus (Hovy et al., 2006). 6.1 Parameters We set the dimension of word embedding d = 50 and the dimension of hidden layer h = 50. The word embeddings are initialized with the publicly available word vectors trained through GloVe (Pennington et al., 2014)5 and updated through back propagation. We updated weights by using Adam (Kingma and Ba, 2014) with a mini-batch size of 1,000 and 30 epochs6 . To evaluate the effectiveness of the CSP model, we replicated the SVO model of Van de Cruys (2014) by training the CSP model only with T YPE A instances (henceforth, SP). 6.2 Pseudo-disambiguation test Inspired by the conventional SP model (Erk, 2007; Van de Cruys, 2014, etc.), we set up three binary classification tasks. We performed hold-out validation on datasets A and B. 6.2.1 Tasks Pseudo-disambiguation (PD) discriminates a positive non-context-in"
C16-1266,P10-1044,0,0.131649,"Missing"
C16-1266,P99-1014,0,0.812898,"phrase as an argument. For example, the object slot of eat is generally filled by a noun phrase denoting food such as an apple; it is rarely filled by a phrase that is not food such as a watch. As the knowledge of SP has been recognized as key for many natural language processing tasks, including semantic role labeling and anaphora resolution, automatic acquisition of SP knowledge has persisted as a popular research topic. In literature, a variety of computational models for SP have been proposed, ranging from thesaurus-based approaches (Resnik, 1996), to probabilistic latent variable models (Rooth et al., 1999; S´eaghdha and Korhonen, 2014), and distributed approaches (Van de Cruys, 2014). Conventionally, SP is defined as the context-independent acceptability of a word as a filler of a predicate in the sense of a semantic type. Suppose that we must identify the referent of him(j) : (1) John(i) beat Bob(j) . Mary comforted him(j) . Henceforth, we call a predicate (e.g., comfort) and an argument (e.g., John and Bob) to be examined as a query predicate and query argument, respectively. Conventional SP models judge the appropriateness of John(i) and Bob(j) in terms of whether comfort can take each noun"
C16-1266,J14-3005,0,0.0513825,"Missing"
C16-1266,D12-1110,0,0.672745,"ntexts relevant to narrative consistency but other candidates may not. The challenges in modeling a CSP are as follows: (i) data sparseness caused by the incorporation of context words, and (ii) an effective means of incorporating context-sensitivity into SP. To address these issues, we propose to extend the state-of-the-art SP model by using a distributed representation (Van de Cruys, 2014). The distributed framework alleviates the data sparseness problem and naturally injects the contextual information of a query argument into its word vector based on compositional distributional semantics (Socher et al., 2012; Socher et al., 2013; Muraoka et al., 2014; Hashimoto et al., 2014, etc.). We empirically evaluate the impacts of incorporating context-sensitivity into SP for two tasks: (i) context-sensitive pseudo-disambiguation, a novel benchmark tailored for evaluating CSP models, and (ii) coreference cluster ranking for pronominal anaphora resolution. The results demonstrate that our approach achieves considerable improvements. Moreover, the results suggest that CSP is a meaningful problem setting and that our model captures the context-sensitivity of SP. 2 Related Work A fundamental approach to modelin"
C16-1266,P13-1045,0,0.254546,"rrative consistency but other candidates may not. The challenges in modeling a CSP are as follows: (i) data sparseness caused by the incorporation of context words, and (ii) an effective means of incorporating context-sensitivity into SP. To address these issues, we propose to extend the state-of-the-art SP model by using a distributed representation (Van de Cruys, 2014). The distributed framework alleviates the data sparseness problem and naturally injects the contextual information of a query argument into its word vector based on compositional distributional semantics (Socher et al., 2012; Socher et al., 2013; Muraoka et al., 2014; Hashimoto et al., 2014, etc.). We empirically evaluate the impacts of incorporating context-sensitivity into SP for two tasks: (i) context-sensitive pseudo-disambiguation, a novel benchmark tailored for evaluating CSP models, and (ii) coreference cluster ranking for pronominal anaphora resolution. The results demonstrate that our approach achieves considerable improvements. Moreover, the results suggest that CSP is a meaningful problem setting and that our model captures the context-sensitivity of SP. 2 Related Work A fundamental approach to modeling SP is to count the"
C16-1266,D14-1004,0,0.303167,"Missing"
C16-1266,N16-1114,0,0.0603717,"Missing"
C16-1266,J90-1003,0,\N,Missing
C18-1286,N16-1162,0,0.0132384,"y considering their posts. Therefore, our method is useful for analyzing opinions, including those of the silent majority. However, the performance of the stance detection in this work is not yet sufficient. To improve this performance, there are several points that need to be considered. First, even though features based on 3388 n-gram and dependencies are currently used as features of users’ posts, it may be possible to replace these with distributed representations. In recent years, multiple researchers have tried to build distributed representations of sentences (Logeswaran and Lee, 2018; Hill et al., 2016). These methods are known to demonstrate high performance in many tasks. By incorporating these methods into our proposed method, we will be able to express users’ posts more compactly, and this is expected to improve the accuracy of predicting users’ stances. In addition, FMs used in this work are advantageous because various types of features can be used simultaneously in an input matrix. Therefore, it is expected that social media specific features such as users’ follow-follower relationships, retweet relationships, or profile could be useful information for predicting stances. Moreover, it"
C18-1286,W04-3230,0,0.0266527,"of completing missing stances. Each cell presents the accuracy. “Numbers of stances stated” indicates the condition for evaluation. For example, as to ≥ 5, we evaluate the performance for completing missing stances for users who declared their stances toward no less than five topics. Note that, this treatment is applied only for the test portion of the cross validation. each topic, these patterns are strong clues and it is thought that a proper evaluation could not be performed if they were included. We perform preprocessing on target tweets using the Japanese dependency parsing tool CaboCha (Kudo et al., 2004)4 . We then extracted features about uni-grams, bi-grams and features based on dependencies from posts of each user. Table 1 shows examples of extracted features. Here, we choose only adnominal, adjective → noun phrase, noun phrase → adjective, and noun phrase → verb as features based on dependencies. Verb → noun phrase and triplet such as (subject, predicate, object) are not suitable as features because the number of users using each feature is very small and they become sparse. 4 4.1 Evaluation Completing missing stances The first experiment examines how well FMs complete missing stances of"
C18-1286,S16-1064,0,0.0309518,"Missing"
C18-1286,S16-1003,0,0.180372,"edia data. The experimental results demonstrate that users’ posts are useful to model topic preferences and therefore predict stances of silent users. 1 Introduction Web and social media provide platforms to express, discuss, and shape opinions about events and issues in the real world. However, these platforms suffer from new emerging problems such as filter bubble (Pariser, 2011), echo chamber (Jamieson and Cappella, 2008), and fake news (Lazer et al., 2018). An important step to analyze the argumentation structure of social media and to assist in healthy decisionmaking is stance detection (Mohammad et al., 2016): predicting whether a given text/user is in favor (agree), against (disagree), or neutral toward a target topic (e.g., Donald Trump). Unfortunately, users rarely make an explicit statement about a topic. For example, computers may easily detect a stance for the topic of the Trans-Pacific Partnership (TPP) from the sentence, “I totally disagree with TPP,” containing the topic word ‘TPP’ with the explicit linguistic pattern “I totally disagree with ...” However, many social media posts may not refer to a topic but only to its related topics, for example, “We should protect Japanese agriculture."
C18-1286,C10-2100,0,0.0268129,"election (Hong and Nadler, 2012). However, few studies have addressed the silent majority (who hardly expresses a stance), limiting the target to users who explicitly expressed their opinions (Gayo-Avello, 2012). In our work, we shed light on the silent majority as well as vocal users (who express stances explicitly). We aim to predict stances of the silent majority by modeling the topic preferences of users with explicit stance statements and ordinary posts. Stance detection has been extensively studied in recent years as a task to predict a stance of a user or text toward a specific topic (Murakami and Raymond, 2010; Mohammad et al., 2016; Persing and Ng, 2016). However, most studies have depended heavily on labeled training data (Tutek et al., 2016; Liu et al., 2016). Therefore, these methods had difficulty in predicting a stance toward an unseen topic. Accordingly, the SemEval-2016 task 6B released unlabeled data on a topic to be predicted and labeled data on other topics (Mohammad et al., 2016). However, the accuracy on the setting dropped drastically compared with the setting when labeled data for the topic are given. One reason for the relatively low accuracy is that, if no labeled data are availabl"
C18-1286,P16-1205,0,0.0239929,"dies have addressed the silent majority (who hardly expresses a stance), limiting the target to users who explicitly expressed their opinions (Gayo-Avello, 2012). In our work, we shed light on the silent majority as well as vocal users (who express stances explicitly). We aim to predict stances of the silent majority by modeling the topic preferences of users with explicit stance statements and ordinary posts. Stance detection has been extensively studied in recent years as a task to predict a stance of a user or text toward a specific topic (Murakami and Raymond, 2010; Mohammad et al., 2016; Persing and Ng, 2016). However, most studies have depended heavily on labeled training data (Tutek et al., 2016; Liu et al., 2016). Therefore, these methods had difficulty in predicting a stance toward an unseen topic. Accordingly, the SemEval-2016 task 6B released unlabeled data on a topic to be predicted and labeled data on other topics (Mohammad et al., 2016). However, the accuracy on the setting dropped drastically compared with the setting when labeled data for the topic are given. One reason for the relatively low accuracy is that, if no labeled data are available for a new topic, it is impossible to learn e"
C18-1286,P17-1037,1,0.929526,"unseen topic. Accordingly, the SemEval-2016 task 6B released unlabeled data on a topic to be predicted and labeled data on other topics (Mohammad et al., 2016). However, the accuracy on the setting dropped drastically compared with the setting when labeled data for the topic are given. One reason for the relatively low accuracy is that, if no labeled data are available for a new topic, it is impossible to learn expressions that may imply stances for that topic. One way to overcome this problem is to incorporate external knowledge about the topics. The most relevant work is our previous work (Sasaki et al., 2017). Main goal of this work was to acquire knowledge such as “those who agree with the TPP also agree with free trade.” We called such knowledge “inter-topic preferences.” We used texts on Twitter and modeled inter-topic preferences using matrix factorization. As a side effect of this modeling, we were able to predict the missing stances of users by considering users’ explicit stances toward other topics. However, the method of this previous work cannot be applied to the silent majority, who does not explicitly express any stances. Several studies have focused on the silent majority and lurkers."
C18-1286,S16-1075,0,0.0612242,"Missing"
D08-1047,H05-1120,0,0.606315,"p yoshimasa.tsuruoka@manchester.ac.uk Sophia Ananiadou‡ Jun’ichi Tsujii†‡ sophia.ananiadou@manchester.ac.uk tsujii@is.s.u-tokyo.ac.jp † Graduate School of Information Science and Technology University of Tokyo 7-3-1 Hongo, Bunkyo-ku Tokyo 113-8656, Japan ‡ School of Computer Science, University of Manchester National Centre for Text Mining (NaCTeM) Manchester Interdisciplinary Biocentre 131 Princess Street, Manchester M1 7DN, UK Abstract matching (Navarro, 2001), and duplicate record detection (Bilenko and Mooney, 2003). Recent studies have formalized the task in the discriminative framework (Ahmad and Kondrak, 2005; Li et al., 2006; Chen et al., 2007), String transformation, which maps a source string s into its desirable form t∗ , is related to various applications including stemming, lemmatization, and spelling correction. The essential and important step for string transformation is to generate candidates to which the given string s is likely to be transformed. This paper presents a discriminative approach for generating candidate strings. We use substring substitution rules as features and score them using an L1 -regularized logistic regression model. We also propose a procedure to generate negative"
D08-1047,I08-1007,0,0.0973065,"native framework of string similarity. MaCallum et al. (2005) proposed a method to train the costs of edit operations using Conditional Random Fields (CRFs). Bergsma and Kondrak (2007) correct comparative and superlative adjectives, e.g., unpopular → unpopularer → unpopularest and refundable → refundabler → refundablest. Therefore, we removed inflection entries for comparative and superlative adjectives from the dataset. presented an alignment-based discriminative string similarity. They extracted features from substring pairs that are consistent to a character-based alignment of two strings. Aramaki et al. (2008) also used features that express the different segments of the two strings. However, these studies are not suited for a candidate generator because the processes of string transformations are intractable in their discriminative models. Dalianis and Jongejan (2006) presented a lemmatiser based on suffix rules. Although they proposed a method to obtain suffix rules from a training data, the method did not use counter-examples (negatives) for reducing incorrect string transformations. Tsuruoka et al. (2008) proposed a scoring method for discovering a list of normalization rules for dictionary loo"
D08-1047,J96-1002,0,0.0553682,"be enumerated by an efficient algorithm because the processes of string transformation are tractable in the model. We demonstrate the remarkable performance of the proposed method in normalizing inflected words and spelling variations. 1 t∗ = argmax P (t|s). (1) t∈gen(s) Here, the candidate generator gen(s) enumerates candidates of destination (correct) strings, and the scorer P (t|s) denotes the conditional probability of the string t for the given s. The scorer was modeled by a noisy-channel model (Shannon, 1948; Brill and Moore, 2000; Ahmad and Kondrak, 2005) and maximum entropy framework (Berger et al., 1996; Li et al., 2006; Chen et al., 2007). The candidate generator gen(s) also affects the accuracy of the string transformation. Previous studies of spelling correction mostly defined gen(s), gen(s) = {t |dist(s, t) &lt; δ}. Introduction String transformation maps a source string s into its destination string t∗ . In the broad sense, string transformation can include labeling tasks such as partof-speech tagging and shallow parsing (Brill, 1995). However, this study addresses string transformation in its narrow sense, in which a part of a source string is rewritten with a substring. Typical applicati"
D08-1047,P07-1083,0,0.0802925,"ormance of the L1 regularized logistic regression as a discriminative model, we also built two classifiers based on the Support Vector Machine (SVM). These SVM classifiers were implemented by the SVMperf 7 on a linear kernel8 . An SVM classifier employs the same feature set (substitution rules) as the proposed method so that we can directly compare the L1 regularized logistic regression and the linear-kernel SVM. Another SVM classifier incorporates the five string metrics; this system can be considered as our reproduction of the discriminative string similarity proposed by Bergsma and Kondrak (2007). Table 3 reports the precision (P), recall (R), and F1 score (F1) based on the number of correct decisions for positive instances. The proposed method outperformed the baseline systems, achieving 0.919, 0.888, and 0.984 of F1 scores, respectively. Porter’s stemmer worked on the Inflection set, but not on the Orthography set, which is beyond the scope of the stemming algorithms. CST’s lemmatizer suffered from low recall on the Inflection set because it removed suffixes of base forms, e.g., (cloning, clone) → (clone, clo). Morpha and CST’s lemma6 We used CST’s lemmatiser version 2.13: http://ww"
D08-1047,P00-1037,0,0.103699,"ary of the model. The advantage of this approach is that candidate strings can be enumerated by an efficient algorithm because the processes of string transformation are tractable in the model. We demonstrate the remarkable performance of the proposed method in normalizing inflected words and spelling variations. 1 t∗ = argmax P (t|s). (1) t∈gen(s) Here, the candidate generator gen(s) enumerates candidates of destination (correct) strings, and the scorer P (t|s) denotes the conditional probability of the string t for the given s. The scorer was modeled by a noisy-channel model (Shannon, 1948; Brill and Moore, 2000; Ahmad and Kondrak, 2005) and maximum entropy framework (Berger et al., 1996; Li et al., 2006; Chen et al., 2007). The candidate generator gen(s) also affects the accuracy of the string transformation. Previous studies of spelling correction mostly defined gen(s), gen(s) = {t |dist(s, t) &lt; δ}. Introduction String transformation maps a source string s into its destination string t∗ . In the broad sense, string transformation can include labeling tasks such as partof-speech tagging and shallow parsing (Brill, 1995). However, this study addresses string transformation in its narrow sense, in whi"
D08-1047,J95-4004,0,0.0169888,"iven s. The scorer was modeled by a noisy-channel model (Shannon, 1948; Brill and Moore, 2000; Ahmad and Kondrak, 2005) and maximum entropy framework (Berger et al., 1996; Li et al., 2006; Chen et al., 2007). The candidate generator gen(s) also affects the accuracy of the string transformation. Previous studies of spelling correction mostly defined gen(s), gen(s) = {t |dist(s, t) &lt; δ}. Introduction String transformation maps a source string s into its destination string t∗ . In the broad sense, string transformation can include labeling tasks such as partof-speech tagging and shallow parsing (Brill, 1995). However, this study addresses string transformation in its narrow sense, in which a part of a source string is rewritten with a substring. Typical applications of this task include stemming, lemmatization, spelling correction (Brill and Moore, 2000; Wilbur et al., 2006; Carlson and Fette, 2007), OCR error correction (Kolak and Resnik, 2002), approximate string (2) Here, the function dist(s, t) denotes the weighted Levenshtein distance (Levenshtein, 1966) between strings s and t. Furthermore, the threshold δ requires the distance between the source string s and a candidate string t to be less"
D08-1047,D07-1019,0,0.710798,"ia Ananiadou‡ Jun’ichi Tsujii†‡ sophia.ananiadou@manchester.ac.uk tsujii@is.s.u-tokyo.ac.jp † Graduate School of Information Science and Technology University of Tokyo 7-3-1 Hongo, Bunkyo-ku Tokyo 113-8656, Japan ‡ School of Computer Science, University of Manchester National Centre for Text Mining (NaCTeM) Manchester Interdisciplinary Biocentre 131 Princess Street, Manchester M1 7DN, UK Abstract matching (Navarro, 2001), and duplicate record detection (Bilenko and Mooney, 2003). Recent studies have formalized the task in the discriminative framework (Ahmad and Kondrak, 2005; Li et al., 2006; Chen et al., 2007), String transformation, which maps a source string s into its desirable form t∗ , is related to various applications including stemming, lemmatization, and spelling correction. The essential and important step for string transformation is to generate candidates to which the given string s is likely to be transformed. This paper presents a discriminative approach for generating candidate strings. We use substring substitution rules as features and score them using an L1 -regularized logistic regression model. We also propose a procedure to generate negative instances that affect the decision b"
D08-1047,dalianis-jongejan-2006-hand,0,0.022719,"larer → unpopularest and refundable → refundabler → refundablest. Therefore, we removed inflection entries for comparative and superlative adjectives from the dataset. presented an alignment-based discriminative string similarity. They extracted features from substring pairs that are consistent to a character-based alignment of two strings. Aramaki et al. (2008) also used features that express the different segments of the two strings. However, these studies are not suited for a candidate generator because the processes of string transformations are intractable in their discriminative models. Dalianis and Jongejan (2006) presented a lemmatiser based on suffix rules. Although they proposed a method to obtain suffix rules from a training data, the method did not use counter-examples (negatives) for reducing incorrect string transformations. Tsuruoka et al. (2008) proposed a scoring method for discovering a list of normalization rules for dictionary look-ups. However, their objective was to transform given strings, so that strings (e.g., studies and study) referring to the same concept in the dictionary are mapped into the same string (e.g., stud); in contrast, this study maps strings into their destination stri"
D08-1047,2005.mtsummit-papers.40,0,0.165856,"a.tsuruoka@manchester.ac.uk Sophia Ananiadou‡ Jun’ichi Tsujii†‡ sophia.ananiadou@manchester.ac.uk tsujii@is.s.u-tokyo.ac.jp † Graduate School of Information Science and Technology University of Tokyo 7-3-1 Hongo, Bunkyo-ku Tokyo 113-8656, Japan ‡ School of Computer Science, University of Manchester National Centre for Text Mining (NaCTeM) Manchester Interdisciplinary Biocentre 131 Princess Street, Manchester M1 7DN, UK Abstract matching (Navarro, 2001), and duplicate record detection (Bilenko and Mooney, 2003). Recent studies have formalized the task in the discriminative framework (Ahmad and Kondrak, 2005; Li et al., 2006; Chen et al., 2007), String transformation, which maps a source string s into its desirable form t∗ , is related to various applications including stemming, lemmatization, and spelling correction. The essential and important step for string transformation is to generate candidates to which the given string s is likely to be transformed. This paper presents a discriminative approach for generating candidate strings. We use substring substitution rules as features and score them using an L1 -regularized logistic regression model. We also propose a procedure to generate negative"
D08-1047,P06-1129,0,0.339768,"nd refundable → refundabler → refundablest. Therefore, we removed inflection entries for comparative and superlative adjectives from the dataset. presented an alignment-based discriminative string similarity. They extracted features from substring pairs that are consistent to a character-based alignment of two strings. Aramaki et al. (2008) also used features that express the different segments of the two strings. However, these studies are not suited for a candidate generator because the processes of string transformations are intractable in their discriminative models. Dalianis and Jongejan (2006) presented a lemmatiser based on suffix rules. Although they proposed a method to obtain suffix rules from a training data, the method did not use counter-examples (negatives) for reducing incorrect string transformations. Tsuruoka et al. (2008) proposed a scoring method for discovering a list of normalization rules for dictionary look-ups. However, their objective was to transform given strings, so that strings (e.g., studies and study) referring to the same concept in the dictionary are mapped into the same string (e.g., stud); in contrast, this study maps strings into their destination stri"
D08-1047,J99-1003,0,0.038867,"f the tasks: classification (Section 3.2) and normalization (Section 3.3). 3.2 Experiment 1: Candidate classification In this experiment, we measured the performance of the classification task in which pairs of strings were assigned with positive or negative labels. We trained and evaluated the proposed method by performing ten-fold cross validation on each dataset5 . Eight baseline systems were prepared for comparison: Levenshtein distance (LD), normalized Levenshtein distance (NLD), Dice coefficient on letter bigrams (DICE) (Adamson and Boreham, 1974), Longest Common Substring Ratio (LCSR) (Melamed, 1999), Longest Common Prefix Ratio (PREFIX) (Kondrak, 2005), Porter’s stemmer (Porter, 1980), Morpha (Minnen et al., 2001), and CST’s lemmatiser (Dalianis and Jonge3 LRSPL table includes trivial spelling variants that can be handled using simple character/string operations. For example, the table contains spelling variants related to case sensitivity (e.g., deg and Deg) and symbols (e.g., Feb and Feb.). 4 LRAGR table also provides agreement information even when word forms do not change. For example, the table contains an entry indicating that the first-singular present form of the verb study is st"
D08-1047,P99-1004,0,\N,Missing
D16-1112,W13-2322,0,0.0542076,"ted edges represent a relationship between nodes. Concepts &lt;s&gt; canada od m a1 a3 country na m e a2 nato prime op 1 name … summary AMR of the input sentence “canadian” yi+1 encAMR aj … announce … … a1 &lt;s&gt; E 0 yi C+1 canada … nato headline prime op 1 name a3 country od m na m e a2 E 0 yi ABS “canadian” AMR of the input sentence Figure 2: Model structure of our proposed attentionbased AMR encoder; it outputs a headline using ABS and encoded AMR with attention. consist of English words, PropBank event predicates, and special labels such as “person”. For edges, AMR has approximately 100 relations (Banarescu et al., 2013) including semantic roles based on the PropBank annotations in OntoNotes (Hovy et al., 2006). To acquire AMRs for input sentences, we use the state-of-the-art transition-based AMR parser (Wang et al., 2015). 3.2 Attention-Based AMR Encoder Figure 2 shows a brief sketch of the model structure of our attention-based AMR encoder model. We utilize a variant of child-sum Tree-LSTM originally proposed in (Tai et al., 2015) to encode syntactic and semantic information obtained from output of the AMR parser into certain fixed-length embedding vectors. To simplify the computation, we transform a DAG st"
D16-1112,N16-1012,0,0.30206,"e 3 supports this observation. For example, ABS+AMR successfully added the correct modifier ‘saudi’ to “crown prince” in the first example. Moreover, ABS+AMR generated a consistent subject in the third example. The comparison between ABS+AMR(w/o attn) and ABS+AMR (with attention) suggests that the attention mechanism is necessary for AMR encoding. In other words, the encoder without the attention mechanism tends to be overfitting. 1058 5 Related Work Recently, the Recurrent Neural Network (RNN) and its variant have been applied successfully to various NLP tasks. For headline generation tasks, Chopra et al. (2016) exploited the RNN decoder (and its variant) with the attention mechanism instead of the method of Rush et al. (2015): the combination of the feed-forward neural network language model and attention-based sentence encoder. Nallapati et al. (2016) also adapted the RNN encoder-decoder with attention for headline generation tasks. Moreover, they made some efforts such as hierarchical attention to improve the performance. In addition to using a variant of RNN, Gulcehre et al. (2016) proposed a method to handle infrequent words in natural language generation. Note that these recent developments do"
D16-1112,P16-1014,0,0.0597202,"ural Network (RNN) and its variant have been applied successfully to various NLP tasks. For headline generation tasks, Chopra et al. (2016) exploited the RNN decoder (and its variant) with the attention mechanism instead of the method of Rush et al. (2015): the combination of the feed-forward neural network language model and attention-based sentence encoder. Nallapati et al. (2016) also adapted the RNN encoder-decoder with attention for headline generation tasks. Moreover, they made some efforts such as hierarchical attention to improve the performance. In addition to using a variant of RNN, Gulcehre et al. (2016) proposed a method to handle infrequent words in natural language generation. Note that these recent developments do not conflict with our method using the AMR encoder. This is because the AMR encoder can be straightforwardly incorporated into their methods as we have done in this paper, incorporating the AMR encoder into the baseline. We believe that our AMR encoder can possibly further improve the performance of their methods. We will test that hypothesis in future study. 6 Conclusion This paper mainly discussed the usefulness of incorporating structural syntactic and semantic information in"
D16-1112,N06-2015,0,0.0110087,"2 nato prime op 1 name … summary AMR of the input sentence “canadian” yi+1 encAMR aj … announce … … a1 &lt;s&gt; E 0 yi C+1 canada … nato headline prime op 1 name a3 country od m na m e a2 E 0 yi ABS “canadian” AMR of the input sentence Figure 2: Model structure of our proposed attentionbased AMR encoder; it outputs a headline using ABS and encoded AMR with attention. consist of English words, PropBank event predicates, and special labels such as “person”. For edges, AMR has approximately 100 relations (Banarescu et al., 2013) including semantic roles based on the PropBank annotations in OntoNotes (Hovy et al., 2006). To acquire AMRs for input sentences, we use the state-of-the-art transition-based AMR parser (Wang et al., 2015). 3.2 Attention-Based AMR Encoder Figure 2 shows a brief sketch of the model structure of our attention-based AMR encoder model. We utilize a variant of child-sum Tree-LSTM originally proposed in (Tai et al., 2015) to encode syntactic and semantic information obtained from output of the AMR parser into certain fixed-length embedding vectors. To simplify the computation, we transform a DAG structure of AMR parser output to a tree structure, which we refer to as “tree-converted AMR s"
D16-1112,W04-1013,0,0.222018,"esults if we observed statistical difference (p &lt; 0.05) between ABS (re-run) and ABS+AMR on the t-test. (R-1: ROUGE-1, R-2: ROUGE-2, R-L: ROUGE-L) For a fair comparison, we followed their evaluation setting. The training data was obtained from the first sentence and the headline of a document in the annotated Gigaword corpus (Napoles et al., 2012)4 . The development data is DUC-2003 data, and test data are both DUC-2004 (Over et al., 2007) and sentence-headline pairs obtained from the annotated Gigaword corpus as well as training data5 . All of the generated headlines were evaluated by ROUGE (Lin, 2004)6 . For evaluation on DUC2004, we removed strings after 75-characters for each generated headline as described in the DUC2004 evaluation. For evaluation on Gigaword, we forced the system outputs to be at most 8 words as in Rush et al. (2015) since the average length of headline in Gigaword is 8.3 words. For the preprocessing for all data, all letters were converted to lower case, all digits were replaced with ‘#’, and words appearing less than five times with ‘UNK’. Note that, for further evaluation, we prepared 2,000 sentence-headline pairs randomly sampled from the test data section of the G"
D16-1112,K16-1028,0,0.0442533,"/o attn) and ABS+AMR (with attention) suggests that the attention mechanism is necessary for AMR encoding. In other words, the encoder without the attention mechanism tends to be overfitting. 1058 5 Related Work Recently, the Recurrent Neural Network (RNN) and its variant have been applied successfully to various NLP tasks. For headline generation tasks, Chopra et al. (2016) exploited the RNN decoder (and its variant) with the attention mechanism instead of the method of Rush et al. (2015): the combination of the feed-forward neural network language model and attention-based sentence encoder. Nallapati et al. (2016) also adapted the RNN encoder-decoder with attention for headline generation tasks. Moreover, they made some efforts such as hierarchical attention to improve the performance. In addition to using a variant of RNN, Gulcehre et al. (2016) proposed a method to handle infrequent words in natural language generation. Note that these recent developments do not conflict with our method using the AMR encoder. This is because the AMR encoder can be straightforwardly incorporated into their methods as we have done in this paper, incorporating the AMR encoder into the baseline. We believe that our AMR e"
D16-1112,W12-3018,0,0.0606696,"Missing"
D16-1112,D15-1044,0,0.311302,"ral attention-based model. We encode results obtained from an abstract meaning representation (AMR) parser using a modified version of Tree-LSTM. Our proposed attention-based AMR encoder-decoder model improves headline generation benchmarks compared with the baseline neural attention-based model. 1 Introduction Neural network-based encoder-decoder models are cutting-edge methodologies for tackling natural language generation (NLG) tasks, i.e., machine translation (Cho et al., 2014), image captioning (Vinyals et al., 2015), video description (Venugopalan et al., 2015), and headline generation (Rush et al., 2015). This paper also shares a similar goal and motivation to previous work: improving the encoderdecoder models for natural language generation. There are several directions for enhancement. This paper respects the fact that NLP researchers have expended an enormous amount of effort to develop fundamental NLP techniques such as POS tagging, dependency parsing, named entity recognition, and semantic role labeling. Intuitively, this structural, syntactic, and semantic information underlying input text has the potential for improving the quality of NLG tasks. However, to the best of our knowledge, t"
D16-1112,P15-1150,0,0.0400517,"ing the quality of NLG tasks. However, to the best of our knowledge, there is no clear evidence that syntactic and semantic information can enhance the recently developed encoder-decoder models in NLG tasks. To answer this research question, this paper proposes and evaluates a headline generation method based on an encoder-decoder architecture on Abstract Meaning Representation (AMR). The method is essentially an extension of attention-based summarization (ABS) (Rush et al., 2015). Our proposed method encodes results obtained from an AMR parser by using a modified version of TreeLSTM encoder (Tai et al., 2015) as additional information of the baseline ABS model. Conceptually, the reason for using AMR for headline generation is that information presented in AMR, such as predicate-argument structures and named entities, can be effective clues when producing shorter summaries (headlines) from original longer sentences. We expect that the quality of headlines will improve with this reasonable combination (ABS and AMR). 2 Attention-based summarization (ABS) ABS proposed in Rush et al. (2015) has achieved state-of-the-art performance on the benchmark data of headline generation including the DUC-2004 dat"
D16-1112,N15-1173,0,0.0191451,"ormation additionally incorporated in a baseline neural attention-based model. We encode results obtained from an abstract meaning representation (AMR) parser using a modified version of Tree-LSTM. Our proposed attention-based AMR encoder-decoder model improves headline generation benchmarks compared with the baseline neural attention-based model. 1 Introduction Neural network-based encoder-decoder models are cutting-edge methodologies for tackling natural language generation (NLG) tasks, i.e., machine translation (Cho et al., 2014), image captioning (Vinyals et al., 2015), video description (Venugopalan et al., 2015), and headline generation (Rush et al., 2015). This paper also shares a similar goal and motivation to previous work: improving the encoderdecoder models for natural language generation. There are several directions for enhancement. This paper respects the fact that NLP researchers have expended an enormous amount of effort to develop fundamental NLP techniques such as POS tagging, dependency parsing, named entity recognition, and semantic role labeling. Intuitively, this structural, syntactic, and semantic information underlying input text has the potential for improving the quality of NLG ta"
D16-1112,N15-1040,0,0.0546549,"ed method encodes results obtained from an AMR parser by using a modified version of TreeLSTM encoder (Tai et al., 2015) as additional information of the baseline ABS model. Conceptually, the reason for using AMR for headline generation is that information presented in AMR, such as predicate-argument structures and named entities, can be effective clues when producing shorter summaries (headlines) from original longer sentences. We expect that the quality of headlines will improve with this reasonable combination (ABS and AMR). 2 Attention-based summarization (ABS) ABS proposed in Rush et al. (2015) has achieved state-of-the-art performance on the benchmark data of headline generation including the DUC-2004 dataset (Over et al., 2007). Figure 1 illustrates the model structure of ABS. The model predicts a word sequence (summary) based on the combination of the neural network language model and an input sentence encoder. Let V be a vocabulary. xi is the i-th indicator vector corresponding to the i-th word in the input sentence. Suppose we have M words of an input sentence. X represents an input sentence, which 1054 Proceedings of the 2016 Conference on Empirical Methods in Natural Language"
I05-1055,P03-1069,0,0.590306,"tage. Chronological ordering; ordering sentences according to the published date of the documents they belong to [6], is one solution to this problem. However, showing that this approach is insuﬃcient, Barzilay [1] proposed an reﬁned algorithm which integrates chronology ordering with topical relatedness of documents. Okazaki [7] proposes a improved chronological ordering algorithm using precedence relations among sentences. His algorithm searches for an order which satisﬁes the precedence relations among sentences. In addition to these studies which make use of chronological ordering, Lapata [3] proposes a probabilistic model of text structuring and its application to the sentence ordering. Her system calculates the conditional probabilities between sentences from a corpus and uses a greedy ordering algorithm to arrange sentences according to the conditional probabilities. Even though these previous studies proposed diﬀerent strategies to decide the sentence ordering, the appropriate way to combine these diﬀerent methods to obtain more robust and coherent text remains unknown. In addition to these existing sentence ordering heuristics, we propose a new method which we shall call succ"
I05-1055,P02-1040,0,0.0778618,"arseness of higher order n-grams Pn decreases in an exponential-like curve with n. Therefore, we deﬁne Average Continuity as the logrithmic average of Pn as follows: 1 log(Pn )) 3 n=2 4 Average Continuity = exp( (21) We add a small quantity α to numerator and denominator of Pn in equation 20 so that the logarithm will not diverge when n-grams count is zero. We used α = 0.01 in our evaluations. Experimental results showed that taking n-grams up to four gave contrasting results because the n-grams tend to be sparse for larger n values. BLEU(BiLingual Evaluation Understudy) proposed by Papineni [8] for the task of evaluating machine translations has an analogical form to our average continuity. In BLEU, a machine translation is compared against multiple reference translations and precision values are calculated using word n-grams. BLEU is then deﬁned as the logarithmic average of these precision values. 4 Results We used the 3rd Text Summarization Challenge (TSC) corpus for our experiments. TSC1 corpus contains news articles taken from two leading Japanese newspapers; Mainichi and Yomiuri. TSC-3 corpus contains human selected extracts for 30 diﬀerent topics. However, in the TSC corpus t"
I05-1055,W98-1411,0,\N,Missing
I05-1055,W98-1123,0,\N,Missing
I05-1055,J00-3005,0,\N,Missing
I05-1055,J02-4006,0,\N,Missing
I05-1055,N04-1015,0,\N,Missing
I08-1050,W06-3309,0,0.400917,"Missing"
I08-1050,W95-0107,0,0.0126842,". Table 3 shows typical moves of sections in Medline abstracts. The majority of sequences in this table consists of four sections compatible with the ANSI standard, purpose, methods, results, and conclusions. Moreover, the most frequent sequence is “OBJECTIVE → METHOD(S) → RESULTS → CONCLUSION(S),” supposing that AIM and PURPOSE are equivalent to OBJECTIVE. Hence, this study assumes four sections, OBJECTIVE, METHOD, RESULTS, and CONCLUSIONS. Meanwhile, it is common for NP chunking tasks to represent a chunk (e.g., NP) with two labels, the begin (e.g., B-NP) and inside (e.g., I-NP) of a chunk (Ramshaw and Marcus, 1995). Although none of the previous studies employed this representation, attaching B- and I- prefixes to section labels may improve a classifier by associating clue phrases (e.g., “to determine”) with the starts of sections (e.g., B-OBJECTIVE). We will compare classification performances on two sets of label representations: namely, we will compare four section labels and eight labels with BI prefixes attached to section names. 4 4.1 Evaluation Experiment We constructed two sets of corpora (‘pure’ and ‘expanded’), each of which contains 51,000 abstracts sampled from the abstracts with structured"
I08-1050,N03-1028,0,0.0192388,"esult sentences appearing before method sentences were described in an abstract. Moreover, we would like to model the structure of abstract sentences rather than modeling just the section label for each sentence. Thus, the task is more suitably formalized as a sequence labeling problem: given an abstract with sentences x = (x1 , ..., xn ), determine the optimal sequence of section names y = (y1 , ..., yn ) of all possible sequences. Conditional Random Fields (CRFs) have been successfully applied to various NLP tasks including part-of-speech tagging (Lafferty et al., 2001) and shallow parsing (Sha and Pereira, 2003). CRFs define a conditional probability distribution p(y|x) for output and input sequences, y and x, p(y|x) = 1 exp {λ · F (y, x)} . Zλ (x) (1) Therein: function F (y, x) denotes a global feature X f (y, x, i), (2) i i ranges over the input sequence, function f (y, x, i) is a feature vector for input sequence x and output sequence y at position i (based on state features and transition features), λ is a vector where an element λk represents the weight of feature Fk (y, x), and Zλ (x) is a normalization factor, Zλ (x) = X exp {λ · F (y, x)} . (3) y The optimal output sequence y ˆ for an input s"
I08-1050,J02-4002,0,0.870395,"Missing"
I08-1050,H05-1059,0,0.0588171,"Missing"
I08-1050,P06-4011,0,0.214503,"Missing"
I08-2127,W06-0103,0,0.0295316,"his paper. 2 Japanese Abbreviation Survey Researchers have proposed several approaches to abbreviation recognition for non-alphabetical languages. Hisamitsu and Niwa (2001) compared different statistical measures (e.g., χ2 test, log likeTable 1: Parenthetical expressions used in Japanese newspaper articles lihood ratio) to assess the co-occurrence strength between the inner and outer phrases of parenthetical expressions X (Y). Yamamoto (2002) utilized the similarity of local contexts to measure the paraphrase likelihood of two expressions based on the distributional hypothesis (Harris, 1954). Chang and Teng (2006) formalized the generative processes of Chinese abbreviations with a noisy channel model. Sasano et al. (2007) designed rules about letter types and occurrence frequency to collect lexical paraphrases used for coreference resolution. How are these approaches effective in recognizing Japanese abbreviation definitions? As a preliminary study, we examined abbreviations described in parenthetical expressions in Japanese newspaper articles. We used the 7,887 parenthetical expressions that occurred more than eight times in Japanese articles published by the Mainichi Newspapers and Yomiuri Shimbun in"
I08-2127,W02-2016,0,0.0287337,"es the paraphrase ratio of the expressions X and Y, PR(X, Y ) = dpara (X, Y ) . d(X, Y ) (1) In this formula, dpara (X, Y ) denotes the number of documents satisfying the above conditions, and d(X, Y ) presents the number of documents having the parenthetical expression X(Y ). The function PR(X, Y) ranges from 0 (no abbreviation instance) to 1 (all parenthetical expressions introduce the abbreviation). Similarity of local contexts We regard words that have dependency relations from/to the target expression as the local contexts of the expression, applying all sentences to a dependency parser (Kudo and Matsumoto, 2002). Collecting the local context of the target expressions, we compute the skew divergence (Lee, 2001), which is a weighted version of 892 SKEWα (P ||Q) = KL(P ||αQ + (1 − α)P ), (2) KL(P ||Q) = X i P (i) log P (i) . Q(i) (3) Other features In addition, we designed twelve features for abbreviation recognition: five features, freq(X), freq(Y ), freq(X, Y ), χ2 (X, Y ), and LLR(X, Y ) to measure the co-occurrence strength of the expressions X and Y (Hisamitsu and Niwa, 2001), match(X, Y ) feature to test whether or not all letters in an abbreviation appear in its full form, three features letter t"
I08-2127,P06-2083,1,0.780857,"lications by recognizing a set of expressions referring to the same entity/concept. For example, text retrieval systems can associate a query with alternative words to find documents where the query is not obviously stated. 889 Abbreviations are among a highly productive type of term variants, which substitutes fully expanded terms with shortened term-forms. Most previous studies aimed at establishing associations between abbreviations and their full forms in English (Park and Byrd, 2001; Pakhomov, 2002; Schwartz and Hearst, 2003; Adar, 2004; Nadeau and Turney, 2005; Chang and Sch¨utze, 2006; Okazaki and Ananiadou, 2006). Although researchers have proposed various approaches to solving abbreviation recognition through methods such as deterministic algorithm, scoring function, and machine learning, these studies rely on the phenomenon specific to English abbreviations: all letters in an abbreviation appear in its full form. However, abbreviation phenomena are heavily dependent on languages. For example, the term onesegment broadcasting is usually abbreviated as oneseg in Japanese; English speakers may find this peculiar as the term is likely to be abbreviated as 1SB or OSB in English. We show that letters do n"
I08-2127,P02-1021,0,0.0165456,"s or syntactic structures. Lexical resources such as WordNet (Miller et al., 1990) enhance various NLP applications by recognizing a set of expressions referring to the same entity/concept. For example, text retrieval systems can associate a query with alternative words to find documents where the query is not obviously stated. 889 Abbreviations are among a highly productive type of term variants, which substitutes fully expanded terms with shortened term-forms. Most previous studies aimed at establishing associations between abbreviations and their full forms in English (Park and Byrd, 2001; Pakhomov, 2002; Schwartz and Hearst, 2003; Adar, 2004; Nadeau and Turney, 2005; Chang and Sch¨utze, 2006; Okazaki and Ananiadou, 2006). Although researchers have proposed various approaches to solving abbreviation recognition through methods such as deterministic algorithm, scoring function, and machine learning, these studies rely on the phenomenon specific to English abbreviations: all letters in an abbreviation appear in its full form. However, abbreviation phenomena are heavily dependent on languages. For example, the term onesegment broadcasting is usually abbreviated as oneseg in Japanese; English spe"
I08-2127,W01-0516,0,0.112671,"sing alternative words or syntactic structures. Lexical resources such as WordNet (Miller et al., 1990) enhance various NLP applications by recognizing a set of expressions referring to the same entity/concept. For example, text retrieval systems can associate a query with alternative words to find documents where the query is not obviously stated. 889 Abbreviations are among a highly productive type of term variants, which substitutes fully expanded terms with shortened term-forms. Most previous studies aimed at establishing associations between abbreviations and their full forms in English (Park and Byrd, 2001; Pakhomov, 2002; Schwartz and Hearst, 2003; Adar, 2004; Nadeau and Turney, 2005; Chang and Sch¨utze, 2006; Okazaki and Ananiadou, 2006). Although researchers have proposed various approaches to solving abbreviation recognition through methods such as deterministic algorithm, scoring function, and machine learning, these studies rely on the phenomenon specific to English abbreviations: all letters in an abbreviation appear in its full form. However, abbreviation phenomena are heavily dependent on languages. For example, the term onesegment broadcasting is usually abbreviated as oneseg in Japan"
I08-2127,W02-1411,0,0.0178085,"sents a supervised learning approach to Japanese abbreviations. We then evaluate the proposed method on a test corpus from newspaper articles in Section 4 and conclude this paper. 2 Japanese Abbreviation Survey Researchers have proposed several approaches to abbreviation recognition for non-alphabetical languages. Hisamitsu and Niwa (2001) compared different statistical measures (e.g., χ2 test, log likeTable 1: Parenthetical expressions used in Japanese newspaper articles lihood ratio) to assess the co-occurrence strength between the inner and outer phrases of parenthetical expressions X (Y). Yamamoto (2002) utilized the similarity of local contexts to measure the paraphrase likelihood of two expressions based on the distributional hypothesis (Harris, 1954). Chang and Teng (2006) formalized the generative processes of Chinese abbreviations with a noisy channel model. Sasano et al. (2007) designed rules about letter types and occurrence frequency to collect lexical paraphrases used for coreference resolution. How are these approaches effective in recognizing Japanese abbreviation definitions? As a preliminary study, we examined abbreviations described in parenthetical expressions in Japanese newsp"
I17-1048,D16-1245,0,0.0328659,"Missing"
I17-1048,P16-1061,0,0.211469,"c entity-wise representation. Our experiments have demonstrated the benefits of such integration. Another related trend in recent studies is the use of neural network to capture the information flow of a discourse. One approach has been to link RNNs across sentences (Wang and Cho, 2016; Serban et al., 2016), while a second approach has expolited a type of memory space to store contextual information (Sukhbaatar et al., 2015; Tran et al., 2016; Merity et al., 2017). Research on reading comprehension (Kobayashi et al., 2016; Henaff et al., 2017) and coreference resolution (Wiseman et al., 2016; Clark and Manning, 2016b,a) has shown the salience of entitywise context information. Our model could be located within such approaches, but is distinct in being the first model to make use of entity-wise context information in both the input and output layers for sentence generation. Table 4: Mean Quantile of a true coreferent entity among antecedent entities. 6 Related Work An approach to addressing the unknown word problem used in recent studies (Kim et al., 2016; Sennrich et al., 2016; Luong and Manning, 2016; Schuster and Nakajima, 2012) comprises the embeddings of unknown words from character embeddings or sub"
I17-1048,N16-1099,1,0.0606823,"tute of Technology, Japan okazaki@c.titech.ac.jp Kentaro Inui Tohoku University / RIKEN, Japan inui@ecei.tohoku.ac.jp Abstract ... [ 1 ] killed [ 2 ] with bombs … ... police suspects [ 1 ] attacked ... This study addresses the problem of identifying the meaning of unknown words or entities in a discourse with respect to the word embedding approaches used in neural language models. We proposed a method for on-the-fly construction and exploitation of word embeddings in both the input and output layers of a neural model by tracking contexts. This extends the dynamic entity representation used in Kobayashi et al. (2016) and incorporates a copy mechanism proposed independently by Gu et al. (2016) and Gulcehre et al. (2016). In addition, we construct a new task and dataset called Anonymized Language Modeling for evaluating the ability to capture word meanings while reading. Experiments conducted using our novel dataset show that the proposed variant of RNN language model outperformed the baseline model. Furthermore, the experiments also demonstrate that dynamic updates of an output layer help a model predict reappearing entities, whereas those of an input layer are effective to predict words following reappear"
I17-1048,D15-1200,0,0.0266978,"e salience of entitywise context information. Our model could be located within such approaches, but is distinct in being the first model to make use of entity-wise context information in both the input and output layers for sentence generation. Table 4: Mean Quantile of a true coreferent entity among antecedent entities. 6 Related Work An approach to addressing the unknown word problem used in recent studies (Kim et al., 2016; Sennrich et al., 2016; Luong and Manning, 2016; Schuster and Nakajima, 2012) comprises the embeddings of unknown words from character embeddings or subword embeddings. Li and Jurafsky (2015) applied word disambiguation and use a sense embedding to the target word. Choi et al. (2017) captured the context-sensitive meanings of common words using word embeddings, applied through a gating function controlled by history words, in the context of machine translation. In future work, we will explore a wider range of models, to integrate our dynamic text modeling with methods that estimate the meaning of unknown words or entities from their constituents. When addressing well-known entities such as Obama and Trump, it makes sense to learn their embeddings from external resources, as well a"
I17-1048,P16-1100,0,0.0306988,"ehension (Kobayashi et al., 2016; Henaff et al., 2017) and coreference resolution (Wiseman et al., 2016; Clark and Manning, 2016b,a) has shown the salience of entitywise context information. Our model could be located within such approaches, but is distinct in being the first model to make use of entity-wise context information in both the input and output layers for sentence generation. Table 4: Mean Quantile of a true coreferent entity among antecedent entities. 6 Related Work An approach to addressing the unknown word problem used in recent studies (Kim et al., 2016; Sennrich et al., 2016; Luong and Manning, 2016; Schuster and Nakajima, 2012) comprises the embeddings of unknown words from character embeddings or subword embeddings. Li and Jurafsky (2015) applied word disambiguation and use a sense embedding to the target word. Choi et al. (2017) captured the context-sensitive meanings of common words using word embeddings, applied through a gating function controlled by history words, in the context of machine translation. In future work, we will explore a wider range of models, to integrate our dynamic text modeling with methods that estimate the meaning of unknown words or entities from their consti"
I17-1048,P15-1002,0,0.123525,"Missing"
I17-1048,P16-1154,0,0.317257,"KEN, Japan inui@ecei.tohoku.ac.jp Abstract ... [ 1 ] killed [ 2 ] with bombs … ... police suspects [ 1 ] attacked ... This study addresses the problem of identifying the meaning of unknown words or entities in a discourse with respect to the word embedding approaches used in neural language models. We proposed a method for on-the-fly construction and exploitation of word embeddings in both the input and output layers of a neural model by tracking contexts. This extends the dynamic entity representation used in Kobayashi et al. (2016) and incorporates a copy mechanism proposed independently by Gu et al. (2016) and Gulcehre et al. (2016). In addition, we construct a new task and dataset called Anonymized Language Modeling for evaluating the ability to capture word meanings while reading. Experiments conducted using our novel dataset show that the proposed variant of RNN language model outperformed the baseline model. Furthermore, the experiments also demonstrate that dynamic updates of an output layer help a model predict reappearing entities, whereas those of an input layer are effective to predict words following reappearing entities. 1 d[2],2 ... will arrest [ 1 ] soon … d[1],2 ! x[2] x[1] y[2] !"
I17-1048,K16-1006,0,0.191015,"ccurrences of an entity Train 2725 25.7 15.6 9.3 Valid 335 27.2 16.8 9.9 Test 336 26.4 15.8 9.5 3.2 3.2 3.1 achieve best perplexity on the validation dataset5 . Most hyper-parameters were tuned and fixed by the baseline model on the validation dataset6 . It is difficult to adequately train the all parts of a model using only the small dataset of Anonymized Language Modeling. We therefore pretrained word embeddings and ContextEncoder (the bidirectional RNNs and matrices in Equations 6– 8) on a sentence completion task in which clozes were predicted from the surrounding words in a large corpus (Melamud et al., 2016)7 . We used the objective function with samP negative | ˆ pling (Mikolov et al., 2013): (log σ( x x e e) + e P | ˆ ˆ (log σ(− x x ))). Here, x is a context e v e v∈N eg vector predicted by ContextEncoder, xe denotes the word embedding of a target word e appearing in the corpus, and N eg represents randomly sampled words. These pretrained parameters of ContextEncoder were fixed when the whole language model was trained on the Anonymized Language Modeling dataset. We implemented models in Python using the Chainer neural network library (Tokui et al., 2015). The code and the constructed dataset a"
I17-1048,P16-1014,0,0.312562,"tohoku.ac.jp Abstract ... [ 1 ] killed [ 2 ] with bombs … ... police suspects [ 1 ] attacked ... This study addresses the problem of identifying the meaning of unknown words or entities in a discourse with respect to the word embedding approaches used in neural language models. We proposed a method for on-the-fly construction and exploitation of word embeddings in both the input and output layers of a neural model by tracking contexts. This extends the dynamic entity representation used in Kobayashi et al. (2016) and incorporates a copy mechanism proposed independently by Gu et al. (2016) and Gulcehre et al. (2016). In addition, we construct a new task and dataset called Anonymized Language Modeling for evaluating the ability to capture word meanings while reading. Experiments conducted using our novel dataset show that the proposed variant of RNN language model outperformed the baseline model. Furthermore, the experiments also demonstrate that dynamic updates of an output layer help a model predict reappearing entities, whereas those of an input layer are effective to predict words following reappearing entities. 1 d[2],2 ... will arrest [ 1 ] soon … d[1],2 ! x[2] x[1] y[2] ! ! ! y[1] −−−→ = RNN( ... p"
I17-1048,D15-1038,0,0.0604066,"Missing"
I17-1048,P16-1028,0,0.0259706,"se to learn their embeddings from external resources, as well as dynamically from the preceding context in a given discourse (as in our Dynamic Neural Text Model). The integration of these two sources of information is an intriguing challenge in language modeling. A key aspect of our model is its incorporation of the copy mechanism (Gu et al., 2016; Gulcehre et al., 2016), using dynamic word embeddings in the output layer. Independently of this study, several research groups have explored the use of variants of the copy mechanisms in language modeling (Merity et al., 2017; Grave et al., 2017; Peng and Roth, 2016). These studies, however, did not incorporate dynamic representations in the input layer. In contrast, our proposal incorporates the copy mechanism through the use We summarize and compare works for entitycentric neural networks that read a document. Kobayashi et al. (2016) pioneered entity-centric neural models tracking states in a discourse. They proposed Dynamic Entity Representation, which encodes contexts of entities and updates the states using entity-wise memories. Wiseman et al. (2016) also proposed a method for managing similar entity-wise features on neural networks and improved a co"
I17-1048,D17-1195,0,0.16917,"t al. (2016) also proposed a method for managing similar entity-wise features on neural networks and improved a coreference resolution model. Clark and Manning (2016b,a) incorporated such entitywise representations in mention-ranking coreference models. Our paper follows Kobayashi et al. (2016) and exploits dynamic entity reprensetions in a neural language model, where dynamic reporesentations are used not only in the neural encoder but also in the decoder, applicable to various sequence generation tasks, e.g., machine translation and dialog response generation. Simultaneously with our paper, Ji et al. (2017) use dynamic entity representation in a neural language model for reranking outputs of a coreference resolution system. Yang et al. (2017) experiment language modeling with referring to internal contexts or external data. Henaff et al. (2017) focus on neural networks tracking contexts of entities, achieving the state-of-the-art result in bAbI (Weston et al., 2015), a reading comprehension task. They encode the contexts of each entity by an attention480 like gated RNN instead of using coreference links directly. Dhingra et al. (2017) also try to improve a reading comprehension model using coref"
I17-1048,W12-4501,0,0.260955,"ing. Figure 3 gives an example from the dataset. Briefly, the dataset anonymizes certain noun phrases, treating them as unknown words and retaining their coreference relations. This allows a language model to track the context of every noun phrase in the discourse. Other words are left unchanged, allowing the language model to preserve the context of the anonymized (unknown) words, and to infer their meanings from the known words. The process was inspired by Hermann et al. (2015), whose approach has been explored by the research on reading comprehension. More precisely, we used the OntoNotes (Pradhan et al., 2012) corpus, which includes documents with coreferences and named entity tags manually annotated. We assigned an anonymous identifier to every coreference chain in the corpus3 in order of first appearance4 , and replaced mentions of a coreference chain with its identifier. In our experiments, each coreference chain was given a dynamic representation. Following Mikolov et al. (2010), we limited the vocabulary to 10,000 words appearing frequently in the corpus. Finally, we inserted “&lt;bos&gt;” and “&lt;eos&gt;” tokens to mark the beginning and end of each sentence. An important difference between this dataset"
I17-1048,P16-1162,0,0.0444964,"search on reading comprehension (Kobayashi et al., 2016; Henaff et al., 2017) and coreference resolution (Wiseman et al., 2016; Clark and Manning, 2016b,a) has shown the salience of entitywise context information. Our model could be located within such approaches, but is distinct in being the first model to make use of entity-wise context information in both the input and output layers for sentence generation. Table 4: Mean Quantile of a true coreferent entity among antecedent entities. 6 Related Work An approach to addressing the unknown word problem used in recent studies (Kim et al., 2016; Sennrich et al., 2016; Luong and Manning, 2016; Schuster and Nakajima, 2012) comprises the embeddings of unknown words from character embeddings or subword embeddings. Li and Jurafsky (2015) applied word disambiguation and use a sense embedding to the target word. Choi et al. (2017) captured the context-sensitive meanings of common words using word embeddings, applied through a gating function controlled by history words, in the context of machine translation. In future work, we will explore a wider range of models, to integrate our dynamic text modeling with methods that estimate the meaning of unknown words or e"
I17-1048,N16-1036,0,0.0264423,"37±.005 .620±.002 .613±.002 of dynamic representations in the output layer, integrating them with dynamic mechanisms in both the input and output layers by applying dynamic entity-wise representation. Our experiments have demonstrated the benefits of such integration. Another related trend in recent studies is the use of neural network to capture the information flow of a discourse. One approach has been to link RNNs across sentences (Wang and Cho, 2016; Serban et al., 2016), while a second approach has expolited a type of memory space to store contextual information (Sukhbaatar et al., 2015; Tran et al., 2016; Merity et al., 2017). Research on reading comprehension (Kobayashi et al., 2016; Henaff et al., 2017) and coreference resolution (Wiseman et al., 2016; Clark and Manning, 2016b,a) has shown the salience of entitywise context information. Our model could be located within such approaches, but is distinct in being the first model to make use of entity-wise context information in both the input and output layers for sentence generation. Table 4: Mean Quantile of a true coreferent entity among antecedent entities. 6 Related Work An approach to addressing the unknown word problem used in recent s"
I17-1048,P16-1125,0,0.0303919,"namic GRU-ReLU input & output GRU Max pool. Only latest MQ .525±.001 .630±.005 .633±.005 .617±.002 .600±.004 .519±.001 .522±.000 .519±.001 .519±.003 .642±.004 .637±.005 .620±.002 .613±.002 of dynamic representations in the output layer, integrating them with dynamic mechanisms in both the input and output layers by applying dynamic entity-wise representation. Our experiments have demonstrated the benefits of such integration. Another related trend in recent studies is the use of neural network to capture the information flow of a discourse. One approach has been to link RNNs across sentences (Wang and Cho, 2016; Serban et al., 2016), while a second approach has expolited a type of memory space to store contextual information (Sukhbaatar et al., 2015; Tran et al., 2016; Merity et al., 2017). Research on reading comprehension (Kobayashi et al., 2016; Henaff et al., 2017) and coreference resolution (Wiseman et al., 2016; Clark and Manning, 2016b,a) has shown the salience of entitywise context information. Our model could be located within such approaches, but is distinct in being the first model to make use of entity-wise context information in both the input and output layers for sentence generation."
I17-1048,N16-1114,0,0.0580191,"Missing"
I17-1048,D17-1197,0,0.113034,"odel. Clark and Manning (2016b,a) incorporated such entitywise representations in mention-ranking coreference models. Our paper follows Kobayashi et al. (2016) and exploits dynamic entity reprensetions in a neural language model, where dynamic reporesentations are used not only in the neural encoder but also in the decoder, applicable to various sequence generation tasks, e.g., machine translation and dialog response generation. Simultaneously with our paper, Ji et al. (2017) use dynamic entity representation in a neural language model for reranking outputs of a coreference resolution system. Yang et al. (2017) experiment language modeling with referring to internal contexts or external data. Henaff et al. (2017) focus on neural networks tracking contexts of entities, achieving the state-of-the-art result in bAbI (Weston et al., 2015), a reading comprehension task. They encode the contexts of each entity by an attention480 like gated RNN instead of using coreference links directly. Dhingra et al. (2017) also try to improve a reading comprehension model using coreference links. Similarly to our dynamic entity representation, Bahdanau et al. (2017) construct on-the-fly word embeddings of rare words fr"
I17-2074,W12-2006,0,0.0226451,"Missing"
I17-2074,D15-1166,0,0.061551,"Missing"
I17-2074,N16-1042,0,0.015123,"dden vectors at time step t ∈ {1, · · · , n} using those at time step t − 1 and the (predicted) word wt as follows: −−−−→ [ht ; ct ] = LSTM(wt , [ht−1 ; ct−1 ]). (4) ← − Here, we set h0 = h 1 and c0 = 01 . For t &gt; 1, we feed the yt−1 predicted at the previous time step back as the input word wt of the decoder. The decoder predicts the word yt at time step t ¯ t with using a softmax layer on top of the vector h an integrated attention mechanism: Generating proofread sentences using an encoderdecoder model with attention log p(y|x) = t X log p(yt |y&lt;t , x), (5) n=1 Following recent work on GEC (Yuan and Briscoe, 2016), we use an encoder-decoder model with global attention (Luong et al., 2015) to generate proofread sentences. We use bi-directional Long Short-Term Memory (LSTM) to encode the source sentences. LSTMs recurrently compute the memory and hidden vectors at time step s ∈ {1, · · · , m} using those at time step s − 1 or s + 1 and the word xs in the source sentence, as follows: ¯ t ), p(yt |y&lt;t , x) = softmax(Wo h ¯ t = tanh(Wr [vt ; ht ]), h X e s, vt = αt (s)h (6) (7) (8) s αt (s) = e s) exp(h|t Wa h . (9) P | e 0 exp(h Wa hs0 ) s t (2) Here, vt represents a vector computed by the attention mechani"
I17-2074,P15-2097,0,0.0204916,"t to 100 and, improve computational efficiency, each batch consisted of sentences of the same length. The dimensionality of the distributed representations (word embeddings and hidden states) to was 300. The model parameters were trained using Adam. Following Jozefowicz et al. (2015), forget gate bias was initialized to 1.0, and the other gate biases were initialized to 0. In addition, we used dropout (at a rate of 0.2) for the LSTMs. Breadth-first search was used for decoding, with a beam width of 10 (Yuan, 2017). Six measures were utilized to evaluate the performance of the PSG model: GLEU (Napoles et al., 2015)3 , precision, recall, M 2 score (Dahlmeier and Ng, 2012), Word Error Rate (WER) (Jurafsky and Martin, 2008), and BLEU (Papineni et al., 2002). These measures are often used in GEC and machine translation research. Note that the precision, recall, and M 2 score measures excluded words appearing in both the source and proofread sentences from evaluation (Dahlmeier and Ng, 2012). 3.3 Results Table 2 shows the performance of the proposed method according to the above metrics. Two vari3 The hyperparameter λ of GLEU was set to 1. 439 • Proofread: The reserve players tackle it frantically so as not"
I17-2074,W13-3601,0,0.0356549,"Missing"
I17-2074,W14-1701,0,0.0248716,"Missing"
I17-2074,P02-1040,0,0.102307,"epresentations (word embeddings and hidden states) to was 300. The model parameters were trained using Adam. Following Jozefowicz et al. (2015), forget gate bias was initialized to 1.0, and the other gate biases were initialized to 0. In addition, we used dropout (at a rate of 0.2) for the LSTMs. Breadth-first search was used for decoding, with a beam width of 10 (Yuan, 2017). Six measures were utilized to evaluate the performance of the PSG model: GLEU (Napoles et al., 2015)3 , precision, recall, M 2 score (Dahlmeier and Ng, 2012), Word Error Rate (WER) (Jurafsky and Martin, 2008), and BLEU (Papineni et al., 2002). These measures are often used in GEC and machine translation research. Note that the precision, recall, and M 2 score measures excluded words appearing in both the source and proofread sentences from evaluation (Dahlmeier and Ng, 2012). 3.3 Results Table 2 shows the performance of the proposed method according to the above metrics. Two vari3 The hyperparameter λ of GLEU was set to 1. 439 • Proofread: The reserve players tackle it frantically so as not to miss the opportunity. learning the sub-task. That said, the other metrics decreased because the model made changes in many places where no"
I17-2074,D15-1044,0,0.049854,"ed pairs indicate that it is difficult to predict words that do not appear in the source sentences. However, we can see that weighting sub-task losses improved precision, recall, and M 2 score performance. We believe that active proofreading performance improved because of not overExperimental setup The batch size was set to 100 and, improve computational efficiency, each batch consisted of sentences of the same length. The dimensionality of the distributed representations (word embeddings and hidden states) to was 300. The model parameters were trained using Adam. Following Jozefowicz et al. (2015), forget gate bias was initialized to 1.0, and the other gate biases were initialized to 0. In addition, we used dropout (at a rate of 0.2) for the LSTMs. Breadth-first search was used for decoding, with a beam width of 10 (Yuan, 2017). Six measures were utilized to evaluate the performance of the PSG model: GLEU (Napoles et al., 2015)3 , precision, recall, M 2 score (Dahlmeier and Ng, 2012), Word Error Rate (WER) (Jurafsky and Martin, 2008), and BLEU (Papineni et al., 2002). These measures are often used in GEC and machine translation research. Note that the precision, recall, and M 2 score m"
I17-2074,J00-4006,0,\N,Missing
I17-2074,W11-2838,0,\N,Missing
L18-1477,W14-3348,0,0.010388,"ply the dropout strategy (Srivastava et al., 2014) with the ratio of 0.3 at the frame input layer. All the parameters are jointly learned at training time. We apply the beam search strategy at decoding time (beam size = 5). We implemented our system using Chainer (Tokui et al., 2015), and used the caption evaluation package provided by the Microsoft COCO Image Captioning Challenge (Chen et al., 2015). We performed a quantitative analysis of results based on four evaluation metrics, including BLEU (Papineni et al., 2002), a precision-based evaluation metric used in machine translation. METEOR (Denkowski and Lavie, 2014), an automatic metric for machine translation evaluation based on explicit word-to-word matching. CIDEr (Vedantam et al., 2014), an automatic consensus metric of image description quality. ROUGE-L (Lin, 2004), a recall-oriented evaluation metric popularly used in summarization. These metrics are common for evaluating image captioning and video description generation systems. 4.3. Figure 4: Histogram of number of words in a caption. mean = 7.03, min = 1, max = 45. Most of the captions contain a single activity and can be described using only one sentence as shown in Table 1. Table 1 shows 10 mo"
L18-1477,C16-1005,1,0.567956,"a single vector representation for a video, and then use the vector as input to the RNN decoder to generate a sentence, ignoring the temporal ordering of videos. Subsequently, they have proposed an RNN-based sequence-to-sequence model for generating descriptions of videos (Venugopalan et al., 2015a). They used 2 layers of RNN for both encoding the videos and decoding to sentences, so their model is able to learn This work has been done while the first author was working at AIRC, AIST. both a temporal structure of a sequence of video frames and a sequence model for generating sentences. Later, Laokulrat et al. (2016) applied the temporal attention mechanism to the sequence-to-sequence model to focus on a set of frames while generating each word of the describing sentence. With their attention mechanism, they were able to improve the scores without using any additional features. Yao et al. (2015) used CNN for encoding video frames and used RNN for building a language model at decoding time. They have also incorporated an attentional mechanism to video caption generation, taking into account both local and global temporal structures of videos by incorporating a spatial temporal 3D CNN. Venugopalan et al. (2"
L18-1477,W04-1013,0,0.029151,"We implemented our system using Chainer (Tokui et al., 2015), and used the caption evaluation package provided by the Microsoft COCO Image Captioning Challenge (Chen et al., 2015). We performed a quantitative analysis of results based on four evaluation metrics, including BLEU (Papineni et al., 2002), a precision-based evaluation metric used in machine translation. METEOR (Denkowski and Lavie, 2014), an automatic metric for machine translation evaluation based on explicit word-to-word matching. CIDEr (Vedantam et al., 2014), an automatic consensus metric of image description quality. ROUGE-L (Lin, 2004), a recall-oriented evaluation metric popularly used in summarization. These metrics are common for evaluating image captioning and video description generation systems. 4.3. Figure 4: Histogram of number of words in a caption. mean = 7.03, min = 1, max = 45. Most of the captions contain a single activity and can be described using only one sentence as shown in Table 1. Table 1 shows 10 most frequently occurring captions in the training set. The sentence ‘a man is playing a guitar’ appears 217 times which is the maximum number in the training data. Figure 4 shows the histogram of the number of"
L18-1477,P02-1040,0,0.102606,"Missing"
L18-1477,D14-1162,0,0.0798835,"Missing"
L18-1477,N15-1173,0,0.402828,"feature and then is injected into the RNN-based language model to produce a meaningful sentence. Later, Xu et al. (2015) has proposed an attention-based framework for image captioning which can selectively focus on a portion of an image while producing each word. However, researchers still cannot achieve a satisfying quality of video descriptions generated by computers yet. As with image captioning, automatic video description generation combines two fields of artificial intelligence, computer vision and natural language processing, and has also been tackled by the combination of RNN and CNN. Venugopalan et al. (2015b) proposed the first end-to-end system to translate a video to natural language by extending the CNN-RNN encoder-decoder framework for image captioning proposed by Vinyals et al. (2014) to generate descriptions for videos. They performed a mean pooling over CNN feature vectors of frames to generate a single vector representation for a video, and then use the vector as input to the RNN decoder to generate a sentence, ignoring the temporal ordering of videos. Subsequently, they have proposed an RNN-based sequence-to-sequence model for generating descriptions of videos (Venugopalan et al., 2015a"
N09-1048,2007.mtsummit-papers.9,0,0.832225,"e purely monolingual corpora, wherein frequency-based expectation-maximization (EM, refer to (Dempster et al., 1977)) algorithms and cognate clues play a central role (Koehn and Knight, 2002). Haghighi et al. (2008) presented a generative model based on canonical correlation analysis, in which monolingual features such as the context and orthographic substrings of words were taken into account. The other is multilingual parallel and comparable corpora (e.g., Wikipedia1 ), wherein features such as cooccurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008). In this paper, we focus on a special type of comparable corpus, parenthetical translations. The issue is motivated by the observation that Web pages and technical papers written in Asian languages (e.g., Chinese, Japanese) sometimes annotate named entities or technical terms with their translations in English inside a pair of parentheses. This is considered to be a traditional way to annotate new terms, personal names or other named entities with their English translations expressed in brackets. Formally, a parenthetical translation can be expressed by the following patter"
N09-1048,P04-1068,0,0.0151842,"for automatic lexicon mining. One is the purely monolingual corpora, wherein frequency-based expectation-maximization (EM, refer to (Dempster et al., 1977)) algorithms and cognate clues play a central role (Koehn and Knight, 2002). Haghighi et al. (2008) presented a generative model based on canonical correlation analysis, in which monolingual features such as the context and orthographic substrings of words were taken into account. The other is multilingual parallel and comparable corpora (e.g., Wikipedia1 ), wherein features such as cooccurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008). In this paper, we focus on a special type of comparable corpus, parenthetical translations. The issue is motivated by the observation that Web pages and technical papers written in Asian languages (e.g., Chinese, Japanese) sometimes annotate named entities or technical terms with their translations in English inside a pair of parentheses. This is considered to be a traditional way to annotate new terms, personal names or other named entities with their English translations expressed in brackets. Formally, a parenthetical translation can"
N09-1048,P08-1088,0,0.0250461,"neologisms (e.g., new technical terms, personal names, abbreviations, etc.), the difficulty of keeping up with the neologisms for lexicographers, etc. In order to turn the facts to a better way, one of the simplest strategies is to automatically mine large-scale lexicons from corpora such as the daily updated Web. 424 Generally, there are two kinds of corpora used for automatic lexicon mining. One is the purely monolingual corpora, wherein frequency-based expectation-maximization (EM, refer to (Dempster et al., 1977)) algorithms and cognate clues play a central role (Koehn and Knight, 2002). Haghighi et al. (2008) presented a generative model based on canonical correlation analysis, in which monolingual features such as the context and orthographic substrings of words were taken into account. The other is multilingual parallel and comparable corpora (e.g., Wikipedia1 ), wherein features such as cooccurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008). In this paper, we focus on a special type of comparable corpus, parenthetical translations. The issue is motivated by the observation that Web pages and technical papers written"
N09-1048,P07-2045,0,0.00890689,"nyins). Similar models have been compared in (Oh et al., 2006) for English-to-Korean and Englishto-Japanese transliteration. All the three models are phrase-based, i.e., adjacent phonemes or graphemes are allowable to form phrase-level transliteration units. Building the correspondences on phrase level can effectively tackle the missing or redundant 428 phoneme/grapheme problem during transliteration. For example, when Aamodt is transliterated into a m¯o t`e5 , a and d are missing. The problem can be easily solved when taking Aa and dt as single units for transliterating. Making use of Moses (Koehn et al., 2007), a phrase-based SMT system, Matthews (2007) has shown that the performance was comparable to recent state-of-the-art work (Jiang et al., 2007) in English-to-Chinese personal name transliteration. Matthews (2007) took transliteration as translation at the surface level. Inspired by his idea, we also implemented our transliteration models employing Moses. The main difference is that, while Matthews (2007) tokenized the English names into individual letters before training in Moses, we split them into syllables using the heuristic rules described in (Jiang et al., 2007), such that one syllable o"
N09-1048,W02-0902,0,0.0308471,"e continuous emergence of neologisms (e.g., new technical terms, personal names, abbreviations, etc.), the difficulty of keeping up with the neologisms for lexicographers, etc. In order to turn the facts to a better way, one of the simplest strategies is to automatically mine large-scale lexicons from corpora such as the daily updated Web. 424 Generally, there are two kinds of corpora used for automatic lexicon mining. One is the purely monolingual corpora, wherein frequency-based expectation-maximization (EM, refer to (Dempster et al., 1977)) algorithms and cognate clues play a central role (Koehn and Knight, 2002). Haghighi et al. (2008) presented a generative model based on canonical correlation analysis, in which monolingual features such as the context and orthographic substrings of words were taken into account. The other is multilingual parallel and comparable corpora (e.g., Wikipedia1 ), wherein features such as cooccurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008). In this paper, we focus on a special type of comparable corpus, parenthetical translations. The issue is motivated by the observation that Web pages and t"
N09-1048,P06-1142,0,0.0310302,"ation extraction, self-trained transliteration models and cascaded translation models are described in Section 4, 5, and 6, respectively. In Section 7, we evaluate our mined lexicons by Wikipedia. We conclude in Section 8 finally. 2 Related Work Numerous researchers have proposed a variety of automatic approaches to mine lexicons from the Web pages or other large-scale corpora. Shao and Ng (2004) presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies, combining both transliteration and context information. Kuo et al. (2006) used active learning and unsupervised learning for mining transliteration lexicon from the Web pages, in which an EM process was used for estimating the phonetic similarities between English syllables and Chinese characters. Cao et al. (2007) split parenthetical translation mining task into two parts, transliteration detection and translation detection. They employed a transliteration lexicon for constructing a grapheme-based transliteration model and annotated boundaries manually to train a classifier. Lin et al. (2008) applied a frequency-based word alignment approach, Competitive Link (Mel"
N09-1048,P08-1113,0,0.035358,"Missing"
N09-1048,C08-1071,0,0.0137806,"algorithm Chinese Web pages Require: L, U = {f1J (eI1 )}, T , M ¤L, (labeled) training set; U , (unlabeled) candidate set; T , test set; M, the transliteration or translation model. Parenthetical expression extraction{C(E)} S-MSRSeg Chinese word segmentation{c…(e…)} (Lin et al., 2008) Heuristic filtering{c…(e…)} Section 4 Bilingual abbreviation mining Section 5 Transliteration lexicon mining Section 6 Translation lexicon mining Figure 1: The system framework of mining lexicons from Chinese Web pages. (Zhu, 2007), such as self-training in word sense disambiguation (Yarowsky, 2005) and parsing (McClosky et al., 2008). In this paper, we apply selftraining to a new topic, lexicon mining. 3 System Framework and Self-Training Algorithm Figure 1 illustrates our system framework for mining lexicons from Chinese Web pages. First, parenthetical expressions matching Pattern 1 are extracted. Then, pre-parenthetical Chinese sequences are segmented into word sequences by S-MSRSeg2 (Gao et al., 2006). The initial parenthetical translation corpus is constructed by applying the heuristic rules defined in (Lin et al., 2008)3 . Based on this corpus, we mine three lexicons step by step, a bilingual abbreviation lexicon, a"
N09-1048,J00-2004,0,0.0753482,"Missing"
N09-1048,J03-1002,0,0.00467483,"Lin et al., 2008), wherein the lengthes of prefixes and suffixes of English words were assumed to be three bytes, we segment words into morphemes (sequences of prefixes, stems, and suffixes) by Morfessor 0.9.211 , an unsupervised language-independent morphological analyzer (Creutz and Lagus, 2007). We use the morpheme-level translation similarity explicitly in our cascaded translation model (Wu et al., 2008), which makes use of morpheme, word, and phrase level translation units. We train Moses to gain a phrase-level translation table. To gain a morpheme-level translation table, we run GIZA++ (Och and Ney, 2003) on both directions between English morphemes and Chinese characters, and take the intersection of Viterbi alignments. The Englishto-Chinese translation probabilities computed by GIZA++ are attached to each morpheme-character element in the intersection set. 6.1 Experiment The Wanfang Chinese-English technical term dictionary12 , which contains 525,259 entries in total, was used for training and testing. 10,000 entries were randomly selected as the test set and the remaining as the training set. Again, we investigated the scalability of the self-trained cascaded translation model by respective"
N09-1048,P02-1040,0,0.075371,"Missing"
N09-1048,C04-1089,0,0.175275,"n mining. One is the purely monolingual corpora, wherein frequency-based expectation-maximization (EM, refer to (Dempster et al., 1977)) algorithms and cognate clues play a central role (Koehn and Knight, 2002). Haghighi et al. (2008) presented a generative model based on canonical correlation analysis, in which monolingual features such as the context and orthographic substrings of words were taken into account. The other is multilingual parallel and comparable corpora (e.g., Wikipedia1 ), wherein features such as cooccurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008). In this paper, we focus on a special type of comparable corpus, parenthetical translations. The issue is motivated by the observation that Web pages and technical papers written in Asian languages (e.g., Chinese, Japanese) sometimes annotate named entities or technical terms with their translations in English inside a pair of parentheses. This is considered to be a traditional way to annotate new terms, personal names or other named entities with their English translations expressed in brackets. Formally, a parenthetical translation can be expressed by th"
N09-1048,2008.amta-papers.19,1,0.894739,"ods have been proposed. However, supervised 425 approaches are restricted by the quality and quantity of manually constructed training data, and unsupervised approaches are totally frequency-based without using any semantic clues. In contrast, we propose a semi-supervised framework for mining parenthetical translations. We apply a monolingual abbreviation extraction approach to bilingual abbreviation extraction. We construct an English-syllable to Chinese-pinyin transliteration model which is selftrained using phonemic similarity measurements. We further employ our cascaded translation model (Wu et al., 2008) which is self-trained based on morpheme-level translation similarity. This paper is organized as follows. We briefly review the related work in the next section. Our system framework and self-training algorithm is described in Section 3. Bilingual abbreviation extraction, self-trained transliteration models and cascaded translation models are described in Section 4, 5, and 6, respectively. In Section 7, we evaluate our mined lexicons by Wikipedia. We conclude in Section 8 finally. 2 Related Work Numerous researchers have proposed a variety of automatic approaches to mine lexicons from the Web"
N09-1048,P95-1026,0,0.403079,"Missing"
N09-1048,J05-4005,0,\N,Missing
N09-1048,H91-1026,0,\N,Missing
N16-1099,P14-1062,0,0.119321,"Missing"
N16-1099,D15-1166,0,0.0412953,"Missing"
N16-1099,W10-0911,0,0.0306102,"available at https://github. com/soskek/der-network 1 Raw Highlight ! ""Iron Man"" star Robert Downey Jr. presents a young child with a bionic arm! Context ! ( @entity1 ) @entity0 may be @entity2 in the popular @entity4 superhero ﬁlms , but he recently dealt in some advanced bionic technology himself . @entity0 recently presented a robotic arm to young @entity7 , a @entity8 boy who is missing his right arm from just above his elbow . the arm was made by @entity12 , a …! Query ! "" [X] "" star @entity0 presents a young child with a bionic arm! Answer @entity2! Introduction Machine reading systems (Poon et al., 2010; Richardson et al., 2013) can be tested on their ability to answer queries about contents of documents that they read, thus a central problem is how the information of documents should be organized in the system and retrieved by the queries. Recently, large scale datasets of document-queryanswer triples have been constructed from online newspaper articles and their summaries (Hermann et al., 2015), by replacing named entities in the summaries with placeholders to form Cloze (Taylor, 1953) style questions (Figure 1). These datasets have enabled training and testing of complicated neural networ"
N16-1099,D13-1020,0,0.0514996,"//github. com/soskek/der-network 1 Raw Highlight ! ""Iron Man"" star Robert Downey Jr. presents a young child with a bionic arm! Context ! ( @entity1 ) @entity0 may be @entity2 in the popular @entity4 superhero ﬁlms , but he recently dealt in some advanced bionic technology himself . @entity0 recently presented a robotic arm to young @entity7 , a @entity8 boy who is missing his right arm from just above his elbow . the arm was made by @entity12 , a …! Query ! "" [X] "" star @entity0 presents a young child with a bionic arm! Answer @entity2! Introduction Machine reading systems (Poon et al., 2010; Richardson et al., 2013) can be tested on their ability to answer queries about contents of documents that they read, thus a central problem is how the information of documents should be organized in the system and retrieved by the queries. Recently, large scale datasets of document-queryanswer triples have been constructed from online newspaper articles and their summaries (Hermann et al., 2015), by replacing named entities in the summaries with placeholders to form Cloze (Taylor, 1953) style questions (Figure 1). These datasets have enabled training and testing of complicated neural network models of hypothesized m"
N16-1099,D15-1044,0,0.00800371,"Missing"
N19-1401,W18-2706,0,0.0983287,"r-decoder models have been successfully applied to various natural language generation tasks including machine translation (Sutskever et al., 2014), summarization (Rush et al., 2015), and caption generation (Vinyals et al., 2015). Still, it is necessary to control the output length for abstractive summarization, which generates a summary for a given text while satisfying a space constraint. In fact, Figure 1 shows a large variance in output sequences produced by a widely used encoder-decoder model (Luong et al., 2015), which has no mechanism for controlling the length of the output sequences. Fan et al. (2018) trained embeddings that correspond to each output length to control the output sequence length. Since the embeddings for different lengths are independent, it is hard to generate a sequence of the length that is infrequent in training data. Thus, a method that can model any lengths continuously is required. Kikuchi et al. (2016) proposed two learning based methods for an LSTM encoder-decoder: LenEmb and LenInit. LenEmb inputs an embedding representing the remaining length in each decoding step. Since this approach also prepares embeddings for each length independently, it suffers from the sam"
N19-1401,D16-1140,0,0.241321,"es a summary for a given text while satisfying a space constraint. In fact, Figure 1 shows a large variance in output sequences produced by a widely used encoder-decoder model (Luong et al., 2015), which has no mechanism for controlling the length of the output sequences. Fan et al. (2018) trained embeddings that correspond to each output length to control the output sequence length. Since the embeddings for different lengths are independent, it is hard to generate a sequence of the length that is infrequent in training data. Thus, a method that can model any lengths continuously is required. Kikuchi et al. (2016) proposed two learning based methods for an LSTM encoder-decoder: LenEmb and LenInit. LenEmb inputs an embedding representing the remaining length in each decoding step. Since this approach also prepares embeddings for each length independently, it suffers from the same problem as that in Fan et al. (2018). On the other hand, LenInit can handle arbitrary lengths because it combines the scalar value of a desired length with a trainable embedding. LenInit initializes the LSTM cell of the decoder with the embedding depending on the scalar value of the desired length. Liu et al. (2018) incorporate"
N19-1401,P18-1007,0,0.0145031,"d length len. For Japanese, we used the JNC corpus, which contains a pair of the lead three sentences of a news article and its headline (Hitomi et al., 2019). The training set contains about 1.6M pairs3 . For English, we used sentence-headline pairs extracted from the annotated English Gigaword with the same pre-processing script used in the construction of the test set. The training set contains about 3.8M pairs. In this paper, we used a character-level decoder to control the number of characters. On the encoder side, we used subword units to construct the vocabulary (Sennrich et al., 2016; Kudo, 2018). We set the hyper-parameter to fit the vocabulary size to about 8k for Japanese and 16k for English. 3.2 Baselines We implemented two methods proposed by previous studies to control the output length and handle arbitrary lengths. We employed them and Transformer as baselines. LenInit Kikuchi et al. (2016) proposed LenInit, which controls the output length by initializing the LSTM cell m of the decoder as follows: m = len × b, (7) where b is a trainable vector. We incorporated this method with a widely used LSTM encoderdecoder model (Luong et al., 2015)4 . For a fair 2 https://github.com/faceb"
N19-1401,C18-1121,0,0.0317671,"Missing"
N19-1401,D17-1222,0,0.25101,"nstraint. LDP E returns an identical value at the position where the remaining length to the terminal position is the same. LRP E returns a similar value at the positions where the ratio of the remaining length to the terminal position is similar. Let us consider the d-th dimension as the simplest example. Since we obtain sin(pos/len) (or cos(pos/len)) at this dimension, the equations yield the same value when the remaining length ratio is the same, e.g., pos = 5, len = 10 and pos = 10, len = 20. We add LDP E (or LRP E) to the input layer of Transformer in the same manner as in Vaswani et al. (2017). In the training step, we assign the length of the correct output to len. In the test phase, we control the output length by assigning the desired length to len. 3 Experiments 3.1   pos = sin , 2i d 10000  pos = cos . 2i 10000 d (3) Datasets We conduct experiments on the headline generation task on Japanese and English datasets. The purpose of the experiments is to evaluate the ability of the proposed method to generate a summary of good quality within a specified length. We used JAMUL corpus as the Japanese test set (Hitomi et al., 2019). This test set contains three kinds of headlines f"
N19-1401,D18-1444,0,0.239945,"quired. Kikuchi et al. (2016) proposed two learning based methods for an LSTM encoder-decoder: LenEmb and LenInit. LenEmb inputs an embedding representing the remaining length in each decoding step. Since this approach also prepares embeddings for each length independently, it suffers from the same problem as that in Fan et al. (2018). On the other hand, LenInit can handle arbitrary lengths because it combines the scalar value of a desired length with a trainable embedding. LenInit initializes the LSTM cell of the decoder with the embedding depending on the scalar value of the desired length. Liu et al. (2018) incorporated such scalar values into the initial state of the decoder in a CNN encoder-decoder. These approaches deal with any length but it is reasonable to incorporate the distance to the desired terminal position into each decoding step such as in LenEmb. In this study, we focused on Transformer (Vaswani et al., 2017), which recently achieved the state-of-the-art score on the machine 3999 Proceedings of NAACL-HLT 2019, pages 3999–4004 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics translation task. We extend the sinusoidal positional encodin"
N19-1401,P16-1162,0,0.0204317,"headline as the desired length len. For Japanese, we used the JNC corpus, which contains a pair of the lead three sentences of a news article and its headline (Hitomi et al., 2019). The training set contains about 1.6M pairs3 . For English, we used sentence-headline pairs extracted from the annotated English Gigaword with the same pre-processing script used in the construction of the test set. The training set contains about 3.8M pairs. In this paper, we used a character-level decoder to control the number of characters. On the encoder side, we used subword units to construct the vocabulary (Sennrich et al., 2016; Kudo, 2018). We set the hyper-parameter to fit the vocabulary size to about 8k for Japanese and 16k for English. 3.2 Baselines We implemented two methods proposed by previous studies to control the output length and handle arbitrary lengths. We employed them and Transformer as baselines. LenInit Kikuchi et al. (2016) proposed LenInit, which controls the output length by initializing the LSTM cell m of the decoder as follows: m = len × b, (7) where b is a trainable vector. We incorporated this method with a widely used LSTM encoderdecoder model (Luong et al., 2015)4 . For a fair 2 https://git"
N19-1401,E17-2047,0,0.0568527,"0 0.018 0.009 0.009 +P E 0.021 0.001 0.003 0.021 0.013 0.010 Transformer+LRP E 0.191 0.362 0.043 0.120 0.058 0.133 +P E 0.183 0.406 0.052 0.138 0.081 0.154 Table 4: Variances of generated headlines. words unrelated to a source sentence. Thus, we applied a simple re-ranking to each n-best headlines generated by the proposed method (n = 20 in this experiment) based on the contained words. Our re-ranking strategy selects a headline that contains source-side words the most. Table 3 shows that Transformer+LRP E+P E with this reranking (+Re-ranking) achieved better scores than the state-of-the-art (Suzuki and Nagata, 2017). 3.4 Analysis of Output Length Following Liu et al. (2018), we used the variance of the generated summary lengths against the desired lengths as an indicator of the preciseness of the output lengths. We calculated variance (var) for n generated summaries as follows7 : n var = 1X |li − len|2 , n (8) i=1 where len is the desired length and li is the length of the generated summary. Table 4 shows the values of Equation (8) computed for each method and the desired lengths. This table indicates that LDP E could control the length of headlines precisely. In particular, LDP E could generate headline"
N19-1401,D18-1489,1,0.912592,"Missing"
N19-1401,P17-1101,0,0.0618479,"Missing"
N19-1401,D15-1166,0,0.145768,"tive extension of a sinusoidal positional encoding (Vaswani et al., 2017) to enable neural encoder-decoder model to preserves the length constraint. Unlike in previous studies where that learn embeddings representing each length, the proposed method can generate a text of any length even if the target length is not present in training data. The experimental results show that the proposed method can not only control the generation length but also improve the ROUGE scores. 1 Figure 1: Difference in number of characters between correct headlines and outputs of a widely used LSTM encoder-decoder (Luong et al., 2015) which is trained on sentence-headline pairs created by Rush et al. (2015) from the annotated English Gigaword corpus. The difference was investigated for 3,000 sentence-headline pairs randomly sampled from the test splits. Introduction Neural encoder-decoder models have been successfully applied to various natural language generation tasks including machine translation (Sutskever et al., 2014), summarization (Rush et al., 2015), and caption generation (Vinyals et al., 2015). Still, it is necessary to control the output length for abstractive summarization, which generates a summary for a give"
N19-1401,W12-3018,0,0.057401,"Missing"
okazaki-ananiadou-2006-clustering,C04-1087,1,\N,Missing
okazaki-ananiadou-2006-clustering,W05-1304,1,\N,Missing
P06-1049,N04-1015,0,0.534047,"are their method with chronological ordering as an application of multi-document summarization. As described above, several good strategies/heuristics to deal with the sentence ordering problem have been proposed. In order to integrate multiple strategies/heuristics, we have formalized them in a machine learning framework and have considered an algorithm to arrange sentences using the integrated strategy. and Hovy, 2001; Barzilay et al., 2002; Okazaki et al., 2004); and learning the natural order of sentences from large corpora not necessarily based on chronological information (Lapata, 2003; Barzilay and Lee, 2004). A newspaper usually disseminates descriptions of novel events that have occurred since the last publication. For this reason, ordering sentences according to their publication date is an effective heuristic for multidocument summarization (Lin and Hovy, 2001; McKeown et al., 1999). Barzilay et al. (2002) have proposed an improved version of chronological ordering by first grouping sentences into sub-topics discussed in the source documents and then arranging the sentences in each group chronologically. 3 Method We define notation a Â b to represent that sentence a precedes sentence b. We use"
P06-1049,P03-1069,0,0.238431,"y did not compare their method with chronological ordering as an application of multi-document summarization. As described above, several good strategies/heuristics to deal with the sentence ordering problem have been proposed. In order to integrate multiple strategies/heuristics, we have formalized them in a machine learning framework and have considered an algorithm to arrange sentences using the integrated strategy. and Hovy, 2001; Barzilay et al., 2002; Okazaki et al., 2004); and learning the natural order of sentences from large corpora not necessarily based on chronological information (Lapata, 2003; Barzilay and Lee, 2004). A newspaper usually disseminates descriptions of novel events that have occurred since the last publication. For this reason, ordering sentences according to their publication date is an effective heuristic for multidocument summarization (Lin and Hovy, 2001; McKeown et al., 1999). Barzilay et al. (2002) have proposed an improved version of chronological ordering by first grouping sentences into sub-topics discussed in the source documents and then arranging the sentences in each group chronologically. 3 Method We define notation a Â b to represent that sentence a pr"
P06-1049,C04-1108,1,0.899356,"Computational Linguistics orderings. The evaluation results showed that their method outperformed Lapata’s approach by a wide margin. They did not compare their method with chronological ordering as an application of multi-document summarization. As described above, several good strategies/heuristics to deal with the sentence ordering problem have been proposed. In order to integrate multiple strategies/heuristics, we have formalized them in a machine learning framework and have considered an algorithm to arrange sentences using the integrated strategy. and Hovy, 2001; Barzilay et al., 2002; Okazaki et al., 2004); and learning the natural order of sentences from large corpora not necessarily based on chronological information (Lapata, 2003; Barzilay and Lee, 2004). A newspaper usually disseminates descriptions of novel events that have occurred since the last publication. For this reason, ordering sentences according to their publication date is an effective heuristic for multidocument summarization (Lin and Hovy, 2001; McKeown et al., 1999). Barzilay et al. (2002) have proposed an improved version of chronological ordering by first grouping sentences into sub-topics discussed in the source documents"
P06-1049,P02-1040,0,0.076067,"ing a precision of continuous sentences in an ordering against the reference ordering. We define Pn to measure the precision of (14) Here, k is a parameter to control the range of the logarithmic average; and α is a small value in case if Pn is zero. We set k = 4 (ie, more than five continuous sentences are not included for evaluation) and α = 0.01. Average Continuity becomes 0 when evaluation and reference orderings share no continuous sentences and 1 when the two orderings are identical. In Figure 7, Average Continuity is calculated as 0.63. The underlying idea of Formula 14 was proposed by Papineni et al. (2002) as the BLEU metric for the semi-automatic evaluation of machine-translation systems. The original definition of the BLEU metric is to compare a machine-translated text with its reference translation by using the word n-grams. 4.3 Results of semi-automatic evaluation Table 2 reports the resemblance of orderings produced by six algorithms to the human-made ones with three metrics, Spearman’s rank correlation, Kendall’s rank correlation, and Average Continuity. The proposed method (AGL) outperforms the 391 Precision Pn RND 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 TOP PRE SUC AGL CHR Acknowledgment We"
P06-1049,J98-3005,0,\N,Missing
P06-1049,W07-2312,0,\N,Missing
P06-1049,P00-1010,0,\N,Missing
P06-1049,H05-1079,0,\N,Missing
P06-1049,J00-3005,0,\N,Missing
P06-1049,W06-1662,0,\N,Missing
P06-1049,W01-1313,0,\N,Missing
P06-1049,N03-2019,0,\N,Missing
P06-1049,P06-1051,0,\N,Missing
P06-1049,J06-4002,0,\N,Missing
P06-1049,P01-1023,0,\N,Missing
P06-1049,W05-1621,0,\N,Missing
P06-1049,W02-2111,0,\N,Missing
P06-1049,W02-2112,0,\N,Missing
P06-2083,P02-1021,0,0.219363,"Missing"
P06-2083,W01-0516,0,0.235171,"Missing"
P09-1003,P98-1013,0,0.424254,"Missing"
P09-1003,S07-1018,0,0.157223,"Missing"
P09-1003,C04-1100,0,0.3293,"training instances. Some recent studies have addressed alternative approaches to generalizing semantic roles across different frames (Gordon and Swanson, 2007; Zapi1 Introduction Semantic Role Labeling (SRL) is a task of analyzing predicate-argument structures in texts. More specifically, SRL identifies predicates and their arguments with appropriate semantic roles. Resolving surface divergence of texts (e.g., voice of verbs and nominalizations) into unified semantic representations, SRL has attracted much attention from researchers into various NLP applications including question answering (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007; 19 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 19–27, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP Recipient Transfer::Recipient Giving::Recipient Agent Buyer Commerce_sell::Buyer Commerce_buy::Buyer Transfer::Donor Giving::Donor Commerce_sell::Seller Donor Seller Commerce_buy::Seller role-to-role relation hierarchical class thematic role role descriptor Figure 2: An example of role groupings using different criteria. tic role, Baldewein et al. (2004) re-used the training instances of other roles that were simila"
P09-1003,W04-0817,0,0.429327,"Missing"
P09-1003,J05-1004,0,0.394865,"Missing"
P09-1003,S07-1102,0,0.026985,"Missing"
P09-1003,D07-1002,0,0.0954278,"t studies have addressed alternative approaches to generalizing semantic roles across different frames (Gordon and Swanson, 2007; Zapi1 Introduction Semantic Role Labeling (SRL) is a task of analyzing predicate-argument structures in texts. More specifically, SRL identifies predicates and their arguments with appropriate semantic roles. Resolving surface divergence of texts (e.g., voice of verbs and nominalizations) into unified semantic representations, SRL has attracted much attention from researchers into various NLP applications including question answering (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007; 19 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 19–27, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP Recipient Transfer::Recipient Giving::Recipient Agent Buyer Commerce_sell::Buyer Commerce_buy::Buyer Transfer::Donor Giving::Donor Commerce_sell::Seller Donor Seller Commerce_buy::Seller role-to-role relation hierarchical class thematic role role descriptor Figure 2: An example of role groupings using different criteria. tic role, Baldewein et al. (2004) re-used the training instances of other roles that were similar to the target role. A"
P09-1003,P05-1022,0,0.0285632,"Missing"
P09-1003,P03-1002,0,0.363972,"e roles having a small number of instances affect the average more than the micro average. Micro 89.00 90.78 90.23 90.25 90.36 89.50 91.10 Macro 68.50 76.58 76.19 72.41 74.51 69.21 75.92 −Err. 0.00 16.17 11.23 11.40 12.38 4.52 19.16 Table 1: The accuracy and error reduction rate of role classification for each type of role group. 6.1 Experimental settings We constructed a baseline classifier that uses only the x-role features. The feature design is similar to that of the previous studies (M`arquez et al., 2008). The characteristics of x are: frame, frame evoking word, head word, content word (Surdeanu et al., 2003), first/last word, head word of left/right sister, phrase type, position, voice, syntactic path (directed/undirected/partial), governing category (Gildea and Jurafsky, 2002), WordNet supersense in the phrase, combination features of frame evoking word & headword, combination features of frame evoking word & phrase type, and combination features of voice & phrase type. We also used PoS tags and stem forms as extra features of any word-features. We employed Charniak and Johnson’s reranking parser (Charniak and Johnson, 2005) to analyze syntactic trees. As an alternative for the traditional named"
P09-1003,W06-1670,0,0.0267192,"Missing"
P09-1003,N07-1069,0,0.220313,"ormance of the machinelearning approach, because these definitions produce many roles that have few training instances. PropBank defines a frame for each sense of predicates (e.g., buy.01), and semantic roles are defined in a frame-specific manner (e.g., buyer and seller for buy.01). In addition, these roles are associated with tags such as ARG0-5 and AM-*, which are commonly used in different frames. Most SRL studies on PropBank have used these tags in order to gather a sufficient amount of training data, and to generalize semantic-role classifiers across different frames. However, Yi et al. (2007) reported that tags ARG2–ARG5 were inconsistent and not that suitable as training instances. Some recent studies have addressed alternative approaches to generalizing semantic roles across different frames (Gordon and Swanson, 2007; Zapi1 Introduction Semantic Role Labeling (SRL) is a task of analyzing predicate-argument structures in texts. More specifically, SRL identifies predicates and their arguments with appropriate semantic roles. Resolving surface divergence of texts (e.g., voice of verbs and nominalizations) into unified semantic representations, SRL has attracted much attention from"
P09-1003,J02-3001,0,0.965792,". For instance, FrameNet specifies the constraint that Self motion::Area should be filled by phrases whose semantic type is Location. Since these types suggest a coarse-grained categorization of semantic roles, we construct role groups that contain roles whose semantic types are identical. c˜ = argmax c∈{m(y)|y∈Yf } Pm (c|f, x). (2) Here, Pm (c|f, x) presents the probability of the role group c for f and x. The role y˜ is determined uniquely iff a single role y ∈ Yf is associated with c˜. Some previous studies have employed this idea to remedy the data sparseness problem in the training data (Gildea and Jurafsky, 2002). However, we cannot apply this approach when multiple roles in Yf are contained in the same class. For example, we can construct a semantic-type group St::State of affairs in which Giving::Reason and Giving::Means are included, as illustrated in Figure 4. If c˜ = St::State of affairs, we cannot disambiguate which original role is correct. In addition, it may be more effective to use various 4.4 Thematic roles of VerbNet VerbNet thematic roles are 23 frame-independent semantic categories for arguments of verbs, such as Agent, Patient, Theme and Source. These categories have been used as consis"
P09-1003,P08-1063,0,0.0768702,"Missing"
P09-1003,P06-1117,0,0.0551762,"Missing"
P09-1003,P07-1025,0,0.0341839,"defined in a frame-specific manner (e.g., buyer and seller for buy.01). In addition, these roles are associated with tags such as ARG0-5 and AM-*, which are commonly used in different frames. Most SRL studies on PropBank have used these tags in order to gather a sufficient amount of training data, and to generalize semantic-role classifiers across different frames. However, Yi et al. (2007) reported that tags ARG2–ARG5 were inconsistent and not that suitable as training instances. Some recent studies have addressed alternative approaches to generalizing semantic roles across different frames (Gordon and Swanson, 2007; Zapi1 Introduction Semantic Role Labeling (SRL) is a task of analyzing predicate-argument structures in texts. More specifically, SRL identifies predicates and their arguments with appropriate semantic roles. Resolving surface divergence of texts (e.g., voice of verbs and nominalizations) into unified semantic representations, SRL has attracted much attention from researchers into various NLP applications including question answering (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007; 19 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 19–27, c Sunt"
P09-1003,J08-2001,0,0.228549,"Missing"
P09-1003,W05-0630,0,0.0142305,"ns. 2 3 Role Classification SRL is a complex task wherein several problems are intertwined: frame-evoking word identification, frame disambiguation (selecting a correct frame from candidates for the evoking word), rolephrase identification (identifying phrases that fill semantic roles), and role classification (assigning correct roles to the phrases). In this paper, we focus on role classification, in which the role generalization is particularly critical to the machine learning approach. In the role classification task, we are given a sentence, a frame evoking word, a frame, and Related Work Moschitti et al. (2005) first classified roles by using four coarse-grained classes (Core Roles, Adjuncts, Continuation Arguments and Co-referring Arguments), and built a classifier for each coarsegrained class to tag PropBank ARG tags. Even though the initial classifiers could perform rough estimations of semantic roles, this step was not able to solve the ambiguity problem in PropBank ARG2-5. When training a classifier for a seman20 Commerce_pay::Buyer C_pay::Buyer Evading::Evader Evading::Evader Giving::Donor Avoiding::Agent GIVING::Donor Avoiding::Agent Intentionall_act::Agent Intentionally_ACT::Agent Theme::Age"
P09-1003,P07-1098,0,0.0419443,"Missing"
P09-1003,C98-1013,0,\N,Missing
P09-1102,P08-2016,0,0.319572,"tperformed five out of six state-of-the-art abbreviation recognizers. 1 Introduction Abbreviations represent fully expanded forms (e.g., hidden markov model) through the use of shortened forms (e.g., HMM). At the same time, abbreviations increase the ambiguity in a text. For example, in computational linguistics, the acronym HMM stands for hidden markov model, whereas, in the field of biochemistry, HMM is generally an abbreviation for heavy meromyosin. Associating abbreviations with their fully expanded forms is of great importance in various NLP applications (Pakhomov, 2002; Yu et al., 2006; HaCohen-Kerner et al., 2008). The core technology for abbreviation disambiguation is to recognize the abbreviation defini905 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 905–913, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP pol y g l y c ol i c ac i d PS S S P S S S S S S S S PS S S (a): English Abbreviation Generation [PGA] Institute of History and Philology at Academia Sinica 历史语 言 研究所 S P P S S S P [史语所] y2 ym y1 y2 ym h1 h2 hm x1 x2 xm x1 x2 xm CRF (b): Chinese Abbreviation Generation y1 DPLVM Figure 2: CRF vs. DPLVM. Variables x, y, and h represent observation,"
P09-1102,C08-1083,1,0.911993,"1.9 92.0 90.0 87.1 96.9 94.8 97.8 97.7 98.1 F 95.9 90.5 94.0 92.1 91.0 97.1 92.1 95.9 95.1 96.1 Table 6: Results of English abbreviation recognition. belings. Other labelings are impossible, because they will generate an abbreviation that is not AP. If the first or second labeling is generated, AP is selected as an abbreviation of arterial pressure. If the third or fourth labeling is generated, then AP is selected as an abbreviation of cannulate for arterial pressure. Finally, the fifth labeling (NULL) indicates that AP is not an abbreviation. To evaluate the recognizer, we use the corpus6 of Okazaki et al. (2008), which contains 864 abbreviation definitions collected from 1,000 MEDLINE scientific abstracts. In implementing the recognizer, we simply use the model from the abbreviation generator, with the same feature templates (31,868 features) and training method; the major difference is in the restriction (according to the PE) of the decoding stage and penalizing the probability values of the NULL labelings7 . For the evaluation metrics, following Okazaki et al. (2008), we use precision (P = k/m), recall (R = k/n), and the F-score defined by Recognition as a Generation Task We directly migrate this m"
P09-1102,P02-1021,0,0.485619,"sed model worked robustly, and outperformed five out of six state-of-the-art abbreviation recognizers. 1 Introduction Abbreviations represent fully expanded forms (e.g., hidden markov model) through the use of shortened forms (e.g., HMM). At the same time, abbreviations increase the ambiguity in a text. For example, in computational linguistics, the acronym HMM stands for hidden markov model, whereas, in the field of biochemistry, HMM is generally an abbreviation for heavy meromyosin. Associating abbreviations with their fully expanded forms is of great importance in various NLP applications (Pakhomov, 2002; Yu et al., 2006; HaCohen-Kerner et al., 2008). The core technology for abbreviation disambiguation is to recognize the abbreviation defini905 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 905–913, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP pol y g l y c ol i c ac i d PS S S P S S S S S S S S PS S S (a): English Abbreviation Generation [PGA] Institute of History and Philology at Academia Sinica 历史语 言 研究所 S P P S S S P [史语所] y2 ym y1 y2 ym h1 h2 hm x1 x2 xm x1 x2 xm CRF (b): Chinese Abbreviation Generation y1 DPLVM Figure 2: CRF vs. DPLV"
P09-1102,W01-0516,0,0.56495,"ocally defined in a document. Therefore, a number of studies have attempted to model the generation processes of abbreviations: e.g., inferring the abbreviating mechanism of the hidden markov model into HMM. An obvious approach is to manually design rules for abbreviations. Early studies attempted to determine the generic rules that humans use to intuitively abbreviate given words (Barrett and Grems, 1960; Bourne and Ford, 1961). Since the late 1990s, researchers have presented various methods by which to extract abbreviation definitions that appear in actual texts (Taghva and Gilbreth, 1999; Park and Byrd, 2001; Wren and Garner, 2002; Schwartz and Hearst, 2003; Adar, 2004; Ao and Takagi, 2005). For example, Schwartz and Hearst (2003) implemented a simple algorithm that mapped all alpha-numerical letters in an abbreviation to its expanded form, starting from the end of both the abbreviation and its expanded forms, and moving from right to left. These studies performed highly, especially for English abbreviations. However, a more extensive investigation of abbreviations is needed in order to further improve definition extraction. In addition, we cannot simply transfer the knowledge of the hand-crafted"
P09-1102,N03-1028,0,0.0125058,"uring training and validation, we set σ = 1 for the DPLVM generators. We also set four latent variables for each label, in order to make a compromise between accuracy and efficiency. Note that, for the label encoding with global information, many label transitions (e.g., P2 S3 ) are actually impossible: the label transitions are strictly constrained, i.e., yi yi+1 ∈ {Pj Sj , Pj Pj+1 , Sj Pj+1 , Sj Sj }. These constraints on the model topology (forward-backward lattice) are enforced by giving appropriate features a weight of −∞, thereby forcing all forbidden labelings to have zero probability. Sha and Pereira (2003) originally proposed this concept of implementing transition restrictions. Table 1: Language-independent features (#1 to #3), Chinese-specific features (#4 through #7), and English-specific features (#8 through #11). the other hand, such duplication detection features are not so useful for English abbreviations. Feature templates #8–#11 are designed for English abbreviations. Features #8 and #9 encode the orthographic information of expanded forms. Features #10 and #11 represent a contextual n-gram with a large window size. Since the number of letters in Chinese (more than 10K characters) is m"
P09-1102,W05-1304,1,0.408495,"y outperform previous abbreviation generation studies. In addition, we apply the proposed models to the task of abbreviation recognition, in which a model extracts the abbreviation definitions in a given text. To the extent of our knowledge, this is the first model that can perform both abbreviation generation and recognition at the state-of-the-art level, across different languages and with a simple feature set. other languages, including Chinese and Japanese, do not have word boundaries or case sensitivity. A number of recent studies have investigated the use of machine learning techniques. Tsuruoka et al. (2005) formalized the processes of abbreviation generation as a sequence labeling problem. In the present study, each character in the expanded form is tagged with a label, y ∈ {P, S}1 , where the label P produces the current character and the label S skips the current character. In Figure 1 (a), the abbreviation PGA is generated from the full form polyglycolic acid because the underlined characters are tagged with P labels. In Figure 1 (b), the abbreviation is generated using the 2nd and 3rd characters, skipping the subsequent three characters, and then using the 7th character. In order to formaliz"
P09-1115,H05-1091,0,0.508932,"ate surface patterns. The method is used widely. However, even when patterns are generated from well-written texts, frequent pattern mining is non-trivial because the number of unique patterns is loose, but many patterns are non-discriminative and correlated. A salient challenge and research interest for frequent pattern mining is abstraction away from different surface realizations of semantic relations to discover discriminative patterns efficiently. Linguistic analysis is another effective technology for semantic relation extraction, as described in many reports such as (Kambhatla, 2004); (Bunescu and Mooney, 2005); (Harabagiu et al., 2005); (Nguyen et al., 2007). Currently, linguistic approaches for semantic relation extraction are mostly supervised, relying on pre-specification of the desired relation or initial seed words or patterns from hand-coding. The common process is to generate linguistic features based on analyses of the syntactic features, dependency, or shallow semantic structure of text. Then the system is trained to identify entity pairs that assume a relation and to classify them into pre-defined relations. The advantage of these methods is that they use linguistic technologies to learn"
P09-1115,I05-2045,0,0.465034,"support of ranked relational terms. 4.3.1 Relational Term Ranking To collect relational terms as indicators for each concept pair, we look for verbs and nouns from qualified sentences in the snippets instead of simply finding verbs. Using only verbs as relational terms might engender the loss of various important relations, e.g. noun relations “CEO”, “founder” between a person and a company. Therefore, for each concept pair, a list of relational terms is collected. Then all the collected terms of all concept pairs are combined and ranked using an entropybased algorithm which is described in (Chen et al., 2005). With their algorithm, the importance of terms can be assessed using the entropy criterion, which is based on the assumption that a term is irrelevant if its presence obscures the separability of the dataset. After the ranking, we obtain a global ranked list of relational terms Tall for the whole dataset (all the concept pairs). For each concept pair, a local list of relational terms Tcp is sorted according to the terms’ order in Tall . Then from the relational term list Tcp , a keyword tcp is selected Pattern ec ceo rc ceo rc found ec rc be ceo of ec ec assign rc as ceo ceo of ec rc Pattern"
P09-1115,P04-1054,0,0.0907192,"Missing"
P09-1115,P07-1030,0,0.0155373,"m. 2 Related Work (Hasegawa et al., 2004) introduced a method for discovering a relation by clustering pairs of cooccurring entities represented as vectors of context features. They used a simple representation of contexts; the features were words in sentences between the entities of the candidate pairs. (Turney, 2006) presented an unsupervised algorithm for mining the Web for patterns expressing implicit semantic relations. Given a word pair, the output list of lexicon-syntactic patterns was ranked by pertinence, which showed how well each pattern expresses the relations between word pairs. (Davidov et al., 2007) proposed a method for unsupervised discovery of concept specific relations, requiring initial word seeds. That method used pattern clusters to define general relations, specific to a given concept. (Davidov and Rappoport, 2008) presented an approach to discover and represent general relations present in an arbitrary corpus. That approach incorporated a fully unsupervised algorithm for pattern cluster discovery, which searches, clusters, and merges highfrequency patterns around randomly selected concepts. The field of Unsupervised Relation Identification (URI)—the task of automatically discove"
P09-1115,P06-1015,0,0.127745,"Missing"
P09-1115,P06-2086,0,0.0432868,"Missing"
P09-1115,P06-1040,0,0.0262847,"rmation contribute greatly to the coverage of relation extraction. • The combination of these patterns produces a clustering method to achieve high precision for different Information Extraction applications, especially for bootstrapping a high-recall semi-supervised relation extraction system. 2 Related Work (Hasegawa et al., 2004) introduced a method for discovering a relation by clustering pairs of cooccurring entities represented as vectors of context features. They used a simple representation of contexts; the features were words in sentences between the entities of the candidate pairs. (Turney, 2006) presented an unsupervised algorithm for mining the Web for patterns expressing implicit semantic relations. Given a word pair, the output list of lexicon-syntactic patterns was ranked by pertinence, which showed how well each pattern expresses the relations between word pairs. (Davidov et al., 2007) proposed a method for unsupervised discovery of concept specific relations, requiring initial word seeds. That method used pattern clusters to define general relations, specific to a given concept. (Davidov and Rappoport, 2008) presented an approach to discover and represent general relations pres"
P09-1115,P06-1104,0,0.0180981,"of an entitled concept or related concept are parsed into dependency structures. We define dependency patterns as sub-paths of the shortest dependency path between a concept pair for two reasons. One is that the shortest path dependency kernels outperform dependency tree kernels by offering a highly condensed representation of the information needed to assess their relation (Bunescu and Mooney, 2005). The other reason is that embedded structures of the linguistic representation are important for obtaining good coverage of the pattern acquisition, as explained in (Culotta and Sorensen, 2005); (Zhang et al., 2006). The process of inducing dependency patterns has two steps. 1. Shortest dependency path inducement. From the original dependency tree structure by parsing the selected sentence for each concept pair, we first induce the shortest dependency path with the entitled concept and related concept. 2. Dependency pattern generation. We use a frequent tree-mining algorithm (Zaki, 2002) to generate sub-paths as dependency patterns from the shortest dependency path for relation clustering. paper, we select centroids based on the keyword tcp of each concept pair. First of all, all concept pairs are groupe"
P09-1115,P08-1027,0,0.0826921,"approaches, which use a large unlabeled corpus, manual construction of a small set of seeds known as true instances of the target entity or relation is susceptible to arbitrary human decisions. Consequently, a need exists for development of semantic information-retrieval algorithms that can operate in a manner that is as unsupervised as possible. Currently, the leading methods in unsupervised information extraction collect redundancy information from a local corpus or use the Web as a corpus (Pantel and Pennacchiotti, 2006); (Banko et al., 2007); (Bollegala et al., 2007): (Fan et al., 2008); (Davidov and Rappoport, 2008). The standard process is to scan or search the corpus to collect co-occurrences of word pairs with strings between them, and then to calculate term co-occurrence or generate surface patterns. The method is used widely. However, even when patterns are generated from well-written texts, frequent pattern mining is non-trivial because the number of unique patterns is loose, but many patterns are non-discriminative and correlated. A salient challenge and research interest for frequent pattern mining is abstraction away from different surface realizations of semantic relations to discover discrimin"
P09-1115,P04-1053,0,0.753149,"ple of bridging the gap separating “deep” linguistic technology and redundant Web information for Information Extraction tasks. • Our experimental results reveal that relations are extractable with good precision using linguistic patterns, whereas surface patterns from Web frequency information contribute greatly to the coverage of relation extraction. • The combination of these patterns produces a clustering method to achieve high precision for different Information Extraction applications, especially for bootstrapping a high-recall semi-supervised relation extraction system. 2 Related Work (Hasegawa et al., 2004) introduced a method for discovering a relation by clustering pairs of cooccurring entities represented as vectors of context features. They used a simple representation of contexts; the features were words in sentences between the entities of the candidate pairs. (Turney, 2006) presented an unsupervised algorithm for mining the Web for patterns expressing implicit semantic relations. Given a word pair, the output list of lexicon-syntactic patterns was ranked by pertinence, which showed how well each pattern expresses the relations between word pairs. (Davidov et al., 2007) proposed a method f"
P09-1115,P04-3022,0,0.0333775,"occurrence or generate surface patterns. The method is used widely. However, even when patterns are generated from well-written texts, frequent pattern mining is non-trivial because the number of unique patterns is loose, but many patterns are non-discriminative and correlated. A salient challenge and research interest for frequent pattern mining is abstraction away from different surface realizations of semantic relations to discover discriminative patterns efficiently. Linguistic analysis is another effective technology for semantic relation extraction, as described in many reports such as (Kambhatla, 2004); (Bunescu and Mooney, 2005); (Harabagiu et al., 2005); (Nguyen et al., 2007). Currently, linguistic approaches for semantic relation extraction are mostly supervised, relying on pre-specification of the desired relation or initial seed words or patterns from hand-coding. The common process is to generate linguistic features based on analyses of the syntactic features, dependency, or shallow semantic structure of text. Then the system is trained to identify entity pairs that assume a relation and to classify them into pre-defined relations. The advantage of these methods is that they use lingu"
P09-1115,N07-2032,1,0.524232,"er, even when patterns are generated from well-written texts, frequent pattern mining is non-trivial because the number of unique patterns is loose, but many patterns are non-discriminative and correlated. A salient challenge and research interest for frequent pattern mining is abstraction away from different surface realizations of semantic relations to discover discriminative patterns efficiently. Linguistic analysis is another effective technology for semantic relation extraction, as described in many reports such as (Kambhatla, 2004); (Bunescu and Mooney, 2005); (Harabagiu et al., 2005); (Nguyen et al., 2007). Currently, linguistic approaches for semantic relation extraction are mostly supervised, relying on pre-specification of the desired relation or initial seed words or patterns from hand-coding. The common process is to generate linguistic features based on analyses of the syntactic features, dependency, or shallow semantic structure of text. Then the system is trained to identify entity pairs that assume a relation and to classify them into pre-defined relations. The advantage of these methods is that they use linguistic technologies to learn semantic information from different surface expre"
P13-1038,S07-1103,0,0.331928,"illion, 5,200,390) and unit conversions (e.g., square kilometers and acres). 4. We demonstrate the effectiveness of this approach, reporting experimental results and analyses in detail. Although it would be ideal to evaluate the impact of this study on the overall RTE task, we evaluate each phase separately. We do this because the existing RTE data sets tend to exhibit very diverse linguistic phenomena, and it is difficult to employ such data for evaluating the real impact of this study. 2 Related work Some recent studies delve deeper into the semantic interpretation of numerical expressions. Aramaki et al. (2007) focused on the physical size of an entity to predict the semantic relation between entities. For example, knowing that a book has a physical size of 20 cm × 25 cm and that a library has a size of 10 m × 10 m, we can estimate that a library contains a book (content-container relation). Their method acquires knowledge about entity size from the Web (by issuing queries like “book (*cm x *cm)”), and integrates the knowledge as features for the classification of relations. Surprisingly, NLP research has paid little attention to semantic processing of numerical expressions. This is evident when we"
P13-1038,bentivogli-etal-2010-building,0,0.0160476,"ailment recognition (RTE) involves a wide range of semantic inferences to determine whether the meaning of a hypothesis sentence (h) can be inferred from another text (t) (Dagan et al., 2006). Although several evaluation campaigns (e.g., PASCAL/TAC RTE challenges) have made significant progress, the RTE community recognizes the necessity of a deeper understanding of the core phenomena involved in textual inference. Such recognition comes from the ideas that crucial progress may derive from decomposing the complex RTE task into basic phenomena and from solving each basic phenomenon separately (Bentivogli et al., 2010; Sammons et al., 2010; Cabrio and Magnini, 2011; Toledo et al., 2012). 1. We examine instances in existing RTE corpora, categorize them into groups in terms of the necessary semantic inferences, and discuss the impact of this study for solving RTE problems with numerical expressions. 2. We describe a method of normalizing numerical expressions referring to the same amount in text into a unified semantic representation. 3. We present approaches for aggregating numerical common sense from examples of numerical expressions and for judging whether a given amount is large, small, or normal. 382 Pr"
P13-1038,W11-0135,0,0.0119869,"of semantic inferences to determine whether the meaning of a hypothesis sentence (h) can be inferred from another text (t) (Dagan et al., 2006). Although several evaluation campaigns (e.g., PASCAL/TAC RTE challenges) have made significant progress, the RTE community recognizes the necessity of a deeper understanding of the core phenomena involved in textual inference. Such recognition comes from the ideas that crucial progress may derive from decomposing the complex RTE task into basic phenomena and from solving each basic phenomenon separately (Bentivogli et al., 2010; Sammons et al., 2010; Cabrio and Magnini, 2011; Toledo et al., 2012). 1. We examine instances in existing RTE corpora, categorize them into groups in terms of the necessary semantic inferences, and discuss the impact of this study for solving RTE problems with numerical expressions. 2. We describe a method of normalizing numerical expressions referring to the same amount in text into a unified semantic representation. 3. We present approaches for aggregating numerical common sense from examples of numerical expressions and for judging whether a given amount is large, small, or normal. 382 Proceedings of the 51st Annual Meeting of the Asso"
P13-1038,P10-1133,0,0.235393,"uncertainty about the quantity (e.g., weight airbus A380 pounds). Given a query, their approach retrieves documents relevant to the query and identifies the quantities of numerical expressions in the retrieved documents. They also proposed methods for enumerating and ranking the candidates for the consensus quantity intervals. Even though our study shares a similar spirit (modeling of consensus for quantities) with Banerjee et al. (2009), their goal is different: to determine ground-truth values for queries. In question answering, to help “sanity check” answers with numerical values that were Davidov and Rappoport (2010) presented a method for the extraction from the Web and approximation of numerical object attributes such as height and weight. Given an object-attribute pair, the study expands the object into a set of comparable objects and then approximates the numerical values even when no exact value can be found in a text. Aramaki et al. (2007) and Davidov and Rappoport (2010) rely on hand-crafted patterns (e.g., “Object is * [unit] tall”), focusing on a specific set of numerical attributes (e.g., height, weight, size). In contrast, this study can handle any kind of target and situation that is quantifie"
P13-1038,P11-2057,0,0.0339962,"Missing"
P13-1038,W06-1415,0,0.0303271,"ssuing queries like “book (*cm x *cm)”), and integrates the knowledge as features for the classification of relations. Surprisingly, NLP research has paid little attention to semantic processing of numerical expressions. This is evident when we compare with temporal expressions, for which corpora (e.g., ACE20051 , TimeBank2 ) were developed with annotation schemes (e.g., TIMEX3 , TimeML4 ). Several studies deal with numerical expressions in the context of information extraction (Bakalov et al., 2011), information retrieval (Fontoura et al., 2006; Yoshida et al., 2010), and question answering (Moriceau, 2006). Numbers such as product prices and weights have been common targets of information extraction. Fontoura et al. (2006) and Yoshida et al. (2010) presented algorithms and data structures that allow number-range queries for searching documents. However, these studies do not interpret the quantity (e.g., 3,000,000,000) of a numerical expression (e.g., 3b people), but rather treat numerical expressions as strings. Banerjee et al. (2009) focused on quantity consensus queries, in which there is uncertainty about the quantity (e.g., weight airbus A380 pounds). Given a query, their approach retrieves"
P13-1038,P10-1122,0,0.0147428,"involves a wide range of semantic inferences to determine whether the meaning of a hypothesis sentence (h) can be inferred from another text (t) (Dagan et al., 2006). Although several evaluation campaigns (e.g., PASCAL/TAC RTE challenges) have made significant progress, the RTE community recognizes the necessity of a deeper understanding of the core phenomena involved in textual inference. Such recognition comes from the ideas that crucial progress may derive from decomposing the complex RTE task into basic phenomena and from solving each basic phenomenon separately (Bentivogli et al., 2010; Sammons et al., 2010; Cabrio and Magnini, 2011; Toledo et al., 2012). 1. We examine instances in existing RTE corpora, categorize them into groups in terms of the necessary semantic inferences, and discuss the impact of this study for solving RTE problems with numerical expressions. 2. We describe a method of normalizing numerical expressions referring to the same amount in text into a unified semantic representation. 3. We present approaches for aggregating numerical common sense from examples of numerical expressions and for judging whether a given amount is large, small, or normal. 382 Proceedings of the 51st"
P13-1038,N10-4008,0,\N,Missing
P13-3016,W11-2207,0,0.126335,"lly awful” as an example. The complaint gives viewers a negative impression of Company A and can increase the number of people who think the company is bad. Some complaints are expressed by a specific 110 Proceedings of the ACL Student Research Workshop, pages 110–116, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics example. This comment is probably complaining about Company B but not Company A. In contrast, most of the previous work on sentiment analysis in social media does not consider these kinds of problems (Barbosa and Feng, 2010; Davidov et al., 2010; Speriosu et al., 2011). Switching to the behavior of each user, in social media we often see that users who have similar ideas will tend to cooperate with each other. In fact, previous work suggests that users who have the same opinions tend to create links to each other (Conover et al., 2011b; Yang et al., 2012). Because chronic critics share the purpose of attacking some target’s reputation, they may also decide to cooperate. For this reason, to detect chronic critics, we believe that information about the connections among users will be effective. In this paper, we present a method that combines opinion mining b"
P13-3016,C10-2005,0,0.311897,"comment such as “Working for Company A is really awful” as an example. The complaint gives viewers a negative impression of Company A and can increase the number of people who think the company is bad. Some complaints are expressed by a specific 110 Proceedings of the ACL Student Research Workshop, pages 110–116, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics example. This comment is probably complaining about Company B but not Company A. In contrast, most of the previous work on sentiment analysis in social media does not consider these kinds of problems (Barbosa and Feng, 2010; Davidov et al., 2010; Speriosu et al., 2011). Switching to the behavior of each user, in social media we often see that users who have similar ideas will tend to cooperate with each other. In fact, previous work suggests that users who have the same opinions tend to create links to each other (Conover et al., 2011b; Yang et al., 2012). Because chronic critics share the purpose of attacking some target’s reputation, they may also decide to cooperate. For this reason, to detect chronic critics, we believe that information about the connections among users will be effective. In this paper, we p"
P13-3016,C10-2028,0,0.179006,"g for Company A is really awful” as an example. The complaint gives viewers a negative impression of Company A and can increase the number of people who think the company is bad. Some complaints are expressed by a specific 110 Proceedings of the ACL Student Research Workshop, pages 110–116, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics example. This comment is probably complaining about Company B but not Company A. In contrast, most of the previous work on sentiment analysis in social media does not consider these kinds of problems (Barbosa and Feng, 2010; Davidov et al., 2010; Speriosu et al., 2011). Switching to the behavior of each user, in social media we often see that users who have similar ideas will tend to cooperate with each other. In fact, previous work suggests that users who have the same opinions tend to create links to each other (Conover et al., 2011b; Yang et al., 2012). Because chronic critics share the purpose of attacking some target’s reputation, they may also decide to cooperate. For this reason, to detect chronic critics, we believe that information about the connections among users will be effective. In this paper, we present a method that c"
P13-3016,C10-2100,1,0.927109,"proposed method and discusses the experimental results. Section 5 concludes this paper. users. These studies did not identify the target of the polarized sentiment of each comment. Conover et al. (2011a) proposed a method that predicts the political polarity of a social media user based on the connections between users and tags. They demonstrated that label propagation on the graph representing the connections between users is effective. However, this method is not guaranteed to obtain the optimal solution. In contrast, our research uses graph analysis that converges on the optimal solution. Murakami and Raymond (2010) proposed a method that uses the connections between users to predict each user’s opinion, i.e., support or oppose a topic in online debates. They analyzed the content of the discussions to infer the connections. However, in social media, it is difficult to infer connections based on content because of such complexities as incomplete contexts. To address these problem, we analyzed the behavior of the users to predict the connections between users. Our task is similar to spammer detection (Wang, 2010; Yang et al., 2012). Wang (2010) proposed a method using a classifier to detect spammers. They"
P15-1160,D11-1145,1,0.808815,"suggest that our task is independent of the type of disease/symptom. 1 Introduction Social media services, including Twitter and Facebook, provide opportunities for individuals to share their experiences, thoughts, and opinions. The wide use of social media services has led to the emergence of new approaches for surveying the population and addressing social issues. One popular application of social media data is flu surveillance, i.e., predicting the outbreak of influenza epidemics by detecting mentions of flu infections on social media platforms (Culotta, 2010; Lampos and Cristianini, 2010; Aramaki et al., 2011; Paul and Dredze, 2011; Signorini et al., 2011; Collier, 2012; Dredze et al., 2013; Gesualdo et al., 2013; Stoové and Pedrana, 2014). Previous studies mainly relied on shallow textual clues in Twitter posts in order to predict the number of flu infections, e.g., the number of occurrences of specific keywords (such as “flu” or “influenza”) on Twitter. However, such a simple approach can lead to incorrect predictions. Broniatowski et al. (2013) argued that media attention increases chatter, i.e., the number of tweets that mention the flu without the poster being actually infected. Examples incl"
P15-1160,N13-1121,0,0.066141,"Missing"
P15-1160,J96-2004,0,0.0431357,"Missing"
P15-1160,P14-2111,0,0.0261289,"symptoms of another person in such cases. In other words, the episode classifier can predict a positive label for these symptoms without knowing the subjects of these symptoms. 4 Related Work 4.1 Twitter and NLP NLP researchers have addressed two major directions for Twitter: adapting existing NLP technologies to noisy texts and extracting useful knowledge from Twitter. The former includes improving the accuracy of part-of-speech tagging (Gimpel et al., 2011) and named entity recognition (Plank et al., 2014), as well as normalizing ill-formed words into canonical forms (Han and Baldwin, 2011; Chrupała, 2014). Even though we did not incor7 For the “predicted” setting, first, we predicted the subject labels in a similar manner to five-fold cross validation, and we used the predicted labels as features for the episode classifier. porate the findings of these studies, they could be beneficial to our work in the future. The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock market trends (Si et al., 2013), mining of public opinion (O’Connor et al., 2010), event extraction and"
P15-1160,P10-1160,0,0.0932612,"the task of identifying subjects is independent of the type of diseases/symptom. We verify the possibility of transferring supervision data to different targets of diseases and symptoms. In other words, we verify that it is possible to utilize the supervision data for a particular disease/symptom to improve the accuracy of predicting subjects of another disease/symptom. 1. Novel task setting. The task of identifying the subject of a disease/symptom is similar to predicate-argument structure (PAS) analysis for nominal predicates (Meyers et al., 2004; Sasano et al., 2004; Komachi et al., 2007; Gerber and Chai, 2010). However, these studies do not treat diseases (e.g., “influenza”) and symptoms (e.g., “headache”) as nominal predicates. To the best of our knowledge, this task has not been explored in natural language processing (NLP) thus far. 2. Identifying whether the subject has a disease/symptom. Besides the work on PAS analysis for nominal predicates, the most relevant work is PAS analysis for verb predicates. However, our task is not as simple as predicting the subject of the verb governing a disease/symptom-related noun. For example, the subject of the verb “beat” is the first person “I” in the sent"
P15-1160,P11-2008,0,0.131837,"Missing"
P15-1160,P11-1038,0,0.037841,"we seldom mention the symptoms of another person in such cases. In other words, the episode classifier can predict a positive label for these symptoms without knowing the subjects of these symptoms. 4 Related Work 4.1 Twitter and NLP NLP researchers have addressed two major directions for Twitter: adapting existing NLP technologies to noisy texts and extracting useful knowledge from Twitter. The former includes improving the accuracy of part-of-speech tagging (Gimpel et al., 2011) and named entity recognition (Plank et al., 2014), as well as normalizing ill-formed words into canonical forms (Han and Baldwin, 2011; Chrupała, 2014). Even though we did not incor7 For the “predicted” setting, first, we predicted the subject labels in a similar manner to five-fold cross validation, and we used the predicted labels as features for the episode classifier. porate the findings of these studies, they could be beneficial to our work in the future. The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock market trends (Si et al., 2013), mining of public opinion (O’Connor et al., 2010), even"
P15-1160,P13-4002,0,0.0262922,"cted labels as features for the episode classifier. porate the findings of these studies, they could be beneficial to our work in the future. The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock market trends (Si et al., 2013), mining of public opinion (O’Connor et al., 2010), event extraction and summarization (Sakaki et al., 2010; Thelwall et al., 2011; Marchetti-Bowick and Chambers, 2012; Shen et al., 2013; Li et al., 2014a), user profiling (Bergsma et al., 2013; Han et al., 2013; Li et al., 2014b; Zhou et al., 2014), disaster management (Varga et al., 2013), and extraction of common-sense knowledge (Williams and Katz, 2012). Our work can directly contribute to these applications, e.g., sentiment analysis, user profiling, event extraction, and disaster management. 4.2 Semantic analysis for nouns Our work can be considered as a semantic analysis that identifies an argument (subject) for a disease/symptom-related noun. NomBank (Meyers et al., 2004) provides annotations of noun arguments in a similar manner to PropBank (Palmer et al., 2005), which provides annotations of"
P15-1160,N13-1097,0,0.258011,"Missing"
P15-1160,D14-1214,0,0.0348142,"nner to five-fold cross validation, and we used the predicted labels as features for the episode classifier. porate the findings of these studies, they could be beneficial to our work in the future. The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock market trends (Si et al., 2013), mining of public opinion (O’Connor et al., 2010), event extraction and summarization (Sakaki et al., 2010; Thelwall et al., 2011; Marchetti-Bowick and Chambers, 2012; Shen et al., 2013; Li et al., 2014a), user profiling (Bergsma et al., 2013; Han et al., 2013; Li et al., 2014b; Zhou et al., 2014), disaster management (Varga et al., 2013), and extraction of common-sense knowledge (Williams and Katz, 2012). Our work can directly contribute to these applications, e.g., sentiment analysis, user profiling, event extraction, and disaster management. 4.2 Semantic analysis for nouns Our work can be considered as a semantic analysis that identifies an argument (subject) for a disease/symptom-related noun. NomBank (Meyers et al., 2004) provides annotations of noun arguments in a similar manner to Pro"
P15-1160,P14-1016,0,0.0322227,"nner to five-fold cross validation, and we used the predicted labels as features for the episode classifier. porate the findings of these studies, they could be beneficial to our work in the future. The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock market trends (Si et al., 2013), mining of public opinion (O’Connor et al., 2010), event extraction and summarization (Sakaki et al., 2010; Thelwall et al., 2011; Marchetti-Bowick and Chambers, 2012; Shen et al., 2013; Li et al., 2014a), user profiling (Bergsma et al., 2013; Han et al., 2013; Li et al., 2014b; Zhou et al., 2014), disaster management (Varga et al., 2013), and extraction of common-sense knowledge (Williams and Katz, 2012). Our work can directly contribute to these applications, e.g., sentiment analysis, user profiling, event extraction, and disaster management. 4.2 Semantic analysis for nouns Our work can be considered as a semantic analysis that identifies an argument (subject) for a disease/symptom-related noun. NomBank (Meyers et al., 2004) provides annotations of noun arguments in a similar manner to Pro"
P15-1160,E12-1062,0,0.0297451,", first, we predicted the subject labels in a similar manner to five-fold cross validation, and we used the predicted labels as features for the episode classifier. porate the findings of these studies, they could be beneficial to our work in the future. The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock market trends (Si et al., 2013), mining of public opinion (O’Connor et al., 2010), event extraction and summarization (Sakaki et al., 2010; Thelwall et al., 2011; Marchetti-Bowick and Chambers, 2012; Shen et al., 2013; Li et al., 2014a), user profiling (Bergsma et al., 2013; Han et al., 2013; Li et al., 2014b; Zhou et al., 2014), disaster management (Varga et al., 2013), and extraction of common-sense knowledge (Williams and Katz, 2012). Our work can directly contribute to these applications, e.g., sentiment analysis, user profiling, event extraction, and disaster management. 4.2 Semantic analysis for nouns Our work can be considered as a semantic analysis that identifies an argument (subject) for a disease/symptom-related noun. NomBank (Meyers et al., 2004) provides annotations of noun"
P15-1160,W04-2705,0,0.134012,"challenges in this study. 2. The experimental results show that the task of identifying subjects is independent of the type of diseases/symptom. We verify the possibility of transferring supervision data to different targets of diseases and symptoms. In other words, we verify that it is possible to utilize the supervision data for a particular disease/symptom to improve the accuracy of predicting subjects of another disease/symptom. 1. Novel task setting. The task of identifying the subject of a disease/symptom is similar to predicate-argument structure (PAS) analysis for nominal predicates (Meyers et al., 2004; Sasano et al., 2004; Komachi et al., 2007; Gerber and Chai, 2010). However, these studies do not treat diseases (e.g., “influenza”) and symptoms (e.g., “headache”) as nominal predicates. To the best of our knowledge, this task has not been explored in natural language processing (NLP) thus far. 2. Identifying whether the subject has a disease/symptom. Besides the work on PAS analysis for nominal predicates, the most relevant work is PAS analysis for verb predicates. However, our task is not as simple as predicting the subject of the verb governing a disease/symptom-related noun. For example,"
P15-1160,J05-1004,0,0.0123815,"r profiling (Bergsma et al., 2013; Han et al., 2013; Li et al., 2014b; Zhou et al., 2014), disaster management (Varga et al., 2013), and extraction of common-sense knowledge (Williams and Katz, 2012). Our work can directly contribute to these applications, e.g., sentiment analysis, user profiling, event extraction, and disaster management. 4.2 Semantic analysis for nouns Our work can be considered as a semantic analysis that identifies an argument (subject) for a disease/symptom-related noun. NomBank (Meyers et al., 2004) provides annotations of noun arguments in a similar manner to PropBank (Palmer et al., 2005), which provides annotations of verbs. In NomBank, nominal predicates and their arguments are identified: for example, ARG 0 (typically, subject or agent) is “customer” and ARG 1 (typically, objects, patients, themes) is “issue” for the nominal predicate “complaints” in the sentence “There have been no customer complaints about that issue.” Gerber and Chai (2010) improved the coverage of NomBank by handling implicit arguments. Some studies have addressed the task of identifying implicit and omitted arguments for nominal predicates in Japanese (Komachi et al., 2007; Sasano et al., 2008). Our wo"
P15-1160,C14-1168,0,0.0255805,"ably. This is because the subjects for these symptoms are mostly F IRST P ERSON, as we seldom mention the symptoms of another person in such cases. In other words, the episode classifier can predict a positive label for these symptoms without knowing the subjects of these symptoms. 4 Related Work 4.1 Twitter and NLP NLP researchers have addressed two major directions for Twitter: adapting existing NLP technologies to noisy texts and extracting useful knowledge from Twitter. The former includes improving the accuracy of part-of-speech tagging (Gimpel et al., 2011) and named entity recognition (Plank et al., 2014), as well as normalizing ill-formed words into canonical forms (Han and Baldwin, 2011; Chrupała, 2014). Even though we did not incor7 For the “predicted” setting, first, we predicted the subject labels in a similar manner to five-fold cross validation, and we used the predicted labels as features for the episode classifier. porate the findings of these studies, they could be beneficial to our work in the future. The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock ma"
P15-1160,C04-1174,0,0.0455393,"tudy. 2. The experimental results show that the task of identifying subjects is independent of the type of diseases/symptom. We verify the possibility of transferring supervision data to different targets of diseases and symptoms. In other words, we verify that it is possible to utilize the supervision data for a particular disease/symptom to improve the accuracy of predicting subjects of another disease/symptom. 1. Novel task setting. The task of identifying the subject of a disease/symptom is similar to predicate-argument structure (PAS) analysis for nominal predicates (Meyers et al., 2004; Sasano et al., 2004; Komachi et al., 2007; Gerber and Chai, 2010). However, these studies do not treat diseases (e.g., “influenza”) and symptoms (e.g., “headache”) as nominal predicates. To the best of our knowledge, this task has not been explored in natural language processing (NLP) thus far. 2. Identifying whether the subject has a disease/symptom. Besides the work on PAS analysis for nominal predicates, the most relevant work is PAS analysis for verb predicates. However, our task is not as simple as predicting the subject of the verb governing a disease/symptom-related noun. For example, the subject of the v"
P15-1160,C08-1097,0,0.0123551,"opBank (Palmer et al., 2005), which provides annotations of verbs. In NomBank, nominal predicates and their arguments are identified: for example, ARG 0 (typically, subject or agent) is “customer” and ARG 1 (typically, objects, patients, themes) is “issue” for the nominal predicate “complaints” in the sentence “There have been no customer complaints about that issue.” Gerber and Chai (2010) improved the coverage of NomBank by handling implicit arguments. Some studies have addressed the task of identifying implicit and omitted arguments for nominal predicates in Japanese (Komachi et al., 2007; Sasano et al., 2008). Our work shares a similar goal with the abovementioned studies, i.e., identifying an implicit ARG 0 for a disease and symptom. However, these studies do not regard a disease/symptom as a nominal predicate because they consider verb nominalizations as nominal predicates. In addition, 1667 they use a corpus that consists of newswire text, the writing style and word usage of which differ considerably from those of tweets. For these reasons, we proposed a novel task setting for identifying subjects of diseases and symptoms, and we built an annotated corpus for developing the subject classifier a"
P15-1160,N13-1135,0,0.0604966,"Missing"
P15-1160,P13-2005,0,0.0142294,"normalizing ill-formed words into canonical forms (Han and Baldwin, 2011; Chrupała, 2014). Even though we did not incor7 For the “predicted” setting, first, we predicted the subject labels in a similar manner to five-fold cross validation, and we used the predicted labels as features for the episode classifier. porate the findings of these studies, they could be beneficial to our work in the future. The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock market trends (Si et al., 2013), mining of public opinion (O’Connor et al., 2010), event extraction and summarization (Sakaki et al., 2010; Thelwall et al., 2011; Marchetti-Bowick and Chambers, 2012; Shen et al., 2013; Li et al., 2014a), user profiling (Bergsma et al., 2013; Han et al., 2013; Li et al., 2014b; Zhou et al., 2014), disaster management (Varga et al., 2013), and extraction of common-sense knowledge (Williams and Katz, 2012). Our work can directly contribute to these applications, e.g., sentiment analysis, user profiling, event extraction, and disaster management. 4.2 Semantic analysis for nouns Our work can be"
P15-1160,P12-2044,0,0.0303353,". The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock market trends (Si et al., 2013), mining of public opinion (O’Connor et al., 2010), event extraction and summarization (Sakaki et al., 2010; Thelwall et al., 2011; Marchetti-Bowick and Chambers, 2012; Shen et al., 2013; Li et al., 2014a), user profiling (Bergsma et al., 2013; Han et al., 2013; Li et al., 2014b; Zhou et al., 2014), disaster management (Varga et al., 2013), and extraction of common-sense knowledge (Williams and Katz, 2012). Our work can directly contribute to these applications, e.g., sentiment analysis, user profiling, event extraction, and disaster management. 4.2 Semantic analysis for nouns Our work can be considered as a semantic analysis that identifies an argument (subject) for a disease/symptom-related noun. NomBank (Meyers et al., 2004) provides annotations of noun arguments in a similar manner to PropBank (Palmer et al., 2005), which provides annotations of verbs. In NomBank, nominal predicates and their arguments are identified: for example, ARG 0 (typically, subject or agent) is “customer” and ARG 1"
P15-1160,P14-2114,0,0.0245499,"ode classifier. porate the findings of these studies, they could be beneficial to our work in the future. The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock market trends (Si et al., 2013), mining of public opinion (O’Connor et al., 2010), event extraction and summarization (Sakaki et al., 2010; Thelwall et al., 2011; Marchetti-Bowick and Chambers, 2012; Shen et al., 2013; Li et al., 2014a), user profiling (Bergsma et al., 2013; Han et al., 2013; Li et al., 2014b; Zhou et al., 2014), disaster management (Varga et al., 2013), and extraction of common-sense knowledge (Williams and Katz, 2012). Our work can directly contribute to these applications, e.g., sentiment analysis, user profiling, event extraction, and disaster management. 4.2 Semantic analysis for nouns Our work can be considered as a semantic analysis that identifies an argument (subject) for a disease/symptom-related noun. NomBank (Meyers et al., 2004) provides annotations of noun arguments in a similar manner to PropBank (Palmer et al., 2005), which provides annotations of verbs. In NomBank, nominal predicates"
P15-1160,P15-3005,1,\N,Missing
P15-1160,P13-1159,0,\N,Missing
P15-3005,D11-1145,1,0.801339,"nalysis: shallow modality analysis based on a surface string match and deep modality analysis based on predicate-argument structure analysis. The main contribution of this paper is two-fold: Consequently, this study proposes the use of modality features to improve disease event detection from Twitter messages, or “tweets”. Experimental results demonstrate that the combination of a modality dictionary and a modality analyzer improves the F1-score by 3.5 points. 1 • We annotate a new dataset extracted from Twitter for flu detection and prediction task, and extend the naïve bag-of-words model of Aramaki et al. (2011) and propose several Twitter-specific features for disease event detection tasks. Introduction The rapidly increasing popularity of Social Networking Services (SNSs) such as Twitter and Facebook has greatly eased the dissemination of information. Such data can serve as a valuable information resource for various applications. For instance, Huberman et al. (2009) investigated actual linked structures of human networks, Boyd et al. (2010) mapped out retweeting as a conversational practice, and Sakaki et al. (2010) detected earthquakes using SNSs. An important and widespread application of SNS mi"
P15-3005,P15-1160,1,0.513046,"Missing"
P15-3005,D14-1214,0,0.0613251,"a from Twitter texts, but the main challenge is to filter noise from this data. For example, Aramaki et al. (2011) reported that half of the tweets containing the word “cold (disease)” simply mention some information about a disease, but do not refer to the actual eventuality of having the disease. To address that problem, a classifier was produced to ascertain the factuality of the disease event. This paper follows that approach, using modality analysis, which provides a strong clue for factuality analysis (Saurí and Pustejovsky, 2012). Modality has been used and discussed in various places. Li et al. (2014) employ such modality features, although they do not describe the effect of using modality features in web application tasks. Furthermore, several workshops have been organized around the use of specific modalities, such as Negation and Speculation (e.g. NeSPNLP1 ). In this study, we use generic modality features to improve factuality analysis. 1 3.2 Shallow modality feature In Japanese, multiple words can serve as a function word as a whole (Matsuyoshi et al., 2007). We designate them as “functional expressions.” Even though functional expressions often carry modality information, previous wo"
P15-3005,matsuyoshi-etal-2010-annotating,0,0.0757664,"ies (Aramaki et al., 2011) 2 . If a tweet writer (or anybody near the writer) is infected with influenza, then the label is positive. Otherwise, the label is negative. Additionally, we save the time stamp when the tweet was posted online. Table 1 presents some examples. For this study, we use 10,443 Japanese tweet messages including the word “flu.” In this dataset, the number of positive examples is 1,319; the number of negative examples is 9,124. Because language heavily relies on modality to judge the factuality of sentences, modality analysis is a necessary process for factuality analysis (Matsuyoshi et al., 2010b). In line with this observation, we propose two ways to incorporate modality analysis for factuality analysis. Twitter is the SNS that is most frequently used for influenza detection (Achrekar et al., 2012; Aramaki et al., 2011; Ji et al., 2012; Sadilek et al., 2012; Lamb, 2013). Previous research on the subject has revealed a high correlation ratio between the number of influenza patients and actual tweets related to influenza. It is possible to obtain large amounts of data from Twitter texts, but the main challenge is to filter noise from this data. For example, Aramaki et al. (2011) repor"
P15-3005,J12-2002,0,0.0273849,"and actual tweets related to influenza. It is possible to obtain large amounts of data from Twitter texts, but the main challenge is to filter noise from this data. For example, Aramaki et al. (2011) reported that half of the tweets containing the word “cold (disease)” simply mention some information about a disease, but do not refer to the actual eventuality of having the disease. To address that problem, a classifier was produced to ascertain the factuality of the disease event. This paper follows that approach, using modality analysis, which provides a strong clue for factuality analysis (Saurí and Pustejovsky, 2012). Modality has been used and discussed in various places. Li et al. (2014) employ such modality features, although they do not describe the effect of using modality features in web application tasks. Furthermore, several workshops have been organized around the use of specific modalities, such as Negation and Speculation (e.g. NeSPNLP1 ). In this study, we use generic modality features to improve factuality analysis. 1 3.2 Shallow modality feature In Japanese, multiple words can serve as a function word as a whole (Matsuyoshi et al., 2007). We designate them as “functional expressions.” Even t"
P16-1121,D15-1198,0,0.0245378,"stributional information, which is necessary for calculating semantic similarity. Formal Semantics Our model implements a fragment of logic capable of semantic composition, largely due to the simple framework of Dependency-based Compositional Semantics (Liang et al., 2013). It fits in a long tradition of logic-based semantics (Montague, 1970; Dowty et al., 1981; Kamp and Reyle, 1993), with extensive studies on extracting semantics from syntactic representations such as HPSG (Copestake et al., 2001; Copestake et al., 2005) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015; Mineshima et al., 2015). Logic for Natural Language Inference The pursue of a logic more suitable for natural language inference is also not new. For example, MacCartney and Manning (2008) has implemented a model of natural logic (Lakoff, 1970). We would not reach the current formalization of logic of DCS without reading the work by Calvanese et al. (1998), which is an elegant formalization of database semantics in description logic. Semantic Parsing DCS-related representations have been actively used in semantic parsing and we see potential in applying our model. For example, Berant and Lia"
P16-1121,P02-1041,0,0.0651133,"e previous works did not specify how to integrate contextual distributional information, which is necessary for calculating semantic similarity. Formal Semantics Our model implements a fragment of logic capable of semantic composition, largely due to the simple framework of Dependency-based Compositional Semantics (Liang et al., 2013). It fits in a long tradition of logic-based semantics (Montague, 1970; Dowty et al., 1981; Kamp and Reyle, 1993), with extensive studies on extracting semantics from syntactic representations such as HPSG (Copestake et al., 2001; Copestake et al., 2005) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015; Mineshima et al., 2015). Logic for Natural Language Inference The pursue of a logic more suitable for natural language inference is also not new. For example, MacCartney and Manning (2008) has implemented a model of natural logic (Lakoff, 1970). We would not reach the current formalization of logic of DCS without reading the work by Calvanese et al. (1998), which is an elegant formalization of database semantics in description logic. Semantic Parsing DCS-related representations have been actively used in semantic parsing and we see potent"
P16-1121,P15-1061,0,0.0147595,"baselines on relation classification (Table 5). It makes 16 errors in misclassifying the direction of a relation, as compared to 144 such errors made by “no matrix”, 23 by “no inverse”, 30 by vecUD, and 161 by GloVe. This suggests that models with syntactic-semantic transformations (i.e. vecDCS, “no inverse”, and vecUD) are indeed good at distinguishing the different roles played by e1 and e2 . VecDCS scores moderately lower than the state-of-the-art (Xu et al., 2015), however we note that these results are achieved by adding additional features and training task-specific neural networks (dos Santos et al., 2015; Xu et al., 2015). Our method only uses features constructed from unlabeled corpora. From this point of view, it is comparable to the MV-RNN model (without features) in Socher et al. (2012), and vecDCS actually does better. Table 4 shows an example of clustered training instances as assessed by cosine similarities between their features. It suggests that the features used in our method can actually cluster similar relations. 5.4 Sentence Completion If vecDCS can compose query vectors of DCS trees, one should be able to “execute” the vectors to get a set of answers, as the original DCS trees c"
P16-1121,J10-4006,0,0.181301,"Missing"
P16-1121,D10-1115,0,0.0239659,"can be concisely illustrated by the DCS tree of “banned drugs” (Figure 1a), which is similar to a dependency tree but possesses precise procedural and logical meaning (Section 2). DCS has been shown useful in question answering (Liang et al., 2013) and textual entailment recognition (Tian et al., 2014). Orthogonal to the formal semantics of DCS, distributional vector representations are useful in capturing lexical semantics of words (Turney and Pantel, 2010; Levy et al., 2015), and progress is made in combining the word vectors to form meanings of phrases/sentences (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012; Paperno et al., 2014; Hashimoto et al., 2014). However, less effort is devoted to finding a link between vector-based compositions and the composition operations in any formal semantics. We believe that if a link can be found, then symbolic formulas in the formal semantics will be realized by vectors composed from word embeddings, such that similar things are realized by similar vectors; meanwhile, vectors will acquire formal meanings that can directly be used in execution or inference process. Still, to find a link is challenging becaus"
P16-1121,D11-1129,0,0.296304,"by the DCS tree of “banned drugs” (Figure 1a), which is similar to a dependency tree but possesses precise procedural and logical meaning (Section 2). DCS has been shown useful in question answering (Liang et al., 2013) and textual entailment recognition (Tian et al., 2014). Orthogonal to the formal semantics of DCS, distributional vector representations are useful in capturing lexical semantics of words (Turney and Pantel, 2010; Levy et al., 2015), and progress is made in combining the word vectors to form meanings of phrases/sentences (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012; Paperno et al., 2014; Hashimoto et al., 2014). However, less effort is devoted to finding a link between vector-based compositions and the composition operations in any formal semantics. We believe that if a link can be found, then symbolic formulas in the formal semantics will be realized by vectors composed from word embeddings, such that similar things are realized by similar vectors; meanwhile, vectors will acquire formal meanings that can directly be used in execution or inference process. Still, to find a link is challenging because any vector compositions that rea"
P16-1121,2014.lilt-9.5,0,0.0775601,"ntics to build up structured queries. In this section, we discuss several lines of previous research related to this work. 10 http://research.microsoft.com/en-us/ projects/scc/ 1284 Logic and Distributional Semantics Logic is necessary for implementing the functional aspects of meaning and organizing knowledge in a structured and unambiguous way. In contrast, distributional semantics provides an elegant methodology for assessing semantic similarity and is well suited for learning from data. There have been repeated calls for combining the strength of these two approaches (Coecke et al., 2010; Baroni et al., 2014; Liang and Potts, 2015), and several systems (Lewis and Steedman, 2013; Beltagy et al., 2014; Tian et al., 2014) have contributed to this direction. In the remarkable work by Beltagy et al. (to appear), word and phrase similarities are explicitly transformed to weighted logical rules that are used in a probabilistic inference framework. However, this approach requires considerable amount of engineering, including the generation of rule candidates (e.g. by aligning sentence fragments), converting distributional similarities to weights, and efficiently handling the rules and inference. What if"
P16-1121,P14-1114,0,0.0159302,"research related to this work. 10 http://research.microsoft.com/en-us/ projects/scc/ 1284 Logic and Distributional Semantics Logic is necessary for implementing the functional aspects of meaning and organizing knowledge in a structured and unambiguous way. In contrast, distributional semantics provides an elegant methodology for assessing semantic similarity and is well suited for learning from data. There have been repeated calls for combining the strength of these two approaches (Coecke et al., 2010; Baroni et al., 2014; Liang and Potts, 2015), and several systems (Lewis and Steedman, 2013; Beltagy et al., 2014; Tian et al., 2014) have contributed to this direction. In the remarkable work by Beltagy et al. (to appear), word and phrase similarities are explicitly transformed to weighted logical rules that are used in a probabilistic inference framework. However, this approach requires considerable amount of engineering, including the generation of rule candidates (e.g. by aligning sentence fragments), converting distributional similarities to weights, and efficiently handling the rules and inference. What if the distributional representations are equipped with a logical interface, such that the infer"
P16-1121,P14-1133,0,0.0323368,"i et al., 2015; Mineshima et al., 2015). Logic for Natural Language Inference The pursue of a logic more suitable for natural language inference is also not new. For example, MacCartney and Manning (2008) has implemented a model of natural logic (Lakoff, 1970). We would not reach the current formalization of logic of DCS without reading the work by Calvanese et al. (1998), which is an elegant formalization of database semantics in description logic. Semantic Parsing DCS-related representations have been actively used in semantic parsing and we see potential in applying our model. For example, Berant and Liang (2014) convert λ-DCS queries to canonical utterances and assess paraphrases at the surface level; an alternative could be using vector-based DCS to bring distributional similarity directly into calculation of denotations. We also borrow ideas from previous work, for example our training scheme is similar to Guu et al. (2015) in using paths and composition of matrices, and our method is similar to Poon and Domingos (2009) in building structured knowledge from clustering syntactic parse of unlabeled data. Further Applications Regarding the usability of distributional representations learned by our mod"
P16-1121,D12-1050,0,0.230928,"d to identity (“no matrix”), in order to investigate the effects of meaning changes caused by syntactic-semantic roles and prepositions; (ii) the regularizer enforcing MN−1 to be actually the inverse matrix of MN is set to γ = 0 (“no inverse”), in order to investigate the effects of a semantically motivated constraint; and (iii) applying the same training scheme to UD trees directly, by modeling UD relations as matrices (“vecUD”). In this case, one edge is assigned one UD relation rel, so we implement the transfor1281 vecDCS -no matrix -no inverse vecUD GloVe Grefenstette and Sadrzadeh (2011) Blacoe and Lapata (2012):RAE Grefenstette (2013a) Paperno et al. (2014) Hashimoto et al. (2014):Waddnl Kartsaklis and Sadrzadeh (2014) AN 0.51 0.52 0.47 0.44 0.41 0.31 0.48 - NN 0.49 0.46 0.43 0.46 0.47 0.30 0.40 - VO 0.41 0.42 0.38 0.41 0.41 0.28 0.39 - SVO 0.62 0.62 0.58 0.58 0.60 0.43 GS11 0.29 0.29 0.28 0.25 0.23 0.21 0.34 0.41 GS12 0.33 0.33 0.33 0.25 0.17 0.27 0.36 - Table 3: Spearman’s ρ on phrase similarity mation from child to parent by Mrel , and from par−1 ent to child by Mrel . The same hyper-parameters are used to train vecUD. By comparing vecDCS with vecUD we investigate if applying the semantics framew"
P16-1121,C04-1180,0,0.0490413,"ify how to integrate contextual distributional information, which is necessary for calculating semantic similarity. Formal Semantics Our model implements a fragment of logic capable of semantic composition, largely due to the simple framework of Dependency-based Compositional Semantics (Liang et al., 2013). It fits in a long tradition of logic-based semantics (Montague, 1970; Dowty et al., 1981; Kamp and Reyle, 1993), with extensive studies on extracting semantics from syntactic representations such as HPSG (Copestake et al., 2001; Copestake et al., 2005) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015; Mineshima et al., 2015). Logic for Natural Language Inference The pursue of a logic more suitable for natural language inference is also not new. For example, MacCartney and Manning (2008) has implemented a model of natural logic (Lakoff, 1970). We would not reach the current formalization of logic of DCS without reading the work by Calvanese et al. (1998), which is an elegant formalization of database semantics in description logic. Semantic Parsing DCS-related representations have been actively used in semantic parsing and we see potential in applying ou"
P16-1121,S13-1001,0,0.491291,"in order to investigate the effects of meaning changes caused by syntactic-semantic roles and prepositions; (ii) the regularizer enforcing MN−1 to be actually the inverse matrix of MN is set to γ = 0 (“no inverse”), in order to investigate the effects of a semantically motivated constraint; and (iii) applying the same training scheme to UD trees directly, by modeling UD relations as matrices (“vecUD”). In this case, one edge is assigned one UD relation rel, so we implement the transfor1281 vecDCS -no matrix -no inverse vecUD GloVe Grefenstette and Sadrzadeh (2011) Blacoe and Lapata (2012):RAE Grefenstette (2013a) Paperno et al. (2014) Hashimoto et al. (2014):Waddnl Kartsaklis and Sadrzadeh (2014) AN 0.51 0.52 0.47 0.44 0.41 0.31 0.48 - NN 0.49 0.46 0.43 0.46 0.47 0.30 0.40 - VO 0.41 0.42 0.38 0.41 0.41 0.28 0.39 - SVO 0.62 0.62 0.58 0.58 0.60 0.43 GS11 0.29 0.29 0.28 0.25 0.23 0.21 0.34 0.41 GS12 0.33 0.33 0.33 0.25 0.17 0.27 0.36 - Table 3: Spearman’s ρ on phrase similarity mation from child to parent by Mrel , and from par−1 ent to child by Mrel . The same hyper-parameters are used to train vecUD. By comparing vecDCS with vecUD we investigate if applying the semantics framework of DCS makes any di"
P16-1121,D13-1143,0,0.0620971,"Missing"
P16-1121,D15-1038,0,0.0335383,"hout reading the work by Calvanese et al. (1998), which is an elegant formalization of database semantics in description logic. Semantic Parsing DCS-related representations have been actively used in semantic parsing and we see potential in applying our model. For example, Berant and Liang (2014) convert λ-DCS queries to canonical utterances and assess paraphrases at the surface level; an alternative could be using vector-based DCS to bring distributional similarity directly into calculation of denotations. We also borrow ideas from previous work, for example our training scheme is similar to Guu et al. (2015) in using paths and composition of matrices, and our method is similar to Poon and Domingos (2009) in building structured knowledge from clustering syntactic parse of unlabeled data. Further Applications Regarding the usability of distributional representations learned by our model, a strong point is that the representation takes into account syntactic/structural information of context. Unlike several previous models (Pad´o and Lapata, 2007; Levy and Goldberg, 2014; Pham et al., 2015), our approach learns matrices at the same time that can extract the information according to different syntact"
P16-1121,D14-1163,0,0.600236,"cy tree but possesses precise procedural and logical meaning (Section 2). DCS has been shown useful in question answering (Liang et al., 2013) and textual entailment recognition (Tian et al., 2014). Orthogonal to the formal semantics of DCS, distributional vector representations are useful in capturing lexical semantics of words (Turney and Pantel, 2010; Levy et al., 2015), and progress is made in combining the word vectors to form meanings of phrases/sentences (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012; Paperno et al., 2014; Hashimoto et al., 2014). However, less effort is devoted to finding a link between vector-based compositions and the composition operations in any formal semantics. We believe that if a link can be found, then symbolic formulas in the formal semantics will be realized by vectors composed from word embeddings, such that similar things are realized by similar vectors; meanwhile, vectors will acquire formal meanings that can directly be used in execution or inference process. Still, to find a link is challenging because any vector compositions that realize such a link must conform to the logic of the formal semantics."
P16-1121,W09-2415,0,0.0986545,"Missing"
P16-1121,P01-1019,0,0.117114,"r calculations that realize logic operations. However, the previous works did not specify how to integrate contextual distributional information, which is necessary for calculating semantic similarity. Formal Semantics Our model implements a fragment of logic capable of semantic composition, largely due to the simple framework of Dependency-based Compositional Semantics (Liang et al., 2013). It fits in a long tradition of logic-based semantics (Montague, 1970; Dowty et al., 1981; Kamp and Reyle, 1993), with extensive studies on extracting semantics from syntactic representations such as HPSG (Copestake et al., 2001; Copestake et al., 2005) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015; Mineshima et al., 2015). Logic for Natural Language Inference The pursue of a logic more suitable for natural language inference is also not new. For example, MacCartney and Manning (2008) has implemented a model of natural logic (Lakoff, 1970). We would not reach the current formalization of logic of DCS without reading the work by Calvanese et al. (1998), which is an elegant formalization of database semantics in description logic. Semantic Parsing DCS-related representations"
P16-1121,W11-0607,0,0.0601555,"Missing"
P16-1121,P14-2050,0,0.107063,"Missing"
P16-1121,Q15-1016,0,0.0178691,"the results with drug (intersection “∩”). This procedure defined how words can be combined to form a meaning. Better yet, the procedure can be concisely illustrated by the DCS tree of “banned drugs” (Figure 1a), which is similar to a dependency tree but possesses precise procedural and logical meaning (Section 2). DCS has been shown useful in question answering (Liang et al., 2013) and textual entailment recognition (Tian et al., 2014). Orthogonal to the formal semantics of DCS, distributional vector representations are useful in capturing lexical semantics of words (Turney and Pantel, 2010; Levy et al., 2015), and progress is made in combining the word vectors to form meanings of phrases/sentences (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012; Paperno et al., 2014; Hashimoto et al., 2014). However, less effort is devoted to finding a link between vector-based compositions and the composition operations in any formal semantics. We believe that if a link can be found, then symbolic formulas in the formal semantics will be realized by vectors composed from word embeddings, such that similar things are realized by similar vectors; meanwh"
P16-1121,Q13-1015,0,0.0260398,"several lines of previous research related to this work. 10 http://research.microsoft.com/en-us/ projects/scc/ 1284 Logic and Distributional Semantics Logic is necessary for implementing the functional aspects of meaning and organizing knowledge in a structured and unambiguous way. In contrast, distributional semantics provides an elegant methodology for assessing semantic similarity and is well suited for learning from data. There have been repeated calls for combining the strength of these two approaches (Coecke et al., 2010; Baroni et al., 2014; Liang and Potts, 2015), and several systems (Lewis and Steedman, 2013; Beltagy et al., 2014; Tian et al., 2014) have contributed to this direction. In the remarkable work by Beltagy et al. (to appear), word and phrase similarities are explicitly transformed to weighted logical rules that are used in a probabilistic inference framework. However, this approach requires considerable amount of engineering, including the generation of rule candidates (e.g. by aligning sentence fragments), converting distributional similarities to weights, and efficiently handling the rules and inference. What if the distributional representations are equipped with a logical interfac"
P16-1121,J13-2005,0,0.169374,"le, DCS can guide building vectors for structured queries that can be directly executed. We evaluate this utility on sentence completion task and report a new state-of-the-art. 1 Introduction A major goal of semantic processing is to map natural language utterances to representations that facilitate calculation of meanings, execution of commands, and/or inference of knowledge. Formal semantics supports such representations by defining words as some functional units and combining them via a specific logic. A simple and illustrative example is the Dependency-based Compositional Semantics (DCS) (Liang et al., 2013). DCS composes meanings from denotations of words (i.e. sets of things to which the words apply); say, the denotations of the concept drug and the event ban is shown in Figure 1b, where drug is a list of drug names and ban is a list of the subjectcomplement pairs in any ban event; then, a list of banned drugs can be constructed by first taking the COMP column of all records in ban (projection “πCOMP ”), and then intersecting the results with drug (intersection “∩”). This procedure defined how words can be combined to form a meaning. Better yet, the procedure can be concisely illustrated by the"
P16-1121,C08-1066,0,0.0252211,"y due to the simple framework of Dependency-based Compositional Semantics (Liang et al., 2013). It fits in a long tradition of logic-based semantics (Montague, 1970; Dowty et al., 1981; Kamp and Reyle, 1993), with extensive studies on extracting semantics from syntactic representations such as HPSG (Copestake et al., 2001; Copestake et al., 2005) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015; Mineshima et al., 2015). Logic for Natural Language Inference The pursue of a logic more suitable for natural language inference is also not new. For example, MacCartney and Manning (2008) has implemented a model of natural logic (Lakoff, 1970). We would not reach the current formalization of logic of DCS without reading the work by Calvanese et al. (1998), which is an elegant formalization of database semantics in description logic. Semantic Parsing DCS-related representations have been actively used in semantic parsing and we see potential in applying our model. For example, Berant and Liang (2014) convert λ-DCS queries to canonical utterances and assess paraphrases at the surface level; an alternative could be using vector-based DCS to bring distributional similarity directl"
P16-1121,D15-1244,0,0.155487,"Missing"
P16-1121,J07-2002,0,0.177913,"Missing"
P16-1121,D14-1162,0,0.100047,"o words have overlapping context (Tian et al., 2015); therefore, it is suitable to implement an “and” or intersection operation (Section 3). We design our model such that the resulted distributional representations are expected to have additive compositionality. Second, when intersection is realized as addition, it is natural to implement projection as linear mapping, as suggested by the logical interactions between the two operations (Section 3). Experimentally, we show that vectors and matrices learned by our model exhibit favorable characteristics as compared with vectors trained by GloVe (Pennington et al., 2014) or those learned from syntactic dependencies (Section 5.1). Finally, additive composition brings our model a strong ability to calculate similar vectors for similar phrases, whereas syntactic-semantic roles (e.g. SUBJ, COMP) can be distinguished by different projection matrices (e.g. MSUBJ , MCOMP ). We achieve near state-of-the-art performance on a wide range of phrase similarity tasks (Section 5.2) and relation classification (Section 5.3). Furthermore, we show that a vector as constructed above for “banned drugs” can be used as a query vector to retrieve a coarse-grained candidate list of"
P16-1121,P15-1094,0,0.0757346,"Missing"
P16-1121,D09-1001,0,0.0353216,"base semantics in description logic. Semantic Parsing DCS-related representations have been actively used in semantic parsing and we see potential in applying our model. For example, Berant and Liang (2014) convert λ-DCS queries to canonical utterances and assess paraphrases at the surface level; an alternative could be using vector-based DCS to bring distributional similarity directly into calculation of denotations. We also borrow ideas from previous work, for example our training scheme is similar to Guu et al. (2015) in using paths and composition of matrices, and our method is similar to Poon and Domingos (2009) in building structured knowledge from clustering syntactic parse of unlabeled data. Further Applications Regarding the usability of distributional representations learned by our model, a strong point is that the representation takes into account syntactic/structural information of context. Unlike several previous models (Pad´o and Lapata, 2007; Levy and Goldberg, 2014; Pham et al., 2015), our approach learns matrices at the same time that can extract the information according to different syntactic-semantic roles. A related application is selectional preference (Baroni and Lenci, 2010; Lenci,"
P16-1121,W14-2409,0,0.0172808,"butional Models There has been active exploration on how to combine word vectors such that adequate phrase/sentence similarities can be assessed (Mitchell and Lapata, 2010, inter alia), and there is nothing new in using matrices to model changes of meanings. However, previous model designs mostly rely on linguistic intuitions (Paperno et al., 2014, inter alia), whereas our model has an exact logic interpretation. Furthermore, by using additive composition we enjoy a learning guarantee (Tian et al., 2015). Vector-based Logic Models This work also shares the spirit with Grefenstette (2013b) and Rocktaeschel et al. (2014), in exploring vector calculations that realize logic operations. However, the previous works did not specify how to integrate contextual distributional information, which is necessary for calculating semantic similarity. Formal Semantics Our model implements a fragment of logic capable of semantic composition, largely due to the simple framework of Dependency-based Compositional Semantics (Liang et al., 2013). It fits in a long tradition of logic-based semantics (Montague, 1970; Dowty et al., 1981; Kamp and Reyle, 1993), with extensive studies on extracting semantics from syntactic representa"
P16-1121,D12-1110,0,0.744461,"(Figure 1a), which is similar to a dependency tree but possesses precise procedural and logical meaning (Section 2). DCS has been shown useful in question answering (Liang et al., 2013) and textual entailment recognition (Tian et al., 2014). Orthogonal to the formal semantics of DCS, distributional vector representations are useful in capturing lexical semantics of words (Turney and Pantel, 2010; Levy et al., 2015), and progress is made in combining the word vectors to form meanings of phrases/sentences (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012; Paperno et al., 2014; Hashimoto et al., 2014). However, less effort is devoted to finding a link between vector-based compositions and the composition operations in any formal semantics. We believe that if a link can be found, then symbolic formulas in the formal semantics will be realized by vectors composed from word embeddings, such that similar things are realized by similar vectors; meanwhile, vectors will acquire formal meanings that can directly be used in execution or inference process. Still, to find a link is challenging because any vector compositions that realize such a link must"
P16-1121,P14-1008,1,0.948018,"plement pairs in any ban event; then, a list of banned drugs can be constructed by first taking the COMP column of all records in ban (projection “πCOMP ”), and then intersecting the results with drug (intersection “∩”). This procedure defined how words can be combined to form a meaning. Better yet, the procedure can be concisely illustrated by the DCS tree of “banned drugs” (Figure 1a), which is similar to a dependency tree but possesses precise procedural and logical meaning (Section 2). DCS has been shown useful in question answering (Liang et al., 2013) and textual entailment recognition (Tian et al., 2014). Orthogonal to the formal semantics of DCS, distributional vector representations are useful in capturing lexical semantics of words (Turney and Pantel, 2010; Levy et al., 2015), and progress is made in combining the word vectors to form meanings of phrases/sentences (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012; Paperno et al., 2014; Hashimoto et al., 2014). However, less effort is devoted to finding a link between vector-based compositions and the composition operations in any formal semantics. We believe that if a link can be"
P16-1121,D14-1004,0,0.0416244,"Missing"
P16-1121,D15-1062,0,0.0240654,"lassifier is SVM9 with RBF kernel, C = 2 and Γ = 0.25. The hyper-parameters are selected by 5-fold cross validation. Results VecDCS outperforms baselines on relation classification (Table 5). It makes 16 errors in misclassifying the direction of a relation, as compared to 144 such errors made by “no matrix”, 23 by “no inverse”, 30 by vecUD, and 161 by GloVe. This suggests that models with syntactic-semantic transformations (i.e. vecDCS, “no inverse”, and vecUD) are indeed good at distinguishing the different roles played by e1 and e2 . VecDCS scores moderately lower than the state-of-the-art (Xu et al., 2015), however we note that these results are achieved by adding additional features and training task-specific neural networks (dos Santos et al., 2015; Xu et al., 2015). Our method only uses features constructed from unlabeled corpora. From this point of view, it is comparable to the MV-RNN model (without features) in Socher et al. (2012), and vecDCS actually does better. Table 4 shows an example of clustered training instances as assessed by cosine similarities between their features. It suggests that the features used in our method can actually cluster similar relations. 5.4 Sentence Completion"
P16-1121,P12-1063,0,0.0741783,"w that a vector as constructed above for “banned drugs” can be used as a query vector to retrieve a coarse-grained candidate list of banned drugs, by sorting its dot products with answer vectors that are also learned by our model (Figure 1d). This is due to the ability of our approach to provide a language model that can find likely words to fill in the blanks such as “ is a banned drug” or “the drug is banned by . . . ”. A highlight is the calculation being done as if a query is “executed” by the DCS tree of “banned drugs”. We quantitatively evaluate this utility on sentence completion task (Zweig et al., 2012) and report a new state-of-the-art (Section 5.4). 2 DCS Trees DCS composes meanings from denotations, or sets of things to which words apply. A “thing” (i.e. element of a denotation) is represented by a tuple of features of the form Field=Value, with a fixed inventory of fields. For example, a denotation ban might be a set of tuples ban = {(SUBJ=Canada, COMP=Thalidomide), . . .}, in which each tuple records participants of a banning event (e.g. Canada banning Thalidomide). Operations are applied to sets of things to generate new denotations, for modeling semantic composition. An example is the"
P16-1121,P11-1060,0,\N,Missing
P16-1121,P14-1009,0,\N,Missing
P16-1215,S12-1051,0,0.0389772,"ord phrase pairs with semantic similarity judged by human annotators. Korkontzelos et al. (2013) provided a semantic similarity dataset with pairs of two words and a single word. Wieting et al. (2015) annotated a part of PPDB (Ganitkevitch et al., 2013) to evaluate semantic modeling of paraphrases. Although the target unit of semantic modeling is different from that for these previous studies, we follow the annotation guideline and instruction of Mitchell and Lapata (2010) to build the new dataset. The task addressed in this paper is also related to the Semantic Textual Similarity (STS) task (Agirre et al., 2012). STS is the task to measure the degree of semantic similarity between two sentences. Even though a relational pattern appears as a part of a sentence, it may be difficult to transfer findings from one to another: for example, the encoders of RNN and its variants explored 14 In fact, we made substantial efforts to introduce the negative sampling technique. However, Xu et al. (2015) omits the detail of the technique probably because of the severe page limit of short papers. For this reason, we could not reproduce their method in this study. in this study may exhibit different characteristics, i"
P16-1215,D15-1141,0,0.0260687,"trices (parameters), g(.) is the elementwise activation function (tanh). We set h0 = 0 at t = 1. In essence, RNN computes the hidden state ht based on the one at the previous position (ht−1 ) and the word embedding xt . Applying Equation 1 from t = 1 to T , we use hT as the distributed representation of the relational pattern. 3.3 RNN variants We also employ Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Cho et al., 2014) as an encoder for relational patterns. LSTM has been applied successfully to various NLP tasks including word segmentation (Chen et al., 2015), dependency parsing (Dyer et al., 2015), machine translation (Sutskever et al., 2014), and sentiment analysis (Tai et al., 2015). GRU is also successful in machine translation (Cho et al., 2014) and various 5 We do not use a bias term in this study. We set the number of dimensions of hidden states identical to that of word embeddings (d) so that we can adapt the objective function of the Skip-gram model for training (Section 3.5). 2278 Parameter update by Skip-gram model Gated Additive Composition (GAC) hs ~x s-2 ~x s-1 (context vectors) is fs is+1 (hidden vectors) is+2 h s+L-1 is+3 (3) xs x"
P16-1215,P15-1061,0,0.228046,"ickx et al., 2010). In other words, we explore whether high-quality distributed representations of relational patterns are effective to identify a relation type of an entity pair. The dataset consists of 10, 717 relation instances (8, 000 training and 2, 717 test instances) with their relation types annotated. The dataset 2281 Method SVM SVM + NoComp SVM + LSTM SVM + Add SVM + GRU SVM + RNN SVM + GAC Ranking loss + GAC w/ fine-tuning SVM (Rink and Harabagiu, 2010) MV-RNN (Socher et al., 2012) FCM (Gormley et al., 2015) w/o fine-tuning w/ fine-tuning RelEmb (Hashimoto et al., 2015) CR-CNN (dos Santos et al., 2015) w/ Other w/o Other depLCNN (Xu et al., 2015) depLCNN + NS Feature set BoW, POS embeddings, BoW, POS embeddings, BoW, POS embeddings, BoW, POS embeddings, BoW, POS embeddings, BoW, POS embeddings, BoW, POS + dependency, WordNet, NE embeddings, BoW, POS + dependency, WordNet, NE BoW, POS, dependency, Google n-gram, etc. embeddings, parse trees + WordNet, POS, NE embeddings, dependency + WordNet embeddings, dependency + NE embeddings + dependency, WordNet, NE embeddings, word position embeddings embeddings, word position embeddings embeddings, dependency + WordNet embeddings, dependency + WordNe"
P16-1215,P15-1033,0,0.0255448,"wise activation function (tanh). We set h0 = 0 at t = 1. In essence, RNN computes the hidden state ht based on the one at the previous position (ht−1 ) and the word embedding xt . Applying Equation 1 from t = 1 to T , we use hT as the distributed representation of the relational pattern. 3.3 RNN variants We also employ Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Cho et al., 2014) as an encoder for relational patterns. LSTM has been applied successfully to various NLP tasks including word segmentation (Chen et al., 2015), dependency parsing (Dyer et al., 2015), machine translation (Sutskever et al., 2014), and sentiment analysis (Tai et al., 2015). GRU is also successful in machine translation (Cho et al., 2014) and various 5 We do not use a bias term in this study. We set the number of dimensions of hidden states identical to that of word embeddings (d) so that we can adapt the objective function of the Skip-gram model for training (Section 3.5). 2278 Parameter update by Skip-gram model Gated Additive Composition (GAC) hs ~x s-2 ~x s-1 (context vectors) is fs is+1 (hidden vectors) is+2 h s+L-1 is+3 (3) xs x s+1 x s+2 x s+L-1 (word vectors) (3) ~x"
P16-1215,D11-1142,0,0.354603,"rameters of the encoders on a large unlabeled corpus. Experiments show that the new dataset does not only enable detailed analyses of the different encoders, but also provides a gauge to predict successes of distributed representations of relational patterns in another task (relation classification). Figure 1 illustrates the overview of this study. 2 Data Construction 2.1 Target relation instances We build a new dataset upon the work of Zeichner et al. (2012), which consists of relational patterns with semantic inference labels annotated. The dataset includes 5,555 pairs2 extracted by Reverb (Fader et al., 2011), 2,447 pairs with inference relation and 3,108 pairs (the rest) without one. Initially, we considered using this high-quality dataset as it is for semantic modeling of relational patterns. However, we found that inference relations exhibit quite different properties from those of semantic similarity. Take a relational pattern pair “X be the part of Y” and “X be an essential part of Y” filled with “X = the small intestine, Y = the digestive system” as an instance. The pattern “X be the part of Y” does not entail “X be an essential part of Y” because the meaning of the former does not include ‘"
P16-1215,N13-1092,0,0.088,"Missing"
P16-1215,D15-1205,0,0.0317607,"Missing"
P16-1215,D15-1113,0,0.0186742,"detail of the technique probably because of the severe page limit of short papers. For this reason, we could not reproduce their method in this study. in this study may exhibit different characteristics, influenced by the length and complexity of input text expressions. In addition to data construction, this paper addresses semantic modeling of relational patterns. Nakashole et al. (2012) approached the similar task by constructing a taxonomy of relational patterns. They represented a vector of a relational pattern as the distribution of entity pairs co-occurring with the relational pattern. Grycner et al. (2015) extended Nakashole et al. (2012) to generalize dimensions of the vector space (entity pairs) by incorporating hyponymy relation between entities. They also used external resources to recognize the transitivity of pattern pairs and applied transitivities to find patterns in entailment relation. These studies did not consider semantic composition of relational patterns. Thus, they might suffer from the data sparseness problem, as shown by NoComp in Figure 4. Numerous studies have been aimed at encoding distributed representations of phrases and sentences from word embeddings by using: Recursive"
P16-1215,D14-1163,0,0.0243136,"relational patterns. Thus, they might suffer from the data sparseness problem, as shown by NoComp in Figure 4. Numerous studies have been aimed at encoding distributed representations of phrases and sentences from word embeddings by using: Recursive Neural Network (Socher et al., 2011), Matrix Vector Recursive Neural Network (Socher et al., 2012), Recursive Neural Network with different weight matrices corresponding to syntactic categories (Socher et al., 2013) or word types (Takase et al., 2016), RNN (Sutskever et al., 2011), LSTM (Sutskever et al., 2014), GRU (Cho et al., 2014), PAS-CLBLM (Hashimoto et al., 2014), etc. As described in Section 3, we applied RNN, GRU, and LSTM to compute distributed representations of relational patterns because recent papers have demonstrated their superiority in semantic composition (Sutskever et al., 2014; Tang et al., 2015). In this paper, we presented a comparative study of different encoders for semantic modeling of relational patterns. To investigate usefulness of the distributed representations and the new dataset, we adopted the relation classification task (SemEval 2010 Task 8) as a real application. On the SemEval 2010 Task 8, several studies considered seman"
P16-1215,K15-1027,0,0.0185818,"ilarity task. This fact implies two potential impacts. First, the distributed representations of relational patterns are useful and easily transferable to other tasks such as knowledge base population. Second, the pattern similarity dataset provides a gauge to predict successes of distributed representations in another task. We could further improve the performance of SVM + GAC by incorporating external resources in the similar manner as the previous studies did. Concretely, SVM + GAC achieved 83.7 F1 score by adding features for WordNet, named entities (NE), and dependency paths explained in Hashimoto et al. (2015). Moreover, we could obtain 84.2 F1 score, using the ranking based loss function (dos Santos et al., 2015) and fine-tuning of the distributed representations initially trained by GAC. Currently, this is the second best score among the performance values reported in the previous studies on this task (the second group of Table 3). If we could use the negative sampling technique proposed by Xu et al. (2015), we might improve the performance further14 . 5 Related Work Mitchell and Lapata (2010) was a pioneering work in semantic modeling of short phrases. They constructed the dataset that contains"
P16-1215,S10-1006,0,0.248462,"Missing"
P16-1215,P82-1020,0,0.263933,"Missing"
P16-1215,S13-2007,0,0.0307031,"ction (dos Santos et al., 2015) and fine-tuning of the distributed representations initially trained by GAC. Currently, this is the second best score among the performance values reported in the previous studies on this task (the second group of Table 3). If we could use the negative sampling technique proposed by Xu et al. (2015), we might improve the performance further14 . 5 Related Work Mitchell and Lapata (2010) was a pioneering work in semantic modeling of short phrases. They constructed the dataset that contains two-word phrase pairs with semantic similarity judged by human annotators. Korkontzelos et al. (2013) provided a semantic similarity dataset with pairs of two words and a single word. Wieting et al. (2015) annotated a part of PPDB (Ganitkevitch et al., 2013) to evaluate semantic modeling of paraphrases. Although the target unit of semantic modeling is different from that for these previous studies, we follow the annotation guideline and instruction of Mitchell and Lapata (2010) to build the new dataset. The task addressed in this paper is also related to the Semantic Textual Similarity (STS) task (Agirre et al., 2012). STS is the task to measure the degree of semantic similarity between two s"
P16-1215,Y14-1010,1,0.830135,"ons with strong collocations from a training corpus. However, this approach might be affected by data sparseness, which lowers the quality of distributed representations. Another simple but effective approach is additive composition (Mitchell and Lapata, 2010), where the distributed representation of a relational pattern is computed by the mean of embeddings of constituent words. Presuming that a relational pattern consists of a sequence of T words w1 , ..., wT , then we let xt ∈ Rd the embedding of the word 1 ∑T wt . This approach computes T t=1 xt as the embedding of the relational pattern. Muraoka et al. (2014) reported that the additive composition is a strong baseline among various methods. 3.2 Recurrent Neural Network Recently, a number of studies model semantic compositions of phrases and sentences by using (a variant of) Recurrent Neural Network (RNN) (Sutskever et al., 2014; Tang et al., 2015). For a given embedding xt at position t, the vanilla RNN (Elman, 1990) computes the hidden state ht ∈ Rd by the following recursive equation5 , ht = g(Wx xt + Wh ht−1 ). (1) Here, Wx and Wh are d × d matrices (parameters), g(.) is the elementwise activation function (tanh). We set h0 = 0 at t = 1. In ess"
P16-1215,D12-1104,0,0.106757,"emely important because a relation (e.g., causality) can be mentioned by various expressions (e.g., “X cause Y”, “X lead to Y”, “Y is associated with X”). To make matters worse, relational patterns are highly productive: we can produce a emphasized causality pattern “X increase the severe risk of Y” from “X increase the risk of Y” by inserting severe to the pattern. To model the meanings of relational patterns, the previous studies built a co-occurrence matrix between relational patterns (e.g., “X increase the risk of Y”) and entity pairs (e.g., “X: smoking, Y: cancer”) (Lin and Pantel, 2001; Nakashole et al., 2012). Based on the distributional hypothesis (Harris, 1954), we can compute a semantic vector of a relational pattern from the co-occurrence matrix, and measure the similarity of two relational patterns as the cosine similarity of the vectors. Nowadays, several studies adopt distributed representations computed by neural networks for semantic modeling of relational patterns (Yih et al., 2014; Takase et al., 2016). Notwithstanding, the previous studies paid little attention to explicitly evaluate semantic modeling of relational patterns. In this paper, we construct a new dataset that contains a pai"
P16-1215,D14-1162,0,0.0890972,"ation classification (word2vec) Training Evaluation (§4.2) increase: risk: open: Word embeddings SemEval 2010 Task 8 Figure 1: Overview of this study. Introduction Knowledge about entities and their relations (relation instances) are crucial for a wide spectrum of NLP applications, e.g., information retrieval, question answering, and recognizing textual entailment. Learning distributed representations for relation instances is a central technique in downstream applications as a number of recent studies demonstrated the usefulness of distributed representations for words (Mikolov et al., 2013; Pennington et al., 2014) and sentences (Sutskever et al., 2014; Cho et al., 2014; Kiros et al., 2015). In particular, semantic modeling of relations and their textual realizations (relational patterns hereafter) is extremely important because a relation (e.g., causality) can be mentioned by various expressions (e.g., “X cause Y”, “X lead to Y”, “Y is associated with X”). To make matters worse, relational patterns are highly productive: we can produce a emphasized causality pattern “X increase the severe risk of Y” from “X increase the risk of Y” by inserting severe to the pattern. To model the meanings of relational"
P16-1215,N13-1008,0,0.0424569,"ication. On the SemEval 2010 Task 8, several studies considered semantic composition. Gormley et al. (2015) proposed Feature-rich Compositional Embedding Model (FCM) that can combine binary features (e.g., positional indicators) with word embeddings via outer products. dos Santos et al. (2015) addressed the task using Convolutional Neural Network (CNN). Xu et al. (2015) achieved a higher performance than dos 2283 Santos et al. (2015) by application of CNN on dependency paths. In addition to the relation classification task, we briefly describe other applications. To populate a knowledge base, Riedel et al. (2013) jointly learned latent feature vectors of entities, relational patterns, and relation types in the knowledge base. Toutanova et al. (2015) adapted CNN to capture the compositional structure of a relational pattern during the joint learning. For open domain question answering, Yih et al. (2014) proposed the method to map an interrogative sentence on an entity and a relation type contained in a knowledge base by using CNN. Although these reports described good performance on the respective tasks, we are unsure of the generality of distributed representations trained for a specific task such as"
P16-1215,S10-1057,0,0.0360011,"Missing"
P16-1215,D12-1110,0,0.282802,"tions for a different application, we address the task of relation classification on the SemEval 2010 Task 8 dataset (Hendrickx et al., 2010). In other words, we explore whether high-quality distributed representations of relational patterns are effective to identify a relation type of an entity pair. The dataset consists of 10, 717 relation instances (8, 000 training and 2, 717 test instances) with their relation types annotated. The dataset 2281 Method SVM SVM + NoComp SVM + LSTM SVM + Add SVM + GRU SVM + RNN SVM + GAC Ranking loss + GAC w/ fine-tuning SVM (Rink and Harabagiu, 2010) MV-RNN (Socher et al., 2012) FCM (Gormley et al., 2015) w/o fine-tuning w/ fine-tuning RelEmb (Hashimoto et al., 2015) CR-CNN (dos Santos et al., 2015) w/ Other w/o Other depLCNN (Xu et al., 2015) depLCNN + NS Feature set BoW, POS embeddings, BoW, POS embeddings, BoW, POS embeddings, BoW, POS embeddings, BoW, POS embeddings, BoW, POS embeddings, BoW, POS + dependency, WordNet, NE embeddings, BoW, POS + dependency, WordNet, NE BoW, POS, dependency, Google n-gram, etc. embeddings, parse trees + WordNet, POS, NE embeddings, dependency + WordNet embeddings, dependency + NE embeddings + dependency, WordNet, NE embeddings, wor"
P16-1215,P13-1045,0,0.034643,"e transitivity of pattern pairs and applied transitivities to find patterns in entailment relation. These studies did not consider semantic composition of relational patterns. Thus, they might suffer from the data sparseness problem, as shown by NoComp in Figure 4. Numerous studies have been aimed at encoding distributed representations of phrases and sentences from word embeddings by using: Recursive Neural Network (Socher et al., 2011), Matrix Vector Recursive Neural Network (Socher et al., 2012), Recursive Neural Network with different weight matrices corresponding to syntactic categories (Socher et al., 2013) or word types (Takase et al., 2016), RNN (Sutskever et al., 2011), LSTM (Sutskever et al., 2014), GRU (Cho et al., 2014), PAS-CLBLM (Hashimoto et al., 2014), etc. As described in Section 3, we applied RNN, GRU, and LSTM to compute distributed representations of relational patterns because recent papers have demonstrated their superiority in semantic composition (Sutskever et al., 2014; Tang et al., 2015). In this paper, we presented a comparative study of different encoders for semantic modeling of relational patterns. To investigate usefulness of the distributed representations and the new d"
P16-1215,P15-1150,0,0.0951605,"Missing"
P16-1215,D15-1167,0,0.166823,"n of a relational pattern is computed by the mean of embeddings of constituent words. Presuming that a relational pattern consists of a sequence of T words w1 , ..., wT , then we let xt ∈ Rd the embedding of the word 1 ∑T wt . This approach computes T t=1 xt as the embedding of the relational pattern. Muraoka et al. (2014) reported that the additive composition is a strong baseline among various methods. 3.2 Recurrent Neural Network Recently, a number of studies model semantic compositions of phrases and sentences by using (a variant of) Recurrent Neural Network (RNN) (Sutskever et al., 2014; Tang et al., 2015). For a given embedding xt at position t, the vanilla RNN (Elman, 1990) computes the hidden state ht ∈ Rd by the following recursive equation5 , ht = g(Wx xt + Wh ht−1 ). (1) Here, Wx and Wh are d × d matrices (parameters), g(.) is the elementwise activation function (tanh). We set h0 = 0 at t = 1. In essence, RNN computes the hidden state ht based on the one at the previous position (ht−1 ) and the word embedding xt . Applying Equation 1 from t = 1 to T , we use hT as the distributed representation of the relational pattern. 3.3 RNN variants We also employ Long Short-Term Memory (LSTM) (Hochr"
P16-1215,D15-1174,0,0.0222456,"itional Embedding Model (FCM) that can combine binary features (e.g., positional indicators) with word embeddings via outer products. dos Santos et al. (2015) addressed the task using Convolutional Neural Network (CNN). Xu et al. (2015) achieved a higher performance than dos 2283 Santos et al. (2015) by application of CNN on dependency paths. In addition to the relation classification task, we briefly describe other applications. To populate a knowledge base, Riedel et al. (2013) jointly learned latent feature vectors of entities, relational patterns, and relation types in the knowledge base. Toutanova et al. (2015) adapted CNN to capture the compositional structure of a relational pattern during the joint learning. For open domain question answering, Yih et al. (2014) proposed the method to map an interrogative sentence on an entity and a relation type contained in a knowledge base by using CNN. Although these reports described good performance on the respective tasks, we are unsure of the generality of distributed representations trained for a specific task such as the relation classification. In contrast, this paper demonstrated the contribution of distributed representations trained in a generic mann"
P16-1215,Q15-1025,0,0.0175169,"Currently, this is the second best score among the performance values reported in the previous studies on this task (the second group of Table 3). If we could use the negative sampling technique proposed by Xu et al. (2015), we might improve the performance further14 . 5 Related Work Mitchell and Lapata (2010) was a pioneering work in semantic modeling of short phrases. They constructed the dataset that contains two-word phrase pairs with semantic similarity judged by human annotators. Korkontzelos et al. (2013) provided a semantic similarity dataset with pairs of two words and a single word. Wieting et al. (2015) annotated a part of PPDB (Ganitkevitch et al., 2013) to evaluate semantic modeling of paraphrases. Although the target unit of semantic modeling is different from that for these previous studies, we follow the annotation guideline and instruction of Mitchell and Lapata (2010) to build the new dataset. The task addressed in this paper is also related to the Semantic Textual Similarity (STS) task (Agirre et al., 2012). STS is the task to measure the degree of semantic similarity between two sentences. Even though a relational pattern appears as a part of a sentence, it may be difficult to trans"
P16-1215,D15-1062,0,0.110593,"es in the similar manner as the previous studies did. Concretely, SVM + GAC achieved 83.7 F1 score by adding features for WordNet, named entities (NE), and dependency paths explained in Hashimoto et al. (2015). Moreover, we could obtain 84.2 F1 score, using the ranking based loss function (dos Santos et al., 2015) and fine-tuning of the distributed representations initially trained by GAC. Currently, this is the second best score among the performance values reported in the previous studies on this task (the second group of Table 3). If we could use the negative sampling technique proposed by Xu et al. (2015), we might improve the performance further14 . 5 Related Work Mitchell and Lapata (2010) was a pioneering work in semantic modeling of short phrases. They constructed the dataset that contains two-word phrase pairs with semantic similarity judged by human annotators. Korkontzelos et al. (2013) provided a semantic similarity dataset with pairs of two words and a single word. Wieting et al. (2015) annotated a part of PPDB (Ganitkevitch et al., 2013) to evaluate semantic modeling of paraphrases. Although the target unit of semantic modeling is different from that for these previous studies, we fo"
P16-1215,P14-2105,0,0.0966919,"ational patterns, the previous studies built a co-occurrence matrix between relational patterns (e.g., “X increase the risk of Y”) and entity pairs (e.g., “X: smoking, Y: cancer”) (Lin and Pantel, 2001; Nakashole et al., 2012). Based on the distributional hypothesis (Harris, 1954), we can compute a semantic vector of a relational pattern from the co-occurrence matrix, and measure the similarity of two relational patterns as the cosine similarity of the vectors. Nowadays, several studies adopt distributed representations computed by neural networks for semantic modeling of relational patterns (Yih et al., 2014; Takase et al., 2016). Notwithstanding, the previous studies paid little attention to explicitly evaluate semantic modeling of relational patterns. In this paper, we construct a new dataset that contains a pair of relational patterns with five similarity ratings judged by human annotators. The new dataset shows a 2276 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2276–2286, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics high inter-annotator agreement, following the annotation guideline of Mitchell and Lapat"
P16-1215,P12-2031,0,0.389909,"t Gated Additive Composition, which is an enhancement of additive composition with the gating mechanism. We utilize the Skip-gram objective for training the parameters of the encoders on a large unlabeled corpus. Experiments show that the new dataset does not only enable detailed analyses of the different encoders, but also provides a gauge to predict successes of distributed representations of relational patterns in another task (relation classification). Figure 1 illustrates the overview of this study. 2 Data Construction 2.1 Target relation instances We build a new dataset upon the work of Zeichner et al. (2012), which consists of relational patterns with semantic inference labels annotated. The dataset includes 5,555 pairs2 extracted by Reverb (Fader et al., 2011), 2,447 pairs with inference relation and 3,108 pairs (the rest) without one. Initially, we considered using this high-quality dataset as it is for semantic modeling of relational patterns. However, we found that inference relations exhibit quite different properties from those of semantic similarity. Take a relational pattern pair “X be the part of Y” and “X be an essential part of Y” filled with “X = the small intestine, Y = the digestive"
P16-3021,C10-1032,0,0.0789266,"Missing"
P16-3021,D11-1072,0,0.0413109,"Missing"
P16-3021,Q15-1023,0,0.0618927,"v Jargalsaikhan Naoaki Okazaki Koji Matsuda Kentaro Inui Tohoku University, Japan {davaajav, okazaki, matsuda, inui}@ecei.toholu.ac.jp Abstract EL is useful for various NLP tasks, e.g., Question-Answering (Khalid et al., 2008), Information Retrieval (Blanco et al., 2015), Knowledge Base Population (Dredze et al., 2010), CoReference Resolution (Hajishirzi et al., 2013). There are about a dozen of datasets targeting EL in English, including UIUC datasets (ACE, MSNBC) (Ratinov et al., 2011), AIDA datasets (Hoffart et al., 2011), and TAC-KBP datasets (2009–2012 datasets) (McNamee and Dang, 2009). Ling et al. (2015) discussed various challenges in EL. They argued that the existing datasets are inconsistent with each other. For instance, TACKBP targets only mentions belonging to PERSON, LOCATION, ORGANIZATION classes. Although these entity classes may be dominant in articles, other tasks may require information on natural phenomena, product names, and institution names. In contrast, the MSNBC corpus does not limit entity classes, linking mentions to any Wikipedia article. However, the MSNBC corpus does not have a NIL label even if a mention belongs to an important class such as PERSON or LOCATION, unlike"
P16-3021,P11-1138,0,0.0287818,"itten Japanese (BCCWJ) 3 annotated with finegrained named entity labels defined by Sekine’s Extended Named Entity Hierarchy (Sekine et al., 2002) 4 . is limited to mentions that belong to PERSON, LOCATION or ORGANIZATION classes only. When an article is not present in Wikipedia, UIUC does not record this information in any way. On the contrary, TAC-KBP5 and our datasets have NIL tag used to mark a mention when it does not have an entry in KB. 2.1 Design Policy To give a better understanding of our dataset we briefly compare it with existing English datasets. The most comparable ones are UIUC (Ratinov et al., 2011) and TAC-KBP 2009–2012 datasets (McNamee and Dang, 2009). Although, AIDA datasets are widely used for Disambiguation of Entities, AIDA uses YAGO, an unique Knowledge Base derived from Wikipedia, GeoNames and Wordnet, which makes it difficult to compare. UIUC is similar to our dataset in a sense that it links to any Wikipedia article without any semantic class restrictions, unlike TAC-KBP which Ling et al. (2015) argued that the task definition of EL itself is challenging: whether to target only named entities (NEs) or to include general nouns; whether to limit semantic classes of target NEs; h"
P16-3021,sekine-etal-2002-extended,0,0.0922538,"of word segmentation for unsegmented languages, like Chinese or Japanese. (Murawaki and Mori, 2016) approach the word segmentation problem from point of view of Wikification. Their focus is on the word segmentation rather than on the linking. In this research, we build a Japanese Wikification corpus in which mentions in Japanese documents are associated with Japanese Wikipedia articles. The corpus consists of 340 newspaper articles from Balanced Corpus of Contemporary Written Japanese (BCCWJ) 3 annotated with finegrained named entity labels defined by Sekine’s Extended Named Entity Hierarchy (Sekine et al., 2002) 4 . is limited to mentions that belong to PERSON, LOCATION or ORGANIZATION classes only. When an article is not present in Wikipedia, UIUC does not record this information in any way. On the contrary, TAC-KBP5 and our datasets have NIL tag used to mark a mention when it does not have an entry in KB. 2.1 Design Policy To give a better understanding of our dataset we briefly compare it with existing English datasets. The most comparable ones are UIUC (Ratinov et al., 2011) and TAC-KBP 2009–2012 datasets (McNamee and Dang, 2009). Although, AIDA datasets are widely used for Disambiguation of Enti"
P16-3021,spitkovsky-chang-2012-cross,0,0.026119,"tity mention recognition. Nested entities It was assumed that the role initially served as a temporary peacemaker to persuade Ali al-Sistani, the spiritual leader of Shia Muslims: Position Vocation. Since the mention in the sentence refers to the highest ranking position of a specific religion, it is inappropriate to link the mention to the article Spiritual Leader nor Shia Muslim. Therefore, we decided to mark this mention as NIL. 3.1 Wikification without fine-grained semantic classes Our experiment is based on the disambiguation method that uses the probability distribution of anchor texts (Spitkovsky and Chang, 2012). Given a mention m, the method predicts an entity eˆ that yields the highest probability p(e|m), Entity changes over time In his greeting speech, the representative Ito expressed his opinion on the upcoming gubernatorial election: Event Other and Sapporo city mayoral election. eˆ = argmax p(e|m). e∈E 141 (1) Category Province Country GPE Other Political Party Pro Sports Organization City Company Group Mammal International Organization Company Game Conference Public Institution Book Political Organization Other Organization Other GOE Other Plan Character Occasion Other Example Fukuoka Prefectu"
P16-3021,D13-1029,0,\N,Missing
P16-3021,E12-2021,0,\N,Missing
P17-1037,W11-1701,0,0.0188519,"from liberal to conservative (Adamic and Glance, 2005; Zhou et al., 2011; Cohen and Ruths, 2013; Bakshy et al., 2015; Wong et al., 2016). Even though these studies provide intuitive visualizations and interpretations along the liberalconservative axis, political analysts argue that the axis is flawed and insufficient for representing public opinion and ideologies (Kerlinger, 1984; Maddox and Lilie, 1984). A potential solution for analyzing multiple axes of the political spectrum on social media is stance detection (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Murakami and Raymond, 2010; Anand et al., 2011; Walker et al., 2012; Mohammad et al., 2016; Johnson and Goldwasser, 2016), whose task is to determine whether the author of a text is for, neutral, or against a topic (e.g., free trade, immigration, abortion). However, stance detection across different topics is extremely difficult. Anand et al. (2011) reported that a sophisticated method with topic-dependent features substantially improved the performance of stance detection within a topic, but such an approach could not outperform a baseline method with simple n-gram features when evaluated across topics. More recently, all participants of"
P17-1037,D15-1008,0,0.0175142,"and text data associated with the labels. Employing a single axis (e.g., liberal to conservative) or a few axes (e.g., political parties and candidates of elections), these studies provide intuitive visualizations and interpretations along the respective axes. In contrast, this study is the first attempt to recognize and organize various axes of topics on social media with no prior assumptions regarding the axes. Therefore, we think our study provides a new tool for computational social science and political science that enables researchers to analyze and interpret phenomena on social media. Bamman and Smith (2015) presented an unsupervised method for assessing the political stance of a proposition, such as “global warming is a hoax,” along the political spectrum of liberal to conservative. In their work, a proposition was represented by a tuple in the form hsubject, predicatei, for example, hglobal warming, hoaxi. They presented a generative model for users, subjects, and predicates to find a one-dimensional latent space that corresponded to the political spectrum. Next, we describe previous research focused on acquiring lexical knowledge of politics. Sim et al. (2013) measured ideological positions of"
P17-1037,W03-1210,0,0.0191371,"rget of the opinion from text. Although these previous studies have the potential to improve the quality of the user-topic matrix R, unfortunately, no corpus or resource is available for the Japanese language. We do not currently have a large collection of English tweets, but combining fine-grained opinion analysis with matrix factorization is an immediate future work. Acknowledgements Causality Relation Some of inter-topic preferences in this work can be explained by causality relation, for example, “TPP promotes free trade.” A number of previous studies acquire instances of causal relation (Girju, 2003; Do et al., 2011) and promote/suppress relation (Hashimoto et al., 2012; Fluck et al., 2015) from text. The causality knowledge is useful for predicting (hypotheses of) future events (Radinsky et al., 2012; Radinsky and Davidovich, 2012; Hashimoto et al., 2015). Inter-topic preferences, however, also include pairs of topics in which causality relation hardly holds. As an example, it is unreasonable to infer that nuclear plant and railroading of bills have a causal relation, but those who dislike nuclear plant also oppose railroading of bills because presumably they think the governing politic"
P17-1037,W06-1651,0,0.0427249,"Missing"
P17-1037,D12-1057,0,0.0316662,"Missing"
P17-1037,C10-2028,0,0.0465547,"specifies high requirements for making amendments to Constitution of Japan (including Article 9). 2 399 uct of low-rank user and topic matrices. These low-rank matrices provide latent vector representations of both users and topics. This approach is also useful for completing preferences of “ordinary” (i.e., less vocal) users, which fills the gap between different types of users. The contributions of this paper are threefold. In order to design linguistic patterns, we focus on hashtags appearing in the corpus that have been popular clues for locating subjective statements such as sentiments (Davidov et al., 2010), emotions (Qadir and Riloff, 2014), and ironies (Van Hee et al., 2016). Hashtags are also useful for finding strong supporters and critics, as well as their target topics; for example, #immigrantsWelcome indicates that the author favors immigrants; and #StopAbortion is against abortion. Based on this intuition, we design regular expressions for both pro hashtags “#(.+)sansei”3 and con hashtags “#(.+)hantai”4 , where (.+) matches a target topic. These regular expressions can find users who have strong preferences to topics. Using this approach, we extracted 31,068 occurrences of pro/con hashta"
P17-1037,N15-1146,0,0.0287078,"the topic and user vectors to create a crosstopic stance detector. It is possible to generalize our work to model heterogeneous signals, such as interests and behaviors of people, for example, “those who are interested in A also support B,” and “those who favor A also vote for B”. Therefore, we believe that our work will bring about new applications in the field of NLP and other disciplines. Fine-grained Opinion Analysis The method presented in Section 2 is an instance of finegrained opinion analysis (Wiebe et al., 2005; Choi et al., 2006; Johansson and Moschitti, 2010; Yang and Cardie, 2013; Deng and Wiebe, 2015), which extracts a tuple of a subjective opinion, a holder of the opinion, and a target of the opinion from text. Although these previous studies have the potential to improve the quality of the user-topic matrix R, unfortunately, no corpus or resource is available for the Japanese language. We do not currently have a large collection of English tweets, but combining fine-grained opinion analysis with matrix factorization is an immediate future work. Acknowledgements Causality Relation Some of inter-topic preferences in this work can be explained by causality relation, for example, “TPP promot"
P17-1037,W10-2910,0,0.0535646,"Missing"
P17-1037,C16-1279,0,0.0178466,"l., 2011; Cohen and Ruths, 2013; Bakshy et al., 2015; Wong et al., 2016). Even though these studies provide intuitive visualizations and interpretations along the liberalconservative axis, political analysts argue that the axis is flawed and insufficient for representing public opinion and ideologies (Kerlinger, 1984; Maddox and Lilie, 1984). A potential solution for analyzing multiple axes of the political spectrum on social media is stance detection (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Murakami and Raymond, 2010; Anand et al., 2011; Walker et al., 2012; Mohammad et al., 2016; Johnson and Goldwasser, 2016), whose task is to determine whether the author of a text is for, neutral, or against a topic (e.g., free trade, immigration, abortion). However, stance detection across different topics is extremely difficult. Anand et al. (2011) reported that a sophisticated method with topic-dependent features substantially improved the performance of stance detection within a topic, but such an approach could not outperform a baseline method with simple n-gram features when evaluated across topics. More recently, all participants of SemEval 2016 Task 6A (with five topics) could not outperform the baseline"
P17-1037,D13-1010,0,0.0139787,"phenomena on social media. Bamman and Smith (2015) presented an unsupervised method for assessing the political stance of a proposition, such as “global warming is a hoax,” along the political spectrum of liberal to conservative. In their work, a proposition was represented by a tuple in the form hsubject, predicatei, for example, hglobal warming, hoaxi. They presented a generative model for users, subjects, and predicates to find a one-dimensional latent space that corresponded to the political spectrum. Next, we describe previous research focused on acquiring lexical knowledge of politics. Sim et al. (2013) measured ideological positions of candidates in US presidential elections from their Similar to our present work, their work (Bamman and Smith, 2015) did not require labeled data 404 corporate approaches to acquire causality knowledge. to map users and topics (i.e., subjects) onto a latent feature space. In their paper, they reported that the generative model outperformed Principal Component Analysis (PCA), which is a method for matrix factorization. Empirical results here probably reflected the underlying assumptions that PCA treats missing elements as zero and not as missing data. In contra"
P17-1037,P09-1026,0,0.034675,"communities on social media along the political spectrum from liberal to conservative (Adamic and Glance, 2005; Zhou et al., 2011; Cohen and Ruths, 2013; Bakshy et al., 2015; Wong et al., 2016). Even though these studies provide intuitive visualizations and interpretations along the liberalconservative axis, political analysts argue that the axis is flawed and insufficient for representing public opinion and ideologies (Kerlinger, 1984; Maddox and Lilie, 1984). A potential solution for analyzing multiple axes of the political spectrum on social media is stance detection (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Murakami and Raymond, 2010; Anand et al., 2011; Walker et al., 2012; Mohammad et al., 2016; Johnson and Goldwasser, 2016), whose task is to determine whether the author of a text is for, neutral, or against a topic (e.g., free trade, immigration, abortion). However, stance detection across different topics is extremely difficult. Anand et al. (2011) reported that a sophisticated method with topic-dependent features substantially improved the performance of stance detection within a topic, but such an approach could not outperform a baseline method with simple n-gram features when evaluated a"
P17-1037,W06-1639,0,0.0619255,"ions, influences, and communities on social media along the political spectrum from liberal to conservative (Adamic and Glance, 2005; Zhou et al., 2011; Cohen and Ruths, 2013; Bakshy et al., 2015; Wong et al., 2016). Even though these studies provide intuitive visualizations and interpretations along the liberalconservative axis, political analysts argue that the axis is flawed and insufficient for representing public opinion and ideologies (Kerlinger, 1984; Maddox and Lilie, 1984). A potential solution for analyzing multiple axes of the political spectrum on social media is stance detection (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Murakami and Raymond, 2010; Anand et al., 2011; Walker et al., 2012; Mohammad et al., 2016; Johnson and Goldwasser, 2016), whose task is to determine whether the author of a text is for, neutral, or against a topic (e.g., free trade, immigration, abortion). However, stance detection across different topics is extremely difficult. Anand et al. (2011) reported that a sophisticated method with topic-dependent features substantially improved the performance of stance detection within a topic, but such an approach could not outperform a baseline method with simple n-"
P17-1037,S16-1003,0,0.135031,"Missing"
P17-1037,C10-2100,0,0.200376,"long the political spectrum from liberal to conservative (Adamic and Glance, 2005; Zhou et al., 2011; Cohen and Ruths, 2013; Bakshy et al., 2015; Wong et al., 2016). Even though these studies provide intuitive visualizations and interpretations along the liberalconservative axis, political analysts argue that the axis is flawed and insufficient for representing public opinion and ideologies (Kerlinger, 1984; Maddox and Lilie, 1984). A potential solution for analyzing multiple axes of the political spectrum on social media is stance detection (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Murakami and Raymond, 2010; Anand et al., 2011; Walker et al., 2012; Mohammad et al., 2016; Johnson and Goldwasser, 2016), whose task is to determine whether the author of a text is for, neutral, or against a topic (e.g., free trade, immigration, abortion). However, stance detection across different topics is extremely difficult. Anand et al. (2011) reported that a sophisticated method with topic-dependent features substantially improved the performance of stance detection within a topic, but such an approach could not outperform a baseline method with simple n-gram features when evaluated across topics. More recently,"
P17-1037,C16-1257,0,0.050316,"Missing"
P17-1037,D14-1127,0,0.0300221,"making amendments to Constitution of Japan (including Article 9). 2 399 uct of low-rank user and topic matrices. These low-rank matrices provide latent vector representations of both users and topics. This approach is also useful for completing preferences of “ordinary” (i.e., less vocal) users, which fills the gap between different types of users. The contributions of this paper are threefold. In order to design linguistic patterns, we focus on hashtags appearing in the corpus that have been popular clues for locating subjective statements such as sentiments (Davidov et al., 2010), emotions (Qadir and Riloff, 2014), and ironies (Van Hee et al., 2016). Hashtags are also useful for finding strong supporters and critics, as well as their target topics; for example, #immigrantsWelcome indicates that the author favors immigrants; and #StopAbortion is against abortion. Based on this intuition, we design regular expressions for both pro hashtags “#(.+)sansei”3 and con hashtags “#(.+)hantai”4 , where (.+) matches a target topic. These regular expressions can find users who have strong preferences to topics. Using this approach, we extracted 31,068 occurrences of pro/con hashtags used by 18,582 users for 4,899 t"
P17-1037,P13-1161,0,0.0153711,"work, we plan to embed the topic and user vectors to create a crosstopic stance detector. It is possible to generalize our work to model heterogeneous signals, such as interests and behaviors of people, for example, “those who are interested in A also support B,” and “those who favor A also vote for B”. Therefore, we believe that our work will bring about new applications in the field of NLP and other disciplines. Fine-grained Opinion Analysis The method presented in Section 2 is an instance of finegrained opinion analysis (Wiebe et al., 2005; Choi et al., 2006; Johansson and Moschitti, 2010; Yang and Cardie, 2013; Deng and Wiebe, 2015), which extracts a tuple of a subjective opinion, a holder of the opinion, and a target of the opinion from text. Although these previous studies have the potential to improve the quality of the user-topic matrix R, unfortunately, no corpus or resource is available for the Japanese language. We do not currently have a large collection of English tweets, but combining fine-grained opinion analysis with matrix factorization is an immediate future work. Acknowledgements Causality Relation Some of inter-topic preferences in this work can be explained by causality relation, f"
P19-1202,H05-1042,0,0.183494,"Missing"
P19-1202,P16-1154,0,0.0600758,"ed data including sports commentary (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecast (Liang et al., 2009; Mei et al., 2016), biographical text from infobox in Wikipedia (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018) and market comments from stock prices (Murakami et al., 2017; Aoki et al., 2018). Neural generation methods have become the mainstream approach for data-to-text generation. The encoder-decoder framework (Cho et al., 2014; Sutskever et al., 2014) with the attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanism (Gu et al., 2016; Gulcehre et al., 2016) has successfully applied to data-to-text tasks. However, neural generation methods sometimes yield fluent but inadequate descriptions (Tu et al., 2017). In data-to-text generation, descriptions inconsistent to the input data are problematic. Recently, Wiseman et al. (2017) introduced the ROTOW IRE dataset, which contains multisentence summaries of basketball games with boxscore (Table 1). This dataset requires the selection of a salient subset of data records for generating descriptions. They also proposed automatic evaluation metrics for measuring the informativeness"
P19-1202,P16-1014,0,0.0614873,"sports commentary (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecast (Liang et al., 2009; Mei et al., 2016), biographical text from infobox in Wikipedia (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018) and market comments from stock prices (Murakami et al., 2017; Aoki et al., 2018). Neural generation methods have become the mainstream approach for data-to-text generation. The encoder-decoder framework (Cho et al., 2014; Sutskever et al., 2014) with the attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanism (Gu et al., 2016; Gulcehre et al., 2016) has successfully applied to data-to-text tasks. However, neural generation methods sometimes yield fluent but inadequate descriptions (Tu et al., 2017). In data-to-text generation, descriptions inconsistent to the input data are problematic. Recently, Wiseman et al. (2017) introduced the ROTOW IRE dataset, which contains multisentence summaries of basketball games with boxscore (Table 1). This dataset requires the selection of a salient subset of data records for generating descriptions. They also proposed automatic evaluation metrics for measuring the informativeness of generated summaries."
P19-1202,D18-1130,0,0.0233321,"that the both consider a sequence of data records as content planning. However, our proposal differs from theirs in that ours uses a recurrent neural network for saliency tracking, and that our decoder dynamically chooses a data record to be mentioned without fixing a sequence of data records. 2.2 Memory modules The memory network can be used to maintain and update representations of the salient information (Weston et al., 2015; Sukhbaatar et al., 2015; Graves et al., 2016). This module is often used in natural language understanding to keep track of the entity state (Kobayashi et al., 2016; Hoang et al., 2018; Bosselut et al., 2018). Recently, entity tracking has been popular for generating coherent text (Kiddon et al., 2016; Ji et al., 2017; Yang et al., 2017; Clark et al., 2018). Kiddon et al. (2016) proposed a neural checklist model that updates predefined item states. Ji et al. (2017) proposed an entity representation for the language model. Updating entity tracking states when the entity is introduced, their method selects the salient entity state. Our model extends this entity tracking module for data-to-text generation tasks. The entity tracking module selects the salient entity and appropr"
P19-1202,D17-1195,0,0.0211576,"rrent neural network for saliency tracking, and that our decoder dynamically chooses a data record to be mentioned without fixing a sequence of data records. 2.2 Memory modules The memory network can be used to maintain and update representations of the salient information (Weston et al., 2015; Sukhbaatar et al., 2015; Graves et al., 2016). This module is often used in natural language understanding to keep track of the entity state (Kobayashi et al., 2016; Hoang et al., 2018; Bosselut et al., 2018). Recently, entity tracking has been popular for generating coherent text (Kiddon et al., 2016; Ji et al., 2017; Yang et al., 2017; Clark et al., 2018). Kiddon et al. (2016) proposed a neural checklist model that updates predefined item states. Ji et al. (2017) proposed an entity representation for the language model. Updating entity tracking states when the entity is introduced, their method selects the salient entity state. Our model extends this entity tracking module for data-to-text generation tasks. The entity tracking module selects the salient entity and appropriate attribute in each timestep, updates their states, and generates coherent summaries from the selected data record. 3 Data Through c"
P19-1202,D16-1032,0,0.014764,"that ours uses a recurrent neural network for saliency tracking, and that our decoder dynamically chooses a data record to be mentioned without fixing a sequence of data records. 2.2 Memory modules The memory network can be used to maintain and update representations of the salient information (Weston et al., 2015; Sukhbaatar et al., 2015; Graves et al., 2016). This module is often used in natural language understanding to keep track of the entity state (Kobayashi et al., 2016; Hoang et al., 2018; Bosselut et al., 2018). Recently, entity tracking has been popular for generating coherent text (Kiddon et al., 2016; Ji et al., 2017; Yang et al., 2017; Clark et al., 2018). Kiddon et al. (2016) proposed a neural checklist model that updates predefined item states. Ji et al. (2017) proposed an entity representation for the language model. Updating entity tracking states when the entity is introduced, their method selects the salient entity state. Our model extends this entity tracking module for data-to-text generation tasks. The entity tracking module selects the salient entity and appropriate attribute in each timestep, updates their states, and generates coherent summaries from the selected data record."
P19-1202,N16-1099,1,0.856363,"ea is similar to ours in that the both consider a sequence of data records as content planning. However, our proposal differs from theirs in that ours uses a recurrent neural network for saliency tracking, and that our decoder dynamically chooses a data record to be mentioned without fixing a sequence of data records. 2.2 Memory modules The memory network can be used to maintain and update representations of the salient information (Weston et al., 2015; Sukhbaatar et al., 2015; Graves et al., 2016). This module is often used in natural language understanding to keep track of the entity state (Kobayashi et al., 2016; Hoang et al., 2018; Bosselut et al., 2018). Recently, entity tracking has been popular for generating coherent text (Kiddon et al., 2016; Ji et al., 2017; Yang et al., 2017; Clark et al., 2018). Kiddon et al. (2016) proposed a neural checklist model that updates predefined item states. Ji et al. (2017) proposed an entity representation for the language model. Updating entity tracking states when the entity is introduced, their method selects the salient entity state. Our model extends this entity tracking module for data-to-text generation tasks. The entity tracking module selects the salien"
P19-1202,D16-1128,0,0.141635,"Missing"
P19-1202,N18-1204,0,0.0431553,"Missing"
P19-1202,P09-1011,0,0.277927,"Missing"
P19-1202,D15-1166,0,0.0235551,"riptions from structured or non-structured data including sports commentary (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecast (Liang et al., 2009; Mei et al., 2016), biographical text from infobox in Wikipedia (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018) and market comments from stock prices (Murakami et al., 2017; Aoki et al., 2018). Neural generation methods have become the mainstream approach for data-to-text generation. The encoder-decoder framework (Cho et al., 2014; Sutskever et al., 2014) with the attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanism (Gu et al., 2016; Gulcehre et al., 2016) has successfully applied to data-to-text tasks. However, neural generation methods sometimes yield fluent but inadequate descriptions (Tu et al., 2017). In data-to-text generation, descriptions inconsistent to the input data are problematic. Recently, Wiseman et al. (2017) introduced the ROTOW IRE dataset, which contains multisentence summaries of basketball games with boxscore (Table 1). This dataset requires the selection of a salient subset of data records for generating descriptions. They also proposed automatic evaluation metric"
P19-1202,N16-1086,0,0.161392,"Missing"
P19-1202,P02-1040,0,0.103895,"rmation to each of the two stages, we can clearly see which part of the model to which the writer information contributes to. For Puduppully et al. (2019) model, we attach the writer information in the following three ways: 1. concatenating writer embedding w with the input vector for LSTM in the content planning decoder (stage 1); 2. concatenating writer embedding w with the input vector for LSTM in the text generator (stage 2); 3. using both 1 and 2 above. For more details about each decoding stage, readers can refer to Puduppully et al. (2019). 5.3 As evaluation metrics, we use BLEU score (Papineni et al., 2002) and the extractive metrics proposed by Wiseman et al. (2017), i.e., relation generation (RG), content selection (CS), and content ordering (CO) as evaluation metrics. The extractive metrics measure how well the relations extracted from the generated summary match the correct relations6 : 6 5 Our code is available from https://github.com/ aistairc/sports-reporter Evaluation metrics The model for extracting relation tuples was trained on tuples made from the entity (e.g., team name, city name, player name) and attribute value (e.g., “Lakers”, “92”) ex2107 - RG: the ratio of the correct relation"
P19-1202,Q17-1007,0,0.0173455,"phical text from infobox in Wikipedia (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018) and market comments from stock prices (Murakami et al., 2017; Aoki et al., 2018). Neural generation methods have become the mainstream approach for data-to-text generation. The encoder-decoder framework (Cho et al., 2014; Sutskever et al., 2014) with the attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanism (Gu et al., 2016; Gulcehre et al., 2016) has successfully applied to data-to-text tasks. However, neural generation methods sometimes yield fluent but inadequate descriptions (Tu et al., 2017). In data-to-text generation, descriptions inconsistent to the input data are problematic. Recently, Wiseman et al. (2017) introduced the ROTOW IRE dataset, which contains multisentence summaries of basketball games with boxscore (Table 1). This dataset requires the selection of a salient subset of data records for generating descriptions. They also proposed automatic evaluation metrics for measuring the informativeness of generated summaries. Puduppully et al. (2019) proposed a two-stage method that first predicts the sequence of data records to be mentioned and then generates a summary condi"
P19-1202,D17-1197,0,0.0124483,"ork for saliency tracking, and that our decoder dynamically chooses a data record to be mentioned without fixing a sequence of data records. 2.2 Memory modules The memory network can be used to maintain and update representations of the salient information (Weston et al., 2015; Sukhbaatar et al., 2015; Graves et al., 2016). This module is often used in natural language understanding to keep track of the entity state (Kobayashi et al., 2016; Hoang et al., 2018; Bosselut et al., 2018). Recently, entity tracking has been popular for generating coherent text (Kiddon et al., 2016; Ji et al., 2017; Yang et al., 2017; Clark et al., 2018). Kiddon et al. (2016) proposed a neural checklist model that updates predefined item states. Ji et al. (2017) proposed an entity representation for the language model. Updating entity tracking states when the entity is introduced, their method selects the salient entity state. Our model extends this entity tracking module for data-to-text generation tasks. The entity tracking module selects the salient entity and appropriate attribute in each timestep, updates their states, and generates coherent summaries from the selected data record. 3 Data Through careful examination,"
P19-1202,P98-2209,0,0.875231,"Missing"
S16-1065,C14-1008,0,0.0838307,"aches. In traditional feature-based approaches, we extract various features from a text. The features are usually constructed from n-grams (e.g., bigrams) of the texts and external resources such as lexicons and unlabeled corpora. In Neural Network based approaches, a number of models for text classifications exist; for example, Feed-Forward Neural Network model using an average of embeddings of target word sequences as the input layer (Iyyer et al., 2015), Recursive Neural Network (Socher et al., 2011; Socher et al., 2013), and Convolutional Neural Network (CNN) (Johnson and Zhang, 2015; dos Santos and Gatti, 2014; Kim, 2014). In this paper, we compare feature-based and Neural Network based approaches on the supervised stance classification task for tweets, SemEval2016 Task 6 Subtask A (Mohammad et al., 2016). The feature-based approach classifies tweets using logistic regression model. The features are extracted using external knowledge such as SentiWordNet (Esuli and Sebastiani, 2006) and a collection of crawled tweets, in addition to unigrams or bigrams in the target tweet. For the Neural Network approach, we implement CNN based on Kim (2014). As the input embeddings, we use word embeddings trained"
S16-1065,esuli-sebastiani-2006-sentiwordnet,0,0.0360146,"verage of embeddings of target word sequences as the input layer (Iyyer et al., 2015), Recursive Neural Network (Socher et al., 2011; Socher et al., 2013), and Convolutional Neural Network (CNN) (Johnson and Zhang, 2015; dos Santos and Gatti, 2014; Kim, 2014). In this paper, we compare feature-based and Neural Network based approaches on the supervised stance classification task for tweets, SemEval2016 Task 6 Subtask A (Mohammad et al., 2016). The feature-based approach classifies tweets using logistic regression model. The features are extracted using external knowledge such as SentiWordNet (Esuli and Sebastiani, 2006) and a collection of crawled tweets, in addition to unigrams or bigrams in the target tweet. For the Neural Network approach, we implement CNN based on Kim (2014). As the input embeddings, we use word embeddings trained by Continuous Bag-Of-Words (CBOW) model (Mikolov et al., 2013) on Wikipedia articles. The experimental results show that the CNN based approach performed the best in the cross validation on the training data. However the tendency was opposite on the test data probably because the CNN model overfitted to the training data. In contrast, the feature-based approach was more robust,"
S16-1065,P15-1162,0,0.0332708,"Missing"
S16-1065,P11-1016,0,0.0265287,"target word “feminist”. Then we get a feature TS=n using the same rules in SWN features. (e.g. TS=p, TS=n) SentiWordSubject (SWS): This feature focuses on sentiment expressed by subjective pronouns such as “I” or “we”, which may indicate emotions or stances of the user of a tweet. We obtain a sentiment polarity from the word modifying a subjective pronoun in a tweet, and include it as a feature. A sentiment polarity is obtained by SentiWordNet using the same rules for SWN features. (e.g. SWS=I=love=&gt;p, SWS=We=hate=&gt;n) TargetSentiment (TS): We also consider sentiment or emotion for the topics. Jiang et al. (2011) HighPMI (P): We crawled tweets containing target words, and collected words cooccuring with seed keywords (Table 3) in all crawled tweets for each topic. Table 2 shows query words and the number of crawled tweets for each topic. Then we calculate Point-wise Mutual Information (PMI) for all words. If the word in a tweet is in top 300 of the PMI, we generate a feature. This feature detects a tweet containing words related to the topic. This feature may be effective to classify whether NONE or not. (e.g. P=humanist, P=meninist) 403 e.g. Feminists ?Q Embedding vector ? ? ? 1×? Feminist are are a"
S16-1065,N15-1011,0,0.0507734,"nd Neural Network based approaches. In traditional feature-based approaches, we extract various features from a text. The features are usually constructed from n-grams (e.g., bigrams) of the texts and external resources such as lexicons and unlabeled corpora. In Neural Network based approaches, a number of models for text classifications exist; for example, Feed-Forward Neural Network model using an average of embeddings of target word sequences as the input layer (Iyyer et al., 2015), Recursive Neural Network (Socher et al., 2011; Socher et al., 2013), and Convolutional Neural Network (CNN) (Johnson and Zhang, 2015; dos Santos and Gatti, 2014; Kim, 2014). In this paper, we compare feature-based and Neural Network based approaches on the supervised stance classification task for tweets, SemEval2016 Task 6 Subtask A (Mohammad et al., 2016). The feature-based approach classifies tweets using logistic regression model. The features are extracted using external knowledge such as SentiWordNet (Esuli and Sebastiani, 2006) and a collection of crawled tweets, in addition to unigrams or bigrams in the target tweet. For the Neural Network approach, we implement CNN based on Kim (2014). As the input embeddings, we"
S16-1065,D14-1181,0,0.0201032,"ature-based approaches, we extract various features from a text. The features are usually constructed from n-grams (e.g., bigrams) of the texts and external resources such as lexicons and unlabeled corpora. In Neural Network based approaches, a number of models for text classifications exist; for example, Feed-Forward Neural Network model using an average of embeddings of target word sequences as the input layer (Iyyer et al., 2015), Recursive Neural Network (Socher et al., 2011; Socher et al., 2013), and Convolutional Neural Network (CNN) (Johnson and Zhang, 2015; dos Santos and Gatti, 2014; Kim, 2014). In this paper, we compare feature-based and Neural Network based approaches on the supervised stance classification task for tweets, SemEval2016 Task 6 Subtask A (Mohammad et al., 2016). The feature-based approach classifies tweets using logistic regression model. The features are extracted using external knowledge such as SentiWordNet (Esuli and Sebastiani, 2006) and a collection of crawled tweets, in addition to unigrams or bigrams in the target tweet. For the Neural Network approach, we implement CNN based on Kim (2014). As the input embeddings, we use word embeddings trained by Continuou"
S16-1065,S16-1003,0,0.1106,"Missing"
S16-1065,W02-1011,0,0.0207052,"eatures. (e.g. BoD=hate=&gt;i, BoD=like=&gt;not) BagOfPOSTag (BoP): We also extract features from POS tags. For example, if a tweet contains several interjections, the user probably has a negative opinion to the topic. We include all unigrams of POS tags in a tweet as features. (e.g. BoP=NOUN, BoP=UH) SentiWordNet (SWN): Content words in a tweet may express some sentiment, which indicates stances and emotions of the user. We use SentiWordNet (Esuli and Sebastiani, 2006) for introducing sentiment of a word. It assigns positive/negative/objective scores to each word. In sentiment classification task, Pang et al. (2002) introduce SentiWordNet features. Following their work, we include sentiment polarity features for nouns, verbs, Classifier Atheism 3-way Polarity Topic + 2-way Polarity 0.5314 0.5327 ClimateChange is a Real concern 0.5144 0.5248 Feminist Movement Hillary Clinton 0.5735 0.5860 0.5273 0.5502 Legalization Of Abortion 0.5277 0.5290 ALL topics 0.6083 0.6188 Table 4: Comparison of 3-way Polarity Classifier with Topic + 2-way Polarity Classifier on 10-fold cross validation using the feature-based approach. The scores were measured in a macro average of micro-F1 scores of FAVOR and AGAINST for each t"
S16-1065,D11-1014,0,0.0876461,"Missing"
S16-1065,D13-1170,0,0.00549138,"ication tasks, there are two major approaches; feature-based and Neural Network based approaches. In traditional feature-based approaches, we extract various features from a text. The features are usually constructed from n-grams (e.g., bigrams) of the texts and external resources such as lexicons and unlabeled corpora. In Neural Network based approaches, a number of models for text classifications exist; for example, Feed-Forward Neural Network model using an average of embeddings of target word sequences as the input layer (Iyyer et al., 2015), Recursive Neural Network (Socher et al., 2011; Socher et al., 2013), and Convolutional Neural Network (CNN) (Johnson and Zhang, 2015; dos Santos and Gatti, 2014; Kim, 2014). In this paper, we compare feature-based and Neural Network based approaches on the supervised stance classification task for tweets, SemEval2016 Task 6 Subtask A (Mohammad et al., 2016). The feature-based approach classifies tweets using logistic regression model. The features are extracted using external knowledge such as SentiWordNet (Esuli and Sebastiani, 2006) and a collection of crawled tweets, in addition to unigrams or bigrams in the target tweet. For the Neural Network approach, w"
S19-2061,S19-2005,0,0.0268965,"Missing"
S19-2061,W16-6208,0,0.047614,"Missing"
S19-2061,D14-1162,0,0.0831118,"kshop on Semantic Evaluation (SemEval-2019), pages 350–354 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics ner et al., 2016). Thus, we decided to concatenate word2vec and emoji2vec in our method as word embedding. pre-trained embeddings (Eisner et al., 2016). To achieve generalization and robustness in social media sentiment analysis, pre-trained embeddings should contain the representations of not only words from natural language but also emotion-related symbols, such as emoticons and emoji (Eisner et al., 2016). Both pre-trained embeddings GloVe (Pennington et al., 2014) and word2vec (Mikolov et al., 2013) do not contain representations for emotion-related symbols, which restricts the performance of sentiment analysis in social media. Although pre-trained emoji2vec embedding contains Unicode emoji representation, not all emotion-related symbols are included, such as emoticons. As emotion detection is a part of sentiment analysis, and the data from the task organizers contains emoticons and emoji for emotion expressions, we can utilize a neural network method with pretrained embedding to solve this task. We also need to address the lack of representations of e"
S19-2061,P17-1081,0,0.19292,"sual and voice data, ”Why did you not call me last night” may be classified as sad or angry without an appropriate understanding of context. The SemEval-2019 shared task 3: EmoContext: Contextual Emotion Detection in Text is the task to detect an emotion of a three-turn conversation (Chatterjee et al., 2019). We can consider this task as contextual sentiment analysis, as it requires detecting the emotion of the third-turn conversation by comprehensively understanding the contextual 2 Related Work Natural language processing in social media as an emergent area has attracted a lot of attention (Poria et al., 2017), especially from the recent advances in applying neural network methods with 350 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 350–354 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics ner et al., 2016). Thus, we decided to concatenate word2vec and emoji2vec in our method as word embedding. pre-trained embeddings (Eisner et al., 2016). To achieve generalization and robustness in social media sentiment analysis, pre-trained embeddings should contain the representations of not only words from natural langua"
saetre-etal-2008-connecting,N03-2020,1,\N,Missing
saetre-etal-2008-connecting,P06-1128,1,\N,Missing
saetre-etal-2008-connecting,P05-1011,1,\N,Missing
saetre-etal-2008-connecting,nenadic-etal-2006-towards,1,\N,Missing
tsunakawa-etal-2008-building-bilingual,H05-1059,1,\N,Missing
tsunakawa-etal-2008-building-bilingual,W02-2026,0,\N,Missing
tsunakawa-etal-2008-building-bilingual,C94-1048,0,\N,Missing
tsunakawa-etal-2008-building-bilingual,J90-2002,0,\N,Missing
tsunakawa-etal-2008-building-bilingual,2001.mtsummit-papers.10,0,\N,Missing
tsunakawa-etal-2008-building-bilingual,P07-2055,0,\N,Missing
tsunakawa-etal-2008-building-bilingual,P07-1108,0,\N,Missing
tsunakawa-etal-2008-building-bilingual,I05-1059,0,\N,Missing
tsunakawa-etal-2008-building-bilingual,N07-1061,0,\N,Missing
tsunakawa-etal-2008-building-bilingual,J03-1002,0,\N,Missing
W11-0208,H92-1045,0,0.0635838,"Missing"
W11-0208,P10-1029,0,0.0154374,"values are larger than a threshold (θ). In this way, a self-training algorithm obtains a huge amount of training data. 6. Add the merged data (Unew ) to the training data (Ti ). 2. Annotate recognized expressions as NEs. 69 In this study, we prepared seed data of 683,000 tokens (T0 in Figure 6). In each step, 227,000 tokens were sampled from the remaining set (U ). Because the remaining set U has high precision and low recall, we need not revise NEs that were annotated in Section 2.3. It might lower the quality of the training data to merge annotated entities, thus we used confidence values (Huang and Riloff, 2010) to revise annotations. Therefore, we retain the NE annotations of the remaining set U and overwrite a span of a non-NE annotation only if the current model predicts the span as an NE with high confidence. We compute the confidence of the prediction (f (x)) which a token x is predicted as label y as, f (x) = s(x, y) − max(∀z6=y s(x, z)). Here, s(x, y) denotes the score (the sum of feature weights) computed using the SVM model described in the beginning of Section 2. A confidence score presents the difference of scores between the predicted (the best) label and the second-best label. The confid"
W11-0208,C02-1054,0,0.0477503,"e biomedical field. A na´ıve approach to NER handles the task as a dictionary-matching problem: Prepare a dictionary (gazetteer) containing textual expressions of named entities of specific semantic types. Scan an input text, and recognize a text span as a named entity if the dictionary includes the expression of the span. Although this approach seemingly works well, it presents some critical issues. First, the dictionary Nadeau and Sekine (2007) reported that a strong trend exists recently in applying machine learning (ML) techniques such as Support Vector Machine (SVM) (Kazama et al., 2002; Isozaki and Kazawa, 2002) and Conditional Random Field (CRF) (Settles, 2004) to NER, which can address these issues. In this approach, NER is formalized as a classification problem in which a given expression is classified into a semantic class or other (non-NE) expressions. Because the classification problem is usually modeled using supervised learning methods, we need a manually annotated corpus for training NER classifier. However, preparing manually annotated corpus for a target domain of text and semantic types is cost-intensive and time-consuming because human experts are needed to reliably annotate NEs in text."
W11-0208,W02-0301,1,0.731641,"), and diseases in the biomedical field. A na´ıve approach to NER handles the task as a dictionary-matching problem: Prepare a dictionary (gazetteer) containing textual expressions of named entities of specific semantic types. Scan an input text, and recognize a text span as a named entity if the dictionary includes the expression of the span. Although this approach seemingly works well, it presents some critical issues. First, the dictionary Nadeau and Sekine (2007) reported that a strong trend exists recently in applying machine learning (ML) techniques such as Support Vector Machine (SVM) (Kazama et al., 2002; Isozaki and Kazawa, 2002) and Conditional Random Field (CRF) (Settles, 2004) to NER, which can address these issues. In this approach, NER is formalized as a classification problem in which a given expression is classified into a semantic class or other (non-NE) expressions. Because the classification problem is usually modeled using supervised learning methods, we need a manually annotated corpus for training NER classifier. However, preparing manually annotated corpus for a target domain of text and semantic types is cost-intensive and time-consuming because human experts are needed to rel"
W11-0208,W09-1313,1,0.82982,"onary in Entrez Gene. The training data consisted of nine hundred million tokens. We constructed a NER classifier using only four million tokens of the training data because of memory limitations. For evaluation, we used the Epigenetics and Post-translational Modification (EPI) corpus BioNLP 2011 Shared Task (SIGBioMed, 2011). Only development data and training data are released as the EPI corpus at present, we used both of the data sets for evaluation in this experiment. Named entities in the corpus are annotated exhaustively and belong to a single semantic class, Gene or Gene Product (GGP) (Ohta et al., 2009). We evaluated the performance of the 67 Method dictionary matching trained on acquired data A 92.09 85.76 P 39.03 10.18 R 42.69 23.83 F1 40.78 14.27 Table 2: Results of the preliminary experiment. (a) It is clear that in culture media of AM, cystatin C and cathepsin B are present as proteinase–antiproteinase complexes. (b) Temperature in the puerperium is higher in AM, and lower in PM. Figure 2: Dictionary-based gene name annotating example (annotated words are shown in italic typeface). NER on four measures: Accuracy (a), Precision (P), Recall (R), and F1-measure (F1). We used the strict mat"
W11-0208,W09-1119,0,0.0473033,"Our process to construct a NER classifier is as follows: We apply the GENIA tagger (Tsuruoka et al., 2005) to split the training data into tokens and to attach part of speech (POS) tags and chunk tags. In this work, tokenization is performed by an external program that separates tokens by a space, hyphen, comma, period, semicolon, or colon character. Part of speech tags present grammatical roles of tokens, e.g. verbs, nouns, and prepositions. Chunk tags compose tokens into syntactically correlated segments, e.g. verb phrases, noun phrases, and prepositional phrases. We use the IOBES notation (Ratinov and Roth, 2009) to represent NE mentions with label sequences, thereby NER is formalized as a multiclass classification problem in which a given token is classified into IOBES labels. To classify labels of tokens, we use a linear kernel SVM which applies the one-vs.-the-rest method (Weston and Watkins, 1999) to extend binary classification to multi-class classification. Given the t-th token xt in a sentence, we predict the label yt , yt = argmax s(y|xt , yt−1 ). y In this equation, s(y|xt , yt−1 ) presents the score (sum of feature weights) when the token xt is labeled y. We use yt−1 (the label of the previo"
W11-0208,W04-1221,0,0.0481332,"as a dictionary-matching problem: Prepare a dictionary (gazetteer) containing textual expressions of named entities of specific semantic types. Scan an input text, and recognize a text span as a named entity if the dictionary includes the expression of the span. Although this approach seemingly works well, it presents some critical issues. First, the dictionary Nadeau and Sekine (2007) reported that a strong trend exists recently in applying machine learning (ML) techniques such as Support Vector Machine (SVM) (Kazama et al., 2002; Isozaki and Kazawa, 2002) and Conditional Random Field (CRF) (Settles, 2004) to NER, which can address these issues. In this approach, NER is formalized as a classification problem in which a given expression is classified into a semantic class or other (non-NE) expressions. Because the classification problem is usually modeled using supervised learning methods, we need a manually annotated corpus for training NER classifier. However, preparing manually annotated corpus for a target domain of text and semantic types is cost-intensive and time-consuming because human experts are needed to reliably annotate NEs in text. For this reason, manually annotated corpora for NE"
W11-0208,W06-3328,0,0.0306408,"9 shows the added distribution of entity length (Added) differs from the original one (Original). Results of this analysis show that selftraining mainly annotates entities of the length one and barely recognizes entities of the length two or more. It might be necessary to devise a means to follow the corpus statistics of the ratio among the number of entities of different length as the self-training iteration proceeds. 4 Related Work Our study focuses mainly on achieving high performance NER without manual annotation. Several previous studies aimed at reducing the cost of manual annotations. Vlachos and Gasperin (2006) obtained noisy training data from FlyBase1 with few manually annotated abstracts from FlyBase. This study suggested the possibility of acquiring high-quality training data from noisy training data. It used a bootstrapping method and a highly context-based classifiers to increase the number of NE mentions in the training data. Even though the method achieved a high-performance NER in the biomedical domain, it requires curated seed data. Whitelaw et al. (2008) attempted to create extremely huge training data from the Web using a seed set of entities and relations. In generating training data au"
W11-0208,I08-5007,0,0.017219,"chieving high performance NER without manual annotation. Several previous studies aimed at reducing the cost of manual annotations. Vlachos and Gasperin (2006) obtained noisy training data from FlyBase1 with few manually annotated abstracts from FlyBase. This study suggested the possibility of acquiring high-quality training data from noisy training data. It used a bootstrapping method and a highly context-based classifiers to increase the number of NE mentions in the training data. Even though the method achieved a high-performance NER in the biomedical domain, it requires curated seed data. Whitelaw et al. (2008) attempted to create extremely huge training data from the Web using a seed set of entities and relations. In generating training data automatically, this study used context-based tagging. They reported that quite a few good resources (e.g., Wikipedia2 ) listed entities for obtaining training data automatically. 1 2 http://flybase.org/ http://www.wikipedia.org/ 72 This paper described an approach to the acquisition of huge amounts of training data for highperformance Bio NER automatically from a lexical database and unlabeled text. The results demonstrated that the proposed method outperformed"
W11-0208,P10-3016,0,0.0227351,"e larger than θ Ti+1 ← Ti ∪ Unew D ← DU i←i+1 end while Output Tn end Figure 5: Coordination analysis algorithm. 2.4 Self-training The method described in Section 2.3 reduces false negatives based on coordination structures. However, the training data contain numerous false negatives that cannot be solved through coordination analysis. Therefore, we used a self-training algorithm to automatically correct the training data. In general, a self-training algorithm obtains training data with a small amount of annotated data (seed) and a vast amount of unlabeled text, iterating this process (Zadeh Kaljahi, 2010): 1. Construct a classification model from a seed, then apply the model on the unlabeled text. Figure 6: Self-training algorithm. In contrast, our case is that we have a large amount of training data with numerous false negatives. Therefore, we adapt a self-training algorithm to revise the training data obtained using the method described in Section 2.3. Figure 6 shows the algorithm. We split the data set (D) obtained in Section 2.3 into a seed set (T0 ) and remaining set (DT0 ). Then, we iterate the cycle (0 ≤ i ≤ n): 1. Construct a classification model (Mi ) trained on the training data (Ti"
W13-4505,D11-1147,0,0.100748,"Missing"
W14-4910,D11-1145,0,0.0280298,"exist, we conduct a corpus study to identify such relations on Twitter, a popular microblogging service. We create annotation guidelines, conduct a large-scale annotation phase, and develop a corpus of annotated evidence relations. Finally, we report our observations, annotation difficulties, and data statistics. 1 Introduction Microblogs have become a popular method for users to express their ideas and communicate with other users. Twitter1 , a popular microblogging service, has recently been the attraction of many natural language processing (NLP) tasks ranging from flu epidemic detection (Aramaki et al., 2011) to gender inference for its users (Ciot et al., 2013). While various tasks are available, despite its daily, rapid largescale data, evidence relation studies have yet to be explored using Twitter data. Previous research exists for determining the credibility of information on Twitter (Castillo et al., 2011); however, the focus of this work is to determine and annotate evidence relations on microblogs. Our primary motivation behind focusing on evidence relations includes the possibility of discovering support for a claim which can support the debunking of false information. During the March 20"
W14-4910,D13-1114,0,0.0140061,"ons on Twitter, a popular microblogging service. We create annotation guidelines, conduct a large-scale annotation phase, and develop a corpus of annotated evidence relations. Finally, we report our observations, annotation difficulties, and data statistics. 1 Introduction Microblogs have become a popular method for users to express their ideas and communicate with other users. Twitter1 , a popular microblogging service, has recently been the attraction of many natural language processing (NLP) tasks ranging from flu epidemic detection (Aramaki et al., 2011) to gender inference for its users (Ciot et al., 2013). While various tasks are available, despite its daily, rapid largescale data, evidence relation studies have yet to be explored using Twitter data. Previous research exists for determining the credibility of information on Twitter (Castillo et al., 2011); however, the focus of this work is to determine and annotate evidence relations on microblogs. Our primary motivation behind focusing on evidence relations includes the possibility of discovering support for a claim which can support the debunking of false information. During the March 2011 Great East Japan Earthquake and Tsunami disaster, v"
W14-4910,P08-1051,0,0.367227,"h identification of potentially false information is necessary in order to provide accurate information to victims and others relying on and trusting in the Internet. Therefore, as a start to find support for counterclaims for false information such as the Cosmo Oil explosion, we focus on dialogue between two individuals: a topic starter, or a post with no parent; and a respondent who provides either an agreeing or disagreeing claim and support for their claim. An example is provided in Figure 1. We note that our task can appear similar to the field of Why-QA (Verberne, 2006; Oh et al., 2013; Mrozinski et al., 2008), which attempts to discover the answer for Why questions. Given our task of discovering agreeing or conflicting claims, and finding specific reasoning to support the claim, we end up with a Why question similar to Why is it true/not true that X, where X is the contents of the claim found in the parent post. However, we consider source mentions or hyperlinks, which can either stand alone or be contained in a statement, question, or request, as a way to answer the above question. To the best of our knowledge, no corpora for evidence relations on microblogs currently exists. In terms of argument"
W14-4910,P13-1170,0,0.182584,"Missing"
W14-4910,W13-4505,1,0.830061,"ehind focusing on evidence relations includes the possibility of discovering support for a claim which can support the debunking of false information. During the March 2011 Great East Japan Earthquake and Tsunami disaster, victims turned to the Internet in order to obtain information on current conditions, such as family member whereabouts, refuge center information, and general information (Sakaki et al., 2011). However, false information, such as the popular Cosmo Oil explosion causing toxic rain, interfered with those looking to find correct information on the status of the disaster areas (Okazaki et al., 2013). This is a scenario in which identification of potentially false information is necessary in order to provide accurate information to victims and others relying on and trusting in the Internet. Therefore, as a start to find support for counterclaims for false information such as the Cosmo Oil explosion, we focus on dialogue between two individuals: a topic starter, or a post with no parent; and a respondent who provides either an agreeing or disagreeing claim and support for their claim. An example is provided in Figure 1. We note that our task can appear similar to the field of Why-QA (Verbe"
W14-4910,prasad-etal-2008-penn,0,0.148516,"Missing"
W14-4910,E06-3005,0,0.352283,"2013). This is a scenario in which identification of potentially false information is necessary in order to provide accurate information to victims and others relying on and trusting in the Internet. Therefore, as a start to find support for counterclaims for false information such as the Cosmo Oil explosion, we focus on dialogue between two individuals: a topic starter, or a post with no parent; and a respondent who provides either an agreeing or disagreeing claim and support for their claim. An example is provided in Figure 1. We note that our task can appear similar to the field of Why-QA (Verberne, 2006; Oh et al., 2013; Mrozinski et al., 2008), which attempts to discover the answer for Why questions. Given our task of discovering agreeing or conflicting claims, and finding specific reasoning to support the claim, we end up with a Why question similar to Why is it true/not true that X, where X is the contents of the claim found in the parent post. However, we consider source mentions or hyperlinks, which can either stand alone or be contained in a statement, question, or request, as a way to answer the above question. To the best of our knowledge, no corpora for evidence relations on microbl"
W15-0507,baccianella-etal-2010-sentiwordnet,0,0.0165839,"[9] in order to acquire both dependency, named entity, and coreference resolution features. In the case of coreference resolution, in order to reduce parsing time, the search distance was restricted to the previous two sentences. At this time, we limit our extraction on a simple noun subject/direct objects opposed to passive sentences (e.g. cancer is caused by smoking). In future work, we will integrate more state of the art relation extraction methods for handling such cases. 4.2.1 Sentiment Polarity Calculation For calculating the sentiment of each argument’s head noun, we use SentiWordNet [2], Takamura et al. [19]’s sentiment corpus, and the Subjectivity Lexicon [22]. For each corpus, we assign a value of 1.0 if the sentiment is positive, -1.0 if negative, or otherwise neutral. We base positive and negative as a value greater than 0 and less than 0, respectively. In the case of SentiWordNet, we focus only on the top-ranked synset polarity value for each noun. Afterwards, we combine the values per noun and calculatesentiment using the following:  pos if num pos votes(w) ≥ 2  sp(w)= neg if num neg votes(w) ≤ −2 ,   neutral otherwise where w is the head noun of the direct objec"
W15-0507,W14-2107,0,0.04117,"be banned?. In this case, a passage such as Alcohol causes heart disease can be retrieved; however, the passage is not necessarily concerned with Why is heart disease negative? which can act as a link between the question and answer. In this work, in addition to a claim and it data, or evidence, we explore finding the link, or warrant, and its backing, in order to strengthen the relationship between the claim and data, one of the aspects of the Toulmin model. In terms of determining stance, previous work has utilized attack or support claims in user comments as a method for determining stance [3]. Inspired by Hashimoto et al. [6]’s excitatory and inhibitory templates, in this work, we similarly compose a manual list of P ROMOTE(X,Y) and S UPPRESS(X,Y) relations and rely on these relations, coupled with positive and negative sentiment values, as a means to signify stance. Simultaneously, not only does this assist with stance, but it is an important feature for argument construction in our first round of constructing automatic Toulmin instantiations. Finally, we generate arguments spanned across multiple documents using the P ROMOTE(X,Y) and S UPPRESS(X,Y) relations. Previous work such"
W15-0507,D11-1142,0,0.0772016,"Missing"
W15-0507,P11-1099,0,0.103629,"ction. In Section 5, we experiment with constructing Toulmin instantiations for a given claim and report our findings. In Section 6, we discuss our results. Finally, in Section 7, we conclude our work and describe our future work. 2 Related Work To the best of our knowledge, no prior work has developed a computation model for automatically constructing Toulmin instantiations. However, various components of the Toulmin model have individually been researched and are discussed below. The most similar work to ours is the automatic detection of enthymemes using Walton [21]’s argumentation schemes [5]. Similarly, we aim to discover enthymemes in the Toulmin model explicitly through computational modeling in order to assist with generating constructive debate speeches. In fu46 ture work, we plan to adopt different, less general argumentation theories. Given a motion-like topic, previous work has found relevant claims to support the topic [8]. Other work has utilized a list of controversial topics in order to find relevant claim and evidence segments utilizing discourse markers [17]. Previous Why-QA work [20, 15, 13] has dealt with finding answers for questions such as Why should alcohol be"
W15-0507,D12-1057,0,0.18995,"e such as Alcohol causes heart disease can be retrieved; however, the passage is not necessarily concerned with Why is heart disease negative? which can act as a link between the question and answer. In this work, in addition to a claim and it data, or evidence, we explore finding the link, or warrant, and its backing, in order to strengthen the relationship between the claim and data, one of the aspects of the Toulmin model. In terms of determining stance, previous work has utilized attack or support claims in user comments as a method for determining stance [3]. Inspired by Hashimoto et al. [6]’s excitatory and inhibitory templates, in this work, we similarly compose a manual list of P ROMOTE(X,Y) and S UPPRESS(X,Y) relations and rely on these relations, coupled with positive and negative sentiment values, as a means to signify stance. Simultaneously, not only does this assist with stance, but it is an important feature for argument construction in our first round of constructing automatic Toulmin instantiations. Finally, we generate arguments spanned across multiple documents using the P ROMOTE(X,Y) and S UPPRESS(X,Y) relations. Previous work such as Cross Document Structure theory"
W15-0507,P14-5010,0,0.00785184,"serving as the surface object of R. In our experiment, we used a collection of web pages extracted from ClueWeb12 as a source corpus of knowledge base construction. ClueWeb121 consists of roughly 733 million Web documents ranging from blogs to news articles. All web pages containing less than 30 words were filtered out which resulted in 222 million total web pages. From these web pages, we extract 22,973,104 relations using a manually composed list of 40 P ROMOTE (e.g. increase, cause, raise) and 76 S UPPRESS (e.g. harm, kill, prevent) predicates. We parse each document using Stanford CoreNLP [9] in order to acquire both dependency, named entity, and coreference resolution features. In the case of coreference resolution, in order to reduce parsing time, the search distance was restricted to the previous two sentences. At this time, we limit our extraction on a simple noun subject/direct objects opposed to passive sentences (e.g. cancer is caused by smoking). In future work, we will integrate more state of the art relation extraction methods for handling such cases. 4.2.1 Sentiment Polarity Calculation For calculating the sentiment of each argument’s head noun, we use SentiWordNet [2],"
W15-0507,D12-1048,0,0.0493718,"Missing"
W15-0507,P08-1051,0,0.0816909,"ns, namely P ROMOTE or S UPPRESS in this paper. By utilizing these relations, our task is reduced to finding relation tuples that can satisfy the definitions. We use our evaluation results as a basis of justification as to whether or not the these relation tuples are sufficient for argumentation construction. To ensure the coherency of overall argumentation, we find contextually similar relations. In future work, we plan to apply state-of-the-art technologies from discourse relation recognition and QAs for generating each Toulmin component, where a significant amount of research has been done [20, 15, 13, 8, 17]. The rest of the paper is as follows. We first describe related work in Section 2 and an overview of the Toulmin model in Section 3. In Section 4, we describe our methodology for generating patterns for Toulmin construction. In Section 5, we experiment with constructing Toulmin instantiations for a given claim and report our findings. In Section 6, we discuss our results. Finally, in Section 7, we conclude our work and describe our future work. 2 Related Work To the best of our knowledge, no prior work has developed a computation model for automatically constructing Toulmin instantiations. Ho"
W15-0507,P13-1170,0,0.0836432,"ns, namely P ROMOTE or S UPPRESS in this paper. By utilizing these relations, our task is reduced to finding relation tuples that can satisfy the definitions. We use our evaluation results as a basis of justification as to whether or not the these relation tuples are sufficient for argumentation construction. To ensure the coherency of overall argumentation, we find contextually similar relations. In future work, we plan to apply state-of-the-art technologies from discourse relation recognition and QAs for generating each Toulmin component, where a significant amount of research has been done [20, 15, 13, 8, 17]. The rest of the paper is as follows. We first describe related work in Section 2 and an overview of the Toulmin model in Section 3. In Section 4, we describe our methodology for generating patterns for Toulmin construction. In Section 5, we experiment with constructing Toulmin instantiations for a given claim and report our findings. In Section 6, we discuss our results. Finally, in Section 7, we conclude our work and describe our future work. 2 Related Work To the best of our knowledge, no prior work has developed a computation model for automatically constructing Toulmin instantiations. Ho"
W15-0507,W00-1009,0,0.0392236,"s excitatory and inhibitory templates, in this work, we similarly compose a manual list of P ROMOTE(X,Y) and S UPPRESS(X,Y) relations and rely on these relations, coupled with positive and negative sentiment values, as a means to signify stance. Simultaneously, not only does this assist with stance, but it is an important feature for argument construction in our first round of constructing automatic Toulmin instantiations. Finally, we generate arguments spanned across multiple documents using the P ROMOTE(X,Y) and S UPPRESS(X,Y) relations. Previous work such as Cross Document Structure theory [16] has organized information from multiple documents via relations. Furthermore, the Statement Map [14] project, for a given query, has detected agreeing and conflicting support which are spanned across multiple documents. In this work, we attempt to construct an implicit Warrant and generate its Backing for a Claim (query) and its Data (support). 3 Toulmin Model Toulmin was the first to believe that most arguments could simply be modeled using the following six components: claim, data, warrant, backing, qualifier, and rebuttal [18]. This model is referred to as the Toulmin model and is shown in"
W15-0507,W14-4910,1,0.886531,"ns, namely P ROMOTE or S UPPRESS in this paper. By utilizing these relations, our task is reduced to finding relation tuples that can satisfy the definitions. We use our evaluation results as a basis of justification as to whether or not the these relation tuples are sufficient for argumentation construction. To ensure the coherency of overall argumentation, we find contextually similar relations. In future work, we plan to apply state-of-the-art technologies from discourse relation recognition and QAs for generating each Toulmin component, where a significant amount of research has been done [20, 15, 13, 8, 17]. The rest of the paper is as follows. We first describe related work in Section 2 and an overview of the Toulmin model in Section 3. In Section 4, we describe our methodology for generating patterns for Toulmin construction. In Section 5, we experiment with constructing Toulmin instantiations for a given claim and report our findings. In Section 6, we discuss our results. Finally, in Section 7, we conclude our work and describe our future work. 2 Related Work To the best of our knowledge, no prior work has developed a computation model for automatically constructing Toulmin instantiations. Ho"
W15-0507,P05-1017,0,0.0261901,"ire both dependency, named entity, and coreference resolution features. In the case of coreference resolution, in order to reduce parsing time, the search distance was restricted to the previous two sentences. At this time, we limit our extraction on a simple noun subject/direct objects opposed to passive sentences (e.g. cancer is caused by smoking). In future work, we will integrate more state of the art relation extraction methods for handling such cases. 4.2.1 Sentiment Polarity Calculation For calculating the sentiment of each argument’s head noun, we use SentiWordNet [2], Takamura et al. [19]’s sentiment corpus, and the Subjectivity Lexicon [22]. For each corpus, we assign a value of 1.0 if the sentiment is positive, -1.0 if negative, or otherwise neutral. We base positive and negative as a value greater than 0 and less than 0, respectively. In the case of SentiWordNet, we focus only on the top-ranked synset polarity value for each noun. Afterwards, we combine the values per noun and calculatesentiment using the following:  pos if num pos votes(w) ≥ 2  sp(w)= neg if num neg votes(w) ≤ −2 ,   neutral otherwise where w is the head noun of the direct object in each P ROMOTE and"
W15-0507,E06-3005,0,0.0995363,"ns, namely P ROMOTE or S UPPRESS in this paper. By utilizing these relations, our task is reduced to finding relation tuples that can satisfy the definitions. We use our evaluation results as a basis of justification as to whether or not the these relation tuples are sufficient for argumentation construction. To ensure the coherency of overall argumentation, we find contextually similar relations. In future work, we plan to apply state-of-the-art technologies from discourse relation recognition and QAs for generating each Toulmin component, where a significant amount of research has been done [20, 15, 13, 8, 17]. The rest of the paper is as follows. We first describe related work in Section 2 and an overview of the Toulmin model in Section 3. In Section 4, we describe our methodology for generating patterns for Toulmin construction. In Section 5, we experiment with constructing Toulmin instantiations for a given claim and report our findings. In Section 6, we discuss our results. Finally, in Section 7, we conclude our work and describe our future work. 2 Related Work To the best of our knowledge, no prior work has developed a computation model for automatically constructing Toulmin instantiations. Ho"
W15-0507,H05-1044,0,0.0106515,"olution features. In the case of coreference resolution, in order to reduce parsing time, the search distance was restricted to the previous two sentences. At this time, we limit our extraction on a simple noun subject/direct objects opposed to passive sentences (e.g. cancer is caused by smoking). In future work, we will integrate more state of the art relation extraction methods for handling such cases. 4.2.1 Sentiment Polarity Calculation For calculating the sentiment of each argument’s head noun, we use SentiWordNet [2], Takamura et al. [19]’s sentiment corpus, and the Subjectivity Lexicon [22]. For each corpus, we assign a value of 1.0 if the sentiment is positive, -1.0 if negative, or otherwise neutral. We base positive and negative as a value greater than 0 and less than 0, respectively. In the case of SentiWordNet, we focus only on the top-ranked synset polarity value for each noun. Afterwards, we combine the values per noun and calculatesentiment using the following:  pos if num pos votes(w) ≥ 2  sp(w)= neg if num neg votes(w) ≤ −2 ,   neutral otherwise where w is the head noun of the direct object in each P ROMOTE and S UPPRESS relation. The functions num pos votes(w) an"
W15-0507,N07-4013,0,0.0384393,"Missing"
W15-1609,P14-1016,0,0.033703,"Missing"
W15-1609,I13-2008,1,0.896852,"Missing"
W15-1609,C10-1096,1,0.887204,"Missing"
W15-1609,peters-peters-2000-lexicalised,0,0.0549864,"les of geoparsing for Japanese text, GeoNLP (Kitamoto and Sagara, 2012) exist, but there are no reports of quantitative evaluations of the performance, because there is no corpus for evaluation. 3 Challenges in Annotating LREs on Microblog Text In this section, we describe the new research challenges associated with annotating geographical entities in Microblog text and our policies for addressing these issues. 3.1 Systematic Polysemy of LREs One prominent issue in annotating facility entities is the so-called systematic polysemy inherent in mentions referring to facilities (see, for example, Peters and Peters (2000)). For example, the mention “the Ministry of the Environment” in the sentence (1) below refers to a specific location while the mention “the Ministry of the Environment” in (2) should be interpreted as an organization and does not refer to the location of the organization. (1) 午 後 は 環境省 に い ま す / I’ll be at the Ministry of the Environment this afternoon. (2) こ れ か ら 環境省 の 職 員 に 会って き ま す / I will go to meet a staff member of the Ministry of the Environment. This distinction can be crucial in potential applications of annotated geographical entities. In our annotation guidelines, ambiguities of"
W15-1609,sekine-etal-2002-extended,0,0.0823175,"L:82db] で [岩手県 TYPE=LOC, EN=岩手県, ID=CB:3b4c] まで行く / I go to [Iwate prefecture TYPE=LOC, EN=岩手県, ID=CB:3b4c] from [Tokyo TYPE=FAC, EN=東京駅, ID=LC:4d3b] in [Shinkansen TYPE=FAC, EN=東北新幹線, ID=NL:82db]. sues discussed in Section 3. 4.1 Annotation In the existing named entity tagged corpora in Japanese, expressions are annotated with a named entity class and its boundaries. However, the corpora does not contain annotations as to whether each of the expressions actually relates to an entity. Partly following the annotation guidelines in TAC KBP (Ji et al., 2014)2 , the extended named entity tag set (Sekine et al., 2002) and the Japanese extended Named Entity-tagged corpus, we followed the approach illustrated in Figure 2 to annotate microblog texts. The annotation task consists of the following two subtasks: Mention Detection (MD) Given a microblog text (i.e., a tweet), an annotator annotates all the mentions which refer to specific geographic entities with a predefined set of tags given in Table 1. Entity Resolution (ER) For each detected mention, an annotator searches the gazetteer for its referred entity and annotates the linking. We allow a mention to be linked to multiple gazetteer entries. If the refer"
W15-1609,P13-1144,0,0.198684,"Missing"
W15-1609,P13-1159,0,\N,Missing
W17-4208,W04-3238,0,0.0799744,"f deletions that involve one or two words. Table 3 shows some instances of the replace operations. It may not be straightforward to use the revision logs for error correction because some edit operations add new information and remove useless information. Nevertheless, the logs record the daily activities of how drafts are improved by the editors. In future, we plan to build an editing system that detects errors and suggests wording while the reporters write drafts. We can use natural language processing techniques for these tasks because local error correction has been previously researched (Cucerzan and Brill, 2004). Splitting (S1) 新たな窓口を設けるなど内部通報制度も強化し、 通報は問題が発覚する前の 88 件 (14 年度) から 263 件 (15 年度) と 3 倍に増えた。 They enhance whistle-blowing systems by providing such as new counseling offices, and the number of whistle-blowing was increased three times from 88 in 2014, in which this issue was found out, to 263 in 2015. (S2) 新たな窓口を設けるなど内部通報制度も強化。通 報は 2015 年度に 263 件と、不正会計問題の発覚 前の 14 年度の 88 件から約 3 倍に増えたという。 They enhance whistle-blowing systems by providing such as new counseling offices. As a result, the number of whistle-blowing was increased three times from 88 in 2014, in which this issue was found out, to 263"
W17-4208,P06-1030,0,0.0440693,"ns separated by different part-of-speech. The most frequent target for revisions is nouns, followed by particles (postpositions). This result indicates that revisions in terms of both content and readability are important for improving the quality of articles. 48 Original 同政府関係者 this Government officials 放射線汚染 contamination by radial ray 破顔し broke into a smile 3.2 Establishing guidelines for writing articles Most textbooks on Japanese writing (including the internal handbook for reporters produced by the newspaper company) recommend that a Japanese sentence should be 40 to 50 characters long (Ishioka and Kameda, 2006). We could confirm that the newspaper articles satisfy this criterion: the revised sentences are 41.10 characters long on average. In this way, we can analyze the revision logs to extract various criteria for establishing the guidelines for ‘good’ articles. バラティ Varety タンパク質 Protain: written in Katakana and Kanji 買えた could buy 3.3 Automatic article revision within sentences Table 3: Examples of commonly replaced words/phrases. Another future direction is to build a corpus for improving the quality of articles. The revision logs collected for a year (excluding duplicates) provide 517,545 instan"
W17-4208,C16-1109,0,0.026073,"Missing"
W17-4208,W04-3230,0,0.0322271,"Missing"
W17-6937,P98-1013,0,0.0840769,"e as an event unit. Extending an event unit beyond a multiword expression would impose a severe data sparseness problem, which is to be addressed in our future work. 4 Related Work Previous studies proposed a wide variety of approaches to causality estimation (Do et al., 2011; Kozareva, 2012; Riaz and Girju, 2014). Do et al. (2011) employed statistical measures such as PMI and inverse document frequency (IDF) from a corpus to model causality between events. Kozareva (2012) applied a bootstrap algorithm to acquire causal event pairs. Riaz and Girju (2014) extracted training data from FrameNet (Baker et al., 1998) to learn a classifier for a causal relation. However, these studies assume that an event is representable by a single word. Chambers and Jurafsky (2008)’s Narrative Schema uses a predicate-argument structure as an event unit, but a predicate is restricted to a single word. Roemmele et al. (2011) introduced a baseline model of COPA that uses PMI between words on English documents in Project Gutenberg. Gordon et al. (2011) improved the baseline model introduced by Roemmele et al. (2011) by using personal stories extracted from Weblogs instead of Project Gutenberg. Luo et al. (2016) refined PMI"
W17-6937,P08-1090,0,0.562654,"ng the product. The task is to estimate that sentence (1a) is more causally related to sentence (1b) than non-causally related sentences such as “John opened a door.”. Causality estimation is considered as an essential component of common sense reasoning. A conventional approach to causality estimation is to construct a statistical model of causality relying on a large corpus in a semi-supervised manner. The main idea is two-fold: (i) collect causally related word pairs (e.g. typhoon-die) by exploiting the contextual proximity or discourse markers and (ii) apply them to a correlation measure (Chambers and Jurafsky, 2008; Luo et al., 2016) or a supervised classifier (Riaz and Girju, 2014; Granroth-Wilding and Clark, 2016). A key limitation of the previous studies is that they model causality in terms of word pairs, not taking into account the causality represented by multiword expressions. For example, in example (1), a causality estimation model is expected to consider the causality between tired and gave up (i.e. stop something). However, the previous models consider only word pairs; therefore, it would improperly estimate the causality based on word pairs such as tired-give and tired-up. Because each indiv"
W17-6937,D11-1027,0,0.0257836,"ssions. It reveals that proper causality estimation often requires the system to expand an event unit to another word as well as to recognize a multiword expression; for instance, when the causality between “The stain came out of the shirt.” and “I bleached the shirt” is estimated, stain come out, rather than come out, is more appropriate as an event unit. Extending an event unit beyond a multiword expression would impose a severe data sparseness problem, which is to be addressed in our future work. 4 Related Work Previous studies proposed a wide variety of approaches to causality estimation (Do et al., 2011; Kozareva, 2012; Riaz and Girju, 2014). Do et al. (2011) employed statistical measures such as PMI and inverse document frequency (IDF) from a corpus to model causality between events. Kozareva (2012) applied a bootstrap algorithm to acquire causal event pairs. Riaz and Girju (2014) extracted training data from FrameNet (Baker et al., 1998) to learn a classifier for a causal relation. However, these studies assume that an event is representable by a single word. Chambers and Jurafsky (2008)’s Narrative Schema uses a predicate-argument structure as an event unit, but a predicate is restricted"
W17-6937,W12-4107,0,0.0124448,"s that proper causality estimation often requires the system to expand an event unit to another word as well as to recognize a multiword expression; for instance, when the causality between “The stain came out of the shirt.” and “I bleached the shirt” is estimated, stain come out, rather than come out, is more appropriate as an event unit. Extending an event unit beyond a multiword expression would impose a severe data sparseness problem, which is to be addressed in our future work. 4 Related Work Previous studies proposed a wide variety of approaches to causality estimation (Do et al., 2011; Kozareva, 2012; Riaz and Girju, 2014). Do et al. (2011) employed statistical measures such as PMI and inverse document frequency (IDF) from a corpus to model causality between events. Kozareva (2012) applied a bootstrap algorithm to acquire causal event pairs. Riaz and Girju (2014) extracted training data from FrameNet (Baker et al., 1998) to learn a classifier for a causal relation. However, these studies assume that an event is representable by a single word. Chambers and Jurafsky (2008)’s Narrative Schema uses a predicate-argument structure as an event unit, but a predicate is restricted to a single word"
W17-6937,P14-5010,0,0.00494728,"Missing"
W17-6937,W14-0707,0,0.0720111,"related to sentence (1b) than non-causally related sentences such as “John opened a door.”. Causality estimation is considered as an essential component of common sense reasoning. A conventional approach to causality estimation is to construct a statistical model of causality relying on a large corpus in a semi-supervised manner. The main idea is two-fold: (i) collect causally related word pairs (e.g. typhoon-die) by exploiting the contextual proximity or discourse markers and (ii) apply them to a correlation measure (Chambers and Jurafsky, 2008; Luo et al., 2016) or a supervised classifier (Riaz and Girju, 2014; Granroth-Wilding and Clark, 2016). A key limitation of the previous studies is that they model causality in terms of word pairs, not taking into account the causality represented by multiword expressions. For example, in example (1), a causality estimation model is expected to consider the causality between tired and gave up (i.e. stop something). However, the previous models consider only word pairs; therefore, it would improperly estimate the causality based on word pairs such as tired-give and tired-up. Because each individual word in multiword expressions might have a completely differen"
W17-6937,W93-0310,0,0.314862,"n.wiktionary.org/ During the extraction, all words are lemmatized and lowercased. 3 https://en.wiktionary.org/wiki/Wiktionary:Main_Page 2 and the sufficient factor. Formally, for a causal event ic and a effect event je , the two factors are defined by the following equations: CSnec (ic , je ) = p(ic , ie ) p(ic |je ) = α , α p (ic ) p (ic )p(je ) (1) CSsuf (ic , je ) = p(je |ic ) p(ic , ie ) = , α p (je ) p(ic )pα (je ) (2) where CSnec (ic , je ) is the necessary factor, CSsuf (ic , je ) is the sufficient factor, and α is a hyperparameter. We set α = 0.66, the same value as Luo et al. (2016); Wettler and Rapp (1993). By using CSnec (ic , je ) and CSsuf (ic , je ), Causal Strength is defined as, CS(ic , je ) = CSnec (ic , je )λ CSsuf (ic , je )1−λ , (3) where λ is a hyper-parameter. 3 Experiment To examine the necessity of proper treatment of multiword expressions, we compare the proposed method against existing causality estimation models, and conduct an ablation study. 3.1 Dataset To extract causal event pairs, we used the ClueWeb124 , a large-scale corpus consisting of 700 million documents crawled from the Web. We evaluated the proposed method on the task of Choice of Plausible Alternatives (COPA) (Ro"
W17-6937,C98-1013,0,\N,Missing
W18-5410,D14-1179,0,0.0293295,"Missing"
W18-5410,P17-1106,0,0.0248262,"et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) has been an epoch-making development that has led to great progress in many natural language generation tasks, such as machine translation (Bahdanau et al., 2015), dialog generation (Shang et al., 2015), and headline generation (Rush et al., 2015). An enormous number of studies have attempted to enhance the ability of EncDec. Furthermore, several intensive studies have also attempted to analyze and interpret the inside of black-box EncDec models, especially how they translate a given source sentence to the corresponding target sentence (Ding et al., 2017). One typical approach to this is to visualize an attention matrix, which is a collection of attention vectors (Bahdanau et al., 2015; Luong et al., 2015; Tu et al., 2016). ∗ This work is a product of collaborative research program of Tohoku University and NTT Communication Science Laboratories. 1 Our code for reproducing the experiments is available at https://github.com/butsugiri/UAM 74 Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 74–81 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics 3.1 sp"
W18-5410,I17-1004,0,0.0451013,"EN Center for Advanced Intelligence Project {kiyono,jun.suzuki,inui}@ecei.tohoku.ac.jp, {takase.sho, nagata.masaaki}@lab.ntt.co.jp, okazaki@c.titech.ac.jp Abstract The assumption behind this interpretation is that the attention matrix has a “soft” token-wise alignment between the source and target sequences, and thus we can use EncDec models to skim which tokens in the source are converted to which tokens in the target. However, recent studies have empirically revealed that an attention model can operate not only for token-wise alignment but also for other functionalities, such as reordering (Ghader and Monz, 2017; Liu et al., 2016). In addition, Luong et al. (2015) reported that the quality of attention matrixbased alignment is quantitatively inferior to that of the Berkeley aligner (Liang et al., 2006). Koehn and Knowles (2017) also reported that attention matrix-based alignment is significantly different from that acquired from an off-the-shelf aligner for English-German language pairs. From these recent findings, the goal of this paper is to provide a method that can offer a better interpretation of how EncDec models translate a given source sentence to the corresponding target sentence. In this pa"
W18-5410,P15-1152,0,0.0223308,"se a method that explicitly models the token-wise alignment between the source and target sequences to provide a better analysis. Experiments show that our method can acquire token-wise alignments that are superior to those of an attention mechanism1 . 1 Introduction The Encoder-Decoder model with an attention mechanism (EncDec) (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) has been an epoch-making development that has led to great progress in many natural language generation tasks, such as machine translation (Bahdanau et al., 2015), dialog generation (Shang et al., 2015), and headline generation (Rush et al., 2015). An enormous number of studies have attempted to enhance the ability of EncDec. Furthermore, several intensive studies have also attempted to analyze and interpret the inside of black-box EncDec models, especially how they translate a given source sentence to the corresponding target sentence (Ding et al., 2017). One typical approach to this is to visualize an attention matrix, which is a collection of attention vectors (Bahdanau et al., 2015; Luong et al., 2015; Tu et al., 2016). ∗ This work is a product of collaborative research program of Tohoku"
W18-5410,D17-1227,0,0.0166013,"ns q˜ with the sum of the source-side ˜ as an objective function `src . tokens x Since oj for each j is a vector representation of the ˆ 0:j−1 , X, θ) over the target probabilities of p(y|y vocabularies yˆ ∈ Vt , we can calculate `trg as (8) `trg (Y , X, θ) = − J+1 X  yj> · log oj . (12) j=1 3.3 Inference of EncDec In the inference step, we use the trained parameters to search for the best target sequence. We use beam search to find the target sequence that maximizes the product of the conditional probabilities as described in Equation 1. From among several stopping criteria for beam search (Huang et al., 2017), we adopt the widely used “shrinking beam” implemented in RNNsearch (Bahdanau et al., 2015)3 . (9) where Ws ∈ RH×2H is a parameter matrix. Finally, zj is fed into the softmax layer. The model generates a target-side token based on the probability distribution oj ∈ RVt as (10) where Wo ∈ RVt ×H is a parameter matrix and bo ∈ RVt is a bias term. 3.2 & ? + (?"":$ ) Next, the source-side information is mixed with the decoder hidden state to derive final hidden state zj . Concretely, the context vector cj is concate~j to form vector uj ∈ R2H . uj is then nated with z fed into a single fully-connect"
W18-5410,E17-2047,1,0.939025,"ecent findings, the goal of this paper is to provide a method that can offer a better interpretation of how EncDec models translate a given source sentence to the corresponding target sentence. In this paper, we focus exclusively on the headline generation task, which is categorized as a lossycompression generation (lossy-gen) task (Nallapati et al., 2016). Compared with a machine translation task, which is categorized as a loss-less generation (lossless-gen) task, the headline generation task additionally requires EncDec models to appropriately select salient ideas in given source sentences (Suzuki and Nagata, 2017). Therefore, the lossy-gen task seems to make modeling by EncDec much harder. In fact, our preliminary experiments revealed that the attention mechanism in EncDec models largely fails to capture token-wise alignments, e.g., less than 10 percent accuracy, even if we use one of the current state-of-the-art EncDec models (Table 3). To obtain a better analysis of how EncDec models translate a given source sentence to the correDeveloping a method for understanding the inner workings of black-box neural methods is an important research endeavor. Conventionally, many studies have used an attention ma"
W18-5410,W17-3204,0,0.0232542,"attention matrix has a “soft” token-wise alignment between the source and target sequences, and thus we can use EncDec models to skim which tokens in the source are converted to which tokens in the target. However, recent studies have empirically revealed that an attention model can operate not only for token-wise alignment but also for other functionalities, such as reordering (Ghader and Monz, 2017; Liu et al., 2016). In addition, Luong et al. (2015) reported that the quality of attention matrixbased alignment is quantitatively inferior to that of the Berkeley aligner (Liang et al., 2006). Koehn and Knowles (2017) also reported that attention matrix-based alignment is significantly different from that acquired from an off-the-shelf aligner for English-German language pairs. From these recent findings, the goal of this paper is to provide a method that can offer a better interpretation of how EncDec models translate a given source sentence to the corresponding target sentence. In this paper, we focus exclusively on the headline generation task, which is categorized as a lossycompression generation (lossy-gen) task (Nallapati et al., 2016). Compared with a machine translation task, which is categorized a"
W18-5410,N06-1014,0,0.0899348,"pretation is that the attention matrix has a “soft” token-wise alignment between the source and target sequences, and thus we can use EncDec models to skim which tokens in the source are converted to which tokens in the target. However, recent studies have empirically revealed that an attention model can operate not only for token-wise alignment but also for other functionalities, such as reordering (Ghader and Monz, 2017; Liu et al., 2016). In addition, Luong et al. (2015) reported that the quality of attention matrixbased alignment is quantitatively inferior to that of the Berkeley aligner (Liang et al., 2006). Koehn and Knowles (2017) also reported that attention matrix-based alignment is significantly different from that acquired from an off-the-shelf aligner for English-German language pairs. From these recent findings, the goal of this paper is to provide a method that can offer a better interpretation of how EncDec models translate a given source sentence to the corresponding target sentence. In this paper, we focus exclusively on the headline generation task, which is categorized as a lossycompression generation (lossy-gen) task (Nallapati et al., 2016). Compared with a machine translation ta"
W18-5410,D16-1033,0,0.017834,"hat used in Rush et al. (2015). The dataset consists of pairs of the first sentence of each article and its headline from the annotated English Gigaword corpus (Napoles et al., 2012). Rush et al. (2015) defined the training, validation and test splits, which contain approximately 3.8M, 200K and 400K source-headline pairs, respectively We used the entire training split for training as in the previous studies. We randomly sampled 8K instances as validation data and 10K instances as test data from the validation split. Moreover, we experimented on the test data provided by Zhou et al. (2017) and Toutanova et al. (2016) for comparison with the reported state-of-the-art performance (Zhou et al., 2017). We refer to those test data sets as Test (Ours), Test (Zhou), and MSRATC respectively. Among these test sets, MSRATC is the only dataset created by a human worker. 5.2 5131 5131 200 400 Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) 2-layer bidirectional-LSTM 2-layer LSTM with attention (Luong et al., 2015) Adam (Kingma and Ba, 2015) 0.001 0.5 for each epoch (after epoch 9) 10 256 (shuffled at each epoch) 5 Max 15 epochs with early stopping Dropout (rate 0.3) Beam size 20 with the length norma"
W18-5410,C16-1291,0,0.0193027,"Intelligence Project {kiyono,jun.suzuki,inui}@ecei.tohoku.ac.jp, {takase.sho, nagata.masaaki}@lab.ntt.co.jp, okazaki@c.titech.ac.jp Abstract The assumption behind this interpretation is that the attention matrix has a “soft” token-wise alignment between the source and target sequences, and thus we can use EncDec models to skim which tokens in the source are converted to which tokens in the target. However, recent studies have empirically revealed that an attention model can operate not only for token-wise alignment but also for other functionalities, such as reordering (Ghader and Monz, 2017; Liu et al., 2016). In addition, Luong et al. (2015) reported that the quality of attention matrixbased alignment is quantitatively inferior to that of the Berkeley aligner (Liang et al., 2006). Koehn and Knowles (2017) also reported that attention matrix-based alignment is significantly different from that acquired from an off-the-shelf aligner for English-German language pairs. From these recent findings, the goal of this paper is to provide a method that can offer a better interpretation of how EncDec models translate a given source sentence to the corresponding target sentence. In this paper, we focus exclu"
W18-5410,P16-1008,0,0.10152,"as machine translation (Bahdanau et al., 2015), dialog generation (Shang et al., 2015), and headline generation (Rush et al., 2015). An enormous number of studies have attempted to enhance the ability of EncDec. Furthermore, several intensive studies have also attempted to analyze and interpret the inside of black-box EncDec models, especially how they translate a given source sentence to the corresponding target sentence (Ding et al., 2017). One typical approach to this is to visualize an attention matrix, which is a collection of attention vectors (Bahdanau et al., 2015; Luong et al., 2015; Tu et al., 2016). ∗ This work is a product of collaborative research program of Tohoku University and NTT Communication Science Laboratories. 1 Our code for reproducing the experiments is available at https://github.com/butsugiri/UAM 74 Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 74–81 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics 3.1 sponding target sentence in the headline generation task, this paper introduces the Unsupervised tokenwise Alignment Module (UAM), a novel component that can be plugged into"
W18-5410,D15-1166,0,0.627426,".suzuki,inui}@ecei.tohoku.ac.jp, {takase.sho, nagata.masaaki}@lab.ntt.co.jp, okazaki@c.titech.ac.jp Abstract The assumption behind this interpretation is that the attention matrix has a “soft” token-wise alignment between the source and target sequences, and thus we can use EncDec models to skim which tokens in the source are converted to which tokens in the target. However, recent studies have empirically revealed that an attention model can operate not only for token-wise alignment but also for other functionalities, such as reordering (Ghader and Monz, 2017; Liu et al., 2016). In addition, Luong et al. (2015) reported that the quality of attention matrixbased alignment is quantitatively inferior to that of the Berkeley aligner (Liang et al., 2006). Koehn and Knowles (2017) also reported that attention matrix-based alignment is significantly different from that acquired from an off-the-shelf aligner for English-German language pairs. From these recent findings, the goal of this paper is to provide a method that can offer a better interpretation of how EncDec models translate a given source sentence to the corresponding target sentence. In this paper, we focus exclusively on the headline generation"
W18-5410,P17-1101,0,0.406026,"ments is identical to that used in Rush et al. (2015). The dataset consists of pairs of the first sentence of each article and its headline from the annotated English Gigaword corpus (Napoles et al., 2012). Rush et al. (2015) defined the training, validation and test splits, which contain approximately 3.8M, 200K and 400K source-headline pairs, respectively We used the entire training split for training as in the previous studies. We randomly sampled 8K instances as validation data and 10K instances as test data from the validation split. Moreover, we experimented on the test data provided by Zhou et al. (2017) and Toutanova et al. (2016) for comparison with the reported state-of-the-art performance (Zhou et al., 2017). We refer to those test data sets as Test (Ours), Test (Zhou), and MSRATC respectively. Among these test sets, MSRATC is the only dataset created by a human worker. 5.2 5131 5131 200 400 Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) 2-layer bidirectional-LSTM 2-layer LSTM with attention (Luong et al., 2015) Adam (Kingma and Ba, 2015) 0.001 0.5 for each epoch (after epoch 9) 10 256 (shuffled at each epoch) 5 Max 15 epochs with early stopping Dropout (rate 0.3) Beam s"
W18-5410,K16-1028,0,0.201448,"y inferior to that of the Berkeley aligner (Liang et al., 2006). Koehn and Knowles (2017) also reported that attention matrix-based alignment is significantly different from that acquired from an off-the-shelf aligner for English-German language pairs. From these recent findings, the goal of this paper is to provide a method that can offer a better interpretation of how EncDec models translate a given source sentence to the corresponding target sentence. In this paper, we focus exclusively on the headline generation task, which is categorized as a lossycompression generation (lossy-gen) task (Nallapati et al., 2016). Compared with a machine translation task, which is categorized as a loss-less generation (lossless-gen) task, the headline generation task additionally requires EncDec models to appropriately select salient ideas in given source sentences (Suzuki and Nagata, 2017). Therefore, the lossy-gen task seems to make modeling by EncDec much harder. In fact, our preliminary experiments revealed that the attention mechanism in EncDec models largely fails to capture token-wise alignments, e.g., less than 10 percent accuracy, even if we use one of the current state-of-the-art EncDec models (Table 3). To"
W18-5410,W12-3018,0,0.197412,"Missing"
W18-5410,D15-1044,0,0.46313,"ise alignment between the source and target sequences to provide a better analysis. Experiments show that our method can acquire token-wise alignments that are superior to those of an attention mechanism1 . 1 Introduction The Encoder-Decoder model with an attention mechanism (EncDec) (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) has been an epoch-making development that has led to great progress in many natural language generation tasks, such as machine translation (Bahdanau et al., 2015), dialog generation (Shang et al., 2015), and headline generation (Rush et al., 2015). An enormous number of studies have attempted to enhance the ability of EncDec. Furthermore, several intensive studies have also attempted to analyze and interpret the inside of black-box EncDec models, especially how they translate a given source sentence to the corresponding target sentence (Ding et al., 2017). One typical approach to this is to visualize an attention matrix, which is a collection of attention vectors (Bahdanau et al., 2015; Luong et al., 2015; Tu et al., 2016). ∗ This work is a product of collaborative research program of Tohoku University and NTT Communication Science Lab"
W18-5410,P16-1162,0,0.0510147,"ly for MSR-ATC and a performance comparable to EncDec+sGate in Test (Ours) and Test (Zhou). Considering that the MSR-ATC dataset was created by a human worker, we believe that the improvement in MSR-ATC is the most remarkable result among the three test sets, since it indicates that our model most closely fits the human-generated summary. Implementation Details In the experiment, we selected the hyper-parameter settings commonly used in previous studies e.g., (Rush et al., 2015; Nallapati et al., 2016; Suzuki and Nagata, 2017) We constructed the vocabulary set using byte pair encoding4 (BPE) (Sennrich et al., 2016) to handle low-frequency words, since this is now a common practice in the field of neural machine translation. The BPE merge operations are jointly learned from the source and the target. We set the number of BPE merge operations at 5, 000. 6 Discussion We investigated whether the UAM improves tokenwise alignment between the source and target se5 We restored sub-words to the standard token split for the evaluation. 6 ROUGE script option is: “-n2 -m -w 1.2” 4 https://github.com/rsennrich/ subword-nmt 78 Test (Ours) Test (Zhou) MSR-ATC Model RG-1 RG-2 RG-L RG-1 RG-2 RG-L RG-1 RG-2 RG-L EncDec+s"
W18-5607,S16-1192,0,0.112671,"5 Proceedings of the 9th International Workshop on Health Text Mining and Information Analysis (LOUHI 2018), pages 55–64 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics sion with a variety of features (lexical, syntactic, morphological, and many others) were the predominant approach. In fact, the best performance was achieved by UTHealth team (Lee et al., 2016) using an end-to-end system based on linear and structural Hidden Markov Model (HMM)-SVM. Just a few teams tried a neural based method, including RNN-based models (Fries, 2016) and CNN-based models (Chikka, 2016), (Li and Huang, 2016). Furthermore, among those teams just Chikka (2016) participated in the CONTAINS identification task, being around 0.30 below UTHealth's top performance. Recent work by Lin et al. (2016), Dligach et al. (2017) and Leeuwenberg and Moens (2017) followed the settings of 2016 Clinical TempEval challenge but they did not participate in the competition. Out of these, our results are only directly comparable to those of Lin et al. (2016) and Leeuwenberg and Moens (2017) since the work of Dligach et al. (2017) was not evaluated using the Clinical TempEval official scorer. Even th"
W18-5607,E12-1027,0,0.0610249,"he time expressions combined with actions that we perceive as ongoing. For example, in sentence 9 the action of moving is an activity, done indeterminably throughout the day as multiple times a day imply. In sentence 7, on the other hand, there is a time expression with a definite time interval overlapping the patient's state of being hospitalized. Temporally locating two events on a timeline requires a high level of reasoning that even for humans can turn into a complicated task. All of the aforementioned inferences were done heavily relying on the internal constituency of an event, implying Costa and Branco (2012) claim that temporal information processing can profit from information about aspectual type is valid in the clinical domain. Due to the high similarity of CONTAINS and OVERLAP relations it does not come as a surprise that these two types are easily confused by our system, which performed reasonably well on identifying other TLINK types with similar number of instances. This suggests than the main problem is not the amount of data available but how temporal properties are encoded in language. Aspectual information proved useful for differentiating between two of the most frequent and most simi"
W18-5607,E17-2118,0,0.392835,"ety of features (lexical, syntactic, morphological, and many others) were the predominant approach. In fact, the best performance was achieved by UTHealth team (Lee et al., 2016) using an end-to-end system based on linear and structural Hidden Markov Model (HMM)-SVM. Just a few teams tried a neural based method, including RNN-based models (Fries, 2016) and CNN-based models (Chikka, 2016), (Li and Huang, 2016). Furthermore, among those teams just Chikka (2016) participated in the CONTAINS identification task, being around 0.30 below UTHealth's top performance. Recent work by Lin et al. (2016), Dligach et al. (2017) and Leeuwenberg and Moens (2017) followed the settings of 2016 Clinical TempEval challenge but they did not participate in the competition. Out of these, our results are only directly comparable to those of Lin et al. (2016) and Leeuwenberg and Moens (2017) since the work of Dligach et al. (2017) was not evaluated using the Clinical TempEval official scorer. Even though Leeuwenberg and Moens (2017) established a new state-of-the-art in temporal relation extraction, their result is still below human performance. Moreover, none of the aforementioned works provides a detailed discussion of why i"
W18-5607,S16-1198,0,0.02191,"ll around 0.21. Regardless of the 55 Proceedings of the 9th International Workshop on Health Text Mining and Information Analysis (LOUHI 2018), pages 55–64 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics sion with a variety of features (lexical, syntactic, morphological, and many others) were the predominant approach. In fact, the best performance was achieved by UTHealth team (Lee et al., 2016) using an end-to-end system based on linear and structural Hidden Markov Model (HMM)-SVM. Just a few teams tried a neural based method, including RNN-based models (Fries, 2016) and CNN-based models (Chikka, 2016), (Li and Huang, 2016). Furthermore, among those teams just Chikka (2016) participated in the CONTAINS identification task, being around 0.30 below UTHealth's top performance. Recent work by Lin et al. (2016), Dligach et al. (2017) and Leeuwenberg and Moens (2017) followed the settings of 2016 Clinical TempEval challenge but they did not participate in the competition. Out of these, our results are only directly comparable to those of Lin et al. (2016) and Leeuwenberg and Moens (2017) since the work of Dligach et al. (2017) was not evaluated using the Clinic"
W18-5607,W09-2415,0,0.0315951,"Missing"
W18-5607,S16-1201,0,0.220199,"Missing"
W18-5607,S16-1199,0,0.015557,"ate in the competition. Out of these, our results are only directly comparable to those of Lin et al. (2016) and Leeuwenberg and Moens (2017) since the work of Dligach et al. (2017) was not evaluated using the Clinical TempEval official scorer. Even though Leeuwenberg and Moens (2017) established a new state-of-the-art in temporal relation extraction, their result is still below human performance. Moreover, none of the aforementioned works provides a detailed discussion of why is current performance so low and how can we improve further the results on temporal relation extraction, except from Leeuwenberg and Moens (2016), which in their first attempt on tackling this task on 2016 Clinical TempEval identified false negatives as their major problem. Our contribution is a deep error analysis taking into account the performance of our model on predicting all TLINK types. As a result, we were able to identify important clues on temporal relation extraction and based on these findings, we discuss the next step towards human-like temporal reasoning performance. increase in annotation agreement of temporal relations by relying on narrative containers, there is a consensus within the research community regarding TIE d"
W18-5607,S15-2136,0,0.178224,"Missing"
W18-5607,E17-1108,0,0.581988,"r annotation schema, based on the widely used TIE annotation standard ISOTimeML (Pustejovsky et al., 2010). Narrative containers were defined by Pustejovsky and Stubbs Results of the systems participating in Clinical TempEval suggest that they perform well on timeentity identification tasks. Nevertheless, temporal relation extraction has shown to be the most difficult. UTHealth (Lee et al., 2016), the best ranked system in 2016 Clinical TempEval, showed a significant gap of 0.25 when compared to human performance even with gold-standard entity annotations. Recent work by Lin et al. (2016) and Leeuwenberg and Moens (2017) improved UTHealth's results further but the gap with respect to humans is still around 0.21. Regardless of the 55 Proceedings of the 9th International Workshop on Health Text Mining and Information Analysis (LOUHI 2018), pages 55–64 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics sion with a variety of features (lexical, syntactic, morphological, and many others) were the predominant approach. In fact, the best performance was achieved by UTHealth team (Lee et al., 2016) using an end-to-end system based on linear and structural Hidden Markov Model (HMM)-S"
W18-5607,S16-1165,0,0.0459765,"Missing"
W18-5607,S16-1197,0,0.0183951,"the 9th International Workshop on Health Text Mining and Information Analysis (LOUHI 2018), pages 55–64 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics sion with a variety of features (lexical, syntactic, morphological, and many others) were the predominant approach. In fact, the best performance was achieved by UTHealth team (Lee et al., 2016) using an end-to-end system based on linear and structural Hidden Markov Model (HMM)-SVM. Just a few teams tried a neural based method, including RNN-based models (Fries, 2016) and CNN-based models (Chikka, 2016), (Li and Huang, 2016). Furthermore, among those teams just Chikka (2016) participated in the CONTAINS identification task, being around 0.30 below UTHealth's top performance. Recent work by Lin et al. (2016), Dligach et al. (2017) and Leeuwenberg and Moens (2017) followed the settings of 2016 Clinical TempEval challenge but they did not participate in the competition. Out of these, our results are only directly comparable to those of Lin et al. (2016) and Leeuwenberg and Moens (2017) since the work of Dligach et al. (2017) was not evaluated using the Clinical TempEval official scorer. Even though Leeuwenberg and M"
W18-5607,S17-2093,0,0.236971,"Missing"
W18-5607,W16-2914,0,0.0123189,"ive containers to their annotation schema, based on the widely used TIE annotation standard ISOTimeML (Pustejovsky et al., 2010). Narrative containers were defined by Pustejovsky and Stubbs Results of the systems participating in Clinical TempEval suggest that they perform well on timeentity identification tasks. Nevertheless, temporal relation extraction has shown to be the most difficult. UTHealth (Lee et al., 2016), the best ranked system in 2016 Clinical TempEval, showed a significant gap of 0.25 when compared to human performance even with gold-standard entity annotations. Recent work by Lin et al. (2016) and Leeuwenberg and Moens (2017) improved UTHealth's results further but the gap with respect to humans is still around 0.21. Regardless of the 55 Proceedings of the 9th International Workshop on Health Text Mining and Information Analysis (LOUHI 2018), pages 55–64 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics sion with a variety of features (lexical, syntactic, morphological, and many others) were the predominant approach. In fact, the best performance was achieved by UTHealth team (Lee et al., 2016) using an end-to-end system based on linear and struc"
W18-5607,P16-1105,0,0.185049,"}@ecei.tohoku.ac.jp okazaki@dc.titech.ac.jp Abstract Temporal reasoning remains as an unsolved task for Natural Language Processing (NLP), particularly demonstrated in the clinical domain. The complexity of temporal representation in language is evident as results of the 2016 Clinical TempEval challenge indicate: the current state-of-the-art systems perform well in solving mention-identification tasks of event and time expressions but poorly in temporal relation extraction, showing a gap of around 0.25 point below human performance. We explore to adapt the tree-based LSTMRNN model proposed by Miwa and Bansal (2016) to temporal relation extraction from clinical text, obtaining a five point improvement over the best 2016 Clinical TempEval system and two points over the state-of-theart. We deliver a deep analysis of the results and discuss the next step towards human-like temporal reasoning. 1 Figure 1: Example temporal relation annotation with and without using narrative containers. (2011) as an effort to reduce the scope of temporal relations between pairs of events and time expressions. As illustrated in Figure 1, narrative containers can be thought of as temporal buckets in which an event or series of"
W18-5607,pustejovsky-etal-2010-iso,0,0.0224165,"is a key to text processing tasks including Question Answering and Text Summarization and follows the traditional pipeline of named entity recognition (NER) and relation extraction separately. Research on this area has been led by TempEval shared tasks (Verhagen et al., 2007, 2010; UzZaman et al., 2013) but in recent years, the target domain has been shifted to the clinical domain. The resulting Clinical TempEval challenges (Bethard et al., 2015, 2016, 2017) introduced the adoption of narrative containers to their annotation schema, based on the widely used TIE annotation standard ISOTimeML (Pustejovsky et al., 2010). Narrative containers were defined by Pustejovsky and Stubbs Results of the systems participating in Clinical TempEval suggest that they perform well on timeentity identification tasks. Nevertheless, temporal relation extraction has shown to be the most difficult. UTHealth (Lee et al., 2016), the best ranked system in 2016 Clinical TempEval, showed a significant gap of 0.25 when compared to human performance even with gold-standard entity annotations. Recent work by Lin et al. (2016) and Leeuwenberg and Moens (2017) improved UTHealth's results further but the gap with respect to humans is sti"
W18-5607,W11-0419,0,0.2467,"ess unless explicitly qualified (“We resected the adenocarcinoma, and since margins were clear, we can say it is gone”). This is a non-trivial task for a computer even when relying on context information. Up to now, there have been several attempts on tackling temporal relation extraction from clinical text mostly led by the Clinical TempEval challenges. However, the results are still far from human performance and there is little information of Table 6: Distribution of misclassified CONTAINS and OVERLAP Event-Event pairs by type of EVENT . Abbreviations: V, Verb; NV, Non-Verb As mentioned by Pustejovsky and Stubbs (2011) and further discussed in Styler IV et al. (2014), EVENT-EVENT pairings are a complex and vital component, particularly in clinical narratives where doctors rely on shared domain knowledge and it is essential to read “between the lines”. The distribution of verb/non-verb entities in Table 6 indicates that most of EVENT-EVENT missclasified pairings were either of NV-NV type or include a NV entity. Time intervals of NV entities like “pain” or “resection” are more difficult to understand, while V entities like “removed” or “improving” have their time properties morphologically encoded. Thus, rega"
W18-5607,S13-2001,0,0.0364528,"tification of other temporal relation types. Until now, the only corpus annotated with narrative containers is limited to clinical texts. Introduction Temporal Information Extraction (TIE) is an active research area in NLP, where the ultimate goal is to be able to represent the development of a story over time. TIE is a key to text processing tasks including Question Answering and Text Summarization and follows the traditional pipeline of named entity recognition (NER) and relation extraction separately. Research on this area has been led by TempEval shared tasks (Verhagen et al., 2007, 2010; UzZaman et al., 2013) but in recent years, the target domain has been shifted to the clinical domain. The resulting Clinical TempEval challenges (Bethard et al., 2015, 2016, 2017) introduced the adoption of narrative containers to their annotation schema, based on the widely used TIE annotation standard ISOTimeML (Pustejovsky et al., 2010). Narrative containers were defined by Pustejovsky and Stubbs Results of the systems participating in Clinical TempEval suggest that they perform well on timeentity identification tasks. Nevertheless, temporal relation extraction has shown to be the most difficult. UTHealth (Lee"
W18-5607,S07-1014,0,0.127556,"text and facilitate the identification of other temporal relation types. Until now, the only corpus annotated with narrative containers is limited to clinical texts. Introduction Temporal Information Extraction (TIE) is an active research area in NLP, where the ultimate goal is to be able to represent the development of a story over time. TIE is a key to text processing tasks including Question Answering and Text Summarization and follows the traditional pipeline of named entity recognition (NER) and relation extraction separately. Research on this area has been led by TempEval shared tasks (Verhagen et al., 2007, 2010; UzZaman et al., 2013) but in recent years, the target domain has been shifted to the clinical domain. The resulting Clinical TempEval challenges (Bethard et al., 2015, 2016, 2017) introduced the adoption of narrative containers to their annotation schema, based on the widely used TIE annotation standard ISOTimeML (Pustejovsky et al., 2010). Narrative containers were defined by Pustejovsky and Stubbs Results of the systems participating in Clinical TempEval suggest that they perform well on timeentity identification tasks. Nevertheless, temporal relation extraction has shown to be the m"
W19-8613,D14-1162,0,0.0995173,"ing the target answer in the passage. We use the target answer by concatenating it at the end of the passage. These pairs are unsuitable for the QG dataset because the passage is not related to the question. We thus obtain 74,863, 4,658, and 4,658 pairs for training, development, and test, respectively. 3.2 Experiment setting We use the OpenNMT system (Klein et al., 2017). We retain 45,000 of the most frequent words on the source side and 28,000 of the most frequent words on the target side. All the other words are replaced by the <UNK> token. We use 300 dimensions pretrained glove.840B.300d (Pennington et al., 2014) embeddings for initialization, and we fix them during training. We use 2-layer long short-term memory in both the encoder and the decoder. The size of the hidden state is 600. The dropout rate is 0.3. We use stochastic gradient descent for optimization. The initial learning rate is 1.0. We halve the learning rate every 2,500 steps from 20,000 steps onwards. We finish the training at 50,000 steps. We adopt the model that achieves the highest accuracy in the development set. During inference, we conduct the beam search with a beam width of 2. Decoding stops when the model generates the <EOS> to"
W19-8613,D16-1264,0,0.0271748,"this model is simpler than other QG models, it achieves the surprisingly good performance on the SQuAD v1.1 dataset, as described in Sections 3.4 and 3.5. phrases manually is not so difficult; thus, using the correct interrogative phrases as inputs is an expected experimental setting. We investigate to what extent using proper interrogative phrases as inputs contributes to the quality of the questions. To the best of our knowledge, this is the first study that studies the use of interrogative phrases as inputs to improve the quality of questions in QG. We test our method on the SQuAD dataset (Rajpurkar et al., 2016). We demonstrate that the use of interrogative phrases contributes to improving the performance of QG by automatic metrics and human evaluation. We also conduct a question answering experiment. We show that it is easier for a question answering system to provide the correct target answers when questions are generated using interrogative phrases. 2 3 3.1 Experiments Dataset We use the SQuAD v1.1 dataset (Rajpurkar et al., 2016) in this study. The SQuAD dataset is a question answering dataset containing 107,785 questions with 536 articles. We lowercase all the data. We extract a sentence contain"
W19-8613,W18-6518,0,0.0391829,"Missing"
W19-8613,P17-1099,0,0.0499975,"l., 2017; Tang et al., 2018). 1 Our code is available at https://github.com/ WERimagin/NQG_Interrogative_Phrases. 106 Proceedings of The 12th International Conference on Natural Language Generation, pages 106–111, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics Figure 1: Overview of the proposed method. We concatenate an answer and interrogative phrase at the end of the passage using the special tokens <IP> and <ANS>. Also, we replace the answer phrase with the special token <A>. We use this modified passage as the input and generate a question. mechanism (See et al., 2017) into the model. By copying the words in the passage while generating the question, this mechanism reduces the risk of generating words not included in the passage. We also explore the use of the coverage mechanism (Tu et al., 2016) to avoid generating an inappropriate question. This mechanism can also prevent the model from generating the same word repeatedly. Although this model is simpler than other QG models, it achieves the surprisingly good performance on the SQuAD v1.1 dataset, as described in Sections 3.4 and 3.5. phrases manually is not so difficult; thus, using the correct interrogat"
W19-8613,N19-1423,0,0.033918,"Missing"
W19-8613,N18-2090,0,0.0175284,"the airport yesterday.” can have various candidate questions such as “When did Bob go to the airport?,” “Where did Bob go yesterday?,” and “Who went to the airport yesterday?” It is necessary for QG to specify a desired question from among multiple possibilities. Du et al. (2017) and Chali et al. (2018) used the sequence-to-sequence model (Bahdanau et al., 2015), which takes only a passage as the input. Thus, only one question is generated from multiple possibilities at random. Most recent studies tried to generate a desired question using a target answer as the input. Zhou et al. (2017) and Song et al. (2018) incorporated the target answer using the answer position feature. Kim et al. (2018) separated target answer words from the original passages to address the problem of many generated questions including the target answer words. However, we also want to specify how questions should be asked and improve the quality of the generated questions. The existing method sometimes generates inappropriate questions whose interrogative phrases do not match the target answers. For example, we specify a target answer, “in 1920,” but an interrogative phrase of the generated question is “how much.” Sun et al."
W19-8613,P17-1123,0,0.0868319,"ion Generation using Interrogative Phrases Yuichi Sasazawa Sho Takase Naoaki Okazaki Tokyo Institute of Technology {yuichi.sasazawa, sho.takase} at nlp.c.titech.ac.jp okazaki at c.titech.ac.jp Abstract One of the key requirements of QG is to generate a question such that it asks a target answer. For example, the sentence “Bob went to the airport yesterday.” can have various candidate questions such as “When did Bob go to the airport?,” “Where did Bob go yesterday?,” and “Who went to the airport yesterday?” It is necessary for QG to specify a desired question from among multiple possibilities. Du et al. (2017) and Chali et al. (2018) used the sequence-to-sequence model (Bahdanau et al., 2015), which takes only a passage as the input. Thus, only one question is generated from multiple possibilities at random. Most recent studies tried to generate a desired question using a target answer as the input. Zhou et al. (2017) and Song et al. (2018) incorporated the target answer using the answer position feature. Kim et al. (2018) separated target answer words from the original passages to address the problem of many generated questions including the target answer words. However, we also want to specify ho"
W19-8613,D17-1090,0,0.0203772,"human evaluation. Finally, we show that a question answering system can provide target answers more correctly when the questions are generated with interrogative phrases.1 1 Introduction Question Generation (QG) is the task of generating questions from a given passage. It has several applications: (1) In the area of the education, QG can help to generate questions for reading comprehension materials (Heilman and Smith, 2010). (2) QG can aid development of conversational chatbots, which ask questions (Mostafazadeh et al., 2016). (3) QG is useful for development of question answering datasets (Duan et al., 2017; Tang et al., 2018). 1 Our code is available at https://github.com/ WERimagin/NQG_Interrogative_Phrases. 106 Proceedings of The 12th International Conference on Natural Language Generation, pages 106–111, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics Figure 1: Overview of the proposed method. We concatenate an answer and interrogative phrase at the end of the passage using the special tokens <IP> and <ANS>. Also, we replace the answer phrase with the special token <A>. We use this modified passage as the input and generate a question. mechanism (See et a"
W19-8613,D18-1427,0,0.0232199,"al. (2018) incorporated the target answer using the answer position feature. Kim et al. (2018) separated target answer words from the original passages to address the problem of many generated questions including the target answer words. However, we also want to specify how questions should be asked and improve the quality of the generated questions. The existing method sometimes generates inappropriate questions whose interrogative phrases do not match the target answers. For example, we specify a target answer, “in 1920,” but an interrogative phrase of the generated question is “how much.” Sun et al. (2018) proposed the answer-focused model to address this problem. Heilman and Smith. (2010) extracted target answers from passages and automatically converted them into interrogative phrases by a sequence of general rules. In this study, we explore the use of interrogative phrases as additional sources to control QG. Using appropriate interrogative phrases that match the target answers is important for generating questions. Unlike Heilman and Smith (2010) or Sun et al. (2018), we directly input the correct interrogative phrases. Selecting correct interrogative Question Generation (QG) is the task of"
W19-8613,N10-1086,0,0.129491,"do not match the target answers. For example, we specify a target answer, “in 1920,” but an interrogative phrase of the generated question is “how much.” Sun et al. (2018) proposed the answer-focused model to address this problem. Heilman and Smith. (2010) extracted target answers from passages and automatically converted them into interrogative phrases by a sequence of general rules. In this study, we explore the use of interrogative phrases as additional sources to control QG. Using appropriate interrogative phrases that match the target answers is important for generating questions. Unlike Heilman and Smith (2010) or Sun et al. (2018), we directly input the correct interrogative phrases. Selecting correct interrogative Question Generation (QG) is the task of generating questions from a given passage. One of the key requirements of QG is to generate a question such that it results in a target answer. Previous works used a target answer to obtain a desired question. However, we also want to specify how to ask questions and improve the quality of generated questions. In this study, we explore the use of interrogative phrases as additional sources to control QG. By providing interrogative phrases, we expec"
W19-8613,N18-1141,0,0.0302542,"Missing"
W19-8613,P16-1008,0,0.0309846,"Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics Figure 1: Overview of the proposed method. We concatenate an answer and interrogative phrase at the end of the passage using the special tokens <IP> and <ANS>. Also, we replace the answer phrase with the special token <A>. We use this modified passage as the input and generate a question. mechanism (See et al., 2017) into the model. By copying the words in the passage while generating the question, this mechanism reduces the risk of generating words not included in the passage. We also explore the use of the coverage mechanism (Tu et al., 2016) to avoid generating an inappropriate question. This mechanism can also prevent the model from generating the same word repeatedly. Although this model is simpler than other QG models, it achieves the surprisingly good performance on the SQuAD v1.1 dataset, as described in Sections 3.4 and 3.5. phrases manually is not so difficult; thus, using the correct interrogative phrases as inputs is an expected experimental setting. We investigate to what extent using proper interrogative phrases as inputs contributes to the quality of the questions. To the best of our knowledge, this is the first study"
W19-8613,P17-4012,0,0.0441183,"-Copy: Without the copy mechanism. -Coverage: Without the coverage mechanism. -Answer: Without concatenating the target answer at the end of the passage. We use the target answer by replacing it in the passage. -Answer separation: Without replacing the target answer in the passage. We use the target answer by concatenating it at the end of the passage. These pairs are unsuitable for the QG dataset because the passage is not related to the question. We thus obtain 74,863, 4,658, and 4,658 pairs for training, development, and test, respectively. 3.2 Experiment setting We use the OpenNMT system (Klein et al., 2017). We retain 45,000 of the most frequent words on the source side and 28,000 of the most frequent words on the target side. All the other words are replaced by the <UNK> token. We use 300 dimensions pretrained glove.840B.300d (Pennington et al., 2014) embeddings for initialization, and we fix them during training. We use 2-layer long short-term memory in both the encoder and the decoder. The size of the hidden state is 600. The dropout rate is 0.3. We use stochastic gradient descent for optimization. The initial learning rate is 1.0. We halve the learning rate every 2,500 steps from 20,000 step"
W19-8613,P14-5010,0,0.0028191,"ier for a question answering system to provide the correct target answers when questions are generated using interrogative phrases. 2 3 3.1 Experiments Dataset We use the SQuAD v1.1 dataset (Rajpurkar et al., 2016) in this study. The SQuAD dataset is a question answering dataset containing 107,785 questions with 536 articles. We lowercase all the data. We extract a sentence containing an answer phrase. Then, we use it as the input passage. If the answer phrase spans multiple sentences, we extract these sentencese and use the concatenation of them as the input passage. We use Stanford CoreNLP (Manning et al., 2014) to extract the interrogative phrases from the questions. If the CoreNLP detects multiple words as the interrogative phrases (e.g., “how many”, “in what year” or “what country”), we use all the word as the interrogative phrase. We remove the sentence-question pairs whose questions do not contain an interrogative phrase. For instance, Yes/No questions (e.g., “Did you go to the school?”) or questions whose interrogative phrase is located in the middle of the question, which Stanford CoreNLP cannot detect accurately (e.g., “Bob went to the airport how many times?”). We remove passage-question pai"
W19-8613,P16-1170,0,0.0246563,"e performance of QG. In addition, we report the superiority of using interrogative phrases in human evaluation. Finally, we show that a question answering system can provide target answers more correctly when the questions are generated with interrogative phrases.1 1 Introduction Question Generation (QG) is the task of generating questions from a given passage. It has several applications: (1) In the area of the education, QG can help to generate questions for reading comprehension materials (Heilman and Smith, 2010). (2) QG can aid development of conversational chatbots, which ask questions (Mostafazadeh et al., 2016). (3) QG is useful for development of question answering datasets (Duan et al., 2017; Tang et al., 2018). 1 Our code is available at https://github.com/ WERimagin/NQG_Interrogative_Phrases. 106 Proceedings of The 12th International Conference on Natural Language Generation, pages 106–111, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics Figure 1: Overview of the proposed method. We concatenate an answer and interrogative phrase at the end of the passage using the special tokens <IP> and <ANS>. Also, we replace the answer phrase with the special token <A>. We"
W19-8641,P18-1015,0,0.106173,"Missing"
W19-8641,N18-1154,0,0.0117554,"s. In this example, ‘電動車’(Electric cars) and ‘全’(all) are represented by red letters and are not included in the 24character headline. These tokens cannot be evaluated by 24-character headlines. The blue tokens are not included in 9- and 13-character headlines. These tokens should not be included in shorter headlines. allowed because of limitations in the space where the headline appears. The technology of automatic headline generation has the potential to contribute greatly to this domain, and the problems of news headline generation have motivated a wide range of studies (Wang et al., 2018; Chen et al., 2018; Kiyono et al., 2018; Zhou et al., 2018; Cao et al., 2018; Wang et al., 2019). Table 1 shows sample headlines in three different lengths written by professional editors of a media company for the same news article: The length of the first headline for the digital media is restricted to 10 characters, the second to 13 characIntroduction The news media publish newspapers in print form and in electronic form. In the electric form, articles might be read on various types of devices using any application; thus, news media companies have an increasing need to produce multiple headlines for the same"
W19-8641,N16-1012,0,0.0569508,"racters long covered the content of those generated to be 13 characters long. These facts suggest that we should explore in further research a method not only trained by generic supervision data (print headlines) but also tuned for the desired length. 5 Related Work Rush et al. (2015) created the first approach to neural abstractive summarization. They generated a headline from the first sentence of a news article in the AEG (Napoles et al., 2012), which contains an enormous number of pairs of headlines and articles. After their study, a number of researchers addressed this task: For example, Chopra et al. (2016) used the encoderdecoder framework (Sutskever et al., 2014; Bahdanau et al., 2015) and Nallapati et al. (2016) incorporated additional features into the model, such as parts-of-speech tags and named entities. Suzuki and Nagata (2017) proposed word-frequency estimation to reduce the repeated phrases being generated. Zhou et al. (2017) proposed a gating mechanism (sGate) to ensure that important information is selected at each decoding step. Furthermore, attempts to control the output How Do Length Control Mechanisms Work? We wondered whether a method that could control the output length would p"
W19-8641,P19-1099,1,0.834743,"lengths could be controlled by embedding special tokens given to an input sequence. These two studies used DUC 2004 (Over et al., 2007), which comprises only 75byte summaries, to evaluate the outputs in multiple lengths. Liu et al. (2018) also proposed a method for controlling the number of output tokens in the ConvS2S model. In Transformer (Vaswani et al., 2017), Takase and Okazaki (2019) proposed two length control methods by extending positional embedding. They additionally evaluated the system outputs by using the reconstructed test set of AEG which consists of the fixed length headlines. Makino et al. (2019) proposed a global optimization method under a length constraint. Sun et al. (2019) examined how to compare summarizers by considering the length bias of generated summaries in the test set that includes various length summaries. However, no previous work built a dataset for evaluating headlines of multiple lengths or reported an in-depth perspective on this task during the process of new production in the real world. However, a single length reference that could appropriately evaluate multiple length summaries in multiple document summarization was reported (Shapira et al., 2018). In that stu"
W19-8641,K16-1028,0,0.470615,"th International Conference on Natural Language Generation, pages 333–343, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics ters, and the third to 26 characters. From a practical perspective, headlines must be generated under a rigid length constraint. The first study to consider the length of system outputs in the encoder-decoder framework was Rush et al. (2015). This study controlled the length of an output sequence by reducing the score of the end-of-sentence token to −∞ until the method generated the desired number of words. Subsequently, Kikuchi et al. (2016) and Fan et al. (2018) proposed mechanisms for length control; however, these studies produced summaries of 30, 50, and 75 bytes, and the studies evaluated the summaries by using the reference summaries of a single length (approximately 75 bytes long) in DUC 20041 . In addition, Takase and Okazaki (2019) proposed the mechanism for length control and evaluated their method with part of the test set which is consisted by summaries satisfying some length constraints in Annotated English Gigaword corpus (AEG) (Napoles et al., 2012). Thus, some questions can be posed: (1) Can previous evaluation se"
W19-8641,W12-3018,0,0.135595,"Missing"
W19-8641,P82-1020,0,0.722983,"Missing"
W19-8641,D16-1140,1,0.940941,"dings of The 12th International Conference on Natural Language Generation, pages 333–343, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics ters, and the third to 26 characters. From a practical perspective, headlines must be generated under a rigid length constraint. The first study to consider the length of system outputs in the encoder-decoder framework was Rush et al. (2015). This study controlled the length of an output sequence by reducing the score of the end-of-sentence token to −∞ until the method generated the desired number of words. Subsequently, Kikuchi et al. (2016) and Fan et al. (2018) proposed mechanisms for length control; however, these studies produced summaries of 30, 50, and 75 bytes, and the studies evaluated the summaries by using the reference summaries of a single length (approximately 75 bytes long) in DUC 20041 . In addition, Takase and Okazaki (2019) proposed the mechanism for length control and evaluated their method with part of the test set which is consisted by summaries satisfying some length constraints in Annotated English Gigaword corpus (AEG) (Napoles et al., 2012). Thus, some questions can be posed: (1) Can previous evaluation se"
W19-8641,N19-4009,0,0.0236053,"ormance differences between the methods. Although Transformer + SP-token remained the clear winner, the ranking in ROUGE scores of the other methods are flipped. We also computed rank correlation coefficients (Kendall’s τ ) to assess the discrepancy in the ranking among the methods presented in Table 7 and Table 8. The last row of Table 8 reveals that the rank correlation is not perfect (lower than one) but moderate. We understand that τ is maintained high to some extent because of two reasons: (1) Most of the Implementation We employed OpenNMT8 (Klein et al., 2017) for Seq2Seq, and fairseq9 (Ott et al., 2019) for ConvS2S and Transformer. We extended the implementations to realize LenEmb, LenInit, and LC. We set the dimensions for the token and length embeddings to 512, those for hidden states to 512, and the beam width to 5. These parameters are common in all the models. Table 6 summarizes other parameters specific to each sequenceto-sequence model. We used Nesterov’s accelerated gradient method (NAG) (Sutskever et al., 2013) with a momentum of 0.99 in ConvS2S. In Transformer, we set the number of attention heads to 8, the dimensions for the feed-forward network to 2,048, Adam’s β to 0.98, the war"
W19-8641,W18-5410,1,0.851959,"rence on Natural Language Generation, pages 333–343, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics ters, and the third to 26 characters. From a practical perspective, headlines must be generated under a rigid length constraint. The first study to consider the length of system outputs in the encoder-decoder framework was Rush et al. (2015). This study controlled the length of an output sequence by reducing the score of the end-of-sentence token to −∞ until the method generated the desired number of words. Subsequently, Kikuchi et al. (2016) and Fan et al. (2018) proposed mechanisms for length control; however, these studies produced summaries of 30, 50, and 75 bytes, and the studies evaluated the summaries by using the reference summaries of a single length (approximately 75 bytes long) in DUC 20041 . In addition, Takase and Okazaki (2019) proposed the mechanism for length control and evaluated their method with part of the test set which is consisted by summaries satisfying some length constraints in Annotated English Gigaword corpus (AEG) (Napoles et al., 2012). Thus, some questions can be posed: (1) Can previous evaluation settings adequately eval"
W19-8641,P17-4012,0,0.012013,"adlines. This evaluation setup reduced the performance differences between the methods. Although Transformer + SP-token remained the clear winner, the ranking in ROUGE scores of the other methods are flipped. We also computed rank correlation coefficients (Kendall’s τ ) to assess the discrepancy in the ranking among the methods presented in Table 7 and Table 8. The last row of Table 8 reveals that the rank correlation is not perfect (lower than one) but moderate. We understand that τ is maintained high to some extent because of two reasons: (1) Most of the Implementation We employed OpenNMT8 (Klein et al., 2017) for Seq2Seq, and fairseq9 (Ott et al., 2019) for ConvS2S and Transformer. We extended the implementations to realize LenEmb, LenInit, and LC. We set the dimensions for the token and length embeddings to 512, those for hidden states to 512, and the beam width to 5. These parameters are common in all the models. Table 6 summarizes other parameters specific to each sequenceto-sequence model. We used Nesterov’s accelerated gradient method (NAG) (Sutskever et al., 2013) with a momentum of 0.99 in ConvS2S. In Transformer, we set the number of attention heads to 8, the dimensions for the feed-forwar"
W19-8641,D15-1044,0,0.405379,"an array of devices. All devices and applications used for viewing articles have strict upper bounds regarding the number of characters ∗ This work was done at Retrieva, Inc. within Project. 333 Proceedings of The 12th International Conference on Natural Language Generation, pages 333–343, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics ters, and the third to 26 characters. From a practical perspective, headlines must be generated under a rigid length constraint. The first study to consider the length of system outputs in the encoder-decoder framework was Rush et al. (2015). This study controlled the length of an output sequence by reducing the score of the end-of-sentence token to −∞ until the method generated the desired number of words. Subsequently, Kikuchi et al. (2016) and Fan et al. (2018) proposed mechanisms for length control; however, these studies produced summaries of 30, 50, and 75 bytes, and the studies evaluated the summaries by using the reference summaries of a single length (approximately 75 bytes long) in DUC 20041 . In addition, Takase and Okazaki (2019) proposed the mechanism for length control and evaluated their method with part of the tes"
W19-8641,D18-2012,0,0.0327774,"Missing"
W19-8641,W04-3230,0,0.373641,"Missing"
W19-8641,D16-1248,0,0.0286382,"length. In the analysis, the existing methods could not take into account the word selection according to length constraint. We also found it difficult to evaluate methods to controlling output length, because headlines of different lengths are written based on different goals, and because the training data does not necessarily reflect the goal of the headlines of a specific length. In the future, we plan to explore an approach to adapt a model trained on print headlines to those which dedicated to a different length. length in neural abstractive summarization have been gradually increasing. Shi et al. (2016) reported that hidden states in recurrent neural networks in the encoder-decoder framework could implicitly model the length of the output sequences. Kikuchi et al. (2016) was the first to propose the idea of controlling the output length in the encoder-decoder framework. Their approach inserts length information for the output length into the decoder. Additionally, Fan et al. (2018) reported that output lengths could be controlled by embedding special tokens given to an input sequence. These two studies used DUC 2004 (Over et al., 2007), which comprises only 75byte summaries, to evaluate the"
W19-8641,W04-1013,0,0.0269053,"Missing"
W19-8641,W19-2303,0,0.0153687,"ese two studies used DUC 2004 (Over et al., 2007), which comprises only 75byte summaries, to evaluate the outputs in multiple lengths. Liu et al. (2018) also proposed a method for controlling the number of output tokens in the ConvS2S model. In Transformer (Vaswani et al., 2017), Takase and Okazaki (2019) proposed two length control methods by extending positional embedding. They additionally evaluated the system outputs by using the reconstructed test set of AEG which consists of the fixed length headlines. Makino et al. (2019) proposed a global optimization method under a length constraint. Sun et al. (2019) examined how to compare summarizers by considering the length bias of generated summaries in the test set that includes various length summaries. However, no previous work built a dataset for evaluating headlines of multiple lengths or reported an in-depth perspective on this task during the process of new production in the real world. However, a single length reference that could appropriately evaluate multiple length summaries in multiple document summarization was reported (Shapira et al., 2018). In that study, the authors confirmed the correlation coefficient of ROUGE scores between the s"
W19-8641,D18-1444,0,0.0233964,"in recurrent neural networks in the encoder-decoder framework could implicitly model the length of the output sequences. Kikuchi et al. (2016) was the first to propose the idea of controlling the output length in the encoder-decoder framework. Their approach inserts length information for the output length into the decoder. Additionally, Fan et al. (2018) reported that output lengths could be controlled by embedding special tokens given to an input sequence. These two studies used DUC 2004 (Over et al., 2007), which comprises only 75byte summaries, to evaluate the outputs in multiple lengths. Liu et al. (2018) also proposed a method for controlling the number of output tokens in the ConvS2S model. In Transformer (Vaswani et al., 2017), Takase and Okazaki (2019) proposed two length control methods by extending positional embedding. They additionally evaluated the system outputs by using the reconstructed test set of AEG which consists of the fixed length headlines. Makino et al. (2019) proposed a global optimization method under a length constraint. Sun et al. (2019) examined how to compare summarizers by considering the length bias of generated summaries in the test set that includes various length"
W19-8641,E17-2047,0,0.0123436,"or the desired length. 5 Related Work Rush et al. (2015) created the first approach to neural abstractive summarization. They generated a headline from the first sentence of a news article in the AEG (Napoles et al., 2012), which contains an enormous number of pairs of headlines and articles. After their study, a number of researchers addressed this task: For example, Chopra et al. (2016) used the encoderdecoder framework (Sutskever et al., 2014; Bahdanau et al., 2015) and Nallapati et al. (2016) incorporated additional features into the model, such as parts-of-speech tags and named entities. Suzuki and Nagata (2017) proposed word-frequency estimation to reduce the repeated phrases being generated. Zhou et al. (2017) proposed a gating mechanism (sGate) to ensure that important information is selected at each decoding step. Furthermore, attempts to control the output How Do Length Control Mechanisms Work? We wondered whether a method that could control the output length would produce similar headlines even for different lengths for the same news article. To confirm this suspicion, we reported ROUGE-1 recall scores in Figure 4 with three different configurations: (a) evaluating the first 13 characters of he"
W19-8641,N19-1401,1,0.906635,"t. The first study to consider the length of system outputs in the encoder-decoder framework was Rush et al. (2015). This study controlled the length of an output sequence by reducing the score of the end-of-sentence token to −∞ until the method generated the desired number of words. Subsequently, Kikuchi et al. (2016) and Fan et al. (2018) proposed mechanisms for length control; however, these studies produced summaries of 30, 50, and 75 bytes, and the studies evaluated the summaries by using the reference summaries of a single length (approximately 75 bytes long) in DUC 20041 . In addition, Takase and Okazaki (2019) proposed the mechanism for length control and evaluated their method with part of the test set which is consisted by summaries satisfying some length constraints in Annotated English Gigaword corpus (AEG) (Napoles et al., 2012). Thus, some questions can be posed: (1) Can previous evaluation settings adequately evaluate system outputs in headline generation task? (2) What type of problem should we solve in this task according to the target length? (3) How well do systems solve the problems? In this study, we present novel corpora to investigate these research questions. The contributions of th"
W19-8641,P19-1207,0,0.195072,"letters and are not included in the 24character headline. These tokens cannot be evaluated by 24-character headlines. The blue tokens are not included in 9- and 13-character headlines. These tokens should not be included in shorter headlines. allowed because of limitations in the space where the headline appears. The technology of automatic headline generation has the potential to contribute greatly to this domain, and the problems of news headline generation have motivated a wide range of studies (Wang et al., 2018; Chen et al., 2018; Kiyono et al., 2018; Zhou et al., 2018; Cao et al., 2018; Wang et al., 2019). Table 1 shows sample headlines in three different lengths written by professional editors of a media company for the same news article: The length of the first headline for the digital media is restricted to 10 characters, the second to 13 characIntroduction The news media publish newspapers in print form and in electronic form. In the electric form, articles might be read on various types of devices using any application; thus, news media companies have an increasing need to produce multiple headlines for the same news article based on what would be most appropriate and most compelling on a"
W19-8641,P17-1101,0,0.0581178,"mmarization. They generated a headline from the first sentence of a news article in the AEG (Napoles et al., 2012), which contains an enormous number of pairs of headlines and articles. After their study, a number of researchers addressed this task: For example, Chopra et al. (2016) used the encoderdecoder framework (Sutskever et al., 2014; Bahdanau et al., 2015) and Nallapati et al. (2016) incorporated additional features into the model, such as parts-of-speech tags and named entities. Suzuki and Nagata (2017) proposed word-frequency estimation to reduce the repeated phrases being generated. Zhou et al. (2017) proposed a gating mechanism (sGate) to ensure that important information is selected at each decoding step. Furthermore, attempts to control the output How Do Length Control Mechanisms Work? We wondered whether a method that could control the output length would produce similar headlines even for different lengths for the same news article. To confirm this suspicion, we reported ROUGE-1 recall scores in Figure 4 with three different configurations: (a) evaluating the first 13 characters of headlines generated to be 26 characters long on 13char-ref headlines (blue); (b) evaluating headlines ge"
Y12-1057,J04-3004,0,0.0221972,"expansion is the task of expanding a list of named entities from a few named entities (seed instances). For example, given a few instances of car vehicles “Prius”, “Lexus” and “Insight”, the task outputs new car instances such as “Corolla”, “Civic”, and “Fit”. Set expansion has many applications in NLP including named entity recognition (Collins and Singer, 1999), word sense disambiguation (Pantel and Lin, 2002), document categorization (Pantel et al., 2009), and query suggestion (Cao et al., 2008). Set expansion is often implemented as bootstrapping algorithms (Hearst, 1992; Yarowsky, 1995; Abney, 2004; Pantel and Ravichandran, 2004; Pantel 525 However, bootstrapping algorithms often suffer from patterns that retrieve instances not only of the target category but also of other categories. For example, given the seed instances “Prius” and “Lexus”, a bootstrapping algorithm might choose the pattern “new type of X”, which might extract unrelated instances such as “iPhone” and “ThinkPad”. The semantic drift problem (Curran et al., 2007), the phenomenon by which a bootstrapping algorithm deviates from the target category, has persisted as the major impediment of bootstrapping algorithms. Bootstr"
Y12-1057,W99-0613,0,0.109147,"tic categories as an additional type of prior knowledge. We demonstrate the effectiveness of sibling relations in set expansion on the dataset in which instances and sibling relations are extracted from Wikipedia in a semi-automatic manner. 1 Introduction Set expansion is the task of expanding a list of named entities from a few named entities (seed instances). For example, given a few instances of car vehicles “Prius”, “Lexus” and “Insight”, the task outputs new car instances such as “Corolla”, “Civic”, and “Fit”. Set expansion has many applications in NLP including named entity recognition (Collins and Singer, 1999), word sense disambiguation (Pantel and Lin, 2002), document categorization (Pantel et al., 2009), and query suggestion (Cao et al., 2008). Set expansion is often implemented as bootstrapping algorithms (Hearst, 1992; Yarowsky, 1995; Abney, 2004; Pantel and Ravichandran, 2004; Pantel 525 However, bootstrapping algorithms often suffer from patterns that retrieve instances not only of the target category but also of other categories. For example, given the seed instances “Prius” and “Lexus”, a bootstrapping algorithm might choose the pattern “new type of X”, which might extract unrelated instanc"
Y12-1057,C92-2082,0,0.222983,"tic manner. 1 Introduction Set expansion is the task of expanding a list of named entities from a few named entities (seed instances). For example, given a few instances of car vehicles “Prius”, “Lexus” and “Insight”, the task outputs new car instances such as “Corolla”, “Civic”, and “Fit”. Set expansion has many applications in NLP including named entity recognition (Collins and Singer, 1999), word sense disambiguation (Pantel and Lin, 2002), document categorization (Pantel et al., 2009), and query suggestion (Cao et al., 2008). Set expansion is often implemented as bootstrapping algorithms (Hearst, 1992; Yarowsky, 1995; Abney, 2004; Pantel and Ravichandran, 2004; Pantel 525 However, bootstrapping algorithms often suffer from patterns that retrieve instances not only of the target category but also of other categories. For example, given the seed instances “Prius” and “Lexus”, a bootstrapping algorithm might choose the pattern “new type of X”, which might extract unrelated instances such as “iPhone” and “ThinkPad”. The semantic drift problem (Curran et al., 2007), the phenomenon by which a bootstrapping algorithm deviates from the target category, has persisted as the major impediment of boot"
Y12-1057,P06-1015,0,0.260119,"s on the dataset (seed and test instances) extracted from Wikipedia. This paper is organized as follows. Section 2 reviews the Espresso algorithm as the baseline algorithm of this study. The section also describes the problem of semantic drift and previous approaches to the problem. Section 3 presents the proposed method, which uses sibling relations of semantic categories as an additional source of prior knowledge. Section 4 demonstrates the effectiveness of the proposed method and discusses the experimental results. In section 5, we conclude this paper. 2 Related Work 2.1 Espresso algorithm Pantel and Pennacchiotti (2006) proposed the Espresso algorithm, which fundamentally iterates two steps: candidate extraction and ranking. In candidate extraction, the algorithm collects patterns that are co-occurring with seed instances and instances acquired in the previous iteration. The algorithm also ﬁnds candidates of new instances using patterns extracted in the previous iteration. In the ranking step, the algorithm ﬁnds the top N candidates of patterns and instances based on their scores. The espresso algorithm deﬁnes score rπ (p) for candidate pattern p and score rι (i) for the candidate instance i as rπ (p) = 1"
Y12-1057,N04-1041,0,0.0397058,"the task of expanding a list of named entities from a few named entities (seed instances). For example, given a few instances of car vehicles “Prius”, “Lexus” and “Insight”, the task outputs new car instances such as “Corolla”, “Civic”, and “Fit”. Set expansion has many applications in NLP including named entity recognition (Collins and Singer, 1999), word sense disambiguation (Pantel and Lin, 2002), document categorization (Pantel et al., 2009), and query suggestion (Cao et al., 2008). Set expansion is often implemented as bootstrapping algorithms (Hearst, 1992; Yarowsky, 1995; Abney, 2004; Pantel and Ravichandran, 2004; Pantel 525 However, bootstrapping algorithms often suffer from patterns that retrieve instances not only of the target category but also of other categories. For example, given the seed instances “Prius” and “Lexus”, a bootstrapping algorithm might choose the pattern “new type of X”, which might extract unrelated instances such as “iPhone” and “ThinkPad”. The semantic drift problem (Curran et al., 2007), the phenomenon by which a bootstrapping algorithm deviates from the target category, has persisted as the major impediment of bootstrapping algorithms. Bootstrapping algorithms assume prior"
Y12-1057,W11-0319,0,0.0123588,"vehicle manufacturers using seed instances “Saturn” and “Subaru”, a bootstrapping algorithm might ﬁnd instances representing the star category (e.g., “Jupiter” and “Uranus”). This is because “Saturn” and “Subaru” are polysemous words, belonging not only to motor vehicle manufacture but also to astronomical objects: planets and stars. 2.3 Approaches to semantic drift Many researchers have presented various approaches to reduce the effects of semantic drift. The approaches range from reﬁnement of the seed set (Vyas et al., 2009), applying classiﬁer (Bellare et al., 2007; Sadamitsu et al., 2011; Pennacchiotti and Pantel, 2011), using human judges (Vyas and Pantel, 2009), to using relationships between semantic categories (Curran et al., 2007; Carlson et al., 2010). Vyas et al. (2009) investigated the inﬂuence of seed instances on bootstrapping algorithms. They reported that seed instances selected by human who are not specialists sometimes yield worse results than those selected randomly. They proposed a method that reﬁnes seed sets generated by humans to improve the set expansion performance. Bellare et al. (2007) proposed a method using a classiﬁer instead of scoring functions in the ranking step of bootstrapping"
Y12-1057,P11-2128,0,0.015674,"expand the set of motor vehicle manufacturers using seed instances “Saturn” and “Subaru”, a bootstrapping algorithm might ﬁnd instances representing the star category (e.g., “Jupiter” and “Uranus”). This is because “Saturn” and “Subaru” are polysemous words, belonging not only to motor vehicle manufacture but also to astronomical objects: planets and stars. 2.3 Approaches to semantic drift Many researchers have presented various approaches to reduce the effects of semantic drift. The approaches range from reﬁnement of the seed set (Vyas et al., 2009), applying classiﬁer (Bellare et al., 2007; Sadamitsu et al., 2011; Pennacchiotti and Pantel, 2011), using human judges (Vyas and Pantel, 2009), to using relationships between semantic categories (Curran et al., 2007; Carlson et al., 2010). Vyas et al. (2009) investigated the inﬂuence of seed instances on bootstrapping algorithms. They reported that seed instances selected by human who are not specialists sometimes yield worse results than those selected randomly. They proposed a method that reﬁnes seed sets generated by humans to improve the set expansion performance. Bellare et al. (2007) proposed a method using a classiﬁer instead of scoring functions in"
Y12-1057,sumida-etal-2008-boosting,0,0.0177124,"recision of each method when each method acquires ﬁxed quantities of instances. We asked three human annotators to judge the correctness of acquired instances. We conducted the experiments in Japanese. The results described herein have been translated into English for presentation. Table 2 reports all categories used for the experiments. Each category belongs to only one sibling group. Each sibling group consists of two or more categories. We prepared sibling groups by manually based on Wikipedia. Each category starts with 15 seed instances extracted from Wikipedia in a semi-automatic manner (Sumida et al., 2008). Because the automatic method yields incorrect seed instances, we removed errors manually. We used 110 million Japanese web pages from which patterns and instances are extracted. We parsed sentences in the web pages using KNP, a Japanese dependency parser (Kurohashi et al., 1994). To reduce the computational time for the Espresso algorithm, we removed patterns and instances occurring fewer than three times. 4.2 Results Figure 4 shows the precision of each method in accordance with the number of acquired instances. The dotted line depicts the precision curve of the Espresso algorithm. Espresso"
Y12-1057,N09-1033,0,0.0243643,"nd “Subaru”, a bootstrapping algorithm might ﬁnd instances representing the star category (e.g., “Jupiter” and “Uranus”). This is because “Saturn” and “Subaru” are polysemous words, belonging not only to motor vehicle manufacture but also to astronomical objects: planets and stars. 2.3 Approaches to semantic drift Many researchers have presented various approaches to reduce the effects of semantic drift. The approaches range from reﬁnement of the seed set (Vyas et al., 2009), applying classiﬁer (Bellare et al., 2007; Sadamitsu et al., 2011; Pennacchiotti and Pantel, 2011), using human judges (Vyas and Pantel, 2009), to using relationships between semantic categories (Curran et al., 2007; Carlson et al., 2010). Vyas et al. (2009) investigated the inﬂuence of seed instances on bootstrapping algorithms. They reported that seed instances selected by human who are not specialists sometimes yield worse results than those selected randomly. They proposed a method that reﬁnes seed sets generated by humans to improve the set expansion performance. Bellare et al. (2007) proposed a method using a classiﬁer instead of scoring functions in the ranking step of bootstrapping algorithms. The classiﬁer approach can use"
Y12-1057,P95-1026,0,0.106266,"Introduction Set expansion is the task of expanding a list of named entities from a few named entities (seed instances). For example, given a few instances of car vehicles “Prius”, “Lexus” and “Insight”, the task outputs new car instances such as “Corolla”, “Civic”, and “Fit”. Set expansion has many applications in NLP including named entity recognition (Collins and Singer, 1999), word sense disambiguation (Pantel and Lin, 2002), document categorization (Pantel et al., 2009), and query suggestion (Cao et al., 2008). Set expansion is often implemented as bootstrapping algorithms (Hearst, 1992; Yarowsky, 1995; Abney, 2004; Pantel and Ravichandran, 2004; Pantel 525 However, bootstrapping algorithms often suffer from patterns that retrieve instances not only of the target category but also of other categories. For example, given the seed instances “Prius” and “Lexus”, a bootstrapping algorithm might choose the pattern “new type of X”, which might extract unrelated instances such as “iPhone” and “ThinkPad”. The semantic drift problem (Curran et al., 2007), the phenomenon by which a bootstrapping algorithm deviates from the target category, has persisted as the major impediment of bootstrapping algori"
Y12-1057,D09-1098,0,\N,Missing
Y14-1010,E12-1004,0,0.12775,"su Muraoka† Yotaro Watanabe† Sonse Shimaoka‡ Naoaki Okazaki†∗ Kazeto Yamamoto† Kentaro Inui† Tohoku University†‡ Japan Science and Technology Agency (JST)∗ {muraoka,kazeto,yotaro-w,okazaki,inui} @ecei.tohoku.ac.jp† simaokasonse@yahoo.co.jp‡ Abstract semantic information reliably from co-occurrence statistics of a phrase. Recently, numerous studies have explored compositional semantics, in which the meaning of a phrase, clause, or sentence is computed from those of its constituents (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Guevara, 2010; Zanzotto et al., 2010; Socher et al., 2011; Baroni et al., 2012; Socher et al., 2012; Socher et al., 2013a; Socher et al., 2014). These studies mostly address theories and methods for computing a vector of a phrase from the vectors of its constituents; the simplest but effective approach is to take the average of the two input vectors. The field of distributional-compositional semantics has yielded a range of computational models for composing the vector of a phrase from those of constituent word vectors. Existing models have various ranges of their expressiveness, recursivity, and trainability. However, these models have not been examined closely for the"
Y14-1010,D12-1050,0,0.432171,"orpus. Numerous studies have demonstrated learned word vectors from a large text corpus (Bullinaria and Levy, 2007; Collobert and Weston, 2008; Turney and Pantel, 2010; Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013). In contrast, the same approach is not scalable to a complex linguistic unit (e.g., phrase or sentence) because of the data sparseness problem: the longer the length of a phrase, the fewer times the phrase occurs in a corpus. For this reason, we cannot acquire A simple approach such as additive and multiplicative compositions has been a strong baseline over more complex models (Blacoe and Lapata, 2012; Socher et al., 2013b). However, Erk and Pad´o (2008) argued the importance of syntax relations: the simple additive/multiplicative approach yields the same vector for phrases a horse draws and draw a horse, ignoring the syntactic structure by which horse in the former phrase is a subject whereas horse in the latter is the object. They formulated a generalized composition function including such a composition. However, this generalized composition is too complex to learn. These models usually do not work well for now. As described in this paper, through a humancorrelation experiment, we explo"
Y14-1010,W13-3206,0,0.0514921,"e number of instances of the relation in the dataset, some matrixes are updated frequently, and some are rarely updated. Therefore, we use the diagonal variant of AdaGrad (Duchi et al., 2011; Socher et al., 2013a). This enables the learning rate to vary each matrix Wr . 4 Experiment In this section, we explain the method for constructing vectors for words and phrases for the supervision data, followed by an explanation of some details of the training procedure. We then report experimentally obtained results. 4.1 Obtaining vectors for words and phrases as supervision data Following the work of Dinu et al. (2013), we constructed word and phrase vectors as follows. We used a concatenation of three large corpora: PukWaC2 (Baroni et al., 2009) (2 billion tokens), WaCkypedia EN(Wikipedia 2009 dump) (Baroni et al., 2009) (about 800 million tokens), and ClueWeb093 (5 billion pages in English). The distribution of PukWaC and WaCkypedia EN includes parse results from TreeTagger and MaltParser. We used Stanford CoreNLP4 to parse ClueWeb09. Counting frequencies of occurrences of lemmas of content words (nouns, adjectives, verbs, and adverbs), we identified the top 10,000 most frequent words; we represent the se"
Y14-1010,D08-1094,0,0.0952973,"Missing"
Y14-1010,W10-2805,0,0.125661,"Best Model Among Representative Compositional Models Masayasu Muraoka† Yotaro Watanabe† Sonse Shimaoka‡ Naoaki Okazaki†∗ Kazeto Yamamoto† Kentaro Inui† Tohoku University†‡ Japan Science and Technology Agency (JST)∗ {muraoka,kazeto,yotaro-w,okazaki,inui} @ecei.tohoku.ac.jp† simaokasonse@yahoo.co.jp‡ Abstract semantic information reliably from co-occurrence statistics of a phrase. Recently, numerous studies have explored compositional semantics, in which the meaning of a phrase, clause, or sentence is computed from those of its constituents (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Guevara, 2010; Zanzotto et al., 2010; Socher et al., 2011; Baroni et al., 2012; Socher et al., 2012; Socher et al., 2013a; Socher et al., 2014). These studies mostly address theories and methods for computing a vector of a phrase from the vectors of its constituents; the simplest but effective approach is to take the average of the two input vectors. The field of distributional-compositional semantics has yielded a range of computational models for composing the vector of a phrase from those of constituent word vectors. Existing models have various ranges of their expressiveness, recursivity, and trainabil"
Y14-1010,P08-1028,0,0.433594,"rises the meanings of the constituents and the rule for combining the constituents. Equation 1 formulates this principle mathematically: p = f (u, v). (1) Here, given two input (e.g. word) vectors u ∈ Rd1 and v ∈ Rd1 , the model f yields a phrase vector p ∈ Rd2 as a composition of the input vectors. In other words, the model f is a function that computes a phrase vector p for the inputs u and v. Setting d = d1 = d2 allows recursive compositions, i.e., generating phrase or sentence vectors consisting of three or more words. Table 1 shows representative models from earlier works. The Add model (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010) computes a linear combination of two input vectors u, v ∈ Rd with weights w1 , w2 ∈ R. This model works surprisingly well in practice despite its simplicity. The Fulladd model (Guevara, 2010; Zanzotto et al., 2010) PACLIC 28 extends the Add model, applying a linear transformation to inputs with a weight matrix W ∈ Rd×2d . This model can not only scale but also rotate input vectors, unlike the Add model. Regarding linear transformation with a matrix W , Recursive Neural Network (RNN) model (Socher et al., 2011) achieves a nonlinear transformation through the use of"
Y14-1010,D12-1110,0,0.653556,"tanabe† Sonse Shimaoka‡ Naoaki Okazaki†∗ Kazeto Yamamoto† Kentaro Inui† Tohoku University†‡ Japan Science and Technology Agency (JST)∗ {muraoka,kazeto,yotaro-w,okazaki,inui} @ecei.tohoku.ac.jp† simaokasonse@yahoo.co.jp‡ Abstract semantic information reliably from co-occurrence statistics of a phrase. Recently, numerous studies have explored compositional semantics, in which the meaning of a phrase, clause, or sentence is computed from those of its constituents (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Guevara, 2010; Zanzotto et al., 2010; Socher et al., 2011; Baroni et al., 2012; Socher et al., 2012; Socher et al., 2013a; Socher et al., 2014). These studies mostly address theories and methods for computing a vector of a phrase from the vectors of its constituents; the simplest but effective approach is to take the average of the two input vectors. The field of distributional-compositional semantics has yielded a range of computational models for composing the vector of a phrase from those of constituent word vectors. Existing models have various ranges of their expressiveness, recursivity, and trainability. However, these models have not been examined closely for their compositionality."
Y14-1010,P13-1045,0,0.557909,"a‡ Naoaki Okazaki†∗ Kazeto Yamamoto† Kentaro Inui† Tohoku University†‡ Japan Science and Technology Agency (JST)∗ {muraoka,kazeto,yotaro-w,okazaki,inui} @ecei.tohoku.ac.jp† simaokasonse@yahoo.co.jp‡ Abstract semantic information reliably from co-occurrence statistics of a phrase. Recently, numerous studies have explored compositional semantics, in which the meaning of a phrase, clause, or sentence is computed from those of its constituents (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Guevara, 2010; Zanzotto et al., 2010; Socher et al., 2011; Baroni et al., 2012; Socher et al., 2012; Socher et al., 2013a; Socher et al., 2014). These studies mostly address theories and methods for computing a vector of a phrase from the vectors of its constituents; the simplest but effective approach is to take the average of the two input vectors. The field of distributional-compositional semantics has yielded a range of computational models for composing the vector of a phrase from those of constituent word vectors. Existing models have various ranges of their expressiveness, recursivity, and trainability. However, these models have not been examined closely for their compositionality. We implement and comp"
Y14-1010,Q14-1017,0,0.168223,"zeto Yamamoto† Kentaro Inui† Tohoku University†‡ Japan Science and Technology Agency (JST)∗ {muraoka,kazeto,yotaro-w,okazaki,inui} @ecei.tohoku.ac.jp† simaokasonse@yahoo.co.jp‡ Abstract semantic information reliably from co-occurrence statistics of a phrase. Recently, numerous studies have explored compositional semantics, in which the meaning of a phrase, clause, or sentence is computed from those of its constituents (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Guevara, 2010; Zanzotto et al., 2010; Socher et al., 2011; Baroni et al., 2012; Socher et al., 2012; Socher et al., 2013a; Socher et al., 2014). These studies mostly address theories and methods for computing a vector of a phrase from the vectors of its constituents; the simplest but effective approach is to take the average of the two input vectors. The field of distributional-compositional semantics has yielded a range of computational models for composing the vector of a phrase from those of constituent word vectors. Existing models have various ranges of their expressiveness, recursivity, and trainability. However, these models have not been examined closely for their compositionality. We implement and compare these models under"
Y14-1010,P09-1054,0,0.0244736,"a weight matrix W is updated by the following equation, ′ W =W −α ∂J(θ) , ∂W (10) where α is a learning rate. Using stochastic gradient descent, we update a weight matrix W every time one training instance is processed. The gradient of the objective is ⎡ ⎤T ut ∂J(θ) ∂J(θ) ∂pt ∂ = = e t ⎣ vt ⎦ + λ ∥θ∥1 . ∂W ∂pt ∂W ∂W b (11) Here, et represents a d-dimensional column vector with k-th element of et,k = (pt,k − qt,k )(1 − p2t,k ). (12) d We used dx tanh(x) = 1 − tanh(x)2 to derive this equation. The second term of Equation 11 is not differentiable. Following the work of Langford et al. (2008) and Tsuruoka et al. (2009), we first update the weight matrix W without consideration of the L1 penalty. Then, we use Equation 13 to apply the L1 regularization, ⎧ ⎪ ⎨max(0, wij − αλ) if wij &gt; 0 ′ wij = min(0, wij + αλ) if wij &lt; 0 , (13) ⎪ ⎩ 0 otherwise where wij denotes the (i, j) element of W . A neural network model such as RNN, Relfunc, and Fulllex is nonlinear, which means that the naive training procedure might be trapped with a local minimum. To prevent local minima, we employ some technical methods. We update the learning rate α for every iteration epoch l using the temperature of the simulated annealing algori"
Y14-1010,C10-1142,0,0.0203636,"50 words from a target word as contexts. Then we transform each element of the co-occurrence matrix into Pointwise Mutual 2 http://wacky.sslmit.unibo.it/ http://lemurproject.org/clueweb09/ 4 http://nlp.stanford.edu/software/ corenlp.shtml 3 PACLIC 28 Information (PMI) (Evert, 2005). Finally, we compress the matrix into d dimension using Principal Component Analysis (PCA) (Roweis, 1998) with EM algorithm5 . In this way, we obtained 10,000 word vectors and 17,433 phrase vectors. 4.2 Gold-standard data We conducted a human-correlation experiment using the dataset6 created in Mitchell and Lapata (2010). Each instance in the dataset is a triplet ⟨phrase1, phrase2, similarity⟩: a similarity is a semantic similarity between the phrases annotated by humans, with a value ranging from 1 (least similar) to 7 (most similar). We designate this as humansimilarity. For example, the similarity between vast amount and large quantity is 7 (most similar) whereas the similarity between hear word and remember name is 1 (least similar). For each POS pair (adjective–noun, noun–noun, verb–noun), the dataset includes 108 instances annotated by 18 human subjects (1,944 in total). We measure Spearman’s ρ between"
Y14-1010,N10-1069,0,\N,Missing
Y15-1012,C12-1002,0,0.0248089,"Missing"
Y15-1012,D11-1142,0,0.0159856,"ions because A PPROX -PMI PACLIC 29 would take 7,441 hours (about a year) to calculate all pattern similarity with one thread. We conclude that it is necessary to prepare low dimensional feature vectors using dimension reduction or word vectors for completing similarity calculation in a realistic time. 4 Related work Unsupervised relation extraction poses three major challenges: extraction of relation instances, representing the meaning of relational patterns, and efficient similarity computation. A great number of studies proposed methods for extracting relation instances (Wu and Weld, 2010; Fader et al., 2011; Fader et al., 2011; Akbik et al., 2012). We do not describe the detail of these studies, which are out of the scope of this paper. Previous studies explored various approaches to represent the meaning of relational patterns (Lin and Pantel, 2001; Yao et al., 2012; Mikolov et al., 2013). Lin and Pantel (2001) used co-occurrence statistics of PMI between an entity and a relational pattern. Even though the goal of their research is not on relation extraction but on paraphrase (inference rule) discovery, the work had a great impact to the research on unsupervised relation extraction. Yao et al."
Y15-1012,D12-1098,0,0.0699727,"Missing"
Y15-1012,P04-1053,0,0.688367,"recent years, to acquire a wider range knowledge, Open Information Extraction (Open IE) has received much attention (Banko et al., 2007). Open IE identifies relational patterns and instances automatically without predefined target relations (Banko et al., 2007; Wu and Weld, 2010; Fader et al., 2011; Mausam et al., 2012). In other words, Open IE acquires knowledge to handle open domains. In Open IE paradigm, it is necessary to enumerate semantic relations in open domains and to learn mappings between surface patterns and semantic relations. This task is called unsupervised relation extraction (Hasegawa et al., 2004; Shinyama and Sekine, 2006; Rosenfeld and Feldman, 2007). A common approach to unsupervised relation extraction builds clusters of patterns expressing the same relation. In order to obtain clusters of relational patterns of good quality, we have two major challenges: the semantic representation of relational patterns and the scalability to large data. In this paper, we explore various methods for modeling the meaning of a pattern and for computing the similarity of patterns mined from huge data. In order to achieve this goal, we apply algorithms for approximate frequency counting and efficien"
Y15-1012,D12-1048,0,0.0153259,"pus, and extract noun phrases occurring no less than 1,000 times as a set of entities. 2.2.2 Extracting entity pairs After determining a set of entities, we discover entity pairs that may have semantic relationships in order to locate relational patterns. In this study, we extract a pair of entities if the entities co-occur in more than 5,000 sentences. We denote the set of entity pairs extracted by this procedure E. 2.2.3 Extracting patterns As a relational pattern, this study employs the shortest path between two entities in a dependency tree, following the previous work (Wu and Weld, 2010; Mausam et al., 2012; Akbik et al., 2012). Here, we introduce a restriction that a relational pattern must include a predicate in order to reject semantically1 The threshold values are 10 (first time), 5 (second time), and 0 (third and fourth times). PACLIC 29 nsubj tity pair e ∈ E. P MI refines the strength of cooccurrences with this equation, prep_in dobj det Kafka wrote The Metamorphosis in Germany. PMI(p, e) = log entity nsubj dobj X wrote Y nsubj prep_in X wrote dobj Y prep_in X wrote Y Figure 2: Example of parsed sentence and extracting patterns ambiguous patterns such as “X of Y”. Additionally, we convert"
Y15-1012,D12-1094,0,0.0858617,"ledge base consisting of instances of semantic relations (e.g., authorOf) such as authorOf ⟨Franz Kafka, The Metamorphosis⟩. To recognize these instances in a corpus, we need to obtain patterns (e.g., “X write Y”) that signal instances of the semantic relations. For a long time, many researches have targeted at extracting instances and patterns of specific relations (Riloff, 1996; Pantel and Pennacchiotti, 2006; A common approach to unsupervised relation extraction builds clusters of patterns that represent the same relation (Hasegawa et al., 2004; Shinyama and Sekine, 2006; Yao et al., 2011; Min et al., 2012; Rosenfeld and Feldman, 2007; Nakashole et al., 2012). In brief, each cluster includes patterns corresponding to a semantic relation. For example, consider three patterns, “X write Y”, “X is author of Y” and “X is located in Y”. When we group these patterns into clusters representing the same relation, patterns “X write Y” and “X is author of Y” form a cluster representing the relation authorOf, and the pattern “X is located in Y” does a cluster for locatedIn. In order to obtain these clusters, we need to know the similarity between patterns. The better we model the similarity of patterns, th"
Y15-1012,D12-1104,0,0.145691,"relations (e.g., authorOf) such as authorOf ⟨Franz Kafka, The Metamorphosis⟩. To recognize these instances in a corpus, we need to obtain patterns (e.g., “X write Y”) that signal instances of the semantic relations. For a long time, many researches have targeted at extracting instances and patterns of specific relations (Riloff, 1996; Pantel and Pennacchiotti, 2006; A common approach to unsupervised relation extraction builds clusters of patterns that represent the same relation (Hasegawa et al., 2004; Shinyama and Sekine, 2006; Yao et al., 2011; Min et al., 2012; Rosenfeld and Feldman, 2007; Nakashole et al., 2012). In brief, each cluster includes patterns corresponding to a semantic relation. For example, consider three patterns, “X write Y”, “X is author of Y” and “X is located in Y”. When we group these patterns into clusters representing the same relation, patterns “X write Y” and “X is author of Y” form a cluster representing the relation authorOf, and the pattern “X is located in Y” does a cluster for locatedIn. In order to obtain these clusters, we need to know the similarity between patterns. The better we model the similarity of patterns, the better a clustering result correspond to semantic re"
Y15-1012,P06-1015,0,0.0533433,"ential for many NLP applications such as question answering, textual inference and information extraction (Ravichandran and Hovy, 2002; Szpektor et al., 2004). Therefore, it is important to build a comprehensive knowledge base consisting of instances of semantic relations (e.g., authorOf) such as authorOf ⟨Franz Kafka, The Metamorphosis⟩. To recognize these instances in a corpus, we need to obtain patterns (e.g., “X write Y”) that signal instances of the semantic relations. For a long time, many researches have targeted at extracting instances and patterns of specific relations (Riloff, 1996; Pantel and Pennacchiotti, 2006; A common approach to unsupervised relation extraction builds clusters of patterns that represent the same relation (Hasegawa et al., 2004; Shinyama and Sekine, 2006; Yao et al., 2011; Min et al., 2012; Rosenfeld and Feldman, 2007; Nakashole et al., 2012). In brief, each cluster includes patterns corresponding to a semantic relation. For example, consider three patterns, “X write Y”, “X is author of Y” and “X is located in Y”. When we group these patterns into clusters representing the same relation, patterns “X write Y” and “X is author of Y” form a cluster representing the relation authorOf"
Y15-1012,P02-1006,0,0.0685394,"ing the meaning of a pattern and for computing the similarity of patterns mined from huge data. In order to achieve this goal, we apply algorithms for approximate frequency counting and efficient dimension reduction to unsupervised relation extraction. The experimental results show that approximate frequency counting and dimension reduction not only speeds up similarity computation but also improves the quality of pattern vectors. 1 Introduction Semantic relations between entities are essential for many NLP applications such as question answering, textual inference and information extraction (Ravichandran and Hovy, 2002; Szpektor et al., 2004). Therefore, it is important to build a comprehensive knowledge base consisting of instances of semantic relations (e.g., authorOf) such as authorOf ⟨Franz Kafka, The Metamorphosis⟩. To recognize these instances in a corpus, we need to obtain patterns (e.g., “X write Y”) that signal instances of the semantic relations. For a long time, many researches have targeted at extracting instances and patterns of specific relations (Riloff, 1996; Pantel and Pennacchiotti, 2006; A common approach to unsupervised relation extraction builds clusters of patterns that represent the s"
Y15-1012,N13-1008,0,0.262045,"major challenges in computing the similarity of patterns. First, it is not clear how to represent the semantic meaning of a relational pattern. Previous studies define a feature space for patterns, and express the meaning of patterns by using such as the co-occurrence statistics between a pattern and an entity pair, e.g., co-occurrence frequency and pointwise mutual information (PMI) (Lin and Pantel, 2001). Some studies employed vector representations of a fixed dimension, e.g., Principal Component Analysis (PCA) (Collins et al., 2002) and Latent Dirichlet Allocation (LDA) (Yao et al., 2011; Riedel et al., 2013). However, the previous work did not compare the effectiveness of these representations when applied to a collection of large-scaled unstructured texts. Second, we need design a method scalable to a large data. In Open IE, we utilize a large amount of data in order to improve the quality of unsupervised relation extraction. For this reason, we cannot use a complex and inefficient algorithm that consumes the computation time and memory storage. In this paper, we explore methods for computing pattern similarity of good quality that are scalable to huge data, for example, with several billion sen"
Y15-1012,N06-1039,0,0.189579,"e a wider range knowledge, Open Information Extraction (Open IE) has received much attention (Banko et al., 2007). Open IE identifies relational patterns and instances automatically without predefined target relations (Banko et al., 2007; Wu and Weld, 2010; Fader et al., 2011; Mausam et al., 2012). In other words, Open IE acquires knowledge to handle open domains. In Open IE paradigm, it is necessary to enumerate semantic relations in open domains and to learn mappings between surface patterns and semantic relations. This task is called unsupervised relation extraction (Hasegawa et al., 2004; Shinyama and Sekine, 2006; Rosenfeld and Feldman, 2007). A common approach to unsupervised relation extraction builds clusters of patterns expressing the same relation. In order to obtain clusters of relational patterns of good quality, we have two major challenges: the semantic representation of relational patterns and the scalability to large data. In this paper, we explore various methods for modeling the meaning of a pattern and for computing the similarity of patterns mined from huge data. In order to achieve this goal, we apply algorithms for approximate frequency counting and efficient dimension reduction to un"
Y15-1012,W04-3206,0,0.0525221,"and for computing the similarity of patterns mined from huge data. In order to achieve this goal, we apply algorithms for approximate frequency counting and efficient dimension reduction to unsupervised relation extraction. The experimental results show that approximate frequency counting and dimension reduction not only speeds up similarity computation but also improves the quality of pattern vectors. 1 Introduction Semantic relations between entities are essential for many NLP applications such as question answering, textual inference and information extraction (Ravichandran and Hovy, 2002; Szpektor et al., 2004). Therefore, it is important to build a comprehensive knowledge base consisting of instances of semantic relations (e.g., authorOf) such as authorOf ⟨Franz Kafka, The Metamorphosis⟩. To recognize these instances in a corpus, we need to obtain patterns (e.g., “X write Y”) that signal instances of the semantic relations. For a long time, many researches have targeted at extracting instances and patterns of specific relations (Riloff, 1996; Pantel and Pennacchiotti, 2006; A common approach to unsupervised relation extraction builds clusters of patterns that represent the same relation (Hasegawa e"
Y15-1012,P10-1013,0,0.113866,"phrases in the corpus, and extract noun phrases occurring no less than 1,000 times as a set of entities. 2.2.2 Extracting entity pairs After determining a set of entities, we discover entity pairs that may have semantic relationships in order to locate relational patterns. In this study, we extract a pair of entities if the entities co-occur in more than 5,000 sentences. We denote the set of entity pairs extracted by this procedure E. 2.2.3 Extracting patterns As a relational pattern, this study employs the shortest path between two entities in a dependency tree, following the previous work (Wu and Weld, 2010; Mausam et al., 2012; Akbik et al., 2012). Here, we introduce a restriction that a relational pattern must include a predicate in order to reject semantically1 The threshold values are 10 (first time), 5 (second time), and 0 (third and fourth times). PACLIC 29 nsubj tity pair e ∈ E. P MI refines the strength of cooccurrences with this equation, prep_in dobj det Kafka wrote The Metamorphosis in Germany. PMI(p, e) = log entity nsubj dobj X wrote Y nsubj prep_in X wrote dobj Y prep_in X wrote Y Figure 2: Example of parsed sentence and extracting patterns ambiguous patterns such as “X of Y”. Addi"
Y15-1012,D11-1135,0,0.373896,"comprehensive knowledge base consisting of instances of semantic relations (e.g., authorOf) such as authorOf ⟨Franz Kafka, The Metamorphosis⟩. To recognize these instances in a corpus, we need to obtain patterns (e.g., “X write Y”) that signal instances of the semantic relations. For a long time, many researches have targeted at extracting instances and patterns of specific relations (Riloff, 1996; Pantel and Pennacchiotti, 2006; A common approach to unsupervised relation extraction builds clusters of patterns that represent the same relation (Hasegawa et al., 2004; Shinyama and Sekine, 2006; Yao et al., 2011; Min et al., 2012; Rosenfeld and Feldman, 2007; Nakashole et al., 2012). In brief, each cluster includes patterns corresponding to a semantic relation. For example, consider three patterns, “X write Y”, “X is author of Y” and “X is located in Y”. When we group these patterns into clusters representing the same relation, patterns “X write Y” and “X is author of Y” form a cluster representing the relation authorOf, and the pattern “X is located in Y” does a cluster for locatedIn. In order to obtain these clusters, we need to know the similarity between patterns. The better we model the similari"
Y15-1012,P12-1075,0,0.0181752,"ity calculation in a realistic time. 4 Related work Unsupervised relation extraction poses three major challenges: extraction of relation instances, representing the meaning of relational patterns, and efficient similarity computation. A great number of studies proposed methods for extracting relation instances (Wu and Weld, 2010; Fader et al., 2011; Fader et al., 2011; Akbik et al., 2012). We do not describe the detail of these studies, which are out of the scope of this paper. Previous studies explored various approaches to represent the meaning of relational patterns (Lin and Pantel, 2001; Yao et al., 2012; Mikolov et al., 2013). Lin and Pantel (2001) used co-occurrence statistics of PMI between an entity and a relational pattern. Even though the goal of their research is not on relation extraction but on paraphrase (inference rule) discovery, the work had a great impact to the research on unsupervised relation extraction. Yao et al. (2012) modeled sentence themes and document themes by using LDA, and represented the meaning of a pattern with the themes together with the co-occurrence statistics between patterns and entities. Recently, methods inspired by neural language modeling received much"
Y15-1013,P14-2133,0,0.210642,"komatsu, tianran, okazaki, inui}@ecei.tohoku.ac.jp Abstract s0w The high-dimensionality of lexical features in parsing can be memory consuming and cause over-fitting problems. We propose a general framework to replace all lexical feature templates by low-dimensional features induced from word embeddings. Applied to a near state-of-the-art dependency parser (Huang et al., 2012), our method improves the baseline, performs better than using cluster bit string features, and outperforms a recent neural network based parser. A further analysis shows that our framework has the effect hypothesized by Andreas and Klein (2014), namely (i) connecting unseen words to known ones, and (ii) encouraging common behaviors among invocabulary words. 1 Weights: Templates: q0w Lexicon: … saw look … Lexicon: … you me … s0wsaw = (… 1 0 …) q0wyou = (… 1 0 …) s0wlook = (… 0 1 …) q0wme = (… 0 1 …) … … Replace lexical feature templates by embedding features ⋮ W(s0wsaw) W(s0wlook) · = Scores ⋮ W(q0wyou) W(q0wme) ⋮ Features: s0e1, … , s0ed q0e1, … , q0ed s0esaw = (0.6, … , 0.2) q0eyou = (0.5, … , 0.8) s0elook = (0.4, … , 0.3) q0eme = (0.7, … , 0.9) … … · W(s0e1) ⋮ W(s0ed) = Scores W(q0e1) ⋮ W(q0ed) Figure 1: Each lexical feature templ"
Y15-1013,P14-2131,0,0.238473,"rent word embeddings trained from unlabeled or automatically labeled corpora. We expect word embeddings to augment parsing accuracy, by the mechanism hypothesized in Andreas and Klein (2014), namely (i) to connect unseen words to known ones, and (ii) to encourage common behaviors among in-vocabulary words. In contrast to the negative results reported in Andreas and Klein (2014), we find that our framework indeed has these effects, and significantly improves the baseline. As a comparison, our method performs better than the technique of replacing words by cluster bit strings (Koo et al., 2008; Bansal et al., 2014), and the results outperform a neural network based parser (Chen and Manning, 2014). 2 Related Work A lot of recent work has been done on training word vectors (Mnih and Hinton, 2009; Mikolov et al., 2013; Lebret and Collobert, 2014; Pennington et al., 2014), and utilizing word vectors in various NLP tasks (Turian et al., 2010; Andreas and Klein, 2014; Bansal et al., 2014). The common approach (Turian et al., 2010; Koo et al., 2008; Bansal et al., 2014) is to use vector representations in new features, added to (near) state-of-the-art systems, and make improvement. As a result, the feature spa"
Y15-1013,D12-1091,0,0.0150916,"elated to the parsing model of the baseline, which implies that the basein Chicago, Ohio, and a fewwould other cities . simline...parser and Columbus, our modified parser have ilar behaviors. This may explain the significance 1 1: though our improveresults reported in Table ments against the baseline is fairly moderate, they are still statistically significant because our modified parser behaves similarly as the baseline parser, but would correct the mistakes made by the baseline while preserving most originally correct labels. Such improvements are easier to achieve statistical significance (Berg-Kirkpatrick et al., 2012), and are arguably indicating better generalization. So how does our modified parser improve from the baseline? In Figure 4, we plot cosine similarities between word vectors as X, and cosine similarities between weight vectors of all one-word lexical features as Y , compared to the similarities of weights of the corresponding embedding features. The plots show that, for similar words, the learned weights for the corresponding lexical features are only slightly similar; but after the lexical features are reduced to low-dimensional embedding features, the learned weights for the corresponding fe"
Y15-1013,D14-1082,0,0.184568,"expect word embeddings to augment parsing accuracy, by the mechanism hypothesized in Andreas and Klein (2014), namely (i) to connect unseen words to known ones, and (ii) to encourage common behaviors among in-vocabulary words. In contrast to the negative results reported in Andreas and Klein (2014), we find that our framework indeed has these effects, and significantly improves the baseline. As a comparison, our method performs better than the technique of replacing words by cluster bit strings (Koo et al., 2008; Bansal et al., 2014), and the results outperform a neural network based parser (Chen and Manning, 2014). 2 Related Work A lot of recent work has been done on training word vectors (Mnih and Hinton, 2009; Mikolov et al., 2013; Lebret and Collobert, 2014; Pennington et al., 2014), and utilizing word vectors in various NLP tasks (Turian et al., 2010; Andreas and Klein, 2014; Bansal et al., 2014). The common approach (Turian et al., 2010; Koo et al., 2008; Bansal et al., 2014) is to use vector representations in new features, added to (near) state-of-the-art systems, and make improvement. As a result, the feature space gets even larger. We instead propose to reduce lexical features by word embeddin"
Y15-1013,P15-1033,0,0.0114541,"2014). The common approach (Turian et al., 2010; Koo et al., 2008; Bansal et al., 2014) is to use vector representations in new features, added to (near) state-of-the-art systems, and make improvement. As a result, the feature space gets even larger. We instead propose to reduce lexical features by word embeddings. To our own surprise, though the feature space gets much smaller, the resulted system performs better. Another stream of research is to use word embeddings in whole neural network architectures (Collobert et al., 2011; Socher et al., 2013; Chen and Manning, 2014; Weiss et al., 2015; Dyer et al., 2015; Watanabe and Sumita, 2015). Though this is a promising direction and has brought breakthroughs in the field, the question is left open on what exactly 107 s3 s2 stack s1 s0 saw q0 you s0l q1 q2 her with queue I Figure 2: An internal state of a dependency parser. has contributed to the power of neural based approaches. In this work, we conjecture that the power may partly come from the low-dimensionality of word embeddings, and this advantage can be transferred to traditional feature based systems. Our experiments support this conjecture, and we expect the proposed method to help more mature,"
Y15-1013,P10-1110,0,0.182598,"ny NLP tasks, but the very highdimensional feature space brought by these features can be memory consuming and cause over-fitting problems. Is it possible to use low-dimensional word embeddings to reduce the high-dimensionality of lexical features? In this paper, we propose a general framework for this purpose. As a proof of concept, we apply the framework to dependency parsing, since this is a task where lexical features are essential. Our approach is illustrated in Figure 1. Consider a transition-based dependency parser (Yamada and Matsumoto, 2003; Nivre et al., 2006; Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011), in which the words on top of the stack and the queue (denoted by s0 w and q0 w, respectively) are typically used as features to calculate scores of transitions. When s0 w is used as a feature template, the features in this template (e.g. s0 wsaw and s0 wlook ) can be viewed as one-hot vectors of a dimension of the lexicon size (Figure 1). Corresponding to s0 w, a weight is assigned to each word (e.g. W (s0 wsaw ) and W (s0 wlook )) for calculating a transition score. Instead, we propose to utilize a d-dimensional word embedding, and replace the feature template s0 w b"
Y15-1013,N12-1015,0,0.302065,"lexicon size assigned to s0 w, now we use d 106 29th Pacific Asia Conference on Language, Information and Computation pages 106 - 113 Shanghai, China, October 30 - November 1, 2015 Copyright 2015 by Hiroya Komatsu, Ran Tian, Naoaki Okazaki and Kentaro Inui PACLIC 29 weights (i.e., W (s0 e1 ), . . . , W (s0 ed )) to calculate a transition score. In this work, we reduce feature space dimensionality by replacing all lexical features, including combined features such as s0 wq0 w, by the word embedding features. In experiments, we applied the framework to a near state-of-the-art dependency parser (Huang et al., 2012), evaluated different vector operations for replacing combined lexical features, and explored different word embeddings trained from unlabeled or automatically labeled corpora. We expect word embeddings to augment parsing accuracy, by the mechanism hypothesized in Andreas and Klein (2014), namely (i) to connect unseen words to known ones, and (ii) to encourage common behaviors among in-vocabulary words. In contrast to the negative results reported in Andreas and Klein (2014), we find that our framework indeed has these effects, and significantly improves the baseline. As a comparison, our meth"
Y15-1013,P08-1068,0,0.274886,"and explored different word embeddings trained from unlabeled or automatically labeled corpora. We expect word embeddings to augment parsing accuracy, by the mechanism hypothesized in Andreas and Klein (2014), namely (i) to connect unseen words to known ones, and (ii) to encourage common behaviors among in-vocabulary words. In contrast to the negative results reported in Andreas and Klein (2014), we find that our framework indeed has these effects, and significantly improves the baseline. As a comparison, our method performs better than the technique of replacing words by cluster bit strings (Koo et al., 2008; Bansal et al., 2014), and the results outperform a neural network based parser (Chen and Manning, 2014). 2 Related Work A lot of recent work has been done on training word vectors (Mnih and Hinton, 2009; Mikolov et al., 2013; Lebret and Collobert, 2014; Pennington et al., 2014), and utilizing word vectors in various NLP tasks (Turian et al., 2010; Andreas and Klein, 2014; Bansal et al., 2014). The common approach (Turian et al., 2010; Koo et al., 2008; Bansal et al., 2014) is to use vector representations in new features, added to (near) state-of-the-art systems, and make improvement. As a r"
Y15-1013,E14-1051,0,0.0192615,"to known ones, and (ii) to encourage common behaviors among in-vocabulary words. In contrast to the negative results reported in Andreas and Klein (2014), we find that our framework indeed has these effects, and significantly improves the baseline. As a comparison, our method performs better than the technique of replacing words by cluster bit strings (Koo et al., 2008; Bansal et al., 2014), and the results outperform a neural network based parser (Chen and Manning, 2014). 2 Related Work A lot of recent work has been done on training word vectors (Mnih and Hinton, 2009; Mikolov et al., 2013; Lebret and Collobert, 2014; Pennington et al., 2014), and utilizing word vectors in various NLP tasks (Turian et al., 2010; Andreas and Klein, 2014; Bansal et al., 2014). The common approach (Turian et al., 2010; Koo et al., 2008; Bansal et al., 2014) is to use vector representations in new features, added to (near) state-of-the-art systems, and make improvement. As a result, the feature space gets even larger. We instead propose to reduce lexical features by word embeddings. To our own surprise, though the feature space gets much smaller, the resulted system performs better. Another stream of research is to use word e"
Y15-1013,P14-1130,0,0.0224427,"d to help more mature, proven-towork existing systems. Machine learning techniques have been proposed for reducing model size and imposing feature sparsity (Suzuki et al., 2011; Yogatama and Smith, 2014). Compared to these methods, our approach is simple, without extra twists of objective functions or learning algorithms. More importantly, by using word embeddings to reduce lexical features, we explicitly exploit the inherited syntactic and semantic similarities between words. Another technique to reduce features is dimension reduction by matrix or tensor factorization (Argyriou et al., 2007; Lei et al., 2014), but typically applied to supervised learning. In contrast, we use word embeddings trained from unlabeled or automatically labeled corpora, bringing the aspects of semi-supervised learning or self-training. 3 Formalization In this section, we formalize the framework of reducing lexical features. We take transition-based parsing as an example, but the framework can be applied to other systems using lexical features. 3.1 Transition-based Parsing In typical transition-based parsing, input words are put into a queue and partially built parse trees are cached in a stack (Figure 2). At each step, a"
Y15-1013,P14-2050,0,0.022694,"): Random 86.37 86.19 81.06 P LAIN 90.68 90.48 87.02 T REE 91.06 90.82 87.38 S TATE 91.03 90.57 87.88 Table 1: Parsing Results (UAS). Numbers marked by asterisk (∗ ) are statistically significant (p &lt; 0.05), compared to the baseline (Huang et al., 2012) under a paired bootstrap test. of the embeddings but unseen in PTB training data (Unseen). We built 300 dimensional word embeddings from 6 months articles in New York Times Corpus4 (01/2007-06/2007, 1.5M sentences), for words of frequencies greater than 50. Word vectors are obtained from singular value decomposition (SVD) of the PPMI matrices (Levy and Goldberg, 2014b), for co-occurrence matrices of target words with various types of contexts (Levy and Goldberg, 2014a), to be specified later. We choose SVD for training word vectors because it is fast; and recent research suggests that SVD can perform as well as other embedding methods (Levy et al., 2015). We investigated the following types of contexts for training word vectors: P LAIN, which uses words within a window of 3 to each side of the target word as contexts; T REE, which uses words within 3 steps of the target in the dependency trees, obtained from applying Huang et al. (2012)’s parser to the co"
Y15-1013,Q15-1016,0,0.0747407,"Missing"
Y15-1013,W06-2933,0,0.0976665,"Missing"
Y15-1013,J08-4003,0,0.0284044,"f-training. 3 Formalization In this section, we formalize the framework of reducing lexical features. We take transition-based parsing as an example, but the framework can be applied to other systems using lexical features. 3.1 Transition-based Parsing In typical transition-based parsing, input words are put into a queue and partially built parse trees are cached in a stack (Figure 2). At each step, a shiftreduce action is selected, which consumes words from the queue and/or build new structures in the stack. For the set of actions, we adopt the arcstandard system (Yamada and Matsumoto, 2003; Nivre, 2008; Huang and Sagae, 2010), in which the actions are: PACLIC 29 1. Shift, which pops the top of the queue and pushes it to the stack; 2. Reduce-Left, which replaces the top two trees in the stack by their consolidated tree, left as child; d-dimensional vector representation of the word w, where vi is the i-th entry. Then, we replace sw by se, a linear combination of se1 , . . . , sed : se := d X vi · (sei ). i=1 3. Reduce-Right, which replaces the top two trees in the stack by their consolidated tree, right as child. Following Huang et al. (2012), we use the maxviolation perceptron for global le"
Y15-1013,D14-1162,0,0.0866269,"encourage common behaviors among in-vocabulary words. In contrast to the negative results reported in Andreas and Klein (2014), we find that our framework indeed has these effects, and significantly improves the baseline. As a comparison, our method performs better than the technique of replacing words by cluster bit strings (Koo et al., 2008; Bansal et al., 2014), and the results outperform a neural network based parser (Chen and Manning, 2014). 2 Related Work A lot of recent work has been done on training word vectors (Mnih and Hinton, 2009; Mikolov et al., 2013; Lebret and Collobert, 2014; Pennington et al., 2014), and utilizing word vectors in various NLP tasks (Turian et al., 2010; Andreas and Klein, 2014; Bansal et al., 2014). The common approach (Turian et al., 2010; Koo et al., 2008; Bansal et al., 2014) is to use vector representations in new features, added to (near) state-of-the-art systems, and make improvement. As a result, the feature space gets even larger. We instead propose to reduce lexical features by word embeddings. To our own surprise, though the feature space gets much smaller, the resulted system performs better. Another stream of research is to use word embeddings in whole neural"
Y15-1013,P13-1045,0,0.0371597,"ks (Turian et al., 2010; Andreas and Klein, 2014; Bansal et al., 2014). The common approach (Turian et al., 2010; Koo et al., 2008; Bansal et al., 2014) is to use vector representations in new features, added to (near) state-of-the-art systems, and make improvement. As a result, the feature space gets even larger. We instead propose to reduce lexical features by word embeddings. To our own surprise, though the feature space gets much smaller, the resulted system performs better. Another stream of research is to use word embeddings in whole neural network architectures (Collobert et al., 2011; Socher et al., 2013; Chen and Manning, 2014; Weiss et al., 2015; Dyer et al., 2015; Watanabe and Sumita, 2015). Though this is a promising direction and has brought breakthroughs in the field, the question is left open on what exactly 107 s3 s2 stack s1 s0 saw q0 you s0l q1 q2 her with queue I Figure 2: An internal state of a dependency parser. has contributed to the power of neural based approaches. In this work, we conjecture that the power may partly come from the low-dimensionality of word embeddings, and this advantage can be transferred to traditional feature based systems. Our experiments support this con"
Y15-1013,P11-2112,0,0.0227465,"n on what exactly 107 s3 s2 stack s1 s0 saw q0 you s0l q1 q2 her with queue I Figure 2: An internal state of a dependency parser. has contributed to the power of neural based approaches. In this work, we conjecture that the power may partly come from the low-dimensionality of word embeddings, and this advantage can be transferred to traditional feature based systems. Our experiments support this conjecture, and we expect the proposed method to help more mature, proven-towork existing systems. Machine learning techniques have been proposed for reducing model size and imposing feature sparsity (Suzuki et al., 2011; Yogatama and Smith, 2014). Compared to these methods, our approach is simple, without extra twists of objective functions or learning algorithms. More importantly, by using word embeddings to reduce lexical features, we explicitly exploit the inherited syntactic and semantic similarities between words. Another technique to reduce features is dimension reduction by matrix or tensor factorization (Argyriou et al., 2007; Lei et al., 2014), but typically applied to supervised learning. In contrast, we use word embeddings trained from unlabeled or automatically labeled corpora, bringing the aspec"
Y15-1013,P10-1040,0,0.0610323,"gative results reported in Andreas and Klein (2014), we find that our framework indeed has these effects, and significantly improves the baseline. As a comparison, our method performs better than the technique of replacing words by cluster bit strings (Koo et al., 2008; Bansal et al., 2014), and the results outperform a neural network based parser (Chen and Manning, 2014). 2 Related Work A lot of recent work has been done on training word vectors (Mnih and Hinton, 2009; Mikolov et al., 2013; Lebret and Collobert, 2014; Pennington et al., 2014), and utilizing word vectors in various NLP tasks (Turian et al., 2010; Andreas and Klein, 2014; Bansal et al., 2014). The common approach (Turian et al., 2010; Koo et al., 2008; Bansal et al., 2014) is to use vector representations in new features, added to (near) state-of-the-art systems, and make improvement. As a result, the feature space gets even larger. We instead propose to reduce lexical features by word embeddings. To our own surprise, though the feature space gets much smaller, the resulted system performs better. Another stream of research is to use word embeddings in whole neural network architectures (Collobert et al., 2011; Socher et al., 2013; Ch"
Y15-1013,P15-1113,0,0.0203522,"pproach (Turian et al., 2010; Koo et al., 2008; Bansal et al., 2014) is to use vector representations in new features, added to (near) state-of-the-art systems, and make improvement. As a result, the feature space gets even larger. We instead propose to reduce lexical features by word embeddings. To our own surprise, though the feature space gets much smaller, the resulted system performs better. Another stream of research is to use word embeddings in whole neural network architectures (Collobert et al., 2011; Socher et al., 2013; Chen and Manning, 2014; Weiss et al., 2015; Dyer et al., 2015; Watanabe and Sumita, 2015). Though this is a promising direction and has brought breakthroughs in the field, the question is left open on what exactly 107 s3 s2 stack s1 s0 saw q0 you s0l q1 q2 her with queue I Figure 2: An internal state of a dependency parser. has contributed to the power of neural based approaches. In this work, we conjecture that the power may partly come from the low-dimensionality of word embeddings, and this advantage can be transferred to traditional feature based systems. Our experiments support this conjecture, and we expect the proposed method to help more mature, proven-towork existing syst"
Y15-1013,P15-1032,0,0.0112835,"014; Bansal et al., 2014). The common approach (Turian et al., 2010; Koo et al., 2008; Bansal et al., 2014) is to use vector representations in new features, added to (near) state-of-the-art systems, and make improvement. As a result, the feature space gets even larger. We instead propose to reduce lexical features by word embeddings. To our own surprise, though the feature space gets much smaller, the resulted system performs better. Another stream of research is to use word embeddings in whole neural network architectures (Collobert et al., 2011; Socher et al., 2013; Chen and Manning, 2014; Weiss et al., 2015; Dyer et al., 2015; Watanabe and Sumita, 2015). Though this is a promising direction and has brought breakthroughs in the field, the question is left open on what exactly 107 s3 s2 stack s1 s0 saw q0 you s0l q1 q2 her with queue I Figure 2: An internal state of a dependency parser. has contributed to the power of neural based approaches. In this work, we conjecture that the power may partly come from the low-dimensionality of word embeddings, and this advantage can be transferred to traditional feature based systems. Our experiments support this conjecture, and we expect the proposed method t"
Y15-1013,W03-3023,0,0.392446,"ction Lexical features are powerful machine learning ingredients for many NLP tasks, but the very highdimensional feature space brought by these features can be memory consuming and cause over-fitting problems. Is it possible to use low-dimensional word embeddings to reduce the high-dimensionality of lexical features? In this paper, we propose a general framework for this purpose. As a proof of concept, we apply the framework to dependency parsing, since this is a task where lexical features are essential. Our approach is illustrated in Figure 1. Consider a transition-based dependency parser (Yamada and Matsumoto, 2003; Nivre et al., 2006; Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011), in which the words on top of the stack and the queue (denoted by s0 w and q0 w, respectively) are typically used as features to calculate scores of transitions. When s0 w is used as a feature template, the features in this template (e.g. s0 wsaw and s0 wlook ) can be viewed as one-hot vectors of a dimension of the lexicon size (Figure 1). Corresponding to s0 w, a weight is assigned to each word (e.g. W (s0 wsaw ) and W (s0 wlook )) for calculating a transition score. Instead, we propose to utilize a d-d"
Y15-1013,P14-1074,0,0.0223262,"s3 s2 stack s1 s0 saw q0 you s0l q1 q2 her with queue I Figure 2: An internal state of a dependency parser. has contributed to the power of neural based approaches. In this work, we conjecture that the power may partly come from the low-dimensionality of word embeddings, and this advantage can be transferred to traditional feature based systems. Our experiments support this conjecture, and we expect the proposed method to help more mature, proven-towork existing systems. Machine learning techniques have been proposed for reducing model size and imposing feature sparsity (Suzuki et al., 2011; Yogatama and Smith, 2014). Compared to these methods, our approach is simple, without extra twists of objective functions or learning algorithms. More importantly, by using word embeddings to reduce lexical features, we explicitly exploit the inherited syntactic and semantic similarities between words. Another technique to reduce features is dimension reduction by matrix or tensor factorization (Argyriou et al., 2007; Lei et al., 2014), but typically applied to supervised learning. In contrast, we use word embeddings trained from unlabeled or automatically labeled corpora, bringing the aspects of semi-supervised learn"
Y15-1013,D08-1059,0,0.0307488,"ning ingredients for many NLP tasks, but the very highdimensional feature space brought by these features can be memory consuming and cause over-fitting problems. Is it possible to use low-dimensional word embeddings to reduce the high-dimensionality of lexical features? In this paper, we propose a general framework for this purpose. As a proof of concept, we apply the framework to dependency parsing, since this is a task where lexical features are essential. Our approach is illustrated in Figure 1. Consider a transition-based dependency parser (Yamada and Matsumoto, 2003; Nivre et al., 2006; Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011), in which the words on top of the stack and the queue (denoted by s0 w and q0 w, respectively) are typically used as features to calculate scores of transitions. When s0 w is used as a feature template, the features in this template (e.g. s0 wsaw and s0 wlook ) can be viewed as one-hot vectors of a dimension of the lexicon size (Figure 1). Corresponding to s0 w, a weight is assigned to each word (e.g. W (s0 wsaw ) and W (s0 wlook )) for calculating a transition score. Instead, we propose to utilize a d-dimensional word embedding, and replace the"
Y15-1013,P11-2033,0,0.0263584,"ery highdimensional feature space brought by these features can be memory consuming and cause over-fitting problems. Is it possible to use low-dimensional word embeddings to reduce the high-dimensionality of lexical features? In this paper, we propose a general framework for this purpose. As a proof of concept, we apply the framework to dependency parsing, since this is a task where lexical features are essential. Our approach is illustrated in Figure 1. Consider a transition-based dependency parser (Yamada and Matsumoto, 2003; Nivre et al., 2006; Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011), in which the words on top of the stack and the queue (denoted by s0 w and q0 w, respectively) are typically used as features to calculate scores of transitions. When s0 w is used as a feature template, the features in this template (e.g. s0 wsaw and s0 wlook ) can be viewed as one-hot vectors of a dimension of the lexicon size (Figure 1). Corresponding to s0 w, a weight is assigned to each word (e.g. W (s0 wsaw ) and W (s0 wlook )) for calculating a transition score. Instead, we propose to utilize a d-dimensional word embedding, and replace the feature template s0 w by d features, namely s0"
Y16-2022,J93-2003,0,0.0439778,"e MS COCO dataset contains only 80 object categories (e.g., PERSON or CAR), each object category is referred to by a number of expressions. For example, the object category PERSON can be described by man, person, skateboarder, skate boarder, etc., as shown in Figure 2. Thus, we need to identify the correspondences between objects in an image and their referring expressions in the dataset. In this study, we cast the problem of object-word alignment as a translation task, where the input language is a set of object categories and the output language is a description. Here, we use the IBM Model (Brown et al., 1993) to obtain the translation probability P (w|c), where c denotes an object category in an image and w denotes a word in its description. For instance, the IBM Model gives a higher probability for P (w = man|c = PERSON) after seeing the training instances: PERSON, SKATEBOARD a man is riding a skateboard PERSON, DONUT a man who is eating a donut We use the GIZA++ (Och and Ney, 2003) implementation6 to estimate the alignments. 6 https://github.com/moses-smt/giza-pp PACLIC 30 Proceedings Table 1: Result of object-word alignment. Precision .880 .738 IBM Model WordNet Recall .743 .565 F1 .806 .638 We"
Y16-2022,P15-1005,0,0.0270296,"Missing"
Y16-2022,D13-1128,0,0.163671,"the third sentence in Figure 2 expresses the ride on relation between a man and a skateboard. If we could ground the man with a yellow bounding box (PERSON) and a skateboard with a green box (SKATEBOARD), we could understand the meaning of ride on(o1 , o2 ) relation via the image: the object o1 has a contact with o2 , and o1 is usually located above o2 . Unfortunately, because the MS COCO dataset does not have alignments between images and words in its descriptions, we estimate the alignments, as will be explained in Section 5.1. There are other publicly available datasets, such as the VLT2K (Elliott and Keller, 2013), PASCAL VOC (Everingham et al., 2014), Stanford 40 Actions (Yao et al., 2011), and HICO (Chao et al., 2015). Those datasets contain relation information, although the information restricts only positional or action ones: VLT2K has only positional relations, PASCAL VOC and Stanford 40 Actions contain action relations (e.g., walking and running), and HICO has human-object relations (e.g., riding a bike). We aim at a generic natural image understanding that might involve object-object relationships other than people, and we consider the MS 4 https://creativecommons.org/licenses/ by/4.0/legalcode"
Y16-2022,P14-5010,0,0.00291283,", on(a skateboard/SKATEBOARD, a picnic table/DINING TABLE) Because PERSON, SKATEBOARD, and DINING TABLE are associated with the objects in the image, the two relation instances describe the relations between PERSON and SKATEBOARD objects as ride and between SKATEBOARD and DINING TABLE objects as on. We design a method for extracting relation instances from dependency trees of image descriptions, inspired by the methods for Open Information Extraction (Schmitz et al., 2012; Nakashole et al., 2012; Xu et al., 2013; Moro and Navigli, 2013). We ﬁrst parse a description using the Stanford CoreNLP (Manning et al., 2014)7 . We ﬁnd a set of the longest noun phrases (NPs) whose phrase structures are located at nodes of height no greater than three from their leaves (in blue in Figure 4)8 . 7 We used Stanford CoreNLP 3.5.2. http://stanfordnlp.github.io/CoreNLP/ 8 This ﬁnds noun phrases with four words at most. 243 Templates 1 and 3 extract their example instances from Figure 4. Template 2 is used to extract the example from the sentence, “A man is riding on a skateboard.” In Template 1, we attach a particle (compound:prt) if any to the verb for extracting take off(a man, the hat) from the sentence, “A man is tak"
Y16-2022,D12-1104,0,0.0334189,"wed by a word and a slash. We extract two relation instances from the example: ride(a man/PERSON, a skateboard/SKATEBOARD), on(a skateboard/SKATEBOARD, a picnic table/DINING TABLE) Because PERSON, SKATEBOARD, and DINING TABLE are associated with the objects in the image, the two relation instances describe the relations between PERSON and SKATEBOARD objects as ride and between SKATEBOARD and DINING TABLE objects as on. We design a method for extracting relation instances from dependency trees of image descriptions, inspired by the methods for Open Information Extraction (Schmitz et al., 2012; Nakashole et al., 2012; Xu et al., 2013; Moro and Navigli, 2013). We ﬁrst parse a description using the Stanford CoreNLP (Manning et al., 2014)7 . We ﬁnd a set of the longest noun phrases (NPs) whose phrase structures are located at nodes of height no greater than three from their leaves (in blue in Figure 4)8 . 7 We used Stanford CoreNLP 3.5.2. http://stanfordnlp.github.io/CoreNLP/ 8 This ﬁnds noun phrases with four words at most. 243 Templates 1 and 3 extract their example instances from Figure 4. Template 2 is used to extract the example from the sentence, “A man is riding on a skateboard.” In Template 1, we att"
Y16-2022,J03-1002,0,0.00882758,"Missing"
Y16-2022,D12-1048,0,0.0392453,"ppercase letters followed by a word and a slash. We extract two relation instances from the example: ride(a man/PERSON, a skateboard/SKATEBOARD), on(a skateboard/SKATEBOARD, a picnic table/DINING TABLE) Because PERSON, SKATEBOARD, and DINING TABLE are associated with the objects in the image, the two relation instances describe the relations between PERSON and SKATEBOARD objects as ride and between SKATEBOARD and DINING TABLE objects as on. We design a method for extracting relation instances from dependency trees of image descriptions, inspired by the methods for Open Information Extraction (Schmitz et al., 2012; Nakashole et al., 2012; Xu et al., 2013; Moro and Navigli, 2013). We ﬁrst parse a description using the Stanford CoreNLP (Manning et al., 2014)7 . We ﬁnd a set of the longest noun phrases (NPs) whose phrase structures are located at nodes of height no greater than three from their leaves (in blue in Figure 4)8 . 7 We used Stanford CoreNLP 3.5.2. http://stanfordnlp.github.io/CoreNLP/ 8 This ﬁnds noun phrases with four words at most. 243 Templates 1 and 3 extract their example instances from Figure 4. Template 2 is used to extract the example from the sentence, “A man is riding on a skateboard"
Y16-2022,N13-1107,0,0.0225482,"h. We extract two relation instances from the example: ride(a man/PERSON, a skateboard/SKATEBOARD), on(a skateboard/SKATEBOARD, a picnic table/DINING TABLE) Because PERSON, SKATEBOARD, and DINING TABLE are associated with the objects in the image, the two relation instances describe the relations between PERSON and SKATEBOARD objects as ride and between SKATEBOARD and DINING TABLE objects as on. We design a method for extracting relation instances from dependency trees of image descriptions, inspired by the methods for Open Information Extraction (Schmitz et al., 2012; Nakashole et al., 2012; Xu et al., 2013; Moro and Navigli, 2013). We ﬁrst parse a description using the Stanford CoreNLP (Manning et al., 2014)7 . We ﬁnd a set of the longest noun phrases (NPs) whose phrase structures are located at nodes of height no greater than three from their leaves (in blue in Figure 4)8 . 7 We used Stanford CoreNLP 3.5.2. http://stanfordnlp.github.io/CoreNLP/ 8 This ﬁnds noun phrases with four words at most. 243 Templates 1 and 3 extract their example instances from Figure 4. Template 2 is used to extract the example from the sentence, “A man is riding on a skateboard.” In Template 1, we attach a particle (c"
Y16-2026,P98-1013,0,0.223038,"extraction of knowledge of usable goods. The corpus and guidelines will be available when this paper is presented. (iii) We present our initial attempts toward the automatic extraction of such knowledge using a sequence labeling method. The results in this experiment provide measures to estimate the complexity of this task and suggest future directions to build a large-scale corpus. 2 Related work To our knowledge, there is no resource that focuses on knowledge of usable goods. There are manually constructed and relatively accurate lexical resources such as WordNet (Miller 1995) and FrameNet (Baker et al. 1998), but their coverage is inevitably limited and these ontologies 278 do not contain knowledge of our interest. Current large-scale knowledge bases focus on knowledge of entities and their relations, but the coverage of knowledge of usable goods is still sparse as shown in Section 4.3. OpenIE systems (Etzioni et al. 2011) such as TextRunner (Etzioni et al. 2008) and ReVerb (Fader et al. 2011) extract a large number of relations such as treadmill, burns, more calories using lexico-syntactic patterns from massive corpora drawn from the Web. Though these systems cover a wide variety of relational"
Y16-2026,P07-1112,0,0.0229659,"ormation of usable goods. As for extracting information of objects, there is a body of research on the acquisition of telic and agentive roles in the context of generative lexicon theory (Pustejovsky 1991). Pustejovsky proposes qualia structures that deﬁne prototypical aspects of word’s meaning (Pustejovsky et al. 1993). Of four semantic roles in the qualia structures, the telic role describes the purpose or function of an object (e.g. read is a typical telic role for book). Computational approaches are suggested to automatically extract expressions of this role from text (Yamada et al. 2007, Cimiano and Wenderoth 2007), but these models tend to focus on taking paraphrases of “using X”, rather than the expressions of purpose or function of objects. While the telic roles cover a broader range of expressions (probably due to the unspeciﬁed deﬁnition of telicity in the original theory), our work focuses on eﬀects caused by using/consuming objects, standing as complementary to these previous studies. Information extraction research in biomedical domains concerns eﬀects caused by using drugs such that drug X causes adverse eﬀect Y (Gurulingappa et al. 2012). This kind of information may overlap with what we aim t"
Y16-2026,D11-1142,0,0.0509114,"ed work To our knowledge, there is no resource that focuses on knowledge of usable goods. There are manually constructed and relatively accurate lexical resources such as WordNet (Miller 1995) and FrameNet (Baker et al. 1998), but their coverage is inevitably limited and these ontologies 278 do not contain knowledge of our interest. Current large-scale knowledge bases focus on knowledge of entities and their relations, but the coverage of knowledge of usable goods is still sparse as shown in Section 4.3. OpenIE systems (Etzioni et al. 2011) such as TextRunner (Etzioni et al. 2008) and ReVerb (Fader et al. 2011) extract a large number of relations such as treadmill, burns, more calories using lexico-syntactic patterns from massive corpora drawn from the Web. Though these systems cover a wide variety of relational expressions, they do not intend to extract information of usable goods. As for extracting information of objects, there is a body of research on the acquisition of telic and agentive roles in the context of generative lexicon theory (Pustejovsky 1991). Pustejovsky proposes qualia structures that deﬁne prototypical aspects of word’s meaning (Pustejovsky et al. 1993). Of four semantic roles i"
Y16-2026,P14-5010,0,0.0044019,"ce labeling problem. We use Conditional 282 Random Fields (CRFs), a popular approach to solve sequence labeling problems (Laﬀerty et al. 2001). CRFsuite 7 is used as an implementation of CRF for our purpose. 5.1 Experimental Settings The training and test data consists of 792 sentences from 200 Wikipedia snippets (see Section 4.1). We select the four most frequent labels in the corpus, Effect, Means of use, Composed of and Version, for evaluation. For the data pre-processing, we ﬁrst parse the raw text and assign a part of speech tag and a named entity tag to each word using Stanford CoreNLP (Manning et al. 2014). Then we add a semantic label to each word with BIO format (Beginning, Inside and Outside). 5.2 Features Features shown in Table 5 are used for training. We use these features within a window of ±3 around the current word. Some of these features are used in combination with another feature as shown in Table 5. In addition to standard features, we add three features to exploit the characteristics of this corpus: Target, Disease and Repeat. Target feature is true when the current word is same as the title of Wikipedia article. Disease feature is true when the current word is in a list of diseas"
Y16-2026,J91-4003,0,0.655739,"Missing"
Y16-2026,J93-2005,0,0.402281,"tzioni et al. 2008) and ReVerb (Fader et al. 2011) extract a large number of relations such as treadmill, burns, more calories using lexico-syntactic patterns from massive corpora drawn from the Web. Though these systems cover a wide variety of relational expressions, they do not intend to extract information of usable goods. As for extracting information of objects, there is a body of research on the acquisition of telic and agentive roles in the context of generative lexicon theory (Pustejovsky 1991). Pustejovsky proposes qualia structures that deﬁne prototypical aspects of word’s meaning (Pustejovsky et al. 1993). Of four semantic roles in the qualia structures, the telic role describes the purpose or function of an object (e.g. read is a typical telic role for book). Computational approaches are suggested to automatically extract expressions of this role from text (Yamada et al. 2007, Cimiano and Wenderoth 2007), but these models tend to focus on taking paraphrases of “using X”, rather than the expressions of purpose or function of objects. While the telic roles cover a broader range of expressions (probably due to the unspeciﬁed deﬁnition of telicity in the original theory), our work focuses on eﬀec"
Y16-2026,speer-havasi-2012-representing,0,0.0846175,"Missing"
Y16-2026,E12-2021,0,0.100439,"Missing"
Y16-3027,Y09-1009,0,0.387638,"tion (NER) deﬁned a small number of coarse-grained entity types such as Person and Location and explored computational models for automatizing the task. One recent direction of extending this research ﬁeld is to consider a larger set of ﬁne-grained entity types (Lee et al., 2006; Sekine et al., 2002; Yosef et al., 2012; Corro et al., 2015). Recent studies report that ﬁne-grained NER makes improvements to such applications as entity linking (Ling et al., 2015) and The task of classifying Wikipedia articles into a predeﬁned set of semantic classes has already been addressed by many researchers (Chang et al., 2009; Dakka and Cucerzan, 2008; Higashinaka et al., 2012; Tardif et al., 2009; Toral and Muñoz, 2006; Watanabe et al., 2007). However, most of these studies assume a coarse-grained NE type set (3 to 15 types). Fine-grained classiﬁcation is naturally expected to be more diﬃcult than coarsegrained classiﬁcation. One big challenge is how to alleviate the problem of data sparseness when applying supervised machine learning approaches. For example, articles such as “Japan”, “Mt. Fuji”, and “Tokyo dome”, may be classiﬁed as Country, Mountain, and Sports_Facility respectively in a ﬁne-grained type set wh"
Y16-3027,D15-1103,0,0.0232597,"ally signiﬁcant improvement separately in classiﬁcation accuracy. 1 Introduction Recognizing named entities (NEs) in text is a crucial component task of a broad range of NLP applications including information extraction and question answering. Early work on named entity recognition (NER) deﬁned a small number of coarse-grained entity types such as Person and Location and explored computational models for automatizing the task. One recent direction of extending this research ﬁeld is to consider a larger set of ﬁne-grained entity types (Lee et al., 2006; Sekine et al., 2002; Yosef et al., 2012; Corro et al., 2015). Recent studies report that ﬁne-grained NER makes improvements to such applications as entity linking (Ling et al., 2015) and The task of classifying Wikipedia articles into a predeﬁned set of semantic classes has already been addressed by many researchers (Chang et al., 2009; Dakka and Cucerzan, 2008; Higashinaka et al., 2012; Tardif et al., 2009; Toral and Muñoz, 2006; Watanabe et al., 2007). However, most of these studies assume a coarse-grained NE type set (3 to 15 types). Fine-grained classiﬁcation is naturally expected to be more diﬃcult than coarsegrained classiﬁcation. One big challen"
Y16-3027,I08-1071,0,0.74711,"small number of coarse-grained entity types such as Person and Location and explored computational models for automatizing the task. One recent direction of extending this research ﬁeld is to consider a larger set of ﬁne-grained entity types (Lee et al., 2006; Sekine et al., 2002; Yosef et al., 2012; Corro et al., 2015). Recent studies report that ﬁne-grained NER makes improvements to such applications as entity linking (Ling et al., 2015) and The task of classifying Wikipedia articles into a predeﬁned set of semantic classes has already been addressed by many researchers (Chang et al., 2009; Dakka and Cucerzan, 2008; Higashinaka et al., 2012; Tardif et al., 2009; Toral and Muñoz, 2006; Watanabe et al., 2007). However, most of these studies assume a coarse-grained NE type set (3 to 15 types). Fine-grained classiﬁcation is naturally expected to be more diﬃcult than coarsegrained classiﬁcation. One big challenge is how to alleviate the problem of data sparseness when applying supervised machine learning approaches. For example, articles such as “Japan”, “Mt. Fuji”, and “Tokyo dome”, may be classiﬁed as Country, Mountain, and Sports_Facility respectively in a ﬁne-grained type set whereas all of them fall int"
Y16-3027,doddington-etal-2004-automatic,0,0.0532304,"context of automatic construction of an NE gazetteer 1http://www.cl.ecei.tohoku.ac.jp/~m-suzuki/ jawiki_vector/ PACLIC 30 Proceedings from Wikipedia articles. Toral and Muñoz (2006) proposed a method to classify Wikipedia articles into three NE types (Location, Organization, Person) using words included in the body of the article. They used WordNet as an external knowledge base for collecting hypernym information. They also applied weighted voting heuristics to determine NE types of articles. Dakka and Cucerzan (2008) classiﬁed articles into four NE types (PER, ORG, LOC, MISC) deﬁned in ACE (Doddington et al., 2004) using supervised machine learning algorithms based on SVMs and naive Bayes. They used the bag-of-words in the target article as well as context words from the anchor text linking to the target article. Watanabe et al. (2007) focused on the HTML tree/link structure in Wikipedia articles. They formalized an NE categorization problem as assigning of NE labels to anchor texts in Wikipedia. They constructed graph-based representations of articles and estimated assignments of NE labels over the graphs using conditional random ﬁelds. In addition to these studies, there have been eﬀorts toward automa"
Y16-3027,P98-1068,0,0.0357389,"n in the ﬁrst sentence Headings of the article Direct categories deﬁned in Wikipedia Upper categories deﬁned in Wikipedia • An anchor text is not always identical to the article title to which the anchor refers. For this reason, we need to normalize an anchor text to the title of the article linked by the anchor. able to reproduce features T8, T12, T14, and M22 described in the original paper (Higashinaka et al., 2012) because those features require the authors’ internal resources to implement. For similar reasons, we used MeCab (Kudo et al., 2004) as a morphological analyzer instead of JTAG (Fuchi and Takagi, 1998), which was unavailable to us. For extracting text from Wikipedia dump, we used Wikipedia Extractor (http://medialab.di.unipi.it/wiki/ Wikipedia_Extractor). We denote this baseline feature set as Fb . 4.2.2 Article Vectors To extend the aforementioned basic feature set, we hypothesize that the way how each article (i.e. named entity) is mentioned in other articles can also be a useful clue for classifying that article. To test this hypothesis, we introduce distributed representations of Wikipedia articles. Consider an article “Mount Everest”. This article is hyperlinked from other articles as"
Y16-3027,C12-1071,0,0.253416,"ined entity types such as Person and Location and explored computational models for automatizing the task. One recent direction of extending this research ﬁeld is to consider a larger set of ﬁne-grained entity types (Lee et al., 2006; Sekine et al., 2002; Yosef et al., 2012; Corro et al., 2015). Recent studies report that ﬁne-grained NER makes improvements to such applications as entity linking (Ling et al., 2015) and The task of classifying Wikipedia articles into a predeﬁned set of semantic classes has already been addressed by many researchers (Chang et al., 2009; Dakka and Cucerzan, 2008; Higashinaka et al., 2012; Tardif et al., 2009; Toral and Muñoz, 2006; Watanabe et al., 2007). However, most of these studies assume a coarse-grained NE type set (3 to 15 types). Fine-grained classiﬁcation is naturally expected to be more diﬃcult than coarsegrained classiﬁcation. One big challenge is how to alleviate the problem of data sparseness when applying supervised machine learning approaches. For example, articles such as “Japan”, “Mt. Fuji”, and “Tokyo dome”, may be classiﬁed as Country, Mountain, and Sports_Facility respectively in a ﬁne-grained type set whereas all of them fall into the same type Location i"
Y16-3027,P08-1047,0,0.0208216,"Missing"
Y16-3027,W04-3230,0,0.0226318,"haracters in the title Last character type in the title Last noun in the ﬁrst sentence Headings of the article Direct categories deﬁned in Wikipedia Upper categories deﬁned in Wikipedia • An anchor text is not always identical to the article title to which the anchor refers. For this reason, we need to normalize an anchor text to the title of the article linked by the anchor. able to reproduce features T8, T12, T14, and M22 described in the original paper (Higashinaka et al., 2012) because those features require the authors’ internal resources to implement. For similar reasons, we used MeCab (Kudo et al., 2004) as a morphological analyzer instead of JTAG (Fuchi and Takagi, 1998), which was unavailable to us. For extracting text from Wikipedia dump, we used Wikipedia Extractor (http://medialab.di.unipi.it/wiki/ Wikipedia_Extractor). We denote this baseline feature set as Fb . 4.2.2 Article Vectors To extend the aforementioned basic feature set, we hypothesize that the way how each article (i.e. named entity) is mentioned in other articles can also be a useful clue for classifying that article. To test this hypothesis, we introduce distributed representations of Wikipedia articles. Consider an article"
Y16-3027,Q15-1023,0,0.033578,"s a crucial component task of a broad range of NLP applications including information extraction and question answering. Early work on named entity recognition (NER) deﬁned a small number of coarse-grained entity types such as Person and Location and explored computational models for automatizing the task. One recent direction of extending this research ﬁeld is to consider a larger set of ﬁne-grained entity types (Lee et al., 2006; Sekine et al., 2002; Yosef et al., 2012; Corro et al., 2015). Recent studies report that ﬁne-grained NER makes improvements to such applications as entity linking (Ling et al., 2015) and The task of classifying Wikipedia articles into a predeﬁned set of semantic classes has already been addressed by many researchers (Chang et al., 2009; Dakka and Cucerzan, 2008; Higashinaka et al., 2012; Tardif et al., 2009; Toral and Muñoz, 2006; Watanabe et al., 2007). However, most of these studies assume a coarse-grained NE type set (3 to 15 types). Fine-grained classiﬁcation is naturally expected to be more diﬃcult than coarsegrained classiﬁcation. One big challenge is how to alleviate the problem of data sparseness when applying supervised machine learning approaches. For example, a"
Y16-3027,W02-1111,0,0.0133169,"Missing"
Y16-3027,P13-1146,0,0.0145923,"Auer et al., 2007) have been devoted to provide Wikipedia articles with ontology class labels by applying simple heuristic or hand-crafted rules. However, these approaches heavily rely on metadata (e.g., infobox templates and category labels) and suﬀer from insuﬃcient coverage of rules due to the lack of metadata, as reported by Aprosio et al. (2013). Another trend of research which may seem relevant to our work can be found in eﬀorts for automatically annotating entity mentions in text with ﬁne-grained NE type labels deﬁned in an existing type hierarchy such as Freebase (Ling and Weld, 2012; Nakashole et al., 2013; Shimaoka et al., 2016). While these studies focus on the identiﬁcation and classiﬁcation of individual mentions, our work aims at the classiﬁcation of Wikipedia articles. The two tasks are related and may well beneﬁt from each other. However, they are not the same; techniques proposed for mention classiﬁcation cannot directly apply to our task nor can be compared with our methods. 537 The work closest to our study is done by Higashinaka et al. (2012), who proposed a supervised machine learning model for classifying Wikipedia articles into the 200 ﬁne-grained NE types deﬁned by Sekine et al."
Y16-3027,sekine-etal-2002-extended,1,0.691678,"hat both ideas gained their own statistically signiﬁcant improvement separately in classiﬁcation accuracy. 1 Introduction Recognizing named entities (NEs) in text is a crucial component task of a broad range of NLP applications including information extraction and question answering. Early work on named entity recognition (NER) deﬁned a small number of coarse-grained entity types such as Person and Location and explored computational models for automatizing the task. One recent direction of extending this research ﬁeld is to consider a larger set of ﬁne-grained entity types (Lee et al., 2006; Sekine et al., 2002; Yosef et al., 2012; Corro et al., 2015). Recent studies report that ﬁne-grained NER makes improvements to such applications as entity linking (Ling et al., 2015) and The task of classifying Wikipedia articles into a predeﬁned set of semantic classes has already been addressed by many researchers (Chang et al., 2009; Dakka and Cucerzan, 2008; Higashinaka et al., 2012; Tardif et al., 2009; Toral and Muñoz, 2006; Watanabe et al., 2007). However, most of these studies assume a coarse-grained NE type set (3 to 15 types). Fine-grained classiﬁcation is naturally expected to be more diﬃcult than coa"
Y16-3027,W16-1313,1,0.800285,"been devoted to provide Wikipedia articles with ontology class labels by applying simple heuristic or hand-crafted rules. However, these approaches heavily rely on metadata (e.g., infobox templates and category labels) and suﬀer from insuﬃcient coverage of rules due to the lack of metadata, as reported by Aprosio et al. (2013). Another trend of research which may seem relevant to our work can be found in eﬀorts for automatically annotating entity mentions in text with ﬁne-grained NE type labels deﬁned in an existing type hierarchy such as Freebase (Ling and Weld, 2012; Nakashole et al., 2013; Shimaoka et al., 2016). While these studies focus on the identiﬁcation and classiﬁcation of individual mentions, our work aims at the classiﬁcation of Wikipedia articles. The two tasks are related and may well beneﬁt from each other. However, they are not the same; techniques proposed for mention classiﬁcation cannot directly apply to our task nor can be compared with our methods. 537 The work closest to our study is done by Higashinaka et al. (2012), who proposed a supervised machine learning model for classifying Wikipedia articles into the 200 ﬁne-grained NE types deﬁned by Sekine et al. (2002). They conducted e"
Y16-3027,U09-1015,0,0.778436,"Person and Location and explored computational models for automatizing the task. One recent direction of extending this research ﬁeld is to consider a larger set of ﬁne-grained entity types (Lee et al., 2006; Sekine et al., 2002; Yosef et al., 2012; Corro et al., 2015). Recent studies report that ﬁne-grained NER makes improvements to such applications as entity linking (Ling et al., 2015) and The task of classifying Wikipedia articles into a predeﬁned set of semantic classes has already been addressed by many researchers (Chang et al., 2009; Dakka and Cucerzan, 2008; Higashinaka et al., 2012; Tardif et al., 2009; Toral and Muñoz, 2006; Watanabe et al., 2007). However, most of these studies assume a coarse-grained NE type set (3 to 15 types). Fine-grained classiﬁcation is naturally expected to be more diﬃcult than coarsegrained classiﬁcation. One big challenge is how to alleviate the problem of data sparseness when applying supervised machine learning approaches. For example, articles such as “Japan”, “Mt. Fuji”, and “Tokyo dome”, may be classiﬁed as Country, Mountain, and Sports_Facility respectively in a ﬁne-grained type set whereas all of them fall into the same type Location in a common coarse-gra"
Y16-3027,W06-2809,0,0.899903,"nd explored computational models for automatizing the task. One recent direction of extending this research ﬁeld is to consider a larger set of ﬁne-grained entity types (Lee et al., 2006; Sekine et al., 2002; Yosef et al., 2012; Corro et al., 2015). Recent studies report that ﬁne-grained NER makes improvements to such applications as entity linking (Ling et al., 2015) and The task of classifying Wikipedia articles into a predeﬁned set of semantic classes has already been addressed by many researchers (Chang et al., 2009; Dakka and Cucerzan, 2008; Higashinaka et al., 2012; Tardif et al., 2009; Toral and Muñoz, 2006; Watanabe et al., 2007). However, most of these studies assume a coarse-grained NE type set (3 to 15 types). Fine-grained classiﬁcation is naturally expected to be more diﬃcult than coarsegrained classiﬁcation. One big challenge is how to alleviate the problem of data sparseness when applying supervised machine learning approaches. For example, articles such as “Japan”, “Mt. Fuji”, and “Tokyo dome”, may be classiﬁed as Country, Mountain, and Sports_Facility respectively in a ﬁne-grained type set whereas all of them fall into the same type Location in a common coarse-grained type set. Given th"
Y16-3027,D07-1068,0,0.814403,"al models for automatizing the task. One recent direction of extending this research ﬁeld is to consider a larger set of ﬁne-grained entity types (Lee et al., 2006; Sekine et al., 2002; Yosef et al., 2012; Corro et al., 2015). Recent studies report that ﬁne-grained NER makes improvements to such applications as entity linking (Ling et al., 2015) and The task of classifying Wikipedia articles into a predeﬁned set of semantic classes has already been addressed by many researchers (Chang et al., 2009; Dakka and Cucerzan, 2008; Higashinaka et al., 2012; Tardif et al., 2009; Toral and Muñoz, 2006; Watanabe et al., 2007). However, most of these studies assume a coarse-grained NE type set (3 to 15 types). Fine-grained classiﬁcation is naturally expected to be more diﬃcult than coarsegrained classiﬁcation. One big challenge is how to alleviate the problem of data sparseness when applying supervised machine learning approaches. For example, articles such as “Japan”, “Mt. Fuji”, and “Tokyo dome”, may be classiﬁed as Country, Mountain, and Sports_Facility respectively in a ﬁne-grained type set whereas all of them fall into the same type Location in a common coarse-grained type set. Given the same number of labeled"
Y16-3027,C12-2133,0,0.0668656,"their own statistically signiﬁcant improvement separately in classiﬁcation accuracy. 1 Introduction Recognizing named entities (NEs) in text is a crucial component task of a broad range of NLP applications including information extraction and question answering. Early work on named entity recognition (NER) deﬁned a small number of coarse-grained entity types such as Person and Location and explored computational models for automatizing the task. One recent direction of extending this research ﬁeld is to consider a larger set of ﬁne-grained entity types (Lee et al., 2006; Sekine et al., 2002; Yosef et al., 2012; Corro et al., 2015). Recent studies report that ﬁne-grained NER makes improvements to such applications as entity linking (Ling et al., 2015) and The task of classifying Wikipedia articles into a predeﬁned set of semantic classes has already been addressed by many researchers (Chang et al., 2009; Dakka and Cucerzan, 2008; Higashinaka et al., 2012; Tardif et al., 2009; Toral and Muñoz, 2006; Watanabe et al., 2007). However, most of these studies assume a coarse-grained NE type set (3 to 15 types). Fine-grained classiﬁcation is naturally expected to be more diﬃcult than coarsegrained classiﬁca"
Y16-3027,C98-1065,0,\N,Missing
Y17-1045,doddington-etal-2004-automatic,0,0.470149,"onsense knowledge such as entities and events, and their causal relationships, are indispensable in various natural language processing (NLP) applications, including question answering (Oh et al., 2013; Oh et al., 2016; Sharp et al., 2016), hypothesis generation (Radinsky et al., 2012; Hashimoto et al., 2015), stance detection (Sasaki et al., 2016), and literature curation for systems biology (Pyysalo et al., 2015; Rinaldi et al., 2016). In many previous researches, corpora for acquiring causal relations were built by annotating two text spans (e.g., entities) and their relations in the text (Doddington et al., 2004; Hendrickx et al., 2010; Pyysalo et al., 2015; Rinaldi et al., 2016; This paper presents an approach for harnessing causal relation instances to Wikipedia articles via crowdsourcing. Wikipedia is the central infrastructure for knowledge curation, as exemplified by Freebase (Bollacker et al., 2008) and Wikification (Mihalcea and Csomai, 2007). Therefore, we base Wikipedia articles for building a corpus with causal relation instances. This work represents a first step toward organizing the causal knowledge in Wikipedia articles covering various topics. Recently, researchers have recognized the"
Y17-1045,W17-0812,0,0.0259259,"tool has not been released to the public. In contrast, we combine a crowdsourcing service with brat, a popular open-source annotation tool, to provide an easy-to-use interface and quality control for the annotation work. This approach is not limited to causal relations but can be adapted to any bratsupported tasks (e.g., part-of-speech tagging and information extraction). We also present a quality control mechanism that is applicable to any crowdsourcing services accepting free text for a micro-task. Several studies have dedicated to identify causal relations mentioned in text. For instance, Dunietz et al. (2017) present the version 2.0 of Bank of Effects and Causes Stated Explicitly (BECauSE). The corpus includes annotations of causes and effects as well as seven semantic relations that are frequently associated with causation. Rehbein and Ruppenhofer (2017) use the similar annotation scheme for building a German corpus with some changes in the label set and the scope of causality. Built on top of well-established lingustic theories, these studies focus more on “causal language” (expressions of causation) than real-world causation. In contrast, our ultimate goal is acquisition of real-world causal kn"
Y17-1045,W10-0713,0,0.227381,"ssing causal relation instances to Wikipedia articles via crowdsourcing. Wikipedia is the central infrastructure for knowledge curation, as exemplified by Freebase (Bollacker et al., 2008) and Wikification (Mihalcea and Csomai, 2007). Therefore, we base Wikipedia articles for building a corpus with causal relation instances. This work represents a first step toward organizing the causal knowledge in Wikipedia articles covering various topics. Recently, researchers have recognized the value of crowdsourcing services in constructing wideranging language resources at low cost (Brew et al., 2010; Finin et al., 2010; Gormley et al., 2010; Jha et al., 2010; Fort et al., 2011; Kawahara et al., 2014; Lawson et al., 2010; Hovy et al., 2014; Takase et al., 2016). Unfortunately, causal relations cannot be directly annotated by crowdsourcing. For this purpose, non-expert workers on crowdsourcing services require a clear and simple micro-task. A crowdsourcing service only provides a standardized interface for workers. The micro-tasks on this interface 336 31st Pacific Asia Conference on Language, Information and Computation (PACLIC 31), pages 336–345 Cebu City, Philippines, November 16-18, 2017 c Copyright 2017"
Y17-1045,J11-2010,0,0.0311036,"owdsourcing. Wikipedia is the central infrastructure for knowledge curation, as exemplified by Freebase (Bollacker et al., 2008) and Wikification (Mihalcea and Csomai, 2007). Therefore, we base Wikipedia articles for building a corpus with causal relation instances. This work represents a first step toward organizing the causal knowledge in Wikipedia articles covering various topics. Recently, researchers have recognized the value of crowdsourcing services in constructing wideranging language resources at low cost (Brew et al., 2010; Finin et al., 2010; Gormley et al., 2010; Jha et al., 2010; Fort et al., 2011; Kawahara et al., 2014; Lawson et al., 2010; Hovy et al., 2014; Takase et al., 2016). Unfortunately, causal relations cannot be directly annotated by crowdsourcing. For this purpose, non-expert workers on crowdsourcing services require a clear and simple micro-task. A crowdsourcing service only provides a standardized interface for workers. The micro-tasks on this interface 336 31st Pacific Asia Conference on Language, Information and Computation (PACLIC 31), pages 336–345 Cebu City, Philippines, November 16-18, 2017 c Copyright 2017 Kazuaki Hanawa, Akira Sasaki, Naoaki Okazaki and Kentaro In"
Y17-1045,W10-0732,0,0.146368,"n instances to Wikipedia articles via crowdsourcing. Wikipedia is the central infrastructure for knowledge curation, as exemplified by Freebase (Bollacker et al., 2008) and Wikification (Mihalcea and Csomai, 2007). Therefore, we base Wikipedia articles for building a corpus with causal relation instances. This work represents a first step toward organizing the causal knowledge in Wikipedia articles covering various topics. Recently, researchers have recognized the value of crowdsourcing services in constructing wideranging language resources at low cost (Brew et al., 2010; Finin et al., 2010; Gormley et al., 2010; Jha et al., 2010; Fort et al., 2011; Kawahara et al., 2014; Lawson et al., 2010; Hovy et al., 2014; Takase et al., 2016). Unfortunately, causal relations cannot be directly annotated by crowdsourcing. For this purpose, non-expert workers on crowdsourcing services require a clear and simple micro-task. A crowdsourcing service only provides a standardized interface for workers. The micro-tasks on this interface 336 31st Pacific Asia Conference on Language, Information and Computation (PACLIC 31), pages 336–345 Cebu City, Philippines, November 16-18, 2017 c Copyright 2017 Kazuaki Hanawa, Akira"
Y17-1045,D12-1057,0,0.0655374,"Missing"
Y17-1045,S10-1006,0,0.0818631,"Missing"
Y17-1045,P14-2062,0,0.0366395,"Missing"
Y17-1045,W10-0702,0,0.13337,"ia articles via crowdsourcing. Wikipedia is the central infrastructure for knowledge curation, as exemplified by Freebase (Bollacker et al., 2008) and Wikification (Mihalcea and Csomai, 2007). Therefore, we base Wikipedia articles for building a corpus with causal relation instances. This work represents a first step toward organizing the causal knowledge in Wikipedia articles covering various topics. Recently, researchers have recognized the value of crowdsourcing services in constructing wideranging language resources at low cost (Brew et al., 2010; Finin et al., 2010; Gormley et al., 2010; Jha et al., 2010; Fort et al., 2011; Kawahara et al., 2014; Lawson et al., 2010; Hovy et al., 2014; Takase et al., 2016). Unfortunately, causal relations cannot be directly annotated by crowdsourcing. For this purpose, non-expert workers on crowdsourcing services require a clear and simple micro-task. A crowdsourcing service only provides a standardized interface for workers. The micro-tasks on this interface 336 31st Pacific Asia Conference on Language, Information and Computation (PACLIC 31), pages 336–345 Cebu City, Philippines, November 16-18, 2017 c Copyright 2017 Kazuaki Hanawa, Akira Sasaki, Naoaki Oka"
Y17-1045,C14-1027,0,0.119846,"dia is the central infrastructure for knowledge curation, as exemplified by Freebase (Bollacker et al., 2008) and Wikification (Mihalcea and Csomai, 2007). Therefore, we base Wikipedia articles for building a corpus with causal relation instances. This work represents a first step toward organizing the causal knowledge in Wikipedia articles covering various topics. Recently, researchers have recognized the value of crowdsourcing services in constructing wideranging language resources at low cost (Brew et al., 2010; Finin et al., 2010; Gormley et al., 2010; Jha et al., 2010; Fort et al., 2011; Kawahara et al., 2014; Lawson et al., 2010; Hovy et al., 2014; Takase et al., 2016). Unfortunately, causal relations cannot be directly annotated by crowdsourcing. For this purpose, non-expert workers on crowdsourcing services require a clear and simple micro-task. A crowdsourcing service only provides a standardized interface for workers. The micro-tasks on this interface 336 31st Pacific Asia Conference on Language, Information and Computation (PACLIC 31), pages 336–345 Cebu City, Philippines, November 16-18, 2017 c Copyright 2017 Kazuaki Hanawa, Akira Sasaki, Naoaki Okazaki and Kentaro Inui are often limited to"
Y17-1045,W10-0712,0,0.356389,"astructure for knowledge curation, as exemplified by Freebase (Bollacker et al., 2008) and Wikification (Mihalcea and Csomai, 2007). Therefore, we base Wikipedia articles for building a corpus with causal relation instances. This work represents a first step toward organizing the causal knowledge in Wikipedia articles covering various topics. Recently, researchers have recognized the value of crowdsourcing services in constructing wideranging language resources at low cost (Brew et al., 2010; Finin et al., 2010; Gormley et al., 2010; Jha et al., 2010; Fort et al., 2011; Kawahara et al., 2014; Lawson et al., 2010; Hovy et al., 2014; Takase et al., 2016). Unfortunately, causal relations cannot be directly annotated by crowdsourcing. For this purpose, non-expert workers on crowdsourcing services require a clear and simple micro-task. A crowdsourcing service only provides a standardized interface for workers. The micro-tasks on this interface 336 31st Pacific Asia Conference on Language, Information and Computation (PACLIC 31), pages 336–345 Cebu City, Philippines, November 16-18, 2017 c Copyright 2017 Kazuaki Hanawa, Akira Sasaki, Naoaki Okazaki and Kentaro Inui are often limited to multiple choice ques"
Y17-1045,W17-0813,0,0.0291906,"limited to causal relations but can be adapted to any bratsupported tasks (e.g., part-of-speech tagging and information extraction). We also present a quality control mechanism that is applicable to any crowdsourcing services accepting free text for a micro-task. Several studies have dedicated to identify causal relations mentioned in text. For instance, Dunietz et al. (2017) present the version 2.0 of Bank of Effects and Causes Stated Explicitly (BECauSE). The corpus includes annotations of causes and effects as well as seven semantic relations that are frequently associated with causation. Rehbein and Ruppenhofer (2017) use the similar annotation scheme for building a German corpus with some changes in the label set and the scope of causality. Built on top of well-established lingustic theories, these studies focus more on “causal language” (expressions of causation) than real-world causation. In contrast, our ultimate goal is acquisition of real-world causal knowledge by exploting Wikipedia as an encyclopedia. We thus design a curation process with crowdworkers involved in, focussing on how humans ‘read’ Wikipedia articles for causal knowledge. 3 Annotating promotion/suppression relations in Wikipedia artic"
Y17-1045,D16-1014,0,0.0315376,"We issued the micro-tasks to crowd workers, and collected 95,008 annotations of causal relation instances among 8,745 summary sentences in 1,494 Wikipedia articles. The annotated corpus not only provides supervision data for automatic recognition of causal relation instances, but also reveals valuable facts for improving the annotation process of this task. 1 Introduction Commonsense knowledge such as entities and events, and their causal relationships, are indispensable in various natural language processing (NLP) applications, including question answering (Oh et al., 2013; Oh et al., 2016; Sharp et al., 2016), hypothesis generation (Radinsky et al., 2012; Hashimoto et al., 2015), stance detection (Sasaki et al., 2016), and literature curation for systems biology (Pyysalo et al., 2015; Rinaldi et al., 2016). In many previous researches, corpora for acquiring causal relations were built by annotating two text spans (e.g., entities) and their relations in the text (Doddington et al., 2004; Hendrickx et al., 2010; Pyysalo et al., 2015; Rinaldi et al., 2016; This paper presents an approach for harnessing causal relation instances to Wikipedia articles via crowdsourcing. Wikipedia is the central infrast"
Y17-1045,E12-2021,0,0.100698,"Missing"
Y17-1045,P16-1215,1,0.811198,"Missing"
Y17-1045,W10-0701,0,\N,Missing
Y18-1001,N18-1032,0,0.0243039,"as been found to be helpful in learning translation models for particular dialects. Several previous studies have investigated the characteristics of translation models for closely-related dialects (Meftouh et al., 2015; Honnet et al., 2018). For example, Honnet et al. (2018) reported that a character-level NMT model trained on one Swiss-German dialect performed moderately well at translating sentences in closely-related dialects. Therefore, given this, we use multilingual NMT (Johnson et al., 2016) to learn parameters that encode knowledge of dialects’ shared lexical and syntactic structure. Gu et al. (2018) demonstrated that multilingual NMT can be useful for lowresource language pairs, while Lakew et al. (2018) found that a multilingual NMT system trained on multiple related languages showed improved zeroshot translation performance. We believe that multilingual NMT can be effective for closely-related dialects, and can compensate for a lack of translation data for the different dialects. Multilingual NMT can also help us to analyze the ¨ characteristics of each language. Ostling and Tiedemann (2016) found that clustering the language embeddings learned by a character-level multilingual system"
Y18-1001,W16-4824,0,0.02542,". This approach utilizes dialect embeddings, namely vector representations of Japanese dialects, to inform the model of the input dialect. An interesting by-product of this approach is that the dialect embeddings the system learns illustrate the difference between different dialect types from different geographical areas. In addition, we present an example of using these dialect embeddings for dialectom1 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 etry (Nerbonne and Kretzschmar, 2011; Kumagai, 2016; Guggilla, 2016; Rama and C¸o¨ ltekin, 2016). Another advantage of adopting a the multilingual architecture for multiple related languages is it can enable the system to acquire knowledge of their lexical and syntactic similarities. For example, Lakew et al. (2018) reported that including several related languages in supervised training data can improve multilingual NMT. Our results confirm the effectiveness of using closely-related languages (namely Japanese dialects) in multilingual NMT. (dialect) source ( = Right, definitely. ) divided into “bunsetsu” input sequence ( = Right) dialect label dialect label"
Y18-1001,P07-2045,0,0.0082992,"Missing"
Y18-1001,W11-2123,0,0.0328063,"MT (w/ labels) Mono SMT Multi SMT (w/o labels) standard-to-dialect Multi NMT (w/ labels) BLEU 35.10 22.45 71.29 69.74 75.66 52.98 73.54 64.04 Table 2: Syllable-level BLEU scores for all models (without translation). We used OpenNMT-py2 with its default hyperparameter settings, except for the number of the training epochs (which we set to 20) and selected the model that performed best on the development set. In addition, we employed Moses3 (Koehn et al., 2007) as the baseline SMT model and set the distortion limit to 0. The standard Japanese language model used in Moses was trained with KenLM (Heafield, 2011). Regarding the dialect label order used for the input, our preliminary experiments indicated that the best models were obtained using input sequence (d) (Table 1) for dialect-to-standard translation and input sequence (c) for standard-to-dialect translation. Finally, we used MeCab 0.9964 to analyze the kanji characters’ pronunciations. 5.2 Multi-Dialect NMT Model Performance Table 2 shows the dialect translation performance of all the models considered, with the first row group comparing their scores for dialect-to-standard translation with different input settings. Mono-lingual vs multi-ling"
Y18-1001,Y15-1004,0,0.016121,"t label (=definitely) Multi-dialect NMT output sequence Attention concatenate (standard JPN) “Hyogo” target 2 Related Work Little dialectal text is available since dialects are generally spoken rather than written. For this reason, many dialect MT researchers work in low-resource settings (Zbib et al., 2012; Scherrer and Ljubeˇsi´c, 2016; Hassan et al., 2017). However, the use of similar dialects has been found to be helpful in learning translation models for particular dialects. Several previous studies have investigated the characteristics of translation models for closely-related dialects (Meftouh et al., 2015; Honnet et al., 2018). For example, Honnet et al. (2018) reported that a character-level NMT model trained on one Swiss-German dialect performed moderately well at translating sentences in closely-related dialects. Therefore, given this, we use multilingual NMT (Johnson et al., 2016) to learn parameters that encode knowledge of dialects’ shared lexical and syntactic structure. Gu et al. (2018) demonstrated that multilingual NMT can be useful for lowresource language pairs, while Lakew et al. (2018) found that a multilingual NMT system trained on multiple related languages showed improved zero"
Y18-1001,L18-1597,0,0.0389463,"Multi-dialect NMT output sequence Attention concatenate (standard JPN) “Hyogo” target 2 Related Work Little dialectal text is available since dialects are generally spoken rather than written. For this reason, many dialect MT researchers work in low-resource settings (Zbib et al., 2012; Scherrer and Ljubeˇsi´c, 2016; Hassan et al., 2017). However, the use of similar dialects has been found to be helpful in learning translation models for particular dialects. Several previous studies have investigated the characteristics of translation models for closely-related dialects (Meftouh et al., 2015; Honnet et al., 2018). For example, Honnet et al. (2018) reported that a character-level NMT model trained on one Swiss-German dialect performed moderately well at translating sentences in closely-related dialects. Therefore, given this, we use multilingual NMT (Johnson et al., 2016) to learn parameters that encode knowledge of dialects’ shared lexical and syntactic structure. Gu et al. (2018) demonstrated that multilingual NMT can be useful for lowresource language pairs, while Lakew et al. (2018) found that a multilingual NMT system trained on multiple related languages showed improved zeroshot translation perfo"
Y18-1001,P12-2059,0,0.0245438,"cteristics of each language. Ostling and Tiedemann (2016) found that clustering the language embeddings learned by a character-level multilingual system provided an illustration of the language families involved. In the light of this, we also analyze our dialect embeddings to investigate whether our Figure 1: Proposed multi-dialect NMT model. multi-dialect model can capture similarities between dialects (Section 5). Previous work reported that character-level statistical machine translation (SMT) using words as translation units was effective for translating between closely-related languages (Nakov and Tiedemann, 2012; Scherrer and Ljubeˇsi´c, 2016). There are two reasons for this: the character-level information enables the system to exploit lexical overlaps, while using words as translation units takes advantage of related languages’ syntactic overlaps. In this study, we present a method of translating between Japanese dialects that combines three ideas: multilingual NMT, character-level NMT, and the use of base phrases (i.e., bunsetsu) as translation units. We believe this enables our approach to fully exploit the similarities among dialects and standard Japanese, even in low-resource settings. 3 Data:"
Y18-1001,P17-4012,0,0.0321359,"nerally written in a mix of kanji and kana; therefore, we converted the kanji in the sentences into kana and, then, segmented them into bunsetsu.1 After preprocessing, the average sentence lengths were 14.62 and 15.57 characters for the dialects and standard Japanese, respectively, and the average number of bunsetsus per sentence was 3.42. 4 NMT Model Figure 1 gives an overview of our multi-dialect NMT system’s network structure. Since our focus is on examining the effectiveness of multi-dialect NMT and its detailed behavior, rather than on creating a novel translation model, we used OpenNMT (Klein et al., 2017), a stacking LSTM encoder–decoder model with a multilingual extension similar to that of Johnson et al. (2016). However, to improve its direct translation accuracy, we make the following three modifications. Dialect labels Following a previous multilingual NMT study (Johnson et al., 2016), we train a unified model that handled all 48 dialects simultaneously 1 This is the smallest Japanese phrase unit, containing a single content word and attached postpositions. ID a b c d Encoder input order source label, sentence target label, sentence source label, target label, sentence source label, senten"
Y18-1001,W16-4803,0,0.0668626,"Missing"
Y18-1001,Y18-1000,0,0.271859,"etween different dialect types from different geographical areas. In addition, we present an example of using these dialect embeddings for dialectom1 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 etry (Nerbonne and Kretzschmar, 2011; Kumagai, 2016; Guggilla, 2016; Rama and C¸o¨ ltekin, 2016). Another advantage of adopting a the multilingual architecture for multiple related languages is it can enable the system to acquire knowledge of their lexical and syntactic similarities. For example, Lakew et al. (2018) reported that including several related languages in supervised training data can improve multilingual NMT. Our results confirm the effectiveness of using closely-related languages (namely Japanese dialects) in multilingual NMT. (dialect) source ( = Right, definitely. ) divided into “bunsetsu” input sequence ( = Right) dialect label dialect label (=definitely) Multi-dialect NMT output sequence Attention concatenate (standard JPN) “Hyogo” target 2 Related Work Little dialectal text is available since dialects are generally spoken rather than written. For this reason, many dialect MT researcher"
Y18-1001,N12-1006,0,0.0281621,"related languages in supervised training data can improve multilingual NMT. Our results confirm the effectiveness of using closely-related languages (namely Japanese dialects) in multilingual NMT. (dialect) source ( = Right, definitely. ) divided into “bunsetsu” input sequence ( = Right) dialect label dialect label (=definitely) Multi-dialect NMT output sequence Attention concatenate (standard JPN) “Hyogo” target 2 Related Work Little dialectal text is available since dialects are generally spoken rather than written. For this reason, many dialect MT researchers work in low-resource settings (Zbib et al., 2012; Scherrer and Ljubeˇsi´c, 2016; Hassan et al., 2017). However, the use of similar dialects has been found to be helpful in learning translation models for particular dialects. Several previous studies have investigated the characteristics of translation models for closely-related dialects (Meftouh et al., 2015; Honnet et al., 2018). For example, Honnet et al. (2018) reported that a character-level NMT model trained on one Swiss-German dialect performed moderately well at translating sentences in closely-related dialects. Therefore, given this, we use multilingual NMT (Johnson et al., 2016) to"
Y18-1001,D16-1163,0,0.0219347,"h the same model trained via the standard approach of using entire sentences as input/output sequences (Multi NMT-sentence) shows that Multi NMT outperforms Multi NMTsentence by 5.92 points. One disadvantage of chunk-wise translation is it cannot capture the context beyond each chunk’s boundary. However, despite this disadvantage, our Multi NMT model was still able to outperform a model that had access to a broader context (Multi NMT-sentence), indicating that our fixed-order translation approach is suitable for translating Japanese dialects despite its limited context sensitivity. NMT vs SMT Zoph et al. (2016) found that SMT models largely outperformed state-of-the-art NMT models for low-resource languages. Therefore, for comparison, the second row group in Table 2 shows results for a fixed-order character-based SMT baseline. In these experiments, even though the NMT model trained using a single dialect (Mono NMT) gave the poorest performance, the one with dialect labels outperformed the baseline Multi SMT model, achieving the best performance overall. 5.3 Example Translation Results To demonstrate how each of the proposed components contributed to generating accurate translations, we now present s"
