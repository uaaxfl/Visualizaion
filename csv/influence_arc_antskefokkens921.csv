2019.gwc-1.12,I17-1001,0,0.0535863,"Missing"
2019.gwc-1.12,W18-5449,0,0.130126,"r the property. Whether this evidence is sufficient for a distributional model to represent the property is an open question. 4 A Dataset of Concepts and Properties This section describes the design of our dataset. We first outline the experiments we envision, because they provide the motivation of some of the key properties of our dataset. To conduct experiments on whether the predictions introduced in Section 3 hold, we plan to use approaches suggested in the field of investigating neural network representations, such as diagnostic classification (Belinkov et al., 2017; Hupkes et al., 2018; Derby et al., 2018). In particular, we plan to extend the experiments presented in (accessed 2019/09/30) Sommerauer and Fokkens (2018), which try to investigate whether dimensions of embedding representations can capture semantic properties. While this seems to be implied by the method of inferring the missing word in an analogy pair by means of vector subtraction and addition (Mikolov et al., 2013; Levy and Goldberg, 2014), analogy calculation methods have been heavily criticized, calling this notion into question (Linzen, 2016; Gladkova and Drozd, 2016; Gladkova et al., 2016). To shed light on this, we propose"
2019.gwc-1.12,W15-0107,0,0.113611,"es of a property. In addition to proposing a dataset design, we offer specific hypotheses based on a variety of observations from different fields about information that is likely or unlikely to be expressed in English natural language corpora. Rather than making claims based on entire categories of semantic properties, we base our predictions on underlying factors involved in the relations between concepts and properties. By testing these hypotheses, we hope to go beyond insights from experimental approaches comparing the information captured in embeddings to semantic feature norm sets (e.g. Fagarasan et al. (2015), Herbelot and Vecchi (2015), Tsvetkov et al. (2015), Derby et al. (2018), Sommerauer and Fokkens (2018)). Finally, we hope that comparing the relations captured by our dataset to traditional, taxonomic categories represented in WordNet may yield insights about the relation between properties of concepts and categorization. This could be extended to other languages to enable crosslinguistic comparisons. Acknowledgments This research is funded by the PhD in the Humanities Grant provided by the Netherlands Organization of Scientific Research (Nederlandse Organisatie voor Wetenschappelijk Onderzo"
2019.gwc-1.12,W16-2507,0,0.0141945,"tic classification (Belinkov et al., 2017; Hupkes et al., 2018; Derby et al., 2018). In particular, we plan to extend the experiments presented in (accessed 2019/09/30) Sommerauer and Fokkens (2018), which try to investigate whether dimensions of embedding representations can capture semantic properties. While this seems to be implied by the method of inferring the missing word in an analogy pair by means of vector subtraction and addition (Mikolov et al., 2013; Levy and Goldberg, 2014), analogy calculation methods have been heavily criticized, calling this notion into question (Linzen, 2016; Gladkova and Drozd, 2016; Gladkova et al., 2016). To shed light on this, we proposed an experimental set-up in which we tested whether a supervised machine learning system could successfully learn to distinguish vectors of words clearly associated with a property from vectors of words which are clearly not associated with the property. Any supervised classification approach relies on finding regularities which are shared among all or most examples of a particular class and distinguish them from other classes. Therefore, the distribution of positive and negative examples of properties is crucial to ensure that the vec"
2019.gwc-1.12,N16-2002,0,0.0129859,"ov et al., 2017; Hupkes et al., 2018; Derby et al., 2018). In particular, we plan to extend the experiments presented in (accessed 2019/09/30) Sommerauer and Fokkens (2018), which try to investigate whether dimensions of embedding representations can capture semantic properties. While this seems to be implied by the method of inferring the missing word in an analogy pair by means of vector subtraction and addition (Mikolov et al., 2013; Levy and Goldberg, 2014), analogy calculation methods have been heavily criticized, calling this notion into question (Linzen, 2016; Gladkova and Drozd, 2016; Gladkova et al., 2016). To shed light on this, we proposed an experimental set-up in which we tested whether a supervised machine learning system could successfully learn to distinguish vectors of words clearly associated with a property from vectors of words which are clearly not associated with the property. Any supervised classification approach relies on finding regularities which are shared among all or most examples of a particular class and distinguish them from other classes. Therefore, the distribution of positive and negative examples of properties is crucial to ensure that the vector dimensions discovere"
2019.gwc-1.12,D15-1003,0,0.0993304,"tion to proposing a dataset design, we offer specific hypotheses based on a variety of observations from different fields about information that is likely or unlikely to be expressed in English natural language corpora. Rather than making claims based on entire categories of semantic properties, we base our predictions on underlying factors involved in the relations between concepts and properties. By testing these hypotheses, we hope to go beyond insights from experimental approaches comparing the information captured in embeddings to semantic feature norm sets (e.g. Fagarasan et al. (2015), Herbelot and Vecchi (2015), Tsvetkov et al. (2015), Derby et al. (2018), Sommerauer and Fokkens (2018)). Finally, we hope that comparing the relations captured by our dataset to traditional, taxonomic categories represented in WordNet may yield insights about the relation between properties of concepts and categorization. This could be extended to other languages to enable crosslinguistic comparisons. Acknowledgments This research is funded by the PhD in the Humanities Grant provided by the Netherlands Organization of Scientific Research (Nederlandse Organisatie voor Wetenschappelijk Onderzoek, NWO) PGW.17.041 awarded"
2019.gwc-1.12,W14-1618,0,0.015348,"in Section 3 hold, we plan to use approaches suggested in the field of investigating neural network representations, such as diagnostic classification (Belinkov et al., 2017; Hupkes et al., 2018; Derby et al., 2018). In particular, we plan to extend the experiments presented in (accessed 2019/09/30) Sommerauer and Fokkens (2018), which try to investigate whether dimensions of embedding representations can capture semantic properties. While this seems to be implied by the method of inferring the missing word in an analogy pair by means of vector subtraction and addition (Mikolov et al., 2013; Levy and Goldberg, 2014), analogy calculation methods have been heavily criticized, calling this notion into question (Linzen, 2016; Gladkova and Drozd, 2016; Gladkova et al., 2016). To shed light on this, we proposed an experimental set-up in which we tested whether a supervised machine learning system could successfully learn to distinguish vectors of words clearly associated with a property from vectors of words which are clearly not associated with the property. Any supervised classification approach relies on finding regularities which are shared among all or most examples of a particular class and distinguish t"
2019.gwc-1.12,Q15-1016,0,0.0234751,"y we are confident about (i.e. we do not include concepts returned by a search for a category containing ‘mixed’ examples). This results in a selection of candidate concepts which are very difficult to separate into positive and negative examples based on general similarity. We collect the 200 nearest neighbors of this approximate property representation. We exclude negative examples further away from the centroid than the furthest positive example by manual inspection. The embedding model used in this step is the skip-gram model with negative sampling (using recommended settings according to Levy et al. (2015)), trained on the full Wikipedia corpus (dump from August 2018). 4.4 Sampling for the Crowd The strategies outlined above result in rather large numbers of candidates not all of which are useful (e.g. the distributional model returns non-standard spelling variants and words other than nouns). We reduce and clean the resulting sets (1) by means of preprocessing and (2) sampling based on characteristics with potential impact on how well distributional data can represent information. The characteristics we consider are (1) different types of ambiguity, (2) psycholinguistic factors such as concret"
2019.gwc-1.12,W16-2503,0,0.0135416,"uch as diagnostic classification (Belinkov et al., 2017; Hupkes et al., 2018; Derby et al., 2018). In particular, we plan to extend the experiments presented in (accessed 2019/09/30) Sommerauer and Fokkens (2018), which try to investigate whether dimensions of embedding representations can capture semantic properties. While this seems to be implied by the method of inferring the missing word in an analogy pair by means of vector subtraction and addition (Mikolov et al., 2013; Levy and Goldberg, 2014), analogy calculation methods have been heavily criticized, calling this notion into question (Linzen, 2016; Gladkova and Drozd, 2016; Gladkova et al., 2016). To shed light on this, we proposed an experimental set-up in which we tested whether a supervised machine learning system could successfully learn to distinguish vectors of words clearly associated with a property from vectors of words which are clearly not associated with the property. Any supervised classification approach relies on finding regularities which are shared among all or most examples of a particular class and distinguish them from other classes. Therefore, the distribution of positive and negative examples of properties is cruc"
2019.gwc-1.12,N13-1090,0,0.0780938,"predictions introduced in Section 3 hold, we plan to use approaches suggested in the field of investigating neural network representations, such as diagnostic classification (Belinkov et al., 2017; Hupkes et al., 2018; Derby et al., 2018). In particular, we plan to extend the experiments presented in (accessed 2019/09/30) Sommerauer and Fokkens (2018), which try to investigate whether dimensions of embedding representations can capture semantic properties. While this seems to be implied by the method of inferring the missing word in an analogy pair by means of vector subtraction and addition (Mikolov et al., 2013; Levy and Goldberg, 2014), analogy calculation methods have been heavily criticized, calling this notion into question (Linzen, 2016; Gladkova and Drozd, 2016; Gladkova et al., 2016). To shed light on this, we proposed an experimental set-up in which we tested whether a supervised machine learning system could successfully learn to distinguish vectors of words clearly associated with a property from vectors of words which are clearly not associated with the property. Any supervised classification approach relies on finding regularities which are shared among all or most examples of a particul"
2019.gwc-1.12,W18-5430,1,0.911963,"is an open question. 4 A Dataset of Concepts and Properties This section describes the design of our dataset. We first outline the experiments we envision, because they provide the motivation of some of the key properties of our dataset. To conduct experiments on whether the predictions introduced in Section 3 hold, we plan to use approaches suggested in the field of investigating neural network representations, such as diagnostic classification (Belinkov et al., 2017; Hupkes et al., 2018; Derby et al., 2018). In particular, we plan to extend the experiments presented in (accessed 2019/09/30) Sommerauer and Fokkens (2018), which try to investigate whether dimensions of embedding representations can capture semantic properties. While this seems to be implied by the method of inferring the missing word in an analogy pair by means of vector subtraction and addition (Mikolov et al., 2013; Levy and Goldberg, 2014), analogy calculation methods have been heavily criticized, calling this notion into question (Linzen, 2016; Gladkova and Drozd, 2016; Gladkova et al., 2016). To shed light on this, we proposed an experimental set-up in which we tested whether a supervised machine learning system could successfully learn t"
2019.gwc-1.12,speer-havasi-2012-representing,0,0.30742,"Missing"
2019.gwc-1.12,D15-1243,0,0.0916102,"design, we offer specific hypotheses based on a variety of observations from different fields about information that is likely or unlikely to be expressed in English natural language corpora. Rather than making claims based on entire categories of semantic properties, we base our predictions on underlying factors involved in the relations between concepts and properties. By testing these hypotheses, we hope to go beyond insights from experimental approaches comparing the information captured in embeddings to semantic feature norm sets (e.g. Fagarasan et al. (2015), Herbelot and Vecchi (2015), Tsvetkov et al. (2015), Derby et al. (2018), Sommerauer and Fokkens (2018)). Finally, we hope that comparing the relations captured by our dataset to traditional, taxonomic categories represented in WordNet may yield insights about the relation between properties of concepts and categorization. This could be extended to other languages to enable crosslinguistic comparisons. Acknowledgments This research is funded by the PhD in the Humanities Grant provided by the Netherlands Organization of Scientific Research (Nederlandse Organisatie voor Wetenschappelijk Onderzoek, NWO) PGW.17.041 awarded to Pia Sommerauer and NW"
2020.coling-main.422,J08-4004,0,0.4425,"ions. It includes more fine-grained semantic judgments and intentionally ambiguous words. We can thus expect even more disagreement than already observed in Herbelot and Vecchi (2016). Despite the central nature of phenomena triggering disagreement in annotation tasks, we are not aware of evaluation methods that do not mainly rely on agreement. Traditionally, annotations by a few annotators who worked on the same units are evaluated in terms of Kappa scores (usually Cohens’s kappa) and tasks with varying workers annotating the same units (usually crowd tasks) in terms of Krippendorff’s alpha (Artstein and Poesio, 2008). The CrowdTruth framework suggested in Aroyo and Welty (2014) and Aroyo and Welty (2015) offers a more fine-grained view by distinguishing the levels of workers, units and labels, rather than reducing the entire task to a single score. The goal is to distinguish meaningful disagreements (i.e. agreements by reliable annotators) from noise (i.e. disagreement or agreement by generally unreliable annotators). The framework provides scores for workers, annotation units (clear units receive a high score, units triggering disagreement between reliable annotators a low score), labels and associations"
2020.coling-main.422,Q19-1004,0,0.065104,"es and can thus provide highly relevant semantic information. Existing evaluation and label extraction methods, however, still heavily rely on agreement between annotators, which implies a single correct interpretation. Finished datasets rarely provide indications about difficulty and ambiguity on the level of annotated units. The explanatory power of NLP experiments that aim to evaluate or analyze models depends on the informativeness of the data. This is particularly relevant for experiments which specifically aim to understand models better, such as the tradition of diagnostic experiments (Belinkov and Glass, 2019).Traditional error analyses could also benefit substantially from test sets which contain information about phenomena with a likely impact on model performance. Furthermore, knowing whether model-errors are similar to human disagreements can yield important insights about models. For instance, an analysis of natural language inference models shows that classifiers do not necessarily capture the same type of This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 4798 Proceedings of the 28th Internation"
2020.coling-main.422,W15-1510,0,0.0305681,"n the quality and informativeness of the underlying data (Hupkes et al., 2018). In this paper, we present an approach to crowd-annotation for a diagnostic dataset which attempts to tackle these limitations. The dataset is meant to test which semantic properties are captured by distributional word representations. The task is designed to trigger fine-grained semantic judgements of potentially ambiguous examples. The behavior of ambiguous words in distributional semantic models is not well understood and thus particularly interesting (Sommerauer and Fokkens, 2018; Yaghoobzadeh et al., 2019; Del Tredici and Bel, 2015). We investigate to what extent existing and new quality metrics indicate annotation accuracy on the one hand and ambiguity and difficulty of annotation units on the other hand. We evaluate our task from three perspectives: (1) comparison against an expert-annotated gold standard, (2) a task-specific coherence metric independent of agreement and (3) evaluation in terms of inter-annotator agreement metrics compared to predefined expectations about agreement and disagreement. In particular, we aim to investigate (1) how we can exploit the strengths and weaknesses of various suggested metrics to"
2020.coling-main.422,N19-1224,0,0.329377,"r for cases where we expect worker agreement. The remainder of this paper is structured as follows: After reviewing related work (Section 2), we introduce the use-case of a diagnostic dataset (Section 3) and describe the annotation task (Section 4). We present our expert-annotated gold standard in Section 5 and different quality metrics in Section 6. The results of our experiments are described in Section 7, followed by a discussion and conclusion. 2 Related work Recent annotation studies recognize that ambiguity, vagueness and varying degrees of difficulty are inherent to semantic phenomena (Dumitrache et al., 2019; Aroyo and Welty, 2015; Erk et al., 2003; Kairam and Heer, 2016; Poesio et al., 2019; Pavlick and Kwiatkowski, 2019). Pavlick and Kwiatkowski (2019) demonstrate that the fundamental task of Natural Language Inferencing contains large proportions of instances with multiple valid interpretations and argue that this phenomenon is central to the task rather than an aspect which can be disregarded. Herbelot and Vecchi (2016) show that even experts disagree on a difficult semantic annotation task and that interpretations are likely to vary due to differences in conceptualizations, which are in them"
2020.coling-main.422,P03-1068,0,0.232296,"Missing"
2020.coling-main.422,Q19-1043,0,0.396066,"ermore, knowing whether model-errors are similar to human disagreements can yield important insights about models. For instance, an analysis of natural language inference models shows that classifiers do not necessarily capture the same type of This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 4798 Proceedings of the 28th International Conference on Computational Linguistics, pages 4798–4809 Barcelona, Spain (Online), December 8-13, 2020 ambiguity and uncertainty as reflected in the annotations (Pavlick and Kwiatkowski, 2019). Error analysis often require manual annotation and tend to focus on small and not representative subsections of test sets (Wu et al., 2019). We argue that the behavior of human annotators can provide rich information which should be exploited, rather than reduced to single labels. Information about (dis)agreement is a by-product of the original annotation effort and thus comes for free. It can form the basis of an error analysis or, in the case of our data, should be used to draw informative conclusions from diagnostic experiments. Such experiments crucially depend on the quality and informa"
2020.coling-main.422,N19-1176,0,0.0285436,"s follows: After reviewing related work (Section 2), we introduce the use-case of a diagnostic dataset (Section 3) and describe the annotation task (Section 4). We present our expert-annotated gold standard in Section 5 and different quality metrics in Section 6. The results of our experiments are described in Section 7, followed by a discussion and conclusion. 2 Related work Recent annotation studies recognize that ambiguity, vagueness and varying degrees of difficulty are inherent to semantic phenomena (Dumitrache et al., 2019; Aroyo and Welty, 2015; Erk et al., 2003; Kairam and Heer, 2016; Poesio et al., 2019; Pavlick and Kwiatkowski, 2019). Pavlick and Kwiatkowski (2019) demonstrate that the fundamental task of Natural Language Inferencing contains large proportions of instances with multiple valid interpretations and argue that this phenomenon is central to the task rather than an aspect which can be disregarded. Herbelot and Vecchi (2016) show that even experts disagree on a difficult semantic annotation task and that interpretations are likely to vary due to differences in conceptualizations, which are in themselves justified and cannot simply be disregarded as ‘mistakes’. Information about am"
2020.coling-main.422,N16-3012,0,0.0268404,"ple of things which are light’. I know that having (an/an) blade is necessary for many things (a/an) razor does or is used for. You can find (a/an) t-shirt which is white. White is one of many possible colors (a/an) t-shirt usually has. The range of colors is almost unlimited. I think (a/an) wine glass can is made of plastic, but this is rare or uncommon. I think it is impossible for (a/an) corpse to be alive. typical of property affording activity variability open rare impossible Table 1: Examples of statements expressing semantic relations. . We used the freely available Lingoturk software (Pusse et al., 2016) to set up an annotation environment and distributed the task via the recruitment platform Prolific.3 Peer et al. (2017) show that the annotation quality of annotators recruited via Prolific is higher than for Amazon Mechanical Turk workers. The platform encourages fair payment and asks researchers to pay participants based on the time they estimate for a task rather than per annotated item. We split the dataset into batches of around 70 descriptions. A worker who is proficient in English would need about 10 minutes per batch. While some statements may be difficult to judge and therefore take"
2020.coling-main.422,W18-5430,1,0.911144,"m diagnostic experiments. Such experiments crucially depend on the quality and informativeness of the underlying data (Hupkes et al., 2018). In this paper, we present an approach to crowd-annotation for a diagnostic dataset which attempts to tackle these limitations. The dataset is meant to test which semantic properties are captured by distributional word representations. The task is designed to trigger fine-grained semantic judgements of potentially ambiguous examples. The behavior of ambiguous words in distributional semantic models is not well understood and thus particularly interesting (Sommerauer and Fokkens, 2018; Yaghoobzadeh et al., 2019; Del Tredici and Bel, 2015). We investigate to what extent existing and new quality metrics indicate annotation accuracy on the one hand and ambiguity and difficulty of annotation units on the other hand. We evaluate our task from three perspectives: (1) comparison against an expert-annotated gold standard, (2) a task-specific coherence metric independent of agreement and (3) evaluation in terms of inter-annotator agreement metrics compared to predefined expectations about agreement and disagreement. In particular, we aim to investigate (1) how we can exploit the st"
2020.coling-main.422,2019.gwc-1.12,1,0.917628,"Approaches which aim to capture information about semantics (such as embedding analysis (Sommerauer and Fokkens, 2018; Yaghoobzadeh et al., 2019)), however, are much more complex as ambiguity, vagueness and differences in required knowledge are by no means marginal phenomena and cannot simply be disregarded. Furthermore, the role of ambiguity in the behavior of word embeddings is not fully understood yet (Del Tredici and Bel, 2015; Yaghoobzadeh et al., 2019). We have designed an annotation task to analyze how different aspects of word meaning are represented in distributional representations (Sommerauer et al., 2019). In this paper, we investigate how we can measure the quality of the annotations and capture valid disagreement, which is crucial information for the diagnostic experiments we want to conduct (Sommerauer, 2020). The task is similar to that of Herbelot and Vecchi (2016), but uses basic yes-no questions so that it is suitable for crowd-annotations. It includes more fine-grained semantic judgments and intentionally ambiguous words. We can thus expect even more disagreement than already observed in Herbelot and Vecchi (2016). Despite the central nature of phenomena triggering disagreement in anno"
2020.coling-main.422,2020.acl-srw.18,1,0.707159,"in required knowledge are by no means marginal phenomena and cannot simply be disregarded. Furthermore, the role of ambiguity in the behavior of word embeddings is not fully understood yet (Del Tredici and Bel, 2015; Yaghoobzadeh et al., 2019). We have designed an annotation task to analyze how different aspects of word meaning are represented in distributional representations (Sommerauer et al., 2019). In this paper, we investigate how we can measure the quality of the annotations and capture valid disagreement, which is crucial information for the diagnostic experiments we want to conduct (Sommerauer, 2020). The task is similar to that of Herbelot and Vecchi (2016), but uses basic yes-no questions so that it is suitable for crowd-annotations. It includes more fine-grained semantic judgments and intentionally ambiguous words. We can thus expect even more disagreement than already observed in Herbelot and Vecchi (2016). Despite the central nature of phenomena triggering disagreement in annotation tasks, we are not aware of evaluation methods that do not mainly rely on agreement. Traditionally, annotations by a few annotators who worked on the same units are evaluated in terms of Kappa scores (usua"
2020.coling-main.422,P19-1073,0,0.0252741,"guage inference models shows that classifiers do not necessarily capture the same type of This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 4798 Proceedings of the 28th International Conference on Computational Linguistics, pages 4798–4809 Barcelona, Spain (Online), December 8-13, 2020 ambiguity and uncertainty as reflected in the annotations (Pavlick and Kwiatkowski, 2019). Error analysis often require manual annotation and tend to focus on small and not representative subsections of test sets (Wu et al., 2019). We argue that the behavior of human annotators can provide rich information which should be exploited, rather than reduced to single labels. Information about (dis)agreement is a by-product of the original annotation effort and thus comes for free. It can form the basis of an error analysis or, in the case of our data, should be used to draw informative conclusions from diagnostic experiments. Such experiments crucially depend on the quality and informativeness of the underlying data (Hupkes et al., 2018). In this paper, we present an approach to crowd-annotation for a diagnostic dataset whi"
2020.coling-main.422,P19-1574,0,0.0260658,"Missing"
2020.framenet-1.5,L16-1141,0,0.0222278,"Type Save Frame Element Save Reference Annotation 4.1. Resources In this subsection, we introduce the resources used in the annotation tool, i.e., the lexicon and the data. lexicon We make use of the canonical version 1.7 of FrameNet (Fillmore and Baker, 2010; Ruppenhofer et al., 2006). All annotations make use of a Resource Description Framework (RDF) of FrameNet, for which the two most common resources are Framester (Gangemi et al., 2016) Save Frame Annotation 2 https://getbootstrap.com/docs/3.4/css/ https://jquery.com/ 4 https://nodejs.org/en/ 3 Figure 4: Annotation workflow 35 and PreMOn (Corcoglioniti et al., 2016). We chose to use PreMOn since the project was more active.5 data acquisition We have developed a data architecture (Vossen et al., 2020) to obtain and represent the data according to the data model as presented in Subsection 3.2., for which we primarily make use of Wikidata (Vrandeˇci´c and Kr¨otzsch, 2014). preprocessing spaCy6 is used for sentence splitting, tokenization, and part of speech tagging, for which models in English, Dutch, and Italian are used. The preprocessing is stored in the NLP Annotation Format (NAF) (Fokkens et al., 2014), a stand-off, multilayered annotation schema for r"
2020.framenet-1.5,W11-1506,1,0.764453,"presidential election, with structured information about the location, time, and participants of the incident. 2. each incident to be tagged with one or more event types, e.g., election. This makes it possible to generalize over incidents of the same type to learn which frames are typical. 3. different texts that make reference to the same incident, possibly written in multiple languages with varying document creation times and from different sources, which provides us with insights into cross-lingual differences, source perspectives and the impact of historical distance to the incident time (Cybulska and Vossen, 2011). 4. an environment for efficient and consistent FrameNet and reference annotation to (given) structured data. This makes it possible to consider the framing of the incident throughout all texts that make reference to it as a discourse unit. In this paper, we introduce an annotation tool in which both structured data about an incident and many reference texts describing that one incident are simultaneously presented to the user. This interface enables both FrameNet-based annoWe construct narratives to describe events in the world around us. The language that we use in those narratives forms a"
2020.framenet-1.5,W16-4011,0,0.0530062,"Missing"
2020.framenet-1.5,burchardt-etal-2006-salto,0,0.0880518,"ic set of frame elements that apply to the syntactic realization of the phrases dominated by the frame. We refer to Example (1). (1) FrameNet annotation tools Kidnapping [P ERPETRATOR Two men] kidnapped [V ICTIM the children] [T IME yesterday]. In this example, ‘kidnapped’ evokes Kidnapping, which consists of several frame elements. ‘Two men’ expresses 1 We chose to not use the OntoLex (McCrae et al., 2017) relationship ontolex:reference since it might lead to confusion in distinguishing between conceptual and referential relationships. 32 straint rules in order to speed up annotation. Salto (Burchardt et al., 2006) is a multi-level annotation tool, which can be used to annotate FrameNet information. The annotation starts with a syntactic analysis of a sentence. After determining the target word and labeling it with a frame, the constituents can be tagged with a frame element by means of drag and drop functionality. All four described annotation tools provide the functionality to annotate FrameNet information. All of them start with a syntactic analysis of the sentence and annotate FrameNet information on top of that analysis. They differ in what syntactic information is used and how this drives the anno"
2020.framenet-1.5,2020.framenet-1.3,1,0.862518,"Missing"
2020.framenet-1.5,S10-1008,0,0.0421944,", meaning that these frame elements are analyzed within sentence boundaries. When their absence is assumed to be bounded by sentenceexternal words or phrases, this information is not further specified. The downside of this approach is that we do not gain insight into the way that these core frame elements are linguistically encoded in the full discourse or if they are encoded at all. Certain approaches address this problem by going beyond the predicate scope in annotating unexpressed core frame elements. For instance, in SemEval2010 Task 10: Linking Events and Their Participants in Discourse (Ruppenhofer et al., 2010), unexpressed core frame elements were annotated outside of the scope of the predicate in order to gain insight into the referents of these unexpressed roles. A small number of texts from a work of Arthur Conan Doyle were annotated. There were three Variation of framing in FrameNet Within FrameNet, variation in framing can be observed by measuring the degree to which different subframes stand in a similar frame-to-frame relation to a superframe. See a classic example below. (2) Change of leadership a. Commerce sell [T IME Yesterday,] [S ELLER John] sold [B UYER Mary] [G OODS a book]. b. Commer"
2020.framenet-1.5,L18-1480,1,0.733809,"Missing"
2020.framenet-1.5,2020.lrec-1.387,1,0.878936,"as already observed by Fillmore, evidenced by the following quote: “since FrameNet has been working mainly on single sentences and has done nothing (yet) on connections within whole texts, the FrameNet database has nothing direct to offer.” (Andor, 2010, p.168) We aim at combining FrameNet annotations with referential annotations in order to analyze framing variation in texts describing an event. For this we need to extend FrameNet annotations to the discourse level. Following the data-to-text method described in Vossen et al. (2018), we make use of the data acquisition platform described in Vossen et al. (2020) to enable this type of research, for which we require: 1. a referential representation of an incident, i.e., an event instance such as the 2012 Slovenian presidential election, with structured information about the location, time, and participants of the incident. 2. each incident to be tagged with one or more event types, e.g., election. This makes it possible to generalize over incidents of the same type to learn which frames are typical. 3. different texts that make reference to the same incident, possibly written in multiple languages with varying document creation times and from differen"
2020.framenet-1.5,L16-1601,0,0.0543084,"Missing"
2020.framenet-1.5,L16-1688,0,0.0232612,"components of the real-world event. English FrameNet (Ruppenhofer et al., 2006) brought conceptual framing research to a computational setting. The English lexicon made it possible to gain insight into the relationship between lexical items and the semantic frames that they evoke. English FrameNet has also motivated researchers to create FrameNets in other languages such as in Japanese (Ohara et al., 2004), German (Burchardt et al., 2009), Swedish (Heppin and Gronostaj, 2012), Brazilian Portuguese (Laviola et al., 2017), Spanish (Subirats and Sato, 2003), French (Djemaa et al., 2016), Hebrew (Hayoun and Elhadad, 2016), and Latvian (Gruzitis et al., 2018). Multiple annotation efforts resulted in many corpora and also served as training, development, and test data to train 31 tations as well as referential linking to the incident that the reference texts make reference to. The analysis of conceptual and referential framing enriches research into variation in framing beyond the level of sentences and across different types of reference texts and languages. This paper is structured as follows. In Section 2., we introduce English FrameNet and the related work on frame annotation, followed by a discussion on com"
2020.framenet-1.5,heppin-gronostaj-2012-rocky,0,0.0213697,"ntial information. To perform such an analysis, we need both semantic resources to describe this conceptual information, and information about the components of the real-world event. English FrameNet (Ruppenhofer et al., 2006) brought conceptual framing research to a computational setting. The English lexicon made it possible to gain insight into the relationship between lexical items and the semantic frames that they evoke. English FrameNet has also motivated researchers to create FrameNets in other languages such as in Japanese (Ohara et al., 2004), German (Burchardt et al., 2009), Swedish (Heppin and Gronostaj, 2012), Brazilian Portuguese (Laviola et al., 2017), Spanish (Subirats and Sato, 2003), French (Djemaa et al., 2016), Hebrew (Hayoun and Elhadad, 2016), and Latvian (Gruzitis et al., 2018). Multiple annotation efforts resulted in many corpora and also served as training, development, and test data to train 31 tations as well as referential linking to the incident that the reference texts make reference to. The analysis of conceptual and referential framing enriches research into variation in framing beyond the level of sentences and across different types of reference texts and languages. This paper"
2020.lrec-1.387,cybulska-vossen-2010-event,1,0.716027,"compare specific reference texts grounded to the same incident for framing differences, i.e. which frames are used from the potential bag-of-frames and which are not. The former will help us to disambiguate expressions for the frames that they evoke, while the latter will learn the vocabulary for framing and show how much variation there is to frame the same incident across sources. For example, occurrences of the word fire in texts that refer to murder incidents are more likely to evoke the fn:Shoot projectiles frame than the fn:Firing frame for employment relations. Similarly as observed by Cybulska and Vossen (2010), an incident such as the fall of Srebrenica can be described as a violent conflict with shootings and transport of women and children, focusing on the reporting, or as deportation and genocide, focusing on the intention and the responsibility. There are two crucial questions to address for this to work: 1) at what level of abstraction do we need to aggregate text such that we maximize the volume of text that still exhibits coherent typical frames? 2) at what level of granularity do we need to aggregate text such that we maximize the volume of text and still obtain coherent temporal sequences"
2020.lrec-1.387,W13-1202,1,0.873194,"Missing"
2020.lrec-1.387,fokkens-etal-2017-grasp,1,0.819802,"Missing"
2020.lrec-1.387,E17-1045,0,0.102837,"Missing"
2020.lrec-1.387,C16-1112,1,0.843176,"erent ways, which often reflects different perspectives. Although there are many corpora capturing language, hardly any of these also represent the actual situations that texts refer to, let alone provide indications of which texts refer to the same situation. Event coreference corpora could serve this purpose as they are annotated for mentions of the same event. However, available event coreference corpora are very small and they exhibit hardly any ambiguity, i.e. there typically is one referent for each expression, nor variation, i.e. there are only one or few expressions for each referent (Ilievski et al., 2016; Postma et al., 2016). Not having sufficient texts that refer to the same or similar situations, or not knowing which texts do, makes it difficult to investigate the different ways in which people make reference. It also hampers the development of systems to automatically resolve (cross-document) coreference and to understand and develop technology that detects how events are framed. Imagine you want to create a text corpus that represents the language used to describe murders. How to proceed? You can use public corpora such as the Gigaword corpus (Napoles et al., 2012) and search for texts u"
2020.lrec-1.387,W16-6004,1,0.899937,"Missing"
2020.lrec-1.387,W15-4507,1,0.872197,"Missing"
2020.lrec-1.387,L18-1480,1,0.577993,"search for texts using keywords. How many murders will you find and will you find all murders? Referring to events as murder is actually already subjective and may miss situations that some people describe differently. Even if you get a substantial amount of texts about murders, we still do not know which texts make reference to the same murder. Such referential grounding is however a prerequisite to study differences in framing these events. The project Framing Situations in the Dutch Language1 tries to tackle this problem using the data-to-text method 1 http://dutchframenet.nl described in Vossen et al. (2018), which compiles massive text data (so-called reference texts) in different languages that is referentially grounded to specific event instances represented as so-called microworlds. We not only ground these texts but also automatically disambiguate mentions of these events in texts following a one-sense-per-event-type principle. Furthermore, we automatically derive the typical vocabulary and FrameNet frames (Baker et al., 2003) for different event types. We believe that inferring typical expressions and frames is an efficient and comprehensive way to enrich text collections with framing inter"
2020.lrec-1.387,W12-3018,0,0.0812971,"Missing"
2020.lrec-1.387,W16-5706,0,0.041195,"Missing"
2021.argmining-1.5,N19-1423,0,0.0168085,"ently more connected to sociocultural aspects and topic-specific differences than related tasks such as sentiment analysis. This paper is organized as follows. Section 2 discusses earlier work on stance detection, and specifically generalizability across topics. Section 3 presents the reproduction results. Section 4 adds additional, topic-specific analyses of the classification performance and a bag-of-words-based model to find topic-(in)dependent features. This is followed by our conclusions in Section 5. tion has seen a performance increase due to pretrained Transformer models such as BERT (Devlin et al., 2019). Reimers et al. (2019) reported .20 point F1 improvement over an LSTM baseline with a pre-trained BERT model. Combining multiple stance detection datasets in fine-tuning such a pretrained Transformer again led to a performance increase, though this model lacks robustness against slight test set manipulations (Schiller et al., 2021). 2 Recent work has specifically worked on identifying stances on topics not seen in training. Reimers et al. (2019) train their model on detecting stances and arguments for unseen topics. In their approach however, they treat all topics and stances on these topics"
2021.argmining-1.5,N16-1138,0,0.0304868,"ets on “dimensions in the sociocultural field” (Du Bois, 2007, p. 163). Current work focuses mostly on debates deemed controversial in the U.S. sociopolitical domain, such as abortion and gun control. 2.2 Prior work Early work on stance detection focused on parliamentary debates and longer texts (Thomas et al., 2006). Since Mohammad et al. (2016)’s stance detection shared task, Twitter has attracted a lot of attention in NLP work on stance detection (Zhu et al., 2019; Darwish et al., 2020; Hossain et al., 2020). Others addressed stance detection in the news domain, with (fake) news headlines (Ferreira and Vlachos, 2016; Hanselowski et al., 2018), disinformation (Hardalov et al., 2021) and user comments on news websites (Bošnjak and Karan, 2019). Feature-based approaches have largely been replaced by end-to-end neural models. Stance detec2 We would like to note that the stance expressed in a text unit does not have to be the stance of an author, e.g. in cases where someone is writing a piece in which they express or quote someone else’s opinion. 47 Generalization to new topics against abortion). Some of these studies attempt to leverage these vocabularies to generalize across similar topics. Recent work has"
2021.argmining-1.5,2021.naacl-main.379,0,0.0336439,"pic-specific manner. Topic-specific features were more informative for SVM models than more topic-independent features. In more recent work, Wei and Mao (2019) instead specifically focus on how generalizable certain topics are for transferring knowledge to new topics on stance detection. Some Twitter discussion topics seem to share a latent, underlying topic (e.g. both feminism and abortion have the latent topic of equality). In a (latent) topic-enhanced multi-layer perceptron (MLP) model with RNN representation of the tweet, the model indeed uses shared vocabulary between the related topics. Allaway et al. (2021) notice that earlier work, when considering training on some topics and testing on others, incorporates topic-relatedness. Unlike these other studies however, Allaway et al. (2021, p. 4756) “do not assume a relationship between training and test topics” as a fairer test of robustness. Results they present do show that stance detection is related to topic, but their efforts go to finding topic-invariant stance representations, which improves the generalizability of their model. Their consideration of topic similarity shows that topic difference is very relevant to stance detection. ALDayel and"
2021.argmining-1.5,P13-1166,1,0.85288,"Missing"
2021.argmining-1.5,2021.eacl-main.29,0,0.184586,"sformer models show promising results on cross-topic argument classification (Reimers et al., 2019; Schiller et al., 2021). In this paper, we investigate the ability of crosstopic stance detection approaches to generalize to different debate topics. Our question is: To what extent is stance detection topic-independent and generalizable across topics? Our contributions are threefold. We first complete a reproduction of state-of-the-art cross-topic stance detection work (Reimers et al., 2019), as reproduction has repeatedly shown to be relevant for NLP (Fokkens et al., 2013; Cohen et al., 2018; Belz et al., 2021). The reproduction is largely successful: we obtain similar numeric results. Secondly, we investigate the topic-specific performance of this model, and conclude that BERT’s performance fluctuates on different topics. Additionally, we find that a bag-of-words-based SVM model can rival its performance for some topics. Thirdly, we relate this to the nature of the stance detection modelling Cross-topic stance detection is the task to automatically detect stances (pro, against, or neutral) on unseen topics. We successfully reproduce state-of-the-art cross-topic stance detection work (Reimers et al."
2021.argmining-1.5,C18-1158,0,0.0351058,"Missing"
2021.argmining-1.5,W19-3707,0,0.0148243,"l in the U.S. sociopolitical domain, such as abortion and gun control. 2.2 Prior work Early work on stance detection focused on parliamentary debates and longer texts (Thomas et al., 2006). Since Mohammad et al. (2016)’s stance detection shared task, Twitter has attracted a lot of attention in NLP work on stance detection (Zhu et al., 2019; Darwish et al., 2020; Hossain et al., 2020). Others addressed stance detection in the news domain, with (fake) news headlines (Ferreira and Vlachos, 2016; Hanselowski et al., 2018), disinformation (Hardalov et al., 2021) and user comments on news websites (Bošnjak and Karan, 2019). Feature-based approaches have largely been replaced by end-to-end neural models. Stance detec2 We would like to note that the stance expressed in a text unit does not have to be the stance of an author, e.g. in cases where someone is writing a piece in which they express or quote someone else’s opinion. 47 Generalization to new topics against abortion). Some of these studies attempt to leverage these vocabularies to generalize across similar topics. Recent work has looked into generalizing stance detection across datasets, task definitions, and domains (Schiller et al., 2021), in which topic"
2021.argmining-1.5,2020.nlpcovid19-2.11,0,0.0278334,"omenon of actors communicating their evaluation of targets, by which they place themselves and their targets on “dimensions in the sociocultural field” (Du Bois, 2007, p. 163). Current work focuses mostly on debates deemed controversial in the U.S. sociopolitical domain, such as abortion and gun control. 2.2 Prior work Early work on stance detection focused on parliamentary debates and longer texts (Thomas et al., 2006). Since Mohammad et al. (2016)’s stance detection shared task, Twitter has attracted a lot of attention in NLP work on stance detection (Zhu et al., 2019; Darwish et al., 2020; Hossain et al., 2020). Others addressed stance detection in the news domain, with (fake) news headlines (Ferreira and Vlachos, 2016; Hanselowski et al., 2018), disinformation (Hardalov et al., 2021) and user comments on news websites (Bošnjak and Karan, 2019). Feature-based approaches have largely been replaced by end-to-end neural models. Stance detec2 We would like to note that the stance expressed in a text unit does not have to be the stance of an author, e.g. in cases where someone is writing a piece in which they express or quote someone else’s opinion. 47 Generalization to new topics against abortion). Some"
2021.argmining-1.5,L18-1025,0,0.279716,"es, pre-trained Transformer models show promising results on cross-topic argument classification (Reimers et al., 2019; Schiller et al., 2021). In this paper, we investigate the ability of crosstopic stance detection approaches to generalize to different debate topics. Our question is: To what extent is stance detection topic-independent and generalizable across topics? Our contributions are threefold. We first complete a reproduction of state-of-the-art cross-topic stance detection work (Reimers et al., 2019), as reproduction has repeatedly shown to be relevant for NLP (Fokkens et al., 2013; Cohen et al., 2018; Belz et al., 2021). The reproduction is largely successful: we obtain similar numeric results. Secondly, we investigate the topic-specific performance of this model, and conclude that BERT’s performance fluctuates on different topics. Additionally, we find that a bag-of-words-based SVM model can rival its performance for some topics. Thirdly, we relate this to the nature of the stance detection modelling Cross-topic stance detection is the task to automatically detect stances (pro, against, or neutral) on unseen topics. We successfully reproduce state-of-the-art cross-topic stance detection"
2021.argmining-1.5,P10-2047,0,0.047433,"e topics and testing on others, incorporates topic-relatedness. Unlike these other studies however, Allaway et al. (2021, p. 4756) “do not assume a relationship between training and test topics” as a fairer test of robustness. Results they present do show that stance detection is related to topic, but their efforts go to finding topic-invariant stance representations, which improves the generalizability of their model. Their consideration of topic similarity shows that topic difference is very relevant to stance detection. ALDayel and Magdy (2021) describe in their survey how several studies (Klebanov et al., 2010; Zhu et al., 2019; Darwish et al., 2020) show that texts pro or against an issue use different vocabularies (e.g. using ‘pro-life’ when expressing a stance 2.1 2.3 Background Definition of Stance Detection Stance detection is a long-established task in computational linguistics. Küçük and Can (2020) identify its most commonly used task definition: “For an input in the form of a piece of text and a target pair, stance detection is a classification problem where the stance of the author of the text is sought in the form of a category label from this set: Favor, Against, Neither.” (Küçük and Can"
2021.argmining-1.5,S16-1003,0,0.0730956,"Missing"
2021.argmining-1.5,D18-1402,0,0.194885,"mance on different topics, and addressing topic-specific vocabulary and context, is a future avenue for cross-topic stance detection. 1 Introduction (Online) debate has long been studied and modelled by computational linguistics with argument mining tasks such as stance detection. Stance detection is the task of automatically identifying the stance (agreeing, disagreeing, and/or neutral) of a text towards a debated topic or issue (Küçük and Can, 2020; Schiller et al., 2021).1 Its use-cases increasingly relate to online information environments and societal challenges, such as argument search (Stab et al., 2018), fake news identification (Hanselowski et al., 2018), or diversifying stances in a news recommender (Reuver et al., 2021). Cross-topic stance detection models should thus be able to deal with the quickly changing landscape of (online) public debate, where new topics and issues appear all the time. As Schlangen (2021) described in his recent paper on natural language 1 There is a wide array of datasets, definitions, and operationalizations of stance detection and classification, and recently Schiller et al. (2021) gave a great overview in their Section 2, as do Küçük and Can (2020) in their su"
2021.argmining-1.5,P19-1054,0,0.108146,"on dimensions in the socio-cultural field. This also comes with very topic-specific word use (Somasundaran and Wiebe, 2009; Wei and Mao, 2019). For instance, an against abortion argument might be expressed indirectly with a ‘pro-life’ expression, and someone aware of the socio-cultural context of this debate will be able to recognize this. Knowledge from other debate topics such as gun control may not be useful, since the debate strategies might change per topic. Despite these fundamental challenges, pre-trained Transformer models show promising results on cross-topic argument classification (Reimers et al., 2019; Schiller et al., 2021). In this paper, we investigate the ability of crosstopic stance detection approaches to generalize to different debate topics. Our question is: To what extent is stance detection topic-independent and generalizable across topics? Our contributions are threefold. We first complete a reproduction of state-of-the-art cross-topic stance detection work (Reimers et al., 2019), as reproduction has repeatedly shown to be relevant for NLP (Fokkens et al., 2013; Cohen et al., 2018; Belz et al., 2021). The reproduction is largely successful: we obtain similar numeric results. Sec"
2021.argmining-1.5,W06-1639,0,0.128417,"2 to 4, e.g. by adding ‘comment’ and ‘query’ next to ‘for’ and ‘against’ (Schiller et al., 2021). Küçük and Can (2020) emphasize that this computational definition is built upon the linguistic phenomenon of actors communicating their evaluation of targets, by which they place themselves and their targets on “dimensions in the sociocultural field” (Du Bois, 2007, p. 163). Current work focuses mostly on debates deemed controversial in the U.S. sociopolitical domain, such as abortion and gun control. 2.2 Prior work Early work on stance detection focused on parliamentary debates and longer texts (Thomas et al., 2006). Since Mohammad et al. (2016)’s stance detection shared task, Twitter has attracted a lot of attention in NLP work on stance detection (Zhu et al., 2019; Darwish et al., 2020; Hossain et al., 2020). Others addressed stance detection in the news domain, with (fake) news headlines (Ferreira and Vlachos, 2016; Hanselowski et al., 2018), disinformation (Hardalov et al., 2021) and user comments on news websites (Bošnjak and Karan, 2019). Feature-based approaches have largely been replaced by end-to-end neural models. Stance detec2 We would like to note that the stance expressed in a text unit does"
2021.argmining-1.5,2021.hackashop-1.7,1,0.895114,"ce detection. 1 Introduction (Online) debate has long been studied and modelled by computational linguistics with argument mining tasks such as stance detection. Stance detection is the task of automatically identifying the stance (agreeing, disagreeing, and/or neutral) of a text towards a debated topic or issue (Küçük and Can, 2020; Schiller et al., 2021).1 Its use-cases increasingly relate to online information environments and societal challenges, such as argument search (Stab et al., 2018), fake news identification (Hanselowski et al., 2018), or diversifying stances in a news recommender (Reuver et al., 2021). Cross-topic stance detection models should thus be able to deal with the quickly changing landscape of (online) public debate, where new topics and issues appear all the time. As Schlangen (2021) described in his recent paper on natural language 1 There is a wide array of datasets, definitions, and operationalizations of stance detection and classification, and recently Schiller et al. (2021) gave a great overview in their Section 2, as do Küçük and Can (2020) in their survey. 46 Proceedings of The 8th Workshop on Argument Mining, pages 46–56 Punta Cana, Dominican Republic, November 10–11, 2"
2021.argmining-1.5,2021.starsem-1.25,0,0.0333352,"tance detection over non-Transformer models? We argue this claim needs an asterisk: this cross-topic approach does not work as well for all topics. Different topics show specific vocabularies and socio-cultural contexts, and especially these specific contexts BERT cannot navigate. BERT models still make similar mistakes on gun control as the LSTM-based models in Stab et al. (2018). These findings lead us to two take-aways. Firstly, we hypothesize that models like BERT rely more on topic-specific features for stance detection than topic-independent lexical words related to argumentation. Thorn Jakobsen et al. (2021) also recently found this, and connected BERT’s crosstopic stance detection performance to its focus on spurious topic-specific lexical features (""gun"", ""criminal"") rather than words related to argumentation. They also conclude a fair real-world evaluation of cross-topic stance detection means reporting the worst performing cross-topic pair rather than average performance over topics. Secondly, we also think it is necessary to analyze the context of topics, and its relation to other debate topics within and outside the dataset. Most topics in stance detection studies are currently U.S. socio-p"
2021.argmining-1.5,2021.acl-short.85,0,0.171511,"ically identifying the stance (agreeing, disagreeing, and/or neutral) of a text towards a debated topic or issue (Küçük and Can, 2020; Schiller et al., 2021).1 Its use-cases increasingly relate to online information environments and societal challenges, such as argument search (Stab et al., 2018), fake news identification (Hanselowski et al., 2018), or diversifying stances in a news recommender (Reuver et al., 2021). Cross-topic stance detection models should thus be able to deal with the quickly changing landscape of (online) public debate, where new topics and issues appear all the time. As Schlangen (2021) described in his recent paper on natural language 1 There is a wide array of datasets, definitions, and operationalizations of stance detection and classification, and recently Schiller et al. (2021) gave a great overview in their Section 2, as do Küçük and Can (2020) in their survey. 46 Proceedings of The 8th Workshop on Argument Mining, pages 46–56 Punta Cana, Dominican Republic, November 10–11, 2021. ©2021 Association for Computational Linguistics task, which is inherently more connected to sociocultural aspects and topic-specific differences than related tasks such as sentiment analysis."
2021.argmining-1.5,P18-2095,0,0.0197785,"y chosen from online lists of controversial topics on discussion websites (Stab et al., 2018, p. 3666). Specifically, these topics are abortion, cloning, death penalty, gun control, marijuana legalization, minimum wage, nuclear energy and school uniforms. The stance classes (pro, con, and no argument) were annotated by two argument mining experts Reproduction Experiments Reimers et al. (2019) apply their approach of crosstopic claim classification to two datasets: the UKP Sentential Argument Mining Corpus (Stab et al., 2018) (‘the UKP dataset’) and the IBM Debater: Evidence Sentences dataset (Shnarch et al., 2018) (‘the IBM dataset’). We focus on the UKP Dataset, since the IBM Debater dataset has no ‘pro’ and ‘con’ class, but rather ‘evidence’ and ‘no evidence’ (and our focus is on stance detection). As a second step after stance classification, the authors also attempt to cluster similar arguments within the same topic in a cross-topic training setting. We do not replicate this component, but instead dive deeper into the classification results. We adopt the definition of reproduction by Belz et al. (2021): repeating the experiments as described in the earlier study, with the exact same data and softwa"
2021.argmining-1.5,P09-1026,0,0.34733,"denuniv.nl Abstract processing (NLP) methodology, generalization is a main goal of computational linguistics. A computational model (e.g. a stance detection model) should learn task capabilities beyond one set of datapoints, in our case: beyond one debate topic. Cross-topic stance detection is especially challenging because generalization to a new discussion topic is not trivial. Expressing stances is inherently socio-cultural behavior (Du Bois, 2007), where social actors place themselves and targets on dimensions in the socio-cultural field. This also comes with very topic-specific word use (Somasundaran and Wiebe, 2009; Wei and Mao, 2019). For instance, an against abortion argument might be expressed indirectly with a ‘pro-life’ expression, and someone aware of the socio-cultural context of this debate will be able to recognize this. Knowledge from other debate topics such as gun control may not be useful, since the debate strategies might change per topic. Despite these fundamental challenges, pre-trained Transformer models show promising results on cross-topic argument classification (Reimers et al., 2019; Schiller et al., 2021). In this paper, we investigate the ability of crosstopic stance detection app"
2021.eval4nlp-1.3,2021.acl-long.192,0,0.0306019,"t such subpopulation grouping, in addition to adversarial attacks, perturbations, and evaluation sets. It is possible to be unaware of certain subpopulations for which the model is weak, and therefore d’Eon et al. (2021) introduce a method that looks for such weak groups. Ribeiro et al. (2020) provide a methodology to analyze robustness toward basic capabilities and operationalize this with different test types (e.g. invariance to specific perturbations, basic capabilities). There are also more task specific efforts for evaluation, such as perturbations for robustness in task-oriented dialog (Liu et al., 2021) and evaluation of bias in a sentiment analysis setting (Asyrofi et al., 2021). 3 2. Linguistic information captured by a part of the models: This type of linguistic information is only captured by a part of the models due to their own induced biases. Hence, we expect that not all vanilla models behave similarly on such instances. With the introduction of SWA, more stability thus more overlap between mistakes is expected. Method To examine Stochastic Weight Averaging’s effect on model stability due to underspecification, we finetune a pretrained ALBERT-large version 2 on the SST-2 dataset. We"
2021.eval4nlp-1.3,2021.eacl-main.113,0,0.0280507,"ce or overlap. We make the following contributions: unclear what the effect of this is on different random seeds. Instead of looking at a form of ensembling, Hua et al. (2021) investigate the effect of injecting noise in BERT as a regularizer on the stability (sensitivity to input perturbation) of the models and show that fine-tuning performance improves. They point out that this improves generalizability as well, by looking at the difference in accuracy on the training and test set. However, training and test set might contain the same biases and hence might not reveal generalization issues (Elangovan et al., 2021). • We explore the effects of SWA on the stability and robustness of ALBERT-large that stem from underspecification. • We perform the, to our knowledge, first joint study of SWA and CheckList. Varying Performance Most work until now has focused on behavioral changes of models on train and test data when changing an arbitrary choice of the pipeline, such as the random seed (Zhong et al., 2021; Sellam et al., 2021). Investigating the behavior of language models with different pre-training and fine-tuning random seeds on an instance-level, Zhong et al. (2021) find that the fine-tuning random seed"
2021.eval4nlp-1.3,K19-1087,0,0.028118,"pa score on the development set of SST-2, but results are not conclusive. Finally, we observe a large error rate for one of the random seeds on both SST-2 and CheckList, which also leads to a less strong result on increasing agreement between models. 2 Related Work To the best of our knowledge, we are the first to combine SWA with CheckList and apply it to a BERT-based model to understand its effect on robustness with different random seeds. The work closest to ours has used variations of SWA for investigating the differences in interpretability on CNNs and LSTMs among different random seeds (Madhyastha and Jain, 2019). A similar method to Stochastic Weight Averaging was employed by Xu et al. (2020) with a different objective: improving the fine-tuning process of BERT. They propose to average the BERT model at each time-step and two types of knowledge distillation to improve fine-tuning of the model. The averaging receives slightly better results and their variant of knowledge distillation works the best. However, it is Model Evaluation Evaluating models on a development set might not expose certain biases or weaknesses a model has acquired due to the possibility of the same biases occurring in the training"
2021.eval4nlp-1.3,P13-1166,1,0.83137,"Missing"
2021.eval4nlp-1.3,2020.blackboxnlp-1.21,0,0.0335535,"s the random seed (Zhong et al., 2021; Sellam et al., 2021). Investigating the behavior of language models with different pre-training and fine-tuning random seeds on an instance-level, Zhong et al. (2021) find that the fine-tuning random seed is influential for the variation in performance on an instance-level. This contrast in performance is also highlighted by Sellam et al. (2021); they release multiple BERT checkpoints with a different weight initialization and show diverging performance between similarly trained models. Such behavior has also been observed for out-ofdistribution samples (McCoy et al., 2020; D’Amour et al., 2020; Amir et al., 2021), where different induced biases are found when the random seed is modified and checkpoints behave differently on unseen data, even when evaluation performance is similar. (D’Amour et al., 2020; Amir et al., 2021). Watson et al. (2021) show that outputs from explainability methods also vary when changing hyperparameters, e.g. the random seed. • We provide an in-depth analysis of results by going beyond accuracy to look at overlap and agreement between random seeds and CheckList. • We quantify agreement between different models by calculating overlap ra"
2021.eval4nlp-1.3,2021.naacl-demos.6,0,0.0233534,"They propose to average the BERT model at each time-step and two types of knowledge distillation to improve fine-tuning of the model. The averaging receives slightly better results and their variant of knowledge distillation works the best. However, it is Model Evaluation Evaluating models on a development set might not expose certain biases or weaknesses a model has acquired due to the possibility of the same biases occurring in the training set. Hence, scalable diagnostic methodologies are useful to investigate a model’s capabilities (Wu et al., 2019; Ribeiro et al., 2020; Wu et al., 2021; Goel et al., 2021). Even though these methodologies all focus on evaluation, the approach can vary between the methods. Wu et al. (2021) tackle evaluation from a counterfactual point of view. Wu et al. (2019) not only examine counterfactuals but also grouping queries to ensure that error analysis is scaled to all instances. Likewise, Goel et al. (2021) 17 1. Linguistic information captured by all of the models: We expect all of the models, regardless of the random seed, to be able to perform well on basic capabilities. Hence, we do not expect SWA to make much improvement, as there should not be a different beha"
2021.eval4nlp-1.3,2020.acl-main.442,0,0.307156,"ST-2 evaluation data. We also expect SWA to lead to more overlap in mistakes for CheckList items that are captured by part of the vanilla models. We also anticipate (minor) improvements of general Despite their success, modern language models are fragile. Even small changes in their training pipeline can lead to unexpected results. We study this phenomenon by examining the robustness of ALBERT (Lan et al., 2020) in combination with Stochastic Weight Averaging (SWA)—a cheap way of ensembling—on a sentiment analysis task (SST-2). In particular, we analyze SWA’s stability via CheckList criteria (Ribeiro et al., 2020), examining the agreement on errors made by models differing only in their random seed. We hypothesize that SWA is more stable because it ensembles model snapshots taken along the gradient descent trajectory. We quantify stability by comparing the models’ mistakes with Fleiss’ Kappa (Fleiss, 1971) and overlap ratio scores. We find that SWA reduces error rates in general; yet the models still suffer from their own distinct biases (according to CheckList). 1 Introduction Current language models perform well on data that resemble the distribution they are trained on, but even a slight variation i"
2021.hackashop-1.7,2020.emnlp-main.717,0,0.120448,"model and one set of training data difficult. Previous work often explores one well-known publicly debated topic, such as abortion (Draws et al., 2020a) or misinformation related to COVID-19 (Hossain et al., 2020). However, in an ideal solution we would also be able to continuously identify all kinds of new debates and related views. We believe that a combination of state-of-the-art NLP techniques such as neural language models can help address this problem without resorting to manual or unsupervised techniques. A possible interesting research direction is zero-shot or oneshot learning as in Allaway and McKeown (2020), where a model with the help of large(-scale) language models learns to identify new debates and viewpoints not seen at training time. In our case, this would mean identifying new debates and new viewpoints without explicit training on these when training for our task. We elaborate on potentially 4.1 Viewpoint Detection and Diversity The recommender systems literature specifically uses the term ‘viewpoint’ in relation to diversifying 48 recommendation. In these viewpoint-based papers, we notice a systems-focused tendency. Defining a viewpoint is less of a concern, nor is evaluating the viewpo"
2021.hackashop-1.7,2020.argmining-1.2,0,0.0178968,"., 2017, p. 1). This task usually involves social media texts and, once again, user-generated content. Commonly, these are shorts texts such as tweets. For instance, Mohammad et al. (2017) provide a frequently used Twitter dataset that strongly connects stances with sentiment and/or emotional scores of the text. Another common trend in stance detection is to use text explicitly written in the context of an (online) debate, such as the website debate.org and social media discussions. A recent study on Dutch social media comments highlights the difficulties in annotating stances on vaccination (Bauwelinck and Lefever, 2020). The authors identify the need to annotate topics, but also topic aspects and whether units are expressing an argument or not. Getting to good inter-annotator agreement (IAA) is difficult, showing that these concepts related to debate and stance are not uniform to all annotators even after extensive training. The same is found by Morante et al. (2020): Annotating Dutch social media text as well as other debate text on the vaccination debate, they find obtaining a high IAA is no easy task. With the more NLP-based tasks and definitions in the following sections, we explore how NLP tasks relate"
2021.hackashop-1.7,fokkens-etal-2014-biographynet,1,0.873942,"Missing"
2021.hackashop-1.7,L18-1590,1,0.834357,"Missing"
2021.hackashop-1.7,C18-1158,0,0.133059,"Missing"
2021.hackashop-1.7,2020.argmining-1.7,0,0.037248,"ation of claims and premises may also prove valuable, because supporting a deliberative democracy requires the detection of different deliberations and arguments in news texts. Draws et al. (2020a) use topic models to extract and find viewpoints in news texts with an unsupervised method, with the explicit goal to diversify a news recommender. They explicitly connect different sentiments to different viewpoints or perspectives. For this study, they use clearly argumentative text on abortion from a debating website. The words ‘viewpoint’ and ‘perspective’ are used interchangeably in this study. Carlebach et al. (2020) also address what they call “diverse viewpoint identification”. Here as well, we see a wide range of definitions and terms related to viewpoints and perspectives (e.g. ‘claim’, ‘hypothesis’, ‘entailment’). The authors use stateof-the-art methods including large neural language models, but the study does not seem to consider carefully defining their task, term definitions, and the needs of the problem. As such, it is unclear what they detect exactly. This is mainly due to the detection itself not being the main focus of their paper. 4.3 Stance detection is the computational task of detecting “"
2021.hackashop-1.7,2020.nlpcovid19-2.11,0,0.174126,"he news domain every day, which makes such approaches less useful in this domain. This leads to other recommendation techniques being more common in the news recommendendation domain. The second challenge specific to our problem is the continuous addition of new and many different topics, issues, and entities in public discussion and in the news. This makes detecting viewpoints with one automated, single model and one set of training data difficult. Previous work often explores one well-known publicly debated topic, such as abortion (Draws et al., 2020a) or misinformation related to COVID-19 (Hossain et al., 2020). However, in an ideal solution we would also be able to continuously identify all kinds of new debates and related views. We believe that a combination of state-of-the-art NLP techniques such as neural language models can help address this problem without resorting to manual or unsupervised techniques. A possible interesting research direction is zero-shot or oneshot learning as in Allaway and McKeown (2020), where a model with the help of large(-scale) language models learns to identify new debates and viewpoints not seen at training time. In our case, this would mean identifying new debates"
2021.hackashop-1.7,D19-5024,0,0.0609406,"Missing"
2021.hackashop-1.7,L16-1187,1,0.76215,"Missing"
2021.hackashop-1.7,E17-1017,0,0.0234138,"lowing sections, we explore how NLP tasks relate to this ‘viewpoints’ idea from the recommender systems community, and see what ideas and techniques these other tasks can add to diversity in news recommendation. 4.2 Stance Detection Argument Mining Argument Mining is the automatic extraction and analysis of specific units of argumentative text. It usually involves user-generated texts, such as comments, tweets, or blogposts. Such content is often highly argumentative by design, with high sentiment scores. In some studies, arguments are related to stances, as in the Dagstuhl ArgQuality Corpus (Wachsmuth et al., 2017), where 320 arguments cover 16 (political or societal) topics, and are balanced for different stances on the same topic. These arguments are from websites specifically aimed at debating. 49 Other work related to stance detection is more related to the news domain. The Fake News Classification Task (Hanselowski et al., 2018b) has a sub-task that concerns itself with predicting the stance of a news article towards the news headline. In their setup stances can be ‘Unrelated’, ‘Discuss’, ‘Agree’ or ‘Disagree’. The Fake News Classification tasks also introduces claim verification as a sub-task. Thi"
2021.hackashop-1.7,2020.lrec-1.611,0,0.593619,"Missing"
2021.hackashop-1.7,2020.acl-main.442,0,0.0613625,"Missing"
2021.hackashop-1.7,2020.emnlp-main.620,0,0.424194,"Missing"
2021.hackashop-1.7,J17-3005,0,0.0286479,"ally 4.1 Viewpoint Detection and Diversity The recommender systems literature specifically uses the term ‘viewpoint’ in relation to diversifying 48 recommendation. In these viewpoint-based papers, we notice a systems-focused tendency. Defining a viewpoint is less of a concern, nor is evaluating the viewpoint detection. Instead, researchers centralize viewpoint presentation to users, or how these respond to more diverse news, as in Lu et al. (2020) and Tintarev (2017). As a result, there is no standard definition of ‘viewpoint’ and the concept is operationalized differently by various authors. Stab and Gurevych (2017) identify the different sub-tasks in argumentation mining, and use essays as the argumented texts in question. For instance, one sub-task is separating argumentative from non-argumentative text units. Then, their pipeline involves classifying argument components into claims and premises, and finally it involves identifying argument relations. This first sub-task is also sometimes called claim detection, and is related to detecting stances and viewpoints when connecting claims to issues. For a deliberative democracy, the work on distinguishing argumentative from non-argumentative text in argume"
2021.naacl-main.199,2020.coling-main.586,1,0.619337,"concepts as Sommerauer and Fokkens (2019) recommend on the basis of Betti and van den Berg (2014). Betti and van den Berg argue that concepts are not isolated, but part of conceptual models. Sommerauer and Fokkens (2019) show that translating conceptual models to words representing them is one of the challenges involved in using DS models for studying conceptual change. They ground their conceptual model of ‘Racism’ in literature by sociologists, anthropologists and historians, but argue that domain experts would ideally be involved directly, as is done in the current paChallenging_DMs. per. Betti et al. (2020) introduce a concept-focused ground truth designed by domain experts, QuiNEGT, where paragraphs of philosophical text are annotated in terms of their relation to a conceptual model of the concept of naturalized epistemology in Quine’s works. We also make use of conceptual modeling methodology to build a ground truth, but our task is to extract knowledge on target term relations rather than to perform an information retrieval task searching for paragraphs relevant to a research question. While QuiNE-GT contains exhaustive lists of words pertaining to a particular research question, we aim for b"
2021.naacl-main.199,D17-1118,0,0.012112,". While QuiNE-GT contains exhaustive lists of words pertaining to a particular research question, we aim for broader coverage of different terms used by Quine and their relations. Methodological Challenges An interdisciplinary collaboration with domain experts can lead to hypotheses about shifts or nearest neighbors of specific terms, which can be tested by methods also used for detecting sense shift. These methods are not without challenges. The meaning representations are affected by random factors such as initialization and order of example (Hellrich and Hahn, 2016a) and frequency effects (Dubossarsky et al., 2017). A major obstacle in addressing these critical points is the lack of high quality evaluation sets (Tahmasebi et al., 2018; Kutuzov et al., 2018) and a tendency to use a single evaluation metric (Gladkova and Drozd, 2016) while each metric has downsides (Bakarov, 2018) . Evaluations on small sets of hand-picked examples that exhibit strong sense-shift (e.g. Hamilton et al. (2016a)) leave it unclear whether they are also suitable for making new discoveries or exploring data. van Aggelen et al. (2019) introduce a largescale evaluation set derived from a thesaurus and show that performance of dis"
2021.naacl-main.199,N15-1184,0,0.0803485,"Missing"
2021.naacl-main.199,2020.acl-main.365,0,0.0119015,"other work related to distributional semantics (DS) for specific concepts and conceptual change (2) critical reflection on evaluation and the methodology involved and (3) work on small datasets and identification of domain specific meaning. 2.1 2.2 DS for Concepts and Conceptual Change A well-known application of DS is the use of diachronic word embeddings to track and analyze changes in the meaning of words over periods of time (Kim et al., 2014; Kulkarni et al., 2015; Mitra et al., 2015; Hamilton et al., 2016b,a; Kenter et al., 2015; Tahmasebi and Risse, 2017; Montariol and Allauzen, 2019; Giulianelli et al., 2020, e.g.). Most of these approaches study what is called senseshift, which is the change in (dominant) sense of a specific word by comparing the word’s meaning representations in different time periods (Kutuzov et al., 2018). DS methods have also been used to study concepts related to gender and intersectionality (Herbelot et al., 2012), studying cultural stereotypes (Lewis and Lupyan, 2019) or harm-related concepts in psychological research papers (Vylomova et al., 2019). Wevers and Koolen (2020) survey three ways in which distributional semantic representations can help trace concept change. H"
2021.naacl-main.199,W16-2507,0,0.0999347,"inary collaboration with domain experts can lead to hypotheses about shifts or nearest neighbors of specific terms, which can be tested by methods also used for detecting sense shift. These methods are not without challenges. The meaning representations are affected by random factors such as initialization and order of example (Hellrich and Hahn, 2016a) and frequency effects (Dubossarsky et al., 2017). A major obstacle in addressing these critical points is the lack of high quality evaluation sets (Tahmasebi et al., 2018; Kutuzov et al., 2018) and a tendency to use a single evaluation metric (Gladkova and Drozd, 2016) while each metric has downsides (Bakarov, 2018) . Evaluations on small sets of hand-picked examples that exhibit strong sense-shift (e.g. Hamilton et al. (2016a)) leave it unclear whether they are also suitable for making new discoveries or exploring data. van Aggelen et al. (2019) introduce a largescale evaluation set derived from a thesaurus and show that performance of distributional methods is much lower on this more challenging set. These critical findings stress the need for methodologies that allow us to establish the quality of embeddings and to tell the difference between a stable, r"
2021.naacl-main.199,D16-1229,0,0.124507,"ction 5 which is followed by our conclusions and discussion. 2 Related Work In this section we cover (1) other work related to distributional semantics (DS) for specific concepts and conceptual change (2) critical reflection on evaluation and the methodology involved and (3) work on small datasets and identification of domain specific meaning. 2.1 2.2 DS for Concepts and Conceptual Change A well-known application of DS is the use of diachronic word embeddings to track and analyze changes in the meaning of words over periods of time (Kim et al., 2014; Kulkarni et al., 2015; Mitra et al., 2015; Hamilton et al., 2016b,a; Kenter et al., 2015; Tahmasebi and Risse, 2017; Montariol and Allauzen, 2019; Giulianelli et al., 2020, e.g.). Most of these approaches study what is called senseshift, which is the change in (dominant) sense of a specific word by comparing the word’s meaning representations in different time periods (Kutuzov et al., 2018). DS methods have also been used to study concepts related to gender and intersectionality (Herbelot et al., 2012), studying cultural stereotypes (Lewis and Lupyan, 2019) or harm-related concepts in psychological research papers (Vylomova et al., 2019). Wevers and Koolen"
2021.naacl-main.199,P16-1141,0,0.104799,"ction 5 which is followed by our conclusions and discussion. 2 Related Work In this section we cover (1) other work related to distributional semantics (DS) for specific concepts and conceptual change (2) critical reflection on evaluation and the methodology involved and (3) work on small datasets and identification of domain specific meaning. 2.1 2.2 DS for Concepts and Conceptual Change A well-known application of DS is the use of diachronic word embeddings to track and analyze changes in the meaning of words over periods of time (Kim et al., 2014; Kulkarni et al., 2015; Mitra et al., 2015; Hamilton et al., 2016b,a; Kenter et al., 2015; Tahmasebi and Risse, 2017; Montariol and Allauzen, 2019; Giulianelli et al., 2020, e.g.). Most of these approaches study what is called senseshift, which is the change in (dominant) sense of a specific word by comparing the word’s meaning representations in different time periods (Kutuzov et al., 2018). DS methods have also been used to study concepts related to gender and intersectionality (Herbelot et al., 2012), studying cultural stereotypes (Lewis and Lupyan, 2019) or harm-related concepts in psychological research papers (Vylomova et al., 2019). Wevers and Koolen"
2021.naacl-main.199,W16-2114,0,0.0206329,"ng for paragraphs relevant to a research question. While QuiNE-GT contains exhaustive lists of words pertaining to a particular research question, we aim for broader coverage of different terms used by Quine and their relations. Methodological Challenges An interdisciplinary collaboration with domain experts can lead to hypotheses about shifts or nearest neighbors of specific terms, which can be tested by methods also used for detecting sense shift. These methods are not without challenges. The meaning representations are affected by random factors such as initialization and order of example (Hellrich and Hahn, 2016a) and frequency effects (Dubossarsky et al., 2017). A major obstacle in addressing these critical points is the lack of high quality evaluation sets (Tahmasebi et al., 2018; Kutuzov et al., 2018) and a tendency to use a single evaluation metric (Gladkova and Drozd, 2016) while each metric has downsides (Bakarov, 2018) . Evaluations on small sets of hand-picked examples that exhibit strong sense-shift (e.g. Hamilton et al. (2016a)) leave it unclear whether they are also suitable for making new discoveries or exploring data. van Aggelen et al. (2019) introduce a largescale evaluation set derive"
2021.naacl-main.199,C16-1262,0,0.0258172,"Missing"
2021.naacl-main.199,D17-1030,0,0.351022,"nerate distributional semantic (DS) models that are precise enough to identify differences in concepts? This paper takes a first stab at addressing this question. In particular, we address the challenges involved in dealing with highly technical domainspecific terms that are defined in small corpora. As such, our use case has properties difficult for DS modeling, but typical for disciplines working with comparatively limited data. We compare domain-specific embeddings created using Word2Vec (Mikolov et al., 2013a,b) and a countbased SVD model (Levy et al., 2015) to those created by Nonce2Vec (Herbelot and Baroni, 2017), specifically designed for dealing with tiny data. Taking into account previous work criticizing the use of DS models for detecting sense-shift, we construct a data-specific ground truth, apply multiple evaluation metrics and verify whether results are stable across various random initializations. Our results confirm that SVD representations are superior to Word2Vec for small data and show that Nonce2Vec outperforms Word2Vec and, in most cases, SVD. However, results are currently not accurate enough for providing evidence or new insights to philosophers. Nevertheless, we are hopeful that bett"
2021.naacl-main.199,W12-1008,0,0.920008,"Missing"
2021.naacl-main.199,W14-2517,0,0.0174624,"are created. We then present our evaluation and results in Section 5 which is followed by our conclusions and discussion. 2 Related Work In this section we cover (1) other work related to distributional semantics (DS) for specific concepts and conceptual change (2) critical reflection on evaluation and the methodology involved and (3) work on small datasets and identification of domain specific meaning. 2.1 2.2 DS for Concepts and Conceptual Change A well-known application of DS is the use of diachronic word embeddings to track and analyze changes in the meaning of words over periods of time (Kim et al., 2014; Kulkarni et al., 2015; Mitra et al., 2015; Hamilton et al., 2016b,a; Kenter et al., 2015; Tahmasebi and Risse, 2017; Montariol and Allauzen, 2019; Giulianelli et al., 2020, e.g.). Most of these approaches study what is called senseshift, which is the change in (dominant) sense of a specific word by comparing the word’s meaning representations in different time periods (Kutuzov et al., 2018). DS methods have also been used to study concepts related to gender and intersectionality (Herbelot et al., 2012), studying cultural stereotypes (Lewis and Lupyan, 2019) or harm-related concepts in psycho"
2021.naacl-main.199,R19-1092,0,0.0124032,"k In this section we cover (1) other work related to distributional semantics (DS) for specific concepts and conceptual change (2) critical reflection on evaluation and the methodology involved and (3) work on small datasets and identification of domain specific meaning. 2.1 2.2 DS for Concepts and Conceptual Change A well-known application of DS is the use of diachronic word embeddings to track and analyze changes in the meaning of words over periods of time (Kim et al., 2014; Kulkarni et al., 2015; Mitra et al., 2015; Hamilton et al., 2016b,a; Kenter et al., 2015; Tahmasebi and Risse, 2017; Montariol and Allauzen, 2019; Giulianelli et al., 2020, e.g.). Most of these approaches study what is called senseshift, which is the change in (dominant) sense of a specific word by comparing the word’s meaning representations in different time periods (Kutuzov et al., 2018). DS methods have also been used to study concepts related to gender and intersectionality (Herbelot et al., 2012), studying cultural stereotypes (Lewis and Lupyan, 2019) or harm-related concepts in psychological research papers (Vylomova et al., 2019). Wevers and Koolen (2020) survey three ways in which distributional semantic representations can he"
2021.naacl-main.199,C18-1117,0,0.0819112,"pecific meaning. 2.1 2.2 DS for Concepts and Conceptual Change A well-known application of DS is the use of diachronic word embeddings to track and analyze changes in the meaning of words over periods of time (Kim et al., 2014; Kulkarni et al., 2015; Mitra et al., 2015; Hamilton et al., 2016b,a; Kenter et al., 2015; Tahmasebi and Risse, 2017; Montariol and Allauzen, 2019; Giulianelli et al., 2020, e.g.). Most of these approaches study what is called senseshift, which is the change in (dominant) sense of a specific word by comparing the word’s meaning representations in different time periods (Kutuzov et al., 2018). DS methods have also been used to study concepts related to gender and intersectionality (Herbelot et al., 2012), studying cultural stereotypes (Lewis and Lupyan, 2019) or harm-related concepts in psychological research papers (Vylomova et al., 2019). Wevers and Koolen (2020) survey three ways in which distributional semantic representations can help trace concept change. However, none of these methods requires historians of ideas to fix initial and testable hypotheses on the meaning of concepts as Sommerauer and Fokkens (2019) recommend on the basis of Betti and van den Berg (2014). Betti a"
2021.naacl-main.199,Q15-1016,0,0.0161436,"., 2016; Ginammi et al., 2020), but can we also generate distributional semantic (DS) models that are precise enough to identify differences in concepts? This paper takes a first stab at addressing this question. In particular, we address the challenges involved in dealing with highly technical domainspecific terms that are defined in small corpora. As such, our use case has properties difficult for DS modeling, but typical for disciplines working with comparatively limited data. We compare domain-specific embeddings created using Word2Vec (Mikolov et al., 2013a,b) and a countbased SVD model (Levy et al., 2015) to those created by Nonce2Vec (Herbelot and Baroni, 2017), specifically designed for dealing with tiny data. Taking into account previous work criticizing the use of DS models for detecting sense-shift, we construct a data-specific ground truth, apply multiple evaluation metrics and verify whether results are stable across various random initializations. Our results confirm that SVD representations are superior to Word2Vec for small data and show that Nonce2Vec outperforms Word2Vec and, in most cases, SVD. However, results are currently not accurate enough for providing evidence or new insigh"
2021.naacl-main.199,W13-3512,0,0.0417311,"r philosophy. In addition to the challenges outlined above, we are faced with the issue that domain-specific corpora are typically small, i.e. up to a few million tokens rather than web scale. Learning embeddings from small corpora is not an easy task, where SVD mod3.1 Philosophical Questions els outperform Word2Vec (W2V) (Sahlgren and Many philosophical research questions focus on Lenci, 2016, on 1M words, Asr et al., 2016, on 8M the interpretation and comparison of philosophiwords), and learning them for rare words presents cal views expressed in writing. These questions further difficulty (Luong et al., 2013). Nonce2Vec revolve around specific concepts and how they are (N2V) (Herbelot and Baroni, 2017) addresses this defined and viewed by different philosophers. Ofissue through ‘high-risk’ incremental learning with an initially high but decaying learning rate, allow- ten, different philosophers use the same terms to deing them to learn embeddings from single sen- scribe different concepts. For example, Quine sees tences (called tiny data). Faruqui et al. (2015) reference as a relation between a singular term and incorporate ontological information from lexical- a physical object, where a physical"
2021.naacl-main.199,N18-1044,0,0.0262826,"ilton et al. (2016a)) leave it unclear whether they are also suitable for making new discoveries or exploring data. van Aggelen et al. (2019) introduce a largescale evaluation set derived from a thesaurus and show that performance of distributional methods is much lower on this more challenging set. These critical findings stress the need for methodologies that allow us to establish the quality of embeddings and to tell the difference between a stable, reliable finding and an artefact of the method. Dubossarsky et al. (2017) propose the use of shuffled and synchronic corpora for verification. Rosenfeld and Erk (2018) use synthetic words that consist of two real words merged together that result in a shift of these words’ senses for evaluation. Sommerauer and Fokkens (2019) recommend stress-testing through control words (that should not change) and by comparing results on multiple models. We supplement these proposals for 2512 diachronic models by providing methods that can be used as a strict test of synchronic model quality, independently of measuring change (so that frequency effects are not a risk). To ground these methods, we introduce a novel, high quality ground truth containing fine-grained meaning"
2021.naacl-main.199,D16-1099,0,0.0517575,"Missing"
2021.naacl-main.199,D19-1007,0,0.0111962,"., 2020). Bloem et al. (2019) confirm the domain-specific charac- related to each other, or which concept pairs stand ter of philosophical writings showing that two vec- in similar relations to others. To do this, philosophical experts practice close-reading. The interpretators for the same word, one trained on Wikipedia and one trained on the works of a specific philoso- tion of only a single passage requires close-reading pher, can have low similarity, especially for high- and expertise of not only the work the passage is in, but often also other works by the same author or frequency terms. Shoemark et al. (2019) find that even other authors. Conclusions are often drawn on the top ranked words are domain-specific to the Twitter data they used. Wohlgenannt et al. (2019) a small subset of the relevant available data. It almost always requires making a selection of sources evaluate DS models trained on two fantasy book to consider and thus allows for cherrypicking data. series by having domain experts manually compile The use of computational linguistic methods, inevaluation datasets addressing the relevant word stead, could make it possible to consider all availsenses, incorporating domain knowledge in"
2021.naacl-main.199,P11-2120,0,0.0116038,"n, higher number of negative samples had the greatest impact on the consistency scores. For N2V, the lowest parameter decay We propose a framework for fine-tuning models specifically designed for domain-specific experiments with small data. As the size of our ground truth is comparatively limited (for computational purposes), we do not want to ‘waste’ portions of it for fine-tuning. Instead, we use ‘proxy’ terms and a ‘proxy’ corpus to evaluate and compare models on an artificial task. We aim to select data representative of the target data (inspired by fine-tuning for low-resource languages, Søgaard, 2011). Terms and Corpus. As target terms we select 20 technical terms from the legal domain. Similar to the philosophical target terms, many technical legal terms have distinct or more specific meanings in legal scholarship as opposed to generic corpora. To select a proxy corpus, we compare the contexts of the target terms to the contexts of the legal terms in four candidate corpora: the British Law Corpus (BLC), the Open Access Journal corpus, Wikipedia, and the British National Corpus (BNC). We compare the contexts in terms of easily computable metrics which characterize properties we expect to h"
2021.naacl-main.199,W19-4728,1,0.905528,"comparing the word’s meaning representations in different time periods (Kutuzov et al., 2018). DS methods have also been used to study concepts related to gender and intersectionality (Herbelot et al., 2012), studying cultural stereotypes (Lewis and Lupyan, 2019) or harm-related concepts in psychological research papers (Vylomova et al., 2019). Wevers and Koolen (2020) survey three ways in which distributional semantic representations can help trace concept change. However, none of these methods requires historians of ideas to fix initial and testable hypotheses on the meaning of concepts as Sommerauer and Fokkens (2019) recommend on the basis of Betti and van den Berg (2014). Betti and van den Berg argue that concepts are not isolated, but part of conceptual models. Sommerauer and Fokkens (2019) show that translating conceptual models to words representing them is one of the challenges involved in using DS models for studying conceptual change. They ground their conceptual model of ‘Racism’ in literature by sociologists, anthropologists and historians, but argue that domain experts would ideally be involved directly, as is done in the current paChallenging_DMs. per. Betti et al. (2020) introduce a concept-fo"
2021.naacl-main.199,W19-6105,1,0.872511,"Missing"
2021.naacl-main.199,W19-4704,0,0.0206364,"Mitra et al., 2015; Hamilton et al., 2016b,a; Kenter et al., 2015; Tahmasebi and Risse, 2017; Montariol and Allauzen, 2019; Giulianelli et al., 2020, e.g.). Most of these approaches study what is called senseshift, which is the change in (dominant) sense of a specific word by comparing the word’s meaning representations in different time periods (Kutuzov et al., 2018). DS methods have also been used to study concepts related to gender and intersectionality (Herbelot et al., 2012), studying cultural stereotypes (Lewis and Lupyan, 2019) or harm-related concepts in psychological research papers (Vylomova et al., 2019). Wevers and Koolen (2020) survey three ways in which distributional semantic representations can help trace concept change. However, none of these methods requires historians of ideas to fix initial and testable hypotheses on the meaning of concepts as Sommerauer and Fokkens (2019) recommend on the basis of Betti and van den Berg (2014). Betti and van den Berg argue that concepts are not isolated, but part of conceptual models. Sommerauer and Fokkens (2019) show that translating conceptual models to words representing them is one of the challenges involved in using DS models for studying conc"
2021.nlp4posimpact-1.6,N19-1423,0,0.00655598,"icles. In the NLP field, the detection of different claims (Levy et al., 2014), arguments (Stab and Gurevych, 2017), and stances (Mohammad et al., 2016) are established tasks that are related. Work on such tasks is often on topics publicly debated in a political context, such as vaccinations, abortion, and Why (viewpoint) diversity matters in the news context The literature on the importance of news diversity for a democratic society describes various mod48 immigration. This makes them potentially useful for operationalization of the viewpoint concept. Large-scale pre-trained language models (Devlin et al., 2019) are a recent development in NLP, and could be used to detect viewpoints. Reimers et al. (2019) use such methods for the NLP task claim detection. In this work such language models are also used to cluster similar claims and arguments, giving the opportunity to also detect dissimilar claims or arguments. See section 3.1.2 for more concrete and detailed ideas we have on the operationalization of viewpoints with NLP. 2.3 interested in diverse viewpoints (Kim and Pasek, 2020; Tintarev, 2017). We argue that considering individual users’ latitudes of diversity increases the likelihood that a given"
2021.nlp4posimpact-1.6,D18-1393,0,0.0259475,"ectives and viewpoints easily translatable to identifying two opposing broad political groups, and this is often what happens in such papers: leftwing (i.e. the Democratic party) and right-wing (the Republican party) viewpoints are detected and extracted, as in Roy and Goldwasser (2020). However, not every political climate has such a polarized and two-party political system, so such an approach might not fit every language or context. Nuanced concept from political science such as framing and agenda setting have also been analysed with NLP beyond the U.S context, e.g., in Russian news media (Field et al., 2018). There are several related NLP tasks and solutions to identifying different viewpoints on (politically contentious) issues in news texts. Names for such tasks are stance detection (Hanselowski et al., 2018), argument mining (Stab and Gurevych, 2017), and perspective detection (Morante et al., 2020; van Son et al., 2016). All these tasks focus on capturing an opinion on issues, events, or entities, which make them useful for identifying viewpoints for a recommender system that supports democratic values such as deliberation. In Reuver et al. (2021), several of these aformentioned NLP tasks and"
2021.nlp4posimpact-1.6,D18-2029,0,0.0119834,"e current issues expressed in the media content. Third, measuring diversity computationally. And lastly, measuring and providing different latitudes of diversity to different users. In the following sections, we will discuss these different sub-tasks of the problem of how to represent content of a news recommender system in order to create a diverse recommender. One partic50 Identifying current issues or topics in the news on vector space representations of texts. Traditional vector space models and neural language models such as Doc2vec (Le and Mikolov, 2014), the Universal Sentence Encoder (Cer et al., 2018), and sentence-BERT (Reimers and Gurevych, 2019) semantically represent documents (sentences) in a multi-dimensional space. We argue that vector space representations could also be useful when considering different users with different latitudes of diversity for different topics or viewpoints. It means modelling not only articles in a vector space, but also users. 3.1.2 stances often related to a particular opinion on a contentious issue (e.g. is the text pro, against, or neutral towards immigration?). This is inherently related to the idea of debates in society between different viewpoints on"
2021.nlp4posimpact-1.6,C18-1158,0,0.0276894,"Missing"
2021.nlp4posimpact-1.6,C14-1141,0,0.0168853,"se news recommendation. Section 2.1 addresses why viewpoint news diversity is important from a democratic perspective. In Section 2.2, we introduce how viewpoint diversity is connected to NLP. We then discuss nudging theory and how it can inform news recommender design in Section 2.3. Lastly, we explain how our concept of “latitude of diversity” can help make news recommenders more user-centric in Section 2.4. 2.1 2.2 The connection with NLP The focus on viewpoint diversity has as a central task the detection of viewpoints in news articles. In the NLP field, the detection of different claims (Levy et al., 2014), arguments (Stab and Gurevych, 2017), and stances (Mohammad et al., 2016) are established tasks that are related. Work on such tasks is often on topics publicly debated in a political context, such as vaccinations, abortion, and Why (viewpoint) diversity matters in the news context The literature on the importance of news diversity for a democratic society describes various mod48 immigration. This makes them potentially useful for operationalization of the viewpoint concept. Large-scale pre-trained language models (Devlin et al., 2019) are a recent development in NLP, and could be used to det"
2021.nlp4posimpact-1.6,2020.findings-emnlp.296,0,0.0647154,"Missing"
2021.nlp4posimpact-1.6,S16-1003,0,0.0852004,"Missing"
2021.nlp4posimpact-1.6,2020.nl4xai-1.11,1,0.805973,"Missing"
2021.nlp4posimpact-1.6,2020.lrec-1.611,0,0.057843,"Missing"
2021.nlp4posimpact-1.6,2020.emnlp-main.620,0,0.206114,"tions of news texts. Such debates are for instance ones on immigration, or vaccination. Commonly, only one or a handful recurring contentious debates are discussed in current work on arguments and debates in the news. Some such topics used as case-studies are the benefits and dangers of vaccination (Morante et al., 2020) and the ethics of abortion (Draws et al., 2021). One option for identifying topics is a rule-based method with pre-defined lists or gazetteers of known contentious, newsworthy topics, for instance websites listing (political) debates topics, as done by Draws et al. (2021) and Roy and Goldwasser (2020), and using these lists either for further manual annotation of a training set used to train a classifying machine learning model, or using heuristics and rules to identify these topics in news articles. Another option is manual annotation of (journalistic) topics already being distinguished in the (online) news room by editors or journalists, as used by Lu et al. (2020), who use features such as website sections (e.g.“sports”, “politics” or more fine-grained: “U.S. elections”) and journalistic tags (e.g. “opinion”) to represent news articles for a diverse recommender. One challenge here is th"
2021.nlp4posimpact-1.6,L16-1187,1,0.835877,"Missing"
2021.nlp4posimpact-1.6,N18-5005,0,0.038242,"Missing"
2021.nlp4posimpact-1.6,J17-3005,0,0.0604609,"2.1 addresses why viewpoint news diversity is important from a democratic perspective. In Section 2.2, we introduce how viewpoint diversity is connected to NLP. We then discuss nudging theory and how it can inform news recommender design in Section 2.3. Lastly, we explain how our concept of “latitude of diversity” can help make news recommenders more user-centric in Section 2.4. 2.1 2.2 The connection with NLP The focus on viewpoint diversity has as a central task the detection of viewpoints in news articles. In the NLP field, the detection of different claims (Levy et al., 2014), arguments (Stab and Gurevych, 2017), and stances (Mohammad et al., 2016) are established tasks that are related. Work on such tasks is often on topics publicly debated in a political context, such as vaccinations, abortion, and Why (viewpoint) diversity matters in the news context The literature on the importance of news diversity for a democratic society describes various mod48 immigration. This makes them potentially useful for operationalization of the viewpoint concept. Large-scale pre-trained language models (Devlin et al., 2019) are a recent development in NLP, and could be used to detect viewpoints. Reimers et al. (2019)"
2021.nlp4posimpact-1.6,D19-1410,0,0.0149031,"a content. Third, measuring diversity computationally. And lastly, measuring and providing different latitudes of diversity to different users. In the following sections, we will discuss these different sub-tasks of the problem of how to represent content of a news recommender system in order to create a diverse recommender. One partic50 Identifying current issues or topics in the news on vector space representations of texts. Traditional vector space models and neural language models such as Doc2vec (Le and Mikolov, 2014), the Universal Sentence Encoder (Cer et al., 2018), and sentence-BERT (Reimers and Gurevych, 2019) semantically represent documents (sentences) in a multi-dimensional space. We argue that vector space representations could also be useful when considering different users with different latitudes of diversity for different topics or viewpoints. It means modelling not only articles in a vector space, but also users. 3.1.2 stances often related to a particular opinion on a contentious issue (e.g. is the text pro, against, or neutral towards immigration?). This is inherently related to the idea of debates in society between different viewpoints on these contentious topics, implicitly or explici"
2021.nlp4posimpact-1.6,P19-1054,0,0.0472523,"b and Gurevych, 2017), and stances (Mohammad et al., 2016) are established tasks that are related. Work on such tasks is often on topics publicly debated in a political context, such as vaccinations, abortion, and Why (viewpoint) diversity matters in the news context The literature on the importance of news diversity for a democratic society describes various mod48 immigration. This makes them potentially useful for operationalization of the viewpoint concept. Large-scale pre-trained language models (Devlin et al., 2019) are a recent development in NLP, and could be used to detect viewpoints. Reimers et al. (2019) use such methods for the NLP task claim detection. In this work such language models are also used to cluster similar claims and arguments, giving the opportunity to also detect dissimilar claims or arguments. See section 3.1.2 for more concrete and detailed ideas we have on the operationalization of viewpoints with NLP. 2.3 interested in diverse viewpoints (Kim and Pasek, 2020; Tintarev, 2017). We argue that considering individual users’ latitudes of diversity increases the likelihood that a given user engages with diverse recommendations and potentially prevents unwanted side effects. If in"
2021.nlp4posimpact-1.6,2021.hackashop-1.7,1,0.917535,"o recommender systems. For a functioning democracy, we want users to come into contact with opinions, debates, or ideas they disagree with or even dislike. This implies that simply optimizing a news recommender system for user preference, as is common now, is not enough. The public value of diversity, in terms of a diversity of issues and opinions, is essential (Helberger, 2015, 2019). There already is work on the question of how opinion, sentiment, and argument diversity for news recommendation should be understood and captured by (evaluation) metrics (Vrijenhoek et al., 2021), or NLP tasks (Reuver et al., 2021). In this paper, we want to develop a different additional perspective. We propose that a turn to the user – i.e., democratic citizens reading news recommended by NRS – is needed. This paper explores challenges and opportunities for facilitating this with the help of NLP. More specifically, we propose the notion of individual latitutes of diversity which can help make the diversity of news recommendations more meaningful by taking the user-as-a-human-being into account. Although the promotion of news diversity is desirable from societal perspective, not every user In this position paper, we pr"
E17-1064,D15-1159,0,0.0136074,"al., 2014; Gomez-Rodriguez et al., 2014; Björkelund and Nivre, 2015). In contrast, the reinforcement learning approach proposed in this paper is more general and can be applied to a variety of systems. Related Work In this section, we address related work on dependency parsing, including alternative approaches for reducing error propagation, and reinforcement learning. 2.1 Dependency Parsing We use Chen and Manning (2014)’s parser as a basis for our experiments. Their parser is opensource and has served as a reference point for many recent publications (Dyer et al., 2015; Weiss et al., 2015; Alberti et al., 2015; Honnibal and Johnson, 2015, among others). They provide an efficient neural network that learns dense vector representations of words, PoS-tags and dependency labels. This small set of features makes their parser significantly more efficient than other popular parsers, such as the Malt (Nivre et al., 2007) or MST (McDonald et al., 2005) parser while obtaining higher accuracy. They acknowledge the error propagation problem of greedy parsers, but leave addressing this through (e.g.) beam search for future work. Dyer et al. (2015) introduce an approach that uses Long Short-Term Memory (LSTM). T"
E17-1064,P16-1231,0,0.0717084,"show that reinforcement learning can reduce error propagation in NLP. The rest of this paper is structured as follows. We discuss related work in Section 2. This is followed by a description of the parsers used in our experiments in Section 3. Section 4 outlines our experimental setup and presents our results. The error propagation experiment and its outcome are described in Section 5. Finally, we conclude and discuss future research in Section 6. 2 Kiperwasser and Goldberg (2016) introduce a new way to represent features using a bidirectional LSTM and improve the results of a greedy parser. Andor et al. (2016) present a mathematical proof that globally normalized models are more expressive than locally normalized counterparts and propose to use global normalization with beam search at both training and testing. Our approach differs from all of the work mentioned above, in that it manages to improve results of Chen and Manning (2014) without changing the architecture of the model nor the input representation. The only substantial difference lies in the way the model is trained. In this respect, our research is most similar to training approaches using dynamic oracles (Goldberg and Nivre, 2012). Trad"
E17-1064,Q15-1039,0,0.0288672,"tion as in our best-performing model (see Section 3.4). dependency parsing have been achieved since (notably, Andor et al. (2016)), Chen and Manning’s parser provides a better baseline for our purposes. We aim at investigating the influence of reinforcement learning on error propagation and want to test this in a clean environment, where reinforcement learning does not interfere with other methods that address the same problem. 2.2 Reinforcement Learning Reinforcement learning has been applied to several NLP tasks with success, e.g. agenda-based parsing (Jiang et al., 2012), semantic parsing (Berant and Liang, 2015) and simultaneous machine translation (Grissom II et al., 2014). To our knowledge, however, none of these studies investigated the influence of reinforcement learning on error propagation. Learning to Search (L2S) is probably the most prominent line of research that applies reinforcement learning (more precisely, imitation learning) to NLP. Various algorithms, e.g. SEARN (Daumé III et al., 2009) and DAgger (Ross et al., 2011), have been developed sharing common high-level steps: a roll-in policy is executed to generate training states from which a roll-out policy is used to estimate the loss o"
E17-1064,W15-2210,0,0.0127977,"nerate only one sequence of actions per sentence. A dynamic oracle gives all trajectories leading to the best possible result from every valid parse configuration. They can therefore be used to generate more training sequences including those containing errors. A drawback of this approach is that dynamic oracles have to be developed specifically for individual transition systems (e.g. arc-standard, arceager). Therefore, a large number of dynamic oracles have been developed in recent years (Goldberg and Nivre, 2012; Goldberg and Nivre, 2013; Goldberg et al., 2014; Gomez-Rodriguez et al., 2014; Björkelund and Nivre, 2015). In contrast, the reinforcement learning approach proposed in this paper is more general and can be applied to a variety of systems. Related Work In this section, we address related work on dependency parsing, including alternative approaches for reducing error propagation, and reinforcement learning. 2.1 Dependency Parsing We use Chen and Manning (2014)’s parser as a basis for our experiments. Their parser is opensource and has served as a reference point for many recent publications (Dyer et al., 2015; Weiss et al., 2015; Alberti et al., 2015; Honnibal and Johnson, 2015, among others). They"
E17-1064,Q16-1023,0,0.0140323,". 4. We confirm our hypothesis that reinforcement learning reduces error propagation. To our knowledge, this paper is the first to experimentally show that reinforcement learning can reduce error propagation in NLP. The rest of this paper is structured as follows. We discuss related work in Section 2. This is followed by a description of the parsers used in our experiments in Section 3. Section 4 outlines our experimental setup and presents our results. The error propagation experiment and its outcome are described in Section 5. Finally, we conclude and discuss future research in Section 6. 2 Kiperwasser and Goldberg (2016) introduce a new way to represent features using a bidirectional LSTM and improve the results of a greedy parser. Andor et al. (2016) present a mathematical proof that globally normalized models are more expressive than locally normalized counterparts and propose to use global normalization with beam search at both training and testing. Our approach differs from all of the work mentioned above, in that it manages to improve results of Chen and Manning (2014) without changing the architecture of the model nor the input representation. The only substantial difference lies in the way the model is"
E17-1064,P14-1043,0,0.0413645,"Missing"
E17-1064,D14-1082,0,0.109303,"iciency and accuracy nearing state-of-the-art. They are also known to suffer from error propagation. Because they work by carrying out a sequence of actions without reconsideration, an erroneous action can exert a negative effect on all subsequent decisions. By rendering correct parses unreachable or promoting incorrect features, the first error induces the second error and so on. McDonald and Nivre (2007) argue that the observed negative correlation between parsing accuracy and sentence length indicates error propagation is at work. We compare reinforcement learning to supervised learning on Chen and Manning (2014)’s parser. This high performance parser is available as open source. It does not make use of alternative strategies for tackling error propagation and thus provides a clean experimental setup to test our hypothesis. Reinforcement learning increased both unlabeled and labeled accuracy on the Penn TreeBank and German part of SPMRL (Seddah et al., 2014). This outcome shows that reinforcement learning has a positive effect, but does not yet prove that this is indeed the result of reduced error propagation. We therefore designed an experiment which identified which errors are the result of error pr"
E17-1064,D07-1013,0,0.111511,"Missing"
E17-1064,P15-1033,0,0.014553,"; Goldberg and Nivre, 2013; Goldberg et al., 2014; Gomez-Rodriguez et al., 2014; Björkelund and Nivre, 2015). In contrast, the reinforcement learning approach proposed in this paper is more general and can be applied to a variety of systems. Related Work In this section, we address related work on dependency parsing, including alternative approaches for reducing error propagation, and reinforcement learning. 2.1 Dependency Parsing We use Chen and Manning (2014)’s parser as a basis for our experiments. Their parser is opensource and has served as a reference point for many recent publications (Dyer et al., 2015; Weiss et al., 2015; Alberti et al., 2015; Honnibal and Johnson, 2015, among others). They provide an efficient neural network that learns dense vector representations of words, PoS-tags and dependency labels. This small set of features makes their parser significantly more efficient than other popular parsers, such as the Malt (Nivre et al., 2007) or MST (McDonald et al., 2005) parser while obtaining higher accuracy. They acknowledge the error propagation problem of greedy parsers, but leave addressing this through (e.g.) beam search for future work. Dyer et al. (2015) introduce an approach"
E17-1064,H05-1066,0,0.253806,"Missing"
E17-1064,P02-1031,0,0.111499,"reinforcement learning to greedy dependency parsing which is known to suffer from error propagation. Reinforcement learning improves accuracy of both labeled and unlabeled dependencies of the Stanford Neural Dependency Parser, a high performance greedy parser, while maintaining its efficiency. We investigate the portion of errors which are the result of error propagation and confirm that reinforcement learning reduces the occurrence of error propagation. 1 Introduction Error propagation is a common problem for many NLP tasks (Song et al., 2012; Quirk and CorstonOliver, 2006; Han et al., 2013; Gildea and Palmer, 2002; Yang and Cardie, 2013). It can occur when NLP tools applied early on in a pipeline make mistakes that have negative impact on higher-level tasks further down the pipeline. It can also occur within the application of a specific task, when sequential decisions are taken and errors made early in the process affect decisions made later on. When reinforcement learning is applied, a system actively tries out different sequences of actions. Most of these sequences will contain some errors. We hypothesize that a system trained in this manner will be more robust and less susceptible to error propagat"
E17-1064,C12-1059,0,0.0519983,"edy parser. Andor et al. (2016) present a mathematical proof that globally normalized models are more expressive than locally normalized counterparts and propose to use global normalization with beam search at both training and testing. Our approach differs from all of the work mentioned above, in that it manages to improve results of Chen and Manning (2014) without changing the architecture of the model nor the input representation. The only substantial difference lies in the way the model is trained. In this respect, our research is most similar to training approaches using dynamic oracles (Goldberg and Nivre, 2012). Traditional static oracles can generate only one sequence of actions per sentence. A dynamic oracle gives all trajectories leading to the best possible result from every valid parse configuration. They can therefore be used to generate more training sequences including those containing errors. A drawback of this approach is that dynamic oracles have to be developed specifically for individual transition systems (e.g. arc-standard, arceager). Therefore, a large number of dynamic oracles have been developed in recent years (Goldberg and Nivre, 2012; Goldberg and Nivre, 2013; Goldberg et al., 2"
E17-1064,P15-1111,0,0.128714,"radient w.r.t. the parameters. The advantage of using a policy gradient algorithm in NLP is that gradientbased optimization is already widely used. REINFORCE (Williams, 1992; Ranzato et al., 2016) is a widely-used policy gradient algorithm but it is also well-known for suffering from high variance (Sutton et al., 1999). 2.3 Reinforcement and error propagation As mentioned above, previous work that applied reinforcement learning to NLP has, to our knowledge, not shown that it improved results by reducing error propagation. Work on identifying the impact of error propagation in parsing is rare, Ng and Curran (2015) being a notable exception. They provide a detailed error analysis for parsing and classify which kind of parsing errors are involved with error propagation. There are four main differences between their approaches and ours. First, Ng and Curran correct arcs in the tree and our algorithm corrects decisions of the parsing algorithm. Second, our approach distinguishes between cases where one erroneous action deterministically leads to multiple erroneous arcs and cases where an erroneous action leads to conditions that indirectly result in further errors (see Section 5.1 for a detailed explanatio"
E17-1064,Q13-1033,0,0.0181009,"g dynamic oracles (Goldberg and Nivre, 2012). Traditional static oracles can generate only one sequence of actions per sentence. A dynamic oracle gives all trajectories leading to the best possible result from every valid parse configuration. They can therefore be used to generate more training sequences including those containing errors. A drawback of this approach is that dynamic oracles have to be developed specifically for individual transition systems (e.g. arc-standard, arceager). Therefore, a large number of dynamic oracles have been developed in recent years (Goldberg and Nivre, 2012; Goldberg and Nivre, 2013; Goldberg et al., 2014; Gomez-Rodriguez et al., 2014; Björkelund and Nivre, 2015). In contrast, the reinforcement learning approach proposed in this paper is more general and can be applied to a variety of systems. Related Work In this section, we address related work on dependency parsing, including alternative approaches for reducing error propagation, and reinforcement learning. 2.1 Dependency Parsing We use Chen and Manning (2014)’s parser as a basis for our experiments. Their parser is opensource and has served as a reference point for many recent publications (Dyer et al., 2015; Weiss e"
E17-1064,J14-2002,0,0.0351949,"Missing"
E17-1064,Q14-1010,0,0.0710427,"g and Nivre, 2012). Traditional static oracles can generate only one sequence of actions per sentence. A dynamic oracle gives all trajectories leading to the best possible result from every valid parse configuration. They can therefore be used to generate more training sequences including those containing errors. A drawback of this approach is that dynamic oracles have to be developed specifically for individual transition systems (e.g. arc-standard, arceager). Therefore, a large number of dynamic oracles have been developed in recent years (Goldberg and Nivre, 2012; Goldberg and Nivre, 2013; Goldberg et al., 2014; Gomez-Rodriguez et al., 2014; Björkelund and Nivre, 2015). In contrast, the reinforcement learning approach proposed in this paper is more general and can be applied to a variety of systems. Related Work In this section, we address related work on dependency parsing, including alternative approaches for reducing error propagation, and reinforcement learning. 2.1 Dependency Parsing We use Chen and Manning (2014)’s parser as a basis for our experiments. Their parser is opensource and has served as a reference point for many recent publications (Dyer et al., 2015; Weiss et al., 2015; Alberti et"
E17-1064,nivre-etal-2006-maltparser,0,0.102212,"Missing"
E17-1064,D14-1099,0,0.0142371,"ditional static oracles can generate only one sequence of actions per sentence. A dynamic oracle gives all trajectories leading to the best possible result from every valid parse configuration. They can therefore be used to generate more training sequences including those containing errors. A drawback of this approach is that dynamic oracles have to be developed specifically for individual transition systems (e.g. arc-standard, arceager). Therefore, a large number of dynamic oracles have been developed in recent years (Goldberg and Nivre, 2012; Goldberg and Nivre, 2013; Goldberg et al., 2014; Gomez-Rodriguez et al., 2014; Björkelund and Nivre, 2015). In contrast, the reinforcement learning approach proposed in this paper is more general and can be applied to a variety of systems. Related Work In this section, we address related work on dependency parsing, including alternative approaches for reducing error propagation, and reinforcement learning. 2.1 Dependency Parsing We use Chen and Manning (2014)’s parser as a basis for our experiments. Their parser is opensource and has served as a reference point for many recent publications (Dyer et al., 2015; Weiss et al., 2015; Alberti et al., 2015; Honnibal and Johns"
E17-1064,W06-2933,0,0.117553,"Missing"
E17-1064,D14-1140,0,0.061318,"Missing"
E17-1064,Y13-1026,0,0.0207497,"s paper, we apply reinforcement learning to greedy dependency parsing which is known to suffer from error propagation. Reinforcement learning improves accuracy of both labeled and unlabeled dependencies of the Stanford Neural Dependency Parser, a high performance greedy parser, while maintaining its efficiency. We investigate the portion of errors which are the result of error propagation and confirm that reinforcement learning reduces the occurrence of error propagation. 1 Introduction Error propagation is a common problem for many NLP tasks (Song et al., 2012; Quirk and CorstonOliver, 2006; Han et al., 2013; Gildea and Palmer, 2002; Yang and Cardie, 2013). It can occur when NLP tools applied early on in a pipeline make mistakes that have negative impact on higher-level tasks further down the pipeline. It can also occur within the application of a specific task, when sequential decisions are taken and errors made early in the process affect decisions made later on. When reinforcement learning is applied, a system actively tries out different sequences of actions. Most of these sequences will contain some errors. We hypothesize that a system trained in this manner will be more robust and less susc"
E17-1064,W03-3017,0,0.114505,"where Σ is a stack, β is a buffer containing the remaining input tokens and A are the dependency arcs that are created during parsing process. At initiation, the stack contains only the root symbol (Σ = [ROOT]), the buffer contains the tokens of the sentence (β = [w1 , ..., wn ]) and the set of arcs is empty (A = ∅). The arc-standard system supports three transitions. When σ1 is the top element and σ2 the second element on the stack, and β1 the first element of the buffer:1 To demonstrate that reinforcement learning can train different systems, we also carried out experiments with arc-eager (Nivre, 2003) and swapstandard (Nivre, 2009). Arc-eager is designed for incremental parsing and included in the popular MaltParser (Nivre et al., 2006a). Swap-standard is a simple and effective solution to unprojective dependency trees. Because arc-eager does not guarantee complete parse trees, we used a variation that employs an action called UNSHIFT to resume processing of tokens that would otherwise not be attached to a head (Nivre and FernándezGonzález, 2014). l LEFTl adds an arc σ1 → − σ2 to A and removes σ2 from the stack. l RIGHTl adds an arc σ2 → − σ1 to A and removes σ1 from the stack. SHIFT moves"
E17-1064,W04-0308,0,0.296143,"higher-level tasks further down the pipeline. It can also occur within the application of a specific task, when sequential decisions are taken and errors made early in the process affect decisions made later on. When reinforcement learning is applied, a system actively tries out different sequences of actions. Most of these sequences will contain some errors. We hypothesize that a system trained in this manner will be more robust and less susceptible to error propagation. We test our hypothesis by applying reinforcement learning to greedy transition-based parsers (Yamada and Matsumoto, 2003; Nivre, 2004), 1. We introduce Approximate Policy Gradient (APG), a new algorithm that is suited for dependency parsing and other structured prediction problems. 2. We show that this algorithm improves the accuracy of a high-performance greedy parser. 677 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 677–687, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics smaller embeddings, rectified linear instead of cubic activation function, and two hidden layers instead of one. They furthermo"
E17-1064,D15-1162,0,0.0130835,"iguez et al., 2014; Björkelund and Nivre, 2015). In contrast, the reinforcement learning approach proposed in this paper is more general and can be applied to a variety of systems. Related Work In this section, we address related work on dependency parsing, including alternative approaches for reducing error propagation, and reinforcement learning. 2.1 Dependency Parsing We use Chen and Manning (2014)’s parser as a basis for our experiments. Their parser is opensource and has served as a reference point for many recent publications (Dyer et al., 2015; Weiss et al., 2015; Alberti et al., 2015; Honnibal and Johnson, 2015, among others). They provide an efficient neural network that learns dense vector representations of words, PoS-tags and dependency labels. This small set of features makes their parser significantly more efficient than other popular parsers, such as the Malt (Nivre et al., 2007) or MST (McDonald et al., 2005) parser while obtaining higher accuracy. They acknowledge the error propagation problem of greedy parsers, but leave addressing this through (e.g.) beam search for future work. Dyer et al. (2015) introduce an approach that uses Long Short-Term Memory (LSTM). Their parser still works incr"
E17-1064,P09-1040,0,0.0313598,"fer containing the remaining input tokens and A are the dependency arcs that are created during parsing process. At initiation, the stack contains only the root symbol (Σ = [ROOT]), the buffer contains the tokens of the sentence (β = [w1 , ..., wn ]) and the set of arcs is empty (A = ∅). The arc-standard system supports three transitions. When σ1 is the top element and σ2 the second element on the stack, and β1 the first element of the buffer:1 To demonstrate that reinforcement learning can train different systems, we also carried out experiments with arc-eager (Nivre, 2003) and swapstandard (Nivre, 2009). Arc-eager is designed for incremental parsing and included in the popular MaltParser (Nivre et al., 2006a). Swap-standard is a simple and effective solution to unprojective dependency trees. Because arc-eager does not guarantee complete parse trees, we used a variation that employs an action called UNSHIFT to resume processing of tokens that would otherwise not be attached to a head (Nivre and FernándezGonzález, 2014). l LEFTl adds an arc σ1 → − σ2 to A and removes σ2 from the stack. l RIGHTl adds an arc σ2 → − σ1 to A and removes σ1 from the stack. SHIFT moves β1 to the stack. 3.2 Training"
E17-1064,W06-1608,0,0.0993323,"Missing"
E17-1064,W14-6111,0,0.0816372,"Missing"
E17-1064,P16-1159,0,0.0242849,"ge the model and apply beam search. It is thus unclear to what extend their improvements are due to reinforcement learning. Even though most approaches mentioned above improve the results reported by Chen and Manning (2014) and even more impressive results on 678 We directly compare our approach to REINFORCE, whereas we leave a direct comparison to L2S for future work. Our experiments show that our algorithm results in lower variance and achieves better performance than REINFORCE. Recent work addresses the approximation of reinforcement learning gradient in the context of machine translation. Shen et al. (2016)’s algorithm is roughly equivalent to the combination of an oracle and random sampling. Their approach differs from ours, because it does not retain memory across iteration as in our best-performing model (see Section 3.4). dependency parsing have been achieved since (notably, Andor et al. (2016)), Chen and Manning’s parser provides a better baseline for our purposes. We aim at investigating the influence of reinforcement learning on error propagation and want to test this in a clean environment, where reinforcement learning does not interfere with other methods that address the same problem."
E17-1064,P12-1108,0,0.0312543,"when mistakes are made early in a process. In this paper, we apply reinforcement learning to greedy dependency parsing which is known to suffer from error propagation. Reinforcement learning improves accuracy of both labeled and unlabeled dependencies of the Stanford Neural Dependency Parser, a high performance greedy parser, while maintaining its efficiency. We investigate the portion of errors which are the result of error propagation and confirm that reinforcement learning reduces the occurrence of error propagation. 1 Introduction Error propagation is a common problem for many NLP tasks (Song et al., 2012; Quirk and CorstonOliver, 2006; Han et al., 2013; Gildea and Palmer, 2002; Yang and Cardie, 2013). It can occur when NLP tools applied early on in a pipeline make mistakes that have negative impact on higher-level tasks further down the pipeline. It can also occur within the application of a specific task, when sequential decisions are taken and errors made early in the process affect decisions made later on. When reinforcement learning is applied, a system actively tries out different sequences of actions. Most of these sequences will contain some errors. We hypothesize that a system trained"
E17-1064,P15-1032,0,0.0301242,"e, 2013; Goldberg et al., 2014; Gomez-Rodriguez et al., 2014; Björkelund and Nivre, 2015). In contrast, the reinforcement learning approach proposed in this paper is more general and can be applied to a variety of systems. Related Work In this section, we address related work on dependency parsing, including alternative approaches for reducing error propagation, and reinforcement learning. 2.1 Dependency Parsing We use Chen and Manning (2014)’s parser as a basis for our experiments. Their parser is opensource and has served as a reference point for many recent publications (Dyer et al., 2015; Weiss et al., 2015; Alberti et al., 2015; Honnibal and Johnson, 2015, among others). They provide an efficient neural network that learns dense vector representations of words, PoS-tags and dependency labels. This small set of features makes their parser significantly more efficient than other popular parsers, such as the Malt (Nivre et al., 2007) or MST (McDonald et al., 2005) parser while obtaining higher accuracy. They acknowledge the error propagation problem of greedy parsers, but leave addressing this through (e.g.) beam search for future work. Dyer et al. (2015) introduce an approach that uses Long Short"
E17-1064,W03-3023,0,0.152161,"that have negative impact on higher-level tasks further down the pipeline. It can also occur within the application of a specific task, when sequential decisions are taken and errors made early in the process affect decisions made later on. When reinforcement learning is applied, a system actively tries out different sequences of actions. Most of these sequences will contain some errors. We hypothesize that a system trained in this manner will be more robust and less susceptible to error propagation. We test our hypothesis by applying reinforcement learning to greedy transition-based parsers (Yamada and Matsumoto, 2003; Nivre, 2004), 1. We introduce Approximate Policy Gradient (APG), a new algorithm that is suited for dependency parsing and other structured prediction problems. 2. We show that this algorithm improves the accuracy of a high-performance greedy parser. 677 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 677–687, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics smaller embeddings, rectified linear instead of cubic activation function, and two hidden layers instead of one."
E17-1064,P13-1161,0,0.0304326,"greedy dependency parsing which is known to suffer from error propagation. Reinforcement learning improves accuracy of both labeled and unlabeled dependencies of the Stanford Neural Dependency Parser, a high performance greedy parser, while maintaining its efficiency. We investigate the portion of errors which are the result of error propagation and confirm that reinforcement learning reduces the occurrence of error propagation. 1 Introduction Error propagation is a common problem for many NLP tasks (Song et al., 2012; Quirk and CorstonOliver, 2006; Han et al., 2013; Gildea and Palmer, 2002; Yang and Cardie, 2013). It can occur when NLP tools applied early on in a pipeline make mistakes that have negative impact on higher-level tasks further down the pipeline. It can also occur within the application of a specific task, when sequential decisions are taken and errors made early in the process affect decisions made later on. When reinforcement learning is applied, a system actively tries out different sequences of actions. Most of these sequences will contain some errors. We hypothesize that a system trained in this manner will be more robust and less susceptible to error propagation. We test our hypothe"
E17-1064,W09-3838,0,0.0201243,"but leave addressing this through (e.g.) beam search for future work. Dyer et al. (2015) introduce an approach that uses Long Short-Term Memory (LSTM). Their parser still works incrementally and the number of required operations grows linearly with the length of the sentence, but it uses the complete buffer, stack and history of parsing decisions, giving the model access to global information. Weiss et al. (2015) introduce several improvements on Chen and Manning (2014)’s parser. Most importantly, they put a globally-trained perceptron layer instead of a softmax output layer. Their model uses Zhang and Chan (2009) present the only study we are aware of that also uses reinforcement learning for dependency parsing. They compare their results to Nivre et al. (2006b) using the same features, but they also change the model and apply beam search. It is thus unclear to what extend their improvements are due to reinforcement learning. Even though most approaches mentioned above improve the results reported by Chen and Manning (2014) and even more impressive results on 678 We directly compare our approach to REINFORCE, whereas we leave a direct comparison to L2S for future work. Our experiments show that our al"
fokkens-etal-2012-climb,W05-1522,0,\N,Missing
fokkens-etal-2012-climb,W02-1502,0,\N,Missing
fokkens-etal-2012-climb,C94-2144,0,\N,Missing
fokkens-etal-2012-climb,P98-1033,0,\N,Missing
fokkens-etal-2012-climb,C98-1033,0,\N,Missing
fokkens-etal-2012-climb,P11-1107,1,\N,Missing
fokkens-etal-2012-climb,Y11-1025,1,\N,Missing
fokkens-etal-2012-climb,W11-3404,1,\N,Missing
fokkens-etal-2014-biographynet,N07-4013,0,\N,Missing
fokkens-etal-2014-biographynet,W13-1202,1,\N,Missing
L16-1187,P11-1059,0,0.0216625,"ent to annotate the event, as a representative of the whole proposition, as the target of factuality. This is because factuality cues can target specific relations within a proposition. To clarify, consider the following example, taken from FactBank: 1182 12 13 TimeBank/FactBank – APW19980227.476-S1 GEN AUTHOR denotes a non-explicit generic source. We call this phenomenon perspective scope, referring to those specific propositional relations associated with an event (or entity) that are affected by a perspective cue. It is strongly related to the scope and focus of negation as investigated by Blanco and Moldovan (2011), but to our knowledge its annotation has not been investigated before in the context of factuality (or sentiment). We believe that perspective scope is an important and innovative aspect of our annotation scheme, and our formal model GRaSP allows for the representation of separate factuality assignments for the event and its relations. In the near future, we will work out the details with respect to its annotation. 4.4. Opinion Layer The final annotation layer that we have included in our scheme is that of opinion or sentiment. As our annotations are largely based on Wiebe et al. (2005) and T"
L16-1187,N15-1146,0,0.0316723,"of factuality. In the factuality layer, each event identified in the event layer is to be annotated as the target of a factuality relation. In the opinion layer, annotators have no clear pre-defined targets; instead, they need to look for cues and understand the text in more detail. The first thing to look for are attributional cues, since they are fairly easy to recognize and some of them will already have been identified in the attribution layer. An example of such an attributional cue is support in our example sentence repeated below, which expresses a positive attitude of Mbeki. Following Deng and Wiebe (2015), we aim to identify the specific entities and events that are the target of the opinion. In this case, there are two targets: the entity denoted by Mugabe and the event expressed by elections. 10. Investors and Western diplomats have saide1 they might interprete2 {Mbeki’s}SENT- SOURCE {supporte3 }SENT- CUE for {Mugabe}SENT- TARGET or the {electionse4 }SENT- TARGET as a sign that Africa is not intent on revitalizinge5 its economies through good governmente6 and expanded international tradee7 . The other two types of cues are also present in our example. An example of a factual opinion cue is e"
L16-1187,W09-3012,0,0.198309,"events and their causes (e.g. conspiracy theories on 9/11). Textual data always provide specific perspectives of the author and quoted sources on the information they contain. Mining information from texts thus implies dealing with these perspectives. In the last decade, different aspects of linguistic encoding of perspectives have been targeted as separated phenomena through different annotation initiatives, each with its own approaches and goals. Targeted aspects of perspectives include, for example, attribution (Prasad et al., 2007; Pareti, 2012), factuality (Saur´ı and Pustejovsky, 2009; Diab et al., 2009), and opinion (Wiebe et al., 2005). Coordinating initiatives such as the Unified Linguistic Annotation project1 have tried to technically combine such annotations into a unique annotation model, but they lack an overarching framework for the various layers of annotation from different resources. Furthermore, annotation initiatives such as those proposed by Prasad et al. (2007) and Pareti (2012) have attempted to tackle the annotation of perspectives in a unified approach, but with different levels of success.2 In our approach, the notion of perspectives lies at the semantic-pragmatic interface"
L16-1187,pareti-2012-database,0,0.1945,"ortion, vaccinations, etc.), and interpretative frames on events and their causes (e.g. conspiracy theories on 9/11). Textual data always provide specific perspectives of the author and quoted sources on the information they contain. Mining information from texts thus implies dealing with these perspectives. In the last decade, different aspects of linguistic encoding of perspectives have been targeted as separated phenomena through different annotation initiatives, each with its own approaches and goals. Targeted aspects of perspectives include, for example, attribution (Prasad et al., 2007; Pareti, 2012), factuality (Saur´ı and Pustejovsky, 2009; Diab et al., 2009), and opinion (Wiebe et al., 2005). Coordinating initiatives such as the Unified Linguistic Annotation project1 have tried to technically combine such annotations into a unique annotation model, but they lack an overarching framework for the various layers of annotation from different resources. Furthermore, annotation initiatives such as those proposed by Prasad et al. (2007) and Pareti (2012) have attempted to tackle the annotation of perspectives in a unified approach, but with different levels of success.2 In our approach, the n"
L16-1187,S15-1009,0,0.0145949,"uting the description of the single ‘real’ event in this sentence expressed by demonstrations. In other words, only one event in this sentence should be put on a timeline. In this framework, syntax is only used to decide on the span of the event. The two main corpora for factuality (or belief) are FactBank (Saur´ı and Pustejovsky, 2009) and the Lexical Understanding (LU) Annotation Corpus (Diab et al., 2009). Although both corpora address the same phenomenon (i.e. the commitment of a source towards the truth of some event/proposition), the annotations are quite different (Werner et al., 2015; Prabhakaran et al., 2015). The LU Corpus so far has only addressed the problem from the perspective of the speaker/writer, in contrast to FactBank, which has fully annotated nested sources. Furthermore, the LU corpus ignores negation (the polarity axis of factuality in FactBank) and does not distinguish between POSSI BLE and PROBABLE (the certainty axis of factuality in FactBank). Finally, there is a subtle difference in the targets. Whereas FactBank has assigned factuality values to events, the LU corpus has assigned belief tags to the head words of propositions, disregarding event-denoting noun phrases (e.g. the col"
L16-1187,pustejovsky-etal-2010-iso,0,0.0225563,"d approach. Section 4 describes the four layers that we have currently defined for the an1177 notation of perspectives: events, attribution, factuality, and opinion. Finally, we conclude and summarize our work and give an outlook on future work in Section 5. 2. Related Work In our annotations, events play an important role because we consider them to be the basic semantic elements that may give rise to or be involved in perspectives. A wellknown specification language for events is TimeML (Pustejovsky et al., 2003a), which has been consolidated as an international cross-language ISO standard (Pustejovsky et al., 2010) and has been used as the annotation language for the TempEval shared task series (Verhagen et al., 2009). Its reference corpus is TimeBank (Pustejovsky et al., 2003b). TimeML defines an event as something that can be said to obtain or hold true, to happen or to occur. TimeML adopts a surface-based annotation of texts and morpho-syntactic information plays a key role for detecting all possible mentions of an event. According to TimeML, both demonstrations and taken (place) in Example 1 are to be annotated as valid event mentions. 1. Several pro-Iraq demonstrations have taken place in the last"
L16-1187,P15-5003,0,0.0165306,"e represented as: PARTY −−−−−−−−−→ MARY OrganizedBy Note that both Examples 3 and 4 mention the O RGA NIZED B Y relation between Mary and his birthday party; however, whereas in the former it functions merely as additional information on the target, in the latter it is the target of the perspective. The schematic representation in Figure 1 illustrates the differences between these targets. Not all attitude dimensions can take the same types of targets. For example, opinion can target entities, events or propositional relations, but factuality can only target events or propositional relations (Rambow and Wiebe, 2015). Whereas the source and target are usually expressed by a single linguistic unit (e.g. an NP or a clause), the attitude may be expressed either by a single linguistic cue or a combination of cues.6 For example, the commitment of a source towards the factual nature of an event or proposition may be expressed by a combination of polarity (e.g. not, never) and modality cues (e.g. could, maybe). In turn, one cue can express multiple attitude dimensions. For example, a verb like hope expresses positive sentiment and uncertainty towards the target at the same time. 5 If the author of a document is"
L16-1187,P10-1059,0,0.0384465,"), but to our knowledge its annotation has not been investigated before in the context of factuality (or sentiment). We believe that perspective scope is an important and innovative aspect of our annotation scheme, and our formal model GRaSP allows for the representation of separate factuality assignments for the event and its relations. In the near future, we will work out the details with respect to its annotation. 4.4. Opinion Layer The final annotation layer that we have included in our scheme is that of opinion or sentiment. As our annotations are largely based on Wiebe et al. (2005) and Toprak et al. (2010), the three main elements are defined as follows: • Source: The entity that has a positive or negative attitude towards some target. • Cue: A linguistic cue that, possibly in combination with other cues, expresses the positive or negative attitude of the source towards the target. We regard cues as belonging to one of the following categories: – Attributional cue: contributes a source while directly expressing the positive or negative attitude of the source towards the embedded target; – Indirect cue: signals the positive or negative attitude of the source by the choice of words; – Factual opi"
L16-1187,W15-1304,0,0.0122515,"merely helps constituting the description of the single ‘real’ event in this sentence expressed by demonstrations. In other words, only one event in this sentence should be put on a timeline. In this framework, syntax is only used to decide on the span of the event. The two main corpora for factuality (or belief) are FactBank (Saur´ı and Pustejovsky, 2009) and the Lexical Understanding (LU) Annotation Corpus (Diab et al., 2009). Although both corpora address the same phenomenon (i.e. the commitment of a source towards the truth of some event/proposition), the annotations are quite different (Werner et al., 2015; Prabhakaran et al., 2015). The LU Corpus so far has only addressed the problem from the perspective of the speaker/writer, in contrast to FactBank, which has fully annotated nested sources. Furthermore, the LU corpus ignores negation (the polarity axis of factuality in FactBank) and does not distinguish between POSSI BLE and PROBABLE (the certainty axis of factuality in FactBank). Finally, there is a subtle difference in the targets. Whereas FactBank has assigned factuality values to events, the LU corpus has assigned belief tags to the head words of propositions, disregarding event-denoting"
L16-1714,artola-etal-2014-stream,1,0.527528,"pport a flexible modular pipeline. The details of this approach will be discussed in Section 4. The live streaming computing approach presented in Section 5. relies on the Apache Storm framework6 for implementing scalable processing of data streams. 3 https://hadoop.apache.org/ http://hadoop.apache.org/ docs/current/hadoop-streaming/ HadoopStreaming.html 5 http://www.cascading.org 6 https://storm.apache.org/ 4 Figure 1: Overview of NLP modules Storm is a framework for streaming computing whose aim is to implement highly-scalable and parallel processing of data streams. The system presented in Artola et al. (2014) uses Storm to integrate and orchestrate an NLP pipeline comprised by many modules, but it does so following a batch processing paradigm. The streaming approach presented here is an extension of Artola et al. (2014) to deal with streaming scenarios. 3. A pipeline for event recognition The frameworks we present here are independent of the exact NLP modules that are used in the pipeline. The only requirement posed to the modules is that of using the NLP Annotation Format (Fokkens et al., 2014, NAF) for representing linguistic annotations when using the Storm architecture. The Hadoop setup can in"
L16-1714,vossen-etal-2014-newsreader,0,0.0278599,"., we describe the background of this work. This is followed by a presentation of the NLP modules we use in Section 3. Sections 4. and 5. introduce the Hadoop architecture for batch processing and the Storm architecture for live streaming, respectively. This is followed by the conclusion in Section 6. 2. Background and related work Everyday, around 2 million news articles from thousands of sources are published and this number is increasing.2 Keeping track of all this information, or even a subset of information about a specific domain, is unfeasible without technological support. NewsReader (Vossen et al., 2014) aims to provide such support by performing detailed linguistic analyses identifying what happened to whom, when and where in large amounts of data in four languages, namely, English, Spanish, Italian and Dutch. The extracted information is stored as structured data in RDF (to be specific, as Event-Centric Knowledge Graphs (Rospocher et al., 2016)) so that information specialists can carry out precise search over the data. Vossen et al. (2014) explain how NewsReader functions as a history recorder: keeping track and storing every1 http://www.newsreader-project.eu These numbers are based on an"
L18-1484,P16-1231,0,0.0220767,"es unweighted similarity is used by Schenk and Chiarcos (2016). Our results show that adding multi-way selectional preference improves results on explicit semantic roles, but not for iSRL. Recently work by Do et al. (2017) is closest to our work but they apply their methods on nominal data and did not compare one-way and multi-way selectional preference. 2.3. Neural networks Recent years have witnessed a surge of research interest in neural networks for natural language processing (Goldberg, 2016). Plenty of models have been proposed for various tasks (Godbole et al., 2015; Zhou and Xu, 2015; Andor et al., 2016, among many others). Apart from Do et al. (2017) who uses a different architecture for a different version of the task, we are not aware of work that applies neural networks to iSRL. 3. Models In this study, we focus exclusively on DNI resolution, the last and hardest step in iSRL. For each test case, we assume that the predicate p is already identified and disambiguated, the target role r∗ is given, and overt roles are coupled with their fillers {(rj , gj )|j = 1..m}. The goal is to rank the correct filler highest among the candidates {ci |i = 1..n}. To test a simple multi-way selectional pr"
L18-1484,D14-1082,0,0.084309,"Missing"
L18-1484,S10-1059,0,0.0141869,"indefinite (INI) and definite (DNI). To reuse examples from Ruppenhofer et al. (2010), in the blog headline More babbling about what it means to know, the subject of knowing is not expected to be instantiated within the discourse. In contrast, in the sentence Don’t tell me you didn’t know!, the hearer expects a concrete filler for the role of what (s)he should know and it can be expected to be present in previous context. The first example is a case of INI while the second is a DNI. 2.1. Previous work on iSRL Traditional SRL techniques led to very low results for iSRL due to data sparseness (Chen et al., 2010; Tonelli and Delmonte, 2010). Researchers therefore explored simpler alternatives such as BayesNet (Silberer and Frank, 2012; Roth and Frank, 2013; Roth and Frank, 2015), Na¨ıve Bayes (Feizabadi and Pado, 2015), and memory-based learning (Schenk et al., 2015). Others proposed non-parametric approaches such as observed frequency (Laparra and Rigau, 2012), prototypical vectors (Schenk and Chiarcos, 2016) and other heuristics (Laparra and Rigau, 2013; Gorinski et al., 2013). In addition to methods of machine learning and heuristics, previous work investigated the possibilities of increasing trai"
L18-1484,I17-1010,0,0.0755963,"ti-way selectional preference (van de Cruys, 2014). One-way selectional preferences have been applied to implicit semantic role labeling before. Silberer and Frank (2012)’s system include a feature calculated using weighted similarity to head words that are observed to fill a role. The selectional preference model itself is described in (Erk, 2007) and (Resnik, 1996). A simpler model that uses unweighted similarity is used by Schenk and Chiarcos (2016). Our results show that adding multi-way selectional preference improves results on explicit semantic roles, but not for iSRL. Recently work by Do et al. (2017) is closest to our work but they apply their methods on nominal data and did not compare one-way and multi-way selectional preference. 2.3. Neural networks Recent years have witnessed a surge of research interest in neural networks for natural language processing (Goldberg, 2016). Plenty of models have been proposed for various tasks (Godbole et al., 2015; Zhou and Xu, 2015; Andor et al., 2016, among many others). Apart from Do et al. (2017) who uses a different architecture for a different version of the task, we are not aware of work that applies neural networks to iSRL. 3. Models In this st"
L18-1484,P07-1028,0,0.0119922,"tics, co-occurring roles also have an influence. For example, if lead.01’s role A4 (goal) is filled by the guest house, the nation is an implausible filler for A1 (thing led), while it is perfectly plausible had we not known what fills A4. This is known as multi-way selectional preference (van de Cruys, 2014). One-way selectional preferences have been applied to implicit semantic role labeling before. Silberer and Frank (2012)’s system include a feature calculated using weighted similarity to head words that are observed to fill a role. The selectional preference model itself is described in (Erk, 2007) and (Resnik, 1996). A simpler model that uses unweighted similarity is used by Schenk and Chiarcos (2016). Our results show that adding multi-way selectional preference improves results on explicit semantic roles, but not for iSRL. Recently work by Do et al. (2017) is closest to our work but they apply their methods on nominal data and did not compare one-way and multi-way selectional preference. 2.3. Neural networks Recent years have witnessed a surge of research interest in neural networks for natural language processing (Goldberg, 2016). Plenty of models have been proposed for various task"
L18-1484,S15-1005,0,0.06309,"ore the possibilities of using more complex and expressive machine learning models trained on a large amount of explicit roles. In addition, we compare the impact of one-way and multi-way selectional preference with the hypothesis that the added information in multi-way models are beneficial. Although our models surpass a baseline that uses prototypical vectors for SemEval-2010, we otherwise face mostly negative results. Selectional preference models perform lower than the baseline on ON5V, a dataset of five ambiguous and frequent verbs. They are also outperformed by the Na¨ıve Bayes model of Feizabadi and Pado (2015) on both datasets. We conclude that, even though multi-way selectional preference improves results for predicting explicit semantic roles compared to one-way selectional preference, it harms performance for implicit roles. We release our source code, including the reimplementation of two previously unavailable systems to enable further experimentation. Keywords: neural network, implicit semantic role labeling, selectional preferences 1. Introduction Defined as the recovery of semantic roles beyond immediate syntactic structure, implicit Semantic Roles Labeling (iSRL) can contribute valuable in"
L18-1484,P13-1166,1,0.823151,"differ in the dataset and the metrics they use for evaluation, and (3) to our knowledge, none of the existing systems is available as open source code. This has led to a situation that is typical for challenging tasks using small datasets: it is almost impossible to determine what the state-of-the-art approach is and how new work relates to this. Even results from papers that are evaluated on the same dataset are difficult to compare, because differences in results can be due to the difference in features, machine learning algorithm, method of extending data, heuristics or (as pointed out in Fokkens et al. (2013)) choices in preprocessing and data preparation. As part of this research, we built an experimental platform for iSRL. This platform provides open source implementations for the experiments reported in this paper, for the system described in Schenk and Chiarcos (2016) which inspired our own approach and for Feizabadi and Pado (2015)’s approach which provided state-of-the-art performance on SemEval2010. 3062 The rest of the paper is organized as follows: Section 2 summarizes the foundation of iSRL and related work. In Section 3, we outline our models of selectional preference. Section 4 quantif"
L18-1484,W13-0111,0,0.0215483,"second is a DNI. 2.1. Previous work on iSRL Traditional SRL techniques led to very low results for iSRL due to data sparseness (Chen et al., 2010; Tonelli and Delmonte, 2010). Researchers therefore explored simpler alternatives such as BayesNet (Silberer and Frank, 2012; Roth and Frank, 2013; Roth and Frank, 2015), Na¨ıve Bayes (Feizabadi and Pado, 2015), and memory-based learning (Schenk et al., 2015). Others proposed non-parametric approaches such as observed frequency (Laparra and Rigau, 2012), prototypical vectors (Schenk and Chiarcos, 2016) and other heuristics (Laparra and Rigau, 2013; Gorinski et al., 2013). In addition to methods of machine learning and heuristics, previous work investigated the possibilities of increasing training data. Feizabadi and Pado (2015) combine multiple corpora and apply domain adaptation methods to deal with the difference in genre. They demonstrated that combining two iSRL corpora led to improved performance. Silberer and Frank (2012) and Roth and Frank (2015) used heuristics to generate iSRL training examples from manually and automatically annotated SRL corpora. This work differs from these approaches, because their research focused on creating iSRL training examp"
L18-1484,P13-1116,0,0.100559,"since its first shared task eight years ago (Ruppenhofer et al., 2010). The main difficulty faced by researchers is the small size of training data. Compared to traditional SRL datasets, SemEval-2010 is hundreds-fold smaller, containing only slightly more than a hundred of training examples (Table 1). Early work applying traditional semantic role labeling (SRL) techniques to iSRL was met with deflating results. Therefore, researchers limited themselves to simplistic machine learning models such as Na¨ıve Bayes (Feizabadi and Pado, 2015, among others) or abandoned machine learning altogether (Laparra and Rigau, 2013). Several studies were devoted to the automatic expansion of training data (see Section 2 for an overview). This paper presents an attempt to recover implicit semantic roles using neural networks. We take advantage of the fact that OntoNotes contains a vast amount of manually annotated explicit semantic roles from which we can learn the selectional preference of frames (e.g. look.01 prefers animate fillers for role A0 (looker)). A neural network is used to capture complex interactions between a predicate, a target role and its co-occurring roles. In addition, we compare the impact of one-way s"
L18-1484,W13-0211,0,0.0177417,"plicit semantic roles. The organizers provide two versions of the same dataset: one annotated with FrameNet roles and the other PropBank. Because OntoNotes was compiled using PropBank, we also use the PropBank version of SemEval-2010. Note that OntoNotes differs from SemEval-2010 in task (explicit versus implicit SRL), genres (news, weblogs and conversations versus novel) and time period (20th century versus 19th century). Training on OntoNotes SRL and testing on SemEval-2010 iSRL can be seen as a form of domain adaptation and requires powerful generalization. We also test our models on ON5V (Moor et al., 2013) which poses a different challenge. Implicit semantic roles were manually annotated on top of explicit semantic roles and other linguistic information on a selection of Role A2 A2 A2 A2 A2 A2 A2 A2 A2 Filler head refugees U.S. families trust Warner they lenders lawyers one Filler full the refugees U.S. the victims’ families this trust AOL Time Warner they lenders lawyers one Table 2: Examples from ON5V showing the diversity of fillers in terms of semantic types, part-of-speech, and topics. OntoNotes documents. The authors chose five highly frequent verbs to annotate in order to create “high-vo"
L18-1484,P10-1044,0,0.0272182,"that combining two iSRL corpora led to improved performance. Silberer and Frank (2012) and Roth and Frank (2015) used heuristics to generate iSRL training examples from manually and automatically annotated SRL corpora. This work differs from these approaches, because their research focused on creating iSRL training examples of reasonable quality rather than using a SRL resource directly. 2.2. Selectional preferences Selectional preference has a long research tradition (Katz and Fodor, 1963) and has been applied in various tasks such as syntactic parsing (Zhou et al., 2011), textual inference (Ritter et al., 2010), and semantic role labeling (Zapirain et al., 2013). The idea is simple: a role is filled with some words more frequently than others. For example, the man is much more likely a filler for the role A0 (leader) of the predicate lead.01 than e.g. the bottle (an inanimate object) although one can construct a grammatically and semantically correct example for each filler. Next to the role’s semantics, co-occurring roles also have an influence. For example, if lead.01’s role A4 (goal) is filled by the guest house, the nation is an implausible filler for A1 (thing led), while it is perfectly plausi"
L18-1484,S13-1043,0,0.015587,"o know, the subject of knowing is not expected to be instantiated within the discourse. In contrast, in the sentence Don’t tell me you didn’t know!, the hearer expects a concrete filler for the role of what (s)he should know and it can be expected to be present in previous context. The first example is a case of INI while the second is a DNI. 2.1. Previous work on iSRL Traditional SRL techniques led to very low results for iSRL due to data sparseness (Chen et al., 2010; Tonelli and Delmonte, 2010). Researchers therefore explored simpler alternatives such as BayesNet (Silberer and Frank, 2012; Roth and Frank, 2013; Roth and Frank, 2015), Na¨ıve Bayes (Feizabadi and Pado, 2015), and memory-based learning (Schenk et al., 2015). Others proposed non-parametric approaches such as observed frequency (Laparra and Rigau, 2012), prototypical vectors (Schenk and Chiarcos, 2016) and other heuristics (Laparra and Rigau, 2013; Gorinski et al., 2013). In addition to methods of machine learning and heuristics, previous work investigated the possibilities of increasing training data. Feizabadi and Pado (2015) combine multiple corpora and apply domain adaptation methods to deal with the difference in genre. They demons"
L18-1484,J15-4003,0,0.0136026,"knowing is not expected to be instantiated within the discourse. In contrast, in the sentence Don’t tell me you didn’t know!, the hearer expects a concrete filler for the role of what (s)he should know and it can be expected to be present in previous context. The first example is a case of INI while the second is a DNI. 2.1. Previous work on iSRL Traditional SRL techniques led to very low results for iSRL due to data sparseness (Chen et al., 2010; Tonelli and Delmonte, 2010). Researchers therefore explored simpler alternatives such as BayesNet (Silberer and Frank, 2012; Roth and Frank, 2013; Roth and Frank, 2015), Na¨ıve Bayes (Feizabadi and Pado, 2015), and memory-based learning (Schenk et al., 2015). Others proposed non-parametric approaches such as observed frequency (Laparra and Rigau, 2012), prototypical vectors (Schenk and Chiarcos, 2016) and other heuristics (Laparra and Rigau, 2013; Gorinski et al., 2013). In addition to methods of machine learning and heuristics, previous work investigated the possibilities of increasing training data. Feizabadi and Pado (2015) combine multiple corpora and apply domain adaptation methods to deal with the difference in genre. They demonstrated that combining t"
L18-1484,S10-1008,0,0.254088,"compared to one-way selectional preference, it harms performance for implicit roles. We release our source code, including the reimplementation of two previously unavailable systems to enable further experimentation. Keywords: neural network, implicit semantic role labeling, selectional preferences 1. Introduction Defined as the recovery of semantic roles beyond immediate syntactic structure, implicit Semantic Roles Labeling (iSRL) can contribute valuable information for obtaining complete semantic interpretations of text. Yet, it has been elusive since its first shared task eight years ago (Ruppenhofer et al., 2010). The main difficulty faced by researchers is the small size of training data. Compared to traditional SRL datasets, SemEval-2010 is hundreds-fold smaller, containing only slightly more than a hundred of training examples (Table 1). Early work applying traditional semantic role labeling (SRL) techniques to iSRL was met with deflating results. Therefore, researchers limited themselves to simplistic machine learning models such as Na¨ıve Bayes (Feizabadi and Pado, 2015, among others) or abandoned machine learning altogether (Laparra and Rigau, 2013). Several studies were devoted to the automatic"
L18-1484,N16-1173,0,0.126893,"e to determine what the state-of-the-art approach is and how new work relates to this. Even results from papers that are evaluated on the same dataset are difficult to compare, because differences in results can be due to the difference in features, machine learning algorithm, method of extending data, heuristics or (as pointed out in Fokkens et al. (2013)) choices in preprocessing and data preparation. As part of this research, we built an experimental platform for iSRL. This platform provides open source implementations for the experiments reported in this paper, for the system described in Schenk and Chiarcos (2016) which inspired our own approach and for Feizabadi and Pado (2015)’s approach which provided state-of-the-art performance on SemEval2010. 3062 The rest of the paper is organized as follows: Section 2 summarizes the foundation of iSRL and related work. In Section 3, we outline our models of selectional preference. Section 4 quantifies the effectiveness of selectional preference with regard to iSRL. Section 5 concludes the work and outlines future directions of research. 2. Background and Related Work In this section, we explain what implicit Semantic Role Labeling entails. This is followed by a"
L18-1484,R15-1074,0,0.0135443,"nce Don’t tell me you didn’t know!, the hearer expects a concrete filler for the role of what (s)he should know and it can be expected to be present in previous context. The first example is a case of INI while the second is a DNI. 2.1. Previous work on iSRL Traditional SRL techniques led to very low results for iSRL due to data sparseness (Chen et al., 2010; Tonelli and Delmonte, 2010). Researchers therefore explored simpler alternatives such as BayesNet (Silberer and Frank, 2012; Roth and Frank, 2013; Roth and Frank, 2015), Na¨ıve Bayes (Feizabadi and Pado, 2015), and memory-based learning (Schenk et al., 2015). Others proposed non-parametric approaches such as observed frequency (Laparra and Rigau, 2012), prototypical vectors (Schenk and Chiarcos, 2016) and other heuristics (Laparra and Rigau, 2013; Gorinski et al., 2013). In addition to methods of machine learning and heuristics, previous work investigated the possibilities of increasing training data. Feizabadi and Pado (2015) combine multiple corpora and apply domain adaptation methods to deal with the difference in genre. They demonstrated that combining two iSRL corpora led to improved performance. Silberer and Frank (2012) and Roth and Frank"
L18-1484,S12-1001,0,0.119352,"ling about what it means to know, the subject of knowing is not expected to be instantiated within the discourse. In contrast, in the sentence Don’t tell me you didn’t know!, the hearer expects a concrete filler for the role of what (s)he should know and it can be expected to be present in previous context. The first example is a case of INI while the second is a DNI. 2.1. Previous work on iSRL Traditional SRL techniques led to very low results for iSRL due to data sparseness (Chen et al., 2010; Tonelli and Delmonte, 2010). Researchers therefore explored simpler alternatives such as BayesNet (Silberer and Frank, 2012; Roth and Frank, 2013; Roth and Frank, 2015), Na¨ıve Bayes (Feizabadi and Pado, 2015), and memory-based learning (Schenk et al., 2015). Others proposed non-parametric approaches such as observed frequency (Laparra and Rigau, 2012), prototypical vectors (Schenk and Chiarcos, 2016) and other heuristics (Laparra and Rigau, 2013; Gorinski et al., 2013). In addition to methods of machine learning and heuristics, previous work investigated the possibilities of increasing training data. Feizabadi and Pado (2015) combine multiple corpora and apply domain adaptation methods to deal with the difference"
L18-1484,S10-1065,0,0.0152487,"nd definite (DNI). To reuse examples from Ruppenhofer et al. (2010), in the blog headline More babbling about what it means to know, the subject of knowing is not expected to be instantiated within the discourse. In contrast, in the sentence Don’t tell me you didn’t know!, the hearer expects a concrete filler for the role of what (s)he should know and it can be expected to be present in previous context. The first example is a case of INI while the second is a DNI. 2.1. Previous work on iSRL Traditional SRL techniques led to very low results for iSRL due to data sparseness (Chen et al., 2010; Tonelli and Delmonte, 2010). Researchers therefore explored simpler alternatives such as BayesNet (Silberer and Frank, 2012; Roth and Frank, 2013; Roth and Frank, 2015), Na¨ıve Bayes (Feizabadi and Pado, 2015), and memory-based learning (Schenk et al., 2015). Others proposed non-parametric approaches such as observed frequency (Laparra and Rigau, 2012), prototypical vectors (Schenk and Chiarcos, 2016) and other heuristics (Laparra and Rigau, 2013; Gorinski et al., 2013). In addition to methods of machine learning and heuristics, previous work investigated the possibilities of increasing training data. Feizabadi and Pado"
L18-1484,D14-1004,0,0.0604899,"Missing"
L18-1484,J13-3006,0,0.0558483,"Missing"
L18-1484,P15-1109,0,0.0197123,"mpler model that uses unweighted similarity is used by Schenk and Chiarcos (2016). Our results show that adding multi-way selectional preference improves results on explicit semantic roles, but not for iSRL. Recently work by Do et al. (2017) is closest to our work but they apply their methods on nominal data and did not compare one-way and multi-way selectional preference. 2.3. Neural networks Recent years have witnessed a surge of research interest in neural networks for natural language processing (Goldberg, 2016). Plenty of models have been proposed for various tasks (Godbole et al., 2015; Zhou and Xu, 2015; Andor et al., 2016, among many others). Apart from Do et al. (2017) who uses a different architecture for a different version of the task, we are not aware of work that applies neural networks to iSRL. 3. Models In this study, we focus exclusively on DNI resolution, the last and hardest step in iSRL. For each test case, we assume that the predicate p is already identified and disambiguated, the target role r∗ is given, and overt roles are coupled with their fillers {(rj , gj )|j = 1..m}. The goal is to rank the correct filler highest among the candidates {ci |i = 1..n}. To test a simple mult"
L18-1484,P11-1156,0,0.0309597,"difference in genre. They demonstrated that combining two iSRL corpora led to improved performance. Silberer and Frank (2012) and Roth and Frank (2015) used heuristics to generate iSRL training examples from manually and automatically annotated SRL corpora. This work differs from these approaches, because their research focused on creating iSRL training examples of reasonable quality rather than using a SRL resource directly. 2.2. Selectional preferences Selectional preference has a long research tradition (Katz and Fodor, 1963) and has been applied in various tasks such as syntactic parsing (Zhou et al., 2011), textual inference (Ritter et al., 2010), and semantic role labeling (Zapirain et al., 2013). The idea is simple: a role is filled with some words more frequently than others. For example, the man is much more likely a filler for the role A0 (leader) of the predicate lead.01 than e.g. the bottle (an inanimate object) although one can construct a grammatically and semantically correct example for each filler. Next to the role’s semantics, co-occurring roles also have an influence. For example, if lead.01’s role A4 (goal) is filled by the guest house, the nation is an implausible filler for A1"
L18-1484,W09-2417,0,0.0397631,"Missing"
L18-1590,P15-2072,0,0.0550092,"Missing"
L18-1590,D16-1148,0,0.0544224,"Missing"
L18-1590,fokkens-etal-2014-biographynet,1,0.614035,"Missing"
L18-1590,J13-4004,0,0.0126353,"e that forms a subpart of the pipeline for event extraction developed as part of the larger NewsReader (Vossen et al., 2016) and BiographyNet (Fokkens et al., 2017) pipelines for event extraction. The pipline includes the Alpino parser for dependency parsing (Bouma et al., 2001) and the ixa-pipe named entity recognizer (Agerri and Rigau, 2016). Both modules are run using a wrapper that provides output in the NLP Annotation Format (Fokkens et al., 2014a, NAF).1 We implemented a new system for Dutch coreference resolution, based on the Stanford multisieve-entity coreference resolution approach (Lee et al., 2013) that applies coreference resolution on top of this small pipeline.2 In the first step, we extract descriptions at a sentence level. We start by taking nouns as labels and then identify their properties by extracting their modifiers and attributes through copula constructions using the dependency structure. We also use syntactic dependencies to identify the ‘roles’ an entity plays. Alpino outputs ‘deep’ dependencies that indicate that the subject of a passive sentence has an object relation with the main verb. We take the simplified assumption that (deep) subjects are agents, (deep) objects ar"
L18-1590,W16-3207,0,0.0368861,"Missing"
P10-4001,I05-2035,1,0.893639,"nded entry fields, and the population of drop-down selectors. The lists in an iterated section can be expanded or shortened with “Add” and “Delete” buttons near the items in question. Drop-down selectors can be automatically populated in several different ways.4 These dynamic drop-downs greatly lessen the amount of information the user must remember while filling out the questionnaire and can prevent the user from trying to enter an invalid value. Both of these operations occur without refreshing the page, saving time for the user. Unbounded Content Early versions of the customization system (Bender and Flickinger, 2005; Drellishak and Bender, 2005) only allowed a finite (and small) number of entries for things like lexical types. For instance, users were required to provide exactly one transitive verb type and one intransitive verb type. The current system has an iterator mechanism in the questionnaire that allows for repeated sections, and thus unlimited entries. These repeated sections can also be nested, which allows for much more richly structured information. The utility of the iterator mechanism is most apparent when filling out the Lexicon subpage. Users can create an arbitrary number of lexical rule"
P10-4001,W02-1502,1,0.860814,"system: a repository of distilled linguistic knowledge and a web-based service which elicits a typological description of a language from the user and yields a customized grammar fragment ready for sustained development into a broad-coverage grammar. We describe the implementation of this repository with an emphasis on how the information is made available to users, including in-browser testing capabilities. 1 Introduction This demonstration presents the LinGO Grammar Matrix grammar customization system1 and its functionality for rapidly prototyping grammars. The LinGO Grammar Matrix project (Bender et al., 2002) is situated within the DELPH - IN2 collaboration and is both a repository of reusable linguistic knowledge and a method of delivering this knowledge to a user in the form of an extensible precision implemented grammar. The stored knowledge includes both a cross-linguistic core grammar and a series of “libraries” containing analyses of cross-linguistically variable phenomena. The core grammar handles basic phrase types, semantic compositionality, and general infrastructure such as the feature geometry, while the current set of libraries includes analyses of word order, person/number/gender, te"
P10-4001,monson-etal-2008-linguistic,0,0.01913,"d fills in appropriate predicates. The test-by-generation process then sends these constructed MRSs to the LKB process and displays the generation results, along with a brief explanation of the input semantics that gave rise to them, in HTML for the user.7 4 and an implemented grammar. The latter is in the format required by PC - PATR (McConnel, 1995), and is used primarily to disambiguate morphological analyses of lexical items in the input string. Other systems that attempt to elicit linguistic information from a user include the Expedition (McShane and Nirenburg, 2003) and Avenue projects (Monson et al., 2008), which are specifically targeted at developing machine translation for lowdensity languages. These projects differ from the Grammar Matrix customization system in eliciting information from native speakers (such as paradigms or translations of specifically tailored corpora), rather than linguists. Further, unlike the Grammar Matrix customization system, they do not produce resources meant to sustain further development by a linguist. Related Work As stated above, the engineering goal of the Grammar Matrix is to facilitate the rapid development of large-scale precision grammars. The starter gr"
P10-4001,W02-1503,0,0.0956363,"t of large-scale precision grammars. The starter grammars output by the customization system are compatible in format and semantic representations with existing DELPH - IN tools, including software for grammar development and for applications including machine translation (Oepen et al., 2007) and robust textual entailment (Bergmair, 2008). More broadly, the Grammar Matrix is situated in the field of multilingual grammar engineering, or the practice of developing linguisticallymotivated grammars for multiple languages within a consistent framework. Other projects in this field include ParGram (Butt et al., 2002; King et al., 2005) (LFG), the CoreGram project8 (e.g., (M¨uller, 2009)) (HPSG), and the MetaGrammar project (de la Clergerie, 2005) (TAG). To our knowledge, however, there is only one other system that elicits typological information about a language and outputs an appropriately customized implemented grammar. The system, described in (Black, 2004) and (Black and Black, 2009), is called PAWS (Parser And Writer for Syntax) and is available for download online.9 PAWS is being developed by SIL in the context of both descriptive (prose) grammar writing and “computer-assisted related language ada"
P10-4001,2007.tmi-papers.18,0,0.0104236,"ms or translations of specifically tailored corpora), rather than linguists. Further, unlike the Grammar Matrix customization system, they do not produce resources meant to sustain further development by a linguist. Related Work As stated above, the engineering goal of the Grammar Matrix is to facilitate the rapid development of large-scale precision grammars. The starter grammars output by the customization system are compatible in format and semantic representations with existing DELPH - IN tools, including software for grammar development and for applications including machine translation (Oepen et al., 2007) and robust textual entailment (Bergmair, 2008). More broadly, the Grammar Matrix is situated in the field of multilingual grammar engineering, or the practice of developing linguisticallymotivated grammars for multiple languages within a consistent framework. Other projects in this field include ParGram (Butt et al., 2002; King et al., 2005) (LFG), the CoreGram project8 (e.g., (M¨uller, 2009)) (HPSG), and the MetaGrammar project (de la Clergerie, 2005) (TAG). To our knowledge, however, there is only one other system that elicits typological information about a language and outputs an appropri"
P10-4001,W09-2604,0,0.0154127,"ing classes (types) of lexical entries and lexical rules which apply to those types. The grammars produced are compatible with both the grammar development tools and the 2 System Overview Grammar customization with the LinGO Grammar Matrix consists of three primary activities: filling out the questionnaire, preliminary testing of the grammar fragment, and grammar creation. 2.1 Questionnaire Most of the linguistic phenomena supported by the questionnaire vary across languages along multiple dimensions. It is not enough, for example, 3 Research of this type based on the Grammar Matrix includes (Crysmann, 2009) (tone change in Hausa) and (Fokkens et al., 2009) (Turkish suspended affixation). 1 http://www.delph-in.net/matrix/customize/ 2 http://www.delph-in.net 1 Proceedings of the ACL 2010 System Demonstrations, pages 1–6, c Uppsala, Sweden, 13 July 2010. 2010 Association for Computational Linguistics morphemes which each in turn bear any number of feature constraints. For example, the user could create a tense-agreement morphological slot, which contains multiple portmanteau morphemes each expressing some combination of tense, subject person and subject number values (e.g., French -ez expresses 2nd"
P10-4001,W05-1522,0,\N,Missing
P11-1107,I05-1015,0,0.0341642,"liary-verb structures is (with minor differences) also what is provided by the Matrix customization system. Argument-composition can capture the grammatical behavior of auxiliaries in German and Dutch. However, grammaticality and coverage is not all that matters for grammars of natural language. Efficiency remains an important factor, and argumentcomposition has some undesirable properties on this level. The problem lies in the fact that lexical entries of auxiliaries have underspecified elements on their subcategorization lists. With the current chart parsing and chart generation algorithms (Carroll and Oepen, 2005), an auxiliary in a language with flexible word order will speculatively add edges to the chart for potential analyses with the adjacent constituent as subject or complement. Because the length of the lists are underspecified as well, it can continue wrongly combining with all elements in the string. In the worse case scenario, the number of edges created by an auxiliary grows exponentially in the number of words and constituents in the string. The efficiency problem is even worse for generation: while the parser is restricted by the surface order of 2 Note that the same orders as in the Right"
P11-1107,C69-0101,0,0.62601,"Missing"
P11-1107,W05-1522,0,\N,Missing
P11-1107,W09-2605,0,\N,Missing
P11-1107,W06-1503,0,\N,Missing
P11-1107,W02-1502,0,\N,Missing
P11-1107,P98-1033,0,\N,Missing
P11-1107,C98-1033,0,\N,Missing
P13-1166,O97-1002,0,0.00955661,":// search.cpan.org/dist/WordNet-Similarity/. 1693 measure Spearman ρ Kendall τ ranking min max min max variation path based similarity path 0.70 0.78 0.55 0.62 1-8 wup 0.70 0.79 0.53 0.61 1-6 lch 0.70 0.78 0.55 0.62 1-7 path based information content res 0.65 0.75 0.26 0.57 4-11 lin 0.49 0.73 0.36 0.53 6-10 jcn 0.46 0.73 0.32 0.55 5, 7-11 path based relatedness hso 0.73 0.80 0.36 0.41 1-3,5-10 dictionary and corpus based relatedness vpairs 0.40 0.70 0.26 0.50 7-11 vector 0.48 0.92 0.33 0.76 1,2,4,6-11 lesk 0.66 0.83 -0.02 0.61 1-8,11,12 Leacock and Chodorow (1998) (lch), Resnik (1995) (res), Jiang and Conrath (1997) (jcn), Lin (1998) (lin), Banerjee and Pedersen (2003) (lesk), Hirst and St-Onge (1998) (hso) and Patwardhan and Pedersen (2006) (vector and vpairs) respectively. Consequently, settings and properties were changed systematically and shared with Pedersen who attempted to produce the new results with his own implementations. First, we made sure that the script implemented by Fokkens could produce the same WordNet similarity scores for each individual word pair as those used to calculate the ranking on the mc-set by Pedersen (2010). Finally, the gold standard and exact implementation of the Spear"
P13-1166,J06-1003,0,0.0172015,"es the difference in rank as its basis to calculate a correlation, where Kendall τ uses the number of items with the correct rank. The low Kendall τ for lesk is the result of three pairs receiving a score that is too high. Other pairs that get a relatively accurate score are pushed one place down in rank. Because only items that receive the exact same rank help to increase τ , such a shift can result in a drastic drop in the coefficient. In our opinion, Spearman ρ is therefore preferable over Kendall τ . We included τ , because many authors do not mention the ranking coefficient they use (cf. Budanitsky and Hirst (2006), Resnik (1995)) and both ρ and τ are com1695 monly used coefficients. Except for WordNet, which Budanitsky and Hirst (2006) hold accountable for minor variations in a footnote, the influential categories we investigated in this paper, to our knowledge, have not yet been addressed in the literature. Cramer (2008) points out that results from WordNet-Human similarity correlations lead to scattered results reporting variations similar to ours, but she compares studies using different measures, data and experimental setup. This study shows that even if the main properties are kept stable, results"
P13-1166,W08-2206,0,0.013078,"at receive the exact same rank help to increase τ , such a shift can result in a drastic drop in the coefficient. In our opinion, Spearman ρ is therefore preferable over Kendall τ . We included τ , because many authors do not mention the ranking coefficient they use (cf. Budanitsky and Hirst (2006), Resnik (1995)) and both ρ and τ are com1695 monly used coefficients. Except for WordNet, which Budanitsky and Hirst (2006) hold accountable for minor variations in a footnote, the influential categories we investigated in this paper, to our knowledge, have not yet been addressed in the literature. Cramer (2008) points out that results from WordNet-Human similarity correlations lead to scattered results reporting variations similar to ours, but she compares studies using different measures, data and experimental setup. This study shows that even if the main properties are kept stable, results vary enough to change the identity of the measure that yields the best performance. Table 1 reveals a wide variation in ranking relative to alternative approaches. Results in Table 2 show that it is common for the ranking of a score to change due to variations that are not at the core of the method. This study s"
P13-1166,P05-1045,0,0.0057573,"Missing"
P13-1166,W06-2501,1,0.861219,"n the methodology, such as data sets, evaluation metrics and type of cross-validation can influence the conclusions of an experiment, as we also find in our second use case. However, they focus on the problem of evaluation and recommendations on how to achieve consistent reproducible results. Our contribution is to investigate how much results vary. We cannot control how fellow researchers carry out their evaluation, but if we have an idea of the variations that typically occur within a system, we can better compare approaches for which not all details are known. 3 WordNet Similarity Measures Patwardhan and Pedersen (2006) and Pedersen (2010) present studies where the output of a variety of WordNet similarity and relatedness measures are compared. They rank Miller and Charles (1991)’s set (henceforth “mc-set”) of 30 word pairs according to their semantic relatedness with several WordNet similarity measures. Each measure ranks the mc-set of word pairs and these outputs are compared to Miller and Charles (1991)’s gold standard based on human rankings using the Spearman’s Correlation Coefficient (Spearman, 1904, ρ). Pedersen (2010) also ranks the original set of 65 word pairs ranked by humans in an experiment by R"
P13-1166,N10-1047,1,0.4317,"s, evaluation metrics and type of cross-validation can influence the conclusions of an experiment, as we also find in our second use case. However, they focus on the problem of evaluation and recommendations on how to achieve consistent reproducible results. Our contribution is to investigate how much results vary. We cannot control how fellow researchers carry out their evaluation, but if we have an idea of the variations that typically occur within a system, we can better compare approaches for which not all details are known. 3 WordNet Similarity Measures Patwardhan and Pedersen (2006) and Pedersen (2010) present studies where the output of a variety of WordNet similarity and relatedness measures are compared. They rank Miller and Charles (1991)’s set (henceforth “mc-set”) of 30 word pairs according to their semantic relatedness with several WordNet similarity measures. Each measure ranks the mc-set of word pairs and these outputs are compared to Miller and Charles (1991)’s gold standard based on human rankings using the Spearman’s Correlation Coefficient (Spearman, 1904, ρ). Pedersen (2010) also ranks the original set of 65 word pairs ranked by humans in an experiment by Rubenstein and Gooden"
P13-1166,P94-1019,0,0.0317356,"us research, address the following questions: 1) Which properties have an impact on the performance of WordNet similarity measures? 2) How much does the performance of individual measures vary? 3) How do commonly used measures compare when the variation of their performance are taken into account? 3.2 Methodology and first observations The questions above were addressed in two stages. In the first stage, Fokkens, who was not involved in the first replication attempt implemented a script to calculate similarity measures using WordNet::Similarity. This included similarity measures introduced by Wu and Palmer (1994) (wup), 3 Obtained from http://talisker.d.umn.edu/ cgi-bin/similarity/similarity.cgi, WordNet::Similarity version 2.05. This web interface has now moved to http://maraca.d.umn.edu 4 WordNet::Similarity were obtained http:// search.cpan.org/dist/WordNet-Similarity/. 1693 measure Spearman ρ Kendall τ ranking min max min max variation path based similarity path 0.70 0.78 0.55 0.62 1-8 wup 0.70 0.79 0.53 0.61 1-6 lch 0.70 0.78 0.55 0.62 1-7 path based information content res 0.65 0.75 0.26 0.57 4-11 lin 0.49 0.73 0.36 0.53 6-10 jcn 0.46 0.73 0.32 0.55 5, 7-11 path based relatedness hso 0.73 0.80 0"
P13-1166,J08-3010,1,\N,Missing
P13-1166,J04-4004,0,\N,Missing
P13-1166,J03-4003,0,\N,Missing
R15-1046,N09-1003,0,0.387574,"Missing"
R15-1046,O97-1002,0,0.510972,"izes nouns and verbs in hierarchies of hypernym-hyponym relations. We selected WordNet for our taxonomybased experiments, because it is widely used and probably the most popular taxonomy when it comes to determining word similarity. Many measures of similarity based on WordNet have been proposed over the years. Early work (Rada et al., 1989) advocates the use of is-a hierarchy and later approaches continue to use it heavily. In order to make a clean comparison between WordNet and distributional models, we do not include in our study measures that make use of a corpus such as Resnik (1995) and Jiang and Conrath (1997). Path length similarity takes the inverse of the path length (i.e. the distance in number of nodes) from s1 to s2 plus one. PL = 1 d(s1 , s2 ) + 1 Wu and Palmer’s similarity (Wu and Palmer, 1994) takes the fact into account that senses deeper in the hierarchy tend to be more specific than those high up. It therefore incorporates the depth of the hierarchy in their similarity calculation: WUP = 2depth(lcs) d(s1 , lcs) + d(s2 , lcs) + 2depth(lcs) Leacock and Chodorows similarity (Leacock and Chodorow, 1998) normalizes path-based scores by the maximum depth D of the hierarchy. This corrects for"
R15-1046,W14-1503,0,0.0205644,"l semantics: Word2vec (Mikolov et al., 2013, W 2 V) and dependency-based word embeddings (Levy and Goldberg, 2014a, DEPS). Word2vec is the first model to use a Skip-Gram with Negative Sampling (SGNN) algorithm for constructing semantic models and performed best on SimLex-999 in Hill et al. (2014a). Levy and Goldberg (2014b) argue that SGNN implicitly factorizes a shifted positive mutual information wordcontext matrix, not unlike traditional distributional semantic models. The use of a small window size and the weighting scheme that favors nearby contexts are supported by a systematic study of Kiela and Clark (2014) that shows the superiority of small windows. Moreover, Sahlgren (2006) presents empirical evidence that smaller windows lead to a cleaner distinction between syntagmatic and paradigmatic relations (which can be considered the linguistic version of similarity and association). Levy and Goldberg (2014a) extend SGNN to work with arbitrary contexts and experiment with dependency structures. It is generally believed that dependency structures are better at capturing similarity (Pad´o and Lapata, 2007) although Kiela and Clark (2014) found mixed results. The Skip-gram model captures the distributio"
R15-1046,P14-1023,0,0.0423863,"veloped two alternative evaluation methods that are less sensitive to minor differences in ranking. The first evaluation directly tests the comparison of pairs and, more importantly, allows us to study the contribution of partitions of the dataset. The second evaluation revolves around thresholds for similarity. In this evaluation, we set thresholds to establish a binary distinction between highly similar pairs and other pairs. The pairs above the similarity threshold are compared to those falling above the threshold in the gold (see Section 4.2). Many studies compare similarity measures (see Baroni et al. (2014) and Pedersen (2010), among outline our experimental methodology, including used datasets and evaluation methods. The results are presented in Section 5, and our conclusions and future work in Section 6. 2 Background and Motivation Several gold-standards have been created that rank word-pairs based on their similarity. Agirre et al. (2009) point out that association and similarity are mixed up in these sets, where associated pairs such as coffee and cup rank higher than truly similar pairs such as car and train. The confusion directly influences the performance of corpus-based approaches, whic"
R15-1046,P14-2050,0,0.0621378,"al. (2009) and Banjade et al. (2015) are the only ones that look at both taxonomy-based approaches and distributional approaches. As mentioned above, they do not dive into the details of the differences between the two. Furthermore, apart from Fokkens et al. (2013), who do not propose new rankings, we are not aware of studies applying multiple evaluation metrics for similarity-based rankings. 3 3.2 We selected two representative models from the large and growing literature on corpus-based models of lexical semantics: Word2vec (Mikolov et al., 2013, W 2 V) and dependency-based word embeddings (Levy and Goldberg, 2014a, DEPS). Word2vec is the first model to use a Skip-Gram with Negative Sampling (SGNN) algorithm for constructing semantic models and performed best on SimLex-999 in Hill et al. (2014a). Levy and Goldberg (2014b) argue that SGNN implicitly factorizes a shifted positive mutual information wordcontext matrix, not unlike traditional distributional semantic models. The use of a small window size and the weighting scheme that favors nearby contexts are supported by a systematic study of Kiela and Clark (2014) that shows the superiority of small windows. Moreover, Sahlgren (2006) presents empirical"
R15-1046,P11-2087,0,0.0330004,"dependency-based-word-embeddings i 349 j Model WUP PL LCH W2V DEPS WUP PL LCH W2V DEPS SL-999nv 0.47 0.52 0.55 0.42 0.45 MENnv 0.39 0.39 0.39 0.77 0.61 that taxonomy-based approaches capture similarity rather than association, whereas corpus-based approaches do not clearly distinguish the two. WS-353 0.35 0.30 0.31 0.70 0.63 5.2 The final evaluation measure is based on the observation that many approaches use a threshold to determine which words are similar enough to be used for contributing features or approximations, or to be candidates for lexical substitution (McCarthy and Navigli, 2009; Biran et al., 2011, e.g.). Threshold accuracy sets a similarity threshold and determines how many of the n-highest ranking word pairs in a given measurement are also in the top-n pairs of the gold standard. In other words, this evaluation determines whether the right wordpairs would end up above the threshold of being similar. Results We calculated the similarity scores of all noun and verb pairs in SimLex-999 (a set of 888 pairs), MEN (2,034 pairs), and all pairs in WordSim353 using the measures outlined in Section 3 and ranked the word pairs according to the outcome. 5.1 Ordering Accuracy Table 2 presents the"
R15-1046,P12-1015,0,0.0186698,"on and similarity. SimLex-999’s ranking is based on similarity only. WordSim-353 (Finkelstein et al., 2001) includes 353 word pairs scored for relatedness on a scale from 0 to 10 by 13 or 16 subjects. The interannotator agreement is 0.611 defined as the average pairwise Spearman’s correlation. Researchers have reported correlation as high as 0.81 (Yih and Qazvinian, 2012). Agirre et al. (2009) later divided WordSim-353 into a “similarity” and “relatedness” set. However, Hill et al. (2014b) rightly point out that both remain relatedness datasets, because this is what the annotators rated. MEN (Bruni et al., 2012) is composed of 3,000 word pairs, sampled to include a balanced range of relatedness. Annotators were asked to choose (u,v)∈G (x,y)∈G where G stands for the gold standard and ms,G (·) is a matching function that returns 1 for those two word-pairs whose relative ranking is the same in the gold standard and in the ranking of the similarity measure and 0 otherwise. We also experiment with a variation of m where ties get half score. As shown in Figure 1, ordering accuracy highly correlates with Spearman’s ρ. partitioned into n subsets gi (i.e. T If G can be S gi = ∅ and gi = G) then a can be decom"
R15-1046,J05-3004,0,0.0429827,"t-based and corpus-based similarity measures to SimLex-999, but do not examine or discuss the difference between taxonomy-based approaches and corpus-based approaches in detail. Instead, they focus on the strength of combining several approaches to yield better results.1 We investigate the difference between the approaches in various evaluations showing that taxonomy-based approaches outperform corpus-based approaches, a conclusion that cannot be drawn (clearly) from Banjade et al. (2015)’s results. It should be noted that our conclusions only apply to the task of identifying pure similarity. Markert and Nissim (2005) show, for instance, that a corpus-based approach with sufficiently large corpus works better than WordNet for anaphora resolution. The next step in our investigation was to determine the strengths and weaknesses of each approach. The original idea was to investigate pairs that are ranked more or less correctly by one approach, but are far off in the other to identify patterns of errors in each approach. We did not find such patterns, partially because the examples that have large differences in ranking compared to the gold are relatively rare. We therefore developed two alternative evaluation"
R15-1046,P13-1166,1,0.926946,"rity set, because corpusbased approaches tend to mix-up similarity and association. We carry out several evaluations which investigate (i) the difference in performance on pure similarity sets and sets that combine similarity and association, (ii) the influence of associative pairs while identifying true similarity, and (iii) various evaluation metrics that compare similarity measures to the gold standard of SimLex-999. We perform more than one evaluation metric for two reasons. First, different ranking coefficients can lead to a completely different outcome when evaluating similarity scores (Fokkens et al., 2013). Second, we want to gain more insight into the differences between individual measures. To do so, we introduced two new, more flexible, evaluation methods which reveal high results for all similarity measures. We argue that these new evaluations provide a better insight into how suitable similarity measures are to be used in NLP tasks than the commonly used Spearman’s correlation (henceforth Spearman ρ). Our results show that most of the evaluations confirm our hypothesis. The few cases where corpus-based methods outperformed taxonomybased approaches reveal much smaller differences than the m"
R15-1046,S07-1006,0,0.0215545,"65.5 SL-999 SL-999 Diff. nv nv,assoc assoc Using tie corrections 66.6 67.3 +0.7 68.0 68.2 +0.2 69.2 69.1 -0.1 64.6 57.5 -7.1 65.6 60.9 -4.7 Table 2: Ordering accuracy (percentage) of similarity measures on SimLex-999nv . Figure 1: Ordering accuracy and Spearman’s ρ on a synthesized dataset of 100 word pairs. Model SL-999 nv 5.3 Spearman’s Rank Correlation Decomposition of Ordering Accuracy Palmer et al. (2007) showed that making subtle sense distinction is hard for human subjects leading to evaluations where both coarse-grained and fine-grained word senses are considered (Palmer et al., 2007; Navigli et al., 2007). Similarly, establishing which word-pair is more similar than another is challenging when pairs are close in simTable 1 shows the performance of models on all three benchmarks. Taxonomy based approaches perform higher on SimLex-999, whereas corpusbased approaches reveal high performance on MEN and WordSim-353 and score significantly lower on SimLex-999. This result confirms 350 ∆=0 pollution-president forget-learn take-leave succeed-try army-squad girl-child emotion-passion collect-save sheep-lamb attention-awareness ∆=1 spoon-cup argue-differ remind-sell apple-candy book-topic argument-agree"
R15-1046,J07-2002,0,0.0988941,"Missing"
R15-1046,N10-1047,0,0.0187751,"valuation methods that are less sensitive to minor differences in ranking. The first evaluation directly tests the comparison of pairs and, more importantly, allows us to study the contribution of partitions of the dataset. The second evaluation revolves around thresholds for similarity. In this evaluation, we set thresholds to establish a binary distinction between highly similar pairs and other pairs. The pairs above the similarity threshold are compared to those falling above the threshold in the gold (see Section 4.2). Many studies compare similarity measures (see Baroni et al. (2014) and Pedersen (2010), among outline our experimental methodology, including used datasets and evaluation methods. The results are presented in Section 5, and our conclusions and future work in Section 6. 2 Background and Motivation Several gold-standards have been created that rank word-pairs based on their similarity. Agirre et al. (2009) point out that association and similarity are mixed up in these sets, where associated pairs such as coffee and cup rank higher than truly similar pairs such as car and train. The confusion directly influences the performance of corpus-based approaches, which also tend to have"
R15-1046,P13-1132,0,0.0186259,"sed and corpus-based approaches on SimLex999. The results confirm our hypothesis that taxonomy-based approaches are more suitable to identify similarity. We introduce two new measures of evaluation that show that all measures perform well on a coarse-grained evaluation and that it is not always clear which approach is most suitable when a similarity score is used as a threshold. This leads us to conclude that the inferior performance of corpus-based approaches may not (always) matter. 1 Introduction Similarity measures are used in a wide variety of Natural Language Processing (NLP) tasks (see Pilehvar et al. (2013), among others for examples). They may be used, e.g. to increase coverage of an approach by using information from similar words for unseen data, or to establish average similarity between a question and a potential answer. Due to its importance, similarity measures have received steady attention in computational linguistics. There are two widely followed, but different, schools: taxonomy-based approaches and distributional, or corpus-based, approaches. Apart from a few exceptions, these approaches have mostly been studied separately. Our main goal is to examine how the approaches perform when"
R15-1046,N12-1077,0,0.0127835,"uations. We first describe the datasets and then the evaluation metrics we use. 4.1 Evaluation Metrics Gold-standard Datasets We evaluate the approaches on three datasets. WordSim-353 and MEN allow us to compare performance on sets that mix association and similarity. SimLex-999’s ranking is based on similarity only. WordSim-353 (Finkelstein et al., 2001) includes 353 word pairs scored for relatedness on a scale from 0 to 10 by 13 or 16 subjects. The interannotator agreement is 0.611 defined as the average pairwise Spearman’s correlation. Researchers have reported correlation as high as 0.81 (Yih and Qazvinian, 2012). Agirre et al. (2009) later divided WordSim-353 into a “similarity” and “relatedness” set. However, Hill et al. (2014b) rightly point out that both remain relatedness datasets, because this is what the annotators rated. MEN (Bruni et al., 2012) is composed of 3,000 word pairs, sampled to include a balanced range of relatedness. Annotators were asked to choose (u,v)∈G (x,y)∈G where G stands for the gold standard and ms,G (·) is a matching function that returns 1 for those two word-pairs whose relative ranking is the same in the gold standard and in the ranking of the similarity measure and 0 o"
R15-1046,J15-4004,0,\N,Missing
R15-1046,P94-1019,0,\N,Missing
R19-1016,W16-2507,0,0.106803,"are far smaller than a typical web corpus. Instead of training directly on the philosophical in-domain data, which is too sparse for learning, we rely on a pre-trained background semantic space, thus simulating a speaker with some linguistic knowledge coming to a new domain. Our focus is the evaluation problem encountered when working with domain-specific data. DS models are typically evaluated on gold standard datasets containing word association scores elicited from human subjects (e.g. Bruni et al., 2014; Hill et al., 2015). Beside the limited practical use of such evaluation metrics (e.g. Gladkova and Drozd, 2016), this is not a feasible method for evaluating DS models in low-resource situations. When domain-specific terminology is used and the meaning of words possibly deviate from their most dominant sense, creating regular evaluation resources can require significant time investment from domain experts. Evaluation metrics that do not depend on such resources are valuable. Thus, we introduce the metric of consistency, which requires a model to learn similar word embeddings for a given term across similar sources, for example, two halves of a book. Philosophical texts make a suitable case study for ou"
R19-1016,C16-1262,0,0.170224,"el itself. QVEC (Tsvetkov et al., 2015) evaluates by aligning dimensions of a semantic space to linguistic features, but we are interested only in evaluating some vectors rather than an entire space (target term vectors but not the background space vectors), and this approach requires language-specific resources. Nooralahzadeh et al. (2018) evaluate domainspecific embeddings by building a query inventory for their domain from a glossary containing synonym, antonym and alternative form information. Unfortunately, such structured glossaries are generally not available for specific philosophers. Hellrich and Hahn (2016) test their models for reliability in a study investigating historical English and German texts, another relatively low-resource domain. Their reliability metric involves training three identically parametrized models, and comparing the nearest neighbors of each word in each model using a modified Jaccard coefficient. This metric does not require any language-specific data, but it mainly serves as a test of the impact of the sources of randomness in Word2Vec, and not as a measure of the systematic semantic differences across various data sources. Related Work Learning embeddings for rare words"
R19-1016,D17-1030,1,0.876758,"ing three identically parametrized models, and comparing the nearest neighbors of each word in each model using a modified Jaccard coefficient. This metric does not require any language-specific data, but it mainly serves as a test of the impact of the sources of randomness in Word2Vec, and not as a measure of the systematic semantic differences across various data sources. Related Work Learning embeddings for rare words is a very challenging process (Luong et al., 2013). Word2Vec (W2V, Mikolov et al., 2013a)’s skipgram model can learn embeddings from tiny data after modification, as shown by Herbelot and Baroni (2017) when it consists of just a single highly informative definitional sentence. However, philosophical data is typically small data rather than tiny data. While tiny data consists of a single definitional sentence, our small data consists of multiple context sentences per term that are not necessarily definitional. Herbelot and Baroni’s (2017) Nonce2Vec (N2V) has not been tested on this type of data. W2V has been tested on smaller datasets, but was found to be suboptimal (Asr et al., 2016) and surpassed by SVD models on a 1 million word dataset (Sahlgren and Lenci, 2016). Different DS evaluations"
R19-1016,J15-4004,0,0.492957,"In this paper, we are interested in modeling concepts in philosophical corpora, which are far smaller than a typical web corpus. Instead of training directly on the philosophical in-domain data, which is too sparse for learning, we rely on a pre-trained background semantic space, thus simulating a speaker with some linguistic knowledge coming to a new domain. Our focus is the evaluation problem encountered when working with domain-specific data. DS models are typically evaluated on gold standard datasets containing word association scores elicited from human subjects (e.g. Bruni et al., 2014; Hill et al., 2015). Beside the limited practical use of such evaluation metrics (e.g. Gladkova and Drozd, 2016), this is not a feasible method for evaluating DS models in low-resource situations. When domain-specific terminology is used and the meaning of words possibly deviate from their most dominant sense, creating regular evaluation resources can require significant time investment from domain experts. Evaluation metrics that do not depend on such resources are valuable. Thus, we introduce the metric of consistency, which requires a model to learn similar word embeddings for a given term across similar sour"
R19-1016,P19-2022,1,0.88131,"middle, thus obtaining two subcorpora of equal size and in diachronic order. Given a pre-trained background space kept frozen across experiments, the vector representation of a target is generated by simple vector addition over 133 its context words. Therefore, the obtained vector directly represents the context the target term occurs in, and consequently, similar representations (in terms of cosine similarity) mean that the target term is used in a similar way in different parts of a book/corpus, and is thus consistently learnable. Crucially, though, this measure may interact with data size. Kabbach et al. (2019) recently noted a sum effect in the additive model, where summed vectors are close to each other. It may be the case that additive model vectors summed over more context data contain more information and may have higher similarity between each other, resulting in higher consistency scores. We test this in Section 6. When randomly sampling, we limit the number of sentences per sample to control for this. To evaluate space consistency, we create identically parametrized models as in Hellrich and Hahn’s (2016) reliability metric, but over different parts of the data, with the data being split in"
R19-1016,Q15-1016,0,0.0623472,"ata at the same time. In this procedure, we concatenate a particular Quine dataset with a 140M word Wikipedia corpus sample, in which the Quine terms are marked with special tokens in order to be trained separately from the same term in the Wikipedia data. We create embeddings from this corpus, applying PPMI weighting and singular value decomposition (SVD) to reduce the models to 100 dimensions, to match the dimensionality of our other models and because factorized count models have been shown to work well on smaller datasets (Sahlgren and Lenci, 2016). We use the Hyperwords implementation of Levy et al. (2015), with a window size of 5, and other hyperparameters set to the default values. In both the above approaches, we can then compute vector space consistency between different vectors learned for the same term over different splits of the data. 6 cos-sim 0.794 0.837 0.905 0.923 0.956 0.987 Consistency of Data We start by applying the additive model to quantify data consistency on the different datasets described in Section 4. We compute average similarities and nearest neighbor ranks over the vectors of all target terms in a dataset. For the randomly sampled data sets, we have five vectors per te"
R19-1016,W13-3512,0,0.0307103,"ty in a study investigating historical English and German texts, another relatively low-resource domain. Their reliability metric involves training three identically parametrized models, and comparing the nearest neighbors of each word in each model using a modified Jaccard coefficient. This metric does not require any language-specific data, but it mainly serves as a test of the impact of the sources of randomness in Word2Vec, and not as a measure of the systematic semantic differences across various data sources. Related Work Learning embeddings for rare words is a very challenging process (Luong et al., 2013). Word2Vec (W2V, Mikolov et al., 2013a)’s skipgram model can learn embeddings from tiny data after modification, as shown by Herbelot and Baroni (2017) when it consists of just a single highly informative definitional sentence. However, philosophical data is typically small data rather than tiny data. While tiny data consists of a single definitional sentence, our small data consists of multiple context sentences per term that are not necessarily definitional. Herbelot and Baroni’s (2017) Nonce2Vec (N2V) has not been tested on this type of data. W2V has been tested on smaller datasets, but was"
R19-1016,L18-1228,0,0.0556813,"interesting, but require domain experts, as well as instructions that elicit the desired type of semantic relation (i.e. similarity). Extrinsic evaluation requires a downstream task that can be evaluated, but in this use case we are interested in the information encoded by the DS model itself. QVEC (Tsvetkov et al., 2015) evaluates by aligning dimensions of a semantic space to linguistic features, but we are interested only in evaluating some vectors rather than an entire space (target term vectors but not the background space vectors), and this approach requires language-specific resources. Nooralahzadeh et al. (2018) evaluate domainspecific embeddings by building a query inventory for their domain from a glossary containing synonym, antonym and alternative form information. Unfortunately, such structured glossaries are generally not available for specific philosophers. Hellrich and Hahn (2016) test their models for reliability in a study investigating historical English and German texts, another relatively low-resource domain. Their reliability metric involves training three identically parametrized models, and comparing the nearest neighbors of each word in each model using a modified Jaccard coefficient"
R19-1016,D16-1099,0,0.274109,"odification, as shown by Herbelot and Baroni (2017) when it consists of just a single highly informative definitional sentence. However, philosophical data is typically small data rather than tiny data. While tiny data consists of a single definitional sentence, our small data consists of multiple context sentences per term that are not necessarily definitional. Herbelot and Baroni’s (2017) Nonce2Vec (N2V) has not been tested on this type of data. W2V has been tested on smaller datasets, but was found to be suboptimal (Asr et al., 2016) and surpassed by SVD models on a 1 million word dataset (Sahlgren and Lenci, 2016). Different DS evaluations test different aspects of the learned embeddings (i.e. Wang et al., 2019). Most existing methods are however not easily applicable to our task. The typical evaluation of comparing embedding similarities to a gold standard of word similarity scores, such as the SimLex-999 dataset (Hill et al., 2015) cannot be applied, because we are interested in the representation of specific terms: even if these terms are present in the evaluation set, their meaning in the philosophical domain is likely to differ. Manually creating a domain-specific resource requires labor-intensive"
R19-1016,D15-1036,0,0.309786,"p 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_016 sistency. We show that in spite of being a simple evaluation, consistency actually depends on various combinations of factors, including the nature of the data itself, the model used to train the semantic space, and the frequency of the learned terms, both in the background space and in the in-domain data of interest. This leads us to conclude that the evaluation of in-domain word embeddings from small data has to be controlled extremely carefully in order not to draw incorrect conclusions from experimental results. 2 sic evaluation (Schnabel et al., 2015) are interesting, but require domain experts, as well as instructions that elicit the desired type of semantic relation (i.e. similarity). Extrinsic evaluation requires a downstream task that can be evaluated, but in this use case we are interested in the information encoded by the DS model itself. QVEC (Tsvetkov et al., 2015) evaluates by aligning dimensions of a semantic space to linguistic features, but we are interested only in evaluating some vectors rather than an entire space (target term vectors but not the background space vectors), and this approach requires language-specific resourc"
R19-1016,D15-1243,0,0.0244619,"ground space and in the in-domain data of interest. This leads us to conclude that the evaluation of in-domain word embeddings from small data has to be controlled extremely carefully in order not to draw incorrect conclusions from experimental results. 2 sic evaluation (Schnabel et al., 2015) are interesting, but require domain experts, as well as instructions that elicit the desired type of semantic relation (i.e. similarity). Extrinsic evaluation requires a downstream task that can be evaluated, but in this use case we are interested in the information encoded by the DS model itself. QVEC (Tsvetkov et al., 2015) evaluates by aligning dimensions of a semantic space to linguistic features, but we are interested only in evaluating some vectors rather than an entire space (target term vectors but not the background space vectors), and this approach requires language-specific resources. Nooralahzadeh et al. (2018) evaluate domainspecific embeddings by building a query inventory for their domain from a glossary containing synonym, antonym and alternative form information. Unfortunately, such structured glossaries are generally not available for specific philosophers. Hellrich and Hahn (2016) test their mod"
S15-2133,P09-1068,0,0.0302034,"ent levels); such a representation can be further used to explore and discover additional event relations2 ; • Event coreference: we assume that two event mentions (either in the same document or in different documents) are coreferential if they share the same participant set (i.e., entities) and occur at the same time and place (Chen et al., 2011; Cybulska and Vossen, 2013); • Temporal relations: temporal relation processing can benefit from an entity driven approach as sequences of events sharing the same entities (i.e., co-participant events) can be assumed to stand in precedence relation (Chambers and Jurafsky, 2009; Chambers and Jurafsky, 2010). 2.1 The SPINOZA VU System The NWR pipeline which forms the basis of the SPINOZA VU system consists of 15 modules carrying out various NLP tasks and outputs the results in NLP Annotation Format (Fokkens et al., 2014), a layered standoff representation format. Two versions of the system have been developed, namely: 2 We are referring to a broader set of relations that we labeled as “bridging relations” among events which involve coparticipation, primary and secondary causal relations, temporal relations, and entailment relations. 788 • SPINOZA VU 1 uses the output"
S15-2133,chambers-jurafsky-2010-database,0,0.0132799,"ation can be further used to explore and discover additional event relations2 ; • Event coreference: we assume that two event mentions (either in the same document or in different documents) are coreferential if they share the same participant set (i.e., entities) and occur at the same time and place (Chen et al., 2011; Cybulska and Vossen, 2013); • Temporal relations: temporal relation processing can benefit from an entity driven approach as sequences of events sharing the same entities (i.e., co-participant events) can be assumed to stand in precedence relation (Chambers and Jurafsky, 2009; Chambers and Jurafsky, 2010). 2.1 The SPINOZA VU System The NWR pipeline which forms the basis of the SPINOZA VU system consists of 15 modules carrying out various NLP tasks and outputs the results in NLP Annotation Format (Fokkens et al., 2014), a layered standoff representation format. Two versions of the system have been developed, namely: 2 We are referring to a broader set of relations that we labeled as “bridging relations” among events which involve coparticipation, primary and secondary causal relations, temporal relations, and entailment relations. 788 • SPINOZA VU 1 uses the output of a state of the art system,"
S15-2133,I11-1012,0,0.0308787,"nteractions between the participants involved in an event individually; • Event relations: in an entity based representation, event mentions with more than one entity as their participants will be repeated in the final representation (both at in-document at crossdocument levels); such a representation can be further used to explore and discover additional event relations2 ; • Event coreference: we assume that two event mentions (either in the same document or in different documents) are coreferential if they share the same participant set (i.e., entities) and occur at the same time and place (Chen et al., 2011; Cybulska and Vossen, 2013); • Temporal relations: temporal relation processing can benefit from an entity driven approach as sequences of events sharing the same entities (i.e., co-participant events) can be assumed to stand in precedence relation (Chambers and Jurafsky, 2009; Chambers and Jurafsky, 2010). 2.1 The SPINOZA VU System The NWR pipeline which forms the basis of the SPINOZA VU system consists of 15 modules carrying out various NLP tasks and outputs the results in NLP Annotation Format (Fokkens et al., 2014), a layered standoff representation format. Two versions of the system have"
S15-2133,R13-1021,1,0.8536,"the participants involved in an event individually; • Event relations: in an entity based representation, event mentions with more than one entity as their participants will be repeated in the final representation (both at in-document at crossdocument levels); such a representation can be further used to explore and discover additional event relations2 ; • Event coreference: we assume that two event mentions (either in the same document or in different documents) are coreferential if they share the same participant set (i.e., entities) and occur at the same time and place (Chen et al., 2011; Cybulska and Vossen, 2013); • Temporal relations: temporal relation processing can benefit from an entity driven approach as sequences of events sharing the same entities (i.e., co-participant events) can be assumed to stand in precedence relation (Chambers and Jurafsky, 2009; Chambers and Jurafsky, 2010). 2.1 The SPINOZA VU System The NWR pipeline which forms the basis of the SPINOZA VU system consists of 15 modules carrying out various NLP tasks and outputs the results in NLP Annotation Format (Fokkens et al., 2014), a layered standoff representation format. Two versions of the system have been developed, namely: 2 W"
S15-2133,lopez-de-lacalle-etal-2014-predicate,0,0.0274096,"Missing"
S15-2133,S10-1063,0,0.124598,"SPINOZA VU System The NWR pipeline which forms the basis of the SPINOZA VU system consists of 15 modules carrying out various NLP tasks and outputs the results in NLP Annotation Format (Fokkens et al., 2014), a layered standoff representation format. Two versions of the system have been developed, namely: 2 We are referring to a broader set of relations that we labeled as “bridging relations” among events which involve coparticipation, primary and secondary causal relations, temporal relations, and entailment relations. 788 • SPINOZA VU 1 uses the output of a state of the art system, TIPSem (Llorens et al., 2010), for event detection and temporal relations; • SPINOZA VU 2 is entirely based on data from the NWR pipeline including the temporal (TLINKs) and causal relation (CLINKs) layers. The final output is based on a dedicated rulebased module, the TimeLine (TML) module. We will describe in the following paragraphs how each subtask has been tackled with respect to each version of the system. Entity identification Entity identification relies on the entity detection and disambiguation layer (NERD) of the NWR pipeline. Each detected entity is associated with a URI (a unique identifier), either from DBpe"
S15-2133,S13-2001,0,0.0340615,"extraction. 1 Introduction This paper reports on a system (SPINOZA VU) for timeline extraction developed at the CLTL Lab of the VU Amsterdam in the context of the SemEval 2015 Task 4: Cross Document TimeLines. In this task, a timeline is defined as a set of chronologically anchored and ordered events extracted from a corpus spanning over a (large) period of time with respect to a target entity. Cross-document timeline extraction benefits from previous works and evaluation campaigns in Temporal Processing, such as the TempEval evaluation campaigns (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013) and aims at promoting research in temporal processing by tackling the following issues: cross-document and cross-temporal event detection and ordering; event coreference (indocument and cross-document); and entity-based temporal processing. The SPINOZA VU system is based on the NewsReader (NWR) NLP pipeline (Agerri et al., 2013; Beloki et al., 2014), which has been developed within the context of the NWR project1 and provides multi-layer annotations over raw texts from tokenization up to temporal relations. The goal of the NWR project is to build structured event indexes from large volumes of"
S15-2133,S07-1014,0,0.171512,"towards a more complex task such as storyline extraction. 1 Introduction This paper reports on a system (SPINOZA VU) for timeline extraction developed at the CLTL Lab of the VU Amsterdam in the context of the SemEval 2015 Task 4: Cross Document TimeLines. In this task, a timeline is defined as a set of chronologically anchored and ordered events extracted from a corpus spanning over a (large) period of time with respect to a target entity. Cross-document timeline extraction benefits from previous works and evaluation campaigns in Temporal Processing, such as the TempEval evaluation campaigns (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013) and aims at promoting research in temporal processing by tackling the following issues: cross-document and cross-temporal event detection and ordering; event coreference (indocument and cross-document); and entity-based temporal processing. The SPINOZA VU system is based on the NewsReader (NWR) NLP pipeline (Agerri et al., 2013; Beloki et al., 2014), which has been developed within the context of the NWR project1 and provides multi-layer annotations over raw texts from tokenization up to temporal relations. The goal of the NWR project is to build"
S15-2133,S10-1010,1,\N,Missing
S18-1154,S18-1117,0,0.0975625,"n aiming for high performance, we explore which kind of semantic knowledge is best captured by different methods. The results indicate that WordNet glosses on different levels of the hierarchy capture many attributes relevant for this task. In combination with exploiting word embedding similarities, this source of information yielded our best results. Our best performing system ranked 5th out of 13 final ranks. Our analysis yields insights into the different kinds of attributes represented by different sources of knowledge. 1 Introduction SemEval Task 10 “Capturing Discriminative Attributes” (Krebs et al., 2018) provides participants with triples of words consisting of two concepts and an attribute. The task is to determine whether the attribute is a distinguishing property of the first concept compared to the second concept. This is the case in triple shrimp, spinach, pink, for instance, because shrimp can be pink whereas spinach is usually of a different color. When the first concept does not have a semantic relation with the attribute or both the concepts have the same semantic relation with it, the attribute is considered not to be discriminative. In general, Task 10 can be understood as detectin"
S18-1154,P14-1023,0,0.237153,"Missing"
S18-1154,P14-2050,0,0.216153,"g knowledge and reasoning about attributes we are aware of is an exploratory study examining what knowledge in definitions contributes to question-answering tasks (Clark et al., 2008). Vector representations of word meaning based on the distribution of words in large corpora do not yield explicit information about specific relations, but implicitly encode all kinds of associations between concepts. In contrast to manually constructed resources, their coverage is much larger. More specifically, they have been shown to encode information relevant in solving analogy tasks (Mikolov et al., 2013a; Levy and Goldberg, 2014b; Gladkova et al., 2016; G´abor et al., 2017; Linzen, 2016) and inferring semantic hierarchies (Fu et al., 2014; Pocostales, 2016). This indicates that the dimensions of distributional representations encode information about attributes of concepts (Levy and Goldberg, 2014b, p.177). For instance, in order to find the fourth component in the analogy man is to woman as king is to queen, a model has to detect the relation holding between the pairs in the analogy. In this example, the relations are formed by the two features of royalty and gender. One way of solving this is to use the vector offs"
S18-1154,W14-1618,0,0.398221,"g knowledge and reasoning about attributes we are aware of is an exploratory study examining what knowledge in definitions contributes to question-answering tasks (Clark et al., 2008). Vector representations of word meaning based on the distribution of words in large corpora do not yield explicit information about specific relations, but implicitly encode all kinds of associations between concepts. In contrast to manually constructed resources, their coverage is much larger. More specifically, they have been shown to encode information relevant in solving analogy tasks (Mikolov et al., 2013a; Levy and Goldberg, 2014b; Gladkova et al., 2016; G´abor et al., 2017; Linzen, 2016) and inferring semantic hierarchies (Fu et al., 2014; Pocostales, 2016). This indicates that the dimensions of distributional representations encode information about attributes of concepts (Levy and Goldberg, 2014b, p.177). For instance, in order to find the fourth component in the analogy man is to woman as king is to queen, a model has to detect the relation holding between the pairs in the analogy. In this example, the relations are formed by the two features of royalty and gender. One way of solving this is to use the vector offs"
S18-1154,W16-2503,0,0.0629117,"ratory study examining what knowledge in definitions contributes to question-answering tasks (Clark et al., 2008). Vector representations of word meaning based on the distribution of words in large corpora do not yield explicit information about specific relations, but implicitly encode all kinds of associations between concepts. In contrast to manually constructed resources, their coverage is much larger. More specifically, they have been shown to encode information relevant in solving analogy tasks (Mikolov et al., 2013a; Levy and Goldberg, 2014b; Gladkova et al., 2016; G´abor et al., 2017; Linzen, 2016) and inferring semantic hierarchies (Fu et al., 2014; Pocostales, 2016). This indicates that the dimensions of distributional representations encode information about attributes of concepts (Levy and Goldberg, 2014b, p.177). For instance, in order to find the fourth component in the analogy man is to woman as king is to queen, a model has to detect the relation holding between the pairs in the analogy. In this example, the relations are formed by the two features of royalty and gender. One way of solving this is to use the vector offsets resulting from woman - man + king. The result should be"
S18-1154,P14-1113,0,0.0212918,"ns contributes to question-answering tasks (Clark et al., 2008). Vector representations of word meaning based on the distribution of words in large corpora do not yield explicit information about specific relations, but implicitly encode all kinds of associations between concepts. In contrast to manually constructed resources, their coverage is much larger. More specifically, they have been shown to encode information relevant in solving analogy tasks (Mikolov et al., 2013a; Levy and Goldberg, 2014b; Gladkova et al., 2016; G´abor et al., 2017; Linzen, 2016) and inferring semantic hierarchies (Fu et al., 2014; Pocostales, 2016). This indicates that the dimensions of distributional representations encode information about attributes of concepts (Levy and Goldberg, 2014b, p.177). For instance, in order to find the fourth component in the analogy man is to woman as king is to queen, a model has to detect the relation holding between the pairs in the analogy. In this example, the relations are formed by the two features of royalty and gender. One way of solving this is to use the vector offsets resulting from woman - man + king. The result should be closest to the fourth component (queen). Thus, the f"
S18-1154,D17-1193,0,0.0324207,"Missing"
S18-1154,N13-1090,0,0.690436,"evel of concreteness, but might also be distinguished on a more abstract level (e.g. herbs, root, green v.s. apse, nightgown, royal) we exploit the entire WordNet hierarchy. In both of our systems, the second component exploits information encoded in distributional vector representations of words. Word vectors have not only been shown to capture information about semantic similarity and relatedness but, beyond that, seem to encode information about individual components of word meaning that are necessary to solve analogy tasks such as in the famous example man is to woman as king is to queen (Mikolov et al., 2013b). This indicates that the dimensions of the distributional vector representations encode information about specific attributes of words. We experiment with two approaches: a basic approach comparing cosine similarities and an exploratory approach that deducts word vectors from one another to detect meaning differences. Best performance was obtained by the system using cosine similarity. The second approach perThis paper presents the two systems submitted by the meaning space team in Task 10 of the SemEval competition 2018 entitled Capturing discriminative attributes. The systems consist of c"
S18-1154,S16-1202,0,0.0181551,"question-answering tasks (Clark et al., 2008). Vector representations of word meaning based on the distribution of words in large corpora do not yield explicit information about specific relations, but implicitly encode all kinds of associations between concepts. In contrast to manually constructed resources, their coverage is much larger. More specifically, they have been shown to encode information relevant in solving analogy tasks (Mikolov et al., 2013a; Levy and Goldberg, 2014b; Gladkova et al., 2016; G´abor et al., 2017; Linzen, 2016) and inferring semantic hierarchies (Fu et al., 2014; Pocostales, 2016). This indicates that the dimensions of distributional representations encode information about attributes of concepts (Levy and Goldberg, 2014b, p.177). For instance, in order to find the fourth component in the analogy man is to woman as king is to queen, a model has to detect the relation holding between the pairs in the analogy. In this example, the relations are formed by the two features of royalty and gender. One way of solving this is to use the vector offsets resulting from woman - man + king. The result should be closest to the fourth component (queen). Thus, the first component for"
S18-1154,N16-2002,0,0.0132031,"about attributes we are aware of is an exploratory study examining what knowledge in definitions contributes to question-answering tasks (Clark et al., 2008). Vector representations of word meaning based on the distribution of words in large corpora do not yield explicit information about specific relations, but implicitly encode all kinds of associations between concepts. In contrast to manually constructed resources, their coverage is much larger. More specifically, they have been shown to encode information relevant in solving analogy tasks (Mikolov et al., 2013a; Levy and Goldberg, 2014b; Gladkova et al., 2016; G´abor et al., 2017; Linzen, 2016) and inferring semantic hierarchies (Fu et al., 2014; Pocostales, 2016). This indicates that the dimensions of distributional representations encode information about attributes of concepts (Levy and Goldberg, 2014b, p.177). For instance, in order to find the fourth component in the analogy man is to woman as king is to queen, a model has to detect the relation holding between the pairs in the analogy. In this example, the relations are formed by the two features of royalty and gender. One way of solving this is to use the vector offsets resulting from woman"
S18-1154,W05-1003,0,0.251487,"epts they apply to and thus appear in proximity to them. In a comparative set-up such as in this task, the attribute should be closer to the concept it applies to. In our second system, we attempt to exploit the operations used for solving analogies in order to determine whether an attribute distinguishes two concepts. forms lower in isolation, but performance is comparable to the first system in combination with the WordNet component. The main insights gained from our experiments are the following. First, despite the limited coverage of information on attributes in WordNet (as pointed out by Poesio and Almuhareb (2005)), the contribution of the WordNet component to the overall results indicates that definitions yield a valuable source of knowledge with respect to discriminative attributes. Second, we analyze how individual systems perform across different types of attributes. Our analysis shows that similarity performs best on general descriptive properties and WordNet definitions help most for finding specific properties. These observations indicate that more sophisticated methods of combining these components could lead to superior results in future work. The remainder of this paper is structured as follo"
S18-1154,P04-1048,0,0.060259,"ore from a cognitive (e.g. McRae et al. (2005)) and computational (e.g. Poesio and Almuhareb (2005)) perspective, this task is, to our knowledge, the first task aiming at detecting discriminative features. We use WordNet (Fellbaum, 1998) as a source of explicitly represented knowledge. Whereas the WordNet structure contains a vast amount of information about lexical relations (hyponymy, synonymy, meronymy), its definitions constitute a resource of world knowledge. WordNet definitions have been used successfully in approaches to word sense disambiguation (Lesk, 1986) and inferring verb frames (Green et al., 2004). The only 3 System description Each of our systems1 consists of a WordNet component and a component exploiting word embed1 Code can be found at https://github.com/ cltl/meaning_space 941 ding vectors. If the WordNet component is unable to classify an example, it is passed on to the word embedding component. After presenting the WordNet component, we describe the two embedding-based systems. The word vectors used in all approaches are taken from the Word2Vec Google News model (Mikolov et al., 2013a).2 The systems are developed using training and validation data and evaluated using test data.3"
van-son-etal-2014-hope,miltsakaki-etal-2004-penn,0,\N,Missing
van-son-etal-2014-hope,N07-2036,0,\N,Missing
van-son-etal-2014-hope,bartalesi-lenzi-etal-2012-cat,0,\N,Missing
W13-1202,P10-1143,0,0.0191435,"on of the conflict, which enables use cases that require the analysis of the conflict itself. 5 The GAF Annotation Framework This section explains the basic idea behind GAF by using texts on earthquakes in Indonesia. GAF provides a general model for event representation (including textual and extra-textual mentions) as well as exact representation of linguistic annotation or output of NLP tools. Simply put, GAF is the combination of textual analyses and formal semantic representations in RDF. 5.1 A SEM for earthquakes We selected newspaper texts on the January 2009 West Papua earthquakes from Bejan and Harabagiu (2010) to illustrate GAF. This choice was made because the topic “earthquake” illustrates the advantage of sharing URIs across domains. Gao and Hunter (2011) propose a Linked Data model to capture major geological events such as earthquakes, volcano activity and tsunamis. They combine information from different seismological databases with the intention to provide more complete information 15 to experts which may help to predict the occurrence of such events. The information can also be used in text interpretation. We can verify whether interpretations by NLP tools correspond to the data and relatio"
W13-1202,P11-1098,0,0.0122162,"Missing"
W13-1202,C96-1079,0,0.0462667,"ents, temporal relations, etc.) are represented explicitly in the semantic layer. The remainder of this paper is structured as follows. In Section 2, we present related work and explain the motivation behind our approach. Section 3 describes the in-text annotation approach. Our semantic annotation layer is presented in Section 4. Sections 5-7 present GAF through a use case on earthquakes in Indonesia. This is followed by our conclusions and future work in section 8. 2 Motivation and Background Annotation of events and of relations between them has a long tradition in NLP. The MUC conferences (Grishman and Sundheim, 1996) in the 90s did not explicitly annotate events and coreference relations, but the templates used for evaluating the information extraction tasks indirectly can be seen as annotation of events represented in newswires. Such events are not ordered in time or further related to each other. In response, Setzer and Gaizauskas (2000) describe an annotation framework to create coherent temporal orderings of events represented in documents using closure rules. They suggest that reasoning with text independent models, such as a calendar, helps annotating textual representations. More recently, generic"
W13-1202,P12-2045,0,0.0756526,"Missing"
W13-1202,J05-1004,0,0.020106,"tly annotate events and coreference relations, but the templates used for evaluating the information extraction tasks indirectly can be seen as annotation of events represented in newswires. Such events are not ordered in time or further related to each other. In response, Setzer and Gaizauskas (2000) describe an annotation framework to create coherent temporal orderings of events represented in documents using closure rules. They suggest that reasoning with text independent models, such as a calendar, helps annotating textual representations. More recently, generic corpora, such as Propbank (Palmer et al., 2005) and the Framenet corpus (Baker et al., 2003) have been built according to linguistic principles. The annotations aim at properly representing verb structures within a sentence context, focusing on verb arguments, semantic roles and other elements. In ACE 2004 (Linguistic Data Consortium, 2004b), event detection and linking is included as a pilot task for the first time, inspired by annotation schemes developed for named entities. They distinguish between event mentions and the trigger event, which is the mention that most clearly expresses its occurrence (Linguistic Data Consortium, 2004a). T"
W13-1202,pustejovsky-etal-2010-iso,0,0.0205156,"between instances and instance mentions avoiding the problem of determining a trigger event. Additionally, it facilitates the integration of information from extra-textual sources and information that can be inferred from texts, but is not explicitly mentioned. Sections 5 to 7 will explain how we can achieve this with GAF. 3 The TERENCE annotation format The TERENCE Annotation Format (TAF) is defined within the TERENCE Project1 with the goal to include event mentions, temporal expressions and participant mentions in a single annotation protocol (Moens et al., 2011). TAF is based on ISOTimeML (Pustejovsky et al., 2010), but introduces several adaptations in order to fit the domain of children’s stories for which it was originally developed. The format has been used to annotate around 30 children stories in Italian and 10 in English. We selected TAF as the basis for our in-text annotation for three reasons. First, it incorporates the (in our opinion crucial) distinction between instances and instance mentions. Second, it adapts some consolidated paradigms for linguistic annotation such as TimeML for events and temporal expressions and ACE for participants and participant mentions (Linguistic Data Consortium,"
W13-1202,setzer-gaizauskas-2000-annotating,0,0.0651309,"4. Sections 5-7 present GAF through a use case on earthquakes in Indonesia. This is followed by our conclusions and future work in section 8. 2 Motivation and Background Annotation of events and of relations between them has a long tradition in NLP. The MUC conferences (Grishman and Sundheim, 1996) in the 90s did not explicitly annotate events and coreference relations, but the templates used for evaluating the information extraction tasks indirectly can be seen as annotation of events represented in newswires. Such events are not ordered in time or further related to each other. In response, Setzer and Gaizauskas (2000) describe an annotation framework to create coherent temporal orderings of events represented in documents using closure rules. They suggest that reasoning with text independent models, such as a calendar, helps annotating textual representations. More recently, generic corpora, such as Propbank (Palmer et al., 2005) and the Framenet corpus (Baker et al., 2003) have been built according to linguistic principles. The annotations aim at properly representing verb structures within a sentence context, focusing on verb arguments, semantic roles and other elements. In ACE 2004 (Linguistic Data Cons"
W13-1202,W11-0116,0,\N,Missing
W13-1202,bartalesi-lenzi-etal-2012-cat,1,\N,Missing
W16-2819,bartalesi-lenzi-etal-2012-cat,0,0.0180456,". 169 sentences (n) 150 Round 1 139 91 100 Round 2 83 42 47 50 42 43 32 14 0 1 Task 1: Pilot annotation 2 3 4 annotators (n) 5 Figure 2: Distribution of annotations. This section reports on a pilot annotation experiment targeted at the first subtask. Five expert annotators were asked to identify those sentences in the editorial article that were COMMENTED UPON in the comment. A set of eight editorial articles (152 unique sentences, including titles) and a total of 62 comments were provided. In total, this came down to 1,186 sentences to be annotated. We used the Content Annotation Tool (CAT) (Lenzi et al., 2012) for the annotations. The experiment was performed in two rounds. First, simple instructions were given to the annotators to explore the data and task. For the second round, the instructions were refined by adding two simple rules: exclude titles (they are part of the meta-data), and include cases where a proposition is simply ‘mentioned’ rather than functioning as part of the argumentation. For example, the fact that the closing of Sweet Briar College is repeated in the comment below without its factual status beA deeper analysis of the annotated data and the annotation distribution shows the"
W16-2819,W10-0214,0,0.0249339,"itly mentioned or at least supposed opponent, as for instance in the rebutting of possible objections.” Therefore, these (implicit) interactions between participants should be given a central role when performing argument mining. In recent years, several studies have addressed the annotation and automatic classification of agreement and disagreement in online debates. The main difference between them is the annotation unit they have targeted, i.e. the textual units that are in (dis)agreement. Some studies focused on global (dis)agreement, i.e. the overall stance towards the main debate topic (Somasundaran and Wiebe, 2010). Other studies focused on local (dis)agreement, comparing pairs of posts (Walker 1 http://argmining2016.arg.tech/index. php/home/call-for-papers/unshared-task 160 Proceedings of the 3rd Workshop on Argument Mining, pages 160–165, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics ernment to reduce teen pregnancies?) and article title describing the author’s stance (e.g. Publicly Funded Birth Control Is Crucial); and ii) Discussions (i.e. collections of comments from different users) about these editorial articles (Variant D). The remainder of this paper is st"
W16-2819,L16-1187,1,0.846655,"Missing"
W16-2819,walker-etal-2012-corpus,0,0.0611907,"Missing"
W16-2819,W14-2617,0,0.0324304,"Missing"
W16-2819,W12-3710,0,0.0423338,"Missing"
W16-2819,andreas-etal-2012-annotating,0,\N,Missing
W17-4207,W13-1202,1,0.894307,"Missing"
W17-4207,kingsbury-palmer-2002-treebank,0,0.112657,"l., 2012) visualizes news stories as theme rivers. It also has a network visualization of actor-actor relations. This can be used when the corpus consists of e-mails, to show who writes about what to whom. In StoryTeller, the relations are not based on metadata but are relations in the domain of discourse extracted from text. 3 The following dimensions from the NewsReader data are used for the visualization. Event refers to the SEM-RDF ID: the instance identifier. The actors in the news article, which are described using role labels that come from different event ontologies, such as PropBank (Kingsbury and Palmer, 2002), FrameNet (Baker et al., 1998), and ESO (Segers et al., 2015). A climax score indicating the relevance of the event (normalized between 0 and 100) for a story. The climax score is a normalized score based on the number of mentions of an event and the prominence of each mention, where early mentions count as more prominent. A group label that uniquely identifies the event-group to which the event belongs. In NewsReader, groups are formed by connecting events by topic overlap of the articles in which they are mentioned and by sharing the same actors. Each group also has a groupScore which indic"
W17-4207,W15-4507,1,0.921873,"k (GAF, (Fokkens et al., 2013)) as an extension to SEM through gaf:denotedBy relations between instance representations in SEM and their mentions represented as offsets to different places in the texts. Likewise, information that is the same across mentions in different news articles gets deduplicated and information that is different gets aggregated. For each piece of information, the system stores the source and the perspective of the source on the information. The result is a complex multidimensional data set in which events and their relations are defined according to multiple properties (Vossen et al., 2015). The Storyteller application exploits these dimensions to present events within a context that explains them, approximating a story. ical data and emotions in Dutch 17th century theater plays in Section 5. Section 2 explains how our work differs from others. Section 6 concludes with future plans. 2 Related work Interactive graphics have been used for before to analyze high dimensional data (Buja et al., 1996; Martin and Ward, 1995; Buja et al., 1991), but fast, web-based and highly interactive visualizations with filtering are a fairly new development. With the advent of the open source libra"
W18-5430,W14-1618,0,0.0527188,"ere word embeddings have been shown to be able to predict missing components of analogies of the type A is to B as C is to D (Mikolov et al., 2013; Turney, 2012). Furthermore, most intrinsic evaluation methods take full vectors into consideration. The famous examples P aris − F rance + Italy ≈ Rome or king − man + woman ≈ queen evoke the suggestion that embeddings can capture semantic properties. The task has, however, been criticized substantially (Linzen, 2016; Gladkova and Drozd, 2016; Gladkova et al., 2016; Drozd et al., 2016, among others). Gladkova et al. (2016) follow an observation in Levy and Goldberg (2014) on the large differences in performance on different categories in the Google analogy set (Mikolov et al., 2013). They provide a new, more challenging, analogy dataset that improves existing sets on balance (capturing more semantic categories) and size. Linzen (2016) points out more fundamental problems including the observation that the target vector in the analogy task can often be found by simply taking the vector closest to the source. Drozd et al. (2016) show that classifiers picking out the target word from a set of related terms outperform the stanSeveral approaches have attempted to d"
W18-5430,W16-2503,0,0.0294572,"word embeddings has primarily focused on two main tasks: identifying general semantic relatedness or similarity and the so-called analogy task, where word embeddings have been shown to be able to predict missing components of analogies of the type A is to B as C is to D (Mikolov et al., 2013; Turney, 2012). Furthermore, most intrinsic evaluation methods take full vectors into consideration. The famous examples P aris − F rance + Italy ≈ Rome or king − man + woman ≈ queen evoke the suggestion that embeddings can capture semantic properties. The task has, however, been criticized substantially (Linzen, 2016; Gladkova and Drozd, 2016; Gladkova et al., 2016; Drozd et al., 2016, among others). Gladkova et al. (2016) follow an observation in Levy and Goldberg (2014) on the large differences in performance on different categories in the Google analogy set (Mikolov et al., 2013). They provide a new, more challenging, analogy dataset that improves existing sets on balance (capturing more semantic categories) and size. Linzen (2016) points out more fundamental problems including the observation that the target vector in the analogy task can often be found by simply taking the vector closest to the sourc"
W18-5430,C16-1332,0,0.220378,"fying general semantic relatedness or similarity and the so-called analogy task, where word embeddings have been shown to be able to predict missing components of analogies of the type A is to B as C is to D (Mikolov et al., 2013; Turney, 2012). Furthermore, most intrinsic evaluation methods take full vectors into consideration. The famous examples P aris − F rance + Italy ≈ Rome or king − man + woman ≈ queen evoke the suggestion that embeddings can capture semantic properties. The task has, however, been criticized substantially (Linzen, 2016; Gladkova and Drozd, 2016; Gladkova et al., 2016; Drozd et al., 2016, among others). Gladkova et al. (2016) follow an observation in Levy and Goldberg (2014) on the large differences in performance on different categories in the Google analogy set (Mikolov et al., 2013). They provide a new, more challenging, analogy dataset that improves existing sets on balance (capturing more semantic categories) and size. Linzen (2016) points out more fundamental problems including the observation that the target vector in the analogy task can often be found by simply taking the vector closest to the source. Drozd et al. (2016) show that classifiers picking out the target w"
W18-5430,W15-0107,0,0.276182,"decompose distributional vectors into individual linguistic aspects by means of a supervised classification approach to test which linguistic phenomena are captured by embeddings. They test their approach on an artificially created corpus and do not provide insights into specific semantic knowledge. Faruqui et al. (2015) transform learned embedding matrices into sparse matrices to make them more interpretable, which is complementary to our approach. Previous studies provide (indicative) support for the hypothesis that embeddings lack information people get from other modalities than language. Fagarasan et al. (2015) present a method to ground embedding models in perceptual information by mapping distributional spaces to semantic spaces consisting of feature norms. Several approaches to boosting distributional models with visual information show that the additional information improves the performance of word embedding vectors (Roller and Schulte im Walde, 2013; Lazaridou et al., 2014). Whereas this indicates that word embedding models lack visual information, it does not show to what extent different types of properties are encoded. The method proposed in this paper is, to the best of our knowledge, the"
W18-5430,N13-1090,0,0.816655,"in NLP and have been shown to boost performance in a large selection of tasks ranging from morphological analysis to sentiment analysis (Lazaridou et al., 2013; Socher et al., 2013; Zhou and Xu, 2015, among many others). Despite a number of different approaches to evaluation, our understanding of what type of information is represented by the vectors remains limited. Most approaches focus on full-vector comparison which treat vectors as points in a space (Yaghoobzadeh and Sch¨utze, 2016), which are evaluated by performance on semantic similarity or relatedness test sets and analogy questions (Mikolov et al., 2013; Turney, 2012). Previous work, however, has shown that high performance does not necessarily mean that 1 The hypotheses and code for our experiments can 276 Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 276–286 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics mantic properties that are relevant for the way entities interact with the world are well represented (e.g. functions of objects, activities entities are frequently involved in), whereas properties of relatively little consequence for the"
W18-5430,S17-1017,0,0.0165659,"ll outcome shows that the method and data are complementary to existing intrinsic evaluation methods. The rest of this paper is structured as follows. We discuss related work in Section 2. Our method is outlined in Section 3. Section 4 presents our experiments and results. We finish with a critical discussion and overview of future work in Section 5. 2 dardly applied cosine addition or multiplication methods. Though also boosted by the aforementioned proximity bias, these results indicate that standard methods of solving analogies miss information that is captured by embeddings. Rogers et al. (2017) conclude that the analogy evaluation does not reveal if word embedding representations indeed capture specific semantic properties. On top of that, an embedding may capture specific semantic properties in ways that are not analogous to semantic properties of related categories. Analogy methods assume that semantic properties stand in analogous relation to each other based on the information provided by the context, but there is no reason why (e.g.) things made of wood and things made of plastic result in (combinations of) embedding dimensions that are similar enough to stand in a parallel rel"
W18-5430,P15-1144,0,0.142057,"d negative examples that are closely related in semantic space. Ultimately, we want to verify and increase the extrinsic evaluations than standardly used intrinsic evaluations, they do not provide insights into what kind of semantic information is represented well. Yaghoobzadeh and Sch¨utze (2016) decompose distributional vectors into individual linguistic aspects by means of a supervised classification approach to test which linguistic phenomena are captured by embeddings. They test their approach on an artificially created corpus and do not provide insights into specific semantic knowledge. Faruqui et al. (2015) transform learned embedding matrices into sparse matrices to make them more interpretable, which is complementary to our approach. Previous studies provide (indicative) support for the hypothesis that embeddings lack information people get from other modalities than language. Fagarasan et al. (2015) present a method to ground embedding models in perceptual information by mapping distributional spaces to semantic spaces consisting of feature norms. Several approaches to boosting distributional models with visual information show that the additional information improves the performance of word"
W18-5430,W16-2507,0,0.0337076,"s has primarily focused on two main tasks: identifying general semantic relatedness or similarity and the so-called analogy task, where word embeddings have been shown to be able to predict missing components of analogies of the type A is to B as C is to D (Mikolov et al., 2013; Turney, 2012). Furthermore, most intrinsic evaluation methods take full vectors into consideration. The famous examples P aris − F rance + Italy ≈ Rome or king − man + woman ≈ queen evoke the suggestion that embeddings can capture semantic properties. The task has, however, been criticized substantially (Linzen, 2016; Gladkova and Drozd, 2016; Gladkova et al., 2016; Drozd et al., 2016, among others). Gladkova et al. (2016) follow an observation in Levy and Goldberg (2014) on the large differences in performance on different categories in the Google analogy set (Mikolov et al., 2013). They provide a new, more challenging, analogy dataset that improves existing sets on balance (capturing more semantic categories) and size. Linzen (2016) points out more fundamental problems including the observation that the target vector in the analogy task can often be found by simply taking the vector closest to the source. Drozd et al. (2016) sho"
W18-5430,D13-1115,0,0.0318164,"Missing"
W18-5430,N16-2002,0,0.0340713,"two main tasks: identifying general semantic relatedness or similarity and the so-called analogy task, where word embeddings have been shown to be able to predict missing components of analogies of the type A is to B as C is to D (Mikolov et al., 2013; Turney, 2012). Furthermore, most intrinsic evaluation methods take full vectors into consideration. The famous examples P aris − F rance + Italy ≈ Rome or king − man + woman ≈ queen evoke the suggestion that embeddings can capture semantic properties. The task has, however, been criticized substantially (Linzen, 2016; Gladkova and Drozd, 2016; Gladkova et al., 2016; Drozd et al., 2016, among others). Gladkova et al. (2016) follow an observation in Levy and Goldberg (2014) on the large differences in performance on different categories in the Google analogy set (Mikolov et al., 2013). They provide a new, more challenging, analogy dataset that improves existing sets on balance (capturing more semantic categories) and size. Linzen (2016) points out more fundamental problems including the observation that the target vector in the analogy task can often be found by simply taking the vector closest to the source. Drozd et al. (2016) show that classifiers pick"
W18-5430,D13-1170,0,0.0027552,"annot be identified by either method are not. Our results provide an initial indication that semantic properties relevant for the way entities interact (e.g. dangerous) are captured, while perceptual information (e.g. colors) is not represented. We conclude that, though preliminary, these results show that our method is suitable for identifying which properties are captured by embeddings. 1 Introduction Word embeddings are widely used in NLP and have been shown to boost performance in a large selection of tasks ranging from morphological analysis to sentiment analysis (Lazaridou et al., 2013; Socher et al., 2013; Zhou and Xu, 2015, among many others). Despite a number of different approaches to evaluation, our understanding of what type of information is represented by the vectors remains limited. Most approaches focus on full-vector comparison which treat vectors as points in a space (Yaghoobzadeh and Sch¨utze, 2016), which are evaluated by performance on semantic similarity or relatedness test sets and analogy questions (Mikolov et al., 2013; Turney, 2012). Previous work, however, has shown that high performance does not necessarily mean that 1 The hypotheses and code for our experiments can 276 Pr"
W18-5430,W16-2520,0,0.120466,"Missing"
W18-5430,S18-1117,0,0.0184944,"n possible in this preliminary study. Fourth, the experiments presented here only concern a small subsection of semantic properties too limited to draw general conclusions. Despite these limitations, our results provide preliminary insights that lead us to conclude that the overall idea behind our methods works and opens up promising directions for future work. We first aim to address the limitations of the current dataset. We intend to incorporate other sets designed for similar insights, such as the analogies presented in Drozd et al. (2016) and the SemEval 2018 discriminative property set (Krebs et al., 2018). In addition, we plan to extend and refine the sets with crowd annotations asking for graded judgments (e.g. a property can mostly or possibly apply) and exploit these judgments in future experiments. Once we created a bigger and more balanced dataset, we can carry out experiments on different train and test splits in order to overcome the limitations of the leave-one-out evaluation. Furthermore, we will apply careful parameter tuning on a development set in order to ensure our results are representative of the information captured by the embeddings. The increased size of the set will allow u"
W18-5430,D15-1243,0,0.062443,"ow the idea behind this approach and make a human-elicited property dataset that is created in the same tradition, but larger. Our approach goes beyond the previous work in two ways: first, we add gold negative examples which allows us to go beyond testing for salient properties. Second, we compare full vector proximity to the outcome of a classifier which allows us to verify whether the property is captured for entities that share the property, but are not similar otherwise. A few other studies go beyond full vector comparisons, moving towards the interpretation of word embedding dimensions. Tsvetkov et al. (2015, 2016) evaluate word embeddings by measuring the correlation between word embedding vectors and count vectors representing co-occurrences of words with WordNet supersenses. While they show that their results have a higher correlation with results obtained from be found at: https://cltl.github.io/semantic_ space_navigation 277 lows the tradition of the sets created by McRae et al. (2005); Vinson and Vigliocco (2008) and used in Kelly et al. (2014); Baroni et al. (2010); Barbu (2008) and is the largest available semantic property dataset we are aware of. In the collection process, human subject"
W18-5430,P14-1132,0,0.0261688,"ices to make them more interpretable, which is complementary to our approach. Previous studies provide (indicative) support for the hypothesis that embeddings lack information people get from other modalities than language. Fagarasan et al. (2015) present a method to ground embedding models in perceptual information by mapping distributional spaces to semantic spaces consisting of feature norms. Several approaches to boosting distributional models with visual information show that the additional information improves the performance of word embedding vectors (Roller and Schulte im Walde, 2013; Lazaridou et al., 2014). Whereas this indicates that word embedding models lack visual information, it does not show to what extent different types of properties are encoded. The method proposed in this paper is, to the best of our knowledge, the first approach specifically designed to identify what semantic knowledge is captured in word embeddings. We are not aware of earlier work that provides explicit hypotheses about the kind of information we expect to learn from distributional vectors, making this the first attempt to confirm these hypotheses experimentally. 3 Method The core of our evaluation consists of test"
W18-5430,D13-1196,0,0.0310892,"dings. Properties that cannot be identified by either method are not. Our results provide an initial indication that semantic properties relevant for the way entities interact (e.g. dangerous) are captured, while perceptual information (e.g. colors) is not represented. We conclude that, though preliminary, these results show that our method is suitable for identifying which properties are captured by embeddings. 1 Introduction Word embeddings are widely used in NLP and have been shown to boost performance in a large selection of tasks ranging from morphological analysis to sentiment analysis (Lazaridou et al., 2013; Socher et al., 2013; Zhou and Xu, 2015, among many others). Despite a number of different approaches to evaluation, our understanding of what type of information is represented by the vectors remains limited. Most approaches focus on full-vector comparison which treat vectors as points in a space (Yaghoobzadeh and Sch¨utze, 2016), which are evaluated by performance on semantic similarity or relatedness test sets and analogy questions (Mikolov et al., 2013; Turney, 2012). Previous work, however, has shown that high performance does not necessarily mean that 1 The hypotheses and code for our e"
W18-5430,P15-1109,0,0.0138757,"y either method are not. Our results provide an initial indication that semantic properties relevant for the way entities interact (e.g. dangerous) are captured, while perceptual information (e.g. colors) is not represented. We conclude that, though preliminary, these results show that our method is suitable for identifying which properties are captured by embeddings. 1 Introduction Word embeddings are widely used in NLP and have been shown to boost performance in a large selection of tasks ranging from morphological analysis to sentiment analysis (Lazaridou et al., 2013; Socher et al., 2013; Zhou and Xu, 2015, among many others). Despite a number of different approaches to evaluation, our understanding of what type of information is represented by the vectors remains limited. Most approaches focus on full-vector comparison which treat vectors as points in a space (Yaghoobzadeh and Sch¨utze, 2016), which are evaluated by performance on semantic similarity or relatedness test sets and analogy questions (Mikolov et al., 2013; Turney, 2012). Previous work, however, has shown that high performance does not necessarily mean that 1 The hypotheses and code for our experiments can 276 Proceedings of the 20"
W19-4728,W15-3708,0,0.0340776,"ction component, used by e.g. Michel et al. (2011); Dubossarsky et al. (2015), is smaller and limited in genre but avoids unbalanced differences in genre across time periods. In addition to these model-specific caveats, the translation from (potentially complex) concepts to words which can be observed by a distributional model is not straight forward. Betti and van den Berg (2014) propose the use of conceptual models to study concept change in a clearly defined and somewhat formalized way. This notion is rarely treated explicitly in applications of diachronic embedding models. Studies such as Bjerva and Praet (2015) provide a start, but we are not aware of previous work that investigates a conceptual system consisting of several subconcepts and of a similar complexity to the use-case of “Racism” presented in this paper. To the best of our knowledge, this work is furthermore the first to investigate how arteficial components influence a digital humanities research question. The scope of this research is still limited to investigating the impact of different methods and random artefacts leaving questions concerning the underlying data to future work. 3 Use Case: The Concept of Racism The first step for stu"
W19-4728,W14-2517,0,0.315547,"nt models created from the same data yield different results, but also indicate that (i) using different model architectures, (ii) comparing different corpora and (iii) comparing to control words and relations can help to identify which results are solid and which may be due to artefacts. We propose guidelines to conduct similar studies, but also note that more work is needed to fully understand how we can distinguish artefacts from actual conceptual changes. 1 Introduction Distributional models have been used to detect shifts in meaning with various degrees of success (Hamilton et al., 2016; Kim et al., 2014; Kulkarni et al., 2015; Gulordava and Baroni, 2011, e.g.). Based on promising examples such as the shift of the word gay from meaning ‘carefree’ to ‘homosexual’, researchers in digital humanities have been inspired to explore the use of distributional semantic models for studying the more complex phenomenon of concept drift (Wohlgenannt et al., 2019; Orlikowski et al., 2018; Kenter et al., 2015; Kutuzov et al., 2016; Martinez-Ortiz et al., 2016, e.g.). In most cases, standard methods with high results on identifying known examples of semantic shift are adopted and applied to specific data and"
W19-4728,C18-1117,0,0.0311192,"ate a series of semantic spaces representative of specific time periods that can be compared. While earlier approaches relied on count-based semantic space models (Gulordava and Baroni, 2011), more recent approaches made use of prediction-based models and suggested different methods to make embedding representions comparable across time periods (Kim et al., 2014; Kulkarni et al., 2015; Hamilton et al., 2016). Nowadays, predictionbased models (the skip-gram and CBOW architectures in the word2vec toolkit (Mikolov et al., 2013) and Glove (Pennington et al., 2014) seem to be the dominant choices (Kutuzov et al., 2018). • We propsose ways of critically investigating apparent changes with respect to artefacts of the data and/or model. • We formulate recommendations for Digital Humanities studies that aim to use diachronic embeddings to study conceptual change. A number of studies warn about the reliability of distributional semantic models for detecting change. Dubossarsky et al. (2017) illustrate that it is not known what properties in the underlying corpora are emphasized by various models and that count-based models in particular are sensitive to frequency effects. Hellrich and Hahn (2016a) point out that"
W19-4728,D17-1118,0,0.624504,"n in the digital humanities community. It is, in fact, far from trivial to apply distributional semantic models to study a complex phenomenon such as concept drift in a methodologically sound manner. We distinguish three main challenges: First, distributional semantic models reflect the way words are used and not directly how concepts are perceived. This leads to the question of which words should be studied and how shifts in their meaning relate to the underlying concept. Second, the relation between data, frequency and information emphasized by different model types is not fully understood (Dubossarsky et al., 2017). Third, the semantic models resulting from neural network-inspired architectures as provided by (e.g.) word2vec (Mikolov et al., 2013) depend on random factors such as initialization and the order in which data is presented (Hellrich and Hahn, 2016a). If these challenges are not taken into account, researchers may end up publishing insights and results that are the result of artefacts in the data or models rather than valid observations on change. Existing research has shown that these variations exist, but we are not aware of previous work that explored their consequences in a typical digita"
W19-4728,Q15-1016,0,0.113477,"it can be seen that the number of shared neighbors between all three models ranges between 11 and 18 (Table 7). This means that as much as 14 out of 25 nearest neighbors vary depending on the three initializations, showing that drawing conclusions based on artefacts is Figure 3: Nearest neighbors of racial in 1900 in different models created with the ngrams-corpus. 4 In the experiments, equivalent part of speech (e.g. noun - noun) and number (e.g. plural - plural) have been chosen for investigating changes in word pairs. 5 To train these models, we used a modified version of the code used by Levy et al. (2015) allowing us to fix the initialization vectors. We preprocessed the corpus with our own scripts, which may be slightly different from the preprocessing used by Hamilton et al. (2016). 5.2 Control Words Observations that hold across different models can still be a result of a bias or artefact in the 228 1900 1950 1990 ‘naive’ data models control cultural ethnic stereotypes ethnic backgrounds discrimination discrimination segregation nn of racial indicate shift towards meta-discourse yes yes n.a ←→ yes yes yes races ←→ immigrants no partly no cultural ←→ superior no yes (SVD), data sparsity (PPM"
W19-4728,N13-1090,0,0.344521,"on such as concept drift in a methodologically sound manner. We distinguish three main challenges: First, distributional semantic models reflect the way words are used and not directly how concepts are perceived. This leads to the question of which words should be studied and how shifts in their meaning relate to the underlying concept. Second, the relation between data, frequency and information emphasized by different model types is not fully understood (Dubossarsky et al., 2017). Third, the semantic models resulting from neural network-inspired architectures as provided by (e.g.) word2vec (Mikolov et al., 2013) depend on random factors such as initialization and the order in which data is presented (Hellrich and Hahn, 2016a). If these challenges are not taken into account, researchers may end up publishing insights and results that are the result of artefacts in the data or models rather than valid observations on change. Existing research has shown that these variations exist, but we are not aware of previous work that explored their consequences in a typical digital humanities set-up, which does not just consider the most extreme changes or words in commonly used evaluation sets, but considers wor"
W19-4728,P14-1096,0,0.194141,"s created according to the state-of-the-art and test our hypotheses in Section 4. We then report additional experiments that verify the robustness of the naively obtained insights in Section 5. Section 6 provides a set of recommendations on how to increase the reliability of research using distributional models to study language change based on the outcome and previous work. We then conclude and discuss open challenges. 2 Researchers in other domains (mainly Digital Humanities, but also biomedical text minig (Yan and Zhu, 2018)) have embraced the promising initial results from studies such as Mitra et al. (2014) and Hamilton et al. (2016) without being aware of the pitfalls of these methods. This is particularly concerning, as these fields typically work with comparatively small datasets restricted to a specific domain (Wohlgenannt et al., 2019). For instance, Kenter et al. (2015) and Martinez-Ortiz et al. (2016) study conceptual change in a corpus of Dutch Newspapers collected by the Royal Dutch Libary. The same corpus is taken up by Orlikowski et al. (2018), who proposes a model of conceptual change using analogical relations between words. Kutuzov et al. (2016) extend the Background and Related Wo"
W19-4728,W18-4501,0,0.0609476,"s needed to fully understand how we can distinguish artefacts from actual conceptual changes. 1 Introduction Distributional models have been used to detect shifts in meaning with various degrees of success (Hamilton et al., 2016; Kim et al., 2014; Kulkarni et al., 2015; Gulordava and Baroni, 2011, e.g.). Based on promising examples such as the shift of the word gay from meaning ‘carefree’ to ‘homosexual’, researchers in digital humanities have been inspired to explore the use of distributional semantic models for studying the more complex phenomenon of concept drift (Wohlgenannt et al., 2019; Orlikowski et al., 2018; Kenter et al., 2015; Kutuzov et al., 2016; Martinez-Ortiz et al., 2016, e.g.). In most cases, standard methods with high results on identifying known examples of semantic shift are adopted and applied to specific data and use-cases. 223 Proceedings of the 1st International Workshop on Computational Approaches to Historical Language Change, pages 223–233 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics propose methods that take these risks into account when investigating conceptual change using word embeddings. We illustrate this through a use case of a concep"
W19-4728,D14-1162,0,0.0908095,"idea underlying diachronic distributional models is to create a series of semantic spaces representative of specific time periods that can be compared. While earlier approaches relied on count-based semantic space models (Gulordava and Baroni, 2011), more recent approaches made use of prediction-based models and suggested different methods to make embedding representions comparable across time periods (Kim et al., 2014; Kulkarni et al., 2015; Hamilton et al., 2016). Nowadays, predictionbased models (the skip-gram and CBOW architectures in the word2vec toolkit (Mikolov et al., 2013) and Glove (Pennington et al., 2014) seem to be the dominant choices (Kutuzov et al., 2018). • We propsose ways of critically investigating apparent changes with respect to artefacts of the data and/or model. • We formulate recommendations for Digital Humanities studies that aim to use diachronic embeddings to study conceptual change. A number of studies warn about the reliability of distributional semantic models for detecting change. Dubossarsky et al. (2017) illustrate that it is not known what properties in the underlying corpora are emphasized by various models and that count-based models in particular are sensitive to freq"
W19-6105,C16-1262,0,0.528528,"rs indeed hindered our ability to assess shift direction. For this reason, the two corpus-independent factors, polysemy and centrality, will be incorporated as features in the dataset, to be able to select more or less challenging entries and to assess the effect of these factors on the outcomes. Complementary to the findings above, several studies have demonstrated that noise is inherent to distributional approaches and stems from factors both computational - e.g. cross-temporal vector alignment (Dubossarsky et al., 2017) - and fundamental, by the mere variance found in natural text corpora (Hellrich and Hahn, 2016). Experimental validation was not the focus of this paper, but we would encourage follow-up work with more rigid experimental checks, including control conditions and non-aligned (e.g. see Dubossarsky et al. (2019)) or count-based vectors. Given the presence of noise, it is crucial to cross-check findings. HiT is unique in that it caters for this with multiple synonymous entries per target term. We have presented a number of ways to derive holistic, sense-level insights. Some aggregations were more promising than others. The term pair with the largest and most significant cosine trend often di"
W19-6105,W14-2517,0,0.120196,"bridge from previous document-based approaches (Blei and Lafferty, 2006; Wang and McCallum, 2006, e.g.) to the window-based models that are currently widely used. Gulordava and Baroni (2011) and Jatowt and Duh (2014) were among the first to use trends in similarity between distributional representations. The former detect change within single terms by tracing their self-similarity. The latter, like we do, interpret the change of a term by contrasting it with other terms. In recent work, the most common type of distributional models used to assess semantic shift are known as prediction models (Kim et al., 2014; Kulkarni et al., 2015; Hamilton et al., 2016, e.g). In this paper, we use embeddings that gave the best results in Hamilton et al. (2016) and are created through the skip-gram method included in word2vec (Mikolov et al., 2013). Until recently, semantic shifts were determined by comparing the distributional nearest neighbours of a term in one time period to its neighbours in another (see e.g. Tahmasebi et al. (2011)). However, such an inspection is difficult to carry out at scale, is not suited for disappearing senses - distant neighbours are hard to assess - and is prone to bias, especially"
W19-6105,P19-1044,0,0.0160028,"less challenging entries and to assess the effect of these factors on the outcomes. Complementary to the findings above, several studies have demonstrated that noise is inherent to distributional approaches and stems from factors both computational - e.g. cross-temporal vector alignment (Dubossarsky et al., 2017) - and fundamental, by the mere variance found in natural text corpora (Hellrich and Hahn, 2016). Experimental validation was not the focus of this paper, but we would encourage follow-up work with more rigid experimental checks, including control conditions and non-aligned (e.g. see Dubossarsky et al. (2019)) or count-based vectors. Given the presence of noise, it is crucial to cross-check findings. HiT is unique in that it caters for this with multiple synonymous entries per target term. We have presented a number of ways to derive holistic, sense-level insights. Some aggregations were more promising than others. The term pair with the largest and most significant cosine trend often displayed the predicted trend. However, averaging the vector representations of all synonyms did not sufficiently cancel out noise. A logical next step would be to exploit lexical factors for sense-level evaluations,"
W19-6105,D17-1118,0,0.143002,"e, which a robust model of sense change will need to handle. The analysis showed that these factors indeed hindered our ability to assess shift direction. For this reason, the two corpus-independent factors, polysemy and centrality, will be incorporated as features in the dataset, to be able to select more or less challenging entries and to assess the effect of these factors on the outcomes. Complementary to the findings above, several studies have demonstrated that noise is inherent to distributional approaches and stems from factors both computational - e.g. cross-temporal vector alignment (Dubossarsky et al., 2017) - and fundamental, by the mere variance found in natural text corpora (Hellrich and Hahn, 2016). Experimental validation was not the focus of this paper, but we would encourage follow-up work with more rigid experimental checks, including control conditions and non-aligned (e.g. see Dubossarsky et al. (2019)) or count-based vectors. Given the presence of noise, it is crucial to cross-check findings. HiT is unique in that it caters for this with multiple synonymous entries per target term. We have presented a number of ways to derive holistic, sense-level insights. Some aggregations were more"
W19-6105,C18-1117,0,0.143601,"Missing"
W19-6105,W11-2508,0,0.033115,"ical semantic change through distributional semantic models. Distributional models of meaning are motivated by the hypothesis that words with similar meanings will occur in similar contexts (Harris, 1954; Firth, 1957). Tahmasebi et al. (2011) and Mitra et al. (2014) induce clusters of terms that temporally co-occur in particular syntactic patterns, and (qualitatively or quantitatively) trace their development. Their approach forms a bridge from previous document-based approaches (Blei and Lafferty, 2006; Wang and McCallum, 2006, e.g.) to the window-based models that are currently widely used. Gulordava and Baroni (2011) and Jatowt and Duh (2014) were among the first to use trends in similarity between distributional representations. The former detect change within single terms by tracing their self-similarity. The latter, like we do, interpret the change of a term by contrasting it with other terms. In recent work, the most common type of distributional models used to assess semantic shift are known as prediction models (Kim et al., 2014; Kulkarni et al., 2015; Hamilton et al., 2016, e.g). In this paper, we use embeddings that gave the best results in Hamilton et al. (2016) and are created through the skip-g"
W19-6105,P16-1141,0,0.375285,"sets, as a key challenge for this line of research (Tang, 2018; Kutuzov et al., 2018; Tahmasebi et al., 2018). In this paper, we automatically derive HiT, the largest English evaluation set to date, from a historical thesaurus. HiT consists of terms linked to WordNet (Fellbaum, 2012) entries that represent senses they gained or lost. We introduce sense shift assessment as a task, enabled by this dataset, that identifies whether a sense of a term of interest was coming in our out of use, based on its changed relationship with all lemmas of the sense. This is a variation of a task introduced by Hamilton et al. (2016) that assesses the relationship of the terms of interest with individual other terms. The sense shift assessment instead uncovers the conceptual change that explains multiple observed trends between word pairs. Cross-checking and summarising individual observations also means drawing more informed conclusions. Furthermore, the use of WordNet sense representations allows for the dataset entries to be automatically derived rather than manually (expert) collected, hence limiting the effect of bias. We use HiT to answer two main research questions. First, how well can current methods detect sense"
W19-6105,W16-4005,0,0.0567594,"Missing"
W19-6105,P14-1096,0,0.0307573,", consisting of 756 target words and 3624 word pairs. Second, we show that current methods perform quite poorly on this more challenging set, thus confirming that this set introduces a new benchmark. We also identify lexical factors that contribute to these differences. 2 Related Work This section provides an overview of previous work on detecting lexical semantic change through distributional semantic models. Distributional models of meaning are motivated by the hypothesis that words with similar meanings will occur in similar contexts (Harris, 1954; Firth, 1957). Tahmasebi et al. (2011) and Mitra et al. (2014) induce clusters of terms that temporally co-occur in particular syntactic patterns, and (qualitatively or quantitatively) trace their development. Their approach forms a bridge from previous document-based approaches (Blei and Lafferty, 2006; Wang and McCallum, 2006, e.g.) to the window-based models that are currently widely used. Gulordava and Baroni (2011) and Jatowt and Duh (2014) were among the first to use trends in similarity between distributional representations. The former detect change within single terms by tracing their self-similarity. The latter, like we do, interpret the change"
W19-6105,W18-4501,0,0.0292113,"Missing"
Y11-1025,W02-1502,1,0.874761,"eaning, but also those that exist only to express generalizations over their subtypes. We use these techniques to explore the degree of redundancy in a range of DELPH - IN1 grammars, including the two grammars of Wambaya (Bender, 2010), the BURGER grammar of Bulgarian (Osenova, 2010), the ManGO grammar2 of Mandarin Chinese, all built with the LinGO 1 2 Copyright 2011 by Antske Fokkens, Yi Zhang and Emily M. Bender http://www.delph-in.net/ http://wiki.delph-in.net/moin/MandarinGrammarOnline 25th Pacific Asia Conference on Language, Information and Computation, pages 236–244 236 Grammar Matrix (Bender et al., 2002; Bender et al., 2010), and two much larger grammars, the English Resource Grammar (Flickinger, 2000) and German Grammar (M¨uller and Kasper, 2000; Crysmann, 2005). This paper is structured as follows: First, we describe the overall structure of the grammars under consideration. This section is followed by an overview of the first approach under examination: removing superfluous types from the grammar. Section 4 provides the details of our second investigation of relevant types: maximally reduced computationally equivalent grammars. The next section presents our quantitative results and their"
